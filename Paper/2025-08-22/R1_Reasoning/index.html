<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-08-22  MedReseacher-R1 Expert-Level Medical Deep Researcher via A   Knowledge-Informed Trajectory Synthesis Framework">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-e93679d38feb0e25a65e0ea77deb8081.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    21.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    90 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-22-更新"><a href="#2025-08-22-更新" class="headerlink" title="2025-08-22 更新"></a>2025-08-22 更新</h1><h2 id="MedReseacher-R1-Expert-Level-Medical-Deep-Researcher-via-A-Knowledge-Informed-Trajectory-Synthesis-Framework"><a href="#MedReseacher-R1-Expert-Level-Medical-Deep-Researcher-via-A-Knowledge-Informed-Trajectory-Synthesis-Framework" class="headerlink" title="MedReseacher-R1: Expert-Level Medical Deep Researcher via A   Knowledge-Informed Trajectory Synthesis Framework"></a>MedReseacher-R1: Expert-Level Medical Deep Researcher via A   Knowledge-Informed Trajectory Synthesis Framework</h2><p><strong>Authors:Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, Hansong Xiao, Hualei Zhou, Chunxiao Guo, Peng Wei, Jinjie Gu</strong></p>
<p>Recent developments in Large Language Model (LLM)-based agents have shown impressive capabilities spanning multiple domains, exemplified by deep research systems that demonstrate superior performance on complex information-seeking and synthesis tasks. While general-purpose deep research agents have shown impressive capabilities, they struggle significantly with medical domain challenges, as evidenced by leading proprietary systems achieving limited accuracy on complex medical benchmarks. The key limitations are: (1) the model lacks sufficient dense medical knowledge for clinical reasoning, and (2) the framework is constrained by the absence of specialized retrieval tools tailored for medical contexts.We present a medical deep research agent that addresses these challenges through two core innovations. First, we develop a novel data synthesis framework using medical knowledge graphs, extracting the longest chains from subgraphs around rare medical entities to generate complex multi-hop question-answer pairs. Second, we integrate a custom-built private medical retrieval engine alongside general-purpose tools, enabling accurate medical information synthesis. Our approach generates 2100+ diverse trajectories across 12 medical specialties, each averaging 4.2 tool interactions.Through a two-stage training paradigm combining supervised fine-tuning and online reinforcement learning with composite rewards, our MedResearcher-R1-32B model demonstrates exceptional performance, establishing new state-of-the-art results on medical benchmarks while maintaining competitive performance on general deep research tasks. Our work demonstrates that strategic domain-specific innovations in architecture, tool design, and training data construction can enable smaller open-source models to outperform much larger proprietary systems in specialized domains. </p>
<blockquote>
<p>最近基于大型语言模型（LLM）的代理人的发展显示出令人印象深刻的跨多个领域的能力，以深度研究系统为例，它们在复杂的信息检索和综合任务上表现出卓越的性能。虽然通用深度研究代理人在许多领域表现出强大的能力，但他们在医学领域面临巨大挑战，领先的专有系统在最先进的医学基准测试上的准确率有限，证明了这一点。主要限制因素有：（1）模型缺乏用于临床推理的充足密集医学知识；（2）框架受到缺乏针对医学上下文定制的专用检索工具的制约。我们提出了一种医学深度研究代理人，通过两个核心创新来解决这些挑战。首先，我们利用医学知识图谱开发了一种新型数据合成框架，从围绕罕见医学实体的子图中提取最长的链来生成复杂的多跳问答对。其次，我们将自定义的私有医学检索引擎与通用工具集成在一起，以实现准确医学信息的综合。我们的方法在12个医学专业领域生成了超过2100条不同的轨迹，每个轨迹平均交互4.2个工具。通过结合监督微调的两阶段训练模式和在线强化学习以及复合奖励，我们的MedResearcher-R1-32B模型在医学基准测试上取得了卓越的性能，建立了新的最新结果，同时在一般深度研究任务上保持竞争力。我们的工作表明，在架构、工具设计和训练数据构建方面的战略领域特定创新，可以使较小的开源模型在专门领域中超越更大的专有系统。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14880v1">PDF</a> 13 pages, 5 figures</p>
<p><strong>Summary</strong>：近期大型语言模型（LLM）在多个领域展现出强大的能力，但在医疗领域面临挑战。通过创新的数据合成框架和定制医疗检索引擎，我们提出了一种医疗深度研究代理人来解决这些问题。该代理人生成了超过2100条跨越12个医学专业的不同轨迹，并通过结合监督微调与在线强化学习的两阶段训练模式，实现了在医疗基准测试上的卓越表现。</p>
<p><strong>Key Takeaways</strong>：</p>
<ul>
<li>LLM在多个领域表现出强大的能力，但在医疗领域面临挑战。</li>
<li>医疗深度研究代理人通过数据合成框架和定制医疗检索引擎解决这些问题。</li>
<li>代理人生成了超过2100条医学专业的不同轨迹。</li>
<li>该代理人结合监督微调与在线强化学习进行两阶段训练。</li>
<li>MedResearcher-R1-32B模型在医疗基准测试上实现卓越表现，同时保持对一般深度研究任务的竞争力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14880">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-145333c4ecb400359c97d17f875ee0b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1601c553312993feb55ef505b074d85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e93679d38feb0e25a65e0ea77deb8081.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cff4303881eefc0dbb6dd240bfb6871.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Universal-and-Transferable-Adversarial-Attack-on-Large-Language-Models-Using-Exponentiated-Gradient-Descent"><a href="#Universal-and-Transferable-Adversarial-Attack-on-Large-Language-Models-Using-Exponentiated-Gradient-Descent" class="headerlink" title="Universal and Transferable Adversarial Attack on Large Language Models   Using Exponentiated Gradient Descent"></a>Universal and Transferable Adversarial Attack on Large Language Models   Using Exponentiated Gradient Descent</h2><p><strong>Authors:Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, Xiuwen Liu</strong></p>
<p>As large language models (LLMs) are increasingly deployed in critical applications, ensuring their robustness and safety alignment remains a major challenge. Despite the overall success of alignment techniques such as reinforcement learning from human feedback (RLHF) on typical prompts, LLMs remain vulnerable to jailbreak attacks enabled by crafted adversarial triggers appended to user prompts. Most existing jailbreak methods either rely on inefficient searches over discrete token spaces or direct optimization of continuous embeddings. While continuous embeddings can be given directly to selected open-source models as input, doing so is not feasible for proprietary models. On the other hand, projecting these embeddings back into valid discrete tokens introduces additional complexity and often reduces attack effectiveness. We propose an intrinsic optimization method which directly optimizes relaxed one-hot encodings of the adversarial suffix tokens using exponentiated gradient descent coupled with Bregman projection, ensuring that the optimized one-hot encoding of each token always remains within the probability simplex. We provide theoretical proof of convergence for our proposed method and implement an efficient algorithm that effectively jailbreaks several widely used LLMs. Our method achieves higher success rates and faster convergence compared to three state-of-the-art baselines, evaluated on five open-source LLMs and four adversarial behavior datasets curated for evaluating jailbreak methods. In addition to individual prompt attacks, we also generate universal adversarial suffixes effective across multiple prompts and demonstrate transferability of optimized suffixes to different LLMs. </p>
<blockquote>
<p>随着大型语言模型（LLM）在关键应用中的部署越来越多，确保其鲁棒性和安全对齐仍然是一个主要挑战。尽管强化学习从人类反馈（RLHF）等对齐技术在典型提示上的总体成功，但LLM仍容易受到通过用户提示附加的精心制作的对抗性触发而启动的越狱攻击（jailbreak attacks）的影响。现有的大多数越狱方法要么依赖于离散令牌空间上的低效搜索，要么直接优化连续嵌入。虽然可以将这些连续嵌入直接作为选定开源模型的输入，但对于专有模型这样做并不可行。另一方面，将这些嵌入投影回有效的离散令牌引入了额外的复杂性，并且往往会降低攻击的有效性。我们提出了一种内在的优化方法，该方法直接使用指数梯度下降法结合Bregman投影优化对抗性后缀令牌的松弛独热编码，确保每个令牌优化后的独热编码始终保持在概率单纯形内。我们为所提出的方法提供了收敛性的理论证明，并实现了一种有效的算法，该算法能够有效地突破几种广泛使用的LLM。与三种最先进的基线相比，我们的方法在五个开源LLM和四个用于评估越狱方法的对抗行为数据集上实现了更高的成功率和更快的收敛速度。除了单个提示攻击之外，我们还生成了跨多个提示有效的通用对抗性后缀，并证明了优化后缀在不同LLM之间的可迁移性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14853v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在关键应用中的部署日益增加，但其鲁棒性和安全对齐性仍然是一个重大挑战。尽管强化学习从人类反馈（RLHF）等对齐技术在典型提示上取得了总体成功，但LLMs仍容易遭受由特定恶意触发词触发的越狱攻击。本文提出了一种针对LLMs的内在优化方法，直接优化松弛的one-hot编码对抗后缀令牌，使用指数梯度下降与布雷格曼投影相结合的方法。该方法实现了较高的成功率并实现了快速收敛，与三种最先进的基线相比具有优势，并在五个开源LLMs和四个越狱方法评估的对抗行为数据集上进行了评估。除了针对个别提示的攻击外，我们还生成了有效的跨多个提示的通用对抗后缀，并证明了优化后缀在不同LLMs之间的可转移性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在关键应用中的部署面临鲁棒性和安全性的挑战。</li>
<li>尽管有强化学习从人类反馈（RLHF）等对齐技术，LLMs仍易受特定恶意触发词触发的越狱攻击。</li>
<li>现有越狱方法包括在离散令牌空间上的低效搜索或直接优化连续嵌入，但对于专有模型不可行。</li>
<li>本文提出了一种内在优化方法，直接优化松弛的one-hot编码对抗后缀令牌，使用指数梯度下降与布雷格曼投影结合。</li>
<li>该方法实现了较高的成功率并快速收敛，优于三种最先进的基线方法。</li>
<li>方法在五个开源LLMs和四个越狱方法评估的对抗行为数据集上进行了验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14853">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-03ece2ac5ecc22a45f090986029199a1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Evaluating-Retrieval-Augmented-Generation-vs-Long-Context-Input-for-Clinical-Reasoning-over-EHRs"><a href="#Evaluating-Retrieval-Augmented-Generation-vs-Long-Context-Input-for-Clinical-Reasoning-over-EHRs" class="headerlink" title="Evaluating Retrieval-Augmented Generation vs. Long-Context Input for   Clinical Reasoning over EHRs"></a>Evaluating Retrieval-Augmented Generation vs. Long-Context Input for   Clinical Reasoning over EHRs</h2><p><strong>Authors:Skatje Myers, Dmitriy Dligach, Timothy A. Miller, Samantha Barr, Yanjun Gao, Matthew Churpek, Anoop Mayampurath, Majid Afshar</strong></p>
<p>Electronic health records (EHRs) are long, noisy, and often redundant, posing a major challenge for the clinicians who must navigate them. Large language models (LLMs) offer a promising solution for extracting and reasoning over this unstructured text, but the length of clinical notes often exceeds even state-of-the-art models’ extended context windows. Retrieval-augmented generation (RAG) offers an alternative by retrieving task-relevant passages from across the entire EHR, potentially reducing the amount of required input tokens. In this work, we propose three clinical tasks designed to be replicable across health systems with minimal effort: 1) extracting imaging procedures, 2) generating timelines of antibiotic use, and 3) identifying key diagnoses. Using EHRs from actual hospitalized patients, we test three state-of-the-art LLMs with varying amounts of provided context, using either targeted text retrieval or the most recent clinical notes. We find that RAG closely matches or exceeds the performance of using recent notes, and approaches the performance of using the models’ full context while requiring drastically fewer input tokens. Our results suggest that RAG remains a competitive and efficient approach even as newer models become capable of handling increasingly longer amounts of text. </p>
<blockquote>
<p>电子健康记录（EHRs）内容冗长、繁杂，对临床医生来说是一大挑战。大型语言模型（LLMs）为解决这一问题提供了有效的解决方案，能够通过提取和推理这些非结构化文本来进行处理。然而，临床笔记的长度往往超出了最先进模型的扩展上下文窗口。通过从整个电子健康记录中检索任务相关段落，检索增强生成（RAG）提供了一种替代方案，可以潜在地减少所需输入标记的数量。在这项工作中，我们提出了三项可在健康系统中轻松复制的临床任务：1）提取成像程序，2）生成抗生素使用的时间线，以及3）识别关键诊断。我们使用来自实际住院患者的电子健康记录进行测试，测试了三种最先进的LLMs，它们提供的上下文信息各不相同，要么采用有针对性的文本检索，要么采用最新的临床笔记。我们发现，RAG的表现在使用最新笔记的情况下非常接近或超过其表现，并且在需要大大减少输入标记的情况下接近使用模型全语境的表现。我们的结果表明，即使在新模型能够处理越来越长的文本时，RAG仍然是一种具有竞争力和高效的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14817v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>电子健康记录（EHRs）内容冗长、信息繁杂，给临床医生带来很大挑战。大型语言模型（LLMs）为解决这一问题提供了可能方案，但临床笔记的长度常常超出这些模型的语境窗口限制。检索增强生成（RAG）方法通过从整个EHR中检索任务相关段落，可能减少所需输入标记的数量。本研究提出三项可在各医疗系统中轻松复制的临床任务：1）提取成像程序；2）生成抗生素使用的时间线；3）识别关键诊断。我们测试了三种先进的大型语言模型，通过不同语境量、针对性文本检索或最新临床笔记进行试验。结果发现，RAG的表现接近或超越了使用最新笔记的效果，且能在更少的输入标记下接近使用全语境模型的效果。这表明即使在处理更长的文本时，RAG仍然是一种有竞争力的有效方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>电子健康记录（EHRs）内容冗长、繁杂，给临床医生带来挑战。</li>
<li>大型语言模型（LLMs）可解决这一问题，但临床笔记长度超出模型语境窗口限制。</li>
<li>检索增强生成（RAG）方法通过检索任务相关段落减少输入标记需求。</li>
<li>三项临床任务包括提取成像程序、生成抗生素使用的时间线、识别关键诊断。</li>
<li>RAG表现接近或超越使用最新笔记的效果，且能在更少输入标记下接近全语境模型的效果。</li>
<li>RAG是一种有竞争力的处理长文本的有效方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14817">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-83dbe07bf1a698277068bd104136a117.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32bd56f80139dafae901b30bf9e82c84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbafa5f04d5e1123682364af991d1bf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49d1c0016de24c71db12663581fc3e7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6463600292bf3e6cffe9bab881ae1d63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-184f4ffb59e77c944b686be9be4d311f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e50ac77c394de771017b1d053eec8186.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TransLLM-A-Unified-Multi-Task-Foundation-Framework-for-Urban-Transportation-via-Learnable-Prompting"><a href="#TransLLM-A-Unified-Multi-Task-Foundation-Framework-for-Urban-Transportation-via-Learnable-Prompting" class="headerlink" title="TransLLM: A Unified Multi-Task Foundation Framework for Urban   Transportation via Learnable Prompting"></a>TransLLM: A Unified Multi-Task Foundation Framework for Urban   Transportation via Learnable Prompting</h2><p><strong>Authors:Jiaming Leng, Yunying Bi, Chuan Qin, Bing Yin, Yanyong Zhang, Chao Wang</strong></p>
<p>Urban transportation systems encounter diverse challenges across multiple tasks, such as traffic forecasting, electric vehicle (EV) charging demand prediction, and taxi dispatch. Existing approaches suffer from two key limitations: small-scale deep learning models are task-specific and data-hungry, limiting their generalizability across diverse scenarios, while large language models (LLMs), despite offering flexibility through natural language interfaces, struggle with structured spatiotemporal data and numerical reasoning in transportation domains. To address these limitations, we propose TransLLM, a unified foundation framework that integrates spatiotemporal modeling with large language models through learnable prompt composition. Our approach features a lightweight spatiotemporal encoder that captures complex dependencies via dilated temporal convolutions and dual-adjacency graph attention networks, seamlessly interfacing with LLMs through structured embeddings. A novel instance-level prompt routing mechanism, trained via reinforcement learning, dynamically personalizes prompts based on input characteristics, moving beyond fixed task-specific templates. The framework operates by encoding spatiotemporal patterns into contextual representations, dynamically composing personalized prompts to guide LLM reasoning, and projecting the resulting representations through specialized output layers to generate task-specific predictions. Experiments across seven datasets and three tasks demonstrate the exceptional effectiveness of TransLLM in both supervised and zero-shot settings. Compared to ten baseline models, it delivers competitive performance on both regression and planning problems, showing strong generalization and cross-task adaptability. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/BiYunying/TransLLM">https://github.com/BiYunying/TransLLM</a>. </p>
<blockquote>
<p>城市交通系统面临多种任务中的多样化挑战，如交通预测、电动汽车（EV）充电需求预测和出租车调度等。现有方法存在两个主要局限性：小规模深度学习模型是任务特定的且数据依赖性强，限制了它们在多种场景中的通用性；虽然大型语言模型（LLM）通过自然语言接口提供了灵活性，但在交通领域的结构化时空数据和数值推理方面却遇到了困难。为了解决这些局限性，我们提出了TransLLM，这是一个统一的基础框架，通过可学习的提示组合将时空建模与大型语言模型集成在一起。我们的方法采用轻量级的时空编码器，通过扩张的临时卷积和双重邻接图注意力网络捕捉复杂的依赖关系，与LLM无缝接口通过结构化嵌入。一种新型的实例级提示路由机制，通过强化学习进行训练，根据输入特征动态个性化提示，超越了固定的任务特定模板。该框架通过将时空模式编码为上下文表示，动态组合个性化提示以引导LLM推理，并通过专用输出层投影生成任务特定预测。在七个数据集和三个任务上的实验表明，TransLLM在监督学习和零样本设置中的表现都非常出色。与十个基准模型相比，它在回归和规划问题上均表现出竞争力，显示出强大的泛化和跨任务适应性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/BiYunying/TransLLM%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/BiYunying/TransLLM获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14782v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文针对城市交通系统面临的挑战，如交通预测、电动汽车充电需求预测和出租车调度等任务，提出了TransLLM框架。该框架结合时空建模与大型语言模型，通过可学习的提示组合进行统一处理。采用轻量级时空编码器，通过膨胀时间卷积和双邻接图注意力网络捕捉复杂依赖性，与大型语言模型无缝接口。通过强化学习训练的实例级提示路由机制，根据输入特征动态个性化提示，超越了固定任务特定模板。该框架在七个数据集和三个任务上的实验表明，其在有监督和零样本设置中具有出色的效果，与十种基线模型相比，在回归和规划问题上表现出强大的泛化和跨任务适应性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TransLLM是一个统一的框架，用于处理城市交通系统的多样挑战，如交通预测、电动汽车充电需求预测和出租车调度。</li>
<li>该框架结合了时空建模和大型语言模型（LLM），通过可学习的提示组合进行融合。</li>
<li>TransLLM使用轻量级时空编码器，能够捕捉复杂依赖性，并与LLM无缝对接。</li>
<li>采用强化学习训练的实例级提示路由机制，使框架能根据输入特征动态个性化提示。</li>
<li>框架在多个数据集和任务上展现出卓越效果，包括有监督和零样本设置。</li>
<li>TransLLM在回归和规划问题上表现出强大的泛化能力和跨任务适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14782">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-685171622748fd32dcbc1648e3fcaeba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-755b2642c499f73f93d942309e2db41a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-319c0e6ddb6cf30c1e7a49f45d8d3713.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1c85d6d70c90a4421fd965078a7a817.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Evaluating-Multilingual-and-Code-Switched-Alignment-in-LLMs-via-Synthetic-Natural-Language-Inference"><a href="#Evaluating-Multilingual-and-Code-Switched-Alignment-in-LLMs-via-Synthetic-Natural-Language-Inference" class="headerlink" title="Evaluating Multilingual and Code-Switched Alignment in LLMs via   Synthetic Natural Language Inference"></a>Evaluating Multilingual and Code-Switched Alignment in LLMs via   Synthetic Natural Language Inference</h2><p><strong>Authors:Samir Abdaljalil, Erchin Serpedin, Khalid Qaraqe, Hasan Kurban</strong></p>
<p>Large language models (LLMs) are increasingly applied in multilingual contexts, yet their capacity for consistent, logically grounded alignment across languages remains underexplored. We present a controlled evaluation framework for multilingual natural language inference (NLI) that generates synthetic, logic-based premise-hypothesis pairs and translates them into a typologically diverse set of languages. This design enables precise control over semantic relations and allows testing in both monolingual and mixed-language (code-switched) conditions. Surprisingly, code-switching does not degrade, and can even improve, performance, suggesting that translation-induced lexical variation may serve as a regularization signal. We validate semantic preservation through embedding-based similarity analyses and cross-lingual alignment visualizations, confirming the fidelity of translated pairs. Our findings expose both the potential and the brittleness of current LLM cross-lingual reasoning, and identify code-switching as a promising lever for improving multilingual robustness. Code available at: <a target="_blank" rel="noopener" href="https://github.com/KurbanIntelligenceLab/nli-stress-testing">https://github.com/KurbanIntelligenceLab/nli-stress-testing</a> </p>
<blockquote>
<p>大型语言模型（LLMs）在多语言环境中的应用越来越广泛，然而其在不同语言之间保持一致性、逻辑性的对齐能力仍被低估。我们提出了一种针对多语言自然语言推理（NLI）的控制评估框架，该框架生成基于逻辑的前提假设对，并将其翻译成类型多样的语言集。这种设计能够精确控制语义关系，并允许在单语和混合语言（代码切换）条件下进行测试。令人惊讶的是，代码切换并不会降低性能，甚至可能有所提高，这表明翻译引起的词汇变化可以作为正则化信号。我们通过基于嵌入的相似性分析和跨语言对齐可视化来验证语义的保留，证实了翻译对的保真性。我们的研究揭示了当前LLM跨语言推理的潜力和脆弱性，并将代码切换识别为提高多语言稳健性的有前景的方法。相关代码可在 <a target="_blank" rel="noopener" href="https://github.com/KurbanIntelligenceLab/nli-stress-testing">https://github.com/KurbanIntelligenceLab/nli-stress-testing</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14735v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>大型语言模型在多语言环境中的应用日益广泛，但其跨语言一致、逻辑严谨的对齐能力尚未得到充分探索。本研究提出一种控制评估框架，用于多语言自然语言推理，生成合成逻辑基础的前提假设对，并将其翻译成类型多样的语言集。该研究设计能精确控制语义关系，并可在单语和混合语言（语言转码）条件下进行测试。研究发现，语言转码并不会降低性能，甚至可能有所提升，这表明翻译引起的词汇变化可能作为一种正则化信号。通过嵌入相似性分析和跨语言对齐可视化验证了语义的保留，证实了翻译对的准确性。本研究揭示了当前语言模型跨语言推理的潜力和脆弱性，并确定了语言转码作为提高多语言稳健性的有力工具。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在多语言环境中的应用需求日益增长，但跨语言逻辑对齐能力尚待探索。</li>
<li>提出一种控制评估框架用于多语言自然语言推理，能生成并控制语义关系的合成数据。</li>
<li>发现语言转码不会降低性能，甚至可能提升模型表现，这暗示了翻译引起的词汇变化有正则化效果。</li>
<li>语义保留的验证通过嵌入相似性分析和跨语言对齐可视化得到确认。</li>
<li>研究揭示了当前语言模型跨语言推理的潜力与脆弱性。</li>
<li>语言转码被识别为提高多语言稳健性的有效方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14735">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6cf81e6d49a7795c75a7b96866051b91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-129375487d873b774c2af356fd64c8a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e94d2f70e80d68c07f2ccc991876ce4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2da2bf87a33006695a07d47a252ec39.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ShizhenGPT-Towards-Multimodal-LLMs-for-Traditional-Chinese-Medicine"><a href="#ShizhenGPT-Towards-Multimodal-LLMs-for-Traditional-Chinese-Medicine" class="headerlink" title="ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine"></a>ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine</h2><p><strong>Authors:Junying Chen, Zhenyang Cai, Zhiheng Liu, Yunjin Yang, Rongsheng Wang, Qingying Xiao, Xiangyi Feng, Zhan Su, Jing Guo, Xiang Wan, Guangjun Yu, Haizhou Li, Benyou Wang</strong></p>
<p>Despite the success of large language models (LLMs) in various domains, their potential in Traditional Chinese Medicine (TCM) remains largely underexplored due to two critical barriers: (1) the scarcity of high-quality TCM data and (2) the inherently multimodal nature of TCM diagnostics, which involve looking, listening, smelling, and pulse-taking. These sensory-rich modalities are beyond the scope of conventional LLMs. To address these challenges, we present ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and physiological signals. ShizhenGPT is pretrained and instruction-tuned to achieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect recent national TCM qualification exams and build a visual benchmark for Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that ShizhenGPT outperforms comparable-scale LLMs and competes with larger proprietary models. Moreover, it leads in TCM visual understanding among existing multimodal LLMs and demonstrates unified perception across modalities like sound, pulse, smell, and vision, paving the way toward holistic multimodal perception and diagnosis in TCM. Datasets, models, and code are publicly available. We hope this work will inspire further exploration in this field. </p>
<blockquote>
<p>尽管大型语言模型（LLM）在各个领域取得了成功，但由于两个关键障碍，它们在传统中医（TCM）领域的应用潜力尚未得到充分探索：（1）缺乏高质量的中医数据和（2）中医诊断的固有跨模态特性，涉及观察、聆听、闻诊和诊脉。这些感官丰富的模态超出了传统LLM的范围。为了应对这些挑战，我们推出了适用于中医的多模态LLM“时珍GPT”。为解决数据稀缺问题，我们整理了迄今为止最大的中医数据集，包含超过100GB的文本和超过200GB的多模态数据，包括120万张图像、200小时的音频和生理信号。时珍GPT经过预训练和执行指令微调，以实现深入的中医知识和多模态推理。为了进行评估，我们收集了最近的中医资格考试，并建立了药物识别和视觉诊断的视觉基准测试。实验表明，时珍GPT在同类规模的LLM中表现突出，并与更大的专有模型相竞争。此外，它在中医视觉理解方面领先于现有的多模态LLM，并展示了跨声音、脉象、气味和视觉等模态的统一感知能力，为中医的整体多模态感知和诊断铺平了道路。数据集、模型和代码均公开可用。我们希望这项工作将激发该领域的进一步探索。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14706v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>尽管大型语言模型（LLM）在众多领域取得了成功，它们在中医领域的应用潜力仍未被充分发掘。为解决高质量中医数据缺乏以及中医诊断本身的多模态特性所带来的挑战，研究团队提出了针对中医的多模态大型语言模型ShizhenGPT。ShizhenGPT通过预训练与指令微调，深入理解了中医知识并具备多模态推理能力。实验表明，ShizhenGPT在中医视觉理解方面表现出色，并展现出跨声音、脉象、气味和视觉等模态的统一感知能力，为中医的全方位多模态感知与诊断开辟了新的道路。该研究成果公开了数据集、模型和代码，希望进一步激发该领域的研究潜力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型在中医领域的应用仍处于探索阶段。</li>
<li>缺少高质量中医数据和中医诊断的多模态特性是两大挑战。</li>
<li>ShizhenGPT是一个针对中医领域的多模态大型语言模型。</li>
<li>ShizhenGPT通过预训练和指令微调达到深入理解中医知识和多模态推理的能力。</li>
<li>ShizhenGPT在中医视觉理解方面表现优秀，展现出跨多种模态的统一感知能力。</li>
<li>研究成果公开了数据集、模型和代码，促进进一步的研究和应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14706">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-47460fbb72445ff6077ce716a2e6e2e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e69877002d649150cb83d1634ef7ced3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06977a8a2e667eb13790fe426eed1bb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71fb63110c7981d1e1bf48cd4d757508.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65aa1d8909e397f2b6fd7dc06368436c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b663604c68d1ba6d3e82ef5a0ec1df1e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers"><a href="#MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers" class="headerlink" title="MCP-Universe: Benchmarking Large Language Models with Real-World Model   Context Protocol Servers"></a>MCP-Universe: Benchmarking Large Language Models with Real-World Model   Context Protocol Servers</h2><p><strong>Authors:Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, Junnan Li</strong></p>
<p>The Model Context Protocol has emerged as a transformative standard for connecting large language models to external data sources and tools, rapidly gaining adoption across major AI providers and development platforms. However, existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. Our benchmark encompasses 6 core domains spanning 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching. To ensure rigorous evaluation, we implement execution-based evaluators, including format evaluators for agent format compliance, static evaluators for time-invariant content matching, and dynamic evaluators that automatically retrieve real-time ground truth for temporally sensitive tasks. Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. In addition, our benchmark poses a significant long-context challenge for LLM agents, as the number of input tokens increases rapidly with the number of interaction steps. Moreover, it introduces an unknown-tools challenge, as LLM agents often lack familiarity with the precise usage of the MCP servers. Notably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct frameworks. Beyond evaluation, we open-source our extensible evaluation framework with UI support, enabling researchers and practitioners to seamlessly integrate new agents and MCP servers while fostering innovation in the rapidly evolving MCP ecosystem. </p>
<blockquote>
<p>模型上下文协议（Model Context Protocol）已经成为连接大型语言模型与外部数据源和工具的革命性标准，迅速被各大人工智能提供商和开发平台采纳。然而，现有的基准测试过于简单，无法捕捉实际应用中的挑战，如长期推理和庞大、陌生的工具空间。为了填补这一关键空白，我们推出了MCP宇宙（MCP-Universe），这是第一个专门设计用于评估大型语言模型在真实且困难任务中与真实世界MCP服务器交互的综合性基准测试。我们的基准测试涵盖了6个核心领域，跨越11个不同的MCP服务器：位置导航、仓库管理、财务分析、3D设计、浏览器自动化和网页搜索。为了确保严格的评估，我们实施了基于执行的评估器，包括用于代理格式合规性的格式评估器、用于时间不变内容匹配的静态评估器，以及可自动检索实时真实数据的动态评估器，适用于时间敏感的任务。通过对领先的大型语言模型的广泛评估，我们发现即使是最佳模型，如GPT-5（43.72%）、Grok-4（33.33%）和Claude-4.0-Sonnet（29.44%），也存在显著的性能限制。此外，我们的基准测试为大型语言模型代理提出了一个重大的长期上下文挑战，随着交互步骤的增加，输入令牌的数量会迅速增加。而且，它引入了一个未知工具挑战，因为大型语言模型代理往往不熟悉MCP服务器的精确用途。值得注意的是，像光标这样的企业级代理并不能比标准的ReAct框架取得更好的性能。除了评估之外，我们还以支持UI的方式开源了可扩展的评估框架，使研究者和实践者能够无缝集成新的代理和MCP服务器，同时促进在不断发展的MCP生态系统中的创新。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14704v1">PDF</a> Website: <a target="_blank" rel="noopener" href="https://mcp-universe.github.io/">https://mcp-universe.github.io</a></p>
<p><strong>Summary</strong><br>在大规模语言模型与外部数据源和工具的连接标准中，模型上下文协议（MCP）已崭露头角，成为变革性标准。但现有基准测试过于简单，无法捕捉实际应用程序的挑战，如长期推理和大型、不熟悉的工具空间。为解决这一关键差距，我们推出了MCP-Universe，这是第一个专门设计用于评估语言大模型在真实和困难任务中与真实世界MCP服务器交互的基准测试。涵盖6个核心领域，跨越11个不同的MCP服务器。尽管对顶尖模型如GPT-5等进行广泛评估，但发现存在显著的性能限制。此外，我们的基准测试对LLM代理提出了长期上下文和未知工具的挑战。同时，开源了可扩展的评估框架，支持界面，促进新代理和MCP服务器的无缝集成，推动MCP生态系统快速发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>模型上下文协议（MCP）已成为连接大规模语言模型与外部数据源和工具的重要标准，受到各大AI供应商和开发平台的广泛采纳。</li>
<li>现有基准测试无法充分模拟实际应用程序的挑战，如长期推理和大型、不熟悉工具空间的应用挑战。</li>
<li>MCP-Universe是首个专为评估LLMs在真实和困难任务中而设计的基准测试，涵盖多个领域和不同的MCP服务器。</li>
<li>顶尖模型如GPT-5等虽经广泛评估，但仍存在显著性能限制，面临长期上下文和未知工具的挑战。</li>
<li>企业级代理在处理大量交互步骤时面临输入令牌数量迅速增加的问题。</li>
<li>开源的评估框架支持界面，便于研究人员和实践者无缝集成新代理和MCP服务器。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14704">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f0828be6817961439f551e432382603c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f77fee2dc4a17e80f9c5b1e00ff21b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf20e9916a34924a141d1d9bfb010cac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f2054fc6941363614657be0cfe236d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ddded5442449553444bf8cae05510d2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Adversarial-Generation-and-Collaborative-Evolution-of-Safety-Critical-Scenarios-for-Autonomous-Vehicles"><a href="#Adversarial-Generation-and-Collaborative-Evolution-of-Safety-Critical-Scenarios-for-Autonomous-Vehicles" class="headerlink" title="Adversarial Generation and Collaborative Evolution of Safety-Critical   Scenarios for Autonomous Vehicles"></a>Adversarial Generation and Collaborative Evolution of Safety-Critical   Scenarios for Autonomous Vehicles</h2><p><strong>Authors:Jiangfan Liu, Yongkang Guo, Fangzhi Zhong, Tianyuan Zhang, Zonglei Jing, Siyuan Liang, Jiakai Wang, Mingchuan Zhang, Aishan Liu, Xianglong Liu</strong></p>
<p>The generation of safety-critical scenarios in simulation has become increasingly crucial for safety evaluation in autonomous vehicles prior to road deployment in society. However, current approaches largely rely on predefined threat patterns or rule-based strategies, which limit their ability to expose diverse and unforeseen failure modes. To overcome these, we propose ScenGE, a framework that can generate plentiful safety-critical scenarios by reasoning novel adversarial cases and then amplifying them with complex traffic flows. Given a simple prompt of a benign scene, it first performs Meta-Scenario Generation, where a large language model, grounded in structured driving knowledge, infers an adversarial agent whose behavior poses a threat that is both plausible and deliberately challenging. This meta-scenario is then specified in executable code for precise in-simulator control. Subsequently, Complex Scenario Evolution uses background vehicles to amplify the core threat introduced by Meta-Scenario. It builds an adversarial collaborator graph to identify key agent trajectories for optimization. These perturbations are designed to simultaneously reduce the ego vehicle’s maneuvering space and create critical occlusions. Extensive experiments conducted on multiple reinforcement learning based AV models show that ScenGE uncovers more severe collision cases (+31.96%) on average than SoTA baselines. Additionally, our ScenGE can be applied to large model based AV systems and deployed on different simulators; we further observe that adversarial training on our scenarios improves the model robustness. Finally, we validate our framework through real-world vehicle tests and human evaluation, confirming that the generated scenarios are both plausible and critical. We hope our paper can build up a critical step towards building public trust and ensuring their safe deployment. </p>
<blockquote>
<p>在仿真中生成安全关键场景对于在道路上部署社会自动驾驶汽车之前的安全评估变得愈发关键。然而，当前的方法大多依赖于预先定义的威胁模式或基于规则的策略，这限制了它们暴露多样和未曾预见到的故障模式的能力。为了克服这些缺点，我们提出了ScenGE框架，它可以通过推理新的对抗性案例并使用复杂的交通流来放大这些案例，从而生成大量的安全关键场景。给定一个良性场景的简单提示，它首先执行元场景生成，其中大型语言模型以结构化驾驶知识为基础，推断出对抗性代理的行为构成既合理又故意具有挑战性的威胁。该元场景随后被指定为可执行代码，以在模拟器中进行精确控制。随后，复杂场景演变使用背景车辆来放大元场景引入的核心威胁。它建立了一个对抗性协同图来识别关键代理轨迹进行优化。这些扰动旨在同时减少自我车辆的机动空间并造成关键遮挡。在多个基于强化学习的自动驾驶汽车模型上进行的广泛实验表明，ScenGE平均发现了比最新技术基准更多的严重碰撞情况（+31.96%）。此外，我们的ScenGE可应用于基于大型模型的自动驾驶系统并可在不同的模拟器上部署；我们进一步观察到，在我们的场景上进行对抗性训练可提高模型的稳健性。最后，我们通过真实车辆测试和人类评估验证了我们的框架，证实了生成的场景既合理又关键。我们希望我们的论文能为建立公众信任并确保其安全部署搭建关键一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14527v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>针对自动驾驶车辆在投放社会前的安全评估，模拟生成安全关键场景变得越来越重要。当前方法主要依赖于预设的威胁模式或规则策略，无法暴露多样且未预见到的故障模式。为此，我们提出ScenGE框架，通过推理新型对抗案例并借助复杂的交通流进行放大，生成丰富的安全关键场景。给定一个良性场景的简单提示，它首先进行元场景生成，使用基于结构化驾驶知识的大型语言模型推断一个对抗性代理的行为威胁，既具有可行性又具有挑战性。然后，复杂场景演化利用背景车辆放大元场景引入的核心威胁。它建立对抗性协作图来优化关键代理轨迹。这些扰动旨在同时减少车辆的操作空间并造成关键遮挡。在多个基于强化学习的自动驾驶模型上进行的广泛实验表明，ScenGE平均比最新技术基线发现更严重的碰撞情况（+31.96%）。此外，我们的ScenGE可应用于大型模型为基础的自动驾驶系统并部署在不同的模拟器上；我们进一步观察到在我们的场景上进行对抗性训练可提高模型的稳健性。最后，我们通过真实车辆测试和人类评估验证了我们的框架，确认生成的场景既可行又关键。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>自主车辆的模拟安全关键场景生成对于社会道路部署前的安全评估至关重要。</li>
<li>当前方法受限于预设威胁模式和规则策略，难以展现多样且未预见的失败模式。</li>
<li>ScenGE框架通过推理新型对抗案例并结合复杂交通流进行场景生成，以应对这一挑战。</li>
<li>ScenGE包含元场景生成和复杂场景演化两个阶段，可有效模拟对抗性代理行为和交通流互动。</li>
<li>ScenGE在多种自动驾驶模型上表现出优越性能，平均发现更严重的碰撞情况。</li>
<li>ScenGE具有广泛的应用性，可部署在不同的模拟器上，并且通过对抗训练提高模型的稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14527">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7a2c99a0e7c7ea35e74faf63180bef78.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b73d672a0a69d3c12a2fe79cb2a2b33f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44b29f6ba57e703ff5210a61a83e06aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a8c9bbf44bc7607c0fbb857e05832a2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model"><a href="#NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model" class="headerlink" title="NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid   Mamba-Transformer Reasoning Model"></a>NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid   Mamba-Transformer Reasoning Model</h2><p><strong>Authors: NVIDIA,  :, Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adi Renduchintala, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, Alex Kondratenko, Alex Shaposhnikov, Ali Taghibakhshi, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amy Shen, Andrew Tao, Ann Guan, Anna Shors, Anubhav Mandarwal, Arham Mehta, Arun Venkatesan, Ashton Sharabiani, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Banghua Zhu, Barnaby Simkin, Bilal Kartal, Bita Darvish Rouhani, Bobby Chen, Boris Ginsburg, Brandon Norick, Brian Yu, Bryan Catanzaro, Charles Wang, Charlie Truong, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christian Munley, Christopher Parisien, Dan Su, Daniel Afrimi, Daniel Korzekwa, Daniel Rohrer, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Dima Rekesh, Dina Yared, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Eileen Long, Elliott Ning, Eric Chung, Erick Galinkin, Evelina Bakhturina, Gargi Prasad, Gerald Shen, Haim Elisha, Harsh Sharma, Hayley Ross, Helen Ngo, Herman Sahota, Hexin Wang, Hoo Chang Shin, Hua Huang, Iain Cunningham, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jimmy Zhang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jonathan Cohen, Joseph Jennings, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kezhi Kong, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Kushan Ahmadian, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Luis Vega, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Mark Cai, Markus Kliegl, Marta Stepniewska-Dziubinska, Matvei Novikov, Mehrzad Samadi, Meredith Price, Meriem Boubdir, Michael Boone, Michael Evans, Michal Bien, Michal Zawalski, Miguel Martinez, Mike Chrzanowski, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja, Nave Assaf, Negar Habibi, Nidhi Bhatia, Nikki Pope, Nima Tajbakhsh, Nirmal Kumar Juluru, Oleg Rybakov, Oleksii Hrinchuk, Oleksii Kuchaiev, Oluwatobi Olabiyi, Pablo Ribalta, Padmavathy Subramanian, Parth Chadha, Pavlo Molchanov, Peter Dykas, Peter Jin, Piotr Bialecki, Piotr Januszewski, Pradeep Thalasta, Prashant Gaikwad, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi Mahabadi, Rajen Patel, Ran El-Yaniv, Ranjit Rajan, Ria Cheruvu, Rima Shahbazyan, Ritika Borkar, Ritu Gala, Roger Waleffe, Ruoxi Zhang, Russell J. Hewett, Ryan Prenger, Sahil Jain, Samuel Kriman, Sanjeev Satheesh, Saori Kaji, Sarah Yurick, Saurav Muralidharan, Sean Narenthiran, Seonmyeong Bak, Sepehr Sameni, Seungju Han, Shanmugam Ramasamy, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shizhe Diao, Shreya Gopal, Shrimai Prabhumoye, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Siddhartha Jain, Somshubra Majumdar, Stefania Alborghetti, Syeda Nahida Akter, Terry Kong, Tim Moon, Tomasz Hliwiak, Tomer Asida, Tony Wang, Twinkle Vashishth, Tyler Poon, Udi Karpas, Vahid Noroozi, Venkat Srinivasan, Vijay Korthikanti, Vikram Fugro, Vineeth Kalluru, Vitaly Kurin, Vitaly Lavrukhin, Wasi Uddin Ahmad, Wei Du, Wonmin Byeon, Ximing Lu, Xin Dong, Yashaswi Karnati, Yejin Choi, Yian Zhang, Ying Lin, Yonggan Fu, Yoshi Suhara, Zhen Dong, Zhiyu Li, Zhongbo Zhu, Zijia Chen</strong></p>
<p>We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face. </p>
<blockquote>
<p>我们介绍了Nemotron-Nano-9B-v2，这是一款混合Mamba-Transformer语言模型，旨在提高推理工作负载的吞吐量，同时与类似规模的模型相比实现最先进的准确性。Nemotron-Nano-9B-v2建立在Nemotron-H架构的基础上，该架构将Transformer架构中大部分的自注意力层替换为Mamba-2层，以实现在生成推理所需的长思考轨迹时提高推理速度。我们通过首先在20万亿个令牌上使用FP8训练配方预训练一个12亿参数模型（Nemotron-Nano-12B-v2-Base）来创建Nemotron-Nano-9B-v2。在对Nemotron-Nano-12B-v2-Base进行对齐后，我们采用Minitron策略对模型进行压缩和蒸馏，旨在使用单个NVIDIA A10G GPU（具有22GB内存，bfloat16精度）进行最多达128k令牌的推理。与现有的类似规模模型（例如Qwen3-8B）相比，我们在推理基准测试上证明了Nemotron-Nano-9B-v2的准确率相当或更高，同时在如8k输入和16k输出令牌等推理设置中实现了高达6倍的推理吞吐量。我们将Nemotron-Nano-9B-v2、Nemotron-Nano12B-v2-Base以及Nemotron-Nano-9B-v2的checkpoint和大部分预训练和后续训练数据集一起在Hugging Face上发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14444v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Nemotron-H架构的Nemotron-Nano-9B-v2混合Mamba-Transformer语言模型，旨在提升推理工作负载的处理能力并达到类似模型的顶尖准确度。该模型通过用Mamba-2层替换Transformer架构中的大部分自注意力层，在生成推理所需的长思考轨迹时实现了更快的推理速度。通过预训练一个基于FP8训练食谱的12亿参数模型（Nemotron-Nano-12B-v2-Base），再通过Minitron策略压缩和蒸馏模型，以实现更高效的推理速度，与现有类似规模的模型相比，它在推理设置方面取得了更好的准确度并实现了更高的推理吞吐量。模型已在Hugging Face上发布。</p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14444">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0ac5d6aea88ce279e01ebbd730851f0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fa504cec80d675359af61cce57535a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dbd8c7b29fda38f4dde86f49428b106.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Automated-Optimization-Modeling-through-Expert-Guided-Large-Language-Model-Reasoning"><a href="#Automated-Optimization-Modeling-through-Expert-Guided-Large-Language-Model-Reasoning" class="headerlink" title="Automated Optimization Modeling through Expert-Guided Large Language   Model Reasoning"></a>Automated Optimization Modeling through Expert-Guided Large Language   Model Reasoning</h2><p><strong>Authors:Beinuo Yang, Qishen Zhou, Junyi Li, Xingchen Su, Simon Hu</strong></p>
<p>Optimization Modeling (OM) is essential for solving complex decision-making problems. However, the process remains time-consuming and error-prone, heavily relying on domain experts. While Large Language Models (LLMs) show promise in addressing these challenges through their natural language understanding and reasoning capabilities, current approaches face three critical limitations: high benchmark labeling error rates reaching up to 42%, narrow evaluation scope that only considers optimal values, and computational inefficiency due to heavy reliance on multi-agent systems or model fine-tuning. In this work, we first enhance existing datasets through systematic error correction and more comprehensive annotation. Additionally, we introduce LogiOR, a new optimization modeling benchmark from the logistics domain, containing more complex problems with standardized annotations. Furthermore, we present ORThought, a novel framework that leverages expert-level optimization modeling principles through chain-of-thought reasoning to automate the OM process. Through extensive empirical evaluation, we demonstrate that ORThought outperforms existing approaches, including multi-agent frameworks, with particularly significant advantages on complex optimization problems. Finally, we provide a systematic analysis of our method, identifying critical success factors and failure modes, providing valuable insights for future research on LLM-based optimization modeling. </p>
<blockquote>
<p>优化建模（OM）对于解决复杂的决策问题至关重要。然而，这一过程仍然耗时且容易出错，严重依赖于领域专家。虽然大型语言模型（LLM）通过其自然语言理解和推理能力，在应对这些挑战方面显示出潜力，但当前的方法面临三个关键局限性：高基准标签错误率，最高可达42%，评估范围狭窄，只考虑最优值，以及由于严重依赖多智能体系统或模型微调而导致的计算效率低下。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14410v1">PDF</a> </p>
<p><strong>Summary</strong><br>在解决复杂的决策问题方面，优化建模（OM）十分重要，但其过程耗时且易出错，很大程度上依赖于领域专家。虽然大型语言模型（LLM）显示出通过自然语言理解和推理能力来解决这些挑战的潜力，但当前的方法存在三个关键局限性：高基准标签错误率、评估范围狭窄只考虑最优值和计算效率低下。为改善这些问题，本研究首先通过系统误差校正和更全面的注释增强现有数据集，并引入LogiOR这一新的物流领域优化建模基准。此外，提出ORThought框架，通过链式思维推理利用专家级优化建模原则来自动化OM过程。通过广泛的实证评估，证明ORThought优于现有方法，特别是在复杂优化问题上具有显著优势。最后，对方法进行了系统分析，识别出成功因素和失败模式，为未来基于LLM的优化建模研究提供了宝贵见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>优化建模（OM）是解决复杂决策问题的关键，但过程耗时且易出错。</li>
<li>大型语言模型（LLM）在解决OM挑战方面具潜力，但当前方法存在高错误率、狭窄评估范围和计算效率低下等问题。</li>
<li>研究通过系统误差校正和更全面的注释增强了现有数据集，并引入了新的物流领域优化建模基准LogiOR。</li>
<li>提出ORThought框架，结合专家级优化建模原则，通过链式思维推理自动化OM过程。</li>
<li>ORThought框架在复杂优化问题上的表现优于现有方法。</li>
<li>方法的成功与失败因素被识别，为未来基于LLM的优化建模研究提供了指导。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14410">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4fb6edcbd2c2399bc8abbc7fdec25249.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1217039f00592b49c11bcaf7a2836893.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1909c69cdea75d6dc96d29bc8f00a82b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96093fc8129788b6df44f9cc5a24d7b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ade61ce866f06c45c73bb60746837150.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-127fbbe12fd8e6fb16f6105490a870ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55206638a9556e0a725174980723d238.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Your-Reward-Function-for-RL-is-Your-Best-PRM-for-Search-Unifying-RL-and-Search-Based-TTS"><a href="#Your-Reward-Function-for-RL-is-Your-Best-PRM-for-Search-Unifying-RL-and-Search-Based-TTS" class="headerlink" title="Your Reward Function for RL is Your Best PRM for Search: Unifying RL and   Search-Based TTS"></a>Your Reward Function for RL is Your Best PRM for Search: Unifying RL and   Search-Based TTS</h2><p><strong>Authors:Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, Dimitris N. Metaxas</strong></p>
<p>Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or LLM-generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in LLMs. </p>
<blockquote>
<p>测试时间缩放（TTS）对于大型语言模型（LLM）迄今为止主要分为了两种截然不同的范式：一是强化学习（RL）方法，优化基于稀疏结果的奖励，但存在不稳定和低样本效率的问题；二是基于独立训练的静态过程奖励模型（PRM）指导的搜索技术，这需要昂贵的人工或LLM生成的标签，并且在分布转移的情况下经常会性能下降。在本文中，我们介绍了AIRL-S，这是首个基于RL和搜索的TTS的自然统一。AIRL-S的核心见解是，在RL训练期间学习的奖励函数本质上代表了理想的PRM，用于指导下游搜索。具体来说，我们利用对抗性逆向强化学习（AIRL）结合群体相对策略优化（GRPO），直接从正确的推理轨迹中学习密集、动态的PRM，完全消除了对标记的中间过程数据的需求。在推理时，所得的PRM同时作为RL演练的批评家，并作为有效指导搜索程序的启发式方法，促进稳健的推理链扩展，缓解奖励破解，并增强跨任务泛化。在包括数学、科学推理和代码生成等在内的八个基准测试上的实验结果表明，我们的统一方法平均比基础模型提高了9%的性能，与GPT-4o相匹配。此外，当集成到多种搜索算法中时，我们的PRM始终优于使用标记数据训练的所有基线PRM。这些结果确实表明，你的RL奖励函数是你的最佳PRM搜索工具，为LLM中的复杂推理任务提供了稳健且经济的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14313v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对大型语言模型的测试时缩放（TTS）的新方法——AIRL-S。该方法结合了强化学习（RL）和基于搜索的技术，通过利用对抗性逆向强化学习（AIRL）和群体相对策略优化（GRPO）来学习密集、动态的PRM，直接从正确的推理轨迹中学习，完全消除了对标记的中间过程数据的需求。实验结果表明，该方法在多个基准测试中平均提高了9%的性能，与GPT-4o相匹配。当集成到多种搜索算法中时，本文的PRM始终优于使用标记数据训练的基线PRM。这证明对于大型语言模型的推理任务来说，强化学习的奖励函数是最好的搜索PRM。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AIRL-S结合了强化学习（RL）和搜索技术，为大型语言模型（LLM）的测试时缩放（TTS）提供了新的方法。</li>
<li>AIRL-S通过利用对抗性逆向强化学习（AIRL）和群体相对策略优化（GRPO）直接从正确的推理轨迹中学习奖励函数，消除了对标记数据的需求。</li>
<li>实验结果显示，AIRL-S在多个基准测试中性能提升显著，平均提高9%，与GPT-4o相匹配。</li>
<li>当集成到多种搜索算法中时，AIRL-S的奖励函数作为搜索过程的指导，表现优于其他使用标记数据训练的PRM。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14313">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5198d26e3e08dc7b2acf49f570b44b5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f57c37203deab202935d2ac2bd89c171.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GRILE-A-Benchmark-for-Grammar-Reasoning-and-Explanation-in-Romanian-LLMs"><a href="#GRILE-A-Benchmark-for-Grammar-Reasoning-and-Explanation-in-Romanian-LLMs" class="headerlink" title="GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian   LLMs"></a>GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian   LLMs</h2><p><strong>Authors:Adrian-Marius Dumitran, Alexandra-Mihaela Danila, Angela-Liliana Dumitran</strong></p>
<p>LLMs (Large language models) have revolutionized NLP (Natural Language Processing), yet their pedagogical value for low-resource languages remains unclear. We present GRILE (Grammar Romanian Inference and Language Explanations) , the first open benchmark of 1,151 multiple-choice questions harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate, university admissions). GRILE enables us to probe two complementary abilities of seven state-of-the-art multilingual and Romanian-specific LLMs: (i) selecting the correct answer, and (ii) producing linguistically accurate explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight models stay below 65%, and 48% of their explanations contain factual or pedagogical flaws according to expert review. A detailed error analysis pinpoints systematic weaknesses in morphology and in applying the latest DOOM3 orthographic norms. All data, code and a public web demo are released to catalyze future research. Our findings expose open challenges for trustworthy educational NLP in low-resource settings and establish GRILE as a new test-bed for controllable explanation generation and evaluation. </p>
<blockquote>
<p>大型语言模型（LLMs）已经彻底改变了自然语言处理（NLP）领域，然而它们对低资源语言的教学价值仍不明确。我们推出了GRILE（罗马尼亚语法推理和语言解释），这是从罗马尼亚高风险考试（国家评估、学士学位、大学入学）中收集的首个开放的1151道选择题基准测试。GRILE使我们能够探索最前沿的七种多语言罗马尼亚特定大型语言模型的两种互补能力：（i）选择正确答案，（ii）产生语言上准确的解释。虽然Gemini 2.5 Pro的准确率达到83%，但大多数开放式模型的准确率仍在65%以下，根据专家评审，48%的解释中含有事实或教学上的错误。详细的错误分析指出了形态学以及应用最新DOOM3正字法规范的系统性弱点。所有数据、代码和公共网络演示都已发布，以刺激未来的研究。我们的研究揭示了低资源环境中可信教育NLP的开放挑战，并将GRILE确立为可控解释生成和评估的新测试平台。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14279v1">PDF</a> Accepted as long paper @RANLP2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型在自然语言处理领域带来了革命性的变化，但对于低资源语言的教学价值尚不清楚。本研究推出了GRILE（罗马尼亚语法推理与语言解释基准测试），包含从罗马尼亚高风险考试（国家评估、高中毕业考试、大学入学考试）中收集的1151道选择题。GRILE使我们能够测试七种最先进的跨语种和罗马尼亚特定的大型语言模型的两种互补能力：一是选择正确答案的能力，二是产生语言上准确解释的能力。虽然Gemini 2.5 Pro的准确率达到83%，但大多数开放式权重模型的准确率仍在65%以下，根据专家评审，其解释中有48%存在事实或教学上的缺陷。详细的错误分析指出了形态学以及遵循最新的DOOM3正字法规范方面的系统性弱点。所有数据、代码和公共网络演示都已发布，以推动未来的研究。本研究揭示了低资源环境中可信教育自然语言处理的开放挑战，并将GRILE确立为新的可控解释生成与评价测试平台。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在自然语言处理领域的应用取得了显著进展，但在低资源语言教学中的价值尚不清楚。</li>
<li>推出了GRILE基准测试，包含从罗马尼亚高风险考试中收集的选择题，旨在评估大型语言模型的能力。</li>
<li>大部分大型语言模型在GRILE上的表现并不理想，准确率低，且其解释存在大量缺陷。</li>
<li>模型在形态学和正字法规范方面的系统性弱点被指出。</li>
<li>所有数据、代码和公共网络演示都已公开，以促进未来研究。</li>
<li>研究揭示了低资源环境中教育自然语言处理的挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14279">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-81c09cf8f656a2eb7b9c941daa4f1c3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d8117f19f6cc427cd3520093b17462c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-389251d83667435cefc0a7b6ca3db930.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e829d3458f917c844330a3e77b0af516.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LENS-Learning-to-Segment-Anything-with-Unified-Reinforced-Reasoning"><a href="#LENS-Learning-to-Segment-Anything-with-Unified-Reinforced-Reasoning" class="headerlink" title="LENS: Learning to Segment Anything with Unified Reinforced Reasoning"></a>LENS: Learning to Segment Anything with Unified Reinforced Reasoning</h2><p><strong>Authors:Lianghui Zhu, Bin Ouyang, Yuxuan Zhang, Tianheng Cheng, Rui Hu, Haocheng Shen, Longjin Ran, Xiaoxin Chen, Li Yu, Wenyu Liu, Xinggang Wang</strong></p>
<p>Text-prompted image segmentation enables fine-grained visual understanding and is critical for applications such as human-computer interaction and robotics. However, existing supervised fine-tuning methods typically ignore explicit chain-of-thought (CoT) reasoning at test time, which limits their ability to generalize to unseen prompts and domains. To address this issue, we introduce LENS, a scalable reinforcement-learning framework that jointly optimizes the reasoning process and segmentation in an end-to-end manner. We propose unified reinforcement-learning rewards that span sentence-, box-, and segment-level cues, encouraging the model to generate informative CoT rationales while refining mask quality. Using a publicly available 3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to 5.6%. These results demonstrate that RL-driven CoT reasoning serves as a robust prior for text-prompted segmentation and offers a practical path toward more generalizable Segment Anything models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/hustvl/LENS">https://github.com/hustvl/LENS</a>. </p>
<blockquote>
<p>文本提示的图像分割能够实现精细的视觉理解，对于人机交互和机器人等应用至关重要。然而，现有的监督微调方法通常在测试时忽略了明确的链式思维（CoT）推理，这限制了它们对未见提示和领域的泛化能力。为了解决这一问题，我们引入了LENS，这是一个可扩展的强化学习框架，以端到端的方式联合优化推理过程和分割。我们提出了统一的强化学习奖励，涵盖句子、框和分段级别的线索，鼓励模型在细化掩膜质量的同时生成信息丰富的CoT理由。我们使用公开的3亿参数视觉语言模型（即Qwen2.5-VL-3B-Instruct），在RefCOCO、RefCOCO+和RefCOCOg基准测试上，LENS的平均完全交集（cIoU）达到81.2%，超越了强大的微调方法GLaMM，最高提升了5.6%。这些结果表明，强化学习驱动的CoT推理是文本提示分割的稳健先验，为实现更通用的Segment Anything模型提供了实际路径。代码可在<a target="_blank" rel="noopener" href="https://github.com/hustvl/LENS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hustvl/LENS找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14153v1">PDF</a> Code is released at <a target="_blank" rel="noopener" href="https://github.com/hustvl/LENS">https://github.com/hustvl/LENS</a></p>
<p><strong>Summary</strong></p>
<p>文本内容关于通过提示进行图像分割的技术，强调了精细视觉理解的重要性，并指出其在人机交互和机器人技术等领域的应用价值。针对现有监督微调方法忽略测试时的明确思维链（CoT）推理的问题，提出了一种名为LENS的可扩展强化学习框架，该框架以端到端的方式联合优化推理过程和分割。通过使用统一强化学习奖励来涵盖句子、框和分段级别的线索，鼓励模型生成信息丰富的CoT理由，同时提高掩膜质量。使用公开可用的视觉语言模型，LENS在RefCOCO、RefCOCO+和RefCOCOg基准测试中取得了平均完全交并比（cIoU）为81.2%的成绩，优于精细调参方法GLaMM，最高提升了5.6%。这表明强化学习驱动的CoT推理为文本提示的分割提供了一个稳健的先验，并为更通用的分段模型提供了实际路径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本提示的图像分割技术对于精细视觉理解至关重要，尤其在人机交互和机器人技术等领域。</li>
<li>现有监督微调方法忽略测试时的明确思维链（CoT）推理，限制了其泛化能力。</li>
<li>LENS框架通过联合优化推理过程和分割，解决了这一问题。</li>
<li>LENS框架采用统一强化学习奖励来涵盖不同级别的线索，提升模型生成信息丰富的CoT理由的能力。</li>
<li>使用公开视觉语言模型，LENS在基准测试中实现了高平均完全交并比（cIoU）。</li>
<li>LENS相较于精细调参方法GLaMM有显著优势，体现了强化学习驱动的CoT推理的稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14153">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-14c9c396f0f288498bb5a56bead6bf3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ed583e264da878a7f9a7d740cb51e73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09f46af34f623f7a916628bb36649136.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e31edca7299d20e3515309d7541ec41b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dba8c9f444615125675a7152a246e9a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cdbca333e616a340fe8f5d9f17d0baca.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="T-REX-Table-–-Refute-or-Entail-eXplainer"><a href="#T-REX-Table-–-Refute-or-Entail-eXplainer" class="headerlink" title="T-REX: Table – Refute or Entail eXplainer"></a>T-REX: Table – Refute or Entail eXplainer</h2><p><strong>Authors:Tim Luka Horstmann, Baptiste Geisenberger, Mehwish Alam</strong></p>
<p>Verifying textual claims against structured tabular data is a critical yet challenging task in Natural Language Processing with broad real-world impact. While recent advances in Large Language Models (LLMs) have enabled significant progress in table fact-checking, current solutions remain inaccessible to non-experts. We introduce T-REX (T-REX: Table – Refute or Entail eXplainer), the first live, interactive tool for claim verification over multimodal, multilingual tables using state-of-the-art instruction-tuned reasoning LLMs. Designed for accuracy and transparency, T-REX empowers non-experts by providing access to advanced fact-checking technology. The system is openly available online. </p>
<blockquote>
<p>验证文本声明与结构化表格数据是自然语言处理中的一个关键而具有挑战性的任务，对现实世界具有广泛的影响。虽然最近大型语言模型（LLM）的进步为表格事实核查带来了显著进展，但当前解决方案对非专业人士来说仍然难以接触。我们推出T-REX（T-REX：表格——反驳或阐述解释器），这是一款首个针对多媒体、多语言表格声明验证的实时互动工具，它使用最新的指令调优推理大型语言模型。T-REX旨在准确性和透明度，为非专业人士提供接触先进事实核查技术的能力。该系统已在线公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14055v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本验证与结构化表格数据的对比在自然语言处理中是一项至关重要的任务，但对非专业人士来说具有挑战性。我们推出了T-REX工具，这是首个使用最先进的指令调整推理大型语言模型进行多模态、多语言表格中的声明验证的实时互动工具。它为非专业人士提供了访问先进的事实核查技术的能力，并设计用于准确性和透明度。系统已在线公开可用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本验证与结构化表格数据的对比在自然语言处理中是重要的任务。</li>
<li>T-REX工具是第一款结合了最新自然语言处理技术的实时互动工具，专门用于表格声明验证。</li>
<li>T-REX支持多模态和多语言表格的事实核查。</li>
<li>该工具基于先进的指令调整推理大型语言模型。</li>
<li>T-REX为非专业人士提供了访问先进事实核查技术的机会。</li>
<li>该系统设计注重准确性和透明度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14055">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-10cc43798e6eeb28ec0fefffcb8b6166.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c613312987bceff72dbe913d37a3e22e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f89c0885b713906a361ae6ec9b199acc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Input-Time-Scaling"><a href="#Input-Time-Scaling" class="headerlink" title="Input Time Scaling"></a>Input Time Scaling</h2><p><strong>Authors:Rapheal Huang, Weilong Guo</strong></p>
<p>Current Large Language Models (LLMs) are usually post-trained on large-scale carefully curated datasets (data &amp; training scaling) and doing reasoning in test time (inference time scaling). In this work, we present a new scaling paradigm, Input Time Scaling, to complement previous scaling methods by putting resources on queries (input time). During training and testing, we combine meta-knowledge from LLMs to refine inputs with different strategies. We also find a new phenomenon, training-testing co-design there. We need to apply query strategies during both training and testing. Only applying strategies on training or testing would seriously degrade the performance. We are also surprised to find that seemingly low data quality datasets can gain high performance. Adding irrelevant information to the queries, randomly selecting examples from a minimally filtered dataset, can even perform the best. These findings contradict the widely held inductive bias, “garbage in, garbage out”. Curating datasets with seemingly high-quality data can even potentially limit the performance ceiling. In addition, models trained on more data with similar quality (15k VS 1k) perform worse, simple dataset size scaling should also be carefully inspected. The good news is that our findings are compatible with the Less is More phenomenon. A small set of examples is enough to evoke high-level reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct, we are able to reach SOTA performance among 32B models on AIME24(76.7%) and AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B, the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate reproducibility and further research, we are working on open-source our datasets, data pipelines, evaluation results, and checkpoints. </p>
<blockquote>
<p>当前的大型语言模型（LLM）通常在后训练阶段在大量精心策划的数据集上进行训练（数据和训练扩展），并在测试时间（推理时间扩展）进行推理。在这项工作中，我们提出了一种新的扩展范式——输入时间扩展，以补充先前的扩展方法，将资源放在查询（输入时间）上。在训练和测试过程中，我们结合LLM的元知识，采用不同策略来优化输入。我们还发现了一个新现象——训练测试协同设计。我们需要在训练和测试过程中都应用查询策略。只在训练或测试阶段应用策略会严重降低性能。我们惊讶地发现，看似数据质量不高的数据集也能获得高性能。向查询中添加无关信息，从经过轻微过滤的数据集中随机选择示例，甚至可能表现最佳。这些发现与广为接受的归纳偏见“垃圾进，垃圾出”相矛盾。使用看似高质量数据整理的数据集甚至可能限制性能上限。此外，在类似质量的更多数据上训练的模型（15k对1k）表现更差，也需要谨慎检查简单的数据集大小扩展。好消息是，我们的发现与“少即是多”现象相吻合。少量示例足以激发高级推理能力。在Qwen2.5-32B-Instruct训练的模型上进行的实验，我们在AIME24（76.7%）和AIME25（76.7%）pass@1上达到了32B模型中的最佳性能。通过三个模型的多数投票，我们进一步实现了AIME24（76.7%）和AIME25（80%）。从DeepSeek-R1-Distill-Qwen-32B开始，AIME24的最佳结果将达到86.7%，AIME25的结果将达到76.7%。为了促进可重复性和进一步研究，我们正在开源我们的数据集、数据管道、评估结果和检查点。</p>
</blockquote>
<p><strong>简化版翻译</strong></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13654v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本研究提出一种新的缩放范式——输入时间缩放，以补充先前的缩放方法。研究结合LLM的元知识对输入进行精细化处理，并发现训练与测试协同设计的重要性。研究还发现，看似低质量的数据集也能获得高性能，添加与查询无关的信息、从最少过滤的随机选择数据甚至可能表现最佳。此外，本研究也发现简单数据集规模的缩放需谨慎考虑。研究成果兼容“少即是多”现象，并通过实验验证了其有效性。为便于复制和进一步研究，研究团队正在开源数据集、数据管道、评估结果和检查点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究提出了一种新的缩放范式——输入时间缩放，侧重于查询（输入时间）的资源分配，以补充现有的数据规模扩大和推理时间扩大方法。</li>
<li>结合LLM的元知识，通过不同的策略对输入进行精细化处理。</li>
<li>发现训练与测试协同设计的重要性，需要在两者中都应用查询策略，否则性能会严重下降。</li>
<li>突破广泛持有的归纳偏见，“垃圾进，垃圾出”，即使数据集看似低质量，也可能获得高性能。</li>
<li>简单数据集规模的扩大并不一定带来更好的性能，需要谨慎考虑。</li>
<li>研究成果与“少即是多”现象兼容，小集例子足以激发高级推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13654">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f906f3c7b17a16258ef3b5c4e5e13af5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96d3a38472ce46ece768864f7587f2cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8407b7d1ddf9b5d3aa7a361427b909f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9048ed3662a42e86a1252400be874d4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3217e2975586c188dd7c1c691a55b077.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="STEM-Efficient-Relative-Capability-Evaluation-of-LLMs-through-Structured-Transition-Samples"><a href="#STEM-Efficient-Relative-Capability-Evaluation-of-LLMs-through-Structured-Transition-Samples" class="headerlink" title="STEM: Efficient Relative Capability Evaluation of LLMs through   Structured Transition Samples"></a>STEM: Efficient Relative Capability Evaluation of LLMs through   Structured Transition Samples</h2><p><strong>Authors:Haiquan Hu, Jiazhi Jiang, Shiyou Xu, Ruhan Zeng, Tian Wang</strong></p>
<p>Evaluating large language models (LLMs) has become increasingly challenging as model capabilities advance rapidly. While recent models often achieve higher scores on standard benchmarks, these improvements do not consistently reflect enhanced real-world reasoning capabilities. Moreover, widespread overfitting to public benchmarks and the high computational cost of full evaluations have made it both expensive and less effective to distinguish meaningful differences between models. To address these challenges, we propose the \textbf{S}tructured \textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweight and interpretable evaluation framework for efficiently estimating the relative capabilities of LLMs. STEM identifies \textit{significant transition samples} (STS) by analyzing consistent performance transitions among LLMs of the same architecture but varying parameter scales. These samples enable STEM to effectively estimate the capability position of an unknown model. Qwen3 model family is applied to construct the STS pool on six diverse and representative benchmarks. To assess generalizability. Experimental results indicate that STEM reliably captures performance trends, aligns with ground-truth rankings of model capability. These findings highlight STEM as a practical and scalable method for fine-grained, architecture-agnostic evaluation of LLMs. </p>
<blockquote>
<p>随着模型能力的快速发展，评估大型语言模型（LLM）变得越来越具有挑战性。虽然最近的模型通常在标准基准测试上获得更高的分数，但这些改进并不一致地反映出现实世界推理能力的增强。此外，对公共基准测试的过度拟合以及完整评估的高计算成本，使得在模型之间区分有意义的差异既昂贵又效果不佳。为了解决这些挑战，我们提出了结构化转换评估方法（STEM），这是一个轻量级和可解释的评价框架，用于有效地估计LLM的相对能力。STEM通过分析相同架构但参数规模不同的LLM之间的一致性能转变来识别重要转换样本（STS）。这些样本使STEM能够有效地估计未知模型的能力位置。我们在六个多样且具有代表性的基准测试上应用Qwen3模型家族来构建STS池，以评估其通用性。实验结果表明，STEM能够可靠地捕捉性能趋势，与模型能力的地面真实排名相符。这些发现突显了STEM作为一种实用且可扩展的方法，用于对LLM进行精细粒度的、不受架构限制的评价。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12096v2">PDF</a> Submit to AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>本文指出评估大型语言模型（LLMs）的挑战在于模型能力迅速提升，标准基准测试上的高分数并不能始终反映其在现实世界的推理能力。为应对这些挑战，本文提出了结构化转换评估方法（STEM），这是一个轻量级且可解释的评价框架，用于高效估计LLM的相对能力。STEM通过分析同一架构但参数规模不同的LLM之间的性能过渡，确定重要转换样本（STS），从而估计未知模型的能力。实验结果表明，STEM可靠地捕捉性能趋势，与模型能力的真实排名相符。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）的评估面临挑战，因为模型能力迅速提升。</li>
<li>标准基准测试的高分数并不总能反映LLM在现实世界的推理能力。</li>
<li>STEM是一种轻量级、可解释的评价框架，用于高效估计LLM的相对能力。</li>
<li>STEM通过分析LLM之间的性能过渡来确定重要转换样本（STS）。</li>
<li>STEM可以估计未知模型的能力。</li>
<li>STEM在Qwen3模型家族上构建STS池，涉及六个多样且具有代表性的基准测试。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12096">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8f236e69e4f9282e46a432f4f76b7e7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3207f0ef9aaf932c63dab9bc0e832c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85f909cc2539d6a8b9f62854ab54961d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22cd6d6231da09a3784b41c3d8c20713.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd915b1b1eab5081df93764e7dfc1e0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5440b93db6bd453efb4b62840a347ce.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Ovis2-5-Technical-Report"><a href="#Ovis2-5-Technical-Report" class="headerlink" title="Ovis2.5 Technical Report"></a>Ovis2.5 Technical Report</h2><p><strong>Authors:Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, Zhichao Wei, Yinglun Li, Lunhao Duan, Jianshan Zhao, Yuxuan Han, Haijun Li, Wanying Chen, Junke Tang, Chengkun Hou, Zhixing Du, Tianli Zhou, Wenjie Zhang, Huping Ding, Jiahe Li, Wen Li, Gui Hu, Yiliang Gu, Siran Yang, Jiamang Wang, Hailong Sun, Yibo Wang, Hui Sun, Jinlong Huang, Yuping He, Shengze Shi, Weihong Zhang, Guodong Zheng, Junpeng Jiang, Sensen Gao, Yi-Feng Wu, Sijia Chen, Yuhui Chen, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang</strong></p>
<p>We present Ovis2.5, a successor to Ovis2 designed for native-resolution visual perception and strong multimodal reasoning. Ovis2.5 integrates a native-resolution vision transformer that processes images at their native, variable resolutions, avoiding the degradation from fixed-resolution tiling and preserving both fine detail and global layout – crucial for visually dense content like complex charts. To strengthen reasoning, we train the model to move beyond linear chain-of-thought and perform reflection – including self-checking and revision. This advanced capability is exposed as an optional “thinking mode” at inference time, allowing users to trade latency for enhanced accuracy on difficult inputs. The model is trained via a comprehensive five-phase curriculum that progressively builds its skills. The process begins with foundational visual and multimodal pretraining, advances through large-scale instruction tuning, and culminates in alignment and reasoning enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ multimodal data packing and hybrid parallelism, yielding a significant end-to-end speedup. We release two open-source models: Ovis2.5-9B and Ovis2.5-2B. The latter continues the “small model, big performance” philosophy of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a substantial improvement over its predecessor, Ovis2-8B, and achieving state-of-the-art results among open-source MLLMs in the sub-40B parameter range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong capabilities on grounding and video tasks, and achieves open-source SOTA at its scale for complex chart analysis. </p>
<blockquote>
<p>我们推出了Ovis2.5，它是Ovis2的升级版，专为原生分辨率视觉感知和强大的多模态推理而设计。Ovis2.5集成了一个原生分辨率视觉转换器，该转换器以图像的原始可变分辨率处理图像，避免了固定分辨率分块带来的降级问题，同时保留了精细细节和全局布局，这对于复杂图表等视觉密集内容至关重要。为了加强推理能力，我们训练模型超越线性思维链，进行反思，包括自我检查和修订。这种高级功能在推理时作为可选的“思维模式”暴露出来，允许用户在没有延迟的情况下提高困难输入的准确性。该模型通过全面的五阶段课程进行培训，逐步建立其技能。这个过程从基本的视觉和多模态预训练开始，通过大规模指令调整而发展，最终使用DPO和GRPO进行对齐和推理增强。为了有效地扩展这些升级，我们采用了多模态数据打包和混合并行性，从而实现了端到端的显著加速。我们发布了两个开源模型：Ovis2.5-9B和Ovis2.5-2B。后者继续秉承Ovis2的“小模型、大性能”理念，使其成为资源受限、设备端的理想选择。在OpenCompass多模态排行榜上，Ovis2.5-9B平均得分为78.3，较其前身Ovis2-8B有了显著改进，并在开放式小型多模态语言模型中取得了最先进的成果；Ovis2.5-2B得分为73.9，在其规模内建立了最佳状态。除了总体得分外，Ovis2.5在STEM基准测试上取得了领先的结果，在接地和视频任务上表现出强大的能力，并在复杂图表分析方面达到了其规模的开源最佳状态。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11737v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>本文介绍了Ovis2.5的设计和功能。它是一款设计用于本地分辨率视觉感知和强大跨模态推理的工具的升级版。Ovis2.5采用本地分辨率视觉转换器，可在图像的原生可变分辨率下进行处理，避免固定分辨率分块造成的画质损失，并保留精细细节和全局布局，对于复杂图表等视觉密集内容至关重要。通过训练模型，提高其超越线性思维链的推理能力，能够进行反思，包括自我检查和修订。该模型通过五个阶段的综合课程进行培训，逐步建立技能。训练过程中采用了跨模态数据打包和混合并行技术，实现了端到端的显著加速。模型分为两个开源版本发布：Ovis2.5-9B和Ovis2.5-2B。Ovis2.5在OpenCompass跨模态排行榜上的平均得分为78.3，相较于其前身Ovis2-8B有了显著的提升，并在公开源代码的MLLMs中达到了次40B参数范围内的最佳水平；Ovis2.5-2B得分为73.9，在其规模内达到开源最佳水平。此外，Ovis2.5在STEM基准测试上取得了领先的成果，在接地和视频任务上展现了强大的能力，并在复杂图表分析方面取得了公开源代码的最佳成绩。
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ovis2.5是Ovis2的升级版，支持原生分辨率视觉感知和强大的跨模态推理。</li>
<li>Ovis2.5采用原生分辨率视觉转换器，处理图像时避免固定分辨率分块，保留精细细节和全局布局。</li>
<li>模型具备反思能力，包括自我检查和修订，提供用户在推理时间选择开启“思考模式”以获取更高的准确性。</li>
<li>模型通过五个阶段的综合课程进行培训，包括基础视觉和跨模态预训练、大规模指令调整、对齐和推理增强等。</li>
<li>通过跨模态数据打包和混合并行技术，实现了模型效率的提升和端到端的加速。</li>
<li>Ovis2.5有两个开源版本：Ovis2.5-9B和Ovis2.5-2B，分别适用于不同的应用场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11737">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8d878db4ab62bbb2a770e756fbff75be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85ba831cc6386aff7392e72fc9defe33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eda7bb4aaea9fbd68ee9af5bbf7af400.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de7f3e024ee70fcbce51b4e1b72063fb.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="TFRank-Think-Free-Reasoning-Enables-Practical-Pointwise-LLM-Ranking"><a href="#TFRank-Think-Free-Reasoning-Enables-Practical-Pointwise-LLM-Ranking" class="headerlink" title="TFRank: Think-Free Reasoning Enables Practical Pointwise LLM Ranking"></a>TFRank: Think-Free Reasoning Enables Practical Pointwise LLM Ranking</h2><p><strong>Authors:Yongqi Fan, Xiaoyang Chen, Dezhi Ye, Jie Liu, Haijin Liang, Jin Ma, Ben He, Yingfei Sun, Tong Ruan</strong></p>
<p>Reasoning-intensive ranking models built on Large Language Models (LLMs) have made notable progress, but existing approaches often rely on large-scale LLMs and explicit Chain-of-Thought (CoT) reasoning, resulting in high computational cost and latency that limit real-world use. To address this, we propose \textbf{TFRank}, an efficient pointwise reasoning ranker based on small-scale LLMs. To improve ranking performance, TFRank effectively integrates CoT data, fine-grained score supervision, and multi-task training. Furthermore, it achieves an efficient <code>\textbf&#123;T&#125;hink-\textbf&#123;F&#125;ree&quot; reasoning capability by employing a </code>think-mode switch’’ and pointwise format constraints. Specifically, this allows the model to leverage explicit reasoning during training while delivering precise relevance scores for complex queries at inference without generating any reasoning chains. Experiments show that TFRank (e.g., 1.7B) achieves performance comparable to models with four times more parameters on the BRIGHT benchmark, and demonstrates strong competitiveness on the BEIR benchmark. Further analysis shows that TFRank achieves an effective balance between performance and efficiency, providing a practical solution for integrating advanced reasoning into real-world systems. Our code and data are released in the repository: <a target="_blank" rel="noopener" href="https://github.com/JOHNNY-fans/TFRank">https://github.com/JOHNNY-fans/TFRank</a>. </p>
<blockquote>
<p>基于大型语言模型（LLM）的推理密集型排名模型已经取得了显著的进步，但现有方法往往依赖于大规模LLM和明确的思维链（CoT）推理，导致计算成本高和延迟，限制了其在现实世界中的应用。针对这一问题，我们提出了基于小规模LLM的高效点式推理排名模型TFRank。为了提高排名性能，TFRank有效地整合了CoT数据、精细分数监督和多任务训练。此外，它通过采用“思考模式切换”和点格式约束，实现了高效的“无思考”推理能力。具体来说，这允许模型在训练过程中利用明确的推理，同时在推理过程中为复杂查询提供精确的相关性分数，而无需生成任何推理链。实验表明，TFRank（例如1.7B参数）在BRIGHT基准测试上的性能与参数多四倍的模型相当，并在BEIR基准测试上表现出强大的竞争力。进一步分析表明，TFRank在性能和效率之间实现了有效的平衡，为将高级推理集成到现实系统中提供了实用解决方案。我们的代码和数据已在仓库中发布：<a target="_blank" rel="noopener" href="https://github.com/JOHNNY-fans/TFRank%E3%80%82">https://github.com/JOHNNY-fans/TFRank。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09539v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在大型语言模型（LLM）的基础上构建的推理密集型排名模型已取得显著进展，但现有方法通常依赖于大规模LLM和显式的链式思维（CoT）推理，导致计算成本高和延迟，限制了其在现实世界中的应用。为解决这一问题，我们提出了基于小规模LLM的高效点式推理排名器TFRank。TFRank通过整合CoT数据、精细分数监督和多任务训练，提高排名性能。此外，它采用“思考模式切换”和点式格式约束，实现了“无思考”推理能力。实验表明，TFRank（如1.7B参数）在BRIGHT基准测试上的性能与参数大四倍的模型相当，并在BEIR基准测试上表现出强大的竞争力。进一步分析表明，TFRank在性能和效率之间实现了有效平衡，为将高级推理集成到现实系统中提供了实用解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有推理密集型排名模型依赖于大规模LLM和显式CoT推理，导致高计算成本和延迟。</li>
<li>TFRank是一种基于小规模LLM的点式推理排名器，旨在解决这一问题。</li>
<li>TFRank通过整合CoT数据、精细分数监督和多任务训练来提高排名性能。</li>
<li>TFRank实现“无思考”推理能力，通过“思考模式切换”和点式格式约束。</li>
<li>实验显示，TFRank在性能上与较大的模型相当，并在多个基准测试上表现出强大的竞争力。</li>
<li>TFRank在性能和效率之间实现了有效平衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09539">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6e423eb68c6df71ce51a4511b84a5f1c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0170cc77d2dbf33a7ffcafe1d7e398c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8230866c3c2131249a41ad5b3b9eb998.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b546969c8f726dffa7bc1ccf55054d3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e22dd0c2da4da40a8cff1b3d13f5e4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-861d349d6932281a0758772fa57321ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe0047398983aa4149762a427575013c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-acfe59fc074f1d991bc01715ff1870c2.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Interpretable-Reward-Model-via-Sparse-Autoencoder"><a href="#Interpretable-Reward-Model-via-Sparse-Autoencoder" class="headerlink" title="Interpretable Reward Model via Sparse Autoencoder"></a>Interpretable Reward Model via Sparse Autoencoder</h2><p><strong>Authors:Shuyi Zhang, Wei Shi, Sihang Li, Jiayi Liao, Tao Liang, Hengxing Cai, Xiang Wang</strong></p>
<p>Large language models (LLMs) have been widely deployed across numerous fields. Reinforcement Learning from Human Feedback (RLHF) leverages reward models (RMs) as proxies for human preferences to align LLM behaviors with human values, making the accuracy, reliability, and interpretability of RMs critical for effective alignment. However, traditional RMs lack interpretability, offer limited insight into the reasoning behind reward assignments, and are inflexible toward user preference shifts. While recent multidimensional RMs aim for improved interpretability, they often fail to provide feature-level attribution and require costly annotations. To overcome these limitations, we introduce the Sparse Autoencoder-enhanced Reward Model (SARM), a novel architecture that integrates a pretrained Sparse Autoencoder (SAE) into a reward model. SARM maps the hidden activations of LLM-based RM into an interpretable, sparse, and monosemantic feature space, from which a scalar head aggregates feature activations to produce transparent and conceptually meaningful reward scores. Empirical evaluations demonstrate that SARM facilitates direct feature-level attribution of reward assignments, allows dynamic adjustment to preference shifts, and achieves superior alignment performance compared to conventional reward models. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/schrieffer-z/sarm">https://github.com/schrieffer-z/sarm</a>. </p>
<blockquote>
<p>大型语言模型（LLM）已在多个领域得到广泛应用。强化学习从人类反馈（RLHF）利用奖励模型（RM）作为人类偏好的代理，使LLM行为与人类社会价值相吻合，从而使RM的准确性、可靠性和可解释性对于有效对齐至关重要。然而，传统RM缺乏可解释性，对于奖励分配背后的推理提供有限的见解，并且对用户偏好变化不够灵活。虽然最近的多维RM旨在提高可解释性，但它们往往无法提供特征级别的归属，并且需要昂贵的注释。为了克服这些局限性，我们引入了稀疏自编码器增强奖励模型（SARM），这是一种新型架构，将预训练的稀疏自编码器（SAE）集成到奖励模型中。SARM将基于LLM的RM的隐藏激活映射到可解释、稀疏和单语义特征空间，其中标量头聚合特征激活以产生透明且概念上有意义的奖励分数。经验评估表明，SARM促进了奖励分配的特征级别归属，允许动态调整偏好变化，与传统奖励模型相比实现了优越的对齐性能。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/schrieffer-z/sarm%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/schrieffer-z/sarm找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08746v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在多个领域得到广泛应用。强化学习从人类反馈（RLHF）利用奖励模型（RM）作为人类偏好的代理，使LLM行为与人的价值观保持一致，因此RM的准确性、可靠性和可解释性对于有效的对齐至关重要。针对传统RM缺乏可解释性、对人类偏好背后的推理理解有限以及在用户偏好变化时的灵活性不足的问题，本文提出了结合预训练稀疏自编码器（SAE）的奖励模型（SARM）。SARM将LLM-based RM的隐藏激活映射到可解释、稀疏且语义单一的特性空间，从中产生清晰且具有概念意义的奖励分数。实验评估表明，SARM促进了奖励分配的直接特征级归因、允许动态调整偏好变化，并实现了与传统奖励模型相比更优越的对齐性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在多个领域广泛应用。</li>
<li>强化学习从人类反馈（RLHF）利用奖励模型（RM）与人类偏好对齐。</li>
<li>传统RM缺乏可解释性，对用户偏好背后的推理理解有限，且对用户偏好变化不够灵活。</li>
<li>新型的多维度RMs虽然追求更好的可解释性，但往往无法提供特征级别的归属信息，并且需要昂贵的注释成本。</li>
<li>SARM结合了预训练的稀疏自编码器（SAE），将RM的隐藏激活映射到可解释、稀疏和单一的特性空间。</li>
<li>SARM能够实现奖励分配的直接特征级归因，动态适应偏好变化，并在对齐性能上超越传统RM。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08746">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f3afe35bcae7acf75065e50ccc9d050c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e38c8e6b12330c78ec551a169893282.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-926930e1ed1f7c34ee897abec757b01f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-491c62cf4888b77b5071074321f6bb81.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-in-Vision-A-Survey"><a href="#Reinforcement-Learning-in-Vision-A-Survey" class="headerlink" title="Reinforcement Learning in Vision: A Survey"></a>Reinforcement Learning in Vision: A Survey</h2><p><strong>Authors:Weijia Wu, Chen Gao, Joya Chen, Kevin Qinghong Lin, Qingwei Meng, Yiming Zhang, Yuke Qiu, Hong Zhou, Mike Zheng Shou</strong></p>
<p>Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: <a target="_blank" rel="noopener" href="https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning">https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning</a>. </p>
<blockquote>
<p>近期强化学习（RL）与视觉智能交叉领域的进展，使得智能体不仅能够感知复杂的视觉场景，还能在其中进行推理、生成和行动。这篇综述对该领域进行了批判性和最新的综合。我们首先正式提出视觉强化学习问题，并追溯策略优化策略从RLHF到可验证奖励范式，从近端策略优化到群体相对策略优化的演变。然后，我们将超过200篇具有代表性的作品整理为四个主题支柱：多模态大型语言模型、视觉生成、统一模型框架和视觉语言行动模型。对于每个主题支柱，我们研究了算法设计、奖励工程、基准进度，并总结了趋势，如课程驱动训练、偏好对齐扩散和统一奖励建模。最后，我们回顾了包括集合级保真度、样本级偏好和状态级稳定性在内的评估协议，并确定了开放挑战，包括样本效率、推广和安全部署。我们的目标是为研究人员和实践者提供快速扩展的视觉强化学习景观的连贯地图，并突出未来查询的有希望的方向。资源可通过以下链接获取：<a target="_blank" rel="noopener" href="https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning">https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08189v2">PDF</a> 22 pages</p>
<p><strong>Summary</strong><br>强化学习与视觉智能的交叉融合使得智能体不仅能感知复杂的视觉场景，还能进行推理、生成和行动。本文综述了该领域的最新进展，介绍了视觉强化学习的问题形式化定义，回顾了从RLHF到可验证奖励范式的策略优化策略演变，以及超过200项代表性工作的四个主题支柱：多模态大型语言模型、视觉生成、统一模型框架和视觉语言行动模型。本文还审查了评估协议并指出了开放挑战，包括样本效率、通用性和安全部署。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习与视觉智能的融合使智能体具备感知、推理、生成和行动能力。</li>
<li>视觉强化学习的问题形式化定义及其在多模态大型语言模型、视觉生成、统一模型框架和视觉语言行动模型等方面的应用得到详细介绍。</li>
<li>策略优化策略从RLHF到可验证奖励范式的演变被回顾。</li>
<li>文中提到了多种算法设计趋势，如课程驱动训练、偏好对齐扩散和统一奖励建模。</li>
<li>综述了包括集合级保真度、样本级偏好和状态级稳定性在内的评估协议。</li>
<li>指出样本效率、通用性和安全部署等开放挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08189">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bbe3a630483f68d5dd9b93050bd2f0aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a0bcd259c33592786111b52901dd13f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afd40acd9948d5a52371a5e494c50ca8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95f89a4e2c0b25c1fade35b3973523da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e6699aee2862070236411b972d3c41f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-22/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-22/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-22/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-063dd8fa5c39abd6d572370d41d89a25.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-08-22  Quantization Meets dLLMs A Systematic Study of Post-training   Quantization for Diffusion LLMs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-22/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4dadaeb1284a27cfb99f761cd4d22539.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-08-22  D^3-Talker Dual-Branch Decoupled Deformation Fields for Few-Shot 3D   Talking Head Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29739.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
