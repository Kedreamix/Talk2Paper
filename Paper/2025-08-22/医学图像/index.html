<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-22  DINOv3 with Test-Time Training for Medical Image Registration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-d700281679eec196f1cd12687d59bd19.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-22-æ›´æ–°"><a href="#2025-08-22-æ›´æ–°" class="headerlink" title="2025-08-22 æ›´æ–°"></a>2025-08-22 æ›´æ–°</h1><h2 id="DINOv3-with-Test-Time-Training-for-Medical-Image-Registration"><a href="#DINOv3-with-Test-Time-Training-for-Medical-Image-Registration" class="headerlink" title="DINOv3 with Test-Time Training for Medical Image Registration"></a>DINOv3 with Test-Time Training for Medical Image Registration</h2><p><strong>Authors:Shansong Wang, Mojtaba Safari, Mingzhe Hu, Qiang Li, Chih-Wei Chang, Richard LJ Qiu, Xiaofeng Yang</strong></p>
<p>Prior medical image registration approaches, particularly learning-based methods, often require large amounts of training data, which constrains clinical adoption. To overcome this limitation, we propose a training-free pipeline that relies on a frozen DINOv3 encoder and test-time optimization of the deformation field in feature space. Across two representative benchmarks, the method is accurate and yields regular deformations. On Abdomen MR-CT, it attained the best mean Dice score (DSC) of 0.790 together with the lowest 95th percentile Hausdorff Distance (HD95) of 4.9+-5.0 and the lowest standard deviation of Log-Jacobian (SDLogJ) of 0.08+-0.02. On ACDC cardiac MRI, it improves mean DSC to 0.769 and reduces SDLogJ to 0.11 and HD95 to 4.8, a marked gain over the initial alignment. The results indicate that operating in a compact foundation feature space at test time offers a practical and general solution for clinical registration without additional training. </p>
<blockquote>
<p>ä¹‹å‰çš„åŒ»å­¦å›¾åƒæ³¨å†Œæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸´åºŠä¸­çš„åº”ç”¨ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æµç¨‹ï¼Œå®ƒä¾èµ–äºå†»ç»“çš„DINOv3ç¼–ç å™¨ä»¥åŠåœ¨ç‰¹å¾ç©ºé—´ä¸­å¯¹å˜å½¢åœºçš„æµ‹è¯•æ—¶é—´ä¼˜åŒ–ã€‚åœ¨ä¸¤ä¸ªä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å‡†ç¡®ä¸”äº§ç”Ÿè§„åˆ™å˜å½¢ã€‚åœ¨è…¹éƒ¨MR-CTä¸Šï¼Œå®ƒè·å¾—äº†æœ€ä½³çš„å¹³å‡Diceç³»æ•°ï¼ˆDSCï¼‰ä¸º0.790ï¼Œä»¥åŠæœ€ä½çš„95thç™¾åˆ†ä½Hausdorffè·ç¦»ï¼ˆHD95ï¼‰ä¸º4.9Â±5.0å’Œæœ€ä½çš„Log-Jacobianæ ‡å‡†åå·®ï¼ˆSDLogJï¼‰ä¸º0.08Â±0.02ã€‚åœ¨ACDCå¿ƒè„MRIä¸Šï¼Œå®ƒå°†å¹³å‡DSCæé«˜åˆ°0.769ï¼Œå¹¶å°†SDLogJå‡å°‘åˆ°0.11ï¼Œå°†HD95å‡å°‘åˆ°4.8ï¼Œç›¸å¯¹äºåˆå§‹å¯¹é½æœ‰æ˜æ˜¾çš„æ”¹è¿›ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æµ‹è¯•æ—¶ä»¥ç´§å‡‘çš„åŸºç¡€ç‰¹å¾ç©ºé—´è¿›è¡Œæ“ä½œï¼Œæä¾›äº†ä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒçš„ä¸´åºŠæ³¨å†Œçš„å®ç”¨ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14809v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒæ³¨å†Œæ–¹æ³•ä¸­ï¼Œå­¦ä¹ æ³•éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œé™åˆ¶äº†ä¸´åºŠé‡‡çº³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ— è®­ç»ƒæµæ°´çº¿ï¼Œå®ƒä¾èµ–å†»ç»“çš„DINOv3ç¼–ç å™¨å’Œç‰¹å¾ç©ºé—´ä¸­çš„æµ‹è¯•æ—¶å˜å½¢åœºä¼˜åŒ–ã€‚åœ¨ä¸¤é¡¹ä»£è¡¨æ€§è¯„ä¼°æ ‡å‡†ä¸‹ï¼Œè¯¥æ–¹æ³•ç²¾ç¡®å¹¶äº§ç”Ÿè§„åˆ™çš„å˜å½¢ã€‚åœ¨è…¹éƒ¨MR-CTä¸Šï¼Œæœ€ä½³Diceç³»æ•°ï¼ˆDSCï¼‰ä¸º0.790ï¼Œç¬¬95ç™¾åˆ†ä½Hausdorffè·ç¦»ï¼ˆHD95ï¼‰æœ€ä½ä¸ºÂ±5.0ï¼Œå¯¹æ•°é›…å¯æ¯”çš„æ ‡å‡†åå·®ï¼ˆSDLogJï¼‰ä¸ºæœ€ä½å€¼Â±0.08ï¼›åœ¨ACDCå¿ƒè„MRIä¸Šï¼Œæœ€ä½³DSCå€¼ä¸ºå¹³å‡å¢é•¿è‡³Â±0.769ï¼Œå¹¶ä¸”æ˜¾è‘—å‡å°‘äº†SDLogJå’ŒHD95ã€‚è¿™è¡¨æ˜åœ¨æµ‹è¯•é˜¶æ®µç´§å‡‘ç‰¹å¾ç©ºé—´ä¸­çš„æ“ä½œå¯ä¸ºä¸´åºŠæ³¨å†Œæä¾›å®ç”¨ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆè€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦ä¹ æ³•åŒ»å­¦å›¾åƒæ³¨å†Œéœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li>
<li>æå‡ºä¸€ç§æ— è®­ç»ƒæµæ°´çº¿æ–¹æ³•ç”¨äºåŒ»å­¦å›¾åƒæ³¨å†Œã€‚</li>
<li>æ–¹æ³•ä¾èµ–äºå†»ç»“çš„DINOv3ç¼–ç å™¨ä»¥åŠæµ‹è¯•æ—¶å˜å½¢åœºçš„ä¼˜åŒ–ã€‚</li>
<li>åœ¨ä¸¤ä¸ªä»£è¡¨æ€§çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå‡†ç¡®æ€§å’Œç¨³å®šçš„å˜å½¢æ•ˆæœã€‚</li>
<li>åœ¨è…¹éƒ¨MR-CTä¸Šè·å¾—äº†è¾ƒé«˜çš„DSCå€¼å’Œè¾ƒä½çš„ç¬¬HD95å’Œç¬¬SDLogJç­‰æŒ‡æ ‡å€¼ã€‚è¡¨æ˜æ”¹è¿›äº†åˆå§‹å¯¹é½æ•ˆæœã€‚</li>
<li>æ–¹æ³•åœ¨ç´§å‡‘ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯å®ç°ä¸´åºŠæ³¨å†Œçš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a6acc47364a84c4cc8b7b07d07ea0ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac95d23dc86f1d3401218342e0454f68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84210345a338505aaeedb2de22178181.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-695675110d894c9caf8374af39eae7d4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MUSE-Observations-Reveal-Optical-Coronal-Iron-Lines-from-Shock-Emission-in-Supernova-Remnant-0540-69-3"><a href="#MUSE-Observations-Reveal-Optical-Coronal-Iron-Lines-from-Shock-Emission-in-Supernova-Remnant-0540-69-3" class="headerlink" title="MUSE Observations Reveal Optical Coronal Iron Lines from Shock Emission   in Supernova Remnant 0540-69.3"></a>MUSE Observations Reveal Optical Coronal Iron Lines from Shock Emission   in Supernova Remnant 0540-69.3</h2><p><strong>Authors:L. Tenhu, J. Larsson, P. Lundqvist, I. Saathoff, J. D. Lyman, J. Sollerman</strong></p>
<p>We investigate the optical shock emission from the Large Magellanic Cloud supernova remnant 0540-69.3 (SNR 0540) using MUSE integral-field-unit data from the VLT. The observations cover the spectral range 4650-9300 $\r{A}$ and provide a $1\times1$ arcmin$^{2}$ field of view, encompassing nearly the entire remnant. We analyse the spatial and spectral properties of shock-related emission lines, and identify clumpy optical shock emission e.g. from [S II] $\lambda\lambda$6716,6731 doublet and the coronal [Fe XIV] $\lambda$5303 line (typically at radial velocities $\lesssim|100|$ km s$^{-1}$ and $\lesssim|170|$ km s$^{-1}$, respectively). These features trace the blast-wave shell seen in previous X-ray studies. Post-shock electron density estimates, based on the [S II]-line ratio, reveal spatial variation, with the highest densities ($\sim10^4$ cm$^{-3}$) in the bright knots in the west, and lower densities ($\sim3\times10^3$ cm$^{-3}$) in the east. The density in the north (southwest) appears significantly lower (higher) but remains unconstrained due to limited signal. We also estimate blast-wave shock velocities using the [Fe XIV] $\lambda$5303&#x2F;[Fe XI] $\lambda$7892 ratio, finding low velocities ($\sim400$ km s$^{-1}$), consistent with previous studies. All these results support the scenario that the blast wave is interacting with the surrounding interstellar medium, particularly in the western regions. Additionally, we detect four unidentified emission lines, $\sim$2000-3000 km s$^{-1}$ south from the pulsar in transverse velocity, but their origin remains unclear. Possible explanations, including Fe lines from a high-velocity ejecta clump, all present challenges. Our findings highlight the complex nature of the circum- and interstellar medium surrounding SNR 0540. </p>
<blockquote>
<p>æˆ‘ä»¬åˆ©ç”¨ç”šå¤§æœ›è¿œé•œï¼ˆVLTï¼‰çš„å¤šå•å…ƒå…‰è°±æˆåƒä»ªï¼ˆMUSEï¼‰è§‚æµ‹æ•°æ®ï¼Œç ”ç©¶å¤§éº¦å“²ä¼¦äº‘è¶…æ–°æ˜Ÿæ®‹éª¸0540-69.3ï¼ˆSNR 0540ï¼‰çš„å…‰å†²å‡»å‘å°„ã€‚è§‚æµ‹å…‰è°±èŒƒå›´ä¸º4650-9300 \r{A}ï¼Œè¦†ç›–äº†çº¦æ•´ä¸ªæ®‹éª¸çš„1Ã—1å¹³æ–¹è§’åˆ†çš„è§†é‡ã€‚æˆ‘ä»¬åˆ†æäº†å†²å‡»ç›¸å…³å‘å°„çº¿çš„ç©ºé—´å’Œå…‰è°±ç‰¹æ€§ï¼Œå¹¶è¯†åˆ«å‡ºäº†å—çŠ¶å…‰å†²å‡»å‘å°„ï¼Œä¾‹å¦‚æ¥è‡ª[S II] Î»Î»6716,6731åŒçº¿å’Œç”µæ™•[Fe XIV] Î»5303çº¿ï¼ˆé€šå¸¸åœ¨å¾„å‘é€Ÿåº¦â‰¤|100|å…¬é‡Œç§’-1å’Œâ‰¤|170|å…¬é‡Œç§’-1ï¼‰ã€‚è¿™äº›ç‰¹å¾è¿½è¸ªäº†å…ˆå‰Xå°„çº¿ç ”ç©¶ä¸­çš„å†²å‡»æ³¢å£³å±‚ã€‚åŸºäº[S II]çº¿æ¯”å€¼çš„åå†²å‡»ç”µå­å¯†åº¦ä¼°ç®—æ˜¾ç¤ºå‡ºç©ºé—´å˜åŒ–ï¼Œè¥¿éƒ¨æ˜äº®ç»“ä¸­å¯†åº¦æœ€é«˜ï¼ˆçº¦10^4å˜ç±³-3ï¼‰ï¼Œä¸œéƒ¨å¯†åº¦è¾ƒä½ï¼ˆçº¦3Ã—10^3å˜ç±³-3ï¼‰ã€‚åŒ—æ–¹çš„å¯†åº¦ï¼ˆè¥¿å—æ–¹ï¼‰ä¼¼ä¹è¾ƒä½ï¼ˆè¾ƒé«˜ï¼‰ï¼Œä½†ç”±äºä¿¡å·æœ‰é™ï¼Œä»æ— æ³•ç¡®å®šã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨[Fe XIV] Î»5303&#x2F;[Fe XI] Î»7892æ¯”å€¼ä¼°è®¡äº†å†²å‡»æ³¢é€Ÿåº¦ï¼Œå‘ç°é€Ÿåº¦è¾ƒä½ï¼ˆçº¦400å…¬é‡Œç§’-1ï¼‰ï¼Œè¿™ä¸å…ˆå‰çš„ç ”ç©¶ä¸€è‡´ã€‚æ‰€æœ‰è¿™äº›ç»“æœéƒ½æ”¯æŒå†²å‡»æ³¢ä¸å‘¨å›´æ˜Ÿé™…ä»‹è´¨ç›¸äº’ä½œç”¨çš„æƒ…æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨è¥¿éƒ¨åŒºåŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¨ªå‘é€Ÿåº¦ä¸Šæ£€æµ‹åˆ°æ¥è‡ªè„‰å†²æ˜Ÿä»¥å—çº¦2000-3000å…¬é‡Œç§’-1çš„å››ä¸ªæœªè¯†åˆ«å‘å°„çº¿ï¼Œä½†å…¶èµ·æºä»ä¸æ¸…æ¥šã€‚å¯èƒ½çš„è§£é‡ŠåŒ…æ‹¬æ¥è‡ªé«˜é€Ÿå–·å°„ç‰©å›¢å—çš„é“çº¿ç­‰ï¼Œéƒ½å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†SNR 0540å‘¨å›´ç¯å¢ƒå’Œæ˜Ÿé™…ä»‹è´¨çš„å¤æ‚æ€§è´¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14570v1">PDF</a> 28 pages, 21 Figures, 5 Tables. Accepted for publication in MNRAS</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶åˆ©ç”¨VLTçš„MUSEç§¯åˆ†åœºå•å…ƒæ•°æ®ï¼Œå¯¹å¤§éº¦å“²ä¼¦æ˜Ÿäº‘è¶…æ–°æ˜Ÿé—è¿¹0540-69.3ï¼ˆSNR 0540ï¼‰çš„å…‰å­¦å†²å‡»æ³¢å‘å°„è¿›è¡Œç ”ç©¶ã€‚è§‚æµ‹è¦†ç›–äº†4650-9300Ã…çš„å…‰è°±èŒƒå›´ï¼Œè§†åœºä¸º$ 1\times1$å¹³æ–¹è§’åˆ†ï¼Œå‡ ä¹æ¶µç›–äº†æ•´ä¸ªé—è¿¹ã€‚åˆ†æå†²å‡»ç›¸å…³å‘å°„çº¿çš„ç©ºé—´å’Œå…‰è°±ç‰¹æ€§ï¼Œå¹¶è¯†åˆ«å‡ºå›¢çŠ¶å…‰å­¦å†²å‡»å‘å°„ï¼Œä¾‹å¦‚æ¥è‡ª[SII]Î»Î»6716ï¼Œ6731åŒçº¿å’Œå†•åŒº[Fe XIV]Î»5303çº¿çš„å‘å°„ï¼ˆå…¸å‹çš„å¾„å‘é€Ÿåº¦åˆ†åˆ«â‰¤|100|å…¬é‡Œç§’-1å’Œâ‰¤|170|å…¬é‡Œç§’-1ï¼‰ã€‚è¿™äº›ç‰¹å¾è¿½è¸ªäº†å…ˆå‰Xå°„çº¿ç ”ç©¶ä¸­çš„å†²å‡»æ³¢å£³å±‚ã€‚åŸºäº[SII]çº¿å¼ºåº¦æ¯”å€¼çš„åå†²å‡»ç”µå­å¯†åº¦ä¼°è®¡æ˜¾ç¤ºå‡ºç©ºé—´å˜åŒ–ï¼Œè¥¿éƒ¨äº®ç»“ä¸­çš„å¯†åº¦æœ€é«˜ï¼ˆçº¦10^4å˜ç±³-3ï¼‰ï¼Œè€Œä¸œéƒ¨è¾ƒä½ï¼ˆçº¦3Ã—10^3å˜ç±³-3ï¼‰ã€‚åŒ—éƒ¨ï¼ˆè¥¿å—éƒ¨ï¼‰çš„å¯†åº¦è¾ƒä½ï¼ˆè¾ƒé«˜ï¼‰ï¼Œä½†ç”±äºä¿¡å·æœ‰é™ï¼Œä»æ— æ³•ç¡®å®šã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨[Fe XIV]Î»5303&#x2F;[Fe XI]Î»7892æ¯”ç‡ä¼°è®¡å†²å‡»æ³¢é€Ÿåº¦ï¼Œå‘ç°é€Ÿåº¦è¾ƒä½ï¼ˆçº¦400å…¬é‡Œç§’-1ï¼‰ï¼Œä¸å…ˆå‰çš„ç ”ç©¶ä¸€è‡´ã€‚æ‰€æœ‰è¿™äº›ç»“æœéƒ½æ”¯æŒå†²å‡»æ³¢ä¸å‘¨å›´æ˜Ÿé™…ä»‹è´¨ç›¸äº’ä½œç”¨çš„æƒ…æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨è¥¿éƒ¨åŒºåŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¨ªå‘é€Ÿåº¦æ–¹å‘ä¸Šä»è„‰å†²æ˜Ÿå—éƒ¨çº¦2000-3000å…¬é‡Œç§’-1çš„åœ°æ–¹æ£€æµ‹åˆ°å››æ¡æœªè¯†åˆ«çš„å‘å°„çº¿ï¼Œä½†å…¶èµ·æºä»ä¸æ¸…æ¥šã€‚å¯èƒ½çš„è§£é‡ŠåŒ…æ‹¬æ¥è‡ªé«˜é€Ÿå–·å°„å›¢å—çš„é“çº¿ç­‰ï¼Œæ‰€æœ‰è¿™äº›éƒ½å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†å›´ç»•SNR 0540çš„æ˜Ÿé™…å’Œå‘¨å›´ä»‹è´¨çš„å¤æ‚æ€§è´¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨MUSEç§¯åˆ†åœºå•å…ƒæ•°æ®ç ”ç©¶SNR 0540çš„å…‰å­¦å†²å‡»æ³¢å‘å°„ã€‚</li>
<li>åˆ†æäº†å†²å‡»ç›¸å…³å‘å°„çº¿çš„ç©ºé—´å’Œå…‰è°±ç‰¹æ€§ã€‚</li>
<li>è¯†åˆ«å‡ºå›¢çŠ¶å…‰å­¦å†²å‡»å‘å°„ï¼Œä¾‹å¦‚æ¥è‡ª[SII]å’Œ[Fe XIV]çš„å‘å°„çº¿ã€‚</li>
<li>è§‚å¯Ÿåˆ°ç”µå­å¯†åº¦çš„ç©ºé—´å˜åŒ–ï¼Œè¥¿éƒ¨äº®ç»“å¯†åº¦è¾ƒé«˜ã€‚</li>
<li>å†²å‡»æ³¢é€Ÿåº¦ä¸å…ˆå‰ç ”ç©¶ä¸€è‡´ï¼Œçº¦ä¸º400å…¬é‡Œç§’-1ã€‚</li>
<li>å†²å‡»æ³¢ä¸å‘¨å›´æ˜Ÿé™…ä»‹è´¨çš„ç›¸äº’ä½œç”¨åœ¨è¥¿éƒ¨åŒºåŸŸæ›´ä¸ºæ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14570">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d0cf28670ede79ed03e251f7d6b5a5a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-020cf2d529e44d65ac951025f1b0c638.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d7fc5252029c1680a782f79b33952a24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-551071b2600ea1ea5ed1ff1bc1b6579a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Deep-Skin-Lesion-Segmentation-with-Transformer-CNN-Fusion-Toward-Intelligent-Skin-Cancer-Analysis"><a href="#Deep-Skin-Lesion-Segmentation-with-Transformer-CNN-Fusion-Toward-Intelligent-Skin-Cancer-Analysis" class="headerlink" title="Deep Skin Lesion Segmentation with Transformer-CNN Fusion: Toward   Intelligent Skin Cancer Analysis"></a>Deep Skin Lesion Segmentation with Transformer-CNN Fusion: Toward   Intelligent Skin Cancer Analysis</h2><p><strong>Authors:Xin Wang, Xiaopei Zhang, Xingang Wang</strong></p>
<p>This paper proposes a high-precision semantic segmentation method based on an improved TransUNet architecture to address the challenges of complex lesion structures, blurred boundaries, and significant scale variations in skin lesion images. The method integrates a transformer module into the traditional encoder-decoder framework to model global semantic information, while retaining a convolutional branch to preserve local texture and edge features. This enhances the modelâ€™s ability to perceive fine-grained structures. A boundary-guided attention mechanism and multi-scale upsampling path are also designed to improve lesion boundary localization and segmentation consistency. To verify the effectiveness of the approach, a series of experiments were conducted, including comparative studies, hyperparameter sensitivity analysis, data augmentation effects, input resolution variation, and training data split ratio tests. Experimental results show that the proposed model outperforms existing representative methods in mIoU, mDice, and mAcc, demonstrating stronger lesion recognition accuracy and robustness. In particular, the model achieves better boundary reconstruction and structural recovery in complex scenarios, making it well-suited for the key demands of automated segmentation tasks in skin lesion analysis. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ”¹è¿›å‹TransUNetæ¶æ„çš„é«˜ç²¾åº¦è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œæ—¨åœ¨åº”å¯¹çš®è‚¤ç—…ç¶å›¾åƒä¸­å¤æ‚ç—…ç¶ç»“æ„ã€è¾¹ç•Œæ¨¡ç³Šå’Œæ˜¾è‘—å°ºåº¦å˜åŒ–ç­‰æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•å°†å˜å‹å™¨æ¨¡å—é›†æˆåˆ°ä¼ ç»Ÿçš„ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ä¸­ï¼Œä»¥å»ºæ¨¡å…¨å±€è¯­ä¹‰ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™å·ç§¯åˆ†æ”¯ä»¥ä¿ç•™å±€éƒ¨çº¹ç†å’Œè¾¹ç¼˜ç‰¹å¾ã€‚è¿™å¢å¼ºäº†æ¨¡å‹å¯¹ç»†ç²’åº¦ç»“æ„çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚è¿˜è®¾è®¡äº†è¾¹ç•Œå¼•å¯¼æ³¨æ„æœºåˆ¶å’Œè·¨å°ºåº¦ä¸Šé‡‡æ ·è·¯å¾„ï¼Œä»¥æé«˜ç—…ç¶è¾¹ç•Œå®šä½å’Œåˆ†å‰²çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†éªŒè¯è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¿›è¡Œäº†ä¸€ç³»åˆ—å®éªŒï¼ŒåŒ…æ‹¬å¯¹æ¯”ç ”ç©¶ã€è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æã€æ•°æ®å¢å¼ºæ•ˆæœã€è¾“å…¥åˆ†è¾¨ç‡å˜åŒ–å’Œè®­ç»ƒæ•°æ®åˆ†å‰²æ¯”ä¾‹æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨mIoUã€mDiceå’ŒmAccç­‰æ–¹é¢ä¼˜äºç°æœ‰ä»£è¡¨æ€§æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„ç—…ç¶è¯†åˆ«ç²¾åº¦å’Œç¨³å¥æ€§ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œè¯¥æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¾¹ç•Œé‡å»ºå’Œç»“æ„æ¢å¤æ–¹é¢è¡¨ç°æ›´ä½³ï¼Œéå¸¸é€‚ç”¨äºçš®è‚¤ç—…ç¶åˆ†æä¸­çš„è‡ªåŠ¨åŒ–åˆ†å‰²ä»»åŠ¡çš„å…³é”®éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14509v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ”¹è¿›å‹TransUNetæ¶æ„çš„é«˜ç²¾åº¦è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œç”¨äºåº”å¯¹çš®è‚¤ç—…ç¶å›¾åƒä¸­å¤æ‚ç—…ç¶ç»“æ„ã€æ¨¡ç³Šè¾¹ç•Œå’Œæ˜¾è‘—å°ºåº¦å˜åŒ–ç­‰æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•å°†transformeræ¨¡å—èå…¥ä¼ ç»Ÿç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼Œä»¥å»ºæ¨¡å…¨å±€è¯­ä¹‰ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™å·ç§¯åˆ†æ”¯ä»¥ä¿ç•™å±€éƒ¨çº¹ç†å’Œè¾¹ç¼˜ç‰¹å¾ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹ç»†å¾®ç»“æ„æ„ŸçŸ¥çš„èƒ½åŠ›ã€‚é€šè¿‡è®¾è®¡è¾¹ç•Œå¼•å¯¼æ³¨æ„æœºåˆ¶å’Œå¤šå°ºåº¦ä¸Šé‡‡æ ·è·¯å¾„ï¼Œæé«˜äº†ç—…ç¶è¾¹ç•Œå®šä½å’Œåˆ†å‰²ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨mIoUã€mDiceå’ŒmAccç­‰æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰ä»£è¡¨æ€§æ–¹æ³•ï¼Œè¡¨ç°å‡ºæ›´å¼ºçš„ç—…ç¶è¯†åˆ«å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¾¹ç•Œé‡å»ºå’Œç»“æ„æ¢å¤æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œéå¸¸é€‚åˆçš®è‚¤ç—…ç¶åˆ†æä¸­çš„è‡ªåŠ¨åŒ–åˆ†å‰²ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ”¹è¿›å‹TransUNetæ¶æ„çš„é«˜ç²¾åº¦è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œç”¨äºå¤„ç†çš®è‚¤ç—…ç¶å›¾åƒä¸­çš„å¤šç§æŒ‘æˆ˜ã€‚</li>
<li>æ–¹æ³•èåˆäº†transformeræ¨¡å—å’Œå·ç§¯åˆ†æ”¯ï¼Œä»¥æ•æ‰å…¨å±€è¯­ä¹‰ä¿¡æ¯å’Œå±€éƒ¨ç»†èŠ‚ã€‚</li>
<li>é€šè¿‡å¼•å…¥è¾¹ç•Œå¼•å¯¼æ³¨æ„æœºåˆ¶å’Œå¤šå°ºåº¦ä¸Šé‡‡æ ·è·¯å¾„ï¼Œæé«˜äº†ç—…ç¶è¾¹ç•Œå®šä½å’Œåˆ†å‰²çš„å‡†ç¡®åº¦ã€‚</li>
<li>å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–ç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬mIoUã€mDiceå’ŒmAccã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¾¹ç•Œé‡å»ºå’Œç»“æ„æ¢å¤æ–¹é¢è¡¨ç°çªå‡ºã€‚</li>
<li>æ–¹æ³•å…·æœ‰å¼ºå¤§çš„ç—…ç¶è¯†åˆ«å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œé€‚ç”¨äºçš®è‚¤ç—…ç¶åˆ†æçš„è‡ªåŠ¨åŒ–åˆ†å‰²ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14509">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6473f9f79faf0bcf246b85da602faad8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a41caca2a8b4a3f3ed83a4b71c68c811.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3756d6980a14acfbc60cbb7ba6ee912a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88e92047efe2eb24033f0b78a5da1742.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-def5039308fde214c7a850eaaf708139.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Weakly-Convex-Regularization-for-Magnetic-Resonance-Image-Denoising"><a href="#Weakly-Convex-Regularization-for-Magnetic-Resonance-Image-Denoising" class="headerlink" title="Weakly-Convex Regularization for Magnetic Resonance Image Denoising"></a>Weakly-Convex Regularization for Magnetic Resonance Image Denoising</h2><p><strong>Authors:Akash Prabakar, Abhishek Shreekant Bhandiwad, Abijith Jagannath Kamath, Chandra Sekhar Seelamantula</strong></p>
<p>Regularization for denoising in magnetic resonance imaging (MRI) is typically achieved using convex regularization functions. Recently, deep learning techniques have been shown to provide superior denoising performance. However, this comes at the price of lack of explainability, interpretability and stability, which are all crucial to MRI. In this work, we present a constructive approach for designing weakly-convex regularization functions for MR image denoising. We show that our technique performs on par with state-of-the-art denoisers for diffusion-weighted MR image denoising. Our technique can be applied to design weakly-convex convolutional neural networks with prototype activation functions that impart interpretability and are provably convergent. We also show that our technique exhibits fewer denoising artifacts by demonstrating its effect on brain microstructure modelling. </p>
<blockquote>
<p>åœ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­ï¼Œå»å™ªçš„æ­£åˆ™åŒ–é€šå¸¸é€šè¿‡ä½¿ç”¨å‡¸æ­£åˆ™åŒ–å‡½æ•°æ¥å®ç°ã€‚æœ€è¿‘ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯å·²è¢«è¯æ˜å¯ä»¥æä¾›å“è¶Šçš„å»å™ªæ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™å¸¦æ¥äº†ç¼ºä¹å¯è§£é‡Šæ€§ã€å¯è§£é‡Šæ€§å’Œç¨³å®šæ€§çš„ä»£ä»·ï¼Œè¿™ä¸‰è€…å¯¹äºMRIæ¥è¯´éƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è®¾è®¡ç”¨äºMRå›¾åƒå»å™ªçš„å¼±å‡¸æ­£åˆ™åŒ–å‡½æ•°çš„å»ºè®¾æ€§æ–¹æ³•ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯åœ¨æ‰©æ•£åŠ æƒMRå›¾åƒå»å™ªæ–¹é¢çš„æ€§èƒ½ä¸æœ€å…ˆè¿›çš„å»å™ªå™¨ç›¸å½“ã€‚æˆ‘ä»¬çš„æŠ€æœ¯å¯åº”ç”¨äºè®¾è®¡å…·æœ‰åŸå‹æ¿€æ´»å‡½æ•°çš„å¼±å‡¸å·ç§¯ç¥ç»ç½‘ç»œï¼Œè¿™äº›ç½‘ç»œå…·æœ‰å¯è§£é‡Šæ€§å¹¶ä¸”å¯è¯æ˜æ”¶æ•›ã€‚æˆ‘ä»¬è¿˜é€šè¿‡åœ¨è„‘å¾®è§‚ç»“æ„å»ºæ¨¡ä¸Šå±•ç¤ºå…¶å½±å“ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æŠ€æœ¯äº§ç”Ÿçš„å»å™ªä¼ªå½±æ›´å°‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14438v1">PDF</a> Presented in ISCS25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§è®¾è®¡å¼±å‡¸æ­£åˆ™åŒ–å‡½æ•°çš„æ–¹æ³•ï¼Œç”¨äºç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å»å™ªã€‚è¯¥æ–¹æ³•ä¸å…ˆè¿›çš„å»å™ªå™¨åœ¨æ‰©æ•£åŠ æƒMRIå»å™ªæ–¹é¢è¡¨ç°ç›¸å½“ï¼Œå¹¶å¯åº”ç”¨äºè®¾è®¡å…·æœ‰åŸå‹æ¿€æ´»å‡½æ•°çš„å¼±å‡¸å·ç§¯ç¥ç»ç½‘ç»œï¼Œå…·æœ‰å¯è§£é‡Šæ€§ã€æ”¶æ•›æ€§ï¼Œå¹¶å‡å°‘äº†å»å™ªä¼ªå½±åœ¨è„‘å¾®è§‚ç»“æ„å»ºæ¨¡ä¸­çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ä»‹ç»äº†å¼±å‡¸æ­£åˆ™åŒ–å‡½æ•°çš„è®¾è®¡æ–¹æ³•ï¼Œæ—¨åœ¨ç”¨äºç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­çš„å»å™ªã€‚</li>
<li>è¯¥æ–¹æ³•ä¸å½“å‰å…ˆè¿›çš„å»å™ªå™¨åœ¨æ‰©æ•£åŠ æƒMRIå»å™ªæ–¹é¢è¡¨ç°ç›¸ä¼¼ã€‚</li>
<li>è¯¥æŠ€æœ¯å¯åº”ç”¨äºè®¾è®¡å…·æœ‰åŸå‹æ¿€æ´»å‡½æ•°çš„å¼±å‡¸å·ç§¯ç¥ç»ç½‘ç»œã€‚</li>
<li>æ‰€è®¾è®¡ç½‘ç»œå…·æœ‰å¯è§£é‡Šæ€§å’Œæ”¶æ•›æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å»å™ªè¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¼ªå½±è¾ƒå°‘ã€‚</li>
<li>æ–‡ä¸­å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨è„‘å¾®è§‚ç»“æ„å»ºæ¨¡ä¸­çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14438">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-07067b0f16a6d068e7d26efbac6c6c2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dcdffa272404a2a35e3622a4fc9a735f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2af9ceebb9154b35d1c1ea2176d687a2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TCFNet-Bidirectional-face-bone-transformation-via-a-Transformer-based-coarse-to-fine-point-movement-network"><a href="#TCFNet-Bidirectional-face-bone-transformation-via-a-Transformer-based-coarse-to-fine-point-movement-network" class="headerlink" title="TCFNet: Bidirectional face-bone transformation via a Transformer-based   coarse-to-fine point movement network"></a>TCFNet: Bidirectional face-bone transformation via a Transformer-based   coarse-to-fine point movement network</h2><p><strong>Authors:Runshi Zhang, Bimeng Jie, Yang He, Junchen Wang</strong></p>
<p>Computer-aided surgical simulation is a critical component of orthognathic surgical planning, where accurately simulating face-bone shape transformations is significant. The traditional biomechanical simulation methods are limited by their computational time consumption levels, labor-intensive data processing strategies and low accuracy. Recently, deep learning-based simulation methods have been proposed to view this problem as a point-to-point transformation between skeletal and facial point clouds. However, these approaches cannot process large-scale points, have limited receptive fields that lead to noisy points, and employ complex preprocessing and postprocessing operations based on registration. These shortcomings limit the performance and widespread applicability of such methods. Therefore, we propose a Transformer-based coarse-to-fine point movement network (TCFNet) to learn unique, complicated correspondences at the patch and point levels for dense face-bone point cloud transformations. This end-to-end framework adopts a Transformer-based network and a local information aggregation network (LIA-Net) in the first and second stages, respectively, which reinforce each other to generate precise point movement paths. LIA-Net can effectively compensate for the neighborhood precision loss of the Transformer-based network by modeling local geometric structures (edges, orientations and relative position features). The previous global features are employed to guide the local displacement using a gated recurrent unit. Inspired by deformable medical image registration, we propose an auxiliary loss that can utilize expert knowledge for reconstructing critical organs.Compared with the existing state-of-the-art (SOTA) methods on gathered datasets, TCFNet achieves outstanding evaluation metrics and visualization results. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Runshi-Zhang/TCFNet">https://github.com/Runshi-Zhang/TCFNet</a>. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯æ¨¡æ‹Ÿæ˜¯æ­£é¢Œå¤–ç§‘æ‰‹æœ¯è§„åˆ’çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå…¶ä¸­å¯¹é¢éª¨å½¢çŠ¶å˜åŒ–çš„ç²¾ç¡®æ¨¡æ‹Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚ä¼ ç»Ÿçš„ç”Ÿç‰©åŠ›å­¦æ¨¡æ‹Ÿæ–¹æ³•å—åˆ°è®¡ç®—æ—¶é—´æ¶ˆè€—å¤§ã€åŠ³åŠ¨å¯†é›†å‹æ•°æ®å¤„ç†ç­–ç•¥ä»¥åŠç²¾åº¦ä½ç­‰çš„é™åˆ¶ã€‚æœ€è¿‘ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡æ‹Ÿæ–¹æ³•å·²å°†è¯¥é—®é¢˜è§†ä¸ºéª¨éª¼å’Œé¢éƒ¨ç‚¹äº‘ä¹‹é—´çš„ç‚¹-ç‚¹è½¬æ¢ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æ— æ³•å¤„ç†å¤§è§„æ¨¡çš„ç‚¹ï¼Œå…·æœ‰æœ‰é™çš„æ„Ÿå—é‡è€Œå¯¼è‡´å™ªå£°ç‚¹ï¼Œå¹¶åŸºäºæ³¨å†Œæ‰§è¡Œäº†å¤æ‚çš„é¢„å¤„ç†å’Œåå¤„ç†æ“ä½œã€‚è¿™äº›ç¼ºç‚¹é™åˆ¶äº†æ­¤ç±»æ–¹æ³•çš„æ€§èƒ½å’Œå¹¿æ³›åº”ç”¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„ç²—ç»†ç‚¹ç§»åŠ¨ç½‘ç»œï¼ˆTCFNetï¼‰ï¼Œç”¨äºå­¦ä¹ è¡¥ä¸å’Œç‚¹å¯¹å¯†é›†é¢éƒ¨éª¨éª¼ç‚¹äº‘è½¬æ¢çš„ç‹¬ç‰¹è€Œå¤æ‚çš„å¯¹åº”å…³ç³»ã€‚è¯¥ç«¯åˆ°ç«¯æ¡†æ¶åœ¨ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨åŸºäºTransformerçš„ç½‘ç»œï¼Œåœ¨ç¬¬äºŒé˜¶æ®µé‡‡ç”¨å±€éƒ¨ä¿¡æ¯èšåˆç½‘ç»œï¼ˆLIA-Netï¼‰ï¼Œä¸¤è€…ç›¸äº’å¢å¼ºä»¥ç”Ÿæˆç²¾ç¡®çš„ç‚¹ç§»åŠ¨è·¯å¾„ã€‚LIA-Netå¯ä»¥æœ‰æ•ˆåœ°è¡¥å¿åŸºäºTransformerçš„ç½‘ç»œä¸­é‚»è¿‘ç²¾åº¦æŸå¤±ï¼Œé€šè¿‡å¯¹å±€éƒ¨å‡ ä½•ç»“æ„ï¼ˆè¾¹ç¼˜ã€æ–¹å‘å’Œç›¸å¯¹ä½ç½®ç‰¹å¾ï¼‰è¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡å…ˆå‰çš„å…¨å±€ç‰¹å¾æ¥å¼•å¯¼å±€éƒ¨ä½ç§»ä½¿ç”¨é—¨æ§å¾ªç¯å•å…ƒã€‚å—å¯å˜å½¢åŒ»å­¦å›¾åƒæ³¨å†Œçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¾…åŠ©æŸå¤±ï¼Œå¯ä»¥åˆ©ç”¨ä¸“å®¶çŸ¥è¯†é‡å»ºå…³é”®å™¨å®˜ã€‚ä¸æ”¶é›†çš„æ•°æ®é›†ä¸Šçš„ç°æœ‰æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒTCFNetåœ¨è¯„ä¼°æŒ‡æ ‡å’Œå¯è§†åŒ–ç»“æœæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆç»©ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Runshi-Zhang/TCFNet">https://github.com/Runshi-Zhang/TCFNet</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14373v1">PDF</a> 17 pages, 11 figures</p>
<p><strong>æ‘˜è¦</strong><br>    æå‡ºä¸€ç§åŸºäºTransformerçš„ç²—åˆ°ç»†ç‚¹ç§»åŠ¨ç½‘ç»œï¼ˆTCFNetï¼‰ï¼Œç”¨äºå­¦ä¹ é¢éƒ¨éª¨éª¼ç‚¹äº‘çš„å¯†é›†å˜æ¢çš„ç‹¬ç‰¹å¯¹åº”å…³ç³»ã€‚è¯¥ç½‘ç»œåœ¨è¡¥ä¸å’Œç‚¹çº§åˆ«å…·æœ‰å¤æ‚çš„å¯¹åº”å…³ç³»ï¼Œé‡‡ç”¨ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç¬¬ä¸€é˜¶æ®µé‡‡ç”¨åŸºäºTransformerçš„ç½‘ç»œï¼Œç¬¬äºŒé˜¶æ®µé‡‡ç”¨å±€éƒ¨ä¿¡æ¯èšåˆç½‘ç»œï¼ˆLIA-Netï¼‰ï¼Œä¸¤è€…ç›¸äº’å¢å¼ºï¼Œäº§ç”Ÿç²¾ç¡®çš„ç‚¹ç§»åŠ¨è·¯å¾„ã€‚ä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒTCFNetåœ¨æ”¶é›†çš„æ•°æ®é›†ä¸Šå–å¾—äº†å‡ºè‰²çš„è¯„ä»·æŒ‡æ ‡å’Œå¯è§†åŒ–ç»“æœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯æ¨¡æ‹Ÿåœ¨æ•´å½¢å¤–ç§‘æ‰‹æœ¯è§„åˆ’ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œå…¶ä¸­å‡†ç¡®æ¨¡æ‹Ÿé¢éƒ¨éª¨éª¼å½¢çŠ¶å˜åŒ–å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ä¼ ç»Ÿç”Ÿç‰©åŠ›å­¦æ¨¡æ‹Ÿæ–¹æ³•å­˜åœ¨è®¡ç®—è€—æ—¶ã€æ•°æ®å¤„ç†ç­–ç•¥ç¹çå’Œä½ç²¾åº¦ç­‰å±€é™æ€§ã€‚</li>
<li>åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•å°†é—®é¢˜è§†ä¸ºéª¨éª¼å’Œé¢éƒ¨ç‚¹äº‘ä¹‹é—´çš„ç‚¹å¯¹ç‚¹è½¬æ¢ï¼Œä½†å¤„ç†å¤§è§„æ¨¡ç‚¹æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå¦‚æ„Ÿå—é‡å°ã€ç‚¹å™ªå£°å¤§ã€åŸºäºæ³¨å†Œçš„é¢„å¤„ç†å’Œåå¤„ç†æ“ä½œå¤æ‚ç­‰ã€‚</li>
<li>æå‡ºçš„TCFNetç½‘ç»œç»“åˆTransformerå’ŒLIA-Netï¼Œåˆ†åˆ«åœ¨è¡¥ä¸å’Œç‚¹çº§åˆ«å­¦ä¹ ç‹¬ç‰¹çš„å¤æ‚å¯¹åº”å…³ç³»ï¼Œç”¨äºå¯†é›†é¢éƒ¨éª¨éª¼ç‚¹äº‘å˜æ¢ã€‚</li>
<li>LIA-Netèƒ½æœ‰æ•ˆè¡¥å¿Transformerç½‘ç»œé‚»åŸŸç²¾åº¦æŸå¤±ï¼Œé€šè¿‡å»ºæ¨¡å±€éƒ¨å‡ ä½•ç»“æ„ï¼ˆå¦‚è¾¹ç¼˜ã€æ–¹å‘å’Œç›¸å¯¹ä½ç½®ç‰¹å¾ï¼‰æ¥æé«˜å‡†ç¡®æ€§ã€‚</li>
<li>ç½‘ç»œä½¿ç”¨å…¨å±€ç‰¹å¾æŒ‡å¯¼å±€éƒ¨ä½ç§»ï¼Œé‡‡ç”¨é—¨æ§å¾ªç¯å•å…ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6e6e1448340492f160422de01c912778.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c3caaadc179ea472ff364227f725aa2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LENS-Learning-to-Segment-Anything-with-Unified-Reinforced-Reasoning"><a href="#LENS-Learning-to-Segment-Anything-with-Unified-Reinforced-Reasoning" class="headerlink" title="LENS: Learning to Segment Anything with Unified Reinforced Reasoning"></a>LENS: Learning to Segment Anything with Unified Reinforced Reasoning</h2><p><strong>Authors:Lianghui Zhu, Bin Ouyang, Yuxuan Zhang, Tianheng Cheng, Rui Hu, Haocheng Shen, Longjin Ran, Xiaoxin Chen, Li Yu, Wenyu Liu, Xinggang Wang</strong></p>
<p>Text-prompted image segmentation enables fine-grained visual understanding and is critical for applications such as human-computer interaction and robotics. However, existing supervised fine-tuning methods typically ignore explicit chain-of-thought (CoT) reasoning at test time, which limits their ability to generalize to unseen prompts and domains. To address this issue, we introduce LENS, a scalable reinforcement-learning framework that jointly optimizes the reasoning process and segmentation in an end-to-end manner. We propose unified reinforcement-learning rewards that span sentence-, box-, and segment-level cues, encouraging the model to generate informative CoT rationales while refining mask quality. Using a publicly available 3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to 5.6%. These results demonstrate that RL-driven CoT reasoning serves as a robust prior for text-prompted segmentation and offers a practical path toward more generalizable Segment Anything models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/hustvl/LENS">https://github.com/hustvl/LENS</a>. </p>
<blockquote>
<p>æ–‡æœ¬æç¤ºçš„å›¾åƒåˆ†å‰²èƒ½å¤Ÿå®ç°ç²¾ç»†çš„è§†è§‰ç†è§£ï¼Œå¯¹äºäººæœºäº¤äº’å’Œæœºå™¨äººç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•é€šå¸¸åœ¨æµ‹è¯•æ—¶å¿½ç•¥äº†æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¯¹æœªè§è¿‡çš„æç¤ºå’Œé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LENSï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è”åˆä¼˜åŒ–æ¨ç†è¿‡ç¨‹å’Œåˆ†å‰²ã€‚æˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±ï¼Œæ¶µç›–å¥å­ã€æ¡†å’Œæ®µçº§çº¿ç´¢ï¼Œé¼“åŠ±æ¨¡å‹åœ¨ç»†åŒ–æ©è†œè´¨é‡çš„åŒæ—¶ç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„CoTç†ç”±ã€‚æˆ‘ä»¬ä½¿ç”¨å…¬å¼€çš„3äº¿å‚æ•°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå³Qwen2.5-VL-3B-Instructï¼‰ï¼Œåœ¨RefCOCOã€RefCOCO+å’ŒRefCOCOgåŸºå‡†æµ‹è¯•ä¸Šï¼ŒLENSçš„å¹³å‡å®Œå…¨äº¤å¹¶æ¯”ï¼ˆcIoUï¼‰è¾¾åˆ°81.2%ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„å¾®è°ƒæ–¹æ³•GLaMMï¼Œæœ€é«˜æå‡äº†5.6%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„CoTæ¨ç†ä¸ºæ–‡æœ¬æç¤ºçš„åˆ†å‰²æä¾›äº†ä¸€ä¸ªç¨³å¥çš„å…ˆéªŒï¼Œå¹¶ä¸ºæ›´é€šç”¨çš„â€œä»»æ„å†…å®¹åˆ†å‰²â€æ¨¡å‹æä¾›äº†å®é™…å¯è¡Œçš„è·¯å¾„ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/hustvl/LENS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hustvl/LENSè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14153v1">PDF</a> Code is released at <a target="_blank" rel="noopener" href="https://github.com/hustvl/LENS">https://github.com/hustvl/LENS</a></p>
<p><strong>Summary</strong><br>    æ–‡æœ¬æç¤ºçš„å›¾åƒåˆ†å‰²æŠ€æœ¯å¯¹äºç²¾ç»†è§†è§‰ç†è§£è‡³å…³é‡è¦ï¼Œå¯¹äºäººæœºäº¤äº’å’Œæœºå™¨äººåº”ç”¨ç­‰é¢†åŸŸæœ‰é‡è¦æ„ä¹‰ã€‚ä¸ºè§£å†³ç°æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•å¿½ç•¥æµ‹è¯•æ—¶æ˜¾å¼é“¾å¼æ€ç»´æ¨ç†çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§å¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶LENSï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è”åˆä¼˜åŒ–æ¨ç†è¿‡ç¨‹å’Œåˆ†å‰²ã€‚é€šè¿‡ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ å¥–åŠ±ï¼Œæ¶µç›–å¥å­ã€æ¡†å’Œæ®µçº§åˆ«çš„çº¿ç´¢ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„æ€ç»´è¿‡ç¨‹åŒæ—¶æé«˜æ©è†œè´¨é‡ã€‚ä½¿ç”¨å…¬å¼€å¯ç”¨çš„è§†è§‰è¯­è¨€æ¨¡å‹Qwen2.5-VL-3B-Instructï¼ŒLENSåœ¨RefCOCOã€RefCOCO+å’ŒRefCOCOgåŸºå‡†æµ‹è¯•ä¸Šå¹³å‡å®Œå…¨äº¤å¹¶æ¯”è¾¾åˆ°81.2%ï¼Œä¼˜äºç²¾ç»†è°ƒå‚æ–¹æ³•GLaMMï¼Œæœ€é«˜æå‡5.6%ã€‚ç»“æœè¯æ˜å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„é“¾å¼æ€ç»´æ¨ç†ä¸ºæ–‡æœ¬æç¤ºåˆ†å‰²æä¾›äº†ç¨³å¥å…ˆéªŒï¼Œä¸ºå®ç°æ›´é€šç”¨çš„åˆ†å‰²æ¨¡å‹æä¾›äº†å®è·µè·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æç¤ºçš„å›¾åƒåˆ†å‰²æŠ€æœ¯å¯¹äºç²¾ç»†è§†è§‰ç†è§£é‡è¦ï¼Œå°¤å…¶åœ¨äººæœºäº¤äº’å’Œæœºå™¨äººé¢†åŸŸã€‚</li>
<li>ç°æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•å¿½ç•¥æµ‹è¯•æ—¶çš„æ˜¾å¼é“¾å¼æ€ç»´æ¨ç†ï¼Œå½±å“æ¨¡å‹åœ¨æœªè§æç¤ºå’Œé¢†åŸŸä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>LENSæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è”åˆä¼˜åŒ–æ¨ç†è¿‡ç¨‹å’Œåˆ†å‰²ï¼Œå®ç°ç«¯åˆ°ç«¯çš„ä¼˜åŒ–ã€‚</li>
<li>LENSé‡‡ç”¨ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ å¥–åŠ±ï¼Œæ¶µç›–å¥å­ã€æ¡†å’Œæ®µçº§åˆ«çº¿ç´¢ï¼Œæå‡æ©è†œè´¨é‡ã€‚</li>
<li>ä½¿ç”¨Qwen2.5-VL-3B-Instructè§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒLENSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—ä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>LENSå¹³å‡å®Œå…¨äº¤å¹¶æ¯”è¾¾åˆ°81.2%ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæœ€é«˜æå‡5.6%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14153">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14c9c396f0f288498bb5a56bead6bf3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ed583e264da878a7f9a7d740cb51e73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09f46af34f623f7a916628bb36649136.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e31edca7299d20e3515309d7541ec41b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dba8c9f444615125675a7152a246e9a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdbca333e616a340fe8f5d9f17d0baca.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Impact-of-Clinical-Image-Quality-on-Efficient-Foundation-Model-Finetuning"><a href="#Impact-of-Clinical-Image-Quality-on-Efficient-Foundation-Model-Finetuning" class="headerlink" title="Impact of Clinical Image Quality on Efficient Foundation Model   Finetuning"></a>Impact of Clinical Image Quality on Efficient Foundation Model   Finetuning</h2><p><strong>Authors:Yucheng Tang, Pawel Rajwa, Alexander Ng, Yipei Wang, Wen Yan, Natasha Thorley, Aqua Asif, Clare Allen, Louise Dickinson, Francesco Giganti, Shonit Punwani, Daniel C. Alexander, Veeru Kasivisvanathan, Yipeng Hu</strong></p>
<p>Foundation models in medical imaging have shown promising label efficiency, achieving high performance on downstream tasks using only a fraction of the annotated data otherwise required. In this study, we evaluate this potential in the context of prostate multiparametric MRI using ProFound, a recently developed domain-specific vision foundation model pretrained on large-scale prostate MRI datasets. We investigate the impact of variable image quality on the label-efficient finetuning, by quantifying the generalisability of the finetuned models. We conduct a comprehensive set of experiments by systematically varying the ratios of high- and low-quality images in the finetuning and evaluation sets. Our findings indicate that image quality distribution and its finetune-and-test mismatch significantly affect model performance. In particular: a) Varying the ratio of high- to low-quality images between finetuning and test sets leads to notable differences in downstream performance; and b) The presence of sufficient high-quality images in the finetuning set is critical for maintaining strong performance, whilst the importance of matched finetuning and testing distribution varies between different downstream tasks, such as automated radiology reporting and prostate cancer detection. Importantly, experimental results also show that, although finetuning requires significantly less labeled data compared to training from scratch when the quality ratio is consistent, this label efficiency is not independent of the image quality distribution. For example, we show cases that, without sufficient high-quality images in finetuning, finetuned models may fail to outperform those without pretraining. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼ŒåŸºç¡€æ¨¡å‹æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„æ ‡ç­¾æ•ˆç‡ï¼Œå³ä»…ä½¿ç”¨ä¸€å°éƒ¨åˆ†æ ‡æ³¨æ•°æ®å³å¯åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå®ç°é«˜æ€§èƒ½ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹å‰åˆ—è…ºå¤šå‚æ•°MRIï¼Œè¯„ä¼°äº†æ½œåŠ›å·¨å¤§çš„ProFoundæ¨¡å‹çš„æ½œåŠ›ã€‚ProFoundæ˜¯ä¸€ä¸ªé’ˆå¯¹å‰åˆ—è…ºMRIæ•°æ®é›†çš„ç‰¹å®šé¢†åŸŸé¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡é‡åŒ–å¾®è°ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç ”ç©¶äº†ä¸åŒå›¾åƒè´¨é‡å¯¹æ ‡ç­¾æ•ˆç‡å¾®è°ƒçš„å½±å“ã€‚æˆ‘ä»¬é€šè¿‡ç³»ç»Ÿåœ°æ”¹å˜å¾®è°ƒé›†å’Œè¯„ä¼°é›†ä¸­é«˜è´¨é‡å’Œä½è´¨é‡å›¾åƒçš„æ¯”ä¾‹ï¼Œè¿›è¡Œäº†ä¸€ç³»åˆ—å…¨é¢çš„å®éªŒã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå›¾åƒè´¨é‡åˆ†å¸ƒåŠå…¶å¾®è°ƒä¸æµ‹è¯•çš„ä¸åŒ¹é…æ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼šaï¼‰åœ¨å¾®è°ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´æ”¹å˜é«˜è´¨é‡ä¸ä½è´¨é‡å›¾åƒçš„æ¯”ä¾‹ä¼šå¯¼è‡´ä¸‹æ¸¸æ€§èƒ½å‡ºç°æ˜¾è‘—å·®å¼‚ï¼›bï¼‰å¾®è°ƒé›†ä¸­æœ‰è¶³å¤Ÿçš„é«˜è´¨é‡å›¾åƒå¯¹äºä¿æŒå¼ºå¤§æ€§èƒ½è‡³å…³é‡è¦ï¼Œè€ŒåŒ¹é…å¾®è°ƒé›†å’Œæµ‹è¯•é›†çš„åˆ†å¸ƒçš„é‡è¦æ€§åœ¨ä¸åŒä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´æœ‰æ‰€ä¸åŒï¼Œå¦‚è‡ªåŠ¨åŒ–æ”¾å°„æŠ¥å‘Šå’Œå‰åˆ—è…ºç™Œæ£€æµ‹ç­‰ã€‚é‡è¦çš„æ˜¯ï¼Œå®éªŒç»“æœè¿˜è¡¨æ˜ï¼Œå°½ç®¡åœ¨è´¨é‡æ¯”ä¾‹ä¸€è‡´çš„æƒ…å†µä¸‹ï¼Œå¾®è°ƒæ‰€éœ€çš„æ ‡æ³¨æ•°æ®é‡ä¸ä»å¤´å¼€å§‹è®­ç»ƒç›¸æ¯”å¤§å¤§å‡å°‘ï¼Œä½†è¿™ç§æ ‡ç­¾æ•ˆç‡å¹¶ä¸æ˜¯ç‹¬ç«‹äºå›¾åƒè´¨é‡åˆ†å¸ƒçš„ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¦‚æœåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ²¡æœ‰è¶³å¤Ÿçš„é«˜è´¨é‡å›¾åƒï¼Œå¾®è°ƒåçš„æ¨¡å‹å¯èƒ½æ— æ³•è¶…è¶Šæœªè¿›è¡Œé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11864v2">PDF</a> This paper was accepted to the 1st MICCAI Workshop on Efficient   Medical AI (EMA4MICCAI2025) and selected for oral presentation</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºåŒ»å­¦æˆåƒçš„åŸºç¡€æ¨¡å‹åœ¨å‰åˆ—è…ºå¤šå‚æ•°MRIé¢†åŸŸçš„æ ‡ç­¾æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå›¾åƒè´¨é‡åˆ†å¸ƒåŠå…¶åœ¨å¾®è°ƒä¸æµ‹è¯•ä¸­çš„ä¸åŒ¹é…ä¼šæ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ã€‚å½“é«˜è´¨é‡å›¾åƒåœ¨å¾®è°ƒé›†ä¸­å ä¸€å®šæ¯”ä¾‹æ—¶ï¼Œæ¨¡å‹æ€§èƒ½å¾—ä»¥ä¿æŒã€‚ä½†ä¸åŒä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šå’Œå‰åˆ—è…ºç™Œæ£€æµ‹ï¼‰å¯¹åŒ¹é…åº¦è¦æ±‚ä¸åŒã€‚æ­¤å¤–ï¼Œå°½ç®¡åœ¨è´¨é‡æ¯”ç‡ä¸€è‡´çš„æƒ…å†µä¸‹ï¼Œå¾®è°ƒæ‰€éœ€çš„æ ‡æ³¨æ•°æ®è¿œå°‘äºä»å¤´å¼€å§‹è®­ç»ƒï¼Œä½†è¿™ç§æ ‡ç­¾æ•ˆç‡å¹¶éç‹¬ç«‹äºå›¾åƒè´¨é‡åˆ†å¸ƒã€‚è‹¥æ— è¶³å¤Ÿçš„é«˜è´¨é‡å›¾åƒè¿›è¡Œå¾®è°ƒï¼Œé¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½å¯èƒ½æ— æ³•å±•ç°ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­æ˜¾ç¤ºå‡ºå¯¹æ ‡ç­¾æ•ˆç‡çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å‰åˆ—è…ºMRIä¸­ã€‚</li>
<li>å›¾åƒè´¨é‡åˆ†å¸ƒå¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>å¾®è°ƒé›†å’Œæµ‹è¯•é›†ä¸­é«˜è´¨é‡å›¾åƒçš„æ¯”ä¾‹å¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿæ˜¾è‘—å½±å“ã€‚</li>
<li>å……è¶³çš„é«˜è´¨é‡å›¾åƒåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ä¸åŒä¸‹æ¸¸ä»»åŠ¡å¯¹åŒ¹é…åº¦è¦æ±‚ä¸åŒã€‚</li>
<li>å³ä½¿åœ¨è´¨é‡ä¸€è‡´çš„æƒ…å†µä¸‹ï¼Œæ ‡ç­¾æ•ˆç‡å¹¶éç‹¬ç«‹äºå›¾åƒè´¨é‡åˆ†å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11864">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a305285fea824424479029b28e20731.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-222b3e8ce7e3502931e7e66d52699b9b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SketchDNN-Joint-Continuous-Discrete-Diffusion-for-CAD-Sketch-Generation"><a href="#SketchDNN-Joint-Continuous-Discrete-Diffusion-for-CAD-Sketch-Generation" class="headerlink" title="SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation"></a>SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation</h2><p><strong>Authors:Sathvik Chereddy, John Femiani</strong></p>
<p>We present SketchDNN, a generative model for synthesizing CAD sketches that jointly models both continuous parameters and discrete class labels through a unified continuous-discrete diffusion process. Our core innovation is Gaussian-Softmax diffusion, where logits perturbed with Gaussian noise are projected onto the probability simplex via a softmax transformation, facilitating blended class labels for discrete variables. This formulation addresses 2 key challenges, namely, the heterogeneity of primitive parameterizations and the permutation invariance of primitives in CAD sketches. Our approach significantly improves generation quality, reducing Fr&#39;echet Inception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL) from 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch generation on the SketchGraphs dataset. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†SketchDNNï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåˆæˆCADè‰å›¾çš„ç”Ÿæˆæ¨¡å‹ã€‚å®ƒé€šè¿‡ç»Ÿä¸€çš„è¿ç»­-ç¦»æ•£æ‰©æ•£è¿‡ç¨‹å¯¹è¿ç»­å‚æ•°å’Œç¦»æ•£ç±»æ ‡ç­¾è¿›è¡Œè”åˆå»ºæ¨¡ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé«˜æ–¯Softmaxæ‰©æ•£ï¼Œå…¶ä¸­é€šè¿‡å¯¹æ•°å‡ ç‡æ·»åŠ é«˜æ–¯å™ªå£°å¹¶å°†å…¶æŠ•å½±åˆ°æ¦‚ç‡å•çº¯å½¢ä¸Šè¿›è¡ŒSoftmaxè½¬æ¢ï¼Œä¸ºç¦»æ•£å˜é‡æä¾›æ··åˆç±»æ ‡ç­¾ã€‚è¿™ç§è¡¨è¿°è§£å†³äº†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼Œå³åŸå§‹å‚æ•°åŒ–çš„å¼‚è´¨æ€§å’ŒCADè‰å›¾ä¸­åŸå§‹å›¾å½¢çš„æ’åˆ—ä¸å˜æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†ç”Ÿæˆè´¨é‡ï¼Œå°†FrÃ©chet Inception Distanceï¼ˆFIDï¼‰ä»16.04é™è‡³7.80ï¼Œè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰ä»84.8é™è‡³81.33ï¼Œåœ¨SketchGraphsæ•°æ®é›†ä¸Šå»ºç«‹äº†CADè‰å›¾ç”Ÿæˆçš„æ–°æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11579v2">PDF</a> 17 pages, 63 figures, Proceedings of the 42nd International   Conference on Machine Learning (ICML2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SketchDNNæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡ç»Ÿä¸€çš„è¿ç»­-ç¦»æ•£æ‰©æ•£è¿‡ç¨‹ï¼Œå¯¹CADè‰å›¾è¿›è¡Œç”Ÿæˆã€‚å…¶æ ¸å¿ƒåˆ›æ–°æ˜¯Gaussian-Softmaxæ‰©æ•£ï¼Œé€šè¿‡å°†å¸¦æœ‰é«˜æ–¯å™ªå£°çš„å¯¹æ•°å‡ ç‡æ˜ å°„åˆ°æ¦‚ç‡å•çº¯å½¢ä¸Šï¼Œä¸ºç¦»æ•£å˜é‡æä¾›æ··åˆç±»æ ‡ç­¾ã€‚è¯¥æ–¹æ³•è§£å†³äº†CADè‰å›¾å‚æ•°åŒ–çš„å¼‚è´¨æ€§å’ŒåŸå§‹æ’åˆ—çš„ä¸å˜æ€§ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆè´¨é‡ï¼Œåœ¨SketchGraphsæ•°æ®é›†ä¸Šå»ºç«‹äº†æ–°çš„æœ€é«˜æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SketchDNNæ˜¯ä¸€ä¸ªç”¨äºåˆæˆCADè‰å›¾çš„ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡ç»Ÿä¸€çš„è¿ç»­-ç¦»æ•£æ‰©æ•£è¿‡ç¨‹å»ºæ¨¡ã€‚</li>
<li>æ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°æ˜¯Gaussian-Softmaxæ‰©æ•£ï¼Œè§£å†³äº†ç¦»æ•£å˜é‡çš„æ··åˆç±»æ ‡ç­¾é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•è§£å†³äº†CADè‰å›¾å‚æ•°åŒ–çš„å¼‚è´¨æ€§ï¼Œå³ä¸åŒåŸå§‹æ•°æ®çš„å‚æ•°åŒ–é—®é¢˜ã€‚</li>
<li>æ’åˆ—ä¸å˜æ€§é—®é¢˜è¢«æœ‰æ•ˆè§£å†³ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>SketchDNNåœ¨SketchGraphsæ•°æ®é›†ä¸Šçš„ç”Ÿæˆè´¨é‡æ˜¾è‘—æé«˜ï¼ŒFIDä»16.04é™è‡³7.80ã€‚</li>
<li>NLLä»84.8é™è‡³81.33ï¼Œè¯´æ˜æ¨¡å‹åœ¨é¢„æµ‹å’Œç”Ÿæˆæ–¹é¢çš„æ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f6456c804e9de7f650f9d7535cb3aba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07036579544e31f56a1241f861c0c273.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-216d18a04c9c94e7a7b271378bdc2564.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="BRISC-Annotated-Dataset-for-Brain-Tumor-Segmentation-and-Classification-with-Swin-HAFNet"><a href="#BRISC-Annotated-Dataset-for-Brain-Tumor-Segmentation-and-Classification-with-Swin-HAFNet" class="headerlink" title="BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification   with Swin-HAFNet"></a>BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification   with Swin-HAFNet</h2><p><strong>Authors:Amirreza Fateh, Yasin Rezvani, Sara Moayedi, Sadjad Rezvani, Fatemeh Fateh, Mansoor Fateh</strong></p>
<p>Accurate segmentation and classification of brain tumors from Magnetic Resonance Imaging (MRI) remain key challenges in medical image analysis. This is primarily due to the lack of high-quality, balanced, and diverse datasets. In this work, we present a newly developed MRI dataset named BRISC designed specifically for brain tumor segmentation and classification tasks. The dataset comprises 6,000 contrast-enhanced T1-weighted MRI scans annotated by certified radiologists and physicians. It includes three major tumor types, namely glioma, meningioma, and pituitary, as well as non-tumorous cases. Each sample includes high-resolution labels and is categorized across axial, sagittal, and coronal imaging planes to facilitate robust model development and cross-view generalization. To demonstrate the utility of the dataset, we propose a transformer-based segmentation model and benchmark it against established baselines. In this work, we propose a transformer-based model designed for both segmentation and classification of brain tumors, leveraging multi-scale feature representations from a Swin Transformer backbone. The model is benchmarked against established baselines to demonstrate the utility of the dataset, enabling accurate segmentation and robust classification across four diagnostic categories: glioma, meningioma, pituitary, and non-tumorous cases. In this work, our proposed transformer-based model demonstrates superior performance in both segmentation and classification tasks for brain tumor analysis. For the segmentation task, the method achieves the highest weighted mean Intersection-over-Union (IoU) of 82.3%, with improvements observed across all tumor categories. For the classification task, the model attains an accuracy of 99.63%, effectively distinguishing between glioma, meningioma, pituitary, and non-tumorous cases. <a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/briscdataset/brisc2025/">https://www.kaggle.com/datasets/briscdataset/brisc2025/</a> </p>
<blockquote>
<p>ä»ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å¯¹è„‘è‚¿ç˜¤è¿›è¡Œç²¾ç¡®åˆ†å‰²å’Œåˆ†ç±»ï¼Œä»æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚è¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹é«˜è´¨é‡ã€å¹³è¡¡å’Œå¤šæ ·åŒ–çš„æ•°æ®é›†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªæ–°å¼€å‘çš„ä¸“é—¨ç”¨äºè„‘è‚¿ç˜¤åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡çš„MRIæ•°æ®é›†ï¼Œåä¸ºBRISCæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«ç”±è®¤è¯æ”¾å°„ç§‘åŒ»ç”Ÿå’ŒåŒ»å¸ˆæ³¨é‡Šçš„6000ä»½å¯¹æ¯”å¢å¼ºT1åŠ æƒMRIæ‰«æã€‚å®ƒåŒ…æ‹¬ä¸‰ç§ä¸»è¦çš„è‚¿ç˜¤ç±»å‹ï¼Œå³èƒ¶è´¨ç˜¤ã€è„‘è†œç˜¤å’Œå‚ä½“ç˜¤ï¼Œä»¥åŠéè‚¿ç˜¤æƒ…å†µã€‚æ¯ä¸ªæ ·æœ¬éƒ½åŒ…å«é«˜åˆ†è¾¨ç‡çš„æ ‡ç­¾ï¼Œå¹¶åœ¨è½´å‘ã€çŸ¢çŠ¶é¢å’Œå† çŠ¶é¢æˆåƒå¹³é¢ä¸Šè¿›è¡Œåˆ†ç±»ï¼Œä»¥ä¿ƒè¿›ç¨³å¥çš„æ¨¡å‹å¼€å‘å’Œè·¨è§†å›¾æ³›åŒ–ã€‚ä¸ºäº†è¯æ˜æ•°æ®é›†çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå˜å‹å™¨çš„åˆ†å‰²æ¨¡å‹ï¼Œå¹¶å°†å…¶ä¸æ—¢å®šçš„åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ—¨åœ¨å®ç°è„‘è‚¿ç˜¤çš„åˆ†å‰²å’Œåˆ†ç±»ï¼Œåˆ©ç”¨Swin Transformeréª¨å¹²ç½‘çš„å¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºã€‚è¯¥æ¨¡å‹ä¸æ—¢å®šçš„åŸºçº¿è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯æ˜æ•°æ®é›†çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿå®ç°å››ç§è¯Šæ–­ç±»åˆ«ï¼ˆå³èƒ¶è´¨ç˜¤ã€è„‘è†œç˜¤ã€å‚ä½“ç˜¤å’Œéè‚¿ç˜¤æƒ…å†µï¼‰çš„å‡†ç¡®åˆ†å‰²å’Œç¨³å¥åˆ†ç±»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºçš„åŸºäºå˜å‹å™¨çš„æ¨¡å‹åœ¨è„‘è‚¿ç˜¤åˆ†æçš„åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å¯¹äºåˆ†å‰²ä»»åŠ¡ï¼Œè¯¥æ–¹æ³•è·å¾—äº†æœ€é«˜çš„åŠ æƒå¹³å‡äº¤å¹¶æ¯”ï¼ˆIoUï¼‰ä¸º82.3%ï¼Œæ‰€æœ‰è‚¿ç˜¤ç±»åˆ«çš„æ€§èƒ½éƒ½æœ‰æ‰€æé«˜ã€‚å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œè¯¥æ¨¡å‹çš„å‡†ç¡®ç‡ä¸º99.63%ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°åŒºåˆ†èƒ¶è´¨ç˜¤ã€è„‘è†œç˜¤ã€å‚ä½“ç˜¤å’Œéè‚¿ç˜¤æƒ…å†µã€‚æ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/briscdataset/brisc2025/%E8%8E%B7%E5%8F%96%E3%80%82">https://www.kaggle.com/datasets/briscdataset/brisc2025/è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14318v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°å¼€å‘çš„ä¸“é—¨ç”¨äºè„‘è‚¿ç˜¤åˆ†å‰²å’Œåˆ†ç±»çš„MRIæ•°æ®é›†BRISCã€‚è¯¥æ•°æ®é›†åŒ…å«ç”±è®¤è¯æ”¾å°„å­¦å®¶å’ŒåŒ»å¸ˆæ³¨é‡Šçš„6,000ä»½å¯¹æ¯”å¢å¼ºçš„T1åŠ æƒMRIæ‰«æï¼Œæ¶µç›–ä¸‰ç§ä¸»è¦è‚¿ç˜¤ç±»å‹ï¼ˆèƒ¶è´¨ç˜¤ã€è„‘è†œç˜¤å’Œå‚ä½“ç˜¤ï¼‰ä»¥åŠéè‚¿ç˜¤ç—…ä¾‹ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§åŸºäºtransformerçš„è„‘è‚¿ç˜¤åˆ†å‰²ä¸åˆ†ç±»æ¨¡å‹ï¼Œå¹¶åœ¨è¯¥æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ä¸ªåä¸ºBRISCçš„æ–°MRIæ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºè„‘è‚¿ç˜¤åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>æ•°æ®é›†åŒ…å«6,000ä»½ç”±ä¸“å®¶æ³¨é‡Šçš„å¯¹æ¯”å¢å¼ºT1åŠ æƒMRIæ‰«æã€‚</li>
<li>æ•°æ®é›†æ¶µç›–ä¸‰ç§ä¸»è¦è‚¿ç˜¤ç±»å‹ï¼šèƒ¶è´¨ç˜¤ã€è„‘è†œç˜¤å’Œå‚ä½“ç˜¤ï¼Œä»¥åŠéè‚¿ç˜¤ç—…ä¾‹ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåŸºäºtransformerçš„è„‘è‚¿ç˜¤åˆ†å‰²ä¸åˆ†ç±»æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹åˆ©ç”¨Swin Transformerçš„å¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºï¼Œé’ˆå¯¹å››ç§è¯Šæ–­ç±»åˆ«è¿›è¡Œæ€§èƒ½è¯„ä¼°ã€‚</li>
<li>æ¨¡å‹åœ¨åˆ†å‰²ä»»åŠ¡ä¸Šå–å¾—äº†æœ€é«˜åŠ æƒå¹³å‡IoUå€¼ä¸º82.3%ï¼Œåœ¨å„ç±»è‚¿ç˜¤ä¸­éƒ½è§‚å¯Ÿåˆ°äº†æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5b8c0d4297aeb02710832e155b6247b1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PRS-Med-Position-Reasoning-Segmentation-with-Vision-Language-Model-in-Medical-Imaging"><a href="#PRS-Med-Position-Reasoning-Segmentation-with-Vision-Language-Model-in-Medical-Imaging" class="headerlink" title="PRS-Med: Position Reasoning Segmentation with Vision-Language Model in   Medical Imaging"></a>PRS-Med: Position Reasoning Segmentation with Vision-Language Model in   Medical Imaging</h2><p><strong>Authors:Quoc-Huy Trinh, Minh-Van Nguyen, Jung Zeng, Ulas Bagci, Debesh Jha</strong></p>
<p>Recent advancements in prompt-based medical image segmentation have enabled clinicians to identify tumors using simple input like bounding boxes or text prompts. However, existing methods face challenges when doctors need to interact through natural language or when position reasoning is required - understanding spatial relationships between anatomical structures and pathologies. We present PRS-Med, a framework that integrates vision-language models with segmentation capabilities to generate both accurate segmentation masks and corresponding spatial reasoning outputs. Additionally, we introduce the MMRS dataset (Multimodal Medical in Positional Reasoning Segmentation), which provides diverse, spatially-grounded question-answer pairs to address the lack of position reasoning data in medical imaging. PRS-Med demonstrates superior performance across six imaging modalities (CT, MRI, X-ray, ultrasound, endoscopy, RGB), significantly outperforming state-of-the-art methods in both segmentation accuracy and position reasoning. Our approach enables intuitive doctor-system interaction through natural language, facilitating more efficient diagnoses. Our dataset pipeline, model, and codebase will be released to foster further research in spatially-aware multimodal reasoning for medical applications. </p>
<blockquote>
<p>åœ¨åŸºäºæç¤ºçš„åŒ»ç–—å›¾åƒåˆ†å‰²æ–¹é¢çš„æœ€æ–°è¿›å±•ï¼Œå·²ç»ä½¿ä¸´åºŠåŒ»ç”Ÿèƒ½å¤Ÿä½¿ç”¨ç®€å•çš„è¾“å…¥ï¼Œå¦‚è¾¹ç•Œæ¡†æˆ–æ–‡æœ¬æç¤ºæ¥è¯†åˆ«è‚¿ç˜¤ã€‚ç„¶è€Œï¼Œå½“åŒ»ç”Ÿéœ€è¦é€šè¿‡è‡ªç„¶è¯­è¨€è¿›è¡Œäº¤äº’æˆ–éœ€è¦ä½ç½®æ¨ç†ï¼ˆå³ç†è§£è§£å‰–ç»“æ„å’Œç—…ç†ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼‰æ—¶ï¼Œç°æœ‰æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†PRS-Medæ¡†æ¶ï¼Œå®ƒç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹å’Œåˆ†å‰²èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆå‡†ç¡®çš„åˆ†å‰²æ©è†œå’Œç›¸åº”çš„ç©ºé—´æ¨ç†è¾“å‡ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†MMRSæ•°æ®é›†ï¼ˆå®šä½æ¨ç†åˆ†å‰²ä¸­çš„å¤šæ¨¡æ€åŒ»ç–—æ•°æ®ï¼‰ï¼Œè¯¥æ•°æ®é›†æä¾›äº†å¤šæ ·ä¸”åŸºäºç©ºé—´çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œä»¥è§£å†³åŒ»ç–—å½±åƒä¸­å®šä½æ¨ç†æ•°æ®çš„ç¼ºä¹ã€‚PRS-Medåœ¨å…­ç§æˆåƒæ¨¡å¼ï¼ˆCTã€MRIã€Xå…‰ã€è¶…å£°ã€å†…çª¥é•œã€RGBï¼‰ä¸‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨åˆ†å‰²ç²¾åº¦å’Œä½ç½®æ¨ç†æ–¹é¢éƒ½å¤§å¤§ä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€å®ç°ç›´è§‚çš„åŒ»ç”Ÿä¸ç³»ç»Ÿçš„äº¤äº’ï¼Œä¿ƒè¿›æ›´æœ‰æ•ˆçš„è¯Šæ–­ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ç®¡é“ã€æ¨¡å‹å’Œä»£ç åº“å°†äºˆä»¥å‘å¸ƒï¼Œä»¥ä¿ƒè¿›åœ¨åŒ»ç–—åº”ç”¨ä¸­å…·å¤‡ç©ºé—´æ„ŸçŸ¥çš„å¤šæ¨¡æ€æ¨ç†çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11872v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€åŸºäºæç¤ºçš„åŒ»ç–—å›¾åƒåˆ†å‰²æŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼ŒåŒ»ç”Ÿç°åœ¨å¯ä»¥ä½¿ç”¨ç®€å•çš„è¾“å…¥ï¼Œå¦‚è¾¹ç•Œæ¡†æˆ–æ–‡æœ¬æç¤ºæ¥è¯†åˆ«è‚¿ç˜¤ã€‚ç„¶è€Œï¼Œå½“åŒ»ç”Ÿéœ€è¦é€šè¿‡è‡ªç„¶è¯­è¨€è¿›è¡Œäº¤äº’æˆ–éœ€è¦ä½ç½®æ¨ç†ï¼ˆå³ç†è§£è§£å‰–ç»“æ„å’Œç—…ç†ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼‰æ—¶ï¼Œç°æœ‰æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†PRS-Medæ¡†æ¶ï¼Œå®ƒæ•´åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ä¸åˆ†å‰²èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆå‡†ç¡®çš„åˆ†å‰²æ©è†œå’Œç›¸åº”çš„ç©ºé—´æ¨ç†è¾“å‡ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†MMRSæ•°æ®é›†ï¼ˆä½ç½®æ¨ç†åˆ†å‰²ä¸­çš„å¤šæ¨¡æ€åŒ»ç–—å›¾åƒï¼‰ï¼Œè¯¥æ•°æ®é›†æä¾›äº†å¤šæ ·ä¸”åŸºäºç©ºé—´çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œä»¥è§£å†³åŒ»ç–—æˆåƒä¸­ä½ç½®æ¨ç†æ•°æ®çš„ç¼ºä¹ã€‚PRS-Medåœ¨å…­ç§æˆåƒæ¨¡å¼ï¼ˆCTã€MRIã€Xå…‰ã€è¶…å£°ã€å†…çª¥é•œã€RGBï¼‰ä¸‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨åˆ†å‰²å‡†ç¡®æ€§å’Œä½ç½®æ¨ç†æ–¹é¢éƒ½å¤§å¤§ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è‡ªç„¶è¯­è¨€å®ç°äº†åŒ»ç”Ÿä¸ç³»ç»Ÿä¹‹é—´çš„ç›´è§‚äº¤äº’ï¼Œä¿ƒè¿›äº†æ›´é«˜æ•ˆçš„è¯Šæ–­ã€‚æˆ‘ä»¬å°†å…¬å¼€æ•°æ®é›†ç®¡é“ã€æ¨¡å‹å’Œæºä»£ç ï¼Œä»¥ä¿ƒè¿›åœ¨ç©ºé—´æ„ŸçŸ¥å¤šæ¨¡æ€æ¨ç†åŒ»ç–—åº”ç”¨æ–¹é¢çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PRS-Medæ¡†æ¶ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ä¸åˆ†å‰²æŠ€æœ¯ï¼Œèƒ½ç”Ÿæˆå‡†ç¡®çš„åˆ†å‰²æ©è†œå’Œç©ºé—´æ¨ç†è¾“å‡ºã€‚</li>
<li>MMRSæ•°æ®é›†çš„æ¨å‡ºè§£å†³äº†åŒ»ç–—å›¾åƒä¸­ä½ç½®æ¨ç†æ•°æ®çš„ç¼ºä¹é—®é¢˜ã€‚</li>
<li>PRS-Medæ¡†æ¶åœ¨å…­ç§åŒ»ç–—æˆåƒæ¨¡å¼ä¸‹æ€§èƒ½å“è¶Šï¼Œè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è‡ªç„¶è¯­è¨€å®ç°äº†åŒ»ç”Ÿä¸ç³»ç»Ÿä¹‹é—´çš„ç›´è§‚äº¤äº’ã€‚</li>
<li>PRS-Medæœ‰åŠ©äºæé«˜è¯Šæ–­æ•ˆç‡ã€‚</li>
<li>PRS-Medæ¡†æ¶å’ŒMMRSæ•°æ®é›†å…¬å¼€å¯ç”¨ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11872">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24c72df0c03930dae7b3894c4bd1a122.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88b8259fd00e0a3279f516df426dc1fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b8c7c4661a1d567be659cb5fcdf1fab.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="mAIstro-an-open-source-multi-agentic-system-for-automated-end-to-end-development-of-radiomics-and-deep-learning-models-for-medical-imaging"><a href="#mAIstro-an-open-source-multi-agentic-system-for-automated-end-to-end-development-of-radiomics-and-deep-learning-models-for-medical-imaging" class="headerlink" title="mAIstro: an open-source multi-agentic system for automated end-to-end   development of radiomics and deep learning models for medical imaging"></a>mAIstro: an open-source multi-agentic system for automated end-to-end   development of radiomics and deep learning models for medical imaging</h2><p><strong>Authors:Eleftherios Tzanis, Michail E. Klontzas</strong></p>
<p>Agentic systems built on large language models (LLMs) offer promising capabilities for automating complex workflows in healthcare AI. We introduce mAIstro, an open-source, autonomous multi-agentic framework for end-to-end development and deployment of medical AI models. The system orchestrates exploratory data analysis, radiomic feature extraction, image segmentation, classification, and regression through a natural language interface, requiring no coding from the user. Built on a modular architecture, mAIstro supports both open- and closed-source LLMs, and was evaluated using a large and diverse set of prompts across 16 open-source datasets, covering a wide range of imaging modalities, anatomical regions, and data types. The agents successfully executed all tasks, producing interpretable outputs and validated models. This work presents the first agentic framework capable of unifying data analysis, AI model development, and inference across varied healthcare applications, offering a reproducible and extensible foundation for clinical and research AI integration. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/eltzanis/mAIstro">https://github.com/eltzanis/mAIstro</a> </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„Agenticç³»ç»Ÿä¸ºåŒ»ç–—ä¿å¥AIä¸­çš„å¤æ‚å·¥ä½œæµç¨‹è‡ªåŠ¨åŒ–æä¾›äº†æœ‰å‰é€”çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä»‹ç»äº†mAIstroï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„ã€è‡ªä¸»çš„å¤šAgenticæ¡†æ¶ï¼Œç”¨äºç«¯åˆ°ç«¯çš„åŒ»ç–—AIæ¨¡å‹å¼€å‘å’Œéƒ¨ç½²ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æ¥å£åè°ƒæ¢ç´¢æ€§åˆ†æã€æ”¾å°„å­¦ç‰¹å¾æå–ã€å›¾åƒåˆ†å‰²ã€åˆ†ç±»å’Œå›å½’ï¼Œæ— éœ€ç”¨æˆ·ç¼–å†™ä»£ç ã€‚mAIstroé‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼Œæ”¯æŒå¼€æºå’Œé—­æºçš„LLMï¼Œå¹¶ä½¿ç”¨æ¶µç›–å¹¿æ³›æˆåƒæ¨¡å¼ã€è§£å‰–åŒºåŸŸå’Œæ•°æ®ç±»å‹çš„16ä¸ªå¼€æºæ•°æ®é›†çš„å¤§å‹ä¸”å¤šæ ·åŒ–çš„æç¤ºé›†è¿›è¡Œè¯„ä¼°ã€‚ä»£ç†æˆåŠŸæ‰§è¡Œäº†æ‰€æœ‰ä»»åŠ¡ï¼Œäº§ç”Ÿäº†å¯è§£é‡Šçš„è¾“å‡ºå’Œç»è¿‡éªŒè¯çš„æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ç¬¬ä¸€ä¸ªèƒ½å¤Ÿåœ¨å„ç§åŒ»ç–—ä¿å¥åº”ç”¨ä¸­ç»Ÿä¸€æ•°æ®åˆ†æã€AIæ¨¡å‹å¼€å‘å’Œæ¨ç†çš„Agenticæ¡†æ¶ï¼Œä¸ºä¸´åºŠå’Œç ”ç©¶AIé›†æˆæä¾›äº†å¯å¤åˆ¶å’Œå¯æ‰©å±•çš„åŸºç¡€ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/eltzanis/mAIstro">https://github.com/eltzanis/mAIstro</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03785v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>mAIstroæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¼€æºã€è‡ªä¸»çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºåŒ»ç–—AIçš„ç«¯åˆ°ç«¯å¼€å‘å’Œéƒ¨ç½²ã€‚å®ƒé€šè¿‡è‡ªç„¶è¯­è¨€æ¥å£åè°ƒæ•°æ®æ¢ç´¢æ€§åˆ†æã€æ”¾å°„å­¦ç‰¹å¾æå–ã€å›¾åƒåˆ†å‰²ã€åˆ†ç±»å’Œå›å½’ï¼Œæ— éœ€ç”¨æˆ·ç¼–ç ã€‚è¯¥æ¡†æ¶æ”¯æŒå¼€æºå’Œé—­æºLLMsï¼Œç»è¿‡åœ¨16ä¸ªå¼€æºæ•°æ®é›†ä¸Šçš„å¤§é‡å’Œå¤šæ ·åŒ–çš„æç¤ºè¯„ä¼°ï¼ŒæˆåŠŸæ‰§è¡Œä»»åŠ¡ï¼Œäº§ç”Ÿå¯è§£é‡Šçš„è¾“å‡ºå’Œç»è¿‡éªŒè¯çš„æ¨¡å‹ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿåœ¨åŒ»ç–—ä¿å¥åº”ç”¨ä¸­ç»Ÿä¸€æ•°æ®åˆ†æã€AIæ¨¡å‹å¼€å‘å’Œæ¨ç†çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œä¸ºä¸´åºŠå’Œç ”ç©¶AIé›†æˆæä¾›äº†å¯å¤åˆ¶å’Œå¯æ‰©å±•çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>mAIstroæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æºè‡ªä¸»å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºåŒ»ç–—AIçš„ç«¯åˆ°ç«¯å¼€å‘å’Œéƒ¨ç½²ã€‚</li>
<li>é€šè¿‡è‡ªç„¶è¯­è¨€æ¥å£ï¼ŒmAIstroåè°ƒå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ•°æ®æ¢ç´¢æ€§åˆ†æã€æ”¾å°„å­¦ç‰¹å¾æå–ã€å›¾åƒåˆ†å‰²ã€åˆ†ç±»å’Œå›å½’ã€‚</li>
<li>è¯¥æ¡†æ¶æ— éœ€ç”¨æˆ·ç¼–ç ï¼Œé™ä½äº†ä½¿ç”¨é—¨æ§›ã€‚</li>
<li>mAIstroæ”¯æŒå¼€æºå’Œé—­æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ç»è¿‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼ŒmAIstroæˆåŠŸæ‰§è¡Œä»»åŠ¡ï¼Œäº§ç”Ÿå¯è§£é‡Šçš„è¾“å‡ºå’Œç»è¿‡éªŒè¯çš„æ¨¡å‹ã€‚</li>
<li>mAIstroä¸ºä¸´åºŠå’Œç ”ç©¶AIé›†æˆæä¾›äº†å¯å¤åˆ¶å’Œå¯æ‰©å±•çš„åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5390074709152f2f05ba2776966b9f34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76c2fc88c3e80b59e2fec82b783ac4fd.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Diffusion-Based-Ambiguous-Image-Segmentation"><a href="#Diffusion-Based-Ambiguous-Image-Segmentation" class="headerlink" title="Diffusion Based Ambiguous Image Segmentation"></a>Diffusion Based Ambiguous Image Segmentation</h2><p><strong>Authors:Jakob LÃ¸nborg Christensen, Morten Rieger Hannemose, Anders Bjorholm Dahl, Vedrana Andersen Dahl</strong></p>
<p>Medical image segmentation often involves inherent uncertainty due to variations in expert annotations. Capturing this uncertainty is an important goal and previous works have used various generative image models for the purpose of representing the full distribution of plausible expert ground truths. In this work, we explore the design space of diffusion models for generative segmentation, investigating the impact of noise schedules, prediction types, and loss weightings. Notably, we find that making the noise schedule harder with input scaling significantly improves performance. We conclude that x- and v-prediction outperform epsilon-prediction, likely because the diffusion process is in the discrete segmentation domain. Many loss weightings achieve similar performance as long as they give enough weight to the end of the diffusion process. We base our experiments on the LIDC-IDRI lung lesion dataset and obtain state-of-the-art (SOTA) performance. Additionally, we introduce a randomly cropped variant of the LIDC-IDRI dataset that is better suited for uncertainty in image segmentation. Our model also achieves SOTA in this harder setting. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ç»å¸¸æ¶‰åŠç”±äºä¸“å®¶æ ‡æ³¨å˜åŒ–è€Œå¯¼è‡´çš„å›ºæœ‰ä¸ç¡®å®šæ€§ã€‚æ•æ‰è¿™ç§ä¸ç¡®å®šæ€§æ˜¯ä¸€ä¸ªé‡è¦ç›®æ ‡ï¼Œä¹‹å‰çš„ç ”ç©¶å·²ç»ä½¿ç”¨å„ç§ç”Ÿæˆå›¾åƒæ¨¡å‹æ¥è¡¨ç¤ºä¸“å®¶çœŸå®æ ‡æ³¨çš„å…¨åˆ†å¸ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ç”Ÿæˆåˆ†å‰²æ‰©æ•£æ¨¡å‹çš„è®¾è®¡ç©ºé—´ï¼Œç ”ç©¶äº†å™ªå£°å®‰æ’ã€é¢„æµ‹ç±»å‹å’ŒæŸå¤±æƒé‡çš„å½±å“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°é€šè¿‡è¾“å…¥ç¼©æ”¾ä½¿å™ªå£°å®‰æ’æ›´åŠ å›°éš¾å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œxé¢„æµ‹å’Œvé¢„æµ‹ä¼˜äºÎµé¢„æµ‹ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºæ‰©æ•£è¿‡ç¨‹å¤„äºç¦»æ•£åˆ†å‰²åŸŸä¸­ã€‚åªè¦å¯¹æ‰©æ•£è¿‡ç¨‹çš„ç»“æŸç»™äºˆè¶³å¤Ÿçš„é‡è§†ï¼Œè®¸å¤šæŸå¤±æƒé‡éƒ½èƒ½è¾¾åˆ°ç±»ä¼¼çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒåŸºäºLIDC-IDRIè‚ºç—…å˜æ•°æ®é›†ï¼Œå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†LIDC-IDRIæ•°æ®é›†çš„éšæœºè£å‰ªå˜ç§ï¼Œæ›´é€‚åˆäºå›¾åƒåˆ†å‰²çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨è¿™ä¸ªæ›´å›°éš¾çš„è®¾ç½®ä¸­ä¹Ÿè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05977v2">PDF</a> Accepted at SCIA25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†å‰²çš„ä¸ç¡®å®šæ€§é—®é¢˜ï¼Œé€šè¿‡è°ƒæ•´å™ªå£°è°ƒåº¦ã€é¢„æµ‹ç±»å‹å’ŒæŸå¤±æƒé‡æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œè¾“å…¥å°ºåº¦ä¸ŠåŠ å¤§å™ªå£°è°ƒåº¦éš¾åº¦èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œx-å’Œv-é¢„æµ‹ä¼˜äºepsiloné¢„æµ‹ï¼Œå¤šç§æŸå¤±æƒé‡åœ¨ç»™äºˆè¶³å¤Ÿé‡è§†æ‰©æ•£è¿‡ç¨‹æœ«æœŸçš„æƒ…å†µä¸‹èƒ½å–å¾—ç›¸ä¼¼æ€§èƒ½ã€‚å®éªŒåŸºäºLIDC-IDRIè‚ºç—…ç¶æ•°æ®é›†è¿›è¡Œï¼Œå¹¶å¼•å…¥éšæœºè£å‰ªçš„LIDC-IDRIæ•°æ®é›†å˜ä½“ä»¥æ›´å¥½åœ°é€‚åº”å›¾åƒåˆ†å‰²çš„ä¸ç¡®å®šæ€§é—®é¢˜ï¼Œæ¨¡å‹åœ¨æ›´å›°éš¾çš„ç¯å¢ƒä¸‹ä¹Ÿå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å­˜åœ¨å› ä¸“å®¶æ ‡æ³¨å·®å¼‚å¯¼è‡´çš„å›ºæœ‰ä¸ç¡®å®šæ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒåˆ†å‰²ä¸­çš„è®¾è®¡ç©ºé—´è¿›è¡Œäº†æ¢ç´¢ã€‚</li>
<li>å™ªå£°è°ƒåº¦éš¾åº¦çš„å¢åŠ èƒ½æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>x-å’Œv-é¢„æµ‹åœ¨ç¦»æ•£åˆ†å‰²åŸŸçš„æ‰©æ•£è¿‡ç¨‹ä¸­è¡¨ç°ä¼˜äºepsiloné¢„æµ‹ã€‚</li>
<li>å¤šç§æŸå¤±æƒé‡åœ¨å¹³è¡¡ä¸‹èƒ½å–å¾—è‰¯å¥½æ€§èƒ½ï¼Œé‡ç‚¹æ˜¯ç»™äºˆæ‰©æ•£è¿‡ç¨‹æœ«æœŸè¶³å¤Ÿçš„é‡è§†ã€‚</li>
<li>å®éªŒåŸºäºLIDC-IDRIè‚ºç—…ç¶æ•°æ®é›†è¿›è¡Œï¼Œå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-45a8dca888124eeed1232d087ca9d8ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6de7ffaa87d20e553c841ce75a2cb731.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4512d984a41c79504eec109e680c6028.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Med3DVLM-An-Efficient-Vision-Language-Model-for-3D-Medical-Image-Analysis"><a href="#Med3DVLM-An-Efficient-Vision-Language-Model-for-3D-Medical-Image-Analysis" class="headerlink" title="Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image   Analysis"></a>Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image   Analysis</h2><p><strong>Authors:Yu Xin, Gorkem Can Ates, Kuang Gong, Wei Shao</strong></p>
<p>Vision-language models (VLMs) have shown promise in 2D medical image analysis, but extending them to 3D remains challenging due to the high computational demands of volumetric data and the difficulty of aligning 3D spatial features with clinical text. We present Med3DVLM, a 3D VLM designed to address these challenges through three key innovations: (1) DCFormer, an efficient encoder that uses decomposed 3D convolutions to capture fine-grained spatial features at scale; (2) SigLIP, a contrastive learning strategy with pairwise sigmoid loss that improves image-text alignment without relying on large negative batches; and (3) a dual-stream MLP-Mixer projector that fuses low- and high-level image features with text embeddings for richer multi-modal representations. We evaluate our model on the M3D dataset, which includes radiology reports and VQA data for 120,084 3D medical images. Results show that Med3DVLM achieves superior performance across multiple benchmarks. For image-text retrieval, it reaches 61.00% R@1 on 2,000 samples, significantly outperforming the current state-of-the-art M3D model (19.10%). For report generation, it achieves a METEOR score of 36.42% (vs. 14.38%). In open-ended visual question answering (VQA), it scores 36.76% METEOR (vs. 33.58%), and in closed-ended VQA, it achieves 79.95% accuracy (vs. 75.78%). These results highlight Med3DVLMâ€™s ability to bridge the gap between 3D imaging and language, enabling scalable, multi-task reasoning across clinical applications. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/mirthAI/Med3DVLM">https://github.com/mirthAI/Med3DVLM</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨2DåŒ»å­¦å›¾åƒåˆ†ææ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å°†å…¶æ‰©å±•åˆ°3Dä»å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºä½“ç§¯æ•°æ®çš„è®¡ç®—éœ€æ±‚é«˜ä»¥åŠå°†3Dç©ºé—´ç‰¹å¾å¯¹é½ä¸´åºŠæ–‡æœ¬çš„å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†Med3DVLMï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹è¿™äº›æŒ‘æˆ˜è®¾è®¡çš„3D VLMï¼Œå®ƒå…·æœ‰ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰DCFormerï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„ç¼–ç å™¨ï¼Œä½¿ç”¨åˆ†è§£çš„3Då·ç§¯æ¥æ•è·å¤§è§„æ¨¡çš„ç²¾ç»†ç©ºé—´ç‰¹å¾ï¼›ï¼ˆ2ï¼‰SigLIPï¼Œè¿™æ˜¯ä¸€ç§å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œå…·æœ‰åŸºäºsigmoidçš„é…å¯¹æŸå¤±å‡½æ•°ï¼Œå®ƒæé«˜äº†å›¾åƒæ–‡æœ¬çš„å‡†ç¡®æ€§ï¼Œæ— éœ€ä¾èµ–å¤§å‹è´Ÿé¢æ‰¹æ¬¡ï¼›ï¼ˆ3ï¼‰åŒæµMLPæ··åˆå™¨æŠ•å½±ä»ªèåˆäº†ä½çº§å’Œé«˜çº§å›¾åƒç‰¹å¾ä¸æ–‡æœ¬åµŒå…¥ï¼Œç”¨äºæ›´ä¸°å¯Œçš„å¤šæ¨¡å¼è¡¨ç¤ºã€‚æˆ‘ä»¬åœ¨M3Dæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œè¯¥æ•°æ®é›†åŒ…å«ç”¨äº120,084ä¸ª3DåŒ»å­¦å›¾åƒçš„æ”¾å°„å­¦æŠ¥å‘Šå’Œè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ•°æ®ã€‚ç»“æœè¡¨æ˜ï¼ŒMed3DVLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚åœ¨å›¾åƒæ–‡æœ¬æ£€ç´¢æ–¹é¢ï¼Œå®ƒåœ¨2,000ä¸ªæ ·æœ¬ä¸Šè¾¾åˆ°äº†61.00ï¼…çš„R@1å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„M3Dæ¨¡å‹ï¼ˆ19.10ï¼…ï¼‰ã€‚åœ¨æŠ¥å‘Šç”Ÿæˆæ–¹é¢ï¼Œå®ƒçš„METEORå¾—åˆ†ä¸º36.42ï¼…ï¼ˆå¯¹æ¯”ä¸º14.38ï¼…ï¼‰ã€‚åœ¨å¼€æ”¾å¼çš„è§†è§‰é—®ç­”ä¸­ï¼Œå®ƒçš„METEORå¾—åˆ†ä¸º36.76ï¼…ï¼ˆå¯¹æ¯”ä¸º33.58ï¼…ï¼‰ï¼Œè€Œåœ¨å°é—­å¼çš„VQAä¸­ï¼Œå®ƒè¾¾åˆ°äº†79.95ï¼…çš„å‡†ç¡®ç‡ï¼ˆå¯¹æ¯”ä¸º75.78ï¼…ï¼‰ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†Med3DVLMåœ¨è¿æ¥3Dæˆåƒå’Œè¯­è¨€æ–¹é¢çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸´åºŠåº”ç”¨ä¸­å®ç°å¯æ‰©å±•çš„å¤šä»»åŠ¡æ¨ç†ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mirthAI/Med3DVLM%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/mirthAI/Med3DVLMå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20047v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Med3DVLMæ˜¯ä¸€æ¬¾é’ˆå¯¹åŒ»å­¦å›¾åƒé¢†åŸŸçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤„ç†ä¸‰ç»´åŒ»å­¦å›¾åƒã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‰é¡¹å…³é”®æŠ€æœ¯çªç ´å®ç°äº†é«˜æ•ˆä¸å‡†ç¡®çš„æ€§èƒ½ï¼šDCFormerç¼–ç å™¨ã€SigLIPå¯¹æ¯”å­¦ä¹ ç­–ç•¥å’Œèåˆé«˜ä½å±‚çº§å›¾åƒç‰¹å¾çš„åŒæµMLPæ··åˆå™¨æŠ•å½±å™¨ã€‚åœ¨M3Dæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMed3DVLMåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒæ–‡æœ¬æ£€ç´¢ã€æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med3DVLMæ˜¯ä¸€æ¬¾é’ˆå¯¹ä¸‰ç»´åŒ»å­¦å›¾åƒè®¾è®¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡ä¸‰é¡¹å…³é”®æŠ€æœ¯çªç ´å®ç°æ€§èƒ½æå‡ï¼šDCFormerç¼–ç å™¨ç”¨äºæ•æ‰ç²¾ç»†çš„ç©ºé—´ç‰¹å¾ï¼ŒSigLIPå¯¹æ¯”å­¦ä¹ ç­–ç•¥æ”¹è¿›å›¾åƒæ–‡æœ¬å¯¹é½ï¼ŒåŒæµMLPæ··åˆå™¨æŠ•å½±å™¨èåˆå¤šæ¨¡æ€è¡¨ç¤ºã€‚</li>
<li>åœ¨M3Dæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†Med3DVLMçš„ä¼˜è¶Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒæ–‡æœ¬æ£€ç´¢ã€æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸­ã€‚</li>
<li>Med3DVLMæˆåŠŸç¼©å°äº†ä¸‰ç»´æˆåƒä¸è¯­è¨€ä¹‹é—´çš„é¸¿æ²Ÿï¼Œå®ç°äº†è·¨ä¸´åºŠåº”ç”¨çš„è§„æ¨¡å¤šä»»åŠ¡æ¨ç†ã€‚</li>
<li>è¯¥æ¨¡å‹çš„ä»£ç å·²å…¬å¼€ï¼Œå¯ä¾›å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
<li>Med3DVLMçš„ä¼˜å¼‚æ€§èƒ½ä¸ºåŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸæä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•è®ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-dbc590032ebc0b9f20d3a0555ec4fdf2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88383cdb91e86552e2108209f0e93649.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81141100f689d0b653ada10adea6c9a6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TopoMortar-A-dataset-to-evaluate-image-segmentation-methods-focused-on-topology-accuracy"><a href="#TopoMortar-A-dataset-to-evaluate-image-segmentation-methods-focused-on-topology-accuracy" class="headerlink" title="TopoMortar: A dataset to evaluate image segmentation methods focused on   topology accuracy"></a>TopoMortar: A dataset to evaluate image segmentation methods focused on   topology accuracy</h2><p><strong>Authors:Juan Miguel Valverde, Motoya Koga, Nijihiko Otsuka, Anders Bjorholm Dahl</strong></p>
<p>We present TopoMortar, a brick wall dataset that is the first dataset specifically designed to evaluate topology-focused image segmentation methods, such as topology loss functions. Motivated by the known sensitivity of methods to dataset challenges, such as small training sets, noisy labels, and out-of-distribution test-set images, TopoMortar is created to enable in two ways investigating methodsâ€™ effectiveness at improving topology accuracy. First, by eliminating dataset challenges that, as we show, impact the effectiveness of topology loss functions. Second, by allowing to represent different dataset challenges in the same dataset, isolating methodsâ€™ performance from dataset challenges. TopoMortar includes three types of labels (accurate, pseudo-labels, and noisy labels), two fixed training sets (large and small), and in-distribution and out-of-distribution test-set images. We compared eight loss functions on TopoMortar, and we found that clDice achieved the most topologically accurate segmentations, and that the relative advantageousness of the other loss functions depends on the experimental setting. Additionally, we show that data augmentation and self-distillation can elevate Cross entropy Dice loss to surpass most topology loss functions, and that those simple methods can enhance topology loss functions as well. TopoMortar and our code can be found at <a target="_blank" rel="noopener" href="https://jmlipman.github.io/TopoMortar">https://jmlipman.github.io/TopoMortar</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†TopoMortarï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“é—¨ç”¨äºè¯„ä¼°æ‹“æ‰‘å›¾åƒåˆ†å‰²æ–¹æ³•ï¼ˆå¦‚æ‹“æ‰‘æŸå¤±å‡½æ•°ï¼‰çš„ç –å¢™æ•°æ®é›†ã€‚å—åˆ°æ–¹æ³•å¯¹æ•°æ®é›†æŒ‘æˆ˜ï¼ˆå¦‚å°è®­ç»ƒé›†ã€å™ªå£°æ ‡ç­¾å’Œè¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æµ‹è¯•é›†å›¾åƒï¼‰çš„æ•æ„Ÿæ€§çš„å¯å‘ï¼ŒTopoMortarä»¥ä¸¤ç§æ–¹å¼åˆ›å»ºï¼Œæ—¨åœ¨ç ”ç©¶æé«˜æ‹“æ‰‘ç²¾åº¦çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚é¦–å…ˆï¼Œé€šè¿‡æ¶ˆé™¤æ•°æ®é›†æŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜ä¼šå½±å“æ‹“æ‰‘æŸå¤±å‡½æ•°çš„æœ‰æ•ˆæ€§ï¼Œæ­£å¦‚æˆ‘ä»¬æ‰€å±•ç¤ºçš„é‚£æ ·ã€‚å…¶æ¬¡ï¼Œé€šè¿‡åœ¨åŒä¸€ä¸ªæ•°æ®é›†ä¸­è¡¨ç¤ºä¸åŒçš„æ•°æ®é›†æŒ‘æˆ˜ï¼Œå°†æ–¹æ³•çš„æ€§èƒ½ä¸æ•°æ®é›†æŒ‘æˆ˜éš”ç¦»å¼€æ¥ã€‚TopoMortaråŒ…å«ä¸‰ç§ç±»å‹çš„æ ‡ç­¾ï¼ˆå‡†ç¡®æ ‡ç­¾ã€ä¼ªæ ‡ç­¾å’Œå™ªå£°æ ‡ç­¾ï¼‰ã€ä¸¤ä¸ªå›ºå®šçš„è®­ç»ƒé›†ï¼ˆå¤§å‹å’Œå°å‹ï¼‰ï¼Œä»¥åŠç¬¦åˆåˆ†å¸ƒå’Œè¶…å‡ºåˆ†å¸ƒçš„æµ‹è¯•é›†å›¾åƒã€‚æˆ‘ä»¬åœ¨TopoMortarä¸Šæ¯”è¾ƒäº†å…«ç§æŸå¤±å‡½æ•°ï¼Œå‘ç°clDiceè·å¾—äº†æœ€å‡†ç¡®çš„æ‹“æ‰‘åˆ†å‰²ç»“æœï¼Œè€Œå…¶ä»–æŸå¤±å‡½æ•°çš„ç›¸å¯¹ä¼˜åŠ¿å–å†³äºå®éªŒè®¾ç½®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜æ•°æ®å¢å¼ºå’Œè‡ªæˆ‘è’¸é¦å¯ä»¥ä½¿äº¤å‰ç†µDiceæŸå¤±è¶…è¶Šå¤§å¤šæ•°æ‹“æ‰‘æŸå¤±å‡½æ•°ï¼Œè¿™äº›ç®€å•çš„æ–¹æ³•ä¹Ÿå¯ä»¥å¢å¼ºæ‹“æ‰‘æŸå¤±å‡½æ•°çš„æ•ˆæœã€‚TopoMortarå’Œæˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://jmlipman.github.io/TopoMortar%E6%89%BE%E5%88%B0%E3%80%82">https://jmlipman.github.io/TopoMortaræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03365v2">PDF</a> Accepted to BMVC 2025 (Oral)</p>
<p><strong>Summary</strong><br>    TopoMortaræ•°æ®é›†ä¸“ä¸ºè¯„ä¼°æ‹“æ‰‘å›¾åƒåˆ†å‰²æ–¹æ³•è€Œè®¾è®¡ï¼Œå¦‚æ‹“æ‰‘æŸå¤±å‡½æ•°ã€‚è¯¥æ•°æ®é›†è§£å†³äº†å°è®­ç»ƒé›†ã€æ ‡ç­¾å™ªå£°å’Œéåˆ†å¸ƒæµ‹è¯•é›†å›¾åƒç­‰æŒ‘æˆ˜ï¼Œé€šè¿‡ä¸¤ç§æ–¹å¼ç ”ç©¶æ‹“æ‰‘æŸå¤±å‡½æ•°çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå¯¹æ¯”äº†å…«ç§æŸå¤±å‡½æ•°ï¼Œå‘ç°clDiceåœ¨æ‹“æ‰‘å‡†ç¡®æ€§ä¸Šè¡¨ç°æœ€ä½³ï¼Œå…¶ä»–æŸå¤±å‡½æ•°çš„ç›¸å¯¹ä¼˜åŠ¿å–å†³äºå®éªŒè®¾ç½®ã€‚åŒæ—¶ï¼Œæ•°æ®å¢å¼ºå’Œè‡ªè’¸é¦æŠ€æœ¯èƒ½æé«˜Cross entropy DiceæŸå¤±æ€§èƒ½ï¼Œå¹¶å¢å¼ºæ‹“æ‰‘æŸå¤±å‡½æ•°çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TopoMortaræ•°æ®é›†æ˜¯é¦–ä¸ªä¸“ä¸ºè¯„ä¼°æ‹“æ‰‘å›¾åƒåˆ†å‰²æ–¹æ³•è®¾è®¡çš„ç –å¢™æ•°æ®é›†ã€‚</li>
<li>è¯¥æ•°æ®é›†è§£å†³äº†å°è®­ç»ƒé›†ã€æ ‡ç­¾å™ªå£°å’Œéåˆ†å¸ƒæµ‹è¯•é›†å›¾åƒç­‰æŒ‘æˆ˜ã€‚</li>
<li>TopoMortaræä¾›äº†ä¸‰ç§æ ‡ç­¾ç±»å‹å’Œä¸¤ç§å›ºå®šè®­ç»ƒé›†è§„æ¨¡ï¼Œä»¥ç ”ç©¶æ–¹æ³•çš„æ‹“æ‰‘å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨TopoMortarä¸Šå¯¹æ¯”äº†å…«ç§æŸå¤±å‡½æ•°ï¼Œå‘ç°clDiceåœ¨æ‹“æ‰‘å‡†ç¡®æ€§ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>æ•°æ®å¢å¼ºå’Œè‡ªè’¸é¦æŠ€æœ¯èƒ½æé«˜Cross entropy DiceæŸå¤±æ€§èƒ½ï¼Œå¹¶å¯èƒ½è¶…è¶Šéƒ¨åˆ†æ‹“æ‰‘æŸå¤±å‡½æ•°ã€‚</li>
<li>ç®€å•æ–¹æ³•å¦‚æ•°æ®å¢å¼ºå’Œè‡ªè’¸é¦èƒ½å¢å¼ºæ‹“æ‰‘æŸå¤±å‡½æ•°çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03365">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-484e0442d99f0a36e03f0a23c2484bba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e636ed4e0ea65b5eddad14d7186ef384.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db1769b1ff559768e7ffba8b6bcd4ec7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fdd81e337bac7e331f246a80a09fc0b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Deployment-and-validation-of-predictive-6-dimensional-beam-diagnostics-through-generative-reconstruction-with-standard-accelerator-elements"><a href="#Deployment-and-validation-of-predictive-6-dimensional-beam-diagnostics-through-generative-reconstruction-with-standard-accelerator-elements" class="headerlink" title="Deployment and validation of predictive 6-dimensional beam diagnostics   through generative reconstruction with standard accelerator elements"></a>Deployment and validation of predictive 6-dimensional beam diagnostics   through generative reconstruction with standard accelerator elements</h2><p><strong>Authors:Seongyeol Kim, Juan Pablo Gonzalez-Aguilera, Ryan Roussel, Gyujin Kim, Auralee Edelen, Myung-Hoon Cho, Young-Kee Kim, Chi Hyun Shim, Hoon Heo, Haeryong Yang</strong></p>
<p>Understanding the 6-dimensional phase space distribution of particle beams is essential for optimizing accelerator performance. Conventional diagnostics such as use of transverse deflecting cavities offer detailed characterization but require dedicated hardware and space. Generative phase space reconstruction (GPSR) methods have shown promise in beam diagnostics, yet prior implementations still rely on such components. Here we present the first experimental implementation and validation of the GPSR methodology, realized by the use of standard accelerator elements including accelerating cavities and dipole magnets, to achieve complete 6-dimensional phase space reconstruction. Through simulations and experiments at the Pohang Accelerator Laboratory X-ray Free Electron Laser facility, we successfully reconstruct complex, nonlinear beam structures. Furthermore, we validate the methodology by predicting independent downstream measurements excluded from training, revealing near-unique reconstruction closely resembling ground truth. This advancement establishes a pathway for predictive diagnostics across beamline segments while reducing hardware requirements and expanding applicability to various accelerator facilities. </p>
<blockquote>
<p>äº†è§£ç²’å­æŸåœ¨6ç»´ç›¸ç©ºé—´ä¸­çš„åˆ†å¸ƒå¯¹äºä¼˜åŒ–åŠ é€Ÿå™¨æ€§èƒ½è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„è¯Šæ–­æ–¹æ³•ï¼Œå¦‚ä½¿ç”¨æ¨ªå‘åè½¬è…”ï¼Œå¯ä»¥æä¾›è¯¦ç»†çš„ç‰¹å¾æè¿°ï¼Œä½†éœ€è¦ä¸“é—¨çš„ç¡¬ä»¶å’Œç©ºé—´ã€‚ç”Ÿæˆç›¸ä½ç©ºé—´é‡å»ºï¼ˆGPSRï¼‰æ–¹æ³•åœ¨æŸæµè¯Šæ–­ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä¹‹å‰çš„å®ç°ä»ç„¶ä¾èµ–äºè¿™äº›ç»„ä»¶ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é¦–æ¬¡é€šè¿‡å®éªŒå®ç°äº†ä½¿ç”¨æ ‡å‡†åŠ é€Ÿå™¨å…ƒä»¶ï¼ˆåŒ…æ‹¬åŠ é€Ÿè…”å’Œå¶æç£ä½“ï¼‰çš„GPSRæ–¹æ³•çš„å®ç°å’ŒéªŒè¯ï¼Œä»¥å®ç°å®Œæ•´çš„6ç»´ç›¸ç©ºé—´é‡å»ºã€‚åœ¨æµ¦é¡¹åŠ é€Ÿå™¨å®éªŒå®¤Xå°„çº¿è‡ªç”±ç”µå­æ¿€å…‰è®¾æ–½è¿›è¡Œçš„æ¨¡æ‹Ÿå’Œå®éªŒä¸­ï¼Œæˆ‘ä»¬æˆåŠŸåœ°é‡å»ºäº†å¤æ‚çš„éçº¿æ€§å…‰æŸç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡é¢„æµ‹ç‹¬ç«‹äºè®­ç»ƒä¹‹å¤–çš„ä¸‹æ¸¸æµ‹é‡å€¼éªŒè¯äº†è¯¥æ–¹æ³•ï¼Œæ­ç¤ºäº†æ¥è¿‘ç‹¬ç‰¹çš„é‡å»ºï¼Œä¸çœŸå®æƒ…å†µéå¸¸ç›¸ä¼¼ã€‚è¿™ä¸€è¿›å±•ä¸ºè·¨æŸæµçº¿æ®µçš„é¢„æµ‹è¯Šæ–­å¼€è¾Ÿäº†ä¸€æ¡é“è·¯ï¼ŒåŒæ—¶é™ä½äº†ç¡¬ä»¶è¦æ±‚ï¼Œå¹¶æ‰©å¤§äº†åœ¨å„ç§åŠ é€Ÿå™¨è®¾æ–½ä¸­çš„åº”ç”¨èŒƒå›´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20654v3">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨åŠ é€Ÿå™¨æ€§èƒ½ä¼˜åŒ–ä¸­ï¼Œç†è§£ç²’å­æŸçš„6ç»´ç›¸ç©ºé—´åˆ†å¸ƒè‡³å…³é‡è¦ã€‚ä¼ ç»Ÿè¯Šæ–­æ–¹æ³•å¦‚ä½¿ç”¨æ¨ªå‘åè½¬è…”å¯æä¾›è¯¦ç»†çš„ç‰¹å¾æè¿°ï¼Œä½†éœ€è¦ä¸“é—¨çš„ç¡¬ä»¶å’Œç©ºé—´ã€‚ç”Ÿæˆç›¸ä½ç©ºé—´é‡å»ºï¼ˆGPSRï¼‰æ–¹æ³•åœ¨æŸæµè¯Šæ–­ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ä¹‹å‰çš„å®ç°ä»ä¾èµ–äºè¿™äº›ç»„ä»¶ã€‚æœ¬æ–‡é¦–æ¬¡å®éªŒå®ç°äº†ä½¿ç”¨æ ‡å‡†åŠ é€Ÿå™¨å…ƒä»¶ï¼ˆåŒ…æ‹¬åŠ é€Ÿè…”å’Œå¶æç£é“ï¼‰å®ç°å®Œå…¨6ç»´ç›¸ç©ºé—´é‡å»ºçš„GPRSæ–¹æ³•ï¼ŒæˆåŠŸé‡å»ºå¤æ‚çš„éçº¿æ€§æŸç»“æ„ã€‚è¯¥æ–¹æ³•çš„é¢„æµ‹èƒ½åŠ›ä¸ä¸‹æ¸¸ç‹¬ç«‹æµ‹é‡çš„é¢„æµ‹èƒ½åŠ›ç›¸åŒ¹é…ï¼Œå¹¶å»ºç«‹äº†è·¨è¶ŠæŸçº¿æ®µçš„é¢„æµ‹è¯Šæ–­é€”å¾„ï¼Œé™ä½äº†ç¡¬ä»¶è¦æ±‚å¹¶æ‰©å¤§äº†å…¶åœ¨å„ç§åŠ é€Ÿå™¨è®¾æ–½ä¸­çš„åº”ç”¨èŒƒå›´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç†è§£ç²’å­æŸçš„6ç»´ç›¸ç©ºé—´åˆ†å¸ƒå¯¹åŠ é€Ÿå™¨æ€§èƒ½ä¼˜åŒ–è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿè¯Šæ–­æ–¹æ³•éœ€è¦ä¸“é—¨çš„ç¡¬ä»¶å’Œç©ºé—´æ¥è¯¦ç»†è¡¨å¾ç²’å­æŸçš„ç‰¹æ€§ã€‚</li>
<li>ç”Ÿæˆç›¸ä½ç©ºé—´é‡å»ºï¼ˆGPSRï¼‰æ–¹æ³•åœ¨æŸæµè¯Šæ–­ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>é¦–æ¬¡å®éªŒå®ç°äº†ä½¿ç”¨æ ‡å‡†åŠ é€Ÿå™¨å…ƒä»¶å®ç°å®Œå…¨6ç»´ç›¸ç©ºé—´é‡å»ºçš„GPRSæ–¹æ³•ã€‚</li>
<li>æˆåŠŸé‡å»ºå¤æ‚çš„éçº¿æ€§æŸç»“æ„ã€‚</li>
<li>GPSRæ–¹æ³•é¢„æµ‹èƒ½åŠ›ä¸ç‹¬ç«‹ä¸‹æ¸¸æµ‹é‡ç›¸åŒ¹é…ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-efbcf9aae7b6934a62774e087b56564c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fe548d2ec52a7471adffcf801085181f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e76835336ca5759dbb7c1c556f6fa28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-523ec7aa92940301711e176935dd1408.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6b6525ef04695f65421fb1eff0e4d68.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="RadGPT-Constructing-3D-Image-Text-Tumor-Datasets"><a href="#RadGPT-Constructing-3D-Image-Text-Tumor-Datasets" class="headerlink" title="RadGPT: Constructing 3D Image-Text Tumor Datasets"></a>RadGPT: Constructing 3D Image-Text Tumor Datasets</h2><p><strong>Authors:Pedro R. A. S. Bassi, Mehmet Can Yavuz, Kang Wang, Xiaoxi Chen, Wenxuan Li, Sergio Decherchi, Andrea Cavalli, Yang Yang, Alan Yuille, Zongwei Zhou</strong></p>
<p>Cancers identified in CT scans are usually accompanied by detailed radiology reports, but publicly available CT datasets often lack these essential reports. This absence limits their usefulness for developing accurate report generation AI. To address this gap, we present AbdomenAtlas 3.0, the first public, high-quality abdominal CT dataset with detailed, expert-reviewed radiology reports. All reports are paired with per-voxel masks and they describe liver, kidney and pancreatic tumors. AbdomenAtlas 3.0 has 9,262 triplets of CT, mask and reportâ€“3,955 with tumors. These CT scans come from 17 public datasets. Besides creating the reports for these datasets, we expanded their number of tumor masks by 4.2x, identifying 3,011 new tumor cases. Notably, the reports in AbdomenAtlas 3.0 are more standardized, and generated faster than traditional human-made reports. They provide details like tumor size, location, attenuation and surgical resectability. These reports were created by 12 board-certified radiologists using our proposed RadGPT, a novel framework that converted radiologist-revised tumor segmentation masks into structured and narrative reports. Besides being a dataset creation tool, RadGPT can also become a fully-automatic, segmentation-assisted report generation method. We benchmarked this method and 5 state-of-the-art report generation vision-language models. Our results show that segmentation strongly improves tumor detection in AI-made reports. </p>
<blockquote>
<p>é€šè¿‡CTæ‰«ææ£€æµ‹åˆ°çš„ç™Œç—‡é€šå¸¸ä¼´æœ‰è¯¦ç»†çš„æ”¾å°„å­¦æŠ¥å‘Šï¼Œä½†å…¬å¼€å¯ç”¨çš„CTæ•°æ®é›†å¾€å¾€ç¼ºä¹è¿™äº›å¿…è¦çš„æŠ¥å‘Šã€‚è¿™ä¸€ç¼ºå¤±é™åˆ¶äº†å®ƒä»¬å¯¹äºå¼€å‘å‡†ç¡®æŠ¥å‘Šç”Ÿæˆäººå·¥æ™ºèƒ½çš„ç”¨å¤„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AbdomenAtlas 3.0ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¸¦æœ‰è¯¦ç»†ã€ç»è¿‡ä¸“å®¶å®¡æŸ¥çš„æ”¾å°„å­¦æŠ¥å‘Šçš„å…¬å¼€é«˜è´¨é‡è…¹éƒ¨CTæ•°æ®é›†ã€‚æ‰€æœ‰æŠ¥å‘Šéƒ½é…å¤‡æœ‰é€åƒç´ é®ç½©ï¼Œå¹¶æè¿°è‚è„ã€è‚¾è„å’Œèƒ°è…ºè‚¿ç˜¤ã€‚AbdomenAtlas 3.0åŒ…å«æœ‰CTæ‰«æã€é®ç½©å’ŒæŠ¥å‘Šçš„9,262ä¸ªä¸‰å…ƒç»„ç»„åˆï¼Œå…¶ä¸­å«æœ‰è‚¿ç˜¤çš„ä¸º3,955ä¸ªã€‚è¿™äº›CTæ‰«ææ¥è‡ªäº17ä¸ªå…¬å¼€æ•°æ®é›†ã€‚é™¤äº†ä¸ºè¿™äº›æ•°æ®é›†åˆ›å»ºæŠ¥å‘Šå¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡å°†è‚¿ç˜¤é®ç½©æ•°é‡æ‰©å¤§äº†4.2å€ï¼Œè¯†åˆ«å‡ºäº†æ–°å¢çš„3,011ä¸ªè‚¿ç˜¤ç—…ä¾‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒAbdomenAtlas 3.0ä¸­çš„æŠ¥å‘Šæ›´åŠ æ ‡å‡†åŒ–ï¼Œç”Ÿæˆé€Ÿåº¦ä¹Ÿæ¯”ä¼ ç»Ÿçš„æ‰‹å·¥æŠ¥å‘Šæ›´å¿«ã€‚å®ƒä»¬æä¾›äº†è‚¿ç˜¤çš„å°ºå¯¸ã€ä½ç½®ã€è¡°å‡å’Œæ‰‹æœ¯å¯åˆ‡é™¤æ€§ç­‰è¯¦ç»†ä¿¡æ¯ã€‚è¿™äº›æŠ¥å‘Šæ˜¯ç”±ä½¿ç”¨æˆ‘ä»¬æå‡ºçš„RadGPTå·¥å…·çš„åäºŒä½è®¤è¯æ”¾å°„ç§‘åŒ»ç”Ÿåˆ›å»ºçš„ã€‚è¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå°†æ”¾å°„ç§‘åŒ»ç”Ÿä¿®è®¢çš„è‚¿ç˜¤åˆ†å‰²é®ç½©è½¬æ¢ä¸ºç»“æ„å’Œå™è¿°æ€§æŠ¥å‘Šã€‚é™¤äº†ä½œä¸ºæ•°æ®é›†åˆ›å»ºå·¥å…·å¤–ï¼ŒRadGPTè¿˜å¯ä»¥æˆä¸ºä¸€ç§å…¨è‡ªåŠ¨çš„ã€ç”±åˆ†å‰²è¾…åŠ©çš„æŠ¥å‘Šç”Ÿæˆæ–¹æ³•ã€‚æˆ‘ä»¬å¯¹è¿™ç§æ–¹æ³•ä»¥åŠäº”ç§æœ€æ–°çš„æŠ¥å‘Šç”Ÿæˆè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåˆ†å‰²åœ¨äººå·¥æ™ºèƒ½ç”Ÿæˆçš„æŠ¥å‘Šä¸­æ˜¾è‘—æé«˜äº†è‚¿ç˜¤æ£€æµ‹æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04678v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è…¹éƒ¨CTæ•°æ®é›†AbdomenAtlas 3.0ï¼ŒåŒ…å«è¯¦ç»†çš„ä¸“å®¶å®¡æ ¸è¿‡çš„æ”¾å°„å­¦æŠ¥å‘Šå’Œæ¯åƒç´ è‚¿ç˜¤æ©è†œï¼Œæè¿°è‚è„ã€è‚¾è„å’Œèƒ°è…ºè‚¿ç˜¤ã€‚æ­¤æ•°æ®é›†è§£å†³äº†å…¬å¼€CTæ•°æ®é›†ä¸­ç¼ºä¹æŠ¥å‘Šçš„é—®é¢˜ï¼Œæœ‰åŠ©äºå¼€å‘å‡†ç¡®çš„æŠ¥å‘Šç”ŸæˆAIã€‚æ•°æ®é›†åŒ…å«æ¥è‡ª17ä¸ªå…¬å…±æ•°æ®é›†çš„9,262ä¸ªCTã€æ©è†œå’ŒæŠ¥å‘Šä¸‰å…ƒç»„ï¼Œå…¶ä¸­å¸¦è‚¿ç˜¤çš„ä¸º3,955ä¸ªã€‚æ­¤å¤–ï¼Œé€šè¿‡æ‰©å¤§è‚¿ç˜¤æ©è†œæ•°é‡ï¼Œæ–°å¢äº†3,011ä¸ªè‚¿ç˜¤ç—…ä¾‹ã€‚æŠ¥å‘Šæ ‡å‡†åŒ–ç¨‹åº¦é«˜ä¸”ç”Ÿæˆé€Ÿåº¦å¿«ï¼ŒåŒ…æ‹¬è‚¿ç˜¤å¤§å°ã€ä½ç½®ã€è¡°å‡å’Œæ‰‹æœ¯å¯åˆ‡é™¤æ€§ç­‰è¯¦ç»†ä¿¡æ¯ã€‚ä½¿ç”¨æ–°å‹æ¡†æ¶RadGPTç”ŸæˆæŠ¥å‘Šï¼Œå¯å°†æ”¾å°„ç§‘åŒ»ç”Ÿä¿®è®¢çš„è‚¿ç˜¤åˆ†å‰²æ©è†œè½¬åŒ–ä¸ºç»“æ„åŒ–å’Œå™è¿°æ€§æŠ¥å‘Šã€‚RadGPTä¸ä»…æ˜¯æ•°æ®é›†åˆ›å»ºå·¥å…·ï¼Œè¿˜å¯æˆä¸ºå…¨è‡ªåŠ¨åˆ†å‰²è¾…åŠ©æŠ¥å‘Šç”Ÿæˆæ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåˆ†å‰²èƒ½æ˜¾è‘—æé«˜AIç”Ÿæˆçš„æŠ¥å‘Šä¸­è‚¿ç˜¤æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ul>
<li>AbdomenAtlas 3.0æ˜¯é¦–ä¸ªåŒ…å«è¯¦ç»†ä¸“å®¶å®¡æ ¸è¿‡çš„æ”¾å°„å­¦æŠ¥å‘Šçš„å…¬å¼€é«˜è´¨é‡è…¹éƒ¨CTæ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†ä¸­çš„æŠ¥å‘Šä¸æ¯åƒç´ è‚¿ç˜¤æ©è†œé…å¯¹ï¼Œæè¿°è‚è„ã€è‚¾è„å’Œèƒ°è…ºçš„è‚¿ç˜¤ã€‚</li>
<li>æ•°æ®é›†è§£å†³äº†å…¬å¼€CTæ•°æ®é›†ç¼ºä¹æŠ¥å‘Šçš„é—®é¢˜ï¼Œæœ‰åŠ©äºå¼€å‘å‡†ç¡®çš„æŠ¥å‘Šç”ŸæˆAIã€‚</li>
<li>é€šè¿‡æ‰©å¤§è‚¿ç˜¤æ©è†œæ•°é‡æ–°å¢äº†3,011ä¸ªè‚¿ç˜¤ç—…ä¾‹ã€‚</li>
<li>æŠ¥å‘Šæ ‡å‡†åŒ–ç¨‹åº¦é«˜ä¸”åŒ…å«è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚è‚¿ç˜¤å¤§å°ã€ä½ç½®ç­‰ã€‚</li>
<li>ä½¿ç”¨RadGPTæ¡†æ¶ç”ŸæˆæŠ¥å‘Šï¼Œå¯è½¬åŒ–ä¸ºç»“æ„åŒ–å’Œå™è¿°æ€§æŠ¥å‘Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24b9637fbbee0d20a05ebc65ed24f463.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1ca5aa9c340e1c83f7ed21c15e7e968.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-306de608afaa33390a470e3170b6b7fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c94181041c64fe045b438f15824f73c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d74335de5d979a2177ac87882f854d5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="INSIGHT-Explainable-Weakly-Supervised-Medical-Image-Analysis"><a href="#INSIGHT-Explainable-Weakly-Supervised-Medical-Image-Analysis" class="headerlink" title="INSIGHT: Explainable Weakly-Supervised Medical Image Analysis"></a>INSIGHT: Explainable Weakly-Supervised Medical Image Analysis</h2><p><strong>Authors:Wenbo Zhang, Junyu Chen, Christopher Kanan</strong></p>
<p>Due to their large sizes, volumetric scans and whole-slide pathology images (WSIs) are often processed by extracting embeddings from local regions and then an aggregator makes predictions from this set. However, current methods require post-hoc visualization techniques (e.g., Grad-CAM) and often fail to localize small yet clinically crucial details. To address these limitations, we introduce INSIGHT, a novel weakly-supervised aggregator that integrates heatmap generation as an inductive bias. Starting from pre-trained feature maps, INSIGHT employs a detection module with small convolutional kernels to capture fine details and a context module with a broader receptive field to suppress local false positives. The resulting internal heatmap highlights diagnostically relevant regions. On CT and WSI benchmarks, INSIGHT achieves state-of-the-art classification results and high weakly-labeled semantic segmentation performance. Project website and code are available at: <a target="_blank" rel="noopener" href="https://zhangdylan83.github.io/ewsmia/">https://zhangdylan83.github.io/ewsmia/</a> </p>
<blockquote>
<p>ç”±äºä½“ç§¯æ‰«æå’Œå…¨åˆ‡ç‰‡ç—…ç†å›¾åƒï¼ˆWSIï¼‰çš„å°ºå¯¸è¾ƒå¤§ï¼Œé€šå¸¸é€šè¿‡ä»å±€éƒ¨åŒºåŸŸæå–åµŒå…¥ï¼Œç„¶åèšåˆå™¨æ ¹æ®è¿™äº›åµŒå…¥è¿›è¡Œé¢„æµ‹æ¥å¤„ç†ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•éœ€è¦äº‹åå¯è§†åŒ–æŠ€æœ¯ï¼ˆä¾‹å¦‚Grad-CAMï¼‰ï¼Œå¹¶ä¸”å¾€å¾€æ— æ³•å®šä½è™½å°ä½†å¯¹ä¸´åºŠè‡³å…³é‡è¦çš„ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†INSIGHTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¼±ç›‘ç£èšåˆå™¨ï¼Œå®ƒå°†çƒ­å›¾ç”Ÿæˆä½œä¸ºå½’çº³åè§è¿›è¡Œé›†æˆã€‚ä»é¢„è®­ç»ƒçš„ç‰¹å¾å›¾å¼€å§‹ï¼ŒINSIGHTä½¿ç”¨å…·æœ‰è¾ƒå°å·ç§¯æ ¸çš„æ£€æµ‹æ¨¡å—æ¥æ•è·ç»†èŠ‚å’Œå…·æœ‰æ›´å¹¿æ³›æ„Ÿå—é‡çš„ä¸Šä¸‹æ–‡æ¨¡å—æ¥æŠ‘åˆ¶å±€éƒ¨è¯¯æŠ¥ã€‚ç”Ÿæˆçš„å†…éƒ¨çƒ­å›¾çªå‡ºäº†ä¸è¯Šæ–­ç›¸å…³çš„åŒºåŸŸã€‚åœ¨CTå’ŒWSIåŸºå‡†æµ‹è¯•ä¸­ï¼ŒINSIGHTå–å¾—äº†æœ€å…ˆè¿›çš„åˆ†ç±»ç»“æœå’Œé«˜æ€§èƒ½çš„å¼±æ ‡ç­¾è¯­ä¹‰åˆ†å‰²ã€‚é¡¹ç›®ç½‘ç«™å’Œä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://zhangdylan83.github.io/ewsmia/">https://zhangdylan83.github.io/ewsmia/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02012v3">PDF</a> Accepted at MLHC 2025 (Machine Learning for Healthcare)</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†åœ¨å¤„ç†å¤§è§„æ¨¡ä½“ç§¯æ‰«æå’Œå…¨æ™¯ç—…ç†å›¾åƒæ—¶ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰å±€éƒ¨å°åŒºåŸŸçš„ç»†èŠ‚ä¿¡æ¯ï¼Œéœ€è¦åè§†å¯è§†åŒ–æŠ€æœ¯è¿›è¡Œæ”¹è¿›ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™ï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹å¼±ç›‘ç£èšåˆå™¨INSIGHTï¼Œç»“åˆé¢„è®­ç»ƒç‰¹å¾å›¾ï¼Œåˆ©ç”¨å¸¦æœ‰å°å‹å·ç§¯æ ¸çš„æ£€æµ‹æ¨¡å—æ•è·ç²¾ç»†ç»†èŠ‚å’Œåˆ©ç”¨å…·æœ‰æ›´å¤§æ„ŸçŸ¥åœºçš„ä¸Šä¸‹æ–‡æ¨¡å—æ¥æŠ‘åˆ¶å±€éƒ¨é”™è¯¯ä¿¡å·ï¼Œå¹¶ç”Ÿæˆå†…éƒ¨çƒ­å›¾ä»¥çªå‡ºæ˜¾ç¤ºè¯Šæ–­ç›¸å…³åŒºåŸŸã€‚åœ¨CTå’Œå…¨æ™¯ç—…ç†å›¾åƒæµ‹è¯•ä¸­ï¼ŒINSIGHTè¾¾åˆ°äº†ä¸šç•Œå…ˆè¿›çš„åˆ†ç±»ç»“æœå’Œé«˜çš„å¼±æ ‡ç­¾è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚é¡¹ç›®ç½‘ç«™å’Œä»£ç å¯åœ¨æ­¤é“¾æ¥ä¸‹è½½ï¼š<a target="_blank" rel="noopener" href="https://zhangdylan83.github.io/ewsmia/%E3%80%82">https://zhangdylan83.github.io/ewsmia/ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä½“ç§¯æ‰«æå’Œå…¨æ™¯ç—…ç†å›¾åƒå› è§„æ¨¡å·¨å¤§å¸¸é‡‡ç”¨æå–å±€éƒ¨åŒºåŸŸåµŒå…¥ç„¶åé€šè¿‡èšåˆå™¨é¢„æµ‹çš„æ–¹å¼å¤„ç†ã€‚ä½†å½“å‰æ–¹æ³•æ— æ³•å®šä½å…³é”®ç»†èŠ‚ä¸”éœ€è¦åè§†å¯è§†åŒ–æŠ€æœ¯ã€‚</li>
<li>INSIGHTæ˜¯ä¸€ç§æ–°å‹å¼±ç›‘ç£èšåˆå™¨ï¼Œç»“åˆäº†é¢„è®­ç»ƒç‰¹å¾å›¾ï¼Œé€šè¿‡æ£€æµ‹æ¨¡å—å’Œä¸Šä¸‹æ–‡æ¨¡å—åˆ†åˆ«æ•æ‰ç²¾ç»†ç»†èŠ‚å¹¶æŠ‘åˆ¶å±€éƒ¨é”™è¯¯ä¿¡å·ã€‚</li>
<li>INSIGHTç”Ÿæˆå†…éƒ¨çƒ­å›¾ä»¥çªå‡ºæ˜¾ç¤ºè¯Šæ–­ç›¸å…³åŒºåŸŸã€‚</li>
<li>åœ¨CTå’Œå…¨æ™¯ç—…ç†å›¾åƒæµ‹è¯•ä¸­ï¼ŒINSIGHTå®ç°äº†å…ˆè¿›çš„åˆ†ç±»ç»“æœå’Œé«˜çš„å¼±æ ‡ç­¾è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02012">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-070e9fd56e8a81547753a119dd357951.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d32943378e0f3f37f0dffc41a279f06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a141af7cbd6f75b926ad66d22bb0a08c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe88ce2948fdc41eeeed141d3e27e869.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Automatic-brain-tumor-segmentation-in-2D-intra-operative-ultrasound-images-using-magnetic-resonance-imaging-tumor-annotations"><a href="#Automatic-brain-tumor-segmentation-in-2D-intra-operative-ultrasound-images-using-magnetic-resonance-imaging-tumor-annotations" class="headerlink" title="Automatic brain tumor segmentation in 2D intra-operative ultrasound   images using magnetic resonance imaging tumor annotations"></a>Automatic brain tumor segmentation in 2D intra-operative ultrasound   images using magnetic resonance imaging tumor annotations</h2><p><strong>Authors:Mathilde Faanes, Ragnhild Holden Helland, Ole Solheim, SÃ©bastien Muller, Ingerid Reinertsen</strong></p>
<p>Automatic segmentation of brain tumors in intra-operative ultrasound (iUS) images could facilitate localization of tumor tissue during resection surgery. The lack of large annotated datasets limits the current models performances. In this paper, we investigated the use of tumor annotations in magnetic resonance imaging (MRI) scans, which are more accessible than annotations in iUS images, for training of deep learning models for iUS brain tumor segmentation. We used 180 annotated MRI scans with corresponding unannotated iUS images, and 29 annotated iUS images. Image registration was performed to transfer the MRI annotations to the corresponding iUS images before training the nnU-Net model with different configurations of the data and label origins. The results showed no significant difference in Dice score for a model trained with only MRI annotated tumors compared to models trained with only iUS annotations and both, and to expert annotations, indicating that MRI tumor annotations can be used as a substitute for iUS tumor annotations to train a deep learning model for automatic brain tumor segmentation in iUS images. The best model obtained an average Dice score of $0.62\pm0.31$, compared to $0.67\pm0.25$ for an expert neurosurgeon, where the performance on larger tumors were similar, but lower for the models on smaller tumors. In addition, the results showed that removing smaller tumors from the training sets improved the results. The main models are available here: <a target="_blank" rel="noopener" href="https://github.com/mathildefaanes/us_brain_tumor_segmentation/tree/main">https://github.com/mathildefaanes/us_brain_tumor_segmentation/tree/main</a> </p>
<blockquote>
<p>åœ¨æœ¯ä¸­è¶…å£°ï¼ˆiUSï¼‰å›¾åƒä¸­è‡ªåŠ¨åˆ†å‰²è„‘è‚¿ç˜¤å¯ä»¥æ–¹ä¾¿åœ¨åˆ‡é™¤æ‰‹æœ¯è¿‡ç¨‹ä¸­å®šä½è‚¿ç˜¤ç»„ç»‡ã€‚ç¼ºä¹å¤§å‹æ ‡æ³¨æ•°æ®é›†é™åˆ¶äº†å½“å‰æ¨¡å‹çš„è¡¨ç°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ‰«æä¸­ä½¿ç”¨è‚¿ç˜¤æ ‡æ³¨çš„æ–¹æ³•ï¼Œè¿™äº›æ ‡æ³¨æ¯”iUSå›¾åƒä¸­çš„æ ‡æ³¨æ›´å®¹æ˜“è·å–ï¼Œç”¨äºè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡ŒiUSè„‘è‚¿ç˜¤åˆ†å‰²ã€‚æˆ‘ä»¬ä½¿ç”¨180å¼ æ ‡æ³¨è¿‡çš„MRIæ‰«æå›¾åƒå’Œç›¸åº”çš„æœªæ ‡æ³¨iUSå›¾åƒï¼Œä»¥åŠ29å¼ æ ‡æ³¨è¿‡çš„iUSå›¾åƒã€‚åœ¨å°†MRIæ ‡æ³¨è½¬ç§»åˆ°ç›¸åº”çš„iUSå›¾åƒä¹‹å‰ï¼Œè¿›è¡Œäº†å›¾åƒé…å‡†ï¼Œç„¶åä½¿ç”¨ä¸åŒé…ç½®çš„æ•°æ®å’Œæ ‡ç­¾æ¥æºè®­ç»ƒnnU-Netæ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œä»…ä½¿ç”¨MRIæ ‡æ³¨è‚¿ç˜¤è®­ç»ƒçš„æ¨¡å‹ä¸ä»…ä½¿ç”¨iUSæ ‡æ³¨å’Œä¸¤è€…éƒ½ä½¿ç”¨çš„æ¨¡å‹ï¼Œä»¥åŠä¸“å®¶æ ‡æ³¨ç›¸æ¯”ï¼Œåœ¨Diceå¾—åˆ†ä¸Šæ²¡æœ‰æ˜¾è‘—å·®å¼‚ï¼Œè¿™è¡¨æ˜MRIè‚¿ç˜¤æ ‡æ³¨å¯ä»¥æ›¿ä»£iUSè‚¿ç˜¤æ ‡æ³¨ï¼Œç”¨äºè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå®ç°iUSå›¾åƒä¸­çš„è‡ªåŠ¨è„‘è‚¿ç˜¤åˆ†å‰²ã€‚æœ€ä½³æ¨¡å‹è·å¾—äº†å¹³å‡Diceå¾—åˆ†$0.62\pm0.31$ï¼Œä¸ä¸“å®¶ç¥ç»å¤–ç§‘åŒ»ç”Ÿ$0.67\pm0.25$ç›¸æ¯”ï¼Œè¾ƒå¤§è‚¿ç˜¤çš„ç»©æ•ˆç›¸ä¼¼ï¼Œä½†è¾ƒå°è‚¿ç˜¤çš„ç»©æ•ˆè¾ƒä½ã€‚æ­¤å¤–ï¼Œç»“æœè¡¨æ˜ä»è®­ç»ƒé›†ä¸­å»é™¤è¾ƒå°çš„è‚¿ç˜¤æé«˜äº†ç»“æœã€‚ä¸»è¦æ¨¡å‹å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/mathildefaanes/us_brain_tumor_segmentation/tree/main">https://github.com/mathildefaanes/us_brain_tumor_segmentation/tree/main</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14017v3">PDF</a> 14 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ‰«æä¸­çš„è‚¿ç˜¤æ³¨é‡Šä¿¡æ¯è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä»¥åœ¨æœ¯ä¸­è¶…å£°ï¼ˆiUSï¼‰å›¾åƒä¸­å®ç°è‡ªåŠ¨è„‘è‚¿ç˜¤åˆ†å‰²çš„å¯èƒ½æ€§ã€‚ç ”ç©¶ä½¿ç”¨MRIæ‰«æçš„è‚¿ç˜¤æ³¨é‡Šä¿¡æ¯æ¥æ›¿ä»£åœ¨iUSå›¾åƒä¸­çš„è‚¿ç˜¤æ³¨é‡Šä¿¡æ¯ï¼Œç»“æœå±•ç¤ºäº†ç›¸å½“çš„åˆ†å‰²æ€§èƒ½ã€‚è®­ç»ƒçš„æœ€ä½³æ¨¡å‹å®ç°äº†ä¸ä¸“å®¶ç¥ç»å¤–ç§‘åŒ»ç”Ÿç›¸ä¼¼çš„è¡¨ç°ã€‚æ­¤é¡¹ç ”ç©¶ä¸ºè§£å†³å› ç¼ºä¹å¤§å‹æ ‡æ³¨æ•°æ®é›†å¯¼è‡´çš„æ¨¡å‹æ€§èƒ½å—é™é—®é¢˜æä¾›äº†ä¸€ç§æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚åŒæ—¶æŒ‡å‡ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ’é™¤è¾ƒå°çš„è‚¿ç˜¤å¯èƒ½æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶çš„ä¸»è¦æ¨¡å‹å¯åœ¨GitHubä¸Šè¿›è¡Œè®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åˆ©ç”¨MRIæ‰«æä¸­çš„è‚¿ç˜¤æ³¨é‡Šä¿¡æ¯æ¥è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡ŒiUSå›¾åƒçš„è„‘è‚¿ç˜¤åˆ†å‰²ï¼Œå…‹æœäº†å› ç¼ºä¹å¤§é‡æ ‡æ³¨æ•°æ®é›†çš„æ€§èƒ½é™åˆ¶ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨MRIæ³¨é‡Šä¿¡æ¯è®­ç»ƒçš„æ¨¡å‹ä¸ä»…ä½¿ç”¨iUSæ³¨é‡Šä¿¡æ¯åŠä¸¤è€…ç»“åˆè®­ç»ƒçš„æ¨¡å‹åœ¨Diceè¯„åˆ†ä¸Šæ— æ˜¾è‘—å·®å¼‚ï¼Œè¡¨æ˜MRIè‚¿ç˜¤æ³¨é‡Šå¯ä½œä¸ºæ›¿ä»£iUSè‚¿ç˜¤æ³¨é‡Šæ¥è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>æœ€ä½³æ¨¡å‹çš„æ€§èƒ½ä¸ä¸“å®¶ç¥ç»å¤–ç§‘åŒ»ç”Ÿç›¸ä¼¼ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒå¤§çš„è‚¿ç˜¤ä¸Šï¼Œä½†åœ¨è¾ƒå°çš„è‚¿ç˜¤ä¸Šè¡¨ç°ç•¥ä½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5348948897cc4394cd59fb8d914f4ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fe1337007432baa4c2cea3352fe97f18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5366c2f295f9fe81f32377f5da8fca94.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="KACQ-DCNN-Uncertainty-Aware-Interpretable-Kolmogorov-Arnold-Classical-Quantum-Dual-Channel-Neural-Network-for-Heart-Disease-Detection"><a href="#KACQ-DCNN-Uncertainty-Aware-Interpretable-Kolmogorov-Arnold-Classical-Quantum-Dual-Channel-Neural-Network-for-Heart-Disease-Detection" class="headerlink" title="KACQ-DCNN: Uncertainty-Aware Interpretable Kolmogorov-Arnold   Classical-Quantum Dual-Channel Neural Network for Heart Disease Detection"></a>KACQ-DCNN: Uncertainty-Aware Interpretable Kolmogorov-Arnold   Classical-Quantum Dual-Channel Neural Network for Heart Disease Detection</h2><p><strong>Authors:Md Abrar Jahin, Md. Akmol Masud, M. F. Mridha, Zeyar Aung, Nilanjan Dey</strong></p>
<p>Heart failure is a leading cause of global mortality, necessitating improved diagnostic strategies. Classical machine learning models struggle with challenges such as high-dimensional data, class imbalances, poor feature representations, and a lack of interpretability. While quantum machine learning holds promise, current hybrid models have not fully exploited quantum advantages. In this paper, we propose the Kolmogorov-Arnold Classical-Quantum Dual-Channel Neural Network (KACQ-DCNN), a novel hybrid architecture that replaces traditional multilayer perceptrons with Kolmogorov-Arnold Networks (KANs), enabling learnable univariate activation functions. Our KACQ-DCNN 4-qubit, 1-layer model outperforms 37 benchmark models, including 16 classical and 12 quantum neural networks, achieving an accuracy of 92.03%, with macro-average precision, recall, and F1 scores of 92.00%. It also achieved a ROC-AUC of 94.77%, surpassing other models by significant margins, as validated by paired t-tests with a significance threshold of 0.0056 (after Bonferroni correction). Ablation studies highlight the synergistic effect of classical-quantum integration, improving performance by about 2% over MLP variants. Additionally, LIME and SHAP explainability techniques enhance feature interpretability, while conformal prediction provides robust uncertainty quantification. Our results demonstrate that KACQ-DCNN improves cardiovascular diagnostics by combining high accuracy with interpretability and uncertainty quantification. </p>
<blockquote>
<p>å¿ƒåŠ›è¡°ç«­æ˜¯å…¨çƒä¸»è¦çš„æ­»äº¡åŸå› ä¹‹ä¸€ï¼Œéœ€è¦æ”¹è¿›è¯Šæ–­ç­–ç•¥ã€‚ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¨¡å‹é¢ä¸´é«˜ç»´æ•°æ®ã€ç±»åˆ«ä¸å¹³è¡¡ã€ç‰¹å¾è¡¨ç¤ºä¸ä½³å’Œç¼ºä¹å¯è§£é‡Šæ€§ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚è™½ç„¶é‡å­æœºå™¨å­¦ä¹ å¾ˆæœ‰å‰æ™¯ï¼Œä½†å½“å‰çš„æ··åˆæ¨¡å‹å°šæœªå……åˆ†åˆ©ç”¨é‡å­ä¼˜åŠ¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Kolmogorov-Arnoldç»å…¸-é‡å­åŒé€šé“ç¥ç»ç½‘ç»œï¼ˆKACQ-DCNNï¼‰è¿™ä¸€æ–°å‹æ··åˆæ¶æ„ï¼Œå®ƒç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰æ›¿ä»£äº†ä¼ ç»Ÿçš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œä½¿å­¦ä¹ å•å˜é‡æ¿€æ´»å‡½æ•°æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬çš„KACQ-DCNN 4é‡å­ä½ã€1å±‚æ¨¡å‹ä¼˜äº37ä¸ªåŸºå‡†æ¨¡å‹ï¼ŒåŒ…æ‹¬16ä¸ªç»å…¸æ¨¡å‹å’Œ12ä¸ªé‡å­ç¥ç»ç½‘ç»œï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†92.03%ï¼Œå®è§‚å¹³å‡ç²¾åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°å‡ä¸º92.00%ã€‚å…¶ROC-AUCè¾¾åˆ°äº†94.77%ï¼Œæ˜¾è‘—è¶…è¿‡äº†å…¶ä»–æ¨¡å‹ï¼Œå¹¶é€šè¿‡é…å¯¹tæ£€éªŒå¾—åˆ°äº†éªŒè¯ï¼Œæ˜¾è‘—æ€§é˜ˆå€¼ä¸º0.0056ï¼ˆç»Bonferroniæ ¡æ­£ï¼‰ã€‚æ¶ˆèç ”ç©¶çªå‡ºäº†ç»å…¸ä¸é‡å­é›†æˆçš„ååŒä½œç”¨ï¼Œåœ¨æ€§èƒ½ä¸Šæ¯”å¤šå±‚æ„ŸçŸ¥å™¨å˜ä½“æé«˜äº†çº¦2%ã€‚æ­¤å¤–ï¼ŒLIMEå’ŒSHAPçš„å¯è§£é‡Šæ€§æŠ€æœ¯æé«˜äº†ç‰¹å¾çš„å¯è§£é‡Šæ€§ï¼Œè€Œé¡ºåº”æ€§é¢„æµ‹æä¾›äº†ç¨³å¥çš„ä¸ç¡®å®šæ€§é‡åŒ–ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒKACQ-DCNNé€šè¿‡å°†é«˜å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œä¸ç¡®å®šæ€§é‡åŒ–ç›¸ç»“åˆï¼Œæé«˜äº†å¿ƒè¡€ç®¡ç–¾ç—…çš„è¯Šæ–­æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07446v4">PDF</a> Published as a journal paper at Computers in Biology and Medicine   (Elsevier)</p>
<p><strong>Summary</strong><br>åœ¨å¿ƒè„è¡°ç«­è¯Šæ–­ä¸­ï¼Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ··åˆæ¶æ„Kolmogorov-Arnold Classical-Quantum Dual-Channel Neural Network (KACQ-DCNN)ï¼Œé‡‡ç”¨Kolmogorov-Arnold Networks (KANs)æ›¿ä»£ä¼ ç»Ÿå¤šå±‚æ„ŸçŸ¥å™¨ï¼Œå®ç°å¯å­¦ä¹ çš„ä¸€å…ƒæ¿€æ´»å‡½æ•°ã€‚KACQ-DCNNåœ¨å¿ƒè„è¡°ç«­è¯Šæ–­ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå‡†ç¡®ç‡é«˜è¾¾92.03%ï¼Œä¸”å…·å¤‡é«˜è§£é‡Šæ€§å’Œä¸ç¡®å®šæ€§é‡åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¿ƒè„è¡°ç«­æ˜¯å…¨çƒä¸»è¦æ­»äº¡åŸå› ä¹‹ä¸€ï¼Œéœ€è¦æ”¹è¿›è¯Šæ–­ç­–ç•¥ã€‚</li>
<li>ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨å¿ƒè„è¡°ç«­è¯Šæ–­ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚é«˜ç»´æ•°æ®ã€ç±»åˆ«ä¸å¹³è¡¡ã€ç‰¹å¾è¡¨ç¤ºä¸ä½³å’Œç¼ºä¹å¯è§£é‡Šæ€§ã€‚</li>
<li>é‡å­æœºå™¨å­¦ä¹ å…·æœ‰æ½œåŠ›ï¼Œä½†å½“å‰æ··åˆæ¨¡å‹å°šæœªå……åˆ†åˆ©ç”¨é‡å­ä¼˜åŠ¿ã€‚</li>
<li>å¼•å…¥KACQ-DCNNæ¨¡å‹ï¼Œä¸€ä¸ªç»“åˆäº†ç»å…¸å’Œé‡å­æœºå™¨å­¦ä¹ çš„æ··åˆæ¶æ„ã€‚</li>
<li>KACQ-DCNNé‡‡ç”¨Kolmogorov-Arnold Networks (KANs)ï¼Œå®ç°å­¦ä¹ ä¸€å…ƒæ¿€æ´»å‡½æ•°ï¼Œæé«˜äº†è¯Šæ–­å‡†ç¡®æ€§è‡³92.03%ã€‚</li>
<li>KACQ-DCNNåœ¨å®è§‚å¹³å‡ç²¾åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°å’ŒROC-AUCç­‰æ–¹é¢å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d62bbbfb6f2cebc7ea00c30e27408a7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ec48da43d7a2460eb5483e7ec801d93.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MedVisionLlama-Leveraging-Pre-Trained-Large-Language-Model-Layers-to-Enhance-Medical-Image-Segmentation"><a href="#MedVisionLlama-Leveraging-Pre-Trained-Large-Language-Model-Layers-to-Enhance-Medical-Image-Segmentation" class="headerlink" title="MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to   Enhance Medical Image Segmentation"></a>MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to   Enhance Medical Image Segmentation</h2><p><strong>Authors:Gurucharan Marthi Krishna Kumar, Aman Chadha, Janine Mendola, Amir Shmuel</strong></p>
<p>Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentation by integrating pre-trained LLM transformer blocks. Our approach, which incorporates a frozen LLM transformer block into the encoder of a ViT-based model, leads to substantial improvements in segmentation performance across various medical imaging modalities. We propose a Hybrid Attention Mechanism that combines global and local feature learning with a Multi-Scale Fusion Block for aggregating features across different scales. The enhanced model shows significant performance gains, including an average Dice score increase from 0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index. These results demonstrate the effectiveness of LLM-based transformers in refining medical image segmentation, highlighting their potential to significantly boost model accuracy and robustness. The source code and our implementation are available at: <a target="_blank" rel="noopener" href="https://github.com/AS-Lab/Marthi-et-al-2025-MedVisionLlama-Pre-Trained-LLM-Layers-to-Enhance-Medical-Image-Segmentation">https://github.com/AS-Lab/Marthi-et-al-2025-MedVisionLlama-Pre-Trained-LLM-Layers-to-Enhance-Medical-Image-Segmentation</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥å…¶å¤„ç†æ–‡æœ¬æ•°æ®çš„é€šç”¨æ€§è€Œå¤‡å—å…³æ³¨ï¼Œç›®å‰æ­£è¶Šæ¥è¶Šå¤šåœ°è¢«æ¢ç´¢å…¶åœ¨å¢å¼ºåŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢çš„æ½œåŠ›ã€‚åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯å‡†ç¡®è¯Šæ–­æˆåƒçš„å…³é”®ä»»åŠ¡ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†é€šè¿‡é›†æˆé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è½¬æ¢å™¨å—æ¥å¢å¼ºè§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢çš„åº”ç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯å°†å†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹è½¬æ¢å™¨å—çº³å…¥åŸºäºViTçš„æ¨¡å‹çš„ç¼–ç å™¨ï¼Œä»è€Œåœ¨å„ç§åŒ»å­¦æˆåƒæ¨¡å¼çš„åˆ†å‰²æ€§èƒ½ä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒå°†å…¨å±€å’Œå±€éƒ¨ç‰¹å¾å­¦ä¹ ç›¸ç»“åˆï¼Œå¹¶åˆ©ç”¨å¤šå°ºåº¦èåˆå—æ¥èšåˆä¸åŒå°ºåº¦çš„ç‰¹å¾ã€‚å¢å¼ºåçš„æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬å¹³å‡Diceå¾—åˆ†ä»0.74æé«˜åˆ°0.79ï¼Œä»¥åŠå‡†ç¡®åº¦ã€ç²¾ç¡®åº¦å’ŒJaccardæŒ‡æ•°çš„æ”¹è¿›ã€‚è¿™äº›ç»“æœè¯æ˜äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å˜å‹å™¨åœ¨æ”¹è¿›åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶çªå‡ºäº†å®ƒä»¬åœ¨æé«˜æ¨¡å‹å‡†ç¡®æ€§å’Œç¨³å¥æ€§æ–¹é¢çš„æ½œåŠ›ã€‚æºä»£ç å’Œæˆ‘ä»¬çš„å®ç°å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/AS-Lab/Marthi-et-al-2025-MedVisionLlama-Pre-Trained-LLM-Layers-to-Enhance-Medical-Image-Segmentation">https://github.com/AS-Lab/Marthi-et-al-2025-MedVisionLlama-Pre-Trained-LLM-Layers-to-Enhance-Medical-Image-Segmentation</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02458v3">PDF</a> Accepted to the CVAMD Workshop (Computer Vision for Automated Medical   Diagnosis) at the 2025 IEEE&#x2F;CVF International Conference on Computer Vision   (ICCVW 2025)</p>
<p><strong>Summary</strong></p>
<p>è¿™é¡¹ç ”ç©¶æ¢ç´¢äº†ä½¿ç”¨é¢„è®­ç»ƒçš„LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰transformerå—å¢å¼ºåŒ»ç–—å›¾åƒåˆ†å‰²çš„æ–¹æ³•ã€‚é€šè¿‡å°†LLM transformerå—é›†æˆåˆ°åŸºäºViTï¼ˆè§†è§‰è½¬æ¢å™¨ï¼‰æ¨¡å‹çš„ç¼–ç å™¨ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨å„ç§åŒ»å­¦å½±åƒæ¨¡æ€çš„åˆ†å‰²æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚é€šè¿‡æ··åˆæ³¨æ„åŠ›æœºåˆ¶å’Œè·¨å°ºåº¦çš„å¤šå°ºåº¦èåˆå—ç‰¹å¾èšåˆæŠ€æœ¯ï¼Œå®ç°äº†å…¨å±€å’Œå±€éƒ¨ç‰¹å¾å­¦ä¹ ï¼Œæé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚è¯¥æ¨¡å‹çš„æ€§èƒ½æ˜¾è‘—å¢å¼ºï¼ŒDiceå¹³å‡å¾—åˆ†ä»0.74æé«˜åˆ°0.79ï¼Œå‡†ç¡®æ€§å’ŒJaccardæŒ‡æ•°ä¹Ÿæœ‰æ‰€æ”¹å–„ã€‚å±•ç¤ºäº†LLMåœ¨åŒ»ç–—å›¾åƒåˆ†å‰²æ–¹é¢çš„ç²¾ç»†æ•ˆæœã€‚è¯¦æƒ…è¯·è®¿é—®æŒ‡å®šçš„GitHubåœ°å€è·å–æºä»£ç å’Œå®ç°è¯¦æƒ…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ä½¿ç”¨é¢„è®­ç»ƒçš„LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰transformerå—æ¥å¢å¼ºåŒ»ç–—å›¾åƒåˆ†å‰²çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å°†LLM transformerå—é›†æˆåˆ°åŸºäºViTï¼ˆè§†è§‰è½¬æ¢å™¨ï¼‰æ¨¡å‹çš„ç¼–ç å™¨ä¸­ï¼Œå®ç°äº†æ˜¾è‘—çš„åˆ†å‰²æ€§èƒ½æå‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆäº†å…¨å±€å’Œå±€éƒ¨ç‰¹å¾å­¦ä¹ ã€‚</li>
<li>é€šè¿‡è·¨å°ºåº¦çš„å¤šå°ºåº¦èåˆå—å®ç°ç‰¹å¾èšåˆï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹çš„Diceå¹³å‡å¾—åˆ†æ˜¾è‘—æé«˜ï¼Œä»0.74æé«˜åˆ°0.79ã€‚</li>
<li>æ¨¡å‹çš„å‡†ç¡®æ€§å’Œç²¾åº¦ä¹Ÿæœ‰æ‰€æé«˜ï¼Œè¯æ˜äº†å…¶åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-90e5c673ad0e246fb6b7e923a506c8d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2cef04e26ed78abba8783961bc2b52b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e58df4ca330f895d7b46a2ed9cc827b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edb92b297599fbb3fa8fbc9ea01184da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d700281679eec196f1cd12687d59bd19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec10cdc3fd17cabe2c28db1b63da2ba0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-07940b51c6156be6af45551f57317c94.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-22/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4b930ab27e19b587a0b375e5b562d4d3.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-22  Long-Context Speech Synthesis with Context-Aware Memory
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-22/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0047ae843a9a5e7b30c2ebf47a9d818a.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-22  Tooth-Diffusion Guided 3D CBCT Synthesis with Fine-Grained Tooth   Conditioning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30166.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
