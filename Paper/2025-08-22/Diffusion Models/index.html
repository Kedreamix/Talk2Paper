<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-08-22  TransLight Image-Guided Customized Lighting Control with Generative   Decoupling">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b132ac75a0fbf2df284591750ce55b3c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    47 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-22-更新"><a href="#2025-08-22-更新" class="headerlink" title="2025-08-22 更新"></a>2025-08-22 更新</h1><h2 id="TransLight-Image-Guided-Customized-Lighting-Control-with-Generative-Decoupling"><a href="#TransLight-Image-Guided-Customized-Lighting-Control-with-Generative-Decoupling" class="headerlink" title="TransLight: Image-Guided Customized Lighting Control with Generative   Decoupling"></a>TransLight: Image-Guided Customized Lighting Control with Generative   Decoupling</h2><p><strong>Authors:Zongming Li, Lianghui Zhu, Haocheng Shen, Longjin Ran, Wenyu Liu, Xinggang Wang</strong></p>
<p>Most existing illumination-editing approaches fail to simultaneously provide customized control of light effects and preserve content integrity. This makes them less effective for practical lighting stylization requirements, especially in the challenging task of transferring complex light effects from a reference image to a user-specified target image. To address this problem, we propose TransLight, a novel framework that enables high-fidelity and high-freedom transfer of light effects. Extracting the light effect from the reference image is the most critical and challenging step in our method. The difficulty lies in the complex geometric structure features embedded in light effects that are highly coupled with content in real-world scenarios. To achieve this, we first present Generative Decoupling, where two fine-tuned diffusion models are used to accurately separate image content and light effects, generating a newly curated, million-scale dataset of image-content-light triplets. Then, we employ IC-Light as the generative model and train our model with our triplets, injecting the reference lighting image as an additional conditioning signal. The resulting TransLight model enables customized and natural transfer of diverse light effects. Notably, by thoroughly disentangling light effects from reference images, our generative decoupling strategy endows TransLight with highly flexible illumination control. Experimental results establish TransLight as the first method to successfully transfer light effects across disparate images, delivering more customized illumination control than existing techniques and charting new directions for research in illumination harmonization and editing. </p>
<blockquote>
<p>现有的大多数光照编辑方法未能同时实现对光效的自定义控制和内容完整性的保持，这使得它们在满足实际光照风格化要求方面效果较差，尤其是在将复杂的光效从参考图像转移到用户指定的目标图像这一具有挑战性的任务中。为了解决这一问题，我们提出了TransLight，一个能够实现高保真和高自由度光效转移的新型框架。从参考图像中提取光效是我们方法中最关键且最具挑战性的步骤。其难度在于光效中嵌入的复杂几何结构特征与现实世界场景中的内容高度耦合。为了实现这一点，我们首先提出了生成解耦的方法，使用两个经过微调的分扩散模型来准确分离图像内容和光效，生成一个新的、规模达百万级别的图像-内容-光效三元组数据集。然后，我们将IC-Light作为生成模型，用我们的三元组数据集来训练我们的模型，并将参考照明图像作为额外的条件信号注入。由此产生的TransLight模型能够实现定制和自然的各种光效转移。值得注意的是，通过彻底地将光效从参考图像中分离出来，我们的生成解耦策略使TransLight具备了高度灵活的光照控制。实验结果证明，TransLight是第一种成功在不同图像间转移光效的方法，它提供了比现有技术更定制化的照明控制，并为光照和谐和编辑研究开辟了新的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14814v1">PDF</a> 15 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为TransLight的新型框架，用于实现高保真和高自由度的光效转移。该框架通过采用生成性解耦策略，利用两个精细调整的扩散模型准确分离图像内容和光效，实现光效从参考图像到目标图像的转移。此策略使TransLight具有高度的照明控制灵活性，能够成功地在不同图像间转移光效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有照明编辑方法无法同时实现定制的光效控制和内容完整性的保持，使得它们在满足实际照明风格化要求方面效果有限。</li>
<li>TransLight框架被提出，旨在解决上述问题，并实现高保真和高自由度的光效转移。</li>
<li>生成性解耦策略是TransLight的核心，利用两个精细调整的扩散模型来分离图像内容、生成新的图像-内容-光效三元组数据集。</li>
<li>IC-Light作为生成模型，通过注入参考照明图像作为附加条件信号来训练模型。</li>
<li>TransLight能够实现对不同光效的定制和自然转移。</li>
<li>通过彻底地解耦参考图像中的光效，TransLight具有高度灵活的照明控制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14814">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b5105dfb48b6e00b1419cad02cb68121.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50717b396357c511ac68ec7a3c3cf6c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b16978c96b4e7b292a64ac2fb1d3644.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4f8a133f47eefbf32e9a6aa975c298d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Tinker-Diffusion’s-Gift-to-3D–Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization"><a href="#Tinker-Diffusion’s-Gift-to-3D–Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization" class="headerlink" title="Tinker: Diffusion’s Gift to 3D–Multi-View Consistent Editing From   Sparse Inputs without Per-Scene Optimization"></a>Tinker: Diffusion’s Gift to 3D–Multi-View Consistent Editing From   Sparse Inputs without Per-Scene Optimization</h2><p><strong>Authors:Canyu Zhao, Xiaoman Li, Tianjian Feng, Zhiyue Zhao, Hao Chen, Chunhua Shen</strong></p>
<p>We introduce Tinker, a versatile framework for high-fidelity 3D editing that operates in both one-shot and few-shot regimes without any per-scene finetuning. Unlike prior techniques that demand extensive per-scene optimization to ensure multi-view consistency or to produce dozens of consistent edited input views, Tinker delivers robust, multi-view consistent edits from as few as one or two images. This capability stems from repurposing pretrained diffusion models, which unlocks their latent 3D awareness. To drive research in this space, we curate the first large-scale multi-view editing dataset and data pipeline, spanning diverse scenes and styles. Building on this dataset, we develop our framework capable of generating multi-view consistent edited views without per-scene training, which consists of two novel components: (1) Referring multi-view editor: Enables precise, reference-driven edits that remain coherent across all viewpoints. (2) Any-view-to-video synthesizer: Leverages spatial-temporal priors from video diffusion to perform high-quality scene completion and novel-view generation even from sparse inputs. Through extensive experiments, Tinker significantly reduces the barrier to generalizable 3D content creation, achieving state-of-the-art performance on editing, novel-view synthesis, and rendering enhancement tasks. We believe that Tinker represents a key step towards truly scalable, zero-shot 3D editing. Project webpage: <a target="_blank" rel="noopener" href="https://aim-uofa.github.io/Tinker">https://aim-uofa.github.io/Tinker</a> </p>
<blockquote>
<p>我们介绍了Tinker，这是一个通用框架，用于进行高保真度的3D编辑，它可以在单镜头和少镜头模式下运行，无需针对每个场景进行微调。与需要广泛针对每个场景进行优化以确保多视角一致性或生成数十个一致编辑输入视角的先前技术不同，Tinker仅从一张或两张图像就能实现稳健、多视角一致编辑。这种能力来源于对预训练扩散模型的再利用，这解锁了它们的潜在三维感知能力。为了推动这一领域的研究，我们整理的第一个大规模多视角编辑数据集和数据管道，涵盖了各种场景和风格。基于这个数据集，我们开发了一个框架，能够在没有针对每个场景训练的情况下生成多视角一致编辑的视图，它由两个新颖的部分组成：（1）引用多视角编辑器：实现精确、参考驱动的编辑，保持所有观点的一致性。（2）任意视角视频合成器：利用视频扩散的空间时间先验知识，即使从稀疏输入也能实现高质量的场景补全和新颖视角生成。通过大量实验，Tinker大大降低了通用3D内容创作的障碍，在编辑、新颖视角合成和渲染增强任务上达到了最先进的性能。我们相信，Tinker是朝着真正可扩展的零镜头3D编辑迈出的关键一步。项目网页：<a target="_blank" rel="noopener" href="https://aim-uofa.github.io/Tinker">https://aim-uofa.github.io/Tinker</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14811v1">PDF</a> Project webpage: <a target="_blank" rel="noopener" href="https://aim-uofa.github.io/Tinker">https://aim-uofa.github.io/Tinker</a></p>
<p><strong>Summary</strong></p>
<p>Tinker是一个通用框架，用于高保真3D编辑，它可以在一次拍摄和少数几次拍摄的情况下进行操作，无需对每一个场景进行微调。借助预训练的扩散模型，它实现了强大的多视角一致性编辑。该框架包含两个新颖组件：参照多视角编辑器和任意视角视频合成器。Tinker显著降低了通用3D内容创作的门槛，并在编辑、新视角合成和渲染增强任务上达到了卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Tinker是一个适用于高保真3D编辑的通用框架，可在一次拍摄和少数几次拍摄的情况下操作，无需对每个场景进行微调。</li>
<li>Tinker利用预训练的扩散模型实现强大的多视角一致性编辑。</li>
<li>框架包含两个新颖组件：参照多视角编辑器，实现精确、参考驱动的编辑，保持所有视点的连贯性；任意视角视频合成器，利用视频扩散的空间时间先验信息进行高质量场景补全和新视角生成，即使从稀疏输入也能实现。</li>
<li>Tinker显著降低了通用3D内容创作的门槛。</li>
<li>Tinker在编辑、新视角合成和渲染增强任务上表现卓越。</li>
<li>Tinker代表了一个关键步骤，朝着真正可扩展的零射击3D编辑方向发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14811">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7bc49620e6a2187049a01ee3b8111277.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a57b54c614af3a291589bfa6d752534d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cdcd6065c93557e322b686fd6318041.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68f710c3b78a56f1a3d56ed689de2313.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MUSE-Multi-Subject-Unified-Synthesis-via-Explicit-Layout-Semantic-Expansion"><a href="#MUSE-Multi-Subject-Unified-Synthesis-via-Explicit-Layout-Semantic-Expansion" class="headerlink" title="MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic   Expansion"></a>MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic   Expansion</h2><p><strong>Authors:Fei Peng, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, Huiyuan Fu</strong></p>
<p>Existing text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images guided by textual prompts. However, achieving multi-subject compositional synthesis with precise spatial control remains a significant challenge. In this work, we address the task of layout-controllable multi-subject synthesis (LMS), which requires both faithful reconstruction of reference subjects and their accurate placement in specified regions within a unified image. While recent advancements have separately improved layout control and subject synthesis, existing approaches struggle to simultaneously satisfy the dual requirements of spatial precision and identity preservation in this composite task. To bridge this gap, we propose MUSE, a unified synthesis framework that employs concatenated cross-attention (CCA) to seamlessly integrate layout specifications with textual guidance through explicit semantic space expansion. The proposed CCA mechanism enables bidirectional modality alignment between spatial constraints and textual descriptions without interference. Furthermore, we design a progressive two-stage training strategy that decomposes the LMS task into learnable sub-objectives for effective optimization. Extensive experiments demonstrate that MUSE achieves zero-shot end-to-end generation with superior spatial accuracy and identity consistency compared to existing solutions, advancing the frontier of controllable image synthesis. Our code and model are available at <a target="_blank" rel="noopener" href="https://github.com/pf0607/MUSE">https://github.com/pf0607/MUSE</a>. </p>
<blockquote>
<p>现有的文本到图像的扩散模型已在由文本提示引导生成高质量图像方面表现出显著的能力。然而，实现具有精确空间控制的多主题组合合成仍然是一个重大挑战。在这项工作中，我们解决了布局可控多主题合成（LMS）的任务，该任务要求忠实地重建参考主题，并将它们准确放置在统一图像的指定区域内。虽然最近的进步已经分别提高了布局控制和主题合成的效果，但现有方法在这项组合任务中同时满足空间精度和身份保留的双重要求时仍然感到困难。为了弥合这一差距，我们提出了MUSE，这是一个采用串联交叉注意（CCA）的统一合成框架，通过显式语义空间扩展无缝地将布局规范与文本指导相结合。所提出的CCA机制实现了空间约束和文本描述之间的双向模态对齐，不会相互干扰。此外，我们设计了一种分阶段的两阶段训练策略，将LMS任务分解为可学习的子目标，以实现有效的优化。大量实验表明，MUSE实现了与现有解决方案相比具有卓越的空间精度和身份一致性的零样本端到端生成。我们的代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/pf060a7/MUSE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/pf060a7/MUSE找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14440v1">PDF</a> This paper is accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一个名为MUSE的统一合成框架，用于解决布局可控的多主题合成（LMS）任务。该框架通过采用串联交叉注意力（CCA）机制，将布局规范与文本指导无缝集成，实现空间约束与文本描述之间的双向模态对齐。MUSE框架能够在零样本端到端生成中实现出色的空间准确性和身份一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有文本到图像的扩散模型在高质量图像生成方面表现出色，但在具有精确空间控制的多主题合成方面仍存在挑战。</li>
<li>MUSE框架解决了布局可控的多主题合成（LMS）任务，该任务需要忠实重建参考主题并将它们准确放置在统一图像的指定区域。</li>
<li>MUSE采用串联交叉注意力（CCA）机制，将布局规范和文本指导无缝集成，实现空间约束和文本描述之间的双向模态对齐，解决了现有方法的不足。</li>
<li>MUSE设计了一种渐进的两阶段训练策略，将LMS任务分解成可学习的子目标，实现有效优化。</li>
<li>实验表明，MUSE在零样本端到端生成中实现了卓越的空间准确性和身份一致性，相较于现有解决方案有所突破。</li>
<li>MUSE框架的代码和模型已公开发布，可供研究使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14440">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cf239a50b5d1951e62aa2f9537317b66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b132ac75a0fbf2df284591750ce55b3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6666f761d15292d1d6e9e9752c007639.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b052c0d2139fac90cb3a7cf2c5b88a34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d19aec92c2c44704b1398a6dea8e49c3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Latent-Interpolation-Learning-Using-Diffusion-Models-for-Cardiac-Volume-Reconstruction"><a href="#Latent-Interpolation-Learning-Using-Diffusion-Models-for-Cardiac-Volume-Reconstruction" class="headerlink" title="Latent Interpolation Learning Using Diffusion Models for Cardiac Volume   Reconstruction"></a>Latent Interpolation Learning Using Diffusion Models for Cardiac Volume   Reconstruction</h2><p><strong>Authors:Niklas Bubeck, Suprosanna Shit, Chen Chen, Can Zhao, Pengfei Guo, Dong Yang, Georg Zitzlsberger, Daguang Xu, Bernhard Kainz, Daniel Rueckert, Jiazhen Pan</strong></p>
<p>Cardiac Magnetic Resonance (CMR) imaging is a critical tool for diagnosing and managing cardiovascular disease, yet its utility is often limited by the sparse acquisition of 2D short-axis slices, resulting in incomplete volumetric information. Accurate 3D reconstruction from these sparse slices is essential for comprehensive cardiac assessment, but existing methods face challenges, including reliance on predefined interpolation schemes (e.g., linear or spherical), computational inefficiency, and dependence on additional semantic inputs such as segmentation labels or motion data. To address these limitations, we propose a novel \textbf{Ca}rdiac \textbf{L}atent \textbf{I}nterpolation \textbf{D}iffusion (CaLID) framework that introduces three key innovations. First, we present a data-driven interpolation scheme based on diffusion models, which can capture complex, non-linear relationships between sparse slices and improves reconstruction accuracy. Second, we design a computationally efficient method that operates in the latent space and speeds up 3D whole-heart upsampling time by a factor of 24, reducing computational overhead compared to previous methods. Third, with only sparse 2D CMR images as input, our method achieves SOTA performance against baseline methods, eliminating the need for auxiliary input such as morphological guidance, thus simplifying workflows. We further extend our method to 2D+T data, enabling the effective modeling of spatiotemporal dynamics and ensuring temporal coherence. Extensive volumetric evaluations and downstream segmentation tasks demonstrate that CaLID achieves superior reconstruction quality and efficiency. By addressing the fundamental limitations of existing approaches, our framework advances the state of the art for spatio and spatiotemporal whole-heart reconstruction, offering a robust and clinically practical solution for cardiovascular imaging. </p>
<blockquote>
<p>心脏磁共振（CMR）成像在心血管疾病的诊断和治疗中起着至关重要的作用，但其效用往往受到二维短轴切片稀疏采集的限制，导致体积信息不完整。从稀疏切片进行准确的3D重建对于全面的心脏评估至关重要，但现有方法面临挑战，包括依赖预定义的插值方案（例如线性或球形插值）、计算效率低下以及对额外的语义输入（如分割标签或运动数据）的依赖。为了解决这些局限性，我们提出了一种新颖的Cardiac Latent Interpolation Diffusion（CaLID）框架，该框架引入了三项关键创新。首先，我们提出了一种基于扩散模型的数据驱动插值方案，该方案可以捕捉稀疏切片之间的复杂非线性关系，提高重建精度。其次，我们设计了一种在潜在空间中进行操作的高效计算方法，将心脏整体3D上采样时间缩短了24倍，与以前的方法相比减少了计算开销。第三，我们的方法仅使用稀疏的二维心脏磁共振图像作为输入，无需辅助输入（如形态指导），与基线方法相比取得了先进性能表现。我们进一步将我们的方法扩展到二维+T数据，有效地对时空动态进行建模，确保时间连贯性。广泛的体积评估和下游分割任务证明，CaLID在重建质量和效率方面都达到了卓越的水平。通过解决现有方法的基本局限性，我们的框架在空间和时空心脏重建方面推动了最新技术进展，为心血管成像提供了稳健且实用的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13826v2">PDF</a> </p>
<p><strong>Summary</strong><br>     心脏磁共振（CMR）成像在诊断和治疗心血管疾病中至关重要，但其效用常受限于二维短轴切片的稀疏采集，导致体积信息不完整。针对这一问题，我们提出了全新的心脏潜在插值扩散（CaLID）框架，具有数据驱动插值方案、潜在空间的高效运算方法，以及仅依赖稀疏的二维CMR图像即可达到优越性能等优点。此框架突破了现有方法的局限，提升了重建质量和效率，实现了时空心脏重建的最新进展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CMR成像在心血管疾病的诊断和管理中扮演重要角色，但受限于二维切片的稀疏采集导致的体积信息不完整问题。</li>
<li>现有重建方法面临的挑战包括依赖预设插值方案、计算效率低下以及对额外语义输入的依赖。</li>
<li>CaLID框架引入数据驱动插值方案，能捕捉稀疏切片间的复杂非线性关系，提高重建精度。</li>
<li>该框架设计了一种潜在空间的高效运算方法，将心脏三维上采样的时间提高了24倍。</li>
<li>仅需稀疏的二维CMR图像作为输入，CaLID即可达到卓越性能，无需辅助输入，简化了工作流程。</li>
<li>CaLID框架进一步扩展到二维加时间数据，实现了时空动力学的有效建模和时间的连贯性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13826">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-35557ded8bbf8a22943aa8c1de831129.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b3c2aacf31192a2998523e57c1e2452.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9db76104ad21d445d80009b4eb9b5fd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b3e8fbd563620fc41599e9edf300cdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b595fe2d116ad3426b59833e331eab7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DiffIER-Optimizing-Diffusion-Models-with-Iterative-Error-Reduction"><a href="#DiffIER-Optimizing-Diffusion-Models-with-Iterative-Error-Reduction" class="headerlink" title="DiffIER: Optimizing Diffusion Models with Iterative Error Reduction"></a>DiffIER: Optimizing Diffusion Models with Iterative Error Reduction</h2><p><strong>Authors:Ao Chen, Lihe Ding, Tianfan Xue</strong></p>
<p>Diffusion models have demonstrated remarkable capabilities in generating high-quality samples and enhancing performance across diverse domains through Classifier-Free Guidance (CFG). However, the quality of generated samples is highly sensitive to the selection of the guidance weight. In this work, we identify a critical &#96;&#96;training-inference gap’’ and we argue that it is the presence of this gap that undermines the performance of conditional generation and renders outputs highly sensitive to the guidance weight. We quantify this gap by measuring the accumulated error during the inference stage and establish a correlation between the selection of guidance weight and minimizing this gap. Furthermore, to mitigate this gap, we propose DiffIER, an optimization-based method for high-quality generation. We demonstrate that the accumulated error can be effectively reduced by an iterative error minimization at each step during inference. By introducing this novel plug-and-play optimization framework, we enable the optimization of errors at every single inference step and enhance generation quality. Empirical results demonstrate that our proposed method outperforms baseline approaches in conditional generation tasks. Furthermore, the method achieves consistent success in text-to-image generation, image super-resolution, and text-to-speech generation, underscoring its versatility and potential for broad applications in future research. </p>
<blockquote>
<p>扩散模型通过无分类器引导（CFG）在生成高质量样本和提高不同领域的性能方面表现出了卓越的能力。然而，生成样本的质量对引导权重的选择非常敏感。在这项工作中，我们识别出了一个关键的“训练-推理差距”，我们认为正是这个差距影响了条件生成的性能，并使输出高度依赖于引导权重。我们通过测量推理阶段的累积误差来量化这个差距，并建立了选择引导权重与最小化这个差距之间的关联。此外，为了缓解这一差距，我们提出了DiffIER，这是一种基于优化的高质量生成方法。我们证明，通过推理过程中每一步的迭代误差最小化，可以有效减少累积误差。通过引入这种新颖即插即用的优化框架，我们能够在每个单独推理步骤中优化误差，提高生成质量。经验结果表明，我们所提出的方法在条件生成任务上优于基准方法。此外，该方法在文本到图像生成、图像超分辨率和文本到语音生成方面取得了持续的成功，这突显了其在未来研究中的通用性和广泛应用潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13628v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了扩散模型在生成高质量样本和提升性能方面的显著能力，特别是通过无分类器引导（CFG）实现。然而，生成的样本质量对引导权重的选择非常敏感。本文识别了一个关键的“训练-推理差距”，并认为这个差距影响了条件生成性能，使得输出对引导权重高度敏感。为了量化这个差距，我们测量了推理阶段的累积误差，建立了引导权重选择与缩小差距之间的关联。为了缓解这一问题，我们提出了DiffIER，这是一种基于优化的高质量生成方法。通过在每个推理步骤中引入迭代误差最小化，我们可以有效减少累积误差。我们展示了这个新框架在优化误差方面的优势，并增强了生成质量。经验结果表明，我们的方法在条件生成任务上优于基准方法，并且在文本到图像生成、图像超分辨率和文本到语音生成等任务中取得了持续的成功。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型通过无分类器引导（CFG）展现了强大的生成样本和性能提升能力。</li>
<li>训练生成的模型在样本质量方面对引导权重的选择非常敏感。</li>
<li>存在一个关键的“训练-推理差距”，影响了条件生成性能。</li>
<li>通过测量推理阶段的累积误差来量化这个差距。</li>
<li>提出了DiffIER方法，通过迭代误差最小化的优化框架来缩小差距并提高生成质量。</li>
<li>实证结果表明，DiffIER方法在条件生成任务上优于其他方法。</li>
<li>DiffIER方法在不同领域如文本到图像生成、图像超分辨率和文本到语音生成等任务中均有成功应用，展现了其广泛的应用潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13628">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6660ec06ea983b3718bb6178bb74c51f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b930ab27e19b587a0b375e5b562d4d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e391cf99658d7fa44fd322b3db1d0ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d613ceafcd367c3788f918797f75b6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90754254ba6f2d1faf962c9d63bcf529.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="BadBlocks-Low-Cost-and-Stealthy-Backdoor-Attacks-Tailored-for-Text-to-Image-Diffusion-Models"><a href="#BadBlocks-Low-Cost-and-Stealthy-Backdoor-Attacks-Tailored-for-Text-to-Image-Diffusion-Models" class="headerlink" title="BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for   Text-to-Image Diffusion Models"></a>BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for   Text-to-Image Diffusion Models</h2><p><strong>Authors:Yu Pan, Jiahao Chen, Lin Wang, Bingrong Dai, Yi Du</strong></p>
<p>In recent years, Diffusion models have achieved remarkable progress in the field of image generation. However, recent studies have shown that diffusion models are susceptible to backdoor attacks, in which attackers can manipulate the output by injecting covert triggers such as specific visual patterns or textual phrases into the training dataset. Fortunately, with the continuous advancement of defense techniques, defenders have become increasingly capable of identifying and mitigating most backdoor attacks using visual inspection and neural network-based detection methods. However, in this paper, we identify a novel type of backdoor threat that is more lightweight and covert than existing approaches, which we name BadBlocks, requires only about 30% of the computational resources and 20% GPU time typically needed by previous backdoor attacks, yet it successfully injects backdoors and evades the most advanced defense frameworks. BadBlocks enables attackers to selectively contaminate specific blocks within the UNet architecture of diffusion models while maintaining normal functionality in the remaining components. Experimental results demonstrate that BadBlocks achieves a high attack success rate and low perceptual quality loss , even under extremely constrained computational resources and GPU time. Moreover, BadBlocks is able to bypass existing defense frameworks, especially the attention-based backdoor detection method, highlighting it as a novel and noteworthy threat. Ablation studies further demonstrate that effective backdoor injection does not require fine-tuning the entire network and highlight the pivotal role of certain neural network layers in backdoor mapping. Overall, BadBlocks significantly reduces the barrier to conducting backdoor attacks in all aspects. It enables attackers to inject backdoors into large-scale diffusion models even using consumer-grade GPUs. </p>
<blockquote>
<p>近年来，扩散模型在图像生成领域取得了显著的进步。然而，研究表明，扩散模型容易受到后门攻击的影响，攻击者可以通过在训练数据集中注入隐蔽的触发器（例如特定的视觉模式或文本短语）来操纵输出。幸运的是，随着防御技术的不断进步，防御者越来越能够使用视觉检查和基于神经网络的检测方法来识别和缓解大多数后门攻击。然而，本文发现了一种比现有方法更轻量级、更隐蔽的新型后门威胁，我们称之为BadBlocks。BadBlocks仅需要大约30%的计算资源和20%的GPU时间，这是以前后门攻击通常所需的，然而它却能够成功注入后门并绕过最先进的防御框架。BadBlocks能够使攻击者选择性地污染扩散模型UNet架构中的特定块，同时保持其余组件的正常功能。实验结果表明，即使在极度受限的计算资源和GPU时间下，BadBlocks也实现了高攻击成功率和低感知质量损失。而且，BadBlocks能够绕过现有的防御框架，尤其是基于注意力的后门检测方法，凸显出它作为一种新型且值得关注的威胁。进一步的研究表明，有效的后门注入不需要对整个网络进行微调，并突出了某些神经网络层在后门映射中的关键作用。总体而言，BadBlocks从各个方面大大降低了进行后门攻击的障碍。它使得攻击者即使使用消费级GPU也能将后门注入大规模扩散模型中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03221v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对扩散模型的新型后门攻击方法——BadBlocks。相较于以往的后门攻击方法，BadBlocks更加轻便且隐蔽，能在更低的计算资源和GPU时间内成功注入后门并绕过最先进的防御框架。BadBlocks能够选择性地污染扩散模型中的特定块，同时保持其他组件的正常功能。实验结果显示，BadBlocks具有高攻击成功率和低感知质量损失的特点。该威胁降低了实施后门攻击门槛，即使是消费者级别的GPU也可以攻击大规模扩散模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像生成领域取得了显著进展，但存在后门攻击风险。</li>
<li>BadBlocks是一种新型后门攻击方法，相较于传统方法更轻便且隐蔽。</li>
<li>BadBlocks仅需约30%的计算资源和20%的GPU时间，成功注入后门并绕过最先进的防御框架。</li>
<li>BadBlocks能够选择性地污染扩散模型中的特定块，保持其他部分正常运作。</li>
<li>实验结果显示BadBlocks具有高攻击成功率和低感知质量损失的特点。</li>
<li>BadBlocks能绕过现有的防御框架，特别是基于注意力的后门检测方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03221">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-29f8227a455cce93af33fe64d8bc0d31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c9f685408054dc342919d6a74e05e4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75091792281a2f4eaa2ab61b326e672f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a1a57d1e9d54d805dc013b84b38cb5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-954264da8793b3152041455699afc95a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Marrying-Autoregressive-Transformer-and-Diffusion-with-Multi-Reference-Autoregression"><a href="#Marrying-Autoregressive-Transformer-and-Diffusion-with-Multi-Reference-Autoregression" class="headerlink" title="Marrying Autoregressive Transformer and Diffusion with Multi-Reference   Autoregression"></a>Marrying Autoregressive Transformer and Diffusion with Multi-Reference   Autoregression</h2><p><strong>Authors:Dingcheng Zhen, Qian Qiao, Xu Zheng, Tan Yu, Kangxi Wu, Ziwei Zhang, Siyuan Liu, Shunshun Yin, Ming Tao</strong></p>
<p>We introduce TransDiff, the first image generation model that marries Autoregressive (AR) Transformer with diffusion models. In this joint modeling framework, TransDiff encodes labels and images into high-level semantic features and employs a diffusion model to estimate the distribution of image samples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms other image generation models based on standalone AR Transformer or diffusion models. Specifically, TransDiff achieves a Frechet Inception Distance (FID) of 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster inference latency compared to state-of-the-art methods based on AR Transformer and x112 faster inference compared to diffusion-only models. Furthermore, building on the TransDiff model, we introduce a novel image generation paradigm called Multi-Reference Autoregression (MRAR), which performs autoregressive generation by predicting the next image. MRAR enables the model to reference multiple previously generated images, thereby facilitating the learning of more diverse representations and improving the quality of generated images in subsequent iterations. By applying MRAR, the performance of TransDiff is improved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open up a new frontier in the field of image generation. </p>
<blockquote>
<p>我们介绍了TransDiff，这是第一个将自回归（AR）Transformer与扩散模型相结合的图片生成模型。在这个联合建模框架中，TransDiff将标签和图像编码为高级语义特征，并采用扩散模型来估计图像样本的分布。在ImageNet 256x256基准测试中，TransDiff显著优于其他基于独立AR Transformer或扩散模型的图像生成模型。具体来说，TransDiff达到了Frechet Inception Distance（FID）为1.61和Inception Score（IS）为293.4的水平，与基于AR Transformer的现有先进技术相比，提供了x2更快的推理延迟，与仅使用扩散的模型相比，推理速度提高了x112。此外，我们以TransDiff模型为基础，引入了一种新的图像生成范式——多参考自回归（MRAR）。MRAR通过预测下一个图像进行自回归生成，使模型能够参考多个先前生成的图像，从而更容易学习更多样化的表示，并在后续迭代中提高生成的图像质量。通过应用MRAR，TransDiff的性能得到了提升，FID从1.61降低到了1.42。我们期望TransDiff能在图像生成领域开辟新的前沿。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09482v3">PDF</a> </p>
<p><strong>Summary</strong><br>     本文介绍了结合自回归（AR）Transformer与扩散模型的图像生成模型TransDiff。它采用联合建模框架，将标签和图像编码为高级语义特征，并利用扩散模型估计图像样本的分布。在ImageNet 256x256基准测试中，TransDiff显著优于其他基于独立AR Transformer或扩散模型的图像生成模型。此外，它提供了基于MRAR（多参考自回归）的新图像生成范式，通过预测下一个图像进行自回归生成，提高了模型的多样性和生成图像的质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TransDiff是首个结合自回归（AR）Transformer和扩散模型的图像生成模型。</li>
<li>TransDiff在ImageNet 256x256基准测试中表现优异，FID达到1.61，IS达到293.4。</li>
<li>TransDiff提供了较快的推理速度，与基于AR Transformer的当前方法相比，推理延迟时间加快了x2，与仅使用扩散模型的方法相比，推理延迟时间加快了x112。</li>
<li>TransDiff引入了新的图像生成范式——Multi-Reference Autoregression（MRAR）。</li>
<li>MRAR能够参考多个先前生成的图像，从而提高模型的多样性并改善后续迭代中生成图像的质量。</li>
<li>应用MRAR后，TransDiff的性能得到提升，FID从1.61降至1.42。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09482">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-33f5e40284145f1f4aeef313ac857171.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-619a9ed9863cc171e75a3d7f3ad6e99c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1ccfb2167c6b90a5873dae20b96d820.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Extremum-Flow-Matching-for-Offline-Goal-Conditioned-Reinforcement-Learning"><a href="#Extremum-Flow-Matching-for-Offline-Goal-Conditioned-Reinforcement-Learning" class="headerlink" title="Extremum Flow Matching for Offline Goal Conditioned Reinforcement   Learning"></a>Extremum Flow Matching for Offline Goal Conditioned Reinforcement   Learning</h2><p><strong>Authors:Quentin Rouxel, Clemente Donoso, Fei Chen, Serena Ivaldi, Jean-Baptiste Mouret</strong></p>
<p>Imitation learning is a promising approach for enabling generalist capabilities in humanoid robots, but its scaling is fundamentally constrained by the scarcity of high-quality expert demonstrations. This limitation can be mitigated by leveraging suboptimal, open-ended play data, often easier to collect and offering greater diversity. This work builds upon recent advances in generative modeling, specifically Flow Matching, an alternative to Diffusion models. We introduce a method for estimating the minimum or maximum of the learned distribution by leveraging the unique properties of Flow Matching, namely, deterministic transport and support for arbitrary source distributions. We apply this method to develop several goal-conditioned imitation and reinforcement learning algorithms based on Flow Matching, where policies are conditioned on both current and goal observations. We explore and compare different architectural configurations by combining core components, such as critic, planner, actor, or world model, in various ways. We evaluated our agents on the OGBench benchmark and analyzed how different demonstration behaviors during data collection affect performance in a 2D non-prehensile pushing task. Furthermore, we validated our approach on real hardware by deploying it on the Talos humanoid robot to perform complex manipulation tasks based on high-dimensional image observations, featuring a sequence of pick-and-place and articulated object manipulation in a realistic kitchen environment. Experimental videos and code are available at: <a target="_blank" rel="noopener" href="https://hucebot.github.io/extremum_flow_matching_website/">https://hucebot.github.io/extremum_flow_matching_website/</a> </p>
<blockquote>
<p>模仿学习是在人形机器人中实现通用能力的一种有前途的方法，但其扩展性从根本上受到高质量专家演示稀缺性的限制。通过利用次优的、开放式的游戏数据，可以缓解这种限制，这些数据通常更容易收集并且具有更大的多样性。这项工作建立在生成建模的最新进展之上，特别是流匹配（Flow Matching）技术——一种扩散模型的替代方案。我们引入了一种方法，通过利用流匹配的独特属性，即确定性传输和任意源分布的支持，来估计所学分布的最小值或最大值。我们将这种方法应用于基于流匹配的目标条件模仿和强化学习算法的开发，其中策略既取决于当前观察也取决于目标观察。我们通过组合核心组件，如评论家、规划师、演员或世界模型，以各种方式探索并比较不同的架构配置。我们在OGBench基准上评估了我们的代理，并分析了在数据收集过程中不同演示行为对二维非抓取推动任务性能的影响。此外，我们通过将方法部署在Talos人形机器人上执行基于高维图像观察的复杂操作任务，来验证我们的方法在真实硬件上的有效性。这些任务包括一系列拾取和放置以及关节对象操作，在一个现实厨房环境中进行。实验视频和代码可访问于：<a target="_blank" rel="noopener" href="https://hucebot.github.io/extremum_flow_matching_website/">https://hucebot.github.io/extremum_flow_matching_website/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19717v2">PDF</a> 2025 IEEE-RAS 24th International Conference on Humanoid Robots   (Humanoids), Sep 2025, Seoul, South Korea</p>
<p><strong>Summary</strong><br>     本论文探讨了利用流匹配技术实现基于模仿学习和强化学习的机器人通用能力的方法。研究通过结合流匹配技术的独特属性，例如确定性传输和任意源分布的支持，发展了一系列目标条件下的模仿和强化学习算法。研究在OGBench基准测试上评估了智能体性能，并探讨了数据收集过程中的不同演示行为对二维非抓取推动任务性能的影响。此外，该研究还在Talos人形机器人上进行了复杂操作任务的现实硬件验证，包括一系列基于高维图像观察的拾取和放置以及关节式物体操控任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用模仿学习实现机器人通用能力的方法具有潜力，但受限于高质量专家演示的稀缺性。</li>
<li>提出利用次优、开放式的游戏数据来缓解这一限制，这些数据更容易收集且更具多样性。</li>
<li>引入基于流匹配技术的估计分布极值的方法，利用流匹配的确定性传输和任意源分布支持等独特属性。</li>
<li>发展了一系列目标条件下的模仿和强化学习算法，这些算法结合了核心组件，如评价者、规划者、行动者或世界模型。</li>
<li>在OGBench基准测试上评估了智能体性能，并探索了数据收集过程中不同演示行为对任务性能的影响。</li>
<li>在Talos人形机器人上进行了复杂操作任务的现实硬件验证，展示了一系列基于高维图像观察的操控任务。</li>
<li>实验视频和代码可通过相关网站获取。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19717">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d53ca718be4912c0fa65f91904326d7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-415ac7ad964e97413e3c1be4ccca6293.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e744a74602d78f4c4a713c29b7ab2678.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03393c433d531ae097ad01a004a9ddf8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0b012a0f2f12c97d351b2ab8702c234.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Reconstruction-Free-Anomaly-Detection-with-Diffusion-Models"><a href="#Reconstruction-Free-Anomaly-Detection-with-Diffusion-Models" class="headerlink" title="Reconstruction-Free Anomaly Detection with Diffusion Models"></a>Reconstruction-Free Anomaly Detection with Diffusion Models</h2><p><strong>Authors:Shunsuke Sakai, Xiangteng He, Chunzhi Gu, Leonid Sigal, Tatsuhito Hasegawa</strong></p>
<p>Despite the remarkable success, recent reconstruction-based anomaly detection (AD) methods via diffusion modeling still involve fine-grained noise-strength tuning and computationally expensive multi-step denoising, leading to a fundamental tension between fidelity and efficiency. In this paper, we propose a novel inversion-based AD approach - detection via noising in latent space - which circumvents explicit reconstruction. Importantly, we contend that the limitations in prior reconstruction-based methods originate from the prevailing detection via denoising in RGB space paradigm. To address this, we model AD under a reconstruction-free formulation, which directly infers the final latent variable corresponding to the input image via DDIM inversion, and then measures the deviation based on the known prior distribution for anomaly scoring. Specifically, in approximating the original probability flow ODE using the Euler method, we only enforce very few inversion steps to noise the clean image to pursue inference efficiency. As the added noise is adaptively derived with the learned diffusion model, the original features for the clean testing image can still be leveraged to yield high detection accuracy. We perform extensive experiments and detailed analysis across three widely used image AD datasets under the unsupervised unified setting to demonstrate the effectiveness of our model, regarding state-of-the-art AD performance, and about 2 times inference time speedup without diffusion distillation. </p>
<blockquote>
<p>尽管取得了显著的成果，但最近基于重建的异常检测（AD）方法通过扩散建模仍然涉及精细的噪声强度调整和计算昂贵的多步去噪，从而在保真度和效率之间产生基本矛盾。在本文中，我们提出了一种基于反转的新颖AD方法——通过潜在空间的噪声检测——避免了明确的重建过程。重要的是，我们认为先前基于重建的方法的限制源于流行的基于RGB空间去噪检测的模式。为了解决这个问题，我们在无需重建的公式下对AD进行建模，该公式通过DDIM反转直接推断对应于输入图像的最终潜在变量，然后根据已知先验分布测量偏差以进行异常评分。具体来说，在利用欧拉方法近似原始概率流ODE时，我们只强制执行很少的反转步骤来对干净图像添加噪声，以追求推理效率。由于添加的噪声是借助学习到的扩散模型自适应得出的，因此仍然可以利用干净测试图像的原特征来实现高检测精度。我们在三个广泛使用的图像AD数据集上进行了大量实验和详细分析，以展示我们的模型在无监督统一设置下的有效性，包括在异常检测方面的最新性能以及在未使用扩散蒸馏的情况下约加快2倍的推理时间。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05662v2">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/SkyShunsuke/InversionAD">https://github.com/SkyShunsuke/InversionAD</a></p>
<p><strong>Summary</strong><br>     本文提出了一种基于逆过程的异常检测（AD）方法，通过在潜在空间进行噪声检测，避免了显式的重建过程。该方法通过直接推断输入图像对应的最终潜在变量，并基于已知先验分布测量偏差来进行异常评分，实现了高效且高准确率的异常检测。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>重建方法在处理异常检测时存在精细噪声强度调整和计算昂贵的多步去噪问题，影响保真度和效率。</li>
<li>提出一种新型的基于逆过程的异常检测方法，通过潜在空间的噪声检测绕过显式的重建过程。</li>
<li>现有重建方法的局限性源于RGB空间去噪检测的模式，而新方法采用无重建公式进行建模，直接推断输入图像对应的潜在变量。</li>
<li>通过使用Euler方法近似原始概率流ODE，仅执行少量逆步骤对干净图像进行噪声处理，提高了推理效率。</li>
<li>借助学习到的扩散模型自适应地生成噪声，同时利用干净测试图像的原特征，实现了高检测精度。</li>
<li>在三个广泛使用的图像异常检测数据集上进行了大量实验和详细分析，证明了该方法在异常检测性能上的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05662">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1c050efb66c1747a105dcacc3f457c1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33093207d5ff22dc03ba714a4d8694f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bd3b5718d4bac2922efcdcfe6a56d8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fdc450a1113c84014fe4a33f2a9c27f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-519b468674fd0fbd1a78533de12516c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80d45f79204517e4403686c70321857b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Identity-Preserving-3D-Head-Stylization-with-Multiview-Score-Distillation"><a href="#Identity-Preserving-3D-Head-Stylization-with-Multiview-Score-Distillation" class="headerlink" title="Identity Preserving 3D Head Stylization with Multiview Score   Distillation"></a>Identity Preserving 3D Head Stylization with Multiview Score   Distillation</h2><p><strong>Authors:Bahri Batuhan Bilecen, Ahmet Berke Gokmen, Furkan Guzelant, Aysegul Dundar</strong></p>
<p>3D head stylization transforms realistic facial features into artistic representations, enhancing user engagement across gaming and virtual reality applications. While 3D-aware generators have made significant advancements, many 3D stylization methods primarily provide near-frontal views and struggle to preserve the unique identities of original subjects, often resulting in outputs that lack diversity and individuality. This paper addresses these challenges by leveraging the PanoHead model, synthesizing images from a comprehensive 360-degree perspective. We propose a novel framework that employs negative log-likelihood distillation (LD) to enhance identity preservation and improve stylization quality. By integrating multi-view grid score and mirror gradients within the 3D GAN architecture and introducing a score rank weighing technique, our approach achieves substantial qualitative and quantitative improvements. Our findings not only advance the state of 3D head stylization but also provide valuable insights into effective distillation processes between diffusion models and GANs, focusing on the critical issue of identity preservation. Please visit the <a target="_blank" rel="noopener" href="https://three-bee.github.io/head_stylization">https://three-bee.github.io/head_stylization</a> for more visuals. </p>
<blockquote>
<p>3D头部风格化转换能将真实的面部特征转化为艺术表现形式，增强游戏和虚拟现实应用中的用户参与度。虽然3D感知生成器已经取得了重大进展，但许多3D风格化方法主要提供近乎正面的视角，并且在保留原始主体的独特身份方面存在困难，往往导致输出结果缺乏多样性和个性化。本文通过利用PanoHead模型，从全面的360度视角合成图像，来解决这些挑战。我们提出了一种采用负对数似然蒸馏（LD）的新框架，以提高身份保留和提高风格化质量。通过在3D GAN架构中整合多视图网格评分和镜像梯度，并引入评分排名加权技术，我们的方法在定性和定量方面都取得了显著的改进。我们的研究不仅推动了3D头部风格化的现状，而且为扩散模型和GANs之间的有效蒸馏过程提供了有价值的见解，重点关注身份保留这一关键问题。更多视觉效果请访问 <a target="_blank" rel="noopener" href="https://three-bee.github.io/head_stylization%E3%80%82">https://three-bee.github.io/head_stylization。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13536v3">PDF</a> <a target="_blank" rel="noopener" href="https://three-bee.github.io/head_stylization">https://three-bee.github.io/head_stylization</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于PanoHead模型的3D头部风格化技术，该技术可以从全方位的视角合成图像，解决了现有方法在头部风格化中面临的视角局限和身份保留问题。通过引入负对数似然蒸馏（LD）技术，结合多视图网格评分和镜像梯度在3D GAN架构中的应用，以及评分排名加权技术，实现了实质性的定性和定量改进。该研究不仅推动了3D头部风格化的发展，还为扩散模型和GAN之间的有效蒸馏过程提供了有价值的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D头部风格化能增强游戏和虚拟现实应用中的用户参与度，通过艺术化的面部特征表现提升用户体验。</li>
<li>当前3D风格化方法主要提供近正面视角，难以保留原始主体的独特身份，导致输出缺乏多样性和个性化。</li>
<li>PanoHead模型能从全方位的视角合成图像，解决了视角局限问题。</li>
<li>引入负对数似然蒸馏（LD）技术，提升身份保留和风格化质量。</li>
<li>结合多视图网格评分和镜像梯度在3D GAN架构中，提高了模型性能。</li>
<li>评分排名加权技术有助于实现实质性的定性和定量改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.13536">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8ac76f674ffd8641b0f31b116585c35c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1612f87ccdf6345ee2c899364b1ce24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-553bf29d23535a63e778d67323ea9be0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0afc402e0563d5c0e25c3f0b8adc57cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-136a4a25b7a3be398142db937a06e31e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Six-CD-Benchmarking-Concept-Removals-for-Benign-Text-to-image-Diffusion-Models"><a href="#Six-CD-Benchmarking-Concept-Removals-for-Benign-Text-to-image-Diffusion-Models" class="headerlink" title="Six-CD: Benchmarking Concept Removals for Benign Text-to-image Diffusion   Models"></a>Six-CD: Benchmarking Concept Removals for Benign Text-to-image Diffusion   Models</h2><p><strong>Authors:Jie Ren, Kangrui Chen, Yingqian Cui, Shenglai Zeng, Hui Liu, Yue Xing, Jiliang Tang, Lingjuan Lyu</strong></p>
<p>Text-to-image (T2I) diffusion models have shown exceptional capabilities in generating images that closely correspond to textual prompts. However, the advancement of T2I diffusion models presents significant risks, as the models could be exploited for malicious purposes, such as generating images with violence or nudity, or creating unauthorized portraits of public figures in inappropriate contexts. To mitigate these risks, concept removal methods have been proposed. These methods aim to modify diffusion models to prevent the generation of malicious and unwanted concepts. Despite these efforts, existing research faces several challenges: (1) a lack of consistent comparisons on a comprehensive dataset, (2) ineffective prompts in harmful and nudity concepts, (3) overlooked evaluation of the ability to generate the benign part within prompts containing malicious concepts. To address these gaps, we propose to benchmark the concept removal methods by introducing a new dataset, Six-CD, along with a novel evaluation metric. In this benchmark, we conduct a thorough evaluation of concept removals, with the experimental observations and discussions offering valuable insights in the field. </p>
<blockquote>
<p>文本到图像（T2I）扩散模型在生成与文本提示紧密对应的图像方面表现出卓越的能力。然而，T2I扩散模型的进步也带来了重大风险，因为这些模型可能会被用于恶意目的，例如生成暴力或裸体图像，或在不当背景下创建公众人物的未经授权肖像。为了减少这些风险，已经提出了概念移除方法。这些方法旨在修改扩散模型，以防止生成恶意和不需要的概念。尽管付出了这些努力，现有研究仍面临一些挑战：（1）缺乏在综合数据集上的一致比较，（2）在有害和裸体概念方面的提示无效，（3）忽视了在包含恶意概念的提示中生成良性部分的能力的评估。为了弥补这些不足，我们提议通过引入新的数据集Six-CD以及一种新的评估指标来评估概念移除方法。在此基准测试中，我们对概念移除进行了全面评估，实验观察和讨论为该领域提供了宝贵的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14855v3">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>文本到图像（T2I）的扩散模型在根据文本提示生成图像方面表现出卓越的能力。然而，这些模型的进步也带来了被用于恶意目的的风险，如生成暴力或裸体图像，或在不适当的上下文中创建公众人物的未经授权肖像。为了缓解这些风险，提出了概念移除方法，旨在修改扩散模型以防止生成恶意和不需要的概念。然而，现有研究面临缺乏综合数据集的一致比较、有害和裸体概念提示无效以及忽视在包含恶意概念的提示中生成良性部分的能力评估等挑战。为解决这些问题，我们提议通过引入新的数据集和新的评估指标来评估概念移除方法。在此基准测试中，我们对概念移除进行了全面评估，实验观察和讨论为该领域提供了宝贵的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2I扩散模型具有根据文本提示生成图像的能力，但也存在被用于生成恶意图像的风险。</li>
<li>概念移除方法旨在修改扩散模型，以防止生成恶意和不需要的概念。</li>
<li>现有研究在评估概念移除方法时面临缺乏综合数据集的一致比较的挑战。</li>
<li>有害和裸体概念的提示在现有研究中可能无效。</li>
<li>评估应涵盖在包含恶意概念的提示中生成良性部分的能力。</li>
<li>为了解决现有研究的不足，提出了引入新的数据集和评估指标的基准测试。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14855">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7639785c0d68610dd08062216c717327.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85c814b0aa26acfb876e6c9193787418.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7c8487a4332da90e10d404282267763.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beecbaeddbc0390fc247d595c485a195.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b51759f1e825ba40b8affc6fb25018fd.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-22/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-22/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-22/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0047ae843a9a5e7b30c2ebf47a9d818a.jpg" class="responsive-img" alt="牙齿修复">
                        
                        <span class="card-title">牙齿修复</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            牙齿修复 方向最新论文已更新，请持续关注 Update in 2025-08-22  Tooth-Diffusion Guided 3D CBCT Synthesis with Fine-Grained Tooth   Conditioning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    牙齿修复
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">牙齿修复</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-22/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0afc402e0563d5c0e25c3f0b8adc57cb.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-08-22  SATURN Autoregressive Image Generation Guided by Scene Graphs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27083.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
