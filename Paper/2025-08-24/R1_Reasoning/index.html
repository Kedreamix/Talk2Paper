<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-24  Learning to Drive Ethically Embedding Moral Reasoning into Autonomous   Driving">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-adfcc41ab2f552363f86e2ccc6a4e5b9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    58 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-24-æ›´æ–°"><a href="#2025-08-24-æ›´æ–°" class="headerlink" title="2025-08-24 æ›´æ–°"></a>2025-08-24 æ›´æ–°</h1><h2 id="Learning-to-Drive-Ethically-Embedding-Moral-Reasoning-into-Autonomous-Driving"><a href="#Learning-to-Drive-Ethically-Embedding-Moral-Reasoning-into-Autonomous-Driving" class="headerlink" title="Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous   Driving"></a>Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous   Driving</h2><p><strong>Authors:Dianzhao Li, Ostap Okhrin</strong></p>
<p>Autonomous vehicles hold great promise for reducing traffic fatalities and improving transportation efficiency, yet their widespread adoption hinges on embedding robust ethical reasoning into routine and emergency maneuvers. Here, we present a hierarchical Safe Reinforcement Learning (Safe RL) framework that explicitly integrates moral considerations with standard driving objectives. At the decision level, a Safe RL agent is trained using a composite ethical risk cost, combining collision probability and harm severity, to generate high-level motion targets. A dynamic Prioritized Experience Replay mechanism amplifies learning from rare but critical, high-risk events. At the execution level, polynomial path planning coupled with Proportional-Integral-Derivative (PID) and Stanley controllers translates these targets into smooth, feasible trajectories, ensuring both accuracy and comfort. We train and validate our approach on rich, real-world traffic datasets encompassing diverse vehicles, cyclists, and pedestrians, and demonstrate that it outperforms baseline methods in reducing ethical risk and maintaining driving performance. To our knowledge, this is the first study of ethical decision-making for autonomous vehicles via Safe RL in real-world scenarios. Our results highlight the potential of combining formal control theory and data-driven learning to advance ethically accountable autonomy in complex, human-mixed traffic environments. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶æ±½è½¦å¯¹äºå‡å°‘äº¤é€šäº‹æ•…æ­»äº¡äººæ•°ã€æé«˜äº¤é€šè¿è¾“æ•ˆç‡å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œç„¶è€Œå…¶å¹¿æ³›é‡‡çº³çš„å…³é”®åœ¨äºå°†ç¨³å¥çš„ä¼¦ç†æ¨ç†åµŒå…¥åˆ°å¸¸è§„å’Œç´§æ€¥æ“ä½œä¸­ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å±‚çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ ï¼ˆSafe RLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜¾å¼åœ°å°†é“å¾·è€ƒé‡ä¸æ ‡å‡†é©¾é©¶ç›®æ ‡ç›¸ç»“åˆã€‚åœ¨å†³ç­–å±‚é¢ï¼ŒSafe RL ä»£ç†é€šè¿‡ä½¿ç”¨ç»„åˆçš„é“å¾·é£é™©æˆæœ¬ï¼ˆç»“åˆç¢°æ’æ¦‚ç‡å’Œä¼¤å®³ä¸¥é‡ç¨‹åº¦ï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥ç”Ÿæˆé«˜çº§è¿åŠ¨ç›®æ ‡ã€‚åŠ¨æ€ä¼˜å…ˆç»éªŒå›æ”¾æœºåˆ¶æ”¾å¤§äº†å¯¹ç½•è§ä½†å…³é”®çš„é«˜é£é™©äº‹ä»¶çš„å­¦ä¹ ã€‚åœ¨æ‰§è¡Œå±‚é¢ï¼Œé€šè¿‡å¤šé¡¹å¼è·¯å¾„è§„åˆ’ä¸æ¯”ä¾‹ç§¯åˆ†å¾®åˆ†ï¼ˆPIDï¼‰å’Œæ–¯å¦åˆ©æ§åˆ¶å™¨ç›¸ç»“åˆï¼Œå°†è¿™äº›ç›®æ ‡è½¬åŒ–ä¸ºå¹³ç¨³ä¸”å¯è¡Œçš„è½¨è¿¹ï¼Œç¡®ä¿å‡†ç¡®æ€§å’Œèˆ’é€‚æ€§ã€‚æˆ‘ä»¬åœ¨ä¸°å¯Œçš„çœŸå®ä¸–ç•Œäº¤é€šæ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒå’ŒéªŒè¯ï¼Œè¯¥æ•°æ®é›†æ¶µç›–äº†å„ç§è½¦è¾†ã€è‡ªè¡Œè½¦å’Œè¡Œäººï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡å°‘é“å¾·é£é™©å¹¶ä¿æŒé©¾é©¶æ€§èƒ½æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨ç°å®ä¸–ç•Œä¸­é€šè¿‡å®‰å…¨å¼ºåŒ–å­¦ä¹ å¯¹è‡ªåŠ¨é©¾é©¶æ±½è½¦è¿›è¡Œé“å¾·å†³ç­–çš„ç ”ç©¶ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†ç»“åˆå½¢å¼æ§åˆ¶ç†è®ºå’Œæ•°æ®é©±åŠ¨å­¦ä¹ çš„æ½œåŠ›ï¼Œæœ‰åŠ©äºåœ¨å¤æ‚çš„äººæœºæ··åˆäº¤é€šç¯å¢ƒä¸­å®ç°é“å¾·è´£ä»»è‡ªä¸»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14926v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦é€šè¿‡ç»“åˆæ ‡å‡†é©¾é©¶ç›®æ ‡ä¸é“å¾·è€ƒé‡ï¼Œæå‡ºäº†ä¸€ç§åˆ†å±‚çš„Safe Reinforcement Learningï¼ˆSafe RLï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨å†³ç­–å±‚é¢ä½¿ç”¨å¤åˆé“å¾·é£é™©æˆæœ¬è¿›è¡ŒåŸ¹è®­ï¼Œå¹¶ç»“åˆç¢°æ’æ¦‚ç‡å’Œä¼¤å®³ä¸¥é‡ç¨‹åº¦æ¥ç”Ÿæˆé«˜çº§è¿åŠ¨ç›®æ ‡ã€‚åœ¨æ‰§è¡Œå±‚é¢ï¼Œé€šè¿‡å¤šé¡¹å¼è·¯å¾„è§„åˆ’ä¸PIDå’ŒStanleyæ§åˆ¶å™¨ï¼Œå°†ç›®æ ‡è½¬åŒ–ä¸ºå¹³ç¨³ã€å¯è¡Œçš„è½¨è¿¹ï¼Œç¡®ä¿å‡†ç¡®æ€§å’Œèˆ’é€‚æ€§ã€‚åœ¨çœŸå®ä¸–ç•Œçš„ä¸°å¯Œäº¤é€šæ•°æ®é›†ä¸Šè®­ç»ƒå’ŒéªŒè¯è¯¥æ¡†æ¶ï¼Œè¡¨æ˜å…¶åœ¨å‡å°‘é“å¾·é£é™©å’Œç»´æŠ¤é©¾é©¶æ€§èƒ½ä¸Šä¼˜äºåŸºå‡†æ–¹æ³•ã€‚è¿™æ˜¯é¦–ä¸ªåœ¨çœŸå®åœºæ™¯ä¸­è¿ç”¨Safe RLè¿›è¡Œè‡ªåŠ¨é©¾é©¶é“å¾·å†³ç­–çš„ç ”ç©¶ã€‚ç»“æœå±•ç¤ºäº†ç»“åˆå½¢å¼æ§åˆ¶ç†è®ºä¸æ•°æ®é©±åŠ¨å­¦ä¹ çš„æ½œåŠ›ï¼Œæœ‰åŠ©äºæ¨è¿›å¤æ‚æ··åˆäº¤é€šç¯å¢ƒä¸­çš„é“å¾·è‡ªä¸»é©¾é©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶è½¦è¾†éœ€è¦åµŒå…¥ç¨³å¥çš„é“å¾·æ¨ç†ä»¥åº”å¯¹æ—¥å¸¸å’Œç´§æ€¥æƒ…å†µä¸‹çš„å†³ç­–ã€‚</li>
<li>æå‡ºäº†åˆ†å±‚çš„Safe Reinforcement Learningï¼ˆSafe RLï¼‰æ¡†æ¶ï¼Œå°†é“å¾·è€ƒé‡ä¸é©¾é©¶ç›®æ ‡ç›¸ç»“åˆã€‚</li>
<li>åœ¨å†³ç­–å±‚é¢ä½¿ç”¨å¤åˆé“å¾·é£é™©æˆæœ¬è¿›è¡ŒåŸ¹è®­ï¼Œè€ƒè™‘ç¢°æ’æ¦‚ç‡å’Œä¼¤å®³ä¸¥é‡ç¨‹åº¦ã€‚</li>
<li>é‡‡ç”¨åŠ¨æ€ä¼˜å…ˆçº§ç»éªŒå›æ”¾æœºåˆ¶ï¼Œå¼ºåŒ–ä»é«˜é£é™©äº‹ä»¶ä¸­å­¦ä¹ çš„æ•ˆæœã€‚</li>
<li>åœ¨æ‰§è¡Œå±‚é¢ï¼Œé€šè¿‡å¤šé¡¹å¼è·¯å¾„è§„åˆ’ç»“åˆPIDå’ŒStanleyæ§åˆ¶å™¨ï¼Œå®ç°å¹³ç¨³ã€å¯è¡Œçš„è½¨è¿¹è§„åˆ’ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œçš„äº¤é€šæ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒå’ŒéªŒè¯ï¼Œè¡¨æ˜è¯¥æ¡†æ¶åœ¨å‡å°‘é“å¾·é£é™©å’Œä¿æŒé©¾é©¶æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40d246e4d8e42fa992eaf53cc792a10d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29e6f31b869f19b2ae5e042ba9d725be.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FinAgentBench-A-Benchmark-Dataset-for-Agentic-Retrieval-in-Financial-Question-Answering"><a href="#FinAgentBench-A-Benchmark-Dataset-for-Agentic-Retrieval-in-Financial-Question-Answering" class="headerlink" title="FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial   Question Answering"></a>FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial   Question Answering</h2><p><strong>Authors:Chanyeol Choi, Jihoon Kwon, Alejandro Lopez-Lira, Chaewoon Kim, Minjae Kim, Juneha Hwang, Jaeseon Ha, Hojun Choi, Suyeol Yun, Yongjin Kim, Yongjae Lee</strong></p>
<p>Accurate information retrieval (IR) is critical in the financial domain, where investors must identify relevant information from large collections of documents. Traditional IR methods-whether sparse or dense-often fall short in retrieval accuracy, as it requires not only capturing semantic similarity but also performing fine-grained reasoning over document structure and domain-specific knowledge. Recent advances in large language models (LLMs) have opened up new opportunities for retrieval with multi-step reasoning, where the model ranks passages through iterative reasoning about which information is most relevant to a given query. However, there exists no benchmark to evaluate such capabilities in the financial domain. To address this gap, we introduce FinAgentBench, the first large-scale benchmark for evaluating retrieval with multi-step reasoning in finance â€“ a setting we term agentic retrieval. The benchmark consists of 3,429 expert-annotated examples on S&amp;P-100 listed firms and assesses whether LLM agents can (1) identify the most relevant document type among candidates, and (2) pinpoint the key passage within the selected document. Our evaluation framework explicitly separates these two reasoning steps to address context limitations. This design enables to provide a quantitative basis for understanding retrieval-centric LLM behavior in finance. We evaluate a suite of state-of-the-art models and further demonstrated how targeted fine-tuning can significantly improve agentic retrieval performance. Our benchmark provides a foundation for studying retrieval-centric LLM behavior in complex, domain-specific tasks for finance. We will release the dataset publicly upon acceptance of the paper and plan to expand and share dataset for the full S&amp;P 500 and beyond. </p>
<blockquote>
<p>å‡†ç¡®çš„æƒ…æŠ¥æ£€ç´¢ï¼ˆIRï¼‰åœ¨é‡‘èé¢†åŸŸè‡³å…³é‡è¦ï¼ŒæŠ•èµ„è€…å¿…é¡»ä»å¤§æ–‡æ¡£é›†åˆä¸­è¯†åˆ«å‡ºç›¸å…³ä¿¡æ¯ã€‚ä¼ ç»Ÿçš„IRæ–¹æ³•ï¼Œæ— è®ºæ˜¯ç¨€ç–çš„è¿˜æ˜¯å¯†é›†çš„ï¼Œå¾€å¾€åœ¨æ£€ç´¢å‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¸è¶³ï¼Œå› ä¸ºå®ƒä¸ä»…éœ€è¦æ•æ‰è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œè¿˜éœ€è¦å¯¹æ–‡æ¡£ç»“æ„å’Œç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†è¿›è¡Œç²¾ç»†çš„æ¨ç†ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¸ºå…·æœ‰å¤šæ­¥æ¨ç†çš„æ£€ç´¢æä¾›äº†æ–°çš„æœºä¼šï¼Œè¯¥æ¨¡å‹é€šè¿‡å…³äºå“ªäº›ä¿¡æ¯å¯¹ç»™å®šæŸ¥è¯¢æœ€ç›¸å…³çš„è¿­ä»£æ¨ç†æ¥æ’åæ®µè½ã€‚ç„¶è€Œï¼Œé‡‘èé¢†åŸŸè¿˜æ²¡æœ‰è¯„ä¼°è¿™ç§èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†FinAgentBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°é‡‘èé¢†åŸŸä¸­å…·æœ‰å¤šæ­¥æ¨ç†çš„æ£€ç´¢çš„å¤§å‹åŸºå‡†æµ‹è¯•â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºä»£ç†æ£€ç´¢ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«3429ä¸ªå…³äºæ ‡æ™®100æŒ‡æ•°ä¸Šå¸‚å…¬å¸çš„ä¸“å®¶æ³¨é‡Šç¤ºä¾‹ï¼Œå¹¶è¯„ä¼°LLMä»£ç†æ˜¯å¦èƒ½ï¼ˆ1ï¼‰åœ¨å€™é€‰è€…ä¸­è¯†åˆ«å‡ºæœ€ç›¸å…³çš„æ–‡æ¡£ç±»å‹ï¼Œä»¥åŠï¼ˆ2ï¼‰åœ¨æ‰€é€‰æ–‡æ¡£ä¸­å®šä½å…³é”®æ®µè½ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶æ˜ç¡®åœ°å°†è¿™ä¸¤ä¸ªæ¨ç†æ­¥éª¤åˆ†å¼€ï¼Œä»¥è§£å†³ä¸Šä¸‹æ–‡é™åˆ¶çš„é—®é¢˜ã€‚è¿™ç§è®¾è®¡æœ‰åŠ©äºä¸ºç†è§£é‡‘èé¢†åŸŸä¸­ä»¥æ£€ç´¢ä¸ºä¸­å¿ƒçš„LLMè¡Œä¸ºæä¾›å®šé‡ä¾æ®ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¹¶è¿›ä¸€æ­¥å±•ç¤ºäº†æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒå¦‚ä½•æ˜¾ç€æé«˜ä»£ç†æ£€ç´¢æ€§èƒ½ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸ºç ”ç©¶å¤æ‚ã€ç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ä¸­é‡‘èé¢†åŸŸçš„LLMè¡Œä¸ºæä¾›äº†åŸºç¡€ã€‚è®ºæ–‡æ¥å—åï¼Œæˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒæ•°æ®é›†ï¼Œå¹¶è®¡åˆ’æ‰©å±•å’Œå…±äº«æ•´ä¸ªæ ‡æ™®500åŠä»¥å¤–çš„æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14052v2">PDF</a> 6 pages</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é‡‘èé¢†åŸŸå‡†ç¡®ä¿¡æ¯æ£€ç´¢çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†ä¼ ç»Ÿä¿¡æ¯æ£€ç´¢æ–¹æ³•çš„ä¸è¶³ã€‚æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯ä¸ºå…·æœ‰å¤šæ­¥éª¤æ¨ç†çš„æ£€ç´¢æä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼Œé‡‘èé¢†åŸŸç¼ºä¹ç›¸åº”çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°è¿™ç§èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæ–‡ä¸­ä»‹ç»äº†ä¸€ä¸ªæ–°åŸºå‡†æµ‹è¯•FinAgentBenchï¼Œå®ƒæ˜¯ç”¨äºè¯„ä¼°é‡‘èé¢†åŸŸä¸­å…·æœ‰å¤šæ­¥éª¤æ¨ç†çš„æ£€ç´¢èƒ½åŠ›çš„é¦–ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«3429ä¸ªé’ˆå¯¹æ ‡æ™®100æŒ‡æ•°ä¸Šå¸‚å…¬å¸çš„ä¸“å®¶æ³¨é‡Šç¤ºä¾‹ï¼Œå¹¶è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†æ˜¯å¦èƒ½è¯†åˆ«æœ€ç›¸å…³çš„æ–‡æ¡£ç±»å‹å¹¶åœ¨æ‰€é€‰æ–‡æ¡£ä¸­å®šä½å…³é”®æ®µè½ã€‚è¯„ä»·æ¡†æ¶æ˜ç¡®åœ°å°†è¿™ä¸¤ä¸ªæ¨ç†æ­¥éª¤åˆ†å¼€ï¼Œä»¥è§£å†³ä¸Šä¸‹æ–‡é™åˆ¶é—®é¢˜ã€‚è¿™å°†ä¸ºç†è§£é’ˆå¯¹é‡‘èå¤æ‚ä»»åŠ¡çš„æ£€ç´¢ä¸ºä¸­å¿ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¡Œä¸ºæä¾›å®šé‡ä¾æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‘èé¢†åŸŸçš„ä¿¡æ¯æ£€ç´¢è‡³å…³é‡è¦ï¼Œå¯¹æŠ•èµ„è€…çš„å†³ç­–äº§ç”Ÿç›´æ¥å½±å“ã€‚</li>
<li>ä¼ ç»Ÿä¿¡æ¯æ£€ç´¢æ–¹æ³•å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•å‡†ç¡®æ•æ‰è¯­ä¹‰ç›¸ä¼¼æ€§å’Œè¿›è¡Œç²¾ç»†çš„æ–‡æ¡£ç»“æ„å’Œé¢†åŸŸçŸ¥è¯†æ¨ç†ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯çš„å‘å±•ä¸ºå…·æœ‰å¤šæ­¥éª¤æ¨ç†çš„æ£€ç´¢æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</li>
<li>ç¼ºä¹é‡‘èé¢†åŸŸè¯„ä¼°å¤šæ­¥éª¤æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>FinAgentBenchæ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°é‡‘èé¢†åŸŸä¸­ä»£ç†å¼ä¿¡æ¯æ£€ç´¢èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å¤§é‡ä¸“å®¶æ ‡æ³¨çš„ç¤ºä¾‹ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½è¯†åˆ«æœ€ç›¸å…³çš„æ–‡æ¡£ç±»å‹å¹¶åœ¨æ‰€é€‰æ–‡æ¡£ä¸­å®šä½å…³é”®æ®µè½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fd69921bb210b3b00028254d96dded0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7edd011ecb55ab8f5f1dc0a0d3bc0c4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdc646c6400897963657abaae90790d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b17b2b1e8956436fd5c1ebad0b07f26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e63ef8bc6c0f1a43c6d8c26f11621086.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="IAD-R1-Reinforcing-Consistent-Reasoning-in-Industrial-Anomaly-Detection"><a href="#IAD-R1-Reinforcing-Consistent-Reasoning-in-Industrial-Anomaly-Detection" class="headerlink" title="IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection"></a>IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection</h2><p><strong>Authors:Yanhui Li, Yunkang Cao, Chengliang Liu, Yuan Xiong, Xinghui Dong, Chao Huang</strong></p>
<p>Industrial anomaly detection is a critical component of modern manufacturing, yet the scarcity of defective samples restricts traditional detection methods to scenario-specific applications. Although Vision-Language Models (VLMs) demonstrate significant advantages in generalization capabilities, their performance in industrial anomaly detection remains limited. To address this challenge, we propose IAD-R1, a universal post-training framework applicable to VLMs of different architectures and parameter scales, which substantially enhances their anomaly detection capabilities. IAD-R1 employs a two-stage training strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT) stage utilizes a meticulously constructed high-quality Chain-of-Thought dataset (Expert-AD) for training, enhancing anomaly perception capabilities and establishing reasoning-to-answer correlations; the Structured Control Group Relative Policy Optimization (SC-GRPO) stage employs carefully designed reward functions to achieve a capability leap from â€œAnomaly Perceptionâ€ to â€œAnomaly Interpretationâ€. Experimental results demonstrate that IAD-R1 achieves significant improvements across 7 VLMs, the largest improvement was on the DAGM dataset, with average accuracy 43.3% higher than the 0.5B baseline. Notably, the 0.5B parameter model trained with IAD-R1 surpasses commercial models including GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the effectiveness and superiority of IAD-R1. The dataset, code, and all model weights will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/Yanhui-Lee/IAD-R1">https://github.com/Yanhui-Lee/IAD-R1</a>. </p>
<blockquote>
<p>å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ˜¯ç°ä»£åˆ¶é€ ä¸šçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä½†ç”±äºç¼ºé™·æ ·æœ¬çš„ç¨€ç¼ºæ€§ï¼Œä¼ ç»Ÿçš„æ£€æµ‹æ–¹æ³•åªèƒ½åº”ç”¨äºç‰¹å®šåœºæ™¯ã€‚å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é€šç”¨åŒ–èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†åœ¨å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ–¹é¢çš„æ€§èƒ½ä»ç„¶æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†IAD-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªé€‚ç”¨äºä¸åŒæ¶æ„å’Œå‚æ•°è§„æ¨¡çš„VLMsçš„é€šç”¨åè®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤§å¹…æå‡å…¶å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚IAD-R1é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šæ„ŸçŸ¥æ¿€æ´»ç›‘ç£å¾®è°ƒï¼ˆPA-SFTï¼‰é˜¶æ®µåˆ©ç”¨ç²¾å¿ƒæ„å»ºçš„é«˜è´¨é‡æ€ç»´é“¾æ•°æ®é›†ï¼ˆExpert-ADï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¢å¼ºå¼‚å¸¸æ„ŸçŸ¥èƒ½åŠ›å¹¶å»ºç«‹æ¨ç†åˆ°ç­”æ¡ˆçš„å…³è”ï¼›ç»“æ„åŒ–å¯¹ç…§ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆSC-GRPOï¼‰é˜¶æ®µé‡‡ç”¨ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œå®ç°ä»â€œå¼‚å¸¸æ„ŸçŸ¥â€åˆ°â€œå¼‚å¸¸è§£è¯»â€çš„èƒ½åŠ›é£è·ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIAD-R1åœ¨7ç§VLMsä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå…¶ä¸­åœ¨DAGMæ•°æ®é›†ä¸Šçš„æ”¹è¿›æœ€å¤§ï¼Œå¹³å‡ç²¾åº¦æ¯”0.5BåŸºçº¿é«˜å‡º43.3%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨IAD-R1è®­ç»ƒçš„0. for public access at <a target="_blank" rel="noopener" href="https://github.com/Yanhui-Lee/IAD-R1.%EF%BC%88%E5%85%AC%E5%BC%80%E5%9C%B0%E5%9D%80%EF%BC%9A">https://github.com/Yanhui-Lee/IAD-R1.ï¼ˆå…¬å¼€åœ°å€ï¼š</a><a target="_blank" rel="noopener" href="https://github.com/Yanhui-Lee/IAD-R1">https://github.com/Yanhui-Lee/IAD-R1</a>ï¼‰ã€‚æ‰€æœ‰æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹æƒé‡å‡å¯åœ¨æ­¤å¤„å…¬å¼€è®¿é—®ã€‚ï¼‰å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨IAD-R1è®­ç»ƒçš„0.5Bå‚æ•°æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è¶…è¶Šäº†åŒ…æ‹¬GPT-4.1å’ŒClaude-Sonnet-4åœ¨å†…çš„å•†ä¸šæ¨¡å‹ï¼Œè¿™è¯æ˜äº†IAD-R1çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09178v2">PDF</a> </p>
<p><strong>Summary</strong><br>     å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ˜¯ç°ä»£åˆ¶é€ ä¸šçš„å…³é”®éƒ¨åˆ†ï¼Œä½†ç”±äºç¼ºé™·æ ·æœ¬çš„ç¨€ç¼ºæ€§ï¼Œä¼ ç»Ÿæ£€æµ‹æ–¹æ³•çš„é€šç”¨æ€§å—é™ã€‚ä¸ºåº”å¯¹æŒ‘æˆ˜ï¼Œæå‡ºIAD-R1ï¼Œä¸€ä¸ªé€‚ç”¨äºä¸åŒæ¶æ„å’Œå‚æ•°è§„æ¨¡çš„è§†è§‰è¯­è¨€æ¨¡å‹çš„é€šç”¨åè®­ç»ƒæ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚IAD-R1é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šåˆ©ç”¨é«˜è´¨é‡Chain-of-Thoughtæ•°æ®é›†è¿›è¡Œæ„ŸçŸ¥æ¿€æ´»ç›‘ç£å¾®è°ƒï¼Œå»ºç«‹æ¨ç†åˆ°ç­”æ¡ˆçš„å…³è”ï¼›é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°å®ç°æ§åˆ¶ç»„çš„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œå®ç°ä»â€œå¼‚å¸¸æ„ŸçŸ¥â€åˆ°â€œå¼‚å¸¸è§£è¯»â€çš„èƒ½åŠ›è·ƒå‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIAD-R1åœ¨7ç§è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šå–å¾—æ˜¾è‘—æ”¹è¿›ï¼Œå…¶ä¸­åœ¨DAGMæ•°æ®é›†ä¸Šçš„å¹³å‡å‡†ç¡®ç‡è¾ƒåŸºçº¿æ¨¡å‹æé«˜43.3%ï¼Œä¸”0.5Bå‚æ•°æ¨¡å‹ç»IAD-R1è®­ç»ƒåï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è¶…è¶ŠGPT-4.1å’ŒClaude-Sonnet-4ç­‰å•†ä¸šæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ˜¯ç°ä»£åˆ¶é€ ä¸šçš„é‡è¦éƒ¨åˆ†ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•å—é™äºç¼ºé™·æ ·æœ¬çš„ç¨€ç¼ºæ€§ã€‚</li>
<li>VLMsåœ¨å¼‚å¸¸æ£€æµ‹ä¸­è™½å…·ä¼˜åŠ¿ï¼Œä½†æ€§èƒ½ä»æœ‰å±€é™ã€‚</li>
<li>æå‡ºIAD-R1æ¡†æ¶ï¼Œé€‚ç”¨äºä¸åŒVLMsï¼Œæ˜¾è‘—å¢å¼ºå¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>IAD-R1é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šæ„ŸçŸ¥æ¿€æ´»ç›‘ç£å¾®è°ƒä¸ç»“æ„åŒ–æ§åˆ¶ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ã€‚</li>
<li>åˆ©ç”¨é«˜è´¨é‡Chain-of-Thoughtæ•°æ®é›†å»ºç«‹æ¨ç†åˆ°ç­”æ¡ˆçš„å…³è”ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºIAD-R1åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨DAGMæ•°æ®é›†ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f51c9e879429ba467bce21f2e6aee19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75f73122c677d2326ccc28877ef813de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c35d0278f2fb1a12a6eaafca4cfa8b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2948fdfa87cc1044cdefc6fd9d3e5352.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-548af570c8e40dfcedd9fab2a4a0ff16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53ea0dddfb9f1ef722f2fdc83891a3d8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AURA-A-Fine-Grained-Benchmark-and-Decomposed-Metric-for-Audio-Visual-Reasoning"><a href="#AURA-A-Fine-Grained-Benchmark-and-Decomposed-Metric-for-Audio-Visual-Reasoning" class="headerlink" title="AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual   Reasoning"></a>AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual   Reasoning</h2><p><strong>Authors:Siminfar Samakoush Galougah, Rishie Raj, Sanjoy Chowdhury, Sayan Nag, Ramani Duraiswami</strong></p>
<p>Current audio-visual (AV) benchmarks focus on final answer accuracy, overlooking the underlying reasoning process. This makes it difficult to distinguish genuine comprehension from correct answers derived through flawed reasoning or hallucinations. To address this, we introduce AURA (Audio-visual Understanding and Reasoning Assessment), a benchmark for evaluating the cross-modal reasoning capabilities of Audio-Visual Large Language Models (AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions across six challenging cognitive domains, such as causality, timbre and pitch, tempo and AV synchronization, unanswerability, implicit distractions, and skill profiling, explicitly designed to be unanswerable from a single modality. This forces models to construct a valid logical path grounded in both audio and video, setting AURA apart from AV datasets that allow uni-modal shortcuts. To assess reasoning traces, we propose a novel metric, AuraScore, which addresses the lack of robust tools for evaluating reasoning fidelity. It decomposes reasoning into two aspects: (i) Factual Consistency - whether reasoning is grounded in perceptual evidence, and (ii) Core Inference - the logical validity of each reasoning step. Evaluations of SOTA models on AURA reveal a critical reasoning gap: although models achieve high accuracy (up to 92% on some tasks), their Factual Consistency and Core Inference scores fall below 45%. This discrepancy highlights that models often arrive at correct answers through flawed logic, underscoring the need for our benchmark and paving the way for more robust multimodal evaluation. </p>
<blockquote>
<p>å½“å‰è§†å¬ï¼ˆAVï¼‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œå¿½è§†äº†èƒŒåçš„æ¨ç†è¿‡ç¨‹ã€‚è¿™ä½¿å¾—å¾ˆéš¾åŒºåˆ†çœŸæ­£çš„ç†è§£ä¸é€šè¿‡é”™è¯¯æ¨ç†æˆ–å¹»è§‰å¾—å‡ºçš„æ­£ç¡®ç­”æ¡ˆã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AURAï¼ˆè§†å¬ç†è§£ä¸æ¨ç†è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è§†å¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆAV-LLMï¼‰å’Œå¤šåª’ä½“è¯­è¨€æ¨¡å‹ï¼ˆOLMï¼‰çš„è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚AURAåŒ…å«å…­ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è®¤çŸ¥é¢†åŸŸçš„é—®é¢˜ï¼Œå¦‚å› æœå…³ç³»ã€éŸ³è‰²å’ŒéŸ³è°ƒã€èŠ‚å¥å’ŒAVåŒæ­¥ã€æ— æ³•å›ç­”çš„é—®é¢˜ã€éšå¼å¹²æ‰°å’ŒæŠ€èƒ½åˆ†æï¼Œè¿™äº›é—®é¢˜éƒ½æ˜¯æ˜ç¡®è®¾è®¡ä¸ºæ— æ³•ä»å•ä¸€æ¨¡æ€ä¸­å¾—å‡ºç­”æ¡ˆçš„ã€‚è¿™è¿«ä½¿æ¨¡å‹åœ¨éŸ³é¢‘å’Œè§†é¢‘çš„åŸºç¡€ä¸Šæ„å»ºæœ‰æ•ˆçš„é€»è¾‘è·¯å¾„ï¼Œä½¿AURAä¸å…è®¸å•ä¸€æ¨¡æ€æ·å¾„çš„AVæ•°æ®é›†åŒºåˆ†å¼€æ¥ã€‚ä¸ºäº†è¯„ä¼°æ¨ç†ç—•è¿¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æŒ‡æ ‡â€”â€”AuraScoreï¼Œå®ƒè§£å†³äº†ç¼ºä¹è¯„ä¼°æ¨ç†å¯ä¿¡åº¦çš„ç¨³å¥å·¥å…·çš„é—®é¢˜ã€‚å®ƒå°†æ¨ç†åˆ†ä¸ºä¸¤ä¸ªæ–¹é¢ï¼šï¼ˆiï¼‰äº‹å®ä¸€è‡´æ€§â€”â€”æ¨ç†æ˜¯å¦å»ºç«‹åœ¨æ„ŸçŸ¥è¯æ®ä¹‹ä¸Šï¼›ï¼ˆiiï¼‰æ ¸å¿ƒæ¨æ–­â€”â€”æ¯ä¸ªæ¨ç†æ­¥éª¤çš„é€»è¾‘æœ‰æ•ˆæ€§ã€‚å¯¹å…ˆè¿›æ¨¡å‹åœ¨AURAä¸Šçš„è¯„ä¼°æ˜¾ç¤ºäº†ä¸€ä¸ªå…³é”®çš„æ¨ç†å·®è·ï¼šè™½ç„¶æ¨¡å‹çš„å‡†ç¡®ç‡å¾ˆé«˜ï¼ˆæŸäº›ä»»åŠ¡é«˜è¾¾92%ï¼‰ï¼Œä½†å®ƒä»¬åœ¨äº‹å®ä¸€è‡´æ€§å’Œæ ¸å¿ƒæ¨æ–­æ–¹é¢çš„å¾—åˆ†å´ä½äº45%ã€‚è¿™ç§å·®å¼‚è¡¨æ˜ï¼Œæ¨¡å‹é€šå¸¸æ˜¯é€šè¿‡é”™è¯¯çš„é€»è¾‘å¾—å‡ºæ­£ç¡®ç­”æ¡ˆçš„ï¼Œè¿™çªæ˜¾äº†æˆ‘ä»¬åŸºå‡†æµ‹è¯•çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæ›´ç¨³å¥çš„å¤šæ¨¡æ€è¯„ä¼°é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07470v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°æœ‰çš„éŸ³è§†é¢‘åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œè€Œå¿½è§†äº†èƒŒåçš„æ¨ç†è¿‡ç¨‹çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†AURAåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°éŸ³é¢‘è§†è§‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆAV-LLMsï¼‰å’Œå…¨æ–¹ä½è¯­è¨€æ¨¡å‹ï¼ˆOLMsï¼‰çš„è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚AURAåŒ…æ‹¬å…­ä¸ªæŒ‘æˆ˜æ€§è®¤çŸ¥é¢†åŸŸçš„é¢˜ç›®ï¼ŒåŒ…æ‹¬å› æœã€éŸ³è‰²å’ŒéŸ³è°ƒã€èŠ‚å¥å’ŒéŸ³è§†é¢‘åŒæ­¥ã€æ— æ³•å›ç­”çš„é—®é¢˜ã€éšæ€§å¹²æ‰°ä»¥åŠæŠ€èƒ½åˆ†æï¼Œè¿™äº›é—®é¢˜è®¾è®¡å¾—æ— æ³•ä»å•ä¸€æ¨¡æ€ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œä»è€Œè¿«ä½¿æ¨¡å‹åŸºäºéŸ³é¢‘å’Œè§†é¢‘æ„å»ºæœ‰æ•ˆçš„é€»è¾‘è·¯å¾„ã€‚ä¸ºè¯„ä¼°æ¨ç†è¿‡ç¨‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†â€”â€”AuraScoreï¼Œä»¥è§£å†³ç¼ºä¹å¯é çš„æ¨ç†è¯„ä»·å·¥å…·çš„é—®é¢˜ã€‚è¯¥åˆ†æ•°å…³æ³¨ä¸¤ä¸ªæ ¸å¿ƒæ–¹é¢ï¼šäº‹å®ä¸€è‡´æ€§â€”â€”æ¨ç†æ˜¯å¦åŸºäºæ„ŸçŸ¥è¯æ®ï¼›æ ¸å¿ƒæ¨ç†â€”â€”æ¯ä¸ªæ¨ç†æ­¥éª¤çš„é€»è¾‘æœ‰æ•ˆæ€§ã€‚å¯¹å…ˆè¿›æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå®ƒä»¬åœ¨å…³é”®æ¨ç†æ–¹é¢å­˜åœ¨å·®è·ï¼šè™½ç„¶æŸäº›ä»»åŠ¡çš„å‡†ç¡®ç‡é«˜è¾¾92%ï¼Œä½†äº‹å®ä¸€è‡´æ€§å’Œæ ¸å¿ƒæ¨ç†åˆ†æ•°ä½äº45%ï¼Œè¿™è¡¨æ˜æ¨¡å‹é€šå¸¸é€šè¿‡é€»è¾‘é”™è¯¯å¾—å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œçªæ˜¾äº†éœ€è¦é‡‡ç”¨æ–°çš„åŸºå‡†æµ‹è¯•è¿›è¡Œæ›´ç¨³å¥çš„å¤šæ¨¡æ€è¯„ä¼°çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰éŸ³é¢‘è§†è§‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œå¿½ç•¥äº†æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>AURAåŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°éŸ³é¢‘è§†è§‰å¤§è¯­è¨€æ¨¡å‹å’Œå…¨æ–¹ä½è¯­è¨€æ¨¡å‹çš„è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>AURAåŒ…å«å…­ä¸ªæŒ‘æˆ˜æ€§çš„è®¤çŸ¥é¢†åŸŸé—®é¢˜ï¼Œè¿™äº›é—®é¢˜æ— æ³•ä»å•ä¸€æ¨¡æ€è§£ç­”ï¼Œè¦æ±‚æ¨¡å‹åœ¨éŸ³é¢‘å’Œè§†é¢‘ä¹‹é—´å»ºç«‹é€»è¾‘è”ç³»ã€‚</li>
<li>ä¸ºè¯„ä¼°æ¨ç†è¿‡ç¨‹ï¼Œæå‡ºäº†AuraScoreè¿™ä¸€æ–°çš„åº¦é‡æ ‡å‡†ã€‚</li>
<li>AuraScoreå…³æ³¨ä¸¤ä¸ªæ ¸å¿ƒæ–¹é¢ï¼šäº‹å®ä¸€è‡´æ€§å’Œæ ¸å¿ƒæ¨ç†ã€‚</li>
<li>å¯¹å…ˆè¿›æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå°½ç®¡å®ƒä»¬åœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å…³é”®æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07470">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-306999ce1cdda841bd5cc5fd11bafe5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c808f17f9ed8e35a352978d68f9ce4b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05c2d89b3302afc263790b648e87d7e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b05f090cc304565efe6994040424ae97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c0b364b64e971dc99d4d40b0febc421.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adfcc41ab2f552363f86e2ccc6a4e5b9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References"><a href="#MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References" class="headerlink" title="MultiRef: Controllable Image Generation with Multiple Visual References"></a>MultiRef: Controllable Image Generation with Multiple Visual References</h2><p><strong>Authors:Ruoxi Chen, Dongping Chen, Siyuan Wu, Sinan Wang, Shiyun Lang, Petr Sushko, Gaoyang Jiang, Yao Wan, Ranjay Krishna</strong></p>
<p>Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs â€“ either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: <a target="_blank" rel="noopener" href="https://multiref.github.io/">https://multiref.github.io/</a>. </p>
<blockquote>
<p>è§†è§‰è®¾è®¡å¸ˆè‡ªç„¶ä¼šä»å¤šç§è§†è§‰å‚è€ƒä¸­æ±²å–çµæ„Ÿï¼Œç»“åˆä¸åŒçš„å…ƒç´ å’Œç¾å­¦åŸåˆ™è¿›è¡Œè‰ºæœ¯åˆ›ä½œã€‚ç„¶è€Œï¼Œç›®å‰çš„å›¾åƒç”Ÿæˆæ¡†æ¶ä¸»è¦ä¾èµ–äºå•ä¸€æ¥æºçš„è¾“å…¥ï¼Œæ— è®ºæ˜¯æ–‡æœ¬æç¤ºè¿˜æ˜¯å•ä¸ªå‚è€ƒå›¾åƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨ä½¿ç”¨å¤šä¸ªè§†è§‰å‚è€ƒçš„å¯æ§å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬ä»‹ç»äº†MultiRef-benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«990ä¸ªåˆæˆæ ·æœ¬å’Œ1000ä¸ªçœŸå®ä¸–ç•Œæ ·æœ¬ï¼Œéœ€è¦èåˆå¤šä¸ªå‚è€ƒå›¾åƒä¸­çš„è§†è§‰å†…å®¹ã€‚åˆæˆæ ·æœ¬æ˜¯é€šè¿‡æˆ‘ä»¬çš„æ•°æ®å¼•æ“RefBlendåˆæˆçš„ï¼ŒåŒ…å«10ç§å‚è€ƒç±»å‹å’Œ33ç§å‚è€ƒç»„åˆã€‚åŸºäºRefBlendï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ„å»ºäº†ä¸€ä¸ªåŒ…å«38ké«˜è´¨é‡å›¾åƒçš„æ•°æ®é›†MultiRefï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªäº¤ç»‡çš„å›¾åƒæ–‡æœ¬æ¨¡å‹ï¼ˆå³OmniGenã€ACEå’ŒShow-oï¼‰å’Œå…­ä¸ªä»£ç†æ¡†æ¶ï¼ˆä¾‹å¦‚ChatDiTå’ŒLLM+SDï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„ç³»ç»Ÿä¹Ÿé¢ä¸´å¤šå‚è€ƒæ¡ä»¶è®¾ç½®çš„æŒ‘æˆ˜ï¼Œæœ€å¥½çš„æ¨¡å‹OmniGenåœ¨åˆæˆæ ·æœ¬ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ä»…ä¸º66.6%ï¼Œåœ¨çœŸå®æ¡ˆä¾‹ä¸Šä¸º79.0%ï¼Œä¸æ ‡å‡†ç­”æ¡ˆç›¸æ¯”ä»æœ‰ä¸€å®šå·®è·ã€‚è¿™äº›å‘ç°ä¸ºå¼€å‘æ›´çµæ´»ã€æ›´äººæ€§åŒ–çš„åˆ›æ„å·¥å…·æä¾›äº†å®è´µæ–¹å‘ï¼Œè¿™äº›å·¥å…·å¯ä»¥æœ‰æ•ˆåœ°æ•´åˆå¤šç§è§†è§‰çµæ„Ÿæ¥æºã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://multiref.github.io/%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://multiref.github.io/å…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06905v2">PDF</a> Accepted to ACM MM 2025 Datasets</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰è®¾è®¡å¸ˆä»å¤šä¸ªè§†è§‰å‚è€ƒä¸­æ±²å–çµæ„Ÿè¿›è¡Œåˆ›ä½œçš„æ–¹æ³•ï¼Œè€Œç°æœ‰çš„å›¾åƒç”Ÿæˆæ¡†æ¶ä¸»è¦ä¾èµ–äºå•ä¸€æºè¾“å…¥ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡ä¸“æ³¨äºä½¿ç”¨å¤šä¸ªè§†è§‰å‚è€ƒè¿›è¡Œå¯æ§å›¾åƒç”Ÿæˆçš„ä»»åŠ¡ã€‚æ–‡ç« æå‡ºäº†MultiRef-benchè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«åˆæˆæ ·æœ¬å’ŒçœŸå®ä¸–ç•Œæ ·æœ¬ï¼Œå¹¶åŸºäºRefBlendæ•°æ®å¼•æ“ç”Ÿæˆåˆæˆæ ·æœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„ç³»ç»Ÿä¹Ÿé¢ä¸´å¤šå‚è€ƒæ¡ä»¶æŒ‘æˆ˜ï¼Œæœ€ä½³æ¨¡å‹OmniGenåœ¨åˆæˆæ ·æœ¬å’ŒçœŸå®ä¸–ç•Œæ¡ˆä¾‹ä¸­çš„å¹³å‡è¡¨ç°ä»…ä¸º66.6%å’Œ79.0%ã€‚è¿™ä¸ºå¼€å‘èƒ½æœ‰æ•ˆæ•´åˆå¤šç§è§†è§‰çµæ„Ÿæ¥æºçš„æ›´çµæ´»ã€æ›´äººæ€§åŒ–çš„åˆ›æ„å·¥å…·æä¾›äº†æœ‰ä»·å€¼çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è®¾è®¡å¸ˆä»å¤šä¸ªè§†è§‰å‚è€ƒä¸­æ±²å–çµæ„Ÿè¿›è¡Œåˆ›ä½œï¼Œè€Œç°æœ‰å›¾åƒç”Ÿæˆæ¡†æ¶ä¸»è¦ä¾èµ–å•ä¸€æºè¾“å…¥ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†MultiRef-benchè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°ä½¿ç”¨å¤šä¸ªè§†è§‰å‚è€ƒçš„å¯æ§å›¾åƒç”Ÿæˆã€‚</li>
<li>MultiRef-benchåŒ…å«åˆæˆæ ·æœ¬å’ŒçœŸå®ä¸–ç•Œæ ·æœ¬ï¼Œåˆæˆæ ·æœ¬é€šè¿‡RefBlendæ•°æ®å¼•æ“ç”Ÿæˆã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„ç³»ç»Ÿåœ¨å¤šå‚è€ƒæ¡ä»¶ä¸‹çš„è¡¨ç°ä¹Ÿä¸ç†æƒ³ã€‚</li>
<li>æœ€ä½³æ¨¡å‹OmniGenåœ¨åˆæˆæ ·æœ¬å’ŒçœŸå®ä¸–ç•Œæ¡ˆä¾‹ä¸­çš„å¹³å‡è¡¨ç°åˆ†åˆ«ä¸º66.6%å’Œ79.0%ã€‚</li>
<li>è¿™ä¸ºå¼€å‘èƒ½æ•´åˆå¤šç§è§†è§‰çµæ„Ÿæ¥æºçš„åˆ›æ„å·¥å…·æä¾›äº†æœ‰ä»·å€¼çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06905">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9e34d3ea7f05ce2185407b54b7a90a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b31871c430d0fbf498dd9080bec5071a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8d6b3d99f0accdbf50cbd62532e78f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a75c67ad4d70f35f3ef90c9dae06009.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38dd2bd3ba67c809c984a897b91d8e80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-661a75e9d0f09f34f2f50a19a141e74b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20393970eadc25313042671698f91c2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a3804bfa59e558dcf54da5c1eb216f2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Affordance-R1-Reinforcement-Learning-for-Generalizable-Affordance-Reasoning-in-Multimodal-Large-Language-Model"><a href="#Affordance-R1-Reinforcement-Learning-for-Generalizable-Affordance-Reasoning-in-Multimodal-Large-Language-Model" class="headerlink" title="Affordance-R1: Reinforcement Learning for Generalizable Affordance   Reasoning in Multimodal Large Language Model"></a>Affordance-R1: Reinforcement Learning for Generalizable Affordance   Reasoning in Multimodal Large Language Model</h2><p><strong>Authors:Hanqing Wang, Shaoyang Wang, Yiming Zhong, Zemin Yang, Jiamin Wang, Zhiqing Cui, Jiahao Yuan, Yifan Han, Mingyu Liu, Yuexin Ma</strong></p>
<p>Affordance grounding focuses on predicting the specific regions of objects that are associated with the actions to be performed by robots. It plays a vital role in the fields of human-robot interaction, human-object interaction, embodied manipulation, and embodied perception. Existing models often neglect the affordance shared among different objects because they lack the Chain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD) generalization and explicit reasoning capabilities. To address these challenges, we propose Affordance-R1, the first unified affordance grounding framework that integrates cognitive CoT guided Group Relative Policy Optimization (GRPO) within a reinforcement learning paradigm. Specifically, we designed a sophisticated affordance function, which contains format, perception, and cognition rewards to effectively guide optimization directions. Furthermore, we constructed a high-quality affordance-centric reasoning dataset, ReasonAff, to support training. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Affordance-R1 achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Comprehensive experiments demonstrate that our model outperforms well-established methods and exhibits open-world generalization. To the best of our knowledge, Affordance-R1 is the first to integrate GRPO-based RL with reasoning into affordance reasoning. The code of our method and our dataset is released on <a target="_blank" rel="noopener" href="https://github.com/hq-King/Affordance-R1">https://github.com/hq-King/Affordance-R1</a>. </p>
<blockquote>
<p>äº²å’Œæ€§æ¥åœ°ï¼ˆAffordance groundingï¼‰ä¸»è¦å…³æ³¨é¢„æµ‹ä¸æœºå™¨äººè¦æ‰§è¡Œçš„åŠ¨ä½œç›¸å…³è”çš„ç‰¹å®šå¯¹è±¡åŒºåŸŸã€‚å®ƒåœ¨äººæœºäº¤äº’ã€äººä¸å¯¹è±¡äº¤äº’ã€å…·ä½“æ“æ§å’Œå…·ä½“æ„ŸçŸ¥ç­‰é¢†åŸŸä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç°æœ‰æ¨¡å‹å¾€å¾€å¿½è§†äº†ä¸åŒå¯¹è±¡ä¹‹é—´çš„å…±äº«äº²å’Œæ€§ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰æ¨ç†èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„è·¨åŸŸï¼ˆOODï¼‰æ³›åŒ–å’Œæ˜¾å¼æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†äº²å’Œæ€§R1ï¼ˆAffordance-R1ï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»Ÿä¸€çš„äº²å’Œæ€§æ¥åœ°æ¡†æ¶ï¼Œå®ƒæ•´åˆäº†è®¤çŸ¥æ€ç»´é“¾å¼•å¯¼ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨å¼ºåŒ–å­¦ä¹ èŒƒå¼ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤æ‚çš„äº²å’Œæ€§åŠŸèƒ½ï¼Œå…¶ä¸­åŒ…å«æ ¼å¼ã€æ„ŸçŸ¥å’Œè®¤çŸ¥å¥–åŠ±ï¼Œä»¥æœ‰æ•ˆåœ°æŒ‡å¯¼ä¼˜åŒ–æ–¹å‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„äº²åˆæ€§ä¸­å¿ƒæ¨ç†æ•°æ®é›†ReasonAffï¼Œä»¥æ”¯æŒè®­ç»ƒã€‚ä»…é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸GRPOè¿›è¡Œè®­ç»ƒï¼Œè€Œæ— éœ€æ˜ç¡®çš„æ¨ç†æ•°æ®ï¼Œäº²å’Œæ€§R1å®ç°äº†ç¨³å¥çš„é›¶æ ·æœ¬æ³›åŒ–ï¼Œå¹¶å±•ç°å‡ºæ–°å…´çš„æµ‹è¯•æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºç°æœ‰çš„æ–¹æ³•ï¼Œå¹¶è¡¨ç°å‡ºå¼€æ”¾ä¸–ç•Œçš„æ³›åŒ–èƒ½åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œäº²å’Œæ€§R1æ˜¯é¦–ä¸ªå°†åŸºäºGRPOçš„RLä¸æ¨ç†æ•´åˆåˆ°äº²å’Œæ€§æ¨ç†ä¸­çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•å’Œæ•°æ®é›†çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/hq-King/Affordance-R1">https://github.com/hq-King/Affordance-R1</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06206v3">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºåŠ¨ä½œé¢„æµ‹çš„å¯¹è±¡ç‰¹å®šåŒºåŸŸå®šä½åœ¨æœºå™¨äººäº¤äº’ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚ç°æœ‰æ¨¡å‹å› ç¼ºä¹æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰æ¨ç†èƒ½åŠ›è€Œéš¾ä»¥æŠŠæ¡ä¸åŒå¯¹è±¡é—´çš„å…±äº«æ€§å®šä½ï¼Œå½±å“äº†æ¨¡å‹çš„é¢†åŸŸå¤–æ³›åŒ–èƒ½åŠ›å’Œæ˜¾å¼æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºç»Ÿä¸€èåˆæ€ç»´é“¾å¼•å¯¼çš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„Affordance-R1æ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ èŒƒå¼è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ¡†æ¶è®¾è®¡äº†ä¸€ä¸ªå¤æ‚çš„å®šä½åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ ¼å¼ã€æ„ŸçŸ¥å’Œè®¤çŸ¥å¥–åŠ±æ¥æŒ‡å¯¼ä¼˜åŒ–æ–¹å‘ï¼Œæ„å»ºäº†é«˜è´¨é‡çš„ä¸­å¿ƒåŒ–æ¨ç†æ•°æ®é›†ReasonAffä»¥æ”¯æŒè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®ç°äº†ç¨³å¥çš„é›¶æ ·æœ¬æ³›åŒ–å¹¶å±•ç°å‡ºæ¨ç†èƒ½åŠ›ã€‚Affordance-R1æ˜¯é¦–ä¸ªå°†åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ ä¸æ¨ç†ç»“åˆçš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Affordance grounding ä¸“æ³¨äºé¢„æµ‹ä¸æœºå™¨äººåŠ¨ä½œç›¸å…³è”çš„å¯¹è±¡ç‰¹å®šåŒºåŸŸã€‚</li>
<li>ç°æœ‰æ¨¡å‹ç¼ºä¹ Chain-of-Thought (CoT) æ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥å¤„ç†ä¸åŒå¯¹è±¡é—´çš„å…±äº«æ€§å®šä½ã€‚</li>
<li>Affordance-R1 æ˜¯é¦–ä¸ªç»Ÿä¸€èåˆè®¤çŸ¥ CoT å¼•å¯¼çš„ Group Relative Policy Optimization (GRPO) çš„å®šä½æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ èŒƒå¼è®¾è®¡å¤æ‚çš„å®šä½åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ ¼å¼ã€æ„ŸçŸ¥å’Œè®¤çŸ¥å¥–åŠ±ã€‚</li>
<li>æ„å»ºé«˜è´¨é‡çš„ä¸­å¿ƒåŒ–æ¨ç†æ•°æ®é›†ReasonAffä»¥æ”¯æŒè®­ç»ƒã€‚</li>
<li>Affordance-R1 å®ç°ç¨³å¥çš„é›¶æ ·æœ¬æ³›åŒ–å¹¶å±•ç°å‡ºæ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06206">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c9a9b52e545921883d13c839a0160d7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-027324787b676671f5cbf4fce3a261e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a96dfaf91ef4fa1149402c3affecb3d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbc31bf427f552fee809d62d9b16732f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66bf41a20f2d36ea0aa61545822651e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9fe984d1975b656b9fef1c10d9973d0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Exploring-Superior-Function-Calls-via-Reinforcement-Learning"><a href="#Exploring-Superior-Function-Calls-via-Reinforcement-Learning" class="headerlink" title="Exploring Superior Function Calls via Reinforcement Learning"></a>Exploring Superior Function Calls via Reinforcement Learning</h2><p><strong>Authors:Bingguang Hao, Maolin Wang, Zengzhuang Xu, Yicheng Chen, Cunyin Peng, Jinjie GU, Chenyi Zhuang</strong></p>
<p>Function calling capabilities are crucial for deploying Large Language Models in real-world applications, yet current training approaches fail to develop robust reasoning strategies. Supervised fine-tuning produces models that rely on superficial pattern matching, while standard reinforcement learning methods struggle with the complex action space of structured function calls. We present a novel reinforcement learning framework designed to enhance group relative policy optimization through strategic entropy based exploration specifically tailored for function calling tasks. Our approach addresses three critical challenges in function calling: insufficient exploration during policy learning, lack of structured reasoning in chain-of-thought generation, and inadequate verification of parameter extraction. Our two-stage data preparation pipeline ensures high-quality training samples through iterative LLM evaluation and abstract syntax tree validation. Extensive experiments on the Berkeley Function Calling Leaderboard demonstrate that this framework achieves state-of-the-art performance among open-source models with 86.02% overall accuracy, outperforming standard GRPO by up to 6% on complex multi-function scenarios. Notably, our method shows particularly strong improvements on code-pretrained models, suggesting that structured language generation capabilities provide an advantageous starting point for reinforcement learning in function calling tasks. We will release all the code, models and dataset to benefit the community. </p>
<blockquote>
<p>å‡½æ•°è°ƒç”¨èƒ½åŠ›å¯¹äºåœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­éƒ¨ç½²ç°å®ä¸–ç•Œåº”ç”¨è‡³å…³é‡è¦ï¼Œç„¶è€Œç›®å‰çš„è®­ç»ƒæ–¹æ³•æ— æ³•å¼€å‘ç¨³å¥çš„æ¨ç†ç­–ç•¥ã€‚æœ‰ç›‘ç£çš„å¾®è°ƒä¼šäº§ç”Ÿä¾èµ–è‚¤æµ…æ¨¡å¼åŒ¹é…çš„æ¨¡å‹ï¼Œè€Œæ ‡å‡†å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚çš„ç»“æ„åŒ–å‡½æ•°è°ƒç”¨åŠ¨ä½œç©ºé—´ä¸­è‹¦è‹¦æŒ£æ‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŸºäºç­–ç•¥ç†µçš„æ¢ç´¢å¢å¼ºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œè¯¥æ¡†æ¶ç‰¹åˆ«é’ˆå¯¹å‡½æ•°è°ƒç”¨ä»»åŠ¡é‡èº«å®šåˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•è§£å†³äº†å‡½æ•°è°ƒç”¨ä¸­çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç­–ç•¥å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¢ç´¢ä¸è¶³ã€æ€ç»´é“¾ç”Ÿæˆä¸­ç»“æ„åŒ–æ¨ç†çš„ç¼ºä¹ä»¥åŠå‚æ•°æå–çš„éªŒè¯ä¸è¶³ã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µæ•°æ®å‡†å¤‡ç®¡é“é€šè¿‡è¿­ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°å’ŒæŠ½è±¡è¯­æ³•æ ‘éªŒè¯ï¼Œç¡®ä¿é«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬ã€‚åœ¨ä¼¯å…‹åˆ©å‡½æ•°è°ƒç”¨æ’è¡Œæ¦œä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œæ€»ä½“å‡†ç¡®ç‡ä¸º86.02%ï¼Œåœ¨å¤æ‚çš„å¤šå‡½æ•°åœºæ™¯ä¸­æœ€å¤šå¯æé«˜GRPOæ€§èƒ½6%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»£ç é¢„è®­ç»ƒæ¨¡å‹ä¸Šæ˜¾ç¤ºå‡ºç‰¹åˆ«å¼ºå¤§çš„æ”¹è¿›ï¼Œè¿™è¡¨æ˜ç»“æ„åŒ–è¯­è¨€ç”Ÿæˆèƒ½åŠ›ä¸ºå¼ºåŒ–å­¦ä¹ åœ¨å‡½æ•°è°ƒç”¨ä»»åŠ¡ä¸­æä¾›äº†ä¸€ä¸ªæœ‰åˆ©çš„èµ·ç‚¹ã€‚æˆ‘ä»¬å°†å‘å¸ƒæ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†ä»¥é€ ç¦ç¤¾åŒºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05118v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å‡½æ•°è°ƒç”¨èƒ½åŠ›è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰è®­ç»ƒæ–¹æ³•åœ¨åŸ¹å…»ç¨³å¥æ¨ç†ç­–ç•¥æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ç›‘ç£å¾®è°ƒäº§ç”Ÿçš„æ¨¡å‹ä¾èµ–äºè¡¨é¢æ¨¡å¼åŒ¹é…ï¼Œè€Œæ ‡å‡†å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚çš„å‡½æ•°è°ƒç”¨åŠ¨ä½œç©ºé—´ä¸­è¡¨ç°æŒ£æ‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŸºäºæˆ˜ç•¥ç†µçš„æ¢ç´¢å¢å¼ºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œç‰¹åˆ«é€‚ç”¨äºå‡½æ•°è°ƒç”¨ä»»åŠ¡ã€‚è¯¥æ¡†æ¶è§£å†³äº†å‡½æ•°è°ƒç”¨ä¸­çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæ”¿ç­–å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¢ç´¢ä¸è¶³ã€é“¾å¼æ€ç»´ç”Ÿæˆä¸­ç¼ºä¹ç»“æ„åŒ–æ¨ç†ä»¥åŠå‚æ•°æå–çš„éªŒè¯ä¸è¶³ã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µæ•°æ®å‡†å¤‡ç®¡é“é€šè¿‡è¿­ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°å’ŒæŠ½è±¡è¯­æ³•æ ‘éªŒè¯ï¼Œç¡®ä¿é«˜è´¨é‡çš„åŸ¹è®­æ ·æœ¬ã€‚åœ¨Berkeleyå‡½æ•°è°ƒç”¨æ’è¡Œæ¦œä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œæ€»ä½“å‡†ç¡®åº¦ä¸º86.02%ï¼Œåœ¨å¤æ‚çš„å¤šåŠŸèƒ½åœºæ™¯ä¸‹æ¯”æ ‡å‡†GRPOé«˜å‡º6%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»£ç é¢„è®­ç»ƒæ¨¡å‹ä¸Šè¡¨ç°å‡ºäº†ç‰¹åˆ«å¤§çš„æ”¹è¿›ï¼Œè¿™è¡¨æ˜ç»“æ„åŒ–è¯­è¨€ç”Ÿæˆèƒ½åŠ›ä¸ºå¼ºåŒ–å­¦ä¹ åœ¨å‡½æ•°è°ƒç”¨ä»»åŠ¡ä¸­æä¾›äº†ä¸€ä¸ªæœ‰åˆ©çš„èµ·ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­éœ€è¦å¼ºå¤§çš„å‡½æ•°è°ƒç”¨èƒ½åŠ›ã€‚</li>
<li>å½“å‰è®­ç»ƒæ–¹æ³•åœ¨åŸ¹å…»æ¨¡å‹æ¨ç†ç­–ç•¥æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç›‘ç£å¾®è°ƒæ¨¡å‹ä¾èµ–äºè¡¨é¢æ¨¡å¼åŒ¹é…ã€‚</li>
<li>é’ˆå¯¹å‡½æ•°è°ƒç”¨ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶è§£å†³äº†æ”¿ç­–å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¢ç´¢ä¸è¶³ã€ç¼ºä¹ç»“æ„åŒ–æ¨ç†å’Œå‚æ•°æå–éªŒè¯ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µæ•°æ®å‡†å¤‡ç®¡é“ç¡®ä¿é«˜è´¨é‡åŸ¹è®­æ ·æœ¬ã€‚</li>
<li>åœ¨Berkeleyå‡½æ•°è°ƒç”¨æ’è¡Œæ¦œä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œæ€»ä½“å‡†ç¡®åº¦ä¸º86.02%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4123c5f39102d26a83d95165c011a54f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1253f26143895775e96cfd8eae0001db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33a410925db6fd9f09c5a0eb4ef94157.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-809d9b45f96bf6d6b740e919b9119531.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="When-Good-Sounds-Go-Adversarial-Jailbreaking-Audio-Language-Models-with-Benign-Inputs"><a href="#When-Good-Sounds-Go-Adversarial-Jailbreaking-Audio-Language-Models-with-Benign-Inputs" class="headerlink" title="When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with   Benign Inputs"></a>When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with   Benign Inputs</h2><p><strong>Authors:Bodam Kim, Hiskias Dingeto, Taeyoun Kwon, Dasol Choi, DongGeon Lee, Haon Park, JaeHoon Lee, Jongho Shin</strong></p>
<p>As large language models become increasingly integrated into daily life, audio has emerged as a key interface for human-AI interaction. However, this convenience also introduces new vulnerabilities, making audio a potential attack surface for adversaries. Our research introduces WhisperInject, a two-stage adversarial audio attack framework that can manipulate state-of-the-art audio language models to generate harmful content. Our method uses imperceptible perturbations in audio inputs that remain benign to human listeners. The first stage uses a novel reward-based optimization method, Reinforcement Learning with Projected Gradient Descent (RL-PGD), to guide the target model to circumvent its own safety protocols and generate harmful native responses. This native harmful response then serves as the target for Stage 2, Payload Injection, where we use Projected Gradient Descent (PGD) to optimize subtle perturbations that are embedded into benign audio carriers, such as weather queries or greeting messages. Validated under the rigorous StrongREJECT, LlamaGuard, as well as Human Evaluation safety evaluation framework, our experiments demonstrate a success rate exceeding 86% across Qwen2.5-Omni-3B, Qwen2.5-Omni-7B, and Phi-4-Multimodal. Our work demonstrates a new class of practical, audio-native threats, moving beyond theoretical exploits to reveal a feasible and covert method for manipulating AI behavior. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­å¾—åˆ°è¶Šæ¥è¶Šæ·±å…¥çš„é›†æˆï¼ŒéŸ³é¢‘ä½œä¸ºäººç±»ä¸äººå·¥æ™ºèƒ½äº¤äº’çš„å…³é”®æ¥å£å·²å´­éœ²å¤´è§’ã€‚ç„¶è€Œï¼Œè¿™ç§ä¾¿åˆ©ä¹Ÿå¸¦æ¥äº†æ–°çš„æ¼æ´ï¼Œä½¿éŸ³é¢‘æˆä¸ºæ½œåœ¨çš„å¯¹æ•Œæ”»å‡»é¢ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼•å…¥äº†WhisperInjectï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„å¯¹æŠ—æ€§éŸ³é¢‘æ”»å‡»æ¡†æ¶ï¼Œå¯ä»¥æ“çºµæœ€å…ˆè¿›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ä»¥ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨éŸ³é¢‘è¾“å…¥ä¸­ä¸å¯å¯Ÿè§‰çš„æ‰°åŠ¨ï¼Œè¿™äº›æ‰°åŠ¨å¯¹äººç±»å¬ä¼—æ¥è¯´æ˜¯è‰¯æ€§çš„ã€‚ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨åŸºäºå¥–åŠ±çš„ä¼˜åŒ–æ–¹æ³•ï¼Œå³ä½¿ç”¨å¸¦æœ‰æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRL-PGDï¼‰ï¼Œä»¥å¼•å¯¼ç›®æ ‡æ¨¡å‹ç»•è¿‡å…¶è‡ªå·±çš„å®‰å…¨åè®®å¹¶ç”Ÿæˆæœ‰å®³çš„æœ¬åœ°å“åº”ã€‚ç„¶åè¿™ä¸ªæœ¬åœ°æœ‰å®³å“åº”æˆä¸ºç¬¬äºŒé˜¶æ®µå³è½½è·æ³¨å…¥çš„ç›®æ ‡ï¼Œæˆ‘ä»¬åœ¨æ­¤é˜¶æ®µä½¿ç”¨æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼ˆPGDï¼‰æ¥ä¼˜åŒ–åµŒå…¥è‰¯æ€§éŸ³é¢‘è½½ä½“ä¸­çš„å¾®å¦™æ‰°åŠ¨ï¼Œä¾‹å¦‚å¤©æ°”æŸ¥è¯¢æˆ–é—®å€™ä¿¡æ¯ã€‚åœ¨ä¸¥æ ¼çš„StrongREJECTã€LlamaGuardä»¥åŠäººç±»è¯„ä¼°å®‰å…¨è¯„ä¼°æ¡†æ¶ä¸‹è¿›è¡Œäº†éªŒè¯ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨Qwen2.5-Omni-3Bã€Qwen2.5-Omni-7Bå’ŒPhi-4-Multimodalä¸Šçš„æˆåŠŸç‡è¶…è¿‡86%ã€‚æˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†ä¸€ç§æ–°çš„å®ç”¨ã€åŸç”ŸéŸ³é¢‘å¨èƒç±»åˆ«ï¼Œè¶…è¶Šäº†ç†è®ºä¸Šçš„æ¼æ´åˆ©ç”¨ï¼Œæ­ç¤ºäº†ä¸€ç§å¯è¡Œä¸”éšç§˜çš„æ“çºµäººå·¥æ™ºèƒ½è¡Œä¸ºçš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03365v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯­éŸ³ä½œä¸ºäººæœºäº¤äº’çš„æ–°æ¥å£ï¼Œå¸¦æ¥äº†ä¾¿æ·çš„åŒæ—¶ä¹Ÿå­˜åœ¨å®‰å…¨éšæ‚£ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§åä¸ºWhisperInjectçš„ä¸¤é˜¶æ®µå¯¹æŠ—æ€§éŸ³é¢‘æ”»å‡»æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥æ“æ§å…ˆè¿›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨éŸ³é¢‘è¾“å…¥ä¸­çš„ä¸å¯å¯Ÿè§‰æ‰°åŠ¨ï¼Œå¯¹äººç±»å¬ä¼—æ— å®³ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨åŸºäºå¥–åŠ±ä¼˜åŒ–çš„æ–¹æ³•ï¼Œå³ä½¿ç”¨å¸¦æœ‰æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRL-PGDï¼‰ï¼Œå¼•å¯¼ç›®æ ‡æ¨¡å‹ç»•è¿‡å…¶å®‰å…¨åè®®ç”Ÿæˆæœ‰å®³çš„åŸç”Ÿå“åº”ã€‚ç¬¬äºŒé˜¶æ®µä¸ºè½½è·æ³¨å…¥ï¼Œä½¿ç”¨æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ä¼˜åŒ–åµŒå…¥åˆ°è‰¯æ€§éŸ³é¢‘è½½ä½“ä¸­çš„ç»†å¾®æ‰°åŠ¨ï¼Œå¦‚å¤©æ°”æŸ¥è¯¢æˆ–é—®å€™ä¿¡æ¯ã€‚å®éªŒåœ¨ä¸¥æ ¼çš„å®‰å…¨è¯„ä¼°æ¡†æ¶ä¸‹éªŒè¯äº†å…¶æˆåŠŸç‡è¶…è¿‡86%ï¼Œå¹¶æ­ç¤ºäº†éŸ³é¢‘åŸç”Ÿå¨èƒçš„æ–°ç±»åˆ«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘ä½œä¸ºäººæœºäº¤äº’çš„æ–°æ¥å£å­˜åœ¨æ½œåœ¨çš„å®‰å…¨éšæ‚£ã€‚</li>
<li>ç ”ç©¶äººå‘˜æå‡ºä¸€ç§åä¸ºWhisperInjectçš„ä¸¤é˜¶æ®µå¯¹æŠ—æ€§éŸ³é¢‘æ”»å‡»æ¡†æ¶ã€‚</li>
<li>WhisperInjectå¯ä»¥æ“æ§å…ˆè¿›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨äººç±»å¬è§‰æ— æ³•å¯Ÿè§‰çš„å¾®å°å˜åŒ–æ¥å®æ–½æ”»å‡»ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µä½¿ç”¨åŸºäºå¥–åŠ±ä¼˜åŒ–çš„æ–¹æ³•ç»•è¿‡æ¨¡å‹çš„å®‰å…¨åè®®ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µå°†è½½è·æ³¨å…¥åˆ°çœ‹ä¼¼æ­£å¸¸çš„éŸ³é¢‘ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03365">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa9a6b250f8b0a7dcf656876cb45254e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6754925625f3948944ff104072e8d1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d54ee43ecc5ea68a5515fdd5a0154dd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f26a83e77d178f85d3e239aea0ef8eaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb4c6502dc5e107f3d6c1a13c083b566.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents"><a href="#SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents" class="headerlink" title="SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents"></a>SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents</h2><p><strong>Authors:Jiaye Lin, Yifu Guo, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen, Hongzhang Liu, Ronghao Chen, Yangfan He, Daxin Jiang, Binxing Jiao, Chen Hu, Huacan Wang</strong></p>
<p>Large Language Model (LLM)-based agents have recently shown impressive capabilities in complex reasoning and tool use via multi-step interactions with their environments. While these agents have the potential to tackle complicated tasks, their problem-solving process, i.e., agentsâ€™ interaction trajectory leading to task completion, remains underexploited. These trajectories contain rich feedback that can navigate agents toward the right directions for solving problems correctly. Although prevailing approaches, such as Monte Carlo Tree Search (MCTS), can effectively balance exploration and exploitation, they ignore the interdependence among various trajectories and lack the diversity of search spaces, which leads to redundant reasoning and suboptimal outcomes. To address these challenges, we propose SE-Agent, a Self-Evolution framework that enables Agents to optimize their reasoning processes iteratively. Our approach revisits and enhances former pilot trajectories through three key operations: revision, recombination, and refinement. This evolutionary mechanism enables two critical advantages: (1) it expands the search space beyond local optima by intelligently exploring diverse solution paths guided by previous trajectories, and (2) it leverages cross-trajectory inspiration to efficiently enhance performance while mitigating the impact of suboptimal reasoning paths. Through these mechanisms, SE-Agent achieves continuous self-evolution that incrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench Verified to resolve real-world GitHub issues. Experimental results across five strong LLMs show that integrating SE-Agent delivers up to 55% relative improvement, achieving state-of-the-art performance among all open-source agents on SWE-bench Verified. Our code and demonstration materials are publicly available at <a target="_blank" rel="noopener" href="https://github.com/JARVIS-Xs/SE-Agent">https://github.com/JARVIS-Xs/SE-Agent</a>. </p>
<blockquote>
<p>åŸºäºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†æœ€è¿‘æ˜¾ç¤ºå‡ºé€šè¿‡ä¸ç¯å¢ƒçš„å¤šæ­¥äº¤äº’è¿›è¡Œå¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨çš„ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚è™½ç„¶è¿™äº›ä»£ç†æœ‰æ½œåŠ›å¤„ç†å¤æ‚ä»»åŠ¡ï¼Œä½†ä»–ä»¬çš„è§£å†³é—®é¢˜è¿‡ç¨‹ï¼Œå³ä»£ç†å®Œæˆä»»åŠ¡çš„äº¤äº’è½¨è¿¹ï¼Œä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è¿™äº›è½¨è¿¹åŒ…å«ä¸°å¯Œçš„åé¦ˆï¼Œå¯ä»¥å¼•å¯¼ä»£ç†æœç€æ­£ç¡®çš„æ–¹å‘è§£å†³é—®é¢˜ã€‚å°½ç®¡ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†ä¸åŒè½¨è¿¹ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ï¼Œå¹¶ä¸”ç¼ºä¹æœç´¢ç©ºé—´çš„å¤šæ ·æ€§ï¼Œè¿™å¯¼è‡´å†—ä½™æ¨ç†å’Œæ¬¡ä¼˜ç»“æœã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SE-Agentï¼Œä¸€ç§è‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œä½¿ä»£ç†èƒ½å¤Ÿè¿­ä»£ä¼˜åŒ–ä»–ä»¬çš„æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸‰ä¸ªå…³é”®æ“ä½œï¼šä¿®è®¢ã€é‡ç»„å’Œç»†åŒ–ï¼Œé‡æ–°è®¿é—®å¹¶å¢å¼ºå…ˆå‰çš„è½¨è¿¹ã€‚è¿™ç§è¿›åŒ–æœºåˆ¶å¸¦æ¥äº†ä¸¤ä¸ªå…³é”®ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰å®ƒé€šè¿‡æ™ºèƒ½åœ°æ¢ç´¢ç”±å…ˆå‰è½¨è¿¹å¼•å¯¼çš„å¤šæ ·åŒ–è§£å†³æ–¹æ¡ˆè·¯å¾„ï¼Œæ‰©å¤§äº†æœç´¢ç©ºé—´ï¼Œè¶…è¶Šäº†å±€éƒ¨æœ€ä¼˜ï¼›ï¼ˆ2ï¼‰å®ƒåˆ©ç”¨è·¨è½¨è¿¹çš„çµæ„Ÿæ¥æœ‰æ•ˆåœ°æé«˜æ€§èƒ½ï¼ŒåŒæ—¶å‡è½»æ¬¡ä¼˜æ¨ç†è·¯å¾„çš„å½±å“ã€‚é€šè¿‡è¿™äº›æœºåˆ¶ï¼ŒSE-Agentå®ç°äº†æŒç»­çš„è‡ªæˆ‘è¿›åŒ–ï¼Œé€æ­¥æé«˜äº†æ¨ç†è´¨é‡ã€‚æˆ‘ä»¬åœ¨SWE-bench Verifiedä¸Šè¯„ä¼°äº†SE-Agentï¼Œä»¥è§£å†³ç°å®ä¸–ç•Œä¸­çš„GitHubé—®é¢˜ã€‚åœ¨äº”ä¸ªå¼ºå¤§çš„LLMä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œé›†æˆSE-Agentå¸¦æ¥äº†é«˜è¾¾55%çš„ç›¸å¯¹æ”¹è¿›ï¼Œåœ¨SWE-bench Verifiedä¸Šçš„æ€§èƒ½è¾¾åˆ°äº†å¼€æºä»£ç†ä¸­çš„æœ€ä½³æ°´å¹³ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¼”ç¤ºææ–™å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JARVIS-Xs/SE-Agent">https://github.com/JARVIS-Xs/SE-Agent</a>å…¬å¼€è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02085v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ–¹é¢å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œé€šè¿‡ä¸ç¯å¢ƒçš„å¤šæ­¥éª¤äº¤äº’æ¥å®Œæˆä»»åŠ¡ã€‚ç„¶è€Œï¼ŒLLMåœ¨è§£å†³é—®é¢˜æ—¶çš„äº¤äº’è½¨è¿¹å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSE-Agentçš„è‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œé€šè¿‡ä¿®è®¢ã€é‡ç»„å’Œç»†åŒ–ä¹‹å‰çš„è½¨è¿¹ï¼Œä½¿ä»£ç†èƒ½å¤Ÿä¼˜åŒ–å…¶æ¨ç†è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶æ‰©å¤§äº†æœç´¢ç©ºé—´ï¼Œé€šè¿‡è·¨è½¨è¿¹çš„çµæ„Ÿæ¥æé«˜æ€§èƒ½å¹¶å‡å°‘æ¬¡ä¼˜æ¨ç†è·¯å¾„çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSE-Agentåœ¨è§£å†³ç°å®ä¸–ç•ŒGitHubé—®é¢˜ä¸Šå®ç°äº†é«˜è¾¾55%çš„ç›¸å¯¹æ”¹è¿›ï¼Œè¾¾åˆ°äº†SWE-bench Verifiedä¸Šçš„æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œé€šè¿‡ä¸ç¯å¢ƒçš„å¤šæ­¥äº¤äº’å®Œæˆä»»åŠ¡ã€‚</li>
<li>LLMçš„é—®é¢˜è§£å†³è½¨è¿¹åŒ…å«ä¸°å¯Œçš„åé¦ˆï¼Œå¯ä»¥æŒ‡å¯¼å…¶æ­£ç¡®è§£å†³é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ï¼ˆå¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼‰è™½ç„¶èƒ½æœ‰æ•ˆå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œä½†å¿½ç•¥äº†è½¨è¿¹é—´çš„ç›¸äº’ä¾èµ–æ€§å¹¶ç¼ºä¹æœç´¢ç©ºé—´çš„å¤šæ ·æ€§ï¼Œå¯¼è‡´å†—ä½™æ¨ç†å’Œæ¬¡ä¼˜ç»“æœã€‚</li>
<li>SE-Agenté€šè¿‡ä¿®è®¢ã€é‡ç»„å’Œç»†åŒ–ä¹‹å‰çš„è½¨è¿¹ï¼Œå®ç°äº†ä»£ç†æ¨ç†è¿‡ç¨‹çš„ä¼˜åŒ–è¿­ä»£ã€‚</li>
<li>SE-Agentæ‰©å¤§äº†æœç´¢ç©ºé—´ï¼Œé€šè¿‡è·¨è½¨è¿¹çš„çµæ„Ÿæé«˜æ€§èƒ½å¹¶å‡å°‘æ¬¡ä¼˜æ¨ç†è·¯å¾„çš„å½±å“ã€‚</li>
<li>SE-Agentåœ¨è§£å†³ç°å®ä¸–ç•ŒGitHubé—®é¢˜ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œç›¸å¯¹äºå…¶ä»–å¼€æºä»£ç†è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9bd726db11fbc16937e1c35c985be8d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc136467beb09e19fc1f86d24cbca8bd.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Bench2ADVLM-A-Closed-Loop-Benchmark-for-Vision-language-Models-in-Autonomous-Driving"><a href="#Bench2ADVLM-A-Closed-Loop-Benchmark-for-Vision-language-Models-in-Autonomous-Driving" class="headerlink" title="Bench2ADVLM: A Closed-Loop Benchmark for Vision-language Models in   Autonomous Driving"></a>Bench2ADVLM: A Closed-Loop Benchmark for Vision-language Models in   Autonomous Driving</h2><p><strong>Authors:Tianyuan Zhang, Ting Jin, Lu Wang, Jiangfan Liu, Siyuan Liang, Mingchuan Zhang, Aishan Liu, Xianglong Liu</strong></p>
<p>Vision-Language Models (VLMs) have recently emerged as a promising paradigm in autonomous driving (AD). However, current performance evaluation protocols for VLM-based AD systems (ADVLMs) are predominantly confined to open-loop settings with static inputs, neglecting the more realistic and informative closed-loop setting that captures interactive behavior, feedback resilience, and real-world safety. To address this, we introduce Bench2ADVLM, a unified hierarchical closed-loop evaluation framework for real-time, interactive assessment of ADVLMs across both simulation and physical platforms. Inspired by dual-process theories of cognition, we first adapt diverse ADVLMs to simulation environments via a dual-system adaptation architecture. In this design, heterogeneous high-level driving commands generated by target ADVLMs (fast system) are interpreted by a general-purpose VLM (slow system) into standardized mid-level control actions suitable for execution in simulation. To bridge the gap between simulation and reality, we design a physical control abstraction layer that translates these mid-level actions into low-level actuation signals, enabling, for the first time, closed-loop testing of ADVLMs on physical vehicles. To enable more comprehensive evaluation, Bench2ADVLM introduces a self-reflective scenario generation module that automatically explores model behavior and uncovers potential failure modes for safety-critical scenario generation. Overall, Bench2ADVLM establishes a hierarchical evaluation pipeline that seamlessly integrates high-level abstract reasoning, mid-level simulation actions, and low-level real-world execution. Experiments on diverse scenarios across multiple state-of-the-art ADVLMs and physical platforms validate the diagnostic strength of our framework, revealing that existing ADVLMs still exhibit limited performance under closed-loop conditions. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æœ€è¿‘ä½œä¸ºè‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰é¢†åŸŸçš„ä¸€ç§æœ‰å‰é€”çš„èŒƒå¼è€Œå‡ºç°ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºäºVLMçš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼ˆADVLMï¼‰æ€§èƒ½è¯„ä¼°åè®®ä¸»è¦å±€é™äºå¼€æ”¾å¾ªç¯è®¾ç½®å’Œé™æ€è¾“å…¥ï¼Œå¿½ç•¥äº†æ›´çœŸå®ä¸”ä¿¡æ¯ä¸°å¯Œçš„é—­ç¯è®¾ç½®ï¼Œè¯¥è®¾ç½®å¯ä»¥æ•è·äº¤äº’è¡Œä¸ºã€åé¦ˆæ¢å¤åŠ›å’Œç°å®ä¸–ç•Œå®‰å…¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Bench2ADVLMï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åˆ†å±‚é—­ç¯è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºåœ¨æ¨¡æ‹Ÿå’Œç‰©ç†å¹³å°ä¸Šå¯¹ADVLMè¿›è¡Œå®æ—¶ã€äº¤äº’å¼çš„è¯„ä¼°ã€‚å—åˆ°è®¤çŸ¥åŒè¿‡ç¨‹ç†è®ºçš„å¯å‘ï¼Œæˆ‘ä»¬é¦–å…ˆå°†å„ç§ADVLMé€‚åº”åˆ°ä»¿çœŸç¯å¢ƒï¼Œé€šè¿‡åŒç³»ç»Ÿé€‚åº”æ¶æ„ã€‚åœ¨æ­¤è®¾è®¡ä¸­ï¼Œç”±ç›®æ ‡ADVLMï¼ˆå¿«é€Ÿç³»ç»Ÿï¼‰ç”Ÿæˆçš„ä¸åŒé«˜çº§é©¾é©¶å‘½ä»¤ç”±é€šç”¨VLMï¼ˆæ…¢é€Ÿç³»ç»Ÿï¼‰è§£é‡Šä¸ºæ ‡å‡†åŒ–çš„ä¸­çº§æ§åˆ¶åŠ¨ä½œï¼Œé€‚ç”¨äºä»¿çœŸä¸­çš„æ‰§è¡Œã€‚ä¸ºäº†å¼¥åˆä»¿çœŸä¸ç°å®ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç‰©ç†æ§åˆ¶æŠ½è±¡å±‚ï¼Œå°†è¿™äº›ä¸­çº§åŠ¨ä½œè½¬åŒ–ä¸ºä½çº§æ‰§è¡Œä¿¡å·ï¼Œé¦–æ¬¡å®ç°å¯¹ç‰©ç†è½¦è¾†ä¸ŠADVLMçš„é—­ç¯æµ‹è¯•ã€‚ä¸ºäº†è¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ï¼ŒBench2ADVLMå¼•å…¥äº†ä¸€ä¸ªè‡ªæˆ‘åæ€çš„åœºæ™¯ç”Ÿæˆæ¨¡å—ï¼Œè¯¥æ¨¡å—è‡ªåŠ¨æ¢ç´¢æ¨¡å‹è¡Œä¸ºå¹¶æ­ç¤ºæ½œåœ¨æ•…éšœæ¨¡å¼ï¼Œç”¨äºç”Ÿæˆå®‰å…¨å…³é”®åœºæ™¯ã€‚æ€»çš„æ¥è¯´ï¼ŒBench2ADVLMå»ºç«‹äº†ä¸€ä¸ªåˆ†å±‚çš„è¯„ä¼°æµç¨‹ï¼Œæ— ç¼é›†æˆäº†é«˜çº§æŠ½è±¡æ¨ç†ã€ä¸­çº§ä»¿çœŸåŠ¨ä½œå’Œä½çº§ç°å®ä¸–ç•Œæ‰§è¡Œã€‚åœ¨å¤šä¸ªå…ˆè¿›çš„ADVLMå’Œç‰©ç†å¹³å°ä¸Šçš„å„ç§åœºæ™¯çš„å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶çš„è¯Šæ–­èƒ½åŠ›ï¼Œè¡¨æ˜ç°æœ‰çš„ADVLMåœ¨é—­ç¯æ¡ä»¶ä¸‹ä»è¡¨ç°å‡ºæœ‰é™æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02028v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰é¢†åŸŸæ–°å…´çš„ä¸€ç§æœ‰å‰æ™¯çš„èŒƒå¼â€”â€”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚ç„¶è€Œï¼Œå½“å‰é’ˆå¯¹åŸºäºVLMçš„ADç³»ç»Ÿï¼ˆADVLMsï¼‰çš„æ€§èƒ½è¯„ä¼°åè®®ä¸»è¦å±€é™äºå¼€æ”¾å¾ªç¯è®¾ç½®ï¼Œç¼ºä¹æ›´çœŸå®å’Œå…¨é¢çš„é—­ç¯è¯„ä¼°æ¡†æ¶ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Bench2ADVLMï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åˆ†å±‚é—­ç¯è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå®æ—¶ã€äº¤äº’åœ°è¯„ä¼°ADVLMsåœ¨æ¨¡æ‹Ÿå’Œç‰©ç†å¹³å°ä¸Šçš„è¡¨ç°ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†åŒç³»ç»Ÿé€‚åº”æ¶æ„å’Œç‰©ç†æ§åˆ¶æŠ½è±¡å±‚ï¼Œå®ç°äº†æ¨¡æ‹Ÿç¯å¢ƒä¸çœŸå®ä¸–ç•Œçš„æ— ç¼å¯¹æ¥ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†è‡ªæˆ‘åæ€çš„åœºæ™¯ç”Ÿæˆæ¨¡å—ï¼Œä»¥è‡ªåŠ¨æ¢ç´¢æ¨¡å‹è¡Œä¸ºå¹¶æ­ç¤ºæ½œåœ¨å¤±è´¥æ¨¡å¼ï¼Œä»è€Œè¿›è¡Œå…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ¡†æ¶çš„è¯Šæ–­èƒ½åŠ›ï¼Œå¹¶æ­ç¤ºäº†ç°æœ‰ADVLMsåœ¨é—­ç¯æ¡ä»¶ä¸‹çš„æ€§èƒ½å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰é’ˆå¯¹åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼ˆADVLMsï¼‰çš„æ€§èƒ½è¯„ä¼°ä¸»è¦å±€é™äºå¼€æ”¾å¾ªç¯è®¾ç½®ã€‚</li>
<li>Bench2ADVLMæ¡†æ¶æ—¨åœ¨å®ç°ADVLMsåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®å¹³å°ä¸Šçš„å®æ—¶ã€äº¤äº’è¯„ä¼°ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨åŒç³»ç»Ÿé€‚åº”æ¶æ„ï¼Œå°†é«˜çº§é©¾é©¶å‘½ä»¤è½¬åŒ–ä¸ºæ ‡å‡†åŒ–ä¸­çº§æ§åˆ¶åŠ¨ä½œã€‚</li>
<li>é€šè¿‡ç‰©ç†æ§åˆ¶æŠ½è±¡å±‚ï¼Œå®ç°äº†æ¨¡æ‹Ÿç¯å¢ƒä¸çœŸå®ä¸–ç•Œçš„æ— ç¼å¯¹æ¥ã€‚</li>
<li>å¼•å…¥äº†è‡ªæˆ‘åæ€çš„åœºæ™¯ç”Ÿæˆæ¨¡å—ï¼Œä»¥è‡ªåŠ¨æ¢ç´¢æ¨¡å‹è¡Œä¸ºå¹¶æ­ç¤ºæ½œåœ¨å¤±è´¥æ¨¡å¼ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰ADVLMsåœ¨é—­ç¯æ¡ä»¶ä¸‹çš„æ€§èƒ½å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Bench2ADVLMæ¡†æ¶ä¸ºå…¨é¢è¯„ä¼°ADVLMsæ€§èƒ½æä¾›äº†æœ‰åŠ›çš„å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02028">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-985504df6449d45c511d17a1b2e436c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea3dfef142fa2728c1fcd29cc68ebe9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c6b0c58c4d2a681f7c37de7d80daa2b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-353360e5eabc2b141b0b88de8049686a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b307be4db48537291add794a3b66ef58.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Seed-X-Building-Strong-Multilingual-Translation-LLM-with-7B-Parameters"><a href="#Seed-X-Building-Strong-Multilingual-Translation-LLM-with-7B-Parameters" class="headerlink" title="Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters"></a>Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters</h2><p><strong>Authors:Shanbo Cheng, Yu Bao, Qian Cao, Luyang Huang, Liyan Kang, Zhicheng Liu, Yu Lu, Wenhao Zhu, Jingwen Chen, Zhichao Huang, Tao Li, Yifu Li, Huiying Lin, Sitong Liu, Ningxin Peng, Shuaijie She, Lu Xu, Nuo Xu, Sen Yang, Runsheng Yu, Yiming Yu, Liehao Zou, Hang Li, Lu Lu, Yuxuan Wang, Yonghui Wu</strong></p>
<p>Multilingual translation stands as a challenging task for large language models (LLMs) to handle intricate language patterns and stilted translations that arise in automated translations. In this paper, we introduce Seed-X, a family of open-source LLMs comprising instruct and reasoning models, pushing the limits of translation capability with 7B parameter size. The base model is pre-trained on a diverse, high-quality dataset encompassing both monolingual and bilingual content across 28 languages, harnessing the full potential of multilingual data. The instruct model is then finetuned to translate by Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement learning (RL) to achieve better generalization across diverse language pairs. Seed-X achieves performance comparable to leading closed-source models, including Gemini-2.5 and GPT-4o, across 28 languages, and significantly outperforms larger open-source models in both automatic metrics and human evaluations. We share the best practices through our optimization process, and make the parameter public available for advancing translation research and applications. </p>
<blockquote>
<p>å¤šè¯­è¨€ç¿»è¯‘å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦å¤„ç†å¤æ‚çš„è¯­è¨€æ¨¡å¼å’Œè‡ªåŠ¨åŒ–ç¿»è¯‘ä¸­å‡ºç°çš„ç”Ÿç¡¬ç¿»è¯‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Seed-Xï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«æŒ‡ä»¤å’Œæ¨ç†æ¨¡å‹çš„å¤§å‹å¼€æºLLMå®¶æ—ã€‚å®ƒä»¥7Bçš„å‚æ•°è§„æ¨¡æ¨åŠ¨äº†ç¿»è¯‘èƒ½åŠ›çš„æ–°æé™ã€‚åŸºç¡€æ¨¡å‹åœ¨åŒ…å«28ç§è¯­è¨€çš„å•è¯­å’ŒåŒè¯­å†…å®¹çš„å¤šæ ·åŒ–é«˜è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå……åˆ†åˆ©ç”¨äº†å¤šè¯­è¨€æ•°æ®çš„å…¨éƒ¨æ½œåŠ›ã€‚ç„¶åï¼ŒæŒ‡ä»¤æ¨¡å‹é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œç¿»è¯‘ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥å¢å¼ºï¼Œä»¥å®ç°è·¨ä¸åŒè¯­è¨€å¯¹çš„æ›´å¥½æ³›åŒ–ã€‚Seed-Xåœ¨28ç§è¯­è¨€ä¸Šçš„æ€§èƒ½ä¸é¢†å…ˆçš„é—­æºæ¨¡å‹ï¼ˆåŒ…æ‹¬Gemini-2.5å’ŒGPT-4oï¼‰ç›¸å½“ï¼Œå¹¶ä¸”åœ¨è‡ªåŠ¨æŒ‡æ ‡å’Œäººç±»è¯„ä¼°ä¸­éƒ½æ˜¾è‘—ä¼˜äºæ›´å¤§çš„å¼€æºæ¨¡å‹ã€‚æˆ‘ä»¬åˆ†äº«äº†ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„æœ€ä½³å®è·µï¼Œå¹¶å°†å‚æ•°å…¬å¼€ï¼Œä»¥ä¿ƒè¿›ç¿»è¯‘ç ”ç©¶ä¸åº”ç”¨çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13618v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè¯­è¨€ç¿»è¯‘æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå­˜åœ¨è¯­è¨€æ¨¡å¼å¤„ç†å¤æ‚å’Œæœºæ¢°åŒ–ç¿»è¯‘é—®é¢˜ã€‚æœ¬ç ”ç©¶å¼•å…¥Seed-Xï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«æŒ‡ä»¤å’Œæ¨ç†æ¨¡å‹çš„å¼€æºLLMå®¶æ—ï¼Œä»¥7Bå‚æ•°è§„æ¨¡æ¨åŠ¨ç¿»è¯‘èƒ½åŠ›æé™ã€‚åŸºç¡€æ¨¡å‹åœ¨æ¶µç›–28ç§è¯­è¨€çš„å•è¯­å’ŒåŒè¯­é«˜è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå……åˆ†åˆ©ç”¨å¤šè¯­è¨€æ•°æ®çš„æ½œåŠ›ã€‚é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†å¯¹æŒ‡ä»¤æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥å¢å¼ºï¼Œä»¥å®ç°è·¨ä¸åŒè¯­è¨€å¯¹çš„æ›´å¥½æ³›åŒ–ã€‚Seed-Xåœ¨28ç§è¯­è¨€ä¸Šçš„è¡¨ç°ä¸é¢†å…ˆçš„é—­æºæ¨¡å‹ç›¸å½“ï¼ŒåŒ…æ‹¬Gemini-2.5å’ŒGPT-4oï¼Œå¹¶ä¸”åœ¨è‡ªåŠ¨æŒ‡æ ‡å’Œäººç±»è¯„ä¼°ä¸­éƒ½æ˜¾è‘—ä¼˜äºå…¶ä»–å¤§å‹å¼€æºæ¨¡å‹ã€‚æˆ‘ä»¬åˆ†äº«äº†ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„æœ€ä½³å®è·µï¼Œå¹¶å…¬å¼€äº†å‚æ•°ï¼Œä»¥æ¨åŠ¨ç¿»è¯‘ç ”ç©¶ä¸åº”ç”¨çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè¯­è¨€ç¿»è¯‘ä¸Šä»æœ‰æŒ‘æˆ˜ï¼Œéœ€è¦å¤„ç†å¤æ‚çš„è¯­è¨€æ¨¡å¼å’Œé¿å…æœºæ¢°åŒ–ç¿»è¯‘ã€‚</li>
<li>Seed-Xæ˜¯ä¸€ä¸ªå¼€æºLLMå®¶æ—ï¼ŒåŒ…å«æŒ‡ä»¤å’Œæ¨ç†æ¨¡å‹ï¼Œä»¥7Bå‚æ•°æ¨åŠ¨ç¿»è¯‘èƒ½åŠ›æé™ã€‚</li>
<li>Seed-XåŸºç¡€æ¨¡å‹åœ¨å¤šç§è¯­è¨€çš„ä¸°å¯Œæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ŒåŒ…æ‹¬å•è¯­å’ŒåŒè¯­å†…å®¹ã€‚</li>
<li>é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†å¾®è°ƒæŒ‡ä»¤æ¨¡å‹ï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Seed-Xåœ¨å¤šç§è¯­è¨€ä¸Šçš„è¡¨ç°ä¸é¢†å…ˆçš„é—­æºæ¨¡å‹ç›¸å½“ï¼Œå¹¶åœ¨è‡ªåŠ¨è¯„ä¼°å’Œäººç±»è¯„ä¼°ä¸­ä¼˜äºå…¶ä»–å¼€æºæ¨¡å‹ã€‚</li>
<li>Seed-Xåˆ†äº«äº†ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„æœ€ä½³å®è·µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13618">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-46f214dffdb52afdfab105d1efa8541d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b181bd95f15522b2c31036781216122.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdfcbda71bc8ae8a28d1e2d35f409651.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-918c2974fa7f75c5bc3f4eeed2e35fa3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33ce00ff4302a357e6cda9165d963148.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LaViPlan-Language-Guided-Visual-Path-Planning-with-RLVR"><a href="#LaViPlan-Language-Guided-Visual-Path-Planning-with-RLVR" class="headerlink" title="LaViPlan : Language-Guided Visual Path Planning with RLVR"></a>LaViPlan : Language-Guided Visual Path Planning with RLVR</h2><p><strong>Authors:Hayeon Oh</strong></p>
<p>Out-of-distribution (OOD) scenarios in autonomous driving pose critical challenges, as planners often fail to generalize beyond their training experience, leading to unsafe or unexpected behavior. Vision-Language Models (VLMs) have shown promise in handling such scenarios by providing high-level scene understanding and user-aligned decisions. However, existing VLMs often exhibit a misalignment between their language-based reasoning and the low-level trajectories required for action-level planning. In this paper, we propose LaViPlan, a framework that leverages Reinforcement Learning with Verifiable Rewards (RLVR) to fine-tune VLMs using planning-oriented metrics. Experimental results show that LaViPlan improves planning performance across both in-domain and out-of-domain datasets. While linguistic fidelity slightly decreases after RLVR-based fine-tuning, qualitative evaluation indicates that the outputs remain coherent. We also conduct ablation studies to analyze the effects of sampling ratio and reasoning guidance, highlighting how these design choices influence performance. These findings demonstrate the potential of RLVR as a post-training paradigm for aligning language-guided reasoning with action-level planning in autonomous driving. </p>
<blockquote>
<p>è‡ªä¸»é©¾é©¶ä¸­çš„åˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯å¸¦æ¥é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºè§„åˆ’å™¨å¾€å¾€æ— æ³•æ¨å¹¿å…¶è®­ç»ƒç»éªŒï¼Œå¯¼è‡´ä¸å®‰å…¨æˆ–æ„å¤–è¡Œä¸ºã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤„ç†æ­¤ç±»åœºæ™¯æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œé€šè¿‡æä¾›é«˜çº§åœºæ™¯ç†è§£å’Œç”¨æˆ·å¯¹é½çš„å†³ç­–æ¥å®ç°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLMç»å¸¸åœ¨åŸºäºè¯­è¨€çš„æ¨ç†å’Œè¡ŒåŠ¨çº§è§„åˆ’æ‰€éœ€çš„ä½çº§è½¨è¿¹ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LaViPlanæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å¯¹VLMè¿›è¡Œä»¥è§„åˆ’ä¸ºå¯¼å‘çš„å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaViPlanåœ¨åŸŸå†…å’ŒåŸŸå¤–æ•°æ®é›†ä¸Šå‡æé«˜äº†è§„åˆ’æ€§èƒ½ã€‚è™½ç„¶åŸºäºRLVRçš„å¾®è°ƒåè¯­è¨€ä¿çœŸåº¦ç•¥æœ‰ä¸‹é™ï¼Œä½†å®šæ€§è¯„ä¼°è¡¨æ˜è¾“å‡ºä»ç„¶è¿è´¯ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼Œåˆ†æäº†é‡‡æ ·ç‡å’Œæ¨ç†æŒ‡å¯¼çš„å½±å“ï¼Œçªå‡ºäº†è¿™äº›è®¾è®¡é€‰æ‹©å¯¹æ€§èƒ½çš„å½±å“ã€‚è¿™äº›å‘ç°å±•ç¤ºäº†RLVRä½œä¸ºè‡ªä¸»é©¾é©¶ä¸­è¯­è¨€æŒ‡å¯¼æ¨ç†ä¸è¡ŒåŠ¨çº§è§„åˆ’å¯¹é½çš„åè®­ç»ƒæ¨¡å¼çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12911v4">PDF</a> Accepted to the 2nd ICCV 2025 Workshop on the Challenge of   Out-of-Label Hazards in Autonomous Driving (13 pages, 6 figures)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œç°æœ‰æ¨¡å‹åœ¨é¢ä¸´è¶…å‡ºè®­ç»ƒèŒƒå›´çš„ç¯å¢ƒæ—¶çš„ä¸è¶³ã€‚æå‡ºäº†ä¸€ç§ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å’Œå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œç²¾ç»†è°ƒæ•´çš„æ–°æ¡†æ¶LaViPlanã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaViPlanåœ¨æå‡è§„åˆ’æ€§èƒ½çš„åŒæ—¶ï¼Œä¹Ÿç¡®ä¿äº†è¯­è¨€çš„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶åœ¨é¢ä¸´è¶…å‡ºè®­ç»ƒèŒƒå›´ï¼ˆOODï¼‰çš„åœºæ™¯æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œç°æœ‰æ¨¡å‹æ— æ³•æ™®éåº”å¯¹æ­¤ç±»æƒ…å†µã€‚</li>
<li>Vision-Language Modelsï¼ˆVLMsï¼‰å±•ç°å‡ºå¤„ç†è¿™äº›åœºæ™¯çš„èƒ½åŠ›ï¼Œä½†å­˜åœ¨è¯­è¨€æ¨ç†ä¸è¡ŒåŠ¨çº§åˆ«è§„åˆ’ä¸åŒ¹é…çš„é—®é¢˜ã€‚</li>
<li>LaViPlanæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è¿›è¡Œç²¾ç»†åŒ–è®­ç»ƒï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLaViPlanèƒ½æå‡åœ¨é¢†åŸŸå†…å¤–æ•°æ®çš„è§„åˆ’æ€§èƒ½ã€‚</li>
<li>è™½ç„¶ç»è¿‡RLVRç²¾ç»†è®­ç»ƒåè¯­è¨€ä¿çœŸåº¦ç•¥æœ‰ä¸‹é™ï¼Œä½†å®šæ€§è¯„ä¼°æ˜¾ç¤ºè¾“å‡ºä»ç„¶è¿è´¯ã€‚</li>
<li>æ¶ˆèç ”ç©¶åˆ†æäº†é‡‡æ ·æ¯”ä¾‹å’Œæ¨ç†æŒ‡å¯¼çš„å½±å“ï¼Œæ­ç¤ºäº†è®¾è®¡é€‰æ‹©å¦‚ä½•å½±å“æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8638f8287a51dbd27cf841155fce7155.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-067f5f9c8813d180404fd11da0820fa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f45769cd734257fdcc2a6e581d9773a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74893f132b67c0d2afb1cf490b1162f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-694fa429f7723307e7c6e7aca376c924.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CoT-Segmenter-Enhancing-OOD-Detection-in-Dense-Road-Scenes-via-Chain-of-Thought-Reasoning"><a href="#CoT-Segmenter-Enhancing-OOD-Detection-in-Dense-Road-Scenes-via-Chain-of-Thought-Reasoning" class="headerlink" title="CoT-Segmenter: Enhancing OOD Detection in Dense Road Scenes via   Chain-of-Thought Reasoning"></a>CoT-Segmenter: Enhancing OOD Detection in Dense Road Scenes via   Chain-of-Thought Reasoning</h2><p><strong>Authors:Jeonghyo Song, Kimin Yun, DaeUng Jo, Jinyoung Kim, Youngjoon Yoo</strong></p>
<p>Effective Out-of-Distribution (OOD) detection is criti-cal for ensuring the reliability of semantic segmentation models, particularly in complex road environments where safety and accuracy are paramount. Despite recent advancements in large language models (LLMs), notably GPT-4, which significantly enhanced multimodal reasoning through Chain-of-Thought (CoT) prompting, the application of CoT-based visual reasoning for OOD semantic segmentation remains largely unexplored. In this paper, through extensive analyses of the road scene anomalies, we identify three challenging scenarios where current state-of-the-art OOD segmentation methods consistently struggle: (1) densely packed and overlapping objects, (2) distant scenes with small objects, and (3) large foreground-dominant objects. To address the presented challenges, we propose a novel CoT-based framework targeting OOD detection in road anomaly scenes. Our method leverages the extensive knowledge and reasoning capabilities of foundation models, such as GPT-4, to enhance OOD detection through improved image understanding and prompt-based reasoning aligned with observed problematic scene attributes. Extensive experiments show that our framework consistently outperforms state-of-the-art methods on both standard benchmarks and our newly defined challenging subset of the RoadAnomaly dataset, offering a robust and interpretable solution for OOD semantic segmentation in complex driving environments. </p>
<blockquote>
<p>æœ‰æ•ˆçš„ç¦»åˆ†å¸ƒï¼ˆOODï¼‰æ£€æµ‹å¯¹äºç¡®ä¿è¯­ä¹‰åˆ†å‰²æ¨¡å‹çš„å¯é æ€§è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚é“è·¯ç¯å¢ƒä¸­ï¼Œå®‰å…¨æ€§å’Œå‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚å°½ç®¡æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å–å¾—äº†è¿›å±•ï¼Œç‰¹åˆ«æ˜¯GPT-4é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ˜¾è‘—å¢å¼ºäº†å¤šæ¨¡æ€æ¨ç†ï¼Œä½†åŸºäºCoTçš„æ¨ç†åœ¨OODè¯­ä¹‰åˆ†å‰²ä¸­çš„åº”ç”¨ä»ç„¶é²œæœ‰æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œé€šè¿‡å¯¹é“è·¯åœºæ™¯å¼‚å¸¸å€¼çš„å¹¿æ³›åˆ†æï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸‰ç§å½“å‰æœ€å…ˆè¿›çš„OODåˆ†å‰²æ–¹æ³•å§‹ç»ˆé¢ä¸´æŒ‘æˆ˜çš„åœºæ™¯ï¼šï¼ˆ1ï¼‰å¯†é›†æ’åˆ—å’Œé‡å çš„ç‰©ä½“ï¼Œï¼ˆ2ï¼‰è¿œå¤„çš„å°ç‰©ä½“åœºæ™¯ï¼Œä»¥åŠï¼ˆ3ï¼‰å¤§å‹å‰æ™¯ä¸»å¯¼ç‰©ä½“ã€‚ä¸ºäº†è§£å†³æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹é“è·¯å¼‚å¸¸åœºæ™¯OODæ£€æµ‹çš„æ–°å‹CoTæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„å¹¿æ³›çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œå¦‚GPT-4ï¼Œé€šè¿‡æ”¹è¿›çš„å›¾åƒç†è§£å’Œä¸è§‚å¯Ÿåˆ°çš„æœ‰é—®é¢˜çš„åœºæ™¯å±æ€§ç›¸åŒ¹é…çš„æç¤ºæ¨ç†ï¼Œå¢å¼ºOODæ£€æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•å’Œæˆ‘ä»¬æ–°å®šä¹‰çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„RoadAnomalyæ•°æ®é›†å­é›†ä¸Šå‡ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œä¸ºå¤æ‚é©¾é©¶ç¯å¢ƒä¸­çš„OODè¯­ä¹‰åˆ†å‰²æä¾›äº†ç¨³å¥å’Œå¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03984v2">PDF</a> 6 pages, 3 figures. Accepted at IEEE International Conference on   Advanced Visual and Signal-Based Systems 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åœ¨å¤æ‚é“è·¯ç¯å¢ƒä¸­ï¼Œåˆ©ç”¨GPT-4ç­‰é¢„è®­ç»ƒå¤§æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›è¿›è¡Œå¼‚å¸¸æ£€æµ‹çš„é‡è¦æ€§åŠå…¶æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œæå‡ºäº†åŸºäºCoTçš„OODæ£€æµ‹æ¡†æ¶ï¼Œç”¨äºè§£å†³å¯†é›†é‡å ç‰©ä½“ã€è¿œå¤„å°ç‰©ä½“å’Œå‰æ™¯å¤§ç‰©ä½“çš„ä¸‰å¤§éš¾é¢˜åœºæ™¯ã€‚è¯¥æ¡†æ¶åœ¨å¸¸è§„åŸºå‡†æµ‹è¯•åŠæ–°å®šä¹‰çš„RoadAnomalyæ•°æ®é›†å­é›†ä¸Šçš„è¡¨ç°å‡è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œä¸ºå¤æ‚é©¾é©¶ç¯å¢ƒä¸­çš„OODè¯­ä¹‰åˆ†å‰²æä¾›äº†ç¨³å¥ä¸”å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>OODæ£€æµ‹å¯¹äºç¡®ä¿è¯­ä¹‰åˆ†å‰²æ¨¡å‹çš„å¯é æ€§è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚é“è·¯ç¯å¢ƒä¸­ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„OODåˆ†å‰²æ–¹æ³•åœ¨ç‰¹å®šåœºæ™¯ï¼ˆå¦‚å¯†é›†é‡å ç‰©ä½“ã€è¿œå¤„å°ç‰©ä½“å’Œå‰æ™¯å¤§ç‰©ä½“ï¼‰ä¸­å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>åˆ©ç”¨GPT-4ç­‰é¢„è®­ç»ƒå¤§æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡Chain-of-Thoughtï¼ˆCoTï¼‰æç¤ºå¢å¼ºOODæ£€æµ‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºCoTçš„OODæ£€æµ‹æ¡†æ¶ï¼Œç”¨äºè§£å†³é“è·¯å¼‚å¸¸åœºæ™¯ä¸­çš„OODé—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„å¹¿æ³›çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œæ”¹è¿›å›¾åƒç†è§£å¹¶ä¸è§‚å¯Ÿåˆ°çš„åœºæ™¯å±æ€§å¯¹é½ï¼Œä»è€Œè¿›è¡Œæç¤ºæ¨ç†ã€‚</li>
<li>æ¡†æ¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•å’Œæ–°å®šä¹‰çš„RoadAnomalyæ•°æ®é›†å­é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03984">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4e108e1404974e4e62012581e6595d80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f4f788b97cd31d96bdda5c818bb5332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e836bae2733818d9aab16f15aa8a7d27.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60a49f74a9fdb7737f2f07ea7a4ea70d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9d877a69fef7d159cb116ecbd56fe40.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-24/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-24/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-24/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-12283e0db00358e54a2015ff20ac05fe.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-24  LLM-empowered Dynamic Prompt Routing for Vision-Language Models Tuning   under Long-Tailed Distributions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3fc19417e0f1d62bfd6e8593cda009c7.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-23  From PREVENTion to REACTion Enhancing Failure Resolution in Naval   Systems
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28292.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
