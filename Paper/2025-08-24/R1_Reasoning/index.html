<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-08-24  Learning to Drive Ethically Embedding Moral Reasoning into Autonomous   Driving">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-adfcc41ab2f552363f86e2ccc6a4e5b9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    14.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    58 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-24-更新"><a href="#2025-08-24-更新" class="headerlink" title="2025-08-24 更新"></a>2025-08-24 更新</h1><h2 id="Learning-to-Drive-Ethically-Embedding-Moral-Reasoning-into-Autonomous-Driving"><a href="#Learning-to-Drive-Ethically-Embedding-Moral-Reasoning-into-Autonomous-Driving" class="headerlink" title="Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous   Driving"></a>Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous   Driving</h2><p><strong>Authors:Dianzhao Li, Ostap Okhrin</strong></p>
<p>Autonomous vehicles hold great promise for reducing traffic fatalities and improving transportation efficiency, yet their widespread adoption hinges on embedding robust ethical reasoning into routine and emergency maneuvers. Here, we present a hierarchical Safe Reinforcement Learning (Safe RL) framework that explicitly integrates moral considerations with standard driving objectives. At the decision level, a Safe RL agent is trained using a composite ethical risk cost, combining collision probability and harm severity, to generate high-level motion targets. A dynamic Prioritized Experience Replay mechanism amplifies learning from rare but critical, high-risk events. At the execution level, polynomial path planning coupled with Proportional-Integral-Derivative (PID) and Stanley controllers translates these targets into smooth, feasible trajectories, ensuring both accuracy and comfort. We train and validate our approach on rich, real-world traffic datasets encompassing diverse vehicles, cyclists, and pedestrians, and demonstrate that it outperforms baseline methods in reducing ethical risk and maintaining driving performance. To our knowledge, this is the first study of ethical decision-making for autonomous vehicles via Safe RL in real-world scenarios. Our results highlight the potential of combining formal control theory and data-driven learning to advance ethically accountable autonomy in complex, human-mixed traffic environments. </p>
<blockquote>
<p>自动驾驶汽车对于减少交通事故死亡人数、提高交通运输效率具有巨大潜力，然而其广泛采纳的关键在于将稳健的伦理推理嵌入到常规和紧急操作中。在此，我们提出了一种分层的安全强化学习（Safe RL）框架，该框架显式地将道德考量与标准驾驶目标相结合。在决策层面，Safe RL 代理通过使用组合的道德风险成本（结合碰撞概率和伤害严重程度）进行训练，以生成高级运动目标。动态优先经验回放机制放大了对罕见但关键的高风险事件的学习。在执行层面，通过多项式路径规划与比例积分微分（PID）和斯坦利控制器相结合，将这些目标转化为平稳且可行的轨迹，确保准确性和舒适性。我们在丰富的真实世界交通数据集上进行了训练和验证，该数据集涵盖了各种车辆、自行车和行人，并证明我们的方法在减少道德风险并保持驾驶性能方面优于基线方法。据我们所知，这是首个在现实世界中通过安全强化学习对自动驾驶汽车进行道德决策的研究。我们的结果突出了结合形式控制理论和数据驱动学习的潜力，有助于在复杂的人机混合交通环境中实现道德责任自主。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14926v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本摘要通过结合标准驾驶目标与道德考量，提出了一种分层的Safe Reinforcement Learning（Safe RL）框架。该框架在决策层面使用复合道德风险成本进行培训，并结合碰撞概率和伤害严重程度来生成高级运动目标。在执行层面，通过多项式路径规划与PID和Stanley控制器，将目标转化为平稳、可行的轨迹，确保准确性和舒适性。在真实世界的丰富交通数据集上训练和验证该框架，表明其在减少道德风险和维护驾驶性能上优于基准方法。这是首个在真实场景中运用Safe RL进行自动驾驶道德决策的研究。结果展示了结合形式控制理论与数据驱动学习的潜力，有助于推进复杂混合交通环境中的道德自主驾驶。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动驾驶车辆需要嵌入稳健的道德推理以应对日常和紧急情况下的决策。</li>
<li>提出了分层的Safe Reinforcement Learning（Safe RL）框架，将道德考量与驾驶目标相结合。</li>
<li>在决策层面使用复合道德风险成本进行培训，考虑碰撞概率和伤害严重程度。</li>
<li>采用动态优先级经验回放机制，强化从高风险事件中学习的效果。</li>
<li>在执行层面，通过多项式路径规划结合PID和Stanley控制器，实现平稳、可行的轨迹规划。</li>
<li>在真实世界的交通数据集上进行了训练和验证，表明该框架在减少道德风险和保持驾驶性能上优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14926">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-40d246e4d8e42fa992eaf53cc792a10d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29e6f31b869f19b2ae5e042ba9d725be.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FinAgentBench-A-Benchmark-Dataset-for-Agentic-Retrieval-in-Financial-Question-Answering"><a href="#FinAgentBench-A-Benchmark-Dataset-for-Agentic-Retrieval-in-Financial-Question-Answering" class="headerlink" title="FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial   Question Answering"></a>FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial   Question Answering</h2><p><strong>Authors:Chanyeol Choi, Jihoon Kwon, Alejandro Lopez-Lira, Chaewoon Kim, Minjae Kim, Juneha Hwang, Jaeseon Ha, Hojun Choi, Suyeol Yun, Yongjin Kim, Yongjae Lee</strong></p>
<p>Accurate information retrieval (IR) is critical in the financial domain, where investors must identify relevant information from large collections of documents. Traditional IR methods-whether sparse or dense-often fall short in retrieval accuracy, as it requires not only capturing semantic similarity but also performing fine-grained reasoning over document structure and domain-specific knowledge. Recent advances in large language models (LLMs) have opened up new opportunities for retrieval with multi-step reasoning, where the model ranks passages through iterative reasoning about which information is most relevant to a given query. However, there exists no benchmark to evaluate such capabilities in the financial domain. To address this gap, we introduce FinAgentBench, the first large-scale benchmark for evaluating retrieval with multi-step reasoning in finance – a setting we term agentic retrieval. The benchmark consists of 3,429 expert-annotated examples on S&amp;P-100 listed firms and assesses whether LLM agents can (1) identify the most relevant document type among candidates, and (2) pinpoint the key passage within the selected document. Our evaluation framework explicitly separates these two reasoning steps to address context limitations. This design enables to provide a quantitative basis for understanding retrieval-centric LLM behavior in finance. We evaluate a suite of state-of-the-art models and further demonstrated how targeted fine-tuning can significantly improve agentic retrieval performance. Our benchmark provides a foundation for studying retrieval-centric LLM behavior in complex, domain-specific tasks for finance. We will release the dataset publicly upon acceptance of the paper and plan to expand and share dataset for the full S&amp;P 500 and beyond. </p>
<blockquote>
<p>准确的情报检索（IR）在金融领域至关重要，投资者必须从大文档集合中识别出相关信息。传统的IR方法，无论是稀疏的还是密集的，往往在检索准确性方面表现不足，因为它不仅需要捕捉语义相似性，还需要对文档结构和特定领域的知识进行精细的推理。最近大型语言模型（LLM）的进步为具有多步推理的检索提供了新的机会，该模型通过关于哪些信息对给定查询最相关的迭代推理来排名段落。然而，金融领域还没有评估这种能力的基准测试。为了弥补这一空白，我们引入了FinAgentBench，这是第一个用于评估金融领域中具有多步推理的检索的大型基准测试——我们称之为代理检索。该基准测试包含3429个关于标普100指数上市公司的专家注释示例，并评估LLM代理是否能（1）在候选者中识别出最相关的文档类型，以及（2）在所选文档中定位关键段落。我们的评估框架明确地将这两个推理步骤分开，以解决上下文限制的问题。这种设计有助于为理解金融领域中以检索为中心的LLM行为提供定量依据。我们评估了一系列最先进的模型，并进一步展示了有针对性的微调如何显着提高代理检索性能。我们的基准测试为研究复杂、特定领域的任务中金融领域的LLM行为提供了基础。论文接受后，我们将公开发布数据集，并计划扩展和共享整个标普500及以外的数据集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14052v2">PDF</a> 6 pages</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了金融领域准确信息检索的重要性，并指出了传统信息检索方法的不足。最近的大型语言模型技术为具有多步骤推理的检索提供了新的机会。然而，金融领域缺乏相应的基准测试来评估这种能力。为了解决这一空白，文中介绍了一个新基准测试FinAgentBench，它是用于评估金融领域中具有多步骤推理的检索能力的首个大规模基准测试。该基准测试包含3429个针对标普100指数上市公司的专家注释示例，并评估大型语言模型代理是否能识别最相关的文档类型并在所选文档中定位关键段落。评价框架明确地将这两个推理步骤分开，以解决上下文限制问题。这将为理解针对金融复杂任务的检索为中心的大型语言模型行为提供定量依据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>金融领域的信息检索至关重要，对投资者的决策产生直接影响。</li>
<li>传统信息检索方法存在不足，无法准确捕捉语义相似性和进行精细的文档结构和领域知识推理。</li>
<li>大型语言模型技术的发展为具有多步骤推理的检索提供了新的可能性。</li>
<li>缺乏金融领域评估多步骤推理能力的基准测试。</li>
<li>FinAgentBench是首个用于评估金融领域中代理式信息检索能力的基准测试，包含大量专家标注的示例。</li>
<li>该基准测试评估大型语言模型是否能识别最相关的文档类型并在所选文档中定位关键段落。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14052">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fd69921bb210b3b00028254d96dded0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7edd011ecb55ab8f5f1dc0a0d3bc0c4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdc646c6400897963657abaae90790d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b17b2b1e8956436fd5c1ebad0b07f26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e63ef8bc6c0f1a43c6d8c26f11621086.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="IAD-R1-Reinforcing-Consistent-Reasoning-in-Industrial-Anomaly-Detection"><a href="#IAD-R1-Reinforcing-Consistent-Reasoning-in-Industrial-Anomaly-Detection" class="headerlink" title="IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection"></a>IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection</h2><p><strong>Authors:Yanhui Li, Yunkang Cao, Chengliang Liu, Yuan Xiong, Xinghui Dong, Chao Huang</strong></p>
<p>Industrial anomaly detection is a critical component of modern manufacturing, yet the scarcity of defective samples restricts traditional detection methods to scenario-specific applications. Although Vision-Language Models (VLMs) demonstrate significant advantages in generalization capabilities, their performance in industrial anomaly detection remains limited. To address this challenge, we propose IAD-R1, a universal post-training framework applicable to VLMs of different architectures and parameter scales, which substantially enhances their anomaly detection capabilities. IAD-R1 employs a two-stage training strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT) stage utilizes a meticulously constructed high-quality Chain-of-Thought dataset (Expert-AD) for training, enhancing anomaly perception capabilities and establishing reasoning-to-answer correlations; the Structured Control Group Relative Policy Optimization (SC-GRPO) stage employs carefully designed reward functions to achieve a capability leap from “Anomaly Perception” to “Anomaly Interpretation”. Experimental results demonstrate that IAD-R1 achieves significant improvements across 7 VLMs, the largest improvement was on the DAGM dataset, with average accuracy 43.3% higher than the 0.5B baseline. Notably, the 0.5B parameter model trained with IAD-R1 surpasses commercial models including GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the effectiveness and superiority of IAD-R1. The dataset, code, and all model weights will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/Yanhui-Lee/IAD-R1">https://github.com/Yanhui-Lee/IAD-R1</a>. </p>
<blockquote>
<p>工业异常检测是现代制造业的重要组成部分，但由于缺陷样本的稀缺性，传统的检测方法只能应用于特定场景。尽管视觉语言模型（VLMs）在通用化能力方面显示出显著优势，但在工业异常检测方面的性能仍然有限。为了解决这一挑战，我们提出了IAD-R1，这是一个适用于不同架构和参数规模的VLMs的通用后训练框架，能大幅提升其异常检测能力。IAD-R1采用两阶段训练策略：感知激活监督微调（PA-SFT）阶段利用精心构建的高质量思维链数据集（Expert-AD）进行训练，增强异常感知能力并建立推理到答案的关联；结构化对照组相对策略优化（SC-GRPO）阶段采用精心设计的奖励函数，实现从“异常感知”到“异常解读”的能力飞跃。实验结果表明，IAD-R1在7种VLMs上取得了显著改进，其中在DAGM数据集上的改进最大，平均精度比0.5B基线高出43.3%。值得注意的是，使用IAD-R1训练的0. for public access at <a target="_blank" rel="noopener" href="https://github.com/Yanhui-Lee/IAD-R1.%EF%BC%88%E5%85%AC%E5%BC%80%E5%9C%B0%E5%9D%80%EF%BC%9A">https://github.com/Yanhui-Lee/IAD-R1.（公开地址：</a><a target="_blank" rel="noopener" href="https://github.com/Yanhui-Lee/IAD-R1">https://github.com/Yanhui-Lee/IAD-R1</a>）。所有数据集、代码和模型权重均可在此处公开访问。）值得注意的是，使用IAD-R1训练的0.5B参数模型在零样本设置下超越了包括GPT-4.1和Claude-Sonnet-4在内的商业模型，这证明了IAD-R1的有效性和优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09178v2">PDF</a> </p>
<p><strong>Summary</strong><br>     工业异常检测是现代制造业的关键部分，但由于缺陷样本的稀缺性，传统检测方法的通用性受限。为应对挑战，提出IAD-R1，一个适用于不同架构和参数规模的视觉语言模型的通用后训练框架，显著提升了异常检测能力。IAD-R1采用两阶段训练策略：利用高质量Chain-of-Thought数据集进行感知激活监督微调，建立推理到答案的关联；通过精心设计的奖励函数实现控制组的相对策略优化，实现从“异常感知”到“异常解读”的能力跃升。实验结果显示，IAD-R1在7种视觉语言模型上取得显著改进，其中在DAGM数据集上的平均准确率较基线模型提高43.3%，且0.5B参数模型经IAD-R1训练后，在零样本设置下超越GPT-4.1和Claude-Sonnet-4等商业模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>工业异常检测是现代制造业的重要部分，但传统方法受限于缺陷样本的稀缺性。</li>
<li>VLMs在异常检测中虽具优势，但性能仍有局限。</li>
<li>提出IAD-R1框架，适用于不同VLMs，显著增强异常检测能力。</li>
<li>IAD-R1采用两阶段训练策略：感知激活监督微调与结构化控制组相对策略优化。</li>
<li>利用高质量Chain-of-Thought数据集建立推理到答案的关联。</li>
<li>实验结果显示IAD-R1在多个数据集上表现优异，特别是在DAGM数据集上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09178">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7f51c9e879429ba467bce21f2e6aee19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75f73122c677d2326ccc28877ef813de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c35d0278f2fb1a12a6eaafca4cfa8b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2948fdfa87cc1044cdefc6fd9d3e5352.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-548af570c8e40dfcedd9fab2a4a0ff16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53ea0dddfb9f1ef722f2fdc83891a3d8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AURA-A-Fine-Grained-Benchmark-and-Decomposed-Metric-for-Audio-Visual-Reasoning"><a href="#AURA-A-Fine-Grained-Benchmark-and-Decomposed-Metric-for-Audio-Visual-Reasoning" class="headerlink" title="AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual   Reasoning"></a>AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual   Reasoning</h2><p><strong>Authors:Siminfar Samakoush Galougah, Rishie Raj, Sanjoy Chowdhury, Sayan Nag, Ramani Duraiswami</strong></p>
<p>Current audio-visual (AV) benchmarks focus on final answer accuracy, overlooking the underlying reasoning process. This makes it difficult to distinguish genuine comprehension from correct answers derived through flawed reasoning or hallucinations. To address this, we introduce AURA (Audio-visual Understanding and Reasoning Assessment), a benchmark for evaluating the cross-modal reasoning capabilities of Audio-Visual Large Language Models (AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions across six challenging cognitive domains, such as causality, timbre and pitch, tempo and AV synchronization, unanswerability, implicit distractions, and skill profiling, explicitly designed to be unanswerable from a single modality. This forces models to construct a valid logical path grounded in both audio and video, setting AURA apart from AV datasets that allow uni-modal shortcuts. To assess reasoning traces, we propose a novel metric, AuraScore, which addresses the lack of robust tools for evaluating reasoning fidelity. It decomposes reasoning into two aspects: (i) Factual Consistency - whether reasoning is grounded in perceptual evidence, and (ii) Core Inference - the logical validity of each reasoning step. Evaluations of SOTA models on AURA reveal a critical reasoning gap: although models achieve high accuracy (up to 92% on some tasks), their Factual Consistency and Core Inference scores fall below 45%. This discrepancy highlights that models often arrive at correct answers through flawed logic, underscoring the need for our benchmark and paving the way for more robust multimodal evaluation. </p>
<blockquote>
<p>当前视听（AV）基准测试主要关注最终答案的准确性，忽视了背后的推理过程。这使得很难区分真正的理解与通过错误推理或幻觉得出的正确答案。为解决这一问题，我们引入了AURA（视听理解与推理评估），这是一个基准测试，旨在评估视听大型语言模型（AV-LLM）和多媒体语言模型（OLM）的跨模态推理能力。AURA包含六个具有挑战性的认知领域的问题，如因果关系、音色和音调、节奏和AV同步、无法回答的问题、隐式干扰和技能分析，这些问题都是明确设计为无法从单一模态中得出答案的。这迫使模型在音频和视频的基础上构建有效的逻辑路径，使AURA与允许单一模态捷径的AV数据集区分开来。为了评估推理痕迹，我们提出了一个新的指标——AuraScore，它解决了缺乏评估推理可信度的稳健工具的问题。它将推理分为两个方面：（i）事实一致性——推理是否建立在感知证据之上；（ii）核心推断——每个推理步骤的逻辑有效性。对先进模型在AURA上的评估显示了一个关键的推理差距：虽然模型的准确率很高（某些任务高达92%），但它们在事实一致性和核心推断方面的得分却低于45%。这种差异表明，模型通常是通过错误的逻辑得出正确答案的，这突显了我们基准测试的重要性，并为更稳健的多模态评估铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07470v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了现有的音视频基准测试主要关注最终答案的准确性，而忽视了背后的推理过程的问题。为解决这一问题，提出了AURA基准测试，旨在评估音频视觉大语言模型（AV-LLMs）和全方位语言模型（OLMs）的跨模态推理能力。AURA包括六个挑战性认知领域的题目，包括因果、音色和音调、节奏和音视频同步、无法回答的问题、隐性干扰以及技能分析，这些问题设计得无法从单一模态中找到答案，从而迫使模型基于音频和视频构建有效的逻辑路径。为评估推理过程，本文提出了一种新的度量标准——AuraScore，以解决缺乏可靠的推理评价工具的问题。该分数关注两个核心方面：事实一致性——推理是否基于感知证据；核心推理——每个推理步骤的逻辑有效性。对先进模型的评估显示，它们在关键推理方面存在差距：虽然某些任务的准确率高达92%，但事实一致性和核心推理分数低于45%，这表明模型通常通过逻辑错误得出正确答案，突显了需要采用新的基准测试进行更稳健的多模态评估的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前音频视觉基准测试主要关注最终答案的准确性，忽略了推理过程。</li>
<li>AURA基准测试旨在评估音频视觉大语言模型和全方位语言模型的跨模态推理能力。</li>
<li>AURA包含六个挑战性的认知领域问题，这些问题无法从单一模态解答，要求模型在音频和视频之间建立逻辑联系。</li>
<li>为评估推理过程，提出了AuraScore这一新的度量标准。</li>
<li>AuraScore关注两个核心方面：事实一致性和核心推理。</li>
<li>对先进模型的评估显示，尽管它们在某些任务上表现良好，但在关键推理方面存在显著差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07470">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-306999ce1cdda841bd5cc5fd11bafe5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c808f17f9ed8e35a352978d68f9ce4b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05c2d89b3302afc263790b648e87d7e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b05f090cc304565efe6994040424ae97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c0b364b64e971dc99d4d40b0febc421.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adfcc41ab2f552363f86e2ccc6a4e5b9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References"><a href="#MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References" class="headerlink" title="MultiRef: Controllable Image Generation with Multiple Visual References"></a>MultiRef: Controllable Image Generation with Multiple Visual References</h2><p><strong>Authors:Ruoxi Chen, Dongping Chen, Siyuan Wu, Sinan Wang, Shiyun Lang, Petr Sushko, Gaoyang Jiang, Yao Wan, Ranjay Krishna</strong></p>
<p>Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs – either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: <a target="_blank" rel="noopener" href="https://multiref.github.io/">https://multiref.github.io/</a>. </p>
<blockquote>
<p>视觉设计师自然会从多种视觉参考中汲取灵感，结合不同的元素和美学原则进行艺术创作。然而，目前的图像生成框架主要依赖于单一来源的输入，无论是文本提示还是单个参考图像。在本文中，我们关注使用多个视觉参考的可控图像生成任务。我们介绍了MultiRef-bench，这是一个严格的评估框架，包含990个合成样本和1000个真实世界样本，需要融合多个参考图像中的视觉内容。合成样本是通过我们的数据引擎RefBlend合成的，包含10种参考类型和33种参考组合。基于RefBlend，我们进一步构建了一个包含38k高质量图像的数据集MultiRef，以促进进一步的研究。我们在三个交织的图像文本模型（即OmniGen、ACE和Show-o）和六个代理框架（例如ChatDiT和LLM+SD）上的实验表明，即使是最先进的系统也面临多参考条件设置的挑战，最好的模型OmniGen在合成样本上的平均准确率仅为66.6%，在真实案例上为79.0%，与标准答案相比仍有一定差距。这些发现为开发更灵活、更人性化的创意工具提供了宝贵方向，这些工具可以有效地整合多种视觉灵感来源。数据集可在<a target="_blank" rel="noopener" href="https://multiref.github.io/%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://multiref.github.io/公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06905v2">PDF</a> Accepted to ACM MM 2025 Datasets</p>
<p><strong>Summary</strong></p>
<p>本文介绍了视觉设计师从多个视觉参考中汲取灵感进行创作的方法，而现有的图像生成框架主要依赖于单一源输入。针对这一问题，本文专注于使用多个视觉参考进行可控图像生成的任务。文章提出了MultiRef-bench评估框架，包含合成样本和真实世界样本，并基于RefBlend数据引擎生成合成样本。实验结果显示，即使是最先进的系统也面临多参考条件挑战，最佳模型OmniGen在合成样本和真实世界案例中的平均表现仅为66.6%和79.0%。这为开发能有效整合多种视觉灵感来源的更灵活、更人性化的创意工具提供了有价值的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉设计师从多个视觉参考中汲取灵感进行创作，而现有图像生成框架主要依赖单一源输入。</li>
<li>本文提出了MultiRef-bench评估框架，用于评估使用多个视觉参考的可控图像生成。</li>
<li>MultiRef-bench包含合成样本和真实世界样本，合成样本通过RefBlend数据引擎生成。</li>
<li>实验结果显示，即使是最先进的系统在多参考条件下的表现也不理想。</li>
<li>最佳模型OmniGen在合成样本和真实世界案例中的平均表现分别为66.6%和79.0%。</li>
<li>这为开发能整合多种视觉灵感来源的创意工具提供了有价值的方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06905">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d9e34d3ea7f05ce2185407b54b7a90a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b31871c430d0fbf498dd9080bec5071a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8d6b3d99f0accdbf50cbd62532e78f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a75c67ad4d70f35f3ef90c9dae06009.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38dd2bd3ba67c809c984a897b91d8e80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-661a75e9d0f09f34f2f50a19a141e74b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20393970eadc25313042671698f91c2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a3804bfa59e558dcf54da5c1eb216f2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Affordance-R1-Reinforcement-Learning-for-Generalizable-Affordance-Reasoning-in-Multimodal-Large-Language-Model"><a href="#Affordance-R1-Reinforcement-Learning-for-Generalizable-Affordance-Reasoning-in-Multimodal-Large-Language-Model" class="headerlink" title="Affordance-R1: Reinforcement Learning for Generalizable Affordance   Reasoning in Multimodal Large Language Model"></a>Affordance-R1: Reinforcement Learning for Generalizable Affordance   Reasoning in Multimodal Large Language Model</h2><p><strong>Authors:Hanqing Wang, Shaoyang Wang, Yiming Zhong, Zemin Yang, Jiamin Wang, Zhiqing Cui, Jiahao Yuan, Yifan Han, Mingyu Liu, Yuexin Ma</strong></p>
<p>Affordance grounding focuses on predicting the specific regions of objects that are associated with the actions to be performed by robots. It plays a vital role in the fields of human-robot interaction, human-object interaction, embodied manipulation, and embodied perception. Existing models often neglect the affordance shared among different objects because they lack the Chain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD) generalization and explicit reasoning capabilities. To address these challenges, we propose Affordance-R1, the first unified affordance grounding framework that integrates cognitive CoT guided Group Relative Policy Optimization (GRPO) within a reinforcement learning paradigm. Specifically, we designed a sophisticated affordance function, which contains format, perception, and cognition rewards to effectively guide optimization directions. Furthermore, we constructed a high-quality affordance-centric reasoning dataset, ReasonAff, to support training. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Affordance-R1 achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Comprehensive experiments demonstrate that our model outperforms well-established methods and exhibits open-world generalization. To the best of our knowledge, Affordance-R1 is the first to integrate GRPO-based RL with reasoning into affordance reasoning. The code of our method and our dataset is released on <a target="_blank" rel="noopener" href="https://github.com/hq-King/Affordance-R1">https://github.com/hq-King/Affordance-R1</a>. </p>
<blockquote>
<p>亲和性接地（Affordance grounding）主要关注预测与机器人要执行的动作相关联的特定对象区域。它在人机交互、人与对象交互、具体操控和具体感知等领域中发挥着至关重要的作用。现有模型往往忽视了不同对象之间的共享亲和性，因为它们缺乏思维链（Chain-of-Thought，CoT）推理能力，这限制了它们的跨域（OOD）泛化和显式推理能力。为了应对这些挑战，我们提出了亲和性R1（Affordance-R1），这是第一个统一的亲和性接地框架，它整合了认知思维链引导群体相对策略优化（GRPO）在强化学习范式中。具体来说，我们设计了一个复杂的亲和性功能，其中包含格式、感知和认知奖励，以有效地指导优化方向。此外，我们构建了一个高质量的亲合性中心推理数据集ReasonAff，以支持训练。仅通过强化学习与GRPO进行训练，而无需明确的推理数据，亲和性R1实现了稳健的零样本泛化，并展现出新兴的测试时间推理能力。综合实验表明，我们的模型优于现有的方法，并表现出开放世界的泛化能力。据我们所知，亲和性R1是首个将基于GRPO的RL与推理整合到亲和性推理中的模型。我们的方法和数据集的代码已发布在<a target="_blank" rel="noopener" href="https://github.com/hq-King/Affordance-R1">https://github.com/hq-King/Affordance-R1</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06206v3">PDF</a> </p>
<p><strong>Summary</strong><br>基于动作预测的对象特定区域定位在机器人交互中扮演重要角色。现有模型因缺乏思维链（Chain-of-Thought，CoT）推理能力而难以把握不同对象间的共享性定位，影响了模型的领域外泛化能力和显式推理能力。我们提出统一融合思维链引导的群组相对策略优化（GRPO）的Affordance-R1框架，通过强化学习范式解决这一问题。该框架设计了一个复杂的定位功能，包括格式、感知和认知奖励来指导优化方向，构建了高质量的中心化推理数据集ReasonAff以支持训练。实验表明，我们的模型实现了稳健的零样本泛化并展现出推理能力。Affordance-R1是首个将基于GRPO的强化学习与推理结合的模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Affordance grounding 专注于预测与机器人动作相关联的对象特定区域。</li>
<li>现有模型缺乏 Chain-of-Thought (CoT) 推理能力，难以处理不同对象间的共享性定位。</li>
<li>Affordance-R1 是首个统一融合认知 CoT 引导的 Group Relative Policy Optimization (GRPO) 的定位框架。</li>
<li>该框架通过强化学习范式设计复杂的定位功能，包括格式、感知和认知奖励。</li>
<li>构建高质量的中心化推理数据集ReasonAff以支持训练。</li>
<li>Affordance-R1 实现稳健的零样本泛化并展现出推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06206">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c9a9b52e545921883d13c839a0160d7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-027324787b676671f5cbf4fce3a261e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a96dfaf91ef4fa1149402c3affecb3d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbc31bf427f552fee809d62d9b16732f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66bf41a20f2d36ea0aa61545822651e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9fe984d1975b656b9fef1c10d9973d0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Exploring-Superior-Function-Calls-via-Reinforcement-Learning"><a href="#Exploring-Superior-Function-Calls-via-Reinforcement-Learning" class="headerlink" title="Exploring Superior Function Calls via Reinforcement Learning"></a>Exploring Superior Function Calls via Reinforcement Learning</h2><p><strong>Authors:Bingguang Hao, Maolin Wang, Zengzhuang Xu, Yicheng Chen, Cunyin Peng, Jinjie GU, Chenyi Zhuang</strong></p>
<p>Function calling capabilities are crucial for deploying Large Language Models in real-world applications, yet current training approaches fail to develop robust reasoning strategies. Supervised fine-tuning produces models that rely on superficial pattern matching, while standard reinforcement learning methods struggle with the complex action space of structured function calls. We present a novel reinforcement learning framework designed to enhance group relative policy optimization through strategic entropy based exploration specifically tailored for function calling tasks. Our approach addresses three critical challenges in function calling: insufficient exploration during policy learning, lack of structured reasoning in chain-of-thought generation, and inadequate verification of parameter extraction. Our two-stage data preparation pipeline ensures high-quality training samples through iterative LLM evaluation and abstract syntax tree validation. Extensive experiments on the Berkeley Function Calling Leaderboard demonstrate that this framework achieves state-of-the-art performance among open-source models with 86.02% overall accuracy, outperforming standard GRPO by up to 6% on complex multi-function scenarios. Notably, our method shows particularly strong improvements on code-pretrained models, suggesting that structured language generation capabilities provide an advantageous starting point for reinforcement learning in function calling tasks. We will release all the code, models and dataset to benefit the community. </p>
<blockquote>
<p>函数调用能力对于在大规模语言模型中部署现实世界应用至关重要，然而目前的训练方法无法开发稳健的推理策略。有监督的微调会产生依赖肤浅模式匹配的模型，而标准强化学习方法在复杂的结构化函数调用动作空间中苦苦挣扎。我们提出了一种新型的强化学习框架，旨在通过基于策略熵的探索增强群体相对策略优化，该框架特别针对函数调用任务量身定制。我们的方法解决了函数调用中的三个关键挑战：策略学习过程中的探索不足、思维链生成中结构化推理的缺乏以及参数提取的验证不足。我们的两阶段数据准备管道通过迭代的大型语言模型评估和抽象语法树验证，确保高质量的训练样本。在伯克利函数调用排行榜上的大量实验表明，该框架在开源模型中实现了最佳性能，总体准确率为86.02%，在复杂的多函数场景中最多可提高GRPO性能6%。值得注意的是，我们的方法在代码预训练模型上显示出特别强大的改进，这表明结构化语言生成能力为强化学习在函数调用任务中提供了一个有利的起点。我们将发布所有代码、模型和数据集以造福社区。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05118v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型在实际应用中的函数调用能力至关重要，但现有训练方法在培养稳健推理策略方面存在不足。监督微调产生的模型依赖于表面模式匹配，而标准强化学习方法在复杂的函数调用动作空间中表现挣扎。我们提出了一种新型的强化学习框架，旨在通过基于战略熵的探索增强群体相对策略优化，特别适用于函数调用任务。该框架解决了函数调用中的三个关键挑战：政策学习过程中的探索不足、链式思维生成中缺乏结构化推理以及参数提取的验证不足。我们的两阶段数据准备管道通过迭代的大型语言模型评估和抽象语法树验证，确保高质量的培训样本。在Berkeley函数调用排行榜上的大量实验表明，该框架在开源模型中实现了最佳性能，总体准确度为86.02%，在复杂的多功能场景下比标准GRPO高出6%。值得注意的是，我们的方法在代码预训练模型上表现出了特别大的改进，这表明结构化语言生成能力为强化学习在函数调用任务中提供了一个有利的起点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在实际应用中需要强大的函数调用能力。</li>
<li>当前训练方法在培养模型推理策略方面存在不足，监督微调模型依赖于表面模式匹配。</li>
<li>针对函数调用任务，提出了一种新型的强化学习框架。</li>
<li>该框架解决了政策学习过程中的探索不足、缺乏结构化推理和参数提取验证不足的问题。</li>
<li>通过两阶段数据准备管道确保高质量培训样本。</li>
<li>在Berkeley函数调用排行榜上实现了最佳性能，总体准确度为86.02%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05118">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4123c5f39102d26a83d95165c011a54f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1253f26143895775e96cfd8eae0001db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33a410925db6fd9f09c5a0eb4ef94157.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-809d9b45f96bf6d6b740e919b9119531.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="When-Good-Sounds-Go-Adversarial-Jailbreaking-Audio-Language-Models-with-Benign-Inputs"><a href="#When-Good-Sounds-Go-Adversarial-Jailbreaking-Audio-Language-Models-with-Benign-Inputs" class="headerlink" title="When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with   Benign Inputs"></a>When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with   Benign Inputs</h2><p><strong>Authors:Bodam Kim, Hiskias Dingeto, Taeyoun Kwon, Dasol Choi, DongGeon Lee, Haon Park, JaeHoon Lee, Jongho Shin</strong></p>
<p>As large language models become increasingly integrated into daily life, audio has emerged as a key interface for human-AI interaction. However, this convenience also introduces new vulnerabilities, making audio a potential attack surface for adversaries. Our research introduces WhisperInject, a two-stage adversarial audio attack framework that can manipulate state-of-the-art audio language models to generate harmful content. Our method uses imperceptible perturbations in audio inputs that remain benign to human listeners. The first stage uses a novel reward-based optimization method, Reinforcement Learning with Projected Gradient Descent (RL-PGD), to guide the target model to circumvent its own safety protocols and generate harmful native responses. This native harmful response then serves as the target for Stage 2, Payload Injection, where we use Projected Gradient Descent (PGD) to optimize subtle perturbations that are embedded into benign audio carriers, such as weather queries or greeting messages. Validated under the rigorous StrongREJECT, LlamaGuard, as well as Human Evaluation safety evaluation framework, our experiments demonstrate a success rate exceeding 86% across Qwen2.5-Omni-3B, Qwen2.5-Omni-7B, and Phi-4-Multimodal. Our work demonstrates a new class of practical, audio-native threats, moving beyond theoretical exploits to reveal a feasible and covert method for manipulating AI behavior. </p>
<blockquote>
<p>随着大型语言模型在日常生活中得到越来越深入的集成，音频作为人类与人工智能交互的关键接口已崭露头角。然而，这种便利也带来了新的漏洞，使音频成为潜在的对敌攻击面。我们的研究引入了WhisperInject，这是一个两阶段的对抗性音频攻击框架，可以操纵最先进的音频语言模型以生成有害内容。我们的方法使用音频输入中不可察觉的扰动，这些扰动对人类听众来说是良性的。第一阶段采用基于奖励的优化方法，即使用带有投影梯度下降法的强化学习（RL-PGD），以引导目标模型绕过其自己的安全协议并生成有害的本地响应。然后这个本地有害响应成为第二阶段即载荷注入的目标，我们在此阶段使用投影梯度下降法（PGD）来优化嵌入良性音频载体中的微妙扰动，例如天气查询或问候信息。在严格的StrongREJECT、LlamaGuard以及人类评估安全评估框架下进行了验证，我们的实验表明，在Qwen2.5-Omni-3B、Qwen2.5-Omni-7B和Phi-4-Multimodal上的成功率超过86%。我们的工作展示了一种新的实用、原生音频威胁类别，超越了理论上的漏洞利用，揭示了一种可行且隐秘的操纵人工智能行为的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03365v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>语音作为人机交互的新接口，带来了便捷的同时也存在安全隐患。研究团队提出了一种名为WhisperInject的两阶段对抗性音频攻击框架，该框架可以操控先进的音频语言模型生成有害内容。该方法利用音频输入中的不可察觉扰动，对人类听众无害。第一阶段使用基于奖励优化的方法，即使用带有投影梯度下降法的强化学习（RL-PGD），引导目标模型绕过其安全协议生成有害的原生响应。第二阶段为载荷注入，使用投影梯度下降法优化嵌入到良性音频载体中的细微扰动，如天气查询或问候信息。实验在严格的安全评估框架下验证了其成功率超过86%，并揭示了音频原生威胁的新类别。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频作为人机交互的新接口存在潜在的安全隐患。</li>
<li>研究人员提出一种名为WhisperInject的两阶段对抗性音频攻击框架。</li>
<li>WhisperInject可以操控先进的音频语言模型生成有害内容。</li>
<li>该方法利用人类听觉无法察觉的微小变化来实施攻击。</li>
<li>第一阶段使用基于奖励优化的方法绕过模型的安全协议。</li>
<li>第二阶段将载荷注入到看似正常的音频中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03365">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aa9a6b250f8b0a7dcf656876cb45254e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6754925625f3948944ff104072e8d1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d54ee43ecc5ea68a5515fdd5a0154dd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f26a83e77d178f85d3e239aea0ef8eaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb4c6502dc5e107f3d6c1a13c083b566.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents"><a href="#SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents" class="headerlink" title="SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents"></a>SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents</h2><p><strong>Authors:Jiaye Lin, Yifu Guo, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen, Hongzhang Liu, Ronghao Chen, Yangfan He, Daxin Jiang, Binxing Jiao, Chen Hu, Huacan Wang</strong></p>
<p>Large Language Model (LLM)-based agents have recently shown impressive capabilities in complex reasoning and tool use via multi-step interactions with their environments. While these agents have the potential to tackle complicated tasks, their problem-solving process, i.e., agents’ interaction trajectory leading to task completion, remains underexploited. These trajectories contain rich feedback that can navigate agents toward the right directions for solving problems correctly. Although prevailing approaches, such as Monte Carlo Tree Search (MCTS), can effectively balance exploration and exploitation, they ignore the interdependence among various trajectories and lack the diversity of search spaces, which leads to redundant reasoning and suboptimal outcomes. To address these challenges, we propose SE-Agent, a Self-Evolution framework that enables Agents to optimize their reasoning processes iteratively. Our approach revisits and enhances former pilot trajectories through three key operations: revision, recombination, and refinement. This evolutionary mechanism enables two critical advantages: (1) it expands the search space beyond local optima by intelligently exploring diverse solution paths guided by previous trajectories, and (2) it leverages cross-trajectory inspiration to efficiently enhance performance while mitigating the impact of suboptimal reasoning paths. Through these mechanisms, SE-Agent achieves continuous self-evolution that incrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench Verified to resolve real-world GitHub issues. Experimental results across five strong LLMs show that integrating SE-Agent delivers up to 55% relative improvement, achieving state-of-the-art performance among all open-source agents on SWE-bench Verified. Our code and demonstration materials are publicly available at <a target="_blank" rel="noopener" href="https://github.com/JARVIS-Xs/SE-Agent">https://github.com/JARVIS-Xs/SE-Agent</a>. </p>
<blockquote>
<p>基于大规模语言模型（LLM）的代理最近显示出通过与环境的多步交互进行复杂推理和工具使用的令人印象深刻的能力。虽然这些代理有潜力处理复杂任务，但他们的解决问题过程，即代理完成任务的交互轨迹，仍未得到充分探索。这些轨迹包含丰富的反馈，可以引导代理朝着正确的方向解决问题。尽管现有的方法，如蒙特卡洛树搜索（MCTS），可以有效地平衡探索和利用，但它们忽略了不同轨迹之间的相互依赖性，并且缺乏搜索空间的多样性，这导致冗余推理和次优结果。为了解决这些挑战，我们提出了SE-Agent，一种自我进化框架，使代理能够迭代优化他们的推理过程。我们的方法通过三个关键操作：修订、重组和细化，重新访问并增强先前的轨迹。这种进化机制带来了两个关键优势：（1）它通过智能地探索由先前轨迹引导的多样化解决方案路径，扩大了搜索空间，超越了局部最优；（2）它利用跨轨迹的灵感来有效地提高性能，同时减轻次优推理路径的影响。通过这些机制，SE-Agent实现了持续的自我进化，逐步提高了推理质量。我们在SWE-bench Verified上评估了SE-Agent，以解决现实世界中的GitHub问题。在五个强大的LLM上的实验结果表明，集成SE-Agent带来了高达55%的相对改进，在SWE-bench Verified上的性能达到了开源代理中的最佳水平。我们的代码和演示材料可在<a target="_blank" rel="noopener" href="https://github.com/JARVIS-Xs/SE-Agent">https://github.com/JARVIS-Xs/SE-Agent</a>公开获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02085v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在复杂推理和工具使用方面展现出令人印象深刻的能力，通过与环境的多步骤交互来完成任务。然而，LLM在解决问题时的交互轨迹尚未得到充分探索。本文提出了一种名为SE-Agent的自我进化框架，通过修订、重组和细化之前的轨迹，使代理能够优化其推理过程。该框架扩大了搜索空间，通过跨轨迹的灵感来提高性能并减少次优推理路径的影响。实验结果表明，SE-Agent在解决现实世界GitHub问题上实现了高达55%的相对改进，达到了SWE-bench Verified上的最新性能水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在复杂推理和工具使用方面表现出强大的能力，通过与环境的多步交互完成任务。</li>
<li>LLM的问题解决轨迹包含丰富的反馈，可以指导其正确解决问题。</li>
<li>现有方法（如蒙特卡洛树搜索）虽然能有效平衡探索与利用，但忽略了轨迹间的相互依赖性并缺乏搜索空间的多样性，导致冗余推理和次优结果。</li>
<li>SE-Agent通过修订、重组和细化之前的轨迹，实现了代理推理过程的优化迭代。</li>
<li>SE-Agent扩大了搜索空间，通过跨轨迹的灵感提高性能并减少次优推理路径的影响。</li>
<li>SE-Agent在解决现实世界GitHub问题上实现了显著的性能改进，相对于其他开源代理达到了最新水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02085">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9bd726db11fbc16937e1c35c985be8d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc136467beb09e19fc1f86d24cbca8bd.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Bench2ADVLM-A-Closed-Loop-Benchmark-for-Vision-language-Models-in-Autonomous-Driving"><a href="#Bench2ADVLM-A-Closed-Loop-Benchmark-for-Vision-language-Models-in-Autonomous-Driving" class="headerlink" title="Bench2ADVLM: A Closed-Loop Benchmark for Vision-language Models in   Autonomous Driving"></a>Bench2ADVLM: A Closed-Loop Benchmark for Vision-language Models in   Autonomous Driving</h2><p><strong>Authors:Tianyuan Zhang, Ting Jin, Lu Wang, Jiangfan Liu, Siyuan Liang, Mingchuan Zhang, Aishan Liu, Xianglong Liu</strong></p>
<p>Vision-Language Models (VLMs) have recently emerged as a promising paradigm in autonomous driving (AD). However, current performance evaluation protocols for VLM-based AD systems (ADVLMs) are predominantly confined to open-loop settings with static inputs, neglecting the more realistic and informative closed-loop setting that captures interactive behavior, feedback resilience, and real-world safety. To address this, we introduce Bench2ADVLM, a unified hierarchical closed-loop evaluation framework for real-time, interactive assessment of ADVLMs across both simulation and physical platforms. Inspired by dual-process theories of cognition, we first adapt diverse ADVLMs to simulation environments via a dual-system adaptation architecture. In this design, heterogeneous high-level driving commands generated by target ADVLMs (fast system) are interpreted by a general-purpose VLM (slow system) into standardized mid-level control actions suitable for execution in simulation. To bridge the gap between simulation and reality, we design a physical control abstraction layer that translates these mid-level actions into low-level actuation signals, enabling, for the first time, closed-loop testing of ADVLMs on physical vehicles. To enable more comprehensive evaluation, Bench2ADVLM introduces a self-reflective scenario generation module that automatically explores model behavior and uncovers potential failure modes for safety-critical scenario generation. Overall, Bench2ADVLM establishes a hierarchical evaluation pipeline that seamlessly integrates high-level abstract reasoning, mid-level simulation actions, and low-level real-world execution. Experiments on diverse scenarios across multiple state-of-the-art ADVLMs and physical platforms validate the diagnostic strength of our framework, revealing that existing ADVLMs still exhibit limited performance under closed-loop conditions. </p>
<blockquote>
<p>视觉语言模型（VLM）最近作为自动驾驶（AD）领域的一种有前途的范式而出现。然而，当前的基于VLM的自动驾驶系统（ADVLM）性能评估协议主要局限于开放循环设置和静态输入，忽略了更真实且信息丰富的闭环设置，该设置可以捕获交互行为、反馈恢复力和现实世界安全性。为了解决这个问题，我们引入了Bench2ADVLM，这是一个统一的分层闭环评估框架，用于在模拟和物理平台上对ADVLM进行实时、交互式的评估。受到认知双过程理论的启发，我们首先将各种ADVLM适应到仿真环境，通过双系统适应架构。在此设计中，由目标ADVLM（快速系统）生成的不同高级驾驶命令由通用VLM（慢速系统）解释为标准化的中级控制动作，适用于仿真中的执行。为了弥合仿真与现实之间的差距，我们设计了一个物理控制抽象层，将这些中级动作转化为低级执行信号，首次实现对物理车辆上ADVLM的闭环测试。为了进行更全面的评估，Bench2ADVLM引入了一个自我反思的场景生成模块，该模块自动探索模型行为并揭示潜在故障模式，用于生成安全关键场景。总的来说，Bench2ADVLM建立了一个分层的评估流程，无缝集成了高级抽象推理、中级仿真动作和低级现实世界执行。在多个先进的ADVLM和物理平台上的各种场景的实验验证了我们的框架的诊断能力，表明现有的ADVLM在闭环条件下仍表现出有限性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02028v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了在自动驾驶（AD）领域新兴的一种有前景的范式——视觉语言模型（VLM）。然而，当前针对基于VLM的AD系统（ADVLMs）的性能评估协议主要局限于开放循环设置，缺乏更真实和全面的闭环评估框架。为此，本文提出了Bench2ADVLM，这是一个统一的分层闭环评估框架，用于实时、交互地评估ADVLMs在模拟和物理平台上的表现。该框架引入了双系统适应架构和物理控制抽象层，实现了模拟环境与真实世界的无缝对接。此外，还引入了自我反思的场景生成模块，以自动探索模型行为并揭示潜在失败模式，从而进行全面评估。实验结果证明了该框架的诊断能力，并揭示了现有ADVLMs在闭环条件下的性能局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前针对基于视觉语言模型的自动驾驶系统（ADVLMs）的性能评估主要局限于开放循环设置。</li>
<li>Bench2ADVLM框架旨在实现ADVLMs在模拟和真实平台上的实时、交互评估。</li>
<li>框架采用双系统适应架构，将高级驾驶命令转化为标准化中级控制动作。</li>
<li>通过物理控制抽象层，实现了模拟环境与真实世界的无缝对接。</li>
<li>引入了自我反思的场景生成模块，以自动探索模型行为并揭示潜在失败模式。</li>
<li>实验结果表明，现有ADVLMs在闭环条件下的性能存在局限性。</li>
<li>Bench2ADVLM框架为全面评估ADVLMs性能提供了有力的工具。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02028">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-985504df6449d45c511d17a1b2e436c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea3dfef142fa2728c1fcd29cc68ebe9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c6b0c58c4d2a681f7c37de7d80daa2b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-353360e5eabc2b141b0b88de8049686a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b307be4db48537291add794a3b66ef58.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Seed-X-Building-Strong-Multilingual-Translation-LLM-with-7B-Parameters"><a href="#Seed-X-Building-Strong-Multilingual-Translation-LLM-with-7B-Parameters" class="headerlink" title="Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters"></a>Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters</h2><p><strong>Authors:Shanbo Cheng, Yu Bao, Qian Cao, Luyang Huang, Liyan Kang, Zhicheng Liu, Yu Lu, Wenhao Zhu, Jingwen Chen, Zhichao Huang, Tao Li, Yifu Li, Huiying Lin, Sitong Liu, Ningxin Peng, Shuaijie She, Lu Xu, Nuo Xu, Sen Yang, Runsheng Yu, Yiming Yu, Liehao Zou, Hang Li, Lu Lu, Yuxuan Wang, Yonghui Wu</strong></p>
<p>Multilingual translation stands as a challenging task for large language models (LLMs) to handle intricate language patterns and stilted translations that arise in automated translations. In this paper, we introduce Seed-X, a family of open-source LLMs comprising instruct and reasoning models, pushing the limits of translation capability with 7B parameter size. The base model is pre-trained on a diverse, high-quality dataset encompassing both monolingual and bilingual content across 28 languages, harnessing the full potential of multilingual data. The instruct model is then finetuned to translate by Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement learning (RL) to achieve better generalization across diverse language pairs. Seed-X achieves performance comparable to leading closed-source models, including Gemini-2.5 and GPT-4o, across 28 languages, and significantly outperforms larger open-source models in both automatic metrics and human evaluations. We share the best practices through our optimization process, and make the parameter public available for advancing translation research and applications. </p>
<blockquote>
<p>多语言翻译对于大型语言模型（LLM）来说是一项具有挑战性的任务，需要处理复杂的语言模式和自动化翻译中出现的生硬翻译。在本文中，我们介绍了Seed-X，这是一个包含指令和推理模型的大型开源LLM家族。它以7B的参数规模推动了翻译能力的新极限。基础模型在包含28种语言的单语和双语内容的多样化高质量数据集上进行预训练，充分利用了多语言数据的全部潜力。然后，指令模型通过链式思维（CoT）推理进行微调以进行翻译，并通过强化学习（RL）进一步增强，以实现跨不同语言对的更好泛化。Seed-X在28种语言上的性能与领先的闭源模型（包括Gemini-2.5和GPT-4o）相当，并且在自动指标和人类评估中都显著优于更大的开源模型。我们分享了优化过程中的最佳实践，并将参数公开，以促进翻译研究与应用的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13618v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在多语言翻译方面面临挑战，存在语言模式处理复杂和机械化翻译问题。本研究引入Seed-X，这是一个包含指令和推理模型的开源LLM家族，以7B参数规模推动翻译能力极限。基础模型在涵盖28种语言的单语和双语高质量数据集上进行预训练，充分利用多语言数据的潜力。通过链式思维（CoT）推理对指令模型进行微调，并通过强化学习（RL）进一步增强，以实现跨不同语言对的更好泛化。Seed-X在28种语言上的表现与领先的闭源模型相当，包括Gemini-2.5和GPT-4o，并且在自动指标和人类评估中都显著优于其他大型开源模型。我们分享了优化过程中的最佳实践，并公开了参数，以推动翻译研究与应用的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在多语言翻译上仍有挑战，需要处理复杂的语言模式和避免机械化翻译。</li>
<li>Seed-X是一个开源LLM家族，包含指令和推理模型，以7B参数推动翻译能力极限。</li>
<li>Seed-X基础模型在多种语言的丰富数据集上进行预训练，包括单语和双语内容。</li>
<li>通过链式思维（CoT）推理微调指令模型，并结合强化学习（RL）提高泛化能力。</li>
<li>Seed-X在多种语言上的表现与领先的闭源模型相当，并在自动评估和人类评估中优于其他开源模型。</li>
<li>Seed-X分享了优化过程中的最佳实践。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13618">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-46f214dffdb52afdfab105d1efa8541d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b181bd95f15522b2c31036781216122.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdfcbda71bc8ae8a28d1e2d35f409651.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-918c2974fa7f75c5bc3f4eeed2e35fa3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33ce00ff4302a357e6cda9165d963148.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LaViPlan-Language-Guided-Visual-Path-Planning-with-RLVR"><a href="#LaViPlan-Language-Guided-Visual-Path-Planning-with-RLVR" class="headerlink" title="LaViPlan : Language-Guided Visual Path Planning with RLVR"></a>LaViPlan : Language-Guided Visual Path Planning with RLVR</h2><p><strong>Authors:Hayeon Oh</strong></p>
<p>Out-of-distribution (OOD) scenarios in autonomous driving pose critical challenges, as planners often fail to generalize beyond their training experience, leading to unsafe or unexpected behavior. Vision-Language Models (VLMs) have shown promise in handling such scenarios by providing high-level scene understanding and user-aligned decisions. However, existing VLMs often exhibit a misalignment between their language-based reasoning and the low-level trajectories required for action-level planning. In this paper, we propose LaViPlan, a framework that leverages Reinforcement Learning with Verifiable Rewards (RLVR) to fine-tune VLMs using planning-oriented metrics. Experimental results show that LaViPlan improves planning performance across both in-domain and out-of-domain datasets. While linguistic fidelity slightly decreases after RLVR-based fine-tuning, qualitative evaluation indicates that the outputs remain coherent. We also conduct ablation studies to analyze the effects of sampling ratio and reasoning guidance, highlighting how these design choices influence performance. These findings demonstrate the potential of RLVR as a post-training paradigm for aligning language-guided reasoning with action-level planning in autonomous driving. </p>
<blockquote>
<p>自主驾驶中的分布外（OOD）场景带来重大挑战，因为规划器往往无法推广其训练经验，导致不安全或意外行为。视觉语言模型（VLM）在处理此类场景方面显示出潜力，通过提供高级场景理解和用户对齐的决策来实现。然而，现有的VLM经常在基于语言的推理和行动级规划所需的低级轨迹之间存在不匹配。在本文中，我们提出了LaViPlan框架，它利用强化学习与可验证奖励（RLVR）对VLM进行以规划为导向的微调。实验结果表明，LaViPlan在域内和域外数据集上均提高了规划性能。虽然基于RLVR的微调后语言保真度略有下降，但定性评估表明输出仍然连贯。我们还进行了消融研究，分析了采样率和推理指导的影响，突出了这些设计选择对性能的影响。这些发现展示了RLVR作为自主驾驶中语言指导推理与行动级规划对齐的后训练模式的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12911v4">PDF</a> Accepted to the 2nd ICCV 2025 Workshop on the Challenge of   Out-of-Label Hazards in Autonomous Driving (13 pages, 6 figures)</p>
<p><strong>Summary</strong></p>
<p>本文主要探讨了在自动驾驶领域，现有模型在面临超出训练范围的环境时的不足。提出了一种使用强化学习和可验证奖励（RLVR）对视觉语言模型（VLMs）进行精细调整的新框架LaViPlan。实验结果表明，LaViPlan在提升规划性能的同时，也确保了语言的一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动驾驶在面临超出训练范围（OOD）的场景时存在挑战，现有模型无法普遍应对此类情况。</li>
<li>Vision-Language Models（VLMs）展现出处理这些场景的能力，但存在语言推理与行动级别规划不匹配的问题。</li>
<li>LaViPlan框架通过强化学习和可验证奖励（RLVR）进行精细化训练，旨在解决上述问题。</li>
<li>实验结果显示，LaViPlan能提升在领域内外数据的规划性能。</li>
<li>虽然经过RLVR精细训练后语言保真度略有下降，但定性评估显示输出仍然连贯。</li>
<li>消融研究分析了采样比例和推理指导的影响，揭示了设计选择如何影响性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12911">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8638f8287a51dbd27cf841155fce7155.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-067f5f9c8813d180404fd11da0820fa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f45769cd734257fdcc2a6e581d9773a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74893f132b67c0d2afb1cf490b1162f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-694fa429f7723307e7c6e7aca376c924.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CoT-Segmenter-Enhancing-OOD-Detection-in-Dense-Road-Scenes-via-Chain-of-Thought-Reasoning"><a href="#CoT-Segmenter-Enhancing-OOD-Detection-in-Dense-Road-Scenes-via-Chain-of-Thought-Reasoning" class="headerlink" title="CoT-Segmenter: Enhancing OOD Detection in Dense Road Scenes via   Chain-of-Thought Reasoning"></a>CoT-Segmenter: Enhancing OOD Detection in Dense Road Scenes via   Chain-of-Thought Reasoning</h2><p><strong>Authors:Jeonghyo Song, Kimin Yun, DaeUng Jo, Jinyoung Kim, Youngjoon Yoo</strong></p>
<p>Effective Out-of-Distribution (OOD) detection is criti-cal for ensuring the reliability of semantic segmentation models, particularly in complex road environments where safety and accuracy are paramount. Despite recent advancements in large language models (LLMs), notably GPT-4, which significantly enhanced multimodal reasoning through Chain-of-Thought (CoT) prompting, the application of CoT-based visual reasoning for OOD semantic segmentation remains largely unexplored. In this paper, through extensive analyses of the road scene anomalies, we identify three challenging scenarios where current state-of-the-art OOD segmentation methods consistently struggle: (1) densely packed and overlapping objects, (2) distant scenes with small objects, and (3) large foreground-dominant objects. To address the presented challenges, we propose a novel CoT-based framework targeting OOD detection in road anomaly scenes. Our method leverages the extensive knowledge and reasoning capabilities of foundation models, such as GPT-4, to enhance OOD detection through improved image understanding and prompt-based reasoning aligned with observed problematic scene attributes. Extensive experiments show that our framework consistently outperforms state-of-the-art methods on both standard benchmarks and our newly defined challenging subset of the RoadAnomaly dataset, offering a robust and interpretable solution for OOD semantic segmentation in complex driving environments. </p>
<blockquote>
<p>有效的离分布（OOD）检测对于确保语义分割模型的可靠性至关重要，特别是在复杂道路环境中，安全性和准确性至关重要。尽管最近的大型语言模型（LLM）取得了进展，特别是GPT-4通过思维链（CoT）提示显著增强了多模态推理，但基于CoT的推理在OOD语义分割中的应用仍然鲜有探索。在本文中，通过对道路场景异常值的广泛分析，我们确定了三种当前最先进的OOD分割方法始终面临挑战的场景：（1）密集排列和重叠的物体，（2）远处的小物体场景，以及（3）大型前景主导物体。为了解决所面临的挑战，我们提出了一种针对道路异常场景OOD检测的新型CoT框架。我们的方法利用基础模型的广泛知识和推理能力，如GPT-4，通过改进的图像理解和与观察到的有问题的场景属性相匹配的提示推理，增强OOD检测。大量实验表明，我们的框架在标准基准测试和我们新定义的具有挑战性的RoadAnomaly数据集子集上均优于最新方法，为复杂驾驶环境中的OOD语义分割提供了稳健和可解释的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03984v2">PDF</a> 6 pages, 3 figures. Accepted at IEEE International Conference on   Advanced Visual and Signal-Based Systems 2025</p>
<p><strong>Summary</strong></p>
<p>本文研究了在复杂道路环境中，利用GPT-4等预训练大模型的多模态推理能力进行异常检测的重要性及其挑战。针对现有方法的不足，提出了基于CoT的OOD检测框架，用于解决密集重叠物体、远处小物体和前景大物体的三大难题场景。该框架在常规基准测试及新定义的RoadAnomaly数据集子集上的表现均超越现有方法，为复杂驾驶环境中的OOD语义分割提供了稳健且可解释的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>OOD检测对于确保语义分割模型的可靠性至关重要，特别是在复杂道路环境中。</li>
<li>当前先进的OOD分割方法在特定场景（如密集重叠物体、远处小物体和前景大物体）中存在挑战。</li>
<li>利用GPT-4等预训练大模型的多模态推理能力，通过Chain-of-Thought（CoT）提示增强OOD检测。</li>
<li>提出了一种基于CoT的OOD检测框架，用于解决道路异常场景中的OOD问题。</li>
<li>该框架通过利用基础模型的广泛知识和推理能力，改进图像理解并与观察到的场景属性对齐，从而进行提示推理。</li>
<li>框架在标准基准测试和新定义的RoadAnomaly数据集子集上的表现均优于现有方法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03984">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4e108e1404974e4e62012581e6595d80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f4f788b97cd31d96bdda5c818bb5332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e836bae2733818d9aab16f15aa8a7d27.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60a49f74a9fdb7737f2f07ea7a4ea70d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9d877a69fef7d159cb116ecbd56fe40.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-24/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-24/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-24/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-12283e0db00358e54a2015ff20ac05fe.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-08-24  LLM-empowered Dynamic Prompt Routing for Vision-Language Models Tuning   under Long-Tailed Distributions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3fc19417e0f1d62bfd6e8593cda009c7.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-08-23  From PREVENTion to REACTion Enhancing Failure Resolution in Naval   Systems
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28292.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
