<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="无监督/半监督/对比学习">
    <meta name="description" content="无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-10-01  Vision At Night Exploring Biologically Inspired Preprocessing For   Improved Robustness Via Color And Contrast Transformations">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>无监督/半监督/对比学习 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-fba12c6bf221a464a43bdf56db8dcbf2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">无监督/半监督/对比学习</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">无监督/半监督/对比学习</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                无监督/半监督/对比学习
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    36 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-01-更新"><a href="#2025-10-01-更新" class="headerlink" title="2025-10-01 更新"></a>2025-10-01 更新</h1><h2 id="Vision-At-Night-Exploring-Biologically-Inspired-Preprocessing-For-Improved-Robustness-Via-Color-And-Contrast-Transformations"><a href="#Vision-At-Night-Exploring-Biologically-Inspired-Preprocessing-For-Improved-Robustness-Via-Color-And-Contrast-Transformations" class="headerlink" title="Vision At Night: Exploring Biologically Inspired Preprocessing For   Improved Robustness Via Color And Contrast Transformations"></a>Vision At Night: Exploring Biologically Inspired Preprocessing For   Improved Robustness Via Color And Contrast Transformations</h2><p><strong>Authors:Lorena Stracke, Lia Nimmermann, Shashank Agnihotri, Margret Keuper, Volker Blanz</strong></p>
<p>Inspired by the human visual system’s mechanisms for contrast enhancement and color-opponency, we explore biologically motivated input preprocessing for robust semantic segmentation. By applying Difference-of-Gaussians (DoG) filtering to RGB, grayscale, and opponent-color channels, we enhance local contrast without modifying model architecture or training. Evaluations on Cityscapes, ACDC, and Dark Zurich show that such preprocessing maintains in-distribution performance while improving robustness to adverse conditions like night, fog, and snow. As this processing is model-agnostic and lightweight, it holds potential for integration into imaging pipelines, enabling imaging systems to deliver task-ready, robust inputs for downstream vision models in safety-critical environments. </p>
<blockquote>
<p>受人类视觉系统对比增强和颜色对立机制的启发，我们探索了用于稳健语义分割的生物激励输入预处理。通过对RGB、灰度图和颜色对立通道应用高斯差分（DoG）滤波，我们在不修改模型架构或训练的情况下增强了局部对比。在Cityscapes、ACDC和黑暗苏黎世上的评估表明，这种预处理在保持内部性能的同时，提高了对夜间、雾天和雪天等不利条件的稳健性。由于这种处理具有模型无关性和轻量化特点，因此有潜力集成到成像流水线中，使成像系统能够为安全关键环境中的下游视觉模型提供任务就绪的稳健输入。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24863v1">PDF</a> Accepted at the ICCV 2025 Workshop on Responsible Imaging</p>
<p><strong>Summary</strong><br>     该研究受人类视觉系统对比增强和颜色对抗机制的启发，探索了用于稳健语义分割的生物动机输入预处理。通过应用Difference-of-Gaussians（DoG）滤波对RGB、灰度图和对手色通道进行处理，可在不修改模型架构或训练的情况下增强局部对比。在Cityscapes、ACDC和Dark Zurich上的评估表明，这种预处理在保持内部分布性能的同时，提高了对夜晚、雾和雪等不利条件的鲁棒性。该处理具有模型通用性和轻量级特点，具有潜力融入成像流水线，使成像系统为安全关键环境中的下游视觉模型提供任务就绪的稳健输入。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究受人类视觉系统启发，探索了生物动机输入预处理以增强语义分割的稳健性。</li>
<li>应用Difference-of-Gaussians（DoG）滤波处理图像，增强局部对比。</li>
<li>处理过程不修改模型架构或训练，具有通用性和轻量级特点。</li>
<li>评估显示，该预处理在多种数据集上提高了模型对不利条件的鲁棒性。</li>
<li>预处理有助于在保持性能的同时，提高模型在不同环境条件下的适用性。</li>
<li>该方法具有潜力融入成像流水线，为下游视觉模型提供优化输入。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24863">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-47bff7eb1509fe4b8bdb165b86225037~resize:0:q75.jpg?source=1f5c5e47&expiration=1759928974&auth_key=1759928974-0-0-08196c98e48f49f14c0cc7edadc1c50a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-06f393ce506973342167b3bb1baf79fa.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-ce4ca412c4921ce809266efbfda007dc~resize:0:q75.jpg?source=1f5c5e47&expiration=1759928987&auth_key=1759928987-0-0-9622cf85c56fd25c55897b6714d16967&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-e9cf34848dcaa70de9b0769414934eb9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="An-Efficient-3D-Latent-Diffusion-Model-for-T1-contrast-Enhanced-MRI-Generation"><a href="#An-Efficient-3D-Latent-Diffusion-Model-for-T1-contrast-Enhanced-MRI-Generation" class="headerlink" title="An Efficient 3D Latent Diffusion Model for T1-contrast Enhanced MRI   Generation"></a>An Efficient 3D Latent Diffusion Model for T1-contrast Enhanced MRI   Generation</h2><p><strong>Authors:Zach Eidex, Mojtaba Safari, Jie Ding, Richard Qiu, Justin Roper, David Yu, Hui-Kuo Shu, Zhen Tian, Hui Mao, Xiaofeng Yang</strong></p>
<p>Objective: Gadolinium-based contrast agents (GBCAs) are commonly employed with T1w MRI to enhance lesion visualization but are restricted in patients at risk of nephrogenic systemic fibrosis and variations in GBCA administration can introduce imaging inconsistencies. This study develops an efficient 3D deep-learning framework to generate T1-contrast enhanced images (T1C) from pre-contrast multiparametric MRI. Approach: We propose the 3D latent rectified flow (T1C-RFlow) model for generating high-quality T1C images. First, T1w and T2-FLAIR images are input into a pretrained autoencoder to acquire an efficient latent space representation. A rectified flow diffusion model is then trained in this latent space representation. The T1C-RFlow model was trained on a curated dataset comprised of the BraTS 2024 glioma (GLI; 1480 patients), meningioma (MEN; 1141 patients), and metastases (MET; 1475 patients) datasets. Selected patients were split into train (N&#x3D;2860), validation (N&#x3D;612), and test (N&#x3D;614) sets. Results: Both qualitative and quantitative results demonstrate that the T1C-RFlow model outperforms benchmark 3D models (pix2pix, DDPM, Diffusion Transformers (DiT-3D)) trained in the same latent space. T1C-RFlow achieved the following metrics - GLI: NMSE 0.044 +&#x2F;- 0.047, SSIM 0.935 +&#x2F;- 0.025; MEN: NMSE 0.046 +&#x2F;- 0.029, SSIM 0.937 +&#x2F;- 0.021; MET: NMSE 0.098 +&#x2F;- 0.088, SSIM 0.905 +&#x2F;- 0.082. T1C-RFlow had the best tumor reconstruction performance and significantly faster denoising times (6.9 s&#x2F;volume, 200 steps) than conventional DDPM models in both latent space (37.7s, 1000 steps) and patch-based in image space (4.3 hr&#x2F;volume). Significance: Our proposed method generates synthetic T1C images that closely resemble ground truth T1C in much less time than previous diffusion models. Further development may permit a practical method for contrast-agent-free MRI for brain tumors. </p>
<blockquote>
<p>目标：钆类造影剂（GBCAs）常与T1加权MRI一起使用，以增强病变的可见性，但其在有发生肾源性系统性纤维化风险的患者中受到限制，并且GBCA管理的变化可能会引入成像不一致性。本研究开发了一种高效的3D深度学习框架，用于从预造影多参数MRI生成T1加权造影增强图像（T1C）。方法：我们提出了用于生成高质量T1C图像的3D潜在校正流（T1C-RFlow）模型。首先，将T1加权和T2-FLAIR图像输入预训练的自编码器，以获得有效的潜在空间表示。然后在此潜在空间表示中训练校正流扩散模型。T1C-RFlow模型是在精选数据集上训练的，该数据集包含BraTS 2024胶质细胞瘤（GLI；1480例患者）、脑膜瘤（MEN；1141例患者）和转移瘤（MET；1475例患者）数据集。将选定患者分为训练组（N&#x3D;2860）、验证组（N&#x3D;612）和测试组（N&#x3D;614）。结果：定性和定量结果均表明，T1C-RFlow模型在相同的潜在空间中超越了基准3D模型（pix2pix、DDPM、Diffusion Transformers（DiT-3D））的表现。在GLI中，T1C-RFlow实现了以下指标：NMSE 0.044±0.047，SSIM 0.935±0.025；在MEN中：NMSE 0.046±0.029，SSIM 0.937±0.021；在MET中：NMSE 0.098±0.088，SSIM 0.905±0.082。T1C-RFlow具有最佳的肿瘤重建性能，并且去噪时间显著更快（6.9秒&#x2F;体积，200步），与传统的潜在空间DDPM模型（37.7秒，1000步）和基于图像的图像空间补丁（4.3小时&#x2F;体积）相比。意义：我们提出的方法能够在比以前的扩散模型少得多的时间内生成模拟真实T1C的T1C图像。进一步的开发可能会提供一种无需造影剂的脑部肿瘤MRI检测方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24194v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本研究开发了一种高效的3D深度学习框架，用于从预对比的多参数MRI生成T1对比增强图像（T1C）。该研究使用T1加权和T2流液倒置恢复图像输入预训练的自编码器，获取有效的潜在空间表示。在此潜在空间表示中训练了校正流扩散模型。该模型在包括胶质细胞瘤、脑膜瘤和转移瘤的数据集上进行训练，并在这些疾病中表现出优异的性能。与在同一潜在空间中训练的基准3D模型相比，T1C-RFlow模型具有更高的性能，并且肿瘤重建性能最佳，去噪时间显著更快。本研究为生成合成T1C图像提供了一种新方法，这些图像在时间上更接近于真实T1C，为未来实现无造影剂MRI提供了可能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究提出了一种名为T1C-RFlow的3D深度学习框架，用于从预对比的多参数MRI生成T1对比增强图像（T1C）。</li>
<li>该框架使用自编码器和校正流扩散模型，在潜在空间表示中生成高质量的T1C图像。</li>
<li>T1C-RFlow在多种类型的脑部肿瘤数据集上进行了训练并表现出良好的性能。</li>
<li>T1C-RFlow模型相较于其他在相同潜在空间中训练的模型有更好的性能表现。</li>
<li>T1C-RFlow模型的肿瘤重建性能最佳，去噪时间显著更快。</li>
<li>该研究为无造影剂MRI的实现提供了可能，为未来的医学成像技术提供了新的视角。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24194">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3444a172b2cea0585f06be75ed38e0da.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-dbabea21c0f0464f457481cca0dec9fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759929009&auth_key=1759929009-0-0-e4424a9b515effa98a858ce063c75229&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Modality-Tailored-Graph-Modeling-Framework-for-Urban-Region-Representation-via-Contrastive-Learning"><a href="#A-Modality-Tailored-Graph-Modeling-Framework-for-Urban-Region-Representation-via-Contrastive-Learning" class="headerlink" title="A Modality-Tailored Graph Modeling Framework for Urban Region   Representation via Contrastive Learning"></a>A Modality-Tailored Graph Modeling Framework for Urban Region   Representation via Contrastive Learning</h2><p><strong>Authors:Yaya Zhao, Kaiqi Zhao, Zixuan Tang, Zhiyuan Liu, Xiaoling Lu, Yalei Du</strong></p>
<p>Graph-based models have emerged as a powerful paradigm for modeling multimodal urban data and learning region representations for various downstream tasks. However, existing approaches face two major limitations. (1) They typically employ identical graph neural network architectures across all modalities, failing to capture modality-specific structures and characteristics. (2) During the fusion stage, they often neglect spatial heterogeneity by assuming that the aggregation weights of different modalities remain invariant across regions, resulting in suboptimal representations. To address these issues, we propose MTGRR, a modality-tailored graph modeling framework for urban region representation, built upon a multimodal dataset comprising point of interest (POI), taxi mobility, land use, road element, remote sensing, and street view images. (1) MTGRR categorizes modalities into two groups based on spatial density and data characteristics: aggregated-level and point-level modalities. For aggregated-level modalities, MTGRR employs a mixture-of-experts (MoE) graph architecture, where each modality is processed by a dedicated expert GNN to capture distinct modality-specific characteristics. For the point-level modality, a dual-level GNN is constructed to extract fine-grained visual semantic features. (2) To obtain effective region representations under spatial heterogeneity, a spatially-aware multimodal fusion mechanism is designed to dynamically infer region-specific modality fusion weights. Building on this graph modeling framework, MTGRR further employs a joint contrastive learning strategy that integrates region aggregated-level, point-level, and fusion-level objectives to optimize region representations. Experiments on two real-world datasets across six modalities and three tasks demonstrate that MTGRR consistently outperforms state-of-the-art baselines, validating its effectiveness. </p>
<blockquote>
<p>基于图模型的框架已成为处理多模态城市数据的有力工具，并能够用于学习区域表示为多种下游任务提供服务。然而，现有方法面临两大局限。（1）它们通常使用所有模态的相同图神经网络架构，无法捕捉模态特定的结构和特征。（2）在融合阶段，它们往往会忽略空间异质性，假设不同模态的聚合权重在各个区域保持不变，从而导致次优表示。为了解决这些问题，我们提出了MTGRR，这是一种针对城市区域表示的模态定制的图建模框架，建立在包含兴趣点（POI）、出租车流动性、土地利用、道路元素、遥感和街道视图图像的多模态数据集之上。（1）MTGRR根据空间密度和数据特性将模态分为两组：聚合级和点级模态。对于聚合级模态，MTGRR采用混合专家（MoE）图架构，其中每个模态由专门的专家GNN处理以捕获不同的模态特定特征。对于点级模态，构建双级GNN以提取精细粒度的视觉语义特征。（2）为了在空间异质性下获得有效的区域表示，设计了一种空间感知的多模态融合机制，以动态推断特定区域的模态融合权重。基于这个图建模框架，MTGRR进一步采用联合对比学习策略，该策略结合了区域聚合级、点级和融合级目标来优化区域表示。在两个真实世界数据集上的实验证明了MTGRR在六个模态和三个任务上始终优于最新基线，验证了其有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23772v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种针对城市区域表示的模态定制图建模框架MTGRR，用于处理多模态城市数据。该框架解决了现有方法的两大局限性：一是未能捕捉模态特定结构和特性，二是在融合阶段忽略了空间异质性。MTGRR利用专门的图神经网络架构捕捉不同模态的特性，并设计了一种空间感知的多模态融合机制，以获取有效的区域表示。通过联合对比学习策略优化区域表示，实验证明MTGRR在真实世界数据集上的表现优于最新基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有图模型在处理多模态城市数据时面临两大挑战：缺乏模态特定结构特性的捕捉和在融合阶段忽略空间异质性。</li>
<li>MTGRR框架通过对模态进行分类，使用专门的图神经网络架构来捕捉不同模态的特性。</li>
<li>对于聚合级别的模态，MTGRR采用混合专家图架构，每个模态由专门的专家GNN处理，以捕捉不同的模态特性。</li>
<li>对于点级别模态，MTGRR构建了双级别GNN以提取精细的视觉语义特征。</li>
<li>MTGRR设计了一种空间感知的多模态融合机制，以获取有效的区域表示，该机制可以动态推断特定区域的模态融合权重。</li>
<li>MTGRR采用联合对比学习策略，结合区域聚合级别、点级别和融合级别的目标来优化区域表示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23772">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d033cf4a3b8fb034af1c2489c7926f71~resize:0:q75.jpg?source=1f5c5e47&expiration=1759929016&auth_key=1759929016-0-0-1bc623e999034604b16b867c0e335427&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-20817ae07959a683a99ca554be8739b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1759929024&auth_key=1759929024-0-0-a884a16ef6f5e0c76122ab1938efe8d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-2b410e03aed638e428749cf321c4b92f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GenView-Unifying-Adaptive-View-Generation-and-Quality-Driven-Supervision-for-Contrastive-Representation-Learning"><a href="#GenView-Unifying-Adaptive-View-Generation-and-Quality-Driven-Supervision-for-Contrastive-Representation-Learning" class="headerlink" title="GenView++: Unifying Adaptive View Generation and Quality-Driven   Supervision for Contrastive Representation Learning"></a>GenView++: Unifying Adaptive View Generation and Quality-Driven   Supervision for Contrastive Representation Learning</h2><p><strong>Authors:Xiaojie Li, Bei Wang, Jianlong Wu, Yue Yu, Liqiang Nie, Min Zhang</strong></p>
<p>The success of contrastive learning depends on the construction and utilization of high-quality positive pairs. However, current methods face critical limitations on two fronts: on the construction side, both handcrafted and generative augmentations often suffer from limited diversity and risk semantic corruption; on the learning side, the absence of a quality assessment mechanism leads to suboptimal supervision where all pairs are treated equally. To tackle these challenges, we propose GenView++, a unified framework that addresses both fronts by introducing two synergistic innovations. To improve pair construction, GenView++ introduces a multi-source adaptive view generation mechanism to synthesize diverse yet semantically coherent views by dynamically modulating generative parameters across image-conditioned, text-conditioned, and image-text-conditioned strategies. Second, a quality-driven contrastive learning mechanism assesses each pair’s semantic alignment and diversity to dynamically reweight their training contribution, prioritizing high-quality pairs while suppressing redundant or misaligned pairs. Extensive experiments demonstrate the effectiveness of GenView++ across both vision and vision-language tasks. For vision representation learning, it improves MoCov2 by +2.5% on ImageNet linear classification. For vision-language learning, it raises the average zero-shot classification accuracy by +12.31% over CLIP and +5.31% over SLIP across ten datasets, and further improves Flickr30k text retrieval R@5 by +3.2%. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xiaojieli0903/GenViewPlusPlus">https://github.com/xiaojieli0903/GenViewPlusPlus</a>. </p>
<blockquote>
<p>对比学习的成功取决于高质量正对的构建和利用。然而，当前的方法在两个方面都面临着关键的局限性：在构建方面，手工制作和生成式增强通常受限于有限的多样性，并存在语义损坏的风险；在学习方面，由于缺乏质量评估机制，所有对都被平等对待，导致监督效果不佳。为了解决这些挑战，我们提出了GenView++，这是一个通过引入两项协同创新来解决这些问题的统一框架。为提高配对构建，GenView++引入了一种多源自适应视图生成机制，通过动态调制图像条件、文本条件和图像文本混合条件下的生成参数，合成多样但语义连贯的视图。其次，质量驱动对比学习机制评估每对语义对齐和多样性，以动态调整其训练贡献的权重，优先高质量配对，同时抑制冗余或错位配对。大量实验表明，GenView++在视觉和视觉语言任务中均有效。在视觉表示学习中，它在ImageNet线性分类上提高了MoCov2的性能+2.5%。在视觉语言学习中，与CLIP相比提高了零样本分类平均准确度+12.31%，与SLIP相比提高了+5.31%，跨十个数据集；同时提高了Flickr30k文本检索的R@5指标+3.2%。代码可用在<a target="_blank" rel="noopener" href="https://github.com/xiaojieli0903/GenViewPlusPlus%E3%80%82">https://github.com/xiaojieli0903/GenViewPlusPlus。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23770v1">PDF</a> The code is available at   \url{<a target="_blank" rel="noopener" href="https://github.com/xiaojieli0903/GenViewPlusPlus%7D">https://github.com/xiaojieli0903/GenViewPlusPlus}</a></p>
<p><strong>Summary</strong></p>
<p>该论文探讨了对比学习中高质量正样本对的重要性及其构建和利用的挑战。针对当前方法的局限性，提出了一种名为GenView++的统一框架，通过两项协同创新来解决这些问题。一是改进对样本对的构建，通过多源自适应视图生成机制，动态调整生成参数，合成多样且语义连贯的视图；二是引入质量驱动对比学习机制，根据每对样本的语义对齐程度和多样性来动态调整训练贡献的权重，优先高质量样本对，抑制冗余或误对齐样本对。实验表明，GenView++在视觉和视觉语言任务中都取得了显著效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对比学习的成功取决于高质量正样本对的构建和利用。</li>
<li>当前方法在样本对构建和学习方面存在局限性。</li>
<li>GenView++通过两项协同创新解决这些问题：改进样本对构建和提高学习质量。</li>
<li>GenView++采用多源自适应视图生成机制，合成多样且语义连贯的视图。</li>
<li>GenView++通过质量驱动对比学习机制，优先高质量样本对，抑制冗余或误对齐样本对。</li>
<li>GenView++在视觉和视觉语言任务中取得了显著效果，如MoCov2在ImageNet上的线性分类提高了+2.5%，CLIP和SLIP在多个数据集上的零射击分类准确率分别提高了+12.31%和+5.31%，Flickr30k文本检索R@5提高了+3.2%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23770">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bba62e609720766c10f86677926b3439.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-fba12c6bf221a464a43bdf56db8dcbf2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759929048&auth_key=1759929048-0-0-4da600e5dde52abb18335484310128dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-730f34286895d806f4ae61a4b70395d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759929055&auth_key=1759929055-0-0-1428cf62473889652aaffd81c78314bf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-4bcf83faedf74898861267fb294e6221.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CCD-Mitigating-Hallucinations-in-Radiology-MLLMs-via-Clinical-Contrastive-Decoding"><a href="#CCD-Mitigating-Hallucinations-in-Radiology-MLLMs-via-Clinical-Contrastive-Decoding" class="headerlink" title="CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical   Contrastive Decoding"></a>CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical   Contrastive Decoding</h2><p><strong>Authors:Xi Zhang, Zaiqiao Meng, Jake Lever, Edmond S. L. Ho</strong></p>
<p>Multimodal large language models (MLLMs) have recently achieved remarkable progress in radiology by integrating visual perception with natural language understanding. However, they often generate clinically unsupported descriptions, known as medical hallucinations, which pose serious risks in medical applications that demand accuracy and image-grounded outputs. Through empirical analysis, we find that prompt-induced hallucinations remain prevalent in radiology MLLMs, largely due to over-sensitivity to clinical sections. To address this, we introduce Clinical Contrastive Cecoding (CCD), a training-free and retrieval-free inference framework that integrates structured clinical signals from task-specific radiology expert models. CCD introduces a dual-stage contrastive mechanism to refine token-level logits during generation, thereby enhancing clinical fidelity without modifying the base MLLM. Experiments on three datasets and multiple models demonstrate that CCD consistently improves overall performance on radiology report generation (RRG). On the MIMIC-CXR dataset, it yields up to a 17% improvement in RadGraph-F1 when applied to state-of-the-art RRG models. Our approach provides a lightweight and generalisable solution for mitigating medical hallucinations, effectively bridging expert models and MLLMs in radiology. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）最近通过整合视觉感知与自然语言理解在放射学领域取得了显著进展。然而，它们经常产生临床上不支持的描述，被称为医学幻觉，这在需要准确性和图像基础输出的医学应用中带来了严重的风险。通过实证分析，我们发现提示诱导的幻觉在放射学MLLM中仍然普遍存在，主要是由于对临床部分的过度敏感。为了解决这一问题，我们引入了临床对比编码（CCD），这是一种无需训练和检索的推理框架，它整合了特定任务放射学专家模型的结构化临床信号。CCD引入了一种双阶段对比机制，在生成过程中细化令牌级别的逻辑，从而提高临床保真度，而不会修改基础MLLM。在三个数据集和多个模型上的实验表明，CCD在放射学报告生成（RRG）方面始终提高总体性能。在MIMIC-CXR数据集上，当应用于最先进的RRG模型时，它在RadGraph-F1上提高了高达17%。我们的方法提供了一种轻便且通用的解决方案来缓解医学幻觉问题，有效地桥接了专家模型和MLLM在放射学领域的应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23379v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>本文介绍了多模态大型语言模型（MLLMs）在放射学领域的最新进展，它们通过融合视觉感知和自然语言理解取得了显著成效。然而，MLLMs常常产生未经临床支持的描述，即所谓的医学幻觉，这在需要精确性和图像基础输出的医学应用中带来了严重风险。研究发现，提示诱导的幻觉在放射学MLLMs中普遍存在，主要是由于对临床部分的过度敏感。为解决这一问题，本文提出了临床对比编码（CCD）技术，这是一种无需训练和检索的推断框架，它整合了来自特定任务放射学专家模型的结构化临床信号。CCD引入了一种双阶段对比机制，在生成过程中优化令牌级别的逻辑，从而提高临床保真度，同时不修改基础MLLM。实验表明，CCD在放射学报告生成方面表现优异，特别是在MIMIC-CXR数据集上应用最先进的RRG模型时，RadGraph-F1得分提高了17%。该方法为缓解医学幻觉问题提供了一种轻便且通用的解决方案，有效地架起了专家模型和MLLMs之间的桥梁。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在放射学领域通过融合视觉感知和自然语言理解取得了进展。</li>
<li>MLLMs常产生未经临床支持的描述（医学幻觉），存在临床风险。</li>
<li>医学幻觉在放射学MLLMs中普遍存在的原因是过度敏感于临床部分。</li>
<li>为解决上述问题，提出了临床对比编码（CCD）技术。</li>
<li>CCD是一种无需训练和检索的推断框架，整合了特定任务放射学专家模型的结构化临床信号。</li>
<li>CCD通过双阶段对比机制优化生成过程中的令牌级别逻辑，提高临床保真度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23379">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-37875193aa0b2021b77baebf70a1d78c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759929070&auth_key=1759929070-0-0-5f31cc381d9c48e86fc8e6f79456d33a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-3412e7b58bfccebaa5bc45f58e0bd04c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d2c1c62f194a8d3cd92f88de510f6d0.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-93aa9c93345d9824f91974d4dc0cf83b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759929091&auth_key=1759929091-0-0-18eeb472219afa7a88b6dcf6673565c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Benchmarking-DINOv3-for-Multi-Task-Stroke-Analysis-on-Non-Contrast-CT"><a href="#Benchmarking-DINOv3-for-Multi-Task-Stroke-Analysis-on-Non-Contrast-CT" class="headerlink" title="Benchmarking DINOv3 for Multi-Task Stroke Analysis on Non-Contrast CT"></a>Benchmarking DINOv3 for Multi-Task Stroke Analysis on Non-Contrast CT</h2><p><strong>Authors:Donghao Zhang, Yimin Chen, Kauê TN Duarte, Taha Aslan, Mohamed AlShamrani, Brij Karmur, Yan Wan, Shengcai Chen, Bo Hu, Bijoy K Menon, Wu Qiu</strong></p>
<p>Non-contrast computed tomography (NCCT) is essential for rapid stroke diagnosis but is limited by low image contrast and signal to noise ratio. We address this challenge by leveraging DINOv3, a state-of-the-art self-supervised vision transformer, to generate powerful feature representations for a comprehensive set of stroke analysis tasks. Our evaluation encompasses infarct and hemorrhage segmentation, anomaly classification (normal vs. stroke and normal vs. infarct vs. hemorrhage), hemorrhage subtype classification (EDH, SDH, SAH, IPH, IVH), and dichotomized ASPECTS classification (&lt;&#x3D;6 vs. &gt;6) on multiple public and private datasets. This study establishes strong benchmarks for these tasks and demonstrates the potential of advanced self-supervised models to improve automated stroke diagnosis from NCCT, providing a clear analysis of both the advantages and current constraints of the approach. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Zzz0251/DINOv3-stroke">https://github.com/Zzz0251/DINOv3-stroke</a>. </p>
<blockquote>
<p>非对比计算机断层扫描（NCCT）对于快速中风诊断至关重要，但由于图像对比度和信噪比低而受到限制。我们通过利用最先进的自监督视觉转换器DINOv3来解决这一挑战，为一系列中风分析任务生成强大的特征表示。我们的评估涵盖了梗死和出血分割、异常分类（正常与中风、正常与梗死与出血）、出血亚型分类（EDH、SDH、SAH、IPH、IVH），以及在多个公共和私有数据集上进行二分化的ASPECTS分类（&lt;&#x3D;6与&gt;6）。本研究为这些任务建立了强大的基准测试，证明了先进的自监督模型在提高非对比计算机断层扫描（NCCT）自动化中风诊断方面的潜力，同时提供了该方法的优势和当前局限性的清晰分析。代码可在<a target="_blank" rel="noopener" href="https://github.com/Zzz0251/DINOv3-stroke%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Zzz0251/DINOv3-stroke获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23132v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于非对比计算层析成像（NCCT）在快速卒中诊断中的重要性，但受限于图像对比度和信噪比的问题，研究团队利用先进的自监督视觉转换器DINOv3生成强大的特征表示，以解决这一问题。该研究在梗死、出血分割、异常分类（正常与卒中、正常与梗死与出血）、出血亚型分类（EDH、SDH、SAH、IPH、IVH）以及分级的ASPECTS分类（&lt;&#x3D;6与&gt; 6）等多个公共和私有数据集上进行了评估。研究确立了强有力的基准线，证明了先进的自监督模型在提高NCCT自动化卒中诊断方面的潜力，并对该方法的优点和当前限制进行了清晰分析。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>非对比计算层析成像（NCCT）在快速卒中诊断中扮演重要角色，但存在图像对比度和信噪比的问题。</li>
<li>研究使用DINOv3这一先进的自监督视觉转换器来解决上述问题，生成强大的特征表示。</li>
<li>研究涵盖了梗死和出血分割、异常分类、出血亚型分类等多个卒中分析任务。</li>
<li>在多个公共和私有数据集上进行了评估，确立了强有力的基准线。</li>
<li>先进自监督模型在改善NCCT自动化卒中诊断方面的潜力得到了验证。</li>
<li>研究提供了关于该方法的优点和限制的清晰分析。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23132">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-f826d3c1ebf8dd1236c94fb2c8711b1a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759929098&auth_key=1759929098-0-0-8ca1b1f2f6e1a7aa4d06c024812d759c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-767558bf210a2c44c02461d9508ec07f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759929105&auth_key=1759929105-0-0-0d2974683af7ea10149931e46fe162b1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-978a2955f5aad223db43ae52e33d009a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759929112&auth_key=1759929112-0-0-a427911aedcdd5242edc2eddc08a60c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Self-supervised-Contrastive-Learning-for-Multimodal-Text-Image-Analysis"><a href="#A-Survey-on-Self-supervised-Contrastive-Learning-for-Multimodal-Text-Image-Analysis" class="headerlink" title="A Survey on Self-supervised Contrastive Learning for Multimodal   Text-Image Analysis"></a>A Survey on Self-supervised Contrastive Learning for Multimodal   Text-Image Analysis</h2><p><strong>Authors:Asifullah Khan, Laiba Asmatullah, Anza Malik, Shahzaib Khan, Hamna Asif</strong></p>
<p>Self-supervised learning is a machine learning approach that generates implicit labels by learning underlined patterns and extracting discriminative features from unlabeled data without manual labelling. Contrastive learning introduces the concept of “positive” and “negative” samples, where positive pairs (e.g., variation of the same image&#x2F;object) are brought together in the embedding space, and negative pairs (e.g., views from different images&#x2F;objects) are pushed farther away. This methodology has shown significant improvements in image understanding and image text analysis without much reliance on labeled data. In this paper, we comprehensively discuss the terminologies, recent developments and applications of contrastive learning with respect to text-image models. Specifically, we provide an overview of the approaches of contrastive learning in text-image models in recent years. Secondly, we categorize the approaches based on different model structures. Thirdly, we further introduce and discuss the latest advances of the techniques used in the process such as pretext tasks for both images and text, architectural structures, and key trends. Lastly, we discuss the recent state-of-art applications of self-supervised contrastive learning Text-Image based models. </p>
<blockquote>
<p>自监督学习是一种机器学习的方法，它通过学习和识别潜在的模式，并从无标签的数据中提取辨别特征，生成隐含的标签，无需人工标注。对比学习引入了“正样本”和“负样本”的概念，其中正样本对（例如，同一图像&#x2F;对象的变体）被汇集到嵌入空间中，而负样本对（例如，来自不同图像&#x2F;对象的视图）则被推开得更远。这种方法在图像理解和图像文本分析方面取得了显著的改进，无需大量依赖标记数据。在本文中，我们全面探讨了与文本图像模型相关的对比学习的术语、最新发展以及应用。具体来说，我们概述了近年来文本图像模型中对比学习的方法。其次，我们根据不同的模型结构对这些方法进行了分类。此外，我们还进一步介绍了对比学习过程中的最新技术进展，如图像和文本的预训练任务、架构结构和关键趋势。最后，我们讨论了基于文本图像的最新先进的自监督对比学习的应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11101v4">PDF</a> 38 pages, 8 figures, survey paper</p>
<p><strong>Summary</strong><br>自我监督学习是一种机器学习的方法，通过隐式标签生成学习底层模式并从无标签数据中提取判别特征，无需手动标注。对比学习引入了“正样本”和“负样本”的概念，通过将同一图像或对象的不同变化作为正样本在嵌入空间中聚合，而将不同图像或对象的视图作为负样本推开，取得了显著成效。本文全面探讨了对比学习的术语、最新进展及其在文本图像模型中的应用。文章概述了近年来的文本图像模型对比学习方法，按模型结构分类，并介绍了最新的技术进展，如图像和文本的预训练任务、架构结构和关键趋势等。最后讨论了基于文本图像的自我监督对比学习的最新应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自我监督学习通过隐式标签学习底层模式并从无标签数据中提取判别特征。</li>
<li>对比学习是自我监督学习的一种形式，引入正样本和负样本的概念，以提升学习效果。</li>
<li>正样本指的是同一图像或对象的不同变化，在嵌入空间中聚合；负样本则是不同图像或对象的视图，被推开。</li>
<li>对比学习方法在图像理解和文本分析方面表现出显著的改进，减少了对标注数据的依赖。</li>
<li>文章提供了关于文本图像模型对比学习的全面概述，包括近年来的方法和按模型结构的分类。</li>
<li>介绍了最新的技术进展，包括图像和文本的预训练任务、架构结构和关键趋势等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11101">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-35b9b4a11f71855bba70d7517f93a159.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-2313a47b591bc59ade6a1c8862ed1fe2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759929126&auth_key=1759929126-0-0-e676857e3181018f07acbd9213c0ad91&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LoRACLR-Contrastive-Adaptation-for-Customization-of-Diffusion-Models"><a href="#LoRACLR-Contrastive-Adaptation-for-Customization-of-Diffusion-Models" class="headerlink" title="LoRACLR: Contrastive Adaptation for Customization of Diffusion Models"></a>LoRACLR: Contrastive Adaptation for Customization of Diffusion Models</h2><p><strong>Authors:Enis Simsar, Thomas Hofmann, Federico Tombari, Pinar Yanardag</strong></p>
<p>Recent advances in text-to-image customization have enabled high-fidelity, context-rich generation of personalized images, allowing specific concepts to appear in a variety of scenarios. However, current methods struggle with combining multiple personalized models, often leading to attribute entanglement or requiring separate training to preserve concept distinctiveness. We present LoRACLR, a novel approach for multi-concept image generation that merges multiple LoRA models, each fine-tuned for a distinct concept, into a single, unified model without additional individual fine-tuning. LoRACLR uses a contrastive objective to align and merge the weight spaces of these models, ensuring compatibility while minimizing interference. By enforcing distinct yet cohesive representations for each concept, LoRACLR enables efficient, scalable model composition for high-quality, multi-concept image synthesis. Our results highlight the effectiveness of LoRACLR in accurately merging multiple concepts, advancing the capabilities of personalized image generation. </p>
<blockquote>
<p>文本到图像的定制技术的最新进展已经实现了高保真、富含语境的个性化图像生成，允许特定概念出现在多种场景中。然而，当前的方法在结合多个个性化模型时遇到了困难，经常导致属性纠缠，或者需要单独的培训来保持概念的独特性。我们提出了LoRACLR，这是一种新的多概念图像生成方法，它将多个针对特定概念进行微调LoRA模型合并为一个单一、统一的模型，而无需额外的个别微调。LoRACLR使用对比目标来对齐和合并这些模型的权重空间，确保兼容性同时最小化干扰。通过实施清晰而连贯的表示，每个概念都有自己的特点，LoRACLR能够实现高效、可扩展的模型组合，用于高质量的多概念图像合成。我们的结果突出了LoRACLR在准确合并多个概念方面的有效性，提高了个性化图像生成的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09622v2">PDF</a> Accepted to CVPR’25. Project page: <a target="_blank" rel="noopener" href="https://loraclr.github.io/">https://loraclr.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了LoRACLR方法，这是一种将多个针对特定概念进行微调（fine-tuned）的LoRA模型合并为一个统一模型的技术。该方法使用对比目标（contrastive objective）来对齐和合并这些模型的权重空间，确保它们之间的兼容性并最小化干扰。通过为每个概念强制实施独特而连贯的表示，LoRACLR实现了高效、可扩展的模型组合，用于高质量的多概念图像合成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoRACLR是一种用于多概念图像生成的新方法，可以合并多个LoRA模型，每个模型针对一个特定概念进行微调。</li>
<li>LoRACLR使用对比目标来对齐和合并这些模型的权重空间，确保它们在合并过程中的兼容性和稳定性。</li>
<li>该方法通过强制实施独特且连贯的表示形式，为每个概念在图像合成中提供清晰的定义和表现。</li>
<li>LoRACLR不需要对每个模型进行单独的微调，从而提高了效率，并简化了多概念图像合成的过程。</li>
<li>该方法实现了高效、可扩展的模型组合，可以应用于高质量、多概念的图像合成。</li>
<li>LoRACLR在准确合并多个概念方面表现出良好的效果，进一步提高了个性化图像生成的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09622">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4968eb8eece4002896f21748b3823798~resize:0:q75.jpg?source=1f5c5e47&expiration=1759929134&auth_key=1759929134-0-0-a7e118da9c7ae72fdfc9326002d1e03d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-479d7c88d7ae4e7f6419306107da324f.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a269170193560a8faffa405e778c2eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759929149&auth_key=1759929149-0-0-aa21fe1ecde81345143c298a5fdc3e08&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-3fe3c18a3179948bcefb4103f7dd56c5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Similarity-Dissimilarity-Loss-for-Multi-label-Supervised-Contrastive-Learning"><a href="#Similarity-Dissimilarity-Loss-for-Multi-label-Supervised-Contrastive-Learning" class="headerlink" title="Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive   Learning"></a>Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive   Learning</h2><p><strong>Authors:Guangming Huang, Yunfei Long, Cunjin Luo</strong></p>
<p>Supervised contrastive learning has achieved remarkable success by leveraging label information; however, determining positive samples in multi-label scenarios remains a critical challenge. In multi-label supervised contrastive learning (MSCL), multi-label relations are not yet fully defined, leading to ambiguity in identifying positive samples and formulating contrastive loss functions to construct the representation space. To address these challenges, we: (i) systematically formulate multi-label relations in MSCL, (ii) propose a novel Similarity-Dissimilarity Loss, which dynamically re-weights samples based on similarity and dissimilarity factors, (iii) further provide theoretical grounded proofs for our method through rigorous mathematical analysis that supports the formulation and effectiveness, and (iv) offer a unified form and paradigm for both single-label and multi-label supervised contrastive loss. We conduct experiments on both image and text modalities and further extend the evaluation to the medical domain. The results show that our method consistently outperforms baselines in comprehensive evaluations, demonstrating its effectiveness and robustness. Moreover, the proposed approach achieves state-of-the-art performance on MIMIC-III-Full. </p>
<blockquote>
<p>监督对比学习通过利用标签信息取得了显著的成功；然而，在多标签场景中确定正样本仍然是一个关键挑战。在多标签监督对比学习（MSCL）中，多标签关系尚未得到充分定义，导致在识别正样本和制定对比损失函数以构建表示空间时存在模糊性。为了应对这些挑战，我们：（i）系统地制定了MSCL中的多标签关系，（ii）提出了一种新的相似性-差异性损失，该损失根据相似性和差异性因素动态地重新加权样本，（iii）通过严格的数学分析进一步为我们的方法提供了理论证明，支持其制定和有效性，（iv）为单标签和多标签监督对比损失提供了统一的形式和范式。我们在图像和文本模式上进行了实验，并将评估扩展到了医疗领域。结果表明，我们的方法在综合评估中始终优于基线，证明了其有效性和稳健性。此外，所提出的方法在MIMIC-III-Full上达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13439v5">PDF</a> </p>
<p><strong>Summary</strong><br>无监督对比学习取得了显著的成功，但在多标签场景中确定正样本是一个挑战。在多标签监督对比学习（MSCL）中，由于多标签关系尚未明确界定，导致难以确定正样本并构建对比损失函数以构建表示空间。为解决这些问题，我们系统地提出了多标签关系的界定、新的相似性-差异性损失函数设计，提供了理论支撑与数学分析证明其有效性，并给出了统一的多标签和单标签监督对比损失范式。实验结果表明，我们的方法在图像和文本模态上的表现均优于基线方法，且在医疗领域的表现尤其突出。我们的方法在MIMIC-III-Full数据集上取得了最优的性能表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多标签监督对比学习（MSCL）在确定正样本方面存在挑战，因为多标签关系尚未明确界定。</li>
<li>提出了一种新的相似性-差异性损失函数，根据相似性和差异性因素动态地重新加权样本。</li>
<li>通过严格的数学分析证明了我们方法的有效性和理论支撑。</li>
<li>提供了统一的多标签和单标签监督对比损失范式。</li>
<li>实验结果表明，该方法在图像和文本模态上的表现均优于基线方法。</li>
<li>在医疗领域的评估中，该方法表现出强大的性能表现，尤其在MIMIC-III-Full数据集上取得了最优表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13439">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c37097d8c7c0fe13d117059bd9df2408~resize:0:q75.jpg?source=1f5c5e47&expiration=1759929166&auth_key=1759929166-0-0-845f26574d3d1813988b7327d19acda4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e46d0be79e6af45377b46fb5bad8c26f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759929173&auth_key=1759929173-0-0-b355ae2ba58ffd3880122170303c341d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">无监督/半监督/对比学习</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-efb5bf9e1a25adeb0b4768bf6937bbf3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759929179&auth_key=1759929179-0-0-834736eed384594b750d5807e4eed1ec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-10-01  UESA-Net U-Shaped Embedded Multidirectional Shrinkage Attention Network   for Ultrasound Nodule Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-e91ca0fa49ab6cc348f37002c35a8f86~resize:0:q75.jpg?source=1f5c5e47&expiration=1759928787&auth_key=1759928787-0-0-ea09183c4f71c2c90149e679abd400d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-10-01  Evaluation of Polarimetric Fusion for Semantic Segmentation in Aquatic   Environments
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
