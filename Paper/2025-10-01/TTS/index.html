<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  MGM-Omni Scaling Omni LLMs to Personalized Long-Horizon Speech">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-6999087191f6f9a62755b349dfb59dbe.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    63 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-01-æ›´æ–°"><a href="#2025-10-01-æ›´æ–°" class="headerlink" title="2025-10-01 æ›´æ–°"></a>2025-10-01 æ›´æ–°</h1><h2 id="MGM-Omni-Scaling-Omni-LLMs-to-Personalized-Long-Horizon-Speech"><a href="#MGM-Omni-Scaling-Omni-LLMs-to-Personalized-Long-Horizon-Speech" class="headerlink" title="MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech"></a>MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech</h2><p><strong>Authors:Chengyao Wang, Zhisheng Zhong, Bohao Peng, Senqiao Yang, Yuqi Liu, Haokun Gui, Bin Xia, Jingyao Li, Bei Yu, Jiaya Jia</strong></p>
<p>We present MGM-Omni, a unified Omni LLM for omni-modal understanding and expressive, long-horizon speech generation. Unlike cascaded pipelines that isolate speech synthesis, MGM-Omni adopts a â€œbrain-mouthâ€ design with a dual-track, token-based architecture that cleanly decouples multimodal reasoning from real-time speech generation. This design enables efficient cross-modal interaction and low-latency, streaming speech generation. For understanding, a unified training strategy coupled with a dual audio encoder design enables long-form audio perception across diverse acoustic conditions. For generation, a chunk-based parallel decoding scheme narrows the text speech token-rate gap, accelerating inference and supporting streaming zero-shot voice cloning with stable timbre over extended durations. Compared to concurrent work, MGM-Omni achieves these capabilities with markedly data-efficient training. Extensive experiments demonstrate that MGM-Omni outperforms existing open source models in preserving timbre identity across extended sequences, producing natural and context-aware speech, and achieving superior long-form audio and omnimodal understanding. MGM-Omni establishes an efficient, end-to-end paradigm for omnimodal understanding and controllable, personalised long-horizon speech generation. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†MGM-Omniï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„Omni LLMï¼Œç”¨äºå…¨æ¨¡æ€ç†è§£å’Œè¡¨è¾¾ï¼Œä»¥åŠé•¿æœŸè§†é‡çš„è¯­éŸ³ç”Ÿæˆã€‚ä¸åŒäºéš”ç¦»è¯­éŸ³åˆæˆçš„çº§è”ç®¡é“ï¼ŒMGM-Omnié‡‡ç”¨â€œè„‘-å£â€è®¾è®¡ï¼Œå…·æœ‰åŒè½¨ã€åŸºäºä»¤ç‰Œçš„ç»“æ„ï¼Œèƒ½å¤Ÿå¹²å‡€åœ°å°†å¤šæ¨¡æ€æ¨ç†ä¸å®æ—¶è¯­éŸ³ç”Ÿæˆåˆ†ç¦»å¼€æ¥ã€‚è¿™ç§è®¾è®¡å®ç°äº†é«˜æ•ˆçš„è·¨æ¨¡æ€äº¤äº’å’Œä½å»¶è¿Ÿã€æµå¼è¯­éŸ³ç”Ÿæˆã€‚åœ¨ç†è§£æ–¹é¢ï¼Œé€šè¿‡ç»Ÿä¸€çš„è®­ç»ƒç­–ç•¥å’ŒåŒéŸ³é¢‘ç¼–ç å™¨è®¾è®¡ï¼Œèƒ½å¤Ÿå®ç°å¯¹ä¸åŒå£°å­¦æ¡ä»¶ä¸‹é•¿éŸ³é¢‘çš„æ„ŸçŸ¥ã€‚åœ¨ç”Ÿæˆæ–¹é¢ï¼ŒåŸºäºåˆ†å—çš„å¹¶è¡Œè§£ç æ–¹æ¡ˆç¼©å°äº†æ–‡æœ¬è¯­éŸ³ä»¤ç‰Œç‡å·®è·ï¼ŒåŠ é€Ÿäº†æ¨ç†ï¼Œå¹¶æ”¯æŒæ‰©å±•æŒç»­æ—¶é—´ä¸‹çš„ç¨³å®šéŸ³è‰²é›¶æ ·æœ¬è¯­éŸ³å…‹éš†çš„æµå¼ä¼ è¾“ã€‚ä¸åŒæœŸå·¥ä½œç›¸æ¯”ï¼ŒMGM-Omniåœ¨æ•°æ®é«˜æ•ˆè®­ç»ƒæ–¹é¢å®ç°äº†è¿™äº›åŠŸèƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMGM-Omniåœ¨ä¿ç•™æ‰©å±•åºåˆ—ä¸­çš„éŸ³è‰²èº«ä»½ã€äº§ç”Ÿè‡ªç„¶å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯­éŸ³ã€å®ç°é•¿éŸ³é¢‘å’Œå…¨èƒ½ç†è§£æ–¹é¢ä¼˜äºç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚MGM-Omniä¸ºå…¨æ¨¡æ€ç†è§£å’Œå¯æ§ã€ä¸ªæ€§åŒ–çš„é•¿æœŸè§†é‡è¯­éŸ³ç”Ÿæˆå»ºç«‹äº†é«˜æ•ˆã€ç«¯åˆ°ç«¯çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25131v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/dvlab-research/MGM-Omni">https://github.com/dvlab-research/MGM-Omni</a></p>
<p><strong>Summary</strong></p>
<p>MGM-Omniæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å…¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå…¨æ¨¡æ€ç†è§£å’Œè¡¨è¾¾ã€é•¿æœŸè¯­éŸ³ç”Ÿæˆã€‚å®ƒé‡‡ç”¨â€œè„‘-å£â€è®¾è®¡ï¼Œå…·æœ‰åŒè½¨ã€åŸºäºä»¤ç‰Œæ¶æ„ï¼Œå¹²å‡€åœ°è§£è€¦äº†å¤šæ¨¡æ€æ¨ç†å’Œå®æ—¶è¯­éŸ³ç”Ÿæˆã€‚è¿™ç§è®¾è®¡å®ç°äº†é«˜æ•ˆçš„è·¨æ¨¡æ€äº¤äº’å’Œä½å»¶è¿Ÿã€æµå¼è¯­éŸ³ç”Ÿæˆã€‚ç»Ÿä¸€è®­ç»ƒç­–ç•¥ä¸åŒéŸ³é¢‘ç¼–ç å™¨è®¾è®¡ç›¸ç»“åˆï¼Œå®ç°äº†è·¨å„ç§å£°å­¦æ¡ä»¶çš„é•¿éŸ³é¢‘æ„ŸçŸ¥ã€‚åŸºäºåˆ†å—çš„å¹¶è¡Œè§£ç æ–¹æ¡ˆç¼©å°äº†æ–‡æœ¬è¯­éŸ³ä»¤ç‰Œç‡å·®è·ï¼ŒåŠ é€Ÿäº†æ¨ç†ï¼Œå¹¶æ”¯æŒåœ¨æ‰©å±•æŒç»­æ—¶é—´ä¸Šè¿›è¡Œç¨³å®šçš„éŸ³è‰²é›¶å°„å‡»å£°å…‹éš†ã€‚ä¸åŒæœŸå·¥ä½œç›¸æ¯”ï¼ŒMGM-Omniä»¥æ˜¾è‘—çš„æ•°æ®é«˜æ•ˆè®­ç»ƒå®ç°äº†è¿™äº›åŠŸèƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒMGM-Omniåœ¨æ‰©å±•åºåˆ—ä¸­ä¿ç•™éŸ³è‰²èº«ä»½ã€äº§ç”Ÿè‡ªç„¶å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯­éŸ³ã€å®ç°é•¿æœŸéŸ³é¢‘å’Œå…¨èƒ½ç†è§£æ–¹é¢ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ã€‚MGM-Omniä¸ºå…¨èƒ½ç†è§£å’Œå¯æ§ã€ä¸ªæ€§åŒ–çš„é•¿æœŸè¯­éŸ³ç”Ÿæˆå»ºç«‹äº†é«˜æ•ˆã€ç«¯åˆ°ç«¯çš„èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MGM-Omniæ˜¯ä¸€ä¸ªå…¨æ¨¡æ€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå…¨æ¨¡æ€ç†è§£å’Œè¡¨è¾¾ã€é•¿æœŸè¯­éŸ³ç”Ÿæˆã€‚</li>
<li>é‡‡ç”¨â€œè„‘-å£â€è®¾è®¡å’ŒåŒè½¨ã€åŸºäºä»¤ç‰Œçš„æ¶æ„ï¼Œå®ç°å¤šæ¨¡æ€æ¨ç†å’Œå®æ—¶è¯­éŸ³ç”Ÿæˆçš„è§£è€¦ã€‚</li>
<li>åŒéŸ³é¢‘ç¼–ç å™¨è®¾è®¡å’Œç»Ÿä¸€è®­ç»ƒç­–ç•¥æå‡äº†é•¿éŸ³é¢‘æ„ŸçŸ¥èƒ½åŠ›ï¼Œé€‚åº”å„ç§å£°å­¦æ¡ä»¶ã€‚</li>
<li>åŸºäºåˆ†å—çš„å¹¶è¡Œè§£ç æ–¹æ¡ˆç¼©å°äº†æ–‡æœ¬è¯­éŸ³ä»¤ç‰Œç‡å·®è·ï¼ŒåŠ é€Ÿäº†æ¨ç†ï¼Œæ”¯æŒæµå¼é›¶å°„å‡»å£°å…‹éš†ã€‚</li>
<li>MGM-Omniå®ç°äº†é«˜æ•ˆã€ç«¯åˆ°ç«¯çš„å…¨èƒ½ç†è§£å’Œå¯æ§ã€ä¸ªæ€§åŒ–çš„é•¿æœŸè¯­éŸ³ç”ŸæˆèŒƒå¼ã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼ŒMGM-Omniåœ¨æ‰©å±•åºåˆ—ä¸­è¡¨ç°ä¼˜è¶Šï¼Œèƒ½å¤Ÿä¿ç•™éŸ³è‰²èº«ä»½ã€äº§ç”Ÿè‡ªç„¶å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯­éŸ³ã€‚</li>
<li>MGM-Omniçš„è®­ç»ƒæ˜¯æ•°æ®é«˜æ•ˆçš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a4bf240bf02f7abe13ac7cfa86369eda~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931918&auth_key=1759931918-0-0-0b629c850a89e49511169395f3ba1729&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-72e6a50b2c96bd4a0f4b2ff527f4df47.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-16d02220684bdda929365bfec3d70255~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931932&auth_key=1759931932-0-0-e93d37ddae53c365686c4e22e8174dbf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-628059bf96623bb1dbbbf4edee4d2a4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931939&auth_key=1759931939-0-0-0df3f406fc367e4c9d1b8000dbb492c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1ca188737fae4d2142e5d079c6630047~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931946&auth_key=1759931946-0-0-572fcf7c9e34fcbd24d9c775cab941e9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VSSFlow-Unifying-Video-conditioned-Sound-and-Speech-Generation-via-Joint-Learning"><a href="#VSSFlow-Unifying-Video-conditioned-Sound-and-Speech-Generation-via-Joint-Learning" class="headerlink" title="VSSFlow: Unifying Video-conditioned Sound and Speech Generation via   Joint Learning"></a>VSSFlow: Unifying Video-conditioned Sound and Speech Generation via   Joint Learning</h2><p><strong>Authors:Xin Cheng, Yuyue Wang, Xihua Wang, Yihan Wu, Kaisi Guan, Yijing Chen, Peng Zhang, Xiaojiang Liu, Meng Cao, Ruihua Song</strong></p>
<p>Video-conditioned sound and speech generation, encompassing video-to-sound (V2S) and visual text-to-speech (VisualTTS) tasks, are conventionally addressed as separate tasks, with limited exploration to unify them within a signle framework. Recent attempts to unify V2S and VisualTTS face challenges in handling distinct condition types (e.g., heterogeneous video and transcript conditions) and require complex training stages. Unifying these two tasks remains an open problem. To bridge this gap, we present VSSFlow, which seamlessly integrates both V2S and VisualTTS tasks into a unified flow-matching framework. VSSFlow uses a novel condition aggregation mechanism to handle distinct input signals. We find that cross-attention and self-attention layer exhibit different inductive biases in the process of introducing condition. Therefore, VSSFlow leverages these inductive biases to effectively handle different representations: cross-attention for ambiguous video conditions and self-attention for more deterministic speech transcripts. Furthermore, contrary to the prevailing belief that joint training on the two tasks requires complex training strategies and may degrade performance, we find that VSSFlow benefits from the end-to-end joint learning process for sound and speech generation without extra designs on training stages. Detailed analysis attributes it to the learned general audio prior shared between tasks, which accelerates convergence, enhances conditional generation, and stabilizes the classifier-free guidance process. Extensive experiments demonstrate that VSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S and VisualTTS benchmarks, underscoring the critical potential of unified generative models. </p>
<blockquote>
<p>è§†é¢‘æ¡ä»¶å£°éŸ³å’Œè¯­éŸ³ç”Ÿæˆæ¶µç›–äº†è§†é¢‘åˆ°å£°éŸ³ï¼ˆV2Sï¼‰å’Œè§†è§‰æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆVisualTTSï¼‰ä»»åŠ¡ï¼Œé€šå¸¸è¢«è§†ä¸ºå•ç‹¬çš„ä»»åŠ¡æ¥å¤„ç†ï¼Œå¯¹äºåœ¨å•ä¸€æ¡†æ¶å†…ç»Ÿä¸€å®ƒä»¬çš„æ¢ç´¢æœ‰é™ã€‚æœ€è¿‘å°è¯•å°†V2Så’ŒVisualTTSç»Ÿä¸€èµ·æ¥ï¼Œåœ¨å¤„ç†ä¸åŒæ¡ä»¶ç±»å‹ï¼ˆä¾‹å¦‚ï¼Œä¸åŒçš„è§†é¢‘å’Œæ–‡æœ¬æ¡ä»¶ï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶éœ€è¦å¤æ‚çš„è®­ç»ƒé˜¶æ®µã€‚ç»Ÿä¸€è¿™ä¸¤ä¸ªä»»åŠ¡ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„é—®é¢˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†VSSFlowï¼Œå®ƒæ— ç¼åœ°å°†V2Så’ŒVisualTTSä»»åŠ¡é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æµåŒ¹é…æ¡†æ¶ä¸­ã€‚VSSFlowä½¿ç”¨äº†ä¸€ç§æ–°å‹çš„æ¡ä»¶èšåˆæœºåˆ¶æ¥å¤„ç†ä¸åŒçš„è¾“å…¥ä¿¡å·ã€‚æˆ‘ä»¬å‘ç°ï¼Œäº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å±‚åœ¨å¼•å…¥æ¡ä»¶çš„è¿‡ç¨‹ä¸­è¡¨ç°å‡ºä¸åŒçš„å½’çº³åç½®ã€‚å› æ­¤ï¼ŒVSSFlowåˆ©ç”¨è¿™äº›å½’çº³åç½®æ¥æœ‰æ•ˆåœ°å¤„ç†ä¸åŒçš„è¡¨ç¤ºï¼šäº¤å‰æ³¨æ„åŠ›ç”¨äºæ¨¡ç³Šçš„è§†é¢‘æ¡ä»¶ï¼Œè‡ªæ³¨æ„åŠ›ç”¨äºæ›´ç¡®å®šçš„è¯­éŸ³æ–‡æœ¬ã€‚æ­¤å¤–ï¼Œä¸æ™®éè®¤ä¸ºçš„è”åˆè®­ç»ƒè¿™ä¸¤ä¸ªä»»åŠ¡éœ€è¦å¤æ‚çš„è®­ç»ƒç­–ç•¥å¹¶å¯èƒ½é™ä½æ€§èƒ½ç›¸åï¼Œæˆ‘ä»¬å‘ç°VSSFlowå—ç›Šäºå£°éŸ³å’Œè¯­éŸ³ç”Ÿæˆçš„ç«¯åˆ°ç«¯è”åˆå­¦ä¹ è¿‡ç¨‹ï¼Œè€Œæ— éœ€åœ¨è®­ç»ƒé˜¶æ®µè¿›è¡Œé¢å¤–è®¾è®¡ã€‚è¯¦ç»†åˆ†æå°†å…¶å½’å› äºä»»åŠ¡é—´å­¦åˆ°çš„å…±äº«éŸ³é¢‘å…ˆéªŒï¼Œè¿™åŠ é€Ÿäº†æ”¶æ•›ï¼Œå¢å¼ºäº†æ¡ä»¶ç”Ÿæˆï¼Œå¹¶ç¨³å®šäº†æ— åˆ†ç±»å™¨æŒ‡å¯¼è¿‡ç¨‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVSSFlowåœ¨V2Så’ŒVisualTTSåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æœ€å…ˆè¿›çš„é¢†åŸŸç‰¹å®šåŸºçº¿ï¼Œçªæ˜¾å‡ºç»Ÿä¸€ç”Ÿæˆæ¨¡å‹çš„å…³é”®æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24773v1">PDF</a> Paper Under Review</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºVSSFlowçš„ç»Ÿä¸€æ¡†æ¶ï¼Œå°†è§†é¢‘åˆ°å£°éŸ³ï¼ˆV2Sï¼‰å’Œè§†è§‰æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆVisualTTSï¼‰ä¸¤ä¸ªä»»åŠ¡æ— ç¼é›†æˆã€‚è¯¥æ¡†æ¶ä½¿ç”¨æ–°å‹æ¡ä»¶èšåˆæœºåˆ¶å¤„ç†ä¸åŒçš„è¾“å…¥ä¿¡å·ã€‚é€šè¿‡åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å±‚çš„ä¸åŒè¯±å¯¼åè§ï¼ŒVSSFlowèƒ½æœ‰æ•ˆå¤„ç†ä¸åŒçš„è¡¨å¾ï¼šäº¤å‰æ³¨æ„åŠ›ç”¨äºå¤„ç†æ¨¡ç³Šçš„è§†é¢‘æ¡ä»¶ï¼Œè‡ªæ³¨æ„åŠ›ç”¨äºå¤„ç†æ›´ç¡®å®šçš„è¯­éŸ³æ–‡æœ¬ã€‚ç ”ç©¶å‘ç°ï¼Œç›¸è¾ƒäºå¤æ‚çš„è®­ç»ƒç­–ç•¥å¯èƒ½ä¼šé™ä½æ€§èƒ½çš„è§‚ç‚¹ç›¸åï¼ŒVSSFlowä»ç«¯åˆ°ç«¯çš„è”åˆå­¦ä¹ è¿‡ç¨‹å—ç›Šï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒé˜¶æ®µè®¾è®¡å³å¯ç”Ÿæˆå£°éŸ³å’Œè¯­éŸ³ã€‚è¿™å½’åŠŸäºä»»åŠ¡é—´å…±äº«çš„é€šç”¨éŸ³é¢‘å…ˆéªŒçŸ¥è¯†ï¼Œå…¶åŠ é€Ÿæ”¶æ•›ã€å¢å¼ºæ¡ä»¶ç”Ÿæˆå¹¶ç¨³å®šæ— åˆ†ç±»å™¨æŒ‡å¯¼è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒVSSFlowåœ¨V2Så’ŒVisualTTSåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æœ€æ–°çš„é¢†åŸŸç‰¹å®šåŸºçº¿ï¼Œçªæ˜¾äº†ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹çš„å…³é”®æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VSSFlowæ¡†æ¶ç»Ÿä¸€äº†è§†é¢‘åˆ°å£°éŸ³ï¼ˆV2Sï¼‰å’Œè§†è§‰æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆVisualTTSï¼‰ä»»åŠ¡ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡ä»¶èšåˆæœºåˆ¶æ¥å¤„ç†ä¸åŒçš„è¾“å…¥ä¿¡å·ã€‚</li>
<li>åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å±‚çš„ä¸åŒè¯±å¯¼åè§æ¥å¤„ç†ä¸åŒçš„è¡¨å¾ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œè”åˆå­¦ä¹ ä¸¤ä¸ªä»»åŠ¡çš„è¿‡ç¨‹èƒ½ä»ç«¯åˆ°ç«¯çš„è®­ç»ƒè¿‡ç¨‹ä¸­å—ç›Šï¼Œä¸”æ— éœ€å¤æ‚çš„è®­ç»ƒç­–ç•¥æˆ–é¢å¤–çš„è®­ç»ƒé˜¶æ®µè®¾è®¡ã€‚</li>
<li>ä»»åŠ¡é—´å…±äº«çš„é€šç”¨éŸ³é¢‘å…ˆéªŒçŸ¥è¯†å¯¹åŠ é€Ÿæ”¶æ•›ã€å¢å¼ºæ¡ä»¶ç”Ÿæˆå’Œç¨³å®šæŒ‡å¯¼è¿‡ç¨‹èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>å®éªŒè¯æ˜VSSFlowåœ¨V2Så’ŒVisualTTSçš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f8ca6153c228acbaa456cf6931821840.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa383b8af756e9c393499fe593a9596d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3beec9a7fa106c34c7d5761339c47c73.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LatentEvolve-Self-Evolving-Test-Time-Scaling-in-Latent-Space"><a href="#LatentEvolve-Self-Evolving-Test-Time-Scaling-in-Latent-Space" class="headerlink" title="LatentEvolve: Self-Evolving Test-Time Scaling in Latent Space"></a>LatentEvolve: Self-Evolving Test-Time Scaling in Latent Space</h2><p><strong>Authors:Guibin Zhang, Fanci Meng, Guancheng Wan, Zherui Li, Kun Wang, Zhenfei Yin, Lei Bai, Shuicheng Yan</strong></p>
<p>Test-time Scaling (TTS) has been demonstrated to significantly enhance the reasoning capabilities of Large Language Models (LLMs) during the inference phase without altering model parameters. However, existing TTS methods are largely independent, implying that LLMs have not yet evolved to progressively learn how to scale more effectively. With the objective of evolving LLMs to learn &#96;&#96;how to scale test-time computation,â€™â€™ we propose LatentEvolve, a self-evolving latent TTS framework inspired by the complementary learning system (CLS) theory. Analogous to the human brainâ€™s dual system of a fast-recall hippocampus and a slow-consolidating neocortex, LatentEvolve comprises two evolutionary components: \textit{daytime scaling}, which rapidly retrieves historical latent representations to better guide current LLM reasoning; and \textit{nighttime scaling}, which integrates past latent optimizations in a manner akin to the human brainâ€™s consolidation of experiences during sleep. The alternation of daytime and nighttime processes facilitates a fast and slow evolution of LLM TTS, mirroring human cognitive dynamics in a fully unsupervised manner. Extensive experiments across eight benchmarks and five model backbones demonstrate that our LatentEvolve surpasses state-of-the-art TTS methods such as LatentSeek and TTRL by up to $13.33%$ and exhibits exceptional cross-domain and cross-backbone generalization. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰å·²è¯æ˜å¯ä»¥æ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†é˜¶æ®µçš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€æ›´æ”¹æ¨¡å‹å‚æ•°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„TTSæ–¹æ³•æ˜¯ç›¸å¯¹ç‹¬ç«‹çš„ï¼Œè¿™æ„å‘³ç€LLMå°šæœªè¿›åŒ–åˆ°é€æ­¥å­¦ä¹ å¦‚ä½•æ›´æœ‰æ•ˆåœ°ç¼©æ”¾ã€‚ä¸ºäº†è¿›åŒ–LLMä»¥å­¦ä¹ â€œå¦‚ä½•åœ¨æµ‹è¯•æ—¶è®¡ç®—ç¼©æ”¾â€ï¼Œæˆ‘ä»¬æå‡ºäº†LatentEvolveï¼Œè¿™æ˜¯ä¸€ç§å—è¾…åŠ©å­¦ä¹ ç³»ç»Ÿï¼ˆCLSï¼‰ç†è®ºå¯å‘çš„è‡ªæˆ‘è¿›åŒ–çš„æ½œåœ¨TTSæ¡†æ¶ã€‚ç±»ä¼¼äºäººè„‘çš„å¿«é€Ÿå›å¿†æµ·é©¬ä½“å’Œç¼“æ…¢å·©å›ºçš„æ–°çš®å±‚ç»„æˆçš„åŒé‡ç³»ç»Ÿï¼ŒLatentEvolveåŒ…å«ä¸¤ä¸ªè¿›åŒ–ç»„ä»¶ï¼šæ—¥é—´ç¼©æ”¾ï¼Œç”¨äºå¿«é€Ÿæ£€ç´¢å†å²æ½œåœ¨è¡¨ç¤ºï¼Œä»¥æ›´å¥½åœ°æŒ‡å¯¼å½“å‰LLMæ¨ç†ï¼›å¤œé—´ç¼©æ”¾ï¼Œä»¥ç±»ä¼¼äºäººè„‘åœ¨ç¡çœ ä¸­æ•´åˆè¿‡å»çš„æ½œåœ¨ä¼˜åŒ–çš„æ–¹å¼æ•´åˆè¿‡å»çš„æ½œåœ¨ä¼˜åŒ–ã€‚æ—¥é—´å’Œå¤œé—´è¿‡ç¨‹çš„äº¤æ›¿ä¿ƒè¿›äº†LLM TTSçš„å¿«é€Ÿå’Œç¼“æ…¢è¿›åŒ–ï¼Œä»¥å®Œå…¨æ— ç›‘ç£çš„æ–¹å¼åæ˜ äººç±»è®¤çŸ¥åŠ¨æ€ã€‚åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•å’Œäº”ä¸ªæ¨¡å‹ä¸»å¹²ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„LatentEvolveè¶…è¶Šäº†æœ€å…ˆè¿›çš„TTSæ–¹æ³•ï¼Œå¦‚LatentSeekå’ŒTTRLï¼Œæœ€é«˜æå‡äº†13.33ï¼…ï¼Œå¹¶è¡¨ç°å‡ºå‡ºè‰²çš„è·¨åŸŸå’Œè·¨ä¸»å¹²æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24771v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰æŠ€æœ¯çš„æ½œåœ¨æ¼”åŒ–æ¡†æ¶LatentEvolveï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…å«æ—¥é—´å’Œå¤œé—´ä¸¤ç§ç¼©æ”¾æ–¹å¼ï¼Œåˆ†åˆ«å¿«é€Ÿæ£€ç´¢å†å²æ½œåœ¨è¡¨å¾å’Œå¼•å¯¼å½“å‰LLMæ¨ç†ï¼Œä»¥åŠæ•´åˆè¿‡å»æ½œåœ¨ä¼˜åŒ–ã€‚é€šè¿‡å…«é¡¹åŸºå‡†æµ‹è¯•å’Œäº”ç§æ¨¡å‹éª¨å¹²çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLatentEvolveè¶…è¶Šäº†ç°æœ‰TTSæ–¹æ³•ï¼Œå…·æœ‰å‡ºè‰²çš„è·¨åŸŸå’Œè·¨æ¨¡å‹éª¨å¹²çš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Test-time Scaling (TTS)æŠ€æœ¯å¯æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä¸”æ— éœ€æ›´æ”¹æ¨¡å‹å‚æ•°ã€‚</li>
<li>ç°æœ‰TTSæ–¹æ³•å¤§å¤šæ˜¯ç‹¬ç«‹çš„ï¼ŒLLMå°šæœªå­¦ä¼šå¦‚ä½•æ›´æœ‰æ•ˆåœ°ç¼©æ”¾ã€‚</li>
<li>LatentEvolveæ¡†æ¶å—äººç±»å¤§è„‘çš„äº’è¡¥å­¦ä¹ ç³»ç»Ÿï¼ˆCLSï¼‰ç†è®ºå¯å‘ï¼ŒåŒ…å«æ—¥é—´å’Œå¤œé—´ä¸¤ç§ç¼©æ”¾æ–¹å¼ã€‚</li>
<li>æ—¥é—´ç¼©æ”¾å¿«é€Ÿæ£€ç´¢å†å²æ½œåœ¨è¡¨å¾ä»¥æŒ‡å¯¼å½“å‰LLMæ¨ç†ã€‚</li>
<li>å¤œé—´ç¼©æ”¾æ•´åˆè¿‡å»çš„æ½œåœ¨ä¼˜åŒ–ï¼Œç±»ä¼¼äºäººç±»å¤§è„‘åœ¨ç¡çœ ä¸­å·©å›ºç»éªŒã€‚</li>
<li>æ—¥é—´å’Œå¤œé—´è¿‡ç¨‹çš„äº¤æ›¿å®ç°äº†LLM TTSçš„å¿«é€Ÿå’Œæ…¢é€Ÿè¿›åŒ–ï¼Œå®Œå…¨æ¨¡ä»¿äººç±»è®¤çŸ¥åŠ¨æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24771">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cbc7f1bbfa0b25bfdfcdef8008ad888e.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-cbc8a6a26b49193c9cc58b94714bcc75~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931982&auth_key=1759931982-0-0-cb5cb04fc7387b5bbeeaa287b15127fe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c6a31a921ae3ffed1e37805c84187f6e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931989&auth_key=1759931989-0-0-72f662de81267824044b3adfebb22323&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VoxCPM-Tokenizer-Free-TTS-for-Context-Aware-Speech-Generation-and-True-to-Life-Voice-Cloning"><a href="#VoxCPM-Tokenizer-Free-TTS-for-Context-Aware-Speech-Generation-and-True-to-Life-Voice-Cloning" class="headerlink" title="VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and   True-to-Life Voice Cloning"></a>VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and   True-to-Life Voice Cloning</h2><p><strong>Authors:Yixuan Zhou, Guoyang Zeng, Xin Liu, Xiang Li, Renjie Yu, Ziyang Wang, Runchuan Ye, Weiyue Sun, Jiancheng Gui, Kehan Li, Zhiyong Wu, Zhiyuan Liu</strong></p>
<p>Generative models for speech synthesis face a fundamental trade-off: discrete tokens ensure stability but sacrifice expressivity, while continuous signals retain acoustic richness but suffer from error accumulation due to task entanglement. This challenge has driven the field towards multi-stage pipelines that rely on pre-trained speech tokenizers, but these create a semantic-acoustic divide, limiting holistic and expressive speech generation. We resolve these dilemma through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel tokenizer-free TTS model VoxCPM. Our framework introduces a differentiable quantization bottleneck that induces natural specialization: a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans, while a Residual Acoustic Model (RALM) recovers fine-grained acoustic details. This hierarchical semantic-acoustic representation guides a local diffusion-based decoder to generate high-fidelity speech latents. Critically, the entire architecture is trained end-to-end under a simple diffusion objective, eliminating dependency on external speech tokenizers. Trained on a massive 1.8 million hours of bilingual corpus, our VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis. Besides, VoxCPM shows the capability to comprehend text to infer and generate appropriate prosody and style, delivering speech with context-aware expressiveness and natural flow. To facilitate community-driven research and development, VoxCPM is publicly accessible under Apache 2.0. </p>
<blockquote>
<p>ç”Ÿæˆå¼è¯­éŸ³åˆæˆæ¨¡å‹é¢ä¸´ä¸€ä¸ªåŸºæœ¬æƒè¡¡ï¼šç¦»æ•£ç¬¦å·ä¿è¯äº†ç¨³å®šæ€§ä½†ç‰ºç‰²äº†è¡¨è¾¾åŠ›ï¼Œè€Œè¿ç»­ä¿¡å·ä¿ç•™äº†å£°éŸ³ä¸°å¯Œæ€§ä½†å—åˆ°ä»»åŠ¡çº ç¼ å¯¼è‡´çš„è¯¯å·®ç´¯ç§¯çš„å½±å“ã€‚è¿™ä¸€æŒ‘æˆ˜ä¿ƒä½¿é¢†åŸŸæœç€ä¾èµ–é¢„è®­ç»ƒè¯­éŸ³æ ‡è®°å™¨çš„å¤šé˜¶æ®µæµç¨‹å‘å±•ï¼Œä½†è¿™äº›æµç¨‹äº§ç”Ÿäº†è¯­ä¹‰å£°å­¦é¸¿æ²Ÿï¼Œé™åˆ¶äº†æ•´ä½“çš„è¡¨è¾¾æ€§è¯­éŸ³ç”Ÿæˆã€‚æˆ‘ä»¬é€šè¿‡å…·æœ‰åŠç¦»æ•£æ®‹å·®è¡¨ç¤ºçš„åˆ†å±‚æ¬¡è¯­ä¹‰å£°å­¦å»ºæ¨¡æ¥è§£å†³è¿™äº›éš¾é¢˜ï¼Œå¹¶æ¨å‡ºäº†æ— éœ€æ ‡è®°å™¨çš„æ–‡æœ¬è½¬è¯­éŸ³åˆæˆï¼ˆTTSï¼‰æ¨¡å‹VoxCPMã€‚æˆ‘ä»¬çš„æ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªå¯å¾®åˆ†çš„é‡åŒ–ç“¶é¢ˆï¼Œä»¥äº§ç”Ÿè‡ªç„¶çš„ä¸“é•¿ï¼šæ–‡æœ¬è¯­ä¹‰è¯­è¨€æ¨¡å‹ï¼ˆTSLMï¼‰ç”Ÿæˆè¯­ä¹‰è¯­è°ƒè®¡åˆ’ï¼Œè€Œæ®‹å·®å£°å­¦æ¨¡å‹ï¼ˆRALMï¼‰æ¢å¤ç²¾ç»†çš„å£°å­¦ç»†èŠ‚ã€‚è¿™ç§åˆ†å±‚æ¬¡çš„è¯­ä¹‰å£°å­¦è¡¨ç¤ºå¼•å¯¼åŸºäºå±€éƒ¨æ‰©æ•£çš„è§£ç å™¨ç”Ÿæˆé«˜ä¿çœŸè¯­éŸ³æ½œåœ¨è¡¨ç¤ºã€‚å…³é”®çš„æ˜¯ï¼Œæ•´ä¸ªæ¶æ„åœ¨ä¸€ä¸ªç®€å•çš„æ‰©æ•£ç›®æ ‡ä¸‹è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨è¯­éŸ³æ ‡è®°å™¨çš„ä¾èµ–ã€‚åœ¨180ä¸‡å°æ—¶çš„åŒè¯­è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒåï¼Œæˆ‘ä»¬çš„VoxCPM-0.5Bæ¨¡å‹åœ¨å¼€æºç³»ç»Ÿä¸­å®ç°äº†é›¶å°„å‡»æ–‡æœ¬è½¬è¯­éŸ³åˆæˆçš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæä¾›è¡¨è¾¾æ¸…æ™°ã€ç¨³å®šçš„åˆæˆã€‚æ­¤å¤–ï¼ŒVoxCPMå±•ç°å‡ºç†è§£æ–‡æœ¬å¹¶æ¨æ–­å’Œç”Ÿæˆé€‚å½“è¯­è°ƒå’Œé£æ ¼çš„èƒ½åŠ›ï¼Œç”Ÿæˆå…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¡¨è¾¾åŠ›å’Œè‡ªç„¶æµåˆ©çš„è¯­éŸ³ã€‚ä¸ºäº†æ–¹ä¾¿ç¤¾åŒºé©±åŠ¨çš„ç ”ç©¶å’Œå¼€å‘ï¼ŒVoxCPMåœ¨Apache 2.0ä¸‹å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24650v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­éŸ³åˆæˆé¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜åŠè§£å†³æ–¹æ¡ˆã€‚ä¼ ç»Ÿçš„ç”Ÿæˆæ¨¡å‹åœ¨ç¨³å®šæ€§å’Œè¡¨è¾¾æ€§ä¸Šå­˜æ­¤æ¶ˆå½¼é•¿çš„é—®é¢˜ã€‚ä½œè€…æå‡ºä¸€ç§åŸºäºå±‚æ¬¡è¯­ä¹‰å£°å­¦å»ºæ¨¡çš„TTSæ¨¡å‹VoxCPMï¼Œé€šè¿‡åŠç¦»æ•£æ®‹å·®è¡¨ç¤ºè§£å†³è¿™ä¸€é—®é¢˜ã€‚æ¨¡å‹å¼•å…¥å¯å¾®é‡åŒ–ç“¶é¢ˆï¼Œå®ç°è‡ªç„¶ä¸“é¡¹åŒ–ï¼Œåˆ†ä¸ºæ–‡æœ¬è¯­ä¹‰è¯­è¨€æ¨¡å‹ï¼ˆTSLMï¼‰ç”Ÿæˆè¯­ä¹‰è¯­è°ƒè®¡åˆ’ï¼Œæ®‹å·®å£°å­¦æ¨¡å‹ï¼ˆRALMï¼‰æ¢å¤ç²¾ç»†å£°å­¦ç»†èŠ‚ã€‚è¿™ç§å±‚æ¬¡è¯­ä¹‰å£°å­¦è¡¨ç¤ºå¼•å¯¼å±€éƒ¨æ‰©æ•£å¼è§£ç å™¨ç”Ÿæˆé«˜ä¿çœŸè¯­éŸ³æ½œåœ¨è¡¨ç¤ºã€‚æ•´ä¸ªæ¶æ„åœ¨ç®€å•çš„æ‰©æ•£ç›®æ ‡ä¸‹è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œä¸ä¾èµ–å¤–éƒ¨è¯­éŸ³åˆ†è¯å™¨ã€‚åœ¨18äº¿å°æ—¶åŒè¯­è¯­æ–™åº“ä¸Šè®­ç»ƒçš„VoxCPM-0.5Bæ¨¡å‹åœ¨é›¶æ ·æœ¬TTSæ€§èƒ½æ–¹é¢è¾¾åˆ°å¼€æºç³»ç»Ÿæœ€ä½³æ°´å¹³ï¼Œå±•ç°ç†è§£æ–‡æœ¬ã€æ¨æ–­ç”Ÿæˆé€‚å½“è¯­è°ƒå’Œé£æ ¼çš„èƒ½åŠ›ï¼Œç”Ÿæˆå…·æœ‰è¯­å¢ƒæ„ŸçŸ¥è¡¨è¾¾åŠ›å’Œè‡ªç„¶æµç•…åº¦çš„è¯­éŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³åˆæˆä¸­çš„ç”Ÿæˆæ¨¡å‹é¢ä¸´ç¨³å®šæ€§å’Œè¡¨è¾¾æ€§çš„æƒè¡¡æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºé¢„è®­ç»ƒçš„è¯­éŸ³åˆ†è¯å™¨ï¼Œä½†å­˜åœ¨è¯­ä¹‰ä¸å£°å­¦åˆ†ç¦»çš„å±€é™ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå±‚æ¬¡è¯­ä¹‰å£°å­¦å»ºæ¨¡çš„TTSæ¨¡å‹VoxCPMï¼Œç»“åˆåŠç¦»æ•£æ®‹å·®è¡¨ç¤ºè§£å†³æ­¤æŒ‘æˆ˜ã€‚</li>
<li>æ¨¡å‹åŒ…å«æ–‡æœ¬è¯­ä¹‰è¯­è¨€æ¨¡å‹ï¼ˆTSLMï¼‰å’Œæ®‹å·®å£°å­¦æ¨¡å‹ï¼ˆRALMï¼‰ï¼Œåˆ†åˆ«è´Ÿè´£ç”Ÿæˆè¯­ä¹‰è¯­è°ƒè®¡åˆ’å’Œæ¢å¤å£°å­¦ç»†èŠ‚ã€‚</li>
<li>å¼•å…¥å¯å¾®é‡åŒ–ç“¶é¢ˆå®ç°è‡ªç„¶ä¸“é¡¹åŒ–ï¼Œå¹¶é€šè¿‡å±€éƒ¨æ‰©æ•£å¼è§£ç å™¨ç”Ÿæˆé«˜ä¿çœŸè¯­éŸ³ã€‚</li>
<li>æ¨¡å‹åœ¨å¤§å‹åŒè¯­è¯­æ–™åº“ä¸Šè®­ç»ƒï¼Œè¾¾åˆ°é›¶æ ·æœ¬TTSæ€§èƒ½çš„æœ€ä½³æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-97268c0762655e3eb273dccd1d77cc4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931998&auth_key=1759931998-0-0-a0812d0a1a3f3567d357da73404c9888&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ef319b116e60759dfb4ca13beaccb779~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932009&auth_key=1759932009-0-0-504a559b81bf9f5bed49f8d1c20ec59b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Word-Level-Emotional-Expression-Control-in-Zero-Shot-Text-to-Speech-Synthesis"><a href="#Word-Level-Emotional-Expression-Control-in-Zero-Shot-Text-to-Speech-Synthesis" class="headerlink" title="Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech   Synthesis"></a>Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech   Synthesis</h2><p><strong>Authors:Tianrui Wang, Haoyu Wang, Meng Ge, Cheng Gong, Chunyu Qiang, Ziyang Ma, Zikang Huang, Guanrou Yang, Xiaobao Wang, Eng Siong Chng, Xie Chen, Longbiao Wang, Jianwu Dang</strong></p>
<p>While emotional text-to-speech (TTS) has made significant progress, most existing research remains limited to utterance-level emotional expression and fails to support word-level control. Achieving word-level expressive control poses fundamental challenges, primarily due to the complexity of modeling multi-emotion transitions and the scarcity of annotated datasets that capture intra-sentence emotional and prosodic variation. In this paper, we propose WeSCon, the first self-training framework that enables word-level control of both emotion and speaking rate in a pretrained zero-shot TTS model, without relying on datasets containing intra-sentence emotion or speed transitions. Our method introduces a transition-smoothing strategy and a dynamic speed control mechanism to guide the pretrained TTS model in performing word-level expressive synthesis through a multi-round inference process. To further simplify the inference, we incorporate a dynamic emotional attention bias mechanism and fine-tune the model via self-training, thereby activating its ability for word-level expressive control in an end-to-end manner. Experimental results show that WeSCon effectively overcomes data scarcity, achieving state-of-the-art performance in word-level emotional expression control while preserving the strong zero-shot synthesis capabilities of the original TTS model. </p>
<blockquote>
<p>è™½ç„¶æƒ…æ„Ÿæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰çš„å¤§å¤šæ•°ç ”ç©¶ä»ç„¶ä»…é™äºå¥å­çº§åˆ«çš„æƒ…æ„Ÿè¡¨è¾¾ï¼Œå¹¶ä¸æ”¯æŒå•è¯çº§åˆ«çš„æ§åˆ¶ã€‚å®ç°å•è¯çº§åˆ«çš„æƒ…æ„Ÿè¡¨è¾¾æ§åˆ¶æå‡ºäº†æ ¹æœ¬æ€§çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå»ºæ¨¡å¤šæƒ…ç»ªè½¬æ¢çš„å¤æ‚æ€§å’Œç¼ºä¹æ•æ‰å¥å­å†…æƒ…æ„Ÿå’ŒéŸµå¾‹å˜åŒ–çš„æ ‡æ³¨æ•°æ®é›†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†WeSConï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªæˆ‘è®­ç»ƒæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨é¢„è®­ç»ƒçš„é›¶æ ·æœ¬TTSæ¨¡å‹ä¸­å®ç°å¯¹å•è¯çº§åˆ«çš„æƒ…æ„Ÿå’Œè¯­é€Ÿçš„æ§åˆ¶ï¼Œæ— éœ€ä¾èµ–åŒ…å«å¥å­å†…æƒ…æ„Ÿæˆ–è¯­é€Ÿè½¬æ¢çš„æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§è¿‡æ¸¡å¹³æ»‘ç­–ç•¥å’Œä¸€ä¸ªåŠ¨æ€é€Ÿåº¦æ§åˆ¶æœºåˆ¶ï¼Œä»¥æŒ‡å¯¼é¢„è®­ç»ƒçš„TTSæ¨¡å‹é€šè¿‡å¤šè½®æ¨ç†è¿‡ç¨‹è¿›è¡Œå•è¯çº§åˆ«çš„è¡¨è¾¾åˆæˆã€‚ä¸ºäº†ç®€åŒ–æ¨ç†è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€çš„æƒ…æ„Ÿæ³¨æ„åŠ›åå‘æœºåˆ¶ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘è®­ç»ƒå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»è€Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼æ¿€æ´»å…¶å•è¯çº§åˆ«çš„æƒ…æ„Ÿè¡¨è¾¾æ§åˆ¶èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWeSConæœ‰æ•ˆåœ°å…‹æœäº†æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œåœ¨å•è¯çº§åˆ«çš„æƒ…æ„Ÿè¡¨è¾¾æ§åˆ¶æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿ç•™äº†åŸå§‹TTSæ¨¡å‹çš„é›¶æ ·æœ¬åˆæˆèƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24629v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºWeSConçš„è‡ªè®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨é¢„è®­ç»ƒçš„é›¶æ ·æœ¬TTSæ¨¡å‹ä¸­å®ç°è¯çº§åˆ«çš„æƒ…æ„Ÿå’Œè¯­é€Ÿæ§åˆ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥è¿‡æ¸¡å¹³æ»‘ç­–ç•¥ã€åŠ¨æ€é€Ÿåº¦æ§åˆ¶æœºåˆ¶å’Œå¤šè½®æ¨ç†è¿‡ç¨‹ï¼Œå®ç°è¯çº§åˆ«çš„è¡¨è¾¾åˆæˆã€‚ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œèå…¥åŠ¨æ€æƒ…æ„Ÿæ³¨æ„åŠ›åå·®æœºåˆ¶ï¼Œé€šè¿‡è‡ªæˆ‘è®­ç»ƒå¾®è°ƒæ¨¡å‹ï¼Œä»è€Œæ¿€æ´»ç«¯åˆ°ç«¯çš„è¯çº§åˆ«è¡¨è¾¾æ§åˆ¶èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒWeSConåœ¨è¯çº§åˆ«æƒ…æ„Ÿè¡¨è¾¾æ§åˆ¶ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶ä¿ç•™äº†åŸå§‹TTSæ¨¡å‹çš„é›¶æ ·æœ¬åˆæˆèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æƒ…æ„Ÿæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç ”ç©¶å¤§å¤šä»…é™äºè¯­å¥çº§åˆ«çš„æƒ…æ„Ÿè¡¨è¾¾ï¼Œç¼ºä¹å¯¹è¯çº§åˆ«çš„æ§åˆ¶ã€‚</li>
<li>å®ç°è¯çº§åˆ«æƒ…æ„Ÿè¡¨è¾¾æ§åˆ¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ä½“ç°åœ¨å»ºæ¨¡å¤šæƒ…æ„Ÿè½¬æ¢çš„å¤æ‚æ€§å’Œç¼ºä¹æ•æ‰å¥å†…æƒ…æ„Ÿå’Œè¯­è°ƒå˜åŒ–çš„æ ‡æ³¨æ•°æ®é›†ã€‚</li>
<li>WeSConæ¡†æ¶é€šè¿‡å¼•å…¥è¿‡æ¸¡å¹³æ»‘ç­–ç•¥å’ŒåŠ¨æ€é€Ÿåº¦æ§åˆ¶æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨é¢„è®­ç»ƒçš„TTSæ¨¡å‹ä¸­å®ç°è¯çº§åˆ«çš„æƒ…æ„Ÿå’Œè¯­é€Ÿæ§åˆ¶ã€‚</li>
<li>WeSConæ¡†æ¶é‡‡ç”¨å¤šè½®æ¨ç†è¿‡ç¨‹ï¼Œç®€åŒ–æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>åŠ¨æ€æƒ…æ„Ÿæ³¨æ„åŠ›åå·®æœºåˆ¶çš„èå…¥è§£å†³äº†æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>WeSConæ¡†æ¶åœ¨è¯çº§åˆ«æƒ…æ„Ÿè¡¨è¾¾æ§åˆ¶ä¸Šå–å¾—æœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶ä¿ç•™TTSæ¨¡å‹çš„é›¶æ ·æœ¬åˆæˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d160d1e53b02cbb8e325622e9ff8d172~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932019&auth_key=1759932019-0-0-b93dd1420f782e939f52f3e362cf8138&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-46a5c20b3cc8a6e4dd583a6698ac8d79~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932029&auth_key=1759932029-0-0-11b148e0fc7bde43fb9277804e67cc87&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-f56f420bb80f58dc8dc833e42cf47888.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1940de828357ffef73fe84348fc4f94.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-61e35bbf9a777a7df14cb0ffcdbff2e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932050&auth_key=1759932050-0-0-47bb86dcec7709fb7aac0d035dfa982d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ISSE-An-Instruction-Guided-Speech-Style-Editing-Dataset-And-Benchmark"><a href="#ISSE-An-Instruction-Guided-Speech-Style-Editing-Dataset-And-Benchmark" class="headerlink" title="ISSE: An Instruction-Guided Speech Style Editing Dataset And Benchmark"></a>ISSE: An Instruction-Guided Speech Style Editing Dataset And Benchmark</h2><p><strong>Authors:Yun Chen, Qi Chen, Zheqi Dai, Arshdeep Singh, Philip J. B. Jackson, Mark D. Plumbley</strong></p>
<p>Speech style editing refers to modifying the stylistic properties of speech while preserving its linguistic content and speaker identity. However, most existing approaches depend on explicit labels or reference audio, which limits both flexibility and scalability. More recent attempts to use natural language descriptions remain constrained by oversimplified instructions and coarse style control. To address these limitations, we introduce an Instruction-guided Speech Style Editing Dataset (ISSE). The dataset comprises nearly 400 hours of speech and over 100,000 source-target pairs, each aligned with diverse and detailed textual editing instructions. We also build a systematic instructed speech data generation pipeline leveraging large language model, expressive text-to-speech and voice conversion technologies to construct high-quality paired samples. Furthermore, we train an instruction-guided autoregressive speech model on ISSE and evaluate it in terms of instruction adherence, timbre preservation, and content consistency. Experimental results demonstrate that ISSE enables accurate, controllable, and generalizable speech style editing compared to other datasets. The project page of ISSE is available at <a target="_blank" rel="noopener" href="https://ychenn1.github.io/ISSE/">https://ychenn1.github.io/ISSE/</a>. </p>
<blockquote>
<p>è¯­éŸ³é£æ ¼ç¼–è¾‘æ˜¯æŒ‡ä¿®æ”¹è¯­éŸ³çš„é£æ ¼å±æ€§ï¼ŒåŒæ—¶ä¿ç•™å…¶è¯­è¨€å†…å®¹å’Œè¯´è¯è€…èº«ä»½ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºæ˜ç¡®çš„æ ‡ç­¾æˆ–å‚è€ƒéŸ³é¢‘ï¼Œè¿™é™åˆ¶äº†çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚è¿‘æœŸå°è¯•ä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°ä¹Ÿå—åˆ°è¿‡äºç®€åŒ–çš„æŒ‡ä»¤å’Œç²—ç³™çš„é£æ ¼æ§åˆ¶çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Instruction-guided Speech Style Editing Datasetï¼ˆISSEï¼‰æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«è¿‘400å°æ—¶çš„è¯­éŸ³å’Œè¶…è¿‡10ä¸‡ç»„æºç›®æ ‡é…å¯¹ï¼Œæ¯ç»„éƒ½ä¸å¤šæ ·ä¸”è¯¦ç»†çš„æ–‡æœ¬ç¼–è¾‘æŒ‡ä»¤å¯¹é½ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†ä¸€æ¡ç³»ç»Ÿçš„æŒ‡ä»¤æ€§è¯­éŸ³æ•°æ®ç”Ÿæˆç®¡é“ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ã€è¡¨ç°åŠ›å¼ºçš„æ–‡æœ¬åˆ°è¯­éŸ³å’Œè¯­éŸ³è½¬æ¢æŠ€æœ¯ï¼Œæ„å»ºé«˜è´¨é‡é…å¯¹æ ·æœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ISSEä¸Šè®­ç»ƒäº†ä¸€ä¸ªæŒ‡ä»¤å¼•å¯¼çš„è‡ªå›å½’è¯­éŸ³æ¨¡å‹ï¼Œå¹¶æ ¹æ®æŒ‡ä»¤éµå¾ªæƒ…å†µã€éŸ³è‰²ä¿ç•™å’Œå†…å®¹ä¸€è‡´æ€§å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–æ•°æ®é›†ç›¸æ¯”ï¼ŒISSEèƒ½å¤Ÿå®ç°å‡†ç¡®ã€å¯æ§ä¸”é€šç”¨çš„è¯­éŸ³é£æ ¼ç¼–è¾‘ã€‚ISSEçš„é¡¹ç›®é¡µé¢å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://ychenn1.github.io/ISSE/%E3%80%82">https://ychenn1.github.io/ISSE/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24570v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¯­éŸ³é£æ ¼ç¼–è¾‘çš„æ–°æ•°æ®é›†ISSEã€‚è¯¥æ•°æ®é›†åŒ…å«äº†è¿‘400å°æ—¶çš„è¯­éŸ³æ•°æ®å’Œè¶…è¿‡ç™¾ä¸‡ä¸ªæºç›®æ ‡é…å¯¹ï¼Œæ¯ä¸ªé…å¯¹éƒ½ä¸è¯¦ç»†çš„æ–‡æœ¬ç¼–è¾‘æŒ‡ä»¤å¯¹é½ã€‚é€šè¿‡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ã€æ–‡æœ¬è½¬è¯­éŸ³å’Œè¯­éŸ³è½¬æ¢æŠ€æœ¯ï¼Œæ„å»ºäº†é«˜è´¨é‡é…å¯¹æ ·æœ¬çš„ç³»ç»ŸåŒ–æŒ‡ä»¤è¯­éŸ³æ•°æ®ç”Ÿæˆç®¡é“ã€‚æ­¤å¤–ï¼Œåœ¨ISSEä¸Šè®­ç»ƒäº†ä¸€ç§æŒ‡ä»¤å¼•å¯¼çš„è‡ªåŠ¨å›å½’è¯­éŸ³æ¨¡å‹ï¼Œå¹¶åœ¨æŒ‡ä»¤éµå¾ªã€éŸ³è‰²ä¿ç•™å’Œå†…å®¹ä¸€è‡´æ€§æ–¹é¢è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–æ•°æ®é›†ç›¸æ¯”ï¼ŒISSEèƒ½å¤Ÿå®ç°å‡†ç¡®ã€å¯æ§å’Œé€šç”¨çš„è¯­éŸ³é£æ ¼ç¼–è¾‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯­éŸ³é£æ ¼ç¼–è¾‘æ˜¯ä¿®æ”¹è¯­éŸ³çš„é£æ ¼å±æ€§ï¼ŒåŒæ—¶ä¿ç•™å…¶è¯­è¨€å†…å®¹å’Œè¯´è¯è€…èº«ä»½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–äºæ˜ç¡®çš„æ ‡ç­¾æˆ–å‚è€ƒéŸ³é¢‘ï¼Œé™åˆ¶äº†çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>ISSEæ•°æ®é›†åŒ…å«è¿‘400å°æ—¶çš„è¯­éŸ³æ•°æ®å’Œè¶…è¿‡ç™¾ä¸‡ä¸ªæºç›®æ ‡é…å¯¹ï¼Œä¸è¯¦ç»†çš„æ–‡æœ¬ç¼–è¾‘æŒ‡ä»¤å¯¹é½ã€‚</li>
<li>é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ã€æ–‡æœ¬è½¬è¯­éŸ³å’Œè¯­éŸ³è½¬æ¢æŠ€æœ¯æ„å»ºé«˜è´¨é‡é…å¯¹æ ·æœ¬çš„æ•°æ®ç”Ÿæˆç®¡é“ã€‚</li>
<li>åœ¨ISSEä¸Šè®­ç»ƒçš„æŒ‡ä»¤å¼•å¯¼è‡ªåŠ¨å›å½’è¯­éŸ³æ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªã€éŸ³è‰²ä¿ç•™å’Œå†…å®¹ä¸€è‡´æ€§æ–¹é¢è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>ISSEä¸å…¶ä»–æ•°æ®é›†ç›¸æ¯”ï¼Œèƒ½å¤Ÿå®ç°æ›´å‡†ç¡®ã€å¯æ§å’Œé€šç”¨çš„è¯­éŸ³é£æ ¼ç¼–è¾‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24570">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-319cb6d68cc8b2356112656fc58bb2cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932058&auth_key=1759932058-0-0-77d82a0f799c821fdd05e6695051426f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-caacc0e46ca11eafc6f6fc0a764addf4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932065&auth_key=1759932065-0-0-9d8b098e55c3e36f7ac99966a3a778d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-c7904c4c8b8e41ac512ce6ac31221506.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-5fdafdb5795190059cc7e159b26bbc84~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932079&auth_key=1759932079-0-0-97b4b1e8a6fec42b3b0f4e50fa23435e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="UniFlow-Audio-Unified-Flow-Matching-for-Audio-Generation-from-Omni-Modalities"><a href="#UniFlow-Audio-Unified-Flow-Matching-for-Audio-Generation-from-Omni-Modalities" class="headerlink" title="UniFlow-Audio: Unified Flow Matching for Audio Generation from   Omni-Modalities"></a>UniFlow-Audio: Unified Flow Matching for Audio Generation from   Omni-Modalities</h2><p><strong>Authors:Xuenan Xu, Jiahao Mei, Zihao Zheng, Ye Tao, Zeyu Xie, Yaoyun Zhang, Haohe Liu, Yuning Wu, Ming Yan, Wen Wu, Chao Zhang, Mengyue Wu</strong></p>
<p>Audio generation, including speech, music and sound effects, has advanced rapidly in recent years. These tasks can be divided into two categories: time-aligned (TA) tasks, where each input unit corresponds to a specific segment of the output audio (e.g., phonemes aligned with frames in speech synthesis); and non-time-aligned (NTA) tasks, where such alignment is not available. Since modeling paradigms for the two types are typically different, research on different audio generation tasks has traditionally followed separate trajectories. However, audio is not inherently divided into such categories, making a unified model a natural and necessary goal for general audio generation. Previous unified audio generation works have adopted autoregressive architectures, while unified non-autoregressive approaches remain largely unexplored. In this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching. We propose a dual-fusion mechanism that temporally aligns audio latents with TA features and integrates NTA features via cross-attention in each model block. Task-balanced data sampling is employed to maintain strong performance across both TA and NTA tasks. UniFlow-Audio supports omni-modalities, including text, audio, and video. By leveraging the advantage of multi-task learning and the generative modeling capabilities of flow matching, UniFlow-Audio achieves strong results across 7 tasks using fewer than 8K hours of public training data and under 1B trainable parameters. Even the small variant with only ~200M trainable parameters shows competitive performance, highlighting UniFlow-Audio as a potential non-auto-regressive foundation model for audio generation. Code and models will be available at <a target="_blank" rel="noopener" href="https://wsntxxn.github.io/uniflow_audio">https://wsntxxn.github.io/uniflow_audio</a>. </p>
<blockquote>
<p>éŸ³é¢‘ç”ŸæˆåŒ…æ‹¬è¯­éŸ³ã€éŸ³ä¹å’ŒéŸ³æ•ˆç­‰ï¼Œè¿‘å¹´æ¥å‘å±•è¿…é€Ÿã€‚è¿™äº›ä»»åŠ¡å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼šæ—¶é—´å¯¹é½ï¼ˆTAï¼‰ä»»åŠ¡ï¼Œå…¶ä¸­æ¯ä¸ªè¾“å…¥å•å…ƒå¯¹åº”äºè¾“å‡ºéŸ³é¢‘çš„ç‰¹å®šç‰‡æ®µï¼ˆä¾‹å¦‚ï¼Œè¯­éŸ³åˆæˆä¸­çš„éŸ³ç´ ä¸å¸§å¯¹é½ï¼‰ï¼›ä»¥åŠéæ—¶é—´å¯¹é½ï¼ˆNTAï¼‰ä»»åŠ¡ï¼Œå…¶ä¸­è¿™æ ·çš„å¯¹é½æ˜¯ä¸å¯ç”¨çš„ã€‚ç”±äºä¸¤ç§ç±»å‹çš„å»ºæ¨¡èŒƒå¼é€šå¸¸ä¸åŒï¼Œå› æ­¤é’ˆå¯¹ä¸åŒéŸ³é¢‘ç”Ÿæˆä»»åŠ¡çš„ç ”ç©¶ä¼ ç»Ÿä¸Šéµå¾ªäº†ä¸åŒçš„è½¨è¿¹ã€‚ç„¶è€Œï¼ŒéŸ³é¢‘æœ¬èº«å¹¶ä¸æ˜¯å›ºæœ‰åœ°åˆ†ä¸ºè¿™äº›ç±»åˆ«ï¼Œå› æ­¤æ„å»ºç»Ÿä¸€çš„æ¨¡å‹æˆä¸ºé€šç”¨éŸ³é¢‘ç”Ÿæˆçš„è‡ªç„¶ä¸”å¿…è¦ç›®æ ‡ã€‚å…ˆå‰ç»Ÿä¸€çš„éŸ³é¢‘ç”Ÿæˆå·¥ä½œå·²ç»é‡‡ç”¨äº†è‡ªå›å½’æ¶æ„ï¼Œè€Œç»Ÿä¸€çš„éè‡ªå›å½’æ–¹æ³•ä»ç„¶å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæµåŒ¹é…çš„é€šç”¨éŸ³é¢‘ç”Ÿæˆæ¡†æ¶UniFlow-Audioã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒèåˆæœºåˆ¶ï¼Œè¯¥æœºåˆ¶åœ¨æ—¶é—´ä¸Šå°†éŸ³é¢‘æ½œåœ¨å˜é‡ä¸TAç‰¹å¾å¯¹é½ï¼Œå¹¶é€šè¿‡æ¯ä¸ªæ¨¡å‹å—ä¸­çš„äº¤å‰æ³¨æ„åŠ›æ•´åˆNTAç‰¹å¾ã€‚é‡‡ç”¨ä»»åŠ¡å¹³è¡¡æ•°æ®é‡‡æ ·ä»¥åœ¨TAå’ŒNTAä»»åŠ¡ä¸Šä¿æŒå¼ºåŠ²æ€§èƒ½ã€‚UniFlow-Audioæ”¯æŒæ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ç­‰å¤šç§æ¨¡å¼ã€‚é€šè¿‡åˆ©ç”¨å¤šä»»åŠ¡å­¦ä¹ çš„ä¼˜åŠ¿å’ŒæµåŒ¹é…çš„ç”Ÿæˆå»ºæ¨¡èƒ½åŠ›ï¼ŒUniFlow-Audioåœ¨7ä¸ªä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„ç»“æœï¼Œä½¿ç”¨äº†å°‘äº8Kå°æ—¶çš„å…¬å¼€è®­ç»ƒæ•°æ®å’Œä¸åˆ°1Bçš„å¯è®­ç»ƒå‚æ•°ã€‚å³ä½¿åªæœ‰çº¦2äº¿ä¸ªå¯è®­ç»ƒå‚æ•°çš„å°å‹å˜ä½“ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œè¿™çªæ˜¾äº†UniFlow-Audioä½œä¸ºéŸ³é¢‘ç”Ÿæˆæ½œåœ¨çš„éè‡ªå›å½’åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://wsntxxn.github.io/uniflow_audio%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://wsntxxn.github.io/uniflow_audioä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24391v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://wsntxxn.github.io/uniflow_audio">https://wsntxxn.github.io/uniflow_audio</a></p>
<p><strong>æ‘˜è¦</strong><br>     è¿‘å¹´éŸ³é¢‘ç”Ÿæˆï¼ŒåŒ…æ‹¬è¯­éŸ³ã€éŸ³ä¹å’ŒéŸ³æ•ˆï¼Œå‘å±•è¿…é€Ÿã€‚å¯åˆ†ä¸ºæ—¶é—´å¯¹é½å’Œéæ—¶é—´å¯¹é½ä¸¤ç±»ä»»åŠ¡ã€‚è¿‡å»çš„ç ”ç©¶é€šå¸¸é’ˆå¯¹å•ä¸€ä»»åŠ¡è¿›è¡Œï¼Œä½†éŸ³é¢‘ä¸æ˜¯å›ºæœ‰åœ°åˆ†ä¸ºè¿™ä¸¤ç±»ï¼Œå› æ­¤ç»Ÿä¸€æ¨¡å‹æˆä¸ºéŸ³é¢‘ç”Ÿæˆçš„å¿…è¦ç›®æ ‡ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºæµåŒ¹é…çš„é€šç”¨éŸ³é¢‘ç”Ÿæˆæ¡†æ¶UniFlow-Audioï¼Œé‡‡ç”¨åŒèåˆæœºåˆ¶ï¼Œä¸´æ—¶å¯¹é½éŸ³é¢‘æ½œèƒ½ä¸æ—¶é—´å¯¹é½ç‰¹å¾ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›é›†æˆéæ—¶é—´å¯¹é½ç‰¹å¾ã€‚é‡‡ç”¨ä»»åŠ¡å¹³è¡¡æ•°æ®é‡‡æ ·ä»¥ç»´æŒä¸¤ç§ä»»åŠ¡çš„æ€§èƒ½ã€‚UniFlow-Audioæ”¯æŒæ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ç­‰å¤šç§æ¨¡å¼ï¼Œåˆ©ç”¨å¤šä»»åŠ¡å­¦ä¹ å’ŒæµåŒ¹é…çš„ç”Ÿæˆå»ºæ¨¡èƒ½åŠ›ï¼Œåœ¨7ä¸ªä»»åŠ¡ä¸Šå–å¾—äº†å¼ºå¤§çš„ç»“æœï¼Œä½¿ç”¨å°‘äº8Kå°æ—¶çš„å…¬å¼€è®­ç»ƒæ•°æ®å’Œå°‘äº1Bçš„å¯è®­ç»ƒå‚æ•°ã€‚å³ä½¿åªæœ‰çº¦2äº¿å¯è®­ç»ƒå‚æ•°çš„å°å‹å˜ç§ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå‡¸æ˜¾äº†UniFlow-Audioä½œä¸ºéŸ³é¢‘ç”Ÿæˆçš„æ½œåœ¨éè‡ªå›å½’åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>éŸ³é¢‘ç”Ÿæˆå‘å±•è¿…é€Ÿï¼ŒåŒ…æ‹¬æ—¶é—´å¯¹é½å’Œéæ—¶é—´å¯¹é½ä»»åŠ¡ã€‚</li>
<li>ç»Ÿä¸€æ¨¡å‹æˆä¸ºéŸ³é¢‘ç”Ÿæˆçš„å¿…è¦ç›®æ ‡ï¼Œå› ä¸ºéŸ³é¢‘ä¸æ˜¯å›ºæœ‰åœ°åˆ†ä¸ºè¿™ä¸¤ç±»ä»»åŠ¡ã€‚</li>
<li>UniFlow-Audioæ˜¯ä¸€ä¸ªåŸºäºæµåŒ¹é…çš„é€šç”¨éŸ³é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ”¯æŒå¤šç§æ¨¡å¼ï¼ˆæ–‡æœ¬ã€éŸ³é¢‘ã€è§†é¢‘ï¼‰ã€‚</li>
<li>UniFlow-Audioé‡‡ç”¨åŒèåˆæœºåˆ¶æ¥é›†æˆæ—¶é—´å¯¹é½å’Œéæ—¶é—´å¯¹é½ç‰¹å¾ã€‚</li>
<li>é€šè¿‡ä»»åŠ¡å¹³è¡¡æ•°æ®é‡‡æ ·ç»´æŒæ—¶é—´å¯¹é½å’Œéæ—¶é—´å¯¹é½ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>UniFlow-Audioåœ¨å¤šä¸ªéŸ³é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å¼ºå¤§çš„ç»“æœï¼Œä½¿ç”¨è¾ƒå°‘çš„å…¬å¼€è®­ç»ƒæ•°æ®å’Œå¯è®­ç»ƒå‚æ•°ã€‚</li>
<li>UniFlow-Audioå…·æœ‰æˆä¸ºéè‡ªå›å½’éŸ³é¢‘ç”ŸæˆåŸºç¡€æ¨¡å‹çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2b0af2b7f3b23a372456ef19c20e031e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932087&auth_key=1759932087-0-0-98826e96097cec6cb36a4ca5540b921d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cebbc6c4b275e577f119c5ac82fd92dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932094&auth_key=1759932094-0-0-f61badebfe39615307962e9b61922501&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-814683bf1877375f158b5d52f8715d6f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932101&auth_key=1759932101-0-0-3b038fbcef13b44b4c73a7e64edca337&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-18ef3ab10e3cc0977dea9316bcb4b77f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932108&auth_key=1759932108-0-0-a067969c769a8ab93026f91e3dfc7d54&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Code-switching-Speech-Recognition-Under-the-Lens-Model-and-Data-Centric-Perspectives"><a href="#Code-switching-Speech-Recognition-Under-the-Lens-Model-and-Data-Centric-Perspectives" class="headerlink" title="Code-switching Speech Recognition Under the Lens: Model- and   Data-Centric Perspectives"></a>Code-switching Speech Recognition Under the Lens: Model- and   Data-Centric Perspectives</h2><p><strong>Authors:Hexin Liu, Haoyang Zhang, Qiquan Zhang, Xiangyu Zhang, Dongyuan Shi, Eng Siong Chng, Haizhou Li</strong></p>
<p>Code-switching automatic speech recognition (CS-ASR) presents unique challenges due to language confusion introduced by spontaneous intra-sentence switching and accent bias that blurs the phonetic boundaries. Although the constituent languages may be individually high-resource, the scarcity of annotated code-switching data further compounds these challenges. In this paper, we systematically analyze CS-ASR from both model-centric and data-centric perspectives. By comparing state-of-the-art algorithmic methods, including language-specific processing and auxiliary language-aware multi-task learning, we discuss their varying effectiveness across datasets with different linguistic characteristics. On the data side, we first investigate TTS as a data augmentation method. By varying the textual characteristics and speaker accents, we analyze the impact of language confusion and accent bias on CS-ASR. To further mitigate data scarcity and enhance textual diversity, we propose a prompting strategy by simplifying the equivalence constraint theory (SECT) to guide large language models (LLMs) in generating linguistically valid code-switching text. The proposed SECT outperforms existing methods in ASR performance and linguistic quality assessments, generating code-switching text that more closely resembles real-world code-switching text. When used to generate speech-text pairs via TTS, SECT proves effective in improving CS-ASR performance. Our analysis of both model- and data-centric methods underscores that effective CS-ASR requires strategies to be carefully aligned with the specific linguistic characteristics of the code-switching data. </p>
<blockquote>
<p>ä»£ç åˆ‡æ¢è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆCS-ASRï¼‰ç”±äºå¥å­å†…è‡ªå‘åˆ‡æ¢å¼•å‘çš„è¯­è¨€æ··æ·†å’Œå£éŸ³åå·®å¯¼è‡´çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå£éŸ³åå·®æ¨¡ç³Šäº†è¯­éŸ³è¾¹ç•Œã€‚å°½ç®¡æ„æˆçš„è¯­è¨€å¯èƒ½ä¸ªåˆ«ä¸ºé«˜èµ„æºè¯­è¨€ï¼Œä½†æ ‡æ³¨çš„ä»£ç åˆ‡æ¢æ•°æ®ç¨€ç¼ºè¿›ä¸€æ­¥åŠ å‰§äº†è¿™äº›æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»æ¨¡å‹ä¸­å¿ƒå’Œæ•°æ®ä¸­å¿ƒä¸¤ä¸ªè§’åº¦ç³»ç»Ÿåœ°åˆ†æCS-ASRã€‚é€šè¿‡æ¯”è¾ƒæœ€æ–°çš„ç®—æ³•æ–¹æ³•ï¼ŒåŒ…æ‹¬ç‰¹å®šè¯­è¨€çš„å¤„ç†å’Œè¾…åŠ©è¯­è¨€æ„ŸçŸ¥å¤šä»»åŠ¡å­¦ä¹ ï¼Œæˆ‘ä»¬è®¨è®ºäº†å®ƒä»¬åœ¨å…·æœ‰ä¸åŒè¯­è¨€ç‰¹å¾çš„æ•°æ®é›†ä¸Šçš„ä¸åŒæœ‰æ•ˆæ€§ã€‚åœ¨æ•°æ®æ–¹é¢ï¼Œæˆ‘ä»¬é¦–å…ˆç ”ç©¶TTSä½œä¸ºä¸€ç§æ•°æ®å¢å¼ºæ–¹æ³•ã€‚é€šè¿‡æ”¹å˜æ–‡æœ¬ç‰¹å¾å’Œè¯´è¯äººçš„å£éŸ³ï¼Œæˆ‘ä»¬åˆ†æäº†è¯­è¨€æ··æ·†å’Œå£éŸ³åå·®å¯¹CS-ASRçš„å½±å“ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç¼“è§£æ•°æ®ç¨€ç¼ºå¹¶å¢å¼ºæ–‡æœ¬å¤šæ ·æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç®€åŒ–ç­‰ä»·çº¦æŸç†è®ºï¼ˆSECTï¼‰çš„æç¤ºç­–ç•¥ï¼Œä»¥æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆè¯­è¨€ä¸Šæœ‰æ•ˆçš„ä»£ç åˆ‡æ¢æ–‡æœ¬ã€‚æ‰€æå‡ºçš„SECTåœ¨è¯­éŸ³è¯†åˆ«æ€§èƒ½å’Œè¯­è¨€è´¨é‡è¯„ä¼°æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆæ›´è´´è¿‘ç°å®ä¸–ç•Œçš„ä»£ç åˆ‡æ¢æ–‡æœ¬ã€‚å½“ç”¨äºé€šè¿‡TTSç”Ÿæˆè¯­éŸ³æ–‡æœ¬å¯¹æ—¶ï¼ŒSECTåœ¨æé«˜CS-ASRæ€§èƒ½æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬å¯¹æ¨¡å‹å’Œæ•°æ®ä¸­å¿ƒæ–¹æ³•çš„åˆ†æå¼ºè°ƒï¼Œæœ‰æ•ˆçš„CS-ASRéœ€è¦ç­–ç•¥ä¸ä»£ç åˆ‡æ¢æ•°æ®çš„ç‰¹å®šè¯­è¨€ç‰¹å¾ä»”ç»†å¯¹é½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24310v1">PDF</a> 11 pages, 3 figures, 9 tables, submitted to IEEE TASLP</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦ç ”ç©¶äº†ä»£ç åˆ‡æ¢è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆCS-ASRï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¯­è¨€æ··æ·†å’Œå£éŸ³åå·®é—®é¢˜ã€‚æ–‡ç« ä»æ¨¡å‹å’Œæ•°æ®ä¸¤ä¸ªè§’åº¦ç³»ç»Ÿåˆ†æäº†CS-ASRï¼Œå¯¹æ¯”äº†å½“å‰å…ˆè¿›çš„ç®—æ³•æ–¹æ³•ï¼ŒåŒ…æ‹¬è¯­è¨€ç‰¹å®šå¤„ç†å’Œè¾…åŠ©è¯­è¨€æ„ŸçŸ¥å¤šä»»åŠ¡å­¦ä¹ ã€‚åŒæ—¶ï¼Œç ”ç©¶äº†TTSä½œä¸ºæ•°æ®å¢å¼ºæ–¹æ³•å¯¹CS-ASRçš„å½±å“ã€‚ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºå’Œæ–‡æœ¬å¤šæ ·æ€§é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç®€åŒ–ç­‰ä»·çº¦æŸç†è®ºï¼ˆSECTï¼‰çš„æç¤ºç­–ç•¥ï¼Œç”¨äºæŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆè¯­è¨€æœ‰æ•ˆçš„ä»£ç åˆ‡æ¢æ–‡æœ¬ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SECTåœ¨è¯­éŸ³è¯†åˆ«æ€§èƒ½å’Œè¯­è¨€è´¨é‡è¯„ä¼°æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆçš„ä»£ç åˆ‡æ¢æ–‡æœ¬æ›´æ¥è¿‘çœŸå®ä¸–ç•Œçš„ä»£ç åˆ‡æ¢æ–‡æœ¬ã€‚é€šè¿‡TTSç”Ÿæˆè¯­éŸ³æ–‡æœ¬å¯¹æ—¶ï¼ŒSECTåœ¨æé«˜CS-ASRæ€§èƒ½æ–¹é¢è¯æ˜æœ‰æ•ˆã€‚å¯¹æ¨¡å‹å’Œæ•°æ®çš„åˆ†æå¼ºè°ƒï¼Œæœ‰æ•ˆçš„CS-ASRéœ€è¦ç­–ç•¥ä¸ä»£ç åˆ‡æ¢æ•°æ®çš„ç‰¹å®šè¯­è¨€ç‰¹å¾ä»”ç»†å¯¹é½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»£ç åˆ‡æ¢è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆCS-ASRï¼‰é¢ä¸´è¯­è¨€æ··æ·†å’Œå£éŸ³åå·®çš„æŒ‘æˆ˜ã€‚</li>
<li>æ–‡ç« ä»æ¨¡å‹å’Œæ•°æ®ä¸¤ä¸ªè§’åº¦ç³»ç»Ÿåˆ†æäº†CS-ASRã€‚</li>
<li>å¯¹æ¯”äº†è¯­è¨€ç‰¹å®šå¤„ç†å’Œè¾…åŠ©è¯­è¨€æ„ŸçŸ¥å¤šä»»åŠ¡å­¦ä¹ ç­‰å…ˆè¿›ç®—æ³•æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶äº†TTSä½œä¸ºæ•°æ®å¢å¼ºæ–¹æ³•å¯¹CS-ASRçš„å½±å“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºç®€åŒ–ç­‰ä»·çº¦æŸç†è®ºï¼ˆSECTï¼‰çš„æç¤ºç­–ç•¥ï¼Œç”¨äºæŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè¯­è¨€æœ‰æ•ˆçš„ä»£ç åˆ‡æ¢æ–‡æœ¬ã€‚</li>
<li>SECTåœ¨è¯­éŸ³è¯†åˆ«æ€§èƒ½å’Œè¯­è¨€è´¨é‡è¯„ä¼°æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨TTSç”Ÿæˆçš„è¯­éŸ³æ–‡æœ¬å¯¹æ—¶ï¼ŒSECTèƒ½æœ‰æ•ˆæé«˜CS-ASRæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24310">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2648cb72fb1631105675913ac3f3e78a.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-11b0c149253903f6e020d24c1ad9d680~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932123&auth_key=1759932123-0-0-258937f0132fc1d7f1c9b103aa70f1cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2a855532fd684e1436536b3ec5eee921~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932130&auth_key=1759932130-0-0-08704a29db32b8cb3dc121faa31092f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ccdffa2d5ea09dd5fc49267bd6887918~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932137&auth_key=1759932137-0-0-aff45af257bfe79cb72c4eacdcfd932b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-f6dc32f900f635d534c5bcf3a9235380.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-5b45a276a88a2a167f2d1c0b5303e05e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932150&auth_key=1759932150-0-0-e6c4904014340c81933551d454631c94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Game-Oriented-ASR-Error-Correction-via-RAG-Enhanced-LLM"><a href="#Game-Oriented-ASR-Error-Correction-via-RAG-Enhanced-LLM" class="headerlink" title="Game-Oriented ASR Error Correction via RAG-Enhanced LLM"></a>Game-Oriented ASR Error Correction via RAG-Enhanced LLM</h2><p><strong>Authors:Yan Jiang, Yongle Luo, Qixian Zhou, Elvis S. Liu</strong></p>
<p>With the rise of multiplayer online games, real-time voice communication is essential for team coordination. However, general ASR systems struggle with gaming-specific challenges like short phrases, rapid speech, jargon, and noise, leading to frequent errors. To address this, we propose the GO-AEC framework, which integrates large language models, Retrieval-Augmented Generation (RAG), and a data augmentation strategy using LLMs and TTS. GO-AEC includes data augmentation, N-best hypothesis-based correction, and a dynamic game knowledge base. Experiments show GO-AEC reduces character error rate by 6.22% and sentence error rate by 29.71%, significantly improving ASR accuracy in gaming scenarios. </p>
<blockquote>
<p>éšç€å¤šäººåœ¨çº¿æ¸¸æˆçš„å…´èµ·ï¼Œå®æ—¶è¯­éŸ³äº¤æµå¯¹äºå›¢é˜Ÿåä½œè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œé€šç”¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿé¢ä¸´æ¸¸æˆç‰¹æœ‰çš„æŒ‘æˆ˜ï¼Œå¦‚çŸ­å¥ã€å¿«é€Ÿå‘è¨€ã€è¡Œè¯å’Œå™ªéŸ³ç­‰ï¼Œå¯¼è‡´ç»å¸¸å‡ºç°é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GO-AECæ¡†æ¶ï¼Œå®ƒé›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’ŒåŸºäºLLMå’ŒTTSçš„æ•°æ®å¢å¼ºç­–ç•¥ã€‚GO-AECåŒ…æ‹¬æ•°æ®å¢å¼ºã€åŸºäºN-bestå‡è®¾çš„æ ¡æ­£ä»¥åŠåŠ¨æ€æ¸¸æˆçŸ¥è¯†åº“ã€‚å®éªŒè¡¨æ˜ï¼ŒGO-AECé™ä½äº†6.22%çš„å­—ç¬¦é”™è¯¯ç‡å’Œ29.71%çš„å¥å­é”™è¯¯ç‡ï¼Œæ˜¾è‘—æé«˜äº†æ¸¸æˆåœºæ™¯ä¸­è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23630v1">PDF</a> </p>
<p><strong>Summary</strong><br>     éšç€å¤šäººåœ¨çº¿æ¸¸æˆçš„å…´èµ·ï¼Œå®æ—¶è¯­éŸ³æ²Ÿé€šå¯¹å›¢é˜Ÿåä½œè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œé€šç”¨ASRç³»ç»Ÿåœ¨é¢å¯¹æ¸¸æˆç‰¹æœ‰çš„æŒ‘æˆ˜æ—¶ï¼Œå¦‚çŸ­è¯­çŸ­ä¿ƒã€è¯­é€Ÿå¿«ã€ä¸“ä¸šæœ¯è¯­å’ŒèƒŒæ™¯å™ªéŸ³ç­‰ï¼Œæ˜“å‡ºç°é¢‘ç¹é”™è¯¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºGO-AECæ¡†æ¶ï¼Œå®ƒæ•´åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä»¥åŠä½¿ç”¨TTSå’ŒLLMçš„æ•°æ®å¢å¼ºç­–ç•¥ã€‚GO-AECåŒ…æ‹¬æ•°æ®å¢å¼ºã€åŸºäºN-bestå‡è®¾çš„æ ¡æ­£ä»¥åŠåŠ¨æ€æ¸¸æˆçŸ¥è¯†åº“ã€‚å®éªŒè¡¨æ˜ï¼ŒGO-AECé™ä½äº†6.22%çš„å­—ç¬¦é”™è¯¯ç‡å’Œ29.71%çš„å¥å­é”™è¯¯ç‡ï¼Œæ˜¾è‘—æé«˜äº†æ¸¸æˆåœºæ™¯ä¸­çš„ASRå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šäººåœ¨çº¿æ¸¸æˆä¸­å®æ—¶è¯­éŸ³æ²Ÿé€šçš„é‡è¦æ€§ã€‚</li>
<li>é€šç”¨ASRç³»ç»Ÿåœ¨å¤„ç†æ¸¸æˆè¯­éŸ³æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚çŸ­è¯­å¥ã€å¿«é€Ÿè¯­é€Ÿã€ä¸“ä¸šæœ¯è¯­å’Œå™ªéŸ³ã€‚</li>
<li>GO-AECæ¡†æ¶çš„æå‡ºï¼Œé›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’ŒåŸºäºTTSåŠLLMçš„æ•°æ®å¢å¼ºç­–ç•¥ã€‚</li>
<li>GO-AECæ¡†æ¶åŒ…æ‹¬æ•°æ®å¢å¼ºã€åŸºäºN-bestå‡è®¾çš„æ ¡æ­£ä»¥åŠåŠ¨æ€æ¸¸æˆçŸ¥è¯†åº“ç­‰åŠŸèƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGO-AECèƒ½æ˜¾è‘—é™ä½å­—ç¬¦å’Œå¥å­é”™è¯¯ç‡ã€‚</li>
<li>GO-AECæ¡†æ¶æ˜¾è‘—æé«˜äº†æ¸¸æˆåœºæ™¯ä¸­çš„ASRå‡†ç¡®æ€§ã€‚</li>
<li>GO-AECæ¡†æ¶å¯¹äºæ”¹å–„æ¸¸æˆå›¢é˜Ÿåä½œä¸­çš„å®æ—¶è¯­éŸ³é€šä¿¡å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-35b80501eeabe10e0e8450504a7729e0~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932158&auth_key=1759932158-0-0-7d65ddac3bfe9a0a9b541b56a85c4c8d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-93aaf6127b33e78c2c8eb303ba97f07f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dea293248dcdf1bf8d85633947890c4a.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-e8e86cf676a410badb6f51ea2fb56c8c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932178&auth_key=1759932178-0-0-d646dc51beccd2a248ff52a3a9d6f656&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ce16e0f3923580ce9071b4157e8711e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932184&auth_key=1759932184-0-0-bfbe619f15b4970e78f423ec49810924&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d8321c34d88b04937e90944918b87e67~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932191&auth_key=1759932191-0-0-c4a4e5a8fe51ce418349db1d120e0e29&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Generalizable-Speech-Deepfake-Detection-via-Information-Bottleneck-Enhanced-Adversarial-Alignment"><a href="#Generalizable-Speech-Deepfake-Detection-via-Information-Bottleneck-Enhanced-Adversarial-Alignment" class="headerlink" title="Generalizable Speech Deepfake Detection via Information Bottleneck   Enhanced Adversarial Alignment"></a>Generalizable Speech Deepfake Detection via Information Bottleneck   Enhanced Adversarial Alignment</h2><p><strong>Authors:Pu Huang, Shouguang Wang, Siya Yao, Mengchu Zhou</strong></p>
<p>Neural speech synthesis techniques have enabled highly realistic speech deepfakes, posing major security risks. Speech deepfake detection is challenging due to distribution shifts across spoofing methods and variability in speakers, channels, and recording conditions. We explore learning shared discriminative features as a path to robust detection and propose Information Bottleneck enhanced Confidence-Aware Adversarial Network (IB-CAAN). Confidence-guided adversarial alignment adaptively suppresses attack-specific artifacts without erasing discriminative cues, while the information bottleneck removes nuisance variability to preserve transferable features. Experiments on ASVspoof 2019&#x2F;2021, ASVspoof 5, and In-the-Wild demonstrate that IB-CAAN consistently outperforms baseline and achieves state-of-the-art performance on many benchmarks. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œè¯­éŸ³åˆæˆæŠ€æœ¯å·²ç»èƒ½å¤Ÿå®ç°é«˜åº¦é€¼çœŸçš„è¯­éŸ³æ·±åº¦ä¼ªé€ ï¼Œè¿™å¸¦æ¥äº†é‡å¤§çš„å®‰å…¨é£é™©ã€‚è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ¬ºéª—æ–¹æ³•ä¸­å­˜åœ¨åˆ†å¸ƒåç§»ä»¥åŠè¯´è¯äººã€é€šé“å’Œå½•åˆ¶æ¡ä»¶çš„å·®å¼‚æ€§ã€‚æˆ‘ä»¬æ¢ç´¢å­¦ä¹ å…±äº«åˆ¤åˆ«ç‰¹å¾ä½œä¸ºç¨³å¥æ£€æµ‹çš„é€”å¾„ï¼Œå¹¶æå‡ºåŸºäºä¿¡æ¯ç“¶é¢ˆå¢å¼ºçš„ç½®ä¿¡åº¦æ„ŸçŸ¥å¯¹æŠ—ç½‘ç»œï¼ˆIB-CAANï¼‰ã€‚ç½®ä¿¡åº¦å¼•å¯¼çš„å¯¹é½å¯¹æŠ—æ€§ç½‘ç»œè‡ªé€‚åº”åœ°æŠ‘åˆ¶æ”»å‡»ç‰¹å®šä¼ªé€ çš„ç—•è¿¹è€Œä¸æ¶ˆé™¤é‰´åˆ«çº¿ç´¢ï¼Œè€Œä¿¡æ¯ç“¶é¢ˆæ¶ˆé™¤äº†ä»¤äººå›°æ‰°çš„å˜å¼‚æ€§æ¥ä¿ç•™å¯è¿ç§»çš„ç‰¹å¾ã€‚åœ¨ASVspoof 2019&#x2F;2021ã€ASVspoof 5å’Œé‡å¤–å®éªŒä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒIB-CAANå§‹ç»ˆä¼˜äºåŸºçº¿å¹¶å®ç°äº†è®¸å¤šåŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23618v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>ç¥ç»è¯­éŸ³åˆæˆæŠ€æœ¯å·²ç»èƒ½å¤Ÿç”Ÿæˆé«˜åº¦é€¼çœŸçš„è¯­éŸ³æ·±åº¦ä¼ªé€ å†…å®¹ï¼Œè¿™å¸¦æ¥äº†é‡å¤§çš„å®‰å…¨é£é™©ã€‚ç”±äºä¼ªè£…æ–¹æ³•çš„åˆ†å¸ƒå˜åŒ–ä»¥åŠè¯´è¯äººã€é€šé“å’Œå½•éŸ³æ¡ä»¶çš„å¯å˜æ€§ï¼Œè¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æ¢è®¨äº†å­¦ä¹ å…±äº«åˆ¤åˆ«ç‰¹å¾ä½œä¸ºç¨³å¥æ£€æµ‹çš„é€”å¾„ï¼Œå¹¶æå‡ºäº†åŸºäºä¿¡æ¯ç“¶é¢ˆå¢å¼ºçš„ä¿¡å¿ƒæ„ŸçŸ¥å¯¹æŠ—ç½‘ç»œï¼ˆIB-CAANï¼‰ã€‚ä¿¡å¿ƒå¼•å¯¼çš„å¯¹é½å¯¹æŠ—ç­–ç•¥å¯ä»¥è‡ªé€‚åº”åœ°æŠ‘åˆ¶æ”»å‡»ç‰¹å®šçš„ä¼ªåƒï¼Œè€Œä¸ä¼šæ¶ˆé™¤åˆ¤åˆ«çº¿ç´¢ï¼›è€Œä¿¡æ¯ç“¶é¢ˆåˆ™èƒ½å¤Ÿæ¶ˆé™¤æ— ç”¨å˜é‡çš„å¹²æ‰°ä»¥ä¿ç•™å¯è½¬ç§»ç‰¹å¾ã€‚åœ¨ASVspoof 2019&#x2F;2021ã€ASVspoof 5å’ŒIn-the-Wildä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒIB-CAANæ€§èƒ½ç¨³å®šå¹¶è¶…è¿‡äº†åŸºçº¿æ°´å¹³ï¼Œä¸”åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†ä¸šç•Œå‰æ²¿æ°´å¹³ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¥ç»è¯­éŸ³åˆæˆæŠ€æœ¯å·²ç»èƒ½å¤Ÿç”Ÿæˆé«˜åº¦é€¼çœŸçš„è¯­éŸ³æ·±åº¦ä¼ªé€ å†…å®¹ï¼Œå¸¦æ¥é‡å¤§å®‰å…¨é£é™©ã€‚</li>
<li>è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¼ªè£…æ–¹æ³•çš„åˆ†å¸ƒå˜åŒ–å’Œè¯´è¯äººã€é€šé“åŠå½•éŸ³æ¡ä»¶çš„å˜åŒ–æ€§ã€‚</li>
<li>å­¦ä¹ å…±äº«åˆ¤åˆ«ç‰¹å¾æ˜¯è§£å†³ç¨³å¥æ£€æµ‹çš„å…³é”®é€”å¾„ã€‚</li>
<li>æå‡ºäº†ä¿¡æ¯ç“¶é¢ˆå¢å¼ºçš„ä¿¡å¿ƒæ„ŸçŸ¥å¯¹æŠ—ç½‘ç»œï¼ˆIB-CAANï¼‰ã€‚</li>
<li>ä¿¡å¿ƒå¼•å¯¼çš„å¯¹é½å¯¹æŠ—ç­–ç•¥èƒ½å¤Ÿè‡ªé€‚åº”åœ°æŠ‘åˆ¶æ”»å‡»ç‰¹å®šçš„ä¼ªåƒï¼ŒåŒæ—¶ä¿ç•™åˆ¤åˆ«çº¿ç´¢ã€‚</li>
<li>ä¿¡æ¯ç“¶é¢ˆæœ‰åŠ©äºæ¶ˆé™¤æ— ç”¨å˜é‡çš„å¹²æ‰°ï¼Œä¿ç•™å¯è½¬ç§»ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23618">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9e937bfdde636552f7190e8e6de71338~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932198&auth_key=1759932198-0-0-71e78db0c599bef5d59291fd28b60890&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d66b53ad0a1f15300a711c230a6843b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1759932206&auth_key=1759932206-0-0-f36e1dc0bf5999e00d32b0ca6f4c52cc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-3a5f056c536782a335b06da4f06eb919.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-35ad7ad69fcbf6a834ade422b6369c33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bf05d80f1729d39e2fbec5f27e5ddd0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ArFake-A-Multi-Dialect-Benchmark-and-Baselines-for-Arabic-Spoof-Speech-Detection"><a href="#ArFake-A-Multi-Dialect-Benchmark-and-Baselines-for-Arabic-Spoof-Speech-Detection" class="headerlink" title="ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech   Detection"></a>ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech   Detection</h2><p><strong>Authors:Mohamed Maged, Alhassan Ehab, Ali Mekky, Besher Hassan, Shady Shehata</strong></p>
<p>With the rise of generative text-to-speech models, distinguishing between real and synthetic speech has become challenging, especially for Arabic that have received limited research attention. Most spoof detection efforts have focused on English, leaving a significant gap for Arabic and its many dialects. In this work, we introduce the first multi-dialect Arabic spoofed speech dataset. To evaluate the difficulty of the synthesized audio from each model and determine which produces the most challenging samples, we aimed to guide the construction of our final dataset either by merging audios from multiple models or by selecting the best-performing model, we conducted an evaluation pipeline that included training classifiers using two approaches: modern embedding-based methods combined with classifier heads; classical machine learning algorithms applied to MFCC features; and the RawNet2 architecture. The pipeline further incorporated the calculation of Mean Opinion Score based on human ratings, as well as processing both original and synthesized datasets through an Automatic Speech Recognition model to measure the Word Error Rate. Our results demonstrate that FishSpeech outperforms other TTS models in Arabic voice cloning on the Casablanca corpus, producing more realistic and challenging synthetic speech samples. However, relying on a single TTS for dataset creation may limit generalizability. </p>
<blockquote>
<p>éšç€ç”Ÿæˆå¼æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹çš„å…´èµ·ï¼ŒåŒºåˆ†çœŸå®è¯­éŸ³å’Œåˆæˆè¯­éŸ³å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé˜¿æ‹‰ä¼¯è¯­è¿™ç§ç ”ç©¶å…³æ³¨åº¦ç›¸å¯¹è¾ƒä½çš„è¯­è¨€ã€‚å¤§å¤šæ•°è¯­éŸ³æ¬ºéª—æ£€æµ‹å·¥ä½œä¸»è¦é›†ä¸­åœ¨è‹±è¯­ä¸Šï¼Œä¸ºé˜¿æ‹‰ä¼¯è¯­åŠå…¶å¤šç§æ–¹è¨€ç•™ä¸‹äº†å·¨å¤§çš„ç©ºç™½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¦–ä¸ªå¤šæ–¹è¨€é˜¿æ‹‰ä¼¯è¯­è™šå‡è¯­éŸ³æ•°æ®é›†ã€‚ä¸ºäº†è¯„ä¼°æ¯ä¸ªæ¨¡å‹çš„åˆæˆéŸ³é¢‘çš„éš¾åº¦ï¼Œå¹¶ç¡®å®šå“ªä¸ªæ¨¡å‹äº§ç”Ÿäº†æœ€å…·æŒ‘æˆ˜æ€§çš„æ ·æœ¬ï¼Œæˆ‘ä»¬åœ¨æ„å»ºæœ€ç»ˆæ•°æ®é›†æ—¶ï¼Œæ—¨åœ¨é€šè¿‡åˆå¹¶å¤šä¸ªæ¨¡å‹çš„éŸ³é¢‘æˆ–é€‰æ‹©è¡¨ç°æœ€ä½³çš„æ¨¡å‹æ¥æŒ‡å¯¼ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªè¯„ä¼°ç®¡é“ï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¸¤ç§æ–¹æ³•æ¥è®­ç»ƒåˆ†ç±»å™¨ï¼šç»“åˆåˆ†ç±»å™¨å¤´çš„ç°ä»£åŸºäºåµŒå…¥çš„æ–¹æ³•å’Œåº”ç”¨äºMFCCç‰¹å¾çš„ç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•ï¼›ä»¥åŠRawNet2æ¶æ„ã€‚è¯¥ç®¡é“è¿˜çº³å…¥äº†åŸºäºäººç±»è¯„åˆ†çš„è®¡ç®—å¹³å‡æ„è§å¾—åˆ†ï¼Œä»¥åŠé€šè¿‡è¯­éŸ³è¯†åˆ«æ¨¡å‹å¤„ç†åŸå§‹å’Œåˆæˆæ•°æ®é›†ï¼Œä»¥æµ‹é‡å•è¯é”™è¯¯ç‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨é˜¿æ‹‰ä¼¯è¯­éŸ³å…‹éš†æ–¹é¢ï¼ŒFishSpeechåœ¨Casablancaè¯­æ–™åº“ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–TTSæ¨¡å‹ï¼Œèƒ½äº§ç”Ÿæ›´çœŸå®ã€æ›´å…·æŒ‘æˆ˜æ€§çš„åˆæˆè¯­éŸ³æ ·æœ¬ã€‚ç„¶è€Œï¼Œä»…ä¾èµ–å•ä¸€çš„TTSè¿›è¡Œæ•°æ®é›†åˆ›å»ºå¯èƒ½ä¼šé™åˆ¶å…¶é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22808v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†éšç€ç”Ÿæˆæ–‡æœ¬-è¯­éŸ³æ¨¡å‹çš„å‘å±•ï¼Œè¯†åˆ«é˜¿æ‹‰ä¼¯è¯­éŸ³çš„çœŸä¼ªå˜å¾—è¶Šæ¥è¶Šå›°éš¾ã€‚ç°æœ‰çš„æ¬ºéª—æ£€æµ‹ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­ä¸Šï¼Œè€Œé’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­çš„æ¬ºéª—è¯­éŸ³æ•°æ®é›†ç›¸å¯¹ç¼ºä¹ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ¨å‡ºäº†å¤šæ–¹è¨€é˜¿æ‹‰ä¼¯è¯­æ¬ºéª—è¯­éŸ³æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨å¤šç§æ¨¡å‹è¿›è¡Œæ„å»ºå’Œè¯„ä¼°ï¼ŒåŒ…æ‹¬ç°ä»£åµŒå…¥æ–¹æ³•å’Œåˆ†ç±»å™¨å¤´ç»“åˆçš„è®­ç»ƒåˆ†ç±»å™¨ã€åŸºäºMFCCç‰¹å¾çš„ç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•ä»¥åŠRawNet2æ¶æ„ã€‚é€šè¿‡è®¡ç®—åŸºäºäººç±»è¯„ä»·çš„å‡å€¼æ„è§å¾—åˆ†ä»¥åŠé€šè¿‡è¯­éŸ³è¯†åˆ«æ¨¡å‹æµ‹é‡åŸå§‹å’Œåˆæˆæ•°æ®é›†çš„è¯é”™è¯¯ç‡æ¥è¯„ä¼°åˆæˆéŸ³é¢‘çš„éš¾åº¦ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒFishSpeechåœ¨é˜¿æ‹‰ä¼¯è¯­éŸ³å…‹éš†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–TTSæ¨¡å‹ï¼Œä½†ä¾èµ–å•ä¸€TTSè¿›è¡Œæ•°æ®é›†åˆ›å»ºå¯èƒ½ä¼šé™åˆ¶å…¶æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ–‡æœ¬-è¯­éŸ³æ¨¡å‹çš„å‘å±•ä½¿å¾—è¯†åˆ«é˜¿æ‹‰ä¼¯è¯­éŸ³çš„çœŸä¼ªå˜å¾—å›°éš¾ã€‚</li>
<li>ç›®å‰é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­çš„æ¬ºéª—è¯­éŸ³æ•°æ®é›†ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡æ¨å‡ºå¤šæ–¹è¨€é˜¿æ‹‰ä¼¯è¯­æ¬ºéª—è¯­éŸ³æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡å¤šç§æ¨¡å‹æ„å»ºå’Œè¯„ä¼°æ•°æ®é›†ï¼ŒåŒ…æ‹¬ç°ä»£åµŒå…¥æ–¹æ³•å’Œåˆ†ç±»å™¨å¤´ç»“åˆçš„è®­ç»ƒåˆ†ç±»å™¨ã€åŸºäºMFCCç‰¹å¾çš„ç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•ä»¥åŠRawNet2æ¶æ„ã€‚</li>
<li>é€šè¿‡è®¡ç®—å‡å€¼æ„è§å¾—åˆ†å’Œè¯é”™è¯¯ç‡æ¥è¯„ä¼°åˆæˆéŸ³é¢‘çš„éš¾åº¦ã€‚</li>
<li>FishSpeechæ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­éŸ³å…‹éš†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22808">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3df5598ea1cf36a71d9bfbf7b601f53d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759974235&auth_key=1759974235-0-0-029019f5faa649bca5663bdec115946e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-8264da7a103146e4b5d4200ac83fb8c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a988d711cc16576608c94b0e9499f30.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-24708b42dcf9cc0d105371f9e4c7e5ac~resize:0:q75.jpg?source=1f5c5e47&expiration=1759974256&auth_key=1759974256-0-0-7b783d905afc1187b2ccf155721a1d8a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d3d25a26a37affde074686f7ec36f1c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759974262&auth_key=1759974262-0-0-26395b918694a89626074b4725dd8d44&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-e3899bb8de406c113815b1259a2213f3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="i-LAVA-Insights-on-Low-Latency-Voice-2-Voice-Architecture-for-Agents"><a href="#i-LAVA-Insights-on-Low-Latency-Voice-2-Voice-Architecture-for-Agents" class="headerlink" title="i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents"></a>i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents</h2><p><strong>Authors:Anupam Purwar, Aditya Choudhary</strong></p>
<p>We experiment with a low-latency, end-to-end voice-to-voice communication model to optimize it for real-time conversational applications. By analyzing components essential to voice to voice (V-2-V) system viz. automatic speech recognition (ASR), text-to-speech (TTS), and dialog management, our work analyzes how to reduce processing time while maintaining high-quality interactions to identify the levers for optimizing V-2-V system. Our work identifies that TTS component which generates life-like voice, full of emotions including natural pauses and exclamations has highest impact on Real time factor (RTF). The experimented V-2-V architecture utilizes CSM1b has the capability to understand tone as well as context of conversation by ingesting both audio and text of prior exchanges to generate contextually accurate speech. We explored optimization of Residual Vector Quantization (RVQ) iterations by the TTS decoder which come at a cost of decrease in the quality of voice generated. Our experimental evaluations also demonstrate that for V-2-V implementations based on CSM most important optimizations can be brought by reducing the number of RVQ Iterations along with the codebooks used in Mimi. </p>
<blockquote>
<p>æˆ‘ä»¬å®éªŒäº†ä¸€ç§ä½å»¶è¿Ÿã€ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³é€šä¿¡æ¨¡å‹ï¼Œä»¥ä¼˜åŒ–å…¶é€‚ç”¨äºå®æ—¶å¯¹è¯åº”ç”¨ã€‚é€šè¿‡åˆ†æè¯­éŸ³åˆ°è¯­éŸ³ï¼ˆV-2-Vï¼‰ç³»ç»Ÿçš„å…³é”®ç»„ä»¶ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å’Œå¯¹è¯ç®¡ç†ï¼Œæˆ‘ä»¬çš„å·¥ä½œåˆ†æäº†å¦‚ä½•åœ¨ä¿æŒé«˜è´¨é‡äº¤äº’çš„åŒæ—¶å‡å°‘å¤„ç†æ—¶é—´ï¼Œä»¥ç¡®å®šä¼˜åŒ–V-2-Vç³»ç»Ÿçš„å…³é”®è¦ç´ ã€‚æˆ‘ä»¬çš„å·¥ä½œç¡®å®šï¼ŒTTSç»„ä»¶å¯¹å®æ—¶å› ç´ ï¼ˆRTFï¼‰çš„å½±å“æœ€å¤§ï¼Œè¯¥ç»„ä»¶å¯ä»¥ç”Ÿæˆé€¼çœŸçš„å£°éŸ³ï¼Œå……æ»¡æƒ…æ„Ÿï¼ŒåŒ…æ‹¬è‡ªç„¶åœé¡¿å’Œæ„Ÿå¹ã€‚æ‰€è¯•éªŒçš„V-2-Væ¶æ„åˆ©ç”¨CSM1bï¼Œé€šè¿‡æ‘„å–å…ˆå‰çš„éŸ³é¢‘å’Œæ–‡æœ¬äº¤æ¢ï¼Œç†è§£å¯¹è¯çš„è¯­è°ƒä»¥åŠè¯­å¢ƒï¼Œä»è€Œç”Ÿæˆè¯­å¢ƒå‡†ç¡®çš„è¯­éŸ³ã€‚æˆ‘ä»¬æ¢ç´¢äº†é€šè¿‡TTSè§£ç å™¨ä¼˜åŒ–å‰©ä½™çŸ¢é‡é‡åŒ–ï¼ˆRVQï¼‰è¿­ä»£çš„æ–¹æ³•ï¼Œä½†è¿™ä¼šé™ä½ç”Ÿæˆçš„è¯­éŸ³è´¨é‡ã€‚æˆ‘ä»¬çš„å®éªŒè¯„ä¼°è¿˜è¡¨æ˜ï¼Œå¯¹äºåŸºäºCSMçš„V-2-Vå®ç°ï¼Œæœ€é‡è¦çš„ä¼˜åŒ–å¯ä»¥é€šè¿‡å‡å°‘RVQè¿­ä»£æ¬¡æ•°ä»¥åŠMimiä¸­ä½¿ç”¨çš„ä»£ç æœ¬æ•°é‡æ¥å®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20971v2">PDF</a> This paper analyzes a low-latency, end-to-end voice-to-voice (V-2-V)   architecture, identifying that the Text-to-Speech (TTS) component has the   highest impact on real-time performance. By reducing the number of Residual   Vector Quantization (RVQ) iterations in the TTS model, latency can be   effectively halved. Its accepted at AIML Systems 2025</p>
<p><strong>Summary</strong><br>æ–‡æœ¬å†…å®¹å…³äºå¦‚ä½•é€šè¿‡åˆ†æè¯­éŸ³è½¬è¯­éŸ³ç³»ç»Ÿçš„é‡è¦ç»„ä»¶ï¼ˆå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬è½¬è¯­éŸ³å’Œå¯¹è¯ç®¡ç†ï¼‰æ¥ä¼˜åŒ–å®æ—¶å¯¹è¯åº”ç”¨çš„ä½å»¶è¿Ÿç«¯åˆ°ç«¯è¯­éŸ³è½¬è¯­éŸ³é€šä¿¡æ¨¡å‹ã€‚å…¶ä¸­ï¼Œæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç»„ä»¶å¯¹äºä¿æŒå®æ—¶å› ç´ ï¼ˆRTFï¼‰å…·æœ‰é‡å¤§å½±å“ï¼Œå› æ­¤æœ¬æ–‡ç‰¹åˆ«å…³æ³¨ä¼˜åŒ–è¯¥ç»„ä»¶çš„æ–¹æ³•ã€‚æ–‡ä¸­æ¢è®¨äº†ä¸€ç§å‡å°‘çŸ¢é‡é‡åŒ–è¿­ä»£æ¬¡æ•°ï¼ˆRVQï¼‰ä»¥é™ä½ç”Ÿæˆçš„è¯­éŸ³è´¨é‡æˆæœ¬çš„æ–¹æ³•ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥æ–‡æœ¬å±•ç¤ºäº†å¦‚ä½•é€šè¿‡å‡å°‘RVQè¿­ä»£å’Œä½¿ç”¨çš„ä»£ç æœ¬æ•°é‡æ¥å®ç°è¯­éŸ³è½¬è¯­éŸ³ç³»ç»Ÿçš„ä¼˜åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šè¿‡å®éªŒæ¢è®¨äº†ä½å»¶è¿Ÿçš„ç«¯åˆ°ç«¯è¯­éŸ³è½¬è¯­éŸ³ï¼ˆV-2-Vï¼‰é€šä¿¡æ¨¡å‹ï¼Œæ—¨åœ¨ä¼˜åŒ–å®æ—¶å¯¹è¯åº”ç”¨ã€‚</li>
<li>åˆ†æäº†è¯­éŸ³è½¬è¯­éŸ³ç³»ç»Ÿçš„å…³é”®ç»„ä»¶ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰å’Œå¯¹è¯ç®¡ç†ã€‚</li>
<li>æŒ‡å‡ºTTSç»„ä»¶å¯¹äºä¿æŒå®æ—¶å› ç´ ï¼ˆRTFï¼‰å…·æœ‰é‡å¤§å½±å“ï¼Œå¹¶æ¢è®¨äº†ä¼˜åŒ–è¯¥ç»„ä»¶çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å‡å°‘çŸ¢é‡é‡åŒ–ï¼ˆRVQï¼‰è¿­ä»£æ¬¡æ•°æ¥ä¼˜åŒ–TTSè§£ç å™¨ï¼Œä½†éœ€æ³¨æ„è¿™å¯èƒ½ä¼šé™ä½ç”Ÿæˆçš„è¯­éŸ³è´¨é‡ã€‚</li>
<li>å®éªŒæ€§è¯„ä¼°è¡¨æ˜ï¼Œå¯¹äºåŸºäºCSMçš„V-2-Vå®ç°ï¼Œæœ€é‡è¦çš„ä¼˜åŒ–å¯ä»¥é€šè¿‡å‡å°‘RVQè¿­ä»£æ¬¡æ•°å’Œä½¿ç”¨è¾ƒå°‘çš„ä»£ç æœ¬å®ç°ã€‚</li>
<li>æ‰€é‡‡ç”¨çš„V-2-Væ¶æ„åˆ©ç”¨CSM1bèƒ½å¤Ÿç†è§£è¯­è°ƒä»¥åŠå¯¹è¯ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ï¼Œé€šè¿‡åŒæ—¶å¤„ç†å…ˆå‰çš„éŸ³é¢‘å’Œæ–‡æœ¬äº¤æµæ¥ç”Ÿæˆå‡†ç¡®çš„è¯­å¢ƒåŒ–è¯­éŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-59c821b57aeaa9534dba35b42a820d49.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-d9d49adfa80e0c9bd41043f26e3e711e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759974284&auth_key=1759974284-0-0-33edf38df59f5e141b42a004337c0e64&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-b34709ac926874eba4eb35aeca95e069.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9be302502aa6d7901d79ac3ba0d45e1b.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-0415f218174ef0ecb084055ac4fadefd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759974303&auth_key=1759974303-0-0-8403120c48cd05265ed182ff1f36e9df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-0c674493e85b2953c23110196944d153.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0750e443c0ec570d2f4ab834004e6752.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="FuseCodec-Semantic-Contextual-Fusion-and-Supervision-for-Neural-Codecs"><a href="#FuseCodec-Semantic-Contextual-Fusion-and-Supervision-for-Neural-Codecs" class="headerlink" title="FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs"></a>FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs</h2><p><strong>Authors:Md Mubtasim Ahasan, Rafat Hasan Khan, Tasnim Mohiuddin, Aman Chadha, Tariq Iqbal, M Ashraful Amin, Amin Ahsan Ali, Md Mofijul Islam, A K M Mahbubur Rahman</strong></p>
<p>Speech tokenization enables discrete representation and facilitates speech language modeling. However, existing neural codecs capture low-level acoustic features, overlooking the semantic and contextual cues inherent to human speech. While recent efforts introduced semantic representations from self-supervised speech models or incorporated contextual representations from pre-trained language models, challenges remain in aligning and unifying the semantic and contextual representations. We introduce FuseCodec, which unifies acoustic, semantic, and contextual representations through strong cross-modal alignment and globally informed supervision. We propose three complementary techniques: (i) Latent Representation Fusion, integrating semantic and contextual features directly into the encoder latent space for robust and unified representation learning; (ii) Global Semantic-Contextual Supervision, supervising discrete tokens with globally pooled and broadcasted representations to enhance temporal consistency and cross-modal alignment; and (iii) Temporally Aligned Contextual Supervision, strengthening alignment by dynamically matching contextual and speech tokens within a local window for fine-grained token-level supervision. We further introduce FuseCodec-TTS, demonstrating our methodologyâ€™s applicability to zero-shot speech synthesis. Empirically, FuseCodec achieves state-of-the-art performance in LibriSpeech, surpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy, perceptual quality, intelligibility, and speaker similarity. Results highlight the effectiveness of contextually and semantically guided tokenization for speech tokenization and downstream tasks. Code and pretrained models are available at <a target="_blank" rel="noopener" href="https://github.com/mubtasimahasan/FuseCodec">https://github.com/mubtasimahasan/FuseCodec</a>. </p>
<blockquote>
<p>è¯­éŸ³æ ‡è®°åŒ–èƒ½å¤Ÿå®ç°ç¦»æ•£è¡¨ç¤ºå¹¶ä¿ƒè¿›è¯­éŸ³è¯­è¨€å»ºæ¨¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç¥ç»ç¼–è§£ç å™¨æ•æ‰ä½çº§åˆ«çš„å£°å­¦ç‰¹å¾ï¼Œå¿½ç•¥äº†äººç±»è¯­éŸ³æ‰€å›ºæœ‰çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡çº¿ç´¢ã€‚è™½ç„¶æœ€è¿‘çš„åŠªåŠ›å¼•å…¥äº†æ¥è‡ªè‡ªæˆ‘ç›‘ç£è¯­éŸ³æ¨¡å‹çš„è¯­ä¹‰è¡¨ç¤ºæˆ–ç»“åˆäº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œä½†åœ¨å¯¹é½å’Œç»Ÿä¸€è¯­ä¹‰å’Œä¸Šä¸‹æ–‡è¡¨ç¤ºæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†FuseCodecï¼Œå®ƒé€šè¿‡å¼ºå¤§çš„è·¨æ¨¡æ€å¯¹é½å’Œå…¨å±€ä¿¡æ¯ç›‘ç£ï¼Œç»Ÿä¸€äº†å£°å­¦ã€è¯­ä¹‰å’Œä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§äº’è¡¥çš„æŠ€æœ¯ï¼šï¼ˆiï¼‰æ½œåœ¨è¡¨ç¤ºèåˆï¼Œç›´æ¥å°†è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ç‰¹å¾é›†æˆåˆ°ç¼–ç å™¨æ½œåœ¨ç©ºé—´ï¼Œä»¥å®ç°ç¨³å¥å’Œç»Ÿä¸€çš„è¡¨ç¤ºå­¦ä¹ ï¼›ï¼ˆiiï¼‰å…¨å±€è¯­ä¹‰ä¸Šä¸‹æ–‡ç›‘ç£ï¼Œç”¨å…¨å±€æ± åŒ–å’Œå¹¿æ’­çš„è¡¨ç¤ºæ¥ç›‘ç£ç¦»æ•£æ ‡è®°ï¼Œä»¥å¢å¼ºæ—¶é—´ä¸€è‡´æ€§å’Œè·¨æ¨¡æ€å¯¹é½ï¼›ï¼ˆiiiï¼‰ä¸´æ—¶å¯¹é½ä¸Šä¸‹æ–‡ç›‘ç£ï¼Œé€šè¿‡åœ¨å±€éƒ¨çª—å£å†…åŠ¨æ€åŒ¹é…ä¸Šä¸‹æ–‡å’Œè¯­éŸ³æ ‡è®°æ¥åŠ å¼ºå¯¹é½ï¼Œä»¥å®ç°ç²¾ç»†çš„æ ‡è®°çº§ç›‘ç£ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¨å‡ºäº†FuseCodec-TTSï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨é›¶æ ·æœ¬è¯­éŸ³åˆæˆä¸­çš„é€‚ç”¨æ€§ã€‚ç»éªŒè¡¨æ˜ï¼ŒFuseCodecåœ¨LibriSpeechä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è½¬å½•å‡†ç¡®æ€§ã€æ„ŸçŸ¥è´¨é‡ã€æ¸…æ™°åº¦å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢è¶…è¶Šäº†EnCodecã€SpeechTokenizerå’ŒDACã€‚ç»“æœçªå‡ºäº†ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰å¼•å¯¼çš„æ ‡è®°åŒ–åœ¨è¯­éŸ³æ ‡è®°åŒ–å’Œä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œé¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mubtasimahasan/FuseCodec">https://github.com/mubtasimahasan/FuseCodec</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11425v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Speech tokenizationçš„é‡è¦æ€§ä»¥åŠç°æœ‰ç¥ç»ç¼–ç å™¨çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†FuseCodecï¼Œå®ƒé€šè¿‡å¼ºå¤§çš„è·¨æ¨¡æ€å¯¹é½å’Œå…¨å±€ç›‘ç£ï¼Œç»Ÿä¸€äº†å£°éŸ³ã€è¯­ä¹‰å’Œä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚æ–‡ç« è¿˜ä»‹ç»äº†ä¸‰ç§äº’è¡¥æŠ€æœ¯ï¼šæ½œåœ¨è¡¨ç¤ºèåˆã€å…¨å±€è¯­ä¹‰ä¸Šä¸‹æ–‡ç›‘ç£ã€æ—¶é—´å¯¹é½ä¸Šä¸‹æ–‡ç›‘ç£ã€‚FuseCodec-TTSçš„å¼•å…¥è¯æ˜äº†è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬è¯­éŸ³åˆæˆä¸­çš„é€‚ç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFuseCodecåœ¨LibriSpeechæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒ…æ‹¬EnCodecã€SpeechTokenizerå’ŒDACï¼Œä½“ç°äº†ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰å¼•å¯¼tokenizationåœ¨è¯­éŸ³tokenizationå’Œä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Speech tokenizationå¯¹äºè¯­éŸ³è¯­è¨€å»ºæ¨¡éå¸¸é‡è¦ï¼Œå®ƒå¯ä»¥ä½¿è¯­éŸ³è¡¨è¾¾ç¦»æ•£åŒ–ã€‚</li>
<li>ç°æœ‰ç¥ç»ç¼–ç å™¨ä¸»è¦æ•æ‰ä½å±‚æ¬¡çš„å£°å­¦ç‰¹å¾ï¼Œå¿½ç•¥äº†è¯­éŸ³çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡çº¿ç´¢ã€‚</li>
<li>FuseCodecé€šè¿‡å¼ºå¤§çš„è·¨æ¨¡æ€å¯¹é½å’Œå…¨å±€ç›‘ç£æ¥ç»Ÿä¸€å£°å­¦ã€è¯­ä¹‰å’Œä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚</li>
<li>FuseCodecå¼•å…¥äº†ä¸‰ç§äº’è¡¥æŠ€æœ¯ï¼šæ½œåœ¨è¡¨ç¤ºèåˆã€å…¨å±€è¯­ä¹‰ä¸Šä¸‹æ–‡ç›‘ç£å’Œæ—¶é—´å¯¹é½ä¸Šä¸‹æ–‡ç›‘ç£ã€‚</li>
<li>FuseCodec-TTSçš„å¼•å…¥è¯æ˜äº†è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬è¯­éŸ³åˆæˆä¸­çš„é€‚ç”¨æ€§ã€‚</li>
<li>FuseCodecåœ¨LibriSpeechæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è½¬å½•å‡†ç¡®æ€§ã€æ„ŸçŸ¥è´¨é‡ã€æ¸…æ™°åº¦å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0720c936330af45dbfa84de9dcfcb7be.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-6999087191f6f9a62755b349dfb59dbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1759974333&auth_key=1759974333-0-0-a0a6748a2253e6dddc0fa0fcdb41ebce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-d74e5a95829cb9039559b584f76a172f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a16ba197cab782ddf753fb5c6e17f38.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Streaming-Sequence-to-Sequence-Learning-with-Delayed-Streams-Modeling"><a href="#Streaming-Sequence-to-Sequence-Learning-with-Delayed-Streams-Modeling" class="headerlink" title="Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling"></a>Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling</h2><p><strong>Authors:Neil Zeghidour, Eugene Kharitonov, Manu Orsini, VÃ¡clav Volhejn, Gabriel de Marmiesse, Edouard Grave, Patrick PÃ©rez, Laurent MazarÃ©, Alexandre DÃ©fossez</strong></p>
<p>We introduce Delayed Streams Modeling (DSM), a flexible formulation for streaming, multimodal sequence-to-sequence learning. Sequence-to-sequence generation is often cast in an offline manner, where the model consumes the complete input sequence before generating the first output timestep. Alternatively, streaming sequence-to-sequence rely on learning a policy for choosing when to advance on the input stream, or write to the output stream. DSM instead models already time-aligned streams with a decoder-only language model. By moving the alignment to a pre-processing step,and introducing appropriate delays between streams, DSM provides streaming inference of arbitrary output sequences, from any input combination, making it applicable to many sequence-to-sequence problems. In particular, given text and audio streams, automatic speech recognition (ASR) corresponds to the text stream being delayed, while the opposite gives a text-to-speech (TTS) model. We perform extensive experiments for these two major sequence-to-sequence tasks, showing that DSM provides state-of-the-art performance and latency while supporting arbitrary long sequences, being even competitive with offline baselines. Code, samples and demos are available at <a target="_blank" rel="noopener" href="https://github.com/kyutai-labs/delayed-streams-modeling">https://github.com/kyutai-labs/delayed-streams-modeling</a> </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†å»¶è¿Ÿæµå»ºæ¨¡ï¼ˆDSMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæµå¼ã€å¤šæ¨¡æ€åºåˆ—åˆ°åºåˆ—å­¦ä¹ çš„çµæ´»æ–¹æ³•ã€‚åºåˆ—åˆ°åºåˆ—ç”Ÿæˆé€šå¸¸ä»¥ä¸€ç§ç¦»çº¿çš„æ–¹å¼è¿›è¡Œï¼Œå³æ¨¡å‹åœ¨ç”Ÿæˆç¬¬ä¸€ä¸ªè¾“å‡ºæ—¶é—´æ­¥ä¹‹å‰æ¶ˆè€—å®Œæ•´çš„è¾“å…¥åºåˆ—ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæµå¼åºåˆ—åˆ°åºåˆ—åˆ™ä¾èµ–äºå­¦ä¹ ç­–ç•¥ï¼Œä»¥ç¡®å®šä½•æ—¶æ¨è¿›è¾“å…¥æµæˆ–å†™å…¥è¾“å‡ºæµã€‚ç„¶è€Œï¼ŒDSMä½¿ç”¨ä»…è§£ç çš„è¯­è¨€æ¨¡å‹å¯¹å·²ç»æ—¶é—´å¯¹é½çš„æµè¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡å°†å¯¹é½ç§»è‡³é¢„å¤„ç†æ­¥éª¤ï¼Œå¹¶åœ¨æµä¹‹é—´å¼•å…¥é€‚å½“çš„å»¶è¿Ÿï¼ŒDSMæä¾›äº†ä»»æ„è¾“å‡ºåºåˆ—çš„æµå¼æ¨æ–­ï¼Œé€‚ç”¨äºè®¸å¤šåºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼Œé€‚ç”¨äºä»»æ„è¾“å…¥ç»„åˆã€‚ç‰¹åˆ«æ˜¯ç»™å®šæ–‡æœ¬å’ŒéŸ³é¢‘æµæ—¶ï¼Œè¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å¯¹åº”äºæ–‡æœ¬æµå»¶è¿Ÿï¼Œè€Œç›¸ååˆ™ç»™å‡ºæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ã€‚æˆ‘ä»¬å¯¹è¿™ä¸¤ä¸ªä¸»è¦çš„åºåˆ—åˆ°åºåˆ—ä»»åŠ¡è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒDSMåœ¨æä¾›æœ€å…ˆè¿›çš„æ€§èƒ½å’Œå»¶è¿Ÿçš„åŒæ—¶ï¼Œè¿˜æ”¯æŒä»»æ„é•¿åºåˆ—ï¼Œç”šè‡³ä¸ç¦»çº¿åŸºå‡†ç«äº‰ã€‚ä»£ç ã€æ ·æœ¬å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/kyutai-labs/delayed-streams-modeling%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kyutai-labs/delayed-streams-modelingæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08753v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å»¶è¿Ÿæµå»ºæ¨¡ï¼ˆDSMï¼‰è¿™ä¸€çµæ´»çš„å¤šæ¨¡æ€åºåˆ—åˆ°åºåˆ—å­¦ä¹ æ¡†æ¶ã€‚ä¼ ç»Ÿçš„åºåˆ—åˆ°åºåˆ—ç”Ÿæˆé€šå¸¸æ˜¯ç¦»çº¿è¿›è¡Œçš„ï¼Œè€ŒDSMåˆ™å°†è¾“å…¥æµå’Œè¾“å‡ºæµçš„å¯¹é½è¿‡ç¨‹ç§»è‡³é¢„å¤„ç†é˜¶æ®µï¼Œé€šè¿‡å¼•å…¥é€‚å½“çš„å»¶è¿Ÿï¼Œå®ç°äº†ä»»æ„è¾“å‡ºåºåˆ—çš„æµå¼æ¨æ–­ã€‚è¿™ä½¿å¾—DSMé€‚ç”¨äºå¤šç§åºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼Œå¦‚è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ã€‚å®éªŒè¡¨æ˜ï¼ŒDSMå…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½å’Œä½å»¶è¿Ÿï¼Œæ”¯æŒä»»æ„é•¿åºåˆ—ï¼Œä¸ç¦»çº¿åŸºå‡†æµ‹è¯•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å»¶è¿Ÿæµå»ºæ¨¡ï¼ˆDSMï¼‰æ˜¯ä¸€ä¸ªçµæ´»çš„å¤šæ¨¡æ€åºåˆ—åˆ°åºåˆ—å­¦ä¹ æ¡†æ¶ï¼Œé€‚ç”¨äºæµå¼å¤„ç†ã€‚</li>
<li>ä¼ ç»Ÿåºåˆ—åˆ°åºåˆ—ç”Ÿæˆæ˜¯ç¦»çº¿çš„ï¼Œè€ŒDSMå®ç°æµå¼æ¨æ–­ï¼Œå¯åœ¨ç”Ÿæˆè¾“å‡ºæ—¶é€æ­¥å¤„ç†è¾“å…¥ã€‚</li>
<li>DSMé€šè¿‡é¢„å¤„ç†çš„æ­¥éª¤è¿›è¡Œè¾“å…¥å’Œè¾“å‡ºæµçš„å¯¹é½ã€‚</li>
<li>é€šè¿‡å¼•å…¥é€‚å½“çš„å»¶è¿Ÿï¼ŒDSMå¯ä»¥åº”ç”¨äºå¤šç§åºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼Œå¦‚è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ï¼ˆTTSï¼‰ã€‚</li>
<li>åœ¨ä¸»è¦ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDSMå…·æœ‰å…ˆè¿›çš„æ€§èƒ½å’Œä½å»¶è¿Ÿã€‚</li>
<li>DSMæ”¯æŒä»»æ„é•¿åº¦çš„åºåˆ—å¤„ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-70ff91ccbdaebebb45f0bd79f253971e.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-f5b42e42e834063e3174a20fa29ab19f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759974360&auth_key=1759974360-0-0-875874d3438ab3a2bece9ee28b318f0d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d631c769a50d4ff489a42e5de7e69d29~resize:0:q75.jpg?source=1f5c5e47&expiration=1759974367&auth_key=1759974367-0-0-fa594d768acc6a5505f183763365e86d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CTTS-Collective-Test-Time-Scaling"><a href="#CTTS-Collective-Test-Time-Scaling" class="headerlink" title="CTTS: Collective Test-Time Scaling"></a>CTTS: Collective Test-Time Scaling</h2><p><strong>Authors:Zhende Song, Shengji Tang, Peng Ye, Jiayuan Fan, Lei Bai, Tao Chen, Wanli Ouyang</strong></p>
<p>Test-time scaling (TTS) has emerged as a promising, training-free approach for enhancing large language model (LLM) performance. However, the efficacy of existing methods, such as Best-of-N and Self-Consistency, is fundamentally constrained by the dominant single test-time scaling (STTS) paradigm, which relies on a single LLM agent interacting with a single reward model (SA-SR). Inspired by recent work showing that collective methods can surpass the performance ceiling of individual models, we introduce Collective Test-Time Scaling (CTTS). First, we systematically investigate three primary interaction paradigms of existing multiple models: single-agent-multi-reward (SA-MR), multi-agent-single-reward (MA-SR), and multi-agent-multi-reward (MA-MR). Extensive experiments reveal that the MA-MR paradigm is consistently superior. Based on this finding, we further propose CTTS-MM, a novel framework that operationalizes multi-agent and multi-reward collaboration. CTTS-MM integrates two key technical contributions: (1) for agent collaboration, an Agent Collaboration Search (ACS) that identifies the most effective combination of LLMs from a candidate pool; and (2) for reward model collaboration, a Mixture of Reward Models (MoR) strategy that leverages a Prior Reward model Ensemble Selection (PRES) algorithm to select the optimal ensemble. Evaluations across seven mainstream benchmarks demonstrate that CTTS-MM significantly outperforms leading STTS methods (+4.82% over Best-of-N) and surpasses even flagship proprietary LLMs (+7.06% over GPT-4.1) and open-source LLMs. These results highlight the substantial potential of collective scaling to push the frontier of LLM inference. Code will be released at <a target="_blank" rel="noopener" href="https://github.com/magent4aci/CTTS-MM">https://github.com/magent4aci/CTTS-MM</a>. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ä½œä¸ºä¸€ç§æ— éœ€è®­ç»ƒå³å¯æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½çš„æ–¹æ³•ï¼Œå±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ï¼ˆå¦‚Né€‰æœ€ä½³å’Œè‡ªä¸€è‡´æ€§ï¼‰çš„æ•ˆåŠ›ä»æ ¹æœ¬ä¸Šå—åˆ°å•ä¸€æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆSTTSï¼‰èŒƒå¼çš„é™åˆ¶ï¼Œè¯¥èŒƒå¼ä¾èµ–äºå•ä¸ªLLMä»£ç†ä¸å•ä¸ªå¥–åŠ±æ¨¡å‹ï¼ˆSA-SRï¼‰è¿›è¡Œäº¤äº’ã€‚æœ€è¿‘çš„å·¥ä½œè¡¨æ˜ï¼Œé›†ä½“æ–¹æ³•èƒ½å¤Ÿè¶…è¶Šä¸ªä½“æ¨¡å‹çš„æ€§èƒ½ä¸Šé™ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥äº†é›†ä½“æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆCTTSï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†ç°æœ‰å¤šä¸ªæ¨¡å‹çš„ä¸»è¦äº¤äº’èŒƒå¼ï¼šå•ä»£ç†å¤šå¥–åŠ±ï¼ˆSA-MRï¼‰ã€å¤šä»£ç†å•å¥–åŠ±ï¼ˆMA-SRï¼‰å’Œå¤šä»£ç†å¤šå¥–åŠ±ï¼ˆMA-MRï¼‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMA-MRèŒƒå¼å§‹ç»ˆå…·æœ‰ä¼˜åŠ¿ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†CTTS-MMï¼Œä¸€ä¸ªæ“ä½œåŒ–å¤šä»£ç†å’Œå¤šå¥–åŠ±åä½œçš„æ–°æ¡†æ¶ã€‚CTTS-MMé›†æˆäº†ä¸¤é¡¹å…³é”®æŠ€æœ¯è´¡çŒ®ï¼šï¼ˆ1) å¯¹äºä»£ç†åä½œï¼ŒAgent Collaboration Searchï¼ˆACSï¼‰èƒ½å¤Ÿä»å€™é€‰æ± ä¸­è¯†åˆ«å‡ºæœ€æœ‰æ•ˆçš„LLMç»„åˆï¼›ï¼ˆ2) å¯¹äºå¥–åŠ±æ¨¡å‹åä½œï¼Œé‡‡ç”¨Mixture of Reward Modelsï¼ˆMoRï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨Prior Reward model Ensemble Selectionï¼ˆPRESï¼‰ç®—æ³•é€‰æ‹©æœ€ä½³ç»„åˆã€‚åœ¨ä¸ƒä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCTTS-MMæ˜¾è‘—ä¼˜äºé¢†å…ˆçš„STTSæ–¹æ³•ï¼ˆ+4.82%ä¼˜äºNé€‰æœ€ä½³ï¼‰ï¼Œç”šè‡³è¶…è¿‡æ——èˆ°ä¸“æœ‰LLMï¼ˆ+7.06%ä¼˜äºGPT-4.1ï¼‰å’Œå¼€æºLLMã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†é›†ä½“ç¼©æ”¾æ¨åŠ¨LLMæ¨ç†å‰æ²¿çš„å·¨å¤§æ½œåŠ›ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/magent4aci/CTTS-MM%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/magent4aci/CTTS-MMå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03333v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½æ–¹é¢çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¦‚Best-of-Nå’ŒSelf-Consistencyå—åˆ°å•ä¸€æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆSTTSï¼‰èŒƒå¼çš„é™åˆ¶ã€‚å—å¤šæ¨¡å‹ååŒå·¥ä½œè¶…è¶Šä¸ªä½“æ¨¡å‹æ€§èƒ½ä¸Šé™çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†é›†ä½“æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆCTTSï¼‰ã€‚é€šè¿‡ç³»ç»Ÿç ”ç©¶ç°æœ‰å¤šä¸ªæ¨¡å‹çš„ä¸‰ç±»äº¤äº’èŒƒå¼ï¼Œå‘ç°å¤šä»£ç†å¤šå¥–åŠ±ï¼ˆMA-MRï¼‰èŒƒå¼è¡¨ç°æœ€ä½³ã€‚åŸºäºæ­¤ï¼Œè¿›ä¸€æ­¥æå‡ºäº†CTTS-MMæ¡†æ¶ï¼Œå®ç°äº†å¤šä»£ç†å’Œå¤šå¥–åŠ±çš„ååŒå·¥ä½œã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®æŠ€æœ¯è´¡çŒ®ï¼šä¸€æ˜¯ä»£ç†åä½œæœç´¢ï¼ˆACSï¼‰ï¼Œä»å€™é€‰æ± ä¸­è¯†åˆ«æœ€æœ‰æ•ˆçš„LLMç»„åˆï¼›äºŒæ˜¯å¥–åŠ±æ¨¡å‹åä½œçš„æ··åˆå¥–åŠ±æ¨¡å‹ï¼ˆMoRï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨å…ˆéªŒå¥–åŠ±æ¨¡å‹é›†åˆé€‰æ‹©ï¼ˆPRESï¼‰ç®—æ³•é€‰æ‹©æœ€ä½³é›†åˆã€‚åœ¨ä¸ƒä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCTTS-MMæ˜¾è‘—ä¼˜äºé¢†å…ˆçš„STTSæ–¹æ³•ï¼ˆè¾ƒBest-of-Næé«˜4.82%ï¼‰ï¼Œç”šè‡³è¶…è¿‡äº†æ——èˆ°ä¸“æœ‰LLMï¼ˆè¾ƒGPT-4.1æé«˜7.06%ï¼‰ã€‚è¿™çªæ˜¾äº†é›†ä½“ç¼©æ”¾æ¨åŠ¨LLMæ¨ç†å‰æ²¿çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰æ˜¯æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½çš„ä¸€ç§æœ‰å‰é€”çš„è®­ç»ƒå¤–æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å—é™äºå•ä¸€æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆSTTSï¼‰èŒƒå¼ï¼Œè€Œå¤šæ¨¡å‹ååŒå·¥ä½œå¯çªç ´æ€§èƒ½ä¸Šé™ã€‚</li>
<li>å¼•å…¥é›†ä½“æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆCTTSï¼‰ï¼Œé€šè¿‡ç³»ç»Ÿç ”ç©¶ä¸‰ä¸ªä¸»è¦äº¤äº’èŒƒå¼ï¼Œå‘ç°å¤šä»£ç†å¤šå¥–åŠ±ï¼ˆMA-MRï¼‰èŒƒå¼è¡¨ç°æœ€ä½³ã€‚</li>
<li>æå‡ºCTTS-MMæ¡†æ¶ï¼Œå®ç°å¤šä»£ç†å’Œå¤šå¥–åŠ±çš„ååŒå·¥ä½œï¼ŒåŒ…æ‹¬ä»£ç†åä½œæœç´¢ï¼ˆACSï¼‰å’Œæ··åˆå¥–åŠ±æ¨¡å‹ï¼ˆMoRï¼‰ç­–ç•¥ã€‚</li>
<li>CTTS-MMæ¡†æ¶åœ¨ä¸ƒä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œè¾ƒé¢†å…ˆçš„STTSæ–¹æ³•å’ŒGPT-4.1æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>é›†ä½“ç¼©æ”¾å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå¯æ¨åŠ¨LLMæ¨ç†å‰æ²¿çš„è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4a0e8ae6fc66e8b478334aff78339ed8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759974375&auth_key=1759974375-0-0-ad8cf985a2a5d7ab50f5c89959634791&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a406732ca6097db8bb3fe79fa490b6a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759974382&auth_key=1759974382-0-0-12335002f4db7c04bb53cb18791861f0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-cbeb0992d9bf45c92db7170327c2363f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1633ffcbf7e574ee1160aec5f052fa9e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4dca72cb12cb1778b4471294a5ef8b62.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  The Dialogue That Heals A Comprehensive Evaluation of Doctor Agents'   Inquiry Capability
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c2a648cfee957f9ebca2446ab0a36d4d.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  A Scalable Distributed Framework for Multimodal GigaVoxel Image   Registration
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30806.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
