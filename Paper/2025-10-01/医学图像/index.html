<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  A Scalable Distributed Framework for Multimodal GigaVoxel Image   Registration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c2a648cfee957f9ebca2446ab0a36d4d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-01-æ›´æ–°"><a href="#2025-10-01-æ›´æ–°" class="headerlink" title="2025-10-01 æ›´æ–°"></a>2025-10-01 æ›´æ–°</h1><h2 id="A-Scalable-Distributed-Framework-for-Multimodal-GigaVoxel-Image-Registration"><a href="#A-Scalable-Distributed-Framework-for-Multimodal-GigaVoxel-Image-Registration" class="headerlink" title="A Scalable Distributed Framework for Multimodal GigaVoxel Image   Registration"></a>A Scalable Distributed Framework for Multimodal GigaVoxel Image   Registration</h2><p><strong>Authors:Rohit Jena, Vedant Zope, Pratik Chaudhari, James C. Gee</strong></p>
<p>In this work, we propose FFDP, a set of IO-aware non-GEMM fused kernels supplemented with a distributed framework for image registration at unprecedented scales. Image registration is an inverse problem fundamental to biomedical and life sciences, but algorithms have not scaled in tandem with image acquisition capabilities. Our framework complements existing model parallelism techniques proposed for large-scale transformer training by optimizing non-GEMM bottlenecks and enabling convolution-aware tensor sharding. We demonstrate unprecedented capabilities by performing multimodal registration of a 100 micron ex-vivo human brain MRI volume at native resolution - an inverse problem more than 570x larger than a standard clinical datum in about a minute using only 8 A6000 GPUs. FFDP accelerates existing state-of-the-art optimization and deep learning registration pipelines by upto 6 - 7x while reducing peak memory consumption by 20 - 59%. Comparative analysis on a 250 micron dataset shows that FFDP can fit upto 64x larger problems than existing SOTA on a single GPU, and highlights both the performance and efficiency gains of FFDP compared to SOTA image registration methods. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FFDPï¼Œè¿™æ˜¯ä¸€ç»„IOæ„ŸçŸ¥çš„éGEMMèåˆå†…æ ¸ï¼Œè¾…ä»¥åˆ†å¸ƒå¼æ¡†æ¶ï¼Œç”¨äºå‰æ‰€æœªæœ‰çš„å¤§è§„æ¨¡å›¾åƒé…å‡†ã€‚å›¾åƒé…å‡†æ˜¯ç”Ÿç‰©åŒ»å­¦å’Œç”Ÿå‘½ç§‘å­¦ä¸­çš„åŸºæœ¬é€†å‘é—®é¢˜ï¼Œä½†ç®—æ³•çš„æ‰©å±•å¹¶æ²¡æœ‰ä¸å›¾åƒé‡‡é›†èƒ½åŠ›åŒæ­¥ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡ä¼˜åŒ–éGEMMç“¶é¢ˆå¹¶å¯ç”¨å·ç§¯æ„ŸçŸ¥çš„å¼ é‡åˆ†ç‰‡ï¼Œè¡¥å……äº†é’ˆå¯¹å¤§è§„æ¨¡å˜æ¢å™¨è®­ç»ƒæå‡ºçš„ç°æœ‰æ¨¡å‹å¹¶è¡ŒæŠ€æœ¯ã€‚æˆ‘ä»¬é€šè¿‡ä»¥åŸç”Ÿåˆ†è¾¨ç‡å¯¹ç¦»ä½“äººè„‘MRIä½“ç§¯è¿›è¡Œ100å¾®ç±³çš„å¤šæ¨¡æ€é…å‡†ï¼Œå±•ç¤ºäº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›â€”â€”è¿™æ˜¯ä¸€ä¸ªé€†å‘é—®é¢˜ï¼Œåœ¨å¤§çº¦ä¸€åˆ†é’Ÿå†…ä½¿ç”¨ä»…8ä¸ªA6000 GPUè§£å†³äº†è¶…è¿‡æ ‡å‡†ä¸´åºŠæ•°æ®570å€çš„é—®é¢˜ã€‚FFDPé€šè¿‡é«˜è¾¾6-7å€çš„é€Ÿåº¦ï¼ŒåŠ é€Ÿäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„ä¼˜åŒ–å’Œæ·±åº¦å­¦ä¹ é…å‡†ç®¡é“ï¼ŒåŒæ—¶é™ä½äº†å³°å€¼å†…å­˜æ¶ˆè€—20-59%ã€‚åœ¨250å¾®ç±³æ•°æ®é›†ä¸Šçš„å¯¹æ¯”åˆ†ææ˜¾ç¤ºï¼ŒFFDPå¯ä»¥åœ¨å•ä¸ªGPUä¸Šé€‚åº”æ¯”ç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯é«˜64å€çš„é—®é¢˜ï¼Œå¹¶çªå‡ºäº†FFDPç›¸è¾ƒäºç°æœ‰å›¾åƒé…å‡†æ–¹æ³•çš„æ€§èƒ½å’Œæ•ˆç‡ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25044v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºFFDPï¼Œä¸€å¥—é¢å‘IOçš„éGEMMèåˆå†…æ ¸ï¼Œé…åˆåˆ†å¸ƒå¼æ¡†æ¶ï¼Œç”¨äºå¤§è§„æ¨¡å›¾åƒæ³¨å†Œã€‚æ–‡ç« å¼ºè°ƒå›¾åƒæ³¨å†Œæ˜¯ç”Ÿç‰©åŒ»å­¦å’Œç”Ÿå‘½ç§‘å­¦ä¸­çš„åŸºæœ¬é€†é—®é¢˜ï¼Œä½†ç®—æ³•å°šæœªä¸å›¾åƒé‡‡é›†èƒ½åŠ›åŒæ­¥æ‰©å±•ã€‚FFDPé€šè¿‡ä¼˜åŒ–éGEMMç“¶é¢ˆå¹¶å®ç°å·ç§¯æ„ŸçŸ¥çš„å¼ é‡åˆ†ç‰‡ï¼Œè¡¥å……äº†ç°æœ‰é’ˆå¯¹å¤§è§„æ¨¡å˜æ¢å™¨è®­ç»ƒæå‡ºçš„æ¨¡å‹å¹¶è¡ŒæŠ€æœ¯ã€‚åœ¨ä»…ä½¿ç”¨8ä¸ªA6000 GPUçš„æƒ…å†µä¸‹ï¼Œå¯¹åˆ†è¾¨ç‡ä¸º100å¾®ç±³çš„äººä½“ç¦»ä½“è„‘MRIä½“ç§¯è¿›è¡Œå¤šæ¨¡å¼æ³¨å†Œï¼Œä¸åˆ°ä¸€åˆ†é’Ÿå°±è§£å†³äº†æ¯”æ ‡å‡†ä¸´åºŠæ•°æ®å¤§570å€çš„é€†é—®é¢˜ã€‚FFDPå°†ç°æœ‰æœ€å…ˆè¿›çš„ä¼˜åŒ–å’Œæ·±åº¦å­¦ä¹ æ³¨å†Œç®¡é“åŠ é€Ÿ6-7å€ï¼ŒåŒæ—¶é™ä½å³°å€¼å†…å­˜æ¶ˆè€—20-59%ã€‚å¯¹æ¯”åˆ†ææ˜¾ç¤ºï¼ŒFFDPåœ¨å•GPUä¸Šèƒ½è§£å†³çš„é—®é¢˜æ•°é‡æ˜¯ç°æœ‰æœ€ä½³è§£å†³æ–¹æ¡ˆçš„64å€ï¼Œå¹¶çªå‡ºäº†FFDPç›¸è¾ƒäºå…¶ä»–å›¾åƒæ³¨å†Œæ–¹æ³•çš„æ€§èƒ½å’Œæ•ˆç‡ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FFDPæ˜¯ä¸€å¥—ç”¨äºå¤§è§„æ¨¡å›¾åƒæ³¨å†Œçš„IO-awareéGEMMèåˆå†…æ ¸å’Œåˆ†å¸ƒå¼æ¡†æ¶ã€‚</li>
<li>å›¾åƒæ³¨å†Œæ˜¯ç”Ÿç‰©åŒ»å­¦å’Œç”Ÿå‘½ç§‘å­¦ä¸­çš„åŸºæœ¬é€†é—®é¢˜ï¼Œä½†ç°æœ‰ç®—æ³•å°šæœªä¸å›¾åƒé‡‡é›†èƒ½åŠ›åŒæ­¥æ‰©å±•ã€‚</li>
<li>FFDPé€šè¿‡ä¼˜åŒ–éGEMMç“¶é¢ˆå’Œå·ç§¯æ„ŸçŸ¥çš„å¼ é‡åˆ†ç‰‡æŠ€æœ¯ï¼Œè¡¥å……äº†æ¨¡å‹å¹¶è¡ŒæŠ€æœ¯ã€‚</li>
<li>FFDPèƒ½åœ¨çŸ­æ—¶é—´å†…å¤„ç†å¤§è§„æ¨¡å›¾åƒæ³¨å†Œé—®é¢˜ï¼Œå¦‚å¤„ç†åˆ†è¾¨ç‡ä¸º100å¾®ç±³çš„äººä½“ç¦»ä½“è„‘MRIä½“ç§¯ã€‚</li>
<li>FFDPèƒ½æ˜¾è‘—æé«˜ç°æœ‰æ³¨å†Œæ–¹æ³•çš„æ•ˆç‡å’Œæ€§èƒ½ï¼ŒåŠ é€Ÿç°æœ‰æœ€å…ˆè¿›çš„ä¼˜åŒ–å’Œæ·±åº¦å­¦ä¹ æ³¨å†Œç®¡é“6-7å€ã€‚</li>
<li>FFDPé™ä½äº†å³°å€¼å†…å­˜æ¶ˆè€—ï¼Œå‡å°‘äº†è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eba932ded80f924affcb9dd9a62d980e" align="middle">
<img src="https://picx.zhimg.com/v2-79c4c1376d7a8d23ef9cd35eebee118b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4863d7a610167d0321dc155e6d28e1f1" align="middle">
<img src="https://picx.zhimg.com/v2-e9c9d07ea51ba577c41a7b036a188167" align="middle">
<img src="https://pic1.zhimg.com/v2-4e9050860d5c00eace7c34433a96a74f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41914d89e11ea7de120b4bba4d939a66" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Stellar-flare-detection-in-XMM-Newton-with-gradient-boosted-trees"><a href="#Stellar-flare-detection-in-XMM-Newton-with-gradient-boosted-trees" class="headerlink" title="Stellar flare detection in XMM-Newton with gradient boosted trees"></a>Stellar flare detection in XMM-Newton with gradient boosted trees</h2><p><strong>Authors:Mario Pasquato, Martino Marelli, Andrea De Luca, Ruben Salvaterra, Gaia Carenini, Andrea Belfiore, Andrea Tiengo, Paolo Esposito</strong></p>
<p>The EXTraS project, based on data collected with the XMM-Newton observatory, provided us with a vast amount of light curves for X-ray sources. For each light curve, EXTraS also provided us with a set of features (<a target="_blank" rel="noopener" href="https://extras.inaf.it/">https://extras.inaf.it</a>). We extract from the EXTraS database a tabular dataset of 31,832 variable sources by 108 features. Of these, 13,851 sources were manually labeled as stellar flares or non-flares based on direct visual inspection. We employ a supervised learning approach to produce a catalog of stellar flares based on our dataset, releasing it to the community. We leverage explainable AI tools and interpretable features to better understand our classifier. We train a gradient boosting classifier on 80% of the data for which labels are available. We compute permutation feature importance scores, visualize feature space using UMAP, and analyze some false positive and false negative data points with the help of Shapley additive explanations â€“ an AI explainability technique used to measure the importance of each feature in determining the classifierâ€™s prediction for each instance. On the test set made up of the remainder 20% of our labeled data, we obtain an accuracy of 97.1%, with a precision of 82.4% and a recall of 73.3%. Our classifier outperforms a simple criterion based on fitting the light curve with a flare template and significantly surpasses a gradient-boosted classifier trained only on model-independent features. False positives appear related to flaring light curves that are not associated with a stellar counterpart, while false negatives often correspond to multiple flares or otherwise peculiar or noisy curves. We apply our trained classifier to currently unlabeled sources, releasing the largest catalog of X-ray stellar flares to date. [abridged] </p>
<blockquote>
<p>åŸºäºXMM-ç‰›é¡¿å¤©æ–‡å°æ”¶é›†çš„æ•°æ®ï¼ŒEXTraSé¡¹ç›®ä¸ºæˆ‘ä»¬æä¾›äº†å¤§é‡çš„Xå°„çº¿å…‰æºå…‰å˜æ›²çº¿ã€‚å¯¹äºæ¯æ¡å…‰å˜æ›²çº¿ï¼ŒEXTraSè¿˜ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç³»åˆ—ç‰¹å¾ï¼ˆ<a href="https://extras.inaf.itï¼‰ã€‚æˆ‘ä»¬ä»EXTraSæ•°æ®åº“ä¸­æå–äº†ä¸€ä¸ªåŒ…å«31ï¼Œ832ä¸ªå¯å˜æºå’Œ108ä¸ªç‰¹å¾çš„è¡¨æ ¼æ•°æ®é›†ã€‚å…¶ä¸­ï¼Œæœ‰13ï¼Œ851ä¸ªæºæ˜¯æ ¹æ®ç›´æ¥è§†è§‰æ£€æµ‹æ‰‹åŠ¨æ ‡è®°ä¸ºæ’æ˜Ÿè€€æ–‘æˆ–éè€€æ–‘ã€‚æˆ‘ä»¬é‡‡ç”¨ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ŒåŸºäºæ•°æ®é›†ç”Ÿæˆæ’æ˜Ÿè€€æ–‘ç›®å½•ï¼Œå¹¶å‘ç¤¾åŒºå‘å¸ƒã€‚æˆ‘ä»¬åˆ©ç”¨å¯è§£é‡Šçš„AIå·¥å…·å’Œå¯è§£é‡Šçš„ç‰¹å¾æ¥æ›´å¥½åœ°ç†è§£æˆ‘ä»¬çš„åˆ†ç±»å™¨ã€‚æˆ‘ä»¬å¯¹å¯ç”¨æ ‡ç­¾æ•°æ®çš„80ï¼…è¿›è¡Œæ¢¯åº¦æå‡åˆ†ç±»å™¨çš„è®­ç»ƒã€‚æˆ‘ä»¬è®¡ç®—äº†ç‰¹å¾ç½®æ¢é‡è¦æ€§å¾—åˆ†ï¼Œä½¿ç”¨UMAPå¯è§†åŒ–ç‰¹å¾ç©ºé—´ï¼Œå¹¶ä½¿ç”¨ShapleyåŠ æ³•è§£é‡Šè¿™ä¸€äººå·¥æ™ºèƒ½è§£é‡Šæ€§æŠ€æœ¯åˆ†æäº†æŸäº›è¯¯æŠ¥å’Œæ¼æŠ¥æ•°æ®ç‚¹ï¼Œè¯¥æŠ€æœ¯ç”¨äºè¡¡é‡æ¯ä¸ªç‰¹å¾åœ¨ç¡®å®šåˆ†ç±»å™¨å¯¹æ¯ä¸ªå®ä¾‹çš„é¢„æµ‹ä¸­çš„é‡è¦æ€§ã€‚åœ¨ç”±å‰©ä½™20ï¼…æ ‡è®°æ•°æ®ç»„æˆçš„æµ‹è¯•é›†ä¸Šï¼Œæˆ‘ä»¬è·å¾—äº†97.1ï¼…çš„å‡†ç¡®ç‡ï¼Œç²¾ç¡®åº¦è¾¾åˆ°82.4ï¼…ï¼Œå¬å›ç‡ä¸º73.3ï¼…ã€‚æˆ‘ä»¬çš„åˆ†ç±»å™¨è¡¨ç°ä¼˜äºåŸºäºè€€æ–‘æ¨¡æ¿æ‹Ÿåˆå…‰å˜æ›²çº¿çš„ç®€å•æ ‡å‡†ï¼Œå¹¶ä¸”æ˜æ˜¾ä¼˜äºä»…ä½¿ç”¨æ¨¡å‹ç‹¬ç«‹ç‰¹å¾è®­ç»ƒçš„æ¢¯åº¦æå‡åˆ†ç±»å™¨ã€‚è¯¯æŠ¥ä¼¼ä¹ä¸ä¸ä¸æ’æ˜Ÿå¯¹åº”çš„è€€æ–‘å…‰å˜æ›²çº¿æœ‰å…³ï¼Œè€Œæ¼æŠ¥é€šå¸¸å¯¹åº”äºå¤šæ¬¡è€€æ–‘å‘ä½œæˆ–ç‰¹æ®Šæˆ–å˜ˆæ‚çš„æ›²çº¿ã€‚æˆ‘ä»¬å°†è®­ç»ƒå¥½çš„åˆ†ç±»å™¨åº”ç”¨äºå½“å‰æœªæ ‡è®°çš„æºï¼Œå‘å¸ƒäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„Xå°„çº¿æ’æ˜Ÿè€€æ–‘ç›®å½•ã€‚[æ‘˜è¦]">https://extras.inaf.itï¼‰ã€‚æˆ‘ä»¬ä»EXTraSæ•°æ®åº“ä¸­æå–äº†ä¸€ä¸ªåŒ…å«31ï¼Œ832ä¸ªå¯å˜æºå’Œ108ä¸ªç‰¹å¾çš„è¡¨æ ¼æ•°æ®é›†ã€‚å…¶ä¸­ï¼Œæœ‰13ï¼Œ851ä¸ªæºæ˜¯æ ¹æ®ç›´æ¥è§†è§‰æ£€æµ‹æ‰‹åŠ¨æ ‡è®°ä¸ºæ’æ˜Ÿè€€æ–‘æˆ–éè€€æ–‘ã€‚æˆ‘ä»¬é‡‡ç”¨ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ŒåŸºäºæ•°æ®é›†ç”Ÿæˆæ’æ˜Ÿè€€æ–‘ç›®å½•ï¼Œå¹¶å‘ç¤¾åŒºå‘å¸ƒã€‚æˆ‘ä»¬åˆ©ç”¨å¯è§£é‡Šçš„AIå·¥å…·å’Œå¯è§£é‡Šçš„ç‰¹å¾æ¥æ›´å¥½åœ°ç†è§£æˆ‘ä»¬çš„åˆ†ç±»å™¨ã€‚æˆ‘ä»¬å¯¹å¯ç”¨æ ‡ç­¾æ•°æ®çš„80ï¼…è¿›è¡Œæ¢¯åº¦æå‡åˆ†ç±»å™¨çš„è®­ç»ƒã€‚æˆ‘ä»¬è®¡ç®—äº†ç‰¹å¾ç½®æ¢é‡è¦æ€§å¾—åˆ†ï¼Œä½¿ç”¨UMAPå¯è§†åŒ–ç‰¹å¾ç©ºé—´ï¼Œå¹¶ä½¿ç”¨ShapleyåŠ æ³•è§£é‡Šè¿™ä¸€äººå·¥æ™ºèƒ½è§£é‡Šæ€§æŠ€æœ¯åˆ†æäº†æŸäº›è¯¯æŠ¥å’Œæ¼æŠ¥æ•°æ®ç‚¹ï¼Œè¯¥æŠ€æœ¯ç”¨äºè¡¡é‡æ¯ä¸ªç‰¹å¾åœ¨ç¡®å®šåˆ†ç±»å™¨å¯¹æ¯ä¸ªå®ä¾‹çš„é¢„æµ‹ä¸­çš„é‡è¦æ€§ã€‚åœ¨ç”±å‰©ä½™20ï¼…æ ‡è®°æ•°æ®ç»„æˆçš„æµ‹è¯•é›†ä¸Šï¼Œæˆ‘ä»¬è·å¾—äº†97.1ï¼…çš„å‡†ç¡®ç‡ï¼Œç²¾ç¡®åº¦è¾¾åˆ°82.4ï¼…ï¼Œå¬å›ç‡ä¸º73.3ï¼…ã€‚æˆ‘ä»¬çš„åˆ†ç±»å™¨è¡¨ç°ä¼˜äºåŸºäºè€€æ–‘æ¨¡æ¿æ‹Ÿåˆå…‰å˜æ›²çº¿çš„ç®€å•æ ‡å‡†ï¼Œå¹¶ä¸”æ˜æ˜¾ä¼˜äºä»…ä½¿ç”¨æ¨¡å‹ç‹¬ç«‹ç‰¹å¾è®­ç»ƒçš„æ¢¯åº¦æå‡åˆ†ç±»å™¨ã€‚è¯¯æŠ¥ä¼¼ä¹ä¸ä¸ä¸æ’æ˜Ÿå¯¹åº”çš„è€€æ–‘å…‰å˜æ›²çº¿æœ‰å…³ï¼Œè€Œæ¼æŠ¥é€šå¸¸å¯¹åº”äºå¤šæ¬¡è€€æ–‘å‘ä½œæˆ–ç‰¹æ®Šæˆ–å˜ˆæ‚çš„æ›²çº¿ã€‚æˆ‘ä»¬å°†è®­ç»ƒå¥½çš„åˆ†ç±»å™¨åº”ç”¨äºå½“å‰æœªæ ‡è®°çš„æºï¼Œå‘å¸ƒäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„Xå°„çº¿æ’æ˜Ÿè€€æ–‘ç›®å½•ã€‚[æ‘˜è¦]</a> </p>
</blockquote>
<p>ã€ä¸­æ–‡è§£é‡Šã€‘ï¼ˆæ­¤å¤„ä»…ä½œå‚è€ƒï¼Œè¯·é…Œæƒ…è°ƒæ•´è¡¨è¾¾ç»†èŠ‚ï¼‰</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24954v1">PDF</a> 15 pages, 14 figures, Accepted for publication by A&amp;A</p>
<p><strong>Summary</strong></p>
<p>åŸºäºXMM-Newtonå¤©æ–‡å°çš„æ•°æ®ï¼ŒEXTraSé¡¹ç›®ä¸ºXå°„çº¿æºæä¾›äº†å¤§é‡çš„å…‰å˜æ›²çº¿ã€‚è¯¥é¡¹ç›®æå–äº†åŒ…å«ç‰¹å¾åœ¨å†…çš„æ•°æ®è¡¨ï¼ŒåŒ…æ‹¬æ ‡è®°ä¸ºæ’æ˜Ÿè€€æ–‘æˆ–éè€€æ–‘çš„æ•°æ®ã€‚é€šè¿‡ç›‘ç£å­¦ä¹ æ–¹æ³•ç”Ÿæˆæ’æ˜Ÿè€€æ–‘ç›®å½•ï¼Œå¹¶åˆ©ç”¨å¯è§£é‡Šçš„AIå·¥å…·æ›´å¥½åœ°ç†è§£åˆ†ç±»å™¨ã€‚è®­ç»ƒæ¢¯åº¦æå‡åˆ†ç±»å™¨åï¼Œå…¶å‡†ç¡®ç‡è¾¾åˆ°äº†æƒŠäººçš„æ°´å¹³ã€‚è¯¥é¡¹ç›®å°†å·²è®­ç»ƒçš„åˆ†ç±»å™¨åº”ç”¨äºå½“å‰æœªæ ‡è®°çš„æºï¼Œå‘å¸ƒäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„Xå°„çº¿æ’æ˜Ÿè€€æ–‘ç›®å½•ã€‚ç®€è€Œè¨€ä¹‹ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨æœºå™¨å­¦ä¹ åˆ†ç±»å™¨åŸºäºXå°„çº¿æºå…‰å˜æ›²çº¿è¿›è¡Œæ’æ˜Ÿè€€æ–‘è¯†åˆ«å’Œåˆ†ç±»çš„é¡¹ç›®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EXTraSé¡¹ç›®åŸºäºXMM-Newtonå¤©æ–‡å°æ•°æ®æä¾›äº†å¤§é‡Xå°„çº¿æºçš„å…‰å˜æ›²çº¿ã€‚</li>
<li>è¯¥é¡¹ç›®é€šè¿‡ç‰¹å¾é›†æå–äº†æ•°æ®è¡¨ï¼ŒåŒ…æ‹¬æ‰‹åŠ¨æ ‡è®°ä¸ºæ’æ˜Ÿè€€æ–‘æˆ–éè€€æ–‘çš„æ•°æ®ã€‚</li>
<li>é‡‡ç”¨ç›‘ç£å­¦ä¹ æ–¹æ³•ç”Ÿæˆæ’æ˜Ÿè€€æ–‘ç›®å½•ï¼Œå¹¶åˆ©ç”¨å¯è§£é‡Šçš„AIå·¥å…·è¿›è¡Œç‰¹å¾åˆ†æä»¥æé«˜åˆ†ç±»å™¨æ€§èƒ½ã€‚</li>
<li>é€šè¿‡è®­ç»ƒæ¢¯åº¦æå‡åˆ†ç±»å™¨å¹¶å¯¹å…¶è¿›è¡Œæµ‹è¯•ï¼Œå®ç°äº†é«˜è¾¾97.1%çš„å‡†ç¡®ç‡ã€82.4%çš„ç²¾åº¦å’Œ73.3%çš„å¬å›ç‡ã€‚è¿™ä¸€æ€§èƒ½ä¼˜äºå…¶ä»–ä¸¤ç§æ–¹æ³•ï¼šå•çº¯ä½¿ç”¨å…‰å˜æ›²çº¿åŒ¹é…è€€æ–‘æ¨¡æ¿çš„ç®€å•å‡†åˆ™å’Œä»…ä½¿ç”¨æ¨¡å‹ç‹¬ç«‹ç‰¹å¾çš„æ¢¯åº¦æå‡åˆ†ç±»å™¨ã€‚</li>
<li>åˆ†æäº†è¯¯æŠ¥æƒ…å†µï¼Œå‘ç°å‡é˜³æ€§é€šå¸¸ä¸æœªä¸æ’æ˜Ÿå¯¹åº”çš„è€€æ–‘å…‰å˜æ›²çº¿æœ‰å…³ï¼Œè€Œå‡é˜´æ€§åˆ™ç»å¸¸ä¸å¤šé‡è€€æ–‘æˆ–å…¶ä»–å¼‚å¸¸æˆ–å™ªå£°æ›²çº¿æœ‰å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24954">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6bc07af5c63d22d17acb0bf870f880b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae833fa310f8e06c9652a8837f36ad11" align="middle">
<img src="https://picx.zhimg.com/v2-6d9d754ffca5aa295eb00b61f8294574" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MMRQA-Signal-Enhanced-Multimodal-Large-Language-Models-for-MRI-Quality-Assessment"><a href="#MMRQA-Signal-Enhanced-Multimodal-Large-Language-Models-for-MRI-Quality-Assessment" class="headerlink" title="MMRQA: Signal-Enhanced Multimodal Large Language Models for MRI Quality   Assessment"></a>MMRQA: Signal-Enhanced Multimodal Large Language Models for MRI Quality   Assessment</h2><p><strong>Authors:Fankai Jia, Daisong Gan, Zhe Zhang, Zhaochi Wen, Chenchen Dan, Dong Liang, Haifeng Wang</strong></p>
<p>Magnetic resonance imaging (MRI) quality assessment is crucial for clinical decision-making, yet remains challenging due to data scarcity and protocol variability. Traditional approaches face fundamental trade-offs: signal-based methods like MRIQC provide quantitative metrics but lack semantic understanding, while deep learning approaches achieve high accuracy but sacrifice interpretability. To address these limitations, we introduce the Multimodal MRI Quality Assessment (MMRQA) framework, pioneering the integration of multimodal large language models (MLLMs) with acquisition-aware signal processing. MMRQA combines three key innovations: robust metric extraction via MRQy augmented with simulated artifacts, structured transformation of metrics into question-answer pairs using Qwen, and parameter-efficient fusion through Low-Rank Adaptation (LoRA) of LLaVA-OneVision. Evaluated on MR-ART, FastMRI, and MyConnectome benchmarks, MMRQA achieves state-of-the-art performance with strong zero-shot generalization, as validated by comprehensive ablation studies. By bridging quantitative analysis with semantic reasoning, our framework generates clinically interpretable outputs that enhance quality control in dynamic medical settings. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è´¨é‡è¯„ä¼°å¯¹äºä¸´åºŠå†³ç­–è‡³å…³é‡è¦ï¼Œä½†ç”±äºæ•°æ®ç¨€ç¼ºå’Œåè®®å·®å¼‚ï¼Œä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•é¢ä¸´åŸºæœ¬æƒè¡¡ï¼šMRIQCç­‰åŸºäºä¿¡å·çš„æ–¹æ³•æä¾›å®šé‡æŒ‡æ ‡ï¼Œä½†ç¼ºä¹è¯­ä¹‰ç†è§£ï¼Œè€Œæ·±åº¦å­¦ä¹ æ–¹æ³•è™½ç„¶å‡†ç¡®åº¦é«˜ï¼Œä½†ç‰ºç‰²äº†å¯è§£é‡Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šæ¨¡å¼MRIè´¨é‡è¯„ä¼°ï¼ˆMMRQAï¼‰æ¡†æ¶ï¼Œç‡å…ˆå°†å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸é‡‡é›†æ„ŸçŸ¥ä¿¡å·å¤„ç†ç›¸ç»“åˆã€‚MMRQAç»“åˆäº†ä¸‰å¤§å…³é”®åˆ›æ–°ï¼šé€šè¿‡MRQyè¾…ä»¥æ¨¡æ‹Ÿä¼ªå½±è¿›è¡Œç¨³å¥æŒ‡æ ‡æå–ã€ä½¿ç”¨Qwenå°†æŒ‡æ ‡è½¬æ¢ä¸ºé—®ç­”å¯¹è¿›è¡Œç»“æ„åŒ–è½¬æ¢ï¼Œä»¥åŠé€šè¿‡LoRAå¯¹LLaVA-OneVisionè¿›è¡Œå‚æ•°æœ‰æ•ˆèåˆã€‚åœ¨MR-ARTã€FastMRIå’ŒMyConnectomeåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°ï¼ŒMMRQAå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œè¿™å·²é€šè¿‡å…¨é¢çš„æ¶ˆèç ”ç©¶å¾—åˆ°éªŒè¯ã€‚é€šè¿‡å®šé‡åˆ†æä¸è¯­ä¹‰æ¨ç†ä¹‹é—´çš„æ¡¥æ¢ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ç”Ÿæˆäº†å¯ä¸´åºŠè§£é‡Šçš„è¾“å‡ºç»“æœï¼Œæé«˜äº†åŠ¨æ€åŒ»ç–—ç¯å¢ƒä¸­çš„è´¨é‡æ§åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24888v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è´¨é‡è¯„ä¼°çš„é‡è¦æ€§åŠå…¶åœ¨ä¸´åºŠå†³ç­–ä¸­çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ï¼Œå¦‚ç¼ºä¹è¯­ä¹‰ç†è§£å’Œé«˜å‡†ç¡®æ€§ä½†ç‰ºç‰²äº†è§£é‡Šæ€§ï¼Œæœ¬æ–‡æå‡ºäº†å¤šæ¨¡æ€MRIè´¨é‡è¯„ä¼°ï¼ˆMMRQAï¼‰æ¡†æ¶ï¼Œç‡å…ˆå°†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸é‡‡é›†æ„ŸçŸ¥ä¿¡å·å¤„ç†ç›¸ç»“åˆã€‚MMRQAç»“åˆäº†ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼šé€šè¿‡MRQyå¢å¼ºæ¨¡æ‹Ÿä¼ªå½±è¿›è¡Œç¨³å¥æŒ‡æ ‡æå–ï¼Œä½¿ç”¨Qwenå°†æŒ‡æ ‡è½¬æ¢ä¸ºé—®ç­”å¯¹è¿›è¡Œç»“æ„åŒ–è½¬æ¢ï¼Œä»¥åŠé€šè¿‡LoRAè¿›è¡ŒLLaVA-OneVisionçš„å‚æ•°é«˜æ•ˆèåˆã€‚åœ¨MR-ARTã€FastMRIå’ŒMyConnectomeåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°ï¼ŒMMRQAè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶é€šè¿‡ç»¼åˆæ¶ˆèç ”ç©¶å¾—åˆ°éªŒè¯ã€‚é€šè¿‡å®šé‡åˆ†æä¸è¯­ä¹‰æ¨ç†çš„ç»“åˆï¼Œè¯¥æ¡†æ¶ç”Ÿæˆäº†ä¸´åºŠå¯è§£é‡Šçš„è¾“å‡ºï¼Œæé«˜äº†åŠ¨æ€åŒ»å­¦ç¯å¢ƒä¸­çš„è´¨é‡æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRIè´¨é‡è¯„ä¼°åœ¨ä¸´åºŠå†³ç­–ä¸­è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´æ•°æ®ç¨€ç¼ºå’Œåè®®å¯å˜æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼šåŸºäºä¿¡å·çš„æ–¹æ³•ç¼ºä¹è¯­ä¹‰ç†è§£ï¼Œè€Œæ·±åº¦å­¦ä¹ æ–¹æ³•è™½ç„¶å‡†ç¡®ä½†ç‰ºç‰²äº†è§£é‡Šæ€§ã€‚</li>
<li>å¼•å…¥MMRQAæ¡†æ¶ï¼Œç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸é‡‡é›†æ„ŸçŸ¥ä¿¡å·å¤„ç†ã€‚</li>
<li>MMRQAæ¡†æ¶åŒ…æ‹¬ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼šMRQyæ¨¡æ‹Ÿä¼ªå½±çš„ç¨³å¥æŒ‡æ ‡æå–ã€Qwençš„ç»“æ„åŒ–æŒ‡æ ‡è½¬æ¢å’ŒLLaVA-OneVisionçš„LoRAå‚æ•°é«˜æ•ˆèåˆã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…·å¤‡å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MMRQAæ¡†æ¶ç»“åˆäº†å®šé‡åˆ†æä¸è¯­ä¹‰æ¨ç†ï¼Œç”Ÿæˆä¸´åºŠå¯è§£é‡Šçš„è¾“å‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24888">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6438d67ffcfeee652acb6e8901b6a5b1" align="middle">
<img src="https://pic1.zhimg.com/v2-6f7fb59c5b2dbcbfac6ee007dec9157f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d11eec42a970a251ca6e66cfc01e4a9" align="middle">
<img src="https://picx.zhimg.com/v2-0a5fde75be8af00a9f4a7692f23be960" align="middle">
<img src="https://picx.zhimg.com/v2-d3e0b263df5805c6bd9f952df4338c2e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Causal-Adapter-Taming-Text-to-Image-Diffusion-for-Faithful-Counterfactual-Generation"><a href="#Causal-Adapter-Taming-Text-to-Image-Diffusion-for-Faithful-Counterfactual-Generation" class="headerlink" title="Causal-Adapter: Taming Text-to-Image Diffusion for Faithful   Counterfactual Generation"></a>Causal-Adapter: Taming Text-to-Image Diffusion for Faithful   Counterfactual Generation</h2><p><strong>Authors:Lei Tong, Zhihua Liu, Chaochao Lu, Dino Oglic, Tom Diethe, Philip Teare, Sotirios A. Tsaftaris, Chen Jin</strong></p>
<p>We present Causal-Adapter, a modular framework that adapts frozen text-to-image diffusion backbones for counterfactual image generation. Our method enables causal interventions on target attributes, consistently propagating their effects to causal dependents without altering the core identity of the image. In contrast to prior approaches that rely on prompt engineering without explicit causal structure, Causal-Adapter leverages structural causal modeling augmented with two attribute regularization strategies: prompt-aligned injection, which aligns causal attributes with textual embeddings for precise semantic control, and a conditioned token contrastive loss to disentangle attribute factors and reduce spurious correlations. Causal-Adapter achieves state-of-the-art performance on both synthetic and real-world datasets, with up to 91% MAE reduction on Pendulum for accurate attribute control and 87% FID reduction on ADNI for high-fidelity MRI image generation. These results show that our approach enables robust, generalizable counterfactual editing with faithful attribute modification and strong identity preservation. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†å› æœé€‚é…å™¨ï¼ˆCausal-Adapterï¼‰è¿™ä¸€æ¨¡å—åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿé’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œé€‚é…ï¼Œç”¨äºç”Ÿæˆåäº‹å®å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ç›®æ ‡å±æ€§ä¸Šå®æ–½å› æœå¹²é¢„ï¼Œå¹¶å§‹ç»ˆå¦‚ä¸€åœ°å°†å®ƒä»¬çš„æ•ˆæœä¼ æ’­åˆ°å› æœä¾èµ–å…³ç³»ä¸Šï¼ŒåŒæ—¶ä¸æ”¹å˜å›¾åƒçš„æ ¸å¿ƒèº«ä»½ã€‚ä¸ä¹‹å‰ä¾èµ–äºæç¤ºå·¥ç¨‹ä½†æ²¡æœ‰æ˜ç¡®å› æœç»“æ„çš„æ–¹æ³•ä¸åŒï¼Œå› æœé€‚é…å™¨åˆ©ç”¨ç»“æ„å› æœæ¨¡å‹å¹¶è¾…ä»¥ä¸¤ç§å±æ€§æ­£åˆ™åŒ–ç­–ç•¥ï¼šæç¤ºå¯¹é½æ³¨å…¥èƒ½å¤Ÿé€šè¿‡å¯¹é½å› æœå±æ€§ä¸æ–‡æœ¬åµŒå…¥æ¥è¿›è¡Œç²¾ç¡®è¯­ä¹‰æ§åˆ¶ï¼Œè€Œæ¡ä»¶æ ‡è®°å¯¹æ¯”æŸå¤±æœ‰åŠ©äºæ¶ˆé™¤å±æ€§å› ç´ å¹¶å‡å°‘è¯¯å¯¼å…³è”ã€‚å› æœé€‚é…å™¨åœ¨åˆæˆæ•°æ®é›†å’Œç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šéƒ½å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œåœ¨æ‘†é”¤ï¼ˆPendulumï¼‰ä¸Šçš„å¹³å‡ç»å¯¹è¯¯å·®å‡å°‘äº†é«˜è¾¾91%ï¼Œå¯å®ç°å‡†ç¡®çš„å±æ€§æ§åˆ¶ï¼Œè€Œåœ¨ADNIä¸Šçš„å¼—é›·æ­‡ç‰¹-è´é›·è’‚å°¼æŒ‡æ•°ï¼ˆFIDï¼‰å‡å°‘äº†é«˜è¾¾87%ï¼Œå¯å®ç°é«˜ä¿çœŸMRIå›¾åƒç”Ÿæˆã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°ç¨³å¥ã€é€šç”¨çš„åäº‹å®ç¼–è¾‘ï¼Œå…·æœ‰å¿ å®çš„å±æ€§ä¿®æ”¹å’Œå¼ºå¤§çš„èº«ä»½ä¿ç•™èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24798v1">PDF</a> 9 pages, 26 figures</p>
<p><strong>Summary</strong></p>
<p>Causal-Adapteræ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œç”¨äºé€‚åº”å†·å†»æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£ä¸»å¹²ä»¥è¿›è¡Œåäº‹å®å›¾åƒç”Ÿæˆã€‚æ­¤æ–¹æ³•å¯å®ç°ç›®æ ‡å±æ€§çš„å› æœå¹²é¢„ï¼Œå¹¶å°†è¿™äº›æ•ˆæœä¸€è‡´åœ°ä¼ æ’­åˆ°å› æœä¾èµ–é¡¹ä¸­ï¼Œè€Œä¸æ”¹å˜å›¾åƒçš„æ ¸å¿ƒèº«ä»½ã€‚é€šè¿‡åˆ©ç”¨ç»“æ„å› æœå»ºæ¨¡å¹¶ç»“åˆä¸¤ç§å±æ€§æ­£åˆ™åŒ–ç­–ç•¥ï¼ŒCausal-Adapterèƒ½å¤Ÿè¶…è¶Šä»¥å¾€ä¾èµ–æç¤ºå·¥ç¨‹è€Œæ²¡æœ‰æ˜ç¡®å› æœç»“æ„çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹é½å› æœå±æ€§ä¸æ–‡æœ¬åµŒå…¥æ¥å®ç°ç²¾ç¡®è¯­ä¹‰æ§åˆ¶ï¼Œå¹¶ä½¿ç”¨æ¡ä»¶ä»¤ç‰Œå¯¹æ¯”æŸå¤±æ¥åˆ†ç¦»å±æ€§å› ç´ å¹¶å‡å°‘å¶ç„¶ç›¸å…³æ€§ã€‚åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šï¼ŒCausal-Adapterå‡è¾¾åˆ°äº†å“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Causal-Adapteræ˜¯ä¸€ä¸ªç”¨äºåäº‹å®å›¾åƒç”Ÿæˆçš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œé€‚åº”æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å®ƒèƒ½å¤Ÿå®ç°ç›®æ ‡å±æ€§çš„å› æœå¹²é¢„ï¼Œå¹¶ä¼ æ’­è¿™äº›æ•ˆæœè€Œä¸æ”¹å˜å›¾åƒçš„æ ¸å¿ƒèº«ä»½ã€‚</li>
<li>é€šè¿‡ç»“æ„å› æœå»ºæ¨¡å’Œä¸¤ç§å±æ€§æ­£åˆ™åŒ–ç­–ç•¥ï¼ˆæç¤ºå¯¹é½æ³¨å…¥å’Œæ¡ä»¶ä»¤ç‰Œå¯¹æ¯”æŸå¤±ï¼‰æé«˜æ€§èƒ½ã€‚</li>
<li>æç¤ºå¯¹é½æ³¨å…¥ç­–ç•¥å¯¹é½å› æœå±æ€§ä¸æ–‡æœ¬åµŒå…¥ï¼Œå®ç°ç²¾ç¡®è¯­ä¹‰æ§åˆ¶ã€‚</li>
<li>æ¡ä»¶ä»¤ç‰Œå¯¹æ¯”æŸå¤±ç”¨äºåˆ†ç¦»å±æ€§å› ç´ ï¼Œå‡å°‘å¶ç„¶ç›¸å…³æ€§ã€‚</li>
<li>åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šï¼ŒCausal-Adapterå‡å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åœ¨Pendulumä¸Šçš„MAEå‡å°‘91%ï¼Œåœ¨ADNIä¸Šçš„FIDå‡å°‘87%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ee6de9e12dbab11dc69229e2082765f" align="middle">
<img src="https://picx.zhimg.com/v2-1f91aabcf0a92ff67f7b3aac155b1709" align="middle">
<img src="https://picx.zhimg.com/v2-e5714a8ab2552b1e749234cd71cc645e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57e9111d9d6120f2bb705ee8078ed910.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-832a59492a92d83e43ce9782b3423a50" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Toward-a-Vision-Language-Foundation-Model-for-Medical-Data-Multimodal-Dataset-and-Benchmarks-for-Vietnamese-PET-CT-Report-Generation"><a href="#Toward-a-Vision-Language-Foundation-Model-for-Medical-Data-Multimodal-Dataset-and-Benchmarks-for-Vietnamese-PET-CT-Report-Generation" class="headerlink" title="Toward a Vision-Language Foundation Model for Medical Data: Multimodal   Dataset and Benchmarks for Vietnamese PET&#x2F;CT Report Generation"></a>Toward a Vision-Language Foundation Model for Medical Data: Multimodal   Dataset and Benchmarks for Vietnamese PET&#x2F;CT Report Generation</h2><p><strong>Authors:Huu Tien Nguyen, Dac Thai Nguyen, The Minh Duc Nguyen, Trung Thanh Nguyen, Thao Nguyen Truong, Huy Hieu Pham, Johan Barthelemy, Minh Quan Tran, Thanh Tam Nguyen, Quoc Viet Hung Nguyen, Quynh Anh Chau, Hong Son Mai, Thanh Trung Nguyen, Phi Le Nguyen</strong></p>
<p>Vision-Language Foundation Models (VLMs), trained on large-scale multimodal datasets, have driven significant advances in Artificial Intelligence by enabling rich cross-modal reasoning. Despite their success in general domains, applying these models to medical imaging remains challenging due to the limited availability of diverse imaging modalities and multilingual clinical data. Most existing medical VLMs are trained on a subset of imaging modalities and focus primarily on high-resource languages, thus limiting their generalizability and clinical utility. To address these limitations, we introduce a novel Vietnamese-language multimodal medical dataset comprising 1,567,062 paired CT-PET images and corresponding 2,757 full-length clinical reports. This dataset is designed to fill two pressing gaps in medical AI development: (1) the lack of PET&#x2F;CT imaging data in existing VLMs training corpora, which hinders the development of models capable of handling functional imaging tasks; and (2) the underrepresentation of low-resource languages, particularly the Vietnamese language, in medical vision-language research. To the best of our knowledge, this is the first dataset to provide comprehensive PET&#x2F;CT-report pairs in Vietnamese. We further introduce a training framework to enhance VLMsâ€™ learning, including data augmentation and expert-validated test sets. We conduct comprehensive experiments benchmarking state-of-the-art VLMs on downstream tasks, including medical report generation and visual question answering. The experimental results show that incorporating our dataset significantly improves the performance of existing VLMs. We believe this dataset and benchmark will serve as a pivotal step in advancing the development of more robust VLMs for medical imaging, particularly in low-resource languages, and improving their clinical relevance in Vietnamese healthcare. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†ä¸°å¯Œçš„è·¨æ¨¡æ€æ¨ç†ï¼Œä¸ºäººå·¥æ™ºèƒ½å¸¦æ¥äº†é‡å¤§è¿›å±•ã€‚å°½ç®¡å®ƒä»¬åœ¨ä¸€èˆ¬é¢†åŸŸå–å¾—äº†æˆåŠŸï¼Œä½†å°†è¿™äº›æ¨¡å‹åº”ç”¨äºåŒ»å­¦å½±åƒä»å­˜åœ¨æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹å¤šæ ·åŒ–çš„æˆåƒæ¨¡å¼å’Œå¤šç§è¯­è¨€çš„ä¸´åºŠæ•°æ®ã€‚ç°æœ‰çš„å¤§å¤šæ•°åŒ»å­¦VLMsä»…åœ¨ä¸€éƒ¨åˆ†æˆåƒæ¨¡å¼ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸»è¦å…³æ³¨èµ„æºä¸°å¯Œå‹è¯­è¨€ï¼Œä»è€Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œä¸´åºŠå®ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è¶Šå—è¯­å¤šæ¨¡æ€åŒ»å­¦æ•°æ®é›†ï¼ŒåŒ…å«1,567,062å¯¹CT-PETå›¾åƒå’Œç›¸åº”çš„2,757ç¯‡å…¨æ–‡ä¸´åºŠæŠ¥å‘Šã€‚è¯¥æ•°æ®é›†æ—¨åœ¨å¡«è¡¥åŒ»å­¦äººå·¥æ™ºèƒ½å‘å±•ä¸­çš„ä¸¤ä¸ªç´§è¿«ç©ºç™½ï¼šï¼ˆ1ï¼‰ç°æœ‰VLMsè®­ç»ƒè¯­æ–™åº“ä¸­ç¼ºä¹PET&#x2F;CTæˆåƒæ•°æ®ï¼Œè¿™é˜»ç¢äº†èƒ½å¤Ÿå¤„ç†åŠŸèƒ½æ€§æˆåƒä»»åŠ¡çš„æ¨¡å‹çš„å‘å±•ï¼›ï¼ˆ2ï¼‰ä½èµ„æºè¯­è¨€ï¼Œç‰¹åˆ«æ˜¯è¶Šå—è¯­ï¼Œåœ¨åŒ»å­¦è§†è§‰è¯­è¨€ç ”ç©¶ä¸­çš„ä»£è¡¨æ€§ä¸è¶³ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæä¾›è¶Šå—è¯­ä¸­å…¨é¢çš„PET&#x2F;CTæŠ¥å‘Šå¯¹æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ä¸ªå¢å¼ºVLMså­¦ä¹ çš„è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬æ•°æ®å¢å¼ºå’Œä¸“å®¶éªŒè¯çš„æµ‹è¯•é›†ã€‚æˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œå¯¹æœ€å…ˆè¿›çš„VLMsè¿›è¡Œä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°ï¼ŒåŒ…æ‹¬åŒ»å­¦æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰é—®ç­”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®é›†å¯ä»¥æ˜¾ç€æé«˜ç°æœ‰VLMsçš„æ€§èƒ½ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¯¥æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å°†åœ¨æ¨åŠ¨æ›´ç¨³å¥çš„VLMsç”¨äºåŒ»å­¦å½±åƒçš„å‘å±•æ–¹é¢å‘æŒ¥å…³é”®ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€é¢†åŸŸï¼Œå¹¶æå‡è¶Šå—è¯­åŒ»ç–—ä¿å¥ä¸­çš„ä¸´åºŠç›¸å…³æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24739v1">PDF</a> 39th Conference on Neural Information Processing Systems (NeurIPS   2025)</p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å½±åƒé¢†åŸŸçš„è¶Šå—è¯­å¤šæ¨¡æ€åŒ»ç–—æ•°æ®é›†ï¼Œè§£å†³äº†ç°æœ‰VLMæ¨¡å‹åœ¨åŠŸèƒ½æˆåƒä»»åŠ¡å’Œä½èµ„æºè¯­è¨€æ–¹é¢çš„å±€é™æ€§ã€‚æ•°æ®é›†åŒ…å«1,567,062å¼ é…å¯¹çš„CT-PETå›¾åƒå’Œç›¸åº”çš„2,757ä»½å®Œæ•´ä¸´åºŠæŠ¥å‘Šï¼Œä¸ºè¶Šå—è¯­åŒ»ç–—è§†è§‰è¯­è¨€ç ”ç©¶æä¾›äº†é‡è¦èµ„æºã€‚å¼•å…¥è®­ç»ƒæ¡†æ¶æå‡VLMæ¨¡å‹å­¦ä¹ ï¼ŒåŒ…æ‹¬æ•°æ®å¢å¼ºå’Œä¸“å®¶éªŒè¯æµ‹è¯•é›†ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ•°æ®é›†èƒ½æ˜¾è‘—æå‡ç°æœ‰VLMæ¨¡å‹åœ¨åŒ»ç–—å½±åƒé¢†åŸŸçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥è¶Šå—è¯­å¤šæ¨¡æ€åŒ»ç–—æ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡CT-PETå›¾åƒå’Œä¸´åºŠæŠ¥å‘Šï¼Œè§£å†³åŒ»ç–—AIå‘å±•ä¸­çš„æ•°æ®ç¼ºå£é—®é¢˜ã€‚</li>
<li>æ•°æ®é›†å¡«è¡¥ç°æœ‰VLMæ¨¡å‹ä¸­PET&#x2F;CTæˆåƒæ•°æ®çš„ç©ºç™½ï¼Œæå‡æ¨¡å‹å¤„ç†åŠŸèƒ½æˆåƒä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>æ•°æ®é›†å…³æ³¨ä½èµ„æºè¯­è¨€ï¼Œç‰¹åˆ«æ˜¯è¶Šå—è¯­çš„ä»£è¡¨æ€§ä¸è¶³é—®é¢˜ï¼Œä¸ºåŒ»ç–—è§†è§‰è¯­è¨€ç ”ç©¶æä¾›é‡è¦èµ„æºã€‚</li>
<li>å¼•å…¥è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬æ•°æ®å¢å¼ºå’Œä¸“å®¶éªŒè¯æµ‹è¯•é›†ï¼Œä»¥æå‡VLMæ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œä½¿ç”¨æ­¤æ•°æ®é›†èƒ½æ˜¾è‘—æé«˜VLMæ¨¡å‹åœ¨åŒ»ç–—æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰é—®ç­”ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å¯¹äºæ¨åŠ¨æ›´ç¨³å¥çš„VLMæ¨¡å‹åœ¨åŒ»å­¦å½±åƒé¢†åŸŸçš„å‘å±•è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24739">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b0ef1689641a2ad48b8d628b413b5e35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-849190153c63151c2cfc5db734008dee" align="middle">
<img src="https://picx.zhimg.com/v2-e62d4a1ff79c321bef129cbbd67a6022.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bcbf85b16d877c1e1e06eab9ec568a36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2fdafe7753a542fd87d22ff6342599f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7705f401ab875878d1d047a3be235f42.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Discrete-Variational-Autoencoding-via-Policy-Search"><a href="#Discrete-Variational-Autoencoding-via-Policy-Search" class="headerlink" title="Discrete Variational Autoencoding via Policy Search"></a>Discrete Variational Autoencoding via Policy Search</h2><p><strong>Authors:Michael Drolet, Firas Al-Hafez, Aditya Bhatt, Jan Peters, Oleg Arenz</strong></p>
<p>Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit efficiency and can be modeled with autoregressive discrete distributions, enabling parameter-efficient multimodal search with transformers. However, discrete random variables do not allow for exact differentiable parameterization; therefore, discrete VAEs typically rely on approximations, such as Gumbel-Softmax reparameterization or straight-through gradient estimates, or employ high-variance gradient-free methods such as REINFORCE that have had limited success on high-dimensional tasks such as image reconstruction. Inspired by popular techniques in policy search, we propose a training framework for discrete VAEs that leverages the natural gradient of a non-parametric encoder to update the parametric encoder without requiring reparameterization. Our method, combined with automatic step size adaptation and a transformer-based encoder, scales to challenging datasets such as ImageNet and outperforms both approximate reparameterization methods and quantization-based discrete autoencoders in reconstructing high-dimensional data from compact latent spaces, achieving a 20% improvement on FID Score for ImageNet 256. </p>
<blockquote>
<p>åœ¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ä¸­çš„ç¦»æ•£æ½œåœ¨ç“¶é¢ˆæä¾›äº†é«˜æ¯”ç‰¹æ•ˆç‡ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨è‡ªå›å½’ç¦»æ•£åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œå®ç°äº†å‚æ•°é«˜æ•ˆçš„å¤šå…ƒæœç´¢ã€‚ç„¶è€Œï¼Œç¦»æ•£éšæœºå˜é‡ä¸å…è®¸ç²¾ç¡®çš„å¯å¾®å‚æ•°åŒ–ï¼›å› æ­¤ï¼Œç¦»æ•£VAEé€šå¸¸ä¾èµ–äºè¿‘ä¼¼æ–¹æ³•ï¼Œå¦‚Gumbel-Softmaxé‡å‚æ•°åŒ–æˆ–ç›´é€šæ¢¯åº¦ä¼°è®¡ï¼Œæˆ–è€…é‡‡ç”¨æ— æ¢¯åº¦çš„é«˜æ–¹å·®æ–¹æ³•ï¼Œå¦‚REINFORCEç­‰ã€‚åœ¨é«˜ç»´ä»»åŠ¡ï¼ˆå¦‚å›¾åƒé‡å»ºï¼‰ä¸­è¿™äº›æ–¹æ³•å¹¶ä¸æˆåŠŸã€‚å—æµè¡Œç­–ç•¥æœç´¢æŠ€æœ¯çš„å¯å‘ï¼Œæˆ‘ä»¬ä¸ºç¦»æ•£VAEæå‡ºäº†ä¸€ä¸ªè®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨éå‚æ•°ç¼–ç å™¨çš„è‡ªç„¶æ¢¯åº¦æ¥æ›´æ–°å‚æ•°ç¼–ç å™¨ï¼Œæ— éœ€é‡æ–°å‚æ•°åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†è‡ªåŠ¨æ­¥é•¿è°ƒæ•´å’ŒåŸºäºå˜å‹å™¨çš„ç¼–ç å™¨ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†ï¼ˆå¦‚ImageNetï¼‰ï¼Œå¹¶ä¸”åœ¨ä»ç´§å‡‘æ½œåœ¨ç©ºé—´é‡å»ºé«˜ç»´æ•°æ®æ–¹é¢ä¼˜äºè¿‘ä¼¼é‡å‚æ•°åŒ–æ–¹æ³•å’ŒåŸºäºé‡åŒ–çš„ç¦»æ•£è‡ªç¼–ç å™¨ï¼Œå®ç°äº†ImageNet 256çš„FIDå¾—åˆ†æé«˜äº†20%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24716v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ç¦»æ•£éšå˜é‡ç“¶é¢ˆåœ¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§åŸºäºéå‚æ•°ç¼–ç å™¨è‡ªç„¶æ¢¯åº¦çš„è®­ç»ƒæ¡†æ¶ï¼Œæ— éœ€é‡æ–°å‚æ•°åŒ–å³å¯æ›´æ–°å‚æ•°ç¼–ç å™¨ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è‡ªåŠ¨æ­¥é•¿è°ƒæ•´å’ŒåŸºäºå˜å‹å™¨çš„ç¼–ç å™¨ï¼Œå¯æ‰©å±•åˆ°é«˜ç»´æ•°æ®é›†å¦‚ImageNetï¼Œå¹¶åœ¨é‡å»ºé«˜ç»´æ•°æ®æ–¹é¢ä¼˜äºè¿‘ä¼¼é‡æ–°å‚æ•°åŒ–æ–¹æ³•å’ŒåŸºäºé‡åŒ–çš„ç¦»æ•£è‡ªç¼–ç å™¨ï¼Œæé«˜äº†ImageNet 256çš„FIDå¾—åˆ†ï¼Œè¾¾åˆ°20%çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¦»æ•£éšå˜é‡ç“¶é¢ˆåœ¨å˜åˆ†è‡ªç¼–ç å™¨ä¸­å…·æœ‰é«˜æ¯”ç‰¹æ•ˆç‡ï¼Œå¹¶å¯é€šè¿‡è‡ªå›å½’ç¦»æ•£åˆ†å¸ƒå»ºæ¨¡ã€‚</li>
<li>ç¦»æ•£éšæœºå˜é‡ä¸å…è®¸ç²¾ç¡®çš„å¯å¾®å‚æ•°åŒ–ï¼Œå› æ­¤ç¦»æ•£VAEé€šå¸¸ä¾èµ–äºè¿‘ä¼¼æ–¹æ³•ï¼Œå¦‚Gumbel-Softmaxé‡å‚æ•°åŒ–æˆ–ç›´é€šæ¢¯åº¦ä¼°è®¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºéå‚æ•°ç¼–ç å™¨è‡ªç„¶æ¢¯åº¦çš„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºæ›´æ–°å‚æ•°ç¼–ç å™¨ï¼Œæ— éœ€é‡æ–°å‚æ•°åŒ–ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸è‡ªåŠ¨æ­¥é•¿è°ƒæ•´å’ŒåŸºäºå˜å‹å™¨çš„ç¼–ç å™¨ç›¸ç»“åˆï¼Œå¯æ‰©å±•åˆ°é«˜ç»´æ•°æ®é›†å¦‚ImageNetã€‚</li>
<li>åœ¨é‡å»ºé«˜ç»´æ•°æ®æ–¹é¢ï¼Œè¯¥æ–¹æ³•ä¼˜äºè¿‘ä¼¼é‡æ–°å‚æ•°åŒ–æ–¹æ³•å’ŒåŸºäºé‡åŒ–çš„ç¦»æ•£è‡ªç¼–ç å™¨ã€‚</li>
<li>åœ¨ImageNet 256ä¸Šå®ç°äº†20%çš„FIDå¾—åˆ†æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fb19d8e161158556df81c807c1b5983.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4f29fa845c09ddfd9a799ba70aa9659" align="middle">
<img src="https://picx.zhimg.com/v2-691f577e33f1044da885a08ae785e984" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dark-Signals-in-the-Brain-Augment-Brain-Network-Dynamics-to-the-Complex-valued-Field"><a href="#Dark-Signals-in-the-Brain-Augment-Brain-Network-Dynamics-to-the-Complex-valued-Field" class="headerlink" title="Dark Signals in the Brain: Augment Brain Network Dynamics to the   Complex-valued Field"></a>Dark Signals in the Brain: Augment Brain Network Dynamics to the   Complex-valued Field</h2><p><strong>Authors:Jiangnan Zhang, Chengyuan Qian, Wenlian Lu, Gustavo Deco, Weiyang Ding, Jianfeng Feng</strong></p>
<p>Recordings of brain activity, such as functional MRI (fMRI), provide low-dimensional, indirect observations of neural dynamics evolving in high-dimensional, unobservable spaces. Embedding observed brain dynamics into a higher-dimensional representation may help reveal functional organization, but precisely how remains unclear. Hamiltonian mechanics suggests that, by introducing an additional dimension of conjugate momenta, the dynamical behaviour of a conservative system can be formulated in a more compact and mathematically elegant manner. Here we develop a physics-informed, data-driven framework that lifts whole-brain activity to the complex-valued field. Specifically, we augment observed signals (generalized coordinates) with latent &#96;&#96;dark signalsâ€™â€™ that play the role of conjugate momenta in a whole-brain Hamiltonian system. We show that the Hilbert transform provides an augmentation approach with optimal fitting accuracy within this framework, yielding a Schr&quot;odinger-like equation governing complex-valued, augmented brain dynamics. Empirically, this complex-valued model consistently outperforms its real-valued counterpart, improving short-horizon prediction in the linear regime (correlation 0.12$\to$0.82) and achieving superior fits under nonlinear, nonequilibrium dynamics (0.47$\to$0.88). The framework strengthens structure-function coupling, recovers hierarchical intrinsic timescales, and yields biologically plausible directed effective connectivity that varies systematically with age and reconfigures from rest to task via global rescaling plus targeted rewiring. Together, these results establish a principled, testable paradigm for network neuroscience and offer transformative insight into the spatiotemporal organization and functional roles of large-scale brain dynamics. </p>
<blockquote>
<p>å¤§è„‘æ´»åŠ¨è®°å½•ï¼Œå¦‚åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰ï¼Œæä¾›äº†å¯¹é«˜ç»´ä¸å¯è§‚å¯Ÿç©ºé—´ä¸­ç¥ç»åŠ¨åŠ›å­¦çš„ä½ç»´é—´æ¥è§‚å¯Ÿã€‚å°†è§‚å¯Ÿåˆ°çš„è„‘åŠ¨åŠ›å­¦åµŒå…¥åˆ°é«˜ç»´è¡¨ç¤ºä¸­å¯èƒ½æœ‰åŠ©äºæ­ç¤ºåŠŸèƒ½ç»„ç»‡ï¼Œä½†å…¶ç¡®åˆ‡æ–¹å¼ä»ä¸æ¸…æ¥šã€‚å“ˆå¯†é¡¿åŠ›å­¦è¡¨æ˜ï¼Œé€šè¿‡å¼•å…¥å…±åŠ¨é‡çš„é™„åŠ ç»´åº¦ï¼Œä¿å®ˆç³»ç»Ÿçš„åŠ¨æ€è¡Œä¸ºå¯ä»¥ä»¥æ›´ç´§å‡‘å’Œæ•°å­¦ä¼˜é›…çš„æ–¹å¼è¡¨è¿°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå—ç‰©ç†å­¦å¯å‘ã€æ•°æ®é©±åŠ¨çš„åˆ†ææ¡†æ¶ï¼Œå°†å…¨è„‘æ´»åŠ¨æå‡åˆ°å¤æ•°åœºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç”¨æ½œåœ¨â€œæš—ä¿¡å·â€æ¥å¢å¼ºè§‚æµ‹ä¿¡å·ï¼ˆå¹¿ä¹‰åæ ‡ï¼‰ï¼Œåœ¨å…¨å±€å“ˆå¯†é¡¿ç³»ç»Ÿä¸­æ‰®æ¼”å…±åŠ¨é‡çš„è§’è‰²ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¸Œå°”ä¼¯ç‰¹å˜æ¢åœ¨æ­¤æ¡†æ¶å†…æä¾›äº†æœ€ä½³çš„æ‹Ÿåˆç²¾åº¦å¢å¼ºæ–¹æ³•ï¼Œä»è€Œå¾—å‡ºäº†æ§åˆ¶å¤æ•°å¢å¼ºè„‘åŠ¨åŠ›å­¦çš„è–›å®šè°”æ–¹ç¨‹ã€‚ä»ç»éªŒä¸Šçœ‹ï¼Œè¿™ä¸ªå¤æ•°æ¨¡å‹å§‹ç»ˆä¼˜äºå…¶å¯¹åº”çš„å®æ•°æ¨¡å‹ï¼Œåœ¨çº¿æ€§èŒƒå›´å†…çŸ­æœŸé¢„æµ‹ï¼ˆç›¸å…³æ€§ä»0.12æå‡è‡³0.82ï¼‰æ”¹è¿›æ˜æ˜¾ï¼Œå¹¶åœ¨éçº¿æ€§éå¹³è¡¡åŠ¨åŠ›å­¦ä¸‹è·å¾—æ›´å¥½çš„æ‹Ÿåˆæ•ˆæœï¼ˆä»0.47æå‡è‡³0.88ï¼‰ã€‚è¯¥æ¡†æ¶åŠ å¼ºäº†ç»“æ„-åŠŸèƒ½è€¦åˆï¼Œæ¢å¤äº†åˆ†å±‚å†…åœ¨æ—¶é—´å°ºåº¦ï¼Œå¹¶äº§ç”Ÿäº†ç¬¦åˆç”Ÿç‰©å­¦åŸç†çš„æœ‰æ•ˆè¿æ¥æ€§ï¼Œè¿™ç§è¿æ¥æ€§éšç€å¹´é¾„çš„å¢é•¿è€Œç³»ç»Ÿåœ°å˜åŒ–ï¼Œå¹¶é€šè¿‡å…¨å±€é‡æ–°ç¼©æ”¾å’Œç›®æ ‡é‡æ–°å¸ƒçº¿å®ç°ä»é™æ­¢åˆ°ä»»åŠ¡çš„è½¬å˜ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç»“æœå»ºç«‹äº†ç½‘ç»œç¥ç»ç§‘å­¦çš„æµ‹è¯•åŸåˆ™ï¼Œå¹¶ä¸ºå¤§è§„æ¨¡è„‘åŠ¨åŠ›å­¦çš„æ—¶ç©ºç»„ç»‡å’ŒåŠŸèƒ½è§’è‰²æä¾›äº†å˜é©æ€§çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24715v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ©ç”¨ç‰©ç†å­¦å¯å‘çš„æ–¹æ³•ï¼Œå°†å…¨è„‘æ´»åŠ¨æå‡åˆ°å¤æ•°åœºã€‚é€šè¿‡å¼•å…¥æ½œåœ¨â€œæš—ä¿¡å·â€ä½œä¸ºå¹¿ä¹‰åæ ‡çš„å…±è½­åŠ¨é‡ï¼Œæ„å»ºå…¨è„‘å“ˆå¯†é¡¿ç³»ç»Ÿã€‚é‡‡ç”¨Hilbertå˜æ¢è¿›è¡Œæ•°æ®å¢å¼ºï¼Œå»ºç«‹æè¿°å¤æ‚å…¨è„‘åŠ¨åŠ›å­¦çš„è–›å®šè°”æ–¹ç¨‹ã€‚è¯¥æ¨¡å‹åœ¨é¢„æµ‹çŸ­æœŸçº¿æ€§åŠ¨æ€å’Œéçº¿æ€§éå¹³è¡¡åŠ¨æ€æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œå¹¶æ­ç¤ºäº†ç»“æ„åŠŸèƒ½è€¦åˆã€å†…åœ¨æ—¶é—´å°ºåº¦å±‚çº§ä»¥åŠä»»åŠ¡è¯±å¯¼ä¸‹çš„å…¨çƒå¯é‡å¡‘æ€§ä¸é¶å‘é‡æ–°è¿æ¥çš„æœ‰æ•ˆæ€§ã€‚è¿™æ ‡å¿—ç€å¯¹ç½‘ç»œç¥ç»ç§‘å­¦çš„é‡è¦å‘å±•ï¼Œå¹¶ä¸ºæˆ‘ä»¬æä¾›å¤§è§„æ¨¡è„‘åŠ¨åŠ›å­¦çš„æ—¶é—´å’Œç©ºé—´ç»„ç»‡å’ŒåŠŸèƒ½çš„æ´å¯Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨ç‰©ç†å­¦å¯å‘çš„æ–¹æ³•å°†å…¨è„‘æ´»åŠ¨æå‡åˆ°å¤æ•°åœºï¼Œä»¥æ­ç¤ºå…¶åŠŸèƒ½æ€§ç»„ç»‡ã€‚</li>
<li>é€šè¿‡å¼•å…¥å¹¿ä¹‰åæ ‡çš„å…±è½­åŠ¨é‡ï¼ˆå³æ½œåœ¨â€œæš—ä¿¡å·â€ï¼‰æ„å»ºå…¨è„‘å“ˆå¯†é¡¿ç³»ç»Ÿã€‚</li>
<li>é‡‡ç”¨Hilbertå˜æ¢è¿›è¡Œæ•°æ®å¢å¼ºï¼Œå»ºç«‹æè¿°å¤æ‚å…¨è„‘åŠ¨åŠ›å­¦çš„è–›å®šè°”æ–¹ç¨‹ã€‚</li>
<li>æ¨¡å‹çš„ä¼˜åŒ–æ˜¾è‘—æå‡äº†çŸ­æœŸé¢„æµ‹çš„å‡†ç¡®ç‡ã€‚</li>
<li>æ¨¡å‹æ­ç¤ºäº†ç»“æ„åŠŸèƒ½è€¦åˆå’Œå†…åœ¨æ—¶é—´å°ºåº¦å±‚çº§å…³ç³»ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿæ­ç¤ºå¤§è„‘æ´»åŠ¨çš„å…¨å±€å¯é‡å¡‘æ€§å’Œé¶å‘é‡æ–°è¿æ¥çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24715">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c6e4121e88606ddfba782aaa7cd93d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ff07a333dea4b1c859e657baa33c18a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37250e1492b96f3f9b0a9ccc7c2ba992" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Brain-Harmony-A-Multimodal-Foundation-Model-Unifying-Morphology-and-Function-into-1D-Tokens"><a href="#Brain-Harmony-A-Multimodal-Foundation-Model-Unifying-Morphology-and-Function-into-1D-Tokens" class="headerlink" title="Brain Harmony: A Multimodal Foundation Model Unifying Morphology and   Function into 1D Tokens"></a>Brain Harmony: A Multimodal Foundation Model Unifying Morphology and   Function into 1D Tokens</h2><p><strong>Authors:Zijian Dong, Ruilin Li, Joanna Su Xian Chong, Niousha Dehestani, Yinghui Teng, Yi Lin, Zhizhou Li, Yichi Zhang, Yapei Xie, Leon Qi Rong Ooi, B. T. Thomas Yeo, Juan Helen Zhou</strong></p>
<p>We present Brain Harmony (BrainHarmonix), the first multimodal brain foundation model that unifies structural morphology and functional dynamics into compact 1D token representations. The model was pretrained on two of the largest neuroimaging datasets to date, encompassing 64,594 T1-weighted structural MRI 3D volumes (~ 14 million images) and 70,933 functional MRI (fMRI) time series. BrainHarmonix is grounded in two foundational neuroscience principles: structure complements function - structural and functional modalities offer distinct yet synergistic insights into brain organization; function follows structure - brain functional dynamics are shaped by cortical morphology. The modular pretraining process involves single-modality training with geometric pre-alignment followed by modality fusion through shared brain hub tokens. Notably, our dynamics encoder uniquely handles fMRI time series with heterogeneous repetition times (TRs), addressing a major limitation in existing models. BrainHarmonix is also the first to deeply compress high-dimensional neuroimaging signals into unified, continuous 1D tokens, forming a compact latent space of the human brain. BrainHarmonix achieves strong generalization across diverse downstream tasks, including neurodevelopmental and neurodegenerative disorder classification and cognition prediction - consistently outperforming previous approaches. Our models - pretrained on 8 H100 GPUs - aim to catalyze a new era of AI-driven neuroscience powered by large-scale multimodal neuroimaging. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Brain Harmonyï¼ˆBrainHarmonixï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªå°†ç»“æ„å½¢æ€å’ŒåŠŸèƒ½æ€§åŠ¨æ€ç»Ÿä¸€åˆ°ç´§å‡‘çš„1Dä»¤ç‰Œè¡¨ç¤ºä¸­çš„å¤šæ¨¡å¼è„‘åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ä¸¤ä¸ªç¥ç»æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œæ¶µç›–äº†64594ä¸ªT1åŠ æƒç»“æ„MRI 3Dä½“ç§¯ï¼ˆçº¦1400ä¸‡å¼ å›¾åƒï¼‰å’Œ70933ä¸ªåŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰æ—¶é—´åºåˆ—ã€‚BrainHarmonixåŸºäºä¸¤é¡¹åŸºç¡€ç¥ç»ç§‘å­¦åŸåˆ™ï¼šç»“æ„è¡¥å……åŠŸèƒ½â€”â€”ç»“æ„å’ŒåŠŸèƒ½æ¨¡å¼ä¸ºè„‘ç»„ç»‡çš„ç»„ç»‡æä¾›äº†ç‹¬ç‰¹è€ŒååŒçš„è§è§£ï¼›åŠŸèƒ½æ˜¯ç»“æ„çš„å»¶ä¼¸â€”â€”å¤§è„‘çš„åŠŸèƒ½åŠ¨æ€æ˜¯ç”±çš®å±‚å½¢æ€å¡‘é€ çš„ã€‚æ¨¡å—åŒ–é¢„è®­ç»ƒè¿‡ç¨‹æ¶‰åŠå•æ¨¡æ€è®­ç»ƒä¸å‡ ä½•é¢„å¯¹é½ï¼Œç„¶åé€šè¿‡å…±äº«çš„å¤§è„‘ä¸­æ¢ä»¤ç‰Œè¿›è¡Œæ¨¡æ€èåˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åŠ¨æ€ç¼–ç å™¨èƒ½å¤Ÿç‹¬ç‰¹åœ°å¤„ç†å…·æœ‰ä¸åŒé‡å¤æ—¶é—´ï¼ˆTRï¼‰çš„fMRIæ—¶é—´åºåˆ—ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹çš„ä¸€ä¸ªä¸»è¦å±€é™æ€§ã€‚BrainHarmonixä¹Ÿæ˜¯é¦–ä¸ªå°†é«˜ç»´ç¥ç»æˆåƒä¿¡å·æ·±åº¦å‹ç¼©æˆç»Ÿä¸€ã€è¿ç»­çš„1Dä»¤ç‰Œï¼Œå½¢æˆä¸€ä¸ªç´§å‡‘çš„äººç±»è„‘æ½œç©ºé—´ã€‚BrainHarmonixåœ¨ä¸åŒä¸‹æ¸¸ä»»åŠ¡ä¸­å®ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç¥ç»å‘è‚²å’Œç¥ç»é€€è¡Œæ€§ç–¾ç—…çš„åˆ†ç±»ä»¥åŠè®¤çŸ¥é¢„æµ‹â€”â€”å§‹ç»ˆä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨8ä¸ªH100 GPUä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ—¨åœ¨å‚¬åŒ–ç”±å¤§è§„æ¨¡å¤šæ¨¡å¼ç¥ç»æˆåƒé©±åŠ¨çš„æ–°æ—¶ä»£äººå·¥æ™ºèƒ½ç¥ç»ç§‘å­¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24693v1">PDF</a> NeurIPS 2025. The first two authors contributed equally</p>
<p><strong>Summary</strong></p>
<p>BrainHarmonyï¼ˆBrainHarmonixï¼‰æ˜¯é¦–ä¸ªç»Ÿä¸€ç»“æ„å½¢æ€ä¸åŠŸèƒ½åŠ¨æ€çš„å¤šæ¨¡æ€è„‘åŸºç¡€æ¨¡å‹ï¼Œå®ƒå°†ä¸¤è€…è½¬åŒ–ä¸ºç´§å‡‘çš„1Dä»¤ç‰Œè¡¨ç¤ºã€‚è¯¥æ¨¡å‹åœ¨ä¸¤å¤§ç¥ç»æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›–64,594ä¸ªT1åŠ æƒç»“æ„MRI 3Dä½“ç§¯å’Œ70,933ä¸ªåŠŸèƒ½MRIæ—¶é—´åºåˆ—ã€‚å®ƒåŸºäºç¥ç»ç§‘å­¦çš„ä¸¤ä¸ªåŸºæœ¬åŸåˆ™ï¼šç»“æ„è¡¥å……åŠŸèƒ½ï¼ŒåŠŸèƒ½å’Œç»“æ„ç›¸äº’å…³è”ã€‚BrainHarmonixé€šè¿‡æ¨¡å—åŒ–é¢„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬å•æ¨¡æ€è®­ç»ƒä¸å‡ ä½•é¢„å¯¹é½ï¼Œä»¥åŠé€šè¿‡å…±äº«è„‘ä¸­å¿ƒä»¤ç‰Œè¿›è¡Œæ¨¡æ€èåˆã€‚å®ƒèƒ½ç‹¬ç‰¹åœ°å¤„ç†å…·æœ‰ä¸åŒé‡å¤æ—¶é—´çš„fMRIæ—¶é—´åºåˆ—ï¼Œå¹¶å°†é«˜ç»´ç¥ç»æˆåƒä¿¡å·æ·±æ·±å‹ç¼©æˆç»Ÿä¸€ã€è¿ç»­çš„1Dä»¤ç‰Œï¼Œå½¢æˆäººç±»å¤§è„‘ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ã€‚å®ƒåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šå®ç°äº†å¼ºå¤§çš„æ³›åŒ–æ€§èƒ½ï¼ŒåŒ…æ‹¬ç¥ç»å‘è‚²å’Œç¥ç»é€€è¡Œæ€§ç–¾ç—…çš„åˆ†ç±»ä»¥åŠè®¤çŸ¥é¢„æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BrainHarmonyï¼ˆBrainHarmonixï¼‰æ˜¯é¦–ä¸ªå°†ç»“æ„å½¢æ€ä¸åŠŸèƒ½åŠ¨æ€ç›¸ç»“åˆçš„å¤šæ¨¡æ€è„‘åŸºç¡€æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨ä¸¤å¤§ç¥ç»æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›–å¤§é‡MRIæ•°æ®ã€‚</li>
<li>BrainHarmonixåŸºäºç¥ç»ç§‘å­¦çš„ä¸¤ä¸ªåŸåˆ™ï¼šç»“æ„ä¸åŠŸèƒ½çš„äº’è¡¥æ€§ï¼Œä»¥åŠåŠŸèƒ½ç”±ç»“æ„å¡‘é€ çš„è§‚ç‚¹ã€‚</li>
<li>BrainHarmonixé‡‡ç”¨æ¨¡å—åŒ–é¢„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬å•æ¨¡æ€è®­ç»ƒã€å‡ ä½•é¢„å¯¹é½å’Œæ¨¡æ€èåˆã€‚</li>
<li>åŠ¨åŠ›å­¦ç¼–ç å™¨èƒ½å¤Ÿå¤„ç†å…·æœ‰ä¸åŒé‡å¤æ—¶é—´çš„fMRIæ—¶é—´åºåˆ—ï¼Œè¿™æ˜¯ä¸€ä¸ªç°æœ‰æ¨¡å‹çš„é‡å¤§æ”¹è¿›ã€‚</li>
<li>BrainHarmonixèƒ½å°†é«˜ç»´ç¥ç»æˆåƒä¿¡å·å‹ç¼©æˆç´§å‡‘çš„1Dä»¤ç‰Œè¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8de2f5a77838530e666b5e098a1af57b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1923c48d6a1a6ffd3f85344e038d4ecf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb33e2b8afacb09c6d007f02bc879bd3" align="middle">
<img src="https://picx.zhimg.com/v2-580300325164f0005e031ddb2dbc315b" align="middle">
<img src="https://picx.zhimg.com/v2-a11c51354dc98865f098341fcbcb09ac.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="RIFLE-Removal-of-Image-Flicker-Banding-via-Latent-Diffusion-Enhancement"><a href="#RIFLE-Removal-of-Image-Flicker-Banding-via-Latent-Diffusion-Enhancement" class="headerlink" title="RIFLE: Removal of Image Flicker-Banding via Latent Diffusion Enhancement"></a>RIFLE: Removal of Image Flicker-Banding via Latent Diffusion Enhancement</h2><p><strong>Authors: Zhu,  Libo,  Zhou,  Zihan,  Liu,  Xiaoyang,  Zhang,  Weihang,  Shi,  Keyu,  Fu,  Yifan,  Zhang,  Yulun</strong></p>
<p>Capturing screens is now routine in our everyday lives. But the photographs of emissive displays are often influenced by the flicker-banding (FB), which is alternating bright%u2013dark stripes that arise from temporal aliasing between a cameraâ€™s rolling-shutter readout and the displayâ€™s brightness modulation. Unlike moire degradation, which has been extensively studied, the FB remains underexplored despite its frequent and severe impact on readability and perceived quality. We formulate FB removal as a dedicated restoration task and introduce Removal of Image Flicker-Banding via Latent Diffusion Enhancement, RIFLE, a diffusion-based framework designed to remove FB while preserving fine details. We propose the flicker-banding prior estimator (FPE) that predicts key banding attributes and injects it into the restoration network. Additionally, Masked Loss (ML) is proposed to concentrate supervision on banded regions without sacrificing global fidelity. To overcome data scarcity, we provide a simulation pipeline that synthesizes FB in the luminance domain with stochastic jitter in banding angle, banding spacing, and banding width. Feathered boundaries and sensor noise are also applied for a more realistic simulation. For evaluation, we collect a paired real-world FB dataset with pixel-aligned banding-free references captured via long exposure. Across quantitative metrics and visual comparisons on our real-world dataset, RIFLE consistently outperforms recent image reconstruction baselines from mild to severe flicker-banding. To the best of our knowledge, it is the first work to research the simulation and removal of FB. Our work establishes a great foundation for subsequent research in both the dataset construction and the removal model design. Our dataset and code will be released soon. </p>
<blockquote>
<p>å±å¹•æˆªå›¾ç°åœ¨å·²ç»æˆä¸ºæˆ‘ä»¬æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¸¸è§„æ“ä½œã€‚ç„¶è€Œï¼Œå‘å…‰æ˜¾ç¤ºå±çš„ç…§ç‰‡å¾€å¾€ä¼šå—åˆ°é¢‘é—ªæ¡çº¹ï¼ˆFBï¼‰çš„å½±å“ï¼Œé¢‘é—ªæ¡çº¹æ˜¯ç”±äºç›¸æœºæ»šåŠ¨å¿«é—¨è¯»å‡ºä¸æ˜¾ç¤ºå±äº®åº¦è°ƒåˆ¶ä¹‹é—´çš„æ—¶é—´æ··å è€Œäº§ç”Ÿçš„æ˜æš—äº¤æ›¿æ¡çº¹ã€‚ä¸å·²è¢«å¹¿æ³›ç ”ç©¶çš„æ‘©å°”çº¹é€€åŒ–ä¸åŒï¼Œå°½ç®¡é¢‘é—ªæ¡çº¹å¯¹å¯è¯»æ€§å’Œæ„ŸçŸ¥è´¨é‡é€ æˆé¢‘ç¹ä¸”ä¸¥é‡çš„å½±å“ï¼Œä½†å…¶ä»ç„¶è¢«è¾ƒå°‘æ¢ç´¢ã€‚æˆ‘ä»¬å°†é¢‘é—ªæ¡çº¹çš„å»é™¤åˆ¶å®šä¸ºä¸€é¡¹ä¸“é—¨çš„æ¢å¤ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†é€šè¿‡æ½œåœ¨æ‰©æ•£å¢å¼ºå»é™¤å›¾åƒé¢‘é—ªæ¡çº¹ï¼ˆRIFLEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œæ—¨åœ¨å»é™¤é¢‘é—ªæ¡çº¹åŒæ—¶ä¿ç•™ç»†èŠ‚ã€‚æˆ‘ä»¬æå‡ºäº†é¢‘é—ªæ¡çº¹å…ˆéªŒä¼°è®¡å™¨ï¼ˆFPEï¼‰ï¼Œç”¨äºé¢„æµ‹å…³é”®æ¡çº¹å±æ€§å¹¶å°†å…¶æ³¨å…¥æ¢å¤ç½‘ç»œã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†æ©è†œæŸå¤±ï¼ˆMLï¼‰ï¼Œä»¥å°†ç›‘ç£é›†ä¸­åœ¨å¸¦çŠ¶åŒºåŸŸä¸Šï¼Œè€Œä¸ç‰ºç‰²å…¨å±€ä¿çœŸåº¦ã€‚ä¸ºäº†å…‹æœæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåˆæˆé¢‘é—ªæ¡çº¹çš„æ¨¡æ‹Ÿæµç¨‹ï¼Œè¯¥æµç¨‹åœ¨äº®åº¦åŸŸä¸­åˆæˆé¢‘é—ªæ¡çº¹ï¼Œå¹¶å¸¦æœ‰æ¡çº¹è§’åº¦ã€æ¡çº¹é—´éš”å’Œæ¡çº¹å®½åº¦çš„éšæœºæŠ–åŠ¨ã€‚æˆ‘ä»¬è¿˜åº”ç”¨äº†æŸ”å’Œçš„è¾¹ç•Œå’Œä¼ æ„Ÿå™¨å™ªå£°ï¼Œä»¥è¿›è¡Œæ›´é€¼çœŸçš„æ¨¡æ‹Ÿã€‚ä¸ºäº†è¯„ä¼°ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€å¯¹å¸¦æœ‰åƒç´ å¯¹é½çš„æ— é¢‘é—ªå‚è€ƒçš„çœŸå®ä¸–ç•Œé¢‘é—ªæ•°æ®é›†ï¼Œé€šè¿‡é•¿æ—¶é—´æ›å…‰æ•è·ã€‚åœ¨æˆ‘ä»¬çœŸå®ä¸–ç•Œçš„æ•°æ®é›†ä¸Šï¼Œæ ¹æ®å®šé‡æŒ‡æ ‡å’Œè§†è§‰æ¯”è¾ƒï¼ŒRIFLEåœ¨è½»å¾®åˆ°ä¸¥é‡çš„é¢‘é—ªæ¡çº¹æƒ…å†µä¸‹å‡è¡¨ç°ä¼˜äºæœ€è¿‘çš„å›¾åƒé‡å»ºåŸºçº¿ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒæ˜¯ç¬¬ä¸€é¡¹ç ”ç©¶é¢‘é—ªæ¡çº¹æ¨¡æ‹Ÿå’Œå»é™¤çš„å·¥ä½œã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºåç»­ç ”ç©¶åœ¨æ•°æ®é›†æ„å»ºå’Œå»é™¤æ¨¡å‹è®¾è®¡æ–¹é¢å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24644v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ç ”ç©¶äº†å±å¹•æˆªå›¾ä¸­çš„é—ªçƒæ¡çº¹ï¼ˆFBï¼‰é—®é¢˜ï¼Œæå‡ºä¸€ç§åŸºäºæ‰©æ•£çš„æ¡†æ¶RIFLEï¼Œç”¨äºå»é™¤FBåŒæ—¶ä¿ç•™ç»†èŠ‚ã€‚æ–‡ç« ä»‹ç»äº†é—ªçƒæ¡çº¹å…ˆéªŒä¼°è®¡å™¨ï¼ˆFPEï¼‰å’Œé®ç½©æŸå¤±ï¼ˆMLï¼‰ï¼Œå¹¶æå‡ºåˆæˆé—ªçƒæ¡çº¹æ•°æ®é›†çš„æ–¹æ³•ã€‚RIFLEåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å›¾åƒé‡å»ºæ–¹æ³•ã€‚æœ¬æ–‡ä¸ºFBçš„æ¨¡æ‹Ÿå’Œå»é™¤ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é—ªçƒæ¡çº¹ï¼ˆFBï¼‰æ˜¯å±å¹•æˆªå›¾å¸¸è§çš„é—®é¢˜ï¼Œå½±å“å¯è¯»æ€§å’Œæ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>RIFLEæ¡†æ¶æ—¨åœ¨å»é™¤FBï¼ŒåŒæ—¶ä¿ç•™ç»†èŠ‚ã€‚</li>
<li>å¼•å…¥é—ªçƒæ¡çº¹å…ˆéªŒä¼°è®¡å™¨ï¼ˆFPEï¼‰é¢„æµ‹å…³é”®æ¡çº¹å±æ€§å¹¶æ³¨å…¥ä¿®å¤ç½‘ç»œã€‚</li>
<li>æå‡ºMasked Lossï¼ˆMLï¼‰é›†ä¸­ç›‘ç£å¸¦çŠ¶åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒå…¨å±€ä¿çœŸåº¦ã€‚</li>
<li>ä¸ºå…‹æœæ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆæˆFBçš„æ¨¡æ‹Ÿç®¡é“ï¼ŒåŒ…æ‹¬äº®åº¦åŸŸåˆæˆå’ŒéšæœºæŠ–åŠ¨ã€‚</li>
<li>æ”¶é›†çœŸå®ä¸–ç•ŒFBæ•°æ®é›†ï¼Œå¹¶è¿›è¡Œå®šé‡è¯„ä¼°å’Œè§†è§‰å¯¹æ¯”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-27bddd0e2f88b84b20fb3e5b053a1407.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4feadd52ad0541ebf2adb38d87a2599.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7397039c6acba70dbac88ac52b82cea" align="middle">
<img src="https://pica.zhimg.com/v2-3e508562a7e2e0f7a5d81d636f244395.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55df5ffaa44c89b39a3d8e29070ab48c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="BFSM-3D-Bidirectional-Face-Skull-Morphable-Model"><a href="#BFSM-3D-Bidirectional-Face-Skull-Morphable-Model" class="headerlink" title="BFSM: 3D Bidirectional Face-Skull Morphable Model"></a>BFSM: 3D Bidirectional Face-Skull Morphable Model</h2><p><strong>Authors:Zidu Wang, Meng Xu, Miao Xu, Hengyuan Ma, Jiankuo Zhao, Xutao Li, Xiangyu Zhu, Zhen Lei</strong></p>
<p>Building a joint face-skull morphable model holds great potential for applications such as remote diagnostics, surgical planning, medical education, and physically based facial simulation. However, realizing this vision is constrained by the scarcity of paired face-skull data, insufficient registration accuracy, and limited exploration of reconstruction and clinical applications. Moreover, individuals with craniofacial deformities are often overlooked, resulting in underrepresentation and limited inclusivity. To address these challenges, we first construct a dataset comprising over 200 samples, including both normal cases and rare craniofacial conditions. Each case contains a CT-based skull, a CT-based face, and a high-fidelity textured face scan. Secondly, we propose a novel dense ray matching registration method that ensures topological consistency across face, skull, and their tissue correspondences. Based on this, we introduce the 3D Bidirectional Face-Skull Morphable Model (BFSM), which enables shape inference between the face and skull through a shared coefficient space, while also modeling tissue thickness variation to support one-to-many facial reconstructions from the same skull, reflecting individual changes such as fat over time. Finally, we demonstrate the potential of BFSM in medical applications, including 3D face-skull reconstruction from a single image and surgical planning prediction. Extensive experiments confirm the robustness and accuracy of our method. BFSM is available at <a target="_blank" rel="noopener" href="https://github.com/wang-zidu/BFSM">https://github.com/wang-zidu/BFSM</a> </p>
<blockquote>
<p>æ„å»ºè”åˆé¢éƒ¨-é¢…éª¨å¯å˜å½¢æ¨¡å‹åœ¨è¿œç¨‹è¯Šæ–­ã€æ‰‹æœ¯è§„åˆ’ã€åŒ»å­¦æ•™è‚²å’ŒåŸºäºç‰©ç†çš„é¢éƒ¨æ¨¡æ‹Ÿç­‰æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ç°è¿™ä¸€æ„¿æ™¯å—åˆ°é…å¯¹é¢éƒ¨-é¢…éª¨æ•°æ®ç¨€ç¼ºã€æ³¨å†Œç²¾åº¦ä¸è¶³ä»¥åŠé‡å»ºå’Œä¸´åºŠåº”ç”¨æ¢ç´¢æœ‰é™çš„åˆ¶çº¦ã€‚æ­¤å¤–ï¼Œé¢…é¢ç•¸å½¢æ‚£è€…å¾€å¾€è¢«å¿½è§†ï¼Œå¯¼è‡´ä»£è¡¨æ€§ä¸è¶³å’ŒåŒ…å®¹æ€§æœ‰é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåŒ…å«200å¤šä¸ªæ ·æœ¬çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬æ­£å¸¸ç—…ä¾‹å’Œç½•è§çš„é¢…é¢å¼‚å¸¸ç—…ä¾‹ã€‚æ¯ä¸ªç—…ä¾‹åŒ…å«åŸºäºCTçš„é¢…éª¨ã€åŸºäºCTçš„é¢éƒ¨å›¾åƒå’Œé«˜æ¸…çº¹ç†é¢éƒ¨æ‰«æã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¯†é›†å°„çº¿åŒ¹é…æ³¨å†Œæ–¹æ³•ï¼Œç¡®ä¿é¢éƒ¨ã€é¢…éª¨åŠå…¶ç»„ç»‡å¯¹åº”çš„æ‹“æ‰‘ä¸€è‡´æ€§ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†3DåŒå‘é¢éƒ¨-é¢…éª¨å¯å˜å½¢æ¨¡å‹ï¼ˆBFSMï¼‰ï¼Œè¯¥æ¨¡å‹é€šè¿‡å…±äº«ç³»æ•°ç©ºé—´å®ç°é¢éƒ¨å’Œé¢…éª¨ä¹‹é—´çš„å½¢çŠ¶æ¨æ–­ï¼ŒåŒæ—¶å»ºæ¨¡ç»„ç»‡åšåº¦å˜åŒ–ï¼Œæ”¯æŒåŒä¸€é¢…éª¨çš„ä¸€å¯¹å¤šé¢éƒ¨é‡å»ºï¼Œåæ˜ ä¸ªäººéšæ—¶é—´å˜åŒ–çš„ç‰¹å¾ï¼Œå¦‚è„‚è‚ªå˜åŒ–ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡3Dé¢éƒ¨-é¢…éª¨å•å›¾åƒé‡å»ºå’Œæ‰‹æœ¯è§„åˆ’é¢„æµ‹ç­‰åŒ»ç–—åº”ç”¨å±•ç¤ºäº†BFSMçš„æ½œåŠ›ã€‚å¤§é‡å®éªŒè¯å®äº†æˆ‘ä»¬æ–¹æ³•çš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚BFSMå¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/wang-zidu/BFSM">https://github.com/wang-zidu/BFSM</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24577v1">PDF</a> Under review</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ„å»ºè”åˆé¢éƒ¨-é¢…éª¨å¯å˜å½¢æ¨¡å‹åœ¨è¿œç¨‹è¯Šæ–­ã€æ‰‹æœ¯è§„åˆ’ã€åŒ»å­¦æ•™è‚²å’Œç‰©ç†é¢éƒ¨æ¨¡æ‹Ÿç­‰æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ç°è¿™ä¸€æ„¿æ™¯å—åˆ°é…å¯¹é¢éƒ¨-é¢…éª¨æ•°æ®ç¼ºä¹ã€æ³¨å†Œå‡†ç¡®åº¦ä¸è¶³å’Œé‡å»ºåŠä¸´åºŠåº”ç”¨æ¢ç´¢æœ‰é™çš„åˆ¶çº¦ã€‚æ­¤å¤–ï¼Œé¢…é¢ç•¸å½¢ä¸ªä½“å¸¸è¢«å¿½è§†ï¼Œå¯¼è‡´ä»£è¡¨æ€§ä¸è¶³å’ŒåŒ…å®¹æ€§æœ‰é™ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«200å¤šä¸ªæ ·æœ¬çš„æ•°æ®é›†ï¼Œæ¶µç›–æ­£å¸¸ç—…ä¾‹å’Œç½•è§çš„é¢…é¢ç•¸å½¢ã€‚æ¯ä¸ªç—…ä¾‹åŒ…æ‹¬åŸºäºCTçš„é¢…éª¨å’Œé¢éƒ¨å›¾åƒï¼Œä»¥åŠé«˜åˆ†è¾¨ç‡çº¹ç†é¢éƒ¨æ‰«æã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¯†é›†å°„çº¿åŒ¹é…æ³¨å†Œæ–¹æ³•ï¼Œç¡®ä¿é¢éƒ¨ã€é¢…éª¨åŠå…¶ç»„ç»‡å¯¹åº”å…³ç³»çš„æ‹“æ‰‘ä¸€è‡´æ€§ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†3DåŒå‘é¢éƒ¨-é¢…éª¨å¯å˜å½¢æ¨¡å‹ï¼ˆBFSMï¼‰ï¼Œé€šè¿‡å…±äº«ç³»æ•°ç©ºé—´å®ç°é¢éƒ¨å’Œé¢…éª¨ä¹‹é—´çš„å½¢çŠ¶æ¨æ–­ï¼ŒåŒæ—¶å»ºæ¨¡ç»„ç»‡åšåº¦å˜åŒ–ï¼Œæ”¯æŒä»åŒä¸€é¢…éª¨è¿›è¡Œä¸€å¯¹ä¸€å’Œå¤šé¢éƒ¨é‡å»ºï¼Œåæ˜ ä¸ªäººéšæ—¶é—´å˜åŒ–ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡åŒ»å­¦åº”ç”¨å±•ç¤ºäº†BFSMçš„æ½œåŠ›ï¼ŒåŒ…æ‹¬ä»å•ä¸€å›¾åƒè¿›è¡Œ3Dé¢éƒ¨-é¢…éª¨é‡å»ºå’Œæ‰‹æœ¯è§„åˆ’é¢„æµ‹ã€‚å¹¿æ³›å®éªŒè¯å®äº†æˆ‘ä»¬æ–¹æ³•çš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚BFSMå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wang-zidu/BFSM">https://github.com/wang-zidu/BFSM</a>è®¿é—®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ„å»ºé¢éƒ¨-é¢…éª¨å¯å˜å½¢æ¨¡å‹å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬è¿œç¨‹è¯Šæ–­ã€æ‰‹æœ¯è§„åˆ’ç­‰ã€‚</li>
<li>å½“å‰é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯æ•°æ®ç¼ºä¹ã€æ³¨å†Œå‡†ç¡®åº¦ä¸è¶³å’Œä¸´åºŠåº”ç”¨æ¢ç´¢æœ‰é™ã€‚</li>
<li>å¼•å…¥äº†åŒ…å«æ­£å¸¸å’Œé¢…é¢ç•¸å½¢ç—…ä¾‹çš„æ•°æ®é›†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¯†é›†å°„çº¿åŒ¹é…æ³¨å†Œæ–¹æ³•ï¼Œç¡®ä¿é¢éƒ¨ã€é¢…éª¨å’Œç»„ç»‡çš„æ‹“æ‰‘ä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥äº†3DåŒå‘é¢éƒ¨-é¢…éª¨å¯å˜å½¢æ¨¡å‹ï¼ˆBFSMï¼‰ï¼Œå®ç°é¢éƒ¨å’Œé¢…éª¨ä¹‹é—´çš„å½¢çŠ¶æ¨æ–­ï¼Œå¹¶è€ƒè™‘ç»„ç»‡åšåº¦å˜åŒ–ã€‚</li>
<li>BFSMæ”¯æŒä»åŒä¸€é¢…éª¨è¿›è¡Œä¸€å¯¹ä¸€å’Œå¤šé¢éƒ¨é‡å»ºï¼Œåæ˜ ä¸ªäººéšæ—¶é—´çš„å˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24577">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c06e84e9f7b453acc8fe90e7657fcd01" align="middle">
<img src="https://pica.zhimg.com/v2-dd6e44a9a10aea1a4208847775980135.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c0a6897f6a4531d47cc9c0846686bee" align="middle">
<img src="https://pic1.zhimg.com/v2-11262465bd4601dca6f9aec48572e2ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f86964f815a86f938b819e213763e9a0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="An-Enhanced-Pyramid-Feature-Network-Based-on-Long-Range-Dependencies-for-Multi-Organ-Medical-Image-Segmentation"><a href="#An-Enhanced-Pyramid-Feature-Network-Based-on-Long-Range-Dependencies-for-Multi-Organ-Medical-Image-Segmentation" class="headerlink" title="An Enhanced Pyramid Feature Network Based on Long-Range Dependencies for   Multi-Organ Medical Image Segmentation"></a>An Enhanced Pyramid Feature Network Based on Long-Range Dependencies for   Multi-Organ Medical Image Segmentation</h2><p><strong>Authors:Dayu Tan, Cheng Kong, Yansen Su, Hai Chen, Dongliang Yang, Junfeng Xia, Chunhou Zheng</strong></p>
<p>In the field of multi-organ medical image segmentation, recent methods frequently employ Transformers to capture long-range dependencies from image features. However, these methods overlook the high computational cost of Transformers and their deficiencies in extracting local detailed information. To address high computational costs and inadequate local detail information, we reassess the design of feature extraction modules and propose a new deep-learning network called LamFormer for fine-grained segmentation tasks across multiple organs. LamFormer is a novel U-shaped network that employs Linear Attention Mamba (LAM) in an enhanced pyramid encoder to capture multi-scale long-range dependencies. We construct the Parallel Hierarchical Feature Aggregation (PHFA) module to aggregate features from different layers of the encoder, narrowing the semantic gap among features while filtering information. Finally, we design the Reduced Transformer (RT), which utilizes a distinct computational approach to globally model up-sampled features. RRT enhances the extraction of detailed local information and improves the networkâ€™s capability to capture long-range dependencies. LamFormer outperforms existing segmentation methods on seven complex and diverse datasets, demonstrating exceptional performance. Moreover, the proposed network achieves a balance between model performance and model complexity. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒå¤šå™¨å®˜åˆ†å‰²é¢†åŸŸï¼Œè¿‘æœŸçš„æ–¹æ³•ç»å¸¸é‡‡ç”¨Transformeræ¥æ•æ‰å›¾åƒç‰¹å¾çš„é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¿½ç•¥äº†Transformerçš„é«˜è®¡ç®—æˆæœ¬å’Œå®ƒä»¬åœ¨æå–å±€éƒ¨è¯¦ç»†ä¿¡æ¯æ–¹é¢çš„ä¸è¶³ã€‚ä¸ºäº†è§£å†³é«˜è®¡ç®—æˆæœ¬å’Œå±€éƒ¨ç»†èŠ‚ä¿¡æ¯ä¸è¶³çš„é—®é¢˜ï¼Œæˆ‘ä»¬é‡æ–°è¯„ä¼°ç‰¹å¾æå–æ¨¡å—çš„è®¾è®¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹çš„æ·±åº¦å­¦ä¹ ç½‘ç»œï¼Œç§°ä¸ºLamFormerï¼Œç”¨äºå¤šå™¨å®˜çš„ç²¾ç»†åˆ†å‰²ä»»åŠ¡ã€‚LamFormeræ˜¯ä¸€ä¸ªæ–°å‹Uå‹ç½‘ç»œï¼Œåœ¨å¢å¼ºçš„é‡‘å­—å¡”ç¼–ç å™¨ä¸­ä½¿ç”¨çº¿æ€§æ³¨æ„åŠ›ç›å§†å·´ï¼ˆLAMï¼‰æ¥æ•æ‰å¤šå°ºåº¦é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬æ„å»ºäº†å¹¶è¡Œå±‚æ¬¡ç‰¹å¾èšåˆï¼ˆPHFAï¼‰æ¨¡å—ï¼Œä»¥èšåˆç¼–ç å™¨ä¸åŒå±‚çº§çš„ç‰¹å¾ï¼Œç¼©å°ç‰¹å¾ä¹‹é—´çš„è¯­ä¹‰å·®è·ï¼ŒåŒæ—¶è¿‡æ»¤ä¿¡æ¯ã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†ç²¾ç®€Transformerï¼ˆRTï¼‰ï¼Œå®ƒé‡‡ç”¨ç‹¬ç‰¹çš„è®¡ç®—æ–¹æ³•å¯¹ä¸Šé‡‡æ ·ç‰¹å¾è¿›è¡Œå…¨å±€å»ºæ¨¡ã€‚RRTå¢å¼ºäº†æå–è¯¦ç»†å±€éƒ¨ä¿¡æ¯çš„èƒ½åŠ›ï¼Œæé«˜äº†ç½‘ç»œæ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»çš„èƒ½åŠ›ã€‚LamFormeråœ¨ä¸ƒä¸ªå¤æ‚ä¸”å¤šæ ·çš„æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„åˆ†å‰²æ–¹æ³•ï¼Œå±•ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„ç½‘ç»œåœ¨æ¨¡å‹æ€§èƒ½å’Œæ¨¡å‹å¤æ‚æ€§ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24358v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤šå™¨å®˜åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„ä¸€ç§æ–°å‹æ·±åº¦å­¦ä¹ ç½‘ç»œâ€”â€”LamFormerã€‚ä¸ºåº”å¯¹Transformerçš„é«˜è®¡ç®—æˆæœ¬å’Œå±€éƒ¨ç»†èŠ‚ä¿¡æ¯æå–ä¸è¶³çš„é—®é¢˜ï¼ŒLamFormeré‡‡ç”¨çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶Mambaå¢å¼ºé‡‘å­—å¡”ç¼–ç å™¨ï¼Œæ•æ‰å¤šå°ºåº¦é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œæ„å»ºå¹¶è¡Œå±‚æ¬¡ç‰¹å¾èšåˆæ¨¡å—ä»¥ç¼©å°ç‰¹å¾é—´çš„è¯­ä¹‰å·®è·å¹¶è¿‡æ»¤ä¿¡æ¯ã€‚åŒæ—¶ï¼Œé€šè¿‡é‡‡ç”¨ç‹¬ç‰¹çš„è®¡ç®—æ–¹æ³•çš„ç®€åŒ–ç‰ˆTransformerï¼ˆRTï¼‰ï¼Œå®ç°ä¸Šé‡‡æ ·ç‰¹å¾çš„å…¨å±€å»ºæ¨¡ï¼Œæé«˜äº†å±€éƒ¨ç»†èŠ‚ä¿¡æ¯çš„æå–èƒ½åŠ›ã€‚åœ¨ä¸ƒä¸ªå¤æ‚å¤šæ ·çš„æ•°æ®é›†ä¸Šï¼ŒLamFormerè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶å®ç°äº†æ¨¡å‹æ€§èƒ½å’Œå¤æ‚æ€§çš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LamFormeræ˜¯ä¸€ç§ç”¨äºå¤šå™¨å®˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ·±åº¦å­¦ä¹ ç½‘ç»œã€‚</li>
<li>è¯¥ç½‘ç»œé’ˆå¯¹Transformeré«˜è®¡ç®—æˆæœ¬å’Œå±€éƒ¨ç»†èŠ‚ä¿¡æ¯æå–ä¸è¶³çš„é—®é¢˜è¿›è¡Œäº†ä¼˜åŒ–ã€‚</li>
<li>LamFormeré‡‡ç”¨çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶Mambaå¢å¼ºé‡‘å­—å¡”ç¼–ç å™¨ä»¥æ•æ‰å¤šå°ºåº¦é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚</li>
<li>æ„å»ºäº†å¹¶è¡Œå±‚æ¬¡ç‰¹å¾èšåˆæ¨¡å—ä»¥æ•´åˆä¸åŒå±‚æ¬¡ç‰¹å¾ï¼Œç¼©å°è¯­ä¹‰å·®è·å¹¶è¿‡æ»¤ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨ç®€åŒ–ç‰ˆTransformerï¼ˆRTï¼‰ï¼Œå®ç°äº†ä¸Šé‡‡æ ·ç‰¹å¾çš„å…¨å±€å»ºæ¨¡ï¼Œæé«˜äº†å±€éƒ¨ç»†èŠ‚ä¿¡æ¯çš„æå–èƒ½åŠ›ã€‚</li>
<li>LamFormeråœ¨ä¸ƒä¸ªå¤æ‚æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰åˆ†å‰²æ–¹æ³•ï¼Œå…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-622790e217d154ae1c744c2e76d58aad" align="middle">
<img src="https://picx.zhimg.com/v2-e23195113fbc46d9c5b3d75e12f98020" align="middle">
<img src="https://picx.zhimg.com/v2-e77a4bcf184f14bb8ec20fefad9659c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78eaa084a31db94b77ba721b433e872a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77f42c3c6af7b5081193e6b9021512dc" align="middle">
<img src="https://pic1.zhimg.com/v2-28746cbbf29c38892cb05c66429971a3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="In-situ-X-ray-imaging-of-reduction-nitridation-in-ferric-oxide-under-high-Pressure"><a href="#In-situ-X-ray-imaging-of-reduction-nitridation-in-ferric-oxide-under-high-Pressure" class="headerlink" title="In-situ X-ray imaging of reduction-nitridation in ferric oxide under   high Pressure"></a>In-situ X-ray imaging of reduction-nitridation in ferric oxide under   high Pressure</h2><p><strong>Authors:Yu Tao, Depu Liu, Chunyin Zhou, Xv Jia, Jingyi Liu, Xue Chang, Yangbin Wang, Li Lei</strong></p>
<p>The investigation of high-pressure chemical reaction dynamics has long been constrained by the absence of effective in-situ characterization methods. Here we performed the state-of-the-art synchrotron radiation in-situ X-ray imaging based on large-volume press (LVP) technology to uncover the crucial information on the dynamics of the underlying phenomena of the formation of iron-based spherical product in the high-pressure solid-state metathesis reaction (HSM). We successfully give access to the entire reduction-nitridation process of ferric oxide under the reaction conditions. By analyzing the variation of image intensity (Im) with temperature, two critical stages have been revealed, the formation of nitrogen-containing molten borate Fe[BO]+N[BO] melt and the phase separation of iron nitrides from molten borate. The experimental observation provides direct evidence for the existence of nitrogen-containing molten borate under high pressure, and the formation of molten borate plays a crucial role as a transport medium for nonmetallic ion exchange with metal elements. The M[BO]+N[BO] melt (M&#x3D;other metals) represents a general pathway for the synthesis of metal nitrides under high pressure. The combination of LVP high-pressure experimental technology with X-ray radioscopy has resulted in a leap in the understanding of reaction dynamics and has opened new paths in the fields of high-pressure chemistry. </p>
<blockquote>
<p>å¯¹äºé«˜å‹åŒ–å­¦ååº”åŠ¨åŠ›å­¦çš„ç ”ç©¶é•¿æœŸä»¥æ¥ä¸€ç›´å—é™äºç¼ºä¹æœ‰æ•ˆçš„åŸä½è¡¨å¾æ–¹æ³•ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åŸºäºå¤§ä½“å‹ï¼ˆLVPï¼‰æŠ€æœ¯çš„æœ€å…ˆè¿›çš„åŒæ­¥è¾å°„åŸä½Xå°„çº¿æˆåƒï¼Œæ­ç¤ºäº†é«˜å‹å›ºæ€ç½®æ¢ååº”ï¼ˆHSMï¼‰ä¸­çƒå½¢é“åŸºäº§å“å½¢æˆè¿‡ç¨‹ä¸­åŸºæœ¬ç°è±¡çš„åŠ¨æ€è¿‡ç¨‹ä¸­çš„å…³é”®ä¿¡æ¯ã€‚æˆ‘ä»¬æˆåŠŸåœ°è·å¾—äº†é“æ°§åŒ–ç‰©åœ¨æ•´ä¸ªè¿˜åŸæ°®åŒ–è¿‡ç¨‹ä¸­çš„ååº”æ¡ä»¶ã€‚é€šè¿‡åˆ†æå›¾åƒå¼ºåº¦ï¼ˆImï¼‰éšæ¸©åº¦çš„å˜åŒ–ï¼Œæ­ç¤ºäº†ä¸¤ä¸ªé˜¶æ®µçš„å…³é”®æ€§è½¬å˜ï¼šå«æ°®ç†”ç¡¼é…¸ç›Fe[BO]+N[BO]ç†”ä½“çš„å½¢æˆä»¥åŠé“æ°®åŒ–ç‰©ä»ç†”ç¡¼é…¸ç›ä¸­çš„ç›¸åˆ†ç¦»ã€‚å®éªŒè§‚å¯Ÿä¸ºé«˜å‹ä¸‹å«æ°®ç†”ç¡¼é…¸ç›çš„å­˜åœ¨æä¾›äº†ç›´æ¥è¯æ®ï¼Œç†”ç¡¼é…¸ç›çš„å½¢æˆåœ¨é«˜å‹ä¸‹ä½œä¸ºéé‡‘å±ç¦»å­äº¤æ¢ä¸é‡‘å±å…ƒç´ çš„ä¼ è¾“ä»‹è´¨èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚M[BO]+N[BO]ï¼ˆMä¸ºå…¶ä»–é‡‘å±ï¼‰ç†”ä½“ä»£è¡¨ç€é«˜å‹åˆæˆé‡‘å±æ°®åŒ–ç‰©çš„ä¸€èˆ¬é€”å¾„ã€‚å°†LVPé«˜å‹å®éªŒæŠ€æœ¯ä¸Xå°„çº¿æ”¾å°„ç…§ç›¸æœ¯ç›¸ç»“åˆï¼Œå®ç°äº†å¯¹ååº”åŠ¨åŠ›å­¦çš„ç†è§£çš„é£è·ƒï¼Œå¹¶ä¸ºé«˜å‹åŒ–å­¦é¢†åŸŸå¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24289v1">PDF</a> 13 pages, 4 figures</p>
<p><strong>Summary</strong><br>     é‡‡ç”¨åŸºäºå¤§ä½“å‹æŠ€æœ¯çš„é«˜å‹åŠ›åŒæ­¥è¾å°„Xå°„çº¿æˆåƒï¼Œæ­ç¤ºé“åŸºçƒå½¢äº§ç‰©åœ¨é«˜å‹åŠ›å›ºæ€è½¬æ¢ååº”ï¼ˆHSMï¼‰ä¸­å½¢æˆæœºç†çš„é‡è¦ä¿¡æ¯ã€‚æˆåŠŸè§‚å¯Ÿåˆ°æ°§åŒ–é“åœ¨ååº”æ¡ä»¶ä¸‹çš„æ•´ä¸ªè¿˜åŸæ°®åŒ–è¿‡ç¨‹ï¼Œå‘ç°æ°®åŒ–ç‰©ç†”ä½“å½¢æˆåŠé“æ°®åŒ–ç‰©ä»ç†”ä½“ä¸­åˆ†ç¦»çš„å…³é”®é˜¶æ®µã€‚å®éªŒè§‚å¯Ÿä¸ºé«˜å‹ä¸‹å«æ°®ç†”ä½“çš„å­˜åœ¨æä¾›äº†ç›´æ¥è¯æ®ï¼Œå¹¶è¯æ˜äº†ç†”ä½“å½¢æˆä½œä¸ºéé‡‘å±ç¦»å­ä¸é‡‘å±å…ƒç´ äº¤æ¢çš„ä¼ è¾“ä»‹è´¨çš„é‡è¦ä½œç”¨ã€‚è¯¥ç ”ç©¶å¯¹äºé«˜å‹ä¸‹é‡‘å±æ°®åŒ–ç‰©åˆæˆå…·æœ‰æ™®éæ„ä¹‰ï¼Œå°†é«˜å‹å®éªŒæŠ€æœ¯ä¸Xå°„çº¿æ”¾å°„ç…§ç›¸æœ¯ç›¸ç»“åˆï¼Œå®ç°äº†ååº”åŠ¨åŠ›å­¦ç†è§£çš„é£è·ƒï¼Œå¹¶ä¸ºé«˜å‹åŒ–å­¦é¢†åŸŸå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨å…ˆè¿›çš„åŒæ­¥è¾å°„Xå°„çº¿æˆåƒæŠ€æœ¯ï¼Œå®ç°äº†åœ¨é«˜å‹åŠ›ç¯å¢ƒä¸‹çš„å®æ—¶è§‚å¯Ÿã€‚</li>
<li>é€šè¿‡LVPæŠ€æœ¯æ­ç¤ºäº†é“åŸºçƒå½¢äº§ç‰©åœ¨é«˜å‹å›ºæ€è½¬æ¢ååº”ï¼ˆHSMï¼‰ä¸­çš„å½¢æˆæœºç†ã€‚</li>
<li>è§‚å¯Ÿåˆ°äº†æ°§åŒ–é“åœ¨ååº”æ¡ä»¶ä¸‹çš„æ•´ä¸ªè¿˜åŸæ°®åŒ–è¿‡ç¨‹ã€‚</li>
<li>å‘ç°äº†æ°®åŒ–ç‰©ç†”ä½“çš„å½¢æˆä»¥åŠé“æ°®åŒ–ç‰©ä»ç†”ä½“ä¸­çš„åˆ†ç¦»æ˜¯ä¸¤ä¸ªå…³é”®é˜¶æ®µã€‚</li>
<li>å®éªŒç›´æ¥è¯æ®æ”¯æŒäº†é«˜å‹ä¸‹å«æ°®ç†”ä½“çš„å­˜åœ¨ã€‚</li>
<li>ç†”ä½“çš„å½¢æˆå¯¹äºéé‡‘å±ç¦»å­ä¸é‡‘å±å…ƒç´ çš„äº¤æ¢èµ·åˆ°äº†å…³é”®çš„ä¼ è¾“ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24289">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b628414a04b0df2b15c59ff48d35cdd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b79d1b774985b597496aae0fed7b3e7" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="BALR-SAM-Boundary-Aware-Low-Rank-Adaptation-of-SAM-for-Resource-Efficient-Medical-Image-Segmentation"><a href="#BALR-SAM-Boundary-Aware-Low-Rank-Adaptation-of-SAM-for-Resource-Efficient-Medical-Image-Segmentation" class="headerlink" title="BALR-SAM: Boundary-Aware Low-Rank Adaptation of SAM for   Resource-Efficient Medical Image Segmentation"></a>BALR-SAM: Boundary-Aware Low-Rank Adaptation of SAM for   Resource-Efficient Medical Image Segmentation</h2><p><strong>Authors:Zelin Liu, Sicheng Dong, Bocheng Li, Yixuan Yang, Jiacheng Ruan, Chenxu Zhou, Suncheng Xiang</strong></p>
<p>Vision foundation models like the Segment Anything Model (SAM), pretrained on large-scale natural image datasets, often struggle in medical image segmentation due to a lack of domain-specific adaptation. In clinical practice, fine-tuning such models efficiently for medical downstream tasks with minimal resource demands, while maintaining strong performance, is challenging. To address these issues, we propose BALR-SAM, a boundary-aware low-rank adaptation framework that enhances SAM for medical imaging. It combines three tailored components: (1) a Complementary Detail Enhancement Network (CDEN) using depthwise separable convolutions and multi-scale fusion to capture boundary-sensitive features essential for accurate segmentation; (2) low-rank adapters integrated into SAMâ€™s Vision Transformer blocks to optimize feature representation and attention for medical contexts, while simultaneously significantly reducing the parameter space; and (3) a low-rank tensor attention mechanism in the mask decoder, cutting memory usage by 75% and boosting inference speed. Experiments on standard medical segmentation datasets show that BALR-SAM, without requiring prompts, outperforms several state-of-the-art (SOTA) methods, including fully fine-tuned MedSAM, while updating just 1.8% (11.7M) of its parameters. </p>
<blockquote>
<p>åƒSegment Anything Modelï¼ˆSAMï¼‰è¿™æ ·çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œåœ¨å¤§è§„æ¨¡è‡ªç„¶å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¾€å¾€ç”±äºç¼ºå°‘ç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§è€Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°æŒ£æ‰ã€‚åœ¨ä¸´åºŠå®è·µä¸­ï¼Œä»¥æœ€å°çš„èµ„æºéœ€æ±‚é«˜æ•ˆåœ°å¯¹è¿™ç±»æ¨¡å‹è¿›è¡ŒåŒ»å­¦ä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„æ€§èƒ½ï¼Œæ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BALR-SAMï¼Œè¿™æ˜¯ä¸€ä¸ªè¾¹ç•Œæ„ŸçŸ¥çš„ä½ç§©é€‚åº”æ¡†æ¶ï¼Œç”¨äºå¢å¼ºSAMåœ¨åŒ»å­¦æˆåƒä¸­çš„åº”ç”¨ã€‚å®ƒç»“åˆäº†ä¸‰ä¸ªå®šåˆ¶ç»„ä»¶ï¼šï¼ˆ1ï¼‰ä½¿ç”¨æ·±åº¦å¯åˆ†ç¦»å·ç§¯å’Œå¤šå°ºåº¦èåˆçš„äº’è¡¥ç»†èŠ‚å¢å¼ºç½‘ç»œï¼ˆCDENï¼‰ï¼Œä»¥æ•è·å¯¹å‡†ç¡®åˆ†å‰²è‡³å…³é‡è¦çš„è¾¹ç•Œæ•æ„Ÿç‰¹å¾ï¼›ï¼ˆ2ï¼‰å°†ä½ç§©é€‚é…å™¨é›†æˆåˆ°SAMçš„è§†è§‰è½¬æ¢å™¨å—ä¸­ï¼Œä»¥ä¼˜åŒ–åŒ»ç–—ç¯å¢ƒä¸‹çš„ç‰¹å¾è¡¨ç¤ºå’Œæ³¨æ„åŠ›ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘å‚æ•°ç©ºé—´ï¼›ï¼ˆ3ï¼‰åœ¨æ©è†œè§£ç å™¨ä¸­ä½¿ç”¨ä½ç§©å¼ é‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œå‡å°‘75%çš„å†…å­˜ä½¿ç”¨ï¼Œå¹¶æé«˜æ¨ç†é€Ÿåº¦ã€‚åœ¨æ ‡å‡†åŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBALR-SAMæ— éœ€æç¤ºå³å¯è¶…è¶ŠåŒ…æ‹¬å®Œå…¨å¾®è°ƒè¿‡çš„MedSAMåœ¨å†…çš„å‡ ç§æœ€æ–°æŠ€æœ¯æ–¹æ³•ï¼ŒåŒæ—¶ä»…æ›´æ–°å…¶1.8%ï¼ˆ11.7Mï¼‰çš„å‚æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24204v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºSegment Anything Modelï¼ˆSAMï¼‰çš„é¢„è®­ç»ƒæ¨¡å‹åœ¨å¤§è§„æ¨¡è‡ªç„¶å›¾åƒæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œä½†åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢å­˜åœ¨é¢†åŸŸç‰¹å®šé€‚åº”ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºBALR-SAMæ¨¡å‹ï¼Œç»“åˆä¸‰ç§å®šåˆ¶ç»„ä»¶ï¼ŒåŒ…æ‹¬ç»†èŠ‚å¢å¼ºç½‘ç»œã€ä½ç§©é€‚é…å™¨å’Œä½ç§©å¼ é‡æ³¨æ„åŠ›æœºåˆ¶ã€‚å®éªŒè¯æ˜ï¼ŒBALR-SAMåœ¨ä¸ä½¿ç”¨æç¤ºçš„æƒ…å†µä¸‹ï¼Œä»…æ›´æ–°å°‘é‡å‚æ•°å³å¯è¶…è¶Šå¤šä¸ªå…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒæ¨¡å‹å¦‚SAMåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸Šå­˜åœ¨é¢†åŸŸé€‚åº”æ€§æŒ‘æˆ˜ã€‚</li>
<li>BALR-SAMæ˜¯ä¸€ä¸ªè¾¹ç•Œæ„ŸçŸ¥çš„ä½ç§©é€‚åº”æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>BALR-SAMç»“åˆäº†ä¸‰ç§å®šåˆ¶ç»„ä»¶ï¼šç»†èŠ‚å¢å¼ºç½‘ç»œã€ä½ç§©é€‚é…å™¨å’Œä½ç§©å¼ é‡æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>ç»†èŠ‚å¢å¼ºç½‘ç»œä½¿ç”¨æ·±åº¦å¯åˆ†ç¦»å·ç§¯å’Œå¤šå°ºåº¦èåˆæ¥æ•æ‰å…³é”®è¾¹ç•Œç‰¹å¾ã€‚</li>
<li>ä½ç§©é€‚é…å™¨ä¼˜åŒ–äº†ç‰¹å¾è¡¨ç¤ºå’Œæ³¨æ„åŠ›ï¼Œæ˜¾è‘—å‡å°‘äº†å‚æ•°ç©ºé—´ã€‚</li>
<li>ä½ç§©å¼ é‡æ³¨æ„åŠ›æœºåˆ¶å‡å°‘äº†å†…å­˜ä½¿ç”¨å¹¶æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24204">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c2a648cfee957f9ebca2446ab0a36d4d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a3f99733555f2658755c7ecfb656919.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="An-Efficient-3D-Latent-Diffusion-Model-for-T1-contrast-Enhanced-MRI-Generation"><a href="#An-Efficient-3D-Latent-Diffusion-Model-for-T1-contrast-Enhanced-MRI-Generation" class="headerlink" title="An Efficient 3D Latent Diffusion Model for T1-contrast Enhanced MRI   Generation"></a>An Efficient 3D Latent Diffusion Model for T1-contrast Enhanced MRI   Generation</h2><p><strong>Authors:Zach Eidex, Mojtaba Safari, Jie Ding, Richard Qiu, Justin Roper, David Yu, Hui-Kuo Shu, Zhen Tian, Hui Mao, Xiaofeng Yang</strong></p>
<p>Objective: Gadolinium-based contrast agents (GBCAs) are commonly employed with T1w MRI to enhance lesion visualization but are restricted in patients at risk of nephrogenic systemic fibrosis and variations in GBCA administration can introduce imaging inconsistencies. This study develops an efficient 3D deep-learning framework to generate T1-contrast enhanced images (T1C) from pre-contrast multiparametric MRI. Approach: We propose the 3D latent rectified flow (T1C-RFlow) model for generating high-quality T1C images. First, T1w and T2-FLAIR images are input into a pretrained autoencoder to acquire an efficient latent space representation. A rectified flow diffusion model is then trained in this latent space representation. The T1C-RFlow model was trained on a curated dataset comprised of the BraTS 2024 glioma (GLI; 1480 patients), meningioma (MEN; 1141 patients), and metastases (MET; 1475 patients) datasets. Selected patients were split into train (N&#x3D;2860), validation (N&#x3D;612), and test (N&#x3D;614) sets. Results: Both qualitative and quantitative results demonstrate that the T1C-RFlow model outperforms benchmark 3D models (pix2pix, DDPM, Diffusion Transformers (DiT-3D)) trained in the same latent space. T1C-RFlow achieved the following metrics - GLI: NMSE 0.044 +&#x2F;- 0.047, SSIM 0.935 +&#x2F;- 0.025; MEN: NMSE 0.046 +&#x2F;- 0.029, SSIM 0.937 +&#x2F;- 0.021; MET: NMSE 0.098 +&#x2F;- 0.088, SSIM 0.905 +&#x2F;- 0.082. T1C-RFlow had the best tumor reconstruction performance and significantly faster denoising times (6.9 s&#x2F;volume, 200 steps) than conventional DDPM models in both latent space (37.7s, 1000 steps) and patch-based in image space (4.3 hr&#x2F;volume). Significance: Our proposed method generates synthetic T1C images that closely resemble ground truth T1C in much less time than previous diffusion models. Further development may permit a practical method for contrast-agent-free MRI for brain tumors. </p>
<blockquote>
<p>ç›®çš„ï¼šé’†åŸºé€ å½±å‰‚ï¼ˆGBCAsï¼‰å¸¸ä¸T1åŠ æƒç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸€èµ·ä½¿ç”¨ï¼Œä»¥æé«˜ç—…å˜çš„å¯è§†åŒ–ç¨‹åº¦ï¼Œä½†å¯¹äºæœ‰å‘ç”Ÿè‚¾æºæ€§ç³»ç»Ÿæ€§çº¤ç»´åŒ–é£é™©çš„æ‚£è€…ï¼Œå…¶ä½¿ç”¨å—åˆ°é™åˆ¶ï¼Œä¸”GBCAçš„ç»™è¯å˜åŒ–å¯èƒ½ä¼šå¼•å…¥æˆåƒä¸ä¸€è‡´æ€§ã€‚æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§é«˜æ•ˆçš„3Dæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºä»é¢„é€ å½±çš„å¤šå‚æ•°MRIç”ŸæˆT1åŠ æƒé€ å½±å‰‚å¢å¼ºå›¾åƒï¼ˆT1Cï¼‰ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ç”¨äºç”Ÿæˆé«˜è´¨é‡T1Cå›¾åƒçš„3Dæ½œåœ¨æ ¡æ­£æµï¼ˆT1C-RFlowï¼‰æ¨¡å‹ã€‚é¦–å…ˆï¼Œå°†T1åŠ æƒå’ŒT2-FLAIRå›¾åƒè¾“å…¥é¢„è®­ç»ƒçš„è‡ªç¼–ç å™¨ï¼Œä»¥è·å¾—æœ‰æ•ˆçš„æ½œåœ¨ç©ºé—´è¡¨ç¤ºã€‚ç„¶åï¼Œåœ¨æ­¤æ½œåœ¨ç©ºé—´è¡¨ç¤ºä¸­è®­ç»ƒæ ¡æ­£æµæ‰©æ•£æ¨¡å‹ã€‚T1C-RFlowæ¨¡å‹æ˜¯åœ¨ç²¾é€‰æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œè¯¥æ•°æ®é›†ç”±BraTS 2024èƒ¶è´¨ç»†èƒç˜¤ï¼ˆGLIï¼›1480åæ‚£è€…ï¼‰ã€è„‘è†œç˜¤ï¼ˆMENï¼›1141åæ‚£è€…ï¼‰å’Œè½¬ç§»ç˜¤ï¼ˆMETï¼›1475åæ‚£è€…ï¼‰æ•°æ®é›†ç»„æˆã€‚æ‰€é€‰æ‚£è€…è¢«åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆN&#x3D;2860ï¼‰ã€éªŒè¯é›†ï¼ˆN&#x3D;612ï¼‰å’Œæµ‹è¯•é›†ï¼ˆN&#x3D;614ï¼‰ã€‚</p>
<p>ç»“æœï¼šå®šæ€§å’Œå®šé‡ç»“æœå‡è¡¨æ˜ï¼ŒT1C-RFlowæ¨¡å‹åœ¨ç›¸åŒçš„æ½œåœ¨ç©ºé—´å†…ä¼˜äºåŸºå‡†çš„3Dæ¨¡å‹ï¼ˆpix2pixã€DDPMã€Diffusion Transformersï¼ˆDiT-3Dï¼‰ï¼‰ã€‚T1C-RFlowçš„åº¦é‡æŒ‡æ ‡å¦‚ä¸‹ï¼šGLIçš„NMSEä¸º0.044Â±0.047ï¼ŒSSIMä¸º0.935Â±0.025ï¼›MENçš„NMSEä¸º0.046Â±0.029ï¼ŒSSIMä¸º0.937Â±0.021ï¼›METçš„NMSEä¸º0.098Â±0.088ï¼ŒSSIMä¸º0.905Â±0.082ã€‚åœ¨è‚¿ç˜¤é‡å»ºæ€§èƒ½æ–¹é¢ï¼ŒT1C-RFlowè¡¨ç°æœ€ä½³ï¼Œå¹¶ä¸”å…¶é™å™ªæ—¶é—´ï¼ˆ6.9ç§’&#x2F;ä½“ç§¯ï¼Œ200æ­¥ï¼‰æ˜¾è‘—å¿«äºæ½œåœ¨ç©ºé—´çš„ä¼ ç»ŸDDPMæ¨¡å‹å’ŒåŸºäºå›¾åƒçš„å›¾åƒç©ºé—´ï¼ˆ4.3å°æ—¶&#x2F;ä½“ç§¯ï¼‰ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24194v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶é‡‡ç”¨3Dæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ŒåŸºäºT1åŠ æƒç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å’Œéå¯¹æ¯”å‰‚å¤šå‚æ•°MRIç”ŸæˆT1å¯¹æ¯”åº¦å¢å¼ºå›¾åƒï¼ˆT1Cï¼‰ã€‚æå‡ºçš„T1C-RFlowæ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡T1Cå›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ï¼Œä¸ºå¯¹æ¯”å‰‚ç›¸å…³çš„MRIæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é‡‡ç”¨3Dæ·±åº¦å­¦ä¹ æ¡†æ¶ç”ŸæˆT1å¯¹æ¯”åº¦å¢å¼ºå›¾åƒï¼ˆT1Cï¼‰ã€‚</li>
<li>T1C-RFlowæ¨¡å‹ç”¨äºç”Ÿæˆé«˜è´¨é‡T1Cå›¾åƒã€‚</li>
<li>T1C-RFlowæ¨¡å‹åœ¨å¤šç§ç–¾ç—…æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬èƒ¶è´¨ç»†èƒç˜¤ã€è„‘è†œç˜¤å’Œè½¬ç§»ç˜¤ã€‚</li>
<li>T1C-RFlowæ¨¡å‹åœ¨è‚¿ç˜¤é‡å»ºæ–¹é¢è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¿«çš„å»å™ªæ—¶é—´ã€‚</li>
<li>è¯¥æ–¹æ³•ç”Ÿæˆçš„T1Cå›¾åƒä¸çœŸå®T1Cå›¾åƒé«˜åº¦ç›¸ä¼¼ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„ç”Ÿæˆæ—¶é—´çŸ­ã€‚</li>
<li>ç ”ç©¶ä¸ºæœªæ¥å®ç°æ— å¯¹æ¯”å‰‚çš„MRIæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3444a172b2cea0585f06be75ed38e0da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbabea21c0f0464f457481cca0dec9fd" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Tumor-Synthesis-conditioned-on-Radiomics"><a href="#Tumor-Synthesis-conditioned-on-Radiomics" class="headerlink" title="Tumor Synthesis conditioned on Radiomics"></a>Tumor Synthesis conditioned on Radiomics</h2><p><strong>Authors:Jonghun Kim, Inye Na, Eun Sook Ko, Hyunjin Park</strong></p>
<p>Due to privacy concerns, obtaining large datasets is challenging in medical image analysis, especially with 3D modalities like Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Existing generative models, developed to address this issue, often face limitations in output diversity and thus cannot accurately represent 3D medical images. We propose a tumor-generation model that utilizes radiomics features as generative conditions. Radiomics features are high-dimensional handcrafted semantic features that are biologically well-grounded and thus are good candidates for conditioning. Our model employs a GAN-based model to generate tumor masks and a diffusion-based approach to generate tumor texture conditioned on radiomics features. Our method allows the user to generate tumor images according to user-specified radiomics features such as size, shape, and texture at an arbitrary location. This enables the physicians to easily visualize tumor images to better understand tumors according to changing radiomics features. Our approach allows for the removal, manipulation, and repositioning of tumors, generating various tumor types in different scenarios. The model has been tested on tumors in four different organs (kidney, lung, breast, and brain) across CT and MRI. The synthesized images are shown to effectively aid in training for downstream tasks and their authenticity was also evaluated through expert evaluations. Our method has potential usage in treatment planning with diverse synthesized tumors. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œç”±äºéšç§é—®é¢˜ï¼Œè·å–å¤§å‹æ•°æ®é›†å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å¦‚è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å’Œç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ç­‰3Dæ¨¡å¼çš„æƒ…å†µä¸‹ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜è€Œå¼€å‘çš„ç°æœ‰ç”Ÿæˆæ¨¡å‹åœ¨è¾“å‡ºå¤šæ ·æ€§æ–¹é¢é€šå¸¸é¢ä¸´å±€é™æ€§ï¼Œå› æ­¤æ— æ³•å‡†ç¡®ä»£è¡¨3DåŒ»å­¦å›¾åƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨æ”¾å°„å­¦ç‰¹å¾è¿›è¡Œè‚¿ç˜¤ç”Ÿæˆçš„æ¨¡å‹ã€‚æ”¾å°„å­¦ç‰¹å¾æ˜¯é«˜ç»´çš„æ‰‹å·¥è¯­ä¹‰ç‰¹å¾ï¼Œåœ¨ç”Ÿç‰©å­¦ä¸Šæœ‰å¾ˆå¥½çš„ä¾æ®ï¼Œå› æ­¤æ˜¯æ¡ä»¶è®¾å®šçš„è‰¯å¥½å€™é€‰è€…ã€‚æˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨åŸºäºGANçš„æ¨¡å‹æ¥ç”Ÿæˆè‚¿ç˜¤æ©è†œï¼Œå¹¶é‡‡ç”¨åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œæ ¹æ®æ”¾å°„å­¦ç‰¹å¾ç”Ÿæˆè‚¿ç˜¤çº¹ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸ç”¨æˆ·æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„æ”¾å°„å­¦ç‰¹å¾ï¼ˆå¦‚å¤§å°ã€å½¢çŠ¶å’Œçº¹ç†ï¼‰åœ¨ä»»æ„ä½ç½®ç”Ÿæˆè‚¿ç˜¤å›¾åƒã€‚è¿™ä½¿å¾—åŒ»ç”Ÿèƒ½å¤Ÿè½»æ¾åœ°æ ¹æ®å˜åŒ–çš„æ”¾å°„å­¦ç‰¹å¾å¯è§†åŒ–è‚¿ç˜¤å›¾åƒï¼Œä»è€Œæ›´å¥½åœ°ç†è§£è‚¿ç˜¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸åˆ é™¤ã€æ“ä½œå’Œé‡æ–°å®šä½è‚¿ç˜¤ï¼Œåœ¨ä¸åŒåœºæ™¯ä¸‹ç”Ÿæˆå„ç§è‚¿ç˜¤ç±»å‹ã€‚è¯¥æ¨¡å‹å·²åœ¨CTå’ŒMRIä¸­çš„å››ä¸ªä¸åŒå™¨å®˜ï¼ˆè‚¾è„ã€è‚ºéƒ¨ã€ä¹³æˆ¿å’Œå¤§è„‘ï¼‰çš„è‚¿ç˜¤ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚åˆæˆçš„å›¾åƒå·²è¢«è¯æ˜å¯ä»¥æœ‰æ•ˆåœ°è¾…åŠ©ä¸‹æ¸¸ä»»åŠ¡è®­ç»ƒï¼Œå…¶çœŸå®æ€§ä¹Ÿé€šè¿‡ä¸“å®¶è¯„ä¼°å¾—åˆ°äº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å…·æœ‰å¤šç§åˆæˆè‚¿ç˜¤çš„æ²»ç–—è®¡åˆ’ä¸­å…·æœ‰æ½œåœ¨ç”¨é€”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24182v1">PDF</a> WACVâ€™25</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§åŸºäºæ”¾å°„å­¦ç‰¹å¾çš„è‚¿ç˜¤ç”Ÿæˆæ¨¡å‹ï¼Œåˆ©ç”¨GANå’Œæ‰©æ•£æ¨¡å‹ç”Ÿæˆè‚¿ç˜¤æ©è†œå’Œçº¹ç†ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„æ”¾å°„å­¦ç‰¹å¾åœ¨ä»»æ„ä½ç½®ç”Ÿæˆè‚¿ç˜¤å›¾åƒã€‚æ­¤æ¨¡å‹æœ‰åŠ©äºæé«˜åŒ»ç”Ÿå¯¹è‚¿ç˜¤å˜åŒ–çš„ç†è§£ï¼Œè¾…åŠ©è‚¿ç˜¤æ²»ç–—è®¡åˆ’ã€‚è¯¥æ¨¡å‹å·²ç»åœ¨å››ç§ä¸åŒå™¨å®˜ï¼ˆè‚¾è„ã€è‚ºéƒ¨ã€ä¹³æˆ¿å’Œå¤§è„‘ï¼‰çš„CTå’ŒMRIå›¾åƒä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œå¹¶è¯å®äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œè·å–å¤§è§„æ¨¡æ•°æ®é›†é¢ä¸´éšç§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯3Dæ¨¡æ€å¦‚CTå’ŒMRIã€‚</li>
<li>ç°æœ‰ç”Ÿæˆæ¨¡å‹åœ¨è¾“å‡ºå¤šæ ·æ€§æ–¹é¢å­˜åœ¨å±€é™ï¼Œæ— æ³•å‡†ç¡®ä»£è¡¨3DåŒ»å­¦å›¾åƒã€‚</li>
<li>æå‡ºçš„è‚¿ç˜¤ç”Ÿæˆæ¨¡å‹åˆ©ç”¨æ”¾å°„å­¦ç‰¹å¾ä½œä¸ºç”Ÿæˆæ¡ä»¶ï¼Œè¿™äº›ç‰¹å¾æ˜¯ç”Ÿç‰©ä¸Šåˆç†çš„é«˜ç»´æ‰‹å·¥è¯­ä¹‰ç‰¹å¾ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨GANç”Ÿæˆè‚¿ç˜¤æ©è†œï¼Œæ‰©æ•£æ–¹æ³•ç”Ÿæˆæ¡ä»¶åŒ–çš„è‚¿ç˜¤çº¹ç†ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥æ ¹æ®æŒ‡å®šçš„æ”¾å°„å­¦ç‰¹å¾ï¼ˆå¦‚å¤§å°ã€å½¢çŠ¶å’Œçº¹ç†ï¼‰åœ¨ä»»æ„ä½ç½®ç”Ÿæˆè‚¿ç˜¤å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰åŠ©äºåŒ»ç”Ÿæ›´å¥½åœ°ç†è§£è‚¿ç˜¤å˜åŒ–ï¼Œè¾…åŠ©è‚¿ç˜¤æ²»ç–—è®¡åˆ’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24182">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c4117a11f2489af18ca816e397f0010" align="middle">
<img src="https://pic1.zhimg.com/v2-a9d4f2a46b7daf3f16169198a1268a7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6106e76377cce919aab211e35768875" align="middle">
<img src="https://picx.zhimg.com/v2-2736080f7059bd7c43861f18c908566a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e16b4fa77bd7475f1d119c2865a636d5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="High-Order-Progressive-Trajectory-Matching-for-Medical-Image-Dataset-Distillation"><a href="#High-Order-Progressive-Trajectory-Matching-for-Medical-Image-Dataset-Distillation" class="headerlink" title="High-Order Progressive Trajectory Matching for Medical Image Dataset   Distillation"></a>High-Order Progressive Trajectory Matching for Medical Image Dataset   Distillation</h2><p><strong>Authors:Le Dong, Jinghao Bian, Jingyang Hou, Jingliang Hu, Yilei Shi, Weisheng Dong, Xiao Xiang Zhu, Lichao Mou</strong></p>
<p>Medical image analysis faces significant challenges in data sharing due to privacy regulations and complex institutional protocols. Dataset distillation offers a solution to address these challenges by synthesizing compact datasets that capture essential information from real, large medical datasets. Trajectory matching has emerged as a promising methodology for dataset distillation; however, existing methods primarily focus on terminal states, overlooking crucial information in intermediate optimization states. We address this limitation by proposing a shape-wise potential that captures the geometric structure of parameter trajectories, and an easy-to-complex matching strategy that progressively addresses parameters based on their complexity. Experiments on medical image classification tasks demonstrate that our method improves distillation performance while preserving privacy and maintaining model accuracy comparable to training on the original datasets. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Bian-jh/HoP-TM">https://github.com/Bian-jh/HoP-TM</a>. </p>
<blockquote>
<p>åŒ»ç–—å›¾åƒåˆ†æåœ¨æ•°æ®å…±äº«æ–¹é¢é¢ä¸´ç€éšç§æ³•è§„å’Œå¤æ‚æœºæ„åè®®çš„é‡å¤§æŒ‘æˆ˜ã€‚æ•°æ®é›†è’¸é¦é€šè¿‡åˆæˆç´§å‡‘çš„æ•°æ®é›†æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¿™äº›åˆæˆæ•°æ®é›†ä»çœŸå®çš„å¤§å‹åŒ»ç–—æ•°æ®é›†ä¸­æå–äº†å…³é”®ä¿¡æ¯ã€‚è½¨è¿¹åŒ¹é…å·²æˆä¸ºæ•°æ®é›†è’¸é¦çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼›ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç»ˆç«¯çŠ¶æ€ï¼Œå¿½ç•¥äº†ä¸­é—´ä¼˜åŒ–çŠ¶æ€çš„å…³é”®ä¿¡æ¯ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§æ•æ‰å‚æ•°è½¨è¿¹å‡ ä½•ç»“æ„çš„å½¢çŠ¶åŠ¿èƒ½ï¼Œä»¥åŠä¸€ç§åŸºäºå‚æ•°å¤æ‚æ€§é€æ­¥è§£å†³çš„ç®€å•åˆ°å¤æ‚çš„åŒ¹é…ç­–ç•¥æ¥è§£å†³è¿™ä¸€å±€é™æ€§ã€‚åœ¨åŒ»ç–—å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ”¹å–„è’¸é¦æ€§èƒ½çš„åŒæ—¶ï¼Œä¹Ÿä¿ç•™äº†éšç§ï¼Œå¹¶ä¸”æ¨¡å‹ç²¾åº¦ä¸åœ¨åŸå§‹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒç›¸å½“ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Bian-jh/HoP-TM%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Bian-jh/HoP-TMä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24177v1">PDF</a> MICCAI 2025 (early accept, top 9%)</p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒåˆ†æåœ¨æ•°æ®å…±äº«æ–¹é¢é¢ä¸´éšç§æ³•è§„å’Œå¤æ‚æœºæ„åè®®çš„æŒ‘æˆ˜ã€‚æ•°æ®é›†è’¸é¦ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡åˆæˆç´§å‡‘æ•°æ®é›†æ¥æ•è·æ¥è‡ªçœŸå®å¤§å‹åŒ»å­¦æ•°æ®é›†çš„æœ¬è´¨ä¿¡æ¯ã€‚è½¨è¿¹åŒ¹é…å·²æˆä¸ºæ•°æ®é›†è’¸é¦ä¸­ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç»ˆç«¯çŠ¶æ€ï¼Œå¿½ç•¥äº†ä¸­é—´ä¼˜åŒ–çŠ¶æ€çš„å…³é”®ä¿¡æ¯ã€‚æœ¬ç ”ç©¶é€šè¿‡æå‡ºä¸€ç§æ•æ‰å‚æ•°è½¨è¿¹å‡ ä½•ç»“æ„çš„å½¢çŠ¶åŠ¿èƒ½ä»¥åŠä¸€ç§æ ¹æ®å‚æ•°å¤æ‚æ€§é€æ­¥è§£å†³çš„ç®€å•åˆ°å¤æ‚çš„åŒ¹é…ç­–ç•¥æ¥è§£å†³è¿™ä¸€å±€é™æ€§ã€‚åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†è’¸é¦æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŠ¤äº†éšç§ï¼Œå¹¶ä¿æŒæ¨¡å‹å‡†ç¡®ç‡ä¸åœ¨åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„å‡†ç¡®ç‡ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†æåœ¨æ•°æ®å…±äº«æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºéšç§æ³•è§„å’Œå¤æ‚æœºæ„åè®®ã€‚</li>
<li>æ•°æ®é›†è’¸é¦æ˜¯è§£å†³è¿™äº›æŒ‘æˆ˜çš„ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåˆæˆç´§å‡‘æ•°æ®é›†ï¼Œæ•è·çœŸå®å¤§å‹åŒ»å­¦æ•°æ®é›†çš„æœ¬è´¨ä¿¡æ¯ã€‚</li>
<li>è½¨è¿¹åŒ¹é…æ˜¯æ•°æ®é›†è’¸é¦çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç»ˆç«¯çŠ¶æ€ï¼Œå¿½ç•¥äº†ä¸­é—´ä¼˜åŒ–çŠ¶æ€ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å½¢çŠ¶åŠ¿èƒ½ï¼Œç”¨äºæ•æ‰å‚æ•°è½¨è¿¹çš„å‡ ä½•ç»“æ„ã€‚</li>
<li>ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ç®€å•åˆ°å¤æ‚çš„åŒ¹é…ç­–ç•¥ï¼Œæ ¹æ®å‚æ•°çš„å¤æ‚æ€§é€æ­¥è§£å†³ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†è’¸é¦æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŠ¤äº†éšç§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-427e9049ab5149b550143f5ba69b5608" align="middle">
<img src="https://picx.zhimg.com/v2-a3063b2a40c87f8b021cac54b226d966" align="middle">
<img src="https://picx.zhimg.com/v2-33498f854ce0ad3dc355048ce44a44ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-281f5808614783b4c6ab3827fa5e5aae" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="EYE-DEX-Eye-Disease-Detection-and-EXplanation-System"><a href="#EYE-DEX-Eye-Disease-Detection-and-EXplanation-System" class="headerlink" title="EYE-DEX: Eye Disease Detection and EXplanation System"></a>EYE-DEX: Eye Disease Detection and EXplanation System</h2><p><strong>Authors:Youssef Sabiri, Walid Houmaidi, Amine Abouaomar</strong></p>
<p>Retinal disease diagnosis is critical in preventing vision loss and reducing socioeconomic burdens. Globally, over 2.2 billion people are affected by some form of vision impairment, resulting in annual productivity losses estimated at $411 billion. Traditional manual grading of retinal fundus images by ophthalmologists is time-consuming and subjective. In contrast, deep learning has revolutionized medical diagnostics by automating retinal image analysis and achieving expert-level performance. In this study, we present EYE-DEX, an automated framework for classifying 10 retinal conditions using the large-scale Retinal Disease Dataset comprising 21,577 eye fundus images. We benchmark three pre-trained Convolutional Neural Network (CNN) modelsâ€“VGG16, VGG19, and ResNet50â€“with our finetuned VGG16 achieving a state-of-the-art global benchmark test accuracy of 92.36%. To enhance transparency and explainability, we integrate the Gradient-weighted Class Activation Mapping (Grad-CAM) technique to generate visual explanations highlighting disease-specific regions, thereby fostering clinician trust and reliability in AI-assisted diagnostics. </p>
<blockquote>
<p>è§†ç½‘è†œç–¾ç—…è¯Šæ–­å¯¹äºé¢„é˜²è§†åŠ›ä¸§å¤±å’Œå‡å°‘ç¤¾ä¼šç»æµè´Ÿæ‹…è‡³å…³é‡è¦ã€‚å…¨çƒæœ‰è¶…è¿‡2.2äº¿äººå—åˆ°æŸç§å½¢å¼çš„è§†åŠ›éšœç¢å½±å“ï¼Œå¯¼è‡´æ¯å¹´ç”Ÿäº§åŠ›æŸå¤±ä¼°è®¡ä¸º4.1ä¸‡äº¿ç¾å…ƒã€‚çœ¼ç§‘åŒ»ç”Ÿå¯¹è§†ç½‘è†œçœ¼åº•å›¾åƒçš„ä¼ ç»Ÿæ‰‹åŠ¨åˆ†çº§è€—æ—¶ä¸”ä¸»è§‚ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ·±åº¦å­¦ä¹ é€šè¿‡è‡ªåŠ¨åŒ–è§†ç½‘è†œå›¾åƒåˆ†æå¹¶è¾¾åˆ°ä¸“å®¶çº§æ€§èƒ½ï¼Œä»è€Œå½»åº•æ”¹å˜äº†åŒ»å­¦è¯Šæ–­é¢†åŸŸã€‚åœ¨ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†EYE-DEXï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨å¤§è§„æ¨¡è§†ç½‘è†œç–¾ç—…æ•°æ®é›†ï¼ˆåŒ…å«21577å¼ çœ¼åº•å›¾åƒï¼‰å¯¹10ç§è§†ç½‘è†œçŠ¶å†µè¿›è¡Œåˆ†ç±»çš„è‡ªåŠ¨åŒ–æ¡†æ¶ã€‚æˆ‘ä»¬å¯¹ä¸‰ä¸ªé¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹â€”â€”VGG16ã€VGG19å’ŒResNet50è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¾®è°ƒåçš„VGG16åœ¨æœ€æ–°çš„å…¨çƒåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†92.36%çš„å‡†ç¡®ç‡ã€‚ä¸ºäº†æé«˜é€æ˜åº¦å’Œè§£é‡Šæ€§ï¼Œæˆ‘ä»¬ç»“åˆäº†æ¢¯åº¦åŠ æƒç±»æ¿€æ´»æ˜ å°„ï¼ˆGrad-CAMï¼‰æŠ€æœ¯ï¼Œç”Ÿæˆè§†è§‰è§£é‡Šï¼Œçªå‡ºæ˜¾ç¤ºç‰¹å®šäºç–¾ç—…çš„åŒºåŸŸï¼Œä»è€Œä¿ƒè¿›ä¸´åºŠåŒ»ç”Ÿå¯¹äººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­çš„ä¿¡ä»»å’Œå¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24136v1">PDF</a> 6 pages, 4 figures, 3 tables. Accepted at the 12th International   Conference on Wireless Networks and Mobile Communications 2025 (WINCOM 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯å¯¹è§†ç½‘è†œç–¾ç—…è¿›è¡Œè‡ªåŠ¨è¯Šæ–­çš„é‡è¦æ€§ã€‚å…¨çƒæœ‰è¶…è¿‡2.2äº¿äººå­˜åœ¨è§†åŠ›å—æŸé—®é¢˜ï¼Œå¯¼è‡´æ¯å¹´ç”Ÿäº§åŠ›æŸå¤±ä¼°è®¡è¾¾411äº¿ç¾å…ƒã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºEYE-DEXçš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œç”¨äºå¯¹å¤§å‹è§†ç½‘è†œç–¾ç—…æ•°æ®é›†ï¼ˆåŒ…å«21,577å¼ çœ¼åº•å›¾åƒï¼‰ä¸­çš„10ç§è§†ç½‘è†œç—…å˜è¿›è¡Œåˆ†ç±»ã€‚é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹ï¼Œå¦‚VGG16ã€VGG19å’ŒResNet50ï¼Œå…¶ä¸­VGG16æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€é«˜çš„å‡†ç¡®åº¦ä¸º92.36%ã€‚ä¸ºäº†æé«˜é€æ˜åº¦å’Œè§£é‡Šæ€§ï¼Œé›†æˆäº†æ¢¯åº¦åŠ æƒç±»åˆ«æ¿€æ´»æ˜ å°„ï¼ˆGrad-CAMï¼‰æŠ€æœ¯ï¼Œç”Ÿæˆçªå‡ºæ˜¾ç¤ºç—…å˜ç‰¹å®šåŒºåŸŸçš„è§†è§‰è§£é‡Šï¼Œä¿ƒè¿›ä¸´åºŠåŒ»ç”Ÿå¯¹AIè¾…åŠ©è¯Šæ–­çš„ä¿¡ä»»å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†ç½‘è†œç–¾ç—…è¯Šæ–­å¯¹é¢„é˜²è§†åŠ›æŸå¤±å’Œå‡å°‘ç¤¾ä¼šç»æµè´Ÿæ‹…è‡³å…³é‡è¦ã€‚</li>
<li>å…¨çƒæœ‰è¶…è¿‡2.2äº¿äººå—åˆ°æŸç§å½¢å¼çš„è§†åŠ›éšœç¢çš„å½±å“ã€‚</li>
<li>ä¼ ç»Ÿçš„æ‰‹åŠ¨è¯„ä¼°è§†ç½‘è†œåŸºé‡‘å›¾åƒçš„æ–¹æ³•æ—¢è€—æ—¶åˆä¸»è§‚ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æŠ€æœ¯é€šè¿‡è‡ªåŠ¨åˆ†æè§†ç½‘è†œå›¾åƒå¹¶å®ç°ä¸“å®¶çº§æ€§èƒ½ï¼Œå·²ç»å½»åº•æ”¹å˜äº†åŒ»ç–—è¯Šæ–­ã€‚</li>
<li>åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºEYE-DEXçš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œç”¨äºå¯¹è§†ç½‘è†œç–¾ç—…æ•°æ®é›†è¿›è¡Œåˆ†ç±»ï¼Œå¹¶ä½¿ç”¨å¤§å‹æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</li>
<li>è°ƒä¼˜çš„VGG16æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€é«˜çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-396c47341b4b50ea3595b4791edc5f41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52800f1ae082580adaa242368837b1fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62174c5c6b827e1f0180a77a77668f71" align="middle">
<img src="https://picx.zhimg.com/v2-c94680ffa44f0e047d361a7eee674a0e" align="middle">
<img src="https://picx.zhimg.com/v2-11328c95b49834781c5cbad1270ad5ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-586150a7e4c0692a9f3255e4c2fc0e09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f0fb5e869c02fdba90b39446c566df5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f427669fb35b290cb2c9734b58fc861" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="TREAT-Net-Tabular-Referenced-Echocardiography-Analysis-for-Acute-Coronary-Syndrome-Treatment-Prediction"><a href="#TREAT-Net-Tabular-Referenced-Echocardiography-Analysis-for-Acute-Coronary-Syndrome-Treatment-Prediction" class="headerlink" title="TREAT-Net: Tabular-Referenced Echocardiography Analysis for Acute   Coronary Syndrome Treatment Prediction"></a>TREAT-Net: Tabular-Referenced Echocardiography Analysis for Acute   Coronary Syndrome Treatment Prediction</h2><p><strong>Authors:Diane Kim, Minh Nguyen Nhat To, Sherif Abdalla, Teresa S. M. Tsang, Purang Abolmaesumi, and Christina Luong</strong></p>
<p>Coronary angiography remains the gold standard for diagnosing Acute Coronary Syndrome (ACS). However, its resource-intensive and invasive nature can expose patients to procedural risks and diagnostic delays, leading to postponed treatment initiation. In this work, we introduce TREAT-Net, a multimodal deep learning framework for ACS treatment prediction that leverages non-invasive modalities, including echocardiography videos and structured clinical records. TREAT-Net integrates tabular-guided cross-attention to enhance video interpretation, along with a late fusion mechanism to align predictions across modalities. Trained on a dataset of over 9000 ACS cases, the model outperforms unimodal and non-fused baselines, achieving a balanced accuracy of 67.6% and an AUROC of 71.1%. Cross-modality agreement analysis demonstrates 88.6% accuracy for intervention prediction. These findings highlight the potential of TREAT-Net as a non-invasive tool for timely and accurate patient triage, particularly in underserved populations with limited access to coronary angiography. </p>
<blockquote>
<p>å† çŠ¶åŠ¨è„‰é€ å½±ä»æ˜¯æ€¥æ€§å† çŠ¶åŠ¨è„‰ç»¼åˆå¾ï¼ˆACSï¼‰è¯Šæ–­çš„é‡‘æ ‡å‡†ã€‚ç„¶è€Œï¼Œå…¶èµ„æºå¯†é›†å‹å’Œä¾µå…¥æ€§çš„ç‰¹æ€§å¯èƒ½ä¼šä½¿æ‚£è€…é¢ä¸´æ‰‹æœ¯é£é™©å’Œè¯Šæ–­å»¶è¯¯ï¼Œå¯¼è‡´æ²»ç–—å¯åŠ¨æ¨è¿Ÿã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†TREAT-Netï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºACSæ²»ç–—é¢„æµ‹çš„å¤šæ¨¡å¼æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨éä¾µå…¥æ€§æ¨¡å¼ï¼ŒåŒ…æ‹¬è¶…å£°å¿ƒåŠ¨å›¾è§†é¢‘å’Œç»“æ„åŒ–ä¸´åºŠè®°å½•ã€‚TREAT-Neté›†æˆè¡¨æ ¼å¼•å¯¼äº¤å‰æ³¨æ„ä»¥å¢å¼ºè§†é¢‘è§£é‡Šï¼Œä»¥åŠåæœŸèåˆæœºåˆ¶ä»¥è°ƒæ•´è·¨æ¨¡å¼çš„é¢„æµ‹ã€‚åœ¨è¶…è¿‡9000ä¾‹ACSç—…ä¾‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œå…¶æ€§èƒ½è¶…è¿‡äº†å•æ¨¡æ€å’ŒéèåˆåŸºçº¿ï¼Œå®ç°äº†67.6%çš„å¹³è¡¡å‡†ç¡®ç‡å’Œ71.1%çš„AUROCã€‚è·¨æ¨¡å¼åè®®åˆ†ææ˜¾ç¤ºå¹²é¢„é¢„æµ‹çš„å‡†ç¡®ç‡ä¸º88.6%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†TREAT-Netä½œä¸ºéä¾µå…¥æ€§å·¥å…·åœ¨åŠæ—¶å’Œå‡†ç¡®çš„æ‚£è€…åˆ†æµä¸­çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éš¾ä»¥è·å¾—å† çŠ¶åŠ¨è„‰é€ å½±çš„äººç¾¤ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23999v1">PDF</a> 11 pages, 2 figures, MICCAI ASMUS 2025 paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTREAT-Netçš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ€¥æ€§å† è„‰ç»¼åˆå¾ï¼ˆACSï¼‰æ²»ç–—é¢„æµ‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨éä¾µå…¥æ€§æ–¹å¼ï¼Œå¦‚è¶…å£°å¿ƒåŠ¨å›¾è§†é¢‘å’Œç»“æ„åŒ–ä¸´åºŠè®°å½•ï¼Œè¿›è¡Œé¢„æµ‹ã€‚é€šè¿‡è¡¨æ ¼å¼•å¯¼äº¤å‰æ³¨æ„åŠ›å’Œæ™šæœŸèåˆæœºåˆ¶ï¼Œæ¨¡å‹åœ¨è¶…è¿‡9000ä¾‹ACSç—…ä¾‹çš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œå¹³è¡¡å‡†ç¡®ç‡è¾¾åˆ°äº†67.6%ï¼ŒAUROCä¸º71.1%ã€‚æ­¤å¤–ï¼Œè·¨æ¨¡æ€åè®®åˆ†ææ˜¾ç¤ºå¹²é¢„é¢„æµ‹çš„å‡†ç¡®ç‡ä¸º88.6%ã€‚è¿™äº›å‘ç°è¡¨æ˜TREAT-Netä½œä¸ºéä¾µå…¥æ€§å·¥å…·åœ¨åŠæ—¶å’Œå‡†ç¡®çš„æ‚£è€…åˆ†æµæ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éš¾ä»¥è·å¾—å† çŠ¶åŠ¨è„‰é€ å½±çš„ç¾¤ä½“ä¸­æœ‰å¹¿é˜”åº”ç”¨å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TREAT-Netæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é¢„æµ‹æ€¥æ€§å† è„‰ç»¼åˆå¾ï¼ˆACSï¼‰çš„æ²»ç–—ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨éä¾µå…¥æ€§æ–¹å¼ï¼ˆå¦‚è¶…å£°å¿ƒåŠ¨å›¾è§†é¢‘å’Œç»“æ„åŒ–ä¸´åºŠè®°å½•ï¼‰è¿›è¡Œé¢„æµ‹ï¼Œå‡å°‘æ‚£è€…æ¥å—å† çŠ¶åŠ¨è„‰é€ å½±çš„ç¨‹åºé£é™©å’Œè¯Šæ–­å»¶è¿Ÿã€‚</li>
<li>TREAT-Neté›†æˆäº†è¡¨æ ¼å¼•å¯¼äº¤å‰æ³¨æ„åŠ›æ¥å¢å¼ºè§†é¢‘è§£è¯»ï¼Œå¹¶ä½¿ç”¨æ™šæœŸèåˆæœºåˆ¶æ¥å¯¹é½ä¸åŒæ¨¡æ€çš„é¢„æµ‹ã€‚</li>
<li>åœ¨è¶…è¿‡9000ä¾‹ACSç—…ä¾‹çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„è®­ç»ƒè¡¨æ˜ï¼ŒTREAT-Netçš„æ€§èƒ½è¶…è¶Šäº†å•æ¨¡æ€å’ŒéèåˆåŸºçº¿ã€‚</li>
<li>TREAT-Netçš„å¹³è¡¡å‡†ç¡®ç‡ä¸º67.6%ï¼ŒAUROCä¸º71.1%ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>è·¨æ¨¡æ€åè®®åˆ†ææ˜¾ç¤ºï¼ŒTREAT-Netåœ¨å¹²é¢„é¢„æµ‹æ–¹é¢çš„å‡†ç¡®ç‡ä¸º88.6%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60fb826eb1901dfadacd6f243b9d22c8" align="middle">
<img src="https://picx.zhimg.com/v2-55a30030723db2c7d86b86af9af00855" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="A-Novel-Hybrid-Deep-Learning-and-Chaotic-Dynamics-Approach-for-Thyroid-Cancer-Classification"><a href="#A-Novel-Hybrid-Deep-Learning-and-Chaotic-Dynamics-Approach-for-Thyroid-Cancer-Classification" class="headerlink" title="A Novel Hybrid Deep Learning and Chaotic Dynamics Approach for Thyroid   Cancer Classification"></a>A Novel Hybrid Deep Learning and Chaotic Dynamics Approach for Thyroid   Cancer Classification</h2><p><strong>Authors:Nada Bouchekout, Abdelkrim Boukabou, Morad Grimes, Yassine Habchi, Yassine Himeur, Hamzah Ali Alkhazaleh, Shadi Atalla, Wathiq Mansoor</strong></p>
<p>Timely and accurate diagnosis is crucial in addressing the global rise in thyroid cancer, ensuring effective treatment strategies and improved patient outcomes. We present an intelligent classification method that couples an Adaptive Convolutional Neural Network (CNN) with Cohen-Daubechies-Feauveau (CDF9&#x2F;7) wavelets whose detail coefficients are modulated by an n-scroll chaotic system to enrich discriminative features. We evaluate on the public DDTI thyroid ultrasound dataset (n &#x3D; 1,638 images; 819 malignant &#x2F; 819 benign) using 5-fold cross-validation, where the proposed method attains 98.17% accuracy, 98.76% sensitivity, 97.58% specificity, 97.55% F1-score, and an AUC of 0.9912. A controlled ablation shows that adding chaotic modulation to CDF9&#x2F;7 improves accuracy by +8.79 percentage points over a CDF9&#x2F;7-only CNN (from 89.38% to 98.17%). To objectively position our approach, we trained state-of-the-art backbones on the same data and splits: EfficientNetV2-S (96.58% accuracy; AUC 0.987), Swin-T (96.41%; 0.986), ViT-B&#x2F;16 (95.72%; 0.983), and ConvNeXt-T (96.94%; 0.987). Our method outperforms the best of these by +1.23 points in accuracy and +0.0042 in AUC, while remaining computationally efficient (28.7 ms per image; 1,125 MB peak VRAM). Robustness is further supported by cross-dataset testing on TCIA (accuracy 95.82%) and transfer to an ISIC skin-lesion subset (n &#x3D; 28 unique images, augmented to 2,048; accuracy 97.31%). Explainability analyses (Grad-CAM, SHAP, LIME) highlight clinically relevant regions. Altogether, the wavelet-chaos-CNN pipeline delivers state-of-the-art thyroid ultrasound classification with strong generalization and practical runtime characteristics suitable for clinical integration. </p>
<blockquote>
<p>é¢å¯¹ç”²çŠ¶è…ºç™Œå…¨çƒå‘ç—…ç‡çš„ä¸Šå‡ï¼ŒåŠæ—¶å‡†ç¡®çš„è¯Šæ–­è‡³å…³é‡è¦ï¼Œå®ƒèƒ½ç¡®ä¿æœ‰æ•ˆçš„æ²»ç–—ç­–ç•¥å’Œæ”¹å–„æ‚£è€…é¢„åã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ™ºèƒ½åˆ†ç±»æ–¹æ³•ï¼Œå®ƒå°†è‡ªé€‚åº”å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸Cohen-Daubechies-Feauveauï¼ˆCDF9&#x2F;7ï¼‰å°æ³¢ç›¸ç»“åˆï¼Œå…¶ç»†èŠ‚ç³»æ•°ç”±næ»šåŠ¨æ··æ²Œç³»ç»Ÿè°ƒåˆ¶ï¼Œä»¥ä¸°å¯Œåˆ¤åˆ«ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨å…¬å…±DDTIç”²çŠ¶è…ºè¶…å£°æ•°æ®é›†ï¼ˆn&#x3D;1638å¼ å›¾åƒï¼›å…¶ä¸­æ¶æ€§819å¼ ï¼Œè‰¯æ€§819å¼ ï¼‰ä¸Šè¿›è¡Œäº†äº”æŠ˜äº¤å‰éªŒè¯è¯„ä¼°ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†98.17%çš„å‡†ç¡®ç‡ã€98.76%çš„çµæ•åº¦ã€97.58%çš„ç‰¹å¼‚åº¦ã€97.55%çš„F1å¾—åˆ†å’Œ0.9912çš„AUCå€¼ã€‚å—æ§æ¶ˆèå®éªŒè¡¨æ˜ï¼Œåœ¨CDF9&#x2F;7ä¸­åŠ å…¥æ··æ²Œè°ƒåˆ¶å¯æé«˜å‡†ç¡®ç‡+æé«˜äº†ä¸åªæœ‰CDFçš„CNNç›¸æ¯”å¢åŠ äº†ç™¾åˆ†ä¹‹8.79ä¸ªç‚¹ï¼ˆä»ç™¾åˆ†ä¹‹æé«˜åˆ°ç™¾åˆ†ä¹‹ï¼‰ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†å½“å‰å…ˆè¿›çš„éª¨å¹²ç½‘ç»œåœ¨åŒä¸€æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼šEfficientNetV2-Sï¼ˆå‡†ç¡®ç‡ç™¾åˆ†ä¹‹ï¼›AUCç™¾åˆ†ä¹‹ï¼‰ï¼ŒSwin-Tï¼ˆç™¾åˆ†ä¹‹ï¼›AUCç™¾åˆ†ä¹‹ï¼‰ï¼ŒViT-B&#x2F;16ï¼ˆå‡†ç¡®ç‡ç™¾åˆ†ä¹‹ï¼›AUCç™¾åˆ†ä¹‹ï¼‰ï¼Œä»¥åŠConvNeXt-Tï¼ˆå‡†ç¡®ç‡ç™¾åˆ†ä¹‹ï¼›AUCç™¾åˆ†ä¹‹ï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®ç‡å’ŒAUCæ–¹é¢åˆ†åˆ«ä¼˜äºæœ€ä½³æ¨¡å‹åŠ ä¸Šå‡ ç‚¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•è™½ç„¶è¿ç®—æ•ˆç‡æé«˜ï¼Œæ¯å¼ å›¾ç‰‡ä»…éœ€çº¦è¿›è¡Œä¸€æ­¥çš„è®¡ç®—å¹¶èƒ½åœ¨å‡ ç§’é’Ÿå†…è·å¾—é¢„æµ‹ç»“æœï¼‰ï¼Œä½†ä»å…·æœ‰è‰¯å¥½çš„è®¡ç®—æ€§èƒ½å³°å€¼å†…å­˜ä½¿ç”¨é‡ä¸ºå³°å€¼ï¼‰ï¼Œè¿™ä¸ºä¸´åºŠåº”ç”¨æä¾›äº†è‰¯å¥½çš„è¿è¡Œç‰¹æ€§æ”¯æŒã€‚åœ¨è·¨æ•°æ®é›†æµ‹è¯•ä¸Šè¡¨ç°ç¨³å¥ï¼Œå¦‚åœ¨TCIAä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†ç™¾åˆ†ä¹‹åœ¨çš®è‚¤ç—…å˜å­é›†ä¸Šçš„è½¬ç§»ä¹Ÿå–å¾—äº†è‰¯å¥½çš„è¡¨ç°ï¼ˆå‡†ç¡®ç‡ä¸ºç™¾åˆ†ä¹‹ï¼Œè¯¥å­é›†ç”±å”¯ä¸€çš„çš®è‚¤ç—…å˜å›¾åƒç»„æˆï¼‰ã€‚è§£é‡Šæ€§åˆ†æï¼ˆåŒ…æ‹¬Grad-CAMã€SHAPå’ŒLIMEï¼‰çªå‡ºäº†ä¸´åºŠç›¸å…³çš„åŒºåŸŸã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œå°æ³¢æ··æ²ŒCNNç®¡é“æä¾›äº†å…·æœ‰å¼ºå¤§æ³›åŒ–èƒ½åŠ›çš„ç”²çŠ¶è…ºè¶…å£°åˆ†ç±»ï¼Œå¹¶å…·å¤‡é€‚åˆä¸´åºŠæ•´åˆçš„å®é™…è¿è¡Œç‰¹æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23968v1">PDF</a> Scientific Reports</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ™ºèƒ½åˆ†ç±»æ–¹æ³•ï¼Œç»“åˆè‡ªé€‚åº”å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸CDF9&#x2F;7å°æ³¢å’Œæ··æ²Œç³»ç»Ÿè°ƒåˆ¶ï¼Œä»¥è¯†åˆ«ç”²çŠ¶è…ºç™Œå›¾åƒç‰¹å¾ã€‚åœ¨å…¬å¼€DDTIç”²çŠ¶è…ºè¶…å£°æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å‡†ç¡®æ€§é«˜ï¼Œå…·æœ‰è‰¯å¥½çš„æ•æ„Ÿæ€§å’Œç‰¹å¼‚æ€§ï¼Œä¸”å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ¨¡å‹ï¼Œæœ¬ç ”ç©¶æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢è¡¨ç°æ›´ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè‡ªé€‚åº”å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸CDF9&#x2F;7å°æ³¢ç»“åˆçš„æ–¹æ³•ï¼Œç”¨äºç”²çŠ¶è…ºç™Œå›¾åƒåˆ†ç±»ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ··æ²Œç³»ç»Ÿè°ƒåˆ¶ï¼Œå¢å¼ºäº†ç‰¹å¾çš„åˆ¤åˆ«èƒ½åŠ›ã€‚</li>
<li>åœ¨DDTIç”²çŠ¶è…ºè¶…å£°æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ã€æ•æ„Ÿæ€§å’Œç‰¹å¼‚æ€§ã€‚</li>
<li>ä¸å…¶ä»–å…ˆè¿›æ¨¡å‹ç›¸æ¯”ï¼Œæœ¬ç ”ç©¶æ–¹æ³•åœ¨å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒæ•°æ®é›†ä¸Šå®ç°ç¨³å¥çš„æ€§èƒ½ã€‚</li>
<li>æ–¹æ³•çš„è®¡ç®—æ•ˆç‡é«˜ï¼Œé€‚ç”¨äºä¸´åºŠåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23968">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-beade2e0ca707c8ab6793acbdb6da799" align="middle">
<img src="https://picx.zhimg.com/v2-e69677e0dc5ac44fd9755c58582ba81e" align="middle">
<img src="https://picx.zhimg.com/v2-edb59d9d70b6ca252b286d59f85c6f8b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Adversarial-Versus-Federated-An-Adversarial-Learning-based-Multi-Modality-Cross-Domain-Federated-Medical-Segmentation"><a href="#Adversarial-Versus-Federated-An-Adversarial-Learning-based-Multi-Modality-Cross-Domain-Federated-Medical-Segmentation" class="headerlink" title="Adversarial Versus Federated: An Adversarial Learning based   Multi-Modality Cross-Domain Federated Medical Segmentation"></a>Adversarial Versus Federated: An Adversarial Learning based   Multi-Modality Cross-Domain Federated Medical Segmentation</h2><p><strong>Authors:You Zhou, Lijiang Chen, Shuchang Lyu, Guangxia Cui, Wenpei Bai, Zheng Zhou, Meng Li, Guangliang Cheng, Huiyu Zhou, Qi Zhao</strong></p>
<p>Federated learning enables collaborative training of machine learning models among different clients while ensuring data privacy, emerging as the mainstream for breaking data silos in the healthcare domain. However, the imbalance of medical resources, data corruption or improper data preservation may lead to a situation where different clients possess medical images of different modality. This heterogeneity poses a significant challenge for cross-domain medical image segmentation within the federated learning framework. To address this challenge, we propose a new Federated Domain Adaptation (FedDA) segmentation training framework. Specifically, we propose a feature-level adversarial learning among clients by aligning feature maps across clients through embedding an adversarial training mechanism. This design can enhance the modelâ€™s generalization on multiple domains and alleviate the negative impact from domain-shift. Comprehensive experiments on three medical image datasets demonstrate that our proposed FedDA substantially achieves cross-domain federated aggregation, endowing single modality client with cross-modality processing capabilities, and consistently delivers robust performance compared to state-of-the-art federated aggregation algorithms in objective and subjective assessment. Our code are available at <a target="_blank" rel="noopener" href="https://github.com/GGbond-study/FedDA">https://github.com/GGbond-study/FedDA</a>. </p>
<blockquote>
<p>è”é‚¦å­¦ä¹ èƒ½å¤Ÿåœ¨ä¸åŒå®¢æˆ·ç«¯ä¹‹é—´è¿›è¡Œæœºå™¨å­¦ä¹ æ¨¡å‹çš„ååŒè®­ç»ƒï¼ŒåŒæ—¶ç¡®ä¿æ•°æ®éšç§ï¼Œæˆä¸ºåŒ»ç–—ä¿å¥é¢†åŸŸä¸­æ‰“ç ´æ•°æ®å­¤å²›çš„ä¸»æµæ–¹æ³•ã€‚ç„¶è€Œï¼ŒåŒ»ç–—èµ„æºçš„ä¸å¹³è¡¡ã€æ•°æ®æŸåæˆ–ä¸å½“çš„æ•°æ®ä¿å­˜å¯èƒ½å¯¼è‡´ä¸åŒå®¢æˆ·ç«¯æ‹¥æœ‰ä¸åŒæ¨¡æ€çš„åŒ»ç–—å›¾åƒã€‚è¿™ç§å¼‚è´¨æ€§ç»™è”é‚¦å­¦ä¹ æ¡†æ¶å†…çš„è·¨åŸŸåŒ»å­¦å›¾åƒåˆ†å‰²å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è”é‚¦åŸŸé€‚åº”ï¼ˆFedDAï¼‰åˆ†å‰²è®­ç»ƒæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡åµŒå…¥å¯¹æŠ—è®­ç»ƒæœºåˆ¶ï¼Œåœ¨å®¢æˆ·ç«¯ä¹‹é—´å®ç°ç‰¹å¾çº§åˆ«çš„å¯¹æŠ—æ€§å­¦ä¹ ï¼Œé€šè¿‡å¯¹é½å®¢æˆ·ç«¯ä¹‹é—´çš„ç‰¹å¾å›¾ã€‚è¿™ç§è®¾è®¡å¯ä»¥å¢å¼ºæ¨¡å‹åœ¨å¤šåŸŸä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡è½»åŸŸåç§»çš„è´Ÿé¢å½±å“ã€‚åœ¨ä¸‰ä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„FedDAå®è´¨åœ°å®ç°äº†è·¨åŸŸè”é‚¦èšåˆï¼Œèµ‹äºˆå•æ¨¡æ€å®¢æˆ·ç«¯è·¨æ¨¡æ€å¤„ç†èƒ½åŠ›ï¼Œä¸æœ€æ–°çš„è”é‚¦èšåˆç®—æ³•ç›¸æ¯”ï¼Œåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­å‡è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/GGbond-study/FedDA%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/GGbond-study/FedDAè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23907v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è”é‚¦å­¦ä¹ å®ç°äº†ä¸åŒå®¢æˆ·ç«¯é—´çš„æœºå™¨å­¦ä¹ æ¨¡å‹ååŒè®­ç»ƒï¼ŒåŒæ—¶ä¿è¯äº†æ•°æ®éšç§ï¼Œå·²æˆä¸ºæ‰“ç ´åŒ»ç–—é¢†åŸŸæ•°æ®å­¤å²›çš„ä¸»æµæ–¹æ³•ã€‚ç„¶è€Œï¼ŒåŒ»ç–—èµ„æºçš„ä¸å¹³è¡¡ã€æ•°æ®æ±¡æŸ“æˆ–æ•°æ®ä¿å­˜ä¸å½“å¯èƒ½å¯¼è‡´ä¸åŒå®¢æˆ·ç«¯æ‹¥æœ‰ä¸åŒæ¨¡æ€çš„åŒ»ç–—å›¾åƒï¼Œç»™è”é‚¦å­¦ä¹ æ¡†æ¶ä¸‹çš„è·¨åŸŸåŒ»å­¦å›¾åƒåˆ†å‰²å¸¦æ¥é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨æ–°çš„è”é‚¦åŸŸé€‚åº”ï¼ˆFedDAï¼‰åˆ†å‰²è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡åœ¨å®¢æˆ·ç«¯é—´è¿›è¡Œç‰¹å¾çº§å¯¹æŠ—æ€§å­¦ä¹ ï¼Œé€šè¿‡å¯¹é½ç‰¹å¾æ˜ å°„åµŒå…¥å¯¹æŠ—è®­ç»ƒæœºåˆ¶ï¼Œå¢å¼ºæ¨¡å‹åœ¨å¤šåŸŸä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡è½»åŸŸåç§»çš„è´Ÿé¢å½±å“ã€‚åœ¨ä¸‰ä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„FedDAå®ç°äº†è·¨åŸŸè”é‚¦èšåˆï¼Œèµ‹äºˆå•æ¨¡æ€å®¢æˆ·ç«¯è·¨æ¨¡æ€å¤„ç†èƒ½åŠ›ï¼Œä¸æœ€å…ˆè¿›çš„è”é‚¦èšåˆç®—æ³•ç›¸æ¯”ï¼Œåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­è¡¨ç°ç¨³å¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ å·²æˆä¸ºåŒ»ç–—é¢†åŸŸæ‰“ç ´æ•°æ®å­¤å²›çš„ä¸»æµæ–¹æ³•ï¼Œèƒ½å¤ŸååŒè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹å¹¶ä¿è¯æ•°æ®éšç§ã€‚</li>
<li>ä¸åŒå®¢æˆ·ç«¯åŒ»ç–—èµ„æºçš„å·®å¼‚å¯èƒ½å¯¼è‡´æ‹¥æœ‰ä¸åŒæ¨¡æ€çš„åŒ»ç–—å›¾åƒï¼Œç»™åŒ»å­¦å›¾åƒåˆ†å‰²å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„è”é‚¦åŸŸé€‚åº”ï¼ˆFedDAï¼‰åˆ†å‰²è®­ç»ƒæ¡†æ¶é€šè¿‡ç‰¹å¾çº§å¯¹æŠ—æ€§å­¦ä¹ å¢å¼ºæ¨¡å‹åœ¨å¤šåŸŸä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>FedDAå®ç°äº†è·¨åŸŸè”é‚¦èšåˆï¼Œå•æ¨¡æ€å®¢æˆ·ç«¯å…·å¤‡è·¨æ¨¡æ€å¤„ç†èƒ½åŠ›ã€‚</li>
<li>FedDAåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­è¡¨ç°ç¨³å¥ï¼Œç›¸è¾ƒäºå…¶ä»–è”é‚¦èšåˆç®—æ³•æœ‰ä¼˜åŠ¿ã€‚</li>
<li>è”é‚¦åŸŸé€‚åº”æ–¹æ³•çš„å®ç°ç»†èŠ‚å’Œæ•ˆæœé€šè¿‡ä¸‰ä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†çš„ç»¼åˆå®éªŒå¾—åˆ°éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23907">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-67cfa0825ff56fd1ee38c23fe2263dad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0744415379e35fb2443248468a2422d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-974039a42cdad3469790ccb2ec615e53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d2845b4615245fc3bd1aac240314726" align="middle">
<img src="https://picx.zhimg.com/v2-56c528076903949700300dc5cddeb8b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f3d22ad1624635e3f15b1fe39e01cd3" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6999087191f6f9a62755b349dfb59dbe.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  MGM-Omni Scaling Omni LLMs to Personalized Long-Horizon Speech
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-abde5572acf07c88dadee9440fb890ee.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  Deep Learning for Oral Health Benchmarking ViT, DeiT, BEiT, ConvNeXt,   and Swin Transformer
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32271.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
