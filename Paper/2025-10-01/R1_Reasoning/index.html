<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  ReasoningBank Scaling Agent Self-Evolving with Reasoning Memory">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-b33639e52b284510aea547ef9808e164~resize:0:q75.jpg?source=1f5c5e47&expiration=1759911873&auth_key=1759911873-0-0-c541a43d6e5ba453e732c75cf1f79989&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-01-æ›´æ–°"><a href="#2025-10-01-æ›´æ–°" class="headerlink" title="2025-10-01 æ›´æ–°"></a>2025-10-01 æ›´æ–°</h1><h2 id="ReasoningBank-Scaling-Agent-Self-Evolving-with-Reasoning-Memory"><a href="#ReasoningBank-Scaling-Agent-Self-Evolving-with-Reasoning-Memory" class="headerlink" title="ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory"></a>ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory</h2><p><strong>Authors:Siru Ouyang, Jun Yan, I-Hung Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long T. Le, Samira Daruki, Xiangru Tang, Vishy Tirumalashetty, George Lee, Mahsan Rofouei, Hangfei Lin, Jiawei Han, Chen-Yu Lee, Tomas Pfister</strong></p>
<p>With the growing adoption of large language model agents in persistent real-world roles, they naturally encounter continuous streams of tasks. A key limitation, however, is their failure to learn from the accumulated interaction history, forcing them to discard valuable insights and repeat past errors. We propose ReasoningBank, a novel memory framework that distills generalizable reasoning strategies from an agentâ€™s self-judged successful and failed experiences. At test time, an agent retrieves relevant memories from ReasoningBank to inform its interaction and then integrates new learnings back, enabling it to become more capable over time. Building on this powerful experience learner, we further introduce memory-aware test-time scaling (MaTTS), which accelerates and diversifies this learning process by scaling up the agentâ€™s interaction experience. By allocating more compute to each task, the agent generates abundant, diverse experiences that provide rich contrastive signals for synthesizing higher-quality memory. The better memory in turn guides more effective scaling, establishing a powerful synergy between memory and test-time scaling. Across web browsing and software engineering benchmarks, ReasoningBank consistently outperforms existing memory mechanisms that store raw trajectories or only successful task routines, improving both effectiveness and efficiency; MaTTS further amplifies these gains. These findings establish memory-driven experience scaling as a new scaling dimension, enabling agents to self-evolve with emergent behaviors naturally arise. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨æŒä¹…çœŸå®ä¸–ç•Œè§’è‰²ä¸­çš„è¶Šæ¥è¶Šæ™®éçš„é‡‡ç”¨ï¼Œå®ƒä»¬è‡ªç„¶ä¼šé‡åˆ°è¿ç»­çš„ä»»åŠ¡æµã€‚ç„¶è€Œï¼Œä¸€ä¸ªå…³é”®çš„å±€é™æ€§åœ¨äºå®ƒä»¬æ— æ³•ä»ç´¯ç§¯çš„äº¤äº’å†å²ä¸­å­¦ä¹ ï¼Œè¿«ä½¿å®ƒä»¬æ”¾å¼ƒæœ‰ä»·å€¼çš„è§è§£å¹¶é‡å¤è¿‡å»çš„é”™è¯¯ã€‚æˆ‘ä»¬æå‡ºäº†ReasoningBankï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è®°å¿†æ¡†æ¶ï¼Œå¯ä»¥ä»ä»£ç†çš„è‡ªæˆ‘åˆ¤æ–­çš„æˆåŠŸå’Œå¤±è´¥ç»éªŒä¸­æç‚¼å‡ºå¯æ¨å¹¿çš„æ¨ç†ç­–ç•¥ã€‚åœ¨æµ‹è¯•æ—¶ï¼Œä»£ç†ä¼šä»ReasoningBankä¸­æ£€ç´¢ç›¸å…³è®°å¿†ä»¥æŒ‡å¯¼å…¶äº¤äº’ï¼Œç„¶åå°†æ–°çš„å­¦ä¹ æ•´åˆå›æ¥ï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œä½¿å…¶èƒ½åŠ›å˜å¾—æ›´å¼ºã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†è®°å¿†æ„ŸçŸ¥æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆMaTTSï¼‰ï¼Œé€šè¿‡æ‰©å¤§ä»£ç†çš„äº¤äº’ç»éªŒæ¥åŠ é€Ÿå’Œå¤šæ ·åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚é€šè¿‡ä¸ºæ¯ä¸ªä»»åŠ¡åˆ†é…æ›´å¤šçš„è®¡ç®—èµ„æºï¼Œä»£ç†ç”Ÿæˆä¸°å¯Œä¸”å¤šæ ·çš„ç»éªŒï¼Œä¸ºåˆæˆé«˜è´¨é‡è®°å¿†æä¾›äº†ä¸°å¯Œçš„å¯¹æ¯”ä¿¡å·ã€‚æ›´å¥½çš„è®°å¿†åè¿‡æ¥åˆæŒ‡å¯¼æ›´æœ‰æ•ˆçš„ç¼©æ”¾ï¼Œåœ¨è®°å¿†å’Œæµ‹è¯•æ—¶ç¼©æ”¾ä¹‹é—´å»ºç«‹äº†å¼ºå¤§çš„ååŒä½œç”¨ã€‚åœ¨ç½‘é¡µæµè§ˆå’Œè½¯ä»¶å·¥ç¨‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasoningBankå§‹ç»ˆä¼˜äºç°æœ‰çš„å­˜å‚¨åŸå§‹è½¨è¿¹æˆ–ä»…å­˜å‚¨æˆåŠŸä»»åŠ¡ä¾‹ç¨‹çš„è®°å¿†æœºåˆ¶ï¼Œæé«˜äº†æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼›MaTTSè¿›ä¸€æ­¥æ‰©å¤§äº†è¿™äº›æ”¶ç›Šã€‚è¿™äº›å‘ç°å°†è®°å¿†é©±åŠ¨çš„ç»éªŒç¼©æ”¾ç¡®ç«‹ä¸ºä¸€ç§æ–°çš„ç¼©æ”¾ç»´åº¦ï¼Œä½¿ä»£ç†èƒ½å¤Ÿè‡ªç„¶åœ°å‡ºç°è‡ªæˆ‘è¿›åŒ–è¡Œä¸ºå’Œæ–°å…´è¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25140v1">PDF</a> 11 pages, 7 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¿ç»­ä»»åŠ¡æ—¶å­˜åœ¨æ— æ³•å­¦ä¹ å†å²äº¤äº’ä¿¡æ¯çš„å±€é™ï¼Œå¯¼è‡´æ— æ³•ç§¯ç´¯æœ‰ä»·å€¼çš„çŸ¥è¯†å¹¶é‡å¤è¿‡å»çš„é”™è¯¯ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ReasoningBankè¿™ä¸€æ–°å‹è®°å¿†æ¡†æ¶ï¼Œå¯ä»¥ä»æ™ºèƒ½ä½“çš„è‡ªæˆ‘åˆ¤æ–­çš„æˆåŠŸå’Œå¤±è´¥ç»éªŒä¸­æç‚¼å‡ºå¯æ¨å¹¿çš„æ¨ç†ç­–ç•¥ã€‚åœ¨æµ‹è¯•é˜¶æ®µï¼Œæ™ºèƒ½ä½“å¯ä»¥ä»ReasoningBankä¸­æ£€ç´¢ç›¸å…³è®°å¿†æ¥æŒ‡å¯¼å…¶äº¤äº’ï¼Œå¹¶å°†æ–°çš„å­¦ä¹ æ•´åˆå›è®°å¿†åº“ï¼Œä½¿å…¶éšæ—¶é—´å˜å¾—æ›´åŠ æ™ºèƒ½ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†åŸºäºè®°å¿†çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆMaTTSï¼‰ï¼Œå®ƒé€šè¿‡æ‰©å¤§æ™ºèƒ½ä½“äº¤äº’ç»éªŒçš„è§„æ¨¡æ¥åŠ é€Ÿå’Œå¤šæ ·åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚ReasoningBankä¸ç°æœ‰çš„å­˜å‚¨åŸå§‹è½¨è¿¹æˆ–ä»…å­˜å‚¨æˆåŠŸä»»åŠ¡ä¾‹ç¨‹çš„è®°å¿†æœºåˆ¶ç›¸æ¯”ï¼Œåœ¨æé«˜æ•ˆæœå’Œæ•ˆç‡æ–¹é¢è¡¨ç°æ›´ä¸ºä¼˜è¶Šï¼›MaTTSè¿›ä¸€æ­¥æ”¾å¤§äº†è¿™äº›ä¼˜åŠ¿ã€‚è®°å¿†é©±åŠ¨çš„ç»éªŒæ‰©å±•å·²æˆä¸ºä¸€ç§æ–°çš„æ‰©å±•ç»´åº¦ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªç„¶åœ°è‡ªæˆ‘è¿›åŒ–å¹¶å‡ºç°æ–°å…´è¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¿ç»­ä»»åŠ¡æ—¶é¢ä¸´æ— æ³•å­¦ä¹ å†å²äº¤äº’ä¿¡æ¯çš„æŒ‘æˆ˜ã€‚</li>
<li>ReasoningBankèƒ½å¤Ÿä»æ™ºèƒ½ä½“çš„è‡ªæˆ‘åˆ¤æ–­çš„æˆåŠŸå’Œå¤±è´¥ç»éªŒä¸­æç‚¼å‡ºå¯æ¨å¹¿çš„æ¨ç†ç­–ç•¥ã€‚</li>
<li>æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨æµ‹è¯•é˜¶æ®µä»ReasoningBankä¸­æ£€ç´¢ç›¸å…³è®°å¿†æ¥æŒ‡å¯¼å…¶äº¤äº’ã€‚</li>
<li>Memory-aware test-time scalingï¼ˆMaTTSï¼‰é€šè¿‡æ‰©å¤§æ™ºèƒ½ä½“çš„äº¤äº’ç»éªŒè§„æ¨¡æ¥åŠ é€Ÿå’Œå¤šæ ·åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>ReasoningBankç›¸è¾ƒäºç°æœ‰è®°å¿†æœºåˆ¶åœ¨æé«˜æ•ˆæœå’Œæ•ˆç‡æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>MaTTSè¿›ä¸€æ­¥æ”¾å¤§äº†ReasoningBankçš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9b5822c3618f30888cb9dd3e49523c05~resize:0:q75.jpg?source=1f5c5e47&expiration=1759911882&auth_key=1759911882-0-0-f18b623e75dcc45d872af566c846c3b0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-7f36c00262851436973ecd9091438381.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-4182b52b88c0ba5ae5622677aa6f29bb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759911896&auth_key=1759911896-0-0-cbc1ae245e2d896616ee39c334dae455&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-951df67fba439596ea0b63c86ce35062~resize:0:q75.jpg?source=1f5c5e47&expiration=1759911902&auth_key=1759911902-0-0-ec0b2c3d6b46f27a672902ba033cc9f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4edc7e1de1d3af0c305e58928a69efcb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759911909&auth_key=1759911909-0-0-74876be4af540398362e1d301fbaa6d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Cogito-Ergo-Ludo-An-Agent-that-Learns-to-Play-by-Reasoning-and-Planning"><a href="#Cogito-Ergo-Ludo-An-Agent-that-Learns-to-Play-by-Reasoning-and-Planning" class="headerlink" title="Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and   Planning"></a>Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and   Planning</h2><p><strong>Authors:Sai Wang, Yu Wu, Zhongwen Xu</strong></p>
<p>The pursuit of artificial agents that can learn to master complex environments has led to remarkable successes, yet prevailing deep reinforcement learning methods often rely on immense experience, encoding their knowledge opaquely within neural network weights. We propose a different paradigm, one in which an agent learns to play by reasoning and planning. We introduce Cogito, ergo ludo (CEL), a novel agent architecture that leverages a Large Language Model (LLM) to build an explicit, language-based understanding of its environmentâ€™s mechanics and its own strategy. Starting from a tabula rasa state with no prior knowledge (except action set), CEL operates on a cycle of interaction and reflection. After each episode, the agent analyzes its complete trajectory to perform two concurrent learning processes: Rule Induction, where it refines its explicit model of the environmentâ€™s dynamics, and Strategy and Playbook Summarization, where it distills experiences into an actionable strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e., Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent successfully learns to master these games by autonomously discovering their rules and developing effective policies from sparse rewards. Ablation studies confirm that the iterative process is critical for sustained learning. Our work demonstrates a path toward more general and interpretable agents that not only act effectively but also build a transparent and improving model of their world through explicit reasoning on raw experience. </p>
<blockquote>
<p>è¿½æ±‚èƒ½å¤Ÿè®©äººå·¥ä»£ç†å­¦ä¹ æŒæ¡å¤æ‚ç¯å¢ƒçš„ç›®æ ‡å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚ç„¶è€Œï¼Œå½“å‰æµè¡Œçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤§é‡çš„ç»éªŒï¼Œå°†çŸ¥è¯†ä»¥ä¸é€æ˜çš„æ–¹å¼ç¼–ç åœ¨ç¥ç»ç½‘ç»œæƒé‡ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸åŒçš„æ¨¡å¼ï¼Œå³ä»£ç†é€šè¿‡æ¨ç†å’Œè§„åˆ’æ¥å­¦ä¹ ç©è€ã€‚æˆ‘ä»¬å¼•å…¥äº†Cogitoï¼Œergo ludoï¼ˆCELï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ä»£ç†æ¶æ„ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å»ºç«‹åŸºäºè¯­è¨€çš„å¯¹å…¶ç¯å¢ƒæœºåˆ¶å’Œè‡ªèº«ç­–ç•¥çš„æ˜ç¡®ç†è§£ã€‚ä»æ— çŸ¥çš„çŠ¶æ€å¼€å§‹ï¼ˆæ²¡æœ‰ä»»ä½•å…ˆéªŒçŸ¥è¯†ï¼Œåªæœ‰åŠ¨ä½œé›†ï¼‰ï¼ŒCELåœ¨äº¤äº’å’Œåæ€çš„å¾ªç¯ä¸­è¿è¡Œã€‚åœ¨æ¯ä¸€é›†ä¹‹åï¼Œä»£ç†åˆ†æå…¶å®Œæ•´çš„è½¨è¿¹ï¼Œä»¥æ‰§è¡Œä¸¤ä¸ªå¹¶è¡Œå­¦ä¹ è¿‡ç¨‹ï¼šè§„åˆ™å½’çº³ï¼Œå®ƒå®Œå–„å…¶å¯¹ç¯å¢ƒåŠ¨æ€çš„æ˜ç¡®æ¨¡å‹ï¼›ç­–ç•¥å’Œæ‹›å¼æ€»ç»“ï¼Œå®ƒå°†ç»éªŒæç‚¼æˆå¯æ“ä½œçš„ç­–ç•¥é›†ã€‚æˆ‘ä»¬åœ¨å¤šæ ·åŒ–çš„ç½‘æ ¼ä¸–ç•Œä»»åŠ¡ï¼ˆå¦‚æ‰«é›·ã€å†°å†»æ¹–å’Œç´¢åšå®‰æ¸¸æˆï¼‰ä¸­å¯¹CELè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶è¯æ˜CELä»£ç†èƒ½å¤ŸæˆåŠŸå­¦ä¹ æŒæ¡è¿™äº›æ¸¸æˆï¼Œé€šè¿‡è‡ªä¸»å‘ç°æ¸¸æˆè§„åˆ™å¹¶ä»ç¨€ç–å¥–åŠ±ä¸­åˆ¶å®šæœ‰æ•ˆç­–ç•¥ã€‚åºŸé™¤ç ”ç©¶è¯å®è¿­ä»£è¿‡ç¨‹å¯¹äºæŒç»­å­¦ä¹ è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†ä¸€æ¡é€šå¾€æ›´é€šç”¨å’Œå¯è§£é‡Šçš„ä»£ç†çš„é“è·¯ï¼Œè¿™äº›ä»£ç†ä¸ä»…èƒ½å¤Ÿæœ‰æ•ˆåœ°è¡ŒåŠ¨ï¼Œè€Œä¸”èƒ½å¤Ÿé€šè¿‡åŸå§‹ç»éªŒè¿›è¡Œæ˜ç¡®çš„æ¨ç†æ¥å»ºç«‹å…¶ä¸–ç•Œçš„é€æ˜å’Œæ”¹è¿›æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25052v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ™ºèƒ½ä½“æ¶æ„Cogitoï¼Œergo ludoï¼ˆCELï¼‰ï¼Œè¯¥æ¶æ„åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å»ºç«‹ç¯å¢ƒæœºåˆ¶å’Œè‡ªèº«ç­–ç•¥æ˜ç¡®çš„è¯­è¨€æè¿°ç†è§£ã€‚CELæ™ºèƒ½ä½“åœ¨æ¯æ¬¡äº¤äº’ååˆ†æè‡ªèº«è¡ŒåŠ¨è½¨è¿¹ï¼ŒåŒæ—¶è¿›è¡Œè§„åˆ™å½’çº³å’Œç­–ç•¥åŠè¡ŒåŠ¨æŒ‡å—æ€»ç»“ä¸¤ç§å­¦ä¹ è¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼ŒCELæ™ºèƒ½ä½“åœ¨ç½‘æ ¼ä¸–ç•Œä»»åŠ¡ä¸­èƒ½å¤Ÿè‡ªä¸»å‘ç°è§„åˆ™å¹¶åˆ¶å®šæœ‰æ•ˆç­–ç•¥ï¼Œä»ç¨€ç–å¥–åŠ±ä¸­å­¦ä¹ æŒæ¡ä»»åŠ¡ã€‚è¿­ä»£è¿‡ç¨‹å¯¹äºæŒç»­å­¦ä¹ è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶ä¸ºé€šç”¨å’Œå¯è§£é‡Šçš„æ™ºèƒ½ä½“çš„å‘å±•é“ºå¹³äº†é“è·¯ï¼Œè¿™äº›æ™ºèƒ½ä½“ä¸ä»…èƒ½å¤Ÿæœ‰æ•ˆåœ°è¡ŒåŠ¨ï¼Œè€Œä¸”èƒ½å¤Ÿé€šè¿‡åŸå§‹ç»éªŒè¿›è¡Œæ˜ç¡®çš„æ¨ç†æ¥å»ºç«‹ä¸æ–­æ”¹å–„çš„ä¸–ç•Œæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°å‹æ™ºèƒ½ä½“æ¶æ„Cogitoï¼Œergo ludoï¼ˆCELï¼‰è¢«æå‡ºï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç¯å¢ƒç†è§£ã€‚</li>
<li>CELæ™ºèƒ½ä½“å…·å¤‡æ˜ç¡®çš„è¯­è¨€æè¿°ç†è§£æœºåˆ¶ï¼Œå¯ä»¥åœ¨æ¯æ¬¡äº¤äº’åè¿›è¡Œè§„åˆ™å½’çº³å’Œç­–ç•¥æ€»ç»“ã€‚</li>
<li>CELæ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨ç½‘æ ¼ä¸–ç•Œä»»åŠ¡ä¸­è‡ªä¸»å‘ç°è§„åˆ™å¹¶åˆ¶å®šæœ‰æ•ˆç­–ç•¥ã€‚</li>
<li>ç¨€ç–å¥–åŠ±ä¸‹ï¼ŒCELæ™ºèƒ½ä½“èƒ½å¤Ÿå­¦ä¹ å¹¶æŒæ¡ä»»åŠ¡ã€‚</li>
<li>è¿­ä»£è¿‡ç¨‹å¯¹äºæ™ºèƒ½ä½“çš„æŒç»­å­¦ä¹ è‡³å…³é‡è¦ã€‚</li>
<li>CELæ™ºèƒ½ä½“çš„æ¶æ„ä¸ºé€šç”¨å’Œå¯è§£é‡Šçš„æ™ºèƒ½ä½“å‘å±•é“ºå¹³äº†é“è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3d008d1696b1bce43bd81c973222de2a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759911917&auth_key=1759911917-0-0-d553476ef187b40390c767727fb2a072&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a95de162af4462ba10c9a96c5a399f9c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759911924&auth_key=1759911924-0-0-e6fdd4ec676dd8a8a143deac5b5d64e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bcec42336d9d83e03ee1e3fd16d6b0cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759911930&auth_key=1759911930-0-0-b667645b80ab90e846cb7a9e231c5bb6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Advantage-Weighted-Matching-Aligning-RL-with-Pretraining-in-Diffusion-Models"><a href="#Advantage-Weighted-Matching-Aligning-RL-with-Pretraining-in-Diffusion-Models" class="headerlink" title="Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion   Models"></a>Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion   Models</h2><p><strong>Authors:Shuchen Xue, Chongjian Ge, Shilong Zhang, Yichen Li, Zhi-Ming Ma</strong></p>
<p>Reinforcement Learning (RL) has emerged as a central paradigm for advancing Large Language Models (LLMs), where pre-training and RL post-training share the same log-likelihood formulation. In contrast, recent RL approaches for diffusion models, most notably Denoising Diffusion Policy Optimization (DDPO), optimize an objective different from the pretraining objectivesâ€“score&#x2F;flow matching loss. In this work, we establish a novel theoretical analysis: DDPO is an implicit form of score&#x2F;flow matching with noisy targets, which increases variance and slows convergence. Building on this analysis, we introduce \textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for diffusion. It uses the same score&#x2F;flow-matching loss as pretraining to obtain a lower-variance objective and reweights each sample by its advantage. In effect, AWM raises the influence of high-reward samples and suppresses low-reward ones while keeping the modeling objective identical to pretraining. This unifies pretraining and RL conceptually and practically, is consistent with policy-gradient theory, reduces variance, and yields faster convergence. This simple yet effective design yields substantial benefits: on GenEval, OCR, and PickScore benchmarks, AWM delivers up to a $24\times$ speedup over Flow-GRPO (which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX, without compromising generation quality. Code is available at <a target="_blank" rel="noopener" href="https://github.com/scxue/advantage_weighted_matching">https://github.com/scxue/advantage_weighted_matching</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²ç»æˆä¸ºæ¨è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒèŒƒå¼ï¼Œå…¶ä¸­é¢„è®­ç»ƒå’ŒRLåè®­ç»ƒé‡‡ç”¨ç›¸åŒçš„å¯¹æ•°ä¼¼ç„¶å…¬å¼ã€‚ä¸æ­¤ç›¸åï¼Œæœ€è¿‘çš„æ‰©æ•£æ¨¡å‹ä¸­çš„RLæ–¹æ³•ï¼Œå°¤å…¶æ˜¯é™å™ªæ‰©æ•£ç­–ç•¥ä¼˜åŒ–ï¼ˆDDPOï¼‰ï¼Œä¼˜åŒ–äº†ä¸€ä¸ªä¸é¢„è®­ç»ƒç›®æ ‡ä¸åŒçš„ç›®æ ‡â€”â€”è¯„åˆ†&#x2F;æµåŒ¹é…æŸå¤±ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†æ–°çš„ç†è®ºåˆ†æï¼šDDPOæ˜¯ä¸€ç§å¸¦æœ‰å™ªå£°ç›®æ ‡çš„è¯„åˆ†&#x2F;æµåŒ¹é…çš„éšå¼å½¢å¼ï¼Œè¿™å¢åŠ äº†æ–¹å·®å¹¶å‡ç¼“äº†æ”¶æ•›é€Ÿåº¦ã€‚åŸºäºè¿™ä¸€åˆ†æï¼Œæˆ‘ä»¬ä»‹ç»äº†é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„<strong>ä¼˜åŠ¿åŠ æƒåŒ¹é…ï¼ˆAWMï¼‰</strong>ï¼Œè¿™æ˜¯ä¸€ç§ç­–ç•¥æ¢¯åº¦æ–¹æ³•ã€‚å®ƒä½¿ç”¨ä¸é¢„è®­ç»ƒç›¸åŒçš„è¯„åˆ†&#x2F;æµåŒ¹é…æŸå¤±æ¥è·å¾—ä½æ–¹å·®ç›®æ ‡ï¼Œå¹¶æŒ‰ä¼˜åŠ¿å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡ŒåŠ æƒã€‚å®é™…ä¸Šï¼ŒAWMæé«˜äº†é«˜å¥–åŠ±æ ·æœ¬çš„å½±å“ï¼ŒæŠ‘åˆ¶äº†ä½å¥–åŠ±æ ·æœ¬ï¼ŒåŒæ—¶ä¿æŒå»ºæ¨¡ç›®æ ‡ä¸é¢„è®­ç»ƒç›¸åŒã€‚è¿™åœ¨æ¦‚å¿µå’Œå®è·µä¸Šç»Ÿä¸€äº†é¢„è®­ç»ƒå’ŒRLï¼Œç¬¦åˆç­–ç•¥æ¢¯åº¦ç†è®ºï¼Œé™ä½äº†æ–¹å·®ï¼Œå®ç°äº†æ›´å¿«çš„æ”¶æ•›ã€‚è¿™ç§ç®€å•è€Œæœ‰æ•ˆçš„è®¾è®¡å¸¦æ¥äº†å®è´¨æ€§çš„å¥½å¤„ï¼šåœ¨GenEvalã€OCRå’ŒPickScoreåŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸åŸºäºDDPOçš„Flow-GRPOç›¸æ¯”ï¼ŒAWMåœ¨åº”ç”¨äºStable Diffusion 3.5 Mediumå’ŒFLUXæ—¶ï¼Œå®ç°äº†é«˜è¾¾24å€çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¸æŸå®³ç”Ÿæˆè´¨é‡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/scxue/advantage_weighted_matching%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/scxue/advantage_weighted_matchingæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25050v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºæ¨è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒèŒƒå¼ã€‚åœ¨LLMçš„é¢„è®­ç»ƒå’ŒRLåè®­ç»ƒä¸­ï¼Œå®ƒä»¬é‡‡ç”¨ç›¸åŒçš„å¯¹æ•°ä¼¼ç„¶å…¬å¼ã€‚ç„¶è€Œï¼Œå¯¹äºæ‰©æ•£æ¨¡å‹ï¼Œæœ€è¿‘çš„RLæ–¹æ³•å¦‚å»å™ªæ‰©æ•£ç­–ç•¥ä¼˜åŒ–ï¼ˆDDPOï¼‰ä¼˜åŒ–çš„æ˜¯ä¸é¢„è®­ç»ƒç›®æ ‡ä¸åŒçš„åˆ†æ•°&#x2F;æµåŒ¹é…æŸå¤±ã€‚æœ¬æ–‡å»ºç«‹äº†DDPOæ˜¯å¸¦æœ‰å™ªå£°ç›®æ ‡çš„éšå¼åˆ†æ•°&#x2F;æµåŒ¹é…çš„ç†è®ºåˆ†æï¼Œè¿™å¢åŠ äº†æ–¹å·®å¹¶å‡ç¼“äº†æ”¶æ•›é€Ÿåº¦ã€‚åŸºäºæ­¤åˆ†æï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¼˜åŠ¿åŠ æƒåŒ¹é…ï¼ˆAWMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ‰©æ•£æ¨¡å‹çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ã€‚å®ƒé‡‡ç”¨ä¸é¢„è®­ç»ƒç›¸åŒçš„åˆ†æ•°&#x2F;æµåŒ¹é…æŸå¤±æ¥è·å¾—ä½æ–¹å·®ç›®æ ‡ï¼Œå¹¶æ ¹æ®æ¯ä¸ªæ ·æœ¬çš„ä¼˜åŠ¿å¯¹å…¶è¿›è¡ŒåŠ æƒã€‚è¿™æ ·ï¼ŒAWMæé«˜äº†é«˜å›æŠ¥æ ·æœ¬çš„å½±å“ï¼Œå¹¶æŠ‘åˆ¶äº†ä½å›æŠ¥æ ·æœ¬çš„å½±å“ï¼ŒåŒæ—¶ä¿æŒå»ºæ¨¡ç›®æ ‡ä¸é¢„è®­ç»ƒç›¸åŒã€‚åœ¨GenEvalã€OCRå’ŒPickScoreåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAWMçš„åº”ç”¨åœ¨Stable Diffusion 3.5 Mediumå’ŒFLUXä¸Šå¤§å¤§æå‡äº†æ€§èƒ½ï¼Œä¸Flow-GRPOç›¸æ¯”å®ç°äº†é«˜è¾¾24å€çš„åŠ é€Ÿï¼Œä¸”ä¸ä¼šç‰ºç‰²ç”Ÿæˆè´¨é‡ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€äºä¼˜åŠ¿åŠ æƒåŒ¹é…GitHubä»“åº“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å·²æˆä¸ºæ¨è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒæ–¹æ³•ã€‚</li>
<li>åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå»å™ªæ‰©æ•£ç­–ç•¥ä¼˜åŒ–ï¼ˆDDPOï¼‰æ–¹æ³•ä¸é¢„è®­ç»ƒç›®æ ‡ä¸åŒã€‚</li>
<li>DDPOæ˜¯å¸¦æœ‰å™ªå£°ç›®æ ‡çš„éšå¼åˆ†æ•°&#x2F;æµåŒ¹é…æ–¹æ³•ï¼Œå¯èƒ½å¯¼è‡´æ–¹å·®å¢åŠ å’Œæ”¶æ•›é€Ÿåº¦å‡æ…¢ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¼˜åŠ¿åŠ æƒåŒ¹é…ï¼ˆAWMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ç”¨äºæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>AWMé‡‡ç”¨ä¸é¢„è®­ç»ƒç›¸åŒçš„åˆ†æ•°&#x2F;æµåŒ¹é…æŸå¤±ä»¥é™ä½æ–¹å·®å¹¶åŠ é€Ÿæ”¶æ•›ã€‚</li>
<li>AWMæé«˜äº†é«˜å›æŠ¥æ ·æœ¬çš„å½±å“ï¼ŒåŒæ—¶æŠ‘åˆ¶ä½å›æŠ¥æ ·æœ¬çš„å½±å“ï¼Œå¹¶ä¿æŒå»ºæ¨¡ç›®æ ‡ä¸å˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-15e072642699233430692c5bd31fc0cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1759911938&auth_key=1759911938-0-0-7e38cdedd4ed025de1e3b396832ce367&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f9d9608f23602cbb072c9ef94b100986~resize:0:q75.jpg?source=1f5c5e47&expiration=1759911945&auth_key=1759911945-0-0-d98c0d9d688a2881d47ec271e7846ece&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GeoVLM-R1-Reinforcement-Fine-Tuning-for-Improved-Remote-Sensing-Reasoning"><a href="#GeoVLM-R1-Reinforcement-Fine-Tuning-for-Improved-Remote-Sensing-Reasoning" class="headerlink" title="GeoVLM-R1: Reinforcement Fine-Tuning for Improved Remote Sensing   Reasoning"></a>GeoVLM-R1: Reinforcement Fine-Tuning for Improved Remote Sensing   Reasoning</h2><p><strong>Authors:Mustansar Fiaz, Hiyam Debary, Paolo Fraccaro, Danda Paudel, Luc Van Gool, Fahad Khan, Salman Khan</strong></p>
<p>Recent advances in reinforcement learning (RL) have delivered strong reasoning capabilities in natural image domains, yet their potential for Earth Observation (EO) remains largely unexplored. EO tasks introduce unique challenges, spanning referred object detection, image or region captioning, change detection, grounding, and temporal analysis, that demand task aware reasoning. We propose a novel post training framework that incorporates task aware rewards to enable effective adaptation of reasoning based RL models to diverse EO tasks. This training strategy enhances reasoning capabilities for remote sensing images, stabilizes optimization, and improves robustness. Extensive experiments across multiple EO benchmarks show consistent performance gains over state of the art generic and specialized vision language models. Code and models will be released publicly at <a target="_blank" rel="noopener" href="https://mustansarfiaz.github.io/GeoVLM-R1/">https://mustansarfiaz.github.io/GeoVLM-R1/</a> . </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è‡ªç„¶å›¾åƒé¢†åŸŸå±•ç°äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œç„¶è€Œå…¶åœ¨åœ°çƒè§‚æµ‹ï¼ˆEOï¼‰æ–¹é¢çš„æ½œåŠ›å´é²œæœ‰ç ”ç©¶ã€‚åœ°çƒè§‚æµ‹ä»»åŠ¡å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ‰€æŒ‡å¯¹è±¡çš„æ£€æµ‹ã€å›¾åƒæˆ–åŒºåŸŸæè¿°ã€å˜åŒ–æ£€æµ‹ã€æ¥åœ°å’Œæ—¶åºåˆ†æç­‰ï¼Œè¿™äº›éƒ½éœ€è¦ä»»åŠ¡æ„ŸçŸ¥æ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åè®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ä»»åŠ¡æ„ŸçŸ¥å¥–åŠ±ï¼Œä»¥å®ç°å¯¹åŸºäºæ¨ç†çš„å¼ºåŒ–æ¨¡å‹åœ¨å¤šç§åœ°çƒè§‚æµ‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆé€‚åº”ã€‚è¿™ç§è®­ç»ƒç­–ç•¥æé«˜äº†é¥æ„Ÿå›¾åƒçš„æ¨ç†èƒ½åŠ›ï¼Œç¨³å®šäº†ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚åœ¨å¤šä¸ªåœ°çƒè§‚æµ‹åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„é€šç”¨å’Œç‰¹æ®Šè§†è§‰è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œå…¶æ€§èƒ½å¾—åˆ°äº†ä¸€è‡´çš„æå‡ã€‚ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å‘å¸ƒåœ¨ <a target="_blank" rel="noopener" href="https://mustansarfiaz.github.io/GeoVLM-R1/">https://mustansarfiaz.github.io/GeoVLM-R1/</a> ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25026v1">PDF</a> Tables 6 and Figures 9. <a target="_blank" rel="noopener" href="https://mustansarfiaz.github.io/GeoVLM-R1/">https://mustansarfiaz.github.io/GeoVLM-R1/</a></p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ åœ¨è‡ªç„¶å›¾åƒé¢†åŸŸçš„æ¨ç†èƒ½åŠ›å·²å¾—åˆ°æ˜¾è‘—æå‡ï¼Œä½†åœ¨åœ°çƒè§‚æµ‹ï¼ˆEOï¼‰é¢†åŸŸçš„åº”ç”¨æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚é’ˆå¯¹åœ°çƒè§‚æµ‹ä»»åŠ¡ä¸­çš„å¯¹è±¡æ£€æµ‹ã€å›¾åƒæˆ–åŒºåŸŸæè¿°ã€å˜åŒ–æ£€æµ‹ã€æ¥åœ°å’Œæ—¶åºåˆ†æç­‰ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åè®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ä»»åŠ¡æ„ŸçŸ¥å¥–åŠ±ï¼Œä½¿æ¨ç†å‹çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿçµæ´»é€‚åº”å¤šç§åœ°çƒè§‚æµ‹ä»»åŠ¡ã€‚æ­¤è®­ç»ƒç­–ç•¥æå‡äº†é¥æ„Ÿå›¾åƒçš„æ¨ç†èƒ½åŠ›ï¼Œç¨³å®šäº†ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚åœ¨å¤šä¸ªåœ°çƒè§‚æµ‹åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„é€šç”¨å’Œä¸“ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥ç­–ç•¥å…·æœ‰ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚æ¨¡å‹å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://mustansarfiaz.github.io/GeoVLM-R1/">https://mustansarfiaz.github.io/GeoVLM-R1/</a>å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨è‡ªç„¶å›¾åƒé¢†åŸŸçš„æ¨ç†èƒ½åŠ›å·²æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>åœ°çƒè§‚æµ‹ï¼ˆEOï¼‰ä»»åŠ¡å…·æœ‰ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œéœ€è¦ä»»åŠ¡æ„ŸçŸ¥æ¨ç†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹åè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ä»»åŠ¡æ„ŸçŸ¥å¥–åŠ±é€‚åº”å¤šç§EOä»»åŠ¡ã€‚</li>
<li>è¯¥è®­ç»ƒç­–ç•¥å¢å¼ºäº†é¥æ„Ÿå›¾åƒçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ­¤ç­–ç•¥ç¨³å®šäº†ä¼˜åŒ–è¿‡ç¨‹ï¼Œæé«˜äº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªåœ°çƒè§‚æµ‹åŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥ç­–ç•¥è¶…è¶Šç°æœ‰æ¨¡å‹ï¼Œå…·æœ‰ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6a2480dc0a7b62c6fc0eaa6ccda3cc0c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759911952&auth_key=1759911952-0-0-c2c2fea581a8291f8528a7e9b0a4db0c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-c2e4c1f46cd032c4beae17954fd429f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8ee1bee5b9e7895c28242f7111bf2b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5de4ed3c91e1cc90db0460a16d457907.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-65cc012425ec6c354eb488fd7a607cf2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759911979&auth_key=1759911979-0-0-5a6ccb1af10160a2903d26310ed8d7b6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MARCOS-Deep-Thinking-by-Markov-Chain-of-Continuous-Thoughts"><a href="#MARCOS-Deep-Thinking-by-Markov-Chain-of-Continuous-Thoughts" class="headerlink" title="MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts"></a>MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts</h2><p><strong>Authors:Jiayu Liu, Zhenya Huang, Anya Sims, Enhong Chen, Yee Whye Teh, Ning Miao</strong></p>
<p>The current paradigm for reasoning in large language models (LLMs) involves models â€œthinking out loudâ€ via a sequence of tokens, known as chain-of-thought (CoT). This approach, while effective, has several significant drawbacks. Firstly, inference requires autoregressive generation of often thousands of CoT tokens, which is slow and computationally expensive. Secondly, it constrains reasoning to the discrete space of tokens, creating an information bottleneck across reasoning steps. Thirdly, it fundamentally entangles reasoning with token generation, forcing LLMs to â€œthink while speaking,â€ which causes potentially short-sighted reasoning. In light of these limitations, we re-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our approach, rather than autoregressively generating tokens, we model reasoning as a hidden Markov chain of continuous, high-dimensional â€œthoughtsâ€. Each reasoning step involves a transition of the internal thoughts, where explicit reasoning steps (which may consist of hundreds of tokens) serve as observable variables, which are windows to peek into the implicit thoughts. Since this latent process is incompatible with the standard supervised learning, we further propose a two-phase variational training scheme. Our experiments on three benchmarks demonstrate that MARCOS outperforms existing continuous reasoning methods and, for the first time, achieves performance comparable to token-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup in inference. Beyond this, MARCOS offers additional advantages, such as step-level instead of token-level control over randomness, opening significant opportunities for reinforcement learning and reasoning in LLMs. </p>
<blockquote>
<p>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èŒƒå¼æ˜¯é€šè¿‡ä¸€ç³»åˆ—æ ‡è®°ï¼ˆç§°ä¸ºæ€ç»´é“¾ï¼ŒCoTï¼‰è¿›è¡Œæ¨¡å‹çš„â€œå¤§å£°æ€è€ƒâ€ã€‚è™½ç„¶è¿™ç§æ–¹æ³•æœ‰æ•ˆï¼Œä½†å®ƒæœ‰å‡ ä¸ªé‡å¤§ç¼ºç‚¹ã€‚é¦–å…ˆï¼Œæ¨ç†éœ€è¦è‡ªå›å½’ç”Ÿæˆæ•°åƒä¸ªCoTæ ‡è®°ï¼Œè¿™æ—¢ç¼“æ…¢åˆè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚å…¶æ¬¡ï¼Œå®ƒå°†æ¨ç†é™åˆ¶åœ¨æ ‡è®°çš„ç¦»æ•£ç©ºé—´ä¸­ï¼Œåœ¨æ¨ç†æ­¥éª¤ä¹‹é—´åˆ›å»ºä¿¡æ¯ç“¶é¢ˆã€‚ç¬¬ä¸‰ï¼Œå®ƒä»æ ¹æœ¬ä¸Šå°†æ¨ç†ä¸æ ‡è®°ç”Ÿæˆæ··æ·†åœ¨ä¸€èµ·ï¼Œè¿«ä½¿LLMåœ¨è¯´è¯æ—¶æ€è€ƒï¼Œå¯¼è‡´æ½œåœ¨çš„çŸ­è§†æ¨ç†ã€‚è€ƒè™‘åˆ°è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬é‡æ–°æ„æƒ³LLMä¸­çš„æ¨ç†ï¼Œå¹¶ä»‹ç»äº†ä¸€ç§æ–°èŒƒå¼ï¼šMARCOSã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œä¸æ˜¯è‡ªå›å½’åœ°ç”Ÿæˆæ ‡è®°ï¼Œè€Œæ˜¯å°†æ¨ç†å»ºæ¨¡ä¸ºè¿ç»­çš„ã€é«˜ç»´åº¦çš„â€œæ€ç»´â€çš„éšé©¬å°”å¯å¤«é“¾ã€‚æ¯ä¸ªæ¨ç†æ­¥éª¤éƒ½æ¶‰åŠå†…éƒ¨æ€ç»´çš„è¿‡æ¸¡ï¼Œå…¶ä¸­æ˜ç¡®çš„æ¨ç†æ­¥éª¤ï¼ˆå¯èƒ½åŒ…å«æ•°ç™¾ä¸ªæ ‡è®°ï¼‰ä½œä¸ºè§‚æµ‹å˜é‡ï¼Œæ˜¯çª¥æ¢éšæ€§æ€ç»´çš„çª—å£ã€‚ç”±äºè¿™ç§æ½œåœ¨è¿‡ç¨‹ä¸æ ‡å‡†ç›‘ç£å­¦ä¹ ä¸å…¼å®¹ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„å˜åˆ†è®­ç»ƒæ–¹æ¡ˆã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMARCOSä¼˜äºç°æœ‰çš„è¿ç»­æ¨ç†æ–¹æ³•ï¼Œå¹¶é¦–æ¬¡å®ç°äº†ä¸åŸºäºæ ‡è®°çš„CoTç›¸å½“çš„æ€§èƒ½ï¼Œåœ¨GSM8Kä¸Šç”šè‡³è¶…è¶Šäº†4.7%ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†é«˜è¾¾15.7å€ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒMARCOSè¿˜æä¾›äº†é¢å¤–çš„ä¼˜åŠ¿ï¼Œå¦‚æ­¥çº§è€Œä¸æ˜¯æ ‡è®°çº§çš„éšæœºæ€§æ§åˆ¶ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ å’ŒLLMä¸­çš„æ¨ç†å¸¦æ¥äº†å·¨å¤§çš„æœºä¼šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25020v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ¨¡å¼é€šè¿‡ç”Ÿæˆä¸€ç³»åˆ—ä»¤ç‰Œæ¥å®ç°æ¨¡å‹çš„â€œæ€è€ƒè¿‡ç¨‹â€ï¼Œä½†å­˜åœ¨é€Ÿåº¦æ…¢ã€è®¡ç®—æˆæœ¬é«˜ã€ä¿¡æ¯ç“¶é¢ˆç­‰é—®é¢˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ¨¡å¼ï¼šMARCOSã€‚è¯¥æ¨¡å¼å°†æ¨ç†è§†ä¸ºä¸€ä¸ªè¿ç»­çš„ã€é«˜ç»´åº¦çš„â€œæ€ç»´â€çš„éšé©¬å°”å¯å¤«é“¾ã€‚æ¯ä¸ªæ¨ç†æ­¥éª¤éƒ½æ˜¯å†…éƒ¨æ€ç»´çš„è¿‡æ¸¡ï¼Œæ˜ç¡®çš„æ¨ç†æ­¥éª¤ä½œä¸ºè§‚å¯Ÿå˜é‡ï¼Œå¯ä»¥çª¥æ¢åˆ°éšæ€§çš„æ€ç»´ã€‚å®éªŒè¡¨æ˜ï¼ŒMARCOSåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„è¿ç»­æ¨ç†æ–¹æ³•ï¼Œé¦–æ¬¡å®ç°äº†ä¸åŸºäºä»¤ç‰Œçš„æ¨ç†æ¨¡å¼ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶åœ¨GSM8Kä¸Šæé«˜äº†4.7%ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†15.7å€ã€‚æ­¤å¤–ï¼ŒMARCOSè¿˜å…·æœ‰æ­¥éª¤çº§åˆ«çš„æ§åˆ¶éšæœºæ€§ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å¸¦æ¥äº†æ›´å¤šæœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„ç°æœ‰æ¨ç†æ¨¡å¼å­˜åœ¨é€Ÿåº¦æ…¢ã€è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚</li>
<li>MARCOSä½œä¸ºä¸€ç§æ–°çš„æ¨ç†æ¨¡å¼ï¼Œå°†æ¨ç†è¿‡ç¨‹è§†ä¸ºè¿ç»­çš„ã€é«˜ç»´åº¦çš„â€œæ€ç»´â€éšé©¬å°”å¯å¤«é“¾ã€‚</li>
<li>MARCOSé€šè¿‡æ˜ç¡®çš„æ¨ç†æ­¥éª¤çª¥æ¢éšæ€§æ€ç»´ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒMARCOSåœ¨æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–è¿ç»­æ¨ç†æ–¹æ³•ï¼Œå®ç°äº†ä¸åŸºäºä»¤ç‰Œçš„æ¨ç†æ¨¡å¼ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶æä¾›äº†æ›´é«˜çš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>MARCOSå…·æœ‰æ­¥éª¤çº§åˆ«çš„æ§åˆ¶éšæœºæ€§ï¼Œæœ‰åŠ©äºå¼ºåŒ–å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›ä¸€æ­¥åº”ç”¨ã€‚</li>
<li>MARCOSæ¨¡å¼å¯èƒ½æœ‰åŠ©äºè§£å†³ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„ä¿¡æ¯ç“¶é¢ˆé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25020">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-448524a8e7e236e503585e0a6cb83e0e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759911986&auth_key=1759911986-0-0-067cbbdbae6558a683a519c69687bb2f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-deda701f8ccd8c75f97a99491f97bea7~resize:0:q75.jpg?source=1f5c5e47&expiration=1759911993&auth_key=1759911993-0-0-7a2a9db4023c9f1b232563b18b2e62b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a3fcf825bf99729f4139207c99b7f28~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912000&auth_key=1759912000-0-0-5e4d7dd6672bc1f85edd4a8e5f24f6c2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CLPO-Curriculum-Learning-meets-Policy-Optimization-for-LLM-Reasoning"><a href="#CLPO-Curriculum-Learning-meets-Policy-Optimization-for-LLM-Reasoning" class="headerlink" title="CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning"></a>CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning</h2><p><strong>Authors:Shijie Zhang, Guohao Sun, Kevin Zhang, Xiang Guo, Rujun Guo</strong></p>
<p>Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing methods typically treat all training samples uniformly, overlooking the vast differences in problem difficulty relative to the modelâ€™s current capabilities. This uniform training strategy leads to inefficient exploration of problems the model has already mastered, while concurrently lacking effective guidance on problems that are challenging its abilities the most, limiting both learning efficiency and upper-bound performance. To address this, we propose CLPO (Curriculum-guided Learning for Policy Optimization), a novel algorithm that creates a dynamic pedagogical feedback loop within the policy optimization process. The core of CLPO leverages the modelâ€™s own rollout performance to conduct real-time difficulty assessment, thereby constructing an Online Curriculum. This curriculum then guides an Adaptive Problem Restructuring mechanism, where the model acts as its own teacher: it diversifies medium-difficulty problems to promote generalization and simplifies challenging problems to make them more attainable. Our approach transforms the static training procedure into a dynamic process that co-evolves with the modelâ€™s capabilities. Experiments show that CLPO achieves state-of-the-art performance across eight challenging mathematical and general reasoning benchmarks, with an average pass@1 improvement of 6.96% over other methods, demonstrating its potential for more efficiently training more capable reasoning models. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¸¦æœ‰å¯éªŒè¯å¥–åŠ±çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å·²æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸€ç§å…³é”®èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å¯¹æ‰€æœ‰è®­ç»ƒæ ·æœ¬è¿›è¡Œç»Ÿä¸€å¤„ç†ï¼Œå¿½ç•¥äº†é—®é¢˜éš¾åº¦ä¸æ¨¡å‹å½“å‰èƒ½åŠ›ä¹‹é—´çš„å·¨å¤§å·®å¼‚ã€‚è¿™ç§ç»Ÿä¸€è®­ç»ƒç­–ç•¥å¯¼è‡´æ¨¡å‹å¯¹å·²æŒæ¡çš„é—®é¢˜è¿›è¡Œæ— æ•ˆæ¢ç´¢ï¼ŒåŒæ—¶ç¼ºä¹å¯¹å…¶èƒ½åŠ›æœ€å…·æŒ‘æˆ˜æ€§é—®é¢˜çš„æœ‰æ•ˆæŒ‡å¯¼ï¼Œä»è€Œé™åˆ¶äº†å­¦ä¹ æ•ˆç‡å’Œæœ€é«˜æ€§èƒ½ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CLPOï¼ˆç”¨äºç­–ç•¥ä¼˜åŒ–çš„è¯¾ç¨‹æŒ‡å¯¼å­¦ä¹ ï¼‰è¿™ä¸€æ–°å‹ç®—æ³•ï¼Œå®ƒåœ¨ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ä¸­åˆ›å»ºäº†ä¸€ä¸ªåŠ¨æ€çš„æ•™å­¦åé¦ˆç¯ã€‚CLPOçš„æ ¸å¿ƒæ˜¯åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„æ»šåŠ¨æ€§èƒ½è¿›è¡Œå®æ—¶éš¾åº¦è¯„ä¼°ï¼Œä»è€Œæ„å»ºåœ¨çº¿è¯¾ç¨‹ã€‚ç„¶åï¼Œè¿™ä¸ªè¯¾ç¨‹å¼•å¯¼è‡ªé€‚åº”é—®é¢˜é‡æ„æœºåˆ¶ï¼Œå…¶ä¸­æ¨¡å‹å……å½“è‡ªå·±çš„è€å¸ˆï¼šå®ƒå¤šæ ·åŒ–ä¸­ç­‰éš¾åº¦çš„é—®é¢˜ä»¥ä¿ƒè¿›æ¨å¹¿ï¼Œå¹¶ç®€åŒ–æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ä»¥ä½¿å…¶æ›´å®¹æ˜“å®ç°ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†é™æ€è®­ç»ƒè¿‡ç¨‹è½¬å˜ä¸ºä¸€ä¸ªä¸æ¨¡å‹èƒ½åŠ›å…±åŒæ¼”åŒ–çš„åŠ¨æ€è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒCLPOåœ¨å…«ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦å’Œé€šç”¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡pass@1æ¯”å…¶ä»–æ–¹æ³•æé«˜äº†6.96%ï¼Œè¿™è¯æ˜äº†å…¶åœ¨æ›´æœ‰æ•ˆåœ°è®­ç»ƒæ›´å…·èƒ½åŠ›çš„æ¨ç†æ¨¡å‹æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25004v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä¸­çš„å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„å…³é”®èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å¯¹æ‰€æœ‰è®­ç»ƒæ ·æœ¬ä¸€è§†åŒä»ï¼Œå¿½è§†äº†é—®é¢˜éš¾åº¦ä¸æ¨¡å‹å½“å‰èƒ½åŠ›ä¹‹é—´çš„å·¨å¤§å·®å¼‚ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CLPOï¼ˆç”¨äºç­–ç•¥ä¼˜åŒ–çš„è¯¾ç¨‹æŒ‡å¯¼å­¦ä¹ ï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ä¸­åˆ›å»ºäº†ä¸€ä¸ªåŠ¨æ€çš„åé¦ˆç¯ã€‚CLPOçš„æ ¸å¿ƒæ˜¯åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„æ»šåŠ¨è¡¨ç°è¿›è¡Œå®æ—¶éš¾åº¦è¯„ä¼°ï¼Œä»è€Œæ„å»ºåœ¨çº¿è¯¾ç¨‹ã€‚æ­¤è¯¾ç¨‹æŒ‡å¯¼è‡ªé€‚åº”é—®é¢˜é‡å»ºæœºåˆ¶ï¼Œä½¿æ¨¡å‹å……å½“è‡ªå·±çš„è€å¸ˆï¼šå®ƒå¤šæ ·åŒ–ä¸­ç­‰éš¾åº¦çš„é—®é¢˜ä»¥ä¿ƒè¿›æ¨å¹¿ï¼Œå¹¶ç®€åŒ–æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ä»¥ä½¿å…¶æ›´å®¹æ˜“è§£å†³ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†é™æ€è®­ç»ƒè¿‡ç¨‹è½¬å˜ä¸ºä¸æ¨¡å‹èƒ½åŠ›å…±åŒæ¼”åŒ–çš„åŠ¨æ€è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒCLPOåœ¨å…«ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦å’Œé€šç”¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å…¶ä»–æ–¹æ³•çš„åŸºç¡€ä¸Šå¹³å‡æé«˜äº†6.96%çš„pass@1ç‡ï¼Œè¯æ˜äº†å…¶åœ¨è®­ç»ƒæ›´å…·èƒ½åŠ›çš„æ¨ç†æ¨¡å‹æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸­çš„å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰è®­ç»ƒç­–ç•¥å¿½è§†äº†é—®é¢˜ä¸æ¨¡å‹èƒ½åŠ›ä¹‹é—´çš„åŒ¹é…ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½ä¸Šé™å—é™ã€‚</li>
<li>CLPOç®—æ³•é€šè¿‡å®æ—¶éš¾åº¦è¯„ä¼°æ„å»ºåœ¨çº¿è¯¾ç¨‹ï¼ŒæŒ‡å¯¼è‡ªé€‚åº”é—®é¢˜é‡å»ºã€‚</li>
<li>æ¨¡å‹è‡ªæˆ‘æ‰®æ¼”è€å¸ˆï¼Œé€šè¿‡å¤šæ ·åŒ–ä¸­ç­‰éš¾åº¦é—®é¢˜ä¿ƒè¿›æ¨å¹¿ï¼Œç®€åŒ–éš¾é¢˜ä»¥æé«˜è§£å†³ç‡ã€‚</li>
<li>CLPOå°†é™æ€è®­ç»ƒè½¬åŒ–ä¸ºåŠ¨æ€è¿‡ç¨‹ï¼Œä¸æ¨¡å‹èƒ½åŠ›å…±åŒè¿›åŒ–ã€‚</li>
<li>CLPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡æå‡6.96%çš„pass@1ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4659e4df212cf30b43803d12afab28dc.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-ab1f96f643f70b2661f0dda92f89758d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912015&auth_key=1759912015-0-0-72d0419e673e1d7ec0a1f46e183df24c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="The-Dialogue-That-Heals-A-Comprehensive-Evaluation-of-Doctor-Agentsâ€™-Inquiry-Capability"><a href="#The-Dialogue-That-Heals-A-Comprehensive-Evaluation-of-Doctor-Agentsâ€™-Inquiry-Capability" class="headerlink" title="The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agentsâ€™   Inquiry Capability"></a>The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agentsâ€™   Inquiry Capability</h2><p><strong>Authors:Linlu Gong, Ante Wang, Yunghwei Lai, Weizhi Ma, Yang Liu</strong></p>
<p>An effective physician should possess a combination of empathy, expertise, patience, and clear communication when treating a patient. Recent advances have successfully endowed AI doctors with expert diagnostic skills, particularly the ability to actively seek information through inquiry. However, other essential qualities of a good doctor remain overlooked. To bridge this gap, we present MAQuE(Medical Agent Questioning Evaluation), the largest-ever benchmark for the automatic and comprehensive evaluation of medical multi-turn questioning. It features 3,000 realistically simulated patient agents that exhibit diverse linguistic patterns, cognitive limitations, emotional responses, and tendencies for passive disclosure. We also introduce a multi-faceted evaluation framework, covering task success, inquiry proficiency, dialogue competence, inquiry efficiency, and patient experience. Experiments on different LLMs reveal substantial challenges across the evaluation aspects. Even state-of-the-art models show significant room for improvement in their inquiry capabilities. These models are highly sensitive to variations in realistic patient behavior, which considerably impacts diagnostic accuracy. Furthermore, our fine-grained metrics expose trade-offs between different evaluation perspectives, highlighting the challenge of balancing performance and practicality in real-world clinical settings. </p>
<blockquote>
<p>ä¸€ä¸ªæœ‰æ•ˆçš„åŒ»ç”Ÿåœ¨æ²»ç–—ç—…äººæ—¶ï¼Œåº”å…·å¤‡åŒæƒ…å¿ƒã€ä¸“ä¸šçŸ¥è¯†ã€è€å¿ƒå’Œæ¸…æ™°çš„æ²Ÿé€šèƒ½åŠ›ã€‚æœ€è¿‘çš„è¿›æ­¥å·²ç»æˆåŠŸèµ‹äºˆAIåŒ»ç”Ÿä¸“ä¸šçš„è¯Šæ–­æŠ€èƒ½ï¼Œå°¤å…¶æ˜¯é€šè¿‡è¯¢é—®ä¸»åŠ¨è·å–ä¿¡æ¯çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶ä»–ä½œä¸ºå¥½åŒ»ç”Ÿçš„é‡è¦å“è´¨ä»ç„¶è¢«å¿½è§†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MAQuEï¼ˆåŒ»ç–—ä»£ç†é—®è¯¢è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„åŒ»ç–—å¤šè½®é—®è¯¢çš„è‡ªåŠ¨ç»¼åˆè¯„ä¼°åŸºå‡†ã€‚å®ƒæ‹¥æœ‰3000ä¸ªçœŸå®æ¨¡æ‹Ÿçš„æ‚£è€…ä»£ç†ï¼Œå±•ç°å‡ºå¤šæ ·çš„è¯­è¨€æ¨¡å¼ã€è®¤çŸ¥å±€é™ã€æƒ…æ„Ÿååº”å’Œè¢«åŠ¨æŠ«éœ²å€¾å‘ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå¤šé¢è¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–ä»»åŠ¡æˆåŠŸã€è¯¢é—®èƒ½åŠ›ã€å¯¹è¯èƒ½åŠ›ã€è¯¢é—®æ•ˆç‡å’Œæ‚£è€…ä½“éªŒã€‚åœ¨ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨è¯„ä¼°æ–¹é¢å­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿæ˜¾ç¤ºå‡ºå…¶è¯¢é—®èƒ½åŠ›ä»æœ‰æ˜¾è‘—çš„æå‡ç©ºé—´ã€‚è¿™äº›æ¨¡å‹å¯¹ç°å®æ‚£è€…è¡Œä¸ºçš„æ•æ„Ÿæ€§å¾ˆé«˜ï¼Œè¿™ä¼šæ˜¾è‘—å½±å“è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç²¾ç»†çš„æŒ‡æ ‡æš´éœ²äº†ä¸åŒè¯„ä¼°è§’åº¦ä¹‹é—´çš„æƒè¡¡ï¼Œçªå‡ºäº†åœ¨ç°å®ä¸–ç•Œä¸´åºŠç¯å¢ƒä¸­å¹³è¡¡æ€§èƒ½å’Œå®ç”¨æ€§çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24958v1">PDF</a> </p>
<p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½åŒ»ç”Ÿåœ¨è¯Šæ–­æŠ€èƒ½ä¸Šå·²æœ‰æ˜¾è‘—è¿›æ­¥ï¼Œä½†ä»éœ€å…·å¤‡åŒ»ç”Ÿçš„å…¶ä»–å…³é”®å“è´¨ï¼Œå¦‚åŒç†å¿ƒã€ä¸“ä¸šè€å¿ƒå’Œæ¸…æ™°æ²Ÿé€šç­‰ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæå‡ºäº†MAQuEï¼ˆåŒ»ç–—ä»£ç†é—®ç­”è¯„ä¼°ï¼‰åŸºå‡†æµ‹è¯•ï¼Œæ¨¡æ‹ŸçœŸå®æ‚£è€…ï¼Œå…¨é¢è¯„ä¼°åŒ»ç–—å¤šè½®é—®ç­”ã€‚å®éªŒæ˜¾ç¤ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯„ä¼°æ–¹é¢é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è¯¢é—®èƒ½åŠ›æ–¹é¢ä¹Ÿæœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åŒ»ç”Ÿéœ€ç»“åˆå¤šç§å“è´¨ï¼ŒåŒ…æ‹¬åŒç†å¿ƒã€ä¸“ä¸šçŸ¥è¯†å’Œæ¸…æ™°æ²Ÿé€šã€‚</li>
<li>MAQuEåŸºå‡†æµ‹è¯•ç”¨äºå…¨é¢è¯„ä¼°åŒ»ç–—å¤šè½®é—®ç­”ã€‚</li>
<li>MAQuEæ¨¡æ‹ŸçœŸå®æ‚£è€…ï¼Œæ¶µç›–å¤šæ ·åŒ–è¯­è¨€æ¨¡å¼ã€è®¤çŸ¥é™åˆ¶ã€æƒ…æ„Ÿå“åº”å’Œè¢«åŠ¨æŠ«éœ²å€¾å‘ã€‚</li>
<li>è¯„ä¼°æ¡†æ¶åŒ…æ‹¬ä»»åŠ¡æˆåŠŸã€è¯¢é—®ç†Ÿç»ƒåº¦ã€å¯¹è¯èƒ½åŠ›ã€è¯¢é—®æ•ˆç‡å’Œæ‚£è€…ä½“éªŒã€‚</li>
<li>å®éªŒæ˜¾ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—é—®ç­”è¯„ä¼°æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å…ˆè¿›æ¨¡å‹åœ¨è¯¢é—®èƒ½åŠ›æ–¹é¢ä»æœ‰æ˜¾è‘—æ”¹è¿›ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-dbacadaee6657794dbe7abe4fe8a0679~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912023&auth_key=1759912023-0-0-12826ddb715025ae01f981b1c319c67d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-601d52f94a9fb6556c4d45bb3f8574a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912030&auth_key=1759912030-0-0-518ee76a6086ef8fbc79bdbdd707216a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c983bcfdc6220f57160f3790453b1251~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912037&auth_key=1759912037-0-0-1355ad591ece128ab38bbdb100aaf50d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MASLegalBench-Benchmarking-Multi-Agent-Systems-in-Deductive-Legal-Reasoning"><a href="#MASLegalBench-Benchmarking-Multi-Agent-Systems-in-Deductive-Legal-Reasoning" class="headerlink" title="MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal   Reasoning"></a>MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal   Reasoning</h2><p><strong>Authors:Huihao Jing, Wenbin Hu, Hongyu Luo, Jianhui Yang, Wei Fan, Haoran Li, Yangqiu Song</strong></p>
<p>Multi-agent systems (MAS), leveraging the remarkable capabilities of Large Language Models (LLMs), show great potential in addressing complex tasks. In this context, integrating MAS with legal tasks is a crucial step. While previous studies have developed legal benchmarks for LLM agents, none are specifically designed to consider the unique advantages of MAS, such as task decomposition, agent specialization, and flexible training. In fact, the lack of evaluation methods limits the potential of MAS in the legal domain. To address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS and designed with a deductive reasoning approach. Our benchmark uses GDPR as the application scenario, encompassing extensive background knowledge and covering complex reasoning processes that effectively reflect the intricacies of real-world legal situations. Furthermore, we manually design various role-based MAS and conduct extensive experiments using different state-of-the-art LLMs. Our results highlight the strengths, limitations, and potential areas for improvement of existing models and MAS architectures. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å“è¶Šèƒ½åŠ›ï¼Œåœ¨åº”å¯¹å¤æ‚ä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œå°†MASä¸æ³•å¾‹ä»»åŠ¡é›†æˆæ˜¯ä¸€ä¸ªå…³é”®æ­¥éª¤ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å·²ç»ä¸ºLLMä»£ç†å¼€å‘äº†æ³•å¾‹åŸºå‡†æµ‹è¯•ï¼Œä½†æ²¡æœ‰ä¸€ä¸ªæ˜¯ä¸“é—¨è€ƒè™‘MASçš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œå¦‚ä»»åŠ¡åˆ†è§£ã€ä»£ç†ä¸“ä¸šåŒ–å’Œçµæ´»è®­ç»ƒã€‚äº‹å®ä¸Šï¼Œç¼ºä¹è¯„ä¼°æ–¹æ³•é™åˆ¶äº†MASåœ¨æ³•å¾‹é¢†åŸŸçš„æ½œåŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MASLegalBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºMASå®šåˆ¶çš„æ³•å¾‹åŸºå‡†æµ‹è¯•ï¼Œå¹¶é‡‡ç”¨æ¼”ç»æ¨ç†æ–¹æ³•è®¾è®¡ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä»¥GDPRä¸ºåº”ç”¨åœºæ™¯ï¼ŒåŒ…å«ä¸°å¯Œçš„èƒŒæ™¯çŸ¥è¯†ï¼Œæ¶µç›–æœ‰æ•ˆçš„å¤æ‚æ¨ç†è¿‡ç¨‹ï¼Œå……åˆ†åæ˜ äº†ç°å®æ³•å¾‹æƒ…å¢ƒçš„å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ‰‹åŠ¨è®¾è®¡äº†å„ç§åŸºäºè§’è‰²çš„MASï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„æœ€æ–°LLMè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚æˆ‘ä»¬çš„ç»“æœçªæ˜¾äº†ç°æœ‰æ¨¡å‹å’ŒMASæ¶æ„çš„ä¼˜åŠ¿ã€å±€é™æ€§å’Œæ½œåœ¨çš„æ”¹è¿›é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24922v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å“è¶Šèƒ½åŠ›ï¼Œå±•ç°å‡ºè§£å†³å¤æ‚ä»»åŠ¡çš„å·¨å¤§æ½œåŠ›ã€‚åœ¨ç ”ç©¶å°†MASåº”ç”¨äºæ³•å¾‹ä»»åŠ¡æ–¹é¢ï¼Œè™½ç„¶å·²æœ‰æ³•å¾‹åŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°LLMä»£ç†ï¼Œä½†å°šæ— ä¸“é—¨é’ˆå¯¹MASç‹¬ç‰¹ä¼˜åŠ¿çš„åŸºå‡†æµ‹è¯•ï¼Œå¦‚ä»»åŠ¡åˆ†è§£ã€ä»£ç†ä¸“ä¸šåŒ–å’Œçµæ´»è®­ç»ƒç­‰ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸“ä¸ºMASè®¾è®¡çš„æ³•å¾‹åŸºå‡†æµ‹è¯•â€”â€”MASLegalBenchï¼Œé‡‡ç”¨æ¼”ç»æ¨ç†æ–¹æ³•ï¼Œä»¥GDPRä¸ºåº”ç”¨åœºæ™¯ï¼Œæ¶µç›–ä¸°å¯Œçš„èƒŒæ™¯çŸ¥è¯†ï¼Œå¹¶æœ‰æ•ˆåæ˜ ç°å®æ³•å¾‹æƒ…å¢ƒçš„å¤æ‚æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ç°æœ‰æ¨¡å‹å’ŒMASæ¶æ„çš„ä¼˜åŠ¿ã€å±€é™æ€§å’Œæ½œåœ¨æ”¹è¿›æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰æ³•å¾‹åŸºå‡†æµ‹è¯•æœªå……åˆ†è€ƒè™‘MASçš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œå¦‚ä»»åŠ¡åˆ†è§£å’Œçµæ´»è®­ç»ƒã€‚</li>
<li>MASLegalBenchæ˜¯ä¸“ä¸ºMASè®¾è®¡çš„æ³•å¾‹åŸºå‡†æµ‹è¯•ï¼Œé‡‡ç”¨æ¼”ç»æ¨ç†æ–¹æ³•ã€‚</li>
<li>MASLegalBenchä»¥GDPRä¸ºåº”ç”¨åœºæ™¯ï¼Œæ¶µç›–ä¸°å¯Œçš„èƒŒæ™¯çŸ¥è¯†ï¼Œåæ˜ ç°å®æ³•å¾‹æƒ…å¢ƒçš„å¤æ‚æ€§ã€‚</li>
<li>ç°æœ‰å®éªŒè¡¨æ˜ï¼ŒMASLegalBenchèƒ½å¤Ÿçªå‡ºä¸åŒæ¨¡å‹å’Œæ¶æ„çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•æœ‰åŠ©äºå‘ç°ç°æœ‰æ¨¡å‹çš„æ½œåœ¨æ”¹è¿›æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24922">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e93571cc23d25a4bb3c0f596f6b0e453~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912045&auth_key=1759912045-0-0-aa7ded8563702d052998a1b490c12e2a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3eb948dffc08ca802c30bb8fd153ea01~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912052&auth_key=1759912052-0-0-f32400d5e34a93c50671a8a9302fa7d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-b62205ba6971dcc12fc0223bf2aa7b18.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-1832f383f430bd17a1c470d4b27c6262~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912066&auth_key=1759912066-0-0-d308f308821cbd80fd37ca2c23d623c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="StreamForest-Efficient-Online-Video-Understanding-with-Persistent-Event-Memory"><a href="#StreamForest-Efficient-Online-Video-Understanding-with-Persistent-Event-Memory" class="headerlink" title="StreamForest: Efficient Online Video Understanding with Persistent Event   Memory"></a>StreamForest: Efficient Online Video Understanding with Persistent Event   Memory</h2><p><strong>Authors:Xiangyu Zeng, Kefan Qiu, Qingyu Zhang, Xinhao Li, Jing Wang, Jiaxin Li, Ziang Yan, Kun Tian, Meng Tian, Xinhai Zhao, Yi Wang, Limin Wang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have recently achieved remarkable progress in video understanding. However, their effectiveness in real-time streaming scenarios remains limited due to storage constraints of historical visual features and insufficient real-time spatiotemporal reasoning. To address these challenges, we propose StreamForest, a novel architecture specifically designed for streaming video understanding. Central to StreamForest is the Persistent Event Memory Forest, a memory mechanism that adaptively organizes video frames into multiple event-level tree structures. This process is guided by penalty functions based on temporal distance, content similarity, and merge frequency, enabling efficient long-term memory retention under limited computational resources. To enhance real-time perception, we introduce a Fine-grained Spatiotemporal Window, which captures detailed short-term visual cues to improve current scene perception. Additionally, we present OnlineIT, an instruction-tuning dataset tailored for streaming video tasks. OnlineIT significantly boosts MLLM performance in both real-time perception and future prediction. To evaluate generalization in practical applications, we introduce ODV-Bench, a new benchmark focused on real-time streaming video understanding in autonomous driving scenarios. Experimental results demonstrate that StreamForest achieves the state-of-the-art performance, with accuracies of 77.3% on StreamingBench, 60.5% on OVBench, and 55.6% on OVO-Bench. In particular, even under extreme visual token compression (limited to 1024 tokens), the model retains 96.8% of its average accuracy in eight benchmarks relative to the default setting. These results underscore the robustness, efficiency, and generalizability of StreamForest for streaming video understanding. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢æœ€è¿‘å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å®æ—¶æµåª’ä½“åœºæ™¯ä¸­çš„åº”ç”¨æ•ˆæœä»ç„¶æœ‰é™ï¼Œä¸»è¦æ˜¯ç”±äºå†å²è§†è§‰ç‰¹å¾çš„å­˜å‚¨é™åˆ¶å’Œå®æ—¶æ—¶ç©ºæ¨ç†çš„ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸“é—¨ç”¨äºæµåª’ä½“è§†é¢‘ç†è§£çš„å…¨æ–°æ¶æ„StreamForestã€‚StreamForestçš„æ ¸å¿ƒæ˜¯æŒä¹…äº‹ä»¶è®°å¿†æ£®æ—ï¼Œè¿™æ˜¯ä¸€ç§è®°å¿†æœºåˆ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°å°†è§†é¢‘å¸§ç»„ç»‡æˆå¤šä¸ªäº‹ä»¶çº§æ ‘ç»“æ„ã€‚è¿™ä¸€è¿‡ç¨‹ç”±åŸºäºæ—¶é—´è·ç¦»ã€å†…å®¹ç›¸ä¼¼æ€§å’Œåˆå¹¶é¢‘ç‡çš„æƒ©ç½šå‡½æ•°å¼•å¯¼ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹å®ç°é«˜æ•ˆçš„é•¿æ—¶è®°å¿†ä¿ç•™ã€‚ä¸ºäº†æé«˜å®æ—¶æ„ŸçŸ¥èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»†ç²’åº¦æ—¶ç©ºçª—å£ï¼Œèƒ½å¤Ÿæ•æ‰çŸ­æœŸçš„è¯¦ç»†è§†è§‰çº¿ç´¢ï¼Œä»¥æé«˜å¯¹å½“å‰åœºæ™¯çš„è®¤çŸ¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†OnlineITï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæµåª’ä½“è§†é¢‘ä»»åŠ¡å®šåˆ¶çš„æŒ‡å¯¼æ€§è°ƒæ•´æ•°æ®é›†ã€‚OnlineITæ˜¾è‘—æå‡äº†MLLMåœ¨å®æ—¶æ„ŸçŸ¥å’Œæœªæ¥é¢„æµ‹æ–¹é¢çš„æ€§èƒ½ã€‚ä¸ºäº†è¯„ä¼°åœ¨å®é™…åº”ç”¨ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ODV-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­å®æ—¶æµåª’ä½“è§†é¢‘ç†è§£çš„æ–°åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStreamForestè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨StreamingBenchä¸Šçš„å‡†ç¡®ç‡ä¸º77.3%ï¼Œåœ¨OVBenchä¸Šçš„å‡†ç¡®ç‡ä¸º60.5%ï¼Œåœ¨OVO-Benchä¸Šçš„å‡†ç¡®ç‡ä¸º55.6%ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå³ä½¿åœ¨æç«¯è§†è§‰ä»¤ç‰Œå‹ç¼©ï¼ˆé™åˆ¶ä¸º1024ä¸ªä»¤ç‰Œï¼‰çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¹³å‡å‡†ç¡®ç‡ä»ä¿æŒåœ¨é»˜è®¤è®¾ç½®çš„96.8%ã€‚è¿™äº›ç»“æœè¯æ˜äº†StreamForeståœ¨æµåª’ä½“è§†é¢‘ç†è§£æ–¹é¢çš„ç¨³å¥æ€§ã€æ•ˆç‡å’Œé€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24871v1">PDF</a> Accepted as a Spotlight at NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å®æ—¶æµåª’ä½“åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶æœ‰é™ï¼Œä¸»è¦ç”±äºå†å²è§†è§‰ç‰¹å¾çš„å­˜å‚¨çº¦æŸå’Œå®æ—¶æ—¶ç©ºæ¨ç†ä¸è¶³ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸“ä¸ºæµåª’ä½“è§†é¢‘ç†è§£è®¾è®¡çš„å…¨æ–°æ¶æ„StreamForestã€‚å…¶æ ¸å¿ƒæ˜¯æŒä¹…äº‹ä»¶è®°å¿†æ£®æ—ï¼Œä¸€ç§è®°å¿†æœºåˆ¶ï¼Œå¯è‡ªé€‚åº”åœ°å°†è§†é¢‘å¸§ç»„ç»‡æˆå¤šä¸ªäº‹ä»¶çº§åˆ«çš„æ ‘ç»“æ„ã€‚æ­¤è¿‡ç¨‹ç”±åŸºäºæ—¶é—´è·ç¦»ã€å†…å®¹ç›¸ä¼¼æ€§å’Œåˆå¹¶é¢‘ç‡çš„æƒ©ç½šå‡½æ•°å¼•å¯¼ï¼Œå¯åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹å®ç°é«˜æ•ˆçš„é•¿æ—¶è®°å¿†ä¿ç•™ã€‚ä¸ºæé«˜å®æ—¶æ„ŸçŸ¥èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»†ç²’åº¦æ—¶ç©ºçª—å£ï¼Œå¯æ•æ‰çŸ­æœŸè§†è§‰çº¿ç´¢ï¼Œæé«˜å½“å‰åœºæ™¯æ„ŸçŸ¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†é’ˆå¯¹æµåª’ä½“è§†é¢‘ä»»åŠ¡çš„OnlineITæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚OnlineITå¯æ˜¾è‘—æé«˜MLLMåœ¨å®æ—¶æ„ŸçŸ¥å’Œæœªæ¥é¢„æµ‹æ–¹é¢çš„æ€§èƒ½ã€‚ä¸ºè¯„ä¼°åœ¨å®é™…åº”ç”¨ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ODV-Benchæ–°åŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯çš„å®æ—¶æµåª’ä½“è§†é¢‘ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStreamForeståœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†ä¸šç•Œå…ˆè¿›æ°´å¹³ï¼Œå…¶ä¸­StreamingBenchä¸Šçš„å‡†ç¡®ç‡ä¸º77.3%ï¼ŒOVBenchä¸Šä¸º60.5%ï¼ŒOVO-Benchä¸Šä¸º55.6%ã€‚å³ä½¿åœ¨æç«¯è§†è§‰ä»¤ç‰Œå‹ç¼©ï¼ˆé™åˆ¶ä¸º1024ä¸ªä»¤ç‰Œï¼‰ä¸‹ï¼Œæ¨¡å‹ç›¸å¯¹äºé»˜è®¤è®¾ç½®åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¹³å‡å‡†ç¡®ç‡ä»ä¿æŒåœ¨96.8%ã€‚è¿™äº›ç»“æœè¯æ˜äº†StreamForeståœ¨æµåª’ä½“è§†é¢‘ç†è§£æ–¹é¢çš„ç¨³å¥æ€§ã€æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å®æ—¶æµåª’ä½“åœºæ™¯ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>StreamForestæ¶æ„é€šè¿‡æŒä¹…äº‹ä»¶è®°å¿†æ£®æ—å’Œç»†ç²’åº¦æ—¶ç©ºçª—å£è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>æŒä¹…äº‹ä»¶è®°å¿†æ£®æ—èƒ½è‡ªé€‚åº”åœ°ç»„ç»‡è§†é¢‘å¸§ï¼Œé€šè¿‡æƒ©ç½šå‡½æ•°å®ç°é«˜æ•ˆé•¿æ—¶è®°å¿†ã€‚</li>
<li>OnlineITæ•°æ®é›†æé«˜äº†MLLMåœ¨å®æ—¶æ„ŸçŸ¥å’Œæœªæ¥é¢„æµ‹æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>ODV-BenchåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°å®æ—¶æµåª’ä½“è§†é¢‘ç†è§£çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>StreamForeståœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°ä¸šç•Œå…ˆè¿›æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bb53fef302d65d603a02ada895667799~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912073&auth_key=1759912073-0-0-f3a4d427b7591c8ce42341475d756289&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-aaa22b2321dd5ecddd6a1c31fbb32ac0.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-63b3041af357c93c471a88f3ab2c5360~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912088&auth_key=1759912088-0-0-9af2fa5cb3aa31ce621fefec818edc9b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-65d4d10f40cd27e1c87e514eb76d895a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912095&auth_key=1759912095-0-0-dfcca1e61750b3319fff8008b3548a8c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Retro-Optimizing-LLMs-for-Reasoning-Intensive-Document-Retrieval"><a href="#Retro-Optimizing-LLMs-for-Reasoning-Intensive-Document-Retrieval" class="headerlink" title="Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval"></a>Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval</h2><p><strong>Authors:Junwei Lan, Jianlyu Chen, Zheng Liu, Chaofan Li, Siqi Bao, Defu Lian</strong></p>
<p>With the growing popularity of LLM agents and RAG, it has become increasingly important to retrieve documents that are essential for solving a task, even when their connection to the task is indirect or implicit. Addressing this problem requires fine-grained reasoning to accurately assess the relevance between the task and each candidate document. This capability, however, poses a significant challenge for existing IR techniques. Despite recent progress in reasoning-enhanced IR, existing approaches still face significant challenges in applicability, scalability, and efficiency. In this work, we propose Retro*, a novel approach for reasoning-intensive document retrieval. Our method introduces a rubric-based relevance scoring mechanism, enabling the model to reason about the relationship between a task and a document based on explicitly defined criteria, whereby producing a fine-grained, interpretable relevance score. Retro* also supports test-time scaling by combining multiple reasoning trajectories via score integration, which produces more reliable relevance estimates. To optimize Retro*â€™s reasoning capabilities, we introduce a novel reinforcement learning algorithm tailored for its relevance scoring mechanism, which employs two composite rewards to fully exploit the trajectories of each training sample. Our experiments show that Retro* outperforms existing document retrieval methods with notable advantages, leading to state-of-the-art performance on the BRIGHT benchmark. </p>
<blockquote>
<p>éšç€LLMä»£ç†å’ŒRAGçš„æ™®åŠï¼Œæ£€ç´¢å¯¹å®Œæˆä»»åŠ¡è‡³å…³é‡è¦çš„æ–‡æ¡£å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œå³ä½¿è¿™äº›æ–‡æ¡£ä¸ä»»åŠ¡çš„å…³è”æ˜¯é—´æ¥æˆ–éšå«çš„ã€‚è§£å†³è¿™ä¸ªé—®é¢˜éœ€è¦è¿›è¡Œç²¾ç»†çš„æ¨ç†ï¼Œä»¥å‡†ç¡®è¯„ä¼°ä»»åŠ¡ä¸æ¯ä¸ªå€™é€‰æ–‡æ¡£ä¹‹é—´çš„ç›¸å…³æ€§ã€‚ç„¶è€Œï¼Œè¿™ä¸€èƒ½åŠ›ä¸ºç°æœ‰çš„ä¿¡æ¯æ£€ç´¢æŠ€æœ¯å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡æœ€è¿‘åœ¨å¢å¼ºæ¨ç†çš„ä¿¡æ¯æ£€ç´¢æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨åº”ç”¨ã€å¯æ‰©å±•æ€§å’Œæ•ˆç‡æ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Retro<em>ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¯†é›†æ¨ç†çš„æ–‡æ¡£æ£€ç´¢çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§åŸºäºè§„åˆ™çš„å…³è”è¯„åˆ†æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ˜ç¡®å®šä¹‰çš„å‡†åˆ™æ¥æ¨ç†ä»»åŠ¡ä¸æ–‡æ¡£ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œäº§ç”Ÿç²¾ç»†ä¸”å¯è§£é‡Šçš„å…³è”è¯„åˆ†ã€‚Retro</em>è¿˜æ”¯æŒæµ‹è¯•æ—¶çš„ç¼©æ”¾ï¼Œé€šè¿‡å¾—åˆ†é›†æˆç»“åˆå¤šä¸ªæ¨ç†è½¨è¿¹ï¼Œä»è€Œäº§ç”Ÿæ›´å¯é çš„å…³è”ä¼°è®¡ã€‚ä¸ºäº†ä¼˜åŒ–Retro<em>çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸ºå…¶å…³è”è¯„åˆ†æœºåˆ¶é‡èº«å®šåˆ¶äº†ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•é‡‡ç”¨ä¸¤ç§ç»„åˆå¥–åŠ±ï¼Œä»¥å……åˆ†åˆ©ç”¨æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„è½¨è¿¹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRetro</em>åœ¨æ–‡æ¡£æ£€ç´¢æ–¹æ³•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24869v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¸­æå‡ºäº†ä¸€ç§åä¸ºRetro<em>çš„æ–°å‹æ¨ç†å¯†é›†å‹æ–‡æ¡£æ£€ç´¢æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§åŸºäºè§„åˆ™çš„å…³è”è¯„åˆ†æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ˜ç¡®çš„æ ‡å‡†å¯¹ä»»åŠ¡ä¸æ–‡æ¡£ä¹‹é—´çš„å…³ç³»è¿›è¡Œæ¨ç†ï¼Œäº§ç”Ÿç²¾ç»†ä¸”å¯è§£é‡Šçš„å…³è”è¯„åˆ†ã€‚æ­¤å¤–ï¼ŒRetro</em>æ”¯æŒæµ‹è¯•æ—¶çš„ç¼©æ”¾ï¼Œé€šè¿‡æ•´åˆå¤šä¸ªæ¨ç†è½¨è¿¹æ¥æé«˜å¯é æ€§ã€‚ä¸ºä¼˜åŒ–å…¶æ¨ç†èƒ½åŠ›ï¼Œæ–‡æœ¬è¿˜ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å…¶å…³è”è¯„åˆ†æœºåˆ¶çš„å®šåˆ¶å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•é‡‡ç”¨ä¸¤ç§ç»„åˆå¥–åŠ±ä»¥å……åˆ†åˆ©ç”¨æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„è½¨è¿¹ã€‚å®éªŒè¡¨æ˜ï¼ŒRetro*åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰æ–‡æ¡£æ£€ç´¢æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å¼ºè°ƒäº†éšç€LLMä»£ç†å’ŒRAGçš„æ™®åŠï¼Œå‡†ç¡®æ£€ç´¢ä¸ä»»åŠ¡é—´æ¥æˆ–éšå«ç›¸å…³çš„å…³é”®æ–‡æ¡£å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚</li>
<li>ç°æœ‰IRæŠ€æœ¯åœ¨é¢å¯¹è¿™ç§éœ€æ±‚æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦ç²¾ç»†çš„æ¨ç†æ¥è¯„ä¼°ä»»åŠ¡ä¸å€™é€‰æ–‡æ¡£ä¹‹é—´çš„å…³è”æ€§ã€‚</li>
<li>Retro*æ˜¯ä¸€ç§æ–°å‹çš„æ¨ç†å¯†é›†å‹æ–‡æ¡£æ£€ç´¢æ–¹æ³•ï¼Œå¼•å…¥äº†ä¸€ç§åŸºäºè§„åˆ™çš„å…³è”è¯„åˆ†æœºåˆ¶ï¼Œä»¥æ˜ç¡®çš„æ ‡å‡†è¿›è¡Œæ¨ç†ã€‚</li>
<li>Retro*æ”¯æŒæµ‹è¯•æ—¶çš„ç¼©æ”¾ï¼Œé€šè¿‡æ•´åˆå¤šä¸ªæ¨ç†è½¨è¿¹æ¥æé«˜å…³è”è¯„åˆ†çš„å¯é æ€§ã€‚</li>
<li>ä¸ºä¼˜åŒ–Retro*çš„æ¨ç†èƒ½åŠ›ï¼Œæ–‡æœ¬ä»‹ç»äº†ä¸€ç§å®šåˆ¶å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•é‡‡ç”¨ä¸¤ç§ç»„åˆå¥–åŠ±æ¥å……åˆ†åˆ©ç”¨è®­ç»ƒæ ·æœ¬çš„è½¨è¿¹ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRetro*åœ¨æ–‡æ¡£æ£€ç´¢æ–¹é¢çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨BRIGHTNESSåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°çªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24869">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-efea5d51c9e5841c8af1a4dfff4e3200.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-40eae616b076ebe915e3b679215d68c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912110&auth_key=1759912110-0-0-dbf8f703d7a27621cc309e05fb498ac5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e7e2bf63e1e76eddb609fbae314d7995~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912116&auth_key=1759912116-0-0-47126a05213cd7bdbccddb3836d11429&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8aed53e3f7d549ce8066d7b2a3b87e94~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912124&auth_key=1759912124-0-0-6aa806d088c42f11945126ad1961d885&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Between-Help-and-Harm-An-Evaluation-of-Mental-Health-Crisis-Handling-by-LLMs"><a href="#Between-Help-and-Harm-An-Evaluation-of-Mental-Health-Crisis-Handling-by-LLMs" class="headerlink" title="Between Help and Harm: An Evaluation of Mental Health Crisis Handling by   LLMs"></a>Between Help and Harm: An Evaluation of Mental Health Crisis Handling by   LLMs</h2><p><strong>Authors:Adrian Arnaiz-Rodriguez, Miguel Baidal, Erik Derner, Jenn Layton Annable, Mark Ball, Mark Ince, Elvira Perez Vallejos, Nuria Oliver</strong></p>
<p>The widespread use of chatbots powered by large language models (LLMs) such as ChatGPT and Llama has fundamentally reshaped how people seek information and advice across domains. Increasingly, these chatbots are being used in high-stakes contexts, including emotional support and mental health concerns. While LLMs can offer scalable support, their ability to safely detect and respond to acute mental health crises remains poorly understood. Progress is hampered by the absence of unified crisis taxonomies, robust annotated benchmarks, and empirical evaluations grounded in clinical best practices. In this work, we address these gaps by introducing a unified taxonomy of six clinically-informed mental health crisis categories, curating a diverse evaluation dataset, and establishing an expert-designed protocol for assessing response appropriateness. We systematically benchmark three state-of-the-art LLMs for their ability to classify crisis types and generate safe, appropriate responses. The results reveal that while LLMs are highly consistent and generally reliable in addressing explicit crisis disclosures, significant risks remain. A non-negligible proportion of responses are rated as inappropriate or harmful, with responses generated by an open-weight model exhibiting higher failure rates than those generated by the commercial ones. We also identify systemic weaknesses in handling indirect or ambiguous risk signals, a reliance on formulaic and inauthentic default replies, and frequent misalignment with user context. These findings underscore the urgent need for enhanced safeguards, improved crisis detection, and context-aware interventions in LLM deployments. Our taxonomy, datasets, and evaluation framework lay the groundwork for ongoing research and responsible innovation in AI-driven mental health support, helping to minimize harm and better protect vulnerable users. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„èŠå¤©æœºå™¨äººï¼ˆå¦‚ChatGPTå’ŒLlamaï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œäººä»¬åœ¨å„é¢†åŸŸå¯»æ±‚ä¿¡æ¯å’Œå»ºè®®çš„æ–¹å¼å‘ç”Ÿäº†æ ¹æœ¬æ€§çš„å˜åŒ–ã€‚è¿™äº›èŠå¤©æœºå™¨äººè¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºé«˜é£é™©æƒ…å¢ƒï¼ŒåŒ…æ‹¬æƒ…æ„Ÿæ”¯æŒå’Œå¿ƒç†å¥åº·é—®é¢˜ã€‚è™½ç„¶LLMå¯ä»¥æä¾›å¯æ‰©å±•çš„æ”¯æŒï¼Œä½†å®ƒä»¬åœ¨å®‰å…¨æ£€æµ‹å’Œåº”å¯¹æ€¥æ€§å¿ƒç†å¥åº·å±æœºæ–¹é¢çš„èƒ½åŠ›ä»çŸ¥ä¹‹ç”šå°‘ã€‚ç¼ºä¹ç»Ÿä¸€çš„å±æœºåˆ†ç±»ã€ç¨³å¥çš„æ³¨é‡ŠåŸºå‡†å’ŒåŸºäºä¸´åºŠæœ€ä½³å®è·µçš„å®è¯è¯„ä¼°é˜»ç¢äº†è¿™ä¸€é¢†åŸŸçš„è¿›å±•ã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ä¸ªåŒ…å«å…­ä¸ªä¸´åºŠä¿¡æ¯çš„å¿ƒç†å¥åº·å±æœºç±»åˆ«ç»Ÿä¸€åˆ†ç±»ã€ç­–åˆ’ä¸€ä¸ªå¤šæ ·åŒ–çš„è¯„ä¼°æ•°æ®é›†ä»¥åŠåˆ¶å®šä¸“å®¶è®¾è®¡çš„è¯„ä¼°å“åº”é€‚å½“æ€§çš„åè®®æ¥å¡«è¡¥è¿™äº›ç©ºç™½ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸‰ç§æœ€å…ˆè¿›çš„LLMåœ¨åˆ†ç±»å±æœºç±»å‹å’Œç”Ÿæˆå®‰å…¨é€‚å½“å“åº”æ–¹é¢çš„èƒ½åŠ›ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24857v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šèŠå¤©æœºå™¨äººå› é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ChatGPTå’ŒLlamaï¼‰è€Œå¹¿æ³›ä½¿ç”¨ï¼Œæ”¹å˜äº†äººä»¬è·¨åŸŸè·å–ä¿¡æ¯å’Œå»ºè®®çš„æ–¹å¼ã€‚è¿™äº›èŠå¤©æœºå™¨äººè¶Šæ¥è¶Šå¤šåœ°åº”ç”¨äºé«˜é£é™©çš„æƒ…å¢ƒï¼Œå¦‚æƒ…æ„Ÿæ”¯æŒå’Œå¿ƒç†å¥åº·é—®é¢˜ã€‚ç„¶è€Œï¼Œå…³äºè¿™äº›æ¨¡å‹å¦‚ä½•å®‰å…¨åœ°æ£€æµ‹å’Œåº”å¯¹æ€¥æ€§å¿ƒç†å¥åº·å±æœºçš„çŸ¥è¯†ä»ç„¶æœ‰é™ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥ç»Ÿä¸€çš„å¿ƒç†å¥åº·å±æœºåˆ†ç±»æ³•ã€å»ºç«‹è¯„ä¼°æ•°æ®é›†å’Œåˆ¶å®šè¯„ä¼°å“åº”é€‚å½“æ€§çš„ä¸“å®¶è®¾è®¡åè®®æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸‰ç§æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹åœ¨åˆ†ç±»å±æœºç±»å‹å’Œç”Ÿæˆå®‰å…¨å“åº”æ–¹é¢çš„èƒ½åŠ›ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†æ˜ç¡®çš„å±æœºæŠ«éœ²æ—¶è¡¨ç°å‡ºé«˜åº¦ä¸€è‡´æ€§å’Œå¯é æ€§ï¼Œä½†ä»å­˜åœ¨é‡å¤§é£é™©ã€‚éå¾®ä¸è¶³é“çš„å“åº”æ¯”ä¾‹è¢«è¯„ä¸ºä¸é€‚å½“æˆ–æœ‰å®³ï¼Œç”±å¼€æ”¾å¼æƒé‡æ¨¡å‹ç”Ÿæˆçš„å“åº”å¤±è´¥ç‡é«˜äºå•†ä¸šæ¨¡å‹ç”Ÿæˆçš„å“åº”ã€‚æˆ‘ä»¬è¿˜å‘ç°äº†å¤„ç†é—´æ¥æˆ–æ¨¡ç³Šé£é™©ä¿¡å·çš„ä½“ç³»æ€§å¼±ç‚¹ï¼Œå¯¹å…¬å¼åŒ–å’ŒéçœŸå®çš„é»˜è®¤å›å¤çš„ä¾èµ–ï¼Œä»¥åŠä¸ç”¨æˆ·æƒ…å¢ƒçš„é¢‘ç¹ä¸åŒ¹é…ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å¯¹å¢å¼ºä¿éšœæªæ–½ã€æ”¹è¿›å±æœºæ£€æµ‹å’Œæƒ…å¢ƒæ„ŸçŸ¥å¹²é¢„çš„è¿«åˆ‡éœ€æ±‚ã€‚æˆ‘ä»¬çš„åˆ†ç±»æ³•ã€æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ä¸ºäººå·¥æ™ºèƒ½é©±åŠ¨çš„å¿ƒç†å¥åº·æ”¯æŒçš„æŒç»­ç ”ç©¶å’Œè´Ÿè´£ä»»åˆ›æ–°å¥ å®šäº†åŸºç¡€ï¼Œæœ‰åŠ©äºå‡å°‘ä¼¤å®³å¹¶æ›´å¥½åœ°ä¿æŠ¤è„†å¼±ç”¨æˆ·ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„èŠå¤©æœºå™¨äººå·²å¹¿æ³›åº”ç”¨äºä¿¡æ¯è·å–å’Œå»ºè®®é¢†åŸŸï¼Œå¯¹äººä»¬çš„ä¿¡æ¯è·å–æ–¹å¼äº§ç”Ÿäº†æ·±è¿œå½±å“ã€‚</li>
<li>åœ¨é«˜é£é™©çš„æƒ…å¢ƒå¦‚æƒ…æ„Ÿæ”¯æŒå’Œå¿ƒç†å¥åº·é—®é¢˜æ–¹é¢ï¼Œè¿™äº›èŠå¤©æœºå™¨äººçš„åº”ç”¨æ—¥ç›Šå¢å¤šã€‚</li>
<li>å½“å‰å¯¹äºè¯­è¨€æ¨¡å‹å¤„ç†æ€¥æ€§å¿ƒç†å¥åº·å±æœºçš„èƒ½åŠ›äº†è§£æœ‰é™ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¿ƒç†å¥åº·å±æœºåˆ†ç±»æ³•ï¼ŒåŒ…å«å…­ä¸ªåŸºäºä¸´åºŠä¿¡æ¯çš„ç±»åˆ«ã€‚</li>
<li>ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªè¯„ä¼°æ•°æ®é›†å’Œä¸“å®¶è®¾è®¡åè®®æ¥è¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨å¿ƒç†å¥åº·å±æœºåº”å¯¹æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œè¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ˜ç¡®çš„å±æœºä¿¡æ¯æ—¶è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä½†åœ¨å¤„ç†é—´æ¥æˆ–æ¨¡ç³Šçš„é£é™©ä¿¡å·æ–¹é¢å­˜åœ¨å¼±ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ece14688791a08563aa5fd0408d944b1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912133&auth_key=1759912133-0-0-1f27b4ed1898594f8ea40f67eeffe03e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a8e7342d6a3afece136d027d657ccfee~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912140&auth_key=1759912140-0-0-492e83737675c2eafe35c1db5f2dc42b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb4c450c45666ed18963f268f41b1791~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912146&auth_key=1759912146-0-0-d6dd64afdca825209b853f5bc5d41c1c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-85d72600f8df4a12ac15807d9554070b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912153&auth_key=1759912153-0-0-3f84120937246b2ffb76ae411f74bfd4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="KnowGuard-Knowledge-Driven-Abstention-for-Multi-Round-Clinical-Reasoning"><a href="#KnowGuard-Knowledge-Driven-Abstention-for-Multi-Round-Clinical-Reasoning" class="headerlink" title="KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical   Reasoning"></a>KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical   Reasoning</h2><p><strong>Authors:Xilin Dang, Kexin Chen, Xiaorui Su, Ayush Noori, IÃ±aki Arango, Lucas Vittor, Xinyi Long, Yuyang Du, Marinka Zitnik, Pheng Ann Heng</strong></p>
<p>In clinical practice, physicians refrain from making decisions when patient information is insufficient. This behavior, known as abstention, is a critical safety mechanism preventing potentially harmful misdiagnoses. Recent investigations have reported the application of large language models (LLMs) in medical scenarios. However, existing LLMs struggle with the abstentions, frequently providing overconfident responses despite incomplete information. This limitation stems from conventional abstention methods relying solely on model self-assessments, which lack systematic strategies to identify knowledge boundaries with external medical evidences. To address this, we propose \textbf{KnowGuard}, a novel \textit{investigate-before-abstain} paradigm that integrates systematic knowledge graph exploration for clinical decision-making. Our approach consists of two key stages operating on a shared contextualized evidence pool: 1) an evidence discovery stage that systematically explores the medical knowledge space through graph expansion and direct retrieval, and 2) an evidence evaluation stage that ranks evidence using multiple factors to adapt exploration based on patient context and conversation history. This two-stage approach enables systematic knowledge graph exploration, allowing models to trace structured reasoning paths and recognize insufficient medical evidence. We evaluate our abstention approach using open-ended multi-round clinical benchmarks that mimic realistic diagnostic scenarios, assessing abstention quality through accuracy-efficiency trade-offs beyond existing closed-form evaluations. Experimental evidences clearly demonstrate that KnowGuard outperforms state-of-the-art abstention approaches, improving diagnostic accuracy by 3.93% while reducing unnecessary interaction by 7.27 turns on average. </p>
<blockquote>
<p>åœ¨ä¸´åºŠå®è·µä¸­ï¼Œå½“ç—…äººä¿¡æ¯ä¸è¶³æ—¶ï¼ŒåŒ»ç”Ÿé¿å…åšå‡ºå†³ç­–ã€‚è¿™ç§è¡Œä¸ºè¢«ç§°ä¸ºå¼ƒæƒï¼Œæ˜¯ä¸€ç§å…³é”®çš„å®‰å…¨æœºåˆ¶ï¼Œå¯ä»¥é˜²æ­¢æ½œåœ¨çš„æœ‰å®³è¯¯è¯Šã€‚æœ€è¿‘çš„è°ƒæŸ¥å·²ç»æŠ¥é“äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMåœ¨å¼ƒæƒæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç»å¸¸åœ¨ä¸å®Œæ•´çš„ä¿¡æ¯åŸºç¡€ä¸Šæä¾›è¿‡äºè‡ªä¿¡çš„å›åº”ã€‚è¿™ä¸€å±€é™æ€§æºäºä¼ ç»Ÿçš„å¼ƒæƒæ–¹æ³•ä»…ä¾èµ–äºæ¨¡å‹è‡ªæˆ‘è¯„ä¼°ï¼Œç¼ºä¹è¯†åˆ«çŸ¥è¯†è¾¹ç•Œä¸å¤–éƒ¨åŒ»å­¦è¯æ®çš„ç³»ç»Ÿç­–ç•¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†KnowGuardï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„â€œè°ƒæŸ¥åå†å¼ƒæƒâ€èŒƒå¼ï¼Œé›†æˆäº†ç³»ç»ŸçŸ¥è¯†å›¾è°±æ¢ç´¢ï¼Œç”¨äºä¸´åºŠå†³ç­–ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼Œåœ¨ä¸€ä¸ªå…±äº«çš„æƒ…æ™¯åŒ–è¯æ®æ± ä¸Šè¿è¡Œï¼š1ï¼‰è¯æ®å‘ç°é˜¶æ®µï¼Œé€šè¿‡å›¾è°±æ‰©å±•å’Œç›´æ¥æ£€ç´¢ç³»ç»Ÿåœ°æ¢ç´¢åŒ»ç–—çŸ¥è¯†ç©ºé—´ï¼›2ï¼‰è¯æ®è¯„ä¼°é˜¶æ®µï¼Œä½¿ç”¨å¤šç§å› ç´ å¯¹è¯æ®è¿›è¡Œæ’åï¼Œæ ¹æ®ç—…äººæƒ…æ™¯å’Œå¯¹è¯å†å²è°ƒæ•´æ¢ç´¢ã€‚è¿™ä¸¤ä¸ªé˜¶æ®µçš„ç­–ç•¥ä½¿ç³»ç»Ÿèƒ½å¤Ÿè¿›è¡Œç³»ç»Ÿçš„çŸ¥è¯†å›¾è°±æ¢ç´¢ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¿½è¸ªç»“æ„åŒ–æ¨ç†è·¯å¾„å¹¶è¯†åˆ«åŒ»ç–—è¯æ®ä¸è¶³ã€‚æˆ‘ä»¬ä½¿ç”¨å¼€æ”¾å¼å¤šè½®ä¸´åºŠåŸºå‡†æµ‹è¯•è¯„ä¼°æˆ‘ä»¬çš„å¼ƒæƒæ–¹æ³•ï¼Œæ¨¡æ‹ŸçœŸå®çš„è¯Šæ–­åœºæ™¯ï¼Œé€šè¿‡ç°æœ‰çš„å°é—­å¼è¯„ä¼°ä¹‹å¤–çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡æƒè¡¡æ¥è¯„ä¼°å¼ƒæƒè´¨é‡ã€‚å®éªŒè¯æ®æ¸…æ¥šåœ°è¡¨æ˜ï¼ŒKnowGuardä¼˜äºæœ€æ–°çš„å¼ƒæƒæ–¹æ³•ï¼Œæé«˜äº†è¯Šæ–­å‡†ç¡®æ€§è¾¾3.93%ï¼ŒåŒæ—¶å¹³å‡å‡å°‘äº†ä¸å¿…è¦çš„äº¤äº’è¾¾7.27è½®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24816v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>åœ¨ä¸´åºŠå®è·µä¸­ï¼ŒåŒ»ç”Ÿåœ¨æ‚£è€…ä¿¡æ¯ä¸è¶³æ—¶é¿å…åšå‡ºå†³ç­–ï¼Œè¿™ç§è¡Œä¸ºç§°ä¸ºå¼ƒæƒï¼Œæ˜¯ä¸€ç§å…³é”®çš„å®‰å…¨æœºåˆ¶ï¼Œé˜²æ­¢æ½œåœ¨çš„æœ‰å®³è¯¯è¯Šã€‚æœ€è¿‘çš„ç ”ç©¶æŠ¥é“äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œä½†ç°æœ‰LLMsåœ¨å¼ƒæƒæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç»å¸¸åœ¨ä¸å®Œæ•´ä¿¡æ¯çš„æƒ…å†µä¸‹æä¾›è¿‡äºè‡ªä¿¡çš„å›åº”ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†KnowGuardï¼Œä¸€ç§æ–°å‹çš„â€œè°ƒæŸ¥åå†å¼ƒæƒâ€èŒƒå¼ï¼Œé›†æˆç³»ç»ŸçŸ¥è¯†å›¾è°±æ¢ç´¢ç”¨äºä¸´åºŠå†³ç­–ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼šè¯æ®å‘ç°é˜¶æ®µå’Œè¯æ®è¯„ä¼°é˜¶æ®µã€‚å‰è€…é€šè¿‡å›¾è°±æ‰©å±•å’Œç›´æ¥æ£€ç´¢ç³»ç»Ÿåœ°æ¢ç´¢åŒ»ç–—çŸ¥è¯†ç©ºé—´ï¼Œåè€…åˆ™é€šè¿‡å¤šé‡å› ç´ æ’åè¯æ®ï¼Œä»¥é€‚åº”æ‚£è€…èƒŒæ™¯å’Œå¯¹è¯å†å²ã€‚è¿™ç§æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿè¿½è¸ªç»“æ„åŒ–æ¨ç†è·¯å¾„å¹¶è¯†åˆ«ä¸è¶³çš„åŒ»ç–—è¯æ®ã€‚å®éªŒè¯æ®è¡¨æ˜ï¼ŒKnowGuardä¼˜äºæœ€æ–°çš„å¼ƒæƒæ–¹æ³•ï¼Œåœ¨æé«˜è¯Šæ–­å‡†ç¡®ç‡çš„åŒæ—¶å‡å°‘ä¸å¿…è¦çš„äº¤äº’ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä¸´åºŠåŒ»ç”Ÿåœ¨ä¿¡æ¯ä¸è¶³æ—¶ä¼šé€‰æ‹©å¼ƒæƒï¼Œä»¥é¿å…æ½œåœ¨çš„æœ‰å®³è¯¯è¯Šã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—å†³ç­–ä¸­å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨å¼ƒæƒæ–¹é¢ã€‚</li>
<li>LLMså¸¸å¸¸åœ¨ç¼ºä¹å®Œæ•´ä¿¡æ¯çš„æƒ…å†µä¸‹ç»™å‡ºè¿‡äºè‡ªä¿¡çš„å›åº”ã€‚</li>
<li>KnowGuardæ˜¯ä¸€ç§æ–°å‹çš„â€œè°ƒæŸ¥åå†å¼ƒæƒâ€æ–¹æ³•ï¼Œé›†æˆç³»ç»ŸçŸ¥è¯†å›¾è°±æ¢ç´¢ç”¨äºä¸´åºŠå†³ç­–ã€‚</li>
<li>KnowGuardåŒ…æ‹¬è¯æ®å‘ç°é˜¶æ®µå’Œè¯æ®è¯„ä¼°é˜¶æ®µï¼Œåˆ†åˆ«é€šè¿‡å›¾è°±æ¢ç´¢å’Œå¤šé‡å› ç´ æ’åè¯æ®æ¥é€‚åº”æ‚£è€…èƒŒæ™¯å’Œå¯¹è¯å†å²ã€‚</li>
<li>KnowGuardé€šè¿‡è¿½è¸ªç»“æ„åŒ–æ¨ç†è·¯å¾„å’Œè¯†åˆ«ä¸è¶³çš„åŒ»ç–—è¯æ®ï¼Œä¼˜åŒ–äº†è¯Šæ–­è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24816">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0b6a15db719df1f7fe52dc0da4b11c2b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912161&auth_key=1759912161-0-0-c2a3a2353f16711933d62e62f8b09573&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fce3c896df954c9c925667b01f17c8b1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912168&auth_key=1759912168-0-0-11827a1ecbaa023f1c7043f2bb3dc4d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9e01f6581b728ae769ab5b4114bfdb9b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912175&auth_key=1759912175-0-0-84c858b13e2cac853d87ea3c549a1d3c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LOVE-R1-Advancing-Long-Video-Understanding-with-an-Adaptive-Zoom-in-Mechanism-via-Multi-Step-Reasoning"><a href="#LOVE-R1-Advancing-Long-Video-Understanding-with-an-Adaptive-Zoom-in-Mechanism-via-Multi-Step-Reasoning" class="headerlink" title="LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in   Mechanism via Multi-Step Reasoning"></a>LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in   Mechanism via Multi-Step Reasoning</h2><p><strong>Authors:Shenghao Fu, Qize Yang, Yuan-Ming Li, Xihan Wei, Xiaohua Xie, Wei-Shi Zheng</strong></p>
<p>Long video understanding is still challenging for recent Large Video-Language Models (LVLMs) due to the conflict between long-form temporal understanding and detailed spatial perception. LVLMs with a uniform frame sampling mechanism, which samples frames with an equal frame size and fixed sampling rate, inevitably sacrifice either temporal clues or spatial details, resulting in suboptimal solutions. To mitigate this dilemma, we propose LOVE-R1, a model that can adaptively zoom in on a video clip. The model is first provided with densely sampled frames but in a small resolution. If some spatial details are needed, the model can zoom in on a clip of interest with a large frame resolution based on its reasoning until key visual information is obtained. The whole process is implemented as a multi-step reasoning process. To train the reasoning ability, we first finetune the model on our collected 38k high-quality CoT data and enhance it with decoupled reinforcement finetuning. As outcome rewards can not provide fine-grained process supervision, we decouple multi-step reasoning into multiple single-step reasoning and optimize the internal zoom-in ability explicitly. Experiments on long video understanding benchmarks show that our model with the slow-fast adaptive frame sampling mechanism achieves a great trade-off between sampling density and frame resolutions, and LOVE-R1 outperforms our baseline Qwen2.5-VL by an average of 3.1% points across 4 common long video understanding benchmarks. </p>
<blockquote>
<p>é•¿è§†é¢‘ç†è§£å¯¹äºå½“å‰çš„Large Video-Language Modelsï¼ˆLVLMsï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºé•¿æ—¶åºç†è§£ä¸è¯¦ç»†ç©ºé—´æ„ŸçŸ¥ä¹‹é—´çš„å†²çªã€‚é‡‡ç”¨ç»Ÿä¸€æ¡†æ¶é‡‡æ ·æœºåˆ¶çš„LVLMsï¼Œä»¥ç­‰å¤§çš„æ¡†æ¶å°ºå¯¸å’Œå›ºå®šçš„é‡‡æ ·ç‡è¿›è¡Œæ¡†æ¶é‡‡æ ·ï¼Œä¸å¯é¿å…åœ°ä¼šç‰ºç‰²æ—¶åºçº¿ç´¢æˆ–ç©ºé—´ç»†èŠ‚ï¼Œå¯¼è‡´ç»“æœä¸å°½äººæ„ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€å›°å¢ƒï¼Œæˆ‘ä»¬æå‡ºäº†LOVE-R1æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥è‡ªé€‚åº”åœ°å¯¹è§†é¢‘ç‰‡æ®µè¿›è¡Œç¼©æ”¾ã€‚è¯¥æ¨¡å‹é¦–å…ˆæ¥æ”¶å¯†é›†é‡‡æ ·çš„ä½åˆ†è¾¨ç‡å¸§ã€‚å¦‚æœéœ€è¦æŸäº›ç©ºé—´ç»†èŠ‚ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®å…¶æ¨ç†ï¼Œåœ¨è·å¾—å…³é”®è§†è§‰ä¿¡æ¯ä¹‹å‰ï¼Œå¯¹æ„Ÿå…´è¶£çš„ç‰‡æ®µè¿›è¡Œå¤§åˆ†è¾¨ç‡çš„ç¼©æ”¾ã€‚æ•´ä¸ªè¿‡ç¨‹è¢«å®ç°ä¸ºä¸€ä¸ªå¤šæ­¥æ¨ç†è¿‡ç¨‹ã€‚ä¸ºäº†è®­ç»ƒæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨æ”¶é›†çš„3.8ä¸‡æ¡é«˜è´¨é‡CoTæ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå¹¶é€šè¿‡è§£è€¦å¼ºåŒ–å¾®è°ƒæ¥å¢å¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚ç”±äºç»“æœå¥–åŠ±æ— æ³•æä¾›ç²¾ç»†çš„è¿‡ç¨‹ç›‘ç£ï¼Œæˆ‘ä»¬å°†å¤šæ­¥æ¨ç†è§£è€¦ä¸ºå¤šä¸ªå•æ­¥æ¨ç†ï¼Œå¹¶æ˜¾å¼ä¼˜åŒ–å†…éƒ¨çš„ç¼©æ”¾èƒ½åŠ›ã€‚åœ¨é•¿æœŸè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¸¦æœ‰å¿«æ…¢è‡ªé€‚åº”æ¡†æ¶é‡‡æ ·æœºåˆ¶çš„æ¨¡å‹åœ¨é‡‡æ ·å¯†åº¦å’Œæ¡†æ¶åˆ†è¾¨ç‡ä¹‹é—´å–å¾—äº†å¾ˆå¥½çš„å¹³è¡¡ï¼ŒLOVE-R1åœ¨å››ä¸ªå¸¸è§çš„é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šå¹³å‡è¶…å‡ºæˆ‘ä»¬çš„åŸºçº¿æ¨¡å‹Qwen2.5-VL 3.1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24786v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºLOVE-R1çš„è§†é¢‘ç†è§£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡è‡ªé€‚åº”ç¼©æ”¾è§†é¢‘ç‰‡æ®µæ¥è§£å†³å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘æ—¶çš„æŒ‘æˆ˜ã€‚æ¨¡å‹é¦–å…ˆä»¥è¾ƒå°çš„åˆ†è¾¨ç‡æä¾›å¯†é›†é‡‡æ ·å¸§ï¼Œç„¶åæ ¹æ®éœ€è¦æ”¾å¤§æ„Ÿå…´è¶£ç‰‡æ®µçš„åˆ†è¾¨ç‡æ¥è·å–å…³é”®è§†è§‰ä¿¡æ¯ã€‚é€šè¿‡æ”¶é›†çš„é«˜è´¨é‡è®¤çŸ¥æ ‘ï¼ˆCoTï¼‰æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä½¿ç”¨è§£è€¦å¼ºåŒ–å¾®è°ƒå¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚LOVE-R1æ¨¡å‹é‡‡ç”¨å¿«æ…¢è‡ªé€‚åº”å¸§é‡‡æ ·æœºåˆ¶ï¼Œåœ¨é‡‡æ ·å¯†åº¦å’Œå¸§åˆ†è¾¨ç‡ä¹‹é—´å–å¾—å¾ˆå¥½çš„å¹³è¡¡ï¼Œå¹¶åœ¨å››ä¸ªå¸¸è§çš„é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­å¹³å‡ä¼˜äºåŸºå‡†æ¨¡å‹Qwen2.5-VL 3.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘ç†è§£æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦åŒæ—¶å¤„ç†é•¿å½¢å¼çš„ä¸´æ—¶ç†è§£å’Œè¯¦ç»†çš„ç©ºé—´æ„ŸçŸ¥ã€‚</li>
<li>ç°æœ‰æ¨¡å‹çš„å‡åŒ€å¸§é‡‡æ ·æœºåˆ¶ç‰ºç‰²äº†æ—¶é—´çº¿ç´¢æˆ–ç©ºé—´ç»†èŠ‚ï¼Œå¯¼è‡´ç»“æœä¸ç†æƒ³ã€‚</li>
<li>LOVE-R1æ¨¡å‹é€šè¿‡è‡ªé€‚åº”ç¼©æ”¾è§†é¢‘ç‰‡æ®µæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå…è®¸æ¨¡å‹åœ¨éœ€è¦æ—¶è·å–æ›´è¯¦ç»†çš„è§†è§‰ä¿¡æ¯ã€‚</li>
<li>æ¨¡å‹ä½¿ç”¨å¯†é›†é‡‡æ ·å¸§ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ ¹æ®éœ€è¦æ”¾å¤§ç‰¹å®šç‰‡æ®µçš„åˆ†è¾¨ç‡ã€‚</li>
<li>æ¨¡å‹è®­ç»ƒåŒ…æ‹¬åœ¨æ”¶é›†çš„é«˜è´¨é‡è®¤çŸ¥æ ‘ï¼ˆCoTï¼‰æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶ä½¿ç”¨è§£è€¦å¼ºåŒ–å¾®è°ƒæ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLOVE-R1æ¨¡å‹çš„å¿«æ…¢è‡ªé€‚åº”å¸§é‡‡æ ·æœºåˆ¶åœ¨é‡‡æ ·å¯†åº¦å’Œå¸§åˆ†è¾¨ç‡ä¹‹é—´å–å¾—äº†å¾ˆå¥½çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24786">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b3dd2c76b7f653c06013b1e1ec7e1b55~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912182&auth_key=1759912182-0-0-1085f2ecedadc743e9b931df342bc355&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-8f3332b840dffb9a7ae18434e39187d4.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-b33639e52b284510aea547ef9808e164~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912196&auth_key=1759912196-0-0-7bbaf0275c1717352cd31bd776eb0edb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-369070260f6d81ae53aa69348e33139b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912203&auth_key=1759912203-0-0-1af263864b4c2837eaa1254521ecfcfc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VTPerception-R1-Enhancing-Multimodal-Reasoning-via-Explicit-Visual-and-Textual-Perceptual-Grounding"><a href="#VTPerception-R1-Enhancing-Multimodal-Reasoning-via-Explicit-Visual-and-Textual-Perceptual-Grounding" class="headerlink" title="VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and   Textual Perceptual Grounding"></a>VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and   Textual Perceptual Grounding</h2><p><strong>Authors:Yizhuo Ding, Mingkang Chen, Zhibang Feng, Tong Xiao, Wanying Qu, Wenqi Shao, Yanwei Fu</strong></p>
<p>Multimodal large language models (MLLMs) often struggle to ground reasoning in perceptual evidence. We present a systematic study of perception strategies-explicit, implicit, visual, and textual-across four multimodal benchmarks and two MLLMs. Our findings show that explicit perception, especially when paired with textual cues, consistently yields the best improvements, particularly for smaller models. Based on this insight, we propose VTPerception-R1, a unified two-stage framework that decouples perception from reasoning. Stage 1 introduces perception-augmented fine-tuning, and Stage 2 applies perception-aware reinforcement learning with novel visual, textual, and consistency rewards. Experiments demonstrate that VTPerception-R1 significantly improves reasoning accuracy and robustness across diverse tasks, offering a scalable and auditable solution for perception-grounded multimodal reasoning. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/yizhuoDi/VTPerceprion-R1">https://github.com/yizhuoDi/VTPerceprion-R1</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¾€å¾€éš¾ä»¥åœ¨æ„ŸçŸ¥è¯æ®ä¸­æ¨ç†ã€‚æˆ‘ä»¬è¿›è¡Œäº†å››é¡¹å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•å’Œä¸¤é¡¹MLLMçš„æ„ŸçŸ¥ç­–ç•¥ç³»ç»Ÿç ”ç©¶ï¼ŒåŒ…æ‹¬æ˜¾å¼ã€éšå¼ã€è§†è§‰å’Œæ–‡æœ¬æ„ŸçŸ¥ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæ˜¾å¼æ„ŸçŸ¥ï¼Œç‰¹åˆ«æ˜¯ä¸æ–‡æœ¬çº¿ç´¢ç›¸ç»“åˆæ—¶ï¼Œå¯ä»¥æŒç»­å–å¾—æœ€ä½³æ”¹è¿›ï¼Œå°¤å…¶æ˜¯å¯¹äºè¾ƒå°çš„æ¨¡å‹ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†VTPerception-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†æ„ŸçŸ¥ä¸æ¨ç†è§£è€¦çš„ç»Ÿä¸€ä¸¤é˜¶æ®µæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µå¼•å…¥æ„ŸçŸ¥å¢å¼ºå¾®è°ƒï¼Œç¬¬äºŒé˜¶æ®µåº”ç”¨æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼ŒåŒ…æ‹¬æ–°å‹è§†è§‰ã€æ–‡æœ¬å’Œä¸€è‡´æ€§å¥–åŠ±ã€‚å®éªŒè¡¨æ˜ï¼ŒVTPerception-R1åœ¨ä¸åŒä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ¨ç†å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œä¸ºåŸºäºæ„ŸçŸ¥çš„å¤šæ¨¡æ€æ¨ç†æä¾›äº†å¯ä¼¸ç¼©å’Œå¯å®¡æ ¸çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/yizhuoDi/VTPerceprion-R1%E3%80%82">https://github.com/yizhuoDi/VTPerceprion-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24776v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥è¯æ®ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚æ–‡ç« ç³»ç»Ÿåœ°æ¢è®¨äº†æ˜ç¡®ã€éšå«ã€è§†è§‰å’Œæ–‡æœ¬æ„ŸçŸ¥ç­–ç•¥åœ¨å››ç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç‰¹åˆ«æ˜¯ä¸æ–‡æœ¬çº¿ç´¢ç›¸ç»“åˆæ—¶ï¼Œæ˜ç¡®çš„æ„ŸçŸ¥ç­–ç•¥ä¸ºå°å‹æ¨¡å‹æä¾›äº†æœ€å¥½çš„æ”¹è¿›ã€‚åŸºäºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ä¸¤ä¸ªé˜¶æ®µæ¡†æ¶VTPerception-R1ï¼Œå®ƒå°†æ„ŸçŸ¥ä¸æ¨ç†åˆ†ç¦»ã€‚ç¬¬ä¸€é˜¶æ®µå¼•å…¥æ„ŸçŸ¥å¢å¼ºå¾®è°ƒï¼Œç¬¬äºŒé˜¶æ®µåº”ç”¨æ„ŸçŸ¥å¢å¼ºå¼ºåŒ–å­¦ä¹ ï¼ŒåŒ…æ‹¬æ–°å‹è§†è§‰ã€æ–‡æœ¬å’Œä¸€è‡´æ€§å¥–åŠ±ã€‚å®éªŒè¯æ˜ï¼ŒVTPerception-R1åœ¨ä¸åŒä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ¨ç†å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œä¸ºæ„ŸçŸ¥æ¥åœ°å¤šæ¨¡æ€æ¨ç†æä¾›äº†å¯ä¼¸ç¼©å’Œå¯å®¡è®¡çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥è¯æ®ä¸­çš„æ¨ç†å­˜åœ¨å›°éš¾ã€‚</li>
<li>ç ”ç©¶æ¶‰åŠå››ç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œæ¢è®¨äº†æ˜ç¡®ã€éšå«ã€è§†è§‰å’Œæ–‡æœ¬æ„ŸçŸ¥ç­–ç•¥çš„è¡¨ç°ã€‚</li>
<li>æ˜ç¡®æ„ŸçŸ¥ç­–ç•¥ä¸æ–‡æœ¬çº¿ç´¢ç»“åˆæ—¶è¡¨ç°æœ€ä½³ï¼Œå¯¹å°å‹æ¨¡å‹å°¤ä¸ºå¦‚æ­¤ã€‚</li>
<li>æå‡ºVTPerception-R1æ¡†æ¶ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šæ„ŸçŸ¥å¢å¼ºå¾®è°ƒé˜¶æ®µå’Œæ„ŸçŸ¥å¢å¼ºå¼ºåŒ–å­¦ä¹ é˜¶æ®µã€‚</li>
<li>VTPerception-R1å¼•å…¥æ–°å‹è§†è§‰ã€æ–‡æœ¬å’Œä¸€è‡´æ€§å¥–åŠ±æœºåˆ¶ã€‚</li>
<li>å®éªŒè¯æ˜VTPerception-R1æé«˜äº†æ¨ç†å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c91c71b6b36ab2fb9212a2ac046ca60d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7643931a8a78ff433ede4bc07d81c42e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7929ced9bc4396fab39437304d4a60de.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="From-Ambiguity-to-Verdict-A-Semiotic-Grounded-Multi-Perspective-Agent-for-LLM-Logical-Reasoning"><a href="#From-Ambiguity-to-Verdict-A-Semiotic-Grounded-Multi-Perspective-Agent-for-LLM-Logical-Reasoning" class="headerlink" title="From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent   for LLM Logical Reasoning"></a>From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent   for LLM Logical Reasoning</h2><p><strong>Authors:Yunyao Zhang, Xinglang Zhang, Junxi Sheng, Wenbing Li, Junqing Yu, Wei Yang, Zikai Song</strong></p>
<p>Logical reasoning is a fundamental capability of large language models (LLMs). However, existing studies largely overlook the interplay between logical complexity and semantic complexity, resulting in methods that struggle to address challenging scenarios involving abstract propositions, ambiguous contexts, and conflicting stances, which are central to human reasoning. For this gap, we propose LogicAgent, a semiotic-square-guided framework designed to jointly address logical complexity and semantic complexity. LogicAgent explicitly performs multi-perspective deduction in first-order logic (FOL), while mitigating vacuous reasoning through existential import checks that incorporate a three-valued decision scheme (True, False, Uncertain) to handle boundary cases more faithfully. Furthermore, to overcome the semantic simplicity and low logical complexity of existing datasets, we introduce RepublicQA, a benchmark that reaches college-level difficulty (FKGL &#x3D; 11.94) and exhibits substantially greater lexical and structural diversity than prior benchmarks. RepublicQA is grounded in philosophical concepts, featuring abstract propositions and systematically organized contrary and contradictory relations, making it the most semantically rich resource for evaluating logical reasoning. Experiments demonstrate that LogicAgent achieves state-of-the-art performance on RepublicQA, with a 6.25% average gain over strong baselines, and generalizes effectively to mainstream logical reasoning benchmarks including ProntoQA, ProofWriter, FOLIO, and ProverQA, achieving an additional 7.05% average gain. These results highlight the strong effectiveness of our semiotic-grounded multi-perspective reasoning in boosting LLMsâ€™ logical performance. </p>
<blockquote>
<p>é€»è¾‘æ¨ç†æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºæœ¬èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶å¤§å¤šå¿½è§†äº†é€»è¾‘å¤æ‚æ€§ä¸è¯­ä¹‰å¤æ‚æ€§ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¯¼è‡´å®ƒä»¬åœ¨å¤„ç†æ¶‰åŠæŠ½è±¡å‘½é¢˜ã€æ¨¡ç³Šä¸Šä¸‹æ–‡å’Œå†²çªç«‹åœºçš„æŒ‘æˆ˜æ€§åœºæ™¯æ—¶è¡¨ç°æŒ£æ‰ï¼Œè€Œè¿™äº›åœºæ™¯æ˜¯äººç±»æ¨ç†çš„æ ¸å¿ƒã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†LogicAgentï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥ç¬¦å·å¹³æ–¹ä¸ºæŒ‡å¯¼çš„æ¡†æ¶ï¼Œæ—¨åœ¨å…±åŒè§£å†³é€»è¾‘å¤æ‚æ€§å’Œè¯­ä¹‰å¤æ‚æ€§ã€‚LogicAgentæ˜¾å¼æ‰§è¡Œä¸€é˜¶é€»è¾‘ï¼ˆFOLï¼‰ä¸­çš„å¤šè§†è§’æ¨ç†ï¼ŒåŒæ—¶é€šè¿‡å­˜åœ¨æ€§å¯¼å…¥æ£€æŸ¥ç¼“è§£ç©ºæ´æ¨ç†ï¼Œé‡‡ç”¨ä¸‰å€¼å†³ç­–æ–¹æ¡ˆï¼ˆçœŸã€å‡ã€ä¸ç¡®å®šï¼‰æ¥å¤„ç†è¾¹ç•Œæƒ…å†µï¼Œä½¿å…¶æ›´åŠ å¿ å®ã€‚æ­¤å¤–ï¼Œä¸ºäº†å…‹æœç°æœ‰æ•°æ®é›†è¯­ä¹‰ç®€å•ã€é€»è¾‘å¤æ‚æ€§ä½çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RepublicQAï¼Œè¿™æ˜¯ä¸€ä¸ªéš¾åº¦è¾¾åˆ°å¤§å­¦æ°´å¹³ï¼ˆFKGL&#x3D;11.94ï¼‰çš„åŸºå‡†æµ‹è¯•ï¼Œå…¶è¯æ±‡å’Œç»“æ„å¤šæ ·æ€§æ˜¾è‘—é«˜äºä»¥å‰çš„åŸºå‡†æµ‹è¯•ã€‚RepublicQAä»¥å“²å­¦æ¦‚å¿µä¸ºåŸºç¡€ï¼ŒåŒ…å«æŠ½è±¡å‘½é¢˜ï¼Œä»¥åŠç³»ç»Ÿç»„ç»‡çš„ç›¸åå’ŒçŸ›ç›¾å…³ç³»ï¼Œæˆä¸ºè¯„ä¼°é€»è¾‘æ¨ç†æ–¹é¢æœ€è¯­ä¹‰ä¸°å¯Œçš„èµ„æºã€‚å®éªŒè¡¨æ˜ï¼ŒLogicAgentåœ¨RepublicQAä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¯”å¼ºåŸºçº¿å¹³å‡æé«˜äº†6.25%ï¼Œå¹¶åœ¨ä¸»æµçš„é€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬ProntoQAã€ProofWriterã€FOLIOå’ŒProverQAï¼‰ä¸Šå®ç°äº†é¢å¤–çš„7.05%çš„å¹³å‡å¢å¹…ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†æˆ‘ä»¬åŸºäºç¬¦å·çš„å¤šè§†è§’æ¨ç†çš„å¼ºçƒˆæœ‰æ•ˆæ€§ï¼Œæå‡äº†LLMçš„é€»è¾‘æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24765v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¯¥æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€»è¾‘æ¨ç†æ–¹é¢çš„ä¸€ä¸ªåŸºæœ¬èƒ½åŠ›ç¼ºå¤±é—®é¢˜ã€‚ç°æœ‰ç ”ç©¶å¿½ç•¥äº†é€»è¾‘å¤æ‚æ€§å’Œè¯­ä¹‰å¤æ‚æ€§ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¯¼è‡´åœ¨å¤„ç†æ¶‰åŠæŠ½è±¡å‘½é¢˜ã€æ¨¡ç³Šä¸Šä¸‹æ–‡å’Œå†²çªç«‹åœºç­‰æŒ‘æˆ˜åœºæ™¯æ—¶æ•ˆæœä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLogicAgentçš„æ¡†æ¶ï¼Œæ—¨åœ¨è”åˆè§£å†³é€»è¾‘å¤æ‚æ€§å’Œè¯­ä¹‰å¤æ‚æ€§ã€‚LogicAgentåœ¨ä¸€é˜¶é€»è¾‘ä¸­è¿›è¡Œäº†å¤šè§†è§’æ¼”ç»ï¼Œå¹¶é€šè¿‡å­˜åœ¨æ€§æ£€æŸ¥æ¥å‡è½»ç©ºæ´æ¨ç†ï¼Œé‡‡ç”¨ä¸‰å€¼å†³ç­–æ–¹æ¡ˆï¼ˆçœŸã€å‡ã€ä¸ç¡®å®šï¼‰ä»¥æ›´å‡†ç¡®åœ°å¤„ç†è¾¹ç•Œæƒ…å†µã€‚æ­¤å¤–ï¼Œä¸ºäº†å…‹æœç°æœ‰æ•°æ®é›†è¯­ä¹‰ç®€å•ã€é€»è¾‘å¤æ‚åº¦ä½çš„é—®é¢˜ï¼Œå¼•å…¥äº†RepublicQAæ•°æ®é›†ï¼Œå…¶éš¾åº¦è¾¾åˆ°å¤§å­¦æ°´å¹³ï¼Œè¯æ±‡å’Œç»“æ„æ›´åŠ å¤šæ ·ã€‚å®éªŒè¡¨æ˜ï¼ŒLogicAgentåœ¨RepublicQAä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå¹³å‡ä¼˜äºå¼ºåŸºçº¿6.25%ï¼Œå¹¶åœ¨ä¸»æµé€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†é¢å¤–çš„7.05%çš„å¹³å‡å¢ç›Šã€‚è¿™è¯æ˜äº†åŸºäºç¬¦å·çš„å¤šè§†è§’æ¨ç†åœ¨æå‡è¯­è¨€æ¨¡å‹çš„é€»è¾‘æ€§èƒ½æ–¹é¢çš„å¼ºå¤§æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é€»è¾‘æ¨ç†æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯æ¶‰åŠæŠ½è±¡å‘½é¢˜ã€æ¨¡ç³Šä¸Šä¸‹æ–‡å’Œå†²çªç«‹åœºçš„åœºæ™¯ã€‚</li>
<li>LogicAgentæ¡†æ¶æ—¨åœ¨è”åˆè§£å†³é€»è¾‘å¤æ‚æ€§å’Œè¯­ä¹‰å¤æ‚æ€§ï¼Œé€šè¿‡å¤šè§†è§’æ¼”ç»å’Œå­˜åœ¨æ€§æ£€æŸ¥æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†åœ¨è¯­ä¹‰å’Œé€»è¾‘å¤æ‚åº¦æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå› æ­¤å¼•å…¥äº†RepublicQAæ•°æ®é›†ï¼Œå…¶éš¾åº¦æ›´é«˜ã€è¯æ±‡å’Œç»“æ„æ›´åŠ å¤šæ ·ã€‚</li>
<li>LogicAgentåœ¨RepublicQAç­‰åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œå¹³å‡ä¼˜äºå…¶ä»–æ–¹æ³•6.25%ï¼Œä¸”åœ¨ä¸»æµé€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šä¹Ÿæœ‰è‰¯å¥½è¡¨ç°ã€‚</li>
<li>LogicAgenté‡‡ç”¨çš„ä¸‰å€¼å†³ç­–æ–¹æ¡ˆï¼ˆçœŸã€å‡ã€ä¸ç¡®å®šï¼‰æœ‰åŠ©äºæ›´å‡†ç¡®åœ°å¤„ç†è¾¹ç•Œæƒ…å†µï¼Œæé«˜æ¨ç†çš„ç²¾å‡†åº¦ã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†é€»è¾‘å¤æ‚æ€§å’Œè¯­ä¹‰å¤æ‚æ€§åœ¨é€»è¾‘æ¨ç†ä¸­çš„é‡è¦æ€§ï¼Œä»¥åŠå®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24765">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-74cca928900552fb3c5f32bc5ec80cef.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ab7f9e8af8f97bb1023a44d300bc257~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912240&auth_key=1759912240-0-0-ffaf141913d99f293aaa6834c0d9a2fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf8dd74052b9c6702370d3c10e3e613a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912249&auth_key=1759912249-0-0-f148a32b048c91eec6047951f85cb4a5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9d755a8f7d0e101f5fcbd0ad98fecb26~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912257&auth_key=1759912257-0-0-7398b048a472d8396a1969b6984b6707&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Socratic-Zero-Bootstrapping-Reasoning-via-Data-Free-Agent-Co-evolution"><a href="#Socratic-Zero-Bootstrapping-Reasoning-via-Data-Free-Agent-Co-evolution" class="headerlink" title="Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution"></a>Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution</h2><p><strong>Authors:Shaobo Wang, Zhengbo Jiao, Zifan Zhang, Yilang Peng, Xu Ze, Boyu Yang, Wei Wang, Hu Wei, Linfeng Zhang</strong></p>
<p>Recent breakthroughs in large language models (LLMs) on reasoning tasks rely heavily on massive, high-quality datasets-typically human-annotated and thus difficult to scale. While data synthesis or distillation offers a promising alternative, existing methods struggle with inconsistent data quality and an inability to dynamically adapt to the evolving capabilities of the model, leading to suboptimal training signals. To address these limitations, we introduce Socratic-Zero, a fully autonomous framework that generates high-quality training data from minimal seed examples through the co-evolution of three agents: the Teacher, the Solver, and the Generator. The Solver continuously refines its reasoning by learning from preference feedback on both successful and failed trajectories; the Teacher adaptively crafts increasingly challenging questions based on the Solverâ€™s weaknesses; and the Generator distills the Teacherâ€™s question-design strategy to enable scalable, high-fidelity curriculum generation. This closed-loop system produces a self-improving curriculum-requiring no pre-existing tasks or labels. Remarkably, starting from only 100 seed questions, our Socratic-Solver-8B achieves an average gain of +20.2 percentage points over prior data synthesis methods across seven mathematical reasoning benchmarks (AMC23, AIME24-25, Olympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3 and GLM4 series models. Even more surprisingly, synthetic data from Socratic-Generator-32B enables student LLMs to achieve superior performance compared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks, including Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, Gemini-2.5-Pro, Grok-4, and Claude-4.1-Opus. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„çªç ´ä¸¥é‡ä¾èµ–äºå¤§è§„æ¨¡çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œé€šå¸¸æ˜¯äººå·¥æ ‡æ³¨çš„ï¼Œå› æ­¤éš¾ä»¥æ‰©å±•ã€‚è™½ç„¶æ•°æ®åˆæˆæˆ–è’¸é¦æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨æ•°æ®è´¨é‡ä¸ä¸€è‡´å’Œæ— æ³•åŠ¨æ€é€‚åº”æ¨¡å‹ä¸æ–­å‘å±•çš„èƒ½åŠ›æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´è®­ç»ƒä¿¡å·ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†Socratic-Zeroï¼Œä¸€ä¸ªå®Œå…¨è‡ªä¸»çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ•™å¸ˆã€æ±‚è§£å™¨å’Œç”Ÿæˆå™¨ä¸‰ä¸ªä»£ç†çš„ååŒè¿›åŒ–ï¼Œä»å°‘é‡ç§å­ç¤ºä¾‹ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚æ±‚è§£å™¨é€šè¿‡æˆåŠŸå’Œå¤±è´¥è½¨è¿¹çš„åå¥½åé¦ˆä¸æ–­ç²¾è¿›å…¶æ¨ç†èƒ½åŠ›ï¼›æ•™å¸ˆæ ¹æ®æ±‚è§£å™¨çš„å¼±ç‚¹è‡ªé€‚åº”åœ°æ„å»ºè¶Šæ¥è¶Šå…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼›ç”Ÿæˆå™¨æç‚¼æ•™å¸ˆçš„é—®é¢˜è®¾è®¡ç­–ç•¥ï¼Œå®ç°å¯æ‰©å±•çš„é«˜ä¿çœŸè¯¾ç¨‹ç”Ÿæˆã€‚è¿™ä¸ªé—­ç¯ç³»ç»Ÿäº§ç”Ÿäº†ä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›çš„è¯¾ç¨‹ï¼Œä¸éœ€è¦é¢„å…ˆå­˜åœ¨çš„ä»»åŠ¡æˆ–æ ‡ç­¾ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»…ä»100ä¸ªç§å­é—®é¢˜å¼€å§‹ï¼Œæˆ‘ä»¬çš„Socratic-Solver-8Båœ¨ä¸ƒä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆAMC23ã€AIME24-25ã€Olympiadã€MATH-500ã€Minervaå’ŒGSM8Kï¼‰ä¸Šçš„è¡¨ç°æ¯”å…ˆå‰çš„æ•°æ®åˆæˆæ–¹æ³•æœ‰å¹³å‡+20.2ä¸ªç™¾åˆ†ç‚¹çš„æå‡ï¼Œåœ¨Qwen3å’ŒGLM4ç³»åˆ—æ¨¡å‹ä¸Šä¹Ÿæœ‰æŒç»­çš„æ”¶ç›Šã€‚æ›´ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæ¥è‡ªSocratic-Generator-32Bçš„åˆæˆæ•°æ®ä½¿å­¦ç”ŸLLMåœ¨è¿™äº›åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›ï¼ˆSOTAï¼‰çš„å•†ä¸šLLMï¼ŒåŒ…æ‹¬Qwen3-235B-A22Bã€DeepSeek-V3.1-671Bã€GPT-5ã€Gemini-2.5-Proã€Grok-4å’ŒClaude-4.1-Opusã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24726v1">PDF</a> 23 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„æœ€æ–°çªç ´ä¸¥é‡ä¾èµ–äºå¤§é‡é«˜è´¨é‡æ•°æ®é›†ï¼Œé€šå¸¸è¿™äº›æ•°æ®é›†éœ€è¦äººå·¥æ ‡æ³¨ï¼Œå› æ­¤éš¾ä»¥æ‰©å±•ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Socratic-Zeroæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ•™å¸ˆã€æ±‚è§£å™¨å’Œç”Ÿæˆå™¨ä¸‰ä¸ªä»£ç†çš„ååŒè¿›åŒ–ï¼Œä»å°‘é‡ç§å­ç¤ºä¾‹ä¸­ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚æ¡†æ¶è‡ªä¸»å·¥ä½œï¼Œæ— éœ€é¢„å…ˆå­˜åœ¨çš„ä»»åŠ¡æˆ–æ ‡ç­¾ï¼Œå¯äº§ç”Ÿè‡ªæˆ‘æ”¹è¿›çš„è¯¾ç¨‹ã€‚é€šè¿‡ä»…ä½¿ç”¨100ä¸ªç§å­é—®é¢˜ï¼Œæˆ‘ä»¬çš„Socratic-Solver-8Båœ¨ä¸ƒä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰çš„æ•°æ®åˆæˆæ–¹æ³•ï¼Œå¹³å‡æé«˜äº†20.2ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œæ¥è‡ªSocratic-Generator-32Bçš„åˆæˆæ•°æ®ä½¿å­¦ç”ŸLLMåœ¨è¿™äº›åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›å•†ä¸šLLMã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Socratic-Zeroæ¡†æ¶è§£å†³äº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­æ•°æ®é›†éš¾ä»¥æ‰©å±•çš„é—®é¢˜ã€‚</li>
<li>æ¡†æ¶åŒ…å«æ•™å¸ˆã€æ±‚è§£å™¨å’Œç”Ÿæˆå™¨ä¸‰ä¸ªä»£ç†ï¼ŒååŒè¿›åŒ–ä»¥ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½è‡ªä¸»å·¥ä½œï¼Œæ— éœ€é¢„å…ˆå­˜åœ¨çš„ä»»åŠ¡æˆ–æ ‡ç­¾ï¼Œå¯äº§ç”Ÿè‡ªæˆ‘æ”¹è¿›çš„è¯¾ç¨‹ã€‚</li>
<li>Socratic-Solver-8Bä»…ä½¿ç”¨100ä¸ªç§å­é—®é¢˜ï¼Œåœ¨ä¸ƒä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–æ•°æ®åˆæˆæ–¹æ³•ã€‚</li>
<li>Socratic-Generator-32Bç”Ÿæˆçš„åˆæˆæ•°æ®ä½¿å­¦ç”ŸLLMçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›å•†ä¸šLLMã€‚</li>
<li>æ•™å¸ˆä»£ç†æ ¹æ®æ±‚è§£å™¨çš„å¼±ç‚¹åˆ¶å®šæ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5c493033248c99d6d52107cc7d745310.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d16d9ab7c5151a6967af0538d543162~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912272&auth_key=1759912272-0-0-07055722dd6b6d9827e150d59619f8df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-da55fddc30f2dc8b668f2e4035c1db32~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912279&auth_key=1759912279-0-0-cf403804e725ae488b291974438e847c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Identity-Bridge-Enabling-Implicit-Reasoning-via-Shared-Latent-Memory"><a href="#Identity-Bridge-Enabling-Implicit-Reasoning-via-Shared-Latent-Memory" class="headerlink" title="Identity Bridge: Enabling Implicit Reasoning via Shared Latent Memory"></a>Identity Bridge: Enabling Implicit Reasoning via Shared Latent Memory</h2><p><strong>Authors:Pengxiao Lin, Zheng-An Chen, Zhi-Qin John Xu</strong></p>
<p>Despite remarkable advances, large language models often fail at compositional reasoning tasks, a phenomenon exemplified by the &#96;&#96;curse of two-hop reasoningâ€™â€™. This paper introduces the Identity Bridge, a simple yet powerful mechanism that resolves this compositionality gap by supervising the model on a zero-hop identity task. We demonstrate empirically that this addition enables models to successfully perform out-of-distribution two-hop reasoning, a task they otherwise completely fail. To explain this phenomenon, we provide a theoretical analysis using a simplified Emb-MLP model, proving that identity supervision reshapes the modelâ€™s latent geometry. We show this alignment is induced by an implicit nuclear-norm regularization during optimization, which favors low-rank solutions that share structure across tasks. For complex tasks, we use small initialization or weight decay to enhance the regularization effect, which enhances the latent space alignment effect and slows down the generalization decay. Finally, we extend our investigation to large-scale models, observing that they still achieve two-hop reasoning through the latent memory, which provides crucial inspiration for enhancing their implicit reasoning abilities. </p>
<blockquote>
<p>å°½ç®¡å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç»„åˆæ¨ç†ä»»åŠ¡ä¸­å¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œè¿™ä¸€ç°è±¡è¢«â€œä¸¤è·³æ¨ç†çš„è¯…å’’â€æ‰€ä¾‹è¯ã€‚æœ¬æ–‡ä»‹ç»äº†Identity Bridgeï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„æœºåˆ¶ï¼Œé€šè¿‡ç›‘ç£æ¨¡å‹è¿›è¡Œé›¶è·³èº«ä»½ä»»åŠ¡æ¥è§£å†³ç»„åˆæ€§å·®è·ã€‚æˆ‘ä»¬å®è¯è¯æ˜ï¼Œè¿™ä¸€è¡¥å……ä½¿å¾—æ¨¡å‹èƒ½å¤ŸæˆåŠŸæ‰§è¡Œè¶…å‡ºåˆ†å¸ƒçš„ä¸¤è·³æ¨ç†ä»»åŠ¡ï¼Œå¦åˆ™è¿™äº›ä»»åŠ¡ä¼šå®Œå…¨å¤±è´¥ã€‚ä¸ºäº†è§£é‡Šè¿™ä¸€ç°è±¡ï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€åŒ–çš„Emb-MLPæ¨¡å‹è¿›è¡Œç†è®ºåˆ†æï¼Œè¯æ˜èº«ä»½ç›‘ç£ä¼šé‡å¡‘æ¨¡å‹çš„æ½œåœ¨å‡ ä½•ç»“æ„ã€‚æˆ‘ä»¬è¡¨æ˜è¿™ç§å¯¹é½æ˜¯ç”±ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„éšå¼æ ¸èŒƒæ•°æ­£åˆ™åŒ–å¼•èµ·çš„ï¼Œå®ƒæœ‰åˆ©äºå…±äº«ä»»åŠ¡ç»“æ„çš„ä½ç§©è§£ã€‚å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œæˆ‘ä»¬é€šè¿‡å¾®è°ƒåˆå§‹åŒ–æˆ–å°æƒé‡è¡°å‡æ¥å¢å¼ºæ­£åˆ™åŒ–æ•ˆæœï¼Œè¿™å¢å¼ºäº†æ½œåœ¨ç©ºé—´å¯¹é½æ•ˆæœå¹¶å‡ç¼“äº†æ³›åŒ–è¡°å‡ã€‚æœ€åï¼Œæˆ‘ä»¬å°†è°ƒæŸ¥æ‰©å±•åˆ°å¤§è§„æ¨¡æ¨¡å‹ï¼Œè§‚å¯Ÿåˆ°å®ƒä»¬ä»ç„¶é€šè¿‡æ½œåœ¨è®°å¿†å®ç°ä¸¤è·³æ¨ç†ï¼Œè¿™ä¸ºå¢å¼ºå®ƒä»¬çš„éšå¼æ¨ç†èƒ½åŠ›æä¾›äº†å…³é”®å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24653v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Identity Bridgeæœºåˆ¶ï¼Œè¯¥æœºåˆ¶é€šè¿‡ç›‘ç£æ¨¡å‹è¿›è¡Œé›¶è·³èº«ä»½ä»»åŠ¡ï¼Œè§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç»„åˆæ¨ç†ä»»åŠ¡ä¸Šçš„ä¸è¶³ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æœºåˆ¶çš„å¼•å…¥ä½¿å¾—æ¨¡å‹èƒ½å¤ŸæˆåŠŸè¿›è¡Œè·¨åˆ†å¸ƒçš„ä¸¤è·³æ¨ç†ï¼Œè¿™æ˜¯å®ƒä»¬ä¹‹å‰æ— æ³•å®Œæˆçš„ä»»åŠ¡ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œèº«ä»½ç›‘ç£é‡å¡‘äº†æ¨¡å‹çš„æ½œåœ¨å‡ ä½•ç»“æ„ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„éšå¼æ ¸èŒƒæ•°æ­£åˆ™åŒ–æ¥å®ç°ã€‚å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œé€šè¿‡å°åˆå§‹åŒ–æˆ–æƒé‡è¡°å‡å¢å¼ºæ­£åˆ™åŒ–æ•ˆæœï¼Œæé«˜æ½œåœ¨ç©ºé—´å¯¹é½æ•ˆæœå¹¶å‡ç¼“æ³›åŒ–è¡°å‡ã€‚æœ€åï¼Œå¯¹å¤§å‹æ¨¡å‹çš„è°ƒæŸ¥è¡¨æ˜ï¼Œå®ƒä»¬ä»é€šè¿‡æ½œåœ¨è®°å¿†å®ç°ä¸¤è·³æ¨ç†ï¼Œè¿™ä¸ºå¢å¼ºå®ƒä»¬çš„éšå¼æ¨ç†èƒ½åŠ›æä¾›äº†å…³é”®å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç»„åˆæ¨ç†ä»»åŠ¡ä¸Šå­˜åœ¨é—®é¢˜ï¼Œå¦‚â€œä¸¤è·³æ¨ç†çš„è¯…å’’â€ã€‚</li>
<li>Identity Bridgeæœºåˆ¶é€šè¿‡ç›‘ç£æ¨¡å‹è¿›è¡Œé›¶è·³èº«ä»½ä»»åŠ¡æ¥è§£å†³è¿™ä¸€éš¾é¢˜ã€‚</li>
<li>é›¶è·³èº«ä»½ä»»åŠ¡çš„å¼•å…¥ä½¿æ¨¡å‹èƒ½å¤ŸæˆåŠŸè¿›è¡Œè·¨åˆ†å¸ƒçš„ä¸¤è·³æ¨ç†ã€‚</li>
<li>ç†è®ºåˆ†æè¡¨æ˜ï¼Œèº«ä»½ç›‘ç£é‡å¡‘äº†æ¨¡å‹çš„æ½œåœ¨å‡ ä½•ç»“æ„ã€‚</li>
<li>éšå¼æ ¸èŒƒæ•°æ­£åˆ™åŒ–åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œé€šè¿‡å°åˆå§‹åŒ–æˆ–æƒé‡è¡°å‡å¢å¼ºæ­£åˆ™åŒ–æ•ˆæœï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24653">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ec833960869d5ea6621a6097586b5fe2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912286&auth_key=1759912286-0-0-5388e2a8b8d85e085fb1ac7a919bd3dc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1f0dc6ae9b6ed97c2b722b81d41d4239~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912294&auth_key=1759912294-0-0-587ae944e40a540f657879a73d28dde6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-790d0ff1f5b0e2693a448058c92c200b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="HiKE-Hierarchical-Evaluation-Framework-for-Korean-English-Code-Switching-Speech-Recognition"><a href="#HiKE-Hierarchical-Evaluation-Framework-for-Korean-English-Code-Switching-Speech-Recognition" class="headerlink" title="HiKE: Hierarchical Evaluation Framework for Korean-English   Code-Switching Speech Recognition"></a>HiKE: Hierarchical Evaluation Framework for Korean-English   Code-Switching Speech Recognition</h2><p><strong>Authors:Gio Paik, Yongbeom Kim, Soungmin Lee, Sangmin Ahn, Chanwoo Kim</strong></p>
<p>Despite advances in multilingual automatic speech recognition (ASR), code-switching (CS), the mixing of languages within an utterance common in daily speech, remains a severely underexplored challenge. In this paper, we introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the first globally accessible evaluation framework for Korean-English CS, aiming to provide a means for the precise evaluation of multilingual ASR models and to foster research in the field. The proposed framework not only consists of high-quality, natural CS data across various topics, but also provides meticulous loanword labels and a hierarchical CS-level labeling scheme (word, phrase, and sentence) that together enable a systematic evaluation of a modelâ€™s ability to handle each distinct level of code-switching. Through evaluations of diverse multilingual ASR models and fine-tuning experiments, this paper demonstrates that while most multilingual ASR models initially struggle with CS-ASR, this capability can be enabled through fine-tuning with CS data. HiKE will be available at <a target="_blank" rel="noopener" href="https://github.com/ThetaOne-AI/HiKE">https://github.com/ThetaOne-AI/HiKE</a>. </p>
<blockquote>
<p>å°½ç®¡å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯æœ‰æ‰€è¿›æ­¥ï¼Œä½†è¯­è¨€åˆ‡æ¢ï¼ˆCSï¼‰åœ¨æ—¥å¸¸å¯¹è¯ä¸­æ™®éå­˜åœ¨ï¼Œå³åœ¨ä¸€å¥è¯ä¸­æ··åˆä½¿ç”¨å¤šç§è¯­è¨€çš„æƒ…å†µä»ç„¶æ˜¯ä¸€ä¸ªè¢«ä¸¥é‡å¿½è§†çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†HiKEï¼šåˆ†å±‚éŸ©è‹±è¯­è¨€åˆ‡æ¢åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯é¦–ä¸ªé¢å‘å…¨çƒçš„éŸ©è‹±CSè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æä¾›ä¸€ç§ç²¾ç¡®è¯„ä¼°å¤šè¯­ç§ASRæ¨¡å‹çš„æ‰‹æ®µï¼Œå¹¶æ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚æ‰€æå‡ºçš„æ¡†æ¶ä¸ä»…åŒ…æ‹¬é«˜è´¨é‡çš„è‡ªç„¶CSæ•°æ®ï¼Œæ¶µç›–å„ç§ä¸»é¢˜ï¼Œè¿˜åŒ…æ‹¬ç»†è‡´çš„å€Ÿè¯æ ‡ç­¾å’Œåˆ†å±‚çº§çš„CSçº§åˆ«æ ‡ç­¾æ–¹æ¡ˆï¼ˆå•è¯ã€çŸ­è¯­å’Œå¥å­ï¼‰ï¼Œè¿™äº›å…±åŒä½¿æ¨¡å‹èƒ½å¤Ÿç³»ç»Ÿåœ°å¤„ç†æ¯ä¸ªç‹¬ç‰¹çš„è¯­è¨€åˆ‡æ¢çº§åˆ«ã€‚é€šè¿‡å¯¹å„ç§å¤šè¯­ç§ASRæ¨¡å‹çš„è¯„ä¼°å’Œå¾®è°ƒå®éªŒï¼Œæœ¬æ–‡è¯æ˜ï¼Œè™½ç„¶å¤§å¤šæ•°å¤šè¯­ç§ASRæ¨¡å‹æœ€åˆå¯¹CS-ASRæ„Ÿåˆ°å›°æƒ‘ï¼Œä½†é€šè¿‡CSæ•°æ®è¿›è¡Œå¾®è°ƒå¯ä»¥å…·å¤‡æ­¤åŠŸèƒ½ã€‚HiKEå°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ThetaOne-AI/HiKE%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/ThetaOne-AI/HiKEä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24613v1">PDF</a> 5 pages, 2 figures, Submitted to ICASSP2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†HiKEï¼šä¸€ä¸ªé¢å‘éŸ©è¯­-è‹±è¯­è½¬æ¢çš„å±‚æ¬¡åŒ–ä»£ç åˆ‡æ¢åŸºå‡†æµ‹è¯•å¹³å°ã€‚è¯¥å¹³å°æ—¨åœ¨æä¾›ç²¾ç¡®è¯„ä¼°å¤šè¯­ç§è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¹¶æ¨åŠ¨ç›¸å…³é¢†åŸŸç ”ç©¶ã€‚è¯¥å¹³å°ä¸ä»…åŒ…å«é«˜è´¨é‡çš„è‡ªç„¶è½¬æ¢æ•°æ®ï¼Œè¿˜æœ‰å„ç§ä¸»é¢˜çš„ç»†è‡´å€Ÿè¯æ ‡ç­¾å’Œå±‚æ¬¡åŒ–çš„ä»£ç åˆ‡æ¢çº§åˆ«æ ‡ç­¾æ–¹æ¡ˆã€‚å®éªŒè¯æ˜ï¼Œé€šè¿‡ç²¾ç»†è°ƒæ•´æ•°æ®ï¼Œå¤§å¤šæ•°å¤šè¯­ç§è¯­éŸ³è¯†åˆ«æ¨¡å‹éƒ½èƒ½åº”å¯¹ä»£ç åˆ‡æ¢æŒ‘æˆ˜ã€‚HiKEå°†åœ¨GitHubä¸Šå¼€æ”¾ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç åˆ‡æ¢ï¼ˆCSï¼‰åœ¨æ—¥å¸¸è¯­è¨€ä¸­æ··åˆä¸åŒè¯­è¨€çš„ç°è±¡åœ¨å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªä¸¥é‡æœªè¢«å……åˆ†ç ”ç©¶çš„æŒ‘æˆ˜ã€‚</li>
<li>HiKEæ˜¯é¦–ä¸ªé¢å‘éŸ©è¯­-è‹±è¯­è½¬æ¢çš„ä»£ç åˆ‡æ¢åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨æä¾›ç²¾ç¡®è¯„ä¼°å¤šè¯­ç§ASRæ¨¡å‹çš„å·¥å…·ï¼Œå¹¶æ¨åŠ¨ç›¸å…³ç ”ç©¶ã€‚</li>
<li>è¯¥å¹³å°åŒ…å«é«˜è´¨é‡çš„è‡ªç„¶è½¬æ¢æ•°æ®ï¼Œæ¶µç›–å„ç§ä¸»é¢˜ã€‚</li>
<li>HiKEæä¾›äº†è¯¦ç»†çš„å€Ÿè¯æ ‡ç­¾å’Œå±‚æ¬¡åŒ–çš„ä»£ç åˆ‡æ¢çº§åˆ«æ ‡ç­¾æ–¹æ¡ˆï¼ˆè¯ã€çŸ­è¯­å’Œå¥å­ï¼‰ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œå¤§å¤šæ•°å¤šè¯­ç§ASRæ¨¡å‹åœ¨æœ€åˆé¢å¯¹ä»£ç åˆ‡æ¢æ—¶ä¼šé‡åˆ°å›°éš¾ï¼Œä½†å¯ä»¥é€šè¿‡ç²¾ç»†è°ƒæ•´æ•°æ®æ¥æé«˜åº”å¯¹èƒ½åŠ›ã€‚</li>
<li>HiKEå¹³å°å°†åœ¨GitHubä¸Šå¼€æ”¾ä½¿ç”¨ï¼Œä¾¿äºå…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24613">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a2aa9f09f869f02d84a8cd44b93c20c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912308&auth_key=1759912308-0-0-76718644b9daf97fe1487f7fe3d547b2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-e72cdfccffaa698d4877eb21572839b4.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-7fc5b07b51a558ef0c483d84b124b138~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912322&auth_key=1759912322-0-0-ebd812ebfc1d24c3e67e20146ba98fbf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-e47cbd129fa12e9c768b7f7700f8eb23.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Euclidâ€™s-Gift-Enhancing-Spatial-Perception-and-Reasoning-in-Vision-Language-Models-via-Geometric-Surrogate-Tasks"><a href="#Euclidâ€™s-Gift-Enhancing-Spatial-Perception-and-Reasoning-in-Vision-Language-Models-via-Geometric-Surrogate-Tasks" class="headerlink" title="Euclidâ€™s Gift: Enhancing Spatial Perception and Reasoning in   Vision-Language Models via Geometric Surrogate Tasks"></a>Euclidâ€™s Gift: Enhancing Spatial Perception and Reasoning in   Vision-Language Models via Geometric Surrogate Tasks</h2><p><strong>Authors:Shijie Lian, Changti Wu, Laurence Tianruo Yang, Hang Yuan, Bin Yu, Lei Zhang, Kai Chen</strong></p>
<p>Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs).To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. To enable the model to acquire and apply Euclidean principles from these geometry problems, we employed Group Relative Policy Optimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family, inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models rose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them, RoboBrain2.0-Euclid-7B achieves 49.6% accuracy, surpassing the previous state-of-the-art model, Spatial-MLLM.To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in <a target="_blank" rel="noopener" href="https://zgca-ai4edu.github.io/Euclids_Gift">https://zgca-ai4edu.github.io/Euclids_Gift</a>. </p>
<blockquote>
<p>ç©ºé—´æ™ºèƒ½æ¶µç›–äº†ä¸€ç³»åˆ—ä¸°å¯Œçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¯è§†åŒ–å¹¶è½¬æ¢å½¢çŠ¶ã€åœ¨è„‘æµ·ä¸­æ—‹è½¬ç‰©ä½“ã€åˆ¤æ–­ç›¸å¯¹ä½ç½®å’ŒåŒ…å«å…³ç³»ï¼Œä»¥åŠä¼°è®¡æ•°é‡ã€‚ç„¶è€Œï¼Œå¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯´ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªå°šæœªè§£å†³çš„å…³é”®æŒ‘æˆ˜ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æè®®å°†æ¬§å‡ é‡Œå¾—å‡ ä½•é—®é¢˜è§£å†³ä½œä¸ºæ›¿ä»£ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç²¾å¿ƒæ„å»ºäº†ä¸€ä¸ªåä¸ºEuclid30Kçš„å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«çº¦30000ä¸ªå¹³é¢å’Œç«‹ä½“å‡ ä½•é—®é¢˜ã€‚ä¸ºäº†è®©æ¨¡å‹ä»è¿™äº›å‡ ä½•é—®é¢˜ä¸­å­¦ä¹ å¹¶åº”ç”¨æ¬§å‡ é‡Œå¾—åŸç†ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥å¯¹Qwen2.5VLå®¶æ—å’ŒRoboBrain2.0å®¶æ—è¿›è¡Œå¾®è°ƒï¼Œæ¿€åŠ±æ¨¡å‹è¯†åˆ«å½¢çŠ¶ã€è®¡æ•°å’Œå…³è”å®ä½“ï¼Œå¹¶ä½¿ç”¨æ¬§å‡ é‡Œå¾—åŸç†è¿›è¡Œå¤šæ­¥éª¤çš„æ¨ç†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ‰€å¾—æ¨¡å‹åœ¨å››ä¸ªç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆSuper-CLEVRã€Omni3DBenchã€VSI-Benchå’ŒMindCubeï¼‰ä¸Šå®ç°äº†æ˜¾è‘—çš„é›¶æ ·æœ¬å¢ç›Šï¼Œæ— éœ€ä»»ä½•é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„é€‚åº”ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨Euclid30Kè®­ç»ƒåï¼Œæ‰€æœ‰è¯„ä¼°æ¨¡å‹çš„VSI-Benchå¹³å‡å‡†ç¡®ç‡ä»34.5%æé«˜åˆ°40.5%ï¼Œæé«˜äº†5.5ä¸ªç™¾åˆ†ç‚¹ã€‚å…¶ä¸­ï¼ŒRoboBrain2.0-Euclid-7Bçš„å‡†ç¡®ç‡è¾¾åˆ°49.6%ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„å…ˆè¿›æ¨¡å‹Spatial-MLLMã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹ç³»ç»Ÿç ”ç©¶ï¼Œè¡¨æ˜ä»¥å‡ ä½•ä¸ºä¸­å¿ƒçš„å¾®è°ƒå¯ä»¥èµ‹äºˆè§†è§‰è¯­è¨€æ¨¡å‹å¹¿æ³›å¯è½¬ç§»çš„ç©ºé—´æŠ€èƒ½ã€‚ä»£ç å’ŒEuclid30Kæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://zgca-ai4edu.github.io/Euclids_Gift">https://zgca-ai4edu.github.io/Euclids_Gift</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24473v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨ç©ºé—´æ™ºèƒ½åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é‡è¦æ€§åŠå…¶æŒ‘æˆ˜ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåä¸ºEuclid30Kçš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œå¹¶é€šè¿‡Group Relative Policy Optimizationï¼ˆGRPOï¼‰å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶æŒæ¡å’Œåº”ç”¨æ¬§å‡ é‡Œå¾—åŸç†ã€‚å®éªŒè¡¨æ˜ï¼Œæ¨¡å‹åœ¨å››ä¸ªç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œä¸”è®­ç»ƒåçš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›æ›´å¼ºã€‚è¯¥ç ”ç©¶æ˜¾ç¤ºäº†å‡ ä½•ä¸­å¿ƒå¾®è°ƒèµ‹äºˆè¯­è¨€æ¨¡å‹å¹¿æ³›å¯è½¬ç§»çš„ç©ºé—´æŠ€èƒ½çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç©ºé—´æ™ºèƒ½åŒ…å«å¤šç§èƒ½åŠ›ï¼Œå¦‚å½¢çŠ¶å¯è§†åŒ–ã€ç‰©ä½“æ—‹è½¬ã€å…³ç³»å®šä½ã€åŒ…å«æ€§åˆ¤æ–­åŠæ•°é‡ä¼°ç®—ç­‰ï¼Œå¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>ä¸ºæé«˜æ¨¡å‹çš„ç©ºé—´æ™ºèƒ½ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†Euclid30Kå¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«çº¦30Kçš„å¹³é¢å’Œç«‹ä½“å‡ ä½•é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½ä»å‡ ä½•é—®é¢˜ä¸­å­¦ä¹ å¹¶åº”ç”¨æ¬§å‡ é‡Œå¾—åŸç†ã€‚</li>
<li>æ¨¡å‹åœ¨å››ä¸ªç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ˜¾è‘—ï¼Œä¸”åœ¨Euclid30Kæ•°æ®é›†è®­ç»ƒåï¼ŒVSI-Benchå‡†ç¡®ç‡ä»34.5%æå‡è‡³40.5%ï¼Œæå‡äº†5.5ä¸ªç™¾åˆ†ç‚¹ã€‚</li>
<li>RoboBrain2.0-Euclid-7Bæ¨¡å‹çš„å‡†ç¡®ç‡è¾¾åˆ°äº†49.6%ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›æ¨¡å‹Spatial-MLLMã€‚</li>
<li>æ­¤ç ”ç©¶æ˜¯é¦–æ¬¡ç³»ç»Ÿç ”ç©¶è¯æ˜å‡ ä½•ä¸­å¿ƒå¾®è°ƒèƒ½å¤Ÿä½¿è§†è§‰è¯­è¨€æ¨¡å‹è·å¾—å¹¿æ³›å¯è½¬ç§»çš„ç©ºé—´æŠ€èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24473">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-123c6f47826c7ca3b0f1ca282267ab64.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-6de5ac3234607d14ad0e6a17732161b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912346&auth_key=1759912346-0-0-4bcb3dcc0608035da7d573534eb4a54c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-ccf55f66205f66ee2f7ff37ac0d08994.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-72332904b33f2e58979d3d48b68bf0af~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912360&auth_key=1759912360-0-0-a1baa970441086e9bccd42a078592e88&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6c34c1254702ff73b3826fa76e96ded8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912367&auth_key=1759912367-0-0-0ddf7dfbdbec9003f9e0ae0ef6cdfdf4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="CDT-A-Comprehensive-Capability-Framework-for-Large-Language-Models-Across-Cognition-Domain-and-Task"><a href="#CDT-A-Comprehensive-Capability-Framework-for-Large-Language-Models-Across-Cognition-Domain-and-Task" class="headerlink" title="CDT: A Comprehensive Capability Framework for Large Language Models   Across Cognition, Domain, and Task"></a>CDT: A Comprehensive Capability Framework for Large Language Models   Across Cognition, Domain, and Task</h2><p><strong>Authors:Haosi Mo, Xinyu Ma, Xuebo Liu, Derek F. Wong, Yu Li, Jie Liu, Min Zhang</strong></p>
<p>Recent advances in Large Language Models (LLMs) have significantly enhanced their capabilities, highlighting the need for comprehensive evaluation frameworks that extend beyond task-specific benchmarks. However, existing benchmarks often focus on isolated abilities, lacking a holistic framework for assessing LLM capabilities. To address this gap, we propose the Cognition-Domain-Task (CDT) framework, which comprehensively measures a modelâ€™s capabilities across three dimensions. We expand the scope of model capability definitions at the cognitive level by incorporating the Cattell-Horn-Carroll cognitive theory, refining the categorization of model capabilities. We apply CDT in two directions: dataset capability evaluation and data selection. Experiments show that our capability metrics correlate well with downstream performance and can support effective dataset analysis and construction. The experiments on data selection also show significant improvements in both general and specific benchmarks, achieving scores of 44.3 and 45.4, with an increase of 1.6 and 2.2 points over the baselines, respectively. These results validate the effectiveness and practicality of CDT. Source code and models are available at <a target="_blank" rel="noopener" href="https://github.com/Alessa-mo/CDT">https://github.com/Alessa-mo/CDT</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ˜¾è‘—å¢å¼ºäº†å…¶èƒ½åŠ›ï¼Œè¿™çªæ˜¾äº†éœ€è¦è¶…è¶Šç‰¹å®šä»»åŠ¡åŸºå‡†æµ‹è¯•çš„ç»¼åˆè¯„ä¼°æ¡†æ¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä¾§é‡äºå­¤ç«‹çš„èƒ½åŠ›ï¼Œç¼ºä¹è¯„ä¼°LLMèƒ½åŠ›çš„æ•´ä½“æ¡†æ¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†è®¤çŸ¥åŸŸä»»åŠ¡ï¼ˆCDTï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»ä¸‰ä¸ªç»´åº¦å…¨é¢è¡¡é‡æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡ç»“åˆå¡ç‰¹å°”-éœæ©-å¡ç½—å°”è®¤çŸ¥ç†è®ºæ¥æ‰©å±•æ¨¡å‹èƒ½åŠ›å®šä¹‰çš„è®¤çŸ¥å±‚é¢ï¼Œå¯¹æ¨¡å‹èƒ½åŠ›è¿›è¡Œåˆ†ç±»çš„ç»†åŒ–ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ–¹å‘åº”ç”¨CDTï¼šæ•°æ®é›†èƒ½åŠ›è¯„ä¼°å’Œæ•°æ®å¤„ç†é€‰æ‹©ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„èƒ½åŠ›æŒ‡æ ‡ä¸ä¸‹æ¸¸æ€§èƒ½å¯†åˆ‡ç›¸å…³ï¼Œå¹¶æ”¯æŒæœ‰æ•ˆçš„æ•°æ®é›†åˆ†æå’Œæ„å»ºã€‚åœ¨æ•°æ®é€‰æ‹©æ–¹é¢çš„å®éªŒä¹Ÿæ˜¾ç¤ºï¼Œåœ¨é€šç”¨å’Œç‰¹å®šåŸºå‡†æµ‹è¯•ä¸­éƒ½æœ‰æ˜¾è‘—æ”¹å–„ï¼Œå¾—åˆ†åˆ†åˆ«ä¸º44.3å’Œ45.4ï¼Œç›¸è¾ƒäºåŸºçº¿åˆ†åˆ«æé«˜äº†1.6å’Œ2.2åˆ†ã€‚è¿™äº›ç»“æœéªŒè¯äº†CDTçš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚æºä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Alessa-mo/CDT%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Alessa-mo/CDTä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24422v1">PDF</a> 20 pages, 5 figures, EMNLP2025 Findings</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•æ˜¾è‘—å¢å¼ºäº†å…¶èƒ½åŠ›ï¼Œå‡¸æ˜¾å‡ºéœ€è¦è¶…è¶Šä»»åŠ¡ç‰¹å®šåŸºå‡†æµ‹è¯•çš„ç»¼åˆè¯„ä¼°æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•é€šå¸¸å…³æ³¨å­¤ç«‹èƒ½åŠ›ã€ç¼ºä¹è¯„ä¼°LLMèƒ½åŠ›çš„æ•´ä½“æ¡†æ¶çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è®¤çŸ¥åŸŸä»»åŠ¡ï¼ˆCDTï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå…¨é¢è¡¡é‡æ¨¡å‹åœ¨ä¸‰ä¸ªç»´åº¦ä¸Šçš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡èåˆCattell-Horn-Carrollè®¤çŸ¥ç†è®ºæ¥æ‰©å±•æ¨¡å‹èƒ½åŠ›å®šä¹‰çš„èŒƒç•´ï¼Œç²¾ç»†åŒ–äº†æ¨¡å‹èƒ½åŠ›çš„åˆ†ç±»ã€‚æˆ‘ä»¬åœ¨æ•°æ®é›†èƒ½åŠ›è¯„ä¼°å’Œæ•°æ®é€‰æ‹©ä¸¤ä¸ªæ–¹é¢åº”ç”¨äº†CDTã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„èƒ½åŠ›æŒ‡æ ‡ä¸ä¸‹æ¸¸æ€§èƒ½å¯†åˆ‡ç›¸å…³ï¼Œå¹¶æ”¯æŒæœ‰æ•ˆçš„æ•°æ®é›†åˆ†æå’Œæ„å»ºã€‚æ•°æ®é€‰æ‹©å®éªŒåœ¨é€šç”¨å’Œç‰¹å®šåŸºå‡†æµ‹è¯•ä¸Šå‡æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ï¼Œå¾—åˆ†åˆ†åˆ«ä¸º44.3å’Œ45.4ï¼Œè¾ƒåŸºçº¿åˆ†åˆ«æé«˜äº†1.6å’Œ2.2åˆ†ã€‚å®éªŒç»“æœéªŒè¯äº†CDTçš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›è¯„ä¼°éœ€è¦æ›´å…¨é¢çš„æ¡†æ¶ï¼Œè¶…è¶Šä»»åŠ¡ç‰¹å®šåŸºå‡†æµ‹è¯•ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ç¼ºä¹æ•´ä½“æ€§ï¼Œé€šå¸¸åªå…³æ³¨æ¨¡å‹å­¤ç«‹çš„èƒ½åŠ›ã€‚</li>
<li>CDTæ¡†æ¶å…¨é¢è¡¡é‡æ¨¡å‹åœ¨ä¸‰ä¸ªç»´åº¦ä¸Šçš„èƒ½åŠ›ï¼šè®¤çŸ¥åŸŸä»»åŠ¡æ¡†æ¶æ¶µç›–äº†æ¨¡å‹çš„å¹¿æ³›èƒ½åŠ›ã€‚</li>
<li>ç»“åˆCattell-Horn-Carrollè®¤çŸ¥ç†è®ºæ‰©å±•äº†æ¨¡å‹èƒ½åŠ›å®šä¹‰ï¼Œæ›´ç²¾ç»†åœ°åˆ†ç±»äº†æ¨¡å‹èƒ½åŠ›ã€‚</li>
<li>CDTæ¡†æ¶åº”ç”¨äºæ•°æ®é›†èƒ½åŠ›è¯„ä¼°å’Œæ•°æ®é€‰æ‹©ï¼Œå®éªŒè¯æ˜å…¶ä¸ä¸‹æ¸¸æ€§èƒ½å¯†åˆ‡ç›¸å…³ã€‚</li>
<li>æ•°æ®é€‰æ‹©å®éªŒåœ¨é€šç”¨å’Œç‰¹å®šåŸºå‡†æµ‹è¯•ä¸Šå‡æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24422">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-54c6091c6cd88a508502d812c41dfbdc~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912375&auth_key=1759912375-0-0-c4128272ab36bb45c3a33faebde90f15&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-197b7494a59a518e34e423859b61f402~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912382&auth_key=1759912382-0-0-8532cf2bf28d52c6d9b7b887e500d634&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9f87f10fe0cec2339a54cc5d7a97889b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912389&auth_key=1759912389-0-0-0dbe451319afdc96a344ea404c45021f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-ab1f96f643f70b2661f0dda92f89758d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759912395&auth_key=1759912395-0-0-7cbdced576095763f5c1e906e9ca523b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  ReasoningBank Scaling Agent Self-Evolving with Reasoning Memory
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-b015247526087c778e54bca1eb166af2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759911451&auth_key=1759911451-0-0-38fc8aa982a37462725138a030947592&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  Walk the Talk Is Your Log-based Software Reliability Maintenance System   Really Reliable?
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
