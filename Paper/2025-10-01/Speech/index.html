<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  Confidence-Guided Error Correction for Disordered Speech Recognition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d9f67a7030a7ec9d0daa6ee8d7a70c98')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-01-æ›´æ–°"><a href="#2025-10-01-æ›´æ–°" class="headerlink" title="2025-10-01 æ›´æ–°"></a>2025-10-01 æ›´æ–°</h1><h2 id="Confidence-Guided-Error-Correction-for-Disordered-Speech-Recognition"><a href="#Confidence-Guided-Error-Correction-for-Disordered-Speech-Recognition" class="headerlink" title="Confidence-Guided Error Correction for Disordered Speech Recognition"></a>Confidence-Guided Error Correction for Disordered Speech Recognition</h2><p><strong>Authors:Abner Hernandez, TomÃ¡s Arias Vergara, Andreas Maier, Paula Andrea PÃ©rez-Toro</strong></p>
<p>We investigate the use of large language models (LLMs) as post-processing modules for automatic speech recognition (ASR), focusing on their ability to perform error correction for disordered speech. In particular, we propose confidence-informed prompting, where word-level uncertainty estimates are embedded directly into LLM training to improve robustness and generalization across speakers and datasets. This approach directs the model to uncertain ASR regions and reduces overcorrection. We fine-tune a LLaMA 3.1 model and compare our approach to both transcript-only fine-tuning and post hoc confidence-based filtering. Evaluations show that our method achieves a 10% relative WER reduction compared to naive LLM correction on the Speech Accessibility Project spontaneous speech and a 47% reduction on TORGO, demonstrating the effectiveness of confidence-aware fine-tuning for impaired speech. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åå¤„ç†æ¨¡å—ä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹å…³æ³¨å®ƒä»¬å¯¹æ··ä¹±è¯­éŸ³çš„é”™è¯¯çº æ­£èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç½®ä¿¡åº¦çš„æç¤ºæ–¹æ³•ï¼Œå°†è¯çº§ä¸ç¡®å®šæ€§ä¼°è®¡ç›´æ¥åµŒå…¥åˆ°LLMè®­ç»ƒä¸­ï¼Œä»¥æé«˜è·¨è¯´è¯è€…å’Œæ•°æ®é›†çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•å¼•å¯¼æ¨¡å‹è¿›å…¥ä¸ç¡®å®šçš„ASRåŒºåŸŸï¼Œå¹¶å‡å°‘è¿‡åº¦çº æ­£ã€‚æˆ‘ä»¬å¯¹LLaMA 3.1æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶å°†æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åŸºäºè½¬å½•çš„å¾®è°ƒä»¥åŠåŸºäºäº‹åç½®ä¿¡åº¦çš„è¿‡æ»¤æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¯­éŸ³è®¿é—®é¡¹ç›®ï¼ˆSpeech Accessibility Projectï¼‰çš„è‡ªå‘è¨€è¯­ä¸Šå®ç°äº†ç›¸å¯¹äºå•çº¯LLMæ ¡æ­£çš„10%ç›¸å¯¹å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½ï¼Œåœ¨TORGOä¸Šé™ä½äº†47%ï¼Œè¯æ˜äº†ç½®ä¿¡åº¦æ„ŸçŸ¥å¾®è°ƒå¯¹äºå—æŸè¯­éŸ³çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25048v1">PDF</a> Preprint submitted to ICASSP</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åå¤„ç†æ¨¡å—ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹å£è¯­éšœç¢è€…çš„è¯­éŸ³è¯†åˆ«é”™è¯¯çº æ­£æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç½®ä¿¡åº¦ä¿¡æ¯çš„æç¤ºæ–¹æ³•ï¼Œå°†è¯çº§ä¸ç¡®å®šæ€§ä¼°è®¡ç›´æ¥åµŒå…¥åˆ°LLMè®­ç»ƒä¸­ï¼Œä»¥æé«˜è·¨è¯´è¯è€…å’Œæ•°æ®é›†çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¾®è°ƒLLaMA 3.1æ¨¡å‹å¹¶ä¸å…¶ä»–æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯­éŸ³è®¿é—®é¡¹ç›®å£è¯­å’ŒTORGOæ•°æ®é›†ä¸Šçš„è¯é”™è¯¯ç‡åˆ†åˆ«é™ä½äº†10%å’Œ47%ï¼Œè¯æ˜äº†ç½®ä¿¡åº¦æ„ŸçŸ¥å¾®è°ƒåœ¨å—æŸè¯­éŸ³ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹è¢«ç”¨ä½œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„åå¤„ç†æ¨¡å—ï¼Œä¸“æ³¨äºå£è¯­éšœç¢è€…çš„è¯­éŸ³è¯†åˆ«é”™è¯¯çº æ­£ã€‚</li>
<li>æå‡ºäº†åŸºäºç½®ä¿¡åº¦ä¿¡æ¯çš„æç¤ºæ–¹æ³•ï¼Œå°†ä¸ç¡®å®šæ€§ä¼°è®¡åµŒå…¥LLMè®­ç»ƒï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ–¹æ³•æŒ‡å¯¼æ¨¡å‹å®šä½åˆ°ASRçš„ä¸ç¡®å®šæ€§åŒºåŸŸï¼Œå‡å°‘è¿‡åº¦çº æ­£çš„æƒ…å†µã€‚</li>
<li>é€šè¿‡å¾®è°ƒLLaMA 3.1æ¨¡å‹è¿›è¡Œå®éªŒéªŒè¯ï¼Œè¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸ä»…åŸºäºè½¬å½•çš„å¾®è°ƒæ–¹æ³•å’ŒåŸºäºç½®ä¿¡åº¦çš„è¿‡æ»¤æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨è¯é”™è¯¯ç‡ä¸Šå–å¾—äº†æ˜¾è‘—å‡å°‘ã€‚</li>
<li>åœ¨è¯­éŸ³è®¿é—®é¡¹ç›®å£è¯­å’ŒTORGOæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f533e368c963fa609aa9428b80ffc153" align="middle">
<img src="https://picx.zhimg.com/v2-4202b496a1cffe20b772516c3b81222d" align="middle">
<img src="https://picx.zhimg.com/v2-4401dac56bea5eda576ce90c241d76cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c98a0b5e066e0c1a9d0734432dd7f7fc" align="middle">
<img src="https://picx.zhimg.com/v2-df7b2a5c9a24c195813daed4a3f8df5d" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DelRec-learning-delays-in-recurrent-spiking-neural-networks"><a href="#DelRec-learning-delays-in-recurrent-spiking-neural-networks" class="headerlink" title="DelRec: learning delays in recurrent spiking neural networks"></a>DelRec: learning delays in recurrent spiking neural networks</h2><p><strong>Authors:Alexandre Queant, Ulysse RanÃ§on, Benoit R Cottereau, TimothÃ©e Masquelier</strong></p>
<p>Spiking neural networks (SNNs) are a bio-inspired alternative to conventional real-valued deep learning models, with the potential for substantially higher energy efficiency. Interest in SNNs has recently exploded due to a major breakthrough: surrogate gradient learning (SGL), which allows training SNNs with backpropagation, strongly outperforming other approaches. In SNNs, each synapse is characterized not only by a weight but also by a transmission delay. While theoretical works have long suggested that trainable delays significantly enhance expressivity, practical methods for learning them have only recently emerged. Here, we introduce â€˜â€™DelRecâ€™â€™, the first SGL-based method to train axonal or synaptic delays in recurrent spiking layers, compatible with any spiking neuron model. DelRec leverages a differentiable interpolation technique to handle non-integer delays with well-defined gradients at training time. We show that trainable recurrent delays outperform feedforward ones, leading to new state-of-the-art (SOTA) on two challenging temporal datasets (Spiking Speech Command, an audio dataset, and Permuted Sequential MNIST, a vision one), and match the SOTA on the now saturated Spiking Heidelberg Digit dataset using only vanilla Leaky-Integrate-and-Fire neurons with stateless (instantaneous) synapses. Our results demonstrate that recurrent delays are critical for temporal processing in SNNs and can be effectively optimized with DelRec, paving the way for efficient deployment on neuromorphic hardware with programmable delays. Our code is available at : <a target="_blank" rel="noopener" href="https://github.com/alexmaxad/DelRec">https://github.com/alexmaxad/DelRec</a>. </p>
<blockquote>
<p>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰æ˜¯å—åˆ°ç”Ÿç‰©å¯å‘çš„ä¼ ç»Ÿå®å€¼æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå…·æœ‰æ›´é«˜çš„èƒ½æºæ•ˆç‡æ½œåŠ›ã€‚ç”±äºå¯¹æ›¿ä»£æ¢¯åº¦å­¦ä¹ ï¼ˆSGLï¼‰çš„é‡å¤§çªç ´ï¼ŒSNNsçš„å…´è¶£è¿‘æœŸçˆ†ç‚¸å¼å¢é•¿ï¼ŒSGLä½¿å¾—SNNså¯ä»¥ä½¿ç”¨åå‘ä¼ æ’­è¿›è¡Œè®­ç»ƒï¼Œå¹¶æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚åœ¨SNNsä¸­ï¼Œæ¯ä¸ªçªè§¦ä¸ä»…ç”±æƒé‡ç‰¹å¾åŒ–ï¼Œè€Œä¸”è¿˜ç”±ä¼ è¾“å»¶è¿Ÿç‰¹å¾åŒ–ã€‚è™½ç„¶ç†è®ºå·¥ä½œé•¿æœŸè¡¨æ˜å¯è®­ç»ƒçš„å»¶è¿Ÿä¼šæ˜¾è‘—å¢å¼ºè¡¨ç°åŠ›ï¼Œä½†å®è·µä¸­çš„å­¦ä¹ æ–¹æ³•æœ€è¿‘æ‰å‡ºç°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†â€œDelRecâ€ï¼Œè¿™æ˜¯åŸºäºSGLçš„æ–¹æ³•ï¼Œå¯åœ¨é€’å½’è„‰å†²å±‚ä¸­è®­ç»ƒè½´çªæˆ–çªè§¦å»¶è¿Ÿï¼Œä¸ä»»ä½•è„‰å†²ç¥ç»å…ƒæ¨¡å‹å…¼å®¹ã€‚DelRecåˆ©ç”¨å¯å¾®åˆ†çš„æ’å€¼æŠ€æœ¯æ¥å¤„ç†éæ•´æ•°å»¶è¿Ÿï¼Œåœ¨è®­ç»ƒæ—¶æœ‰å®šä¹‰æ˜ç¡®çš„æ¢¯åº¦ã€‚æˆ‘ä»¬æ˜¾ç¤ºå¯è®­ç»ƒçš„é€’å½’å»¶è¿Ÿä¼˜äºå‰é¦ˆå»¶è¿Ÿï¼Œä»è€Œåœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ—¶é—´æ•°æ®é›†ï¼ˆè„‰å†²è¯­éŸ³å‘½ä»¤ï¼ˆéŸ³é¢‘æ•°æ®é›†ï¼‰å’Œç½®æ¢åºåˆ—MNISTï¼ˆè§†è§‰æ•°æ®é›†ï¼‰ï¼‰ä¸Šè¾¾åˆ°äº†æœ€æ–°çŠ¶æ€ï¼Œå¹¶åœ¨ç°åœ¨å·²é¥±å’Œçš„Spiking Heidelberg Digitæ•°æ®é›†ä¸Šä½¿ç”¨ä»…ä½¿ç”¨æ™®é€šçš„æ³„æ¼ç§¯åˆ†å’Œç‚¹ç«ç¥ç»å…ƒä»¥åŠæ— çŠ¶æ€ï¼ˆç¬æ—¶ï¼‰çªè§¦è¾¾åˆ°äº†æœ€æ–°çŠ¶æ€ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€’å½’å»¶è¿Ÿå¯¹äºSNNsä¸­çš„æ—¶é—´å¤„ç†è‡³å…³é‡è¦ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨DelRecè¿›è¡Œæœ‰æ•ˆä¼˜åŒ–ï¼Œä¸ºåœ¨å…·æœ‰å¯ç¼–ç¨‹å»¶è¿Ÿçš„ç¥ç»å½¢æ€ç¡¬ä»¶ä¸Šæœ‰æ•ˆéƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/alexmaxad/DelRec%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/alexmaxad/DelRecæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24852v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œè„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰é¢†åŸŸå–å¾—é‡å¤§çªç ´ï¼Œå³å‡ºç°äº†ä¸€ç§åä¸ºä»£ç†æ¢¯åº¦å­¦ä¹ ï¼ˆSGLï¼‰çš„æŠ€æœ¯ï¼Œä½¿å¾—SNNså¯ä»¥é€šè¿‡åå‘ä¼ æ’­è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”æ€§èƒ½è¶…è¶Šäº†å…¶ä»–æ–¹æ³•ã€‚ç°åœ¨æœ‰ä¸€ç§åä¸ºDelRecçš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨é€’å½’è„‰å†²å±‚ä¸­è¿›è¡Œè½´çªæˆ–çªè§¦å»¶è¿Ÿçš„è®­ç»ƒï¼Œå¹¶ä¸”é€‚ç”¨äºä»»ä½•è„‰å†²ç¥ç»å…ƒæ¨¡å‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¯è®­ç»ƒçš„é€’å½’å»¶è¿Ÿæ€§èƒ½ä¼˜äºå‰é¦ˆå»¶è¿Ÿï¼Œå¹¶åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ—¶åºæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚è¿™è¡¨æ˜é€’å½’å»¶è¿Ÿå¯¹äºè„‰å†²ç¥ç»ç½‘ç»œä¸­çš„æ—¶åºå¤„ç†è‡³å…³é‡è¦ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡DelRecè¿›è¡Œæœ‰æ•ˆä¼˜åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰æ˜¯ä¸€ç§ç”Ÿç‰©å¯å‘çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå…·æœ‰æ›´é«˜çš„èƒ½æºæ•ˆç‡æ½œåŠ›ã€‚</li>
<li>ä»£ç†æ¢¯åº¦å­¦ä¹ ï¼ˆSGLï¼‰å…è®¸ä½¿ç”¨åå‘ä¼ æ’­è®­ç»ƒSNNsï¼Œä¸”æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>DelRecæ˜¯ç¬¬ä¸€ç§èƒ½å¤Ÿåœ¨é€’å½’è„‰å†²å±‚ä¸­è®­ç»ƒè½´çªæˆ–çªè§¦å»¶è¿Ÿçš„SGLæ–¹æ³•ã€‚</li>
<li>DelRecé€šè¿‡ä½¿ç”¨å¯å¾®åˆ†çš„æ’å€¼æŠ€æœ¯æ¥å¤„ç†éæ•´æ•°å»¶è¿Ÿï¼Œåœ¨è®­ç»ƒæ—¶å…·æœ‰æ˜ç¡®å®šä¹‰çš„æ¢¯åº¦ã€‚</li>
<li>å¯è®­ç»ƒçš„é€’å½’å»¶è¿Ÿæ€§èƒ½ä¼˜äºå‰é¦ˆå»¶è¿Ÿï¼Œåœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ—¶åºæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
<li>é€’å½’å»¶è¿Ÿå¯¹äºè„‰å†²ç¥ç»ç½‘ç»œä¸­çš„æ—¶åºå¤„ç†è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f686542d90ce007cfb9a12c3d389c888" align="middle">
<img src="https://picx.zhimg.com/v2-d9f67a7030a7ec9d0daa6ee8d7a70c98" align="middle">
<img src="https://picx.zhimg.com/v2-fd3ac2c47453daa5b6377b0594bad6ae.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Sparse-Autoencoders-Make-Audio-Foundation-Models-more-Explainable"><a href="#Sparse-Autoencoders-Make-Audio-Foundation-Models-more-Explainable" class="headerlink" title="Sparse Autoencoders Make Audio Foundation Models more Explainable"></a>Sparse Autoencoders Make Audio Foundation Models more Explainable</h2><p><strong>Authors:ThÃ©o Mariotte, Martin Lebourdais, Antonio AlmudÃ©var, Marie Tahon, Alfonso Ortega, Nicolas DuguÃ©</strong></p>
<p>Audio pretrained models are widely employed to solve various tasks in speech processing, sound event detection, or music information retrieval. However, the representations learned by these models are unclear, and their analysis mainly restricts to linear probing of the hidden representations. In this work, we explore the use of Sparse Autoencoders (SAEs) to analyze the hidden representations of pretrained models, focusing on a case study in singing technique classification. We first demonstrate that SAEs retain both information about the original representations and class labels, enabling their internal structure to provide insights into self-supervised learning systems. Furthermore, we show that SAEs enhance the disentanglement of vocal attributes, establishing them as an effective tool for identifying the underlying factors encoded in the representations. </p>
<blockquote>
<p>éŸ³é¢‘é¢„è®­ç»ƒæ¨¡å‹è¢«å¹¿æ³›åº”ç”¨äºè¯­éŸ³å¤„ç†ã€å£°éŸ³äº‹ä»¶æ£€æµ‹æˆ–éŸ³ä¹ä¿¡æ¯æ£€ç´¢ä¸­çš„å„ç§ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹æ‰€å­¦çš„è¡¨ç¤ºå½¢å¼å°šä¸æ¸…æ¥šï¼Œå…¶åˆ†æä¸»è¦å±€é™äºå¯¹éšè—è¡¨ç¤ºçš„çº¿æ€§æ¢æµ‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„éšè—è¡¨ç¤ºåˆ†æä¸­çš„åº”ç”¨ï¼Œä»¥æ­Œå”±æŠ€å·§åˆ†ç±»ä¸ºä¾‹è¿›è¡Œç ”ç©¶ã€‚æˆ‘ä»¬é¦–å…ˆè¯æ˜SAEæ—¢ä¿ç•™äº†åŸå§‹è¡¨ç¤ºçš„åŸå§‹ä¿¡æ¯åˆä¿ç•™äº†ç±»åˆ«æ ‡ç­¾ï¼Œä½¿å¾—å…¶å†…éƒ¨ç»“æ„èƒ½å¤Ÿä¸ºæˆ‘ä»¬æ·±å…¥äº†è§£è‡ªç›‘ç£å­¦ä¹ ç³»ç»Ÿæä¾›è§è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜SAEå¢å¼ºäº†è¯­éŸ³å±æ€§çš„è§£è€¦ï¼Œä½¿å…¶æˆä¸ºè¯†åˆ«è¡¨ç¤ºä¸­ç¼–ç çš„æ½œåœ¨å› ç´ çš„æœ‰æ•ˆå·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24793v1">PDF</a> 5 pages, 5 figures, 1 table, submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨Sparse Autoencodersï¼ˆSAEï¼‰åˆ†æé¢„è®­ç»ƒæ¨¡å‹éšè—è¡¨å¾çš„æ–¹æ³•ï¼Œå¹¶ä»¥æ­Œå”±æŠ€å·§åˆ†ç±»ä¸ºä¾‹è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚ç ”ç©¶å‘ç°SAEèƒ½å¤Ÿä¿ç•™åŸå§‹è¡¨å¾å’Œç±»åˆ«æ ‡ç­¾çš„ä¿¡æ¯ï¼Œå…¶å†…éƒ¨ç»“æ„æœ‰åŠ©äºäº†è§£è‡ªç›‘ç£å­¦ä¹ ç³»ç»Ÿçš„åŸç†ã€‚æ­¤å¤–ï¼ŒSAEè¿˜èƒ½å¢å¼ºå¯¹å—“éŸ³å±æ€§çš„è§£æ„ï¼Œæˆä¸ºè¯†åˆ«éšè—è¡¨å¾ä¸­ç¼–ç å› ç´ çš„æœ‰æ•ˆå·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒæ¨¡å‹å¹¿æ³›åº”ç”¨äºè¯­éŸ³å¤„ç†ã€å£°éŸ³äº‹ä»¶æ£€æµ‹æˆ–éŸ³ä¹ä¿¡æ¯æ£€ç´¢ç­‰ä»»åŠ¡ã€‚</li>
<li>é¢„è®­ç»ƒæ¨¡å‹çš„éšè—è¡¨å¾åˆ†æä¸»è¦å±€é™äºçº¿æ€§æ¢æµ‹ã€‚</li>
<li>SAEèƒ½å¤Ÿä¿ç•™åŸå§‹è¡¨å¾å’Œç±»åˆ«æ ‡ç­¾çš„ä¿¡æ¯ã€‚</li>
<li>SAEçš„å†…éƒ¨ç»“æ„æœ‰åŠ©äºäº†è§£è‡ªç›‘ç£å­¦ä¹ ç³»ç»Ÿçš„åŸç†ã€‚</li>
<li>SAEåœ¨æ­Œå”±æŠ€å·§åˆ†ç±»çš„æ¡ˆä¾‹ä¸­è¡¨ç°å‡ºå¢å¼ºè§£æ„å—“éŸ³å±æ€§çš„èƒ½åŠ›ã€‚</li>
<li>SAEæˆä¸ºè¯†åˆ«é¢„è®­ç»ƒæ¨¡å‹éšè—è¡¨å¾ä¸­ç¼–ç å› ç´ çš„æœ‰æ•ˆå·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-83e4855378f9de3cfb861ab1021ced69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4597922612051e9adfa30ac7e55eaaa1" align="middle">
<img src="https://picx.zhimg.com/v2-483f306ebb868845254ca39755562e37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b459bd90408d43bc0253216c5bebfef" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VSSFlow-Unifying-Video-conditioned-Sound-and-Speech-Generation-via-Joint-Learning"><a href="#VSSFlow-Unifying-Video-conditioned-Sound-and-Speech-Generation-via-Joint-Learning" class="headerlink" title="VSSFlow: Unifying Video-conditioned Sound and Speech Generation via   Joint Learning"></a>VSSFlow: Unifying Video-conditioned Sound and Speech Generation via   Joint Learning</h2><p><strong>Authors:Xin Cheng, Yuyue Wang, Xihua Wang, Yihan Wu, Kaisi Guan, Yijing Chen, Peng Zhang, Xiaojiang Liu, Meng Cao, Ruihua Song</strong></p>
<p>Video-conditioned sound and speech generation, encompassing video-to-sound (V2S) and visual text-to-speech (VisualTTS) tasks, are conventionally addressed as separate tasks, with limited exploration to unify them within a signle framework. Recent attempts to unify V2S and VisualTTS face challenges in handling distinct condition types (e.g., heterogeneous video and transcript conditions) and require complex training stages. Unifying these two tasks remains an open problem. To bridge this gap, we present VSSFlow, which seamlessly integrates both V2S and VisualTTS tasks into a unified flow-matching framework. VSSFlow uses a novel condition aggregation mechanism to handle distinct input signals. We find that cross-attention and self-attention layer exhibit different inductive biases in the process of introducing condition. Therefore, VSSFlow leverages these inductive biases to effectively handle different representations: cross-attention for ambiguous video conditions and self-attention for more deterministic speech transcripts. Furthermore, contrary to the prevailing belief that joint training on the two tasks requires complex training strategies and may degrade performance, we find that VSSFlow benefits from the end-to-end joint learning process for sound and speech generation without extra designs on training stages. Detailed analysis attributes it to the learned general audio prior shared between tasks, which accelerates convergence, enhances conditional generation, and stabilizes the classifier-free guidance process. Extensive experiments demonstrate that VSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S and VisualTTS benchmarks, underscoring the critical potential of unified generative models. </p>
<blockquote>
<p>è§†é¢‘æ¡ä»¶çš„å£°éŸ³å’Œè¯­éŸ³ç”Ÿæˆï¼ŒåŒ…æ‹¬è§†é¢‘åˆ°å£°éŸ³ï¼ˆV2Sï¼‰å’Œè§†è§‰æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆVisualTTSï¼‰ä»»åŠ¡ï¼Œä¼ ç»Ÿä¸Šè¢«è§†ä¸ºå•ç‹¬çš„ä»»åŠ¡ï¼Œå¹¶ä¸”åœ¨ç»Ÿä¸€æ¡†æ¶å†…å¯¹å…¶è¿›è¡Œæ¢ç´¢çš„å°è¯•æœ‰é™ã€‚æœ€è¿‘å°è¯•å°†V2Så’ŒVisualTTSç»Ÿä¸€èµ·æ¥åœ¨å¤„ç†ä¸åŒæ¡ä»¶ç±»å‹ï¼ˆä¾‹å¦‚ï¼Œä¸åŒçš„è§†é¢‘å’Œæ–‡æœ¬æ¡ä»¶ï¼‰æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶ä¸”éœ€è¦å¤æ‚çš„è®­ç»ƒé˜¶æ®µã€‚ç»Ÿä¸€è¿™ä¸¤ä¸ªä»»åŠ¡ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„é—®é¢˜ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VSSFlowï¼Œå®ƒæ— ç¼åœ°å°†V2Så’ŒVisualTTSä»»åŠ¡é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æµåŒ¹é…æ¡†æ¶ä¸­ã€‚VSSFlowä½¿ç”¨ä¸€ç§æ–°å‹çš„æ¡ä»¶èšåˆæœºåˆ¶æ¥å¤„ç†ä¸åŒçš„è¾“å…¥ä¿¡å·ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨å¼•å…¥æ¡ä»¶çš„è¿‡ç¨‹ä¸­ï¼Œè·¨æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å±‚è¡¨ç°å‡ºä¸åŒçš„å½’çº³åè§ã€‚å› æ­¤ï¼ŒVSSFlowåˆ©ç”¨è¿™äº›å½’çº³åè§æ¥æœ‰æ•ˆåœ°å¤„ç†ä¸åŒçš„è¡¨ç¤ºå½¢å¼ï¼šè·¨æ³¨æ„åŠ›ç”¨äºæ¨¡ç³Šçš„è§†é¢‘æ¡ä»¶ï¼Œè‡ªæ³¨æ„åŠ›ç”¨äºæ›´ç¡®å®šçš„è¯­éŸ³æ–‡æœ¬ã€‚æ­¤å¤–ï¼Œä¸æ™®éçš„è§‚ç‚¹ç›¸åï¼Œå³ä¸¤ä¸ªä»»åŠ¡çš„è”åˆè®­ç»ƒéœ€è¦å¤æ‚çš„è®­ç»ƒç­–ç•¥å¹¶å¯èƒ½é™ä½æ€§èƒ½ï¼Œæˆ‘ä»¬å‘ç°VSSFlowå—ç›Šäºå£°éŸ³å’Œè¯­éŸ³ç”Ÿæˆçš„ç«¯åˆ°ç«¯è”åˆå­¦ä¹ è¿‡ç¨‹ï¼Œè€Œæ— éœ€åœ¨è®­ç»ƒé˜¶æ®µè¿›è¡Œé¢å¤–è®¾è®¡ã€‚è¯¦ç»†åˆ†æå°†å…¶å½’åŠŸäºä»»åŠ¡ä¹‹é—´å­¦åˆ°çš„é€šç”¨éŸ³é¢‘å…ˆéªŒçš„å…±äº«ï¼Œè¿™åŠ é€Ÿäº†æ”¶æ•›ï¼Œå¢å¼ºäº†æ¡ä»¶ç”Ÿæˆï¼Œå¹¶ç¨³å®šäº†æ— åˆ†ç±»å™¨æŒ‡å¯¼è¿‡ç¨‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVSSFlowåœ¨V2Så’ŒVisualTTSåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æœ€å…ˆè¿›çš„é¢†åŸŸç‰¹å®šåŸºå‡†æµ‹è¯•ï¼Œçªæ˜¾äº†ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24773v1">PDF</a> Paper Under Review</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†é¢‘æ¡ä»¶éŸ³é¢‘ä¸è¯­éŸ³ç”Ÿæˆæ¶µç›–è§†é¢‘è½¬å£°éŸ³ï¼ˆV2Sï¼‰ä¸è§†è§‰æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆVisualTTSï¼‰ä»»åŠ¡ï¼Œä¼ ç»Ÿä¸Šè¢«è§†ä¸ºç‹¬ç«‹å¤„ç†ã€‚è¿‘æœŸå°è¯•ç»Ÿä¸€V2Så’ŒVisualTTSåœ¨å¤„ç†ä¸åŒæ¡ä»¶ç±»å‹ï¼ˆå¦‚å¼‚è´¨è§†é¢‘å’Œæ–‡å­—è„šæœ¬ï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶éœ€è¦å¤æ‚çš„è®­ç»ƒé˜¶æ®µã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºVSSFlowï¼Œå®ƒå°†V2Så’ŒVisualTTSä»»åŠ¡æ— ç¼é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æµåŒ¹é…æ¡†æ¶ä¸­ã€‚VSSFlowé‡‡ç”¨æ–°å‹æ¡ä»¶èšåˆæœºåˆ¶æ¥å¤„ç†ä¸åŒçš„è¾“å…¥ä¿¡å·ã€‚æˆ‘ä»¬å‘ç°äº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å±‚åœ¨å¤„ç†æ¡ä»¶å¼•å…¥è¿‡ç¨‹ä¸­å±•ç°å‡ºä¸åŒçš„å½’çº³åç½®ã€‚å› æ­¤ï¼ŒVSSFlowåˆ©ç”¨è¿™äº›å½’çº³åç½®æ¥æœ‰æ•ˆå¤„ç†ä¸åŒçš„è¡¨ç¤ºå½¢å¼ï¼šäº¤å‰æ³¨æ„åŠ›ç”¨äºæ¨¡ç³Šçš„è§†é¢‘æ¡ä»¶ï¼Œè‡ªæ³¨æ„åŠ›ç”¨äºæ›´ç¡®å®šçš„è¯­éŸ³è„šæœ¬ã€‚æ­¤å¤–ï¼Œå°½ç®¡æ™®éè§‚ç‚¹è®¤ä¸ºå¯¹ä¸¤ä¸ªä»»åŠ¡è¿›è¡Œè”åˆè®­ç»ƒéœ€è¦å¤æ‚çš„è®­ç»ƒç­–ç•¥å¹¶å¯èƒ½å½±å“æ€§èƒ½ï¼Œä½†æˆ‘ä»¬å‘ç°VSSFlowå—ç›Šäºå£°éŸ³å’Œè¯­éŸ³ç”Ÿæˆçš„ç«¯åˆ°ç«¯è”åˆå­¦ä¹ è¿‡ç¨‹ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒé˜¶æ®µè®¾è®¡ã€‚è¿™å¾—ç›Šäºä»»åŠ¡é—´å­¦ä¹ çš„é€šç”¨éŸ³é¢‘å…ˆéªŒï¼Œå®ƒåŠ é€Ÿæ”¶æ•›ï¼Œå¢å¼ºæ¡ä»¶ç”Ÿæˆï¼Œå¹¶ç¨³å®šæ— åˆ†ç±»æŒ‡å¯¼è¿‡ç¨‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVSSFlowåœ¨V2Så’ŒVisualTTSåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æœ€æ–°é¢†åŸŸçš„ä¸“é¡¹æ–¹æ³•ï¼Œçªæ˜¾å‡ºç»Ÿä¸€ç”Ÿæˆæ¨¡å‹çš„å…³é”®æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VSSFlowæå‡ºä¸€ä¸ªç»Ÿä¸€çš„æµåŒ¹é…æ¡†æ¶ï¼Œæ•´åˆè§†é¢‘è½¬å£°éŸ³ï¼ˆV2Sï¼‰å’Œè§†è§‰æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆVisualTTSï¼‰ä»»åŠ¡ã€‚</li>
<li>é‡‡ç”¨æ–°å‹æ¡ä»¶èšåˆæœºåˆ¶å¤„ç†ä¸åŒçš„è¾“å…¥ä¿¡å·ã€‚</li>
<li>äº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å±‚åœ¨å¤„ç†ä¸åŒæ¡ä»¶ç±»å‹æ—¶å±•ç°å‡ºä¸åŒå½’çº³åç½®ï¼ŒVSSFlowåˆ©ç”¨è¿™ä¸€ç‰¹æ€§ä¼˜åŒ–å¤„ç†ã€‚</li>
<li>VSSFlowå—ç›Šäºç«¯åˆ°ç«¯çš„è”åˆå­¦ä¹ è¿‡ç¨‹ï¼Œæ— éœ€å¤æ‚è®­ç»ƒç­–ç•¥æˆ–é¢å¤–è®­ç»ƒé˜¶æ®µã€‚</li>
<li>é€šç”¨éŸ³é¢‘å…ˆéªŒåœ¨è”åˆå­¦ä¹ ä»»åŠ¡ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼ŒåŠ é€Ÿæ”¶æ•›å¹¶å¢å¼ºæ€§èƒ½ã€‚</li>
<li>VSSFlowåœ¨V2Så’ŒVisualTTSåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f8ca6153c228acbaa456cf6931821840" align="middle">
<img src="https://picx.zhimg.com/v2-aa383b8af756e9c393499fe593a9596d" align="middle">
<img src="https://pic1.zhimg.com/v2-3beec9a7fa106c34c7d5761339c47c73.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Robust-Multi-Scale-Framework-with-Test-Time-Adaptation-for-sEEG-Based-Speech-Decoding"><a href="#A-Robust-Multi-Scale-Framework-with-Test-Time-Adaptation-for-sEEG-Based-Speech-Decoding" class="headerlink" title="A Robust Multi-Scale Framework with Test-Time Adaptation for sEEG-Based   Speech Decoding"></a>A Robust Multi-Scale Framework with Test-Time Adaptation for sEEG-Based   Speech Decoding</h2><p><strong>Authors:Suli Wang, Yang-yang Li, Siqi Cai, Haizhou Li</strong></p>
<p>Decoding speech from stereo-electroencephalography (sEEG) signals has emerged as a promising direction for brain-computer interfaces (BCIs). Its clinical applicability, however, is limited by the inherent non-stationarity of neural signals, which causes domain shifts between training and testing, undermining decoding reliability. To address this challenge, a two-stage framework is proposed for enhanced robustness. First, a multi-scale decomposable mixing (MDM) module is introduced to model the hierarchical temporal dynamics of speech production, learning stable multi-timescale representations from sEEG signals. Second, a source-free online test-time adaptation (TTA) method performs entropy minimization to adapt the model to distribution shifts during inference. Evaluations on the public DU-IN spoken word decoding benchmark show that the approach outperforms state-of-the-art models, particularly in challenging cases. This study demonstrates that combining invariant feature learning with online adaptation is a principled strategy for developing reliable BCI systems. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/lyyi599/MDM-TENT">https://github.com/lyyi599/MDM-TENT</a>. </p>
<blockquote>
<p>ä»ç«‹ä½“è„‘ç”µå›¾ï¼ˆsEEGï¼‰ä¿¡å·ä¸­è§£ç è¯­éŸ³å·²æˆä¸ºè„‘æœºæ¥å£ï¼ˆBCIï¼‰çš„ä¸€ä¸ªæœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œå…¶ä¸´åºŠåº”ç”¨çš„å¯è¡Œæ€§å—é™äºç¥ç»ä¿¡å·å›ºæœ‰çš„éå¹³ç¨³æ€§ï¼Œå¯¼è‡´è®­ç»ƒå’Œæµ‹è¯•ä¹‹é—´çš„é¢†åŸŸåç§»ï¼Œä»è€Œé™ä½äº†è§£ç çš„å¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ä»¥å¢å¼ºç¨³å¥æ€§ã€‚é¦–å…ˆï¼Œå¼•å…¥äº†ä¸€ç§å¤šå°ºåº¦å¯åˆ†è§£æ··åˆï¼ˆMDMï¼‰æ¨¡å—ï¼Œä»¥æ¨¡æ‹Ÿè¯­éŸ³äº§ç”Ÿçš„åˆ†å±‚æ—¶é—´åŠ¨æ€ï¼Œä»sEEGä¿¡å·ä¸­å­¦ä¹ ç¨³å®šçš„å¤šå°ºåº¦è¡¨ç¤ºã€‚å…¶æ¬¡ï¼Œæ— æºåœ¨çº¿æµ‹è¯•æ—¶é—´é€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•æ‰§è¡Œç†µæœ€å°åŒ–ï¼Œä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿æ¨¡å‹é€‚åº”åˆ†å¸ƒå˜åŒ–ã€‚åœ¨å…¬å…±DU-INå£è¯­å•è¯è§£ç åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºæœ€æ–°æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜çš„æƒ…å†µä¸‹ã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜ï¼Œå°†ä¸å˜ç‰¹å¾å­¦ä¹ ä¸åœ¨çº¿é€‚åº”ç›¸ç»“åˆæ˜¯å¼€å‘å¯é BCIç³»ç»Ÿçš„ä¸€ç§æœ‰åŸåˆ™çš„ç­–ç•¥ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lyyi599/MDM-TENT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lyyi599/MDM-TENTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24700v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç«‹ä½“è„‘ç”µå›¾ï¼ˆsEEGï¼‰ä¿¡å·çš„è¯­éŸ³è§£ç åœ¨è„‘æœºæ¥å£ï¼ˆBCIï¼‰é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¥ç»ä¿¡å·çš„å›ºæœ‰éå¹³ç¨³æ€§ï¼Œå…¶ä¸´åºŠåº”ç”¨å—åˆ°é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ä»¥æé«˜æ¨¡å‹ç¨³å¥æ€§ï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡å¼•å…¥å¤šå°ºåº¦å¯åˆ†è§£æ··åˆï¼ˆMDMï¼‰æ¨¡å—å»ºæ¨¡è¯­éŸ³äº§ç”Ÿçš„å±‚æ¬¡æ—¶é—´åŠ¨æ€ï¼Œä»sEEGä¿¡å·ä¸­å­¦ä¹ ç¨³å®šçš„å¤šå°ºåº¦è¡¨ç¤ºï¼›ç¬¬äºŒé˜¶æ®µé‡‡ç”¨æ— æºåœ¨çº¿æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•è¿›è¡Œç†µæœ€å°åŒ–ï¼Œä½¿æ¨¡å‹åœ¨æ¨ç†æ—¶é€‚åº”åˆ†å¸ƒå˜åŒ–ã€‚åœ¨å…¬å¼€DU-INå£è¯­è¯è§£ç åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºæœ€æ–°æ¨¡å‹ï¼Œå°¤å…¶åœ¨æŒ‘æˆ˜æ€§åœºæ™¯ä¸‹è¡¨ç°æ›´ä¼˜å¼‚ã€‚æœ¬ç ”ç©¶è¯æ˜äº†ç»“åˆä¸å˜ç‰¹å¾å­¦ä¹ ä¸åœ¨çº¿è‡ªé€‚åº”æ˜¯å¼€å‘å¯é BCIç³»ç»Ÿçš„æœ‰æ•ˆç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç«‹ä½“è„‘ç”µå›¾ï¼ˆsEEGï¼‰ä¿¡å·åœ¨è„‘æœºæ¥å£ï¼ˆBCIï¼‰ä¸­çš„è¯­éŸ³è§£ç è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
<li>ç¥ç»ä¿¡å·çš„å›ºæœ‰éå¹³ç¨³æ€§é™åˆ¶äº†sEEGåœ¨ä¸´åºŠåº”ç”¨ä¸­çš„å¯é æ€§ã€‚</li>
<li>æå‡ºä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ä»¥æé«˜æ¨¡å‹ç¨³å¥æ€§ï¼ŒåŒ…æ‹¬å¤šå°ºåº¦å¯åˆ†è§£æ··åˆï¼ˆMDMï¼‰æ¨¡å—å’Œæ— æºåœ¨çº¿æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•ã€‚</li>
<li>MDMæ¨¡å—å­¦ä¹ ç¨³å®šçš„å¤šå°ºåº¦è¡¨ç¤ºï¼Œä»¥å»ºæ¨¡è¯­éŸ³äº§ç”Ÿçš„å±‚æ¬¡æ—¶é—´åŠ¨æ€ã€‚</li>
<li>TTAæ–¹æ³•é€šè¿‡ç†µæœ€å°åŒ–åœ¨æ¨ç†æ—¶é€‚åº”åˆ†å¸ƒå˜åŒ–ã€‚</li>
<li>åœ¨DU-INå£è¯­è¯è§£ç åŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–æœ€æ–°æ¨¡å‹ã€‚</li>
<li>ç»“åˆä¸å˜ç‰¹å¾å­¦ä¹ ä¸åœ¨çº¿è‡ªé€‚åº”æ˜¯å¼€å‘å¯é BCIç³»ç»Ÿçš„æœ‰æ•ˆç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ac1edc90f1d6f9c67565f58568f6dc4" align="middle">
<img src="https://pic1.zhimg.com/v2-017029dc7602ae50819b6005fc1d8ea5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-201edeb49dada1a8685ddbe07e015678" align="middle">
<img src="https://pica.zhimg.com/v2-9edfff694e40e2f7d71c50d3cd48302e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HiKE-Hierarchical-Evaluation-Framework-for-Korean-English-Code-Switching-Speech-Recognition"><a href="#HiKE-Hierarchical-Evaluation-Framework-for-Korean-English-Code-Switching-Speech-Recognition" class="headerlink" title="HiKE: Hierarchical Evaluation Framework for Korean-English   Code-Switching Speech Recognition"></a>HiKE: Hierarchical Evaluation Framework for Korean-English   Code-Switching Speech Recognition</h2><p><strong>Authors:Gio Paik, Yongbeom Kim, Soungmin Lee, Sangmin Ahn, Chanwoo Kim</strong></p>
<p>Despite advances in multilingual automatic speech recognition (ASR), code-switching (CS), the mixing of languages within an utterance common in daily speech, remains a severely underexplored challenge. In this paper, we introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the first globally accessible evaluation framework for Korean-English CS, aiming to provide a means for the precise evaluation of multilingual ASR models and to foster research in the field. The proposed framework not only consists of high-quality, natural CS data across various topics, but also provides meticulous loanword labels and a hierarchical CS-level labeling scheme (word, phrase, and sentence) that together enable a systematic evaluation of a modelâ€™s ability to handle each distinct level of code-switching. Through evaluations of diverse multilingual ASR models and fine-tuning experiments, this paper demonstrates that while most multilingual ASR models initially struggle with CS-ASR, this capability can be enabled through fine-tuning with CS data. HiKE will be available at <a target="_blank" rel="noopener" href="https://github.com/ThetaOne-AI/HiKE">https://github.com/ThetaOne-AI/HiKE</a>. </p>
<blockquote>
<p>å°½ç®¡å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯æœ‰æ‰€è¿›æ­¥ï¼Œä½†è¯­è¨€åˆ‡æ¢ï¼ˆCSï¼‰åœ¨æ—¥å¸¸å¯¹è¯ä¸­å¸¸è§çš„æ··åˆè¯­è¨€ç°è±¡ä»ç„¶æ˜¯ä¸€ä¸ªè¢«ä¸¥é‡å¿½è§†çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†HiKEï¼šåˆ†å±‚éŸ©è‹±è¯­è¨€åˆ‡æ¢åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯é¦–ä¸ªé¢å‘éŸ©è‹±è¯­è¨€åˆ‡æ¢çš„å…¨çƒå¯è®¿é—®è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æä¾›ç²¾ç¡®è¯„ä¼°å¤šè¯­ç§ASRæ¨¡å‹çš„æ‰‹æ®µï¼Œå¹¶æ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚æ‰€æå‡ºçš„æ¡†æ¶ä¸ä»…åŒ…æ‹¬é«˜è´¨é‡ã€è·¨å„ç§ä¸»é¢˜çš„å¤©ç„¶è¯­è¨€åˆ‡æ¢æ•°æ®ï¼Œè¿˜æä¾›ç»†è‡´çš„å€Ÿè¯æ ‡ç­¾å’Œåˆ†å±‚è¯­è¨€åˆ‡æ¢çº§åˆ«æ ‡ç­¾æ–¹æ¡ˆï¼ˆå•è¯ã€çŸ­è¯­å’Œå¥å­ï¼‰ï¼Œå®ƒä»¬å…±åŒä½¿å¾—ç³»ç»Ÿè¯„ä»·æ¨¡å‹å¤„ç†å„ç§ç‹¬ç‰¹çº§åˆ«çš„è¯­è¨€åˆ‡æ¢èƒ½åŠ›æˆä¸ºå¯èƒ½ã€‚é€šè¿‡å¯¹å„ç§å¤šè¯­ç§ASRæ¨¡å‹çš„è¯„ä¼°å’Œå¾®è°ƒå®éªŒï¼Œæœ¬æ–‡è¡¨æ˜ï¼Œè™½ç„¶å¤§å¤šæ•°å¤šè¯­ç§ASRæ¨¡å‹æœ€åˆåœ¨è¯­è¨€åˆ‡æ¢æ–¹é¢é‡åˆ°å›°éš¾ï¼Œä½†é€šè¿‡ç”¨è¯­è¨€åˆ‡æ¢æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œè¿™ç§èƒ½åŠ›æ˜¯å¯ä»¥å®ç°çš„ã€‚HiKEå°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ThetaOne-AI/HiKE%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/ThetaOne-AI/HiKEä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24613v1">PDF</a> 5 pages, 2 figures, Submitted to ICASSP2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†HiKEï¼šä¸€ä¸ªé¢å‘éŸ©è¯­-è‹±è¯­ä»£ç åˆ‡æ¢çš„å±‚æ¬¡åŒ–åŸºå‡†æµ‹è¯•å¹³å°ã€‚è¯¥å¹³å°æ—¨åœ¨æä¾›ç²¾ç¡®è¯„ä¼°å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„èƒ½åŠ›ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸç ”ç©¶ã€‚HiKEä¸ä»…åŒ…å«é«˜è´¨é‡çš„è‡ªç„¶ä»£ç åˆ‡æ¢æ•°æ®ï¼Œè¿˜æä¾›è¯¦ç»†çš„å€Ÿè¯æ ‡ç­¾å’Œå±‚æ¬¡åŒ–çš„ä»£ç åˆ‡æ¢çº§åˆ«æ ‡ç­¾æ–¹æ¡ˆï¼ˆå•è¯ã€çŸ­è¯­å’Œå¥å­ï¼‰ï¼Œé€šè¿‡è¯„ä¼°å’Œå¾®è°ƒå®éªŒè¡¨æ˜ï¼Œå¤§å¤šæ•°å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹åœ¨ä»£ç åˆ‡æ¢è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸Šå…·æœ‰ä¸€å®šæ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç åˆ‡æ¢ï¼ˆCSï¼‰åœ¨æ—¥å¸¸è¯­è¨€ä¸­æ™®éå­˜åœ¨ï¼Œä½†åœ¨å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­ä»æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†ç ”ç©¶çš„æŒ‘æˆ˜ã€‚</li>
<li>HiKEæ˜¯é¦–ä¸ªé¢å‘éŸ©è¯­-è‹±è¯­ä»£ç åˆ‡æ¢çš„å…¨çƒæ€§è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ç²¾ç¡®è¯„ä¼°å¤šè¯­è¨€ASRæ¨¡å‹ã€‚</li>
<li>HiKEæä¾›äº†é«˜è´¨é‡çš„è‡ªç„¶ä»£ç åˆ‡æ¢æ•°æ®ï¼Œæ¶µç›–å„ç§ä¸»é¢˜ã€‚</li>
<li>HiKEå…·æœ‰è¯¦ç»†çš„å€Ÿè¯æ ‡ç­¾å’Œå±‚æ¬¡åŒ–çš„ä»£ç åˆ‡æ¢çº§åˆ«æ ‡ç­¾æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å•è¯ã€çŸ­è¯­å’Œå¥å­ã€‚</li>
<li>å¤šæ•°å¤šè¯­è¨€ASRæ¨¡å‹åœ¨åˆå§‹é˜¶æ®µå¯¹ä»£ç åˆ‡æ¢çš„è¯†åˆ«å­˜åœ¨å›°éš¾ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨HiKEå¹³å°ä¸Šçš„æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æé«˜æ¨¡å‹å¯¹ä»£ç åˆ‡æ¢çš„è¯†åˆ«èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24613">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a2aa9f09f869f02d84a8cd44b93c20c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e72cdfccffaa698d4877eb21572839b4" align="middle">
<img src="https://picx.zhimg.com/v2-7fc5b07b51a558ef0c483d84b124b138" align="middle">
<img src="https://picx.zhimg.com/v2-e47cbd129fa12e9c768b7f7700f8eb23" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Single-Channel-Speech-Separation-with-a-Diffusion-Prior-under-Speaker-Embedding-Guidance"><a href="#Unsupervised-Single-Channel-Speech-Separation-with-a-Diffusion-Prior-under-Speaker-Embedding-Guidance" class="headerlink" title="Unsupervised Single-Channel Speech Separation with a Diffusion Prior   under Speaker-Embedding Guidance"></a>Unsupervised Single-Channel Speech Separation with a Diffusion Prior   under Speaker-Embedding Guidance</h2><p><strong>Authors:Runwu Shi, Kai Li, Chang Li, Jiang Wang, Sihan Tan, Kazuhiro Nakadai</strong></p>
<p>Speech separation is a fundamental task in audio processing, typically addressed with fully supervised systems trained on paired mixtures. While effective, such systems typically rely on synthetic data pipelines, which may not reflect real-world conditions. Instead, we revisit the source-model paradigm, training a diffusion generative model solely on anechoic speech and formulating separation as a diffusion inverse problem. However, unconditional diffusion models lack speaker-level conditioning, they can capture local acoustic structure but produce temporally inconsistent speaker identities in separated sources. To address this limitation, we propose Speaker-Embedding guidance that, during the reverse diffusion process, maintains speaker coherence within each separated track while driving embeddings of different speakers further apart. In addition, we propose a new separation-oriented solver tailored for speech separation, and both strategies effectively enhance performance on the challenging task of unsupervised source-model-based speech separation, as confirmed by extensive experimental results. Audio samples and code are available at <a target="_blank" rel="noopener" href="https://runwushi.github.io/UnSepDiff_demo">https://runwushi.github.io/UnSepDiff_demo</a>. </p>
<blockquote>
<p>è¯­éŸ³åˆ†ç¦»æ˜¯éŸ³é¢‘å¤„ç†ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œé€šå¸¸é€šè¿‡ä½¿ç”¨åœ¨é…å¯¹æ··åˆç‰©ä¸Šè®­ç»ƒçš„å®Œå…¨ç›‘ç£ç³»ç»Ÿæ¥è§£å†³ã€‚è™½ç„¶è¿™äº›æ–¹æ³•å¾ˆæœ‰æ•ˆï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºåˆæˆæ•°æ®ç®¡é“ï¼Œå¯èƒ½æ— æ³•åæ˜ çœŸå®ä¸–ç•Œçš„æ¡ä»¶ã€‚ç›¸åï¼Œæˆ‘ä»¬é‡æ–°è®¿é—®æºæ¨¡å‹èŒƒå¼ï¼Œä»…ä½¿ç”¨æ— å›å£°è¯­éŸ³è®­ç»ƒæ‰©æ•£ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶å°†åˆ†ç¦»é—®é¢˜è¡¨è¿°ä¸ºæ‰©æ•£åé—®é¢˜ã€‚ç„¶è€Œï¼Œæ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ç¼ºä¹è¯´è¯äººçº§åˆ«çš„æ¡ä»¶çº¦æŸï¼Œå®ƒä»¬å¯ä»¥æ•æ‰å±€éƒ¨å£°å­¦ç»“æ„ï¼Œä½†åœ¨åˆ†ç¦»çš„æºä¸­äº§ç”Ÿæ—¶é—´ä¸Šä¸ä¸€è‡´çš„è¯´è¯äººèº«ä»½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†è¯´è¯äººåµŒå…¥å¼•å¯¼ï¼ˆSpeaker-Embedding guidanceï¼‰ï¼Œåœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­ä¿æŒæ¯ä¸ªåˆ†ç¦»è½¨è¿¹å†…çš„è¯´è¯äººä¸€è‡´æ€§ï¼ŒåŒæ—¶ä½¿ä¸åŒè¯´è¯äººçš„åµŒå…¥å½¼æ­¤è¿œç¦»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é’ˆå¯¹è¯­éŸ³åˆ†ç¦»æå‡ºäº†ä¸€ç§æ–°çš„é¢å‘åˆ†ç¦»çš„æ±‚è§£å™¨ï¼ˆseparation-oriented solverï¼‰ï¼Œè¿™ä¸¤ç§ç­–ç•¥å‡æœ‰æ•ˆåœ°æé«˜äº†åŸºäºæºæ¨¡å‹çš„ç›‘ç£è¯­éŸ³åˆ†ç¦»çš„å¤æ‚ä»»åŠ¡æ€§èƒ½ï¼Œå¹¿æ³›çš„å®éªŒç»“æœè¯å®äº†è¿™ä¸€ç‚¹ã€‚éŸ³é¢‘æ ·æœ¬å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://runwushi.github.io/UnSepDiff_demo%E8%8E%B7%E5%8F%96%E3%80%82">https://runwushi.github.io/UnSepDiff_demoè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24395v1">PDF</a> 5 pages, 2 figures, submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­éŸ³åˆ†ç¦»ä½œä¸ºéŸ³é¢‘å¤„ç†ä¸­çš„ä¸€é¡¹åŸºç¡€ä»»åŠ¡ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºåˆæˆæ•°æ®ç®¡é“çš„å…¨ç›‘ç£ç³»ç»Ÿè¿›è¡Œè®­ç»ƒï¼Œè¿™å¯èƒ½æ— æ³•åæ˜ çœŸå®ä¸–ç•Œçš„æƒ…å†µã€‚ä¸ºæ­¤ï¼Œæ–‡ç« é‡æ–°æ¢è®¨äº†æºæ¨¡å‹èŒƒå¼ï¼Œé€šè¿‡è®­ç»ƒä»…é€‚ç”¨äºæ— å›å£°è¯­éŸ³çš„æ‰©æ•£ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶å°†åˆ†ç¦»é—®é¢˜è¡¨è¿°ä¸ºæ‰©æ•£é€†é—®é¢˜ã€‚ä¸ºè§£å†³æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ç¼ºä¹è¯´è¯äººçº§åˆ«çš„æ¡ä»¶é™åˆ¶é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†è¯´è¯äººåµŒå…¥å¼•å¯¼æœºåˆ¶ï¼Œåœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­ä¿æŒæ¯ä¸ªåˆ†ç¦»è½¨è¿¹çš„è¯´è¯äººä¸€è‡´æ€§ï¼ŒåŒæ—¶ä½¿ä¸åŒè¯´è¯äººçš„åµŒå…¥è¿›ä¸€æ­¥åˆ†ç¦»ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æå‡ºäº†ä¸€ç§é’ˆå¯¹è¯­éŸ³åˆ†ç¦»çš„æ–°åˆ†ç¦»å¯¼å‘æ±‚è§£å™¨ï¼Œè¿™ä¸¤ç§ç­–ç•¥å‡èƒ½æœ‰æ•ˆæå‡åŸºäºæºæ¨¡å‹çš„éç›‘ç£è¯­éŸ³åˆ†ç¦»çš„æ€§èƒ½ï¼Œé€šè¿‡å¹¿æ³›çš„å®éªŒç»“æœå¾—åˆ°äº†éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³åˆ†ç¦»æ˜¯éŸ³é¢‘å¤„ç†ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºåˆæˆæ•°æ®ç®¡é“çš„å…¨ç›‘ç£ç³»ç»Ÿï¼Œå­˜åœ¨æ— æ³•åæ˜ çœŸå®ä¸–ç•Œæƒ…å†µçš„å±€é™æ€§ã€‚</li>
<li>æ–‡ç« é‡æ–°æ¢è®¨äº†æºæ¨¡å‹èŒƒå¼ï¼Œé€šè¿‡è®­ç»ƒä»…é€‚ç”¨äºæ— å›å£°è¯­éŸ³çš„æ‰©æ•£ç”Ÿæˆæ¨¡å‹æ¥è§£å†³è¯­éŸ³åˆ†ç¦»é—®é¢˜ã€‚</li>
<li>æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ç¼ºä¹è¯´è¯äººçº§åˆ«çš„æ¡ä»¶é™åˆ¶ï¼Œä¼šå¯¼è‡´åˆ†ç¦»å‡ºçš„è¯­éŸ³æºåœ¨æ—¶é—´ä¸Šå‡ºç°ä¸ä¸€è‡´çš„è¯´è¯äººèº«ä»½ã€‚</li>
<li>æå‡ºäº†è¯´è¯äººåµŒå…¥å¼•å¯¼æœºåˆ¶ï¼Œä»¥ä¿æŒæ¯ä¸ªåˆ†ç¦»è½¨è¿¹çš„è¯´è¯äººä¸€è‡´æ€§ï¼Œå¹¶ä½¿å¾—ä¸åŒè¯´è¯äººçš„åµŒå…¥è¿›ä¸€æ­¥åˆ†ç¦»ã€‚</li>
<li>æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§é’ˆå¯¹è¯­éŸ³åˆ†ç¦»çš„æ–°åˆ†ç¦»å¯¼å‘æ±‚è§£å™¨ï¼Œä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚</li>
<li>æå‡ºçš„ç­–ç•¥åœ¨æŒ‘æˆ˜æ€§çš„åŸºäºæºæ¨¡å‹çš„éç›‘ç£è¯­éŸ³åˆ†ç¦»ä»»åŠ¡ä¸­è¡¨ç°æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7417f134917599e9ecad91510d92534d" align="middle">
<img src="https://pica.zhimg.com/v2-1e1fa3b35a2fbddf4e3b1e0828040e93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e17f717b1e674b87b9fd46ff824465f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10c796cb93aa4d8385eac49b32e8683b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="UniFlow-Audio-Unified-Flow-Matching-for-Audio-Generation-from-Omni-Modalities"><a href="#UniFlow-Audio-Unified-Flow-Matching-for-Audio-Generation-from-Omni-Modalities" class="headerlink" title="UniFlow-Audio: Unified Flow Matching for Audio Generation from   Omni-Modalities"></a>UniFlow-Audio: Unified Flow Matching for Audio Generation from   Omni-Modalities</h2><p><strong>Authors:Xuenan Xu, Jiahao Mei, Zihao Zheng, Ye Tao, Zeyu Xie, Yaoyun Zhang, Haohe Liu, Yuning Wu, Ming Yan, Wen Wu, Chao Zhang, Mengyue Wu</strong></p>
<p>Audio generation, including speech, music and sound effects, has advanced rapidly in recent years. These tasks can be divided into two categories: time-aligned (TA) tasks, where each input unit corresponds to a specific segment of the output audio (e.g., phonemes aligned with frames in speech synthesis); and non-time-aligned (NTA) tasks, where such alignment is not available. Since modeling paradigms for the two types are typically different, research on different audio generation tasks has traditionally followed separate trajectories. However, audio is not inherently divided into such categories, making a unified model a natural and necessary goal for general audio generation. Previous unified audio generation works have adopted autoregressive architectures, while unified non-autoregressive approaches remain largely unexplored. In this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching. We propose a dual-fusion mechanism that temporally aligns audio latents with TA features and integrates NTA features via cross-attention in each model block. Task-balanced data sampling is employed to maintain strong performance across both TA and NTA tasks. UniFlow-Audio supports omni-modalities, including text, audio, and video. By leveraging the advantage of multi-task learning and the generative modeling capabilities of flow matching, UniFlow-Audio achieves strong results across 7 tasks using fewer than 8K hours of public training data and under 1B trainable parameters. Even the small variant with only ~200M trainable parameters shows competitive performance, highlighting UniFlow-Audio as a potential non-auto-regressive foundation model for audio generation. Code and models will be available at <a target="_blank" rel="noopener" href="https://wsntxxn.github.io/uniflow_audio">https://wsntxxn.github.io/uniflow_audio</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒéŸ³é¢‘ç”Ÿæˆï¼ˆåŒ…æ‹¬è¯­éŸ³ã€éŸ³ä¹å’ŒéŸ³æ•ˆï¼‰å–å¾—äº†è¿…é€Ÿçš„å‘å±•ã€‚è¿™äº›ä»»åŠ¡å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼šæ—¶é—´å¯¹é½ï¼ˆTAï¼‰ä»»åŠ¡ï¼Œæ¯ä¸ªè¾“å…¥å•å…ƒå¯¹åº”äºè¾“å‡ºéŸ³é¢‘çš„ç‰¹å®šç‰‡æ®µï¼ˆä¾‹å¦‚è¯­éŸ³åˆæˆä¸­çš„éŸ³ç´ ä¸å¸§å¯¹é½ï¼‰ï¼›ä»¥åŠéæ—¶é—´å¯¹é½ï¼ˆNTAï¼‰ä»»åŠ¡ï¼Œå…¶ä¸­æ— æ³•è·å¾—è¿™æ ·çš„å¯¹é½ä¿¡æ¯ã€‚ç”±äºä¸¤ç§ç±»å‹çš„å»ºæ¨¡èŒƒå¼é€šå¸¸ä¸åŒï¼Œå› æ­¤é’ˆå¯¹ä¸åŒéŸ³é¢‘ç”Ÿæˆä»»åŠ¡çš„ç ”ç©¶ä¸€ç›´éµå¾ªä¸åŒçš„è½¨è¿¹ã€‚ç„¶è€Œï¼ŒéŸ³é¢‘æœ¬èº«å¹¶ä¸å›ºæœ‰åœ°åˆ†ä¸ºè¿™äº›ç±»åˆ«ï¼Œå› æ­¤æ„å»ºç»Ÿä¸€çš„éŸ³é¢‘ç”Ÿæˆæ¨¡å‹æˆä¸ºäº†ä¸€ä¸ªè‡ªç„¶ä¸”å¿…è¦çš„ç›®æ ‡ã€‚ä¹‹å‰ç»Ÿä¸€çš„éŸ³é¢‘ç”Ÿæˆå·¥ä½œå·²ç»é‡‡ç”¨äº†è‡ªå›å½’æ¶æ„ï¼Œè€Œç»Ÿä¸€çš„éè‡ªå›å½’æ–¹æ³•ä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæµåŒ¹é…çš„é€šç”¨éŸ³é¢‘ç”Ÿæˆæ¡†æ¶UniFlow-Audioã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒèåˆæœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥åœ¨æ—¶é—´ä¸Šå¯¹é½éŸ³é¢‘æ½œåœ¨ç©ºé—´ä¸TAç‰¹å¾ï¼Œå¹¶é€šè¿‡æ¯ä¸ªæ¨¡å‹å—ä¸­çš„äº¤å‰æ³¨æ„åŠ›é›†æˆNTAç‰¹å¾ã€‚é‡‡ç”¨ä»»åŠ¡å¹³è¡¡æ•°æ®é‡‡æ ·ä»¥åœ¨TAå’ŒNTAä»»åŠ¡ä¸Šä¿æŒå¼ºåŠ²æ€§èƒ½ã€‚UniFlow-Audioæ”¯æŒå¤šç§æ¨¡å¼ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ã€‚é€šè¿‡åˆ©ç”¨å¤šä»»åŠ¡å­¦ä¹ çš„ä¼˜åŠ¿å’ŒæµåŒ¹é…çš„ç”Ÿæˆå»ºæ¨¡èƒ½åŠ›ï¼ŒUniFlow-Audioåœ¨7é¡¹ä»»åŠ¡ä¸Šå–å¾—äº†å¼ºå¤§çš„ç»“æœï¼Œä½¿ç”¨å°‘äº8Kå°æ—¶çš„å…¬å¼€è®­ç»ƒæ•°æ®å’Œä¸åˆ°1Bçš„å¯è®­ç»ƒå‚æ•°ã€‚å³ä½¿æ˜¯åªæœ‰çº¦2äº¿ä¸ªå¯è®­ç»ƒå‚æ•°çš„å°å‹å˜ä½“ä¹Ÿè¡¨ç°å‡ºæœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œè¿™çªæ˜¾äº†UniFlow-Audioä½œä¸ºéŸ³é¢‘ç”Ÿæˆæ½œåœ¨çš„éè‡ªå›å½’åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://wsntxxn.github.io/uniflow_audio%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://wsntxxn.github.io/uniflow_audioä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24391v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://wsntxxn.github.io/uniflow_audio">https://wsntxxn.github.io/uniflow_audio</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸéŸ³é¢‘ç”ŸæˆæŠ€æœ¯ï¼ŒåŒ…æ‹¬è¯­éŸ³ã€éŸ³ä¹å’ŒéŸ³æ•ˆï¼Œå‘å±•è¿…é€Ÿã€‚éŸ³é¢‘ç”Ÿæˆä»»åŠ¡å¯åˆ†ä¸ºæ—¶é—´å¯¹é½ï¼ˆTAï¼‰å’Œéæ—¶é—´å¯¹é½ï¼ˆNTAï¼‰ä¸¤å¤§ç±»ã€‚ä»¥å¾€é’ˆå¯¹ä¸åŒç±»å‹çš„éŸ³é¢‘ç”Ÿæˆä»»åŠ¡çš„ç ”ç©¶é€šå¸¸é‡‡ç”¨å•ç‹¬çš„æ¨¡å‹ï¼Œä½†ç¼ºä¹ç»Ÿä¸€çš„æ¡†æ¶ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºæµåŒ¹é…çš„é€šç”¨éŸ³é¢‘ç”Ÿæˆæ¡†æ¶UniFlow-Audioï¼Œé‡‡ç”¨åŒèåˆæœºåˆ¶å®ç°TAå’ŒNTAç‰¹å¾çš„æ•´åˆã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šç§æ¨¡æ€ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ã€‚å€ŸåŠ©å¤šä»»åŠ¡å­¦ä¹ çš„ä¼˜åŠ¿å’ŒæµåŒ¹é…çš„ç”Ÿæˆå»ºæ¨¡èƒ½åŠ›ï¼ŒUniFlow-Audioåœ¨7ä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½¿ç”¨å°‘äº8Kå°æ—¶çš„å…¬å¼€è®­ç»ƒæ•°æ®å’Œä¸åˆ°1Bçš„å¯è®­ç»ƒå‚æ•°ã€‚ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://wsntxxn.github.io/uniflow_audio">é“¾æ¥</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>éŸ³é¢‘ç”ŸæˆæŠ€æœ¯å¿«é€Ÿå‘å±•ï¼Œæ¶µç›–è¯­éŸ³ã€éŸ³ä¹å’ŒéŸ³æ•ˆç­‰é¢†åŸŸã€‚</li>
<li>éŸ³é¢‘ç”Ÿæˆä»»åŠ¡åˆ†ä¸ºæ—¶é—´å¯¹é½ï¼ˆTAï¼‰å’Œéæ—¶é—´å¯¹é½ï¼ˆNTAï¼‰ä¸¤å¤§ç±»åˆ«ï¼Œéœ€è¦ä¸åŒçš„å»ºæ¨¡æ–¹æ³•ã€‚</li>
<li>ç°æœ‰çš„éŸ³é¢‘ç”Ÿæˆç ”ç©¶é€šå¸¸é‡‡ç”¨å•ç‹¬çš„æ¨¡å‹ï¼Œç¼ºä¹ç»Ÿä¸€æ¡†æ¶ã€‚</li>
<li>UniFlow-Audioæ˜¯ä¸€ä¸ªåŸºäºæµåŒ¹é…çš„é€šç”¨éŸ³é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œé‡‡ç”¨åŒèåˆæœºåˆ¶æ•´åˆTAå’ŒNTAç‰¹å¾ã€‚</li>
<li>UniFlow-Audioæ”¯æŒå¤šç§æ¨¡æ€ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å¤šä»»åŠ¡å­¦ä¹ å’ŒæµåŒ¹é…çš„ç”Ÿæˆå»ºæ¨¡èƒ½åŠ›ä¸Šè¡¨ç°å‡ºä¼˜åŠ¿ï¼Œåœ¨7ä¸ªä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b0af2b7f3b23a372456ef19c20e031e" align="middle">
<img src="https://picx.zhimg.com/v2-cebbc6c4b275e577f119c5ac82fd92dd" align="middle">
<img src="https://picx.zhimg.com/v2-814683bf1877375f158b5d52f8715d6f" align="middle">
<img src="https://picx.zhimg.com/v2-18ef3ab10e3cc0977dea9316bcb4b77f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Code-switching-Speech-Recognition-Under-the-Lens-Model-and-Data-Centric-Perspectives"><a href="#Code-switching-Speech-Recognition-Under-the-Lens-Model-and-Data-Centric-Perspectives" class="headerlink" title="Code-switching Speech Recognition Under the Lens: Model- and   Data-Centric Perspectives"></a>Code-switching Speech Recognition Under the Lens: Model- and   Data-Centric Perspectives</h2><p><strong>Authors:Hexin Liu, Haoyang Zhang, Qiquan Zhang, Xiangyu Zhang, Dongyuan Shi, Eng Siong Chng, Haizhou Li</strong></p>
<p>Code-switching automatic speech recognition (CS-ASR) presents unique challenges due to language confusion introduced by spontaneous intra-sentence switching and accent bias that blurs the phonetic boundaries. Although the constituent languages may be individually high-resource, the scarcity of annotated code-switching data further compounds these challenges. In this paper, we systematically analyze CS-ASR from both model-centric and data-centric perspectives. By comparing state-of-the-art algorithmic methods, including language-specific processing and auxiliary language-aware multi-task learning, we discuss their varying effectiveness across datasets with different linguistic characteristics. On the data side, we first investigate TTS as a data augmentation method. By varying the textual characteristics and speaker accents, we analyze the impact of language confusion and accent bias on CS-ASR. To further mitigate data scarcity and enhance textual diversity, we propose a prompting strategy by simplifying the equivalence constraint theory (SECT) to guide large language models (LLMs) in generating linguistically valid code-switching text. The proposed SECT outperforms existing methods in ASR performance and linguistic quality assessments, generating code-switching text that more closely resembles real-world code-switching text. When used to generate speech-text pairs via TTS, SECT proves effective in improving CS-ASR performance. Our analysis of both model- and data-centric methods underscores that effective CS-ASR requires strategies to be carefully aligned with the specific linguistic characteristics of the code-switching data. </p>
<blockquote>
<p>ä»£ç åˆ‡æ¢è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆCS-ASRï¼‰ç”±äºå¥å­å†…è‡ªå‘åˆ‡æ¢å¼•å‘çš„è¯­è¨€æ··æ·†å’Œå£éŸ³åå·®å¯¼è‡´çš„è¯­éŸ³è¾¹ç•Œæ¨¡ç³Šè€Œé¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ã€‚å°½ç®¡æ„æˆçš„è¯­è¨€å¯èƒ½å•ç‹¬æ˜¯é«˜èµ„æºçš„ï¼Œä½†ä»£ç åˆ‡æ¢æ•°æ®çš„ç¨€ç¼ºæ€§è¿›ä¸€æ­¥åŠ å‰§äº†è¿™äº›æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»æ¨¡å‹ä¸­å¿ƒå’Œæ•°æ®ä¸­å¿ƒä¸¤ä¸ªè§’åº¦ç³»ç»Ÿåœ°åˆ†æäº†CS-ASRã€‚é€šè¿‡æ¯”è¾ƒæœ€å…ˆè¿›çš„ç®—æ³•æ–¹æ³•ï¼ŒåŒ…æ‹¬è¯­è¨€ç‰¹å®šå¤„ç†å’Œè¾…åŠ©è¯­è¨€æ„ŸçŸ¥å¤šä»»åŠ¡å­¦ä¹ ï¼Œæˆ‘ä»¬è®¨è®ºäº†å®ƒä»¬åœ¨å…·æœ‰ä¸åŒè¯­è¨€ç‰¹å¾çš„æ•°æ®é›†ä¸Šçš„ä¸åŒæ•ˆæœã€‚åœ¨æ•°æ®æ–¹é¢ï¼Œæˆ‘ä»¬é¦–å…ˆç ”ç©¶TTSä½œä¸ºæ•°æ®å¢å¼ºæ–¹æ³•ã€‚é€šè¿‡æ”¹å˜æ–‡æœ¬ç‰¹å¾å’Œè¯´è¯è€…å£éŸ³ï¼Œæˆ‘ä»¬åˆ†æäº†è¯­è¨€æ··æ·†å’Œå£éŸ³åå·®å¯¹CS-ASRçš„å½±å“ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç¼“è§£æ•°æ®ç¨€ç¼ºæ€§å¹¶å¢å¼ºæ–‡æœ¬å¤šæ ·æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç®€åŒ–ç­‰ä»·çº¦æŸç†è®ºï¼ˆSECTï¼‰çš„æç¤ºç­–ç•¥ï¼Œä»¥æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆè¯­è¨€ä¸Šæœ‰æ•ˆçš„ä»£ç åˆ‡æ¢æ–‡æœ¬ã€‚æ‰€æå‡ºçš„SECTåœ¨è¯­éŸ³è¯†åˆ«æ€§èƒ½å’Œè¯­è¨€è´¨é‡è¯„ä¼°æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆæ›´è´´è¿‘ç°å®ä¸–ç•Œçš„ä»£ç åˆ‡æ¢æ–‡æœ¬ã€‚å½“ç”¨äºé€šè¿‡TTSç”Ÿæˆè¯­éŸ³æ–‡æœ¬å¯¹æ—¶ï¼ŒSECTåœ¨æé«˜CS-ASRæ€§èƒ½æ–¹é¢è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚æˆ‘ä»¬å¯¹æ¨¡å‹å’Œæ•°æ®ä¸­å¿ƒæ–¹æ³•çš„åˆ†æå¼ºè°ƒï¼Œæœ‰æ•ˆçš„CS-ASRéœ€è¦ç­–ç•¥ä¸ä»£ç åˆ‡æ¢æ•°æ®çš„ç‰¹å®šè¯­è¨€ç‰¹å¾ä»”ç»†å¯¹é½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24310v1">PDF</a> 11 pages, 3 figures, 9 tables, submitted to IEEE TASLP</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†ä»£ç åˆ‡æ¢è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆCS-ASRï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¯­è¨€æ··æ·†å’Œå£éŸ³åå·®é—®é¢˜ã€‚é€šè¿‡å¯¹æ¯”å…ˆè¿›çš„ç®—æ³•æ–¹æ³•ï¼Œå¦‚ç‰¹å®šè¯­è¨€å¤„ç†å’Œè¾…åŠ©å¤šè¯­è¨€å¤šä»»åŠ¡å­¦ä¹ ï¼Œæ¢è®¨äº†å®ƒä»¬åœ¨å…·æœ‰ä¸åŒè¯­è¨€ç‰¹å¾çš„æ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚åŒæ—¶ï¼Œæœ¬æ–‡ç ”ç©¶äº†æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ä½œä¸ºæ•°æ®å¢å¼ºæ–¹æ³•çš„å½±å“ï¼Œå¹¶æå‡ºäº†åŸºäºç®€åŒ–ç­‰ä»·çº¦æŸç†è®ºï¼ˆSECTï¼‰çš„æç¤ºç­–ç•¥æ¥ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜å¹¶å¢å¼ºæ–‡æœ¬å¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SECTåœ¨è¯­éŸ³è¯†åˆ«æ€§èƒ½ä¸è¯­è¨€è´¨é‡è¯„ä¼°æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆçš„ä»£ç åˆ‡æ¢æ–‡æœ¬æ›´æ¥è¿‘çœŸå®åœºæ™¯ã€‚å°†SECTç”¨äºç”Ÿæˆè¯­éŸ³æ–‡æœ¬å¯¹æ—¶ï¼Œé€šè¿‡TTSå¯æœ‰æ•ˆæé«˜CS-ASRæ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ‰æ•ˆçš„CS-ASRéœ€è¦ç­–ç•¥ä¸ä»£ç åˆ‡æ¢æ•°æ®çš„ç‰¹å®šè¯­è¨€ç‰¹å¾ç´§å¯†é…åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CS-ASRé¢ä¸´è¯­è¨€æ··æ·†å’Œå£éŸ³åå·®çš„æŒ‘æˆ˜ã€‚</li>
<li>å…ˆè¿›çš„ç®—æ³•æ–¹æ³•åœ¨CS-ASRä¸­çš„æœ‰æ•ˆæ€§å› æ•°æ®é›†çš„è¯­è¨€ç‰¹æ€§è€Œå¼‚ã€‚</li>
<li>TTSä½œä¸ºæ•°æ®å¢å¼ºæ–¹æ³•è¢«ç ”ç©¶ï¼Œä»¥åˆ†æè¯­è¨€æ··æ·†å’Œå£éŸ³åå·®å¯¹CS-ASRçš„å½±å“ã€‚</li>
<li>æå‡ºäº†åŸºäºç®€åŒ–ç­‰ä»·çº¦æŸç†è®ºï¼ˆSECTï¼‰çš„æç¤ºç­–ç•¥æ¥ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜å¹¶å¢å¼ºæ–‡æœ¬å¤šæ ·æ€§ã€‚</li>
<li>SECTåœ¨è¯­éŸ³è¯†åˆ«æ€§èƒ½ä¸è¯­è¨€è´¨é‡è¯„ä¼°æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>ç»“åˆTTSï¼ŒSECTåœ¨ç”Ÿæˆè¯­éŸ³æ–‡æœ¬å¯¹æ—¶èƒ½æé«˜CS-ASRæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24310">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2648cb72fb1631105675913ac3f3e78a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11b0c149253903f6e020d24c1ad9d680.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a855532fd684e1436536b3ec5eee921" align="middle">
<img src="https://picx.zhimg.com/v2-ccdffa2d5ea09dd5fc49267bd6887918" align="middle">
<img src="https://pic1.zhimg.com/v2-f6dc32f900f635d534c5bcf3a9235380.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b45a276a88a2a167f2d1c0b5303e05e" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ELASTIQ-EEG-Language-Alignment-with-Semantic-Task-Instruction-and-Querying"><a href="#ELASTIQ-EEG-Language-Alignment-with-Semantic-Task-Instruction-and-Querying" class="headerlink" title="ELASTIQ: EEG-Language Alignment with Semantic Task Instruction and   Querying"></a>ELASTIQ: EEG-Language Alignment with Semantic Task Instruction and   Querying</h2><p><strong>Authors:Muyun Jiang, Shuailei Zhang, Zhenjie Yang, Mengjun Wu, Weibang Jiang, Zhiwei Guo, Wei Zhang, Rui Liu, Shangen Zhang, Yong Li, Yi Ding, Cuntai Guan</strong></p>
<p>Recent advances in electroencephalography (EEG) foundation models, which capture transferable EEG representations, have greatly accelerated the development of brain-computer interfaces (BCI). However, existing approaches still struggle to incorporate language instructions as prior constraints for EEG representation learning, limiting their ability to leverage the semantic knowledge inherent in language to unify different labels and tasks. To address this challenge, we present ELASTIQ, a foundation model for EEG-Language Alignment with Semantic Task Instruction and Querying. ELASTIQ integrates task-aware semantic guidance to produce structured and linguistically aligned EEG embeddings, thereby enhancing decoding robustness and transferability. In the pretraining stage, we introduce a joint Spectral-Temporal Reconstruction (STR) module, which combines frequency masking as a global spectral perturbation with two complementary temporal objectives: random masking to capture contextual dependencies and causal masking to model sequential dynamics. In the instruction tuning stage, we propose the Instruction-conditioned Q-Former (IQF), a query-based cross-attention transformer that injects instruction embeddings into EEG tokens and aligns them with textual label embeddings through learnable queries. We evaluate ELASTIQ on 20 datasets spanning motor imagery, emotion recognition, steady-state visual evoked potentials, covert speech, and healthcare tasks. ELASTIQ achieves state-of-the-art performance on 14 of the 20 datasets and obtains the best average results across all five task categories. Importantly, our analyses reveal for the first time that explicit task instructions serve as semantic priors guiding EEG embeddings into coherent and linguistically grounded spaces. The code and pre-trained weights will be released. </p>
<blockquote>
<p>è¿‘æœŸè„‘ç”µå›¾ï¼ˆEEGï¼‰åŸºç¡€æ¨¡å‹çš„è¿›æ­¥ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿæ•æ‰å¯è¿ç§»çš„EEGè¡¨ç¤ºï¼Œæå¤§åœ°åŠ é€Ÿäº†è„‘æœºæ¥å£ï¼ˆBCIï¼‰çš„å‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»ç„¶éš¾ä»¥å°†è¯­è¨€æŒ‡ä»¤ä½œä¸ºå…ˆéªŒçº¦æŸèå…¥EEGè¡¨ç¤ºå­¦ä¹ ä¸­ï¼Œé™åˆ¶äº†å…¶åˆ©ç”¨è¯­è¨€ä¸­æ‰€å›ºæœ‰çš„è¯­ä¹‰çŸ¥è¯†æ¥ç»Ÿä¸€ä¸åŒæ ‡ç­¾å’Œä»»åŠ¡çš„èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†EEG-è¯­è¨€å¯¹é½åŸºç¡€æ¨¡å‹ELASTIQï¼ˆç»“åˆäº†è¯­ä¹‰ä»»åŠ¡æŒ‡ä»¤å’ŒæŸ¥è¯¢ï¼‰ã€‚ELASTIQé›†æˆäº†ä»»åŠ¡æ„ŸçŸ¥è¯­ä¹‰æŒ‡å¯¼ï¼Œä»¥äº§ç”Ÿç»“æ„åŒ–å’Œè¯­è¨€å¯¹é½çš„EEGåµŒå…¥ï¼Œä»è€Œæé«˜äº†è§£ç çš„ç¨³å¥æ€§å’Œå¯è¿ç§»æ€§ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†è”åˆè°±æ—¶é—´é‡å»ºï¼ˆSTRï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†é¢‘ç‡æ©ç ä½œä¸ºå…¨å±€è°±æ‰°åŠ¨ä¸ä¸¤ç§äº’è¡¥çš„æ—¶é—´ç›®æ ‡ç›¸ç»“åˆï¼šéšæœºæ©ç ä»¥æ•æ‰ä¸Šä¸‹æ–‡ä¾èµ–æ€§ï¼Œå› æœæ©ç ä»¥æ¨¡æ‹Ÿåºåˆ—åŠ¨æ€ã€‚åœ¨æŒ‡ä»¤å¾®è°ƒé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†æŒ‡ä»¤æ¡ä»¶Q-Formersï¼ˆIQFï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæŸ¥è¯¢çš„è·¨æ³¨æ„åŠ›å˜å‹å™¨ï¼Œå®ƒå°†æŒ‡ä»¤åµŒå…¥æ³¨å…¥EEGä»¤ç‰Œä¸­ï¼Œå¹¶é€šè¿‡å¯å­¦ä¹ çš„æŸ¥è¯¢å°†å®ƒä»¬ä¸æ–‡æœ¬æ ‡ç­¾åµŒå…¥å¯¹é½ã€‚æˆ‘ä»¬åœ¨è·¨è¶Šè¿åŠ¨æƒ³è±¡ã€æƒ…æ„Ÿè¯†åˆ«ã€ç¨³æ€è§†è§‰è¯±å‘ç”µä½ã€éšæ€§è¯­è¨€å’ŒåŒ»ç–—ä»»åŠ¡çš„20ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†ELASTIQã€‚ELASTIQåœ¨å…¶ä¸­çš„14ä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨æ‰€æœ‰äº”ä¸ªä»»åŠ¡ç±»åˆ«ä¸­è·å¾—äº†æœ€ä½³å¹³å‡ç»“æœã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åˆ†æé¦–æ¬¡è¡¨æ˜ï¼Œæ˜ç¡®çš„ä»»åŠ¡æŒ‡ä»¤ä½œä¸ºè¯­ä¹‰å…ˆéªŒæŒ‡å¯¼EEGåµŒå…¥åˆ°è¿è´¯ä¸”è¯­è¨€åŸºç¡€çš„ç©ºé—´ä¸­ã€‚ä»£ç å’Œé¢„è®­ç»ƒæƒé‡å°†è¢«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24302v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºELASTIQçš„è„‘ç”µå›¾ï¼ˆEEGï¼‰-è¯­è¨€å¯¹é½åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†ä»»åŠ¡æ„ŸçŸ¥è¯­ä¹‰æŒ‡å¯¼ï¼Œäº§ç”Ÿç»“æ„åŒ–ä¸”è¯­è¨€å¯¹é½çš„è„‘ç”µå›¾åµŒå…¥ï¼Œæé«˜è§£ç çš„ç¨³å¥æ€§å’Œå¯è½¬ç§»æ€§ã€‚æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µé‡‡ç”¨è”åˆè°±æ—¶é‡å»ºï¼ˆSTRï¼‰æ¨¡å—ï¼Œåœ¨æŒ‡ä»¤å¾®è°ƒé˜¶æ®µæå‡ºæŒ‡ä»¤æ¡ä»¶Q-Formersï¼ˆIQFï¼‰ã€‚æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—æœ€ä½³æ€§èƒ½ï¼Œå¹¶é¦–æ¬¡æ­ç¤ºæ˜ç¡®çš„ä»»åŠ¡æŒ‡ä»¤ä½œä¸ºè¯­ä¹‰å…ˆéªŒï¼Œå¼•å¯¼è„‘ç”µå›¾åµŒå…¥åˆ°è¿è´¯ä¸”è¯­è¨€åŸºç¡€çš„ç©ºé—´ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ELASTIQæ˜¯ä¸€ä¸ªç”¨äºè„‘ç”µå›¾ï¼ˆEEGï¼‰-è¯­è¨€å¯¹é½çš„åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•æ— æ³•å°†è¯­è¨€æŒ‡ä»¤ä½œä¸ºå…ˆéªŒçº¦æŸæ¥æ•´åˆEEGè¡¨ç¤ºå­¦ä¹ çš„é—®é¢˜ã€‚</li>
<li>ELASTIQé€šè¿‡ä»»åŠ¡æ„ŸçŸ¥è¯­ä¹‰æŒ‡å¯¼äº§ç”Ÿç»“æ„åŒ–ä¸”è¯­è¨€å¯¹é½çš„EEGåµŒå…¥ï¼Œå¢å¼ºè§£ç çš„ç¨³å¥æ€§å’Œå¯è½¬ç§»æ€§ã€‚</li>
<li>æ¨¡å‹çš„é¢„è®­ç»ƒé˜¶æ®µé‡‡ç”¨è”åˆè°±æ—¶é‡å»ºï¼ˆSTRï¼‰æ¨¡å—ï¼Œç»“åˆå…¨å±€è°±æ‰°åŠ¨å’Œä¸¤ç§äº’è¡¥çš„æ—¶é—´ç›®æ ‡æ¥å®ç°ã€‚</li>
<li>åœ¨æŒ‡ä»¤å¾®è°ƒé˜¶æ®µï¼Œæå‡ºæŒ‡ä»¤æ¡ä»¶çš„Q-Formersï¼ˆIQFï¼‰ï¼Œé€šè¿‡æŸ¥è¯¢å°†æŒ‡ä»¤åµŒå…¥æ³¨å…¥EEGä»¤ç‰Œå¹¶ä¸æ–‡æœ¬æ ‡ç­¾åµŒå…¥å¯¹é½ã€‚</li>
<li>ELASTIQåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¾¾åˆ°æœ€ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ç”µæœºå›¾åƒã€æƒ…ç»ªè¯†åˆ«ã€ç¨³æ€è§†è§‰è¯±å‘ç”µä½ã€éšè”½æ€§è¯­è¨€å’ŒåŒ»ç–—ä»»åŠ¡ç­‰é¢†åŸŸã€‚</li>
<li>ELASTIQçš„å‘å¸ƒå°†ä¸ºå…¬å¼€æºç å’Œé¢„è®­ç»ƒæƒé‡æä¾›ä¾¿åˆ©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-106e7f0100df7b7d5362e4f054dc691f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7227c86c22c142ae0c078c4e21bd61a1" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AISHELL6-whisper-A-Chinese-Mandarin-Audio-visual-Whisper-Speech-Dataset-with-Speech-Recognition-Baselines"><a href="#AISHELL6-whisper-A-Chinese-Mandarin-Audio-visual-Whisper-Speech-Dataset-with-Speech-Recognition-Baselines" class="headerlink" title="AISHELL6-whisper: A Chinese Mandarin Audio-visual Whisper Speech Dataset   with Speech Recognition Baselines"></a>AISHELL6-whisper: A Chinese Mandarin Audio-visual Whisper Speech Dataset   with Speech Recognition Baselines</h2><p><strong>Authors:Cancan Li, Fei Su, Juan Liu, Hui Bu, Yulong Wan, Hongbin Suo, Ming Li</strong></p>
<p>Whisper speech recognition is crucial not only for ensuring privacy in sensitive communications but also for providing a critical communication bridge for patients under vocal restraint and enabling discrete interaction in noise-sensitive environments. The development of Chinese mandarin audio-visual whisper speech recognition is hindered by the lack of large-scale datasets. We present AISHELL6-Whisper, a large-scale open-source audio-visual whisper speech dataset, featuring 30 hours each of whisper speech and parallel normal speech, with synchronized frontal facial videos. Moreover, we propose an audio-visual speech recognition (AVSR) baseline based on the Whisper-Flamingo framework, which integrates a parallel training strategy to align embeddings across speech types, and employs a projection layer to adapt to whisper speechâ€™s spectral properties. The model achieves a Character Error Rate (CER) of 4.13% for whisper speech and 1.11% for normal speech in the test set of our dataset, and establishes new state-of-the-art results on the wTIMIT benchmark. The dataset and the AVSR baseline codes are open-sourced at <a target="_blank" rel="noopener" href="https://zutm.github.io/AISHELL6-Whisper">https://zutm.github.io/AISHELL6-Whisper</a>. </p>
<blockquote>
<p>è½»å£°è¯­éŸ³è¯†åˆ«çš„åº”ç”¨ä¸ä»…å¯¹ç¡®ä¿æ•æ„Ÿé€šä¿¡çš„éšç§è‡³å…³é‡è¦ï¼Œè¿˜ä¸ºå£°éŸ³å—é™çš„æ‚£è€…æä¾›äº†å…³é”®çš„æ²Ÿé€šæ¡¥æ¢ï¼Œå¹¶èƒ½åœ¨å™ªå£°æ•æ„Ÿç¯å¢ƒä¸­å®ç°ç¦»æ•£äº¤äº’ã€‚æ±‰è¯­æ™®é€šè¯è§†å¬è½»å£°è¯­éŸ³è¯†åˆ«çš„å¼€å‘å—åˆ°äº†å¤§è§„æ¨¡æ•°æ®é›†ç¼ºä¹çš„é˜»ç¢ã€‚æˆ‘ä»¬æ¨å‡ºäº†AISHELL6-Whisperæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¼€æºè§†å¬è½»å£°è¯­éŸ³æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«30å°æ—¶çš„è½»å£°è¯­éŸ³å’ŒåŒæ­¥æ­£å¸¸è¯­éŸ³ï¼Œå¹¶é…æœ‰æ­£é¢é¢éƒ¨è§†é¢‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŸºäºWhisper-Flamingoæ¡†æ¶æå‡ºäº†è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰åŸºçº¿æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨å¹¶è¡Œè®­ç»ƒç­–ç•¥æ¥å¯¹é½ä¸åŒè¯­éŸ³ç±»å‹çš„åµŒå…¥å‘é‡ï¼Œå¹¶å€ŸåŠ©æŠ•å½±å±‚ä»¥é€‚åº”è½»å£°è¯­éŸ³çš„é¢‘è°±ç‰¹æ€§ã€‚åœ¨æˆ‘ä»¬çš„æ•°æ®é›†æµ‹è¯•é›†ä¸­ï¼Œè¯¥æ¨¡å‹å–å¾—äº†4.13%çš„è½»å£°è¯­éŸ³å­—ç¬¦é”™è¯¯ç‡å’Œæ­£å¸¸è¯­éŸ³çš„1.11%å­—ç¬¦é”™è¯¯ç‡ï¼Œå¹¶åœ¨wTIMITåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æˆæœã€‚æ•°æ®é›†å’ŒAVSRåŸºçº¿ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://zutm.github.io/AISHELL6-Whisper%E3%80%82">https://zutm.github.io/AISHELL6-Whisperã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23833v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†æ— å£°å£è¯­è¯†åˆ«æŠ€æœ¯åœ¨æ•æ„Ÿé€šè®¯ä¿éšœã€æ‚£è€…æ²Ÿé€šæ¡¥æ¢åŠå™ªéŸ³ç¯å¢ƒä¸‹ç¦»æ•£äº¤äº’ç­‰é¢†åŸŸçš„é‡è¦æ€§ã€‚ä¸ºè§£å†³ä¸­æ–‡æ™®é€šè¯æ— å£°å£è¯­è¯†åˆ«å‘å±•å—é™äºå¤§è§„æ¨¡æ•°æ®é›†çš„é—®é¢˜ï¼Œæ¨å‡ºAISHELL6-Whisperå¤§å‹å¼€æ”¾éŸ³é¢‘æ— å£°å£è¯­æ•°æ®é›†ï¼ŒåŒ…æ‹¬æ¯å°æ—¶æœ‰åŒæ­¥é¢éƒ¨è§†é¢‘çš„30å°æ—¶æ— å£°å£è¯­å’Œå¹¶è¡Œæ­£å¸¸å£è¯­æ•°æ®ã€‚åŸºäºWhisper-Flamingoæ¡†æ¶æå‡ºè§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰åŸºçº¿ç³»ç»Ÿï¼Œé€šè¿‡å¹¶è¡Œè®­ç»ƒç­–ç•¥å¯¹é½ä¸åŒè¯­éŸ³ç±»å‹çš„åµŒå…¥ï¼Œå¹¶é‡‡ç”¨æŠ•å½±å±‚é€‚åº”æ— å£°å£è¯­çš„é¢‘è°±ç‰¹æ€§ã€‚æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šå®ç°äº†å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ä¸ºæ— å£°å£è¯­4.13%ï¼Œæ­£å¸¸å£è¯­1.11%ï¼Œå¹¶åœ¨wTIMITåŸºå‡†æµ‹è¯•ä¸Šå–å¾—æœ€æ–°æˆæœã€‚æ•°æ®é›†å’ŒAVSRåŸºçº¿ä»£ç å·²å…¬å¼€äºï¼š<a target="_blank" rel="noopener" href="https://zutm.github.io/AISHELL6-Whisper%E3%80%82">https://zutm.github.io/AISHELL6-Whisperã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— å£°å£è¯­è¯†åˆ«æŠ€æœ¯å¯¹äºæ•æ„Ÿé€šè®¯ã€æ‚£è€…æ²Ÿé€šå’Œå™ªéŸ³ç¯å¢ƒä¸‹çš„äº¤äº’è‡³å…³é‡è¦ã€‚</li>
<li>ç¼ºä¹å¤§è§„æ¨¡æ•°æ®é›†æ˜¯ä¸­æ–‡æ™®é€šè¯æ— å£°å£è¯­è¯†åˆ«å‘å±•çš„ä¸»è¦éšœç¢ã€‚</li>
<li>æ¨å‡ºAISHELL6-Whisperæ•°æ®é›†ï¼ŒåŒ…å«æ— å£°å£è¯­å’Œå¹¶è¡Œæ­£å¸¸å£è¯­çš„åŒæ­¥æ•°æ®ã€‚</li>
<li>åŸºäºWhisper-Flamingoæ¡†æ¶æå‡ºAVSRåŸºçº¿ç³»ç»Ÿï¼Œé‡‡ç”¨å¹¶è¡Œè®­ç»ƒç­–ç•¥å’Œå¯¹é½åµŒå…¥æŠ€æœ¯ã€‚</li>
<li>AVSRæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†ä½å­—ç¬¦é”™è¯¯ç‡ã€‚</li>
<li>æ¨¡å‹åœ¨wTIMITåŸºå‡†æµ‹è¯•ä¸Šå–å¾—æœ€æ–°æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc817038fdcf892f2feb1ed2402091fa" align="middle">
<img src="https://picx.zhimg.com/v2-5f7e73f93bcfa37ac733d4bd1f9caaee" align="middle">
<img src="https://pic1.zhimg.com/v2-67f5f80b313151839529ab72fdcd2226.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d081655f49e50a986dc05016c9f75494" align="middle">
<img src="https://picx.zhimg.com/v2-29a4e4fe4b2e61a704319f3c573cb923" align="middle">
<img src="https://pica.zhimg.com/v2-a470a1016921a980ce66d8746b5f2865.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LORT-Locally-Refined-Convolution-and-Taylor-Transformer-for-Monaural-Speech-Enhancement"><a href="#LORT-Locally-Refined-Convolution-and-Taylor-Transformer-for-Monaural-Speech-Enhancement" class="headerlink" title="LORT: Locally Refined Convolution and Taylor Transformer for Monaural   Speech Enhancement"></a>LORT: Locally Refined Convolution and Taylor Transformer for Monaural   Speech Enhancement</h2><p><strong>Authors:Junyu Wang, Zizhen Lin, Tianrui Wang, Meng Ge, Longbiao Wang, Jianwu Dang</strong></p>
<p>Achieving superior enhancement performance while maintaining a low parameter count and computational complexity remains a challenge in the field of speech enhancement. In this paper, we introduce LORT, a novel architecture that integrates spatial-channel enhanced Taylor Transformer and locally refined convolution for efficient and robust speech enhancement. We propose a Taylor multi-head self-attention (T-MSA) module enhanced with spatial-channel enhancement attention (SCEA), designed to facilitate inter-channel information exchange and alleviate the spatial attention limitations inherent in Taylor-based Transformers. To complement global modeling, we further present a locally refined convolution (LRC) block that integrates convolutional feed-forward layers, time-frequency dense local convolutions, and gated units to capture fine-grained local details. Built upon a U-Net-like encoder-decoder structure with only 16 output channels in the encoder, LORT processes noisy inputs through multi-resolution T-MSA modules using alternating downsampling and upsampling operations. The enhanced magnitude and phase spectra are decoded independently and optimized through a composite loss function that jointly considers magnitude, complex, phase, discriminator, and consistency objectives. Experimental results on the VCTK+DEMAND and DNS Challenge datasets demonstrate that LORT achieves competitive or superior performance to state-of-the-art (SOTA) models with only 0.96M parameters, highlighting its effectiveness for real-world speech enhancement applications with limited computational resources. </p>
<blockquote>
<p>åœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸï¼Œå®ç°åœ¨ä¿æŒä½å‚æ•°è®¡æ•°å’Œè®¡ç®—å¤æ‚åº¦çš„åŒæ—¶è¾¾åˆ°ä¼˜è¶Šçš„å¢å¼ºæ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹æ¶æ„LORTï¼Œå®ƒé›†æˆäº†ç©ºé—´é€šé“å¢å¼ºæ³°å‹’å˜æ¢å™¨å’Œå±€éƒ¨ç²¾ç»†å·ç§¯ï¼Œç”¨äºé«˜æ•ˆç¨³å¥çš„è¯­éŸ³å¢å¼ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºå‹çš„æ³°å‹’å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆT-MSAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—é…å¤‡äº†ç©ºé—´é€šé“å¢å¼ºæ³¨æ„åŠ›ï¼ˆSCEAï¼‰ï¼Œæ—¨åœ¨ä¿ƒè¿›é€šé“é—´çš„ä¿¡æ¯äº¤æ¢ï¼Œå¹¶ç¼“è§£åŸºäºæ³°å‹’çš„å˜å‹å™¨å›ºæœ‰çš„ç©ºé—´æ³¨æ„åŠ›é™åˆ¶ã€‚ä¸ºäº†è¡¥å……å…¨å±€å»ºæ¨¡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§å±€éƒ¨ç²¾ç»†å·ç§¯ï¼ˆLRCï¼‰å—ï¼Œå®ƒé›†æˆäº†å·ç§¯å‰é¦ˆå±‚ã€æ—¶é¢‘å¯†é›†å±€éƒ¨å·ç§¯å’Œé—¨æ§å•å…ƒï¼Œä»¥æ•è·ç²¾ç»†çš„å±€éƒ¨ç»†èŠ‚ã€‚LORTå»ºç«‹åœ¨ç±»ä¼¼U-Netçš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„ä¸Šï¼Œç¼–ç å™¨åªæœ‰1e6ä¸ªè¾“å‡ºé€šé“ã€‚å®ƒé€šè¿‡äº¤æ›¿çš„ä¸‹é‡‡æ ·å’Œä¸Šé‡‡æ ·æ“ä½œï¼Œä½¿ç”¨å¤šåˆ†è¾¨ç‡T-MSAæ¨¡å—å¤„ç†å™ªå£°è¾“å…¥ã€‚å¢å¼ºçš„å¹…åº¦å’Œç›¸ä½è°±ç‹¬ç«‹è§£ç ï¼Œå¹¶é€šè¿‡è”åˆè€ƒè™‘å¹…åº¦ã€å¤æ•°ã€ç›¸ä½ã€é‰´åˆ«å™¨å’Œä¸€è‡´æ€§ç›®æ ‡çš„å¤åˆæŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ã€‚åœ¨VCTK+DEMANDå’ŒDNSæŒ‘æˆ˜æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLORTä»…ä»¥0.96Mçš„å‚æ•°å–å¾—äº†ä¸æœ€æ–°æŠ€æœ¯ç›¸ç«äº‰æˆ–æ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼Œè¿™çªæ˜¾äº†å…¶åœ¨è®¡ç®—èµ„æºæœ‰é™çš„å®é™…è¯­éŸ³å¢å¼ºåº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23832v1">PDF</a> Speech Communication</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡é’ˆå¯¹è¯­éŸ³å¢å¼ºé¢†åŸŸä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„LORTï¼Œè¯¥æ¶æ„ç»“åˆäº†ç©ºé—´é€šé“å¢å¼ºTaylor Transformerå’Œå±€éƒ¨ç²¾ç»†å·ç§¯ï¼Œä»¥å®ç°é«˜æ•ˆä¸”ç¨³å¥çš„è¯­éŸ³å¢å¼ºã€‚é€šè¿‡å¼•å…¥Taylorå¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—å¹¶ç»“åˆç©ºé—´é€šé“å¢å¼ºæ³¨æ„åŠ›ï¼Œä¿ƒè¿›äº†é€šé“é—´çš„ä¿¡æ¯äº¤æ¢ï¼Œå¹¶ç¼“è§£äº†åŸºäºTaylorçš„Transformerå›ºæœ‰çš„ç©ºé—´æ³¨æ„åŠ›é™åˆ¶ã€‚ä¸ºè¡¥å……å…¨å±€å»ºæ¨¡ï¼Œè¿˜æå‡ºäº†å±€éƒ¨ç²¾ç»†å·ç§¯å—ï¼Œä»¥æ•è·ç²¾ç»†çš„å±€éƒ¨ç»†èŠ‚ã€‚LORTåŸºäºU-Netç±»ä¼¼çš„ç¼–ç è§£ç å™¨ç»“æ„ï¼Œé€šè¿‡å¤šåˆ†è¾¨ç‡çš„T-MSAæ¨¡å—å¤„ç†å¸¦å™ªè¾“å…¥ï¼Œå¹¶é‡‡ç”¨å¤åˆæŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ã€‚åœ¨VCTK+DEMANDå’ŒDNS Challengeæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLORTçš„å‚æ•°ä»…ä¸º0.96Mï¼Œä½†æ€§èƒ½å…·æœ‰ç«äº‰åŠ›ï¼Œçªå‡ºå…¶åœ¨æœ‰é™è®¡ç®—èµ„æºä¸‹å®ç°çœŸå®è¯­éŸ³å¢å¼ºçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LORTæ˜¯ä¸€ç§æ–°çš„è¯­éŸ³å¢å¼ºæ¶æ„ï¼Œç»“åˆäº†ç©ºé—´é€šé“å¢å¼ºTaylor Transformerå’Œå±€éƒ¨ç²¾ç»†å·ç§¯ã€‚</li>
<li>æå‡ºäº†Taylorå¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼ˆT-MSAï¼‰ï¼Œå¹¶ç»“åˆç©ºé—´é€šé“å¢å¼ºæ³¨æ„åŠ›ï¼ˆSCEAï¼‰ï¼Œä¿ƒè¿›é€šé“é—´çš„ä¿¡æ¯äº¤æ¢ã€‚</li>
<li>ä¸ºè¡¥å……å…¨å±€å»ºæ¨¡ï¼Œå¼•å…¥äº†å±€éƒ¨ç²¾ç»†å·ç§¯ï¼ˆLRCï¼‰å—ï¼Œä»¥æ•è·ç²¾ç»†çš„å±€éƒ¨ç»†èŠ‚ã€‚</li>
<li>LORTåŸºäºU-Netç¼–ç è§£ç å™¨ç»“æ„ï¼Œé‡‡ç”¨å¤šåˆ†è¾¨ç‡çš„T-MSAæ¨¡å—å¤„ç†å¸¦å™ªè¾“å…¥ã€‚</li>
<li>é€šè¿‡å¤åˆæŸå¤±å‡½æ•°ä¼˜åŒ–å¢å¼ºå¹…åº¦å’Œç›¸ä½è°±çš„ç‹¬ç«‹è§£ç ã€‚</li>
<li>åœ¨VCTK+DEMANDå’ŒDNS Challengeæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜LORTæ€§èƒ½ä¼˜è¶Šï¼Œå‚æ•°æ•ˆç‡è¾ƒé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ad6d90724b3430cdc4afa1d793ef23e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6ab2f949af1bf2c8f16b01a870550ce" align="middle">
<img src="https://picx.zhimg.com/v2-aacefefcd2fb764b27e09d9619cb5847" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="An-Efficient-Transfer-Learning-Method-Based-on-Adapter-with-Local-Attributes-for-Speech-Emotion-Recognition"><a href="#An-Efficient-Transfer-Learning-Method-Based-on-Adapter-with-Local-Attributes-for-Speech-Emotion-Recognition" class="headerlink" title="An Efficient Transfer Learning Method Based on Adapter with Local   Attributes for Speech Emotion Recognition"></a>An Efficient Transfer Learning Method Based on Adapter with Local   Attributes for Speech Emotion Recognition</h2><p><strong>Authors:Haoyu Song, Ian McLoughlin, Qing Gu, Nan Jiang, Yan Song</strong></p>
<p>Existing speech emotion recognition (SER) methods commonly suffer from the lack of high-quality large-scale corpus, partly due to the complex, psychological nature of emotion which makes accurate labeling difficult and time consuming. Recently, transfer learning based methods that exploit the encoders pretrained on large-scale speech corpus (e.g., Wav2Vec2.0 and HuBERT) have shown strong potential for downstream SER tasks. However, task-specific fine-tuning remains necessary for various conversational scenarios of different topics, speakers and languages to achieve satisfactory performance. It generally requires costly encoder retraining for individual SER tasks. To address this issue, we propose to train an adapter with local attributes for efficient transfer learning. Specifically, a weighted average pooling-Transformer (WAP-Transformer) is proposed as a lightweight backbone to enrich the frame-level representation. An adapter with teacher-student branches is exploited for task-agnostic transfer learning, where the student branch is jointly optimized via mask prediction and self-distillation objectives, and the teacher branch is obtained online from the student via exponential moving average (EMA). Meanwhile, local attributes are learned from the teacher branch via unsupervised clustering, which aims to act as a universal model that provides additional semantic-rich supervisions. A statistical attentive pooling (SAP) module is proposed to obtain utterance representation for fine-tuning. To evaluate the effectiveness of the proposed adapter with local attributes, extensive experiments have been conducted on IEMOCAP. Superior performance has been reported, compared to the previous state-of-the-art methods in similar settings. </p>
<blockquote>
<p>ç°æœ‰è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ–¹æ³•é€šå¸¸é¢ä¸´ç¼ºä¹é«˜è´¨é‡å¤§è§„æ¨¡è¯­æ–™åº“çš„æŒ‘æˆ˜ï¼Œè¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ˜¯ç”±äºæƒ…æ„Ÿçš„å¤æ‚å¿ƒç†ç‰¹æ€§ä½¿å¾—å‡†ç¡®æ ‡æ³¨å›°éš¾å’Œè€—æ—¶ã€‚æœ€è¿‘ï¼ŒåŸºäºè¿ç§»å­¦ä¹ çš„åˆ©ç”¨åœ¨å¤§è§„æ¨¡è¯­éŸ³è¯­æ–™åº“ä¸Šé¢„è®­ç»ƒçš„ç¼–ç å™¨ï¼ˆå¦‚Wav2Vec 2.0å’ŒHuBERTï¼‰çš„æ–¹æ³•åœ¨ä¸‹æ¸¸SERä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œé’ˆå¯¹å„ç§è¯é¢˜ã€å‘è¨€è€…å’Œè¯­è¨€çš„å¯¹è¯åœºæ™¯ï¼Œä»éœ€è¦è¿›è¡Œç‰¹å®šçš„ä»»åŠ¡å¾®è°ƒä»¥è¾¾åˆ°ä»¤äººæ»¡æ„çš„è¡¨ç°ï¼Œé€šå¸¸éœ€è¦é’ˆå¯¹ä¸ªåˆ«SERä»»åŠ¡è¿›è¡Œæ˜‚è´µçš„ç¼–ç å™¨é‡æ–°è®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºè®­ç»ƒå¸¦æœ‰å±€éƒ¨å±æ€§çš„é€‚é…å™¨ä»¥å®ç°æœ‰æ•ˆçš„è¿ç§»å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼Œæå‡ºäº†ä¸€ç§åŠ æƒå¹³å‡æ± åŒ–-Transformerï¼ˆWAP-Transformerï¼‰ä½œä¸ºè½»é‡çº§ä¸»å¹²ï¼Œä»¥ä¸°å¯Œå¸§çº§è¡¨ç¤ºã€‚åˆ©ç”¨å¸¦æœ‰æ•™å¸ˆå­¦ç”Ÿåˆ†æ”¯çš„é€‚é…å™¨è¿›è¡Œä»»åŠ¡æ— å…³çš„è¿ç§»å­¦ä¹ ï¼Œå…¶ä¸­å­¦ç”Ÿåˆ†æ”¯é€šè¿‡æ©æ¨¡é¢„æµ‹å’Œè‡ªæˆ‘è’¸é¦ç›®æ ‡è¿›è¡Œä¼˜åŒ–ï¼Œæ•™å¸ˆåˆ†æ”¯åˆ™é€šè¿‡å­¦ç”Ÿåˆ†æ”¯çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰åœ¨çº¿è·å¾—ã€‚åŒæ—¶ï¼Œä»æ•™å¸ˆåˆ†æ”¯ä¸­å­¦ä¹ å±€éƒ¨å±æ€§ï¼Œé€šè¿‡æ— ç›‘ç£èšç±»ï¼Œæ—¨åœ¨å……å½“é€šç”¨æ¨¡å‹ï¼Œæä¾›é¢å¤–çš„è¯­ä¹‰ä¸°å¯Œçš„ç›‘ç£ã€‚è¿˜æå‡ºäº†ä¸€ç§ç»Ÿè®¡æ³¨æ„åŠ›æ± åŒ–ï¼ˆSAPï¼‰æ¨¡å—ï¼Œç”¨äºè·å¾—ç”¨äºç²¾ç»†è°ƒæ•´çš„è¯­å¥è¡¨ç¤ºã€‚ä¸ºäº†è¯„ä¼°å¸¦æœ‰å±€éƒ¨å±æ€§çš„æè®®é€‚é…å™¨çš„æœ‰æ•ˆæ€§ï¼Œå·²ç»åœ¨IEMOCAPä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚ä¸ç±»ä¼¼è®¾ç½®ä¸­çš„å…ˆå‰æœ€å…ˆè¿›çš„ç›¸æ¯”ï¼ŒæŠ¥å‘Šäº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23795v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰é¢†åŸŸä¸­çš„é—®é¢˜ï¼ŒåŒ…æ‹¬ç¼ºä¹é«˜è´¨é‡çš„å¤§è§„æ¨¡è¯­æ–™åº“å’Œé’ˆå¯¹ä¸åŒä»»åŠ¡ã€è¯´è¯è€…ã€è¯­è¨€çš„ä¸ªæ€§åŒ–å¾®è°ƒæŒ‘æˆ˜ã€‚æ–‡ç« æå‡ºäº†ä½¿ç”¨å¸¦æœ‰æœ¬åœ°å±æ€§çš„é€‚é…å™¨è¿›è¡Œé«˜æ•ˆè¿ç§»å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡WAP-Transformerä½œä¸ºè½»é‡çº§ä¸»å¹²æ¥ä¸°å¯Œå¸§çº§è¡¨ç¤ºï¼Œå¹¶å¼•å…¥æ•™å¸ˆ-å­¦ç”Ÿåˆ†æ”¯çš„é€‚é…å™¨è¿›è¡Œä»»åŠ¡æ— å…³çš„è¿ç§»å­¦ä¹ ã€‚åŒæ—¶ï¼Œé€šè¿‡æ— ç›‘ç£èšç±»ä»æ•™å¸ˆåˆ†æ”¯ä¸­å­¦ä¹ æœ¬åœ°å±æ€§ï¼Œæ—¨åœ¨ä½œä¸ºé€šç”¨æ¨¡å‹æä¾›ä¸°å¯Œçš„è¯­ä¹‰ç›‘ç£ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†SAPæ¨¡å—ä»¥è·å¾—ç”¨äºç²¾ç»†è°ƒæ•´çš„è¯­å¥è¡¨ç¤ºã€‚åœ¨IEMOCAPä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ç±»ä¼¼è®¾ç½®ä¸­çš„å…ˆå‰æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥é€‚é…å™¨å…·æœ‰ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰é¢ä¸´ç¼ºä¹é«˜è´¨é‡å¤§è§„æ¨¡è¯­æ–™åº“å’Œå¤æ‚æƒ…ç»ªæ ‡ç­¾çš„æŒ‘æˆ˜ã€‚</li>
<li>è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨å¤§è§„æ¨¡è¯­éŸ³è¯­æ–™åº“çš„é¢„è®­ç»ƒç¼–ç å™¨ï¼ˆå¦‚Wav2Vec2.0å’ŒHuBERTï¼‰ï¼Œåœ¨SERä»»åŠ¡ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>ä»»åŠ¡ç‰¹å®šçš„å¾®è°ƒå¯¹äºåº”å¯¹ä¸åŒè¯é¢˜ã€è¯´è¯è€…å’Œè¯­è¨€çš„å¯¹è¯åœºæ™¯æ˜¯å¿…è¦çš„ã€‚</li>
<li>å¼•å…¥å¸¦æœ‰æœ¬åœ°å±æ€§çš„é€‚é…å™¨æ¥è§£å†³è¿ç§»å­¦ä¹ ä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬WAP-Transformerä½œä¸ºè½»é‡çº§ä¸»å¹²å’Œå¼•å…¥æ•™å¸ˆ-å­¦ç”Ÿåˆ†æ”¯ã€‚</li>
<li>æ•™å¸ˆåˆ†æ”¯é€šè¿‡æ— ç›‘ç£èšç±»å­¦ä¹ æœ¬åœ°å±æ€§ï¼Œæ—¨åœ¨ä½œä¸ºé€šç”¨æ¨¡å‹æä¾›ä¸°å¯Œçš„è¯­ä¹‰ç›‘ç£ã€‚</li>
<li>å¼•å…¥SAPæ¨¡å—ä»¥è·å–ç”¨äºç²¾ç»†è°ƒæ•´çš„è¯­å¥è¡¨ç¤ºã€‚</li>
<li>åœ¨IEMOCAPä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç±»ä¼¼è®¾ç½®ä¸­å…·æœ‰ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23795">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e9c06826ab257c49f0ab9e58467b3ed9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47784154e985d4053e5060ebc5e55430.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58784623490e54ca7ad524d28b45ee48" align="middle">
<img src="https://picx.zhimg.com/v2-d12ea9a98304bf4ebb5f555a05a8fcd7" align="middle">
<img src="https://pic1.zhimg.com/v2-4250f3d9de72bc722e02cc9958f61de4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Game-Oriented-ASR-Error-Correction-via-RAG-Enhanced-LLM"><a href="#Game-Oriented-ASR-Error-Correction-via-RAG-Enhanced-LLM" class="headerlink" title="Game-Oriented ASR Error Correction via RAG-Enhanced LLM"></a>Game-Oriented ASR Error Correction via RAG-Enhanced LLM</h2><p><strong>Authors:Yan Jiang, Yongle Luo, Qixian Zhou, Elvis S. Liu</strong></p>
<p>With the rise of multiplayer online games, real-time voice communication is essential for team coordination. However, general ASR systems struggle with gaming-specific challenges like short phrases, rapid speech, jargon, and noise, leading to frequent errors. To address this, we propose the GO-AEC framework, which integrates large language models, Retrieval-Augmented Generation (RAG), and a data augmentation strategy using LLMs and TTS. GO-AEC includes data augmentation, N-best hypothesis-based correction, and a dynamic game knowledge base. Experiments show GO-AEC reduces character error rate by 6.22% and sentence error rate by 29.71%, significantly improving ASR accuracy in gaming scenarios. </p>
<blockquote>
<p>éšç€å¤šäººåœ¨çº¿æ¸¸æˆçš„å…´èµ·ï¼Œå®æ—¶è¯­éŸ³äº¤æµå¯¹äºå›¢é˜Ÿåä½œè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œé€šç”¨çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿé¢ä¸´ç€æ¸¸æˆç‰¹æœ‰çš„æŒ‘æˆ˜ï¼Œå¦‚çŸ­å¥ã€è¯­é€Ÿå¿«ã€ä¸“ä¸šæœ¯è¯­å’Œå™ªéŸ³ï¼Œå¯¼è‡´é¢‘ç¹å‡ºé”™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GO-AECæ¡†æ¶ï¼Œå®ƒèåˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’ŒåŸºäºLLMå’ŒTTSçš„æ•°æ®å¢å¼ºç­–ç•¥ã€‚GO-AECåŒ…æ‹¬æ•°æ®å¢å¼ºã€åŸºäºN-bestå‡è®¾çš„æ ¡æ­£ä»¥åŠåŠ¨æ€æ¸¸æˆçŸ¥è¯†åº“ã€‚å®éªŒè¡¨æ˜ï¼ŒGO-AECé™ä½äº†6.22%çš„å­—ç¬¦é”™è¯¯ç‡å’Œ29.71%çš„å¥å­é”™è¯¯ç‡ï¼Œæ˜¾è‘—æé«˜äº†æ¸¸æˆåœºæ™¯ä¸­è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23630v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>éšç€å¤šäººåœ¨çº¿æ¸¸æˆçš„å…´èµ·ï¼Œå®æ—¶è¯­éŸ³é€šä¿¡å¯¹å›¢é˜Ÿåä½œè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œé€šç”¨ASRç³»ç»Ÿåœ¨é¢å¯¹æ¸¸æˆç‰¹å®šæŒ‘æˆ˜æ—¶ï¼Œå¦‚çŸ­å¥ã€å¿«é€Ÿè¯­éŸ³ã€è¡Œè¯å’Œå™ªéŸ³ç­‰ï¼Œå®¹æ˜“å‡ºç°é¢‘ç¹é”™è¯¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GO-AECæ¡†æ¶ï¼Œå®ƒé›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’ŒåŸºäºLLMå’ŒTTSçš„æ•°æ®å¢å¼ºç­–ç•¥ã€‚GO-AECåŒ…æ‹¬æ•°æ®å¢å¼ºã€åŸºäºN-bestå‡è®¾çš„æ ¡æ­£å’Œä¸€ä¸ªåŠ¨æ€æ¸¸æˆçŸ¥è¯†åº“ã€‚å®éªŒè¡¨æ˜ï¼ŒGO-AECé™ä½äº†å­—ç¬¦é”™è¯¯ç‡6.22%ï¼Œå¥å­é”™è¯¯ç‡é™ä½äº†29.71%ï¼Œæ˜¾è‘—æé«˜äº†æ¸¸æˆåœºæ™¯ä¸­ASRçš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å®æ—¶è¯­éŸ³é€šä¿¡åœ¨å¤šç©å®¶åœ¨çº¿æ¸¸æˆä¸­å¯¹å›¢é˜Ÿåä½œè‡³å…³é‡è¦ã€‚</li>
<li>é€šç”¨ASRç³»ç»Ÿåœ¨å¤„ç†æ¸¸æˆç‰¹å®šæŒ‘æˆ˜ï¼ˆå¦‚çŸ­å¥ã€å¿«é€Ÿè¯­éŸ³ã€è¡Œè¯å’Œå™ªéŸ³ï¼‰æ—¶å­˜åœ¨é¢‘ç¹é”™è¯¯ã€‚</li>
<li>GO-AECæ¡†æ¶é›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’ŒåŸºäºLLMåŠTTSçš„æ•°æ®å¢å¼ºç­–ç•¥ã€‚</li>
<li>GO-AECé€šè¿‡æ•°æ®å¢å¼ºã€åŸºäºN-bestå‡è®¾çš„æ ¡æ­£ä»¥åŠåŠ¨æ€æ¸¸æˆçŸ¥è¯†åº“æ¥æé«˜ASRæ€§èƒ½ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒGO-AECé™ä½äº†å­—ç¬¦é”™è¯¯ç‡å’Œå¥å­é”™è¯¯ç‡ï¼Œåˆ†åˆ«è¾¾åˆ°äº†6.22%å’Œ29.71%çš„æ”¹å–„ã€‚</li>
<li>GO-AECæ¡†æ¶æ˜¾è‘—æé«˜åœ¨æ¸¸æˆåœºæ™¯ä¸­ASRçš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35b80501eeabe10e0e8450504a7729e0" align="middle">
<img src="https://pic1.zhimg.com/v2-93aaf6127b33e78c2c8eb303ba97f07f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dea293248dcdf1bf8d85633947890c4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8e86cf676a410badb6f51ea2fb56c8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce16e0f3923580ce9071b4157e8711e4" align="middle">
<img src="https://pica.zhimg.com/v2-d8321c34d88b04937e90944918b87e67.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Generalizable-Speech-Deepfake-Detection-via-Information-Bottleneck-Enhanced-Adversarial-Alignment"><a href="#Generalizable-Speech-Deepfake-Detection-via-Information-Bottleneck-Enhanced-Adversarial-Alignment" class="headerlink" title="Generalizable Speech Deepfake Detection via Information Bottleneck   Enhanced Adversarial Alignment"></a>Generalizable Speech Deepfake Detection via Information Bottleneck   Enhanced Adversarial Alignment</h2><p><strong>Authors:Pu Huang, Shouguang Wang, Siya Yao, Mengchu Zhou</strong></p>
<p>Neural speech synthesis techniques have enabled highly realistic speech deepfakes, posing major security risks. Speech deepfake detection is challenging due to distribution shifts across spoofing methods and variability in speakers, channels, and recording conditions. We explore learning shared discriminative features as a path to robust detection and propose Information Bottleneck enhanced Confidence-Aware Adversarial Network (IB-CAAN). Confidence-guided adversarial alignment adaptively suppresses attack-specific artifacts without erasing discriminative cues, while the information bottleneck removes nuisance variability to preserve transferable features. Experiments on ASVspoof 2019&#x2F;2021, ASVspoof 5, and In-the-Wild demonstrate that IB-CAAN consistently outperforms baseline and achieves state-of-the-art performance on many benchmarks. </p>
<blockquote>
<p>ç¥ç»è¯­éŸ³åˆæˆæŠ€æœ¯å·²ç»èƒ½å¤Ÿå®ç°é«˜åº¦é€¼çœŸçš„è¯­éŸ³æ·±åº¦ä¼ªé€ ï¼Œè¿™å¸¦æ¥äº†é‡å¤§çš„å®‰å…¨é£é™©ã€‚è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹é¢‡å…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå‡å†’æ–¹æ³•ä¹‹é—´å­˜åœ¨åˆ†å¸ƒè½¬ç§»ï¼Œä¸”è¯´è¯äººã€é€šé“å’Œå½•éŸ³æ¡ä»¶å…·æœ‰å¤šå˜æ€§å’Œå·®å¼‚æ€§ã€‚æˆ‘ä»¬æ¢ç´¢å­¦ä¹ å…±äº«åˆ¤åˆ«ç‰¹å¾ä½œä¸ºå®ç°ç¨³å¥æ£€æµ‹çš„é€”å¾„ï¼Œå¹¶æå‡ºåŸºäºä¿¡æ¯ç“¶é¢ˆå¢å¼ºçš„ä¿¡å¿ƒæ„ŸçŸ¥å¯¹æŠ—ç½‘ç»œï¼ˆIB-CAANï¼‰ã€‚ä¿¡å¿ƒå¼•å¯¼å¯¹æŠ—å¯¹é½èƒ½å¤Ÿè‡ªé€‚åº”åœ°æŠ‘åˆ¶ç‰¹å®šæ”»å‡»çš„ä¼ªè¿¹ï¼Œè€Œä¸ä¼šæ¶ˆé™¤åˆ¤åˆ«çº¿ç´¢ï¼Œè€Œä¿¡æ¯ç“¶é¢ˆåˆ™èƒ½å¤Ÿæ¶ˆé™¤å¹²æ‰°å˜é‡ä»¥ä¿ç•™å¯è¿ç§»ç‰¹å¾ã€‚åœ¨ASVspoof 2019&#x2F;2021ã€ASVspoof 5å’Œé‡å¤–å®éªŒä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒIB-CAANå§‹ç»ˆä¼˜äºåŸºçº¿å¹¶å®ç°äº†è®¸å¤šåŸºå‡†æµ‹è¯•çš„æœ€æ–°æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23618v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œè¯­éŸ³åˆæˆæŠ€æœ¯å·²èƒ½ç”Ÿæˆé«˜åº¦é€¼çœŸçš„è¯­éŸ³æ·±åº¦ä¼ªé€ å†…å®¹ï¼Œç»™å®‰å…¨å¸¦æ¥é‡å¤§é£é™©ã€‚ç”±äºä¼ªè£…æ–¹æ³•çš„åˆ†å¸ƒå˜åŒ–ä»¥åŠè¯´è¯äººã€ä¿¡é“å’Œå½•éŸ³æ¡ä»¶çš„å¯å˜æ€§ï¼Œè¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹çš„éš¾ç‚¹åœ¨äºå®ç°ç¨³å¥çš„æ£€æµ‹ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†å­¦ä¹ å…±äº«åˆ¤åˆ«ç‰¹å¾ä½œä¸ºç¨³å¥æ£€æµ‹çš„é€”å¾„ï¼Œå¹¶æå‡ºäº†åŸºäºä¿¡æ¯ç“¶é¢ˆå¢å¼ºå‹ä¿¡å¿ƒæ„ŸçŸ¥å¯¹æŠ—ç½‘ç»œï¼ˆIB-CAANï¼‰ã€‚ä¿¡å¿ƒå¼•å¯¼çš„å¯¹æŠ—å¯¹é½å¯ä»¥è‡ªé€‚åº”åœ°æŠ‘åˆ¶æ”»å‡»ç‰¹å®šçš„ä¼ªåƒï¼Œè€Œä¸ä¼šæ¶ˆé™¤åˆ¤åˆ«çº¿ç´¢ï¼›ä¿¡æ¯ç“¶é¢ˆåˆ™èƒ½å¤Ÿæ¶ˆé™¤å¹²æ‰°å˜åŒ–ä»¥ä¿ç•™å¯è½¬ç§»çš„ç‰¹å¾ã€‚åœ¨ASVspoof 2019&#x2F;2021ã€ASVspoof 5ä»¥åŠçœŸå®ç¯å¢ƒä¸‹çš„å®éªŒè¡¨æ˜ï¼ŒIB-CAANæŒç»­ä¼˜äºåŸºçº¿æ–¹æ³•å¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°ä¸šç•Œæœ€ä½³æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œè¯­éŸ³åˆæˆæŠ€æœ¯å¯ä»¥ç”Ÿæˆé«˜åº¦é€¼çœŸçš„è¯­éŸ³æ·±åº¦ä¼ªé€ å†…å®¹ï¼Œå­˜åœ¨é‡å¤§å®‰å…¨é£é™©ã€‚</li>
<li>è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹é¢ä¸´å¤šç§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åˆ†å¸ƒå˜åŒ–ã€è¯´è¯äººã€ä¿¡é“å’Œå½•éŸ³æ¡ä»¶çš„å¯å˜æ€§ã€‚</li>
<li>å­¦ä¹ å…±äº«åˆ¤åˆ«ç‰¹å¾æ˜¯å®ç°ç¨³å¥è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹çš„é‡è¦é€”å¾„ã€‚</li>
<li>æå‡ºçš„IB-CAANæ–¹æ³•ç»“åˆäº†ä¿¡æ¯ç“¶é¢ˆå’Œä¿¡å¿ƒæ„ŸçŸ¥å¯¹æŠ—ç½‘ç»œã€‚</li>
<li>ä¿¡å¿ƒå¼•å¯¼çš„å¯¹æŠ—å¯¹é½èƒ½å¤Ÿè‡ªé€‚åº”åœ°æŠ‘åˆ¶æ”»å‡»ç‰¹å®šçš„ä¼ªåƒï¼ŒåŒæ—¶ä¿ç•™åˆ¤åˆ«çº¿ç´¢ã€‚</li>
<li>ä¿¡æ¯ç“¶é¢ˆæœ‰åŠ©äºæ¶ˆé™¤å¹²æ‰°å˜åŒ–ï¼Œä¿ç•™å¯è½¬ç§»çš„ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23618">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9e937bfdde636552f7190e8e6de71338.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d66b53ad0a1f15300a711c230a6843b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a5f056c536782a335b06da4f06eb919.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35ad7ad69fcbf6a834ade422b6369c33" align="middle">
<img src="https://picx.zhimg.com/v2-9bf05d80f1729d39e2fbec5f27e5ddd0" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention"><a href="#Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention" class="headerlink" title="Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and   Multi-Scale Global-Local Attention"></a>Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and   Multi-Scale Global-Local Attention</h2><p><strong>Authors:Kai Li, Kejun Gao, Xiaolin Hu</strong></p>
<p>Audio-visual speech separation (AVSS) methods leverage visual cues to extract target speech and have demonstrated strong separation quality in noisy acoustic environments. However, these methods usually involve a large number of parameters and require high computational cost, which is unacceptable in many applications where speech separation serves as only a preprocessing step for further speech processing. To address this issue, we propose an efficient AVSS method, named Dolphin. For visual feature extraction, we develop DP-LipCoder, a dual-path lightweight video encoder that transforms lip-motion into discrete audio-aligned semantic tokens. For audio separation, we construct a lightweight encoder-decoder separator, in which each layer incorporates a global-local attention (GLA) block to efficiently capture multi-scale dependencies. Experiments on three benchmark datasets showed that Dolphin not only surpassed the current state-of-the-art (SOTA) model in separation quality but also achieved remarkable improvements in efficiency: over 50% fewer parameters, more than 2.4x reduction in MACs, and over 6x faster GPU inference speed. These results indicate that Dolphin offers a practical and deployable solution for high-performance AVSS in real-world scenarios. Our code and demo page are publicly available at <a target="_blank" rel="noopener" href="http://cslikai.cn/Dolphin/">http://cslikai.cn/Dolphin/</a>. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³åˆ†ç¦»ï¼ˆAVSSï¼‰æ–¹æ³•åˆ©ç”¨è§†è§‰çº¿ç´¢æå–ç›®æ ‡è¯­éŸ³ï¼Œåœ¨å˜ˆæ‚çš„å£°å­¦ç¯å¢ƒä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„åˆ†ç¦»è´¨é‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸æ¶‰åŠå¤§é‡å‚æ•°ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼Œè¿™åœ¨è®¸å¤šä»…å°†è¯­éŸ³åˆ†ç¦»ä½œä¸ºè¿›ä¸€æ­¥è¯­éŸ³å¤„ç†çš„é¢„å¤„ç†æ­¥éª¤çš„åº”ç”¨ä¸­æ˜¯ä¸å¯æ¥å—çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„AVSSæ–¹æ³•ï¼Œå‘½åä¸ºDolphinã€‚å¯¹äºè§†è§‰ç‰¹å¾æå–ï¼Œæˆ‘ä»¬å¼€å‘äº†DP-LipCoderï¼Œè¿™æ˜¯ä¸€ç§åŒè·¯å¾„è½»é‡åŒ–è§†é¢‘ç¼–ç å™¨ï¼Œå¯å°†å”‡éƒ¨è¿åŠ¨è½¬æ¢ä¸ºç¦»æ•£çš„éŸ³é¢‘å¯¹é½è¯­ä¹‰æ ‡è®°ã€‚å¯¹äºéŸ³é¢‘åˆ†ç¦»ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªè½»é‡çº§çš„ç¼–ç å™¨-è§£ç å™¨åˆ†ç¦»å™¨ï¼Œå…¶ä¸­æ¯ä¸€å±‚éƒ½åµŒå…¥äº†ä¸€ä¸ªå…¨å±€-å±€éƒ¨æ³¨æ„åŠ›ï¼ˆGLAï¼‰å—ï¼Œä»¥æœ‰æ•ˆåœ°æ•è·å¤šå°ºåº¦ä¾èµ–æ€§ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDolphinä¸ä»…åœ¨åˆ†ç¦»è´¨é‡ä¸Šè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œè€Œä¸”åœ¨æ•ˆç‡ä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼šå‚æ•°å‡å°‘äº†50%ä»¥ä¸Šï¼ŒMACså‡å°‘äº†2.4å€ä»¥ä¸Šï¼ŒGPUæ¨ç†é€Ÿåº¦æé«˜äº†6å€ä»¥ä¸Šã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDolphinä¸ºç°å®åœºæ™¯ä¸­çš„é«˜æ€§èƒ½AVSSæä¾›äº†å®ç”¨ä¸”å¯éƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¼”ç¤ºé¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="http://cslikai.cn/Dolphin/%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">http://cslikai.cn/Dolphin/å…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23610v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é«˜æ•ˆçš„è§†å¬è¯­éŸ³åˆ†ç¦»æ–¹æ³•â€”â€”æµ·è±šï¼ˆDolphinï¼‰ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŒè·¯å¾„è½»é‡çº§è§†é¢‘ç¼–ç å™¨DP-LipCoderæå–è§†è§‰ç‰¹å¾ï¼Œé€šè¿‡å…¨çƒå±€éƒ¨æ³¨æ„åŠ›ï¼ˆGLAï¼‰å—æ„å»ºè½»é‡çº§ç¼–ç å™¨-è§£ç å™¨åˆ†ç¦»å™¨ï¼Œå®ç°å¤šå°ºåº¦ä¾èµ–æ€§çš„é«˜æ•ˆæ•è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDolphinä¸ä»…åœ¨åˆ†ç¦»è´¨é‡ä¸Šè¶…è¶Šäº†å½“å‰æœ€ä½³æ¨¡å‹ï¼Œè€Œä¸”åœ¨æ•ˆç‡ä¸Šä¹Ÿæœ‰æ˜¾è‘—æé«˜ï¼šå‚æ•°å‡å°‘50%ä»¥ä¸Šï¼ŒMACså‡å°‘2.4å€ä»¥ä¸Šï¼ŒGPUæ¨ç†é€Ÿåº¦æé«˜6å€ä»¥ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Dolphinæ˜¯ä¸€ç§é«˜æ•ˆçš„è§†å¬è¯­éŸ³åˆ†ç¦»æ–¹æ³•ï¼Œå¯åº”ç”¨äºå™ªå£°ç¯å¢ƒä¸‹çš„è¯­éŸ³åˆ†ç¦»ã€‚</li>
<li>DP-LipCoderæ˜¯åŒè·¯å¾„è½»é‡çº§è§†é¢‘ç¼–ç å™¨ï¼Œå¯å°†å”‡éƒ¨åŠ¨ä½œè½¬åŒ–ä¸ºç¦»æ•£éŸ³é¢‘å¯¹é½è¯­ä¹‰ä»¤ç‰Œï¼Œç”¨äºè§†è§‰ç‰¹å¾æå–ã€‚</li>
<li>å…¨çƒå±€éƒ¨æ³¨æ„åŠ›ï¼ˆGLAï¼‰å—è¢«ç”¨äºæ„å»ºè½»é‡çº§ç¼–ç å™¨-è§£ç å™¨åˆ†ç¦»å™¨ï¼Œä»¥æ•è·å¤šå°ºåº¦ä¾èµ–æ€§ã€‚</li>
<li>Dolphinåœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶åˆ†ç¦»è´¨é‡è¶…è¶Šå½“å‰æœ€ä½³æ¨¡å‹ã€‚</li>
<li>Dolphinåœ¨æ•ˆç‡ä¸Šæœ‰æ˜¾è‘—æ”¹å–„ï¼ŒåŒ…æ‹¬å‚æ•°å‡å°‘ã€MACså‡å°‘å’ŒGPUæ¨ç†é€Ÿåº¦æé«˜ã€‚</li>
<li>æµ·è±šæ–¹æ³•æä¾›å®ç”¨ä¸”å¯éƒ¨ç½²çš„è§†å¬è¯­éŸ³åˆ†ç¦»è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºçœŸå®ä¸–ç•Œåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23610">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ef6881ced6bc9d9d0626df1798d14d2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44cafe3461238491ee5aa2bd435d367f" align="middle">
<img src="https://picx.zhimg.com/v2-79e047400eaa3e5de51cdcd4ee7be8bb" align="middle">
<img src="https://pic1.zhimg.com/v2-8f72daf0586f67395d56a23bd88bac37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e82cdfe47ce1f2f1fb72660c6ab1a2d0" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MeanFlowSE-One-Step-Generative-Speech-Enhancement-via-MeanFlow"><a href="#MeanFlowSE-One-Step-Generative-Speech-Enhancement-via-MeanFlow" class="headerlink" title="MeanFlowSE: One-Step Generative Speech Enhancement via MeanFlow"></a>MeanFlowSE: One-Step Generative Speech Enhancement via MeanFlow</h2><p><strong>Authors:Yike Zhu, Boyi Kang, Ziqian Wang, Xingchen Li, Zihan Zhang, Wenjie Li, Longshuai Xiao, Wei Xue, Lei Xie</strong></p>
<p>Speech enhancement (SE) recovers clean speech from noisy signals and is vital for applications such as telecommunications and automatic speech recognition (ASR). While generative approaches achieve strong perceptual quality, they often rely on multi-step sampling (diffusion&#x2F;flow-matching) or large language models, limiting real-time deployment. To mitigate these constraints, we present MeanFlowSE, a one-step generative SE framework. It adopts MeanFlow to predict an average-velocity field for one-step latent refinement and conditions the model on self-supervised learning (SSL) representations rather than VAE latents. This design accelerates inference and provides robust acoustic-semantic guidance during training. In the Interspeech 2020 DNS Challenge blind test set and simulated test set, MeanFlowSE attains state-of-the-art (SOTA) level perceptual quality and competitive intelligibility while significantly lowering both real-time factor (RTF) and model size compared with recent generative competitors, making it suitable for practical use. The code will be released upon publication at <a target="_blank" rel="noopener" href="https://github.com/Hello3orld/MeanFlowSE">https://github.com/Hello3orld/MeanFlowSE</a>. </p>
<blockquote>
<p>è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰ä»å™ªå£°ä¿¡å·ä¸­æ¢å¤æ¸…æ´è¯­éŸ³ï¼Œå¯¹äºç”µä¿¡å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚è™½ç„¶ç”Ÿæˆæ–¹æ³•è¾¾åˆ°äº†å¾ˆå¼ºçš„æ„ŸçŸ¥è´¨é‡ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºå¤šæ­¥é‡‡æ ·ï¼ˆæ‰©æ•£&#x2F;æµåŒ¹é…ï¼‰æˆ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé™åˆ¶äº†å®æ—¶éƒ¨ç½²ã€‚ä¸ºäº†ç¼“è§£è¿™äº›çº¦æŸï¼Œæˆ‘ä»¬æå‡ºäº†MeanFlowSEï¼Œè¿™æ˜¯ä¸€ä¸ªä¸€æ­¥ç”ŸæˆSEæ¡†æ¶ã€‚å®ƒé‡‡ç”¨MeanFlowæ¥é¢„æµ‹ä¸€æ­¥æ½œåœ¨ç²¾åŒ–çš„å¹³å‡é€Ÿåº¦åœºï¼Œå¹¶ä»¥è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰è¡¨ç¤ºè€Œä¸æ˜¯VAEæ½œè—é‡å¯¹æ¨¡å‹è¿›è¡Œæ¡ä»¶å¤„ç†ã€‚è¿™ç§è®¾è®¡åŠ é€Ÿäº†æ¨ç†ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æä¾›äº†ç¨³å¥çš„å£°å­¦è¯­ä¹‰æŒ‡å¯¼ã€‚åœ¨Interspeech 2020 DNS Challengeçš„ç›²æµ‹è¯•é›†å’Œæ¨¡æ‹Ÿæµ‹è¯•é›†ä¸­ï¼ŒMeanFlowSEè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ„ŸçŸ¥è´¨é‡å’Œæœ‰ç«äº‰åŠ›çš„æ¸…æ™°åº¦ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†å®æ—¶å› å­ï¼ˆRTFï¼‰å’Œæ¨¡å‹å¤§å°ï¼Œä¸æœ€è¿‘çš„ç”Ÿæˆç«äº‰å¯¹æ‰‹ç›¸æ¯”ï¼Œé€‚åˆå®é™…åº”ç”¨ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hello3orld/MeanFlowSE%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Hello3orld/MeanFlowSEä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23299v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æ–‡ç« ä»‹ç»äº†MeanFlowSEï¼Œè¿™æ˜¯ä¸€ç§ä¸€æ­¥ç”Ÿæˆå¼è¯­éŸ³å¢å¼ºæ¡†æ¶ã€‚å®ƒé‡‡ç”¨MeanFlowé¢„æµ‹å¹³å‡é€Ÿåº¦åœºè¿›è¡Œä¸€æ­¥æ½œåœ¨ä¼˜åŒ–ï¼Œå¹¶åœ¨è‡ªç›‘ç£å­¦ä¹ è¡¨ç¤ºæ¡ä»¶ä¸‹è¿›è¡Œè®­ç»ƒã€‚è¯¥æ¡†æ¶åœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸå®ç°äº†ä¸šç•Œé¢†å…ˆçš„æ„ŸçŸ¥è´¨é‡å’Œå¯è¯†åˆ«æ€§ï¼ŒåŒæ—¶é™ä½äº†å®æ—¶å› å­å’Œæ¨¡å‹å¤§å°ï¼Œé€‚åˆå®é™…åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MeanFlowSEæ˜¯ä¸€ç§ä¸€æ­¥ç”Ÿæˆå¼è¯­éŸ³å¢å¼ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç”Ÿæˆå¼æ–¹æ³•é¢ä¸´çš„çº¦æŸé—®é¢˜ã€‚</li>
<li>å®ƒé‡‡ç”¨MeanFlowé¢„æµ‹å¹³å‡é€Ÿåº¦åœºï¼Œç”¨äºä¸€æ­¥æ½œåœ¨ä¼˜åŒ–ï¼Œæé«˜äº†è¯­éŸ³è´¨é‡ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†è‡ªç›‘ç£å­¦ä¹ è¡¨ç¤ºæ¡ä»¶è®­ç»ƒæ¨¡å‹ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨Interspeech 2020 DNS Challengeçš„ç›²æµ‹é›†å’Œæ¨¡æ‹Ÿæµ‹è¯•é›†ä¸­ï¼ŒMeanFlowSEè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆçš„æ„ŸçŸ¥è´¨é‡å’Œå¯è¯†åˆ«æ€§ã€‚</li>
<li>ä¸å…¶ä»–ç”Ÿæˆå¼æ–¹æ³•ç›¸æ¯”ï¼ŒMeanFlowSEæ˜¾è‘—é™ä½äº†å®æ—¶å› å­å’Œæ¨¡å‹å¤§å°ï¼Œæ›´é€‚åˆå®é™…åº”ç”¨ã€‚</li>
<li>MeanFlowSEçš„ä»£ç å°†åœ¨å‘å¸ƒæ—¶å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-744d8938428c5fd74c67b2267e34e291" align="middle">
<img src="https://picx.zhimg.com/v2-6001b1a912c0553937b4279f34ed1085" align="middle">
<img src="https://picx.zhimg.com/v2-a0b72033986e0860109dbe26d764c7aa" align="middle">
<img src="https://picx.zhimg.com/v2-c2564543a513ee6228feb84dafc4fc5c" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Speech-Enhancement-using-Data-defined-Priors"><a href="#Unsupervised-Speech-Enhancement-using-Data-defined-Priors" class="headerlink" title="Unsupervised Speech Enhancement using Data-defined Priors"></a>Unsupervised Speech Enhancement using Data-defined Priors</h2><p><strong>Authors:Dominik Klement, Matthew Maciejewski, Sanjeev Khudanpur, Jan ÄŒernockÃ½, LukÃ¡Å¡ Burget</strong></p>
<p>The majority of deep learning-based speech enhancement methods require paired clean-noisy speech data. Collecting such data at scale in real-world conditions is infeasible, which has led the community to rely on synthetically generated noisy speech. However, this introduces a gap between the training and testing phases. In this work, we propose a novel dual-branch encoder-decoder architecture for unsupervised speech enhancement that separates the input into clean speech and residual noise. Adversarial training is employed to impose priors on each branch, defined by unpaired datasets of clean speech and, optionally, noise. Experimental results show that our method achieves performance comparable to leading unsupervised speech enhancement approaches. Furthermore, we demonstrate the critical impact of clean speech data selection on enhancement performance. In particular, our findings reveal that performance may appear overly optimistic when in-domain clean speech data are used for prior definition â€“ a practice adopted in previous unsupervised speech enhancement studies. </p>
<blockquote>
<p>å¤§å¤šæ•°åŸºäºæ·±åº¦å­¦ä¹ çš„è¯­éŸ³å¢å¼ºæ–¹æ³•éƒ½éœ€è¦æˆå¯¹çš„å¹²å‡€å’Œå¸¦å™ªå£°çš„è¯­éŸ³æ•°æ®ã€‚åœ¨ç°å®ä¸–ç•Œçš„æ¡ä»¶ä¸‹å¤§è§„æ¨¡æ”¶é›†æ­¤ç±»æ•°æ®æ˜¯ä¸å¯è¡Œçš„ï¼Œè¿™å¯¼è‡´ç¤¾åŒºä¾èµ–äºåˆæˆç”Ÿæˆçš„å¸¦å™ªå£°è¯­éŸ³ã€‚ç„¶è€Œï¼Œè¿™ä¼šåœ¨è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µä¹‹é—´äº§ç”Ÿå·®è·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæ— ç›‘ç£è¯­éŸ³å¢å¼ºçš„æ–°å‹åŒåˆ†æ”¯ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œè¯¥æ¶æ„å°†è¾“å…¥åˆ†ç¦»ä¸ºå¹²å‡€è¯­éŸ³å’Œå‰©ä½™å™ªå£°ã€‚é‡‡ç”¨å¯¹æŠ—è®­ç»ƒå¯¹æ¯åˆ†æ”¯æ–½åŠ å…ˆéªŒï¼Œå…ˆéªŒç”±å¹²å‡€è¯­éŸ³çš„æœªé…å¯¹æ•°æ®é›†å®šä¹‰ï¼Œä»¥åŠå¯é€‰çš„å™ªå£°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸é¢†å…ˆçš„æ— ç›‘ç£è¯­éŸ³å¢å¼ºæ–¹æ³•ç›¸æ¯”å–å¾—äº†ç›¸å½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†å¹²å‡€è¯­éŸ³æ•°æ®é€‰æ‹©å¯¹å¢å¼ºæ€§èƒ½çš„å…³é”®å½±å“ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“ä½¿ç”¨é¢†åŸŸå†…çš„å¹²å‡€è¯­éŸ³æ•°æ®è¿›è¡Œå…ˆéªŒå®šä¹‰æ—¶ï¼Œæ€§èƒ½å¯èƒ½è¿‡äºä¹è§‚â€”â€”è¿™æ˜¯ä¹‹å‰åœ¨æ— ç›‘ç£è¯­éŸ³å¢å¼ºç ”ç©¶ä¸­é‡‡ç”¨çš„åšæ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22942v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>æ‘˜è¦</strong><br>æ·±åº¦å­¦ä¹ ä¸ºåŸºç¡€çš„è¯­éŸ³å¢å¼ºæ–¹æ³•å¤šæ•°éœ€è¦é…å¯¹æ¸…æ´å™ªå£°è¯­éŸ³æ•°æ®ã€‚åœ¨ç°å®ä¸–ç•Œçš„æ¡ä»¶ä¸‹å¤§è§„æ¨¡æ”¶é›†æ­¤ç±»æ•°æ®æ˜¯ä¸å¯è¡Œçš„ï¼Œè¿™ä¿ƒä½¿äººä»¬ä¾èµ–åˆæˆç”Ÿæˆçš„å™ªå£°è¯­éŸ³ã€‚ç„¶è€Œï¼Œè¿™ä¼šåœ¨è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µä¹‹é—´äº§ç”Ÿå·®è·ã€‚åœ¨æ­¤ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒåˆ†æ”¯ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œç”¨äºæ— ç›‘ç£è¯­éŸ³å¢å¼ºï¼Œå°†è¾“å…¥åˆ†ä¸ºæ¸…æ´è¯­éŸ³å’Œå‰©ä½™å™ªå£°ã€‚åˆ©ç”¨å¯¹æŠ—è®­ç»ƒå¯¹æ¯ä¸ªåˆ†æ”¯æ–½åŠ å…ˆéªŒçŸ¥è¯†ï¼Œè¿™äº›å…ˆéªŒçŸ¥è¯†ç”±æ¸…æ´è¯­éŸ³å’Œï¼ˆå¯é€‰çš„ï¼‰å™ªå£°çš„éé…å¯¹æ•°æ®é›†å®šä¹‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¾¾åˆ°é¢†å…ˆçš„éç›‘ç£è¯­éŸ³å¢å¼ºæ–¹æ³•çš„å¯æ¯”æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ¸…æ´è¯­éŸ³æ•°æ®é€‰æ‹©å¯¹å¢å¼ºæ€§èƒ½çš„å…³é”®å½±å“ã€‚ç‰¹åˆ«æ˜¯æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå½“ä½¿ç”¨é¢†åŸŸå†…çš„æ¸…æ´è¯­éŸ³æ•°æ®è¿›è¡Œå…ˆéªŒå®šä¹‰æ—¶ï¼Œæ€§èƒ½å¯èƒ½è¿‡äºä¹è§‚â€”â€”è¿™æ˜¯ä¹‹å‰éç›‘ç£è¯­éŸ³å¢å¼ºç ”ç©¶ä¸­é‡‡ç”¨çš„åšæ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ è¯­éŸ³å¢å¼ºæ–¹æ³•é€šå¸¸éœ€è¦é…å¯¹æ¸…æ´-å™ªå£°è¯­éŸ³æ•°æ®ï¼Œä½†ç°å®æ¡ä»¶ä¸‹æ”¶é›†å¤§è§„æ¨¡æ•°æ®ä¸å¯è¡Œã€‚</li>
<li>åˆæˆå™ªå£°è¯­éŸ³å¯¼è‡´è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µçš„å·®è·ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹åŒåˆ†æ”¯ç¼–ç å™¨-è§£ç å™¨æ¶æ„ç”¨äºæ— ç›‘ç£è¯­éŸ³å¢å¼ºã€‚</li>
<li>å¯¹æŠ—è®­ç»ƒç”¨äºå¯¹æ¯ä¸ªåˆ†æ”¯æ–½åŠ å…ˆéªŒçŸ¥è¯†ï¼Œè¿™äº›å…ˆéªŒçŸ¥è¯†ç”±éé…å¯¹æ•°æ®é›†å®šä¹‰ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•æ€§èƒ½ä¸é¢†å…ˆçš„éç›‘ç£è¯­éŸ³å¢å¼ºæ–¹æ³•ç›¸å½“ã€‚</li>
<li>æ¸…æ´è¯­éŸ³æ•°æ®é€‰æ‹©å¯¹å¢å¼ºæ€§èƒ½æœ‰å…³é”®å½±å“ã€‚</li>
<li>ä½¿ç”¨é¢†åŸŸå†…çš„æ¸…æ´è¯­éŸ³æ•°æ®è¿›è¡Œå…ˆéªŒå®šä¹‰å¯èƒ½å¯¼è‡´æ€§èƒ½è¯„ä¼°è¿‡äºä¹è§‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-efadc0bc2ae4e4ebad07f01625aa4bbe" align="middle">
<img src="https://picx.zhimg.com/v2-ec65fe0ccbf412ee823cc257c8f5f64e" align="middle">
<img src="https://picx.zhimg.com/v2-fa02aa4cee2522f95500945f50a0724e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b4987e2dbcb3ded02266f45174f5cb6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d7b0f4f9f66bd3b65ce674c5b172544.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67fa3edb7d8667c971cfac3e7e802c56" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ArFake-A-Multi-Dialect-Benchmark-and-Baselines-for-Arabic-Spoof-Speech-Detection"><a href="#ArFake-A-Multi-Dialect-Benchmark-and-Baselines-for-Arabic-Spoof-Speech-Detection" class="headerlink" title="ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech   Detection"></a>ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech   Detection</h2><p><strong>Authors:Mohamed Maged, Alhassan Ehab, Ali Mekky, Besher Hassan, Shady Shehata</strong></p>
<p>With the rise of generative text-to-speech models, distinguishing between real and synthetic speech has become challenging, especially for Arabic that have received limited research attention. Most spoof detection efforts have focused on English, leaving a significant gap for Arabic and its many dialects. In this work, we introduce the first multi-dialect Arabic spoofed speech dataset. To evaluate the difficulty of the synthesized audio from each model and determine which produces the most challenging samples, we aimed to guide the construction of our final dataset either by merging audios from multiple models or by selecting the best-performing model, we conducted an evaluation pipeline that included training classifiers using two approaches: modern embedding-based methods combined with classifier heads; classical machine learning algorithms applied to MFCC features; and the RawNet2 architecture. The pipeline further incorporated the calculation of Mean Opinion Score based on human ratings, as well as processing both original and synthesized datasets through an Automatic Speech Recognition model to measure the Word Error Rate. Our results demonstrate that FishSpeech outperforms other TTS models in Arabic voice cloning on the Casablanca corpus, producing more realistic and challenging synthetic speech samples. However, relying on a single TTS for dataset creation may limit generalizability. </p>
<blockquote>
<p>éšç€ç”Ÿæˆå¼æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹çš„å…´èµ·ï¼ŒåŒºåˆ†çœŸå®è¯­éŸ³å’Œåˆæˆè¯­éŸ³å·²ç»å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯å¯¹äºç ”ç©¶å…³æ³¨åº¦è¾ƒä½çš„é˜¿æ‹‰ä¼¯è¯­æ¥è¯´æ›´æ˜¯å¦‚æ­¤ã€‚å¤§å¤šæ•°çš„ä¼ªè£…æ£€æµ‹å·¥ä½œä¸»è¦é›†ä¸­åœ¨è‹±è¯­ä¸Šï¼Œè¿™ä¸ºé˜¿æ‹‰ä¼¯è¯­åŠå…¶ä¼—å¤šæ–¹è¨€ç•™ä¸‹äº†å·¨å¤§çš„ç©ºç™½ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç¬¬ä¸€ä¸ªå¤šæ–¹è¨€é˜¿æ‹‰ä¼¯è¯­ä¼ªè£…è¯­éŸ³æ•°æ®é›†ã€‚ä¸ºäº†è¯„ä¼°æ¯ä¸ªæ¨¡å‹çš„åˆæˆéŸ³é¢‘çš„å›°éš¾ç¨‹åº¦ï¼Œå¹¶ç¡®å®šå“ªä¸ªæ¨¡å‹äº§ç”Ÿäº†æœ€å…·æŒ‘æˆ˜æ€§çš„æ ·æœ¬ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡åˆå¹¶å¤šä¸ªæ¨¡å‹çš„éŸ³é¢‘æˆ–é€‰æ‹©è¡¨ç°æœ€ä½³çš„æ¨¡å‹æ¥æŒ‡å¯¼æˆ‘ä»¬æœ€ç»ˆæ•°æ®é›†çš„å»ºè®¾ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªè¯„ä¼°æµç¨‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¸¤ç§æ–¹æ³•è¿›è¡Œåˆ†ç±»å™¨è®­ç»ƒï¼šç°ä»£åŸºäºåµŒå…¥çš„æ–¹æ³•ä¸åˆ†ç±»å™¨å¤´ç›¸ç»“åˆï¼›åº”ç”¨äºMFCCç‰¹å¾çš„ç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•ï¼›ä»¥åŠRawNet2æ¶æ„ã€‚è¯¥æµç¨‹è¿˜çº³å…¥äº†åŸºäºäººç±»è¯„åˆ†çš„è®¡ç®—å¹³å‡æ„è§å¾—åˆ†ï¼Œä»¥åŠé€šè¿‡è¯­éŸ³è¯†åˆ«æ¨¡å‹å¤„ç†åŸå§‹å’Œåˆæˆæ•°æ®é›†ï¼Œä»¥æµ‹é‡å•è¯é”™è¯¯ç‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨Casablancaè¯­æ–™åº“ä¸Šï¼ŒFishSpeechåœ¨é˜¿æ‹‰ä¼¯è¯­è¯­éŸ³å…‹éš†æ–¹é¢ä¼˜äºå…¶ä»–TTSæ¨¡å‹ï¼Œèƒ½å¤Ÿäº§ç”Ÿæ›´çœŸå®ã€æ›´å…·æŒ‘æˆ˜æ€§çš„åˆæˆè¯­éŸ³æ ·æœ¬ã€‚ç„¶è€Œï¼Œä»…ä¾èµ–å•ä¸€çš„TTSè¿›è¡Œæ•°æ®é›†åˆ›å»ºå¯èƒ½ä¼šé™åˆ¶å…¶é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22808v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€ç”Ÿæˆå¼æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹çš„å…´èµ·ï¼ŒåŒºåˆ†çœŸå®å’Œåˆæˆè¯­éŸ³å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯é˜¿æ‹‰ä¼¯è¯­çš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚å¤§å¤šæ•°è¯­éŸ³æ¬ºéª—æ£€æµ‹å·¥ä½œéƒ½é›†ä¸­åœ¨è‹±è¯­ä¸Šï¼Œä¸ºé˜¿æ‹‰ä¼¯è¯­åŠå…¶å¤šç§æ–¹è¨€ç•™ä¸‹äº†æ˜¾è‘—å·®è·ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é¦–ä¸ªå¤šæ–¹è¨€é˜¿æ‹‰ä¼¯è¯­æ¬ºéª—è¯­éŸ³æ•°æ®é›†ã€‚ä¸ºäº†è¯„ä¼°å„åˆæˆéŸ³é¢‘æ¨¡å‹çš„éš¾åº¦å¹¶ç¡®å®šå“ªäº›æ¨¡å‹äº§ç”Ÿæœ€å…·æŒ‘æˆ˜æ€§çš„æ ·æœ¬ï¼Œæˆ‘ä»¬é€šè¿‡åˆå¹¶å¤šä¸ªæ¨¡å‹çš„éŸ³é¢‘æˆ–é€‰æ‹©è¡¨ç°æœ€ä½³çš„æ¨¡å‹æ¥æŒ‡å¯¼æœ€ç»ˆæ•°æ®é›†çš„å»ºè®¾ã€‚æˆ‘ä»¬é‡‡ç”¨äº†åŒ…æ‹¬ä½¿ç”¨ç°ä»£åµŒå…¥æ–¹æ³•å’Œåˆ†ç±»å™¨å¤´è®­ç»ƒåˆ†ç±»å™¨ã€åº”ç”¨MFCCç‰¹å¾çš„ç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•å’ŒRawNet2æ¶æ„çš„è¯„ä»·æµç¨‹ã€‚æµç¨‹è¿˜çº³å…¥äº†åŸºäºäººç±»è¯„åˆ†çš„å¹³å‡æ„è§å¾—åˆ†è®¡ç®—ï¼Œä»¥åŠé€šè¿‡è¯­éŸ³è¯†åˆ«æ¨¡å‹å¤„ç†åŸå§‹å’Œåˆæˆæ•°æ®é›†ä»¥æµ‹é‡è¯é”™è¯¯ç‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒFishSpeechåœ¨é˜¿æ‹‰ä¼¯è¯­éŸ³å…‹éš†çš„Casablancaè¯­æ–™åº“ä¸Šä¼˜äºå…¶ä»–TTSæ¨¡å‹ï¼Œèƒ½äº§ç”Ÿæ›´çœŸå®å’Œæ›´å…·æŒ‘æˆ˜æ€§çš„åˆæˆè¯­éŸ³æ ·æœ¬ã€‚ä½†ä»…ä¾èµ–å•ä¸€TTSè¿›è¡Œæ•°æ®é›†åˆ›å»ºå¯èƒ½ä¼šé™åˆ¶å…¶é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹çš„å…´èµ·ä½¿å¾—åŒºåˆ†çœŸå®å’Œåˆæˆè¯­éŸ³å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯åœ¨é˜¿æ‹‰ä¼¯è¯­ä¸­ã€‚</li>
<li>ç°æœ‰çš„è¯­éŸ³æ¬ºéª—æ£€æµ‹å·¥ä½œä¸»è¦é›†ä¸­åœ¨è‹±è¯­ï¼Œå¿½è§†äº†é˜¿æ‹‰ä¼¯è¯­åŠå…¶å¤šæ–¹è¨€çš„éœ€æ±‚ã€‚</li>
<li>ä»‹ç»äº†é¦–ä¸ªå¤šæ–¹è¨€é˜¿æ‹‰ä¼¯è¯­æ¬ºéª—è¯­éŸ³æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡åˆå¹¶å¤šä¸ªæ¨¡å‹çš„éŸ³é¢‘æˆ–é€‰æ‹©è¡¨ç°æœ€ä½³çš„æ¨¡å‹æ¥æ„å»ºæ•°æ®é›†ã€‚</li>
<li>é‡‡ç”¨äº†å¤šç§è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…æ‹¬åˆ†ç±»å™¨è®­ç»ƒã€å¹³å‡æ„è§å¾—åˆ†è®¡ç®—å’Œè¯é”™è¯¯ç‡æµ‹é‡ã€‚</li>
<li>FishSpeechæ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­éŸ³å…‹éš†ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22808">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3df5598ea1cf36a71d9bfbf7b601f53d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8264da7a103146e4b5d4200ac83fb8c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a988d711cc16576608c94b0e9499f30.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-24708b42dcf9cc0d105371f9e4c7e5ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3d25a26a37affde074686f7ec36f1c2" align="middle">
<img src="https://pic1.zhimg.com/v2-e3899bb8de406c113815b1259a2213f3.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Index-MSR-A-high-efficiency-multimodal-fusion-framework-for-speech-recognition"><a href="#Index-MSR-A-high-efficiency-multimodal-fusion-framework-for-speech-recognition" class="headerlink" title="Index-MSR: A high-efficiency multimodal fusion framework for speech   recognition"></a>Index-MSR: A high-efficiency multimodal fusion framework for speech   recognition</h2><p><strong>Authors:Jinming Chen, Lu Wang, Zheshu Song, Wei Deng</strong></p>
<p>Driven by large scale datasets and LLM based architectures, automatic speech recognition (ASR) systems have achieved remarkable improvements in accuracy. However, challenges persist for domain-specific terminology, and short utterances lacking semantic coherence, where recognition performance often degrades significantly. In this work, we present Index-MSR, an efficient multimodal speech recognition framework. At its core is a novel Multimodal Fusion Decoder (MFD), which effectively incorporates text-related information from videos (e.g., subtitles and presentation slides) into the speech recognition. This cross-modal integration not only enhances overall ASR accuracy but also yields substantial reductions in substitution errors. Extensive evaluations on both an in-house subtitle dataset and a public AVSR dataset demonstrate that Index-MSR achieves sota accuracy, with substitution errors reduced by 20,50%. These results demonstrate that our approach efficiently exploits text-related cues from video to improve speech recognition accuracy, showing strong potential in applications requiring strict audio text synchronization, such as audio translation. </p>
<blockquote>
<p>é©±åŠ¨çš„å¤§è§„æ¨¡æ•°æ®é›†å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¶æ„ä½¿å¾—è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨ç²¾åº¦ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œå¯¹äºç‰¹å®šé¢†åŸŸçš„æœ¯è¯­ä»¥åŠç¼ºä¹è¯­ä¹‰è¿è´¯æ€§çš„ç®€çŸ­è¯è¯­ï¼ŒæŒ‘æˆ˜ä»ç„¶å­˜åœ¨ï¼Œè¯†åˆ«æ€§èƒ½å¾€å¾€ä¼šæ˜¾è‘—ä¸‹é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Index-MSRï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„è·¨æ¨¡æ€è¯­éŸ³è¯†åˆ«æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€èåˆè§£ç å™¨ï¼ˆMFDï¼‰ï¼Œå®ƒæœ‰æ•ˆåœ°å°†è§†é¢‘ä¸­çš„æ–‡æœ¬ç›¸å…³ä¿¡æ¯ï¼ˆä¾‹å¦‚å­—å¹•å’Œæ¼”ç¤ºå¹»ç¯ç‰‡ï¼‰èå…¥è¯­éŸ³è¯†åˆ«ã€‚è¿™ç§è·¨æ¨¡æ€èåˆä¸ä»…æé«˜äº†æ•´ä½“çš„ASRç²¾åº¦ï¼Œè¿˜å¤§å¤§å‡å°‘äº†æ›¿æ¢é”™è¯¯ã€‚å¯¹å†…éƒ¨å­—å¹•æ•°æ®é›†å’Œå…¬å…±AVSRæ•°æ®é›†çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒIndex-MSRè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œæ›¿æ¢é”™è¯¯å‡å°‘äº†20.5%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨è§†é¢‘ä¸­çš„æ–‡æœ¬ç›¸å…³çº¿ç´¢æ¥æé«˜è¯­éŸ³è¯†åˆ«çš„å‡†ç¡®æ€§ï¼Œåœ¨éœ€è¦ä¸¥æ ¼éŸ³è§†é¢‘åŒæ­¥çš„åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œå¦‚éŸ³é¢‘ç¿»è¯‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22744v1">PDF</a> Submit to icassp 2026</p>
<p><strong>Summary</strong><br>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå—ç›Šäºå¤§è§„æ¨¡æ•°æ®é›†å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¶æ„ï¼Œåœ¨å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚ç„¶è€Œï¼Œå¯¹äºç‰¹å®šé¢†åŸŸçš„æœ¯è¯­å’Œç¼ºä¹è¯­ä¹‰è¿è´¯æ€§çš„ç®€çŸ­è¯è¯­ï¼Œä»å­˜åœ¨æŒ‘æˆ˜ï¼Œè¯†åˆ«æ€§èƒ½å¾€å¾€ä¼šæ˜¾è‘—ä¸‹é™ã€‚æœ¬ç ”ç©¶æå‡ºäº†Index-MSRï¼Œä¸€ä¸ªé«˜æ•ˆçš„å¤šæ¨¡æ€è¯­éŸ³è¯†åˆ«æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€èåˆè§£ç å™¨ï¼ˆMFDï¼‰ï¼Œå®ƒæœ‰æ•ˆåœ°å°†è§†é¢‘ä¸­çš„æ–‡æœ¬ç›¸å…³ä¿¡æ¯ï¼ˆå¦‚å­—å¹•å’Œæ¼”ç¤ºå¹»ç¯ç‰‡ï¼‰èå…¥è¯­éŸ³è¯†åˆ«ã€‚è¿™ç§è·¨æ¨¡æ€èåˆä¸ä»…æé«˜äº†ASRçš„æ•´ä½“å‡†ç¡®æ€§ï¼Œè¿˜å¤§å¤§é™ä½äº†æ›¿æ¢é”™è¯¯ã€‚åœ¨å†…éƒ¨å­—å¹•æ•°æ®é›†å’Œå…¬å…±AVSRæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒIndex-MSRè¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³çš„å‡†ç¡®æ€§ï¼Œæ›¿æ¢é”™è¯¯é™ä½äº†20.5%ã€‚è¿™äº›ç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨è§†é¢‘ä¸­çš„æ–‡æœ¬çº¿ç´¢æ¥æé«˜è¯­éŸ³è¯†åˆ«å‡†ç¡®æ€§ï¼Œåœ¨éœ€è¦ä¸¥æ ¼éŸ³è§†é¢‘åŒæ­¥çš„åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œå¦‚éŸ³é¢‘ç¿»è¯‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡æ•°æ®é›†å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¶æ„æå‡äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚</li>
<li>å¯¹äºç‰¹å®šé¢†åŸŸçš„æœ¯è¯­å’Œç¼ºä¹è¯­ä¹‰è¿è´¯æ€§çš„ç®€çŸ­è¯è¯­ï¼ŒASRç³»ç»Ÿä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>Index-MSRæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€è¯­éŸ³è¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡èåˆè§†é¢‘ä¸­çš„æ–‡æœ¬ä¿¡æ¯æ¥æé«˜ASRçš„å‡†ç¡®æ€§ã€‚</li>
<li>å¤šæ¨¡æ€èåˆè§£ç å™¨ï¼ˆMFDï¼‰æ˜¯Index-MSRçš„æ ¸å¿ƒï¼Œå®ƒæœ‰æ•ˆåœ°ç»“åˆæ–‡æœ¬å’Œè¯­éŸ³ä¿¡æ¯ã€‚</li>
<li>è·¨æ¨¡æ€èåˆé™ä½äº†æ›¿æ¢é”™è¯¯ï¼Œæé«˜äº†ASRçš„æ•´ä½“æ€§èƒ½ã€‚</li>
<li>åœ¨å†…éƒ¨å’Œå…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒIndex-MSRè¾¾åˆ°äº†å…ˆè¿›çš„å‡†ç¡®æ€§æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22744">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-195c6ef443a8c9cb24dae1f8c797dc1b" align="middle">
<img src="https://picx.zhimg.com/v2-f502c1549f2cdcbc9cd080c56218fcab" align="middle">
<img src="https://picx.zhimg.com/v2-aa2d29ec02bb43b7e54df17570d385d4" align="middle">
<img src="https://picx.zhimg.com/v2-4ae52dedc1a8ed0ca75848cf68a4cf95.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f2ce24a54ac9d5d07c303d24014773b4" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  PHASE-Net Physics-Grounded Harmonic Attention System for Efficient   Remote Photoplethysmography Measurement
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-efb5bf9e1a25adeb0b4768bf6937bbf3" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  UESA-Net U-Shaped Embedded Multidirectional Shrinkage Attention Network   for Ultrasound Nodule Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
