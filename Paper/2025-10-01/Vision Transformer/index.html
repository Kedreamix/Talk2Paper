<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  VT-FSL Bridging Vision and Text with LLMs for Few-Shot Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-41df3e1cdbc12a89cef3f735b859d656')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    59 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-01-æ›´æ–°"><a href="#2025-10-01-æ›´æ–°" class="headerlink" title="2025-10-01 æ›´æ–°"></a>2025-10-01 æ›´æ–°</h1><h2 id="VT-FSL-Bridging-Vision-and-Text-with-LLMs-for-Few-Shot-Learning"><a href="#VT-FSL-Bridging-Vision-and-Text-with-LLMs-for-Few-Shot-Learning" class="headerlink" title="VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning"></a>VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning</h2><p><strong>Authors:Wenhao Li, Qiangchang Wang, Xianjing Meng, Zhibin Wu, Yilong Yin</strong></p>
<p>Few-shot learning (FSL) aims to recognize novel concepts from only a few labeled support samples. Recent studies enhance support features by incorporating additional semantic information or designing complex semantic fusion modules. However, they still suffer from hallucinating semantics that contradict the visual evidence due to the lack of grounding in actual instances, resulting in noisy guidance and costly corrections. To address these issues, we propose a novel framework, bridging Vision and Text with LLMs for Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts conditioned on Large Language Models (LLMs) and support images, seamlessly integrating them through a geometry-aware alignment. It mainly consists of Cross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment (CGA). Specifically, the CIP conditions an LLM on both class names and support images to generate precise class descriptions iteratively in a single structured reasoning pass. These descriptions not only enrich the semantic understanding of novel classes but also enable the zero-shot synthesis of semantically consistent images. The descriptions and synthetic images act respectively as complementary textual and visual prompts, providing high-level class semantics and low-level intra-class diversity to compensate for limited support data. Furthermore, the CGA jointly aligns the fused textual, support, and synthetic visual representations by minimizing the kernelized volume of the 3-dimensional parallelotope they span. It captures global and nonlinear relationships among all representations, enabling structured and consistent multimodal integration. The proposed VT-FSL method establishes new state-of-the-art performance across ten diverse benchmarks, including standard, cross-domain, and fine-grained few-shot learning scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL">https://github.com/peacelwh/VT-FSL</a>. </p>
<blockquote>
<p>å°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰æ—¨åœ¨ä»ä»…æœ‰çš„å°‘é‡æ ‡è®°æ ·æœ¬ä¸­è¯†åˆ«å‡ºæ–°æ¦‚å¿µã€‚æœ€è¿‘çš„ç ”ç©¶é€šè¿‡èå…¥é¢å¤–çš„è¯­ä¹‰ä¿¡æ¯æˆ–è®¾è®¡å¤æ‚çš„è¯­ä¹‰èåˆæ¨¡å—æ¥æå‡æ”¯æ’‘ç‰¹å¾ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹åœ¨å®é™…å®ä¾‹ä¸­çš„å®šä½ï¼Œå®ƒä»¬ä»ç„¶ä¼šå‡ºç°ä¸è§†è§‰è¯æ®ç›¸çŸ›ç›¾çš„å¹»è§‰è¯­ä¹‰ï¼Œå¯¼è‡´äº§ç”Ÿå˜ˆæ‚çš„æŒ‡å¯¼å’Œæ˜‚è´µçš„ä¿®æ­£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§†è§‰ä¸æ–‡æœ¬æ¡¥æ¥å°‘é‡å­¦ä¹ ï¼ˆVT-FSLï¼‰ï¼Œå®ƒæ„å»ºç²¾ç¡®çš„è·¨æ¨¡æ€æç¤ºï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ”¯æ’‘å›¾åƒï¼Œé€šè¿‡å‡ ä½•æ„ŸçŸ¥å¯¹é½æ— ç¼é›†æˆå®ƒä»¬ã€‚å®ƒä¸»è¦ç”±è·¨æ¨¡æ€è¿­ä»£æç¤ºï¼ˆCIPï¼‰å’Œè·¨æ¨¡æ€å‡ ä½•å¯¹é½ï¼ˆCGAï¼‰ç»„æˆã€‚å…·ä½“æ¥è¯´ï¼ŒCIPä»¥ç±»åå’Œæ”¯æ’‘å›¾åƒä¸ºæ¡ä»¶æ¥å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨ä¸€æ¬¡å•ä¸€çš„ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ä¸­è¿­ä»£ç”Ÿæˆç²¾ç¡®çš„ç±»æè¿°ã€‚è¿™äº›æè¿°ä¸ä»…ä¸°å¯Œäº†å¯¹æ–°é¢–ç±»çš„è¯­ä¹‰ç†è§£ï¼Œè¿˜å®ç°äº†è¯­ä¹‰ä¸€è‡´å›¾åƒçš„é›¶æ ·æœ¬åˆæˆã€‚è¿™äº›æè¿°å’Œåˆæˆå›¾åƒåˆ†åˆ«ä½œä¸ºè¡¥å……çš„æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œæä¾›é«˜çº§åˆ«çš„ç±»è¯­ä¹‰å’Œä½çº§åˆ«çš„ç±»å†…å¤šæ ·æ€§ï¼Œä»¥å¼¥è¡¥æœ‰é™çš„æ”¯æ’‘æ•°æ®ã€‚æ­¤å¤–ï¼ŒCGAé€šè¿‡æœ€å°åŒ–å®ƒä»¬æ‰€è·¨è¶Šçš„3ç»´å¹³è¡Œå››è¾¹å½¢çš„å†…æ ¸ä½“ç§¯æ¥è”åˆå¯¹é½èåˆçš„æ–‡æœ¬ã€æ”¯æ’‘å’Œåˆæˆè§†è§‰è¡¨ç¤ºã€‚å®ƒæ•æ‰äº†æ‰€æœ‰è¡¨ç¤ºä¹‹é—´çš„å…¨å±€å’Œéçº¿æ€§å…³ç³»ï¼Œå®ç°äº†ç»“æ„åŒ–ä¸”ä¸€è‡´çš„å¤šæ¨¡æ€é›†æˆã€‚æ‰€æå‡ºçš„VT-FSLæ–¹æ³•åœ¨åŒ…æ‹¬æ ‡å‡†ã€è·¨åŸŸå’Œç»†ç²’åº¦å°‘é‡å­¦ä¹ åœºæ™¯åœ¨å†…çš„åä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/peacelwh/VT-FSLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25033v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹ä»…ç”±å°‘é‡æ ‡è®°æ ·æœ¬æ”¯æ’‘è¯†åˆ«æ–°æ¦‚å¿µçš„few-shotå­¦ä¹ ï¼ˆFSLï¼‰ä»»åŠ¡ï¼Œæå‡ºä¸€ç§èåˆè§†è§‰ä¸æ–‡æœ¬çš„å¤§æ¨¡å‹è¾…åŠ©çš„å°‘é‡æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼ˆVT-FSLï¼‰ã€‚é€šè¿‡è·¨æ¨¡æ€è¿­ä»£æç¤ºå’Œå‡ ä½•å¯¹é½æœºåˆ¶ï¼Œæ„å»ºç²¾ç¡®çš„è·¨æ¨¡æ€æç¤ºï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ”¯æ’‘å›¾åƒå®ç°æ— ç¼é›†æˆã€‚æ¡†æ¶å®ç°äº†å•ä¸€ç»“æ„æ¨ç†è¿‡ç¨‹ä¸­å¯¹ç±»åˆ«åç§°å’Œæ”¯æ’‘å›¾åƒçš„è¿­ä»£ç²¾ç¡®ç±»æè¿°ç”Ÿæˆã€‚è¿™äº›æè¿°æ—¢ä¸°å¯Œäº†æ–°ç±»åˆ«çš„è¯­ä¹‰ç†è§£ï¼Œä¹Ÿå®ç°äº†è¯­ä¹‰ä¸€è‡´çš„å›¾åƒé›¶æ ·æœ¬åˆæˆã€‚æè¿°å’Œåˆæˆå›¾åƒåˆ†åˆ«ä½œä¸ºæ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œæä¾›é«˜çº§ç±»åˆ«è¯­ä¹‰å’Œä½çº§ç±»åˆ«å†…å¤šæ ·æ€§ï¼Œä»¥å¼¥è¡¥æœ‰é™çš„æ”¯æ’‘æ•°æ®ã€‚åŒæ—¶ï¼Œé€šè¿‡æœ€å°åŒ–å¼ é‡ç©ºé—´ä¸­å¤šç»´åº¦å¯¹é½æ–‡æœ¬ã€æ”¯æ’‘å›¾åƒåŠåˆæˆå›¾åƒçš„æ–¹å¼æ•æ‰ä¸åŒå…ƒç´ ä¹‹é—´çš„å…¨å±€éçº¿æ€§å…³ç³»ã€‚è¯¥æ–¹æ³•åœ¨æ¶µç›–æ ‡å‡†ã€è·¨åŸŸåŠç²¾ç»†ç²’åº¦çš„å°‘é‡æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­éƒ½å®ç°äº†å‰æ‰€æœªæœ‰çš„ä¼˜å¼‚æ€§èƒ½ã€‚å…·ä½“ä¿¡æ¯è§<a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL%E3%80%82">https://github.com/peacelwh/VT-FSLã€‚</a></p>
<p><strong>å…³é”®å‘ç°</strong></p>
<ol>
<li>æå‡ºçš„VT-FSLæ¡†æ¶ç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ”¯æ’‘å›¾åƒè¿›è¡Œfew-shotå­¦ä¹ ã€‚</li>
<li>é€šè¿‡è·¨æ¨¡æ€è¿­ä»£æç¤ºï¼ˆCIPï¼‰ç”Ÿæˆç²¾ç¡®ç±»æè¿°ï¼Œæ—¢ä¸°å¯Œæ–°ç±»åˆ«çš„è¯­ä¹‰ç†è§£ï¼Œä¹Ÿå®ç°è¯­ä¹‰ä¸€è‡´çš„å›¾åƒé›¶æ ·æœ¬åˆæˆã€‚</li>
<li>è·¨æ¨¡æ€å‡ ä½•å¯¹é½ï¼ˆCGAï¼‰æœºåˆ¶èƒ½å¤Ÿæ•æ‰æ–‡æœ¬ã€æ”¯æ’‘å›¾åƒåŠåˆæˆå›¾åƒä¹‹é—´çš„å…¨å±€éçº¿æ€§å…³ç³»ã€‚</li>
<li>CIPå’ŒCGAå…±åŒæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œä½¿å¾—VT-FSLåœ¨å¤šç§few-shotå­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°é¢†å…ˆã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿæ— ç¼é›†æˆä¸åŒçš„ä¿¡æ¯æºï¼Œå¦‚ç±»åã€æ”¯æ’‘å›¾åƒä»¥åŠåˆæˆå›¾åƒç­‰ï¼Œæå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>VT-FSLæ¡†æ¶åœ¨å¤šç§åœºæ™¯ä¸‹éƒ½è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æ ‡å‡†ã€è·¨åŸŸä»¥åŠç²¾ç»†ç²’åº¦çš„few-shotå­¦ä¹ åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c6781d5aac0565d4a5da14d7fc5718e" align="middle">
<img src="https://picx.zhimg.com/v2-9af705fa082c84d4e881514f8fff464d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fda6a8e6593952cbc22784b0fba62ed6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76bea1641940b6f14a37bc0429de3487" align="middle">
<img src="https://pica.zhimg.com/v2-1a09d3087398134cf568e19fcf367318.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b1ec418aca878ee6ac090b3416189aa" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Instruction-Guided-Multi-Object-Image-Editing-with-Quantity-and-Layout-Consistency"><a href="#Instruction-Guided-Multi-Object-Image-Editing-with-Quantity-and-Layout-Consistency" class="headerlink" title="Instruction Guided Multi Object Image Editing with Quantity and Layout   Consistency"></a>Instruction Guided Multi Object Image Editing with Quantity and Layout   Consistency</h2><p><strong>Authors:Jiaqi Tan, Fangyu Li, Yang Liu</strong></p>
<p>Instruction driven image editing with standard CLIP text encoders often fails in complex scenes with many objects. We present QL-Adapter, a framework for multiple object editing that tackles two challenges: enforcing object counts and spatial layouts, and accommodating diverse categories. QL-Adapter consists of two core modules: the Image-Layout Fusion Module (ILFM) and the Cross-Modal Augmentation Module (CMAM). ILFM fuses layout priors with ViT patch tokens from the CLIP image encoder to strengthen spatial structure understanding. CMAM injects image features into the text branch to enrich textual embeddings and improve instruction following. We further build QL-Dataset, a benchmark that spans broad category, layout, and count variations, and define the task of quantity and layout consistent image editing (QL-Edit). Extensive experiments show that QL-Adapter achieves state of the art performance on QL-Edit and significantly outperforms existing models. </p>
<blockquote>
<p>åœ¨å…·æœ‰å¤šä¸ªå¯¹è±¡çš„å¤æ‚åœºæ™¯ä¸­ï¼Œä½¿ç”¨æ ‡å‡†CLIPæ–‡æœ¬ç¼–ç å™¨çš„æŒ‡ä»¤é©±åŠ¨å›¾åƒç¼–è¾‘å¾€å¾€ä¼šå¤±è´¥ã€‚æˆ‘ä»¬æå‡ºäº†QL-Adapterï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤šä¸ªå¯¹è±¡ç¼–è¾‘çš„æ¡†æ¶ï¼Œè§£å†³äº†ä¸¤ä¸ªæŒ‘æˆ˜ï¼šå¼ºåˆ¶å¯¹è±¡è®¡æ•°å’Œç©ºé—´å¸ƒå±€ï¼Œå¹¶é€‚åº”å„ç§ç±»åˆ«ã€‚QL-AdapteråŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šå›¾åƒå¸ƒå±€èåˆæ¨¡å—ï¼ˆILFMï¼‰å’Œè·¨æ¨¡æ€å¢å¼ºæ¨¡å—ï¼ˆCMAMï¼‰ã€‚ILFMå°†å¸ƒå±€å…ˆéªŒä¸CLIPå›¾åƒç¼–ç å™¨çš„ViTè¡¥ä¸æ ‡è®°èåˆï¼Œä»¥åŠ å¼ºç©ºé—´ç»“æ„ç†è§£ã€‚CMAMå°†å›¾åƒç‰¹å¾æ³¨å…¥æ–‡æœ¬åˆ†æ”¯ï¼Œä»¥ä¸°å¯Œæ–‡æœ¬åµŒå…¥å¹¶æ”¹å–„æŒ‡ä»¤éµå¾ªã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ„å»ºäº†QL-Datasetï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–å¹¿æ³›ç±»åˆ«ã€å¸ƒå±€å’Œè®¡æ•°å˜åŒ–çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶å®šä¹‰äº†æ•°é‡å¸ƒå±€ä¸€è‡´å›¾åƒç¼–è¾‘çš„ä»»åŠ¡ï¼ˆQL-Editï¼‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒQL-Adapteråœ¨QL-Editä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24514v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd476b7346124477f5ca99e77e2e0939.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15c0e154957436acb6d677203af3f348.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d09d5233721e718ab17ecc6e08f0393" align="middle">
<img src="https://pica.zhimg.com/v2-89ed5e61f4bad40a2cc6865337dac335.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HieraTok-Multi-Scale-Visual-Tokenizer-Improves-Image-Reconstruction-and-Generation"><a href="#HieraTok-Multi-Scale-Visual-Tokenizer-Improves-Image-Reconstruction-and-Generation" class="headerlink" title="HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and   Generation"></a>HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and   Generation</h2><p><strong>Authors:Cong Chen, Ziyuan Huang, Cheng Zou, Muzhi Zhu, Kaixiang Ji, Jiajia Liu, Jingdong Chen, Hao Chen, Chunhua Shen</strong></p>
<p>In this work, we present HieraTok, a novel multi-scale Vision Transformer (ViT)-based tokenizer that overcomes the inherent limitation of modeling single-scale representations. This is realized through two key designs: (1) multi-scale downsampling applied to the token map generated by the tokenizer encoder, producing a sequence of multi-scale tokens, and (2) a scale-causal attention mechanism that enables the progressive flow of information from low-resolution global semantic features to high-resolution structural details. Coupling these designs, HieraTok achieves significant improvements in both image reconstruction and generation tasks. Under identical settings, the multi-scale visual tokenizer outperforms its single-scale counterpart by a 27.2% improvement in rFID ($1.47 \rightarrow 1.07$). When integrated into downstream generation frameworks, it achieves a $1.38\times$ faster convergence rate and an 18.9% boost in gFID ($16.4 \rightarrow 13.3$), which may be attributed to the smoother and more uniformly distributed latent space. Furthermore, by scaling up the tokenizerâ€™s training, we demonstrate its potential by a sota rFID of 0.45 and a gFID of 1.82 among ViT tokenizers. To the best of our knowledge, we are the first to introduce multi-scale ViT-based tokenizer in image reconstruction and image generation. We hope our findings and designs advance the ViT-based tokenizers in visual generation tasks. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†HieraTokï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šå°ºåº¦Vision Transformerï¼ˆViTï¼‰åŸºäºçš„ä»¤ç‰ŒåŒ–å™¨ï¼Œå…‹æœäº†å»ºæ¨¡å•å°ºåº¦è¡¨ç¤ºçš„å›ºæœ‰å±€é™æ€§ã€‚è¿™æ˜¯é€šè¿‡ä¸¤ä¸ªå…³é”®è®¾è®¡å®ç°çš„ï¼šï¼ˆ1ï¼‰å¯¹ä»¤ç‰Œæ˜ å°„è¿›è¡Œå¤šå°ºåº¦é™é‡‡æ ·ï¼Œç”Ÿæˆä¸€ç³»åˆ—å¤šå°ºåº¦ä»¤ç‰Œï¼›ï¼ˆ2ï¼‰ä¸€ç§å°ºåº¦å› æœæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿ä¿¡æ¯ä»ä½åˆ†è¾¨ç‡å…¨å±€è¯­ä¹‰ç‰¹å¾æµå‘é«˜åˆ†è¾¨ç‡ç»“æ„ç»†èŠ‚ã€‚é€šè¿‡ç»“åˆè¿™äº›è®¾è®¡ï¼ŒHieraTokåœ¨å›¾åƒé‡å»ºå’Œç”Ÿæˆä»»åŠ¡ä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚åœ¨ç›¸åŒè®¾ç½®ä¸‹ï¼Œå¤šå°ºåº¦è§†è§‰ä»¤ç‰ŒåŒ–å™¨åœ¨rFIDæŒ‡æ ‡ä¸Šè¶…è¿‡äº†å…¶å•å°ºåº¦å¯¹åº”ç‰©ï¼ˆä»1.47æé«˜åˆ°1.07ï¼Œæ”¹è¿›äº†27.2ï¼…ï¼‰ã€‚å½“é›†æˆåˆ°ä¸‹æ¸¸ç”Ÿæˆæ¡†æ¶ä¸­æ—¶ï¼Œå®ƒå®ç°äº†1.38å€çš„æ›´å¿«æ”¶æ•›ç‡ï¼Œå¹¶åœ¨gFIDä¸Šæé«˜äº†18.9ï¼…ï¼ˆä»16.4é™è‡³13.3ï¼‰ã€‚è¿™å¯èƒ½æ˜¯å½’å› äºæ›´å¹³æ»‘ä¸”åˆ†å¸ƒæ›´å‡åŒ€çš„æ½œåœ¨ç©ºé—´ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ‰©å¤§ä»¤ç‰ŒåŒ–å™¨çš„è®­ç»ƒè§„æ¨¡ï¼Œæˆ‘ä»¬åœ¨ViTä»¤ç‰ŒåŒ–å™¨ä¸­å®ç°äº†æœ€å…ˆè¿›çš„rFIDä¸º0.45å’ŒgFIDä¸º1.82ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬é¦–æ¬¡åœ¨å›¾åƒé‡å»ºå’Œå›¾åƒç”Ÿæˆä¸­å¼•å…¥äº†å¤šå°ºåº¦ViTåŸºäºçš„ä»¤ç‰ŒåŒ–å™¨ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å‘ç°å’Œè®¾è®¡èƒ½æ¨åŠ¨ViTåœ¨è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­çš„ä»¤ç‰ŒåŒ–å™¨çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23736v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºViTçš„HieraTokå¤šå°ºåº¦è§†è§‰ä»¤ç‰Œå™¨é¦–æ¬¡äº®ç›¸ã€‚é€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒè®¾è®¡ï¼šå¯¹ä»¤ç‰Œå›¾çš„å¤šå°ºåº¦é™é‡‡æ ·ä»¥ç”Ÿæˆå¤šå°ºåº¦ä»¤ç‰Œåºåˆ—å’Œè§„æ¨¡å› æœæ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°ä¿¡æ¯ä»ä½åˆ†è¾¨ç‡å…¨å±€è¯­ä¹‰ç‰¹å¾åˆ°é«˜åˆ†è¾¨ç‡ç»“æ„ç»†èŠ‚çš„æ¸è¿›æµåŠ¨ã€‚è¿™æ”¹è¿›äº†å›¾åƒé‡å»ºå’Œç”Ÿæˆä»»åŠ¡ï¼Œæ”¹å–„äº†rFIDæŒ‡æ ‡å’Œç”Ÿæˆæ¡†æ¶çš„æ”¶æ•›é€Ÿåº¦ã€‚ç»è¿‡è®­ç»ƒåï¼Œå…¶å±•ç°å‡ºæ½œåœ¨çš„é«˜æ€§èƒ½ã€‚è¿™é¡¹åˆ›æ–°æœ‰æœ›æå‡ViTåœ¨è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­çš„ä»¤ç‰ŒåŒ–æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>HieraTokæ˜¯ä¸€ä¸ªåŸºäºVision Transformerï¼ˆViTï¼‰çš„å¤šå°ºåº¦è§†è§‰ä»¤ç‰Œå™¨ã€‚</li>
<li>é€šè¿‡å¤šå°ºåº¦é™é‡‡æ ·å’Œè§„æ¨¡å› æœæ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†ä¿¡æ¯ä»å…¨å±€è¯­ä¹‰ç‰¹å¾åˆ°ç»“æ„ç»†èŠ‚çš„æ¸è¿›æµåŠ¨ã€‚</li>
<li>HieraTokåœ¨å›¾åƒé‡å»ºå’Œç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œæ”¹å–„äº†rFIDæŒ‡æ ‡ï¼Œæé«˜äº†ç”Ÿæˆæ¡†æ¶çš„æ”¶æ•›é€Ÿåº¦å’ŒgFIDæŒ‡æ ‡ã€‚</li>
<li>é€šè¿‡è®­ç»ƒæå‡ï¼ŒHieraTokå±•ç°å‡ºé«˜æ€§èƒ½æ½œåŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2af0182de1593727bf806b076ba4521f" align="middle">
<img src="https://picx.zhimg.com/v2-3ed8b28a455a50c49940842dd43c909e" align="middle">
<img src="https://picx.zhimg.com/v2-c7b2a4f2a2d54b391d7f47ed9ab9de13" align="middle">
<img src="https://picx.zhimg.com/v2-c766f9ad67406e7f73b3580072462bbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7e4acfe7a983c86785de84816641574" align="middle">
<img src="https://picx.zhimg.com/v2-9b48ac7f2f1bbf96254e02c1b21bf49c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Patch-Rebirth-Toward-Fast-and-Transferable-Model-Inversion-of-Vision-Transformers"><a href="#Patch-Rebirth-Toward-Fast-and-Transferable-Model-Inversion-of-Vision-Transformers" class="headerlink" title="Patch Rebirth: Toward Fast and Transferable Model Inversion of Vision   Transformers"></a>Patch Rebirth: Toward Fast and Transferable Model Inversion of Vision   Transformers</h2><p><strong>Authors:Seongsoo Heo, Dong-Wan Choi</strong></p>
<p>Model inversion is a widely adopted technique in data-free learning that reconstructs synthetic inputs from a pretrained model through iterative optimization, without access to original training data. Unfortunately, its application to state-of-the-art Vision Transformers (ViTs) poses a major computational challenge, due to their expensive self-attention mechanisms. To address this, Sparse Model Inversion (SMI) was proposed to improve efficiency by pruning and discarding seemingly unimportant patches, which were even claimed to be obstacles to knowledge transfer. However, our empirical findings suggest the opposite: even randomly selected patches can eventually acquire transferable knowledge through continued inversion. This reveals that discarding any prematurely inverted patches is inefficient, as it suppresses the extraction of class-agnostic features essential for knowledge transfer, along with class-specific features. In this paper, we propose Patch Rebirth Inversion (PRI), a novel approach that incrementally detaches the most important patches during the inversion process to construct sparse synthetic images, while allowing the remaining patches to continue evolving for future selection. This progressive strategy not only improves efficiency, but also encourages initially less informative patches to gradually accumulate more class-relevant knowledge, a phenomenon we refer to as the Re-Birth effect, thereby effectively balancing class-agnostic and class-specific knowledge. Experimental results show that PRI achieves up to 10x faster inversion than standard Dense Model Inversion (DMI) and 2x faster than SMI, while consistently outperforming SMI in accuracy and matching the performance of DMI. </p>
<blockquote>
<p>æ¨¡å‹åè½¬æ˜¯ä¸€ç§æ— æ•°æ®å­¦ä¹ çš„å¹¿æ³›é‡‡ç”¨çš„æŠ€æœ¯ï¼Œå®ƒé€šè¿‡è¿­ä»£ä¼˜åŒ–ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­é‡å»ºåˆæˆè¾“å…¥ï¼Œæ— éœ€è®¿é—®åŸå§‹è®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œå°†å…¶åº”ç”¨äºæœ€å…ˆè¿›çš„è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰å¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶éå¸¸æ˜‚è´µã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ç¨€ç–æ¨¡å‹åè½¬ï¼ˆSMIï¼‰æ¥æé«˜æ•ˆç‡ï¼Œé€šè¿‡ä¿®å‰ªå’Œä¸¢å¼ƒä¼¼ä¹ä¸é‡è¦çš„è¡¥ä¸ï¼Œè¿™äº›è¡¥ä¸ç”šè‡³è¢«å£°ç§°æ˜¯çŸ¥è¯†è½¬ç§»çš„éšœç¢ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„å®è¯å‘ç°æ°æ°ç›¸åï¼šå³ä½¿éšæœºé€‰æ‹©çš„è¡¥ä¸ä¹Ÿå¯ä»¥é€šè¿‡æŒç»­çš„åè½¬æœ€ç»ˆè·å¾—å¯è½¬ç§»çš„çŸ¥è¯†ã€‚è¿™è¡¨æ˜ï¼Œæå‰ä¸¢å¼ƒä»»ä½•åè½¬çš„è¡¥ä¸éƒ½æ˜¯ä½æ•ˆçš„ï¼Œå› ä¸ºå®ƒä¼šæŠ‘åˆ¶ç±»æ— å…³ç‰¹å¾å’Œç±»ç‰¹å®šç‰¹å¾çš„æå–ï¼Œè¿™å¯¹äºçŸ¥è¯†è½¬ç§»è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Patché‡ç”Ÿåè½¬ï¼ˆPRIï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒåœ¨åè½¬è¿‡ç¨‹ä¸­é€æ­¥å‰¥ç¦»æœ€é‡è¦çš„è¡¥ä¸æ¥æ„å»ºç¨€ç–åˆæˆå›¾åƒï¼ŒåŒæ—¶å…è®¸å‰©ä½™è¡¥ä¸ç»§ç»­æ¼”åŒ–ä»¥ä¾›æœªæ¥é€‰æ‹©ã€‚è¿™ç§æ¸è¿›çš„ç­–ç•¥ä¸ä»…æé«˜äº†æ•ˆç‡ï¼Œè¿˜é¼“åŠ±æœ€åˆä¿¡æ¯è¾ƒå°‘çš„è¡¥ä¸é€æ¸ç§¯ç´¯æ›´å¤šä¸ç±»åˆ«ç›¸å…³çš„çŸ¥è¯†ï¼Œæˆ‘ä»¬å°†è¿™ä¸€ç°è±¡ç§°ä¸ºâ€œé‡ç”Ÿæ•ˆåº”â€ï¼Œä»è€Œæœ‰æ•ˆåœ°å¹³è¡¡äº†ç±»æ— å…³å’Œç±»ç‰¹å®šçš„çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRIçš„åè½¬é€Ÿåº¦æ¯”æ ‡å‡†çš„Dense Model Inversionï¼ˆDMIï¼‰å¿«10å€ï¼Œæ¯”SMIå¿«2å€ï¼ŒåŒæ—¶åœ¨å‡†ç¡®æ€§æ–¹é¢å§‹ç»ˆä¼˜äºSMIï¼Œå¹¶åŒ¹é…DMIçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23235v1">PDF</a> 22 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹Vision Transformeræ¨¡å‹çš„Sparse Model Inversionï¼ˆSMIï¼‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†æ–°çš„è§£å†³æ–¹æ¡ˆPatch Rebirth Inversionï¼ˆPRIï¼‰ã€‚PRIé€šè¿‡å¢é‡åœ°åˆ†ç¦»æœ€é‡è¦çš„patchesæ¥æ„å»ºç¨€ç–åˆæˆå›¾åƒï¼ŒåŒæ—¶å…è®¸å‰©ä½™çš„patchesç»§ç»­æ¼”åŒ–ä»¥ä¾›æœªæ¥é€‰æ‹©ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†æ•ˆç‡ï¼Œè€Œä¸”å®ç°äº†æœ€åˆä¿¡æ¯é‡è¾ƒå°‘çš„patchesé€æ¸ç§¯ç´¯ä¸ç±»åˆ«ç›¸å…³çš„çŸ¥è¯†ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œRe-Birthæ•ˆåº”â€ï¼Œä»è€Œæœ‰æ•ˆåœ°å¹³è¡¡äº†ç±»åˆ«æ— å…³å’Œç±»åˆ«ç‰¹å®šçš„çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRIç›¸è¾ƒäºæ ‡å‡†Dense Model Inversionï¼ˆDMIï¼‰å®ç°äº†é«˜è¾¾10å€çš„å¿«é€Ÿåè½¬ï¼Œå¹¶ä¸”ç›¸è¾ƒäºSMIæœ‰æ›´é«˜çš„å‡†ç¡®ç‡å’Œæ›´å¿«çš„é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹åè½¬æ˜¯ä¸€ç§åœ¨æ— æ•°æ®å­¦ä¹ ä¸­å¹¿æ³›é‡‡ç”¨çš„æŠ€æœ¯ï¼Œå¯ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­é‡å»ºåˆæˆè¾“å…¥ï¼Œæ— éœ€è®¿é—®åŸå§‹è®­ç»ƒæ•°æ®ã€‚ä½†åº”ç”¨åœ¨Vision Transformerï¼ˆViTï¼‰æ¨¡å‹ä¸Šè®¡ç®—æˆæœ¬é«˜ã€‚</li>
<li>Sparse Model Inversionï¼ˆSMIï¼‰æ–¹æ³•è¯•å›¾é€šè¿‡ä¸¢å¼ƒçœ‹ä¼¼ä¸é‡è¦çš„patchesæ¥æé«˜æ•ˆç‡ï¼Œä½†å®è¯ç ”ç©¶ç»“æœæ˜¾ç¤ºä¸¢å¼ƒä»»ä½•æå‰åè½¬çš„patchesæ˜¯ä¸é«˜æ•ˆçš„ã€‚</li>
<li>æ–°æ–¹æ³•Patch Rebirth Inversionï¼ˆPRIï¼‰é€šè¿‡å¢é‡åœ°åˆ†ç¦»æœ€é‡è¦çš„patchesæ„å»ºç¨€ç–åˆæˆå›¾åƒï¼Œå…è®¸å‰©ä½™patchesç»§ç»­æ¼”åŒ–ä»¥ä¾›æœªæ¥é€‰æ‹©ï¼Œæé«˜äº†æ•ˆç‡å¹¶å®ç°äº†ç±»åˆ«çŸ¥è¯†çš„æœ‰æ•ˆå¹³è¡¡ã€‚</li>
<li>PRIå®ç°äº†æ›´é«˜çš„åè½¬é€Ÿåº¦å’Œå‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºæ ‡å‡†Dense Model Inversionï¼ˆDMIï¼‰è¾¾åˆ°10å€åŠ é€Ÿï¼Œç›¸è¾ƒäºSMIåˆ™å®ç°äº†æ›´å¿«çš„é€Ÿåº¦å’Œæ›´é«˜çš„å‡†ç¡®ç‡ã€‚</li>
<li>PRIæ–¹æ³•é¼“åŠ±æœ€åˆä¿¡æ¯é‡è¾ƒå°‘çš„patchesé€æ¸ç§¯ç´¯ä¸ç±»åˆ«ç›¸å…³çš„çŸ¥è¯†ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºâ€œRe-Birthæ•ˆåº”â€ã€‚</li>
<li>ä¸¢å¼ƒä¸é‡è¦çš„patchesä¼šæŠ‘åˆ¶ç±»åˆ«æ— å…³ç‰¹å¾çš„çŸ¥è¯†æå–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23235">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20b3e1527d853b487ed7d1bcc2cb8721.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce298cb37b159ceab041436f5082d734.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df6812ec5ab1c60873b174cbb4732cb1" align="middle">
<img src="https://picx.zhimg.com/v2-442b97d86eda06ea9bd5a280608f9f65" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Towards-Comprehensive-Interactive-Change-Understanding-in-Remote-Sensing-A-Large-scale-Dataset-and-Dual-granularity-Enhanced-VLM"><a href="#Towards-Comprehensive-Interactive-Change-Understanding-in-Remote-Sensing-A-Large-scale-Dataset-and-Dual-granularity-Enhanced-VLM" class="headerlink" title="Towards Comprehensive Interactive Change Understanding in Remote   Sensing: A Large-scale Dataset and Dual-granularity Enhanced VLM"></a>Towards Comprehensive Interactive Change Understanding in Remote   Sensing: A Large-scale Dataset and Dual-granularity Enhanced VLM</h2><p><strong>Authors:Junxiao Xue, Quan Deng, Xuecheng Wu, Kelu Yao, Xinyi Yin, Fei Yu, Wei Zhou, Yanfei Zhong, Yang Liu, Dingkang Yang</strong></p>
<p>Remote sensing change understanding (RSCU) is essential for analyzing remote sensing images and understanding how human activities affect the environment. However, existing datasets lack deep understanding and interactions in the diverse change captioning, counting, and localization tasks. To tackle these gaps, we construct ChangeIMTI, a new large-scale interactive multi-task instruction dataset that encompasses four complementary tasks including change captioning, binary change classification, change counting, and change localization. Building upon this new dataset, we further design a novel vision-guided vision-language model (ChangeVG) with dual-granularity awareness for bi-temporal remote sensing images (i.e., two remote sensing images of the same area at different times). The introduced vision-guided module is a dual-branch architecture that synergistically combines fine-grained spatial feature extraction with high-level semantic summarization. These enriched representations further serve as the auxiliary prompts to guide large vision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning, thereby facilitating the hierarchical cross-modal learning. We extensively conduct experiments across four tasks to demonstrate the superiority of our approach. Remarkably, on the change captioning task, our method outperforms the strongest method Semantic-CC by 1.39 points on the comprehensive S*m metric, which integrates the semantic similarity and descriptive accuracy to provide an overall evaluation of change caption. Moreover, we also perform a series of ablation studies to examine the critical components of our method. </p>
<blockquote>
<p>é¥æ„Ÿå˜åŒ–ç†è§£ï¼ˆRSCUï¼‰åœ¨åˆ†æé¥æ„Ÿå›¾åƒä»¥åŠäº†è§£äººç±»æ´»åŠ¨å¦‚ä½•å½±å“ç¯å¢ƒæ–¹é¢è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†åœ¨å¤šæ ·åŒ–çš„å˜åŒ–æè¿°ã€è®¡æ•°å’Œå®šä½ä»»åŠ¡ä¸­ç¼ºä¹æ·±åº¦ç†è§£å’Œäº¤äº’ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›ä¸è¶³ï¼Œæˆ‘ä»¬æ„å»ºäº†ChangeIMTIï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤§å‹äº¤äº’å¼å¤šä»»åŠ¡æŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«å››ä¸ªäº’è¡¥ä»»åŠ¡ï¼ŒåŒ…æ‹¬å˜åŒ–æè¿°ã€äºŒå…ƒå˜åŒ–åˆ†ç±»ã€å˜åŒ–è®¡æ•°å’Œå˜åŒ–å®šä½ã€‚åŸºäºè¿™ä¸ªæ–°æ•°æ®é›†ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ç§æ–°å‹è§†è§‰å¼•å¯¼çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆChangeVGï¼‰ï¼Œè¯¥æ¨¡å‹å…·æœ‰åŒç²’åº¦æ„è¯†ï¼Œé€‚ç”¨äºåŒæ—¶æ€é¥æ„Ÿå›¾åƒï¼ˆå³åŒä¸€åŒºåŸŸåœ¨ä¸åŒæ—¶é—´çš„ä¸¤å¼ é¥æ„Ÿå›¾åƒï¼‰ã€‚å¼•å…¥çš„è§†è§‰å¼•å¯¼æ¨¡å—æ˜¯ä¸€ä¸ªåŒåˆ†æ”¯æ¶æ„ï¼ŒååŒç»“åˆäº†ç²¾ç»†çš„çš„ç©ºé—´ç‰¹å¾æå–å’Œé«˜å±‚æ¬¡è¯­ä¹‰æ‘˜è¦ã€‚è¿™äº›ä¸°å¯Œçš„è¡¨ç¤ºå½¢å¼è¿›ä¸€æ­¥ä½œä¸ºè¾…åŠ©æç¤ºï¼Œç”¨äºæŒ‡å¯¼å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼ˆä¾‹å¦‚Qwen2.5-VL-7Bï¼‰åœ¨æŒ‡ä»¤è°ƒæ•´è¿‡ç¨‹ä¸­çš„å­¦ä¹ ï¼Œä»è€Œä¿ƒè¿›åˆ†å±‚è·¨æ¨¡æ€å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨å››ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œä»¥è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¼˜è¶Šæ€§ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œåœ¨å˜åŒ–æè¿°ä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»¼åˆS*mæŒ‡æ ‡ä¸Šè¶…è¶Šäº†æœ€å¼ºæ–¹æ³•Semantic-CC 1.39åˆ†ï¼Œè¯¥æŒ‡æ ‡ç»“åˆäº†è¯­ä¹‰ç›¸ä¼¼æ€§å’Œæè¿°å‡†ç¡®æ€§ï¼Œä»¥æä¾›å¯¹å˜åŒ–æè¿°çš„æ€»ä½“è¯„ä»·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸€ç³»åˆ—æ¶ˆèç ”ç©¶æ¥æ£€éªŒæˆ‘ä»¬æ–¹æ³•çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23105v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è¯¥æ–‡æœ¬ä»‹ç»äº†è¿œç¨‹æ„Ÿåº”å˜åŒ–ç†è§£çš„é‡è¦æ€§ä»¥åŠç°æœ‰æ•°æ®é›†çš„ä¸è¶³ã€‚ä¸ºå¼¥è¡¥è¿™äº›ä¸è¶³ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤§å‹äº¤äº’å¼å¤šä»»åŠ¡æŒ‡ä»¤æ•°æ®é›†ChangeIMTIï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ–°å‹è§†è§‰å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ChangeVGï¼Œå…·æœ‰åŒç²’åº¦æ„ŸçŸ¥èƒ½åŠ›ï¼Œç”¨äºå¤„ç†åŒæ—¶æ€é¥æ„Ÿå›¾åƒã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆç²¾ç»†ç©ºé—´ç‰¹å¾æå–ä¸é«˜çº§è¯­ä¹‰æ‘˜è¦ï¼Œä¸°å¯Œäº†è¡¨ç¤ºå½¢å¼ï¼Œå¹¶ä½œä¸ºè¾…åŠ©æç¤ºå¼•å¯¼å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æŒ‡ä»¤è°ƒæ•´æœŸé—´è¿›è¡Œåˆ†å±‚è·¨æ¨¡æ€å­¦ä¹ ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å››é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å˜åŒ–æè¿°ä»»åŠ¡ä¸Šï¼Œå…¶æ€§èƒ½è¾ƒæœ€å¼ºæ–¹æ³•Semantic-CCé«˜å‡º1.39ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¿œç¨‹æ„Ÿåº”å˜åŒ–ç†è§£(RSCU)åœ¨åˆ†æé¥æ„Ÿå›¾åƒå’Œäº†è§£äººç±»æ´»åŠ¨å¯¹ç¯å¢ƒå½±å“æ–¹é¢è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†åœ¨å¤šæ ·å˜åŒ–æè¿°ã€è®¡æ•°å’Œå®šä½ä»»åŠ¡ä¸­ç¼ºä¹æ·±åº¦ç†è§£å’Œäº¤äº’ã€‚</li>
<li>æ„å»ºäº†ChangeIMTIå¤§å‹äº¤äº’å¼å¤šä»»åŠ¡æŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«å››ç§äº’è¡¥ä»»åŠ¡ï¼šå˜åŒ–æè¿°ã€äºŒå…ƒå˜åŒ–åˆ†ç±»ã€å˜åŒ–è®¡æ•°å’Œå˜åŒ–å®šä½ã€‚</li>
<li>è®¾è®¡äº†å…·æœ‰åŒç²’åº¦æ„ŸçŸ¥èƒ½åŠ›çš„è§†è§‰å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ChangeVGï¼Œç”¨äºå¤„ç†åŒæ—¶æ€é¥æ„Ÿå›¾åƒã€‚</li>
<li>ChangeVGæ¨¡å‹ç»“åˆäº†ç²¾ç»†ç©ºé—´ç‰¹å¾æå–å’Œé«˜çº§è¯­ä¹‰æ‘˜è¦ï¼Œä¸°å¯Œäº†è¡¨ç¤ºå½¢å¼ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œåˆ†å±‚è·¨æ¨¡æ€å­¦ä¹ ï¼Œé€šè¿‡è¾…åŠ©æç¤ºå¼•å¯¼æ¨¡å‹è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-26b2520792c60fd4b4edff91a32878cb" align="middle">
<img src="https://picx.zhimg.com/v2-f7639a33f2f172376d8a153686912f48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d262624cf47e465cc9f69eed387411c0" align="middle">
<img src="https://pic1.zhimg.com/v2-94a2618d2a175424fbf9fbafbbd8f4cb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Deep-Learning-for-Oral-Health-Benchmarking-ViT-DeiT-BEiT-ConvNeXt-and-Swin-Transformer"><a href="#Deep-Learning-for-Oral-Health-Benchmarking-ViT-DeiT-BEiT-ConvNeXt-and-Swin-Transformer" class="headerlink" title="Deep Learning for Oral Health: Benchmarking ViT, DeiT, BEiT, ConvNeXt,   and Swin Transformer"></a>Deep Learning for Oral Health: Benchmarking ViT, DeiT, BEiT, ConvNeXt,   and Swin Transformer</h2><p><strong>Authors:Ajo Babu George, Sadhvik Bathini, Niranjana S R</strong></p>
<p>Objective: The aim of this study was to systematically evaluate and compare the performance of five state-of-the-art transformer-based architectures - Vision Transformer (ViT), Data-efficient Image Transformer (DeiT), ConvNeXt, Swin Transformer, and Bidirectional Encoder Representation from Image Transformers (BEiT) - for multi-class dental disease classification. The study specifically focused on addressing real-world challenges such as data imbalance, which is often overlooked in existing literature.   Study Design: The Oral Diseases dataset was used to train and validate the selected models. Performance metrics, including validation accuracy, precision, recall, and F1-score, were measured, with special emphasis on how well each architecture managed imbalanced classes.   Results: ConvNeXt achieved the highest validation accuracy at 81.06, followed by BEiT at 80.00 and Swin Transformer at 79.73, all demonstrating strong F1-scores. ViT and DeiT achieved accuracies of 79.37 and 78.79, respectively, but both struggled particularly with Caries-related classes.   Conclusions: ConvNeXt, Swin Transformer, and BEiT showed reliable diagnostic performance, making them promising candidates for clinical application in dental imaging. These findings provide guidance for model selection in future AI-driven oral disease diagnostic tools and highlight the importance of addressing data imbalance in real-world scenarios </p>
<blockquote>
<p>ç›®æ ‡ï¼šæœ¬ç ”ç©¶æ—¨åœ¨ç³»ç»Ÿè¯„ä¼°å¹¶æ¯”è¾ƒäº”ç§æœ€æ–°åŸºäºtransformeræ¶æ„ï¼ˆåŒ…æ‹¬Vision Transformerï¼ˆViTï¼‰ã€Data-efficient Image Transformerï¼ˆDeiTï¼‰ã€ConvNeXtã€Swin Transformerå’ŒBidirectional Encoder Representation from Image Transformersï¼ˆBEiTï¼‰ï¼‰åœ¨å¤šç±»åˆ«ç‰™ç§‘ç–¾ç—…åˆ†ç±»ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶ç‰¹åˆ«å…³æ³¨è§£å†³ç°å®ä¸–ç•Œä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œè¿™åœ¨ç°æœ‰æ–‡çŒ®ä¸­ç»å¸¸è¢«å¿½è§†ã€‚</p>
</blockquote>
<p>ç ”ç©¶è®¾è®¡ï¼šä½¿ç”¨å£è…”ç–¾ç—…æ•°æ®é›†å¯¹æ‰€é€‰æ¨¡å‹è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ã€‚æµ‹é‡æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬éªŒè¯å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ï¼Œç‰¹åˆ«å…³æ³¨æ¯ç§æ¶æ„å¦‚ä½•ç®¡ç†ä¸å¹³è¡¡ç±»åˆ«ã€‚</p>
<p>ç»“æœï¼šConvNeXtåœ¨éªŒè¯å‡†ç¡®ç‡æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè¾¾åˆ°81.06%ï¼Œå…¶æ¬¡æ˜¯BEiTï¼ˆ80.00%ï¼‰å’ŒSwin Transformerï¼ˆ79.73%ï¼‰ï¼Œä¸‰è€…å‡è¡¨ç°å‡ºè¾ƒé«˜çš„F1åˆ†æ•°ã€‚ViTå’ŒDeiTçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º79.37%å’Œ78.79%ï¼Œä½†ä¸¤è€…åœ¨å¤„ç†ä¸é¾‹é½¿ç›¸å…³çš„ç±»åˆ«æ—¶å°¤å…¶å›°éš¾ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23100v1">PDF</a> 9 pages,3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†äº”ç§å…ˆè¿›çš„åŸºäºTransformerçš„æ¶æ„ï¼ˆVision Transformerã€Data-efficient Image Transformerã€ConvNeXtã€Swin Transformerå’ŒBidirectional Encoder Representation from Image Transformersï¼‰åœ¨å¤šç±»ç‰™ç§‘ç–¾ç—…åˆ†ç±»ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥ç ”ç©¶ç€é‡è§£å†³æ•°æ®ä¸å¹³è¡¡ç­‰ç°å®æŒ‘æˆ˜ï¼Œä½¿ç”¨å£è…”ç–¾ç—…æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ï¼Œå¹¶è¯„ä¼°å„æ¶æ„åœ¨è§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒConvNeXtã€BEiTå’ŒSwin Transformerè¡¨ç°è¾ƒå¥½ï¼Œå…¶ä¸­ConvNeXtè·å¾—æœ€é«˜éªŒè¯å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨è¯„ä¼°äº”ç§å…ˆè¿›çš„åŸºäºTransformerçš„æ¶æ„åœ¨å¤šç±»ç‰™ç§‘ç–¾ç—…åˆ†ç±»ä¸­çš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å…³æ³¨ç°å®æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨å£è…”ç–¾ç—…æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ã€‚</li>
<li>ConvNeXtåœ¨éªŒè¯å‡†ç¡®ç‡æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè¾¾åˆ°81.06%ã€‚</li>
<li>BEiTå’ŒSwin Transformerä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>ViTå’ŒDeiTåœ¨æŸäº›ç±»åˆ«ï¼ˆå¦‚Cariesï¼‰ä¸Šçš„è¡¨ç°è¾ƒå·®ã€‚</li>
<li>ConvNeXtã€Swin Transformerå’ŒBEiTåœ¨ç‰™ç§‘å½±åƒä¸´åºŠåº”ç”¨ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1b59c404c486f8bb27a4818f317cb48" align="middle">
<img src="https://pica.zhimg.com/v2-5580e53f50971b245562a0cbd529923c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2ba632da469a8de8c9af75b79856a63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47b4b0f7a839e8753ef92ef17f0daba9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abde5572acf07c88dadee9440fb890ee" align="middle">
<img src="https://picx.zhimg.com/v2-21496726c5f92be70f80371a8cd3ffbf" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Mask-What-Matters-Controllable-Text-Guided-Masking-for-Self-Supervised-Medical-Image-Analysis"><a href="#Mask-What-Matters-Controllable-Text-Guided-Masking-for-Self-Supervised-Medical-Image-Analysis" class="headerlink" title="Mask What Matters: Controllable Text-Guided Masking for Self-Supervised   Medical Image Analysis"></a>Mask What Matters: Controllable Text-Guided Masking for Self-Supervised   Medical Image Analysis</h2><p><strong>Authors:Ruilang Wang, Shuotong Xu, Bowen Liu, Runlin Huang, Donglong Chen, Weifeng Su</strong></p>
<p>The scarcity of annotated data in specialized domains such as medical imaging presents significant challenges to training robust vision models. While self-supervised masked image modeling (MIM) offers a promising solution, existing approaches largely rely on random high-ratio masking, leading to inefficiency and poor semantic alignment. Moreover, region-aware variants typically depend on reconstruction heuristics or supervised signals, limiting their adaptability across tasks and modalities. We propose Mask What Matters, a controllable text-guided masking framework for self-supervised medical image analysis. By leveraging vision-language models for prompt-based region localization, our method flexibly applies differentiated masking to emphasize diagnostically relevant regions while reducing redundancy in background areas. This controllable design enables better semantic alignment, improved representation learning, and stronger cross-task generalizability. Comprehensive evaluation across multiple medical imaging modalities, including brain MRI, chest CT, and lung X-ray, shows that Mask What Matters consistently outperforms existing MIM methods (e.g., SparK), achieving gains of up to +3.1 percentage points in classification accuracy, +1.3 in box average precision (BoxAP), and +1.1 in mask average precision (MaskAP) for detection. Notably, it achieves these improvements with substantially lower overall masking ratios (e.g., 40% vs. 70%). This work demonstrates that controllable, text-driven masking can enable semantically aligned self-supervised learning, advancing the development of robust vision models for medical image analysis. </p>
<blockquote>
<p>åœ¨åŒ»ç–—æˆåƒç­‰ä¸“ä¸šåŒ–é¢†åŸŸï¼Œæ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§ä¸ºè®­ç»ƒç¨³å¥çš„è§†è§‰æ¨¡å‹å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶è‡ªç›‘ç£çš„æ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–äºéšæœºçš„é«˜æ¯”ä¾‹æ©ç ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œè¯­ä¹‰å¯¹é½ä¸ä½³ã€‚æ­¤å¤–ï¼ŒåŒºåŸŸæ„ŸçŸ¥å˜ä½“é€šå¸¸ä¾èµ–äºé‡å»ºå¯å‘å¼æˆ–ç›‘ç£ä¿¡å·ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä»»åŠ¡å’Œæ¨¡æ€ä¹‹é—´çš„é€‚åº”æ€§ã€‚æˆ‘ä»¬æå‡ºäº†â€œMask What Mattersâ€ï¼ˆæ©ç å…³é”®éƒ¨åˆ†ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¯æ§çš„æ–‡æœ¬å¼•å¯¼å‹æ©ç æ¡†æ¶ï¼Œç”¨äºè‡ªç›‘ç£åŒ»ç–—å›¾åƒåˆ†æã€‚é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºäºæç¤ºçš„åŒºåŸŸå®šä½ï¼Œæˆ‘ä»¬çš„æ–¹æ³•çµæ´»åœ°åº”ç”¨å·®å¼‚åŒ–æ©ç ï¼Œä»¥å¼ºè°ƒè¯Šæ–­ç›¸å…³åŒºåŸŸï¼ŒåŒæ—¶å‡å°‘èƒŒæ™¯åŒºåŸŸçš„å†—ä½™ã€‚è¿™ç§å¯æ§çš„è®¾è®¡å®ç°äº†æ›´å¥½çš„è¯­ä¹‰å¯¹é½ã€æ”¹è¿›çš„è¡¨ç¤ºå­¦ä¹ å’Œæ›´å¼ºçš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šç§åŒ»ç–—æˆåƒæ¨¡æ€çš„å…¨é¢è¯„ä¼°ä¸­ï¼ŒåŒ…æ‹¬è„‘éƒ¨MRIã€èƒ¸éƒ¨CTå’Œè‚ºéƒ¨Xå°„çº¿ï¼Œç»“æœæ˜¾ç¤ºMask What Matterså§‹ç»ˆä¼˜äºç°æœ‰çš„MIMæ–¹æ³•ï¼ˆä¾‹å¦‚SparKï¼‰ï¼Œåœ¨åˆ†ç±»ç²¾åº¦ä¸Šæé«˜äº†é«˜è¾¾+3.1ä¸ªç™¾åˆ†ç‚¹ï¼Œæ¡†å¹³å‡ç²¾åº¦ï¼ˆBoxAPï¼‰æé«˜äº†+1.3ï¼Œæ©è†œå¹³å‡ç²¾åº¦ï¼ˆMaskAPï¼‰åœ¨æ£€æµ‹æ–¹é¢æé«˜äº†+1.1ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨å®ç°è¿™äº›æ”¹è¿›çš„åŒæ—¶ï¼Œæ€»ä½“æ©ç æ¯”ä¾‹å¤§å¹…é™ä½ï¼ˆä¾‹å¦‚ï¼Œ40%å¯¹70%ï¼‰ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œå¯æ§çš„ã€æ–‡æœ¬é©±åŠ¨çš„æ©ç å¯ä»¥å®ç°è¯­ä¹‰å¯¹é½çš„è‡ªç›‘ç£å­¦ä¹ ï¼Œæ¨åŠ¨åŒ»ç–—å›¾åƒåˆ†æé¢†åŸŸç¨³å¥è§†è§‰æ¨¡å‹çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23054v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹åŒ»å­¦æˆåƒç­‰ç‰¹å®šé¢†åŸŸæ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œè‡ªç›‘ç£çš„æ©è†œå›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–äºéšæœºé«˜æ¯”ä¾‹æ©è†œï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œè¯­ä¹‰å¯¹é½ä¸ä½³ã€‚æœ¬æ–‡æå‡ºMask What Mattersï¼Œä¸€ç§å¯æ§çš„æ–‡æœ¬å¼•å¯¼æ©è†œæ¡†æ¶ï¼Œç”¨äºè‡ªç›‘ç£åŒ»å­¦å›¾åƒåˆ†æã€‚é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºäºæç¤ºçš„åŒºåŸŸå®šä½ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿçµæ´»åœ°åº”ç”¨å·®å¼‚åŒ–æ©è†œï¼Œä»¥å¼ºè°ƒè¯Šæ–­ç›¸å…³åŒºåŸŸï¼ŒåŒæ—¶å‡å°‘èƒŒæ™¯åŒºåŸŸçš„å†—ä½™ã€‚è¯¥å¯æ§è®¾è®¡å®ç°äº†æ›´å¥½çš„è¯­ä¹‰å¯¹é½ã€æ”¹è¿›äº†è¡¨å¾å­¦ä¹ å’Œæ›´å¼ºçš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ¨¡æ€ä¸Šçš„ç»¼åˆè¯„ä¼°ï¼ŒåŒ…æ‹¬è„‘MRIã€èƒ¸éƒ¨CTå’Œè‚ºéƒ¨Xå°„çº¿ï¼Œè¡¨æ˜Mask What Mattersåœ¨åˆ†ç±»ç²¾åº¦ä¸Šè¾ƒç°æœ‰MIMæ–¹æ³•å¹³å‡æé«˜äº†+3.1ä¸ªç™¾åˆ†ç‚¹ï¼Œæ¡†å¹³å‡ç²¾åº¦ï¼ˆBoxAPï¼‰æé«˜äº†+1.3ï¼Œæ©è†œå¹³å‡ç²¾åº¦ï¼ˆMaskAPï¼‰æé«˜äº†+1.1ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨å®ç°è¿™äº›æ”¹è¿›çš„åŒæ—¶ï¼Œæ•´ä½“æ©è†œæ¯”ç‡å¤§å¹…é™ä½ï¼ˆä¾‹å¦‚ï¼Œ40%å¯¹70%ï¼‰ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œå¯æ§çš„ã€æ–‡æœ¬é©±åŠ¨çš„æ©è†œèƒ½å¤Ÿå®ç°è¯­ä¹‰å¯¹é½çš„è‡ªç›‘ç£å­¦ä¹ ï¼Œæ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸç¨³å¥è§†è§‰æ¨¡å‹çš„å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒé¢†åŸŸæ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§å¯¹è®­ç»ƒç¨³å¥çš„è§†è§‰æ¨¡å‹æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>è‡ªç›‘ç£çš„æ©è†œå›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰ä¸ºè¿™ä¸€é—®é¢˜æä¾›äº†æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰MIMæ–¹æ³•ä¸»è¦ä¾èµ–éšæœºé«˜æ¯”ä¾‹æ©è†œï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œè¯­ä¹‰å¯¹é½ä¸ä½³ã€‚</li>
<li>Mask What Mattersæ¡†æ¶é€šè¿‡æ–‡æœ¬å¼•å¯¼çš„æ©è†œå®ç°å¯æ§è‡ªç›‘ç£å­¦ä¹ ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºäºæç¤ºçš„åŒºåŸŸå®šä½ï¼Œå¼ºè°ƒè¯Šæ–­ç›¸å…³åŒºåŸŸã€‚</li>
<li>Mask What Mattersåœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ¨¡æ€ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰MIMæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å®ç°é«˜æ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½äº†æ•´ä½“æ©è†œæ¯”ç‡ï¼Œå±•ç°äº†æ›´å¼ºçš„æ•ˆç‡å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-47bd848eccc3debcba5394e4b681a2cf" align="middle">
<img src="https://picx.zhimg.com/v2-d7308ccad5ffd63231ea134f6bcf72f1" align="middle">
<img src="https://picx.zhimg.com/v2-fe1061d4ddbf133c81b10db1428f0a39" align="middle">
<img src="https://picx.zhimg.com/v2-0b9a818f2a1ca134a250753278bd4fc8" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TRUST-Test-Time-Refinement-using-Uncertainty-Guided-SSM-Traverses"><a href="#TRUST-Test-Time-Refinement-using-Uncertainty-Guided-SSM-Traverses" class="headerlink" title="TRUST: Test-Time Refinement using Uncertainty-Guided SSM Traverses"></a>TRUST: Test-Time Refinement using Uncertainty-Guided SSM Traverses</h2><p><strong>Authors:Sahar Dastani, Ali Bahri, Gustavo Adolfo Vargas Hakim, Moslem Yazdanpanah, Mehrdad Noori, David Osowiechi, Samuel Barbeau, Ismail Ben Ayed, Herve Lombaert, Christian Desrosiers</strong></p>
<p>State Space Models (SSMs) have emerged as efficient alternatives to Vision Transformers (ViTs), with VMamba standing out as a pioneering architecture designed for vision tasks. However, their generalization performance degrades significantly under distribution shifts. To address this limitation, we propose TRUST (Test-Time Refinement using Uncertainty-Guided SSM Traverses), a novel test-time adaptation (TTA) method that leverages diverse traversal permutations to generate multiple causal perspectives of the input image. Model predictions serve as pseudo-labels to guide updates of the Mamba-specific parameters, and the adapted weights are averaged to integrate the learned information across traversal scans. Altogether, TRUST is the first approach that explicitly leverages the unique architectural properties of SSMs for adaptation. Experiments on seven benchmarks show that TRUST consistently improves robustness and outperforms existing TTA methods. </p>
<blockquote>
<p>çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ä½œä¸ºè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆå·²ç»å‡ºç°ï¼Œå…¶ä¸­VMambaä½œä¸ºä¸€ç§é’ˆå¯¹è§†è§‰ä»»åŠ¡è®¾è®¡çš„å¼€åˆ›æ€§æ¶æ„å°¤ä¸ºçªå‡ºã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨åˆ†å¸ƒå˜åŒ–ä¸‹çš„æ³›åŒ–æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºTRUSTï¼ˆä½¿ç”¨ä¸ç¡®å®šæ€§å¼•å¯¼SSMéå†çš„æµ‹è¯•æ—¶é—´ç»†åŒ–ï¼‰çš„æ–°å‹æµ‹è¯•æ—¶é—´é€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä¸åŒçš„éå†æ’åˆ—æ¥ç”Ÿæˆè¾“å…¥å›¾åƒçš„å¤šä¸ªå› æœè§†è§’ã€‚æ¨¡å‹é¢„æµ‹ä½œä¸ºä¼ªæ ‡ç­¾æ¥æŒ‡å¯¼Mambaç‰¹å®šå‚æ•°çš„æ›´æ–°ï¼Œå¹¶ä¸”é€‚åº”çš„æƒé‡è¢«å¹³å‡åŒ–ä»¥æ•´åˆå¤šæ¬¡éå†æ‰«æä¸­æ‰€å­¦ä¹ çš„ä¿¡æ¯ã€‚æ€»ä½“è€Œè¨€ï¼ŒTRUSTæ˜¯ç¬¬ä¸€ä¸ªæ˜ç¡®åˆ©ç”¨SSMsçš„ç‹¬ç‰¹æ¶æ„å±æ€§è¿›è¡Œé€‚åº”çš„æ–¹æ³•ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTRUSTæŒç»­æé«˜äº†ç¨³å¥æ€§å¹¶åœ¨æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„TTAæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22813v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ä½œä¸ºé«˜æ•ˆçš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰æ›¿ä»£æ–¹æ¡ˆå·²å´­éœ²å¤´è§’ï¼Œå…¶ä¸­VMambaæ˜¯ä¸ºè§†è§‰ä»»åŠ¡è®¾è®¡çš„å¼€åˆ›æ€§æ¶æ„ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨åˆ†å¸ƒè½¬ç§»ä¸‹çš„æ³›åŒ–æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºTRUSTçš„æ–°å‹æµ‹è¯•æ—¶é—´é€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä¸åŒçš„éå†æ’åˆ—ç”Ÿæˆè¾“å…¥å›¾åƒçš„å¤šä¸ªå› æœè§†è§’ã€‚æ¨¡å‹é¢„æµ‹ä½œä¸ºä¼ªæ ‡ç­¾æŒ‡å¯¼Mambaç‰¹å®šå‚æ•°çš„æ›´æ–°ï¼Œå¹¶å¹³å‡é€‚åº”æƒé‡ä»¥æ•´åˆéå†æ‰«æä¸­çš„å­¦ä¹ ä¿¡æ¯ã€‚æ€»ä½“è€Œè¨€ï¼ŒTRUSTæ˜¯ç¬¬ä¸€ä¸ªæ˜ç¡®åˆ©ç”¨SSMsç‹¬ç‰¹æ¶æ„å±æ€§è¿›è¡Œé€‚åº”çš„æ–¹æ³•ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTRUSTåœ¨æå‡ç¨³å¥æ€§æ–¹é¢è¡¨ç°ç¨³å®šï¼Œå¹¶ä¼˜äºç°æœ‰çš„TTAæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰å·²æˆä¸ºVision Transformersï¼ˆViTsï¼‰çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>VMambaæ˜¯ä¸€ç§ä¸“ä¸ºè§†è§‰ä»»åŠ¡è®¾è®¡çš„å¼€åˆ›æ€§æ¶æ„ã€‚</li>
<li>SSMsåœ¨åˆ†å¸ƒè½¬ç§»ä¸‹çš„æ³›åŒ–æ€§èƒ½å­˜åœ¨æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>TRUSTæ˜¯ä¸€ç§æ–°å‹çš„æµ‹è¯•æ—¶é—´é€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨ä¸åŒçš„éå†æ’åˆ—ç”Ÿæˆè¾“å…¥å›¾åƒçš„å¤šä¸ªå› æœè§†è§’ã€‚</li>
<li>æ¨¡å‹é¢„æµ‹ä½œä¸ºä¼ªæ ‡ç­¾æŒ‡å¯¼Mambaç‰¹å®šå‚æ•°çš„æ›´æ–°ã€‚</li>
<li>TRUSTé€šè¿‡å¹³å‡é€‚åº”æƒé‡æ•´åˆéå†æ‰«æä¸­çš„å­¦ä¹ ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22813">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-195182add7d857c359cc135987e27297" align="middle">
<img src="https://pic1.zhimg.com/v2-5065c2b83b9cf04fd54633540ff659c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adfebb9cb8c21e2405e2237e8255b670" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Learning-Hyperspectral-Images-with-Curated-Text-Prompts-for-Efficient-Multimodal-Alignment"><a href="#Learning-Hyperspectral-Images-with-Curated-Text-Prompts-for-Efficient-Multimodal-Alignment" class="headerlink" title="Learning Hyperspectral Images with Curated Text Prompts for Efficient   Multimodal Alignment"></a>Learning Hyperspectral Images with Curated Text Prompts for Efficient   Multimodal Alignment</h2><p><strong>Authors:Abhiroop Chatterjee, Susmita Ghosh</strong></p>
<p>As data requirements continue to grow, efficient learning increasingly depends on the curation and distillation of high-value data rather than brute-force scaling of model sizes. In the case of a hyperspectral image (HSI), the challenge is amplified by the high-dimensional 3D voxel structure, where each spatial location is associated with hundreds of contiguous spectral channels. While vision and language models have been optimized effectively for natural image or text tasks, their cross-modal alignment in the hyperspectral domain remains an open and underexplored problem. In this article, we make an attempt to optimize a Vision-Language Model (VLM) for hyperspectral scene understanding by exploiting a CLIP-style contrastive training framework. Our framework maps voxel-level embeddings from a vision backbone onto the latent space of a frozen large embedding model (LEM), where a trainable probe aligns vision features with the modelâ€™s textual token representations. The two modalities are aligned via a contrastive loss restricted to a curated set of hard (closest wrong classes) and semi-hard (random distractors) negatives, along with positive pairs. To further enhance alignment, descriptive prompts that encode class semantics are introduced and act as structured anchors for the HSI embeddings. It is seen that the proposed method updates only 0.07 percent of the total parameters, yet yields state-of-the-art performance. For example, on Indian Pines (IP) the model produces better results over unimodal and multimodal baselines by +0.92 Overall Accuracy (OA) and +1.60 Kappa ($\kappa$), while on Pavia University (PU) data it provides gains of +0.69 OA and +0.90 $\kappa$. Moreover, this is achieved with the set of parameters, nearly 50$\times$ smaller than DCTN and 90$\times$ smaller than SS-TMNet. </p>
<blockquote>
<p>éšç€æ•°æ®éœ€æ±‚çš„æŒç»­å¢é•¿ï¼Œé«˜æ•ˆå­¦ä¹ è¶Šæ¥è¶Šä¾èµ–äºé«˜ä»·å€¼æ•°æ®çš„ç­›é€‰å’Œæç‚¼ï¼Œè€Œä¸æ˜¯ç²—æš´åœ°æ‰©å¤§æ¨¡å‹è§„æ¨¡ã€‚å¯¹äºé«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰è€Œè¨€ï¼Œç”±äºæ¯ä¸ªç©ºé—´ä½ç½®éƒ½ä¸æ•°ç™¾ä¸ªè¿ç»­å…‰è°±é€šé“ç›¸å…³è”çš„é«˜ç»´ä¸‰ç»´ä½“ç´ ç»“æ„ï¼ŒæŒ‘æˆ˜è¢«æ”¾å¤§äº†ã€‚è™½ç„¶è§†è§‰å’Œè¯­è¨€æ¨¡å‹å·²é’ˆå¯¹è‡ªç„¶å›¾åƒæˆ–æ–‡æœ¬ä»»åŠ¡è¿›è¡Œäº†æœ‰æ•ˆä¼˜åŒ–ï¼Œä½†å®ƒä»¬åœ¨è¶…å…‰è°±åŸŸä¸­çš„è·¨æ¨¡æ€å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾ä¸”å°šæœªå……åˆ†æ¢ç´¢çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°è¯•åˆ©ç”¨CLIPé£æ ¼çš„å¯¹æ¯”è®­ç»ƒæ¡†æ¶æ¥ä¼˜åŒ–ç”¨äºè¶…å…‰è°±åœºæ™¯ç†è§£çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶å°†ä»è§†è§‰ä¸»å¹²ä¸­æå–çš„ä½“ç´ çº§åµŒå…¥æ˜ å°„åˆ°å†»ç»“çš„å¤§å‹åµŒå…¥æ¨¡å‹ï¼ˆLEMï¼‰çš„æ½œåœ¨ç©ºé—´ä¸Šï¼Œå…¶ä¸­å¯è®­ç»ƒæ¢é’ˆå°†è§†è§‰ç‰¹å¾ä¸æ¨¡å‹çš„æ–‡æœ¬ä»¤ç‰Œè¡¨ç¤ºè¿›è¡Œå¯¹é½ã€‚ä¸¤ç§æ¨¡æ€é€šè¿‡å¯¹æ¯”æŸå¤±ä¸ç²¾é€‰çš„ç¡¬ï¼ˆæœ€æ¥è¿‘çš„é”™ç±»ï¼‰å’ŒåŠç¡¬ï¼ˆéšæœºå¹²æ‰°é¡¹ï¼‰è´Ÿæ ·æœ¬ä»¥åŠæ­£æ ·æœ¬å¯¹è¿›è¡Œå¯¹é½ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºå¯¹é½æ•ˆæœï¼Œå¼•å…¥äº†ç¼–ç ç±»è¯­ä¹‰çš„æè¿°æ€§æç¤ºï¼Œè¿™äº›æç¤ºå……å½“è¶…å…‰è°±å›¾åƒåµŒå…¥çš„ç»“æ„é”šç‚¹ã€‚å¯ä»¥çœ‹åˆ°ï¼Œè¯¥æ–¹æ³•ä»…æ›´æ–°æ€»å‚æ•°çš„0.07%ï¼Œä½†å–å¾—äº†æœ€æ–°æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨å°ç¬¬å®‰æ¾ï¼ˆIPï¼‰ä¸Šï¼Œè¯¥æ¨¡å‹ç›¸å¯¹äºå•æ¨¡æ€å’Œå¤šæ¨¡æ€åŸºçº¿æé«˜äº†+0.92çš„æ•´ä½“ç²¾åº¦ï¼ˆOAï¼‰å’Œ+1.60çš„å¡å¸•ç³»æ•°ï¼ˆÎºï¼‰ï¼Œè€Œåœ¨å¸•ç»´äºšå¤§å­¦ï¼ˆPUï¼‰æ•°æ®é›†ä¸Šï¼Œå®ƒæä¾›äº†+0.69çš„OAå’Œ+0.90çš„å¡å¸•ç³»æ•°å¢ç›Šã€‚æ­¤å¤–ï¼Œè¿™æ˜¯é€šè¿‡ä½¿ç”¨å‚æ•°é›†å®ç°çš„ï¼Œå‡ ä¹æ˜¯DCTNçš„50å€å°ï¼Œå¹¶ä¸”æ˜¯SS-TMNetçš„90å€å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22697v1">PDF</a> Accepted at the IEEE&#x2F;CVF International Conference on Computer Vision   (ICCV 2025), Workshop on Curated Data for Efficient Learning</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†éšç€æ•°æ®é‡å¢é•¿ï¼Œé«˜æ•ˆå­¦ä¹ è¶Šæ¥è¶Šä¾èµ–äºé«˜è´¨é‡æ•°æ®çš„ç­›é€‰å’Œæç‚¼ï¼Œè€Œéæ¨¡å‹è§„æ¨¡çš„ç®€å•æ‰©å±•ã€‚é’ˆå¯¹é«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰çš„ç‰¹æ®ŠæŒ‘æˆ˜ï¼Œæ–‡ç« å°è¯•ä¼˜åŒ–è·¨æ¨¡æ€å¯¹é½çš„è§†è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œåˆ©ç”¨CLIPé£æ ¼çš„å¯¹æ¯”è®­ç»ƒæ¡†æ¶è¿›è¡Œé«˜å…‰è°±åœºæ™¯ç†è§£ã€‚é€šè¿‡æ˜ å°„è§†è§‰éª¨å¹²çš„åƒç´ çº§åµŒå…¥åˆ°é¢„è®­ç»ƒçš„å¤§å‹åµŒå…¥æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶ç”¨å¯è®­ç»ƒæ¢å¤´å¯¹é½è§†è§‰ç‰¹å¾ä¸æ¨¡å‹æ–‡æœ¬ç¬¦å·è¡¨ç¤ºæ¥å®ç°æ¨¡å‹å¯¹é½ã€‚åˆ©ç”¨ç²¾é€‰çš„ç¡¬ï¼ˆæœ€æ¥è¿‘çš„é”™è¯¯ç±»åˆ«ï¼‰å’ŒåŠç¡¬ï¼ˆéšæœºå¹²æ‰°é¡¹ï¼‰è´Ÿæ ·æœ¬ä»¥åŠæ­£æ ·æœ¬å¯¹è¿›è¡Œå¯¹æ¯”æŸå¤±çš„é™åˆ¶ï¼ŒåŒæ—¶å¼•å…¥ç¼–ç ç±»åˆ«è¯­ä¹‰çš„æè¿°æ€§æç¤ºä½œä¸ºç»“æ„åŒ–é”šç‚¹ä»¥å¢å¼ºå¯¹é½æ•ˆæœã€‚æ­¤æ–¹æ³•ä»…æ›´æ–°æ€»å‚æ•°çš„0.07%ï¼Œå´åœ¨å°åº¦æ¾æ ‘å’Œå¸•ç»´äºšå¤§å­¦æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜æ•ˆå­¦ä¹ æ›´æ³¨é‡é«˜è´¨é‡æ•°æ®çš„ç­›é€‰å’Œæç‚¼ï¼Œè€Œéæ¨¡å‹è§„æ¨¡çš„ç®€å•æ‰©å±•ã€‚</li>
<li>é’ˆå¯¹é«˜å…‰è°±å›¾åƒçš„ç‰¹æ®ŠæŒ‘æˆ˜ï¼Œä¼˜åŒ–è§†è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨è¶…å…‰è°±åœºæ™¯ç†è§£ä¸­çš„è¡¨ç°æ˜¯ä¸€ä¸ªå…³é”®è¯¾é¢˜ã€‚</li>
<li>åˆ©ç”¨CLIPé£æ ¼çš„å¯¹æ¯”è®­ç»ƒæ¡†æ¶å®ç°æ¨¡å‹çš„è·¨æ¨¡æ€å¯¹é½ã€‚</li>
<li>æ¨¡å‹é€šè¿‡å¯¹è§†è§‰åµŒå…¥ä¸é¢„è®­ç»ƒçš„å¤§å‹åµŒå…¥æ¨¡å‹å¯¹é½ï¼Œä»¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¯¹æ¯”æŸå¤±åˆ©ç”¨ç²¾é€‰çš„è´Ÿæ ·æœ¬å’Œæ­£æ ·æœ¬å¯¹è¿›è¡Œè®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹çš„å¯¹é½ç²¾åº¦ã€‚</li>
<li>æè¿°æ€§æç¤ºè¢«å¼•å…¥ä½œä¸ºç»“æ„åŒ–é”šç‚¹ï¼Œè¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„å¯¹é½æ•ˆæœã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šçš„å…ˆè¿›æ€§èƒ½ï¼Œä¸”å‚æ•°è§„æ¨¡ç›¸å¯¹è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22697">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-782f418678c2686b0ca341292ef5c01a" align="middle">
<img src="https://pica.zhimg.com/v2-d787633233b6ff4fffddd7822f818657.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38ca57f0990ed8e9e3477f349def2e0a" align="middle">
<img src="https://picx.zhimg.com/v2-4c51efbdc156f43d59fab7b4362164c8" align="middle">
<img src="https://picx.zhimg.com/v2-0be2ff932f6832424004d35c09829a48" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ViSpec-Accelerating-Vision-Language-Models-with-Vision-Aware-Speculative-Decoding"><a href="#ViSpec-Accelerating-Vision-Language-Models-with-Vision-Aware-Speculative-Decoding" class="headerlink" title="ViSpec: Accelerating Vision-Language Models with Vision-Aware   Speculative Decoding"></a>ViSpec: Accelerating Vision-Language Models with Vision-Aware   Speculative Decoding</h2><p><strong>Authors:Jialiang Kang, Han Shu, Wenshuo Li, Yingjie Zhai, Xinghao Chen</strong></p>
<p>Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), yet its application to vision-language models (VLMs) remains underexplored, with existing methods achieving only modest speedups (&lt;1.5x). This gap is increasingly significant as multimodal capabilities become central to large-scale models. We hypothesize that large VLMs can effectively filter redundant image information layer by layer without compromising textual comprehension, whereas smaller draft models struggle to do so. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a novel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor module to compress image tokens into a compact representation, which is seamlessly integrated into the draft modelâ€™s attention mechanism while preserving original image positional information. Additionally, we extract a global feature vector for each input image and augment all subsequent text tokens with this feature to enhance multimodal coherence. To overcome the scarcity of multimodal datasets with long assistant responses, we curate a specialized training dataset by repurposing existing datasets and generating extended outputs using the target VLM with modified prompts. Our training strategy mitigates the risk of the draft model exploiting direct access to the target modelâ€™s hidden states, which could otherwise lead to shortcut learning when training solely on target model outputs. Extensive experiments validate ViSpec, achieving, to our knowledge, the first substantial speedup in VLM speculative decoding. Code is available at <a target="_blank" rel="noopener" href="https://github.com/KangJialiang/ViSpec">https://github.com/KangJialiang/ViSpec</a>. </p>
<blockquote>
<p>æ¨æµ‹è§£ç æ˜¯åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„å¹¿æ³›é‡‡ç”¨çš„æŠ€æœ¯ï¼Œç„¶è€Œå®ƒåœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ï¼Œç°æœ‰æ–¹æ³•åªå®ç°äº†é€‚ä¸­çš„åŠ é€Ÿï¼ˆ&lt;1.5å€ï¼‰ã€‚éšç€å¤šæ¨¡æ€èƒ½åŠ›æˆä¸ºå¤§è§„æ¨¡æ¨¡å‹çš„æ ¸å¿ƒï¼Œè¿™ä¸€å·®è·è¶Šæ¥è¶Šæ˜¾è‘—ã€‚æˆ‘ä»¬å‡è®¾å¤§å‹VLMå¯ä»¥é€å±‚æœ‰æ•ˆåœ°è¿‡æ»¤æ‰å†—ä½™çš„å›¾åƒä¿¡æ¯ï¼Œè€Œä¸æŸå®³æ–‡æœ¬ç†è§£ï¼Œè€Œè¾ƒå°çš„è‰ç¨¿æ¨¡å‹åˆ™å¾ˆéš¾åšåˆ°è¿™ä¸€ç‚¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“ä¸ºVLMå®šåˆ¶çš„Vision-Aware Speculative Decodingï¼ˆViSpecï¼‰æ–°å‹æ¡†æ¶ã€‚ViSpecé‡‡ç”¨è½»é‡çº§çš„è§†è§‰é€‚é…å™¨æ¨¡å—ï¼Œå°†å›¾åƒä»¤ç‰Œå‹ç¼©æˆç´§å‡‘çš„è¡¨ç¤ºå½¢å¼ï¼Œæ— ç¼é›†æˆåˆ°è‰ç¨¿æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å›¾åƒçš„ä½ç½®ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªè¾“å…¥å›¾åƒæå–å…¨å±€ç‰¹å¾å‘é‡ï¼Œå¹¶å°†å…¶å¢å¼ºåˆ°æ‰€æœ‰åç»­çš„æ–‡æœ¬ä»¤ç‰Œä¸­ï¼Œä»¥æé«˜å¤šæ¨¡æ€ä¸€è‡´æ€§ã€‚ä¸ºäº†å…‹æœç¼ºä¹å¸¦æœ‰é•¿è¾…åŠ©å“åº”çš„å¤šæ¨¡æ€æ•°æ®é›†çš„é—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡é‡æ–°åˆ©ç”¨ç°æœ‰æ•°æ®é›†å¹¶ä½¿ç”¨ç›®æ ‡VLMç”Ÿæˆæ‰©å±•è¾“å‡ºï¼Œæ¥åˆ¶ä½œä¸€ä¸ªä¸“é—¨ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ã€‚æˆ‘ä»¬çš„è®­ç»ƒç­–ç•¥å‡è½»äº†è‰ç¨¿æ¨¡å‹ç›´æ¥è®¿é—®ç›®æ ‡æ¨¡å‹çš„éšè—çŠ¶æ€çš„é£é™©ï¼Œå› ä¸ºå¦‚æœä»…åœ¨ç›®æ ‡æ¨¡å‹è¾“å‡ºä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯èƒ½ä¼šå¯¼è‡´èµ°æ·å¾„å­¦ä¹ ã€‚å¤§é‡å®éªŒéªŒè¯äº†ViSpecçš„æœ‰æ•ˆæ€§ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯VLMæ¨æµ‹è§£ç ä¸­çš„é¦–æ¬¡å®è´¨æ€§åŠ é€Ÿã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/KangJialiang/ViSpec">https://github.com/KangJialiang/ViSpec</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15235v4">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong>ï¼š<br>é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨æµ‹è§£ç æŠ€æœ¯å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œç°æœ‰æ–¹æ³•çš„é€Ÿåº¦æå‡æœ‰é™ï¼ˆå°äº1.5å€ï¼‰ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹VLMsé‡èº«å®šåˆ¶çš„æ–°å‹æ¡†æ¶â€”â€”Vision-Aware Speculative Decodingï¼ˆViSpecï¼‰ã€‚å®ƒé€šè¿‡è½»é‡çº§è§†è§‰é€‚é…å™¨æ¨¡å—å‹ç¼©å›¾åƒæ ‡è®°ä¸ºç´§å‡‘è¡¨ç¤ºå½¢å¼ï¼Œæ— ç¼é›†æˆåˆ°è‰æ¡ˆæ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å›¾åƒçš„ä½ç½®ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é€šè¿‡é‡æ–°åˆ©ç”¨ç°æœ‰æ•°æ®é›†å¹¶ç”Ÿæˆæ‰©å±•è¾“å‡ºï¼Œåˆ›å»ºäº†ä¸€ç§ç‰¹æ®Šè®­ç»ƒæ•°æ®é›†ã€‚å®éªŒè¯æ˜ï¼ŒViSpecå®ç°äº†å¯¹VLMæ¨æµ‹è§£ç çš„é¦–æ¬¡å®è´¨æ€§åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨æµ‹è§£ç æŠ€æœ¯åº”ç”¨ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•æé€Ÿæœ‰é™ã€‚</li>
<li>å¤§å‹VLMsèƒ½æœ‰æ•ˆè¿‡æ»¤å†—ä½™å›¾åƒä¿¡æ¯å±‚ï¼Œè€Œå°å‹æ¨¡å‹åˆ™éš¾ä»¥åšåˆ°ã€‚</li>
<li>å¼•å…¥æ–°å‹æ¡†æ¶Vision-Aware Speculative Decodingï¼ˆViSpecï¼‰ï¼Œä¸“ä¸ºVLMsè®¾è®¡ã€‚</li>
<li>ViSpecä½¿ç”¨è½»é‡çº§è§†è§‰é€‚é…å™¨æ¨¡å—å‹ç¼©å›¾åƒæ ‡è®°ï¼Œå¹¶é›†æˆåˆ°æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ã€‚</li>
<li>ViSpecä¿ç•™åŸå§‹å›¾åƒçš„ä½ç½®ä¿¡æ¯ï¼Œå¢å¼ºå¤šæ¨¡æ€è¿è´¯æ€§ã€‚</li>
<li>é€šè¿‡é‡æ–°åˆ©ç”¨ç°æœ‰æ•°æ®é›†å¹¶ç”Ÿæˆæ‰©å±•è¾“å‡ºï¼Œåˆ›å»ºç‰¹æ®Šè®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>ViSpecå®ç°äº†å¯¹VLMæ¨æµ‹è§£ç çš„é¦–æ¬¡å®è´¨æ€§åŠ é€Ÿï¼Œä»£ç å·²å…¬å¼€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15235">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3177badf947f3af6da04d9ab01700c19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-235428602e43a141b7ca45b7c6f867fa" align="middle">
<img src="https://picx.zhimg.com/v2-a4188160b4159f71ce59eee4ce462e16" align="middle">
<img src="https://pic1.zhimg.com/v2-1e17e2f836736fc394f63c0c0d8038f4.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Power-Battery-Detection"><a href="#Power-Battery-Detection" class="headerlink" title="Power Battery Detection"></a>Power Battery Detection</h2><p><strong>Authors:Xiaoqi Zhao, Peiqian Cao, Chenyang Yu, Zonglei Feng, Lihe Zhang, Hanqi Liu, Jiaming Zuo, Youwei Pang, Jinsong Ouyang, Weisi Lin, Georges El Fakhri, Huchuan Lu, Xiaofeng Liu</strong></p>
<p>Power batteries are essential components in electric vehicles, where internal structural defects can pose serious safety risks. We conduct a comprehensive study on a new task, power battery detection (PBD), which aims to localize the dense endpoints of cathode and anode plates from industrial X-ray images for quality inspection. Manual inspection is inefficient and error-prone, while traditional vision algorithms struggle with densely packed plates, low contrast, scale variation, and imaging artifacts. To address this issue and drive more attention into this meaningful task, we present PBD5K, the first large-scale benchmark for this task, consisting of 5,000 X-ray images from nine battery types with fine-grained annotations and eight types of real-world visual interference. To support scalable and consistent labeling, we develop an intelligent annotation pipeline that combines image filtering, model-assisted pre-labeling, cross-verification, and layered quality evaluation. We formulate PBD as a point-level segmentation problem and propose MDCNeXt, a model designed to extract and integrate multi-dimensional structure clues including point, line, and count information from the plate itself. To improve discrimination between plates and suppress visual interference, MDCNeXt incorporates two state space modules. The first is a prompt-filtered module that learns contrastive relationships guided by task-specific prompts. The second is a density-aware reordering module that refines segmentation in regions with high plate density. In addition, we propose a distance-adaptive mask generation strategy to provide robust supervision under varying spatial distributions of anode and cathode positions. The source code and datasets will be publicly available at \href{<a target="_blank" rel="noopener" href="https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD%7D%7BPBD5K%7D">https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD}{PBD5K}</a>. </p>
<blockquote>
<p>åŠ¨åŠ›ç”µæ± æ˜¯ç”µåŠ¨æ±½è½¦ä¸­çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå…¶å†…éƒ¨ç»“æ„æ€§ç¼ºé™·å¯èƒ½å¸¦æ¥ä¸¥é‡çš„å®‰å…¨é£é™©ã€‚æˆ‘ä»¬å¯¹ä¸€é¡¹æ–°ä»»åŠ¡â€”â€”åŠ¨åŠ›ç”µæ± æ£€æµ‹ï¼ˆPBDï¼‰è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œè¯¥ä»»åŠ¡æ—¨åœ¨ä»å·¥ä¸šXå°„çº¿å›¾åƒä¸­å®šä½é˜´æå’Œé˜³ææ¿çš„å¯†é›†ç«¯ç‚¹ï¼Œä»¥è¿›è¡Œè´¨é‡æ£€æµ‹ã€‚äººå·¥æ£€æµ‹æ•ˆç‡ä½ä¸‹ä¸”æ˜“å‡ºé”™ï¼Œè€Œä¼ ç»Ÿè§†è§‰ç®—æ³•åœ¨å¯†é›†æ’åˆ—çš„æ¿æã€ä½å¯¹æ¯”åº¦ã€å°ºåº¦å˜åŒ–å’Œå›¾åƒä¼ªå½±æ–¹é¢è¡¨ç°æŒ£æ‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜å¹¶å¸å¼•æ›´å¤šæ³¨æ„åŠ›å…³æ³¨è¿™é¡¹æœ‰æ„ä¹‰çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PBD5Kï¼Œè¿™æ˜¯è¯¥ä»»åŠ¡çš„é¦–ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æ¥è‡ªä¹ç§ç”µæ± ç±»å‹çš„5000å¼ Xå°„çº¿å›¾åƒï¼Œå…·æœ‰ç»†ç²’åº¦æ³¨é‡Šå’Œå…«ç§ç°å®ä¸–ç•Œä¸­çš„è§†è§‰å¹²æ‰°ç±»å‹ã€‚ä¸ºäº†æ”¯æŒå¯æ‰©å±•å’Œä¸€è‡´çš„æ ‡æ³¨ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ™ºèƒ½æ ‡æ³¨ç®¡é“ï¼Œè¯¥ç®¡é“ç»“åˆäº†å›¾åƒè¿‡æ»¤ã€æ¨¡å‹è¾…åŠ©é¢„æ ‡æ³¨ã€äº¤å‰éªŒè¯å’Œåˆ†å±‚è´¨é‡è¯„ä¼°ã€‚æˆ‘ä»¬å°†PBDåˆ¶å®šä¸ºç‚¹çº§åˆ†å‰²é—®é¢˜ï¼Œå¹¶æå‡ºMDCNeXtæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ—¨åœ¨æå–å’Œæ•´åˆåŒ…æ‹¬ç‚¹ã€çº¿å’Œè®¡æ•°ä¿¡æ¯åœ¨å†…çš„å¤šç»´ç»“æ„çº¿ç´¢ã€‚ä¸ºäº†æé«˜æ¿ä¹‹é—´çš„è¾¨åˆ«åŠ›å¹¶æŠ‘åˆ¶è§†è§‰å¹²æ‰°ï¼ŒMDCNeXtç»“åˆäº†ä¸¤ç§çŠ¶æ€ç©ºé—´æ¨¡å—ã€‚ç¬¬ä¸€ä¸ªæ˜¯æç¤ºè¿‡æ»¤æ¨¡å—ï¼Œå®ƒå­¦ä¹ åœ¨ä»»åŠ¡ç‰¹å®šæç¤ºçš„æŒ‡å¯¼ä¸‹å¯¹æ¯”å…³ç³»ã€‚ç¬¬äºŒä¸ªæ˜¯å¯†åº¦æ„ŸçŸ¥é‡æ’åºæ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥ç»†åŒ–é«˜æ¿å¯†åº¦åŒºåŸŸçš„åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è·ç¦»è‡ªé€‚åº”æ©è†œç”Ÿæˆç­–ç•¥ï¼Œä»¥åœ¨é˜³æå’Œé˜´æä½ç½®ç©ºé—´åˆ†å¸ƒå˜åŒ–çš„æƒ…å†µä¸‹æä¾›ç¨³å¥çš„ç›‘ç£ã€‚æºä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD">https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD</a>å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07797v2">PDF</a> Under submission to International Journal of Computer Vision (IJCV)</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŠ¨åŠ›ç”µæ± æ£€æµ‹ï¼ˆPBDï¼‰ä»»åŠ¡ï¼Œæœ¬ç ”ç©¶æå‡ºPBD5Kæ•°æ®é›†å’ŒMDCNeXtæ¨¡å‹ã€‚æ•°æ®é›†åŒ…å«äº”åƒå¼ Xå°„çº¿å›¾åƒå’Œç²¾ç»†æ ‡æ³¨ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè§†è§‰ç®—æ³•åœ¨ç”µæ± æ¿å¯†é›†æ’åˆ—ã€ä½å¯¹æ¯”åº¦ã€å°ºåº¦å˜åŒ–å’Œæˆåƒä¼ªå½±ä¸Šçš„æŒ‘æˆ˜ã€‚MDCNeXtèƒ½æå–å’Œæ•´åˆå¤šç»´ç»“æ„çº¿ç´¢ï¼Œé‡‡ç”¨åŒæ€ç©ºé—´æ¨¡å—æŠ‘åˆ¶è§†è§‰å¹²æ‰°å¹¶æé«˜åŒºåˆ†åº¦ã€‚ç ”ç©¶ä¿ƒè¿›äº†æŠ€æœ¯è¿›æ­¥å’Œèµ„æºå…±äº«ã€‚è¯¦æƒ…è¯·å‚è§æ•°æ®é›†åŠä»£ç é“¾æ¥ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD%EF%BC%89%E3%80%82">https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBDï¼‰ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŠ¨åŠ›è“„ç”µæ± æ£€æµ‹ï¼ˆPBDï¼‰æ—¨åœ¨ä»å·¥ä¸šXå°„çº¿å›¾åƒä¸­å®šä½é˜´æå’Œé˜³ææ¿çš„å¯†é›†ç«¯ç‚¹è¿›è¡Œè´¨é‡æ£€æµ‹ã€‚</li>
<li>æ‰‹åŠ¨æ£€æµ‹æ•ˆç‡ä½ä¸‹ä¸”æ˜“å‡ºé”™ï¼Œä¼ ç»Ÿè§†è§‰ç®—æ³•é¢ä¸´å¤šé¡¹æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºPBD5Kæ•°æ®é›†ï¼ŒåŒ…å«äº”åƒå¼ å›¾åƒå’Œç²¾ç»†æ ‡æ³¨ï¼Œç”¨äºæ¨åŠ¨è¯¥ä»»åŠ¡çš„ç ”ç©¶ã€‚</li>
<li>å¼€å‘æ™ºèƒ½æ ‡æ³¨ç®¡é“ï¼Œæ”¯æŒå¯æ‰©å±•å’Œä¸€è‡´çš„æ ‡æ³¨å·¥ä½œã€‚</li>
<li>å°†PBDå®šä¹‰ä¸ºç‚¹çº§åˆ«åˆ†å‰²é—®é¢˜ï¼Œå¹¶æå‡ºMDCNeXtæ¨¡å‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>MDCNeXté€šè¿‡ä¸¤ä¸ªæ€ç©ºé—´æ¨¡å—æé«˜æ¿é—´åŒºåˆ†åº¦å’ŒæŠ‘åˆ¶è§†è§‰å¹²æ‰°ã€‚</li>
<li>é‡‡ç”¨è‡ªé€‚åº”è·ç¦»æ©è†œç”Ÿæˆç­–ç•¥ï¼Œé€‚åº”ä¸åŒç©ºé—´åˆ†å¸ƒçš„é˜³æå’Œé˜´æä½ç½®æ ‡æ³¨é—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07797">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4f874b64c65508c0484d1fd97f3d1aa" align="middle">
<img src="https://picx.zhimg.com/v2-41df3e1cdbc12a89cef3f735b859d656" align="middle">
<img src="https://picx.zhimg.com/v2-1b4006cb27199c4addde000fdb26a4cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b750a320aa9443528486f91a0c7bfc8b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FA-Forced-Prompt-Learning-of-Vision-Language-Models-for-Out-of-Distribution-Detection"><a href="#FA-Forced-Prompt-Learning-of-Vision-Language-Models-for-Out-of-Distribution-Detection" class="headerlink" title="FA: Forced Prompt Learning of Vision-Language Models for   Out-of-Distribution Detection"></a>FA: Forced Prompt Learning of Vision-Language Models for   Out-of-Distribution Detection</h2><p><strong>Authors:Xinhua Lu, Runhe Lai, Yanqi Wu, Kanghao Chen, Wei-Shi Zheng, Ruixuan Wang</strong></p>
<p>Pre-trained vision-language models (VLMs) have advanced out-of-distribution (OOD) detection recently. However, existing CLIP-based methods often focus on learning OOD-related knowledge to improve OOD detection, showing limited generalization or reliance on external large-scale auxiliary datasets. In this study, instead of delving into the intricate OOD-related knowledge, we propose an innovative CLIP-based framework based on Forced prompt leArning (FA), designed to make full use of the In-Distribution (ID) knowledge and ultimately boost the effectiveness of OOD detection. Our key insight is to learn a prompt (i.e., forced prompt) that contains more diversified and richer descriptions of the ID classes beyond the textual semantics of class labels. Specifically, it promotes better discernment for ID images, by forcing more notable semantic similarity between ID images and the learnable forced prompt. Moreover, we introduce a forced coefficient, encouraging the forced prompt to learn more comprehensive and nuanced descriptions of the ID classes. In this way, FA is capable of achieving notable improvements in OOD detection, even when trained without any external auxiliary datasets, while maintaining an identical number of trainable parameters as CoOp. Extensive empirical evaluations confirm our method consistently outperforms current state-of-the-art methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/0xFAFA/FA">https://github.com/0xFAFA/FA</a>. </p>
<blockquote>
<p>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æœ€è¿‘åœ¨å¼‚å¸¸åˆ†å¸ƒï¼ˆOODï¼‰æ£€æµ‹æ–¹é¢å–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºCLIPçš„æ–¹æ³•å¾€å¾€ä¾§é‡äºå­¦ä¹ OODç›¸å…³çŸ¥è¯†ä»¥æé«˜OODæ£€æµ‹èƒ½åŠ›ï¼Œè¡¨ç°å‡ºæœ‰é™çš„æ³›åŒ–èƒ½åŠ›æˆ–å¯¹å¤–éƒ¨å¤§è§„æ¨¡è¾…åŠ©æ•°æ®é›†çš„ä¾èµ–ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¹¶æœªæ·±å…¥ç ”ç©¶å¤æ‚çš„OODç›¸å…³çŸ¥è¯†ï¼Œè€Œæ˜¯æå‡ºäº†ä¸€ç§åŸºäºå¼ºåˆ¶æç¤ºå­¦ä¹ ï¼ˆFAï¼‰çš„CLIPæ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨å……åˆ†åˆ©ç”¨å†…åˆ†å¸ƒï¼ˆIDï¼‰çŸ¥è¯†ï¼Œå¹¶æœ€ç»ˆæé«˜OODæ£€æµ‹çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯å­¦ä¹ ä¸€ç§æç¤ºï¼ˆå³å¼ºåˆ¶æç¤ºï¼‰ï¼Œå…¶ä¸­åŒ…å«è¶…è¶Šç±»åˆ«æ ‡ç­¾æ–‡æœ¬è¯­ä¹‰çš„IDç±»åˆ«çš„æ›´å¤šæ ·åŒ–å’Œä¸°å¯Œçš„æè¿°ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé€šè¿‡å¼ºåˆ¶IDå›¾åƒå’Œå¯å­¦ä¹ çš„å¼ºåˆ¶æç¤ºä¹‹é—´æ›´æ˜¾è‘—çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œä¿ƒè¿›äº†å¯¹IDå›¾åƒçš„æ›´ä½³è¾¨åˆ«åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¼ºåˆ¶ç³»æ•°ï¼Œé¼“åŠ±å¼ºåˆ¶æç¤ºå­¦ä¹ æ›´å…¨é¢ã€æ›´å¾®å¦™çš„IDç±»åˆ«æè¿°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå³ä½¿åœ¨æ²¡æœ‰ä»»ä½•å¤–éƒ¨è¾…åŠ©æ•°æ®é›†çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒï¼ŒFAä¹Ÿèƒ½åœ¨OODæ£€æµ‹æ–¹é¢å®ç°æ˜¾è‘—çš„æ”¹è¿›ï¼ŒåŒæ—¶ä¿æŒä¸CoOpç›¸åŒçš„å¯è®­ç»ƒå‚æ•°æ•°é‡ã€‚å¤§é‡çš„å®è¯ç ”ç©¶è¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/0xFAFA/FA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/0xFAFA/FAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04511v3">PDF</a> 12 pages, 4 figures, Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºCLIPçš„å¼ºåˆ¶æç¤ºå­¦ä¹ ï¼ˆFAï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å……åˆ†åˆ©ç”¨åˆ†å¸ƒå†…ï¼ˆIDï¼‰çŸ¥è¯†ï¼Œæé«˜å¼‚å¸¸æ£€æµ‹æ•ˆæœã€‚é€šè¿‡å¼ºåˆ¶æç¤ºå­¦ä¹ ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åŒ…å«è¶…è¿‡ç±»åˆ«æ ‡ç­¾æ–‡æœ¬è¯­ä¹‰çš„æ›´å¤šæ ·åŒ–å’Œä¸°å¯Œçš„æè¿°ï¼Œä¿ƒè¿›å¯¹åˆ†å¸ƒå†…å›¾åƒçš„æ›´å¥½è¾¨åˆ«ã€‚å¼•å…¥å¼ºåˆ¶ç³»æ•°ï¼Œé¼“åŠ±å¼ºåˆ¶æç¤ºå­¦ä¹ æ›´å…¨é¢ã€æ›´å¾®å¦™çš„åˆ†å¸ƒå†…ç±»åˆ«æè¿°ï¼Œæ— éœ€ä»»ä½•å¤–éƒ¨è¾…åŠ©æ•°æ®é›†å³å¯å®ç°æ˜¾è‘—çš„å¼‚å¸¸æ£€æµ‹æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºCLIPçš„å¼ºåˆ¶æç¤ºå­¦ä¹ ï¼ˆFAï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¼‚å¸¸æ£€æµ‹æ•ˆæœã€‚</li>
<li>FAæ¡†æ¶åˆ©ç”¨åˆ†å¸ƒå†…ï¼ˆIDï¼‰çŸ¥è¯†ï¼Œé€šè¿‡å¼ºåˆ¶æç¤ºå­¦ä¹ æ›´å¤šæ ·åŒ–å’Œä¸°å¯Œçš„æè¿°ã€‚</li>
<li>å¼ºåˆ¶æç¤ºå­¦ä¹ èƒ½å¤Ÿä¿ƒè¿›å¯¹åˆ†å¸ƒå†…å›¾åƒçš„æ›´å¥½è¾¨åˆ«ã€‚</li>
<li>å¼•å…¥å¼ºåˆ¶ç³»æ•°ï¼Œé¼“åŠ±å¼ºåˆ¶æç¤ºå­¦ä¹ æ›´å…¨é¢ã€æ›´å¾®å¦™çš„åˆ†å¸ƒå†…ç±»åˆ«æè¿°ã€‚</li>
<li>FAæ¡†æ¶æ— éœ€ä»»ä½•å¤–éƒ¨è¾…åŠ©æ•°æ®é›†å³å¯å®ç°æ˜¾è‘—çš„å¼‚å¸¸æ£€æµ‹æ”¹è¿›ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒFAæ¡†æ¶çš„æ€§èƒ½æŒç»­è¶…è¶Šå½“å‰å…ˆè¿›æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa495c1b0cdbafe968b89d775acb88ac" align="middle">
<img src="https://pic1.zhimg.com/v2-ac70a64138fefdfeb3da581102d3b10a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f373d1245bbde7e2076086a6ac743da.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Advancing-Marine-Research-UWSAM-Framework-and-UIIS10K-Dataset-for-Precise-Underwater-Instance-Segmentation"><a href="#Advancing-Marine-Research-UWSAM-Framework-and-UIIS10K-Dataset-for-Precise-Underwater-Instance-Segmentation" class="headerlink" title="Advancing Marine Research: UWSAM Framework and UIIS10K Dataset for   Precise Underwater Instance Segmentation"></a>Advancing Marine Research: UWSAM Framework and UIIS10K Dataset for   Precise Underwater Instance Segmentation</h2><p><strong>Authors:Hua Li, Shijie Lian, Zhiyuan Li, Runmin Cong, Chongyi Li, Laurence T. Yang, Weidong Zhang, Sam Kwong</strong></p>
<p>With recent breakthroughs in large-scale modeling, the Segment Anything Model (SAM) has demonstrated significant potential in a variety of visual applications. However, due to the lack of underwater domain expertise, SAM and its variants face performance limitations in end-to-end underwater instance segmentation tasks, while their higher computational requirements further hinder their application in underwater scenarios. To address this challenge, we propose a large-scale underwater instance segmentation dataset, UIIS10K, which includes 10,048 images with pixel-level annotations for 10 categories. Then, we introduce UWSAM, an efficient model designed for automatic and accurate segmentation of underwater instances. UWSAM efficiently distills knowledge from the SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the Mask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective visual representation learning. Furthermore, we design an End-to-end Underwater Prompt Generator (EUPG) for UWSAM, which automatically generates underwater prompts instead of explicitly providing foreground points or boxes as prompts, thus enabling the network to locate underwater instances accurately for efficient segmentation. Comprehensive experimental results show that our model is effective, achieving significant performance improvements over state-of-the-art methods on multiple underwater instance datasets. Datasets and codes are available at <a target="_blank" rel="noopener" href="https://github.com/LiamLian0727/UIIS10K">https://github.com/LiamLian0727/UIIS10K</a>. </p>
<blockquote>
<p>éšç€å¤§è§„æ¨¡å»ºæ¨¡çš„æœ€æ–°çªç ´ï¼ŒSegment Anything Modelï¼ˆSAMï¼‰åœ¨å„ç§è§†è§‰åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ°´ä¸‹é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼ŒSAMåŠå…¶å˜ä½“åœ¨ç«¯åˆ°ç«¯çš„æ°´ä¸‹å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­é¢ä¸´æ€§èƒ½å±€é™ï¼Œè€Œå®ƒä»¬è¾ƒé«˜çš„è®¡ç®—è¦æ±‚è¿›ä¸€æ­¥é˜»ç¢äº†åœ¨æ°´ä¸‹åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æ°´ä¸‹å®ä¾‹åˆ†å‰²æ•°æ®é›†UIIS10Kï¼Œå…¶ä¸­åŒ…æ‹¬10,048å¼ å…·æœ‰10ç±»åƒç´ çº§æ³¨é‡Šçš„å›¾åƒã€‚æ¥ç€ï¼Œæˆ‘ä»¬ä»‹ç»äº†UWSAMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºæ°´ä¸‹å®ä¾‹è‡ªåŠ¨å‡†ç¡®åˆ†å‰²è€Œè®¾è®¡çš„é«˜æ•ˆæ¨¡å‹ã€‚UWSAMé€šè¿‡åŸºäºMask GATçš„æ°´ä¸‹çŸ¥è¯†è’¸é¦ï¼ˆMG-UKDï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°ä»SAMçš„ViT-Hugeå›¾åƒç¼–ç å™¨è’¸é¦çŸ¥è¯†åˆ°è¾ƒå°çš„ViT-Smallå›¾åƒç¼–ç å™¨ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„è§†è§‰è¡¨ç¤ºå­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºUWSAMè®¾è®¡äº†ç«¯åˆ°ç«¯çš„æ°´ä¸‹æç¤ºç”Ÿæˆå™¨ï¼ˆEUPGï¼‰ï¼Œå®ƒä¼šè‡ªåŠ¨ç”Ÿæˆæ°´ä¸‹æç¤ºï¼Œè€Œä¸æ˜¯æ˜¾å¼æä¾›å‰æ™¯ç‚¹æˆ–æ¡†ä½œä¸ºæç¤ºï¼Œä»è€Œä½¿ç½‘ç»œèƒ½å¤Ÿå‡†ç¡®å®šä½æ°´ä¸‹å®ä¾‹ï¼Œä»¥å®ç°é«˜æ•ˆçš„åˆ†å‰²ã€‚ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹éå¸¸æœ‰æ•ˆï¼Œåœ¨å¤šä¸ªæ°´ä¸‹å®ä¾‹æ•°æ®é›†ä¸Šå®ç°äº†å¯¹æœ€å…ˆè¿›æ–¹æ³•çš„å¤§å¹…æ€§èƒ½æå‡ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/LiamLian0727/UIIS10K%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/LiamLian0727/UIIS10Kè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15581v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SAMæ¨¡å‹åœ¨å¤šç§è§†è§‰åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨æ°´ä¸‹å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å—é™ã€‚ä¸ºåº”å¯¹æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤§è§„æ¨¡æ°´ä¸‹å®ä¾‹åˆ†å‰²æ•°æ®é›†UIIS10Kï¼ŒåŒ…å«10,048å¼ å¸¦åƒç´ çº§æ ‡æ³¨çš„å›¾åƒã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“ä¸ºæ°´ä¸‹å®ä¾‹è‡ªåŠ¨å‡†ç¡®åˆ†å‰²è®¾è®¡çš„UWSAMæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡Mask GATåŸºç¡€æ°´ä¸‹çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œä»SAMçš„ViT-Hugeå›¾åƒç¼–ç å™¨æç‚¼çŸ¥è¯†ï¼Œç”¨äºæœ‰æ•ˆè§†è§‰è¡¨å¾å­¦ä¹ ã€‚æˆ‘ä»¬è¿˜ä¸ºUWSAMè®¾è®¡äº†ç«¯åˆ°ç«¯æ°´ä¸‹æç¤ºç”Ÿæˆå™¨ï¼ˆEUPGï¼‰ï¼Œå¯è‡ªåŠ¨ç”Ÿæˆæ°´ä¸‹æç¤ºï¼Œæ— éœ€æ˜ç¡®æä¾›å‰æ™¯ç‚¹æˆ–æ¡†ï¼Œä»è€Œæé«˜ç½‘ç»œåœ¨æ°´ä¸‹å®ä¾‹å®šä½çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªæ°´ä¸‹å®ä¾‹æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAMæ¨¡å‹åœ¨è§†è§‰åº”ç”¨ä¸­æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨æ°´ä¸‹å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å—é™ï¼Œç¼ºä¹æ°´ä¸‹é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>æå‡ºäº†å¤§è§„æ¨¡æ°´ä¸‹å®ä¾‹åˆ†å‰²æ•°æ®é›†UIIS10Kï¼ŒåŒ…å«10,048å¼ å¸¦åƒç´ çº§æ ‡æ³¨çš„å›¾åƒï¼Œä¸ºæ°´ä¸‹å®ä¾‹åˆ†å‰²ç ”ç©¶æä¾›æ•°æ®æ”¯æŒã€‚</li>
<li>å¼•å…¥äº†UWSAMæ¨¡å‹ï¼Œä¸“ä¸ºæ°´ä¸‹å®ä¾‹è‡ªåŠ¨å‡†ç¡®åˆ†å‰²è®¾è®¡ã€‚</li>
<li>UWSAMé€šè¿‡Mask GATåŸºç¡€æ°´ä¸‹çŸ¥è¯†è’¸é¦æ–¹æ³•æç‚¼çŸ¥è¯†ï¼Œæé«˜è§†è§‰è¡¨å¾å­¦ä¹ æ•ˆæœã€‚</li>
<li>ç«¯åˆ°ç«¯æ°´ä¸‹æç¤ºç”Ÿæˆå™¨ï¼ˆEUPGï¼‰çš„å¼•å…¥ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæ°´ä¸‹æç¤ºï¼Œæé«˜ç½‘ç»œåœ¨æ°´ä¸‹å®ä¾‹å®šä½çš„å‡†ç¡®æ€§ã€‚</li>
<li>UWSAMæ¨¡å‹åœ¨å¤šä¸ªæ°´ä¸‹å®ä¾‹æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€ï¼Œä¾¿äºç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d78a50051f63d531fda21cdb6783071" align="middle">
<img src="https://picx.zhimg.com/v2-e91ca0fa49ab6cc348f37002c35a8f86" align="middle">
<img src="https://picx.zhimg.com/v2-0d1577acb1b9089f418810a328be399e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e236095838fc56aa69cea1c8cc259276" align="middle">
<img src="https://picx.zhimg.com/v2-7a91b39393a16a3d55d80e35c78ca89a" align="middle">
<img src="https://picx.zhimg.com/v2-c6a0d5b5490b8a8af3e9b72fac8e335e" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MIAFEx-An-Attention-based-Feature-Extraction-Method-for-Medical-Image-Classification"><a href="#MIAFEx-An-Attention-based-Feature-Extraction-Method-for-Medical-Image-Classification" class="headerlink" title="MIAFEx: An Attention-based Feature Extraction Method for Medical Image   Classification"></a>MIAFEx: An Attention-based Feature Extraction Method for Medical Image   Classification</h2><p><strong>Authors:Oscar Ramos-Soto, Jorge Ramos-Frutos, Ezequiel Perez-Zarate, Diego Oliva, Sandra E. Balderas-Mata</strong></p>
<p>Feature extraction techniques are crucial in medical image classification; however, classical feature extractors, in addition to traditional machine learning classifiers, often exhibit significant limitations in providing sufficient discriminative information for complex image sets. While Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) have shown promise in feature extraction, they are prone to overfitting due to the inherent characteristics of medical imaging data, including small sample sizes or high intra-class variance. In this work, the Medical Image Attention-based Feature Extractor (MIAFEx) is proposed, a novel method that employs a learnable refinement mechanism to enhance the classification token within the Transformer encoder architecture. This mechanism adjusts the token based on learned weights, improving the extraction of salient features and enhancing the modelâ€™s adaptability to the challenges presented by medical imaging data. The MIAFEx output feature quality is compared against classical feature extractors using traditional and hybrid classifiers. Also, the performance of these features is compared against modern CNN and ViT models in classification tasks, demonstrating their superiority in accuracy and robustness across multiple complex medical imaging datasets. This advantage is particularly pronounced in scenarios with limited training data, where traditional and modern models often struggle to generalize effectively. The source code of this proposal can be found at <a target="_blank" rel="noopener" href="https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx">https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx</a> </p>
<blockquote>
<p>ç‰¹å¾æå–æŠ€æœ¯åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­è‡³å…³é‡è¦ï¼›ç„¶è€Œï¼Œé™¤äº†ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ åˆ†ç±»å™¨å¤–ï¼Œç»å…¸çš„ç‰¹å¾æå–å™¨åœ¨å¤„ç†å¤æ‚å›¾åƒé›†æ—¶ï¼Œåœ¨æä¾›è¶³å¤Ÿçš„åˆ¤åˆ«ä¿¡æ¯æ–¹é¢å¾€å¾€å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰åœ¨ç‰¹å¾æå–æ–¹é¢å·²æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç”±äºåŒ»å­¦æˆåƒæ•°æ®å†…åœ¨çš„ç‰¹æ€§ï¼ˆå¦‚æ ·æœ¬é‡å°æˆ–ç±»å†…æ–¹å·®é«˜ï¼‰ï¼Œå®ƒä»¬å®¹æ˜“å‘ç”Ÿè¿‡æ‹Ÿåˆã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæå‡ºäº†ä¸€ç§åŒ»å­¦å›¾åƒæ³¨æ„åŠ›ç‰¹å¾æå–å™¨ï¼ˆMIAFExï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œé‡‡ç”¨å¯å­¦ä¹ çš„ç»†åŒ–æœºåˆ¶æ¥å¢å¼ºè½¬æ¢å™¨ç¼–ç å™¨æ¶æ„ä¸­çš„åˆ†ç±»æ ‡è®°ã€‚è¯¥æœºåˆ¶æ ¹æ®å­¦ä¹ åˆ°çš„æƒé‡è°ƒæ•´æ ‡è®°ï¼Œæé«˜æ˜¾è‘—ç‰¹å¾çš„æå–èƒ½åŠ›ï¼Œå¢å¼ºæ¨¡å‹å¯¹åŒ»å­¦æˆåƒæ•°æ®æ‰€å‘ˆç°æŒ‘æˆ˜çš„é€‚åº”æ€§ã€‚å°†MIAFExçš„è¾“å‡ºç‰¹å¾è´¨é‡ä¸ä½¿ç”¨ä¼ ç»Ÿå’Œæ··åˆåˆ†ç±»å™¨çš„ç»å…¸ç‰¹å¾æå–å™¨è¿›è¡Œæ¯”è¾ƒã€‚æ­¤å¤–ï¼Œè¿˜æ¯”è¾ƒäº†è¿™äº›ç‰¹å¾åœ¨ç°ä»£CNNå’ŒViTæ¨¡å‹ä¸­çš„åˆ†ç±»ä»»åŠ¡æ€§èƒ½ï¼Œåœ¨å¤šä¸ªå¤æ‚çš„åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¯æ˜äº†å…¶åœ¨å‡†ç¡®æ€§å’Œç¨³å¥æ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚è¿™ä¸€ä¼˜åŠ¿åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹å°¤ä¸ºçªå‡ºï¼Œä¼ ç»Ÿå’Œç°ä»£æ¨¡å‹åœ¨è¿™ç§æƒ…å†µä¸‹å¾€å¾€éš¾ä»¥æœ‰æ•ˆåœ°æ¨å¹¿ã€‚è¯¥ææ¡ˆçš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFExæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08562v2">PDF</a> This is the preprint version of an article that has been accepted for   publication in Knowledge-Based Systems</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»ç–—å›¾åƒåˆ†ç±»ä¸­ç‰¹å¾æå–æŠ€æœ¯çš„é‡è¦æ€§ã€‚é’ˆå¯¹ä¼ ç»Ÿç‰¹å¾æå–å™¨å’Œæœºå™¨å­¦ä¹ åˆ†ç±»å™¨åœ¨å¤„ç†å¤æ‚å›¾åƒé›†æ—¶æä¾›çš„åˆ¤åˆ«ä¿¡æ¯ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºæ³¨æ„åŠ›çš„åŒ»ç–—å›¾åƒç‰¹å¾æå–å™¨ï¼ˆMIAFExï¼‰ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¯å­¦ä¹ çš„ç»†åŒ–æœºåˆ¶ï¼Œæ ¹æ®å­¦ä¹ åˆ°çš„æƒé‡è°ƒæ•´åˆ†ç±»ä»¤ç‰Œï¼Œæé«˜äº†æ˜¾è‘—ç‰¹å¾çš„æå–èƒ½åŠ›ï¼Œå¹¶å¢å¼ºäº†æ¨¡å‹å¯¹åŒ»ç–—å›¾åƒæ•°æ®æŒ‘æˆ˜çš„é€‚åº”æ€§ã€‚MIAFExçš„ç‰¹å¾è¾“å‡ºè´¨é‡åœ¨å¤šä¸ªå¤æ‚çš„åŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šè¢«è¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†ç›¸è¾ƒäºä¼ ç»Ÿå’Œç°ä»£æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å›¾åƒåˆ†ç±»ä¸­ç‰¹å¾æå–æŠ€æœ¯è‡³å…³é‡è¦ï¼Œä½†ä¼ ç»Ÿç‰¹å¾æå–å™¨å’Œæœºå™¨å­¦ä¹ åˆ†ç±»å™¨å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Convolutional Neural Networks (CNNs) å’Œ Vision Transformer (ViT) åœ¨ç‰¹å¾æå–æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ä»å­˜åœ¨è¿‡æ‹Ÿåˆé£é™©ã€‚</li>
<li>æå‡ºçš„ Medical Image Attention-based Feature Extractor (MIAFEx) é‡‡ç”¨å¯å­¦ä¹ ç»†åŒ–æœºåˆ¶ï¼Œæ”¹è¿›åˆ†ç±»ä»¤ç‰Œçš„æå–ã€‚</li>
<li>MIAFEx èƒ½æé«˜æ˜¾è‘—ç‰¹å¾çš„æå–èƒ½åŠ›ï¼Œå¹¶å¢å¼ºæ¨¡å‹å¯¹åŒ»ç–—å›¾åƒæ•°æ®æŒ‘æˆ˜çš„é€‚åº”æ€§ã€‚</li>
<li>MIAFEx ç‰¹å¾è¾“å‡ºè´¨é‡åœ¨å¤šä¸ªåŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šå¾—åˆ°è¯„ä¼°ï¼Œè¡¨ç°å‡ºä¼˜äºä¼ ç»Ÿå’Œç°ä»£æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>ç‰¹åˆ«åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼ŒMIAFEx çš„ä¼˜åŠ¿æ›´ä¸ºæ˜æ˜¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f284d3e55af2dce07ffe7871bd87160" align="middle">
<img src="https://pic1.zhimg.com/v2-2caa5136a9f3f5d46967b9c9a449f40c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc8fea52bb5c10ae6a0ce3041de773d3" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e91ca0fa49ab6cc348f37002c35a8f86" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  Evaluation of Polarimetric Fusion for Semantic Segmentation in Aquatic   Environments
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-346e37c305d77c6f6fe037bee9124c7f.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  Perceive, Reflect and Understand Long Video Progressive Multi-Granular   Clue Exploration with Interactive Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
