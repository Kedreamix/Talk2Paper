<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  UniLat3D Geometry-Appearance Unified Latents for Single-Stage 3D   Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-eced55fcc4237184017a848e99237130~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930630&auth_key=1759930630-0-0-9a791dba09b3113af20bd0dc1a470e11&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-01-æ›´æ–°"><a href="#2025-10-01-æ›´æ–°" class="headerlink" title="2025-10-01 æ›´æ–°"></a>2025-10-01 æ›´æ–°</h1><h2 id="UniLat3D-Geometry-Appearance-Unified-Latents-for-Single-Stage-3D-Generation"><a href="#UniLat3D-Geometry-Appearance-Unified-Latents-for-Single-Stage-3D-Generation" class="headerlink" title="UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D   Generation"></a>UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D   Generation</h2><p><strong>Authors:Guanjun Wu, Jiemin Fang, Chen Yang, Sikuang Li, Taoran Yi, Jia Lu, Zanwei Zhou, Jiazhong Cen, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Xinggang Wang, Qi Tian</strong></p>
<p>High-fidelity 3D asset generation is crucial for various industries. While recent 3D pretrained models show strong capability in producing realistic content, most are built upon diffusion models and follow a two-stage pipeline that first generates geometry and then synthesizes appearance. Such a decoupled design tends to produce geometry-texture misalignment and non-negligible cost. In this paper, we propose UniLat3D, a unified framework that encodes geometry and appearance in a single latent space, enabling direct single-stage generation. Our key contribution is a geometry-appearance Unified VAE, which compresses high-resolution sparse features into a compact latent representation â€“ UniLat. UniLat integrates structural and visual information into a dense low-resolution latent, which can be efficiently decoded into diverse 3D formats, e.g., 3D Gaussians and meshes. Based on this unified representation, we train a single flow-matching model to map Gaussian noise directly into UniLat, eliminating redundant stages. Trained solely on public datasets, UniLat3D produces high-quality 3D assets in seconds from a single image, achieving superior appearance fidelity and geometric quality. More demos &amp; code are available at <a target="_blank" rel="noopener" href="https://unilat3d.github.io/">https://unilat3d.github.io/</a> </p>
<blockquote>
<p>é«˜ä¿çœŸ3Dèµ„äº§ç”Ÿæˆå¯¹äºå„è¡Œå„ä¸šæ¥è¯´è‡³å…³é‡è¦ã€‚è™½ç„¶æœ€è¿‘çš„3Dé¢„è®­ç»ƒæ¨¡å‹åœ¨ç”ŸæˆçœŸå®å†…å®¹æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å¤§å¤šæ•°åŸºäºæ‰©æ•£æ¨¡å‹ï¼Œå¹¶éµå¾ªä¸€ä¸ªä¸¤é˜¶æ®µçš„ç®¡é“ï¼Œé¦–å…ˆç”Ÿæˆå‡ ä½•ç»“æ„ï¼Œç„¶ååˆæˆå¤–è§‚ã€‚è¿™ç§è§£è€¦çš„è®¾è®¡å¾€å¾€ä¼šå¯¼è‡´å‡ ä½•çº¹ç†ä¸å¯¹é½å’Œä¸å¯å¿½è§†çš„æˆæœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†UniLat3Dï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå°†å‡ ä½•å’Œå¤–è§‚ç¼–ç åˆ°å•ä¸ªæ½œåœ¨ç©ºé—´ä¸­ï¼Œå®ç°ç›´æ¥çš„å•é˜¶æ®µç”Ÿæˆã€‚æˆ‘ä»¬çš„å…³é”®è´¡çŒ®æ˜¯å‡ ä½•å¤–è§‚ç»Ÿä¸€å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆUnified VAEï¼‰ï¼Œå®ƒå°†é«˜åˆ†è¾¨ç‡ç¨€ç–ç‰¹å¾å‹ç¼©æˆç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºâ€”â€”UniLatã€‚UniLatå°†ç»“æ„å’Œè§†è§‰ä¿¡æ¯é›†æˆåˆ°å¯†é›†çš„ä½åˆ†è¾¨ç‡æ½œåœ¨ç©ºé—´ä¸­ï¼Œå¯ä»¥é«˜æ•ˆåœ°è§£ç ä¸ºå¤šç§3Dæ ¼å¼ï¼Œä¾‹å¦‚3Dé«˜æ–¯å’Œç½‘æ ¼ã€‚åŸºäºè¿™ç§ç»Ÿä¸€è¡¨ç¤ºï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå•ä¸€æµé‡åŒ¹é…æ¨¡å‹ï¼Œç›´æ¥å°†é«˜æ–¯å™ªå£°æ˜ å°„åˆ°UniLatï¼Œæ¶ˆé™¤äº†å†—ä½™é˜¶æ®µã€‚ä»…é€šè¿‡å…¬å…±æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼ŒUniLat3Då¯åœ¨å‡ ç§’é’Ÿå†…ç”Ÿæˆé«˜è´¨é‡3Dèµ„äº§ï¼Œä»å•å¼ å›¾åƒå¼€å§‹ï¼Œå®ç°å“è¶Šçš„å¤–è§‚ä¿çœŸåº¦å’Œå‡ ä½•è´¨é‡ã€‚æ›´å¤šæ¼”ç¤ºå’Œä»£ç å¯è§äº<a target="_blank" rel="noopener" href="https://unilat3d.github.io/%E3%80%82">https://unilat3d.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25079v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://unilat3d.github.io/">https://unilat3d.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºUniLat3Dçš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºé«˜ä¿çœŸ3Dèµ„äº§ç”Ÿæˆã€‚è¯¥æ¡†æ¶å°†å‡ ä½•å’Œå¤–è§‚ä¿¡æ¯ç¼–ç åœ¨å•ä¸ªæ½œåœ¨ç©ºé—´ä¸­ï¼Œå®ç°ç›´æ¥å•é˜¶æ®µç”Ÿæˆã€‚å…¶æ ¸å¿ƒè´¡çŒ®æ˜¯å‡ ä½•å¤–è§‚ç»Ÿä¸€VAEï¼Œå¯å°†é«˜åˆ†è¾¨ç‡ç¨€ç–ç‰¹å¾å‹ç¼©æˆç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºå½¢å¼â€”â€”UniLatã€‚åŸºäºè¿™ç§ç»Ÿä¸€è¡¨ç¤ºï¼Œè®­ç»ƒäº†ä¸€ä¸ªå•ä¸€æµç¨‹åŒ¹é…æ¨¡å‹ï¼Œç›´æ¥å°†é«˜æ–¯å™ªå£°æ˜ å°„åˆ°UniLatï¼Œæ¶ˆé™¤äº†å†—ä½™é˜¶æ®µã€‚è¯¥æ¡†æ¶åœ¨å…¬å¼€æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œèƒ½åœ¨å‡ ç§’å†…ç”Ÿæˆé«˜è´¨é‡3Dèµ„äº§ï¼Œå®ç°å‡ºè‰²çš„å¤–è§‚ä¿çœŸåº¦å’Œå‡ ä½•è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniLat3Dæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºé«˜ä¿çœŸ3Dèµ„äº§ç”Ÿæˆï¼Œå¯å°†å‡ ä½•å’Œå¤–è§‚ä¿¡æ¯æ•´åˆåœ¨å•ä¸ªæ½œåœ¨ç©ºé—´ä¸­ã€‚</li>
<li>UniLatæ˜¯æ ¸å¿ƒç»„ä»¶ï¼Œå°†é«˜åˆ†è¾¨ç‡ç¨€ç–ç‰¹å¾å‹ç¼©æˆç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºå½¢å¼ã€‚</li>
<li>UniLatå°†ç»“æ„å’Œè§†è§‰ä¿¡æ¯é›†æˆåˆ°å¯†é›†çš„ä½åˆ†è¾¨ç‡æ½œåœ¨ç©ºé—´ä¸­ã€‚</li>
<li>è¯¥æ¡†æ¶å®ç°äº†ç›´æ¥å•é˜¶æ®µç”Ÿæˆï¼Œé¿å…äº†ä¼ ç»Ÿä¸¤é˜¶æ®µç®¡é“è®¾è®¡çš„ç¼ºç‚¹ã€‚</li>
<li>é€šè¿‡è®­ç»ƒå•ä¸€æµç¨‹åŒ¹é…æ¨¡å‹ï¼Œç›´æ¥å°†é«˜æ–¯å™ªå£°æ˜ å°„åˆ°UniLatè¡¨ç¤ºä¸­ã€‚</li>
<li>UniLat3Dåœ¨å…¬å¼€æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œç”Ÿæˆé«˜è´¨é‡3Dèµ„äº§çš„é€Ÿåº¦å¿«ï¼Œä»…éœ€å‡ ç§’é’Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c303915045f5a5163ee396e617e22a3b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930637&auth_key=1759930637-0-0-e5c033cc17c6060b61dbda3388af4d1e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-31ef34c882dbd83d6ff8e68861d37153~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930645&auth_key=1759930645-0-0-216e6e34106bf8fa3f1818b8e8290d60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f542e5833bd50246643af21db7ac0ad3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930652&auth_key=1759930652-0-0-77798ed45df6f83da6e9010b81b50a12&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-162cd654412d64b1cc62e3a51962b6f2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930658&auth_key=1759930658-0-0-02bf1398eba18ffa97a58838c637eb71&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CharGen-Fast-and-Fluent-Portrait-Modification"><a href="#CharGen-Fast-and-Fluent-Portrait-Modification" class="headerlink" title="CharGen: Fast and Fluent Portrait Modification"></a>CharGen: Fast and Fluent Portrait Modification</h2><p><strong>Authors:Jan-Niklas Dihlmann, Arnela Killguss, Hendrik P. A. Lensch</strong></p>
<p>Interactive editing of character images with diffusion models remains challenging due to the inherent trade-off between fine-grained control, generation speed, and visual fidelity. We introduce CharGen, a character-focused editor that combines attribute-specific Concept Sliders, trained to isolate and manipulate attributes such as facial feature size, expression, and decoration with the StreamDiffusion sampling pipeline for more interactive performance. To counteract the loss of detail that often accompanies accelerated sampling, we propose a lightweight Repair Step that reinstates fine textures without compromising structural consistency. Throughout extensive ablation studies and in comparison to open-source InstructPix2Pix and closed-source Google Gemini, and a comprehensive user study, CharGen achieves two-to-four-fold faster edit turnaround with precise editing control and identity-consistent results. Project page: <a target="_blank" rel="noopener" href="https://chargen.jdihlmann.com/">https://chargen.jdihlmann.com/</a> </p>
<blockquote>
<p>åŸºäºæ‰©æ•£æ¨¡å‹å¯¹å­—ç¬¦å›¾åƒè¿›è¡Œäº¤äº’ç¼–è¾‘ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç²¾ç»†æ§åˆ¶ã€ç”Ÿæˆé€Ÿåº¦å’Œè§†è§‰ä¿çœŸåº¦ä¹‹é—´å­˜åœ¨ç€å›ºæœ‰çš„æƒè¡¡ã€‚æˆ‘ä»¬å¼•å…¥äº†CharGenï¼Œè¿™æ˜¯ä¸€æ¬¾é¢å‘å­—ç¬¦çš„ç¼–è¾‘å™¨ï¼Œå®ƒç»“åˆäº†å±æ€§ç‰¹å®šçš„æ¦‚å¿µæ»‘å—ï¼ˆConcept Slidersï¼‰ï¼Œé€šè¿‡è®­ç»ƒæ¥éš”ç¦»å’Œæ“ä½œè¯¸å¦‚é¢éƒ¨ç‰¹å¾å¤§å°ã€è¡¨æƒ…å’Œè£…é¥°ç­‰å±æ€§ï¼Œå¹¶ä½¿ç”¨StreamDiffusioné‡‡æ ·ç®¡é“æ¥æé«˜äº¤äº’æ€§èƒ½ã€‚ä¸ºäº†æŠµæ¶ˆåŠ é€Ÿé‡‡æ ·é€šå¸¸ä¼´éšçš„ç»†èŠ‚æŸå¤±ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„ä¿®å¤æ­¥éª¤ï¼ˆRepair Stepï¼‰ï¼Œå¯ä»¥åœ¨ä¸æŸå®³ç»“æ„ä¸€è‡´æ€§çš„æƒ…å†µä¸‹é‡æ–°å»ºç«‹ç»†è…»çº¹ç†ã€‚é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶ã€ä¸å¼€æºçš„InstructPix2Pixå’Œé—­æºçš„Google Geminiçš„å¯¹æ¯”ï¼Œä»¥åŠå…¨é¢çš„ç”¨æˆ·ç ”ç©¶ï¼ŒCharGenå®ç°äº†ä¸¤åˆ°å››å€çš„å¿«é€Ÿç¼–è¾‘å‘¨è½¬æ—¶é—´ï¼Œå…·æœ‰ç²¾ç¡®çš„ç¼–è¾‘æ§åˆ¶å’Œèº«ä»½ä¸€è‡´æ€§ç»“æœã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://chargen.jdihlmann.com/%EF%BC%88https://chargen.jdihlmann.com/%EF%BC%89">https://chargen.jdihlmann.com/ï¼ˆhttps://chargen.jdihlmann.com/ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25058v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://chargen.jdihlmann.com/">https://chargen.jdihlmann.com/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„å­—ç¬¦å›¾åƒäº¤äº’ç¼–è¾‘ä»é¢ä¸´ç²¾ç»†æ§åˆ¶ã€ç”Ÿæˆé€Ÿåº¦å’Œè§†è§‰ä¿çœŸåº¦ä¹‹é—´çš„å›ºæœ‰æƒè¡¡æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºCharGenè¿™ä¸€é¢å‘å­—ç¬¦çš„ç¼–è¾‘å™¨ï¼Œå®ƒç»“åˆäº†å±æ€§ç‰¹å®šçš„æ¦‚å¿µæ»‘å—ï¼Œé€šè¿‡è®­ç»ƒæ¥éš”ç¦»å’Œæ“ä½œå¦‚é¢éƒ¨ç‰¹å¾å¤§å°ã€è¡¨æƒ…å’Œè£…é¥°ç­‰å±æ€§ï¼Œå¹¶é‡‡ç”¨StreamDiffusioné‡‡æ ·ç®¡é“ä»¥æé«˜äº¤äº’æ€§èƒ½ã€‚ä¸ºäº†æŠµæ¶ˆåŠ é€Ÿé‡‡æ ·å¸¦æ¥çš„ç»†èŠ‚æŸå¤±ï¼Œæˆ‘ä»¬æå‡ºäº†è½»é‡çº§çš„ä¿®å¤æ­¥éª¤ï¼Œä»¥æ¢å¤ç²¾ç»†çº¹ç†è€Œä¸æŸå®³ç»“æ„ä¸€è‡´æ€§ã€‚é€šè¿‡å¹¿æ³›æ¶ˆèç ”ç©¶ä»¥åŠä¸å¼€æºInstructPix2Pixå’Œé—­æºGoogle Geminiçš„å¯¹æ¯”ä»¥åŠå…¨é¢çš„ç”¨æˆ·ç ”ç©¶ï¼ŒCharGenå®ç°äº†ä¸¤åˆ°å››å€çš„å¿«é€Ÿç¼–è¾‘å‘¨è½¬ï¼Œå…·æœ‰ç²¾ç¡®çš„ç¼–è¾‘æ§åˆ¶å’Œèº«ä»½ä¸€è‡´æ€§ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å­—ç¬¦å›¾åƒäº¤äº’ç¼–è¾‘ä¸­é¢ä¸´ç²¾ç»†æ§åˆ¶ã€ç”Ÿæˆé€Ÿåº¦å’Œè§†è§‰ä¿çœŸåº¦çš„æŒ‘æˆ˜ã€‚</li>
<li>CharGenæ˜¯ä¸€ä¸ªé¢å‘å­—ç¬¦çš„ç¼–è¾‘å™¨ï¼Œé€šè¿‡ç»“åˆå±æ€§ç‰¹å®šçš„æ¦‚å¿µæ»‘å—æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>æ¦‚å¿µæ»‘å—ç»è¿‡è®­ç»ƒï¼Œå¯ä»¥éš”ç¦»å’Œæ“çºµå¦‚é¢éƒ¨ç‰¹å¾å¤§å°ã€è¡¨æƒ…å’Œè£…é¥°ç­‰å±æ€§ã€‚</li>
<li>CharGené‡‡ç”¨StreamDiffusioné‡‡æ ·ç®¡é“ï¼Œæé«˜äº¤äº’æ€§èƒ½ã€‚</li>
<li>ä¸ºäº†å¼¥è¡¥åŠ é€Ÿé‡‡æ ·å¯èƒ½å¯¼è‡´çš„ç»†èŠ‚æŸå¤±ï¼ŒCharGenæå‡ºäº†è½»é‡çº§çš„ä¿®å¤æ­¥éª¤ã€‚</li>
<li>CharGenå®ç°äº†å¿«é€Ÿç¼–è¾‘å‘¨è½¬ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰ç²¾ç¡®çš„ç¼–è¾‘æ§åˆ¶å’Œèº«ä»½ä¸€è‡´æ€§ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25058">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4f8cd53854c213cbc804dca63cf9f2c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930665&auth_key=1759930665-0-0-511ed43e917f628293a970296cf30278&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4850c230c26c60eb29ae0eb465ea6aec~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930672&auth_key=1759930672-0-0-09510bd666fc780027b7d02267cb4235&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3d465a971ffea6a47e27ba2dd5ff285f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930679&auth_key=1759930679-0-0-97432c3d06428455a3b771ba3691d196&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-7810d7bb7770660bb6a28d750070fe4b.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d1011b69de0d2026743164949eb7c51~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930694&auth_key=1759930694-0-0-4ca0c9b379a1ae557ac398b43733b41f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-0073ebec0a8e6d7881797b4631889356.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SDPose-Exploiting-Diffusion-Priors-for-Out-of-Domain-and-Robust-Pose-Estimation"><a href="#SDPose-Exploiting-Diffusion-Priors-for-Out-of-Domain-and-Robust-Pose-Estimation" class="headerlink" title="SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose   Estimation"></a>SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose   Estimation</h2><p><strong>Authors:Shuang Liang, Jing He, Chuanmeizhi Wang, Lejun Liao, Guo Zhang, Yingcong Chen, Yuan Yuan</strong></p>
<p>Pre-trained diffusion models provide rich multi-scale latent features and are emerging as powerful vision backbones. While recent works such as Marigold<del>\citep{ke2024repurposing} and Lotus</del>\citep{he2024lotus} adapt diffusion priors for dense prediction with strong cross-domain generalization, their potential for structured outputs (e.g., human pose estimation) remains underexplored. In this paper, we propose \textbf{SDPose}, a fine-tuning framework built upon Stable Diffusion to fully exploit pre-trained diffusion priors for human pose estimation. First, rather than modifying cross-attention modules or introducing learnable embeddings, we directly predict keypoint heatmaps in the SD U-Netâ€™s image latent space to preserve the original generative priors. Second, we map these latent features into keypoint heatmaps through a lightweight convolutional pose head, which avoids disrupting the pre-trained backbone. Finally, to prevent overfitting and enhance out-of-distribution robustness, we incorporate an auxiliary RGB reconstruction branch that preserves domain-transferable generative semantics. To evaluate robustness under domain shift, we further construct \textbf{COCO-OOD}, a style-transferred variant of COCO with preserved annotations. With just one-fifth of the training schedule used by Sapiens on COCO, SDPose attains parity with Sapiens-1B&#x2F;2B on the COCO validation set and establishes a new state of the art on the cross-domain benchmarks HumanArt and COCO-OOD. Furthermore, we showcase SDPose as a zero-shot pose annotator for downstream controllable generation tasks, including ControlNet-based image synthesis and video generation, where it delivers qualitatively superior pose guidance. </p>
<blockquote>
<p>é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æä¾›äº†ä¸°å¯Œçš„å¤šå°ºåº¦æ½œåœ¨ç‰¹å¾ï¼Œå¹¶ä½œä¸ºå¼ºå¤§çš„è§†è§‰ä¸»å¹²æ­£é€æ¸å´­éœ²å¤´è§’ã€‚è™½ç„¶æœ€è¿‘çš„Marigoldï¼ˆKeç­‰ï¼Œ2024å¹´ï¼‰å’ŒLotusï¼ˆHeç­‰ï¼Œ2024å¹´ï¼‰ç­‰ä½œå“é€‚åº”äº†æ‰©æ•£å…ˆéªŒè¿›è¡Œå¯†é›†é¢„æµ‹ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œä½†å®ƒä»¬å¯¹äºç»“æ„åŒ–è¾“å‡ºï¼ˆä¾‹å¦‚äººä½“å§¿æ€ä¼°è®¡ï¼‰çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºStable Diffusionæ„å»ºçš„å¾®è°ƒæ¡†æ¶SDPoseï¼Œä»¥å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£å…ˆéªŒè¿›è¡Œäººä½“å§¿æ€ä¼°è®¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸æ˜¯åœ¨äº¤å‰æ³¨æ„åŠ›æ¨¡å—ä¸­è¿›è¡Œä¿®æ”¹æˆ–å¼•å…¥å¯å­¦ä¹ åµŒå…¥ï¼Œè€Œæ˜¯åœ¨SD U-Netçš„å›¾åƒæ½œåœ¨ç©ºé—´ä¸­ç›´æ¥é¢„æµ‹å…³é”®ç‚¹çƒ­å›¾ï¼Œä»¥ä¿ç•™åŸå§‹çš„ç”Ÿæˆå…ˆéªŒã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡è½»é‡çº§çš„å·ç§¯å§¿æ€å¤´å°†è¿™äº›æ½œåœ¨ç‰¹å¾æ˜ å°„åˆ°å…³é”®ç‚¹çƒ­å›¾ï¼Œè¿™é¿å…äº†ç ´åé¢„è®­ç»ƒçš„ä¸»å¹²ç½‘ç»œã€‚æœ€åï¼Œä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆå¹¶å¢å¼ºåˆ†å¸ƒå¤–çš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¾…åŠ©RGBé‡å»ºåˆ†æ”¯ï¼Œè¯¥åˆ†æ”¯èƒ½å¤Ÿä¿ç•™åŸŸå¯è¿ç§»çš„ç”Ÿæˆè¯­ä¹‰ã€‚ä¸ºäº†è¯„ä¼°åœ¨åŸŸåç§»ä¸‹çš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ„å»ºäº†COCO-OODæ•°æ®é›†ï¼Œè¿™æ˜¯å…·æœ‰ä¿ç•™æ³¨é‡Šçš„é£æ ¼è½¬ç§»COCOæ•°æ®é›†ã€‚ä»…ä½¿ç”¨Sapiensåœ¨COCOä¸Šè®­ç»ƒæ—¶é—´çš„äº”åˆ†ä¹‹ä¸€æ—¶é—´è¡¨å’Œè¶…å‚è®¾ç½®çš„å‰æä¸‹ï¼ŒSDPoseå®ç°äº†ä¸Sapiens-1B&#x2F;2Bç›¸å½“çš„COCOéªŒè¯é›†æ€§èƒ½å¹¶è¾¾åˆ°äº†äººä½“å§¿æ€ä¼°è®¡è·¨åŸŸåŸºå‡†æµ‹è¯•çš„å‰æ²¿æ°´å¹³ï¼ŒåŒ…æ‹¬HumanArtå’ŒCOCO-OODç­‰åŸºå‡†æµ‹è¯•å‡åˆ›ä¸‹äº†æœ€æ–°ä¸–ç•Œçºªå½•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†SDPoseå¯ä½œä¸ºé’ˆå¯¹ä¸‹æ¸¸å¯æ§ç”Ÿæˆä»»åŠ¡çš„é›¶æ ·æœ¬å§¿æ€æ ‡æ³¨å™¨ï¼ŒåŒ…æ‹¬åŸºäºControlNetçš„å›¾åƒåˆæˆå’Œè§†é¢‘ç”Ÿæˆç­‰ä»»åŠ¡åœºæ™¯ä¸‹çš„å®é™…åº”ç”¨ç¤ºä¾‹è¯æ˜äº†å…¶åœ¨å§¿åŠ¿æŒ‡å¯¼æ–¹é¢å…·æœ‰è‰¯å¥½çš„æ€§èƒ½ä¼˜åŠ¿ã€‚åœ¨è¿™ä¸€åº”ç”¨åœºæ™¯ä¸‹å‡¸æ˜¾å…¶å§¿æ€å¼•å¯¼çš„å‡ºè‰²æ€§èƒ½å’Œå¯é æ€§ï¼Œä¸ä»…åœ¨ä»¿çœŸæ¨¡æ‹Ÿä¸­æœ‰å®ç”¨ä»·å€¼ä¸”åœ¨é¢å‘å¤æ‚ç°å®ä¸–ç•Œç¯å¢ƒçš„å¯åº”ç”¨èŒƒå›´å†…ä½“ç°æ˜æ˜¾ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24980v1">PDF</a> 18 pages, 9 figures, 9 tables</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å…·å¤‡å¤šå°ºåº¦æ½œåœ¨ç‰¹å¾ï¼Œæ­£æˆä¸ºå¼ºå¤§çš„è§†è§‰ä¸»å¹²ã€‚æœ¬ç ”ç©¶æå‡ºSDPoseæ¡†æ¶ï¼ŒåŸºäºStable Diffusionè¿›è¡Œå¾®è°ƒï¼Œå……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£å…ˆéªŒè¿›è¡Œäººä½“å§¿æ€ä¼°è®¡ã€‚é€šè¿‡ç›´æ¥åœ¨SD U-Netçš„å›¾åƒæ½œåœ¨ç©ºé—´ä¸­é¢„æµ‹å…³é”®ç‚¹çƒ­å›¾ï¼Œå¹¶æ˜ å°„è¿™äº›æ½œåœ¨ç‰¹å¾åˆ°å…³é”®ç‚¹çƒ­å›¾ï¼Œå®ç°å§¿æ€ä¼°è®¡ã€‚ä¸ºæé«˜é²æ£’æ€§å’Œè·¨åŸŸæ€§èƒ½ï¼Œå¼•å…¥è¾…åŠ©RGBé‡å»ºåˆ†æ”¯ã€‚å®éªŒæ˜¾ç¤ºSDPoseåœ¨COCOéªŒè¯é›†ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¹¶åœ¨è·¨åŸŸåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼ŒSDPoseè¿˜å¯ä½œä¸ºä¸‹æ¸¸å¯æ§ç”Ÿæˆä»»åŠ¡çš„é›¶æ ·æœ¬å§¿æ€æ ‡æ³¨å™¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å…·å¤‡å¤šå°ºåº¦æ½œåœ¨ç‰¹å¾ï¼Œæ˜¯å¼ºå¤§çš„è§†è§‰ä¸»å¹²ã€‚</li>
<li>SDPoseæ¡†æ¶åŸºäºStable Diffusionå¾®è°ƒï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£å…ˆéªŒè¿›è¡Œäººä½“å§¿æ€ä¼°è®¡ã€‚</li>
<li>ç›´æ¥é¢„æµ‹å…³é”®ç‚¹çƒ­å›¾åœ¨SD U-Netçš„å›¾åƒæ½œåœ¨ç©ºé—´ä¸­ï¼Œä¿ç•™åŸå§‹ç”Ÿæˆå…ˆéªŒã€‚</li>
<li>é€šè¿‡æ˜ å°„æ½œåœ¨ç‰¹å¾åˆ°å…³é”®ç‚¹çƒ­å›¾ï¼Œå®ç°å§¿æ€ä¼°è®¡ï¼Œé¿å…å¹²æ‰°é¢„è®­ç»ƒä¸»å¹²ã€‚</li>
<li>å¼•å…¥è¾…åŠ©RGBé‡å»ºåˆ†æ”¯ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¢å¼ºè·¨åŸŸç¨³å¥æ€§ã€‚</li>
<li>SDPoseåœ¨COCOéªŒè¯é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86d6c77b43bf7756a8df998aaf620407.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-90d3aeebbfbbcabce28ff7019c743587~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930715&auth_key=1759930715-0-0-fa1875e1edcb965431aaa04e9c22e186&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-48f94c5b6ae4602386fc98aa4184f0c4.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-61b2da04c59e61d18ee976f6d592567a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930728&auth_key=1759930728-0-0-ab195bf1368a5ddce4657eb1c0c68de3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Environment-Aware-Satellite-Image-Generation-with-Diffusion-Models"><a href="#Environment-Aware-Satellite-Image-Generation-with-Diffusion-Models" class="headerlink" title="Environment-Aware Satellite Image Generation with Diffusion Models"></a>Environment-Aware Satellite Image Generation with Diffusion Models</h2><p><strong>Authors:Nikos Kostagiolas, Pantelis Georgiades, Yannis Panagakis, Mihalis A. Nicolaou</strong></p>
<p>Diffusion-based foundation models have recently garnered much attention in the field of generative modeling due to their ability to generate images of high quality and fidelity. Although not straightforward, their recent application to the field of remote sensing signaled the first successful trials towards harnessing the large volume of publicly available datasets containing multimodal information. Despite their success, existing methods face considerable limitations: they rely on limited environmental context, struggle with missing or corrupted data, and often fail to reliably reflect user intentions in generated outputs. In this work, we propose a novel diffusion model conditioned on environmental context, that is able to generate satellite images by conditioning from any combination of three different control signals: a) text, b) metadata, and c) visual data. In contrast to previous works, the proposed method is i) to our knowledge, the first of its kind to condition satellite image generation on dynamic environmental conditions as part of its control signals, and ii) incorporating a metadata fusion strategy that models attribute embedding interactions to account for partially corrupt and&#x2F;or missing observations. Our method outperforms previous methods both qualitatively (robustness to missing metadata, higher responsiveness to control inputs) and quantitatively (higher fidelity, accuracy, and quality of generations measured using 6 different metrics) in the trials of single-image and temporal generation. The reported results support our hypothesis that conditioning on environmental context can improve the performance of foundation models for satellite imagery, and render our model a promising candidate for usage in downstream tasks. The collected 3-modal dataset is to our knowledge, the first publicly-available dataset to combine data from these three different mediums. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„åŸºç¡€æ¨¡å‹å› å…¶ç”Ÿæˆé«˜è´¨é‡å’Œé«˜ä¿çœŸåº¦å›¾åƒçš„èƒ½åŠ›è€Œåœ¨ç”Ÿæˆå»ºæ¨¡é¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚è™½ç„¶åº”ç”¨èµ·æ¥å¹¶ä¸ç®€å•ï¼Œä½†å®ƒä»¬æœ€è¿‘åœ¨é¥æ„Ÿé¢†åŸŸçš„åº”ç”¨æ ‡å¿—ç€åˆ©ç”¨åŒ…å«å¤šæ¨¡å¼ä¿¡æ¯çš„å…¬å¼€æ•°æ®é›†çš„å¤§ä½“ç§¯æ•°æ®çš„é¦–æ¬¡æˆåŠŸå°è¯•ã€‚å°½ç®¡å®ƒä»¬å–å¾—äº†æˆåŠŸï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´ç›¸å½“å¤§çš„å±€é™æ€§ï¼šå®ƒä»¬ä¾èµ–äºæœ‰é™çš„ç¯å¢ƒä¸Šä¸‹æ–‡ï¼Œåœ¨å¤„ç†ç¼ºå¤±æˆ–æŸåçš„æ•°æ®æ—¶è¡¨ç°æŒ£æ‰ï¼Œå¹¶ä¸”å¾€å¾€ä¸èƒ½åœ¨ç”Ÿæˆçš„è¾“å‡ºä¸­å¯é åœ°åæ˜ ç”¨æˆ·æ„å›¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºç¯å¢ƒä¸Šä¸‹æ–‡çš„æ‰©æ•£æ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡ä¸‰ç§ä¸åŒæ§åˆ¶ä¿¡å·çš„ä»»ä½•ç»„åˆæ¥ç”Ÿæˆå«æ˜Ÿå›¾åƒï¼šaï¼‰æ–‡æœ¬ï¼Œbï¼‰å…ƒæ•°æ®ï¼Œcï¼‰è§†è§‰æ•°æ®ã€‚ä¸ä¹‹å‰çš„å·¥ä½œç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•iï¼‰æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ˜¯é¦–ä¸ªåœ¨åŠ¨æ€ç¯å¢ƒæ¡ä»¶ä¸‹å¯¹å«æ˜Ÿå›¾åƒç”Ÿæˆè¿›è¡Œæ§åˆ¶çš„æ­¤ç±»æ–¹æ³•ï¼Œå¹¶ä¸”å…¶äºŒï¼‰é‡‡ç”¨äº†ä¸€ç§å…ƒæ•°æ®èåˆç­–ç•¥ï¼Œå¯¹å±æ€§åµŒå…¥äº¤äº’è¿›è¡Œå»ºæ¨¡ä»¥è§£é‡Šéƒ¨åˆ†æŸåå’Œ&#x2F;æˆ–ç¼ºå¤±çš„è§‚å¯Ÿç»“æœã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å•å›¾åƒå’Œä¸´æ—¶ç”Ÿæˆæµ‹è¯•ä¸­ï¼Œåœ¨å®šæ€§ï¼ˆå¯¹ç¼ºå¤±å…ƒæ•°æ®çš„ç¨³å¥æ€§ï¼Œå¯¹æ§åˆ¶è¾“å…¥çš„æ›´é«˜å“åº”èƒ½åŠ›ï¼‰å’Œå®šé‡ï¼ˆæ›´é«˜çš„ä¿çœŸåº¦ã€å‡†ç¡®æ€§å’Œç”Ÿæˆçš„æµ‹é‡ä½¿ç”¨6ä¸ªä¸åŒçš„æŒ‡æ ‡çš„è´¨é‡ï¼‰æ–¹é¢å‡ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚æŠ¥å‘Šçš„ç»“æœæ”¯æŒæˆ‘ä»¬çš„å‡è®¾ï¼Œå³åŸºäºç¯å¢ƒä¸Šä¸‹æ–‡çš„æ¡ä»¶å¯ä»¥æ”¹å–„å«æ˜Ÿå›¾åƒåŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä½¿æˆ‘ä»¬çš„æ¨¡å‹æˆä¸ºä¸‹æ¸¸ä»»åŠ¡ä¸­é¢‡æœ‰å‰æ™¯çš„å€™é€‰æ¨¡å‹ã€‚æ‰€æ”¶é›†çš„3æ¨¡æ€æ•°æ®é›†æ˜¯æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œç¬¬ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®é›†ï¼Œç»“åˆäº†è¿™ä¸‰ç§ä¸åŒåª’ä½“çš„æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24875v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç¯å¢ƒä¸Šä¸‹æ–‡æ¡ä»¶ç”Ÿæˆå«æ˜Ÿå›¾åƒçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ–‡æœ¬ã€å…ƒæ•°æ®å’Œè§†è§‰æ•°æ®ä¸‰ç§æ§åˆ¶ä¿¡å·ï¼Œèƒ½åœ¨åŠ¨æ€ç¯å¢ƒæ¡ä»¶ä¸‹ç”Ÿæˆå«æ˜Ÿå›¾åƒã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å•å›¾åƒå’Œæ—¶åºç”Ÿæˆæµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ï¼Œä¸ºå«æ˜Ÿå›¾åƒçš„åŸºç¡€æ¨¡å‹æä¾›äº†æ–°çš„æ”¹è¿›æ–¹å‘ã€‚åŒæ—¶ï¼Œæœ¬æ–‡é¦–æ¬¡å…¬å¼€äº†ä¸€ä¸ªä¸‰æ¨¡æ€æ•°æ®é›†ï¼Œä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›äº†æœ‰åŠ›çš„æ•°æ®æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå»ºæ¨¡é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é¥æ„Ÿé¢†åŸŸçš„åº”ç”¨ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹é¢ä¸´ç¯å¢ƒä¸Šä¸‹æ–‡æœ‰é™ã€æ•°æ®ç¼ºå¤±æˆ–æŸåä»¥åŠæ— æ³•åæ˜ ç”¨æˆ·æ„å›¾çš„å±€é™æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç¯å¢ƒä¸Šä¸‹æ–‡çš„æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆäº†æ–‡æœ¬ã€å…ƒæ•°æ®å’Œè§†è§‰æ•°æ®ä¸‰ç§æ§åˆ¶ä¿¡å·æ¥ç”Ÿæˆå«æ˜Ÿå›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•é¦–æ¬¡åœ¨åŠ¨æ€ç¯å¢ƒæ¡ä»¶ä¸‹è¿›è¡Œå«æ˜Ÿå›¾åƒç”Ÿæˆï¼Œå¹¶é‡‡ç”¨å±æ€§åµŒå…¥äº¤äº’æ¨¡å‹æ¥å¤„ç†éƒ¨åˆ†æŸåæˆ–ç¼ºå¤±çš„è§‚å¯Ÿæ•°æ®ã€‚</li>
<li>æ–¹æ³•åœ¨å•å›¾åƒå’Œæ—¶åºç”Ÿæˆæµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå®šé‡å’Œå®šæ€§è¯„ä¼°å‡ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</li>
<li>æ”¶é›†çš„ä¸‰æ¨¡æ€æ•°æ®é›†æ˜¯é¦–ä¸ªå…¬å¼€çš„ä¸‰æ¨¡æ€æ•°æ®é›†ï¼Œä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›äº†æœ‰åŠ›çš„æ•°æ®æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24875">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-83ef6a93e49710b265c84169a77e90b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930735&auth_key=1759930735-0-0-f8ef24108d95df4ee61cbf2323d56902&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-bd7a7ec3a67d85ec7724243ad18f80bb.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-f2d9cd2282820eed8ea5e1e972f128bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930750&auth_key=1759930750-0-0-5c74d6d6002e221ca4c5b21a9eb118bb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1db66f3b24031fc51bcccf8da4d9be99~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930757&auth_key=1759930757-0-0-ef49119e4730512bde414aa8c4dc0b1f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-d80c9025ea6ce6ceaefb4704ca8646ca.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-Object-Centric-Representations-Based-on-Slots-in-Real-World-Scenarios"><a href="#Learning-Object-Centric-Representations-Based-on-Slots-in-Real-World-Scenarios" class="headerlink" title="Learning Object-Centric Representations Based on Slots in Real World   Scenarios"></a>Learning Object-Centric Representations Based on Slots in Real World   Scenarios</h2><p><strong>Authors:Adil Kaan Akan</strong></p>
<p>A central goal in AI is to represent scenes as compositions of discrete objects, enabling fine-grained, controllable image and video generation. Yet leading diffusion models treat images holistically and rely on text conditioning, creating a mismatch for object-level editing. This thesis introduces a framework that adapts powerful pretrained diffusion models for object-centric synthesis while retaining their generative capacity.   We identify a core challenge: balancing global scene coherence with disentangled object control. Our method integrates lightweight, slot-based conditioning into pretrained models, preserving their visual priors while providing object-specific manipulation. For images, SlotAdapt augments diffusion models with a register token for background&#x2F;style and slot-conditioned modules for objects, reducing text-conditioning bias and achieving state-of-the-art results in object discovery, segmentation, compositional editing, and controllable image generation.   We further extend the framework to video. Using Invariant Slot Attention (ISA) to separate object identity from pose and a Transformer-based temporal aggregator, our approach maintains consistent object representations and dynamics across frames. This yields new benchmarks in unsupervised video object segmentation and reconstruction, and supports advanced editing tasks such as object removal, replacement, and insertion without explicit supervision.   Overall, this work establishes a general and scalable approach to object-centric generative modeling for images and videos. By bridging human object-based perception and machine learning, it expands the design space for interactive, structured, and user-driven generative tools in creative, scientific, and practical domains. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒç›®æ ‡æ˜¯å°†åœºæ™¯è¡¨ç¤ºä¸ºç¦»æ•£å¯¹è±¡çš„ç»„åˆï¼Œä»è€Œå®ç°ç²¾ç»†å¯æ§çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆã€‚ç„¶è€Œï¼Œé¢†å…ˆçš„æ‰©æ•£æ¨¡å‹ä»¥æ•´ä½“æ–¹å¼å¤„ç†å›¾åƒï¼Œå¹¶ä¾èµ–äºæ–‡æœ¬æ¡ä»¶ï¼Œè¿™å¯¹å¯¹è±¡çº§åˆ«çš„ç¼–è¾‘é€ æˆäº†ä¸åŒ¹é…ã€‚æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€‚åº”å¼ºå¤§çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡Œä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„åˆæˆï¼ŒåŒæ—¶ä¿ç•™å…¶ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬ç¡®å®šäº†ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šå¹³è¡¡å…¨å±€åœºæ™¯è¿è´¯æ€§ä¸è§£è€¦çš„å¯¹è±¡æ§åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è½»é‡çº§çš„åŸºäºæ’æ§½çš„æ¡ä»¶é›†æˆåˆ°é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œä¿ç•™å…¶è§†è§‰å…ˆéªŒçŸ¥è¯†ï¼ŒåŒæ—¶ä¸ºç‰¹å®šå¯¹è±¡æä¾›æ“ä½œã€‚å¯¹äºå›¾åƒï¼ŒSlotAdapté€šè¿‡ä¸ºèƒŒæ™¯å’Œé£æ ¼å¢åŠ ä¸€ä¸ªæ³¨å†Œä»¤ç‰Œä»¥åŠä¸ºå¯¹è±¡æä¾›æ’æ§½æ¡ä»¶æ¨¡å—æ¥å¢å¼ºæ‰©æ•£æ¨¡å‹ï¼Œä»è€Œå‡å°‘æ–‡æœ¬æ¡ä»¶çš„åè§ï¼Œå¹¶åœ¨å¯¹è±¡å‘ç°ã€åˆ†å‰²ã€ç»„åˆç¼–è¾‘å’Œå¯æ§å›¾åƒç”Ÿæˆæ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³çš„ç»“æœã€‚æˆ‘ä»¬å°†æ¡†æ¶è¿›ä¸€æ­¥æ‰©å±•åˆ°è§†é¢‘ã€‚é€šè¿‡ä½¿ç”¨ä¸å˜æ’æ§½æ³¨æ„åŠ›ï¼ˆISAï¼‰æ¥åˆ†ç¦»å¯¹è±¡èº«ä»½å’Œå§¿åŠ¿ä»¥åŠåŸºäºTransformerçš„ä¸´æ—¶èšåˆå™¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¸§ä¹‹é—´ä¿æŒä¸€è‡´çš„å¯¹è±¡è¡¨ç¤ºå’ŒåŠ¨æ€ã€‚è¿™ä¸ºå®ç°æ— ç›‘ç£è§†é¢‘å¯¹è±¡åˆ†å‰²å’Œé‡å»ºçš„æ–°åŸºå‡†é“ºå¹³äº†é“è·¯ï¼Œå¹¶æ”¯æŒé«˜çº§ç¼–è¾‘ä»»åŠ¡ï¼Œå¦‚å¯¹è±¡ç§»é™¤ã€æ›¿æ¢å’Œæ’å…¥ï¼Œæ— éœ€æ˜ç¡®çš„ç›‘ç£ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹å·¥ä½œä¸ºå›¾åƒå’Œè§†é¢‘å»ºç«‹äº†ä¸€ç§é€šç”¨ä¸”å¯æ‰©å±•çš„å¯¹è±¡ä¸­å¿ƒç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡å¼¥åˆåŸºäºäººç±»å¯¹è±¡çš„æ„ŸçŸ¥å’Œæœºå™¨å­¦ä¹ ä¹‹é—´çš„é¸¿æ²Ÿï¼Œæ‰©å±•äº†äº¤äº’å¼ã€ç»“æ„åŒ–ã€ç”¨æˆ·é©±åŠ¨çš„ç”Ÿæˆå·¥å…·åœ¨åˆ›æ„ã€ç§‘å­¦å’Œå®é™…é¢†åŸŸçš„è®¾è®¡ç©ºé—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24652v1">PDF</a> PhD Thesis, overlap with arXiv:2507.20855 and arXiv:2501.15878</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€‚åº”äºå¯¹è±¡ä¸­å¿ƒåˆæˆçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨è§£å†³äººå·¥æ™ºèƒ½ä¸­åœºæ™¯è¡¨ç¤ºä¸ºç¦»æ•£å¯¹è±¡ç»„åˆçš„ä¸­å¿ƒé—®é¢˜ã€‚å®ƒé€šè¿‡å¯¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œæ”¹è¿›ï¼Œä½¿å…¶èƒ½åœ¨ä¿æŒç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°å¯¹è±¡çº§åˆ«çš„ç¼–è¾‘ã€‚æ–¹æ³•åŒ…æ‹¬åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ å…¥åŸºäºæ’æ§½çš„è°ƒèŠ‚æœºåˆ¶ï¼Œä»¥å‡å°‘æ–‡æœ¬è°ƒèŠ‚çš„åè§ï¼Œå¹¶åœ¨å›¾åƒå’Œè§†é¢‘ä¸­å®ç°å¯¹è±¡å‘ç°ã€åˆ†å‰²ã€ç»„åˆç¼–è¾‘å’Œæ§åˆ¶ç”Ÿæˆç­‰ä»»åŠ¡ã€‚è¿™é¡¹å·¥ä½œå»ºç«‹äº†ä¸€ç§é€šç”¨ä¸”å¯æ‰©å±•çš„å¯¹è±¡ä¸­å¿ƒç”Ÿæˆæ¨¡å‹ï¼Œä¸ºå›¾åƒå’Œè§†é¢‘è®¾è®¡æä¾›äº†æ›´å¤šçš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç°åœ¨èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¡¨ç¤ºåœºæ™¯ä½œä¸ºç¦»æ•£å¯¹è±¡çš„ç»„åˆï¼Œæ”¯æŒç²¾ç»†ç²’åº¦çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œä½¿é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿé€‚åº”å¯¹è±¡ä¸­å¿ƒçš„åˆæˆï¼ŒåŒæ—¶ä¿æŒå…¶ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå¹³è¡¡å…¨å±€åœºæ™¯è¿è´¯æ€§å’Œè§£è€¦çš„å¯¹è±¡æ§åˆ¶ã€‚</li>
<li>é€šè¿‡å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒåŸºäºæ’æ§½çš„è°ƒèŠ‚ï¼Œå¯ä»¥åœ¨ä¿ç•™è§†è§‰å…ˆéªŒçš„åŒæ—¶ï¼Œå®ç°å¯¹ç‰¹å®šå¯¹è±¡çš„æ“ä½œã€‚</li>
<li>SlotAdaptæ–¹æ³•é€šè¿‡åœ¨æ‰©æ•£æ¨¡å‹ä¸­å¢åŠ æ³¨å†Œä»¤ç‰Œå’Œæ’æ§½æ¡ä»¶æ¨¡å—ï¼Œå‡å°‘äº†æ–‡æœ¬è°ƒèŠ‚çš„åè§ï¼Œå¹¶åœ¨å¯¹è±¡å‘ç°ã€åˆ†å‰²ã€ç»„åˆç¼–è¾‘å’Œæ§åˆ¶ç”Ÿæˆç­‰æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æˆæœã€‚</li>
<li>è¯¥æ¡†æ¶å·²æˆåŠŸæ‰©å±•åˆ°è§†é¢‘é¢†åŸŸï¼Œé€šè¿‡ä½¿ç”¨ä¸å˜æ’æ§½æ³¨æ„åŠ›å’ŒåŸºäºTransformerçš„ä¸´æ—¶èšåˆå™¨ï¼Œå¯ä»¥åœ¨å„å¸§ä¹‹é—´ä¿æŒä¸€è‡´çš„å¯¹è±¡è¡¨ç¤ºå’ŒåŠ¨æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c09f99543fd178cddf392aced307affd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930771&auth_key=1759930771-0-0-ba9b257e08aedb987aa85bd33bf2d407&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PoseDiff-A-Unified-Diffusion-Model-Bridging-Robot-Pose-Estimation-and-Video-to-Action-Control"><a href="#PoseDiff-A-Unified-Diffusion-Model-Bridging-Robot-Pose-Estimation-and-Video-to-Action-Control" class="headerlink" title="PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and   Video-to-Action Control"></a>PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and   Video-to-Action Control</h2><p><strong>Authors:Haozhuo Zhang, Michele Caprio, Jing Shao, Qiang Zhang, Jian Tang, Shanghang Zhang, Wei Pan</strong></p>
<p>We present PoseDiff, a conditional diffusion model that unifies robot state estimation and control within a single framework. At its core, PoseDiff maps raw visual observations into structured robot states-such as 3D keypoints or joint angles-from a single RGB image, eliminating the need for multi-stage pipelines or auxiliary modalities. Building upon this foundation, PoseDiff extends naturally to video-to-action inverse dynamics: by conditioning on sparse video keyframes generated by world models, it produces smooth and continuous long-horizon action sequences through an overlap-averaging strategy. This unified design enables scalable and efficient integration of perception and control. On the DREAM dataset, PoseDiff achieves state-of-the-art accuracy and real-time performance for pose estimation. On Libero-Object manipulation tasks, it substantially improves success rates over existing inverse dynamics modules, even under strict offline settings. Together, these results show that PoseDiff provides a scalable, accurate, and efficient bridge between perception, planning, and control in embodied AI. The video visualization results can be found on the project page: <a target="_blank" rel="noopener" href="https://haozhuo-zhang.github.io/PoseDiff-project-page/">https://haozhuo-zhang.github.io/PoseDiff-project-page/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†PoseDiffï¼Œè¿™æ˜¯ä¸€ç§æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶å†…èåˆäº†æœºå™¨äººçš„çŠ¶æ€ä¼°è®¡å’Œæ§åˆ¶ã€‚PoseDiffçš„æ ¸å¿ƒåŠŸèƒ½æ˜¯å°†åŸå§‹è§†è§‰è§‚å¯Ÿæ˜ å°„åˆ°ç»“æ„åŒ–æœºå™¨äººçŠ¶æ€ï¼ˆä¾‹å¦‚ä»å•ä¸ªRGBå›¾åƒä¸­çš„3Då…³é”®ç‚¹æˆ–å…³èŠ‚è§’åº¦ï¼‰ï¼Œä»è€Œæ— éœ€å¤šé˜¶æ®µç®¡é“æˆ–è¾…åŠ©æ¨¡å¼ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒPoseDiffè‡ªç„¶åœ°æ‰©å±•åˆ°äº†è§†é¢‘åˆ°åŠ¨ä½œçš„é€†å‘åŠ¨åŠ›å­¦ï¼šé€šè¿‡ä»¥ä¸–ç•Œæ¨¡å‹ç”Ÿæˆç¨€ç–è§†é¢‘å…³é”®å¸§ä¸ºæ¡ä»¶ï¼Œå®ƒé‡‡ç”¨é‡å å¹³å‡ç­–ç•¥ç”Ÿæˆå¹³æ»‘ä¸”è¿ç»­çš„é•¿å‘¨æœŸåŠ¨ä½œåºåˆ—ã€‚è¿™ç§ç»Ÿä¸€çš„è®¾è®¡å®ç°äº†æ„ŸçŸ¥å’Œæ§åˆ¶çš„å¯æ‰©å±•å’Œæœ‰æ•ˆé›†æˆã€‚åœ¨DREAMæ•°æ®é›†ä¸Šï¼ŒPoseDiffå®ç°äº†å§¿æ€ä¼°è®¡çš„å…ˆè¿›å‡†ç¡®æ€§å’Œå®æ—¶æ€§èƒ½ã€‚åœ¨Libero-Objectæ“ä½œä»»åŠ¡ä¸Šï¼Œå³ä½¿åœ¨ä¸¥æ ¼çš„ç¦»çº¿è®¾ç½®ä¸‹ï¼Œå®ƒä¹Ÿå¤§å¤§æé«˜äº†ç°æœ‰é€†å‘åŠ¨åŠ›å­¦æ¨¡å—çš„æˆåŠŸç‡ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç»“æœè¡¨æ˜PoseDiffåœ¨åµŒå…¥å¼äººå·¥æ™ºèƒ½çš„æ„ŸçŸ¥ã€è§„åˆ’å’Œæ§åˆ¶ä¹‹é—´æä¾›äº†å¯æ‰©å±•ã€å‡†ç¡®å’Œæœ‰æ•ˆçš„æ¡¥æ¢ã€‚è§†é¢‘å¯è§†åŒ–ç»“æœå¯åœ¨é¡¹ç›®é¡µé¢æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://haozhuo-zhang.github.io/PoseDiff-project-page/%E3%80%82">https://haozhuo-zhang.github.io/PoseDiff-project-page/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24591v1">PDF</a> </p>
<p><strong>Summary</strong><br>     PoseDiffæ˜¯ä¸€ä¸ªæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå®ƒå°†æœºå™¨äººçŠ¶æ€ä¼°è®¡å’Œæ§åˆ¶æ•´åˆåˆ°ä¸€ä¸ªå•ä¸€çš„æ¡†æ¶ä¸­ã€‚PoseDiffèƒ½å¤Ÿå°†åŸå§‹è§†è§‰è§‚å¯Ÿæ˜ å°„åˆ°ç»“æ„åŒ–æœºå™¨äººçŠ¶æ€ï¼ˆå¦‚ä»å•ä¸ªRGBå›¾åƒçš„3Då…³é”®ç‚¹æˆ–å…³èŠ‚è§’ï¼‰ï¼Œä»è€Œä¸éœ€è¦å¤šé˜¶æ®µç®¡é“æˆ–è¾…åŠ©æ¨¡å¼ã€‚æ­¤å¤–ï¼ŒPoseDiffè‡ªç„¶åœ°æ‰©å±•åˆ°è§†é¢‘åˆ°åŠ¨ä½œé€†å‘åŠ¨åŠ›å­¦ï¼šé€šè¿‡ä»¥ä¸–ç•Œæ¨¡å‹ç”Ÿæˆçš„ç¨€ç–è§†é¢‘å…³é”®å¸§ä¸ºæ¡ä»¶ï¼Œå®ƒé‡‡ç”¨é‡å å¹³å‡ç­–ç•¥äº§ç”Ÿå¹³æ»‘å’Œè¿ç»­çš„é•¿å‘¨æœŸåŠ¨ä½œåºåˆ—ã€‚è¿™ä¸€ç»Ÿä¸€çš„è®¾è®¡å®ç°äº†æ„ŸçŸ¥å’Œæ§åˆ¶çš„å¯ä¼¸ç¼©å’Œé«˜æ•ˆé›†æˆã€‚PoseDiffåœ¨DREAMæ•°æ®é›†ä¸Šå®ç°äº†å§¿æ€ä¼°è®¡çš„å…ˆè¿›å‡†ç¡®æ€§å’Œå®æ—¶æ€§èƒ½ï¼Œå¹¶åœ¨Liberoå¯¹è±¡æ“ä½œä»»åŠ¡ä¸Šå¤§å¹…åº¦æé«˜äº†ç°æœ‰é€†å‘åŠ¨åŠ›å­¦æ¨¡å—çš„æˆåŠŸç‡ï¼Œå³ä½¿åœ¨ä¸¥æ ¼çš„ç¦»çº¿è®¾ç½®ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ€»çš„æ¥è¯´ï¼ŒPoseDiffä¸ºæ„ŸçŸ¥ã€è§„åˆ’å’Œæ§åˆ¶åœ¨åµŒå…¥å¼äººå·¥æ™ºèƒ½ä¹‹é—´æä¾›äº†å¯ä¼¸ç¼©ã€å‡†ç¡®å’Œé«˜æ•ˆçš„æ¡¥æ¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PoseDiffæ˜¯ä¸€ä¸ªæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç»Ÿä¸€äº†æœºå™¨äººçŠ¶æ€ä¼°è®¡å’Œæ§åˆ¶ã€‚</li>
<li>PoseDiffèƒ½å¤Ÿå°†åŸå§‹è§†è§‰è§‚å¯Ÿæ˜ å°„åˆ°ç»“æ„åŒ–æœºå™¨äººçŠ¶æ€ã€‚</li>
<li>PoseDiffæ‰©å±•äº†è§†é¢‘åˆ°åŠ¨ä½œé€†å‘åŠ¨åŠ›å­¦ï¼Œç”Ÿæˆå¹³æ»‘å’Œè¿ç»­çš„åŠ¨ä½œåºåˆ—ã€‚</li>
<li>PoseDiffå®ç°äº†æ„ŸçŸ¥å’Œæ§åˆ¶çš„å¯ä¼¸ç¼©å’Œé«˜æ•ˆé›†æˆã€‚</li>
<li>PoseDiffåœ¨DREAMæ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›çš„å§¿æ€ä¼°è®¡æ€§èƒ½ã€‚</li>
<li>åœ¨Liberoå¯¹è±¡æ“ä½œä»»åŠ¡ä¸Šï¼ŒPoseDiffæé«˜äº†é€†å‘åŠ¨åŠ›å­¦æ¨¡å—çš„æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aab8b967318f88509b6aee7df6082d92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1f9677181cdea4acc95a3c98ea855b7.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-a4481dd21af9fdf34e3c28567b268e3e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930793&auth_key=1759930793-0-0-7d867cc8f4f8d1fc1084fe0e4378075f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-8881e8fa241e1c26a30b8fce0550824d.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-0857dde043f110769977091dfa1652f4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930806&auth_key=1759930806-0-0-e6cb350b79eebe209d4d2d27428149f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-749548d331e4dfa257fdc0badb113339~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930813&auth_key=1759930813-0-0-67556bc3a31df855fb4acd9a1dd85bdf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ASIA-Adaptive-3D-Segmentation-using-Few-Image-Annotations"><a href="#ASIA-Adaptive-3D-Segmentation-using-Few-Image-Annotations" class="headerlink" title="ASIA: Adaptive 3D Segmentation using Few Image Annotations"></a>ASIA: Adaptive 3D Segmentation using Few Image Annotations</h2><p><strong>Authors:Sai Raj Kishore Perla, Aditya Vora, Sauradip Nag, Ali Mahdavi-Amiri, Hao Zhang</strong></p>
<p>We introduce ASIA (Adaptive 3D Segmentation using few Image Annotations), a novel framework that enables segmentation of possibly non-semantic and non-text-describable â€œpartsâ€ in 3D. Our segmentation is controllable through a few user-annotated in-the-wild images, which are easier to collect than multi-view images, less demanding to annotate than 3D models, and more precise than potentially ambiguous text descriptions. Our method leverages the rich priors of text-to-image diffusion models, such as Stable Diffusion (SD), to transfer segmentations from image space to 3D, even when the annotated and target objects differ significantly in geometry or structure. During training, we optimize a text token for each segment and fine-tune our model with a novel cross-view part correspondence loss. At inference, we segment multi-view renderings of the 3D mesh, fuse the labels in UV-space via voting, refine them with our novel Noise Optimization technique, and finally map the UV-labels back onto the mesh. ASIA provides a practical and generalizable solution for both semantic and non-semantic 3D segmentation tasks, outperforming existing methods by a noticeable margin in both quantitative and qualitative evaluations. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ASIAï¼ˆä½¿ç”¨å°‘é‡å›¾åƒæ ‡æ³¨è¿›è¡Œè‡ªé€‚åº”3Dåˆ†å‰²ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå¯¹å¯èƒ½æ˜¯éè¯­ä¹‰å’Œéæ–‡æœ¬æè¿°çš„3Dâ€œéƒ¨åˆ†â€è¿›è¡Œåˆ†å‰²ã€‚æˆ‘ä»¬çš„åˆ†å‰²å¯ä»¥é€šè¿‡å°‘æ•°ç”¨æˆ·æ ‡æ³¨çš„é‡å¤–å›¾åƒè¿›è¡Œæ§åˆ¶ï¼Œè¿™äº›å›¾åƒæ¯”å¤šè§’åº¦å›¾åƒæ›´å®¹æ˜“æ”¶é›†ï¼Œæ¯”3Dæ¨¡å‹æ›´æ˜“äºæ ‡æ³¨ï¼Œå¹¶ä¸”æ¯”æ½œåœ¨æ¨¡ç³Šæ–‡æœ¬æè¿°æ›´å‡†ç¡®ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸°å¯Œå…ˆéªŒçŸ¥è¯†ï¼Œå¦‚Stable Diffusionï¼ˆSDï¼‰ï¼Œå°†å›¾åƒç©ºé—´çš„åˆ†å‰²è½¬ç§»åˆ°3Dï¼Œå³ä½¿æ ‡æ³¨å’Œç›®æ ‡å¯¹è±¡åœ¨å‡ ä½•æˆ–ç»“æ„ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªåˆ†æ®µè¿›è¡Œä¼˜åŒ–æ–‡æœ¬æ ‡è®°ï¼Œå¹¶é€šè¿‡æ–°çš„è·¨è§†å›¾éƒ¨åˆ†å¯¹åº”æŸå¤±å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯¹3Dç½‘æ ¼çš„å¤šè§†è§’æ¸²æŸ“è¿›è¡Œåˆ†å‰²ï¼Œé€šè¿‡æŠ•ç¥¨åœ¨UVç©ºé—´èåˆæ ‡ç­¾ï¼Œä½¿ç”¨æˆ‘ä»¬æ–°çš„å™ªå£°ä¼˜åŒ–æŠ€æœ¯è¿›è¡Œç»†åŒ–å¤„ç†ï¼Œæœ€åå°†UVæ ‡ç­¾æ˜ å°„å›ç½‘æ ¼ã€‚ASIAä¸ºè¯­ä¹‰å’Œéè¯­ä¹‰çš„3Dåˆ†å‰²ä»»åŠ¡æä¾›äº†å®ç”¨ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­éƒ½ä»¥æ˜æ˜¾çš„ä¼˜åŠ¿è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24288v1">PDF</a> SIGGRAPH Asia, 2025. Project Page: <a target="_blank" rel="noopener" href="https://sairajk.github.io/asia/">https://sairajk.github.io/asia/</a></p>
<p><strong>Summary</strong></p>
<p>ASIAï¼ˆåŸºäºå°‘é‡å›¾åƒæ ‡æ³¨çš„è‡ªé€‚åº”3Dåˆ†å‰²ï¼‰æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œå¯é€šè¿‡å°‘æ•°ç”¨æˆ·æ ‡æ³¨çš„é‡å¤–å›¾åƒæ§åˆ¶3Dä¸­å¯èƒ½éè¯­ä¹‰å’Œéæ–‡æœ¬æè¿°çš„â€œéƒ¨åˆ†â€åˆ†å‰²ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸°å¯Œå…ˆéªŒçŸ¥è¯†ï¼ˆå¦‚Stable Diffusionï¼‰ï¼Œå°†åˆ†å‰²ä»å›¾åƒç©ºé—´è½¬ç§»åˆ°3Dï¼Œå³ä½¿æ ‡æ³¨å’Œç›®æ ‡å¯¹è±¡åœ¨å‡ ä½•æˆ–ç»“æ„ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚é€šè¿‡ä¼˜åŒ–æ–‡æœ¬æ ‡è®°å’Œæ–°å‹è·¨è§†å›¾éƒ¨åˆ†å¯¹åº”å…³ç³»æŸå¤±è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æ¨æ–­æ—¶è¿›è¡Œå¤šè§†å›¾æ¸²æŸ“ã€UVç©ºé—´æ ‡ç­¾èåˆã€å™ªå£°ä¼˜åŒ–æŠ€æœ¯ï¼Œæœ€ç»ˆå°†UVæ ‡ç­¾æ˜ å°„å›ç½‘æ ¼ã€‚ASIAä¸ºè¯­ä¹‰å’Œéè¯­ä¹‰3Dåˆ†å‰²ä»»åŠ¡æä¾›äº†å®ç”¨ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ASIAæ˜¯ä¸€ç§æ–°çš„3Dåˆ†å‰²æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡å°‘é‡ç”¨æˆ·æ ‡æ³¨çš„é‡å¤–å›¾åƒè¿›è¡Œæ§åˆ¶ã€‚</li>
<li>å®ƒåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸°å¯Œå…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>ASIAå¯ä»¥å¤„ç†éè¯­ä¹‰å’Œéæ–‡æœ¬æè¿°çš„â€œéƒ¨åˆ†â€åˆ†å‰²ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–æ–‡æœ¬æ ‡è®°å’Œæ–°å‹è·¨è§†å›¾éƒ¨åˆ†å¯¹åº”å…³ç³»æŸå¤±è¿›è¡Œè®­ç»ƒã€‚</li>
<li>åœ¨æ¨æ–­è¿‡ç¨‹ä¸­ï¼ŒASIAè¿›è¡Œå¤šè§†å›¾æ¸²æŸ“ã€UVç©ºé—´æ ‡ç­¾èåˆå’Œå™ªå£°ä¼˜åŒ–ã€‚</li>
<li>ASIAä¸ºè¯­ä¹‰å’Œéè¯­ä¹‰3Dåˆ†å‰²ä»»åŠ¡æä¾›äº†å®ç”¨ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24288">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c5220a92775ff5f049fc2412a03e0678.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-36ec0320829f4451c0db680903ce2b06~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930827&auth_key=1759930827-0-0-85ae2ebf5ef7b7800e0bce29cfab4cea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-c598aa5db217217506f9b1fb6c438527.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-e74c4b66e2465c3c2e85273134ded8b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930841&auth_key=1759930841-0-0-7dbfacd4acaad61e3b16b2207ffdb13c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-40d68cbb9d9b3184daa2c1426174d183~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930848&auth_key=1759930848-0-0-40c96047a2c26e07cd734cc566ff27d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="An-Efficient-3D-Latent-Diffusion-Model-for-T1-contrast-Enhanced-MRI-Generation"><a href="#An-Efficient-3D-Latent-Diffusion-Model-for-T1-contrast-Enhanced-MRI-Generation" class="headerlink" title="An Efficient 3D Latent Diffusion Model for T1-contrast Enhanced MRI   Generation"></a>An Efficient 3D Latent Diffusion Model for T1-contrast Enhanced MRI   Generation</h2><p><strong>Authors:Zach Eidex, Mojtaba Safari, Jie Ding, Richard Qiu, Justin Roper, David Yu, Hui-Kuo Shu, Zhen Tian, Hui Mao, Xiaofeng Yang</strong></p>
<p>Objective: Gadolinium-based contrast agents (GBCAs) are commonly employed with T1w MRI to enhance lesion visualization but are restricted in patients at risk of nephrogenic systemic fibrosis and variations in GBCA administration can introduce imaging inconsistencies. This study develops an efficient 3D deep-learning framework to generate T1-contrast enhanced images (T1C) from pre-contrast multiparametric MRI. Approach: We propose the 3D latent rectified flow (T1C-RFlow) model for generating high-quality T1C images. First, T1w and T2-FLAIR images are input into a pretrained autoencoder to acquire an efficient latent space representation. A rectified flow diffusion model is then trained in this latent space representation. The T1C-RFlow model was trained on a curated dataset comprised of the BraTS 2024 glioma (GLI; 1480 patients), meningioma (MEN; 1141 patients), and metastases (MET; 1475 patients) datasets. Selected patients were split into train (N&#x3D;2860), validation (N&#x3D;612), and test (N&#x3D;614) sets. Results: Both qualitative and quantitative results demonstrate that the T1C-RFlow model outperforms benchmark 3D models (pix2pix, DDPM, Diffusion Transformers (DiT-3D)) trained in the same latent space. T1C-RFlow achieved the following metrics - GLI: NMSE 0.044 +&#x2F;- 0.047, SSIM 0.935 +&#x2F;- 0.025; MEN: NMSE 0.046 +&#x2F;- 0.029, SSIM 0.937 +&#x2F;- 0.021; MET: NMSE 0.098 +&#x2F;- 0.088, SSIM 0.905 +&#x2F;- 0.082. T1C-RFlow had the best tumor reconstruction performance and significantly faster denoising times (6.9 s&#x2F;volume, 200 steps) than conventional DDPM models in both latent space (37.7s, 1000 steps) and patch-based in image space (4.3 hr&#x2F;volume). Significance: Our proposed method generates synthetic T1C images that closely resemble ground truth T1C in much less time than previous diffusion models. Further development may permit a practical method for contrast-agent-free MRI for brain tumors. </p>
<blockquote>
<p>ç›®æ ‡ï¼šé’†ç±»é€ å½±å‰‚ï¼ˆGBCAsï¼‰é€šå¸¸ä¸T1åŠ æƒç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸€èµ·ä½¿ç”¨ï¼Œä»¥æé«˜ç—…å˜çš„å¯è§†åŒ–æ•ˆæœï¼Œä½†æ˜¯å¯¹äºæ‚£æœ‰è‚¾æºæ€§ç³»ç»Ÿæ€§çº¤ç»´åŒ–é£é™©çš„æ‚£è€…æœ‰æ‰€é™åˆ¶ï¼ŒGBCAç®¡ç†çš„å˜åŒ–å¯èƒ½ä¼šå¼•å…¥æˆåƒçš„ä¸ä¸€è‡´æ€§ã€‚æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªé«˜æ•ˆçš„3Dæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºä»é¢„é€ å½±å¤šå‚æ•°MRIç”ŸæˆT1åŠ æƒå¢å¼ºå›¾åƒï¼ˆT1Cï¼‰ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ç”¨äºç”Ÿæˆé«˜è´¨é‡T1Cå›¾åƒçš„3Dæ½œåœ¨æ ¡æ­£æµï¼ˆT1C-RFlowï¼‰æ¨¡å‹ã€‚é¦–å…ˆï¼Œå°†T1wå’ŒT2-FLAIRå›¾åƒè¾“å…¥é¢„è®­ç»ƒçš„è‡ªåŠ¨ç¼–ç å™¨ï¼Œä»¥è·å¾—æœ‰æ•ˆçš„æ½œåœ¨ç©ºé—´è¡¨ç¤ºã€‚ç„¶ååœ¨æ­¤æ½œåœ¨ç©ºé—´è¡¨ç¤ºä¸­è®­ç»ƒæ ¡æ­£æµæ‰©æ•£æ¨¡å‹ã€‚T1C-RFlowæ¨¡å‹æ˜¯åœ¨ç²¾é€‰æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œè¯¥æ•°æ®é›†ç”±BraTS 2024èƒ¶è´¨ç»†èƒç˜¤ï¼ˆGLIï¼›1480åæ‚£è€…ï¼‰ã€è„‘è†œç˜¤ï¼ˆMENï¼›1141åæ‚£è€…ï¼‰å’Œè½¬ç§»ç˜¤ï¼ˆMETï¼›1475åæ‚£è€…ï¼‰æ•°æ®é›†ç»„æˆã€‚æ‰€é€‰æ‚£è€…è¢«åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆN&#x3D;2860ï¼‰ã€éªŒè¯é›†ï¼ˆN&#x3D;612ï¼‰å’Œæµ‹è¯•é›†ï¼ˆN&#x3D;614ï¼‰ã€‚ç»“æœï¼šå®šæ€§å’Œå®šé‡ç»“æœå‡è¡¨æ˜ï¼ŒT1C-RFlowæ¨¡å‹åœ¨ç›¸åŒçš„æ½œåœ¨ç©ºé—´ä¸­è¶…è¶Šäº†åŸºå‡†çš„3Dæ¨¡å‹ï¼ˆpix2pixã€DDPMã€Diffusion Transformersï¼ˆDiT-3Dï¼‰ï¼‰ã€‚åœ¨GLIä¸­ï¼ŒT1C-RFlowè¾¾åˆ°äº†NMSE 0.044 +&#x2F;- 0.047ï¼ŒSSIM 0.935 +&#x2F;- 0.025ï¼›åœ¨MENä¸­ï¼ŒNMSE 0.046 +&#x2F;- 0.029ï¼ŒSSIM 0.937 +&#x2F;- 0.021ï¼›åœ¨METä¸­ï¼ŒNMSE 0.098 +&#x2F;- 0.088ï¼ŒSSIM 0.905 +&#x2F;- 0.082ã€‚T1C-RFlowå…·æœ‰æœ€ä½³çš„è‚¿ç˜¤é‡å»ºæ€§èƒ½ï¼Œå¹¶ä¸”å»å™ªæ—¶é—´æ˜¾è‘—å¿«äºä¼ ç»ŸDDPMæ¨¡å‹ï¼Œæ— è®ºæ˜¯æ½œåœ¨ç©ºé—´è¿˜æ˜¯å»å›¾åƒç©ºé—´çš„è¡¥ä¸åŸºç¡€ã€‚æ„ä¹‰ï¼šæˆ‘ä»¬æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿåœ¨æ¯”ä»¥å‰çš„æ‰©æ•£æ¨¡å‹æ›´çŸ­çš„æ—¶é—´å†…ç”Ÿæˆæ¨¡æ‹ŸT1Cå›¾åƒã€‚è¿›ä¸€æ­¥çš„å¼€å‘å¯èƒ½ä¼šæä¾›ä¸€ç§æ— éœ€é€ å½±å‰‚çš„MRIæ£€æµ‹è„‘éƒ¨è‚¿ç˜¤çš„å®é™…æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24194v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§é«˜æ•ˆçš„3Dæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»é¢„å¯¹æ¯”çš„å¤šå‚æ•°MRIç”ŸæˆT1å¯¹æ¯”å¢å¼ºå›¾åƒï¼ˆT1Cï¼‰ã€‚è¯¥ç ”ç©¶ä½¿ç”¨3Dæ½œåœ¨æ•´æµæµï¼ˆT1C-RFlowï¼‰æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡T1Cå›¾åƒï¼Œè¯¥æ¨¡å‹åœ¨åŒ…å«èƒ¶è´¨è‚¿ç˜¤çš„æ•°æ®åº“ä¸Šè¿›è¡Œè®­ç»ƒå¹¶å®ç°äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚æ­¤æ¨¡å‹æ¯”å…¶å®ƒåŸºå‡†3Dæ¨¡å‹æ›´å¿«ï¼Œå¯¹è‚¿ç˜¤é‡å»ºå…·æœ‰æœ€ä½³æ€§èƒ½ã€‚æ­¤ç ”ç©¶ä¸ºå®ç°æ— é€ å½±å‰‚MRIçš„å®ç”¨æ–¹æ³•æä¾›äº†æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ä½¿ç”¨3Dæ·±åº¦å­¦ä¹ æ¡†æ¶ç”ŸæˆT1å¯¹æ¯”å¢å¼ºå›¾åƒï¼ˆT1Cï¼‰ã€‚</li>
<li>T1C-RFlowæ¨¡å‹ä½¿ç”¨é¢„å¯¹æ¯”çš„T1wå’ŒT2-FLAIRå›¾åƒè¿›è¡Œè®­ç»ƒï¼Œç”Ÿæˆé«˜è´¨é‡T1Cå›¾åƒã€‚</li>
<li>T1C-RFlowæ¨¡å‹åœ¨å¤šç§ç–¾ç—…æ•°æ®é›†ï¼ˆåŒ…æ‹¬èƒ¶è´¨è‚¿ç˜¤ã€è„‘è†œç˜¤å’Œè½¬ç§»ç˜¤ï¼‰ä¸Šè¿›è¡Œäº†è®­ç»ƒå’ŒéªŒè¯ã€‚</li>
<li>T1C-RFlowæ¨¡å‹çš„æ€§èƒ½ä¼˜äºå…¶ä»–åŸºå‡†3Dæ¨¡å‹ï¼Œå…·æœ‰æ›´å¥½çš„è‚¿ç˜¤é‡å»ºèƒ½åŠ›å’Œæ›´å¿«çš„å»å™ªæ—¶é—´ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿå®ç°å¯¹æ¯”å‰‚MRIçš„å¿«é€Ÿæˆåƒï¼Œå…·æœ‰æ— é€ å½±å‰‚MRIçš„æ½œåŠ›ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºè§£å†³é’†ç±»é€ å½±å‰‚ä½¿ç”¨å—é™é—®é¢˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¦‚è‚¾æºæ€§ç³»ç»Ÿæ€§çº¤ç»´åŒ–å’Œæˆåƒä¸ä¸€è‡´ç­‰é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3444a172b2cea0585f06be75ed38e0da.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-dbabea21c0f0464f457481cca0dec9fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930862&auth_key=1759930862-0-0-38ed44513fa457a73fe9721bc22f49bc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Tunable-Generalization-Diffusion-Powered-by-Self-Supervised-Contextual-Sub-Data-for-Low-Dose-CT-Reconstruction"><a href="#Tunable-Generalization-Diffusion-Powered-by-Self-Supervised-Contextual-Sub-Data-for-Low-Dose-CT-Reconstruction" class="headerlink" title="Tunable-Generalization Diffusion Powered by Self-Supervised Contextual   Sub-Data for Low-Dose CT Reconstruction"></a>Tunable-Generalization Diffusion Powered by Self-Supervised Contextual   Sub-Data for Low-Dose CT Reconstruction</h2><p><strong>Authors:Guoquan Wei, Zekun Zhou, Liu Shi, Wenzhe Shan, Qiegen Liu</strong></p>
<p>Current models based on deep learning for low-dose CT denoising rely heavily on paired data and generalize poorly. Even the more concerned diffusion models need to learn the distribution of clean data for reconstruction, which is difficult to satisfy in medical clinical applications. At the same time, self-supervised-based methods face the challenge of significant degradation of generalizability of models pre-trained for the current dose to expand to other doses. To address these issues, this paper proposes a novel method of tunable-generalization diffusion powered by self-supervised contextual sub-data for low-dose CT reconstruction, named SuperDiff. Firstly, a contextual subdata similarity adaptive sensing strategy is designed for denoising centered on the LDCT projection domain, which provides an initial prior for the subsequent progress. Subsequently, the initial prior is used to combine knowledge distillation with a deep combination of latent diffusion models for optimizing image details. The pre-trained model is used for inference reconstruction, and the pixel-level self-correcting fusion technique is proposed for fine-grained reconstruction of the image domain to enhance the image fidelity, using the initial prior and the LDCT image as a guide. In addition, the technique is flexibly applied to the generalization of upper and lower doses or even unseen doses. Dual-domain strategy cascade for self-supervised LDCT denoising, SuperDiff requires only LDCT projection domain data for training and testing. Full qualitative and quantitative evaluations on both datasets and real data show that SuperDiff consistently outperforms existing state-of-the-art methods in terms of reconstruction and generalization performance. </p>
<blockquote>
<p>å½“å‰åŸºäºæ·±åº¦å­¦ä¹ çš„ä½å‰‚é‡CTå»å™ªæ¨¡å‹å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºé…å¯¹æ•°æ®ï¼Œå…¶æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚å³ä½¿æ›´å—å…³æ³¨çš„æ‰©æ•£æ¨¡å‹ä¹Ÿéœ€è¦å­¦ä¹ æ¸…æ´æ•°æ®çš„åˆ†å¸ƒæ¥è¿›è¡Œé‡å»ºï¼Œè¿™åœ¨åŒ»å­¦ä¸´åºŠåº”ç”¨ä¸­æ˜¯éš¾ä»¥æ»¡è¶³çš„ã€‚åŒæ—¶ï¼ŒåŸºäºè‡ªç›‘ç£çš„æ–¹æ³•è¿˜é¢ä¸´ç€æŒ‘æˆ˜ï¼Œå³å½“å‰å‰‚é‡é¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›åœ¨æ‰©å±•åˆ°å…¶ä»–å‰‚é‡æ—¶æ˜¾è‘—ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯è°ƒèŠ‚é€šç”¨æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡è‡ªç›‘ç£ä¸Šä¸‹æ–‡å­æ•°æ®è¿›è¡Œä½å‰‚é‡CTé‡å»ºï¼Œåä¸ºSuperDiffã€‚é¦–å…ˆï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºLDCTæŠ•å½±åŸŸçš„ä¸Šä¸‹æ–‡å­æ•°æ®ç›¸ä¼¼æ€§è‡ªé€‚åº”æ„ŸçŸ¥ç­–ç•¥ï¼Œä¸ºå»å™ªæä¾›åˆå§‹å…ˆéªŒï¼Œä¸ºåç»­è¿›å±•æ‰“ä¸‹åŸºç¡€ã€‚éšåï¼Œåˆ©ç”¨åˆå§‹å…ˆéªŒç»“åˆçŸ¥è¯†è’¸é¦å’Œæ·±åº¦æ½œæ‰©æ•£æ¨¡å‹ä¼˜åŒ–å›¾åƒç»†èŠ‚ã€‚ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†é‡å»ºï¼Œå¹¶æå‡ºåƒç´ çº§è‡ªæ ¡æ­£èåˆæŠ€æœ¯ï¼Œä»¥åˆå§‹å…ˆéªŒå’ŒLDCTå›¾åƒä¸ºæŒ‡å¯¼ï¼Œå¯¹å›¾åƒåŸŸè¿›è¡Œç²¾ç»†é‡å»ºï¼Œæé«˜å›¾åƒä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯å¯çµæ´»åº”ç”¨äºé«˜ä½å‰‚é‡ç”šè‡³æœªè§å‰‚é‡çš„æ³›åŒ–ã€‚SuperDiffé‡‡ç”¨åŒåŸŸç­–ç•¥çº§è”è‡ªç›‘ç£LDCTå»å™ªï¼Œä»…éœ€è¦LDCTæŠ•å½±åŸŸæ•°æ®è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚åœ¨æ•°æ®é›†å’ŒçœŸå®æ•°æ®ä¸Šçš„å…¨é¢å®šæ€§å’Œå®šé‡è¯„ä¼°è¡¨æ˜ï¼ŒSuperDiffåœ¨é‡å»ºå’Œæ³›åŒ–æ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23885v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£ä¸Šä¸‹æ–‡å­æ•°æ®çš„å¯è°ƒé€šç”¨æ‰©æ•£æ¨¡å‹ï¼ˆSuperDiffï¼‰ï¼Œç”¨äºä½å‰‚é‡CTé‡å»ºï¼Œä»¥è§£å†³å½“å‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ä½å‰‚é‡CTå»å™ªä¸­è¿‡åº¦ä¾èµ–é…å¯¹æ•°æ®ã€é€šç”¨æ€§è¾ƒå·®çš„é—®é¢˜ã€‚SuperDiffé¦–å…ˆè®¾è®¡äº†ä¸€ç§åŸºäºLDCTæŠ•å½±åŸŸçš„ä¸Šä¸‹æ–‡å­æ•°æ®ç›¸ä¼¼æ€§è‡ªé€‚åº”æ„ŸçŸ¥ç­–ç•¥ï¼Œä¸ºåç»­è¿›ç¨‹æä¾›åˆå§‹å…ˆéªŒã€‚ç„¶åç»“åˆçŸ¥è¯†è’¸é¦å’Œæ·±åº¦æ½œæ‰©æ•£æ¨¡å‹ä¼˜åŒ–å›¾åƒç»†èŠ‚ã€‚ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨æ–­é‡å»ºï¼Œå¹¶æå‡ºåƒç´ çº§è‡ªæˆ‘æ ¡æ­£èåˆæŠ€æœ¯ï¼Œä»¥åˆå§‹å…ˆéªŒå’ŒLDCTå›¾åƒä¸ºæŒ‡å¯¼ï¼Œå¯¹å›¾åƒåŸŸè¿›è¡Œç²¾ç»†é‡å»ºï¼Œæé«˜å›¾åƒä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯å¯çµæ´»åº”ç”¨äºä¸Šä¸‹å‰‚é‡ç”šè‡³æœªè§å‰‚é‡çš„æ¨å¹¿ã€‚SuperDiffåªéœ€LDCTæŠ•å½±åŸŸæ•°æ®è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œé‡‡ç”¨åŒåŸŸç­–ç•¥çº§è”è¿›è¡Œè‡ªç›‘ç£LDCTå»å™ªã€‚åœ¨æ•°æ®é›†å’ŒçœŸå®æ•°æ®ä¸Šçš„å…¨é¢å®šæ€§å’Œå®šé‡è¯„ä¼°è¡¨æ˜ï¼ŒSuperDiffåœ¨é‡å»ºå’Œæ³›åŒ–æ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ä½å‰‚é‡CTå»å™ªä¸­è¿‡äºä¾èµ–é…å¯¹æ•°æ®ï¼Œé€šç”¨æ€§è¾ƒå·®ã€‚</li>
<li>SuperDiffé€šè¿‡ç»“åˆè‡ªç›‘ç£å­¦ä¹ å’Œæ‰©æ•£æ¨¡å‹è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>SuperDiffé‡‡ç”¨åŸºäºLDCTæŠ•å½±åŸŸçš„ä¸Šä¸‹æ–‡å­æ•°æ®ç›¸ä¼¼æ€§è‡ªé€‚åº”æ„ŸçŸ¥ç­–ç•¥ï¼Œä¸ºåç»­è¿›ç¨‹æä¾›åˆå§‹å…ˆéªŒã€‚</li>
<li>ç»“åˆçŸ¥è¯†è’¸é¦å’Œæ·±åº¦æ½œæ‰©æ•£æ¨¡å‹ä¼˜åŒ–å›¾åƒç»†èŠ‚ã€‚</li>
<li>åƒç´ çº§è‡ªæˆ‘æ ¡æ­£èåˆæŠ€æœ¯ç”¨äºæé«˜å›¾åƒä¿çœŸåº¦çš„ç²¾ç»†é‡å»ºã€‚</li>
<li>SuperDiffæŠ€æœ¯å¯çµæ´»åº”ç”¨äºä¸åŒå‰‚é‡çš„æ³›åŒ–ã€‚</li>
<li>ä»…éœ€LDCTæŠ•å½±åŸŸæ•°æ®è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•çš„SuperDiffï¼Œåœ¨é‡å»ºå’Œæ³›åŒ–æ€§èƒ½ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-93cbfcf357e755d2facfe171e6bcb063.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6e9c0ad6c5a67d49422b047597b4177.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-133fa10bb6c7a21a05bbe998898d4e1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5177b7c527249112358513b00e936d3d.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-db3bf5a843f73cc3e05da75e135fd49c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930897&auth_key=1759930897-0-0-4b87812a7625ff4a689883e0e83d50d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing"><a href="#FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing" class="headerlink" title="FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image   Editing"></a>FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image   Editing</h2><p><strong>Authors:Junyi Wu, Zhiteng Li, Haotong Qin, Xiaohong Liu, Linghe Kong, Yulun Zhang, Xiaokang Yang</strong></p>
<p>Text-guided image editing with diffusion models has achieved remarkable quality but suffers from prohibitive latency, hindering real-world applications. We introduce FlashEdit, a novel framework designed to enable high-fidelity, real-time image editing. Its efficiency stems from three key innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses costly iterative processes; (2) a Background Shield (BG-Shield) technique that guarantees background preservation by selectively modifying features only within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA) mechanism that ensures precise, localized edits by suppressing semantic leakage to the background. Extensive experiments demonstrate that FlashEdit maintains superior background consistency and structural integrity, while performing edits in under 0.2 seconds, which is an over 150$\times$ speedup compared to prior multi-step methods. Our code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/JunyiWuCode/FlashEdit">https://github.com/JunyiWuCode/FlashEdit</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘å·²ç»å–å¾—äº†æ˜¾è‘—çš„å“è´¨ï¼Œä½†ä»ç„¶å­˜åœ¨å»¶è¿Ÿé—®é¢˜ï¼Œé˜»ç¢äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬å¼•å…¥äº†FlashEditï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸã€å®æ—¶çš„å›¾åƒç¼–è¾‘ã€‚å…¶æ•ˆç‡æºäºä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰ä¸€æ­¥åæ¼”ç¼–è¾‘ï¼ˆOSIEï¼‰ç®¡é“ï¼Œç»•è¿‡æ˜‚è´µçš„è¿­ä»£è¿‡ç¨‹ï¼›ï¼ˆ2ï¼‰èƒŒæ™¯å±è”½ï¼ˆBG-Shieldï¼‰æŠ€æœ¯ï¼Œé€šè¿‡é€‰æ‹©æ€§ä¿®æ”¹ç¼–è¾‘åŒºåŸŸå†…çš„ç‰¹å¾æ¥ä¿è¯èƒŒæ™¯ä¿ç•™ï¼›ï¼ˆ3ï¼‰ç¨€ç–ç©ºé—´äº¤å‰æ³¨æ„åŠ›ï¼ˆSSCAï¼‰æœºåˆ¶ï¼Œé€šè¿‡æŠ‘åˆ¶è¯­ä¹‰æ³„éœ²åˆ°èƒŒæ™¯æ¥ç¡®ä¿ç²¾ç¡®ã€å±€éƒ¨åŒ–çš„ç¼–è¾‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFlashEditåœ¨ä¿æŒèƒŒæ™¯ä¸€è‡´æ€§å’Œç»“æ„å®Œæ•´æ€§çš„åŒæ—¶ï¼Œç¼–è¾‘æ—¶é—´ç¼©çŸ­è‡³0.2ç§’ä»¥å†…ï¼Œç›¸è¾ƒäºå…ˆå‰çš„å¤šæ­¥éª¤æ–¹æ³•å®ç°äº†è¶…è¿‡150å€çš„é€Ÿåº¦æå‡ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/JunyiWuCode/FlashEdit%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/JunyiWuCode/FlashEditä¸Šå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22244v2">PDF</a> We need to further improve our research</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„æ–°æ¡†æ¶FlashEditï¼Œå®ç°äº†é«˜ä¿çœŸå®æ—¶å›¾åƒç¼–è¾‘ã€‚å…¶é€šè¿‡ä¸‰é¡¹åˆ›æ–°æŠ€æœ¯æé«˜ç¼–è¾‘æ•ˆç‡ï¼šä¸€æ­¥åè½¬ç¼–è¾‘æµç¨‹ï¼ˆOSIEï¼‰ã€èƒŒæ™¯å±è”½æŠ€æœ¯ä¿è¯èƒŒæ™¯ä¸€è‡´æ€§å’Œç²¾ç¡®å®šä½ç¼–è¾‘ã€‚å®éªŒè¯æ˜ï¼ŒFlashEditåœ¨ä¿è¯èƒŒæ™¯å’Œç»“æ„ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œç¼–è¾‘é€Ÿåº¦åœ¨0.2ç§’å†…ï¼Œè¾ƒä¼ ç»Ÿå¤šæ­¥éª¤æ–¹æ³•æé«˜äº†è¶…è¿‡150å€ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/JunyiWuCode/FlashEdit%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/JunyiWuCode/FlashEditå…¬å¼€ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlashEditå®ç°äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„é«˜ä¿çœŸå®æ—¶å›¾åƒç¼–è¾‘ã€‚</li>
<li>é€šè¿‡ä¸€æ­¥åè½¬ç¼–è¾‘æµç¨‹ï¼ˆOSIEï¼‰æé«˜äº†å›¾åƒç¼–è¾‘æ•ˆç‡ã€‚</li>
<li>èƒŒæ™¯å±è”½æŠ€æœ¯ï¼ˆBG-Shieldï¼‰ä¿è¯äº†å›¾åƒèƒŒæ™¯çš„ä¸€è‡´æ€§ã€‚</li>
<li>Sparsified Spatial Cross-Attentionï¼ˆSSCAï¼‰æœºåˆ¶ç¡®ä¿ç²¾å‡†ã€å±€éƒ¨åŒ–çš„ç¼–è¾‘ã€‚</li>
<li>FlashEdité€šè¿‡æŠ‘åˆ¶è¯­ä¹‰æ³„æ¼è‡³èƒŒæ™¯å®ç°ç²¾ç¡®ç¼–è¾‘ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒFlashEditçš„ç¼–è¾‘é€Ÿåº¦è¾¾åˆ°äº†æƒŠäººçš„é€Ÿåº¦æå‡ï¼Œä»…éœ€ä¸åˆ°0.2ç§’å®Œæˆã€‚ä¸ä¹‹å‰çš„å¤šæ­¥æ–¹æ³•ç›¸æ¯”ï¼Œæå‡äº†è¶…è¿‡150å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22244">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a144b7ac0f5c52f6aca10fb0e0b9ecc4.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-80471badc6c7ca6afa4e754f5cc736d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930912&auth_key=1759930912-0-0-062bda894b3b9928a2a145cbdb9eb91b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dc20b6aa555095d2f4ff9260c9f8b880~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930919&auth_key=1759930919-0-0-33c13ed065207a06377c206a64c01739&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-041e844f4eeff7a5652e4f87107a3e47~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930926&auth_key=1759930926-0-0-042e994ca3c97852b2f7cec10cebb484&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Physics-Guided-Null-Space-Diffusion-with-Sparse-Masking-for-Corrective-Sparse-View-CT-Reconstruction"><a href="#Physics-Guided-Null-Space-Diffusion-with-Sparse-Masking-for-Corrective-Sparse-View-CT-Reconstruction" class="headerlink" title="Physics-Guided Null-Space Diffusion with Sparse Masking for Corrective   Sparse-View CT Reconstruction"></a>Physics-Guided Null-Space Diffusion with Sparse Masking for Corrective   Sparse-View CT Reconstruction</h2><p><strong>Authors:Zekun Zhou, Yanru Gong, Liu Shi, Qiegen Liu</strong></p>
<p>Diffusion models have demonstrated remarkable generative capabilities in image processing tasks. We propose a Sparse condition Temporal Rewighted Integrated Distribution Estimation guided diffusion model (STRIDE) for sparse-view CT reconstruction. Specifically, we design a joint training mechanism guided by sparse conditional probabilities to facilitate the model effective learning of missing projection view completion and global information modeling. Based on systematic theoretical analysis, we propose a temporally varying sparse condition reweighting guidance strategy to dynamically adjusts weights during the progressive denoising process from pure noise to the real image, enabling the model to progressively perceive sparse-view information. The linear regression is employed to correct distributional shifts between known and generated data, mitigating inconsistencies arising during the guidance process. Furthermore, we construct a dual-network parallel architecture to perform global correction and optimization across multiple sub-frequency components, thereby effectively improving the model capability in both detail restoration and structural preservation, ultimately achieving high-quality image reconstruction. Experimental results on both public and real datasets demonstrate that the proposed method achieves the best improvement of 2.58 dB in PSNR, increase of 2.37% in SSIM, and reduction of 0.236 in MSE compared to the best-performing baseline methods. The reconstructed images exhibit excellent generalization and robustness in terms of structural consistency, detail restoration, and artifact suppression. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç¨€ç–è§†å›¾CTé‡å»ºçš„ç¨€ç–æ¡ä»¶æ—¶é—´åŠ æƒç»¼åˆåˆ†å¸ƒä¼°è®¡å¯¼å‘æ‰©æ•£æ¨¡å‹ï¼ˆSTRIDEï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç”±ç¨€ç–æ¡ä»¶æ¦‚ç‡å¼•å¯¼çš„è”åˆè®­ç»ƒæœºåˆ¶ï¼Œä»¥ä¿ƒè¿›æ¨¡å‹æœ‰æ•ˆåœ°å­¦ä¹ ç¼ºå¤±æŠ•å½±è§†å›¾è¡¥å…¨å’Œå…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚åŸºäºç³»ç»Ÿçš„ç†è®ºåˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶é—´å˜åŒ–çš„ç¨€ç–æ¡ä»¶é‡åŠ æƒæŒ‡å¯¼ç­–ç•¥ï¼Œåœ¨ä»çº¯å™ªå£°åˆ°çœŸå®å›¾åƒçš„æ¸è¿›å»å™ªè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æƒé‡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€æ­¥æ„ŸçŸ¥ç¨€ç–è§†å›¾ä¿¡æ¯ã€‚é‡‡ç”¨çº¿æ€§å›å½’æ¥æ ¡æ­£å·²çŸ¥æ•°æ®å’Œç”Ÿæˆæ•°æ®ä¹‹é—´çš„åˆ†å¸ƒåç§»ï¼Œå‡è½»æŒ‡å¯¼è¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¸ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒç½‘ç»œå¹¶è¡Œæ¶æ„ï¼Œä»¥åœ¨å¤šä¸ªå­é¢‘ç‡åˆ†é‡ä¸Šæ‰§è¡Œå…¨å±€æ ¡æ­£å’Œä¼˜åŒ–ï¼Œä»è€Œæœ‰æ•ˆåœ°æé«˜äº†æ¨¡å‹åœ¨ç»†èŠ‚æ¢å¤å’Œç»“æ„ä¿æŒæ–¹é¢çš„èƒ½åŠ›ï¼Œæœ€ç»ˆå®ç°äº†é«˜è´¨é‡å›¾åƒé‡å»ºã€‚åœ¨å…¬å…±å’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ‰©æ•£æ¨¡å‹å±•ç¤ºå‡ºäº†å‡ºè‰²çš„ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç¨€ç–è§†å›¾CTé‡å»ºçš„ç¨€ç–æ¡ä»¶ä¸´æ—¶åŠ æƒé›†æˆåˆ†å¸ƒä¼°è®¡å¯¼å‘æ‰©æ•£æ¨¡å‹ï¼ˆSTRIDEï¼‰ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç”±ç¨€ç–æ¡ä»¶æ¦‚ç‡å¼•å¯¼çš„è”åˆè®­ç»ƒæœºåˆ¶ï¼Œå¸®åŠ©æ¨¡å‹æœ‰æ•ˆå­¦ä¹ ç¼ºå¤±æŠ•å½±è§†å›¾çš„è¡¥å…¨å’Œå…¨å±€ä¿¡æ¯çš„å»ºæ¨¡ã€‚é€šè¿‡ç³»ç»Ÿçš„ç†è®ºåˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§éšæ—¶é—´å˜åŒ–çš„ç¨€ç–æ¡ä»¶é‡åŠ æƒæŒ‡å¯¼ç­–ç•¥ï¼Œåœ¨ç”±çº¯å™ªå£°è‡³æ¸…æ™°å›¾åƒçš„æ¸è¿›å»å™ªè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æƒé‡åˆ†é…ã€‚è¿™æ ·èƒ½ä½¿æ¨¡å‹é€æ­¥æ„ŸçŸ¥ç¨€ç–è§†å›¾çš„ä¿¡æ¯ã€‚æ­¤å¤–è¿˜é‡‡ç”¨äº†çº¿æ€§å›å½’çš„æ–¹æ³•ï¼Œç”¨ä»¥æ ¡æ­£å·²çŸ¥å’Œç”Ÿæˆæ•°æ®ä¹‹é—´çš„åˆ†å¸ƒåç§»æƒ…å†µï¼Œä»¥æ­¤å‡å°‘æŒ‡å¯¼è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„ä¸ä¸€è‡´é—®é¢˜ã€‚æ›´è¿›ä¸€æ­¥çš„æ˜¯ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŒç½‘ç»œå¹¶è¡Œæ¶æ„ï¼Œç”¨ä»¥åœ¨ä¸åŒå­é¢‘ç‡åˆ†é‡ä¸Šè¿›è¡Œå…¨å±€çš„æ ¡æ­£ä¸ä¼˜åŒ–ï¼Œè¿›è€Œæœ‰æ•ˆæé«˜æ¨¡å‹åœ¨ç»†èŠ‚å¤åŸå’Œç»“æ„ä¿æŒæ–¹é¢çš„æ€§èƒ½ï¼Œå¹¶æœ€ç»ˆå®ç°é«˜è´¨é‡çš„å›¾åƒé‡å»ºã€‚åœ¨å…¬å…±å’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºè¡¨ç°æœ€ä½³çš„åŸºå‡†æ–¹æ³•ï¼Œæ‰€ææ–¹æ³•åœ¨æé«˜å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æ–¹é¢å–å¾—äº†2.58åˆ†è´çš„æœ€ä½³æ”¹è¿›ï¼Œåœ¨æé«˜ç»“æ„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆSSIMï¼‰æ–¹é¢å¢åŠ äº†2.37%ï¼Œåœ¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æ–¹é¢å‡å°‘äº†0.236ã€‚é‡å»ºçš„å›¾åƒåœ¨ç»“æ„ä¸€è‡´æ€§ã€ç»†èŠ‚æ¢å¤å’Œä¼ªå½±æŠ‘åˆ¶ç­‰æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ³›åŒ–å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05992v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æå‡ºçš„Sparse condition Temporal Rewighted Integrated Distribution Estimationæ‰©æ•£æ¨¡å‹ï¼ˆSTRIDEï¼‰ç”¨äºç¨€ç–è§†å›¾CTé‡å»ºã€‚é€šè¿‡è”åˆè®­ç»ƒæœºåˆ¶å’Œç¨€ç–æ¡ä»¶æ¦‚ç‡æŒ‡å¯¼ï¼Œæ¨¡å‹èƒ½æ›´æœ‰æ•ˆåœ°å­¦ä¹ ç¼ºå¤±æŠ•å½±è§†å›¾è¡¥å…¨å’Œå…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚è¯¥æ¨¡å‹åœ¨æ¸è¿›å»å™ªè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æƒé‡ï¼Œå®ç°ç¨€ç–è§†å›¾ä¿¡æ¯çš„é€æ­¥æ„ŸçŸ¥ã€‚é€šè¿‡çº¿æ€§å›å½’æ ¡æ­£å·²çŸ¥å’Œç”Ÿæˆæ•°æ®ä¹‹é—´çš„åˆ†å¸ƒåç§»ï¼Œå‡å°‘æŒ‡å¯¼è¿‡ç¨‹ä¸­çš„ä¸ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæ„å»ºåŒç½‘ç»œå¹¶è¡Œæ¶æ„ï¼Œè¿›è¡Œå¤šå­é¢‘ç‡ç»„ä»¶çš„å…¨å±€æ ¡æ­£å’Œä¼˜åŒ–ï¼Œæé«˜æ¨¡å‹åœ¨ç»†èŠ‚æ¢å¤å’Œç»“æ„ä¿æŒæ–¹é¢çš„èƒ½åŠ›ï¼Œå®ç°é«˜è´¨é‡å›¾åƒé‡å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>æå‡ºçš„Sparse condition Temporal Rewighted Integrated Distribution Estimationæ‰©æ•£æ¨¡å‹ï¼ˆSTRIDEï¼‰ç”¨äºç¨€ç–è§†å›¾CTé‡å»ºã€‚</li>
<li>é€šè¿‡è”åˆè®­ç»ƒæœºåˆ¶å’Œç¨€ç–æ¡ä»¶æ¦‚ç‡æŒ‡å¯¼ï¼Œæ¨¡å‹èƒ½å­¦ä¹ ç¼ºå¤±æŠ•å½±è§†å›¾è¡¥å…¨å’Œå…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚</li>
<li>æ¨¡å‹åœ¨æ¸è¿›å»å™ªè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æƒé‡ï¼Œå®ç°ç¨€ç–è§†å›¾ä¿¡æ¯çš„é€æ­¥æ„ŸçŸ¥ã€‚</li>
<li>çº¿æ€§å›å½’ç”¨äºæ ¡æ­£å·²çŸ¥å’Œç”Ÿæˆæ•°æ®ä¹‹é—´çš„åˆ†å¸ƒåç§»ã€‚</li>
<li>æ„å»ºåŒç½‘ç»œå¹¶è¡Œæ¶æ„ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ç»†èŠ‚æ¢å¤å’Œç»“æ„ä¿æŒæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-33c8163010ad1428b150df0ce2541eac~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930933&auth_key=1759930933-0-0-47ce3102eed33d4592a62b77fc900ad4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-af090789c344dd32a55d91b8fcf28767.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-44cd64dd13fdce4d833c4e19b62c8a3a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930947&auth_key=1759930947-0-0-f73ce32d504d8fb0582f93de43a21227&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-c7babe7b00ddbfcc87159842cd4a4385.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fc24b01b0c60e85f3d45dd013723d68.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Diffusion-Generative-Models-Meet-Compressed-Sensing-with-Applications-to-Imaging-and-Finance"><a href="#Diffusion-Generative-Models-Meet-Compressed-Sensing-with-Applications-to-Imaging-and-Finance" class="headerlink" title="Diffusion Generative Models Meet Compressed Sensing, with Applications   to Imaging and Finance"></a>Diffusion Generative Models Meet Compressed Sensing, with Applications   to Imaging and Finance</h2><p><strong>Authors:Zhengyi Guo, Jiatu Li, Wenpin Tang, David D. Yao</strong></p>
<p>In this study we develop dimension-reduction techniques to accelerate diffusion model inference in the context of synthetic data generation. The idea is to integrate compressed sensing into diffusion models (hence, CSDM): First, compress the dataset into a latent space (from an ambient space), and train a diffusion model in the latent space; next, apply a compressed sensing algorithm to the samples generated in the latent space for decoding back to the original space; and the goal is to facilitate the efficiency of both model training and inference. Under certain sparsity assumptions on data, our proposed approach achieves provably faster convergence, via combining diffusion model inference with sparse recovery. It also sheds light on the best choice of the latent space dimension. To illustrate the effectiveness of this approach, we run numerical experiments on a range of datasets, including handwritten digits, medical and climate images, and financial time series for stress testing. </p>
<blockquote>
<p>æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼€å‘é™ç»´æŠ€æœ¯æ¥åŠ é€Ÿåˆæˆæ•°æ®ç”ŸæˆèƒŒæ™¯ä¸‹çš„æ‰©æ•£æ¨¡å‹æ¨ç†ã€‚æˆ‘ä»¬çš„æƒ³æ³•æ˜¯å°†å‹ç¼©æ„ŸçŸ¥æŠ€æœ¯é›†æˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼ˆå› æ­¤ç§°ä¸ºCSDMï¼‰ï¼šé¦–å…ˆï¼Œå°†æ•°æ®é›†å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ï¼ˆä»ç¯å¢ƒç©ºé—´ä¸­ï¼‰ï¼›åœ¨æ½œåœ¨ç©ºé—´ä¸­è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼›ç„¶åï¼Œå¯¹æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆçš„æ ·æœ¬åº”ç”¨å‹ç¼©æ„ŸçŸ¥ç®—æ³•ä»¥è§£ç å›åŸå§‹ç©ºé—´ï¼›æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æé«˜æ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚åœ¨æ•°æ®çš„æŸäº›ç¨€ç–æ€§å‡è®¾ä¸‹ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•é€šè¿‡ç»“åˆæ‰©æ•£æ¨¡å‹æ¨ç†å’Œç¨€ç–æ¢å¤ï¼Œèƒ½å¤Ÿå®ç°æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚å®ƒè¿˜æŒ‡å‡ºäº†é€‰æ‹©æ½œåœ¨ç©ºé—´ç»´åº¦çš„æœ€ä½³æ–¹æ³•ã€‚ä¸ºäº†è¯´æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨ä¸€ç³»åˆ—æ•°æ®é›†ä¸Šè¿›è¡Œäº†æ•°å€¼å®éªŒï¼ŒåŒ…æ‹¬æ‰‹å†™æ•°å­—ã€åŒ»ç–—å’Œæ°”å€™å›¾åƒä»¥åŠé‡‘èæ—¶é—´åºåˆ—çš„å‹åŠ›æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03898v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶å¼€å‘é™ç»´æŠ€æœ¯ä»¥åŠ é€Ÿåˆæˆæ•°æ®ç”Ÿæˆä¸­çš„æ‰©æ•£æ¨¡å‹æ¨ç†ã€‚ç ”ç©¶å°†å‹ç¼©æ„ŸçŸ¥é›†æˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼ˆå³CSDMï¼‰ï¼šé¦–å…ˆï¼Œå°†æ•°æ®å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ï¼ˆä»ç¯å¢ƒç©ºé—´ï¼‰ï¼Œåœ¨æ½œåœ¨ç©ºé—´è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼›ç„¶åï¼Œå¯¹æ½œåœ¨ç©ºé—´ç”Ÿæˆçš„æ ·æœ¬åº”ç”¨å‹ç¼©æ„ŸçŸ¥ç®—æ³•ä»¥è§£ç å›åŸå§‹ç©ºé—´ï¼›ç›®æ ‡æ˜¯ä¿ƒè¿›æ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚åœ¨æ•°æ®çš„æŸäº›ç¨€ç–æ€§å‡è®¾ä¸‹ï¼Œæ‰€æå‡ºçš„æ–¹æ³•é€šè¿‡ç»“åˆæ‰©æ•£æ¨¡å‹æ¨ç†ä¸ç¨€ç–æ¢å¤ï¼Œå¯å®ç°æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é˜é‡Šäº†é€‰æ‹©æ½œåœ¨ç©ºé—´ç»´åº¦çš„æœ€ä½³åšæ³•ã€‚é€šè¿‡åœ¨æ‰‹å†™æ•°å­—ã€åŒ»ç–—å’Œæ°”å€™å›¾åƒä»¥åŠé‡‘èæ—¶é—´åºåˆ—ç­‰æ•°æ®é›†ä¸Šè¿›è¡Œçš„æ•°å€¼å®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºå°†å‹ç¼©æ„ŸçŸ¥é›†æˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå½¢æˆCSDMæ–¹æ³•ï¼Œæ—¨åœ¨åŠ é€Ÿæ‰©æ•£æ¨¡å‹æ¨ç†å¹¶ä¿ƒè¿›æ¨¡å‹è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>CSDMæ–¹æ³•é€šè¿‡åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œæ•°æ®å’Œæ¨¡å‹æ“ä½œï¼Œæœ‰æ•ˆé™ä½æ•°æ®å¤æ‚æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å‹ç¼©æ„ŸçŸ¥ç®—æ³•å°†æ½œåœ¨ç©ºé—´ä¸­çš„æ ·æœ¬è§£ç å›åŸå§‹ç©ºé—´ã€‚</li>
<li>åœ¨æ•°æ®çš„æŸäº›ç¨€ç–æ€§å‡è®¾ä¸‹ï¼ŒCSDMæ–¹æ³•é€šè¿‡ç»“åˆæ‰©æ•£æ¨¡å‹æ¨ç†ä¸ç¨€ç–æ¢å¤å®ç°æ›´å¿«æ”¶æ•›ã€‚</li>
<li>ç ”ç©¶é˜é‡Šäº†å¦‚ä½•é€‰æ‹©æ½œåœ¨ç©ºé—´çš„æœ€ä½³ç»´åº¦ã€‚</li>
<li>é€šè¿‡å¤šç§æ•°æ®é›†ä¸Šçš„æ•°å€¼å®éªŒï¼ŒéªŒè¯äº†CSDMæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0576a5bc254b3717bd02deae2491451f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930968&auth_key=1759930968-0-0-988d3a3cf95e5befa7d33b1bc60a368a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d6f7bfe6a1c012e20f3f4d86a3c6a8e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930975&auth_key=1759930975-0-0-6413046af5963a6f80bdb7fe54f68894&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="3D-LATTE-Latent-Space-3D-Editing-from-Textual-Instructions"><a href="#3D-LATTE-Latent-Space-3D-Editing-from-Textual-Instructions" class="headerlink" title="3D-LATTE: Latent Space 3D Editing from Textual Instructions"></a>3D-LATTE: Latent Space 3D Editing from Textual Instructions</h2><p><strong>Authors:Maria Parelli, Michael Oechsle, Michael Niemeyer, Federico Tombari, Andreas Geiger</strong></p>
<p>Despite the recent success of multi-view diffusion models for text&#x2F;image-based 3D asset generation, instruction-based editing of 3D assets lacks surprisingly far behind the quality of generation models. The main reason is that recent approaches using 2D priors suffer from view-inconsistent editing signals. Going beyond 2D prior distillation methods and multi-view editing strategies, we propose a training-free editing method that operates within the latent space of a native 3D diffusion model, allowing us to directly manipulate 3D geometry. We guide the edit synthesis by blending 3D attention maps from the generation with the source object. Coupled with geometry-aware regularization guidance, a spectral modulation strategy in the Fourier domain and a refinement step for 3D enhancement, our method outperforms previous 3D editing methods enabling high-fidelity, precise, and robust edits across a wide range of shapes and semantic manipulations. </p>
<blockquote>
<p>å°½ç®¡åŸºäºå¤šè§†è§’çš„æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬&#x2F;å›¾åƒé©±åŠ¨çš„3Dèµ„äº§ç”Ÿæˆæ–¹é¢å–å¾—äº†æœ€æ–°æˆåŠŸï¼Œä½†åŸºäºæŒ‡ä»¤çš„3Dèµ„äº§ç¼–è¾‘å´å‡ºäººæ„æ–™åœ°è¿œè¿œè½åäºç”Ÿæˆæ¨¡å‹çš„è´¨é‡ã€‚ä¸»è¦åŸå› æ˜¯è¿‘æœŸä½¿ç”¨äºŒç»´å…ˆéªŒçš„æ–¹æ³•å—åˆ°äº†è§†è§’ä¸ä¸€è‡´ç¼–è¾‘ä¿¡å·çš„å½±å“ã€‚è¶…è¶ŠäºŒç»´å…ˆéªŒè’¸é¦æ–¹æ³•å’Œå¤šè§†è§’ç¼–è¾‘ç­–ç•¥ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ç¼–è¾‘æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨åŸç”Ÿä¸‰ç»´æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´å†…è¿›è¡Œæ“ä½œï¼Œå…è®¸æˆ‘ä»¬ç›´æ¥æ“ä½œä¸‰ç»´å‡ ä½•ã€‚æˆ‘ä»¬é€šè¿‡æ··åˆç”Ÿæˆä¸­çš„ä¸‰ç»´æ³¨æ„åŠ›å›¾ä¸æºå¯¹è±¡æ¥æŒ‡å¯¼ç¼–è¾‘åˆæˆã€‚ç»“åˆå‡ ä½•æ„ŸçŸ¥æ­£åˆ™åŒ–æŒ‡å¯¼ã€å‚…é‡Œå¶åŸŸçš„é¢‘è°±è°ƒåˆ¶ç­–ç•¥ä»¥åŠç”¨äºä¸‰ç»´å¢å¼ºçš„ç»†åŒ–æ­¥éª¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹¿æ³›çš„å½¢çŠ¶å’Œè¯­ä¹‰æ“ä½œæ–¹é¢è¶…è¶Šäº†ä¹‹å‰çš„3Dç¼–è¾‘æ–¹æ³•ï¼Œå®ç°äº†é«˜ä¿çœŸã€ç²¾ç¡®å’Œç¨³å¥çš„ç¼–è¾‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00269v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦è®¨è®ºäº†å¤šè§†å›¾æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å’Œå›¾åƒé©±åŠ¨çš„3Dèµ„äº§ç”Ÿæˆä¸­çš„æˆåŠŸåº”ç”¨ï¼Œä½†ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå…³äºåŸºäºæŒ‡ä»¤çš„ç¼–è¾‘æŠ€æœ¯ç›¸å¯¹æ»åçš„é—®é¢˜ã€‚åŸå› åœ¨äºé‡‡ç”¨äºŒç»´å…ˆéªŒçš„æ–¹æ³•ä¼šå‡ºç°è§†å›¾ä¸ä¸€è‡´çš„ç¼–è¾‘ä¿¡å·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŸç”Ÿä¸‰ç»´æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒå¼å¤–ç¼–è¾‘æ–¹æ³•ï¼Œå¯ä»¥åœ¨å…¶æ½œåœ¨ç©ºé—´å†…æ“ä½œè€Œä¸è¿›è¡Œä»»ä½•è®­ç»ƒï¼Œä»è€Œå®ç°ç›´æ¥çš„å‡ ä½•å›¾å½¢æ“ä½œã€‚é€šè¿‡å°†ç”Ÿæˆçš„ä¸‰ç»´æ³¨æ„åŠ›åœ°å›¾ä¸åŸå§‹å¯¹è±¡ç›¸ç»“åˆè¿›è¡Œåˆæˆç¼–è¾‘æŒ‡å¯¼ï¼Œè¾…ä»¥å‡ ä½•æ„ŸçŸ¥æ­£åˆ™åŒ–æŒ‡å¯¼ã€é¢‘åŸŸå…‰è°±è°ƒåˆ¶ç­–ç•¥å’Œç”¨äºä¸‰ç»´å¢å¼ºçš„ç²¾ç‚¼æ­¥éª¤ï¼Œè¯¥æ–¹æ³•è¶…è¶Šäº†å…ˆå‰çš„ä¸‰ç»´ç¼–è¾‘æŠ€æœ¯ï¼Œèƒ½å¤Ÿå®ç°é«˜è´¨é‡ã€ç²¾ç¡®ä¸”ç¨³å¥çš„è·¨å„ç§å½¢çŠ¶å’Œè¯­ä¹‰æ“çºµç¼–è¾‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè§†å›¾æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å’Œå›¾åƒé©±åŠ¨çš„3Dèµ„äº§ç”Ÿæˆä¸­å–å¾—äº†æˆåŠŸã€‚</li>
<li>åŸºäºæŒ‡ä»¤çš„ç¼–è¾‘æŠ€æœ¯åœ¨è´¨é‡å’Œè¡¨ç°æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚å…¶ä¸»è¦é—®é¢˜åœ¨äºé‡‡ç”¨äºŒç»´å…ˆéªŒæ–¹æ³•äº§ç”Ÿçš„è§†å›¾ä¸ä¸€è‡´ç¼–è¾‘ä¿¡å·ã€‚</li>
<li>ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒç¼–è¾‘æ–¹æ³•ï¼Œç›´æ¥åœ¨åŸç”Ÿä¸‰ç»´æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´å†…æ“ä½œï¼Œä»¥å®ç°ç²¾ç¡®çš„å‡ ä½•å›¾å½¢æ“ä½œã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†ç”Ÿæˆçš„ä¸‰ç»´æ³¨æ„åŠ›åœ°å›¾ä¸åŸå§‹å¯¹è±¡è¿›è¡Œåˆæˆç¼–è¾‘æŒ‡å¯¼ã€‚</li>
<li>é€šè¿‡å‡ ä½•æ„ŸçŸ¥æ­£åˆ™åŒ–æŒ‡å¯¼ã€é¢‘åŸŸå…‰è°±è°ƒåˆ¶ç­–ç•¥å’Œç”¨äºä¸‰ç»´å¢å¼ºçš„ç²¾ç‚¼æ­¥éª¤ï¼Œè¯¥æ–¹æ³•å®ç°äº†é«˜è´¨é‡ã€ç²¾ç¡®ä¸”ç¨³å¥çš„è·¨å„ç§å½¢çŠ¶å’Œè¯­ä¹‰æ“çºµç¼–è¾‘ã€‚</li>
<li>è¯¥æ–¹æ³•è¶…è¶Šäº†å…ˆå‰çš„ä¸‰ç»´ç¼–è¾‘æŠ€æœ¯ï¼Œå±•ç°å‡ºåœ¨å¤šç§å½¢çŠ¶å’Œè¯­ä¹‰æ“çºµæ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00269">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6be72b2534ccc62d15a98a20bb2e0c83~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930983&auth_key=1759930983-0-0-e31d5a9ae3bcffef683158cbe402a769&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5325a44dc5dd80c23b59801b382c6c90~resize:0:q75.jpg?source=1f5c5e47&expiration=1759930991&auth_key=1759930991-0-0-6817720899052e46820a1e4c0c2ce39c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-b6c5f222263fe5e7973d5c8b97b22740.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-f962e15a7f5c2ae473455c29f53ae784~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931007&auth_key=1759931007-0-0-c9f4260fa2e958ab50c4c95a57b28102&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Representation-Entanglement-for-Generation-Training-Diffusion-Transformers-Is-Much-Easier-Than-You-Think"><a href="#Representation-Entanglement-for-Generation-Training-Diffusion-Transformers-Is-Much-Easier-Than-You-Think" class="headerlink" title="Representation Entanglement for Generation: Training Diffusion   Transformers Is Much Easier Than You Think"></a>Representation Entanglement for Generation: Training Diffusion   Transformers Is Much Easier Than You Think</h2><p><strong>Authors:Ge Wu, Shen Zhang, Ruijing Shi, Shanghua Gao, Zhenyuan Chen, Lei Wang, Zhaowei Chen, Hongcheng Gao, Yao Tang, Jian Yang, Ming-Ming Cheng, Xiang Li</strong></p>
<p>REPA and its variants effectively mitigate training challenges in diffusion models by incorporating external visual representations from pretrained models, through alignment between the noisy hidden projections of denoising networks and foundational clean image representations. We argue that the external alignment, which is absent during the entire denoising inference process, falls short of fully harnessing the potential of discriminative representations. In this work, we propose a straightforward method called Representation Entanglement for Generation (REG), which entangles low-level image latents with a single high-level class token from pretrained foundation models for denoising. REG acquires the capability to produce coherent image-class pairs directly from pure noise, substantially improving both generation quality and training efficiency. This is accomplished with negligible additional inference overhead, requiring only one single additional token for denoising (&lt;0.5% increase in FLOPs and latency). The inference process concurrently reconstructs both image latents and their corresponding global semantics, where the acquired semantic knowledge actively guides and enhances the image generation process. On ImageNet 256$\times$256, SiT-XL&#x2F;2 + REG demonstrates remarkable convergence acceleration, achieving $\textbf{63}\times$ and $\textbf{23}\times$ faster training than SiT-XL&#x2F;2 and SiT-XL&#x2F;2 + REPA, respectively. More impressively, SiT-L&#x2F;2 + REG trained for merely 400K iterations outperforms SiT-XL&#x2F;2 + REPA trained for 4M iterations ($\textbf{10}\times$ longer). Code is available at: <a target="_blank" rel="noopener" href="https://github.com/Martinser/REG">https://github.com/Martinser/REG</a>. </p>
<blockquote>
<p>REPAåŠå…¶å˜ä½“é€šè¿‡èå…¥é¢„è®­ç»ƒæ¨¡å‹çš„å¤–éƒ¨è§†è§‰è¡¨å¾ï¼Œä»¥åŠé€šè¿‡å¯¹å»å™ªç½‘ç»œçš„å™ªå£°éšè—æŠ•å½±å’ŒåŸºç¡€æ€§å¹²å‡€å›¾åƒè¡¨å¾ä¹‹é—´çš„å¯¹é½ï¼Œæœ‰æ•ˆç¼“è§£äº†æ‰©æ•£æ¨¡å‹ä¸­çš„è®­ç»ƒæŒ‘æˆ˜ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨æ•´ä¸ªå»å™ªæ¨ç†è¿‡ç¨‹ä¸­ç¼ºå¤±çš„å¤–éƒ¨å¯¹é½ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨åˆ¤åˆ«æ€§è¡¨å¾çš„æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºç”Ÿæˆè¡¨ç¤ºçº ç¼ ï¼ˆREGï¼‰çš„ç›´è§‚æ–¹æ³•ï¼Œå®ƒå°†å›¾åƒçš„ä½çº§æ½œåœ¨ç‰¹å¾ä¸é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„å•ä¸ªé«˜çº§ç±»åˆ«ä»¤ç‰Œçº ç¼ åœ¨ä¸€èµ·ï¼Œç”¨äºå»å™ªã€‚REGå…·å¤‡ç›´æ¥ä»çº¯å™ªå£°ä¸­äº§ç”Ÿè¿è´¯çš„å›¾åƒç±»åˆ«å¯¹çš„èƒ½åŠ›ï¼Œå¯å¤§å¹…æé«˜ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡ã€‚è¿™åªéœ€å¢åŠ æå°‘é‡çš„æ¨ç†å¼€é”€ï¼Œä»…éœ€è¦ä¸€ä¸ªé¢å¤–çš„ä»¤ç‰Œç”¨äºå»å™ªï¼ˆå¢åŠ FLOPså’Œå»¶è¿Ÿçš„æ—¶é—´ä¸åˆ°0.5%ï¼‰ã€‚æ¨ç†è¿‡ç¨‹åŒæ—¶é‡å»ºå›¾åƒæ½œåœ¨ç‰¹å¾åŠå…¶å¯¹åº”çš„å…¨å±€è¯­ä¹‰ï¼Œæ‰€è·å–çš„è¯­ä¹‰çŸ¥è¯†ç§¯ææŒ‡å¯¼å¹¶å¢å¼ºå›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨ImageNet 256Ã—256ä¸Šï¼ŒSiT-XL&#x2F;2 + REGæ˜¾ç¤ºå‡ºä»¤äººç©ç›®çš„æ”¶æ•›åŠ é€Ÿæ•ˆæœï¼Œç›¸å¯¹äºSiT-XL&#x2F;2å’ŒSiT-XL&#x2F;2 + REPAåˆ†åˆ«è¾¾åˆ°äº†63å€å’Œ23å€çš„æ›´å¿«è®­ç»ƒé€Ÿåº¦ã€‚æ›´ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼Œä»…ç»è¿‡40ä¸‡æ¬¡è¿­ä»£çš„SiT-L&#x2F;2 + REGçš„æ€§èƒ½è¶…è¶Šäº†ç»è¿‡4ç™¾ä¸‡æ¬¡è¿­ä»£ï¼ˆé•¿è¾¾10å€ï¼‰çš„SiT-XL&#x2F;2 + REPAã€‚ç›¸å…³ä»£ç å¯åœ¨ä»¥ä¸‹é“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/Martinser/REG%E3%80%82">https://github.com/Martinser/REGã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01467v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>REPAåŠå…¶å˜ä½“é€šè¿‡èå…¥é¢„è®­ç»ƒæ¨¡å‹çš„å¤–éƒ¨è§†è§‰è¡¨å¾ï¼Œæœ‰æ•ˆç¼“è§£äº†æ‰©æ•£æ¨¡å‹åœ¨è®­ç»ƒæ–¹é¢çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡é€šè¿‡å¯¹é½å»å™ªç½‘ç»œçš„å™ªå£°éšè—æŠ•å½±å’Œæ¸…æ´å›¾åƒçš„åŸºç¡€è¡¨å¾ï¼Œå®ç°äº†è¿™ä¸€ç‚¹ã€‚ç„¶è€Œï¼Œæœ¬æ–‡è®¤ä¸ºï¼Œåœ¨å»å™ªæ¨æ–­è¿‡ç¨‹ä¸­å®Œå…¨ç¼ºå¤±çš„å¤–éƒ¨å¯¹é½ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨åˆ¤åˆ«å¼è¡¨å¾çš„æ½œåŠ›ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºREGï¼ˆç”¨äºç”Ÿæˆçš„è¡¨ç¤ºçº ç¼ ï¼‰çš„ç›´è§‚æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ä½çº§åˆ«å›¾åƒæ½œèƒ½ä¸é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ä¸­çš„å•ä¸ªé«˜çº§ç±»åˆ«ä»¤ç‰Œçº ç¼ åœ¨ä¸€èµ·ï¼Œç”¨äºå»å™ªã€‚REGèƒ½å¤Ÿä»çº¯å™ªå£°ä¸­ç›´æ¥ç”Ÿæˆè¿è´¯çš„å›¾åƒç±»åˆ«å¯¹ï¼Œä»è€Œå¤§å¤§æé«˜äº†ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡ã€‚è¿™ä¸€åˆ‡çš„å®ç°åªéœ€å¢åŠ ä¸€ä¸ªé¢å¤–çš„ä»¤ç‰Œç”¨äºå»å™ªï¼ˆå¢åŠ çš„è®¡ç®—é‡å’Œå»¶è¿Ÿå‡ä¸åˆ°0.5%ï¼‰ï¼Œå‡ ä¹æ— éœ€å¢åŠ ä»»ä½•æ¨ç†å¼€é”€ã€‚åœ¨ImageNet 256Ã—256ä¸Šï¼ŒSiT-XL&#x2F;2 + REGæ˜¾ç¤ºå‡ºä»¤äººç©ç›®çš„æ”¶æ•›åŠ é€Ÿèƒ½åŠ›ï¼Œç›¸å¯¹äºSiT-XL&#x2F;2å’ŒSiT-XL&#x2F;2 + REPAåˆ†åˆ«è¾¾åˆ°äº†63å€å’Œ23å€çš„è®­ç»ƒåŠ é€Ÿã€‚æ›´ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼Œä»…è®­ç»ƒäº†40ä¸‡æ¬¡çš„SiT-L&#x2F;2 + REGè¶…è¿‡äº†è®­ç»ƒäº†4ç™¾ä¸‡æ¬¡çš„SiT-XL&#x2F;2 + REPAï¼ˆæ—¶é—´é•¿è¾¾åå€ï¼‰ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Martinser/REG%E3%80%82">https://github.com/Martinser/REGã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>REPAåŠå…¶å˜ä½“é€šè¿‡èå…¥å¤–éƒ¨è§†è§‰è¡¨å¾ç¼“è§£äº†æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒæŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„REGæ–¹æ³•ç»“åˆäº†ä½çº§åˆ«å›¾åƒæ½œèƒ½å’Œé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ä¸­çš„é«˜çº§ç±»åˆ«ä»¤ç‰Œï¼Œç”¨äºå»å™ªã€‚</li>
<li>REGèƒ½ç›´æ¥ä»å™ªå£°ç”Ÿæˆè¿è´¯çš„å›¾åƒç±»åˆ«å¯¹ï¼Œæé«˜äº†ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡ã€‚</li>
<li>REGå¢åŠ çš„è®¡ç®—é‡å’Œå»¶è¿Ÿå¾®ä¹å…¶å¾®ï¼ˆä¸åˆ°0.5%ï¼‰ã€‚</li>
<li>SiT-XL&#x2F;2 + REGåœ¨ImageNet 256Ã—256ä¸Šå®ç°äº†å¿«é€Ÿæ”¶æ•›ï¼Œç›¸è¾ƒäºå…¶ä»–æ¨¡å‹æœ‰æ˜¾è‘—åŠ é€Ÿã€‚</li>
<li>SiT-L&#x2F;2 + REGåœ¨è¾ƒçŸ­çš„è®­ç»ƒæ—¶é—´å†…ï¼ˆä»…40ä¸‡æ¬¡è¿­ä»£ï¼‰è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†é•¿æ—¶é—´è®­ç»ƒçš„SiT-XL&#x2F;2 + REPAã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-19f5ae4da13ae5b6259c4116329284d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931017&auth_key=1759931017-0-0-e5de03b3b0ddb2a42b9b330e13a51217&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a94440b5033f6d65205635b7dcfc0243~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931027&auth_key=1759931027-0-0-acba9d42f1cb084437ceef29a5da1ab0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d6004161a0d9b0d3ed32e7d0df68aac3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931038&auth_key=1759931038-0-0-3d375454d2f74f89e79b3b04f79d6328&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Reward-Agnostic-Prompt-Optimization-for-Text-to-Image-Diffusion-Models"><a href="#Reward-Agnostic-Prompt-Optimization-for-Text-to-Image-Diffusion-Models" class="headerlink" title="Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models"></a>Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models</h2><p><strong>Authors:Semin Kim, Yeonwoo Cha, Jaehoon Yoo, Seunghoon Hong</strong></p>
<p>We investigate a general approach for improving user prompts in text-to-image (T2I) diffusion models by finding prompts that maximize a reward function specified at test-time. Although diverse reward models are used for evaluating image generation, existing automated prompt engineering methods typically target specific reward configurations. Consequently, these specialized designs exhibit suboptimal performance when applied to new prompt engineering scenarios involving different reward models. To address this limitation, we introduce RATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time optimization method applicable across various reward scenarios without modification. RATTPO iteratively searches for optimized prompts by querying large language models (LLMs) \textit{without} requiring reward-specific task descriptions. Instead, it uses the optimization trajectory and a novel reward-aware feedback signal (termed a â€œhintâ€) as context. Empirical results demonstrate the versatility of RATTPO, effectively enhancing user prompts across diverse reward setups that assess various generation aspects, such as aesthetics, general human preference, or spatial relationships between objects. RATTPO surpasses other test-time search baselines in search efficiency, running 4.8 times faster than naive reward-agnostic test-time search baseline on average. Furthermore, with sufficient inference budget, it can achieve comparable performance to learning-based baselines that require reward-specific fine-tuning. The code is available at <a target="_blank" rel="noopener" href="https://github.com/seminkim/RATTPO">https://github.com/seminkim/RATTPO</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§é€šè¿‡å¯»æ‰¾æœ€å¤§åŒ–æµ‹è¯•æ—¶æŒ‡å®šå¥–åŠ±å‡½æ•°çš„æç¤ºæ¥æ”¹å–„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸­çš„ç”¨æˆ·æç¤ºçš„é€šç”¨æ–¹æ³•ã€‚å°½ç®¡ç”¨äºè¯„ä¼°å›¾åƒç”Ÿæˆçš„ä¸åŒå¥–åŠ±æ¨¡å‹å·²ç»è¢«ä½¿ç”¨ï¼Œä½†ç°æœ‰çš„è‡ªåŠ¨åŒ–æç¤ºå·¥ç¨‹æ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šçš„å¥–åŠ±é…ç½®ã€‚å› æ­¤ï¼Œå½“å°†è¿™äº›ç‰¹æ®Šè®¾è®¡åº”ç”¨äºæ¶‰åŠä¸åŒå¥–åŠ±æ¨¡å‹çš„æ–°æç¤ºå·¥ç¨‹åœºæ™¯æ—¶ï¼Œå…¶æ€§èƒ½è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¥–åŠ±æ— å…³æµ‹è¯•æ—¶é—´æç¤ºä¼˜åŒ–ï¼ˆRATTPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§çµæ´»çš„æµ‹è¯•æ—¶é—´ä¼˜åŒ–æ–¹æ³•ï¼Œå¯è·¨å„ç§å¥–åŠ±åœºæ™¯è¿›è¡Œåº”ç”¨ï¼Œæ— éœ€è¿›è¡Œä¿®æ”¹ã€‚RATTPOé€šè¿‡æŸ¥è¯¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¿­ä»£æœç´¢ä¼˜åŒ–çš„æç¤ºï¼Œè€Œæ— éœ€ç‰¹å®šçš„å¥–åŠ±ä»»åŠ¡æè¿°ã€‚ç›¸åï¼Œå®ƒä½¿ç”¨ä¼˜åŒ–è½¨è¿¹å’Œä¸€ç§æ–°é¢–çš„å¥–åŠ±æ„ŸçŸ¥åé¦ˆä¿¡å·ï¼ˆç§°ä¸ºâ€œçº¿ç´¢â€ï¼‰ä½œä¸ºä¸Šä¸‹æ–‡ã€‚ç»éªŒç»“æœè¡¨æ˜RATTPOçš„é€šç”¨æ€§ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºç”¨æˆ·åœ¨ä¸åŒå¥–åŠ±è®¾ç½®ä¸‹çš„æç¤ºï¼Œè¿™äº›å¥–åŠ±è®¾ç½®è¯„ä¼°äº†è¯¸å¦‚ç¾å­¦ã€ä¸€èˆ¬äººç±»åå¥½æˆ–å¯¹è±¡ä¹‹é—´çš„ç©ºé—´å…³ç³»ç­‰å„ç§ç”Ÿæˆæ–¹é¢ã€‚RATTPOåœ¨æœç´¢æ•ˆç‡ä¸Šè¶…è¶Šäº†å…¶ä»–æµ‹è¯•æ—¶é—´æœç´¢åŸºçº¿ï¼Œå¹³å‡é€Ÿåº¦æ¯”ç®€å•çš„å¥–åŠ±æ— å…³æµ‹è¯•æ—¶é—´æœç´¢åŸºçº¿å¿«4.8å€ã€‚æ­¤å¤–ï¼Œåœ¨è¶³å¤Ÿçš„æ¨ç†é¢„ç®—ä¸‹ï¼Œå®ƒå¯ä»¥å®ç°ä¸å­¦ä¹ å‹åŸºçº¿çš„ç›¸å½“æ€§èƒ½ï¼Œè€Œå­¦ä¹ å‹åŸºçº¿éœ€è¦é’ˆå¯¹å¥–åŠ±è¿›è¡Œç‰¹å®šå¾®è°ƒã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/seminkim/RATTPO%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/seminkim/RATTPOä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16853v2">PDF</a> 29 pages, Under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†é€šè¿‡æ‰¾åˆ°æœ€å¤§åŒ–æµ‹è¯•æ—¶æŒ‡å®šçš„å¥–åŠ±å‡½æ•°çš„ç”¨æˆ·æç¤ºæ¥æ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸­çš„ç”¨æˆ·æç¤ºçš„é€šç”¨æ–¹æ³•ã€‚é’ˆå¯¹å›¾åƒç”Ÿæˆè¯„ä¼°ä¸­ä½¿ç”¨çš„ä¸åŒå¥–åŠ±æ¨¡å‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§çµæ´»çš„åœ¨æµ‹è¯•æ—¶è¿›è¡Œä¼˜åŒ–æ–¹æ³•â€”â€”RATTPOï¼ˆå¥–åŠ±æ— å…³æµ‹è¯•æ—¶é—´æç¤ºä¼˜åŒ–ï¼‰ã€‚RATTPOé€šè¿‡æŸ¥è¯¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è€Œæ— éœ€ç‰¹å®šå¥–åŠ±ä»»åŠ¡çš„æè¿°ï¼Œåˆ©ç”¨ä¼˜åŒ–è½¨è¿¹å’Œä¸€ç§æ–°é¢–çš„å¥–åŠ±æ„ŸçŸ¥åé¦ˆä¿¡å·ï¼ˆç§°ä¸ºâ€œçº¿ç´¢â€ï¼‰ä½œä¸ºä¸Šä¸‹æ–‡æ¥è¿­ä»£æœç´¢ä¼˜åŒ–çš„æç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRATTPOåœ¨å„ç§å¥–åŠ±è®¾ç½®ä¸‹éƒ½èƒ½æœ‰æ•ˆåœ°å¢å¼ºç”¨æˆ·æç¤ºï¼Œå¹¶åœ¨æœç´¢æ•ˆç‡ä¸Šè¶…è¶Šäº†å…¶ä»–æµ‹è¯•æ—¶é—´æœç´¢åŸºçº¿ï¼Œåœ¨å¹³å‡æƒ…å†µä¸‹è¿è¡Œé€Ÿåº¦æ˜¯æœ´ç´ å¥–åŠ±æ— å…³æµ‹è¯•æ—¶é—´æœç´¢åŸºçº¿çš„4.8å€ã€‚åœ¨å……è¶³æ¨ç†é¢„ç®—ä¸‹ï¼Œå…¶æ€§èƒ½å¯ä¸éœ€è¦å¥–åŠ±ç‰¹å®šå¾®è°ƒçš„å­¦ä¹ åŸºçº¿ç›¸åª²ç¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§é€šç”¨çš„æ–¹æ³•ï¼Œé€šè¿‡æ‰¾åˆ°æœ€å¤§åŒ–æµ‹è¯•æ—¶å¥–åŠ±å‡½æ•°çš„æç¤ºæ¥æ”¹å–„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„ç”¨æˆ·æç¤ºã€‚</li>
<li>ä»‹ç»äº†RATTPOï¼ˆå¥–åŠ±æ— å…³æµ‹è¯•æ—¶é—´æç¤ºä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§çµæ´»çš„æ–¹æ³•ï¼Œå¯åº”ç”¨äºå„ç§å¥–åŠ±åœºæ™¯ï¼Œè€Œæ— éœ€è¿›è¡Œä¿®æ”¹ã€‚</li>
<li>RATTPOé€šè¿‡æŸ¥è¯¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¿­ä»£æœç´¢ä¼˜åŒ–çš„æç¤ºï¼Œä¸éœ€è¦ç‰¹å®šå¥–åŠ±ä»»åŠ¡çš„æè¿°ã€‚</li>
<li>RATTPOåˆ©ç”¨ä¼˜åŒ–è½¨è¿¹å’Œä¸€ç§æ–°é¢–çš„å¥–åŠ±æ„ŸçŸ¥åé¦ˆä¿¡å·ï¼ˆç§°ä¸ºâ€œçº¿ç´¢â€ï¼‰ä½œä¸ºä¸Šä¸‹æ–‡ã€‚</li>
<li>RATTPOåœ¨å„ç§å¥–åŠ±è®¾ç½®ä¸‹éƒ½èƒ½æœ‰æ•ˆåœ°å¢å¼ºç”¨æˆ·æç¤ºï¼Œå®éªŒç»“æœè¡¨æ˜å…¶æœç´¢æ•ˆç‡è¶…è¶Šäº†å…¶ä»–æµ‹è¯•æ—¶é—´æœç´¢åŸºçº¿ã€‚</li>
<li>RATTPOè¿è¡Œé€Ÿåº¦å¿«ï¼Œåœ¨å¹³å‡æƒ…å†µä¸‹æ˜¯æœ´ç´ æœç´¢åŸºçº¿çš„4.8å€ã€‚</li>
<li>åœ¨å……è¶³æ¨ç†é¢„ç®—ä¸‹ï¼ŒRATTPOçš„æ€§èƒ½å¯ä¸éœ€è¦å¥–åŠ±ç‰¹å®šå¾®è°ƒçš„å­¦ä¹ åŸºçº¿ç›¸åª²ç¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16853">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-380abdec9fbeeff740989aab7bde931d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931053&auth_key=1759931053-0-0-c1e5c5eee93e64d3efc0e16f84afa969&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-ca1edc0f98efe81630e20901035fa292.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-12df4ace6d4bfef2124de6cc6feeddf9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931067&auth_key=1759931067-0-0-b1d03730e79765f88784f440ad32b5b3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-318a3cdbdf4a044682a9c8d00358c079~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931074&auth_key=1759931074-0-0-a4867f14234a5123807bf18cd214d69f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CryoCCD-Conditional-Cycle-consistent-Diffusion-with-Biophysical-Modeling-for-Cryo-EM-Synthesis"><a href="#CryoCCD-Conditional-Cycle-consistent-Diffusion-with-Biophysical-Modeling-for-Cryo-EM-Synthesis" class="headerlink" title="CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical   Modeling for Cryo-EM Synthesis"></a>CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical   Modeling for Cryo-EM Synthesis</h2><p><strong>Authors:Runmin Jiang, Genpei Zhang, Yuntian Yang, Siqi Wu, Minhao Wu, Wanyue Feng, Yizhou Zhao, Xi Xiao, Xiao Wang, Tianyang Wang, Xingjian Li, Muyuan Chen, Min Xu</strong></p>
<p>Single-particle cryo-electron microscopy (cryo-EM) has become a cornerstone of structural biology, enabling near-atomic resolution analysis of macromolecules through advanced computational methods. However, the development of cryo-EM processing tools is constrained by the scarcity of high-quality annotated datasets. Synthetic data generation offers a promising alternative, but existing approaches lack thorough biophysical modeling of heterogeneity and fail to reproduce the complex noise observed in real imaging. To address these limitations, we present CryoCCD, a synthesis framework that unifies versatile biophysical modeling with the first conditional cycle-consistent diffusion model tailored for cryo-EM. The biophysical engine provides multi-functional generation capabilities to capture authentic biological organization, and the diffusion model is enhanced with cycle consistency and mask-guided contrastive learning to ensure realistic noise while preserving structural fidelity. Extensive experiments demonstrate that CryoCCD generates structurally faithful micrographs, enhances particle picking and pose estimation, as well as achieves superior performance over state-of-the-art baselines, while also generalizing effectively to held-out protein families. </p>
<blockquote>
<p>å•é¢—ç²’å†·å†»ç”µå­æ˜¾å¾®é•œï¼ˆcryo-EMï¼‰å·²æˆä¸ºç»“æ„ç”Ÿç‰©å­¦çš„æ ¸å¿ƒï¼Œèƒ½å¤Ÿé€šè¿‡å…ˆè¿›çš„è®¡ç®—æ–¹æ³•å®ç°å¯¹å¤§åˆ†å­çš„æ¥è¿‘åŸå­åˆ†è¾¨ç‡åˆ†æã€‚ç„¶è€Œï¼Œç”±äºé«˜è´¨é‡æ³¨é‡Šæ•°æ®é›†çš„ç¨€ç¼ºï¼Œå†·å†»ç”µé•œå¤„ç†å·¥å…·çš„å‘å±•å—åˆ°é™åˆ¶ã€‚åˆæˆæ•°æ®ç”Ÿæˆæä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•ç¼ºä¹å¼‚è´¨æ€§çš„å½»åº•ç”Ÿç‰©ç‰©ç†å»ºæ¨¡ï¼Œä¸”æ— æ³•å¤åˆ¶çœŸå®æˆåƒä¸­è§‚å¯Ÿåˆ°çš„å¤æ‚å™ªå£°ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†CryoCCDï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆæ¡†æ¶ï¼Œå®ƒå°†é€šç”¨çš„ç”Ÿç‰©ç‰©ç†å»ºæ¨¡ä¸é¦–ä¸ªé’ˆå¯¹å†·å†»ç”µé•œè®¾è®¡çš„æ¡ä»¶å¾ªç¯ä¸€è‡´æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆã€‚ç”Ÿç‰©ç‰©ç†å¼•æ“æä¾›å¤šåŠŸèƒ½ç”Ÿæˆèƒ½åŠ›ï¼Œä»¥æ•æ‰çœŸå®çš„ç”Ÿç‰©ç»„ç»‡ï¼Œæ‰©æ•£æ¨¡å‹é€šè¿‡å¾ªç¯ä¸€è‡´æ€§å’Œæ©è†œå¼•å¯¼å¯¹æ¯”å­¦ä¹ è¿›è¡Œå¢å¼ºï¼Œä»¥ç¡®ä¿äº§ç”Ÿé€¼çœŸçš„å™ªå£°åŒæ—¶ä¿æŒç»“æ„ä¿çœŸåº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCryoCCDç”Ÿæˆçš„ç»“æ„å¿ å®æ˜¾å¾®å›¾ï¼Œæé«˜äº†ç²’å­æŒ‘é€‰å’Œå§¿æ€ä¼°è®¡ï¼Œå¹¶ä¸”ç›¸è¾ƒäºæœ€å…ˆè¿›çš„åŸºç¡€çº¿æ¨¡å‹å®ç°äº†å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°æ¨å¹¿åˆ°æœªå‚ä¸è®­ç»ƒçš„è›‹ç™½è´¨å®¶æ—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23444v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å•é¢—ç²’å†·å†»ç”µå­æ˜¾å¾®é•œï¼ˆcryo-EMï¼‰æ˜¯ç»“æ„ç”Ÿç‰©å­¦é¢†åŸŸçš„æ ¸å¿ƒï¼Œå¯é€šè¿‡å…ˆè¿›çš„è®¡ç®—æ–¹æ³•å®ç°è¿‘åŸå­åˆ†è¾¨ç‡çš„åˆ†å­åˆ†æã€‚ç„¶è€Œï¼Œç”±äºé«˜è´¨é‡æ ‡æ³¨æ•°æ®é›†çš„ç¨€ç¼ºï¼Œcryo-EMå¤„ç†å·¥å…·çš„å‘å±•å—åˆ°é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†CryoCCDåˆæˆæ¡†æ¶ï¼Œç»“åˆäº†å¤šåŠŸèƒ½ç”Ÿç‰©ç‰©ç†å»ºæ¨¡ä¸é’ˆå¯¹cryo-EMçš„é¦–ä¸ªæ¡ä»¶å¾ªç¯ä¸€è‡´æ€§æ‰©æ•£æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒCryoCCDç”Ÿæˆçš„ç»“æ„å¿ å®äºå¾®å›¾ï¼Œæé«˜äº†ç²’å­é€‰å–å’Œå§¿æ€ä¼°è®¡çš„ç²¾ç¡®åº¦ï¼Œä¸”åœ¨å¯¹æ¯”å‰æ²¿åŸºçº¿æ—¶è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶å¯æœ‰æ•ˆæ¨å¹¿åˆ°æœªçŸ¥è›‹ç™½è´¨å®¶æ—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å•é¢—ç²’å†·å†»ç”µå­æ˜¾å¾®é•œï¼ˆcryo-EMï¼‰å·²æˆä¸ºç»“æ„ç”Ÿç‰©å­¦ä¸­çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œå…·æœ‰è¿‘åŸå­åˆ†è¾¨ç‡åˆ†æåŠŸèƒ½ã€‚</li>
<li>ç°æœ‰çš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ç¼ºä¹ç”Ÿç‰©ç‰©ç†å¼‚è´¨æ€§çš„å»ºæ¨¡ï¼Œæœªèƒ½å®Œå…¨å¤åˆ¶çœŸå®æˆåƒä¸­çš„å¤æ‚å™ªå£°ã€‚</li>
<li>é’ˆå¯¹è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†CryoCCDåˆæˆæ¡†æ¶ï¼Œèåˆäº†å¤šåŠŸèƒ½ç”Ÿç‰©ç‰©ç†å»ºæ¨¡å’Œä¸“ä¸ºcryo-EMè®¾è®¡çš„é¦–ä¸ªæ¡ä»¶å¾ªç¯ä¸€è‡´æ€§æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>ç”Ÿç‰©ç‰©ç†å¼•æ“å¯æ¨¡æ‹ŸçœŸå®çš„ç”Ÿç‰©ç»„ç»‡ç»“æ„ã€‚æ‰©æ•£æ¨¡å‹åˆ™é€šè¿‡å¾ªç¯ä¸€è‡´æ€§å’Œæ©è†œå¼•å¯¼å¯¹æ¯”å­¦ä¹ ç¡®ä¿ç”Ÿæˆçš„å™ªå£°å…·æœ‰çœŸå®æ€§å¹¶ç»´æŒç»“æ„å¿ å®åº¦ã€‚</li>
<li>å®éªŒè¯æ˜CryoCCDç”Ÿæˆçš„å¾®å›¾ç»“æ„çœŸå®å¯é ï¼Œæ”¹è¿›äº†ç²’å­é€‰å–å’Œå§¿æ€ä¼°è®¡çš„å‡†ç¡®åº¦ã€‚</li>
<li>ä¸å½“å‰æœ€å‰æ²¿çš„æ–¹æ³•ç›¸æ¯”ï¼ŒCryoCCDå±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3c773267d299ad25b188274c3aa9d71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-507641e78048c5776fdac7f08be4bfc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbc7df44b26bddbdbf4dfa152a6f6421.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SANA-Sprint-One-Step-Diffusion-with-Continuous-Time-Consistency-Distillation"><a href="#SANA-Sprint-One-Step-Diffusion-with-Continuous-Time-Consistency-Distillation" class="headerlink" title="SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency   Distillation"></a>SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency   Distillation</h2><p><strong>Authors:Junsong Chen, Shuchen Xue, Yuyang Zhao, Jincheng Yu, Sayak Paul, Junyu Chen, Han Cai, Song Han, Enze Xie</strong></p>
<p>This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast text-to-image (T2I) generation. SANA-Sprint is built on a pre-trained foundation model and augmented with hybrid distillation, dramatically reducing inference steps from 20 to 1-4. We introduce three key innovations: (1) We propose a training-free approach that transforms a pre-trained flow-matching model for continuous-time consistency distillation (sCM), eliminating costly training from scratch and achieving high training efficiency. Our hybrid distillation strategy combines sCM with latent adversarial distillation (LADD): sCM ensures alignment with the teacher model, while LADD enhances single-step generation fidelity. (2) SANA-Sprint is a unified step-adaptive model that achieves high-quality generation in 1-4 steps, eliminating step-specific training and improving efficiency. (3) We integrate ControlNet with SANA-Sprint for real-time interactive image generation, enabling instant visual feedback for user interaction. SANA-Sprint establishes a new Pareto frontier in speed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID and 0.74 GenEval in only 1 step - outperforming FLUX-schnell (7.94 FID &#x2F; 0.71 GenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s (T2I) and 0.25s (ControlNet) latency for 1024 x 1024 images on H100, and 0.31s (T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for AI-powered consumer applications (AIPC). Code and pre-trained models will be open-sourced. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†SANA-Sprintï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè¶…å¿«é€Ÿæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆã€‚SANA-Sprintå»ºç«‹åœ¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ä¸Šï¼Œå¹¶è¾…ä»¥æ··åˆè’¸é¦æŠ€æœ¯ï¼Œå°†æ¨ç†æ­¥éª¤ä»20æ­¥å¤§å¹…å‡å°‘åˆ°1-4æ­¥ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå°†é¢„è®­ç»ƒçš„æµåŒ¹é…æ¨¡å‹è½¬æ¢ä¸ºè¿ç»­æ—¶é—´ä¸€è‡´æ€§è’¸é¦ï¼ˆsCMï¼‰ï¼Œæ¶ˆé™¤äº†ä»å¤´å¼€å§‹çš„é«˜æˆæœ¬è®­ç»ƒï¼Œå®ç°äº†é«˜è®­ç»ƒæ•ˆç‡ã€‚æˆ‘ä»¬çš„æ··åˆè’¸é¦ç­–ç•¥ç»“åˆäº†sCMå’Œæ½œåœ¨å¯¹æŠ—æ€§è’¸é¦ï¼ˆLADDï¼‰ï¼šsCMç¡®ä¿ä¸æ•™å¸ˆæ¨¡å‹çš„å¯¹é½ï¼Œè€ŒLADDå¢å¼ºå•æ­¥ç”Ÿæˆä¿çœŸåº¦ã€‚ï¼ˆ2ï¼‰SANA-Sprintæ˜¯ä¸€ç§ç»Ÿä¸€çš„æ­¥é•¿è‡ªé€‚åº”æ¨¡å‹ï¼Œåœ¨1-4æ­¥å†…å®ç°é«˜è´¨é‡ç”Ÿæˆï¼Œæ¶ˆé™¤äº†é’ˆå¯¹ç‰¹å®šæ­¥éª¤çš„è®­ç»ƒï¼Œæé«˜äº†æ•ˆç‡ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬å°†ControlNetä¸SANA-Sprinté›†æˆï¼Œç”¨äºå®æ—¶äº¤äº’å¼å›¾åƒç”Ÿæˆï¼Œå®ç°å³æ—¶è§†è§‰åé¦ˆä»¥ä¾›ç”¨æˆ·äº¤äº’ã€‚SANA-Sprintåœ¨é€Ÿåº¦ä¸è´¨é‡æƒè¡¡æ–¹é¢å»ºç«‹äº†æ–°çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œä»¥7.59çš„FIDå’Œ0.74çš„GenEvalåœ¨ä»…1æ­¥å†…å®ç°äº†æœ€æ–°æ€§èƒ½â€”â€”ä¼˜äºFLUX-schnellï¼ˆ7.94 FID &#x2F; 0.71 GenEvalï¼‰ï¼ŒåŒæ—¶é€Ÿåº¦æé«˜äº†10å€ï¼ˆH100ä¸Šä¸º0.1ç§’å¯¹1.1ç§’ï¼‰ã€‚åœ¨H100ä¸Šï¼Œå¯¹äº1024 x 1024å›¾åƒï¼Œå®ƒçš„T2Iå’ŒControlNetå»¶è¿Ÿåˆ†åˆ«è¾¾åˆ°äº†0.1ç§’å’Œ0.25ç§’ï¼›åœ¨RTX 4090ä¸Šï¼ŒT2Iå»¶è¿Ÿä¸º0.31ç§’ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„æ•ˆç‡å’Œåœ¨AIé©±åŠ¨çš„æ¶ˆè´¹çº§åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†å¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09641v4">PDF</a> 22 pages, 11 figures, 8 tables, In submission</p>
<p><strong>Summary</strong></p>
<pre><code>æœ¬æ–‡ä»‹ç»äº†SANA-Sprintï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè¶…å¿«é€Ÿæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆã€‚å®ƒé€šè¿‡é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹æ„å»ºï¼Œå¹¶é‡‡ç”¨æ··åˆè’¸é¦æŠ€æœ¯ï¼Œæ˜¾è‘—å°†æ¨ç†æ­¥éª¤ä»20å‡å°‘åˆ°1-4æ­¥ã€‚å¼•å…¥ä¸‰ç§å…³é”®æŠ€æœ¯ï¼šè®­ç»ƒè‡ªç”±æ–¹æ³•ã€ç»Ÿä¸€æ­¥é•¿è‡ªé€‚åº”æ¨¡å‹å’Œä¸æ§åˆ¶ç½‘çš„é›†æˆã€‚SANA-Sprintåœ¨é€Ÿåº¦å’Œè´¨é‡çš„æƒè¡¡æ–¹é¢å»ºç«‹äº†æ–°çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SANA-Sprintæ˜¯åŸºäºé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè¶…å¿«é€Ÿæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚</li>
<li>é€šè¿‡æ··åˆè’¸é¦æŠ€æœ¯ï¼Œæ˜¾è‘—å‡å°‘æ¨ç†æ­¥éª¤ï¼Œä»20æ­¥é™è‡³1-4æ­¥ã€‚</li>
<li>å¼•å…¥è®­ç»ƒè‡ªç”±æ–¹æ³•ï¼Œå®ç°ä¸é¢„è®­ç»ƒæµåŒ¹é…æ¨¡å‹çš„è½¬æ¢ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>æ··åˆè’¸é¦ç­–ç•¥ç»“åˆsCMå’ŒLADDï¼Œæé«˜ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ã€‚</li>
<li>SANA-Sprintæ˜¯ç»Ÿä¸€æ­¥é•¿è‡ªé€‚åº”æ¨¡å‹ï¼Œå®ç°é«˜è´¨é‡ç”Ÿæˆåœ¨1-4æ­¥å†…ã€‚</li>
<li>ä¸ControlNeté›†æˆï¼Œå®ç°å®æ—¶äº¤äº’å¼å›¾åƒç”Ÿæˆï¼Œæä¾›ç”¨æˆ·å³æ—¶è§†è§‰åé¦ˆã€‚</li>
<li>SANA-Sprintåœ¨é€Ÿåº¦å’Œè´¨é‡çš„æƒè¡¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2f88751f3c8b0537a931c0ff8d00b60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d0293c17fc0384d6c127ad3288563b7.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a4d94983aed96a4d802208b6f0ce0ee~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931130&auth_key=1759931130-0-0-9d820096b52427ab14f102c46f495558&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-3d41f8e4617bd2d11ee2eab33a58815a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-562949c5d16dd0230f533e291cd9a7e4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Diffusion-Based-mmWave-Radar-Point-Cloud-Enhancement-Driven-by-Range-Images"><a href="#Diffusion-Based-mmWave-Radar-Point-Cloud-Enhancement-Driven-by-Range-Images" class="headerlink" title="Diffusion-Based mmWave Radar Point Cloud Enhancement Driven by Range   Images"></a>Diffusion-Based mmWave Radar Point Cloud Enhancement Driven by Range   Images</h2><p><strong>Authors:Ruixin Wu, Zihan Li, Jin Wang, Xiangyu Xu, Zhi Zheng, Kaixiang Huang, Guodong Lu</strong></p>
<p>Millimeter-wave (mmWave) radar has attracted significant attention in robotics and autonomous driving. However, despite the perception stability in harsh environments, the point cloud generated by mmWave radar is relatively sparse while containing significant noise, which limits its further development. Traditional mmWave radar enhancement approaches often struggle to leverage the effectiveness of diffusion models in super-resolution, largely due to the unnatural range-azimuth heatmap (RAH) or birdâ€™s eye view (BEV) representation. To overcome this limitation, we propose a novel method that pioneers the application of fusing range images with image diffusion models, achieving accurate and dense mmWave radar point clouds that are similar to LiDAR. Benefitting from the projection that aligns with human observation, the range image representation of mmWave radar is close to natural images, allowing the knowledge from pre-trained image diffusion models to be effectively transferred, significantly improving the overall performance. Extensive evaluations on both public datasets and self-constructed datasets demonstrate that our approach provides substantial improvements, establishing a new state-of-the-art performance in generating truly three-dimensional LiDAR-like point clouds via mmWave radar. Code will be released after publication. </p>
<blockquote>
<p>æ¯«ç±³æ³¢é›·è¾¾ï¼ˆmmWave radarï¼‰åœ¨æœºå™¨äººæŠ€æœ¯å’Œè‡ªåŠ¨é©¾é©¶é¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå°½ç®¡æ¯«ç±³æ³¢é›·è¾¾åœ¨æ¶åŠ£ç¯å¢ƒä¸‹çš„æ„ŸçŸ¥ç¨³å®šæ€§è‰¯å¥½ï¼Œä½†å…¶ç”Ÿæˆçš„ç‚¹äº‘ç›¸å¯¹ç¨€ç–ä¸”å«æœ‰å¤§é‡å™ªå£°ï¼Œé™åˆ¶äº†å…¶è¿›ä¸€æ­¥å‘å±•ã€‚ä¼ ç»Ÿçš„æ¯«ç±³æ³¢é›·è¾¾å¢å¼ºæ–¹æ³•å¾€å¾€éš¾ä»¥åˆ©ç”¨æ‰©æ•£æ¨¡å‹åœ¨è¶…åˆ†è¾¨ç‡æ–¹é¢çš„ä¼˜åŠ¿ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºèŒƒå›´-æ–¹ä½çƒ­å›¾ï¼ˆRAHï¼‰æˆ–é¸Ÿç°å›¾ï¼ˆBEVï¼‰è¡¨ç¤ºä¸è‡ªç„¶ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç‡å…ˆå°†èŒƒå›´å›¾åƒä¸å›¾åƒæ‰©æ•£æ¨¡å‹èåˆåº”ç”¨ã€‚è¯¥æ–¹æ³•å¯å®ç°å‡†ç¡®ä¸”å¯†é›†çš„æ¯«ç±³æ³¢é›·è¾¾ç‚¹äº‘ï¼Œç±»ä¼¼äºæ¿€å…‰é›·è¾¾ç‚¹äº‘ã€‚å¾—ç›Šäºä¸äººç±»è§‚å¯Ÿç›¸åŒ¹é…çš„æŠ•å½±ï¼Œæ¯«ç±³æ³¢é›·è¾¾çš„èŒƒå›´å›¾åƒè¡¨ç¤ºæ¥è¿‘è‡ªç„¶å›¾åƒï¼Œè¿™ä½¿å¾—é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†å¯ä»¥é«˜æ•ˆè¿ç§»ï¼Œä»è€Œæ˜¾è‘—æé«˜æ•´ä½“æ€§èƒ½ã€‚åœ¨å…¬å…±æ•°æ®é›†å’Œè‡ªæˆ‘æ„å»ºçš„æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”ŸæˆçœŸæ­£ä¸‰ç»´æ¿€å…‰é›·è¾¾ç±»ç‚¹äº‘æ–¹é¢æä¾›äº†é‡å¤§æ”¹è¿›ï¼Œç¡®ç«‹äº†åˆ©ç”¨æ¯«ç±³æ³¢é›·è¾¾ç”Ÿæˆç‚¹äº‘çš„æ–°å…ˆè¿›æ€§èƒ½æ ‡å‡†ã€‚ä»£ç å°†åœ¨å‘è¡¨åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02300v2">PDF</a> 8 pages, 7 figures. This work has been submitted to the IEEE for   possible publication</p>
<p><strong>æ‘˜è¦</strong><br>    æ¯«ç±³æ³¢é›·è¾¾åœ¨æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†å…¶ç”Ÿæˆçš„ç‚¹äº‘ç›¸å¯¹ç¨€ç–ä¸”å«å™ªï¼Œé™åˆ¶äº†å…¶è¿›ä¸€æ­¥å‘å±•ã€‚ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥åˆ©ç”¨æ‰©æ•£æ¨¡å‹å®ç°è¶…åˆ†è¾¨ç‡å¢å¼ºã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§èåˆèŒƒå›´å›¾åƒä¸å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œå¯ç”Ÿæˆå‡†ç¡®ã€å¯†é›†çš„æ¯«ç±³æ³¢é›·è¾¾ç‚¹äº‘ï¼Œç±»ä¼¼äºæ¿€å…‰é›·è¾¾ç‚¹äº‘ã€‚èŒƒå›´å›¾åƒè¡¨ç¤ºæ³•ä¸äººç±»è§‚å¯Ÿç›¸ç¬¦ï¼Œæ¥è¿‘è‡ªç„¶å›¾åƒï¼Œå¯æœ‰æ•ˆåœ°è¿ç§»é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†ï¼Œæ˜¾è‘—æé«˜æ•´ä½“æ€§èƒ½ã€‚åœ¨å…¬å…±å’Œè‡ªåˆ¶æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç”ŸæˆçœŸæ­£çš„ä¸‰ç»´æ¿€å…‰é›·è¾¾ç‚¹äº‘æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æ¯«ç±³æ³¢é›·è¾¾åœ¨æ¶åŠ£ç¯å¢ƒä¸‹å…·æœ‰ç¨³å®šçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†å…¶ç”Ÿæˆçš„ç‚¹äº‘ç¨€ç–ä¸”å«å™ªã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œæœ‰æ•ˆå¢å¼ºã€‚</li>
<li>æå‡ºäº†ä¸€ç§èåˆèŒƒå›´å›¾åƒä¸å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œç”¨äºå¢å¼ºæ¯«ç±³æ³¢é›·è¾¾ç‚¹äº‘ã€‚</li>
<li>èŒƒå›´å›¾åƒè¡¨ç¤ºæ³•ä¸äººç±»è§‚å¯Ÿç›¸ç¬¦ï¼Œå…è®¸è¿ç§»é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹çŸ¥è¯†ã€‚</li>
<li>è¯¥æ–¹æ³•ç”Ÿæˆçš„ç‚¹äº‘æ€§èƒ½æ¥è¿‘æ¿€å…‰é›·è¾¾ç‚¹äº‘ã€‚</li>
<li>åœ¨å…¬å…±å’Œè‡ªåˆ¶æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆä¸‰ç»´æ¿€å…‰é›·è¾¾ç‚¹äº‘æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</li>
<li>ä»£ç å°†åœ¨å‘è¡¨åå…¬å¼€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02300">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ee1277b82eb9c7992f7d880b8e689712~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931152&auth_key=1759931152-0-0-620f989598dae788c944e73baa610ef6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7c8b555ea4b1c0509a535dbcd999c883~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931160&auth_key=1759931160-0-0-887096ddaee744e4f9ae3faae397f831&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-79d3d4d78518f7508ad6cfdbbdebaf48~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931167&auth_key=1759931167-0-0-df266ac3d9301444b1e8f903b27854f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-b5cee4522b6cb9d1fd773b2acd8fc63e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Rare-to-Frequent-Unlocking-Compositional-Generation-Power-of-Diffusion-Models-on-Rare-Concepts-with-LLM-Guidance"><a href="#Rare-to-Frequent-Unlocking-Compositional-Generation-Power-of-Diffusion-Models-on-Rare-Concepts-with-LLM-Guidance" class="headerlink" title="Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion   Models on Rare Concepts with LLM Guidance"></a>Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion   Models on Rare Concepts with LLM Guidance</h2><p><strong>Authors:Dongmin Park, Sebin Kim, Taehong Moon, Minkyu Kim, Kangwook Lee, Jaewoong Cho</strong></p>
<p>State-of-the-art text-to-image (T2I) diffusion models often struggle to generate rare compositions of concepts, e.g., objects with unusual attributes. In this paper, we show that the compositional generation power of diffusion models on such rare concepts can be significantly enhanced by the Large Language Model (LLM) guidance. We start with empirical and theoretical analysis, demonstrating that exposing frequent concepts relevant to the target rare concepts during the diffusion sampling process yields more accurate concept composition. Based on this, we propose a training-free approach, R2F, that plans and executes the overall rare-to-frequent concept guidance throughout the diffusion inference by leveraging the abundant semantic knowledge in LLMs. Our framework is flexible across any pre-trained diffusion models and LLMs, and can be seamlessly integrated with the region-guided diffusion approaches. Extensive experiments on three datasets, including our newly proposed benchmark, RareBench, containing various prompts with rare compositions of concepts, R2F significantly surpasses existing models including SD3.0 and FLUX by up to 28.1%p in T2I alignment. Code is available at <a target="_blank" rel="noopener" href="https://github.com/krafton-ai/Rare-to-Frequent">https://github.com/krafton-ai/Rare-to-Frequent</a>. </p>
<blockquote>
<p>å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆç½•è§æ¦‚å¿µç»„åˆï¼ˆä¾‹å¦‚å…·æœ‰ä¸å¯»å¸¸å±æ€§çš„ç‰©ä½“ï¼‰æ—¶ç»å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‡å¯¼ï¼Œå¯ä»¥æ˜¾è‘—å¢å¼ºæ‰©æ•£æ¨¡å‹åœ¨è¿™ç§ç½•è§æ¦‚å¿µä¸Šçš„ç»„åˆç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬ä»å®è¯å’Œç†è®ºåˆ†æå¼€å§‹ï¼Œè¯æ˜åœ¨æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ä¸­æš´éœ²ä¸ç›®æ ‡ç½•è§æ¦‚å¿µç›¸å…³çš„é¢‘ç¹æ¦‚å¿µï¼Œå¯ä»¥äº§ç”Ÿæ›´ç²¾ç¡®çš„æ¦‚å¿µç»„åˆã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„R2Fæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨LLMä¸­çš„ä¸°å¯Œè¯­ä¹‰çŸ¥è¯†ï¼Œé€šè¿‡æ‰©æ•£æ¨æ–­ï¼Œè§„åˆ’å’Œæ‰§è¡Œä»ç½•è§åˆ°é¢‘ç¹æ¦‚å¿µçš„æ€»ä½“æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€‚ç”¨äºä»»ä½•é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’ŒLLMï¼Œå¹¶èƒ½æ— ç¼åœ°ä¸åŒºåŸŸå¼•å¯¼æ‰©æ•£æ–¹æ³•ç›¸ç»“åˆã€‚åœ¨åŒ…æ‹¬æˆ‘ä»¬æ–°æå‡ºçš„åŸºå‡†æµ‹è¯•é›†RareBenchåœ¨å†…çš„ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒR2Fåœ¨å„ç§åŒ…å«ç½•è§æ¦‚å¿µç»„åˆçš„æç¤ºä¸‹ï¼Œæ˜¾è‘—è¶…è¶Šäº†SD3.0å’ŒFLUXæ¨¡å‹ï¼Œåœ¨T2Iå¯¹é½æ–¹é¢çš„æ€§èƒ½æé«˜äº†é«˜è¾¾28.1%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/krafton-ai/rare-to-frequent%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/krafton-ai/Rare-to-Frequentæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.22376v4">PDF</a> ICLR 2025 (spotlight)</p>
<p><strong>Summary</strong><br>     æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆç½•è§æ¦‚å¿µç»„åˆæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡å±•ç¤ºé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŒ‡å¯¼ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ‰©æ•£æ¨¡å‹åœ¨ç½•è§æ¦‚å¿µä¸Šçš„ç»„åˆç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡å®è¯å’Œç†è®ºåˆ†æï¼Œè¡¨æ˜åœ¨æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ä¸­æš´éœ²ä¸ç›®æ ‡ç½•è§æ¦‚å¿µç›¸å…³çš„é¢‘ç¹æ¦‚å¿µï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¿›è¡Œæ¦‚å¿µç»„åˆã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•R2Fï¼Œåˆ©ç”¨LLMä¸­çš„ä¸°å¯Œè¯­ä¹‰çŸ¥è¯†ï¼Œé€šè¿‡æ‰©æ•£æ¨ç†ï¼Œå®ç°ç½•è§åˆ°é¢‘ç¹çš„æ¦‚å¿µæŒ‡å¯¼ã€‚è¯¥æ–¹æ³•çµæ´»é€‚ç”¨äºä»»ä½•é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’ŒLLMï¼Œå¹¶å¯æ— ç¼é›†æˆåˆ°åŒºåŸŸå¼•å¯¼æ‰©æ•£æ–¹æ³•ä¸­ã€‚åœ¨åŒ…æ‹¬æ–°æå‡ºçš„åŸºå‡†æµ‹è¯•æ•°æ®é›†RareBenchä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒR2Fåœ¨T2Iå¯¹é½æ–¹é¢æ˜¾è‘—è¶…è¶Šäº†SD3.0å’ŒFLUXæ¨¡å‹ï¼Œæé«˜äº†é«˜è¾¾28.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆç½•è§æ¦‚å¿µç»„åˆæ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŒ‡å¯¼èƒ½æé«˜æ‰©æ•£æ¨¡å‹åœ¨ç½•è§æ¦‚å¿µä¸Šçš„ç»„åˆç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>åœ¨æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ä¸­æš´éœ²ä¸ç›®æ ‡ç½•è§æ¦‚å¿µç›¸å…³çš„é¢‘ç¹æ¦‚å¿µï¼Œèƒ½æ›´å‡†ç¡®è¿›è¡Œæ¦‚å¿µç»„åˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„R2Fæ–¹æ³•ï¼Œåˆ©ç”¨LLMçš„ä¸°å¯Œè¯­ä¹‰çŸ¥è¯†ï¼Œå®ç°ç½•è§åˆ°é¢‘ç¹çš„æ¦‚å¿µæŒ‡å¯¼ã€‚</li>
<li>R2Fæ–¹æ³•çµæ´»é€‚ç”¨äºå„ç§é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’ŒLLMï¼Œå¹¶å¯é›†æˆåˆ°åŒºåŸŸå¼•å¯¼æ‰©æ•£æ–¹æ³•ä¸­ã€‚</li>
<li>åœ¨æ–°çš„åŸºå‡†æµ‹è¯•æ•°æ®é›†RareBenchä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒR2Fæ˜¾è‘—æé«˜äº†T2Iå¯¹é½æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.22376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-26523245f179f078e4e02c247e462bcb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931185&auth_key=1759931185-0-0-b00b139754e1b430b90fe0b36e8e73a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-5897bd093eb21b557f8a21ea169c61f9.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-eced55fcc4237184017a848e99237130~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931198&auth_key=1759931198-0-0-ae56c9d38acde37a8c2d8b127241584b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-24a0e57a546dc854bd1f260364ed7271.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afa214eb2a8a76bab78bc7454887aaa4.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DiffusionGuard-A-Robust-Defense-Against-Malicious-Diffusion-based-Image-Editing"><a href="#DiffusionGuard-A-Robust-Defense-Against-Malicious-Diffusion-based-Image-Editing" class="headerlink" title="DiffusionGuard: A Robust Defense Against Malicious Diffusion-based Image   Editing"></a>DiffusionGuard: A Robust Defense Against Malicious Diffusion-based Image   Editing</h2><p><strong>Authors:June Suk Choi, Kyungmin Lee, Jongheon Jeong, Saining Xie, Jinwoo Shin, Kimin Lee</strong></p>
<p>Recent advances in diffusion models have introduced a new era of text-guided image manipulation, enabling users to create realistic edited images with simple textual prompts. However, there is significant concern about the potential misuse of these methods, especially in creating misleading or harmful content. Although recent defense strategies, which introduce imperceptible adversarial noise to induce model failure, have shown promise, they remain ineffective against more sophisticated manipulations, such as editing with a mask. In this work, we propose DiffusionGuard, a robust and effective defense method against unauthorized edits by diffusion-based image editing models, even in challenging setups. Through a detailed analysis of these models, we introduce a novel objective that generates adversarial noise targeting the early stage of the diffusion process. This approach significantly improves the efficiency and effectiveness of adversarial noises. We also introduce a mask-augmentation technique to enhance robustness against various masks during test time. Finally, we introduce a comprehensive benchmark designed to evaluate the effectiveness and robustness of methods in protecting against privacy threats in realistic scenarios. Through extensive experiments, we show that our method achieves stronger protection and improved mask robustness with lower computational costs compared to the strongest baseline. Additionally, our method exhibits superior transferability and better resilience to noise removal techniques compared to all baseline methods. Our source code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/choi403/DiffusionGuard">https://github.com/choi403/DiffusionGuard</a>. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰çš„è¿›å±•å¼•é¢†äº†æ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ“ä½œæ–°çºªå…ƒï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡ç®€å•çš„æ–‡æœ¬æç¤ºåˆ›å»ºé€¼çœŸçš„ç¼–è¾‘å›¾åƒã€‚ç„¶è€Œï¼Œå…³äºè¿™äº›æ–¹æ³•å¯èƒ½è¢«è¯¯ç”¨çš„æ‹…å¿§ä¹Ÿæ—¥ç›Šæ˜¾è‘—ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ›å»ºè¯¯å¯¼æ€§æˆ–æœ‰å®³å†…å®¹ä¸Šã€‚å°½ç®¡æœ€è¿‘çš„é˜²å¾¡ç­–ç•¥é€šè¿‡åœ¨æ¨¡å‹ä¸­å¼•å…¥å‡ ä¹æ— æ³•å¯Ÿè§‰çš„å¯¹æŠ—å™ªå£°æ¥å¼•å‘æ¨¡å‹å¤±æ•ˆï¼Œä½†å®ƒä»¬å¯¹äºæ›´å¤æ‚çš„æ“ä½œï¼ˆå¦‚ä½¿ç”¨é®ç½©è¿›è¡Œç¼–è¾‘ï¼‰ä»ç„¶æ— æ•ˆã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DiffusionGuardï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹åŸºäºæ‰©æ•£çš„å›¾åƒç¼–è¾‘æ¨¡å‹çš„æœªç»æˆæƒç¼–è¾‘çš„ç¨³å¥ä¸”æœ‰æ•ˆçš„é˜²å¾¡æ–¹æ³•ï¼Œå³ä½¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è®¾ç½®ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚é€šè¿‡å¯¹è¿™äº›æ¨¡å‹çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„ç›®æ ‡ï¼Œè¯¥ç›®æ ‡ç”Ÿæˆé’ˆå¯¹æ‰©æ•£è¿‡ç¨‹æ—©æœŸçš„å¯¹æŠ—æ€§å™ªå£°ã€‚è¿™ç§æ–¹æ³•å¤§å¤§æé«˜äº†å¯¹æŠ—æ€§å™ªå£°çš„æ•ˆç‡å’Œæœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ©è†œå¢å¼ºæŠ€æœ¯ï¼Œä»¥æé«˜åœ¨æµ‹è¯•æœŸé—´å¯¹å„ç§æ©è†œçš„ç¨³å¥æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ–¹æ³•åœ¨çœŸå®åœºæ™¯ä¸­æŠµå¾¡éšç§å¨èƒçš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŠ¤å¼ºåº¦ã€æ©è†œç¨³å¥æ€§å’Œè®¡ç®—æˆæœ¬æ–¹é¢ä¼˜äºæœ€å¼ºåŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜è¡¨ç°å‡ºå“è¶Šçš„è¿ç§»æ€§å’Œå¯¹å»å™ªæŠ€æœ¯çš„æ›´å¥½é€‚åº”æ€§ï¼Œä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/choi403/DiffusionGuard%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/choi403/DiffusionGuardä¸Šå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05694v2">PDF</a> Preprint. Under review</p>
<p><strong>Summary</strong><br>     è¿‘æœŸæ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•å¼€å¯äº†æ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ“ä½œæ–°æ—¶ä»£ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡ç®€å•çš„æ–‡æœ¬æç¤ºåˆ›å»ºé€¼çœŸçš„ç¼–è¾‘å›¾åƒã€‚ç„¶è€Œï¼Œäººä»¬æ‹…å¿ƒè¿™äº›æ–¹æ³•å¯èƒ½è¢«æ»¥ç”¨ï¼Œç”¨äºåˆ¶é€ è¯¯å¯¼æ€§æˆ–æœ‰å®³å†…å®¹ã€‚å°½ç®¡æœ€è¿‘çš„é˜²å¾¡ç­–ç•¥é€šè¿‡åœ¨æ¨¡å‹ä¸­å¼•å…¥å‡ ä¹æ— æ³•å¯Ÿè§‰çš„å¯¹æŠ—å™ªå£°æ¥ä½¿å…¶å¤±æ•ˆï¼Œä½†åœ¨é¢å¯¹æ›´å¤æ‚çš„æ“ä½œï¼ˆå¦‚ä½¿ç”¨é®ç½©è¿›è¡Œç¼–è¾‘ï¼‰æ—¶ï¼Œè¿™äº›ç­–ç•¥ä»æ˜¾å¾—ä¸å¤Ÿæœ‰æ•ˆã€‚æœ¬ç ”ç©¶æå‡ºäº†DiffusionGuardï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹åŸºäºæ‰©æ•£çš„å›¾åƒç¼–è¾‘æ¨¡å‹çš„æœªç»æˆæƒç¼–è¾‘çš„ç¨³å¥æœ‰æ•ˆé˜²å¾¡æ–¹æ³•ï¼Œå³ä½¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚é€šè¿‡æ·±å…¥åˆ†æè¿™äº›æ¨¡å‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„ç›®æ ‡ï¼Œè¯¥ç›®æ ‡ç”Ÿæˆé’ˆå¯¹æ‰©æ•£è¿‡ç¨‹æ—©æœŸçš„å¯¹æŠ—æ€§å™ªå£°ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ©è†œå¢å¼ºæŠ€æœ¯ï¼Œä»¥æé«˜å¯¹å„ç§æ©è†œåœ¨æµ‹è¯•æ—¶çš„é²æ£’æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ä¿æŠ¤æ–¹æ³•æŠµå¾¡ç°å®åœºæ™¯ä¸­çš„éšç§å¨èƒçš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®¡ç®—æˆæœ¬è¾ƒä½çš„æƒ…å†µä¸‹å®ç°äº†æ›´å¼ºçš„ä¿æŠ¤å’Œæ›´å¥½çš„é®ç½©ç¨³å¥æ€§ã€‚ç›¸è¾ƒäºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å±•ç°å‡ºæ›´é«˜çš„å¯è¿ç§»æ€§å’Œå¯¹å»å™ªæŠ€æœ¯çš„æ›´å¥½éŸ§æ€§ã€‚ä»£ç å…¬å¼€åœ¨ <a target="_blank" rel="noopener" href="https://github.com/choi403/DiffusionGuard%E3%80%82">https://github.com/choi403/DiffusionGuardã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•è®©æ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ“ä½œæ›´åŠ ä¾¿æ·å’Œé€¼çœŸã€‚</li>
<li>æ»¥ç”¨æ‰©æ•£æ¨¡å‹å¯èƒ½å¯¼è‡´è¯¯å¯¼æ€§æˆ–æœ‰å®³å†…å®¹çš„äº§ç”Ÿï¼Œå¼•å‘å…³æ³¨ã€‚</li>
<li>å½“å‰é˜²å¾¡ç­–ç•¥åœ¨é¢å¯¹å¤æ‚å›¾åƒç¼–è¾‘ï¼ˆå¦‚ä½¿ç”¨é®ç½©ï¼‰æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>DiffusionGuardæ–¹æ³•è¢«æå‡ºï¼Œæ—¨åœ¨æœ‰æ•ˆé˜²å¾¡æœªç»æˆæƒçš„åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¼–è¾‘ã€‚</li>
<li>DiffusionGuardé€šè¿‡é’ˆå¯¹æ‰©æ•£è¿‡ç¨‹æ—©æœŸç”Ÿæˆå¯¹æŠ—æ€§å™ªå£°æ¥æé«˜æ•ˆç‡å’Œæ•ˆæœã€‚</li>
<li>å¼•å…¥æ©è†œå¢å¼ºæŠ€æœ¯ï¼Œå¢å¼ºå¯¹æµ‹è¯•æ—¶ä¸åŒæ©è†œçš„é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05694">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-735ec6306210adcecb89b9db8b1f1306.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-ceb3168d0df727151c05049d52cdbecd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931226&auth_key=1759931226-0-0-f64452fa29ff3332549c9a40b633b0ce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-e4940d7ca3ed17b53a52cca203b37de8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29a9ecdf59626bd70b04b64d63212879.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c64145d0dd2e83be3af9115e715c3fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759931246&auth_key=1759931246-0-0-e2063b18a169ee88d3843bb381bbca69&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-abde5572acf07c88dadee9440fb890ee.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  Deep Learning for Oral Health Benchmarking ViT, DeiT, BEiT, ConvNeXt,   and Swin Transformer
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4085ada383e57ca40ad86b2e077ab791.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  Triangle Splatting+ Differentiable Rendering with Opaque Triangles
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32251.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
