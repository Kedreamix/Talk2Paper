<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-10-01  ThermalGen Style-Disentangled Flow-Based Generative Models for   RGB-to-Thermal Image Translation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24165v1/page_5_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-01-更新"><a href="#2025-10-01-更新" class="headerlink" title="2025-10-01 更新"></a>2025-10-01 更新</h1><h2 id="ThermalGen-Style-Disentangled-Flow-Based-Generative-Models-for-RGB-to-Thermal-Image-Translation"><a href="#ThermalGen-Style-Disentangled-Flow-Based-Generative-Models-for-RGB-to-Thermal-Image-Translation" class="headerlink" title="ThermalGen: Style-Disentangled Flow-Based Generative Models for   RGB-to-Thermal Image Translation"></a>ThermalGen: Style-Disentangled Flow-Based Generative Models for   RGB-to-Thermal Image Translation</h2><p><strong>Authors:Jiuhong Xiao, Roshan Nayak, Ning Zhang, Daniel Tortei, Giuseppe Loianno</strong></p>
<p>Paired RGB-thermal data is crucial for visual-thermal sensor fusion and cross-modality tasks, including important applications such as multi-modal image alignment and retrieval. However, the scarcity of synchronized and calibrated RGB-thermal image pairs presents a major obstacle to progress in these areas. To overcome this challenge, RGB-to-Thermal (RGB-T) image translation has emerged as a promising solution, enabling the synthesis of thermal images from abundant RGB datasets for training purposes. In this study, we propose ThermalGen, an adaptive flow-based generative model for RGB-T image translation, incorporating an RGB image conditioning architecture and a style-disentangled mechanism. To support large-scale training, we curated eight public satellite-aerial, aerial, and ground RGB-T paired datasets, and introduced three new large-scale satellite-aerial RGB-T datasets–DJI-day, Bosonplus-day, and Bosonplus-night–captured across diverse times, sensor types, and geographic regions. Extensive evaluations across multiple RGB-T benchmarks demonstrate that ThermalGen achieves comparable or superior translation performance compared to existing GAN-based and diffusion-based methods. To our knowledge, ThermalGen is the first RGB-T image translation model capable of synthesizing thermal images that reflect significant variations in viewpoints, sensor characteristics, and environmental conditions. Project page: <a target="_blank" rel="noopener" href="http://xjh19971.github.io/ThermalGen">http://xjh19971.github.io/ThermalGen</a> </p>
<blockquote>
<p>配对RGB-热数据对于视觉-热传感器融合和跨模态任务至关重要，包括多模态图像对齐和检索等重要应用。然而，同步且校准的RGB-热图像对稀缺，成为这些领域进步的主要障碍。为了克服这一挑战，RGB到热(RGB-T)图像翻译作为一种有前景的解决方案应运而生，它能够从丰富的RGB数据集中合成热图像，用于训练目的。在这项研究中，我们提出了ThermalGen，这是一种基于自适应流的RGB-T图像翻译生成模型，它结合了RGB图像条件结构和风格分离机制。为了支持大规模训练，我们精选了八个公共卫星航空、航空和地面RGB-T配对数据集，并引入了三个新的大规模卫星航空RGB-T数据集——DJI日间、博森加日间和博森加夜间，这些数据集在多种时间、传感器类型和地理区域下捕获。在多个RGB-T基准测试上的广泛评估表明，ThermalGen与现有的基于GAN和扩散的方法相比，实现了相当的或更优的翻译性能。据我们所知，ThermalGen是首个能够合成反映视点、传感器特性和环境条件的热图像的RGB-T图像翻译模型。项目页面：<a target="_blank" rel="noopener" href="http://xjh19971.github.io/ThermalGen">http://xjh19971.github.io/ThermalGen</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24878v1">PDF</a> 23 pages including the checklist and appendix. Accepted at NeurIPS   2025</p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了RGB-T图像翻译技术的重要性及其在研究中的应用。为克服同步校准RGB-热图像对缺乏的难题，研究团队提出一种基于流的自适应生成模型ThermalGen。它采用RGB图像调节架构和风格分离机制，能够合成反映视点、传感器特性、环境条件等多种变化的热图像。同时，研究还引入了三个新的大规模卫星RGB-T数据集，并在多个RGB-T基准测试中证明了ThermalGen的优异性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>RGB-热数据对在视觉热传感器融合和跨模态任务中至关重要，如多模态图像对齐和检索。</li>
<li>缺乏同步和校准的RGB-热图像对是这些领域的主要挑战之一。</li>
<li>RGB-T图像翻译技术作为解决方案出现，能从丰富的RGB数据集中合成热图像以供训练用途。</li>
<li>ThermalGen是一个基于流的自适应生成模型，用于RGB-T图像翻译，包含RGB图像调节架构和风格分离机制。</li>
<li>ThermalGen能够合成反映视点、传感器特性和环境条件的热图像。</li>
<li>研究引入了三个新的大规模卫星RGB-T数据集：DJI-day、Bosonplus-day和Bosonplus-night。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24878">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24878v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24878v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24878v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24878v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24878v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="IWR-Bench-Can-LVLMs-reconstruct-interactive-webpage-from-a-user-interaction-video"><a href="#IWR-Bench-Can-LVLMs-reconstruct-interactive-webpage-from-a-user-interaction-video" class="headerlink" title="IWR-Bench: Can LVLMs reconstruct interactive webpage from a user   interaction video?"></a>IWR-Bench: Can LVLMs reconstruct interactive webpage from a user   interaction video?</h2><p><strong>Authors:Yang Chen, Minghao Liu, Yufan Shen, Yunwen Li, Tianyuan Huang, Xinyu Fang, Tianyu Zheng, Wenxuan Huang, Cheng Yang, Daocheng Fu, Jianbiao Mei, Rong Wu, Licheng Wen, Xuemeng Yang, Song Mao, Qunshu Lin, Zhi Yu, Yongliang Shen, Yu Qiao, Botian Shi</strong></p>
<p>The webpage-to-code task requires models to understand visual representations of webpages and generate corresponding code. However, existing benchmarks primarily focus on static screenshot-to-code tasks, thereby overlooking the dynamic interactions fundamental to real-world web applications. To address this limitation, this paper introduces IWR-Bench, a novel benchmark for evaluating the capabilities of Large Vision-Language Models (LVLMs) in interactive webpage reconstruction from video. IWR-Bench comprises 113 meticulously curated tasks from 100 real-world websites, with 1,001 actions and featuring diverse interaction complexities (e.g., web games), visual styles, and domains. Aligning with standard web development practices, each task includes not only user interaction videos but also all crawled static assets (e.g., images, videos). This benchmark evaluates models on two fundamental challenges: comprehensive multi-modal reasoning to infer interaction logic from video and assets, and advanced code generation to translate this logic into functional code. An agent-as-a-judge framework with a comprehensive metric system automatically assesses the functional correctness and visual fidelity of generated webpages. Extensive experiments on 28 LVLMs reveal a significant challenge: the best model achieves an overall score of only 36.35%, as functional correctness (24.39% IFS) lags significantly behind visual fidelity (64.25% VFS). These results highlight critical limitations in current models’ ability to reason about temporal dynamics and synthesize event-driven logic, establishing IWR-Bench as a challenging frontier for vision-language research. The benchmark and evaluation code will be made publicly available. Code is available at <a target="_blank" rel="noopener" href="https://github.com/L-O-I/IWR-Bench">https://github.com/L-O-I/IWR-Bench</a>. </p>
<blockquote>
<p>网页到代码的任务要求模型理解网页的视觉表示并生成相应的代码。然而，现有的基准测试主要关注静态截图到代码的任务，从而忽略了现实世界网页应用中的基本动态交互。为了解决这一局限性，本文介绍了IWR-Bench，这是一个新的基准测试，用于评估大型视觉语言模型（LVLMs）从视频中进行交互式网页重建的能力。IWR-Bench包含113个精心策划的任务，来自100个真实网站，包含1001个动作，并展示多样的交互复杂性（如网页游戏）、视觉风格和领域。与标准的网页开发实践相一致，每个任务不仅包括用户交互视频，还包括所有爬取的静态资产（如图像、视频）。此基准测试评估了模型两个基本挑战：通过视频和资产进行全面的多模式推理以推断交互逻辑，以及将这一逻辑转化为功能代码的先进代码生成。一个以代理作为法官的框架，配合一个综合的度量系统，可以自动评估生成网页的功能正确性和视觉保真度。在28个LVLMs上的大量实验显示了一个重大挑战：最好的模型总体得分只有36.35%，因为功能正确性（24.39% IFS）远远落后于视觉保真度（64.25% VFS）。这些结果突显了当前模型在推理时间动态和合成事件驱动逻辑方面的关键局限性，使IWR-Bench成为视觉语言研究的前沿挑战。基准测试和评估代码将公开提供。代码可在<a target="_blank" rel="noopener" href="https://github.com/L-O-I/IWR-Bench">链接</a>中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24709v1">PDF</a> </p>
<p><strong>Summary</strong><br>网页到代码的转换任务需要模型理解网页的视觉表示并生成相应的代码。然而，现有的基准测试主要关注静态截图到代码的转换，忽略了网页实际交互的重要性。为解决这一问题，本文引入了IWR-Bench，这是一个新的基准测试，旨在评估大型视觉语言模型（LVLMs）从视频重建交互式网页的能力。IWR-Bench包含来自真实网站的113个精心策划的任务，涵盖丰富的交互复杂性（如网页游戏）、视觉风格和领域。该基准测试评估模型两个基本挑战的能力：从视频和资产中推断交互逻辑的综合多模态推理，以及将这一逻辑转化为功能代码的先进代码生成能力。通过大量实验发现，现有模型面临巨大挑战，最佳模型总体得分仅为36.35%，功能正确性远远落后于视觉保真度。这表明当前模型在理解时间动态和合成事件驱动逻辑方面存在关键局限。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有网页到代码的基准测试主要关注静态截图，忽略了真实网页的交互性。</li>
<li>IWR-Bench是一个新的基准测试，旨在评估模型从视频重建交互式网页的能力。</li>
<li>IWR-Bench包含来自真实网站的多样化任务，涵盖复杂的交互、视觉风格和领域。</li>
<li>模型需要综合多模态推理来推断交互逻辑，并将逻辑转化为功能代码。</li>
<li>当前模型在理解和生成交互式网页方面面临挑战，功能正确性远低于视觉保真度。</li>
<li>最佳模型的总体得分仅为36.35%，表明在理解时间动态和合成事件驱动逻辑方面存在局限。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24709">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24709v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24709v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24709v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24709v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24709v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24709v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Diffusion-Bridge-or-Flow-Matching-A-Unifying-Framework-and-Comparative-Analysis"><a href="#Diffusion-Bridge-or-Flow-Matching-A-Unifying-Framework-and-Comparative-Analysis" class="headerlink" title="Diffusion Bridge or Flow Matching? A Unifying Framework and Comparative   Analysis"></a>Diffusion Bridge or Flow Matching? A Unifying Framework and Comparative   Analysis</h2><p><strong>Authors:Kaizhen Zhu, Mokai Pan, Zhechuan Yu, Jingya Wang, Jingyi Yu, Ye Shi</strong></p>
<p>Diffusion Bridge and Flow Matching have both demonstrated compelling empirical performance in transformation between arbitrary distributions. However, there remains confusion about which approach is generally preferable, and the substantial discrepancies in their modeling assumptions and practical implementations have hindered a unified theoretical account of their relative merits. We have, for the first time, provided a unified theoretical and experimental validation of these two models. We recast their frameworks through the lens of Stochastic Optimal Control and prove that the cost function of the Diffusion Bridge is lower, guiding the system toward more stable and natural trajectories. Simultaneously, from the perspective of Optimal Transport, interpolation coefficients $t$ and $1-t$ of Flow Matching become increasingly ineffective when the training data size is reduced. To corroborate these theoretical claims, we propose a novel, powerful architecture for Diffusion Bridge built on a latent Transformer, and implement a Flow Matching model with the same structure to enable a fair performance comparison in various experiments. Comprehensive experiments are conducted across Image Inpainting, Super-Resolution, Deblurring, Denoising, Translation, and Style Transfer tasks, systematically varying both the distributional discrepancy (different difficulty) and the training data size. Extensive empirical results align perfectly with our theoretical predictions and allow us to delineate the respective advantages and disadvantages of these two models. Our code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DBFM-3E8E/">https://anonymous.4open.science/r/DBFM-3E8E/</a>. </p>
<blockquote>
<p>扩散桥和流匹配在任意分布转换中都表现出令人信服的实证性能。然而，关于哪种方法通常更可取仍存在困惑，它们建模假设和实践实施中的巨大差异阻碍了对它们相对优势进行统一的理论解释。我们首次为这两种模型提供了统一的理论和实验验证。我们通过随机最优控制的视角重塑了它们的框架，并证明扩散桥的成本函数较低，引导系统走向更稳定、更自然的轨迹。同时，从最优传输的角度来看，当训练数据规模减少时，流匹配的插值系数t和1-t变得越来越无效。为了证实这些理论主张，我们提出了一种基于潜在变压器的扩散桥新型强大架构，并使用相同结构实现了流匹配模型，以便在各种实验中公平比较性能。在图像修复、超分辨率、去模糊、去噪、翻译和风格转换等任务上进行了全面的实验，系统地改变了分布差异（不同难度）和训练数据规模。广泛的实证结果与我们的理论预测完美吻合，使我们能够详细阐述这两种模型的各自优缺点。我们的代码可在<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DBFM-3E8E/%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/DBFM-3E8E/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24531v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文对Diffusion Bridge和Flow Matching两种模型进行了统一的理论和实验验证。通过引入随机最优控制，证明了Diffusion Bridge的成本函数较低，使系统轨迹更稳定自然。从最优传输角度看，Flow Matching的插值系数$t$和$1-t$在训练数据量减少时效果减弱。新型Diffusion Bridge架构在图像修复、超分辨率、去模糊、去噪、翻译和风格转换等任务上的实验结果与理论预测一致，凸显了各自的优势和不足。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Bridge和Flow Matching在任意分布转换上表现出强大的性能，但两者的建模假设和实际实施存在显著差异。</li>
<li>通过引入随机最优控制，证明了Diffusion Bridge的成本函数较低，使系统轨迹更稳定自然。</li>
<li>从最优传输角度看，Flow Matching的插值系数在训练数据量减少时效果减弱。</li>
<li>新型Diffusion Bridge架构基于潜在Transformer，与Flow Matching模型在同一结构中进行公平的性能比较。</li>
<li>全面的实验在多种任务上进行，包括图像修复、超分辨率、去模糊、去噪、翻译和风格转换等。</li>
<li>实验结果与理论预测一致，凸显了Diffusion Bridge和Flow Matching各自的优势和不足。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24531">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24531v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24531v1/page_1_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LatXGen-Towards-Radiation-Free-and-Accurate-Quantitative-Analysis-of-Sagittal-Spinal-Alignment-Via-Cross-Modal-Radiographic-View-Synthesis"><a href="#LatXGen-Towards-Radiation-Free-and-Accurate-Quantitative-Analysis-of-Sagittal-Spinal-Alignment-Via-Cross-Modal-Radiographic-View-Synthesis" class="headerlink" title="LatXGen: Towards Radiation-Free and Accurate Quantitative Analysis of   Sagittal Spinal Alignment Via Cross-Modal Radiographic View Synthesis"></a>LatXGen: Towards Radiation-Free and Accurate Quantitative Analysis of   Sagittal Spinal Alignment Via Cross-Modal Radiographic View Synthesis</h2><p><strong>Authors:Moxin Zhao, Nan Meng, Jason Pui Yin Cheung, Chris Yuk Kwan Tang, Chenxi Yu, Wenting Zhong, Pengyu Lu, Chang Shi, Yipeng Zhuang, Teng Zhang</strong></p>
<p>Adolescent Idiopathic Scoliosis (AIS) is a complex three-dimensional spinal deformity, and accurate morphological assessment requires evaluating both coronal and sagittal alignment. While previous research has made significant progress in developing radiation-free methods for coronal plane assessment, reliable and accurate evaluation of sagittal alignment without ionizing radiation remains largely underexplored. To address this gap, we propose LatXGen, a novel generative framework that synthesizes realistic lateral spinal radiographs from posterior Red-Green-Blue and Depth (RGBD) images of unclothed backs. This enables accurate, radiation-free estimation of sagittal spinal alignment. LatXGen tackles two core challenges: (1) inferring sagittal spinal morphology changes from a lateral perspective based on posteroanterior surface geometry, and (2) performing cross-modality translation from RGBD input to the radiographic domain. The framework adopts a dual-stage architecture that progressively estimates lateral spinal structure and synthesizes corresponding radiographs. To enhance anatomical consistency, we introduce an attention-based Fast Fourier Convolution (FFC) module for integrating anatomical features from RGBD images and 3D landmarks, and a Spatial Deformation Network (SDN) to model morphological variations in the lateral view. Additionally, we construct the first large-scale paired dataset for this task, comprising 3,264 RGBD and lateral radiograph pairs. Experimental results demonstrate that LatXGen produces anatomically accurate radiographs and outperforms existing GAN-based methods in both visual fidelity and quantitative metrics. This study offers a promising, radiation-free solution for sagittal spine assessment and advances comprehensive AIS evaluation. </p>
<blockquote>
<p>青少年特发性脊柱侧弯（AIS）是一种复杂的三维脊柱畸形，准确的形态学评估需要评估冠状面和矢状面的排列。虽然之前的研究在开发无辐射的冠状面评估方法方面取得了重大进展，但无电离辐射的矢状面排列的可靠和准确评估仍然在很大程度上被忽视。为了解决这一空白，我们提出了LatXGen，这是一种新的生成框架，它可以从裸露背部后方的红绿蓝深度（RGBD）图像合成逼真的侧位脊柱X光片，从而实现准确的无辐射矢状面脊柱排列估计。LatXGen解决了两个核心挑战：（1）基于前后表面几何的侧位矢状面形态变化推断；（2）实现从RGBD输入到放射学领域的跨模态转换。该框架采用两阶段架构，逐步估计侧位脊柱结构并合成相应的X光片。为了提高解剖一致性，我们引入了一个基于注意力的快速傅立叶卷积（FFC）模块，用于整合RGBD图像和3D地标的解剖特征，以及一个空间变形网络（SDN）来模拟侧视图中的形态变化。此外，我们为此任务构建了第一个大规模配对数据集，包含3264个RGBD和侧位X光片对。实验结果表明，LatXGen产生的X光图像解剖准确，且在视觉保真度和定量指标上均优于现有的基于GAN的方法。这项研究为矢状面评估提供了有前景的无辐射解决方案，并推动了全面的AIS评估的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24165v1">PDF</a> 8 pages, 6 figures</p>
<p><strong>Summary</strong><br>     青少年特发性脊柱侧弯（AIS）是一种复杂的三维脊柱畸形，准确的形态学评估需要评估冠状面和矢状面的对齐情况。针对矢状面对齐的无辐射评估方法尚待探索，本研究提出LatXGen，一种能从后部红绿蓝深度（RGBD）图像合成逼真的侧位脊柱放射图像的新颖生成框架，从而实现准确、无辐射的矢状位脊柱对齐评估。LatXGen解决了两个核心挑战：一是基于前后表面几何的侧位脊柱形态变化的推断，二是从RGBD输入到放射领域的跨模态转换。该框架采用分阶段架构，逐步估计侧位脊柱结构并合成相应的放射图像。通过引入基于注意力的快速傅立叶卷积（FFC）模块和空间变形网络（SDN），提高了解剖一致性和形态变化的建模能力。此外，本研究构建了首个大规模配对数据集，包含3264个RGBD和侧位放射图像对。实验结果表明，LatXGen生成的解剖图像准确度高，在视觉保真度和定量指标上均优于现有的基于GAN的方法。本研究为矢状面脊柱评估和全面的AIS评估提供了有前景的无辐射解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>青少年特发性脊柱侧弯（AIS）需要同时评估冠状面和矢状面的对齐情况。</li>
<li>矢状面对齐的无辐射评估方法尚待探索。</li>
<li>LatXGen是一种生成框架，能从RGBD图像合成侧位脊柱放射图像。</li>
<li>LatXGen解决了侧位脊柱形态变化的推断和跨模态转换两个核心挑战。</li>
<li>该框架采用分阶段架构，并引入FFC模块和SDN提高解剖一致性和形态变化建模。</li>
<li>构建了首个大规模配对数据集用于此任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24165">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24165v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24165v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24165v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.24165v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="WeatherCycle-Unpaired-Multi-Weather-Restoration-via-Color-Space-Decoupled-Cycle-Learning"><a href="#WeatherCycle-Unpaired-Multi-Weather-Restoration-via-Color-Space-Decoupled-Cycle-Learning" class="headerlink" title="WeatherCycle: Unpaired Multi-Weather Restoration via Color Space   Decoupled Cycle Learning"></a>WeatherCycle: Unpaired Multi-Weather Restoration via Color Space   Decoupled Cycle Learning</h2><p><strong>Authors:Wenxuan Fang, Jiangwei Weng, Jianjun Qian, Jian Yang, Jun Li</strong></p>
<p>Unsupervised image restoration under multi-weather conditions remains a fundamental yet underexplored challenge. While existing methods often rely on task-specific physical priors, their narrow focus limits scalability and generalization to diverse real-world weather scenarios. In this work, we propose \textbf{WeatherCycle}, a unified unpaired framework that reformulates weather restoration as a bidirectional degradation-content translation cycle, guided by degradation-aware curriculum regularization. At its core, WeatherCycle employs a \textit{lumina-chroma decomposition} strategy to decouple degradation from content without modeling complex weather, enabling domain conversion between degraded and clean images. To model diverse and complex degradations, we propose a \textit{Lumina Degradation Guidance Module} (LDGM), which learns luminance degradation priors from a degraded image pool and injects them into clean images via frequency-domain amplitude modulation, enabling controllable and realistic degradation modeling. Additionally, we incorporate a \textit{Difficulty-Aware Contrastive Regularization (DACR)} module that identifies hard samples via a CLIP-based classifier and enforces contrastive alignment between hard samples and restored features to enhance semantic consistency and robustness. Extensive experiments across serve multi-weather datasets, demonstrate that our method achieves state-of-the-art performance among unsupervised approaches, with strong generalization to complex weather degradations. </p>
<blockquote>
<p>在多天气条件下的无监督图像修复仍然是一个基础但尚未被充分探索的挑战。尽管现有方法经常依赖于特定任务的物理先验知识，但它们狭窄的焦点限制了其在多种现实世界天气场景中的可扩展性和泛化能力。在这项工作中，我们提出了<strong>WeatherCycle</strong>，这是一个统一的非配对框架，它将天气修复重新制定为一个双向的退化内容翻译周期，由退化感知课程正则化指导。WeatherCycle的核心采用了一种<em>亮度-色度分解</em>策略，能够在不建立复杂天气模型的情况下将退化与内容进行分离，从而实现退化图像和干净图像之间的域转换。为了模拟多样化和复杂的退化，我们提出了一个<em>亮度退化指导模块</em>（LDGM），该模块从退化图像池中学习亮度退化先验知识，并通过频率域振幅调制将其注入干净图像，从而实现可控且逼真的退化建模。此外，我们融入了一个<em>难度感知对比正则化（DACR）</em>模块，该模块通过基于CLIP的分类器识别硬样本，并在硬样本和恢复的特征之间强制执行对比对齐，以增强语义一致性和稳健性。在多天气数据集上的广泛实验表明，我们的方法在无监督方法中达到了最新性能，对复杂天气退化具有很强的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23150v1">PDF</a> </p>
<p><strong>Summary</strong><br>天气多变条件下的无监督图像修复是一个基础但尚未被充分研究的问题。现有方法通常依赖于特定任务的物理先验，但其局限性在于难以扩展到多种真实世界天气场景。本研究提出一种统一的非配对框架——WeatherCycle，它将天气修复重新定义为双向的退化-内容翻译循环，受退化感知课程正则化的引导。WeatherCycle采用亮度-色度分解策略，无需建模复杂天气即可实现退化与内容的解耦，实现退化图像与干净图像之间的域转换。为模拟多样化和复杂的退化，我们提出了亮度退化引导模块（LDGM），从退化图像池中学习亮度退化先验，并通过频率域振幅调制将其注入干净图像，从而实现可控且真实的退化建模。此外，还引入了难度感知对比正则化（DACR）模块，通过CLIP分类器识别困难样本，并在困难样本与恢复特征之间执行对比对齐，以增强语义一致性和稳健性。跨多天气数据集的广泛实验表明，该方法在无监督方法中达到最佳性能，对复杂天气退化具有较强的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究解决了多天气条件下的无监督图像修复问题，这是一个尚未被充分探索的基础挑战。</li>
<li>提出了统一的非配对框架——WeatherCycle，将天气修复重新定义为双向的退化-内容翻译循环。</li>
<li>采用亮度-色度分解策略，实现退化与内容的解耦，使得在不需要建模复杂天气的情况下进行域转换。</li>
<li>提出亮度退化引导模块（LDGM），能够从退化图像池中学习亮度退化先验，实现可控且真实的退化建模。</li>
<li>引入难度感知对比正则化（DACR）模块，增强语义一致性和模型稳健性。</li>
<li>跨多天气数据集的实验表明，该方法在无监督方法中表现最佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23150">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.23150v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.23150v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.23150v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.23150v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.23150v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.23150v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ML2B-Multi-Lingual-ML-Benchmark-For-AutoML"><a href="#ML2B-Multi-Lingual-ML-Benchmark-For-AutoML" class="headerlink" title="ML2B: Multi-Lingual ML Benchmark For AutoML"></a>ML2B: Multi-Lingual ML Benchmark For AutoML</h2><p><strong>Authors:Ekaterina Trofimova, Zosia Shamina, Maria Selifanova, Artem Zaitsev, Remi Savchuk, Maxim Minets, Daria Ozerova, Emil Sataev, Denis Zuenko, Andrey E. Ustyuzhanin</strong></p>
<p>Large language models (LLMs) have recently demonstrated strong capabilities in generating machine learning (ML) code, enabling end-to-end pipeline construction from natural language instructions. However, existing benchmarks for ML code generation are mainly restricted to English, overlooking the global and multilingual nature of ML research and practice. To address this gap, we present ML2B, the first benchmark for evaluating multilingual ML code generation. ML2B consists of 30 Kaggle competitions translated into 13 natural languages, covering tabular, text, and image data types, with structured metadata and validated human-reviewed translations. For evaluation, we employ AIDE, an automated framework for end-to-end assessment of data science pipelines, and provide insights into cross-lingual model performance. Our results reveal substantial 15-45% performance degradation on non-English tasks, highlighting critical challenges in multilingual representation learning for code generation. The benchmark, evaluation framework, and comprehensive results are made available through our GitHub repository to facilitate future research in multilingual ML code generation: <a target="_blank" rel="noopener" href="https://github.com/enaix/ml2b">https://github.com/enaix/ml2b</a>. </p>
<blockquote>
<p>大型语言模型（LLM）最近展现出强大的生成机器学习（ML）代码的能力，能够通过自然语言指令实现端到端的管道构建。然而，现有的ML代码生成基准测试主要局限于英语，忽略了ML研究和实践的全球和多语言性质。为了弥补这一空白，我们推出了ML2B，这是首个用于评估多语言ML代码生成的基准测试。ML2B由30个Kaggle竞赛的题目组成，这些题目被翻译为13种自然语言，涵盖了表格、文本和图像数据类型，具备结构化元数据和经过验证的人工审查翻译。为了进行评估，我们采用了AIDE，这是一个用于数据科学管道端到端评估的自动化框架，并提供了关于跨语言模型性能的见解。我们的结果表明，在非英语任务上的性能降低了15-45%，这突显了代码生成中多语言表示学习的关键挑战。基准测试、评估框架和全面结果已通过我们的GitHub仓库公开，以促进未来在多语言ML代码生成方面的研究：<a target="_blank" rel="noopener" href="https://github.com/enaix/ml2b%E3%80%82">https://github.com/enaix/ml2b。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22768v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型在生成机器学习代码方面展现出强大能力，能够实现从自然语言指令到端到端管道构建的全程自动化。然而，现有机器学习代码生成基准测试主要局限于英语，忽略了机器学习研究和实践的全球和多语言特性。为解决这一差距，我们推出了ML2B，首个多语言机器学习代码生成评估基准。ML2B包含翻译为13种自然语言的30个Kaggle竞赛，涵盖表格、文本和图像数据类型，具备结构化元数据和经过验证的人工审查翻译。我们使用AIDE对数据安全管道进行端到端评估的自动化框架进行评估，并深入洞察跨语言模型性能。结果显示，在非英语任务上性能下降15-45%，突显出代码生成中多语言表示学习的关键挑战。基准测试、评估框架和全面结果已发布至GitHub仓库，以便未来研究多语言机器学习代码生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型具备强大的机器学习代码生成能力，实现从自然语言指令到端到端管道构建的自动化。</li>
<li>现有机器学习代码生成基准测试主要局限于英语，忽略了多语言特性。</li>
<li>ML2B是首个多语言机器学习代码生成评估基准，包含30个Kaggle竞赛，翻译为13种自然语言。</li>
<li>ML2B涵盖表格、文本和图像数据类型，具备结构化元数据和经过验证的人工审查翻译。</li>
<li>采用AIDE进行数据安全管道的端到端评估，深入了解跨语言模型性能。</li>
<li>在非英语任务上，模型性能下降15-45%，突显出多语言表示学习的关键挑战。</li>
<li>基准测试、评估框架和结果已发布至GitHub仓库，以促进多语言机器学习代码生成的研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22768">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.22768v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.22768v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.22768v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Loc-2-Interpretable-Cross-View-Localization-via-Depth-Lifted-Local-Feature-Matching"><a href="#Loc-2-Interpretable-Cross-View-Localization-via-Depth-Lifted-Local-Feature-Matching" class="headerlink" title="Loc$^2$: Interpretable Cross-View Localization via Depth-Lifted Local   Feature Matching"></a>Loc$^2$: Interpretable Cross-View Localization via Depth-Lifted Local   Feature Matching</h2><p><strong>Authors:Zimin Xia, Chenghao Xu, Alexandre Alahi</strong></p>
<p>We propose an accurate and interpretable fine-grained cross-view localization method that estimates the 3 Degrees of Freedom (DoF) pose of a ground-level image by matching its local features with a reference aerial image. Unlike prior approaches that rely on global descriptors or bird’s-eye-view (BEV) transformations, our method directly learns ground-aerial image-plane correspondences using weak supervision from camera poses. The matched ground points are lifted into BEV space with monocular depth predictions, and scale-aware Procrustes alignment is then applied to estimate camera rotation, translation, and optionally the scale between relative depth and the aerial metric space. This formulation is lightweight, end-to-end trainable, and requires no pixel-level annotations. Experiments show state-of-the-art accuracy in challenging scenarios such as cross-area testing and unknown orientation. Furthermore, our method offers strong interpretability: correspondence quality directly reflects localization accuracy and enables outlier rejection via RANSAC, while overlaying the re-scaled ground layout on the aerial image provides an intuitive visual cue of localization accuracy. </p>
<blockquote>
<p>我们提出了一种准确且可解释性的细粒度跨视图定位方法。该方法通过匹配地面图像的局部特征与参考的航空图像，来估计地面图像的3自由度（DoF）姿态。不同于依赖全局描述符或鸟瞰图转换的先前方法，我们的方法直接使用来自相机姿态的弱监督来学习地面-航空图像平面的对应关系。匹配的地面点通过单目深度预测提升到鸟瞰图空间，然后应用规模感知Procrustes对齐来估计相机旋转、平移以及可选的相对深度与航空度量空间之间的比例。这种公式轻巧、端对端可训练，并且无需像素级的注释。实验表明，在跨区域测试和未知方向等具有挑战性的场景中，其准确性处于最新技术的前沿。此外，我们的方法具有很强的可解释性：对应质量直接反映定位精度，并通过RANSAC实现了异常值剔除，同时把重新调整比例的地面布局覆盖在航空图像上提供了直观的定位精度视觉提示。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09792v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种准确且可解释的细粒度跨视图定位方法，通过匹配地面图像与参考航拍图像的局部特征来估计地面图像的3自由度姿态。该方法通过弱监督学习直接学习地面与航拍图像平面之间的对应关系，无需全局描述符或鸟瞰变换。通过单目测距预测将匹配的地面点提升到鸟瞰空间，并应用规模感知的Procrustes对齐来估计相机旋转、平移以及可选的相对深度与航拍度量空间之间的比例。该方法具有轻量级、端到端可训练的特点，无需像素级注释。实验表明，在跨区域测试和未知方向等挑战场景下具有最先进的准确性。此外，该方法具有很强的可解释性，对应质量直接反映定位精度，可通过RANSAC拒绝异常值，通过在航拍图像上叠加重新缩放的地面布局提供直观的定位精度视觉线索。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新颖的跨视图定位方法，通过匹配地面与航拍图像的局部特征进行姿态估计。</li>
<li>采用弱监督学习直接学习图像平面间的对应关系。</li>
<li>通过单目测距预测将地面点提升到鸟瞰空间，应用Procrustes对齐进行相机姿态估计。</li>
<li>方法具有轻量级、端到端可训练的特点，无需像素级注释。</li>
<li>在挑战场景下具有最先进的准确性，如跨区域测试和未知方向。</li>
<li>对应质量直接反映定位精度，可通过RANSAC拒绝异常值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09792">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.09792v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.09792v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.09792v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_I2I Translation/2509.09792v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_视频理解/2509.24943v1/page_1_0.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-10-01  Perceive, Reflect and Understand Long Video Progressive Multi-Granular   Clue Exploration with Interactive Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-01\./crop_Few-Shot/2505.15298v4/page_5_1.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-10-01  VT-FSL Bridging Vision and Text with LLMs for Few-Shot Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29774.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
