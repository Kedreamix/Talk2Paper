<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  ReasoningBank Scaling Agent Self-Evolving with Reasoning Memory">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ab1f96f643f70b2661f0dda92f89758d')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-01-æ›´æ–°"><a href="#2025-10-01-æ›´æ–°" class="headerlink" title="2025-10-01 æ›´æ–°"></a>2025-10-01 æ›´æ–°</h1><h2 id="ReasoningBank-Scaling-Agent-Self-Evolving-with-Reasoning-Memory"><a href="#ReasoningBank-Scaling-Agent-Self-Evolving-with-Reasoning-Memory" class="headerlink" title="ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory"></a>ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory</h2><p><strong>Authors:Siru Ouyang, Jun Yan, I-Hung Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long T. Le, Samira Daruki, Xiangru Tang, Vishy Tirumalashetty, George Lee, Mahsan Rofouei, Hangfei Lin, Jiawei Han, Chen-Yu Lee, Tomas Pfister</strong></p>
<p>With the growing adoption of large language model agents in persistent real-world roles, they naturally encounter continuous streams of tasks. A key limitation, however, is their failure to learn from the accumulated interaction history, forcing them to discard valuable insights and repeat past errors. We propose ReasoningBank, a novel memory framework that distills generalizable reasoning strategies from an agentâ€™s self-judged successful and failed experiences. At test time, an agent retrieves relevant memories from ReasoningBank to inform its interaction and then integrates new learnings back, enabling it to become more capable over time. Building on this powerful experience learner, we further introduce memory-aware test-time scaling (MaTTS), which accelerates and diversifies this learning process by scaling up the agentâ€™s interaction experience. By allocating more compute to each task, the agent generates abundant, diverse experiences that provide rich contrastive signals for synthesizing higher-quality memory. The better memory in turn guides more effective scaling, establishing a powerful synergy between memory and test-time scaling. Across web browsing and software engineering benchmarks, ReasoningBank consistently outperforms existing memory mechanisms that store raw trajectories or only successful task routines, improving both effectiveness and efficiency; MaTTS further amplifies these gains. These findings establish memory-driven experience scaling as a new scaling dimension, enabling agents to self-evolve with emergent behaviors naturally arise. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨æŒä¹…ç°å®è§’è‰²ä¸­çš„æ—¥ç›Šæ™®åŠï¼Œå®ƒä»¬è‡ªç„¶ä¼šé‡åˆ°è¿ç»­çš„ä»»åŠ¡æµã€‚ç„¶è€Œï¼Œä¸€ä¸ªå…³é”®çš„å±€é™æ€§åœ¨äºå®ƒä»¬æ— æ³•ä»ç´¯ç§¯çš„äº¤äº’å†å²ä¸­å­¦ä¹ ï¼Œè¿«ä½¿å®ƒä»¬ä¸¢å¼ƒæœ‰ä»·å€¼çš„è§è§£å¹¶é‡å¤è¿‡å»çš„é”™è¯¯ã€‚æˆ‘ä»¬æå‡ºäº†ReasoningBankï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è®°å¿†æ¡†æ¶ï¼Œå¯ä»¥ä»ä»£ç†çš„è‡ªæˆ‘åˆ¤æ–­çš„æˆåŠŸå’Œå¤±è´¥ç»éªŒä¸­æç‚¼å‡ºå¯æ¨å¹¿çš„æ¨ç†ç­–ç•¥ã€‚åœ¨æµ‹è¯•æ—¶ï¼Œä»£ç†ä»ReasoningBankä¸­æ£€ç´¢ç›¸å…³è®°å¿†ä»¥æŒ‡å¯¼å…¶äº¤äº’ï¼Œç„¶åå°†æ–°å­¦ä¹ æ•´åˆå›æ¥ï¼Œéšç€æ—¶é—´çš„æ¨ç§»ä½¿å…¶èƒ½åŠ›å˜å¾—æ›´å¼ºã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†è®°å¿†æ„ŸçŸ¥æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆMaTTSï¼‰ï¼Œé€šè¿‡æ‰©å¤§ä»£ç†çš„äº¤äº’ç»éªŒæ¥åŠ é€Ÿå’Œå¤šæ ·åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚é€šè¿‡ä¸ºæ¯ä¸ªä»»åŠ¡åˆ†é…æ›´å¤šçš„è®¡ç®—èµ„æºï¼Œä»£ç†ç”Ÿæˆä¸°å¯Œã€å¤šæ ·çš„ç»éªŒï¼Œä¸ºåˆæˆé«˜è´¨é‡è®°å¿†æä¾›ä¸°å¯Œçš„å¯¹æ¯”ä¿¡å·ã€‚æ›´å¥½çš„è®°å¿†åè¿‡æ¥åˆæŒ‡å¯¼æ›´æœ‰æ•ˆçš„ç¼©æ”¾ï¼Œåœ¨è®°å¿†å’Œæµ‹è¯•æ—¶ç¼©æ”¾ä¹‹é—´å»ºç«‹å¼ºå¤§çš„ååŒä½œç”¨ã€‚åœ¨ç½‘é¡µæµè§ˆå’Œè½¯ä»¶å·¥ç¨‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReasoningBankå§‹ç»ˆä¼˜äºç°æœ‰çš„å­˜å‚¨åŸå§‹è½¨è¿¹æˆ–ä»…å­˜å‚¨æˆåŠŸä»»åŠ¡ä¾‹ç¨‹çš„è®°å¿†æœºåˆ¶ï¼Œæé«˜äº†æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼›MaTTSè¿›ä¸€æ­¥æ‰©å¤§äº†è¿™äº›æ”¶ç›Šã€‚è¿™äº›å‘ç°ç¡®ç«‹äº†ä»¥è®°å¿†é©±åŠ¨çš„ç»éªŒç¼©æ”¾ä½œä¸ºæ–°çš„ç¼©æ”¾ç»´åº¦ï¼Œä½¿ä»£ç†èƒ½å¤Ÿè‡ªç„¶åœ°è‡ªæˆ‘è¿›åŒ–å¹¶å‡ºç°æ–°å…´è¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25140v1">PDF</a> 11 pages, 7 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æŒç»­ç°å®ä¸–ç•Œä»»åŠ¡æ—¶ï¼Œé¢ä¸´æ— æ³•ä»ç´¯ç§¯çš„äº’åŠ¨å†å²ä¸­å­¦ä¹ çš„é—®é¢˜ï¼Œå¯¼è‡´æœ‰ä»·å€¼çš„ä¿¡æ¯ä¸¢å¤±å’Œé‡å¤é”™è¯¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ReasoningBankï¼Œä¸€ç§èƒ½å¤Ÿè’¸é¦å¯æ¦‚æ‹¬æ¨ç†ç­–ç•¥çš„è®°å¿†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºæ™ºèƒ½ä½“çš„è‡ªæˆ‘åˆ¤æ–­æˆåŠŸå’Œå¤±è´¥ç»éªŒã€‚æµ‹è¯•æ—¶ï¼Œæ™ºèƒ½ä½“ä»ReasoningBankæ£€ç´¢ç›¸å…³è®°å¿†ä»¥æŒ‡å¯¼äº¤äº’ï¼Œå¹¶å°†æ–°å­¦ä¹ æ•´åˆå›æ¡†æ¶ä¸­ï¼Œä½¿å…¶éšæ—¶é—´å˜å¾—æ›´åŠ æ™ºèƒ½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†è®°å¿†æ„ŸçŸ¥æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆMaTTSï¼‰ï¼ŒåŠ é€Ÿå¹¶ä¼˜åŒ–è¿™ä¸€å­¦ä¹ è¿‡ç¨‹ã€‚é€šè¿‡ä¸ºæ¯é¡¹ä»»åŠ¡åˆ†é…æ›´å¤šè®¡ç®—èµ„æºï¼Œæ™ºèƒ½ä½“äº§ç”Ÿä¸°å¯Œå¤šæ ·çš„ç»éªŒï¼Œä¸ºåˆæˆé«˜è´¨é‡è®°å¿†æä¾›ä¸°å¯Œçš„å¯¹æ¯”ä¿¡å·ã€‚é«˜è´¨é‡è®°å¿†è¿›ä¸€æ­¥å¼•å¯¼æ›´æœ‰æ•ˆçš„ç¼©æ”¾ï¼Œåœ¨è®°å¿†å’Œæµ‹è¯•æ—¶é—´ç¼©æ”¾ä¹‹é—´å»ºç«‹å¼ºå¤§ååŒä½œç”¨ã€‚ReasoningBankåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºä»…å­˜å‚¨åŸå§‹è½¨è¿¹æˆ–æˆåŠŸä»»åŠ¡ä¾‹ç¨‹çš„ç°æœ‰è®°å¿†æœºåˆ¶ï¼›MaTTSè¿›ä¸€æ­¥æ‰©å¤§äº†è¿™äº›ä¼˜åŠ¿ã€‚è¿™äº›å‘ç°ç¡®ç«‹äº†ä»¥è®°å¿†é©±åŠ¨çš„ç»éªŒç¼©æ”¾ä½œä¸ºä¸€ç§æ–°å‹ç¼©æ”¾ç»´åº¦ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªç„¶åœ°è¿›è¡Œè‡ªæˆ‘è¿›åŒ–å¹¶äº§ç”Ÿæ–°å…´è¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶å¤„ç†æŒç»­ç°å®ä¸–ç•Œä»»åŠ¡æ—¶é¢ä¸´æ— æ³•ä»äº’åŠ¨å†å²ä¸­å­¦ä¹ çš„é™åˆ¶ã€‚</li>
<li>ReasoningBankä½œä¸ºä¸€ç§æ–°å‹è®°å¿†æ¡†æ¶ï¼Œèƒ½å¤ŸåŸºäºæ™ºèƒ½ä½“çš„è‡ªæˆ‘åˆ¤æ–­æˆåŠŸå’Œå¤±è´¥ç»éªŒæ¥æç‚¼å¯æ¦‚æ‹¬çš„æ¨ç†ç­–ç•¥ã€‚</li>
<li>åœ¨æµ‹è¯•é˜¶æ®µï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿä»ReasoningBankæ£€ç´¢ç›¸å…³è®°å¿†ä»¥æŒ‡å¯¼äº¤äº’ï¼Œå¹¶æ•´åˆæ–°å­¦ä¹ ã€‚</li>
<li>è®°å¿†æ„ŸçŸ¥æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆMaTTSï¼‰èƒ½å¤ŸåŠ é€Ÿå’Œä¼˜åŒ–æ™ºèƒ½ä½“çš„å­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡ä¸ºä»»åŠ¡åˆ†é…æ›´å¤šè®¡ç®—èµ„æºï¼Œæ™ºèƒ½ä½“èƒ½äº§ç”Ÿä¸°å¯Œå¤šæ ·çš„ç»éªŒï¼Œæå‡è®°å¿†è´¨é‡ã€‚</li>
<li>é«˜è´¨é‡è®°å¿†èƒ½æœ‰æ•ˆå¼•å¯¼æ›´é«˜æ•ˆçš„ç¼©æ”¾è¿‡ç¨‹ï¼Œå®ç°è®°å¿†ä¸æµ‹è¯•æ—¶é—´ç¼©æ”¾ä¹‹é—´çš„ååŒä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b5822c3618f30888cb9dd3e49523c05" align="middle">
<img src="https://picx.zhimg.com/v2-7f36c00262851436973ecd9091438381.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4182b52b88c0ba5ae5622677aa6f29bb" align="middle">
<img src="https://picx.zhimg.com/v2-951df67fba439596ea0b63c86ce35062" align="middle">
<img src="https://picx.zhimg.com/v2-4edc7e1de1d3af0c305e58928a69efcb" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Vision-and-Language-Navigation-with-Analogical-Textual-Descriptions-in-LLMs"><a href="#Vision-and-Language-Navigation-with-Analogical-Textual-Descriptions-in-LLMs" class="headerlink" title="Vision-and-Language Navigation with Analogical Textual Descriptions in   LLMs"></a>Vision-and-Language Navigation with Analogical Textual Descriptions in   LLMs</h2><p><strong>Authors:Yue Zhang, Tianyi Ma, Zun Wang, Yanyuan Qiao, Parisa Kordjamshidi</strong></p>
<p>Integrating large language models (LLMs) into embodied AI models is becoming increasingly prevalent. However, existing zero-shot LLM-based Vision-and-Language Navigation (VLN) agents either encode images as textual scene descriptions, potentially oversimplifying visual details, or process raw image inputs, which can fail to capture abstract semantics required for high-level reasoning. In this paper, we improve the navigation agentâ€™s contextual understanding by incorporating textual descriptions from multiple perspectives that facilitate analogical reasoning across images. By leveraging text-based analogical reasoning, the agent enhances its global scene understanding and spatial reasoning, leading to more accurate action decisions. We evaluate our approach on the R2R dataset, where our experiments demonstrate significant improvements in navigation performance. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆåˆ°åµŒå…¥å¼äººå·¥æ™ºèƒ½æ¨¡å‹ä¸­çš„è¶‹åŠ¿æ—¥ç›Šæ™®éã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºé›¶æ ·æœ¬çš„LLMè§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ä»£ç†è¦ä¹ˆå°†å›¾åƒç¼–ç ä¸ºæ–‡æœ¬åœºæ™¯æè¿°ï¼Œè¿™å¯èƒ½ç®€åŒ–äº†è§†è§‰ç»†èŠ‚ï¼Œè¦ä¹ˆå¤„ç†åŸå§‹å›¾åƒè¾“å…¥ï¼Œè¿™å¯èƒ½ä¼šå¿½ç•¥ç”¨äºé«˜çº§æ¨ç†æ‰€éœ€çš„æŠ½è±¡è¯­ä¹‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆæ¥è‡ªå¤šä¸ªè§’åº¦çš„æ–‡æœ¬æè¿°æ¥æé«˜å¯¼èˆªä»£ç†çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œè¿™äº›æè¿°æœ‰åŠ©äºå›¾åƒä¹‹é—´çš„ç±»æ¯”æ¨ç†ã€‚é€šè¿‡åˆ©ç”¨åŸºäºæ–‡æœ¬çš„ç±»æ¯”æ¨ç†ï¼Œä»£ç†æé«˜äº†å…¶å¯¹å…¨å±€åœºæ™¯çš„ç†è§£å’Œç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä»è€Œåšå‡ºæ›´å‡†ç¡®çš„è¡ŒåŠ¨å†³ç­–ã€‚æˆ‘ä»¬åœ¨R2Ræ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå®éªŒè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯¼èˆªæ€§èƒ½ä¸Šæœ‰äº†æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25139v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èå…¥å®ä½“AIæ¨¡å‹ä¸­çš„è¶‹åŠ¿ã€‚ç°æœ‰çš„é›¶æ ·æœ¬LLMåŸºç¡€è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ä»£ç†åœ¨å¤„ç†å›¾åƒæ—¶å­˜åœ¨ç¼ºé™·ï¼Œè¦ä¹ˆå°†å›¾åƒç¼–ç ä¸ºæ–‡æœ¬åœºæ™¯æè¿°ä»è€Œå¯èƒ½ç®€åŒ–è§†è§‰ç»†èŠ‚ï¼Œè¦ä¹ˆå¤„ç†åŸå§‹å›¾åƒè¾“å…¥ä»¥è‡´éš¾ä»¥æ•è·æŠ½è±¡è¯­ä¹‰ç”¨äºé«˜çº§æ¨ç†ã€‚æœ¬æ–‡é€šè¿‡ç»“åˆå¤šè§†è§’çš„æ–‡æœ¬æè¿°å¢å¼ºä»£ç†çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œå¹¶ä¿ƒè¿›å›¾åƒé—´çš„ç±»æ¯”æ¨ç†ã€‚åˆ©ç”¨åŸºäºæ–‡æœ¬çš„ç±»æ¯”æ¨ç†ï¼Œä»£ç†æå‡äº†å…¨å±€åœºæ™¯ç†è§£å’Œç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œè¿›è€Œä½œå‡ºæ›´å‡†ç¡®çš„è¡ŒåŠ¨å†³ç­–ã€‚åœ¨R2Ræ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æå¤§æå‡äº†å¯¼èˆªæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨è¶Šæ¥è¶Šå¤šåœ°è¢«é›†æˆåˆ°å®ä½“AIæ¨¡å‹ä¸­ã€‚</li>
<li>å½“å‰VLNä»£ç†åœ¨å¤„ç†å›¾åƒæ—¶å­˜åœ¨å±€é™æ€§ï¼Œè¦ä¹ˆç®€åŒ–è§†è§‰ç»†èŠ‚ï¼Œè¦ä¹ˆéš¾ä»¥æ•è·æŠ½è±¡è¯­ä¹‰ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡ç»“åˆå¤šè§†è§’çš„æ–‡æœ¬æè¿°æ¥æ”¹å–„ä»£ç†çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨åŸºäºæ–‡æœ¬çš„ç±»æ¯”æ¨ç†å¢å¼ºäº†ä»£ç†çš„å…¨å±€åœºæ™¯ç†è§£å’Œç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä»£ç†èƒ½å¤ŸåŸºäºæ”¹è¿›çš„ç†è§£åšå‡ºæ›´å‡†ç¡®çš„è¡ŒåŠ¨å†³ç­–ã€‚</li>
<li>åœ¨R2Ræ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†å¯¼èˆªæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25139">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5c9d3616c01b67f77092f6ed34a974c" align="middle">
<img src="https://picx.zhimg.com/v2-2ead3707940b0b23a53c0410defa3fcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7fba920a942213690108f0e95843092.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MGM-Omni-Scaling-Omni-LLMs-to-Personalized-Long-Horizon-Speech"><a href="#MGM-Omni-Scaling-Omni-LLMs-to-Personalized-Long-Horizon-Speech" class="headerlink" title="MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech"></a>MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech</h2><p><strong>Authors:Chengyao Wang, Zhisheng Zhong, Bohao Peng, Senqiao Yang, Yuqi Liu, Haokun Gui, Bin Xia, Jingyao Li, Bei Yu, Jiaya Jia</strong></p>
<p>We present MGM-Omni, a unified Omni LLM for omni-modal understanding and expressive, long-horizon speech generation. Unlike cascaded pipelines that isolate speech synthesis, MGM-Omni adopts a â€œbrain-mouthâ€ design with a dual-track, token-based architecture that cleanly decouples multimodal reasoning from real-time speech generation. This design enables efficient cross-modal interaction and low-latency, streaming speech generation. For understanding, a unified training strategy coupled with a dual audio encoder design enables long-form audio perception across diverse acoustic conditions. For generation, a chunk-based parallel decoding scheme narrows the text speech token-rate gap, accelerating inference and supporting streaming zero-shot voice cloning with stable timbre over extended durations. Compared to concurrent work, MGM-Omni achieves these capabilities with markedly data-efficient training. Extensive experiments demonstrate that MGM-Omni outperforms existing open source models in preserving timbre identity across extended sequences, producing natural and context-aware speech, and achieving superior long-form audio and omnimodal understanding. MGM-Omni establishes an efficient, end-to-end paradigm for omnimodal understanding and controllable, personalised long-horizon speech generation. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†MGM-Omniï¼Œè¿™æ˜¯ä¸€æ¬¾ç»Ÿä¸€çš„Omni LLMï¼Œç”¨äºå…¨æ¨¡æ€ç†è§£å’Œè¡¨è¾¾ï¼Œä»¥åŠé•¿æœŸè§†é‡çš„è¯­éŸ³ç”Ÿæˆã€‚ä¸åŒäºå°†è¯­éŸ³åˆæˆå­¤ç«‹çš„çº§è”ç®¡é“ï¼ŒMGM-Omnié‡‡ç”¨äº†â€œå¤§è„‘-å˜´å·´â€è®¾è®¡ï¼Œå…·æœ‰åŒè½¨ã€åŸºäºä»¤ç‰Œçš„ç»“æ„ï¼Œèƒ½å¤Ÿå¹²å‡€åœ°å°†å¤šæ¨¡æ€æ¨ç†ä¸å®æ—¶è¯­éŸ³ç”Ÿæˆåˆ†å¼€ã€‚è¿™ç§è®¾è®¡å®ç°äº†é«˜æ•ˆçš„è·¨æ¨¡æ€äº¤äº’å’Œä½å»¶è¿Ÿçš„æµå¼è¯­éŸ³ç”Ÿæˆã€‚åœ¨ç†è§£æ–¹é¢ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„è®­ç»ƒç­–ç•¥å’ŒåŒéŸ³é¢‘ç¼–ç å™¨è®¾è®¡ï¼Œå¯å®ç°ä¸åŒå£°å­¦ç¯å¢ƒä¸‹çš„é•¿éŸ³é¢‘æ„ŸçŸ¥ã€‚åœ¨ç”Ÿæˆæ–¹é¢ï¼ŒåŸºäºå—çš„å¹¶è¡Œè§£ç æ–¹æ¡ˆç¼©å°äº†æ–‡æœ¬è¯­éŸ³ä»¤ç‰Œé€Ÿç‡å·®è·ï¼ŒåŠ é€Ÿäº†æ¨ç†ï¼Œå¹¶æ”¯æŒåœ¨æ‰©å±•æ—¶é—´æ®µä¸Šå®ç°ç¨³å®šçš„éŸ³è‰²é›¶å°„å‡»å…‹éš†å’Œæµå¼ä¼ è¾“ã€‚ä¸åŒæœŸå·¥ä½œç›¸æ¯”ï¼ŒMGM-Omniåœ¨æ•°æ®é«˜æ•ˆè®­ç»ƒæ–¹é¢å®ç°äº†è¿™äº›åŠŸèƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMGM-Omniåœ¨ä¿ç•™æ‰©å±•åºåˆ—çš„éŸ³è‰²èº«ä»½ã€äº§ç”Ÿè‡ªç„¶å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯­éŸ³ä»¥åŠå®ç°é•¿æœŸéŸ³é¢‘å’Œå…¨æ¨¡æ€ç†è§£æ–¹é¢ä¼˜äºç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚MGM-Omniä¸ºå…¨æ¨¡æ€ç†è§£å’Œå¯æ§ã€ä¸ªæ€§åŒ–çš„é•¿æœŸè§†é‡è¯­éŸ³ç”Ÿæˆå»ºç«‹äº†é«˜æ•ˆã€ç«¯åˆ°ç«¯çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25131v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/dvlab-research/MGM-Omni">https://github.com/dvlab-research/MGM-Omni</a></p>
<p><strong>Summary</strong></p>
<p>MGM-Omniæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å…¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå…¨æ¨¡æ€ç†è§£å’Œè¡¨è¾¾ï¼Œä»¥åŠé•¿å‘¨æœŸè¯­éŸ³ç”Ÿæˆã€‚å®ƒé‡‡ç”¨â€œè„‘-å£â€è®¾è®¡ï¼Œå…·æœ‰åŒè½¨ã€åŸºäºä»¤ç‰Œçš„ç»“æ„ï¼Œèƒ½å¤Ÿå¹²å‡€åœ°è§£è€¦å¤šæ¨¡æ€æ¨ç†å’Œå®æ—¶è¯­éŸ³ç”Ÿæˆã€‚è¿™ç§è®¾è®¡å®ç°äº†é«˜æ•ˆçš„è·¨æ¨¡æ€äº¤äº’å’Œä½å»¶è¿Ÿçš„æµå¼è¯­éŸ³ç”Ÿæˆã€‚é€šè¿‡ç»Ÿä¸€çš„è®­ç»ƒç­–ç•¥å’ŒåŒéŸ³é¢‘ç¼–ç å™¨è®¾è®¡ï¼Œæé«˜äº†é•¿éŸ³é¢‘æ„ŸçŸ¥èƒ½åŠ›ï¼Œè·¨è¶Šä¸åŒçš„å£°å­¦ç¯å¢ƒã€‚åœ¨ç”Ÿæˆæ–¹é¢ï¼ŒåŸºäºåˆ†å—çš„å¹¶è¡Œè§£ç æ–¹æ¡ˆç¼©å°äº†æ–‡æœ¬è¯­éŸ³ä»¤ç‰Œç‡å·®è·ï¼ŒåŠ å¿«äº†æ¨ç†é€Ÿåº¦ï¼Œæ”¯æŒæ‰©å±•æ—¶é•¿å†…çš„ç¨³å®šéŸ³è‰²é›¶æ ·æœ¬è¯­éŸ³å…‹éš†å’Œæµå¼ä¼ è¾“ã€‚ä¸åŒæœŸå·¥ä½œç›¸æ¯”ï¼ŒMGM-Omniçš„æ•°æ®è®­ç»ƒæ•ˆç‡æ˜¾è‘—ã€‚å®éªŒè¡¨æ˜ï¼ŒMGM-Omniåœ¨ä¿æŒæ‰©å±•åºåˆ—çš„éŸ³è‰²èº«ä»½ã€äº§ç”Ÿè‡ªç„¶å’Œè¯­å¢ƒæ„ŸçŸ¥çš„è¯­éŸ³ã€å®ç°é•¿æœŸéŸ³é¢‘å’Œå…¨èƒ½ç†è§£æ–¹é¢ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ã€‚MGM-Omniä¸ºå…¨èƒ½ç†è§£å’Œå¯æ§ã€ä¸ªæ€§åŒ–çš„é•¿å‘¨æœŸè¯­éŸ³ç”Ÿæˆå»ºç«‹äº†é«˜æ•ˆã€ç«¯åˆ°ç«¯çš„èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MGM-Omniæ˜¯ä¸€ä¸ªå…¨æ¨¡æ€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå…¨æ¨¡æ€ç†è§£å’Œè¡¨è¾¾ä»¥åŠé•¿å‘¨æœŸè¯­éŸ³ç”Ÿæˆã€‚</li>
<li>é‡‡ç”¨â€œè„‘-å£â€è®¾è®¡å’ŒåŒè½¨ã€åŸºäºä»¤ç‰Œçš„ç»“æ„ï¼Œå®ç°å¤šæ¨¡æ€æ¨ç†å’Œå®æ—¶è¯­éŸ³ç”Ÿæˆçš„è§£è€¦ã€‚</li>
<li>é€šè¿‡ç»Ÿä¸€çš„è®­ç»ƒç­–ç•¥å’ŒåŒéŸ³é¢‘ç¼–ç å™¨è®¾è®¡ï¼Œæé«˜äº†é•¿éŸ³é¢‘æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>åŸºäºåˆ†å—çš„å¹¶è¡Œè§£ç æ–¹æ¡ˆæ”¯æŒé›¶æ ·æœ¬è¯­éŸ³å…‹éš†å’Œæµå¼ä¼ è¾“ã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼ŒMGM-Omniçš„æ•°æ®è®­ç»ƒæ•ˆç‡æ˜¾è‘—ã€‚</li>
<li>MGM-Omniåœ¨ä¿æŒéŸ³è‰²èº«ä»½ã€äº§ç”Ÿè‡ªç„¶å’Œè¯­å¢ƒæ„ŸçŸ¥çš„è¯­éŸ³æ–¹é¢ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a4bf240bf02f7abe13ac7cfa86369eda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72e6a50b2c96bd4a0f4b2ff527f4df47" align="middle">
<img src="https://pic1.zhimg.com/v2-16d02220684bdda929365bfec3d70255.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-628059bf96623bb1dbbbf4edee4d2a4e" align="middle">
<img src="https://picx.zhimg.com/v2-1ca188737fae4d2142e5d079c6630047" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ORPO-Distill-Mixed-Policy-Preference-Optimization-for-Cross-Architecture-LLM-Distillation"><a href="#ORPO-Distill-Mixed-Policy-Preference-Optimization-for-Cross-Architecture-LLM-Distillation" class="headerlink" title="ORPO-Distill: Mixed-Policy Preference Optimization for   Cross-Architecture LLM Distillation"></a>ORPO-Distill: Mixed-Policy Preference Optimization for   Cross-Architecture LLM Distillation</h2><p><strong>Authors:Aasheesh Singh, Vishal Vaddina, Dagnachew Birru</strong></p>
<p>We introduce ORPO-Distill, a general-purpose method for cross-architecture LLM distillation that formulates the problem as a preference optimization task. Unlike standard CoT distillation, the approach transfers knowledge through diverse reasoning traces. It employs an Odds-Ratio Preference Optimization objective that contrasts teacher and student traces for more effective learning, and adopts a mixed-policy strategy for utilizing student-generated outputs, outperforming both off- and on-policy alternatives. Experiments on five datasets and multiple student models show consistent improvements over conventional black-box KD baselines. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ORPO-Distillï¼Œè¿™æ˜¯ä¸€ç§è·¨æ¶æ„å¤§å‹è¯­è¨€æ¨¡å‹è’¸é¦çš„é€šç”¨æ–¹æ³•ï¼Œå®ƒå°†é—®é¢˜è¡¨è¿°ä¸ºåå¥½ä¼˜åŒ–ä»»åŠ¡ã€‚ä¸åŒäºæ ‡å‡†çš„è®¤çŸ¥è½¨è¿¹è’¸é¦ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¤šæ ·çš„æ¨ç†è½¨è¿¹è¿›è¡ŒçŸ¥è¯†è¿ç§»ã€‚å®ƒé‡‡ç”¨Odds-Ratio Preference Optimizationç›®æ ‡ï¼Œå¯¹æ¯”æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹çš„è½¨è¿¹ä»¥å®ç°æ›´æœ‰æ•ˆçš„å­¦ä¹ ï¼Œå¹¶é‡‡çº³æ··åˆç­–ç•¥åˆ©ç”¨å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºï¼Œè¶…è¶Šäº†ç¦»çº¿ç­–ç•¥å’Œåœ¨çº¿ç­–ç•¥çš„é€‰æ‹©ã€‚åœ¨äº”ä¸ªæ•°æ®é›†å’Œå¤šä¸ªå­¦ç”Ÿæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„é»‘ç›’çŸ¥è¯†è’¸é¦åŸºçº¿ï¼Œå…¶è¡¨ç°æŒç»­æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25100v1">PDF</a> Accepted at NeurIPS 2025, Efficient Reasoning Workshop</p>
<p><strong>Summary</strong></p>
<p>ORPO-Distillæ–¹æ³•æ˜¯ä¸€ç§é€šç”¨çš„è·¨æ¶æ„å¤§å‹è¯­è¨€æ¨¡å‹è’¸é¦æ–¹æ³•ï¼Œå°†é—®é¢˜è¡¨è¿°ä¸ºåå¥½ä¼˜åŒ–ä»»åŠ¡ã€‚ä¸åŒäºä¼ ç»Ÿçš„è®¤çŸ¥é“¾è’¸é¦ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¤šæ ·çš„æ¨ç†è½¨è¿¹è¿›è¡ŒçŸ¥è¯†è¿ç§»ã€‚å®ƒé‡‡ç”¨Odds-Ratio Preference Optimizationç›®æ ‡ï¼Œå¯¹æ¯”æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹çš„æ¨ç†è½¨è¿¹ä»¥æ›´æœ‰æ•ˆåœ°å­¦ä¹ ï¼ŒåŒæ—¶é‡‡ç”¨æ··åˆç­–ç•¥åˆ©ç”¨å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºï¼Œè¶…è¶Šäº†ç¦»çº¿ä¸åœ¨çº¿ç­–ç•¥çš„é€‰æ‹©ã€‚åœ¨äº”ä¸ªæ•°æ®é›†å’Œå¤šä¸ªå­¦ç”Ÿæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„é»‘ç®±çŸ¥è¯†è’¸é¦åŸºçº¿æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…·æœ‰æŒç»­çš„æå‡æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ORPO-Distillæ˜¯ä¸€ç§è·¨æ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹è’¸é¦æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å°†é—®é¢˜è¡¨è¿°ä¸ºåå¥½ä¼˜åŒ–ä»»åŠ¡ï¼Œä¸åŒäºä¼ ç»Ÿçš„è®¤çŸ¥é“¾è’¸é¦ã€‚</li>
<li>ORPO-Distillé€šè¿‡å¤šæ ·çš„æ¨ç†è½¨è¿¹è¿›è¡ŒçŸ¥è¯†è¿ç§»ã€‚</li>
<li>é‡‡ç”¨Odds-Ratio Preference Optimizationç›®æ ‡ï¼Œå¯¹æ¯”æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹çš„æ¨ç†è½¨è¿¹ä»¥æ›´æœ‰æ•ˆåœ°å­¦ä¹ ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨æ··åˆç­–ç•¥åˆ©ç”¨å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºï¼Œè¶…è¶Šäº†ç¦»çº¿ä¸åœ¨çº¿ç­–ç•¥çš„é€‰æ‹©ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†å’Œå­¦ç”Ÿæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒORPO-Distillæ–¹æ³•è¾ƒä¼ ç»Ÿæ–¹æ³•å…·æœ‰æŒç»­çš„æå‡æ•ˆæœã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å…·æœ‰æ½œåœ¨çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1521b178581ccacb1e37c0777ee40381.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-461854dcc2838ac53bb065002b65637e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89324e292526ecdfb0a7f4352f6df8de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ce5d84a6c02bc5e3ea65a5e115550be" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Cogito-Ergo-Ludo-An-Agent-that-Learns-to-Play-by-Reasoning-and-Planning"><a href="#Cogito-Ergo-Ludo-An-Agent-that-Learns-to-Play-by-Reasoning-and-Planning" class="headerlink" title="Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and   Planning"></a>Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and   Planning</h2><p><strong>Authors:Sai Wang, Yu Wu, Zhongwen Xu</strong></p>
<p>The pursuit of artificial agents that can learn to master complex environments has led to remarkable successes, yet prevailing deep reinforcement learning methods often rely on immense experience, encoding their knowledge opaquely within neural network weights. We propose a different paradigm, one in which an agent learns to play by reasoning and planning. We introduce Cogito, ergo ludo (CEL), a novel agent architecture that leverages a Large Language Model (LLM) to build an explicit, language-based understanding of its environmentâ€™s mechanics and its own strategy. Starting from a tabula rasa state with no prior knowledge (except action set), CEL operates on a cycle of interaction and reflection. After each episode, the agent analyzes its complete trajectory to perform two concurrent learning processes: Rule Induction, where it refines its explicit model of the environmentâ€™s dynamics, and Strategy and Playbook Summarization, where it distills experiences into an actionable strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e., Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent successfully learns to master these games by autonomously discovering their rules and developing effective policies from sparse rewards. Ablation studies confirm that the iterative process is critical for sustained learning. Our work demonstrates a path toward more general and interpretable agents that not only act effectively but also build a transparent and improving model of their world through explicit reasoning on raw experience. </p>
<blockquote>
<p>è¿½æ±‚èƒ½å¤Ÿå­¦ä¹ æŒæ¡å¤æ‚ç¯å¢ƒçš„æ™ºèƒ½ä»£ç†å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œç„¶è€Œæµè¡Œçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤§é‡çš„ç»éªŒï¼Œå°†ä»–ä»¬çš„çŸ¥è¯†ç¼–ç åœ¨ç¥ç»ç½‘ç»œæƒé‡ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸åŒçš„èŒƒå¼ï¼Œå³ä»£ç†é€šè¿‡æ¨ç†å’Œè§„åˆ’æ¥å­¦ä¹ ç©è€ã€‚æˆ‘ä»¬å¼•å…¥äº†Cogitoï¼Œergo ludoï¼ˆCELï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ä»£ç†æ¶æ„ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å»ºç«‹åŸºäºè¯­è¨€çš„å¯¹ç¯å¢ƒæœºåˆ¶å’Œè‡ªèº«ç­–ç•¥çš„æ˜ç¡®ç†è§£ã€‚ä»æ— çŸ¥çš„çŠ¶æ€å¼€å§‹ï¼ˆä»…çŸ¥é“åŠ¨ä½œé›†ï¼‰ï¼ŒCELè¿›è¡Œäº¤äº’å’Œåæ€çš„å¾ªç¯ã€‚åœ¨æ¯ä¸€é›†ä¹‹åï¼Œä»£ç†åˆ†æå…¶å®Œæ•´çš„è½¨è¿¹ï¼ŒåŒæ—¶è¿›è¡Œä¸¤ä¸ªå¹¶è¡Œå­¦ä¹ è¿‡ç¨‹ï¼šè§„åˆ™å½’çº³ï¼Œå®ƒç²¾ç‚¼äº†å¯¹ç¯å¢ƒåŠ¨æ€çš„æ˜ç¡®æ¨¡å‹ï¼›ç­–ç•¥å’Œå‰§æœ¬æ‘˜è¦ï¼Œå®ƒå°†ç»éªŒæç‚¼æˆå¯æ“ä½œçš„æˆ˜ç•¥å‰§æœ¬ã€‚æˆ‘ä»¬åœ¨å¤šæ ·çš„ç½‘æ ¼ä¸–ç•Œä»»åŠ¡ï¼ˆå¦‚æ‰«é›·ã€å†°å†»æ¹–å’Œç´¢ç§‘ç­ï¼‰ä¸­å¯¹CELè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ˜¾ç¤ºCELä»£ç†é€šè¿‡è‡ªä¸»å‘ç°è§„åˆ™å¹¶ä»ç¨€ç–å¥–åŠ±ä¸­å‘å±•å‡ºæœ‰æ•ˆç­–ç•¥ï¼ŒæˆåŠŸæŒæ¡äº†è¿™äº›æ¸¸æˆã€‚æ¶ˆèç ”ç©¶è¯å®ï¼Œè¿­ä»£è¿‡ç¨‹å¯¹äºæŒç»­å­¦ä¹ è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†ä¸€æ¡é€šå¾€æ›´é€šç”¨å’Œå¯è§£é‡Šçš„ä»£ç†çš„é“è·¯ï¼Œè¿™äº›ä»£ç†ä¸ä»…æœ‰æ•ˆåœ°è¡ŒåŠ¨ï¼Œè€Œä¸”é€šè¿‡åŸå§‹ç»éªŒçš„æ˜ç¡®æ¨ç†æ¥å»ºç«‹é€æ˜ä¸”ä¸æ–­æ”¹å–„çš„ä¸–ç•Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25052v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºCogitoï¼Œergo ludoï¼ˆCELï¼‰çš„æ–°å‹æ™ºèƒ½ä½“æ¶æ„ã€‚è¯¥æ¶æ„åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å»ºç«‹å¯¹ç¯å¢ƒæœºåˆ¶å’Œè‡ªèº«ç­–ç•¥çš„è¯­è¨€åŒ–ç†è§£ã€‚é€šè¿‡äº’åŠ¨å’Œåæ€å¾ªç¯ï¼ŒCELæ™ºèƒ½ä½“åœ¨ç»å†æ¯ä¸€é›†åï¼ŒåŒæ—¶è¿›è¡Œè§„åˆ™å½’çº³å’Œç­–ç•¥åŠè¡ŒåŠ¨æŒ‡å—æ€»ç»“ï¼Œä»è€Œè‡ªä¸»å‘ç°æ¸¸æˆè§„åˆ™å¹¶å‘å±•å‡ºæœ‰æ•ˆçš„ç­–ç•¥ã€‚åœ¨å¤šç§ç½‘æ ¼ä¸–ç•Œä»»åŠ¡ä¸­ï¼ŒCELæ™ºèƒ½ä½“æˆåŠŸæŒæ¡äº†æ¸¸æˆã€‚ç ”ç©¶è¯å®è¿­ä»£è¿‡ç¨‹å¯¹æŒç»­å­¦ä¹ è‡³å…³é‡è¦ã€‚æ­¤ç ”ç©¶å±•ç¤ºäº†ä¸€ç§é€šç”¨ä¸”å¯è§£é‡Šçš„æ™ºèƒ½ä½“è·¯å¾„ï¼Œè¿™äº›æ™ºèƒ½ä½“ä¸ä»…èƒ½å¤Ÿæœ‰æ•ˆè¡ŒåŠ¨ï¼Œè€Œä¸”èƒ½å¤Ÿé€šè¿‡åŸå§‹ç»éªŒè¿›è¡Œæ˜ç¡®æ¨ç†æ¥æ„å»ºå…¶ä¸–ç•Œçš„é€æ˜æ€§å’Œæ”¹è¿›æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Cogitoï¼Œergo ludoï¼ˆCELï¼‰æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“æ¶æ„ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡è¯­è¨€ç†è§£ç¯å¢ƒå’Œåˆ¶å®šç­–ç•¥ã€‚</li>
<li>CELæ™ºèƒ½ä½“é€šè¿‡äº’åŠ¨å’Œåæ€å¾ªç¯è¿›è¡Œå­¦ä¹ ï¼Œè‡ªä¸»å‘ç°æ¸¸æˆè§„åˆ™å¹¶å‘å±•å‡ºæœ‰æ•ˆçš„ç­–ç•¥ã€‚</li>
<li>CELæ™ºèƒ½ä½“åœ¨å¤šç§ç½‘æ ¼ä¸–ç•Œä»»åŠ¡ä¸­æˆåŠŸæŒæ¡æ¸¸æˆï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>è¿­ä»£è¿‡ç¨‹å¯¹äºæ™ºèƒ½ä½“çš„æŒç»­å­¦ä¹ è‡³å…³é‡è¦ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒCELæ™ºèƒ½ä½“é€šè¿‡æ˜ç¡®æ¨ç†å’Œæ€»ç»“ç»éªŒæ¥æ„å»ºå…¶ç¯å¢ƒæ¨¡å‹ï¼Œä½¿å…¶çŸ¥è¯†æ›´åŠ é€æ˜å’Œå¯è§£é‡Šã€‚</li>
<li>CELæ™ºèƒ½ä½“çš„å­¦ä¹ æ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒä»»åŠ¡ä¸­é€‚åº”å¹¶å­¦ä¹ æ–°è§„åˆ™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d008d1696b1bce43bd81c973222de2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a95de162af4462ba10c9a96c5a399f9c" align="middle">
<img src="https://picx.zhimg.com/v2-bcec42336d9d83e03ee1e3fd16d6b0cb" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Advantage-Weighted-Matching-Aligning-RL-with-Pretraining-in-Diffusion-Models"><a href="#Advantage-Weighted-Matching-Aligning-RL-with-Pretraining-in-Diffusion-Models" class="headerlink" title="Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion   Models"></a>Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion   Models</h2><p><strong>Authors:Shuchen Xue, Chongjian Ge, Shilong Zhang, Yichen Li, Zhi-Ming Ma</strong></p>
<p>Reinforcement Learning (RL) has emerged as a central paradigm for advancing Large Language Models (LLMs), where pre-training and RL post-training share the same log-likelihood formulation. In contrast, recent RL approaches for diffusion models, most notably Denoising Diffusion Policy Optimization (DDPO), optimize an objective different from the pretraining objectivesâ€“score&#x2F;flow matching loss. In this work, we establish a novel theoretical analysis: DDPO is an implicit form of score&#x2F;flow matching with noisy targets, which increases variance and slows convergence. Building on this analysis, we introduce \textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for diffusion. It uses the same score&#x2F;flow-matching loss as pretraining to obtain a lower-variance objective and reweights each sample by its advantage. In effect, AWM raises the influence of high-reward samples and suppresses low-reward ones while keeping the modeling objective identical to pretraining. This unifies pretraining and RL conceptually and practically, is consistent with policy-gradient theory, reduces variance, and yields faster convergence. This simple yet effective design yields substantial benefits: on GenEval, OCR, and PickScore benchmarks, AWM delivers up to a $24\times$ speedup over Flow-GRPO (which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX, without compromising generation quality. Code is available at <a target="_blank" rel="noopener" href="https://github.com/scxue/advantage_weighted_matching">https://github.com/scxue/advantage_weighted_matching</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²ç»æˆä¸ºæ¨è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒèŒƒå¼ï¼Œå…¶ä¸­é¢„è®­ç»ƒå’ŒRLåè®­ç»ƒé‡‡ç”¨ç›¸åŒçš„å¯¹æ•°ä¼¼ç„¶å…¬å¼ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ€è¿‘çš„æ‰©æ•£æ¨¡å‹ä¸­çš„RLæ–¹æ³•ï¼Œå°¤å…¶æ˜¯é™å™ªæ‰©æ•£ç­–ç•¥ä¼˜åŒ–ï¼ˆDDPOï¼‰ï¼Œä¼˜åŒ–çš„ç›®æ ‡ä¸é¢„è®­ç»ƒç›®æ ‡ä¸åŒâ€”â€”åˆ†æ•°&#x2F;æµåŒ¹é…æŸå¤±ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†æ–°çš„ç†è®ºåˆ†æï¼šDDPOæ˜¯å¸¦æœ‰å™ªå£°ç›®æ ‡çš„åˆ†æ•°&#x2F;æµåŒ¹é…çš„éšå¼å½¢å¼ï¼Œè¿™å¢åŠ äº†æ–¹å·®å¹¶å‡ç¼“äº†æ”¶æ•›é€Ÿåº¦ã€‚åŸºäºè¿™ä¸€åˆ†æï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¼˜åŠ¿åŠ æƒåŒ¹é…ï¼ˆAWMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ‰©æ•£çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ã€‚å®ƒä½¿ç”¨ä¸é¢„è®­ç»ƒç›¸åŒçš„åˆ†æ•°&#x2F;æµåŒ¹é…æŸå¤±æ¥è·å¾—ä½æ–¹å·®ç›®æ ‡ï¼Œå¹¶æŒ‰ä¼˜åŠ¿å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡ŒåŠ æƒã€‚å®é™…ä¸Šï¼ŒAWMæé«˜äº†é«˜å¥–åŠ±æ ·æœ¬çš„å½±å“ï¼Œå¹¶æŠ‘åˆ¶äº†ä½å¥–åŠ±æ ·æœ¬ï¼ŒåŒæ—¶ä¿æŒå»ºæ¨¡ç›®æ ‡ä¸é¢„è®­ç»ƒç›¸åŒã€‚è¿™åœ¨æ¦‚å¿µå’Œå®è·µä¸Šç»Ÿä¸€äº†é¢„è®­ç»ƒå’ŒRLï¼Œç¬¦åˆç­–ç•¥æ¢¯åº¦ç†è®ºï¼Œé™ä½äº†æ–¹å·®ï¼Œå¹¶å®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚è¿™ç§ç®€å•è€Œæœ‰æ•ˆçš„è®¾è®¡å¸¦æ¥äº†å®è´¨æ€§çš„å¥½å¤„ï¼šåœ¨GenEvalã€OCRå’ŒPickScoreåŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸åŸºäºDDPOçš„Flow-GRPOç›¸æ¯”ï¼ŒAWMåœ¨åº”ç”¨äºStable Diffusion 3.5 Mediumå’ŒFLUXæ—¶å®ç°äº†é«˜è¾¾24å€çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¸æŸå®³ç”Ÿæˆè´¨é‡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/scxue/advantage_weighted_matching%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/scxue/advantage_weighted_matchingæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25050v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„RLæ–¹æ³•ï¼Œå¦‚Denoising Diffusion Policy Optimizationï¼ˆDDPOï¼‰ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹ç†è®ºåˆ†ææ–¹æ³•ï¼ŒæŒ‡å‡ºDDPOæ˜¯å¸¦æœ‰å™ªå£°ç›®æ ‡éšå¼çš„score&#x2F;flowåŒ¹é…å½¢å¼ï¼Œä¼šå¢åŠ æ–¹å·®å¹¶å‡æ…¢æ”¶æ•›é€Ÿåº¦ã€‚åŸºäºæ­¤åˆ†æï¼Œæ–‡ç« å¼•å…¥äº†Advantage Weighted Matchingï¼ˆAWMï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ‰©æ•£æ¨¡å‹çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ã€‚AWMä½¿ç”¨ä¸é¢„è®­ç»ƒç›¸åŒçš„score&#x2F;flowåŒ¹é…æŸå¤±æ¥è·å¾—ä½æ–¹å·®ç›®æ ‡ï¼Œå¹¶é€šè¿‡ä¼˜åŠ¿å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡ŒåŠ æƒã€‚è¯¥æ–¹æ³•åœ¨æ¦‚å¿µå’Œå®è·µä¸Šå°†é¢„è®­ç»ƒå’ŒRLç»Ÿä¸€èµ·æ¥ï¼Œç¬¦åˆç­–ç•¥æ¢¯åº¦ç†è®ºï¼Œå‡å°‘æ–¹å·®å¹¶å®ç°æ›´å¿«çš„æ”¶æ•›ã€‚åœ¨GenEvalã€OCRå’ŒPickScoreåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAWMåœ¨Stable Diffusion 3.5 Mediumå’ŒFLUXä¸Šåº”ç”¨æ—¶ï¼Œç”Ÿæˆè´¨é‡ä¸é™ä½çš„æƒ…å†µä¸‹ï¼Œé€Ÿåº¦æ¯”åŸºäºDDPOçš„Flow-GRPOæé«˜äº†24å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚</li>
<li>Denoising Diffusion Policy Optimizationï¼ˆDDPOï¼‰ä½œä¸ºå½“å‰ä¸»æµçš„RLæ–¹æ³•ï¼Œå­˜åœ¨å¢åŠ æ–¹å·®å’Œå‡æ…¢æ”¶æ•›é€Ÿåº¦çš„é—®é¢˜ã€‚</li>
<li>æ–‡ç« æå‡ºäº†Advantage Weighted Matchingï¼ˆAWMï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ç”¨äºæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>AWMä½¿ç”¨ä¸é¢„è®­ç»ƒç›¸åŒçš„score&#x2F;flowåŒ¹é…æŸå¤±ï¼Œè·å¾—ä½æ–¹å·®ç›®æ ‡å¹¶æå‡é«˜å›æŠ¥æ ·æœ¬çš„å½±å“ã€‚</li>
<li>AWMåœ¨ä¿è¯ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå¯ä»¥æ˜¾è‘—æé«˜è®­ç»ƒé€Ÿåº¦ï¼Œä¾‹å¦‚åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜è¾¾24å€çš„é€Ÿåº¦æå‡ã€‚</li>
<li>AWMå°†é¢„è®­ç»ƒå’ŒRLåœ¨ç†è®ºå’Œå®è·µå±‚é¢ç»Ÿä¸€èµ·æ¥ï¼Œä¸ç­–ç•¥æ¢¯åº¦ç†è®ºä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15e072642699233430692c5bd31fc0cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9d9608f23602cbb072c9ef94b100986.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Confidence-Guided-Error-Correction-for-Disordered-Speech-Recognition"><a href="#Confidence-Guided-Error-Correction-for-Disordered-Speech-Recognition" class="headerlink" title="Confidence-Guided Error Correction for Disordered Speech Recognition"></a>Confidence-Guided Error Correction for Disordered Speech Recognition</h2><p><strong>Authors:Abner Hernandez, TomÃ¡s Arias Vergara, Andreas Maier, Paula Andrea PÃ©rez-Toro</strong></p>
<p>We investigate the use of large language models (LLMs) as post-processing modules for automatic speech recognition (ASR), focusing on their ability to perform error correction for disordered speech. In particular, we propose confidence-informed prompting, where word-level uncertainty estimates are embedded directly into LLM training to improve robustness and generalization across speakers and datasets. This approach directs the model to uncertain ASR regions and reduces overcorrection. We fine-tune a LLaMA 3.1 model and compare our approach to both transcript-only fine-tuning and post hoc confidence-based filtering. Evaluations show that our method achieves a 10% relative WER reduction compared to naive LLM correction on the Speech Accessibility Project spontaneous speech and a 47% reduction on TORGO, demonstrating the effectiveness of confidence-aware fine-tuning for impaired speech. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åå¤„ç†æ¨¡å—ä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹ç ”ç©¶å…¶åœ¨çº æ­£ä¹±åºè¯­éŸ³æ–¹é¢çš„èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä¿¡å¿ƒçš„æç¤ºæ–¹æ³•ï¼Œå°†è¯çº§ä¸ç¡®å®šæ€§ä¼°è®¡ç›´æ¥åµŒå…¥åˆ°LLMè®­ç»ƒä¸­ï¼Œä»¥æé«˜è·¨è¯´è¯è€…å’Œæ•°æ®é›†çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•å¼•å¯¼æ¨¡å‹è¿›å…¥ä¸ç¡®å®šçš„ASRåŒºåŸŸï¼Œå‡å°‘è¿‡åº¦çº æ­£ã€‚æˆ‘ä»¬å¯¹LLaMA 3.1æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶å°†æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åŸºäºæ–‡æœ¬çš„å¾®è°ƒä»¥åŠåŸºäºäº‹åä¿¡å¿ƒçš„è¿‡æ»¤æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¯­éŸ³æ— éšœç¢é¡¹ç›®è‡ªç„¶è¯­éŸ³ä¸Šçš„ç›¸å¯¹è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½äº†10%ï¼Œåœ¨TORGOä¸Šé™ä½äº†47%ï¼Œè¯æ˜äº†ä¿¡å¿ƒæ„ŸçŸ¥å¾®è°ƒå¯¹äºå—æŸè¯­éŸ³çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25048v1">PDF</a> Preprint submitted to ICASSP</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åå¤„ç†æ¨¡å—ä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹æ¢è®¨äº†å…¶åœ¨çº æ­£æ··ä¹±è¯­éŸ³æ–¹é¢çš„é”™è¯¯çº æ­£èƒ½åŠ›ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç½®ä¿¡åº¦çš„æç¤ºæ–¹æ³•ï¼Œå°†è¯çº§ä¸ç¡®å®šæ€§ä¼°è®¡ç›´æ¥åµŒå…¥åˆ°LLMè®­ç»ƒä¸­ï¼Œä»¥æé«˜æ¨¡å‹å¯¹ä¸åŒå‘è¨€è€…å’Œæ•°æ®é›†çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å¼•å¯¼æ¨¡å‹å®šä½åˆ°ä¸ç¡®å®šçš„ASRåŒºåŸŸï¼Œå¹¶å‡å°‘è¿‡åº¦çº æ­£ã€‚é€šè¿‡å¯¹LLaMA 3.1æ¨¡å‹çš„å¾®è°ƒï¼Œä¸ä»…åŸºäºæ–‡æœ¬çš„å¾®è°ƒä»¥åŠåŸºäºç½®ä¿¡åº¦çš„åæœŸè¿‡æ»¤æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è¯­éŸ³å¯è®¿é—®æ€§é¡¹ç›®çš„å³å…´æ¼”è®²ä¸Šç›¸å¯¹äºç®€å•çš„LLMæ ¡æ­£å®ç°äº†10%çš„ç›¸å¯¹å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½ï¼Œåœ¨TORGOä¸Šçš„é™ä½å¹…åº¦æ›´æ˜¯é«˜è¾¾47%ï¼Œè¯æ˜äº†åŸºäºç½®ä¿¡åº¦è°ƒæ•™çš„ç²¾ç»†è°ƒæ•´å¯¹äºå—æŸè¯­éŸ³çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«ç”¨ä½œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„åå¤„ç†æ¨¡å—ï¼Œç”¨äºæ”¹å–„æ··ä¹±è¯­éŸ³çš„é”™è¯¯çº æ­£èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ç½®ä¿¡åº¦æ„ŸçŸ¥æç¤ºæ–¹æ³•ï¼Œå°†è¯çº§ä¸ç¡®å®šæ€§ä¼°è®¡èå…¥LLMè®­ç»ƒä¸­ï¼Œæé«˜æ¨¡å‹ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ–¹æ³•å®šä½ASRä¸­çš„ä¸ç¡®å®šåŒºåŸŸï¼Œå‡å°‘è¿‡åº¦çº æ­£ã€‚</li>
<li>é€šè¿‡å¾®è°ƒLLaMA 3.1æ¨¡å‹ï¼Œä¸ä»…åŸºäºæ–‡æœ¬çš„å¾®è°ƒåŠåŸºäºç½®ä¿¡åº¦çš„åæœŸè¿‡æ»¤æ¯”è¾ƒï¼Œè¯„ä¼°è¯å®æ‰€ææ–¹æ³•æœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨è¯­éŸ³å¯è®¿é—®æ€§é¡¹ç›®çš„å³å…´æ¼”è®²ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸å¯¹ç®€å•çš„LLMæ ¡æ­£å®ç°äº†10%çš„WERé™ä½ã€‚</li>
<li>åœ¨TORGOæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†é«˜è¾¾47%çš„WERé™ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f533e368c963fa609aa9428b80ffc153" align="middle">
<img src="https://pica.zhimg.com/v2-4202b496a1cffe20b772516c3b81222d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4401dac56bea5eda576ce90c241d76cc" align="middle">
<img src="https://pic1.zhimg.com/v2-c98a0b5e066e0c1a9d0734432dd7f7fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df7b2a5c9a24c195813daed4a3f8df5d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-for-Software-Testing-A-Research-Roadmap"><a href="#Large-Language-Models-for-Software-Testing-A-Research-Roadmap" class="headerlink" title="Large Language Models for Software Testing: A Research Roadmap"></a>Large Language Models for Software Testing: A Research Roadmap</h2><p><strong>Authors:Cristian Augusto, Antonia Bertolino, Guglielmo De Angelis, Francesca Lonetti, JesÃºs MorÃ¡n</strong></p>
<p>Large Language Models (LLMs) are starting to be profiled as one of the most significant disruptions in the Software Testing field.   Specifically, they have been successfully applied in software testing tasks such as generating test code, or summarizing documentation.   This potential has attracted hundreds of researchers, resulting in dozens of new contributions every month, hardening researchers to   stay at the forefront of the wave. Still, to the best of our knowledge, no prior work has provided a structured vision of the progress   and most relevant research trends in LLM-based testing. In this article, we aim to provide a roadmap that illustrates its current state,   grouping the contributions into different categories, and also sketching the most promising and active research directions for the field.   To achieve this objective, we have conducted a semi-systematic literature review, collecting articles and mapping them into the most   prominent categories, reviewing the current and ongoing status, and analyzing the open challenges of LLM-based software testing.   Lastly, we have outlined several expected long-term impacts of LLMs over the whole software testing field. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¼€å§‹è¢«è§†ä¸ºè½¯ä»¶æµ‹è¯•é¢†åŸŸæœ€é‡è¦çš„æŠ€æœ¯çªç ´ä¹‹ä¸€ã€‚ç‰¹åˆ«æ˜¯ï¼Œå®ƒä»¬å·²æˆåŠŸåº”ç”¨äºè½¯ä»¶æµ‹è¯•ä»»åŠ¡ï¼Œå¦‚ç”Ÿæˆæµ‹è¯•ä»£ç æˆ–æ€»ç»“æ–‡æ¡£ã€‚è¿™ç§æ½œåŠ›å¸å¼•äº†æ•°ç™¾åç ”ç©¶äººå‘˜ï¼Œæ¯æœˆéƒ½æœ‰æ•°åé¡¹æ–°è´¡çŒ®ï¼Œæ¨åŠ¨ç ”ç©¶äººå‘˜ç«™åœ¨æµªæ½®çš„å‰æ²¿ã€‚ç„¶è€Œï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ²¡æœ‰å…ˆå‰çš„å·¥ä½œæä¾›è¿‡åŸºäºLLMçš„æµ‹è¯•çš„è¿›æ­¥å’Œç›¸å…³ç ”ç©¶è¶‹åŠ¿çš„ç»“æ„æ€§è§†è§’ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨æä¾›ä¸€æ¡è·¯çº¿å›¾ï¼Œä»¥è¯´æ˜å…¶å½“å‰çŠ¶æ€ï¼Œå°†è´¡çŒ®åˆ†ç»„æˆä¸åŒçš„ç±»åˆ«ï¼Œå¹¶æç»˜è¯¥é¢†åŸŸæœ€æœ‰å‰é€”å’Œæ´»è·ƒçš„ç ”ç©¶æ–¹å‘ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬è¿›è¡Œäº†åŠç³»ç»Ÿçš„æ–‡çŒ®ç»¼è¿°ï¼Œæ”¶é›†æ–‡ç« å¹¶å°†å…¶æ˜ å°„åˆ°æœ€çªå‡ºçš„ç±»åˆ«ä¸­ï¼Œå›é¡¾å½“å‰å’Œæ­£åœ¨è¿›è¡Œçš„çŠ¶æ€ï¼Œå¹¶åˆ†æåŸºäºLLMçš„è½¯ä»¶æµ‹è¯•æ‰€é¢ä¸´çš„å¼€æ”¾æŒ‘æˆ˜ã€‚æœ€åï¼Œæˆ‘ä»¬æ¦‚è¿°äº†LLMså¯¹æ•´ä¸ªè½¯ä»¶æµ‹è¯•é¢†åŸŸçš„å‡ ä¸ªé¢„æœŸé•¿æœŸå½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25043v1">PDF</a> 40 pages &amp; 10 figures Submitted on 29th September 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶æµ‹è¯•é¢†åŸŸå±•ç°å‡ºé‡å¤§çªç ´ï¼Œè¢«åº”ç”¨äºç”Ÿæˆæµ‹è¯•ä»£ç ã€æ€»ç»“æ–‡æ¡£ç­‰ä»»åŠ¡ã€‚æœ¬æ–‡æ—¨åœ¨æä¾›è¯¥é¢†åŸŸå½“å‰çŠ¶æ€çš„é“è·¯å›¾ï¼Œå°†è´¡çŒ®åˆ†ä¸ºä¸åŒç±»åˆ«ï¼Œå¹¶æ¦‚è¿°æœ€æ´»è·ƒå’Œæœ€æœ‰å¸Œæœ›çš„ç ”ç©¶æ–¹å‘ã€‚é€šè¿‡åŠç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°ï¼Œæœ¬æ–‡æ”¶é›†äº†æ–‡ç« å¹¶å°†å…¶æ˜ å°„åˆ°æœ€çªå‡ºçš„ç±»åˆ«ä¸­ï¼Œå›é¡¾å½“å‰å’Œæ­£åœ¨è¿›è¡Œçš„çŠ¶æ€ï¼Œå¹¶åˆ†æLLMåœ¨è½¯ä»¶æµ‹è¯•ä¸­çš„å¼€æ”¾æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æ¦‚è¿°äº†LLMå¯¹è½¯ä»¶æµ‹è¯•é¢†åŸŸçš„é•¿æœŸå½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶æµ‹è¯•é¢†åŸŸå…·æœ‰æ˜¾è‘—çš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>LLMå·²è¢«æˆåŠŸåº”ç”¨äºç”Ÿæˆæµ‹è¯•ä»£ç å’Œæ€»ç»“æ–‡æ¡£ç­‰ä»»åŠ¡ã€‚</li>
<li>ç›®å‰å°šæœªæœ‰ç ”ç©¶å·¥ä½œå…¨é¢æ¦‚è¿°LLMåœ¨è½¯ä»¶æµ‹è¯•ä¸­çš„è¿›å±•å’Œç›¸å…³ç ”ç©¶è¶‹åŠ¿ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡åŠç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°ï¼Œå¯¹LLMåœ¨è½¯ä»¶æµ‹è¯•ä¸­çš„ç ”ç©¶è¿›è¡Œäº†åˆ†ç±»å’Œå›é¡¾ã€‚</li>
<li>å½“å‰å’Œæ­£åœ¨è¿›è¡Œçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è§£å†³LLMåœ¨è½¯ä»¶æµ‹è¯•ä¸­çš„å¼€æ”¾æŒ‘æˆ˜ã€‚</li>
<li>LLMå¯¹è½¯ä»¶æµ‹è¯•é¢†åŸŸå…·æœ‰é•¿æœŸçš„é¢„æœŸå½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25043">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-002da74392d3137d43f230476b83477e" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GRACE-MoE-Grouping-and-Replication-with-Locality-Aware-Routing-for-Efficient-Distributed-MoE-Inference"><a href="#GRACE-MoE-Grouping-and-Replication-with-Locality-Aware-Routing-for-Efficient-Distributed-MoE-Inference" class="headerlink" title="GRACE-MoE: Grouping and Replication with Locality-Aware Routing for   Efficient Distributed MoE Inference"></a>GRACE-MoE: Grouping and Replication with Locality-Aware Routing for   Efficient Distributed MoE Inference</h2><p><strong>Authors:Yu Han, Lehan Pan, Jie Peng, Ziyang Tao, Wuyang Zhang, Yanyong Zhang</strong></p>
<p>Sparse Mixture of Experts (SMoE) performs conditional computation by selectively activating a subset of experts, thereby enabling scalable parameter growth in large language models (LLMs). However, the expanded parameter scale exceeds the memory capacity of a single device, necessitating distributed deployment for inference. This setup introduces two critical challenges: (1) Communication Issue: Transferring features to devices with activated experts leads to significant communication overhead. (2) Computational Load Issue: Skewed expert activation overloads certain GPUs, resulting in load imbalance across devices. Among these, communication overhead is identified as the main bottleneck in SMoE inference. Nevertheless, reducing communication between devices may exacerbate computational load imbalance, leading to device idleness and resource waste. Therefore, we present GRACE-MoE, short for Grouping and Replication with Locality-Aware Routing for SMoE inference. GRACE-MoE is a co-optimization framework that jointly reduces communication overhead and alleviates computational load imbalance. Specifically, the framework comprises two key phases: (1) Grouping &amp; Replication: This phase groups experts based on their affinity to reduce cross-device communication. Additionally, dynamic replication is applied to address load skew, improving computational load balance across GPUs. (2) Routing: This phase employs a locality-aware routing strategy with load prediction. It prioritizes local replicas to minimize communication overhead and balances requests across remote replicas when necessary. Experiments on diverse models and multi-node, multi-GPU environments demonstrate that GRACE-MoE efficiently reduces end-to-end inference latency, achieving up to 3.79x speedup over state-of-the-art systems. Code for GRACE-MoE will be released upon acceptance. </p>
<blockquote>
<p>Sparse Mixture of Expertsï¼ˆSMoEï¼‰é€šè¿‡æœ‰é€‰æ‹©åœ°æ¿€æ´»ä¸“å®¶å­é›†æ¥å®ç°æ¡ä»¶è®¡ç®—ï¼Œä»è€Œåœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å®ç°äº†å¯æ‰©å±•çš„å‚æ•°å¢é•¿ã€‚ç„¶è€Œï¼Œæ‰©å±•çš„å‚æ•°è§„æ¨¡è¶…å‡ºäº†å•ä¸ªè®¾å¤‡çš„å†…å­˜å®¹é‡ï¼Œéœ€è¦è¿›è¡Œåˆ†å¸ƒå¼éƒ¨ç½²ä»¥è¿›è¡Œæ¨ç†ã€‚è¿™ç§è®¾ç½®å¸¦æ¥äº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰é€šä¿¡é—®é¢˜ï¼šå°†ç‰¹å¾ä¼ è¾“åˆ°æœ‰æ¿€æ´»ä¸“å®¶çš„è®¾å¤‡ä¼šå¯¼è‡´é€šä¿¡å¼€é”€æ˜¾è‘—å¢åŠ ã€‚ï¼ˆ2ï¼‰è®¡ç®—è´Ÿè½½é—®é¢˜ï¼šä¸å‡è¡¡çš„ä¸“å®¶æ¿€æ´»ä¼šè¿‡è½½æŸäº›GPUï¼Œå¯¼è‡´è®¾å¤‡é—´è´Ÿè½½ä¸å‡è¡¡ã€‚å…¶ä¸­ï¼Œé€šä¿¡å¼€é”€è¢«è¯†åˆ«ä¸ºSMoEæ¨ç†ä¸­çš„ä¸»è¦ç“¶é¢ˆã€‚ç„¶è€Œï¼Œå‡å°‘è®¾å¤‡ä¹‹é—´çš„é€šä¿¡å¯èƒ½ä¼šåŠ å‰§è®¡ç®—è´Ÿè½½ä¸å¹³è¡¡ï¼Œå¯¼è‡´è®¾å¤‡é—²ç½®å’Œèµ„æºæµªè´¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†GRACE-MoEï¼Œå…¨ç§°åŸºäºå±€éƒ¨æ„ŸçŸ¥è·¯ç”±çš„SMoEæ¨ç†åˆ†ç»„ä¸å¤åˆ¶ç­–ç•¥ã€‚GRACE-MoEæ˜¯ä¸€ä¸ªååŒä¼˜åŒ–æ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶å‡å°‘é€šä¿¡å¼€é”€å¹¶ç¼“è§£è®¡ç®—è´Ÿè½½ä¸å¹³è¡¡ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰åˆ†ç»„ä¸å¤åˆ¶ï¼šæ­¤é˜¶æ®µæ ¹æ®ä¸“å®¶çš„äº²å’ŒåŠ›å¯¹ä¸“å®¶è¿›è¡Œåˆ†ç»„ï¼Œä»¥å‡å°‘è·¨è®¾å¤‡é€šä¿¡ã€‚æ­¤å¤–ï¼Œåº”ç”¨åŠ¨æ€å¤åˆ¶æ¥è§£å†³è´Ÿè½½åæ–œé—®é¢˜ï¼Œä»¥æé«˜GPUä¹‹é—´çš„è®¡ç®—è´Ÿè½½å¹³è¡¡ã€‚ï¼ˆ2ï¼‰è·¯ç”±ï¼šæ­¤é˜¶æ®µé‡‡ç”¨å…·æœ‰è´Ÿè½½é¢„æµ‹åŠŸèƒ½çš„å±€éƒ¨æ„ŸçŸ¥è·¯ç”±ç­–ç•¥ã€‚å®ƒä¼˜å…ˆæœ¬åœ°å‰¯æœ¬ä»¥æœ€å°åŒ–é€šä¿¡å¼€é”€ï¼Œå¹¶åœ¨å¿…è¦æ—¶å¹³è¡¡è¿œç¨‹å‰¯æœ¬çš„è¯·æ±‚ã€‚åœ¨å¤šç§æ¨¡å‹å’Œå¤šèŠ‚ç‚¹ã€å¤šGPUç¯å¢ƒä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGRACE-MoEæœ‰æ•ˆåœ°å‡å°‘äº†ç«¯åˆ°ç«¯æ¨ç†å»¶è¿Ÿï¼Œæ¯”æœ€æ–°ç³»ç»Ÿå®ç°äº†é«˜è¾¾3.79å€çš„é€Ÿåº¦æå‡ã€‚GRACE-MoEçš„ä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25041v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>Sparse Mixture of Experts (SMoE)é€šè¿‡é€‰æ‹©æ€§æ¿€æ´»ä¸“å®¶æ¥å®ç°æ¡ä»¶è®¡ç®—ï¼Œè¿™åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å®ç°äº†å¯æ‰©å±•çš„å‚æ•°å¢é•¿ã€‚ç„¶è€Œï¼Œéšç€å‚æ•°è§„æ¨¡çš„æ‰©å¤§ï¼Œè¶…å‡ºäº†å•ä¸ªè®¾å¤‡çš„å†…å­˜å®¹é‡ï¼Œéœ€è¦è¿›è¡Œåˆ†å¸ƒå¼éƒ¨ç½²ä»¥è¿›è¡Œæ¨ç†ã€‚è¿™å¸¦æ¥äº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä¸€æ˜¯é€šä¿¡é—®é¢˜ï¼Œå°†ç‰¹å¾è½¬ç§»åˆ°æ¿€æ´»çš„ä¸“å®¶è®¾å¤‡ä¸Šå¯¼è‡´äº†å·¨å¤§çš„é€šä¿¡å¼€é”€ï¼›äºŒæ˜¯è®¡ç®—è´Ÿè½½é—®é¢˜ï¼Œä¸“å®¶æ¿€æ´»ä¸å‡è¡¡å¯¼è‡´æŸäº›GPUè¿‡è½½ã€‚åœ¨è¿™å…¶ä¸­ï¼Œé€šä¿¡å¼€é”€è¢«è®¤ä¸ºæ˜¯SMoEæ¨ç†çš„ä¸»è¦ç“¶é¢ˆã€‚GRACE-MoEæ˜¯ä¸€ä¸ªååŒä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶å‡å°‘é€šä¿¡å¼€é”€å¹¶ç¼“è§£è®¡ç®—è´Ÿè½½ä¸å¹³è¡¡é—®é¢˜ã€‚å®ƒé€šè¿‡åˆ†ç»„å’Œå¤åˆ¶ä»¥åŠè·¯ç”±ç­–ç•¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚å®éªŒè¡¨æ˜ï¼ŒGRACE-MoEèƒ½æ˜¾è‘—æé«˜æ•ˆç‡ï¼Œé™ä½ç«¯åˆ°ç«¯æ¨ç†å»¶è¿Ÿï¼Œè¾ƒç°æœ‰ç³»ç»Ÿæœ€é«˜å¯å®ç°3.79å€åŠ é€Ÿã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SMoEé€šè¿‡é€‰æ‹©æ€§æ¿€æ´»ä¸“å®¶å®ç°æ¡ä»¶è®¡ç®—ï¼Œæ¨åŠ¨LLMä¸­çš„å‚æ•°å¢é•¿ã€‚</li>
<li>åˆ†å¸ƒå¼éƒ¨ç½²å¸¦æ¥é€šä¿¡å’Œè®¡ç®—è´Ÿè½½ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>é€šä¿¡å¼€é”€è¢«è®¤ä¸ºæ˜¯SMoEæ¨ç†çš„ä¸»è¦ç“¶é¢ˆã€‚</li>
<li>GRACE-MoEæ˜¯ä¸€ä¸ªååŒä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘é€šä¿¡å¼€é”€å¹¶ç¼“è§£è®¡ç®—è´Ÿè½½ä¸å¹³è¡¡ã€‚</li>
<li>GRACE-MoEé€šè¿‡åˆ†ç»„å’Œå¤åˆ¶ä»¥åŠè·¯ç”±ç­–ç•¥å®ç°ç›®æ ‡ã€‚</li>
<li>åˆ†ç»„ä¸å¤åˆ¶é˜¶æ®µåŸºäºä¸“å®¶äº²å’Œåº¦è¿›è¡Œåˆ†ç»„ï¼Œåº”ç”¨åŠ¨æ€å¤åˆ¶ä»¥è§£å†³è´Ÿè½½å€¾æ–œé—®é¢˜ã€‚</li>
<li>è·¯ç”±é˜¶æ®µé‡‡ç”¨å±€éƒ¨æ„ŸçŸ¥è·¯ç”±ç­–ç•¥ï¼Œä¼˜å…ˆæœ¬åœ°å‰¯æœ¬ä»¥æœ€å°åŒ–é€šä¿¡å¼€é”€ï¼Œå¹¶åœ¨å¿…è¦æ—¶å¹³è¡¡è¿œç¨‹è¯·æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25041">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60e3fa95e959a42155320f314a6c4cbd" align="middle">
<img src="https://picx.zhimg.com/v2-afb3d4321ed2ac4eed6beaa5d61d809c" align="middle">
<img src="https://picx.zhimg.com/v2-289c664fd0cc8899958ecfe3a8328e00" align="middle">
<img src="https://pic1.zhimg.com/v2-dcb7ca8bc9049f8194b65a649f4a315c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="VT-FSL-Bridging-Vision-and-Text-with-LLMs-for-Few-Shot-Learning"><a href="#VT-FSL-Bridging-Vision-and-Text-with-LLMs-for-Few-Shot-Learning" class="headerlink" title="VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning"></a>VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning</h2><p><strong>Authors:Wenhao Li, Qiangchang Wang, Xianjing Meng, Zhibin Wu, Yilong Yin</strong></p>
<p>Few-shot learning (FSL) aims to recognize novel concepts from only a few labeled support samples. Recent studies enhance support features by incorporating additional semantic information or designing complex semantic fusion modules. However, they still suffer from hallucinating semantics that contradict the visual evidence due to the lack of grounding in actual instances, resulting in noisy guidance and costly corrections. To address these issues, we propose a novel framework, bridging Vision and Text with LLMs for Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts conditioned on Large Language Models (LLMs) and support images, seamlessly integrating them through a geometry-aware alignment. It mainly consists of Cross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment (CGA). Specifically, the CIP conditions an LLM on both class names and support images to generate precise class descriptions iteratively in a single structured reasoning pass. These descriptions not only enrich the semantic understanding of novel classes but also enable the zero-shot synthesis of semantically consistent images. The descriptions and synthetic images act respectively as complementary textual and visual prompts, providing high-level class semantics and low-level intra-class diversity to compensate for limited support data. Furthermore, the CGA jointly aligns the fused textual, support, and synthetic visual representations by minimizing the kernelized volume of the 3-dimensional parallelotope they span. It captures global and nonlinear relationships among all representations, enabling structured and consistent multimodal integration. The proposed VT-FSL method establishes new state-of-the-art performance across ten diverse benchmarks, including standard, cross-domain, and fine-grained few-shot learning scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL">https://github.com/peacelwh/VT-FSL</a>. </p>
<blockquote>
<p>å°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰æ—¨åœ¨ä»ä»…æœ‰çš„å‡ ä¸ªæ ‡è®°æ”¯æŒæ ·æœ¬ä¸­è¯†åˆ«å‡ºæ–°æ¦‚å¿µã€‚æœ€è¿‘çš„ç ”ç©¶é€šè¿‡èå…¥é¢å¤–çš„è¯­ä¹‰ä¿¡æ¯æˆ–è®¾è®¡å¤æ‚çš„è¯­ä¹‰èåˆæ¨¡å—æ¥å¢å¼ºæ”¯æŒç‰¹å¾ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹åœ¨å®é™…å®ä¾‹ä¸­çš„åŸºç¡€ï¼Œå®ƒä»¬ä»ç„¶ä¼šå‡ºç°ä¸è§†è§‰è¯æ®ç›¸çŸ›ç›¾çš„å¹»è§‰è¯­ä¹‰ï¼Œå¯¼è‡´äº§ç”Ÿå˜ˆæ‚çš„æŒ‡å¯¼å’Œæ˜‚è´µçš„ä¿®æ­£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”è·¨è§†ç•Œä¸æ–‡æœ¬ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå°‘é‡å­¦ä¹ ï¼ˆVT-FSLï¼‰ï¼Œè¯¥æ¡†æ¶æ„å»ºç²¾ç¡®çš„è·¨æ¨¡æ€æç¤ºï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ”¯æŒå›¾åƒï¼Œé€šè¿‡å‡ ä½•æ„ŸçŸ¥å¯¹é½æ— ç¼é›†æˆå®ƒä»¬ã€‚å®ƒä¸»è¦ç”±è·¨æ¨¡æ€è¿­ä»£æç¤ºï¼ˆCIPï¼‰å’Œè·¨æ¨¡æ€å‡ ä½•å¯¹é½ï¼ˆCGAï¼‰ç»„æˆã€‚å…·ä½“æ¥è¯´ï¼ŒCIPæ ¹æ®ç±»åå’Œå›¾åƒæç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”Ÿæˆç²¾ç¡®ç±»æè¿°ï¼Œå¹¶åœ¨å•ä¸ªç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œè¿­ä»£ã€‚è¿™äº›æè¿°ä¸ä»…ä¸°å¯Œäº†å¯¹æ–°é¢–ç±»çš„è¯­ä¹‰ç†è§£ï¼Œè¿˜å®ç°äº†è¯­ä¹‰ä¸€è‡´å›¾åƒçš„é›¶æ ·æœ¬åˆæˆã€‚è¿™äº›æè¿°å’Œåˆæˆå›¾åƒåˆ†åˆ«ä½œä¸ºäº’è¡¥çš„æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œæä¾›é«˜çº§ç±»è¯­ä¹‰å’Œä½çº§ç±»å†…å¤šæ ·æ€§ï¼Œä»¥å¼¥è¡¥æœ‰é™çš„æ”¯æŒæ•°æ®ã€‚æ­¤å¤–ï¼ŒCGAé€šè¿‡æœ€å°åŒ–å…¶è·¨è¶Šçš„ä¸‰ç»´å¹³è¡Œå››è¾¹å½¢çš„å†…æ ¸ä½“ç§¯æ¥è”åˆå¯¹é½èåˆçš„æ–‡æœ¬ã€æ”¯æŒå’Œåˆæˆè§†è§‰è¡¨ç¤ºã€‚å®ƒæ•æ‰æ‰€æœ‰è¡¨ç¤ºä¹‹é—´çš„å…¨å±€å’Œéçº¿æ€§å…³ç³»ï¼Œå®ç°ç»“æ„åŒ–ä¸”ä¸€è‡´çš„å¤šæ¨¡å¼é›†æˆã€‚æå‡ºçš„VT-FSLæ–¹æ³•åœ¨åŒ…æ‹¬æ ‡å‡†ã€è·¨åŸŸå’Œç²¾ç»†ç²’åº¦å°‘é‡å­¦ä¹ åœºæ™¯åœ¨å†…çš„åä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€æ–°çš„å“è¶Šæ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/peacelwh/VT-FSLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25033v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå›¾åƒæ”¯æŒçš„è·¨æ¨¡æ€ç²¾ç¡®æç¤ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ä¸­ç”±äºç¼ºä¹å®é™…å®ä¾‹è€Œå¯¼è‡´çš„è¯­ä¹‰æ··æ·†é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è·¨æ¨¡æ€è¿­ä»£æç¤ºå’Œè·¨æ¨¡æ€å‡ ä½•å¯¹é½ï¼Œå°†LLMä¸è§†è§‰å’Œæ–‡æœ¬ç›¸ç»“åˆï¼Œç”Ÿæˆç²¾ç¡®çš„åˆ†ç±»æè¿°å’Œåˆæˆå›¾åƒï¼Œä»¥ä¸°å¯Œå¯¹æ–°å‹ç±»åˆ«çš„è¯­ä¹‰ç†è§£å¹¶è¡¥å¿æœ‰é™çš„æ”¯æŒæ•°æ®ã€‚è¯¥æ¡†æ¶åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬å­¦ä¹ æ—¨åœ¨ä»å°‘é‡æ ‡è®°æ ·æœ¬ä¸­è¯†åˆ«æ–°æ¦‚å¿µã€‚</li>
<li>ç°æœ‰ç ”ç©¶é€šè¿‡å¼•å…¥é¢å¤–è¯­ä¹‰ä¿¡æ¯æˆ–è®¾è®¡å¤æ‚è¯­ä¹‰èåˆæ¨¡å—æ¥å¢å¼ºæ”¯æŒç‰¹å¾ã€‚</li>
<li>ç¼ºä¹åŸºäºå®é™…å®ä¾‹çš„æ¥åœ°ä¼šå¯¼è‡´è¯­ä¹‰æ··æ·†å’Œå™ªå£°æŒ‡å¯¼ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è·¨æ¨¡æ€æ¡†æ¶VT-FSLï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ”¯æŒå›¾åƒè¿›è¡Œå°‘æ ·æœ¬å­¦ä¹ ã€‚</li>
<li>VT-FSLä¸»è¦åŒ…æ‹¬è·¨æ¨¡æ€è¿­ä»£æç¤ºå’Œè·¨æ¨¡æ€å‡ ä½•å¯¹é½ã€‚</li>
<li>è·¨æ¨¡æ€è¿­ä»£æç¤ºé€šè¿‡ç»“åˆç±»åå’Œå›¾åƒç”Ÿæˆç²¾ç¡®çš„åˆ†ç±»æè¿°ï¼Œå¹¶é€šè¿‡åˆæˆå›¾åƒå¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c6781d5aac0565d4a5da14d7fc5718e" align="middle">
<img src="https://picx.zhimg.com/v2-9af705fa082c84d4e881514f8fff464d" align="middle">
<img src="https://picx.zhimg.com/v2-fda6a8e6593952cbc22784b0fba62ed6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76bea1641940b6f14a37bc0429de3487" align="middle">
<img src="https://picx.zhimg.com/v2-1a09d3087398134cf568e19fcf367318" align="middle">
<img src="https://picx.zhimg.com/v2-5b1ec418aca878ee6ac090b3416189aa" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MARCOS-Deep-Thinking-by-Markov-Chain-of-Continuous-Thoughts"><a href="#MARCOS-Deep-Thinking-by-Markov-Chain-of-Continuous-Thoughts" class="headerlink" title="MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts"></a>MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts</h2><p><strong>Authors:Jiayu Liu, Zhenya Huang, Anya Sims, Enhong Chen, Yee Whye Teh, Ning Miao</strong></p>
<p>The current paradigm for reasoning in large language models (LLMs) involves models â€œthinking out loudâ€ via a sequence of tokens, known as chain-of-thought (CoT). This approach, while effective, has several significant drawbacks. Firstly, inference requires autoregressive generation of often thousands of CoT tokens, which is slow and computationally expensive. Secondly, it constrains reasoning to the discrete space of tokens, creating an information bottleneck across reasoning steps. Thirdly, it fundamentally entangles reasoning with token generation, forcing LLMs to â€œthink while speaking,â€ which causes potentially short-sighted reasoning. In light of these limitations, we re-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our approach, rather than autoregressively generating tokens, we model reasoning as a hidden Markov chain of continuous, high-dimensional â€œthoughtsâ€. Each reasoning step involves a transition of the internal thoughts, where explicit reasoning steps (which may consist of hundreds of tokens) serve as observable variables, which are windows to peek into the implicit thoughts. Since this latent process is incompatible with the standard supervised learning, we further propose a two-phase variational training scheme. Our experiments on three benchmarks demonstrate that MARCOS outperforms existing continuous reasoning methods and, for the first time, achieves performance comparable to token-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup in inference. Beyond this, MARCOS offers additional advantages, such as step-level instead of token-level control over randomness, opening significant opportunities for reinforcement learning and reasoning in LLMs. </p>
<blockquote>
<p>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èŒƒå¼æ˜¯é€šè¿‡ä¸€ç³»åˆ—æ ‡è®°ï¼ˆtokenï¼‰è¿›è¡Œæ¨¡å‹â€œå¤§å£°æ€è€ƒâ€ï¼Œè¿™è¢«ç§°ä¸ºæ€ç»´é“¾ï¼ˆCoTï¼‰ã€‚è™½ç„¶è¿™ç§æ–¹æ³•å¾ˆæœ‰æ•ˆï¼Œä½†å®ƒæœ‰å‡ ä¸ªé‡å¤§ç¼ºç‚¹ã€‚é¦–å…ˆï¼Œæ¨ç†éœ€è¦è‡ªå›å½’ç”Ÿæˆå¾€å¾€æ•°åƒä¸ªæ€ç»´é“¾æ ‡è®°ï¼Œè¿™æ—¢ç¼“æ…¢åˆè®¡ç®—æˆæœ¬é«˜ã€‚å…¶æ¬¡ï¼Œå®ƒå°†æ¨ç†é™åˆ¶åœ¨ç¦»æ•£æ ‡è®°ç©ºé—´ä¸­ï¼Œåœ¨æ¨ç†æ­¥éª¤ä¹‹é—´é€ æˆä¿¡æ¯ç“¶é¢ˆã€‚ç¬¬ä¸‰ï¼Œå®ƒä»æ ¹æœ¬ä¸Šå°†æ¨ç†ä¸æ ‡è®°ç”Ÿæˆçº ç¼ åœ¨ä¸€èµ·ï¼Œè¿«ä½¿LLMâ€œæ€è€ƒæ—¶è¯´è¯â€ï¼Œå¯¼è‡´æ½œåœ¨çš„çŸ­è§†æ¨ç†ã€‚é‰´äºè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬é‡æ–°æƒ³è±¡LLMä¸­çš„æ¨ç†å¹¶å‘ˆç°ä¸€ç§æ–°çš„èŒƒå¼ï¼šMARCOSã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä¸æ˜¯è‡ªå›å½’åœ°ç”Ÿæˆæ ‡è®°ï¼Œè€Œæ˜¯å°†æ¨ç†å»ºæ¨¡ä¸ºè¿ç»­çš„ã€é«˜ç»´åº¦çš„â€œæ€ç»´â€çš„éšé©¬å°”å¯å¤«é“¾ã€‚æ¯ä¸ªæ¨ç†æ­¥éª¤æ¶‰åŠå†…éƒ¨æ€ç»´çš„è¿‡æ¸¡ï¼Œå…¶ä¸­æ˜ç¡®çš„æ¨ç†æ­¥éª¤ï¼ˆå¯èƒ½åŒ…å«æ•°ç™¾ä¸ªæ ‡è®°ï¼‰ä½œä¸ºå¯è§‚å¯Ÿå˜é‡ï¼Œæ˜¯çª¥æ¢éšæ€§æ€ç»´çš„çª—å£ã€‚ç”±äºè¿™ç§æ½œåœ¨è¿‡ç¨‹ä¸æ ‡å‡†ç›‘ç£å­¦ä¹ ä¸å…¼å®¹ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„å˜åˆ†è®­ç»ƒæ–¹æ¡ˆã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMARCOSä¼˜äºç°æœ‰çš„è¿ç»­æ¨ç†æ–¹æ³•ï¼Œå¹¶ä¸”é¦–æ¬¡å®ç°äº†ä¸åŸºäºæ ‡è®°çš„CoTç›¸å½“çš„æ€§èƒ½ï¼Œç”šè‡³åœ¨GSM8Kä¸Šè¶…è¶Šäº†å®ƒ4.7%ï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦æé«˜äº†é«˜è¾¾15.7å€ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒMARCOSè¿˜æä¾›äº†é¢å¤–çš„ä¼˜åŠ¿ï¼Œå¦‚æ­¥éª¤çº§åˆ«çš„æ§åˆ¶ï¼Œè€Œä¸æ˜¯æ ‡è®°çº§åˆ«çš„æ§åˆ¶éšæœºæ€§ï¼Œè¿™ä¸ºå¼ºåŒ–å­¦ä¹ å’ŒLLMä¸­çš„æ¨ç†æ‰“å¼€äº†é‡è¦çš„æœºä¼šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25020v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ¨ç†æ–°èŒƒå¼MARCOSã€‚è¯¥èŒƒå¼å°†æ¨ç†è¿‡ç¨‹è§†ä¸ºä¸€ä¸ªéšè—çš„é©¬å°”å¯å¤«é“¾ä¸­çš„è¿ç»­é«˜ç»´â€œæ€ç»´â€ï¼Œè€Œéé€šè¿‡ç”Ÿæˆä¸€ç³»åˆ—æ ‡è®°ï¼ˆtokenï¼‰æ¥è¿›è¡Œã€‚MARCOSé€šè¿‡éšæ€§æ€ç»´è¿‡ç¨‹çš„è½¬æ¢å®ç°æ¨ç†ï¼Œå¹¶é€šè¿‡è§‚å¯Ÿåˆ°çš„æ ‡è®°ï¼ˆå¯èƒ½æ˜¯æ•°ç™¾ä¸ªæ ‡è®°ï¼‰æ¥è§‚å¯Ÿå†…éƒ¨æ€ç»´ã€‚å®éªŒè¡¨æ˜ï¼ŒMARCOSåœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä¸åŸºäºæ ‡è®°çš„æ¨ç†ç›¸å½“ç”šè‡³æ›´ä¼˜ï¼Œå¹¶å¤§å¤§åŠ å¿«äº†æ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼ŒMARCOSè¿˜æä¾›äº†é¢å¤–çš„ä¼˜åŠ¿ï¼Œå¦‚æ­¥éª¤çº§åˆ«çš„æ§åˆ¶ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ å’ŒLLMä¸­çš„æ¨ç†æä¾›äº†é‡å¤§æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å½“å‰æ¨ç†èŒƒå¼æ˜¯é€šè¿‡â€œæ€ç»´å¤–åŒ–â€çš„æ–¹å¼ï¼Œå³ç”Ÿæˆä¸€ç³»åˆ—æ ‡è®°ï¼ˆtokenï¼‰è¿›è¡Œæ¨ç†ï¼Œç§°ä¸ºé“¾å¼æ€ç»´ï¼ˆCoTï¼‰ã€‚</li>
<li>CoTèŒƒå¼å­˜åœ¨å¤šä¸ªæ˜¾è‘—ç¼ºç‚¹ï¼ŒåŒ…æ‹¬è®¡ç®—æˆæœ¬é«˜ã€çº¦æŸåœ¨ç¦»æ•£æ ‡è®°ç©ºé—´ã€ä»¥åŠå°†æ¨ç†ä¸æ ‡è®°ç”Ÿæˆç´§å¯†ç»“åˆã€‚</li>
<li>MARCOSæ˜¯ä¸€ç§æ–°å‹çš„LLMæ¨ç†èŒƒå¼ï¼Œå°†æ¨ç†è¿‡ç¨‹è§†ä¸ºä¸€ä¸ªéšè—çš„é©¬å°”å¯å¤«é“¾ä¸­çš„è¿ç»­é«˜ç»´æ€ç»´ã€‚</li>
<li>åœ¨MARCOSä¸­ï¼Œæ¨ç†æ­¥éª¤æ˜¯å†…éƒ¨æ€ç»´çš„è½¬æ¢ï¼Œè€Œè§‚å¯Ÿåˆ°çš„æ ‡è®°æ˜¯äº†è§£è¿™äº›æ€ç»´çš„çª—å£ã€‚</li>
<li>MARCOSé€šè¿‡ä¸¤é˜¶æ®µå˜åˆ†è®­ç»ƒæ–¹æ¡ˆå®ç°ï¼Œè¿™ä¸€æ–¹æ¡ˆè§£å†³äº†éšæ€§è¿‡ç¨‹ä¸æ ‡å‡†ç›‘ç£å­¦ä¹ çš„ä¸å…¼å®¹é—®é¢˜ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMARCOSåœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä¸CoTç›¸å½“ç”šè‡³æ›´ä¼˜ï¼ŒåŒæ—¶å¤§å¤§æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25020">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-448524a8e7e236e503585e0a6cb83e0e" align="middle">
<img src="https://picx.zhimg.com/v2-deda701f8ccd8c75f97a99491f97bea7" align="middle">
<img src="https://picx.zhimg.com/v2-0a3fcf825bf99729f4139207c99b7f28" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CLPO-Curriculum-Learning-meets-Policy-Optimization-for-LLM-Reasoning"><a href="#CLPO-Curriculum-Learning-meets-Policy-Optimization-for-LLM-Reasoning" class="headerlink" title="CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning"></a>CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning</h2><p><strong>Authors:Shijie Zhang, Guohao Sun, Kevin Zhang, Xiang Guo, Rujun Guo</strong></p>
<p>Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing methods typically treat all training samples uniformly, overlooking the vast differences in problem difficulty relative to the modelâ€™s current capabilities. This uniform training strategy leads to inefficient exploration of problems the model has already mastered, while concurrently lacking effective guidance on problems that are challenging its abilities the most, limiting both learning efficiency and upper-bound performance. To address this, we propose CLPO (Curriculum-guided Learning for Policy Optimization), a novel algorithm that creates a dynamic pedagogical feedback loop within the policy optimization process. The core of CLPO leverages the modelâ€™s own rollout performance to conduct real-time difficulty assessment, thereby constructing an Online Curriculum. This curriculum then guides an Adaptive Problem Restructuring mechanism, where the model acts as its own teacher: it diversifies medium-difficulty problems to promote generalization and simplifies challenging problems to make them more attainable. Our approach transforms the static training procedure into a dynamic process that co-evolves with the modelâ€™s capabilities. Experiments show that CLPO achieves state-of-the-art performance across eight challenging mathematical and general reasoning benchmarks, with an average pass@1 improvement of 6.96% over other methods, demonstrating its potential for more efficiently training more capable reasoning models. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æˆä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸»è¦èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å¯¹æ‰€æœ‰è®­ç»ƒæ ·æœ¬è¿›è¡Œç»Ÿä¸€å¤„ç†ï¼Œå¿½ç•¥äº†é—®é¢˜éš¾åº¦ä¸æ¨¡å‹å½“å‰èƒ½åŠ›ä¹‹é—´çš„å·¨å¤§å·®å¼‚ã€‚è¿™ç§ç»Ÿä¸€è®­ç»ƒç­–ç•¥å¯¼è‡´æ¨¡å‹å¯¹å·²æŒæ¡çš„é—®é¢˜è¿›è¡Œä½æ•ˆæ¢ç´¢ï¼ŒåŒæ—¶ç¼ºä¹é’ˆå¯¹æœ€å…·æŒ‘æˆ˜æ€§é—®é¢˜çš„æœ‰æ•ˆæŒ‡å¯¼ï¼Œä»è€Œé™åˆ¶äº†å­¦ä¹ æ•ˆç‡å’Œæœ€é«˜æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CLPOï¼ˆç”¨äºç­–ç•¥ä¼˜åŒ–çš„è¯¾ç¨‹æŒ‡å¯¼å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç®—æ³•ï¼Œå¯åœ¨ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ä¸­åˆ›å»ºåŠ¨æ€çš„æ•™å­¦åé¦ˆå¾ªç¯ã€‚CLPOçš„æ ¸å¿ƒæ˜¯åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„æ»šåŠ¨æ€§èƒ½è¿›è¡Œå®æ—¶éš¾åº¦è¯„ä¼°ï¼Œä»è€Œæ„å»ºåœ¨çº¿è¯¾ç¨‹ã€‚ç„¶åï¼Œæ­¤è¯¾ç¨‹å¼•å¯¼è‡ªé€‚åº”é—®é¢˜é‡å»ºæœºåˆ¶ï¼Œæ¨¡å‹åœ¨å…¶ä¸­æ‰®æ¼”è‡ªå·±çš„è€å¸ˆï¼šå®ƒé€šè¿‡å¤šæ ·åŒ–ä¸­ç­‰éš¾åº¦é—®é¢˜æ¥ä¿ƒè¿›æ¨å¹¿ï¼Œå¹¶ç®€åŒ–æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ä½¿å®ƒä»¬æ›´å®¹æ˜“è§£å†³ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†é™æ€è®­ç»ƒç¨‹åºè½¬å˜ä¸ºä¸€ä¸ªä¸æ¨¡å‹èƒ½åŠ›å…±åŒå‘å±•çš„åŠ¨æ€è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒCLPOåœ¨å…«ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦å’Œé€šç”¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œå¹³å‡é€šè¿‡ç‡æé«˜äº†6.96%ï¼Œè¯æ˜äº†å…¶åœ¨æ›´é«˜æ•ˆè®­ç»ƒæ›´å¼ºå¤§çš„æ¨ç†æ¨¡å‹æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25004v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„å…³é”®èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å¯¹æ‰€æœ‰è®­ç»ƒæ ·æœ¬ä¸€è§†åŒä»ï¼Œå¿½ç•¥äº†é—®é¢˜éš¾åº¦ä¸æ¨¡å‹å½“å‰èƒ½åŠ›ä¹‹é—´çš„å·¨å¤§å·®å¼‚ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CLPOï¼ˆç”¨äºç­–ç•¥ä¼˜åŒ–çš„è¯¾ç¨‹å¼•å¯¼å­¦ä¹ ï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ä¸­åˆ›å»ºäº†ä¸€ä¸ªåŠ¨æ€çš„åé¦ˆå¾ªç¯ã€‚CLPOçš„æ ¸å¿ƒæ˜¯åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„æ»šåŠ¨æ€§èƒ½è¿›è¡Œå®æ—¶éš¾åº¦è¯„ä¼°ï¼Œä»è€Œæ„å»ºåœ¨çº¿è¯¾ç¨‹ã€‚è¿™é—¨è¯¾ç¨‹ç„¶åå¼•å¯¼è‡ªé€‚åº”é—®é¢˜é‡æ„æœºåˆ¶ï¼Œæ¨¡å‹å……å½“è‡ªå·±çš„è€å¸ˆï¼šå®ƒä½¿ä¸­ç­‰éš¾åº¦çš„é—®é¢˜å¤šæ ·åŒ–ä»¥ä¿ƒè¿›æ¨å¹¿ï¼Œå¹¶ç®€åŒ–å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ä½¿å…¶æ›´å®¹æ˜“è·å¾—ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†é™æ€è®­ç»ƒè¿‡ç¨‹è½¬å˜ä¸ºä¸æ¨¡å‹èƒ½åŠ›å…±åŒæ¼”åŒ–çš„åŠ¨æ€è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒCLPOåœ¨å…«ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦å’Œä¸€èˆ¬æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æ€§èƒ½ï¼Œåœ¨å…¶ä»–æ–¹æ³•çš„åŸºç¡€ä¸Šå¹³å‡æé«˜äº†6.96%çš„é€šè¿‡ç‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è®­ç»ƒæ›´å…·èƒ½åŠ›çš„æ¨ç†æ¨¡å‹æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å¯¹äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ˜¯å…³é”®èŒƒå¼ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥é—®é¢˜éš¾åº¦ä¸æ¨¡å‹å½“å‰èƒ½åŠ›çš„å·®å¼‚ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚</li>
<li>CLPOç®—æ³•åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„æ»šåŠ¨æ€§èƒ½è¿›è¡Œå®æ—¶éš¾åº¦è¯„ä¼°ï¼Œæ„å»ºåœ¨çº¿è¯¾ç¨‹ã€‚</li>
<li>CLPOå¼•å¯¼è‡ªé€‚åº”é—®é¢˜é‡æ„æœºåˆ¶ï¼Œæ¨¡å‹å……å½“è‡ªå·±çš„è€å¸ˆï¼Œä¿ƒè¿›é—®é¢˜çš„å¤šæ ·åŒ–å¹¶ç®€åŒ–æŒ‘æˆ˜æ€§ä»»åŠ¡ã€‚</li>
<li>CLPOå°†é™æ€è®­ç»ƒè¿‡ç¨‹è½¬å˜ä¸ºåŠ¨æ€è¿‡ç¨‹ï¼Œä¸æ¨¡å‹èƒ½åŠ›å…±åŒæ¼”åŒ–ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºCLPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€æ–°æ€§èƒ½ï¼Œå¹³å‡é€šè¿‡ç‡æœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4659e4df212cf30b43803d12afab28dc" align="middle">
<img src="https://picx.zhimg.com/v2-ab1f96f643f70b2661f0dda92f89758d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Attention-Surgery-An-Efficient-Recipe-to-Linearize-Your-Video-Diffusion-Transformer"><a href="#Attention-Surgery-An-Efficient-Recipe-to-Linearize-Your-Video-Diffusion-Transformer" class="headerlink" title="Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion   Transformer"></a>Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion   Transformer</h2><p><strong>Authors:Mohsen Ghafoorian, Denis Korzhenkov, Amirhossein Habibian</strong></p>
<p>Transformer-based video diffusion models (VDMs) deliver state-of-the-art video generation quality but are constrained by the quadratic cost of self-attention, making long sequences and high resolutions computationally expensive. While linear attention offers sub-quadratic complexity, prior attempts fail to match the expressiveness of softmax attention without costly retraining. We introduce \textit{Attention Surgery}, an efficient framework for \textit{linearizing} or \textit{hybridizing} attention in pretrained VDMs without training from scratch. Inspired by recent advances in language models, our method combines a novel hybrid attention mechanism-mixing softmax and linear tokens-with a lightweight distillation and fine-tuning pipeline requiring only a few GPU-days. Additionally, we incorporate a cost-aware block-rate strategy to balance expressiveness and efficiency across layers. Applied to Wan2.1 1.3B, a state-of-the-art DiT-based VDM, Attention Surgery achieves the first competitive sub-quadratic attention video diffusion models, reducing attention cost by up to 40% in terms of FLOPs, while maintaining generation quality as measured on the standard VBench and VBench-2.0 benchmarks. </p>
<blockquote>
<p>åŸºäºTransformerçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰æä¾›äº†æœ€å…ˆè¿›çš„è§†é¢‘ç”Ÿæˆè´¨é‡ï¼Œä½†ç”±äºè‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡æˆæœ¬è€Œå—åˆ°é™åˆ¶ï¼Œä½¿å¾—é•¿åºåˆ—å’Œé«˜åˆ†è¾¨ç‡çš„è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚è™½ç„¶çº¿æ€§æ³¨æ„åŠ›æä¾›äº†æ¬¡äºŒæ¬¡å¤æ‚æ€§ï¼Œä½†ä¹‹å‰çš„å°è¯•åœ¨æ²¡æœ‰æ˜‚è´µé‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæ— æ³•åŒ¹é…softmaxæ³¨æ„åŠ›çš„è¡¨ç°åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†â€æ³¨æ„åŠ›æ‰‹æœ¯â€ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ¡†æ¶ï¼Œå¯ä»¥åœ¨é¢„è®­ç»ƒçš„VDMä¸­å¯¹æ³¨æ„åŠ›è¿›è¡Œâ€çº¿æ€§åŒ–â€æˆ–â€æ··åˆåŒ–â€ï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹è®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†æœ€è¿‘è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ··åˆæ³¨æ„åŠ›æœºåˆ¶â€”â€”æ··åˆsoftmaxå’Œçº¿æ€§ä»¤ç‰Œï¼Œä»¥åŠä¸€ä¸ªè½»é‡çº§çš„è’¸é¦å’Œå¾®è°ƒç®¡é“ï¼Œåªéœ€å‡ å¤©çš„GPUæ—¶é—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†ä¸€ç§æˆæœ¬æ„ŸçŸ¥çš„å—ç‡ç­–ç•¥ï¼Œä»¥åœ¨å±‚ä¹‹é—´å¹³è¡¡è¡¨ç°åŠ›å’Œæ•ˆç‡ã€‚å°†è¯¥æ–¹æ³•åº”ç”¨äºWan2.1 .DiTè§†é¢‘æ‰©æ•£æ¨¡å‹çš„å‰ç»æŠ€æœ¯åæ˜¾ç¤ºï¼Œâ€œæ³¨æ„åŠ›æ‰‹æœ¯â€åœ¨ä¿æŒäº†ç±»ä¼¼äºVBenchå’ŒVBench-2.0æ ‡å‡†çš„åŸºå‡†ç”Ÿæˆè´¨é‡çš„æƒ…å†µä¸‹æˆåŠŸæ¨å‡ºç¬¬ä¸€æ‰¹å…·æœ‰ç«äº‰åŠ›çš„æ¬¡äºŒæ¬¡æ³¨æ„åŠ›è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»æµ®ç‚¹è¿ç®—é‡æ¥çœ‹ï¼Œæ³¨æ„åŠ›æˆæœ¬é™ä½äº†é«˜è¾¾ç™¾åˆ†ä¹‹å››åã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24899v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹åŸºäºTransformerçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰çš„é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶æ”¹è¿›æ–¹æ¡ˆã€‚ç”±äºç°æœ‰VDMsé¢ä¸´è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡æˆæœ¬é—®é¢˜ï¼Œå¯¼è‡´é•¿åºåˆ—å’Œé«˜åˆ†è¾¨ç‡çš„è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡æå‡ºäº†â€œæ³¨æ„åŠ›æ‰‹æœ¯â€æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å¯¹é¢„è®­ç»ƒVDMä¸­çš„æ³¨æ„åŠ›è¿›è¡Œçº¿æ€§åŒ–æˆ–æ··åˆåŒ–æ”¹è¿›ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ–°å‹çš„æ··åˆæ³¨æ„åŠ›æœºåˆ¶å’Œè½»é‡çº§è’¸é¦ä¸å¾®è°ƒç®¡é“ï¼Œä»…éœ€å‡ å¤©çš„GPUæ—¶é—´ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥æˆæœ¬æ„ŸçŸ¥çš„å—ç‡ç­–ç•¥ï¼Œå¹³è¡¡äº†å„å±‚çš„è¡¨è¾¾æ€§å’Œæ•ˆç‡ã€‚åº”ç”¨äºWan2.1 1.3Bè¿™ä¸€æœ€å…ˆè¿›çš„åŸºäºDiTçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæ³¨æ„åŠ›æ‰‹æœ¯å®ç°äº†é¦–ä¸ªå…·æœ‰ç«äº‰åŠ›çš„æ¬¡äºŒæ¬¡æ³¨æ„åŠ›è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œåœ¨FLOPsæ–¹é¢å‡å°‘äº†é«˜è¾¾40%çš„æ³¨æ„åŠ›æˆæœ¬ï¼ŒåŒæ—¶åœ¨VBenchå’ŒVBench-2.0æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ç»´æŒäº†ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer-basedè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰ç”±äºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡æˆæœ¬é¢ä¸´è®¡ç®—æ•ˆç‡é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„â€œæ³¨æ„åŠ›æ‰‹æœ¯â€æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„é¢„è®­ç»ƒVDMä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°çº¿æ€§åŒ–æˆ–æ··åˆåŒ–ã€‚</li>
<li>ç»“åˆäº†æ–°å‹çš„æ··åˆæ³¨æ„åŠ›æœºåˆ¶å’Œè½»é‡çº§è’¸é¦ä¸å¾®è°ƒç®¡é“ï¼Œå‡å°‘äº†è®¡ç®—æˆæœ¬ã€‚</li>
<li>é€šè¿‡å¼•å…¥æˆæœ¬æ„ŸçŸ¥çš„å—ç‡ç­–ç•¥ï¼Œä¼˜åŒ–äº†è¡¨è¾¾æ€§å’Œæ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>æˆåŠŸåº”ç”¨äºWan2.1 1.3Bæ¨¡å‹ï¼Œå®ç°äº†é¦–ä¸ªå…·æœ‰ç«äº‰åŠ›çš„æ¬¡äºŒæ¬¡æ³¨æ„åŠ›è§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>åœ¨FLOPsæ–¹é¢å‡å°‘äº†é«˜è¾¾40%çš„æ³¨æ„åŠ›æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24899">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ebb3278071bfe2ff6d1c9bd3412d96c3" align="middle">
<img src="https://picx.zhimg.com/v2-6f8177d43614092147559b396ecaad10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bec84dd02159870e233cb60cd3d8d87b" align="middle">
<img src="https://pica.zhimg.com/v2-963569a31187361476fd2db2b5a81b4a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="StreamForest-Efficient-Online-Video-Understanding-with-Persistent-Event-Memory"><a href="#StreamForest-Efficient-Online-Video-Understanding-with-Persistent-Event-Memory" class="headerlink" title="StreamForest: Efficient Online Video Understanding with Persistent Event   Memory"></a>StreamForest: Efficient Online Video Understanding with Persistent Event   Memory</h2><p><strong>Authors:Xiangyu Zeng, Kefan Qiu, Qingyu Zhang, Xinhao Li, Jing Wang, Jiaxin Li, Ziang Yan, Kun Tian, Meng Tian, Xinhai Zhao, Yi Wang, Limin Wang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have recently achieved remarkable progress in video understanding. However, their effectiveness in real-time streaming scenarios remains limited due to storage constraints of historical visual features and insufficient real-time spatiotemporal reasoning. To address these challenges, we propose StreamForest, a novel architecture specifically designed for streaming video understanding. Central to StreamForest is the Persistent Event Memory Forest, a memory mechanism that adaptively organizes video frames into multiple event-level tree structures. This process is guided by penalty functions based on temporal distance, content similarity, and merge frequency, enabling efficient long-term memory retention under limited computational resources. To enhance real-time perception, we introduce a Fine-grained Spatiotemporal Window, which captures detailed short-term visual cues to improve current scene perception. Additionally, we present OnlineIT, an instruction-tuning dataset tailored for streaming video tasks. OnlineIT significantly boosts MLLM performance in both real-time perception and future prediction. To evaluate generalization in practical applications, we introduce ODV-Bench, a new benchmark focused on real-time streaming video understanding in autonomous driving scenarios. Experimental results demonstrate that StreamForest achieves the state-of-the-art performance, with accuracies of 77.3% on StreamingBench, 60.5% on OVBench, and 55.6% on OVO-Bench. In particular, even under extreme visual token compression (limited to 1024 tokens), the model retains 96.8% of its average accuracy in eight benchmarks relative to the default setting. These results underscore the robustness, efficiency, and generalizability of StreamForest for streaming video understanding. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç”±äºå…¶å†å²è§†è§‰ç‰¹å¾çš„å­˜å‚¨é™åˆ¶å’Œå®æ—¶æ—¶ç©ºæ¨ç†çš„ä¸è¶³ï¼Œå®ƒä»¬åœ¨å®æ—¶æµåª’ä½“åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸“é—¨ç”¨äºæµåª’ä½“è§†é¢‘ç†è§£çš„å…¨æ–°æ¶æ„StreamForestã€‚StreamForestçš„æ ¸å¿ƒæ˜¯æŒä¹…äº‹ä»¶è®°å¿†æ£®æ—ï¼ˆPersistent Event Memory Forestï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è®°å¿†æœºåˆ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°å°†è§†é¢‘å¸§ç»„ç»‡æˆå¤šä¸ªäº‹ä»¶çº§çš„æ ‘ç»“æ„ã€‚è¿™ä¸€è¿‡ç¨‹ç”±åŸºäºæ—¶é—´è·ç¦»ã€å†…å®¹ç›¸ä¼¼æ€§å’Œåˆå¹¶é¢‘ç‡çš„æƒ©ç½šå‡½æ•°å¼•å¯¼ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹å®ç°é«˜æ•ˆçš„é•¿æœŸè®°å¿†ä¿ç•™ã€‚ä¸ºäº†æé«˜å®æ—¶æ„ŸçŸ¥èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»†ç²’åº¦æ—¶ç©ºçª—å£ï¼ˆFine-grained Spatiotemporal Windowï¼‰ï¼Œå¯ä»¥æ•æ‰çŸ­æœŸçš„è¯¦ç»†è§†è§‰çº¿ç´¢ï¼Œä»¥æé«˜å¯¹å½“å‰åœºæ™¯çš„è®¤çŸ¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†é€‚ç”¨äºæµåª’ä½“è§†é¢‘ä»»åŠ¡çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†OnlineITã€‚OnlineITæ˜¾è‘—æå‡äº†MLLMåœ¨å®æ—¶æ„ŸçŸ¥å’Œæœªæ¥é¢„æµ‹æ–¹é¢çš„æ€§èƒ½ã€‚ä¸ºäº†è¯„ä¼°åœ¨å®é™…åº”ç”¨ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ODV-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­å®æ—¶æµåª’ä½“è§†é¢‘ç†è§£çš„æ–°åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStreamForestè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨StreamingBenchä¸Šè¾¾åˆ°äº†77.3%çš„å‡†ç¡®ç‡ï¼Œåœ¨OVBenchä¸Šè¾¾åˆ°äº†60.5%çš„å‡†ç¡®ç‡ï¼Œåœ¨OVO-Benchä¸Šè¾¾åˆ°äº†55.6%çš„å‡†ç¡®ç‡ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå³ä½¿åœ¨æç«¯è§†è§‰ä»¤ç‰Œå‹ç¼©ï¼ˆé™åˆ¶ä¸º1024ä¸ªä»¤ç‰Œï¼‰çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¹³å‡å‡†ç¡®ç‡ä»èƒ½ä¿æŒ96.8%ï¼Œç›¸å¯¹äºé»˜è®¤è®¾ç½®ã€‚è¿™äº›ç»“æœè¯æ˜äº†StreamForeståœ¨æµåª’ä½“è§†é¢‘ç†è§£æ–¹é¢çš„ç¨³å¥æ€§ã€æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24871v1">PDF</a> Accepted as a Spotlight at NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å®æ—¶æµåª’ä½“åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶æœ‰é™ï¼Œä¸»è¦æ˜¯ç”±äºå†å²è§†è§‰ç‰¹å¾çš„å­˜å‚¨çº¦æŸå’Œå®æ—¶æ—¶ç©ºæ¨ç†ä¸è¶³ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†StreamForestï¼Œä¸€ç§ä¸“ä¸ºæµåª’ä½“è§†é¢‘ç†è§£è®¾è®¡çš„æ–°å‹æ¶æ„ã€‚å…¶æ ¸å¿ƒæ˜¯æŒä¹…äº‹ä»¶è®°å¿†æ£®æ—ï¼Œä¸€ç§è®°å¿†æœºåˆ¶ï¼Œå¯è‡ªé€‚åº”åœ°å°†è§†é¢‘å¸§ç»„ç»‡æˆå¤šä¸ªäº‹ä»¶çº§æ ‘ç»“æ„ã€‚è¿™ä¸€è¿‡ç¨‹å—åˆ°åŸºäºæ—¶é—´è·ç¦»ã€å†…å®¹ç›¸ä¼¼æ€§å’Œåˆå¹¶é¢‘ç‡çš„æƒ©ç½šå‡½æ•°çš„æŒ‡å¯¼ï¼Œå¯åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹å®ç°é«˜æ•ˆçš„é•¿ç¨‹è®°å¿†ä¿ç•™ã€‚ä¸ºæé«˜å®æ—¶æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¼•å…¥äº†ç»†ç²’åº¦æ—¶ç©ºçª—å£ï¼Œå¯æ•æ‰çŸ­æœŸè§†è§‰çº¿ç´¢ï¼Œæé«˜å½“å‰åœºæ™¯æ„ŸçŸ¥ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†OnlineITï¼Œä¸€ä¸ªé’ˆå¯¹æµåª’ä½“è§†é¢‘ä»»åŠ¡é‡èº«å®šåˆ¶çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚OnlineITæ˜¾è‘—æé«˜äº†MLLMåœ¨å®æ—¶æ„ŸçŸ¥å’Œæœªæ¥é¢„æµ‹æ–¹é¢çš„æ€§èƒ½ã€‚ä¸ºè¯„ä¼°åœ¨å®é™…åº”ç”¨ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¼•å…¥äº†ODV-Benchï¼Œä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­çš„å®æ—¶æµåª’ä½“è§†é¢‘ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStreamForestè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒStreamingBenchä¸Šçš„å‡†ç¡®ç‡ä¸º77.3%ï¼ŒOVBenchä¸Šä¸º60.5%ï¼ŒOVO-Benchä¸Šä¸º55.6%ã€‚å³ä½¿åœ¨æç«¯è§†è§‰ä»¤ç‰Œå‹ç¼©ï¼ˆé™åˆ¶ä¸º1024ä¸ªä»¤ç‰Œï¼‰ä¸‹ï¼Œæ¨¡å‹åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¹³å‡å‡†ç¡®ç‡ä»ä¿æŒåœ¨é»˜è®¤è®¾ç½®çš„96.8%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†StreamForeståœ¨æµåª’ä½“è§†é¢‘ç†è§£æ–¹é¢çš„ç¨³å¥æ€§ã€æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å®æ—¶æµåª’ä½“åœºæ™¯ä¸­é¢ä¸´å­˜å‚¨å’Œæ¨ç†æŒ‘æˆ˜ã€‚</li>
<li>StreamForestæ¶æ„é€šè¿‡æŒä¹…äº‹ä»¶è®°å¿†æ£®æ—å’Œç»†ç²’åº¦æ—¶ç©ºçª—å£è§£å†³äº†è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>æŒä¹…äº‹ä»¶è®°å¿†æ£®æ—é€šè¿‡è‡ªé€‚åº”ç»„ç»‡è§†é¢‘å¸§æˆäº‹ä»¶çº§æ ‘ç»“æ„ï¼Œå®ç°äº†é«˜æ•ˆçš„é•¿ç¨‹è®°å¿†ä¿ç•™ã€‚</li>
<li>OnlineITæ•°æ®é›†çš„å¼•å…¥æ˜¾è‘—æå‡äº†MLLMåœ¨å®æ—¶æ„ŸçŸ¥å’Œæœªæ¥é¢„æµ‹æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>ODV-BenchåŸºå‡†æµ‹è¯•çš„å¼•å…¥ï¼Œä¸ºè¯„ä¼°å®æ—¶æµåª’ä½“è§†é¢‘ç†è§£çš„æ³›åŒ–èƒ½åŠ›æä¾›äº†æ–°æ ‡å‡†ã€‚</li>
<li>StreamForeståœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒåŒ…æ‹¬StreamingBenchã€OVBenchå’ŒOVO-Benchã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb53fef302d65d603a02ada895667799" align="middle">
<img src="https://picx.zhimg.com/v2-aaa22b2321dd5ecddd6a1c31fbb32ac0" align="middle">
<img src="https://pic1.zhimg.com/v2-63b3041af357c93c471a88f3ab2c5360.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65d4d10f40cd27e1c87e514eb76d895a" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Diamonds-in-the-rough-Transforming-SPARCs-of-imagination-into-a-game-concept-by-leveraging-medium-sized-LLMs"><a href="#Diamonds-in-the-rough-Transforming-SPARCs-of-imagination-into-a-game-concept-by-leveraging-medium-sized-LLMs" class="headerlink" title="Diamonds in the rough: Transforming SPARCs of imagination into a game   concept by leveraging medium sized LLMs"></a>Diamonds in the rough: Transforming SPARCs of imagination into a game   concept by leveraging medium sized LLMs</h2><p><strong>Authors:Julian Geheeb, Farhan Abid Ivan, Daniel Dyrda, Miriam AnschÃ¼tz, Georg Groh</strong></p>
<p>Recent research has demonstrated that large language models (LLMs) can support experts across various domains, including game design. In this study, we examine the utility of medium-sized LLMs, models that operate on consumer-grade hardware typically available in small studios or home environments. We began by identifying ten key aspects that contribute to a strong game concept and used ChatGPT to generate thirty sample game ideas. Three medium-sized LLMs, LLaMA 3.1, Qwen 2.5, and DeepSeek-R1, were then prompted to evaluate these ideas according to the previously identified aspects. A qualitative assessment by two researchers compared the modelsâ€™ outputs, revealing that DeepSeek-R1 produced the most consistently useful feedback, despite some variability in quality. To explore real-world applicability, we ran a pilot study with ten students enrolled in a storytelling course for game development. At the early stages of their own projects, students used our prompt and DeepSeek-R1 to refine their game concepts. The results indicate a positive reception: most participants rated the output as high quality and expressed interest in using such tools in their workflows. These findings suggest that current medium-sized LLMs can provide valuable feedback in early game design, though further refinement of prompting methods could improve consistency and overall effectiveness. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥æ”¯æŒåŒ…æ‹¬æ¸¸æˆè®¾è®¡åœ¨å†…çš„å„ä¸ªé¢†åŸŸçš„ä¸“å®¶ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸­å‹LLMçš„å®ç”¨æ€§ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸åœ¨å°å‹å·¥ä½œå®¤æˆ–å®¶åº­ç¯å¢ƒä¸­å¯ç”¨çš„æ¶ˆè´¹è€…çº§ç¡¬ä»¶ä¸Šè¿è¡Œã€‚æˆ‘ä»¬é¦–å…ˆç¡®å®šäº†æ„æˆå¼ºå¤§æ¸¸æˆæ¦‚å¿µçš„åä¸ªå…³é”®æ–¹é¢ï¼Œå¹¶ä½¿ç”¨ChatGPTç”Ÿæˆäº†ä¸‰åä¸ªæ¸¸æˆæƒ³æ³•æ ·æœ¬ã€‚ç„¶åï¼Œæˆ‘ä»¬æç¤ºäº†ä¸‰ä¸ªä¸­å‹LLMï¼Œå³LLaMA 3.1ã€Qwen 2.5å’ŒDeepSeek-R1ï¼Œæ ¹æ®å…ˆå‰ç¡®å®šçš„å…³é”®æ–¹é¢å¯¹è¿™äº›æƒ³æ³•è¿›è¡Œè¯„ä¼°ã€‚ä¸¤ä½ç ”ç©¶äººå‘˜è¿›è¡Œçš„å®šæ€§è¯„ä¼°æ¯”è¾ƒäº†è¿™äº›æ¨¡å‹çš„è¾“å‡ºï¼Œç»“æœè¡¨æ˜ï¼Œå°½ç®¡è´¨é‡ä¸Šå­˜åœ¨ä¸€äº›å·®å¼‚ï¼Œä½†DeepSeek-R1æä¾›çš„åé¦ˆå§‹ç»ˆæ˜¯æœ€æœ‰ç”¨çš„ã€‚ä¸ºäº†æ¢ç´¢ç°å®ä¸–ç•Œçš„é€‚ç”¨æ€§ï¼Œæˆ‘ä»¬å¯¹ååå‚åŠ æ¸¸æˆå¼€å‘å™äº‹è¯¾ç¨‹çš„å­¦ç”Ÿè¿›è¡Œäº†è¯•ç‚¹ç ”ç©¶ã€‚åœ¨å„è‡ªé¡¹ç›®çš„æ—©æœŸé˜¶æ®µï¼Œå­¦ç”Ÿä»¬ä½¿ç”¨æˆ‘ä»¬çš„æç¤ºå’ŒDeepSeek-R1æ¥å®Œå–„ä»–ä»¬çš„æ¸¸æˆæ¦‚å¿µã€‚ç»“æœè¡¨æ˜åé¦ˆç§¯æï¼šå¤§å¤šæ•°å‚ä¸è€…è®¤ä¸ºè¾“å‡ºè´¨é‡å¾ˆé«˜ï¼Œå¹¶è¡¨ç¤ºæœ‰å…´è¶£å°†è¿™ç§å·¥å…·çº³å…¥ä»–ä»¬çš„å·¥ä½œæµç¨‹ä¸­ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå½“å‰çš„ä¸­å‹LLMå¯ä»¥åœ¨æ—©æœŸæ¸¸æˆè®¾è®¡ä¸­æä¾›æœ‰ä»·å€¼çš„åé¦ˆï¼Œä¸è¿‡å¯¹æç¤ºæ–¹æ³•çš„è¿›ä¸€æ­¥æ”¹è¿›å¯ä»¥æé«˜ä¸€è‡´æ€§å’Œæ€»ä½“æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24730v1">PDF</a> Appears in Proceedings of AI4HGI â€˜25, the First Workshop on   Artificial Intelligence for Human-Game Interaction at the 28th European   Conference on Artificial Intelligence (ECAI â€˜25), Bologna, October 25-30,   2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯æ”¯æŒæ¸¸æˆè®¾è®¡ç­‰é¢†åŸŸçš„ä¸“å®¶å·¥ä½œã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä¸­ç­‰è§„æ¨¡LLMï¼ˆå¯åœ¨å°å‹å·¥ä½œå®¤æˆ–å®¶åº­ç¯å¢ƒä¸­è¿è¡Œçš„å¸¸è§„ç¡¬ä»¶ä¸Šè¿è¡Œçš„æ¨¡å‹ï¼‰çš„å®ç”¨æ€§ã€‚é€šè¿‡è¯†åˆ«å½±å“æ¸¸æˆæ¦‚å¿µçš„å…³é”®è¦ç´ å¹¶åˆ©ç”¨ChatGPTç”Ÿæˆæ¸¸æˆæƒ³æ³•æ ·æœ¬ï¼Œç ”ç©¶å‘ç°DeepSeek-R1æ¨¡å‹åœ¨è¯„ä¼°æ¸¸æˆæƒ³æ³•æ—¶è¡¨ç°å‡ºæœ€ä¸€è‡´çš„åé¦ˆæ•ˆæœã€‚é€šè¿‡è¯•ç‚¹ç ”ç©¶è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­çš„é€‚ç”¨æ€§ï¼Œç»“æœè¡¨æ˜å¤§å¤šæ•°å‚ä¸è€…å¯¹æ¨¡å‹è¾“å‡ºæŒç§¯ææ€åº¦ï¼Œå¹¶æœ‰å…´è¶£å°†å…¶çº³å…¥å·¥ä½œæµç¨‹ä¸­ã€‚æ€»ä½“è€Œè¨€ï¼Œå½“å‰ä¸­ç­‰è§„æ¨¡çš„LLMåœ¨æ—©æœŸæ¸¸æˆè®¾è®¡ä¸­å¯æä¾›æœ‰ä»·å€¼çš„åé¦ˆï¼Œæç¤ºæ–¹æ³•çš„æ”¹è¿›å¯è¿›ä¸€æ­¥æé«˜ä¸€è‡´æ€§å’Œæ€»ä½“æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥æ”¯æŒåŒ…æ‹¬æ¸¸æˆè®¾è®¡åœ¨å†…çš„å„ç§é¢†åŸŸçš„ä¸“å®¶å·¥ä½œã€‚</li>
<li>ä¸­ç­‰è§„æ¨¡çš„LLMå…·æœ‰å®ç”¨ä»·å€¼ï¼Œèƒ½åœ¨å¸¸è§„ç¡¬ä»¶ä¸Šè¿è¡Œï¼Œé€‚ç”¨äºå°å‹å·¥ä½œå®¤æˆ–å®¶åº­ç¯å¢ƒã€‚</li>
<li>é€šè¿‡è¯†åˆ«å…³é”®è¦ç´ å¹¶åˆ©ç”¨ChatGPTç”Ÿæˆæ¸¸æˆæƒ³æ³•æ ·æœ¬ï¼Œç ”ç©¶å‘ç°DeepSeek-R1æ¨¡å‹åœ¨è¯„ä¼°æ¸¸æˆæ¦‚å¿µæ—¶è¡¨ç°å‡ºæœ€ä½³æ•ˆæœã€‚</li>
<li>LLMèƒ½å¤Ÿç”Ÿæˆæœ‰ä»·å€¼çš„åé¦ˆæ¥æ”¯æŒæ—©æœŸæ¸¸æˆè®¾è®¡ã€‚</li>
<li>è¯•ç‚¹ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°å‚ä¸è€…å¯¹LLMçš„è¾“å‡ºæŒç§¯ææ€åº¦ï¼Œå¹¶æ„¿æ„å°†å…¶çº³å…¥å·¥ä½œæµç¨‹ä¸­ã€‚</li>
<li>å½“å‰ä¸­ç­‰è§„æ¨¡çš„LLMåœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½å­˜åœ¨è´¨é‡æ³¢åŠ¨ï¼Œéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›æç¤ºæ–¹æ³•æ¥æé«˜ä¸€è‡´æ€§å’Œæ€»ä½“æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24730">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7dc5c0aebf3d9e415420b42ef40102a3" align="middle">
<img src="https://pic1.zhimg.com/v2-dcb74010ed7ca6e42d63955f5834ac82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd2ae208c19142b897979771e552162f" align="middle">
<img src="https://picx.zhimg.com/v2-a8a8b65d8c1f9d48d85f141a0cc99de5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LLaDA-MoE-A-Sparse-MoE-Diffusion-Language-Model"><a href="#LLaDA-MoE-A-Sparse-MoE-Diffusion-Language-Model" class="headerlink" title="LLaDA-MoE: A Sparse MoE Diffusion Language Model"></a>LLaDA-MoE: A Sparse MoE Diffusion Language Model</h2><p><strong>Authors:Fengqi Zhu, Zebin You, Yipeng Xing, Zenan Huang, Lin Liu, Yihong Zhuang, Guoshan Lu, Kangyu Wang, Xudong Wang, Lanning Wei, Hongrui Guo, Jiaqi Hu, Wentao Ye, Tieyuan Chen, Chenchen Li, Chengfu Tang, Haibo Feng, Jun Hu, Jun Zhou, Xiaolu Zhang, Zhenzhong Lan, Junbo Zhao, Da Zheng, Chongxuan Li, Jianguo Li, Ji-Rong Wen</strong></p>
<p>We introduce LLaDA-MoE, a large language diffusion model with the Mixture-of-Experts (MoE) architecture, trained from scratch on approximately 20T tokens. LLaDA-MoE achieves competitive performance with significantly reduced computational overhead by maintaining a 7B-parameter capacity while activating only 1.4B parameters during inference. Our empirical evaluation reveals that LLaDA-MoE achieves state-of-the-art performance among diffusion language models with larger parameters, surpassing previous diffusion language models LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The instruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities comparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation, mathematical reasoning, agent and alignment tasks, despite using fewer active parameters. Our results show that integrating a sparse MoE architecture into the training objective of masked diffusion language models still brings out MoEâ€™s strengths under efficient inference with few active parameters, and opens ample room for further exploration of diffusion language models. LLaDA-MoE models are available at Huggingface. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†LLaDA-MoEï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„çš„å¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨çº¦20ä¸‡äº¿ä¸ªä»¤ç‰Œä»å¤´å¼€å§‹è®­ç»ƒã€‚LLaDA-MoEä»¥7Bå‚æ•°çš„å®¹é‡å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ¨ç†æ—¶ä»…æ¿€æ´»1.4Bå‚æ•°ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬çš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒLLaDA-MoEåœ¨å…·æœ‰æ›´å¤§å‚æ•°çš„æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ‰©æ•£è¯­è¨€æ¨¡å‹LLaDAã€LLaDA 1.5å’ŒDreamç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ã€‚æŒ‡ä»¤è°ƒæ•´æ¨¡å‹LLaDA-MoE-7B-A1B-Instructåœ¨çŸ¥è¯†ç†è§£ã€ä»£ç ç”Ÿæˆã€æ•°å­¦æ¨ç†ã€ä»£ç†å’Œå¯¹é½ä»»åŠ¡ä¸­ï¼Œè¡¨ç°å‡ºä¸Qwen2.5-3B-Instructç›¸å½“çš„èƒ½åŠ›ï¼Œå°½ç®¡ä½¿ç”¨çš„æ¿€æ´»å‚æ•°è¾ƒå°‘ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°†ç¨€ç–çš„MoEæ¶æ„æ•´åˆåˆ°æ©è”½æ‰©æ•£è¯­è¨€æ¨¡å‹çš„è®­ç»ƒç›®æ ‡ä¸­ï¼Œä»ç„¶å¯ä»¥åœ¨é«˜æ•ˆçš„æ¨ç†å’Œå°‘é‡æ¿€æ´»å‚æ•°ä¸‹å‘æŒ¥å‡ºMoEçš„ä¼˜åŠ¿ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥æ¢ç´¢æ‰©æ•£è¯­è¨€æ¨¡å‹æä¾›äº†å¹¿é˜”çš„ç©ºé—´ã€‚LLaDA-MoEæ¨¡å‹å·²åœ¨Huggingfaceä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24389v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLaDA-MoEæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹ä¸ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„çš„æ¨¡å‹ï¼Œä½¿ç”¨çº¦20Tçš„ä»¤ç‰Œä»å¤´å¼€å§‹è®­ç»ƒã€‚LLaDA-MoEåœ¨ç»´æŒ7Bå‚æ•°å®¹é‡çš„åŒæ—¶ï¼Œæ¨ç†æ—¶ä»…æ¿€æ´»1.4Bå‚æ•°ï¼Œå®ç°äº†æ˜¾è‘—çš„è®¡ç®—æ•ˆç‡ã€‚å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒLLaDA-MoEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ‰©æ•£è¯­è¨€æ¨¡å‹çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„LLaDAã€LLaDA 1.5å’ŒDreamç­‰æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒLLaDA-MoE-7B-A1B-Instructæ¨¡å‹åœ¨çŸ¥è¯†ç†è§£ã€ä»£ç ç”Ÿæˆã€æ•°å­¦æ¨ç†ã€ä»£ç†å’Œå¯¹é½ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ä¸Qwen2.5-3B-Instructç›¸å½“ã€‚æ•´åˆç¨€ç–MoEæ¶æ„åˆ°æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹çš„è®­ç»ƒç›®æ ‡ä¸­ï¼Œä»èƒ½åœ¨é«˜æ•ˆçš„æ¨ç†å’Œå°‘é‡æ´»è·ƒå‚æ•°ä¸‹å‘æŒ¥MoEçš„ä¼˜åŠ¿ï¼Œä¸ºæ‰©æ•£è¯­è¨€æ¨¡å‹çš„è¿›ä¸€æ­¥æ¢ç´¢æä¾›äº†å¹¿é˜”çš„ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaDA-MoEæ˜¯ä¸€ä¸ªç»“åˆå¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹å’Œä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„çš„æ¨¡å‹ã€‚</li>
<li>LLaDA-MoEé€šè¿‡ç»´æŠ¤7Bå‚æ•°çš„å®¹é‡ï¼Œåœ¨æ¨ç†æ—¶ä»…æ¿€æ´»1.4Bå‚æ•°ï¼Œå®ç°äº†è®¡ç®—æ•ˆç‡çš„æ˜¾è‘—æé«˜ã€‚</li>
<li>å®è¯è¯„ä¼°æ˜¾ç¤ºLLaDA-MoEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†å…¶ä»–æ‰©æ•£è¯­è¨€æ¨¡å‹ã€‚</li>
<li>LLaDA-MoE-7B-A1B-Instructæ¨¡å‹åœ¨çŸ¥è¯†ç†è§£ã€ä»£ç ç”Ÿæˆç­‰æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>MoEæ¶æ„æ•´åˆåˆ°æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹çš„è®­ç»ƒç›®æ ‡ä¸­ï¼Œå®ç°äº†é«˜æ•ˆæ¨ç†å’Œä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>LLaDA-MoEæ¨¡å‹åœ¨æ‰©æ•£è¯­è¨€æ¨¡å‹çš„è¿›ä¸€æ­¥æ¢ç´¢ä¸­å…·æœ‰å¹¿é˜”çš„ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24389">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f8c5880a56f570f3afb3b3fd472f5ac" align="middle">
<img src="https://picx.zhimg.com/v2-247b5cc325583c75bd393f0380fa603a" align="middle">
<img src="https://picx.zhimg.com/v2-a28fd8ba09dd8d5a7bdb5da52dd65ed8" align="middle">
<img src="https://picx.zhimg.com/v2-a2c476a0af5ecdc3622be5c7af6623a3" align="middle">
<img src="https://picx.zhimg.com/v2-6f82f41cf51745ba3473d21fbe54a80a" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Speculative-Verification-Exploiting-Information-Gain-to-Refine-Speculative-Decoding"><a href="#Speculative-Verification-Exploiting-Information-Gain-to-Refine-Speculative-Decoding" class="headerlink" title="Speculative Verification: Exploiting Information Gain to Refine   Speculative Decoding"></a>Speculative Verification: Exploiting Information Gain to Refine   Speculative Decoding</h2><p><strong>Authors:Sungkyun Kim, Jaemin Kim, Dogyung Yoon, Jiho Shin, Junyeol Lee, Jiwon Seo</strong></p>
<p>LLMs have low GPU efficiency and high latency due to autoregressive decoding. Speculative decoding (SD) mitigates this using a small draft model to speculatively generate multiple tokens, which are then verified in parallel by a target model. However, when speculation accuracy is low, the overhead from rejected tokens can offset the benefits, limiting SDâ€™s effectiveness, especially at large batch sizes. To address this, we propose Speculative Verification (SV), an efficient augmentation to SD that dynamically predicts speculation accuracy and adapts the verification length to maximize throughput. SV introduces a companion model - a small auxiliary model similar in size to the draft model - to estimate the alignment between draft and target model distributions. By maximizing the information gain from quantifying this alignment, SV refines verification decisions, reducing wasted computation on rejected tokens and improving decoding efficiency. Moreover, SV requires no modifications to the draft or target models and is compatible with existing SD variants. We extensively evaluated SV on publicly available LLMs across three NLP tasks using nine combinations of draft, companion, and target models, including 13B-72B target models and three types of variations: base (no finetuning), instruction-tuned, and task fine-tuned. Across all experiments and batch sizes (4-80), SV consistently outperforms both SD and standard decoding with the target model. It improves SD performance by up to 2$\times$, with an average speedup of 1.4 $\times$ in large-batch settings (batch sizes 32-80). These results demonstrate SVâ€™s robustness, scalability, and practical utility for efficient LLM inference. </p>
<blockquote>
<p>LLMç”±äºè‡ªå›å½’è§£ç è€Œå…·æœ‰è¾ƒä½çš„GPUæ•ˆç‡å’Œè¾ƒé«˜çš„å»¶è¿Ÿã€‚æŠ•æœºè§£ç ï¼ˆSDï¼‰é€šè¿‡ä½¿ç”¨å°å‹è‰ç¨¿æ¨¡å‹æ¥æŠ•æœºç”Ÿæˆå¤šä¸ªä»¤ç‰Œï¼Œç„¶åå°†å…¶å¹¶è¡ŒéªŒè¯ç›®æ ‡æ¨¡å‹æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œå½“æŠ•æœºå‡†ç¡®æ€§è¾ƒä½æ—¶ï¼Œæ‹’ç»ä»¤ç‰Œçš„å¼€é”€å¯èƒ½ä¼šæŠµæ¶ˆå¥½å¤„ï¼Œé™åˆ¶SDçš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§æ‰¹é‡æƒ…å†µä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæŠ•æœºéªŒè¯â€(SV)ï¼Œè¿™æ˜¯SDçš„ä¸€ç§æœ‰æ•ˆå¢å¼ºåŠŸèƒ½ï¼Œèƒ½å¤ŸåŠ¨æ€é¢„æµ‹æŠ•æœºå‡†ç¡®æ€§å¹¶é€‚åº”éªŒè¯é•¿åº¦ä»¥æœ€å¤§åŒ–ååé‡ã€‚SVå¼•å…¥äº†ä¸€ä¸ªé…å¥—æ¨¡å‹â€”â€”ä¸€ä¸ªä¸è‰ç¨¿æ¨¡å‹å¤§å°ç›¸ä¼¼çš„è¾…åŠ©å°æ¨¡å‹ï¼Œä»¥ä¼°è®¡è‰ç¨¿å’Œç›®æ ‡æ¨¡å‹åˆ†å¸ƒä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚é€šè¿‡æœ€å¤§åŒ–è¿™ç§å¯¹é½çš„é‡åŒ–ä¿¡æ¯æ”¶ç›Šï¼ŒSVä¼˜åŒ–äº†éªŒè¯å†³ç­–ï¼Œå‡å°‘äº†æ‹’ç»ä»¤ç‰Œä¸Šçš„æµªè´¹è®¡ç®—ï¼Œæé«˜äº†è§£ç æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒSVä¸éœ€è¦å¯¹è‰ç¨¿æˆ–ç›®æ ‡æ¨¡å‹è¿›è¡Œä»»ä½•ä¿®æ”¹ï¼Œå¹¶ä¸”å¯ä»¥ä¸ç°æœ‰çš„SDå˜ä½“å…¼å®¹ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªNLPä»»åŠ¡ä¸Šå…¨é¢è¯„ä¼°äº†SVï¼Œä½¿ç”¨ä¹ä¸ªç»„åˆçš„è‰ç¨¿ã€é…å¥—å’Œç›®æ ‡æ¨¡å‹ï¼ŒåŒ…æ‹¬ç›®æ ‡æ¨¡å‹çš„åŸºå‡†ï¼ˆæ— å¾®è°ƒï¼‰ã€æŒ‡ä»¤å¾®è°ƒä»¥åŠä»»åŠ¡å¾®è°ƒã€‚åœ¨æ‰€æœ‰å®éªŒå’Œæ‰¹é‡å¤§å°ï¼ˆ4-80ï¼‰ä¸­ï¼ŒSVå§‹ç»ˆä¼˜äºSDå’Œç›®æ ‡æ¨¡å‹çš„å¸¸è§„è§£ç ã€‚å®ƒæé«˜äº†SDçš„æ€§èƒ½é«˜è¾¾ä¸¤å€ï¼Œåœ¨å¤§æ‰¹é‡è®¾ç½®ï¼ˆæ‰¹é‡å¤§å°32-80ï¼‰ä¸­å¹³å‡åŠ é€Ÿ1.4å€ã€‚è¿™äº›ç»“æœè¯æ˜äº†SVåœ¨é«˜æ•ˆLLMæ¨ç†ä¸­çš„ç¨³å¥æ€§ã€å¯æ‰©å±•æ€§å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24328v1">PDF</a> 14 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>LLMså­˜åœ¨GPUæ•ˆç‡ä½å’Œå»¶è¿Ÿé«˜çš„é—®é¢˜ï¼ŒåŸå› æ˜¯è‡ªåŠ¨å›å½’è§£ç ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†åŸºäºæ¨æµ‹è§£ç ï¼ˆSpeculative Decodingï¼‰çš„æ”¹è¿›æ–¹æ¡ˆâ€”â€”Speculative Verificationï¼ˆSVï¼‰ã€‚SVé€šè¿‡åŠ¨æ€é¢„æµ‹æ¨æµ‹ç²¾åº¦ï¼Œå¹¶æ ¹æ®ç›®æ ‡æ¨¡å‹çš„åˆ†å¸ƒæƒ…å†µè¿›è¡Œé€‚åº”æ€§åœ°éªŒè¯é•¿åº¦ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„æ¨æ–­å¤„ç†ã€‚ç›¸æ¯”äºå·²æœ‰çš„SDæŠ€æœ¯ï¼ŒSVåœ¨ä¸åŒçš„å¤§è§„æ¨¡æ¨¡å‹å’ŒNLPä»»åŠ¡æµ‹è¯•ä¸­ï¼Œéƒ½æœ‰ç€æ›´ä½³çš„è¡¨ç°ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤§æ‰¹æ¬¡åœºæ™¯ä¸­ï¼ŒSVä¸ä»…å…·æœ‰ä¼˜å¼‚çš„åŠ é€Ÿæ•ˆæœï¼Œè¿˜èƒ½å¤Ÿå®ç°çº¦å¹³å‡ä¸€å€çš„æå‡æ•ˆæœã€‚è¿™ä¸ä»…æ˜¾ç¤ºå‡ºå…¶ç†è®ºç ”ç©¶çš„å®ç”¨æ€§ä»·å€¼ï¼ŒåŒæ—¶è¿˜åœ¨æŠ€æœ¯ä¸­ä½“ç°å‡ºé«˜åº¦ä¼˜åŒ–çš„é«˜æ•ˆæ¨ç†æ•ˆç‡ä¸è§„æ¨¡åŒ–æ‹“å±•æ€§ä¼˜åŠ¿ã€‚åŒæ—¶ä¹Ÿä¸éœ€è¦ä¿®æ”¹è‰å›¾å’Œç›®æ ‡æ¨¡å‹ã€‚ç›¸è¾ƒäºå¸¸è§„è§£ç æŠ€æœ¯ï¼ŒSVè¡¨ç°å‡ºäº†æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚æ€»ä¹‹ï¼ŒSVçš„æå‡ºæœ‰æœ›æ˜¾è‘—æé«˜LLMçš„æ€§èƒ½æ•ˆç‡å¹¶å‡å°‘è®¡ç®—èµ„æºæ¶ˆè€—ã€‚ä¸ºæ­¤é¡¹ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œè§†è§’ã€‚æé«˜äº†å·¥ä½œæ•ˆç‡ã€‚å…·æœ‰æ½œåœ¨çš„å®é™…åº”ç”¨ä»·å€¼å‰æ™¯å¹¿é˜”ã€‚æé«˜äº†æ¨ç†æ€§èƒ½ä¸æ‰©å±•æ€§ä¼˜åŠ¿ã€‚ç ”ç©¶å†…å®¹ä¸°å¯Œã€æŠ€æœ¯æ–°é¢–ï¼Œå…·æœ‰ä¸€å®šçš„å®ç”¨ä»·å€¼å’Œåº”ç”¨å‰æ™¯ã€‚ä½“ç°äº†åˆ›æ–°æ€§æŠ€æœ¯å®åŠ›ä¸åº”ç”¨ä»·å€¼é«˜åº¦ç»Ÿä¸€çš„ä¼˜åŠ¿ç‰¹å¾ï¼Œä½“ç°äº†é«˜åº¦çš„å‰ç»æ€§è¶‹åŠ¿ç‰¹ç‚¹ã€‚ã€‚é’ˆå¯¹å½“å‰ç ”ç©¶é—®é¢˜æå‡ºäº†ä¸€ç§å…·æœ‰å®é™…å¯è¡Œæ€§çš„é«˜æ•ˆè§£å†³ç­–ç•¥ã€‚ã€‚ä¸ºæ­¤é¢†åŸŸæä¾›äº†åˆ‡å®å¯è¡Œçš„æŠ€æœ¯æ–¹æ¡ˆå…·æœ‰é‡è¦çš„å­¦æœ¯ä»·å€¼å’Œå®é™…åº”ç”¨ä»·å€¼ã€‚ã€‚ç ”ç©¶ç»“æœè¡¨æ˜å…¶å®é™…åº”ç”¨å‰æ™¯å¹¿é˜”å¹¶å±•ç°å‡ºæ˜¾è‘—çš„æŠ€æœ¯ä¼˜åŠ¿ç‰¹å¾ã€‚åœ¨å®é™…åº”ç”¨ä¸­å±•ç°å‡ºè¾ƒé«˜çš„æ€§èƒ½å’Œç¨³å®šæ€§è¡¨ç°çªå‡ºã€‚å°†å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œè¡Œä¸šå½±å“ç©ºé—´å¹¿ä»¥åŠå¯¹æ¨è¿›æ•´ä¸ªé¢†åŸŸçš„è·¨è¶Šå‘å±•ä¹Ÿå…·æœ‰ä¸å¯å°è§‘çš„é‡è¦ä½œç”¨ä¸ä»·å€¼æ½œèƒ½æŒç»­å¯¹è¡Œä¸šæŠ€æœ¯çš„åˆ›æ–°å’Œæ”¹é©å‡çº§ä½œå‡ºé‡è¦çš„æ¨åŠ¨ä½œç”¨å…·æœ‰å®ç”¨ä»·å€¼ä½“ç°åŒæ—¶å¹¶å±•æœ›æœªæ¥ä¿æŒå‰æ²¿çš„ç ”ç©¶æ¢ç´¢ã€‚ã€‚è¿™å°†æ˜¯ä¸€é¡¹åˆ›æ–°çªç ´å¹¶å¯¹æœªæ¥çš„AIåº”ç”¨æä¾›å¼ºå¤§åŠ©åŠ›ä¸ºåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æé«˜å¼€æ‹“äº†æ›´å¹¿é˜”çš„æ€è·¯ä¸æ–¹æ³•é¢†åŸŸå¸¦æ¥æ–°çš„åº”ç”¨çªç ´å’Œå˜é©ã€‚ä¸ºè¡Œä¸šå¸¦æ¥é©å‘½æ€§çš„è¿›æ­¥å’Œå˜é©è¶‹åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå­˜åœ¨GPUæ•ˆç‡ä½å’Œå»¶è¿Ÿé«˜çš„é—®é¢˜ï¼Œå½±å“äº†å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>Speculative Verificationï¼ˆSVï¼‰ä½œä¸ºä¸€ç§æ–°çš„è§£ç æŠ€æœ¯è¢«æå‡ºä»¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå®ƒé€šè¿‡åŠ¨æ€é¢„æµ‹æ¨æµ‹ç²¾åº¦å¹¶ä¼˜åŒ–éªŒè¯è¿‡ç¨‹æ¥æé«˜æ•ˆç‡ã€‚</li>
<li>SVç›¸è¾ƒäºå·²æœ‰çš„æ¨æµ‹è§£ç æŠ€æœ¯æœ‰æ›´å¥½çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡æ‰¹æ¬¡å¤„ç†åœºæ™¯ä¸‹ã€‚å®ƒåœ¨æé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œè¿˜å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-26c7106c705f6f250d326bd67f8975a9" align="middle">
<img src="https://picx.zhimg.com/v2-e20aecfd68628b3d1db42ffbb5958b00" align="middle">
<img src="https://picx.zhimg.com/v2-98ef288d4e01abfe4d64f6541bf8889c" align="middle">
<img src="https://picx.zhimg.com/v2-a0b7471f4d11e6f8d65207f36f1b46be.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ELASTIQ-EEG-Language-Alignment-with-Semantic-Task-Instruction-and-Querying"><a href="#ELASTIQ-EEG-Language-Alignment-with-Semantic-Task-Instruction-and-Querying" class="headerlink" title="ELASTIQ: EEG-Language Alignment with Semantic Task Instruction and   Querying"></a>ELASTIQ: EEG-Language Alignment with Semantic Task Instruction and   Querying</h2><p><strong>Authors:Muyun Jiang, Shuailei Zhang, Zhenjie Yang, Mengjun Wu, Weibang Jiang, Zhiwei Guo, Wei Zhang, Rui Liu, Shangen Zhang, Yong Li, Yi Ding, Cuntai Guan</strong></p>
<p>Recent advances in electroencephalography (EEG) foundation models, which capture transferable EEG representations, have greatly accelerated the development of brain-computer interfaces (BCI). However, existing approaches still struggle to incorporate language instructions as prior constraints for EEG representation learning, limiting their ability to leverage the semantic knowledge inherent in language to unify different labels and tasks. To address this challenge, we present ELASTIQ, a foundation model for EEG-Language Alignment with Semantic Task Instruction and Querying. ELASTIQ integrates task-aware semantic guidance to produce structured and linguistically aligned EEG embeddings, thereby enhancing decoding robustness and transferability. In the pretraining stage, we introduce a joint Spectral-Temporal Reconstruction (STR) module, which combines frequency masking as a global spectral perturbation with two complementary temporal objectives: random masking to capture contextual dependencies and causal masking to model sequential dynamics. In the instruction tuning stage, we propose the Instruction-conditioned Q-Former (IQF), a query-based cross-attention transformer that injects instruction embeddings into EEG tokens and aligns them with textual label embeddings through learnable queries. We evaluate ELASTIQ on 20 datasets spanning motor imagery, emotion recognition, steady-state visual evoked potentials, covert speech, and healthcare tasks. ELASTIQ achieves state-of-the-art performance on 14 of the 20 datasets and obtains the best average results across all five task categories. Importantly, our analyses reveal for the first time that explicit task instructions serve as semantic priors guiding EEG embeddings into coherent and linguistically grounded spaces. The code and pre-trained weights will be released. </p>
<blockquote>
<p>æœ€è¿‘è„‘ç”µå›¾ï¼ˆEEGï¼‰åŸºç¡€æ¨¡å‹çš„è¿›æ­¥ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿæ•æ‰å¯è¿ç§»çš„EEGè¡¨ç¤ºï¼Œæå¤§åœ°åŠ é€Ÿäº†è„‘æœºæ¥å£ï¼ˆBCIï¼‰çš„å‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»ç„¶éš¾ä»¥å°†è¯­è¨€æŒ‡ä»¤ä½œä¸ºå…ˆéªŒçº¦æŸèå…¥EEGè¡¨ç¤ºå­¦ä¹ ä¸­ï¼Œé™åˆ¶äº†å…¶åˆ©ç”¨è¯­è¨€ä¸­å›ºæœ‰çš„è¯­ä¹‰çŸ¥è¯†æ¥ç»Ÿä¸€ä¸åŒæ ‡ç­¾å’Œä»»åŠ¡çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ELA STIQï¼Œä¸€ä¸ªç»“åˆè¯­ä¹‰ä»»åŠ¡æŒ‡ä»¤å’ŒæŸ¥è¯¢çš„EEG-è¯­è¨€å¯¹é½åŸºç¡€æ¨¡å‹ã€‚ELA STIQæ•´åˆä»»åŠ¡æ„ŸçŸ¥è¯­ä¹‰æŒ‡å¯¼ï¼Œä»¥äº§ç”Ÿç»“æ„åŒ–ä¸”è¯­è¨€å¯¹é½çš„EEGåµŒå…¥ï¼Œä»è€Œæé«˜è§£ç çš„ç¨³å¥æ€§å’Œå¯è¿ç§»æ€§ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè”åˆçš„æ—¶é¢‘é‡å»ºï¼ˆSTRï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†é¢‘ç‡æ©ç ä½œä¸ºå…¨å±€é¢‘è°±æ‰°åŠ¨ä¸ä¸¤ä¸ªäº’è¡¥çš„æ—¶é—´ç›®æ ‡ç›¸ç»“åˆï¼šéšæœºæ©ç ä»¥æ•æ‰ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»å’Œæœ‰å‘æ©ç ä»¥æ¨¡æ‹Ÿåºåˆ—åŠ¨æ€ã€‚åœ¨æŒ‡ä»¤å¾®è°ƒé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†æŒ‡ä»¤æ¡ä»¶Q-Formersï¼ˆIQFï¼‰ï¼Œä¸€ç§åŸºäºæŸ¥è¯¢çš„è·¨æ³¨æ„åŠ›è½¬æ¢å™¨ï¼Œå®ƒå°†æŒ‡ä»¤åµŒå…¥æ³¨å…¥EEGä»¤ç‰Œä¸­ï¼Œå¹¶é€šè¿‡å¯å­¦ä¹ çš„æŸ¥è¯¢å°†å®ƒä»¬ä¸æ–‡æœ¬æ ‡ç­¾åµŒå…¥å¯¹é½ã€‚æˆ‘ä»¬åœ¨è·¨è¶Šè¿åŠ¨æƒ³è±¡ã€æƒ…ç»ªè¯†åˆ«ã€ç¨³æ€è§†è§‰è¯±å‘ç”µä½ã€éšè”½æ€§è¯­è¨€å’ŒåŒ»ç–—ä»»åŠ¡çš„20ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†ELA STIQã€‚ELA STIQåœ¨å…¶ä¸­çš„14ä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶åœ¨æ‰€æœ‰äº”ä¸ªä»»åŠ¡ç±»åˆ«ä¸­è·å¾—äº†æœ€ä½³å¹³å‡ç»“æœã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åˆ†æé¦–æ¬¡è¡¨æ˜ï¼Œæ˜ç¡®çš„ä»»åŠ¡æŒ‡ä»¤ä½œä¸ºè¯­ä¹‰å…ˆéªŒæŒ‡å¯¼EEGåµŒå…¥åˆ°è¿è´¯ä¸”è¯­è¨€åŸºç¡€çš„ç©ºé—´ä¸­ã€‚ä»£ç å’Œé¢„è®­ç»ƒæƒé‡å°†è¢«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24302v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†EEGè¯­è¨€å¯¹é½æ¨¡å‹ELASTIQï¼Œè¯¥æ¨¡å‹ç»“åˆäº†ä»»åŠ¡æ„ŸçŸ¥è¯­ä¹‰æŒ‡å¯¼ï¼Œç”Ÿæˆç»“æ„åŒ–ä¸”è¯­è¨€å¯¹é½çš„EEGåµŒå…¥ï¼Œæé«˜äº†è§£ç ç¨³å¥æ€§å’Œè¿ç§»æ€§ã€‚é€šè¿‡å¼•å…¥è”åˆè°±æ—¶åºé‡å»ºï¼ˆSTRï¼‰æ¨¡å—ä»¥åŠæŒ‡ä»¤è°ƒèŠ‚Q-Formersï¼ˆIQFï¼‰ï¼Œä½¿EEGä»¤ç‰Œä¸æ–‡æœ¬æ ‡ç­¾åµŒå…¥å¯¹é½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒELASTIQåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—æœ€ä½³æ€§èƒ½ï¼Œå¹¶é¦–æ¬¡æ­ç¤ºæ˜ç¡®çš„ä»»åŠ¡æŒ‡ä»¤ä½œä¸ºè¯­ä¹‰å…ˆéªŒï¼Œå¼•å¯¼EEGåµŒå…¥è¿›å…¥è¿è´¯ä¸”è¯­è¨€åŸºç¡€çš„ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ELASTIQæ˜¯ä¸€ä¸ªç»“åˆä»»åŠ¡æ„ŸçŸ¥è¯­ä¹‰æŒ‡å¯¼çš„EEGè¯­è¨€å¯¹é½æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆç»“æ„åŒ–ä¸”è¯­è¨€å¯¹é½çš„EEGåµŒå…¥ã€‚</li>
<li>ELASTIQé€šè¿‡å¼•å…¥è”åˆè°±æ—¶åºé‡å»ºï¼ˆSTRï¼‰æ¨¡å—ï¼Œç»“åˆé¢‘ç‡æ©ç å’Œä¸¤ç§æ—¶é—´ç›®æ ‡ï¼ˆéšæœºæ©ç å’Œå› æœæ©ç ï¼‰ï¼Œæé«˜EEGæ•°æ®çš„è§£ç ç¨³å¥æ€§å’Œè¿ç§»æ€§ã€‚</li>
<li>ELASTIQé‡‡ç”¨æŒ‡ä»¤è°ƒèŠ‚Q-Formersï¼ˆIQFï¼‰ï¼Œä¸€ä¸ªåŸºäºæŸ¥è¯¢çš„äº¤å‰æ³¨æ„å˜å‹å™¨ï¼Œå°†æŒ‡ä»¤åµŒå…¥åˆ°EEGä»¤ç‰Œä¸­ï¼Œå¹¶ä¸æ–‡æœ¬æ ‡ç­¾åµŒå…¥å¯¹é½ã€‚</li>
<li>ELASTIQåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå®ƒåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—æœ€ä½³æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶é¦–æ¬¡æ­ç¤ºæ˜ç¡®çš„ä»»åŠ¡æŒ‡ä»¤ä½œä¸ºè¯­ä¹‰å…ˆéªŒï¼Œå¯¹å¼•å¯¼EEGåµŒå…¥è¿›å…¥è¿è´¯ä¸”è¯­è¨€åŸºç¡€çš„ç©ºé—´å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>ELASTIQçš„ä»£ç å’Œé¢„è®­ç»ƒæƒé‡å°†å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-106e7f0100df7b7d5362e4f054dc691f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7227c86c22c142ae0c078c4e21bd61a1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Transformer-Tafsir-at-QIAS-2025-Shared-Task-Hybrid-Retrieval-Augmented-Generation-for-Islamic-Knowledge-Question-Answering"><a href="#Transformer-Tafsir-at-QIAS-2025-Shared-Task-Hybrid-Retrieval-Augmented-Generation-for-Islamic-Knowledge-Question-Answering" class="headerlink" title="Transformer Tafsir at QIAS 2025 Shared Task: Hybrid Retrieval-Augmented   Generation for Islamic Knowledge Question Answering"></a>Transformer Tafsir at QIAS 2025 Shared Task: Hybrid Retrieval-Augmented   Generation for Islamic Knowledge Question Answering</h2><p><strong>Authors:Muhammad Abu Ahmad, Mohamad Ballout, Raia Abu Ahmad, Elia Bruni</strong></p>
<p>This paper presents our submission to the QIAS 2025 shared task on Islamic knowledge understanding and reasoning. We developed a hybrid retrieval-augmented generation (RAG) system that combines sparse and dense retrieval methods with cross-encoder reranking to improve large language model (LLM) performance. Our three-stage pipeline incorporates BM25 for initial retrieval, a dense embedding retrieval model for semantic matching, and cross-encoder reranking for precise content retrieval. We evaluate our approach on both subtasks using two LLMs, Fanar and Mistral, demonstrating that the proposed RAG pipeline enhances performance across both, with accuracy improvements up to 25%, depending on the task and model configuration. Our best configuration is achieved with Fanar, yielding accuracy scores of 45% in Subtask 1 and 80% in Subtask 2. </p>
<blockquote>
<p>æœ¬æ–‡æ˜¯ä¸ºQIAS 2025å…³äºä¼Šæ–¯å…°çŸ¥è¯†ç†è§£ä¸æ¨ç†çš„å…±äº«ä»»åŠ¡æäº¤çš„ä½œå“ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ··åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†ç¨€ç–å’Œå¯†é›†æ£€ç´¢æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨è·¨ç¼–ç å™¨é‡æ–°æ’åºï¼Œä»¥æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä¸‰é˜¶æ®µç®¡é“ç»“åˆäº†BM25è¿›è¡Œåˆæ­¥æ£€ç´¢ã€å¯†é›†åµŒå…¥æ£€ç´¢æ¨¡å‹è¿›è¡Œè¯­ä¹‰åŒ¹é…å’Œè·¨ç¼–ç å™¨é‡æ–°æ’åºä»¥è¿›è¡Œç²¾ç¡®å†…å®¹æ£€ç´¢ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå­ä»»åŠ¡ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œä½¿ç”¨äº†Fanarå’ŒMistralä¸¤ä¸ªLLMï¼Œç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„RAGç®¡é“åœ¨è¿™ä¸¤ä¸ªæ–¹é¢éƒ½æé«˜äº†æ€§èƒ½ï¼Œå‡†ç¡®åº¦çš„æé«˜å¹…åº¦é«˜è¾¾25%ï¼Œè¿™å–å†³äºä»»åŠ¡å’Œæ¨¡å‹é…ç½®ã€‚ä½¿ç”¨Fanarçš„æœ€ä½³é…ç½®åœ¨Subtask 1ä¸­è¾¾åˆ°45%çš„å‡†ç¡®ç‡ï¼Œåœ¨Subtask 2ä¸­è¾¾åˆ°80%çš„å‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23793v1">PDF</a> Accepted at ArabicNLP 2025, co-located with EMNLP 2025</p>
<p><strong>æ€»ç»“</strong><br>    æœ¬æ–‡ä»‹ç»äº†å¯¹QIAS 2025å…±äº«ä»»åŠ¡çš„ä¼Šæ–¯å…°çŸ¥è¯†ç†è§£ä¸æ¨ç†çš„æäº¤å†…å®¹ã€‚å¼€å‘äº†ä¸€ç§æ··åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†ç¨€ç–å’Œå¯†é›†æ£€ç´¢æ–¹æ³•ä»¥åŠè·¨ç¼–ç å™¨é‡æ’ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚è¯¥ç³»ç»Ÿçš„ä¸‰é˜¶æ®µç®¡é“ç»“åˆäº†BM25åˆå§‹æ£€ç´¢ã€å¯†é›†åµŒå…¥æ£€ç´¢æ¨¡å‹è¿›è¡Œè¯­ä¹‰åŒ¹é…å’Œè·¨ç¼–ç å™¨é‡æ’è¿›è¡Œç²¾ç¡®å†…å®¹æ£€ç´¢ã€‚é€šè¿‡ä¸¤ä¸ªLLMï¼ˆFanarå’ŒMistralï¼‰å¯¹ä¸¤ä¸ªå­ä»»åŠ¡è¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜äº†æ‰€æå‡ºçš„RAGç®¡é“åœ¨ä¸¤è€…ä¸­éƒ½æé«˜äº†æ€§èƒ½ï¼Œå‡†ç¡®ç‡æé«˜äº†é«˜è¾¾25%ï¼Œå…·ä½“å–å†³äºä»»åŠ¡å’Œæ¨¡å‹é…ç½®ã€‚ä½¿ç”¨Fanarçš„æœ€ä½³é…ç½®åœ¨Subtask 1ä¸­è¾¾åˆ°45%çš„å‡†ç¡®ç‡ï¼Œåœ¨Subtask 2ä¸­è¾¾åˆ°80%çš„å‡†ç¡®ç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡æäº¤äºQIAS 2025å…±äº«ä»»åŠ¡ï¼Œä¸“æ³¨äºä¼Šæ–¯å…°çŸ¥è¯†ç†è§£ä¸æ¨ç†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ··åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚</li>
<li>RAGç³»ç»Ÿé‡‡ç”¨ä¸‰é˜¶æ®µç®¡é“ï¼šBM25åˆå§‹æ£€ç´¢ã€å¯†é›†åµŒå…¥æ£€ç´¢æ¨¡å‹è¿›è¡Œè¯­ä¹‰åŒ¹é…å’Œè·¨ç¼–ç å™¨é‡æ’è¿›è¡Œç²¾ç¡®å†…å®¹æ£€ç´¢ã€‚</li>
<li>åœ¨ä¸¤ä¸ªå­ä»»åŠ¡ä¸Šè¯„ä¼°äº†ä¸¤ç§LLMï¼ˆFanarå’ŒMistralï¼‰ï¼Œè¯æ˜RAGç³»ç»Ÿèƒ½å¤Ÿæé«˜æ€§èƒ½ã€‚</li>
<li>RAGç³»ç»Ÿçš„å‡†ç¡®ç‡æé«˜å¹…åº¦æœ€é«˜å¯è¾¾25%ï¼Œå…·ä½“å–å†³äºä»»åŠ¡å’Œæ¨¡å‹é…ç½®ã€‚</li>
<li>ä½¿ç”¨Fanarçš„æœ€ä½³é…ç½®åœ¨Subtask 1çš„å‡†ç¡®ç‡ä¸º45%ï¼Œåœ¨Subtask 2çš„å‡†ç¡®ç‡ä¸º80%ã€‚</li>
<li>è¯¥ç ”ç©¶å±•ç¤ºäº†è·¨ç¼–ç å™¨é‡æ’æŠ€æœ¯åœ¨æé«˜LLMæ€§èƒ½æ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b922ff24e5b996aca27648d141532976" align="middle">
<img src="https://picx.zhimg.com/v2-03d8e9f5f6ced5f8d88035cdb268803b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5030e80ac1793e56bed3ccffe974356.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ca6f4c32634ab5d8bcc38c5ee133068" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="FraudTransformer-Time-Aware-GPT-for-Transaction-Fraud-Detection"><a href="#FraudTransformer-Time-Aware-GPT-for-Transaction-Fraud-Detection" class="headerlink" title="FraudTransformer: Time-Aware GPT for Transaction Fraud Detection"></a>FraudTransformer: Time-Aware GPT for Transaction Fraud Detection</h2><p><strong>Authors:Gholamali Aminian, Andrew Elliott, Tiger Li, Timothy Cheuk Hin Wong, Victor Claude Dehon, Lukasz Szpruch, Carsten Maple, Christopher Read, Martin Brown, Gesine Reinert, Mo Mamouei</strong></p>
<p>Detecting payment fraud in real-world banking streams requires models that can exploit both the order of events and the irregular time gaps between them. We introduce FraudTransformer, a sequence model that augments a vanilla GPT-style architecture with (i) a dedicated time encoder that embeds either absolute timestamps or inter-event values, and (ii) a learned positional encoder that preserves relative order. Experiments on a large industrial dataset â€“ tens of millions of transactions and auxiliary events â€“ show that FraudTransformer surpasses four strong classical baselines (Logistic Regression, XGBoost and LightGBM) as well as transformer ablations that omit either the time or positional component. On the held-out test set it delivers the highest AUROC and PRAUC. </p>
<blockquote>
<p>åœ¨çœŸå®ä¸–ç•Œçš„é“¶è¡Œæµæ°´äº¤æ˜“ä¸­æ£€æµ‹æ”¯ä»˜æ¬ºè¯ˆï¼Œéœ€è¦èƒ½å¤Ÿåˆ©ç”¨äº‹ä»¶é¡ºåºå’Œäº‹ä»¶ä¹‹é—´ä¸è§„åˆ™æ—¶é—´é—´éš”çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¼•å…¥äº†FraudTransformerï¼Œè¿™æ˜¯ä¸€ä¸ªåºåˆ—æ¨¡å‹ï¼Œå®ƒå¢å¼ºäº†ä¸€èˆ¬GPTé£æ ¼çš„æ¶æ„ï¼ŒåŒ…æ‹¬ï¼ˆiï¼‰ä¸“ç”¨æ—¶é—´ç¼–ç å™¨ï¼Œå¯ä»¥åµŒå…¥ç»å¯¹æ—¶é—´æˆ³æˆ–äº‹ä»¶é—´å€¼ï¼Œï¼ˆiiï¼‰å­¦ä¹ ä½ç½®ç¼–ç å™¨ï¼Œå¯ä»¥ä¿ç•™ç›¸å¯¹é¡ºåºã€‚åœ¨å¤§å‹å·¥ä¸šæ•°æ®é›†ï¼ˆæ•°åƒä¸‡ç¬”äº¤æ˜“å’Œè¾…åŠ©äº‹ä»¶ï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFraudTransformerè¶…è¶Šäº†å››ä¸ªå¼ºå¤§çš„ç»å…¸åŸºçº¿ï¼ˆé€»è¾‘å›å½’ã€XGBoostå’ŒLightGBMï¼‰ï¼Œä»¥åŠçœç•¥äº†æ—¶é—´æˆ–ä½ç½®ç»„ä»¶çš„è½¬æ¢å™¨æ¶ˆèæ¨¡å‹ã€‚åœ¨ä¿ç•™çš„æµ‹è¯•é›†ä¸Šï¼Œå®ƒè¾¾åˆ°äº†æœ€é«˜çš„AUROCå’ŒPRAUCã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23712v1">PDF</a> Pre-print</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†FraudTransformeræ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†äº‹ä»¶é¡ºåºå’Œäº‹ä»¶é—´çš„ä¸è§„åˆ™æ—¶é—´é—´éš”ï¼Œç”¨äºæ£€æµ‹çœŸå®é“¶è¡Œæµä¸­çš„æ¬ºè¯ˆè¡Œä¸ºã€‚æ¨¡å‹é‡‡ç”¨GPTé£æ ¼çš„æ¶æ„ï¼Œå¹¶é…å¤‡äº†ä¸“é—¨çš„æ—¶é—´ç¼–ç å™¨å’Œå­¦ä¹ çš„ä½ç½®ç¼–ç å™¨ã€‚å®éªŒè¯æ˜ï¼ŒFraudTransformeråœ¨å¤§å‹å·¥ä¸šæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†å››ä¸ªå¼ºå¤§çš„ç»å…¸åŸºçº¿ä»¥åŠçœç•¥äº†æ—¶é—´æˆ–ä½ç½®ç»„ä»¶çš„è½¬æ¢å™¨ã€‚å®ƒåœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº†æœ€é«˜çš„AUROCå’ŒPRAUCã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FraudTransformeræ¨¡å‹ç”¨äºæ£€æµ‹çœŸå®é“¶è¡Œæµä¸­çš„æ”¯ä»˜æ¬ºè¯ˆã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†äº‹ä»¶é¡ºåºå’Œäº‹ä»¶é—´æ—¶é—´é—´éš”çš„ä¿¡æ¯ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨GPTé£æ ¼æ¶æ„ï¼Œå¹¶é…å¤‡äº†æ—¶é—´ç¼–ç å™¨å’Œä½ç½®ç¼–ç å™¨ã€‚</li>
<li>æ—¶é—´ç¼–ç å™¨èƒ½å¤ŸåµŒå…¥ç»å¯¹æ—¶é—´æˆ³æˆ–äº‹ä»¶é—´éš”å€¼ã€‚</li>
<li>ä½ç½®ç¼–ç å™¨èƒ½å¤Ÿä¿æŒç›¸å¯¹é¡ºåºã€‚</li>
<li>åœ¨å¤§å‹å·¥ä¸šæ•°æ®é›†ä¸Šï¼ŒFraudTransformerè¡¨ç°è¶…è¿‡å››ä¸ªå¼ºå¤§çš„ç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹å’Œçœç•¥äº†æŸäº›ç»„ä»¶çš„è½¬æ¢å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ce806eff730da83212fb56b2a9f392f" align="middle">
<img src="https://picx.zhimg.com/v2-6150184e30cfa2f37aa73377945c06eb" align="middle">
<img src="https://picx.zhimg.com/v2-8b45a1f445153155d0c581ac72911a99" align="middle">
<img src="https://picx.zhimg.com/v2-d73aefb68f566fac666013a344e2ed05.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-93cb44bdbf4381d392e7c2881b18dcd7.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  ReasoningBank Scaling Agent Self-Evolving with Reasoning Memory
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b33639e52b284510aea547ef9808e164" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  ReasoningBank Scaling Agent Self-Evolving with Reasoning Memory
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
