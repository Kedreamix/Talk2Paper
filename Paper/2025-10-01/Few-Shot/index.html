<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  VT-FSL Bridging Vision and Text with LLMs for Few-Shot Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-599a29349cceb911f8507014c3af8512~resize:0:q75.jpg?source=1f5c5e47&expiration=1759913369&auth_key=1759913369-0-0-f8b50c837132e53dfe1e0b8d1776d932&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-01-æ›´æ–°"><a href="#2025-10-01-æ›´æ–°" class="headerlink" title="2025-10-01 æ›´æ–°"></a>2025-10-01 æ›´æ–°</h1><h2 id="VT-FSL-Bridging-Vision-and-Text-with-LLMs-for-Few-Shot-Learning"><a href="#VT-FSL-Bridging-Vision-and-Text-with-LLMs-for-Few-Shot-Learning" class="headerlink" title="VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning"></a>VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning</h2><p><strong>Authors:Wenhao Li, Qiangchang Wang, Xianjing Meng, Zhibin Wu, Yilong Yin</strong></p>
<p>Few-shot learning (FSL) aims to recognize novel concepts from only a few labeled support samples. Recent studies enhance support features by incorporating additional semantic information or designing complex semantic fusion modules. However, they still suffer from hallucinating semantics that contradict the visual evidence due to the lack of grounding in actual instances, resulting in noisy guidance and costly corrections. To address these issues, we propose a novel framework, bridging Vision and Text with LLMs for Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts conditioned on Large Language Models (LLMs) and support images, seamlessly integrating them through a geometry-aware alignment. It mainly consists of Cross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment (CGA). Specifically, the CIP conditions an LLM on both class names and support images to generate precise class descriptions iteratively in a single structured reasoning pass. These descriptions not only enrich the semantic understanding of novel classes but also enable the zero-shot synthesis of semantically consistent images. The descriptions and synthetic images act respectively as complementary textual and visual prompts, providing high-level class semantics and low-level intra-class diversity to compensate for limited support data. Furthermore, the CGA jointly aligns the fused textual, support, and synthetic visual representations by minimizing the kernelized volume of the 3-dimensional parallelotope they span. It captures global and nonlinear relationships among all representations, enabling structured and consistent multimodal integration. The proposed VT-FSL method establishes new state-of-the-art performance across ten diverse benchmarks, including standard, cross-domain, and fine-grained few-shot learning scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL">https://github.com/peacelwh/VT-FSL</a>. </p>
<blockquote>
<p>å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰æ—¨åœ¨ä»ä»…æœ‰çš„å°‘é‡æ ‡è®°æ”¯æŒæ ·æœ¬ä¸­è¯†åˆ«æ–°æ¦‚å¿µã€‚æœ€è¿‘çš„ç ”ç©¶é€šè¿‡èå…¥é¢å¤–çš„è¯­ä¹‰ä¿¡æ¯æˆ–è®¾è®¡å¤æ‚çš„è¯­ä¹‰èåˆæ¨¡å—æ¥å¢å¼ºæ”¯æŒç‰¹å¾ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹åœ¨å®é™…å®ä¾‹ä¸­çš„å®šä½ï¼Œå®ƒä»¬ä»ç„¶ä¼šé­å—ä¸è§†è§‰è¯æ®ç›¸çŸ›ç›¾çš„å¹»è§‰è¯­ä¹‰çš„å›°æ‰°ï¼Œå¯¼è‡´äº§ç”Ÿå˜ˆæ‚çš„æŒ‡å¯¼å’Œæ˜‚è´µçš„ä¿®æ­£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè§†è§‰å’Œæ–‡æœ¬æ¡¥æ¥çš„å°‘æ ·æœ¬å­¦ä¹ ï¼ˆVT-FSLï¼‰ï¼Œè¯¥æ¡†æ¶æ„å»ºäº†ç²¾ç¡®è·¨æ¨¡æ€æç¤ºï¼Œè¯¥æç¤ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ”¯æŒå›¾åƒï¼Œé€šè¿‡å‡ ä½•æ„ŸçŸ¥å¯¹é½æ— ç¼é›†æˆå®ƒä»¬ã€‚å®ƒä¸»è¦ç”±è·¨æ¨¡æ€è¿­ä»£æç¤ºï¼ˆCIPï¼‰å’Œè·¨æ¨¡æ€å‡ ä½•å¯¹é½ï¼ˆCGAï¼‰ç»„æˆã€‚å…·ä½“è€Œè¨€ï¼ŒCIPæ ¹æ®ç±»åˆ«åç§°å’Œæ”¯æŒå›¾åƒå¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œä»¥åœ¨å•ä¸ªç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ä¸­è¿­ä»£ç”Ÿæˆç²¾ç¡®ç±»åˆ«æè¿°ã€‚è¿™äº›æè¿°ä¸ä»…ä¸°å¯Œäº†å¯¹æ–°é¢–ç±»åˆ«çš„è¯­ä¹‰ç†è§£ï¼Œè¿˜å®ç°äº†è¯­ä¹‰ä¸€è‡´å›¾åƒçš„é›¶æ ·æœ¬åˆæˆã€‚è¿™äº›æè¿°å’Œåˆæˆå›¾åƒåˆ†åˆ«ä½œä¸ºè¡¥å……çš„æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œæä¾›é«˜çº§ç±»åˆ«è¯­ä¹‰å’Œä½çº§ç±»åˆ«å†…å¤šæ ·æ€§ï¼Œä»¥å¼¥è¡¥æœ‰é™çš„æ”¯æŒæ•°æ®ã€‚æ­¤å¤–ï¼ŒCGAé€šè¿‡æœ€å°åŒ–å®ƒä»¬æ‰€è·¨è¶Šçš„3Då¹³è¡Œå››è¾¹å½¢çš„æ ¸åŒ–ä½“ç§¯æ¥è”åˆå¯¹é½èåˆçš„æ–‡æœ¬ã€æ”¯æŒå’Œåˆæˆè§†è§‰è¡¨ç¤ºã€‚å®ƒæ•æ‰äº†æ‰€æœ‰è¡¨ç¤ºä¹‹é—´çš„å…¨å±€å’Œéçº¿æ€§å…³ç³»ï¼Œå®ç°äº†ç»“æ„åŒ–ä¸”ä¸€è‡´çš„å¤šæ¨¡æ€é›†æˆã€‚æ‰€æå‡ºçš„VT-FSLæ–¹æ³•åœ¨åŒ…æ‹¬æ ‡å‡†ã€è·¨åŸŸå’Œç»†ç²’åº¦å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯åœ¨å†…çš„åä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/peacelwh/VT-FSLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25033v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§æ–°çš„é¢å‘å°‘æ ·æœ¬å­¦ä¹ çš„è·¨æ¨¡æ€æ¡†æ¶VT-FSLï¼Œå®ƒç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ”¯æŒå›¾åƒç”Ÿæˆç²¾ç¡®è·¨æ¨¡æ€æç¤ºã€‚è¯¥æ¡†æ¶åŒ…æ‹¬è·¨æ¨¡æ€è¿­ä»£æç¤ºï¼ˆCIPï¼‰å’Œè·¨æ¨¡æ€å‡ ä½•å¯¹é½ï¼ˆCGAï¼‰ã€‚CIPé€šè¿‡ç»“åˆç±»åå’Œå›¾åƒç”Ÿæˆç²¾ç¡®ç±»æè¿°ï¼Œè€ŒCGAåˆ™è”åˆå¯¹é½æ–‡æœ¬ã€æ”¯æŒå›¾åƒå’Œåˆæˆè§†è§‰è¡¨ç¤ºï¼Œå®ç°ç»“æ„åŒ–ä¸€è‡´çš„å¤šæ¨¡æ€èåˆã€‚VT-FSLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VT-FSLæ˜¯ä¸€ä¸ªé¢å‘å°‘æ ·æœ¬å­¦ä¹ çš„è·¨æ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>å®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ”¯æŒå›¾åƒç”Ÿæˆç²¾ç¡®è·¨æ¨¡æ€æç¤ºã€‚</li>
<li>CIPé€šè¿‡ç»“åˆç±»åå’Œå›¾åƒç”Ÿæˆç²¾ç¡®çš„ç±»æè¿°ï¼Œæé«˜è¯­ä¹‰ç†è§£å¹¶åˆæˆè¯­ä¹‰ä¸€è‡´å›¾åƒã€‚</li>
<li>CGAè”åˆå¯¹é½æ–‡æœ¬ã€æ”¯æŒå›¾åƒå’Œåˆæˆè§†è§‰è¡¨ç¤ºï¼Œå®ç°ç»“æ„åŒ–ä¸€è‡´çš„å¤šæ¨¡æ€èåˆã€‚</li>
<li>VT-FSLæ¡†æ¶é€šè¿‡æœ€å°åŒ–ä»–ä»¬è·¨è¶Šçš„æ ¸åŒ–ä½“ç§¯çš„å¹³è¡Œå››è¾¹å½¢çš„ä½“ç§¯æ¥æ•æ‰æ‰€æœ‰è¡¨ç¤ºä¹‹é—´çš„å…¨å±€å’Œéçº¿æ€§å…³ç³»ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼ŒåŒ…æ‹¬æ ‡å‡†ã€è·¨åŸŸå’Œç»†ç²’åº¦å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8c6781d5aac0565d4a5da14d7fc5718e.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-9af705fa082c84d4e881514f8fff464d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759913383&auth_key=1759913383-0-0-bf68bbdc54e858698303feb21ab85301&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-fda6a8e6593952cbc22784b0fba62ed6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76bea1641940b6f14a37bc0429de3487.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a09d3087398134cf568e19fcf367318.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-5b1ec418aca878ee6ac090b3416189aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1759913410&auth_key=1759913410-0-0-da9c6cf4c9a72f769c5bc1498a08bf59&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MSG-Multi-Stream-Generative-Policies-for-Sample-Efficient-Robotic-Manipulation"><a href="#MSG-Multi-Stream-Generative-Policies-for-Sample-Efficient-Robotic-Manipulation" class="headerlink" title="MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic   Manipulation"></a>MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic   Manipulation</h2><p><strong>Authors:Jan Ole von Hartz, Lukas Schweizer, Joschka Boedecker, Abhinav Valada</strong></p>
<p>Generative robot policies such as Flow Matching offer flexible, multi-modal policy learning but are sample-inefficient. Although object-centric policies improve sample efficiency, it does not resolve this limitation. In this work, we propose Multi-Stream Generative Policy (MSG), an inference-time composition framework that trains multiple object-centric policies and combines them at inference to improve generalization and sample efficiency. MSG is model-agnostic and inference-only, hence widely applicable to various generative policies and training paradigms. We perform extensive experiments both in simulation and on a real robot, demonstrating that our approach learns high-quality generative policies from as few as five demonstrations, resulting in a 95% reduction in demonstrations, and improves policy performance by 89 percent compared to single-stream approaches. Furthermore, we present comprehensive ablation studies on various composition strategies and provide practical recommendations for deployment. Finally, MSG enables zero-shot object instance transfer. We make our code publicly available at <a target="_blank" rel="noopener" href="https://msg.cs.uni-freiburg.de/">https://msg.cs.uni-freiburg.de</a>. </p>
<blockquote>
<p>ç”Ÿæˆå¼æœºå™¨äººç­–ç•¥ï¼Œå¦‚æµåŒ¹é…ï¼ˆFlow Matchingï¼‰ï¼Œæä¾›äº†çµæ´»å¤šå˜æ¨¡æ€çš„ç­–ç•¥å­¦ä¹ ï¼Œä½†æ ·æœ¬æ•ˆç‡ä¸é«˜ã€‚å°½ç®¡ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ç­–ç•¥æé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œä½†å¹¶æ²¡æœ‰è§£å†³è¿™ä¸€å±€é™æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæµç”Ÿæˆç­–ç•¥ï¼ˆMSGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ¨ç†æ—¶é—´ç»„åˆæ¡†æ¶ï¼Œè®­ç»ƒå¤šä¸ªä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ç­–ç•¥ï¼Œå¹¶åœ¨æ¨ç†æ—¶å°†å®ƒä»¬ç»“åˆèµ·æ¥ï¼Œä»¥æé«˜æ³›åŒ–å’Œæ ·æœ¬æ•ˆç‡ã€‚MSGå…·æœ‰æ¨¡å‹æ— å…³æ€§å’Œä»…æ¨ç†æ€§ï¼Œå› æ­¤å¯å¹¿æ³›åº”ç”¨äºå„ç§ç”Ÿæˆç­–ç•¥å’Œè®­ç»ƒèŒƒå¼ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•èƒ½ä»ä»…äº”ä¸ªæ¼”ç¤ºä¸­å­¦ä¹ é«˜è´¨é‡ç”Ÿæˆç­–ç•¥ï¼Œæ¼”ç¤ºå‡å°‘äº†95%ï¼Œå¹¶ä¸”ç›¸è¾ƒäºå•æµæ–¹æ³•ï¼Œç­–ç•¥æ€§èƒ½æé«˜äº†89%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹å„ç§ç»„åˆç­–ç•¥è¿›è¡Œäº†å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œå¹¶ä¸ºéƒ¨ç½²æä¾›äº†å®ç”¨å»ºè®®ã€‚æœ€åï¼ŒMSGå®ç°äº†é›¶é•œå¤´å¯¹è±¡å®ä¾‹è¿ç§»ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://msg.cs.uni-freiburg.de/">https://msg.cs.uni-freiburg.de</a>å…¬å¼€äº†æˆ‘ä»¬çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24956v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æå‡ºä¸€ç§åä¸ºMulti-Stream Generative Policyï¼ˆMSGï¼‰çš„æ–°æ–¹æ³•ï¼Œå®ƒåœ¨æ¨ç†æ—¶ç»„åˆå¤šä¸ªå¯¹è±¡ä¸­å¿ƒç­–ç•¥ä»¥æé«˜æ³›åŒ–å’Œæ ·æœ¬æ•ˆç‡ã€‚ MSGæ˜¯æ¨¡å‹æ— å…³çš„ï¼Œä»…åœ¨æ¨ç†æ—¶ä½¿ç”¨ï¼Œå› æ­¤å¯å¹¿æ³›åº”ç”¨äºå„ç§ç”Ÿæˆç­–ç•¥å’Œè®­ç»ƒèŒƒå¼ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººä¸Šçš„è¡¨ç°å‡ä¼˜äºå•æµæ–¹æ³•ï¼Œä»äº”ä¸ªæ¼”ç¤ºä¸­å­¦ä¹ é«˜è´¨é‡ç”Ÿæˆç­–ç•¥ï¼Œæ¼”ç¤ºå‡å°‘95%ï¼Œæ€§èƒ½æé«˜89%ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›å„ç§ç»„åˆç­–ç•¥çš„å…¨é¢æ¶ˆèç ”ç©¶åŠå®é™…éƒ¨ç½²å»ºè®®ã€‚ MSGè¿˜æ”¯æŒé›¶å°„å‡»å¯¹è±¡å®ä¾‹è½¬ç§»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„Multi-Stream Generative Policyï¼ˆMSGï¼‰æ–¹æ³•ï¼Œç»“åˆäº†å¤šä¸ªå¯¹è±¡ä¸­å¿ƒç­–ç•¥ä»¥æé«˜æ³›åŒ–å’Œæ ·æœ¬æ•ˆç‡ã€‚</li>
<li>MSGæ–¹æ³•é€šè¿‡è®­ç»ƒå¤šä¸ªå¯¹è±¡ä¸­å¿ƒç­–ç•¥å¹¶åœ¨æ¨ç†æ—¶è¿›è¡Œç»„åˆï¼Œè§£å†³äº†ç”Ÿæˆç­–ç•¥æ ·æœ¬æ•ˆç‡ä½çš„é—®é¢˜ã€‚</li>
<li>MSGå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ï¼Œå¯ä»¥é€‚ç”¨äºå„ç§ç”Ÿæˆç­–ç•¥å’Œè®­ç»ƒèŒƒå¼ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMSGæ–¹æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººä¸Šçš„è¡¨ç°å‡ä¼˜äºå•æµæ–¹æ³•ã€‚</li>
<li>MSGæ–¹æ³•èƒ½å¤Ÿä»å°‘é‡çš„æ¼”ç¤ºä¸­å­¦ä¹ é«˜è´¨é‡çš„ç”Ÿæˆç­–ç•¥ï¼Œæ¼”ç¤ºæ•°é‡å‡å°‘95%ï¼Œå¹¶ä¸”æ€§èƒ½æå‡89%ã€‚</li>
<li>MSGæä¾›äº†å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œæ¢è®¨äº†ä¸åŒçš„ç»„åˆç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7659e66960c103f915e56a0a01a55cbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1759913418&auth_key=1759913418-0-0-cd139c4594318ff9fa210f592fa9e437&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-81596a897aca3e8db5f6635072e38e4f.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-a55af723b06c41c35585febb9d37fcbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1759913431&auth_key=1759913431-0-0-0e8c2a0a53381d7467b2483b69f018ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-51fc5f8a06fcbc81773bd1388dc9773b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759913438&auth_key=1759913438-0-0-115df0f3ef6520738836f5f0b5c280ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-12f2f0186bc320357c85cf980c04a7e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759913444&auth_key=1759913444-0-0-468330f03e59e95f2450829e5ec8c6f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-28f4c5d16325f4a76689d83a4983bc2b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759913451&auth_key=1759913451-0-0-420a74551518442d5a1d11a3deeb06e3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Metaphor-identification-using-large-language-models-A-comparison-of-RAG-prompt-engineering-and-fine-tuning"><a href="#Metaphor-identification-using-large-language-models-A-comparison-of-RAG-prompt-engineering-and-fine-tuning" class="headerlink" title="Metaphor identification using large language models: A comparison of   RAG, prompt engineering, and fine-tuning"></a>Metaphor identification using large language models: A comparison of   RAG, prompt engineering, and fine-tuning</h2><p><strong>Authors:Matteo Fuoli, Weihang Huang, Jeannette Littlemore, Sarah Turner, Ellen Wilding</strong></p>
<p>Metaphor is a pervasive feature of discourse and a powerful lens for examining cognition, emotion, and ideology. Large-scale analysis, however, has been constrained by the need for manual annotation due to the context-sensitive nature of metaphor. This study investigates the potential of large language models (LLMs) to automate metaphor identification in full texts. We compare three methods: (i) retrieval-augmented generation (RAG), where the model is provided with a codebook and instructed to annotate texts based on its rules and examples; (ii) prompt engineering, where we design task-specific verbal instructions; and (iii) fine-tuning, where the model is trained on hand-coded texts to optimize performance. Within prompt engineering, we test zero-shot, few-shot, and chain-of-thought strategies. Our results show that state-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning yielding a median F1 score of 0.79. A comparison of human and LLM outputs reveals that most discrepancies are systematic, reflecting well-known grey areas and conceptual challenges in metaphor theory. We propose that LLMs can be used to at least partly automate metaphor identification and can serve as a testbed for developing and refining metaphor identification protocols and the theory that underpins them. </p>
<blockquote>
<p>éšå–»æ˜¯è¯è¯­çš„æ™®éç‰¹å¾å’Œå¼ºå¤§çš„è®¤çŸ¥ã€æƒ…æ„Ÿå’Œæ„è¯†å½¢æ€ç ”ç©¶å·¥å…·ã€‚ç„¶è€Œï¼Œç”±äºéšå–»çš„è¯­å¢ƒæ•æ„Ÿæ€§ï¼Œå¤§è§„æ¨¡åˆ†æä¸€ç›´å—åˆ°éœ€è¦æ‰‹åŠ¨æ³¨é‡Šçš„é™åˆ¶ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…¨æ–‡è‡ªåŠ¨éšå–»è¯†åˆ«ä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸‰ç§æ–¹æ³•ï¼šï¼ˆiï¼‰æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œè¯¥æ–¹æ³•ä¸ºæ¨¡å‹æä¾›ä¸€ä¸ªä»£ç æœ¬ï¼Œå¹¶æ ¹æ®å…¶è§„åˆ™å’Œç¤ºä¾‹è¿›è¡Œæ–‡æœ¬æ³¨é‡Šï¼›ï¼ˆiiï¼‰æŒ‡ä»¤å·¥ç¨‹ï¼Œæˆ‘ä»¬è®¾è®¡é’ˆå¯¹ä»»åŠ¡çš„ç‰¹å®šå£å¤´æŒ‡ä»¤ï¼›ï¼ˆiiiï¼‰å¾®è°ƒï¼Œè¯¥æ–¹æ³•å°†æ¨¡å‹è®­ç»ƒåœ¨æ‰‹å·¥ç¼–ç çš„æ–‡æœ¬ä¸Šï¼Œä»¥ä¼˜åŒ–æ€§èƒ½ã€‚åœ¨æŒ‡ä»¤å·¥ç¨‹ä¸­ï¼Œæˆ‘ä»¬æµ‹è¯•äº†é›¶æ ·æœ¬ã€å°æ ·æœ¬å’Œé“¾å¼æ€ç»´ç­–ç•¥ã€‚ç»“æœè¡¨æ˜ï¼Œæœ€æ–°çš„å°é—­æºä»£ç LLMå¯ä»¥è¾¾åˆ°é«˜å‡†ç¡®ç‡ï¼Œå¾®è°ƒåçš„ä¸­ä½æ•°F1åˆ†æ•°ä¸º0.79ã€‚å¯¹æ¯”äººç±»å’ŒLLMçš„è¾“å‡ºç»“æœï¼Œå‘ç°å¤§éƒ¨åˆ†å·®å¼‚å…·æœ‰ç³»ç»Ÿæ€§ï¼Œåæ˜ äº†éšå–»ç†è®ºä¸­çš„ç°è‰²åœ°å¸¦å’Œæ¦‚å¿µæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºï¼Œå¯ä»¥åˆ©ç”¨LLMè‡³å°‘éƒ¨åˆ†è‡ªåŠ¨è¿›è¡Œéšå–»è¯†åˆ«ï¼Œå¹¶å¯ä»¥ä½œä¸ºå¼€å‘å’Œä¼˜åŒ–éšå–»è¯†åˆ«åè®®åŠå…¶ç†è®ºåŸºç¡€çš„æµ‹è¯•å¹³å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24866v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨è¯†åˆ«æ–‡æœ¬ä¸­çš„éšå–»æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶æ¯”è¾ƒäº†ä¸‰ç§æ–¹æ³•ï¼šåŸºäºè§„åˆ™åº“çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ã€åŸºäºç‰¹å®šä»»åŠ¡æç¤ºçš„è®¾è®¡æ–¹æ³•å’Œé€šè¿‡å¾®è°ƒæ¨¡å‹ä»¥æé«˜æ€§èƒ½çš„æ–¹æ³•ã€‚ç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„è¯­è¨€æ¨¡å‹å¯ä»¥è¾¾åˆ°è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œå…¶ä¸­ä½F1åˆ†æ•°ä¸º0.79ã€‚å¯¹æ¯”äººç±»ä¸è¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼Œå¤§éƒ¨åˆ†å·®å¼‚åœ¨äºéšå–»ç†è®ºä¸­çš„ç°è‰²åœ°å¸¦å’Œæ¦‚å¿µæŒ‘æˆ˜ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œè¯­è¨€æ¨¡å‹è‡³å°‘å¯ä»¥éƒ¨åˆ†åœ°è‡ªåŠ¨è¿›è¡Œéšå–»è¯†åˆ«ï¼Œå¹¶å¯ä½œä¸ºå¼€å‘å’Œä¼˜åŒ–éšå–»è¯†åˆ«åè®®åŠå…¶ç†è®ºåŸºç¡€çš„æµ‹è¯•å¹³å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨éšå–»è¯†åˆ«æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶æ¯”è¾ƒäº†ä¸‰ç§éšå–»è¯†åˆ«æ–¹æ³•ï¼šåŸºäºè§„åˆ™åº“çš„æ£€ç´¢å¢å¼ºç”Ÿæˆã€åŸºäºä»»åŠ¡æç¤ºçš„å·¥ç¨‹è®¾è®¡å’Œå¾®è°ƒæ¨¡å‹ã€‚</li>
<li>å¾®è°ƒæ¨¡å‹çš„æ–¹æ³•å–å¾—äº†æœ€é«˜çš„å‡†ç¡®æ€§ï¼Œä¸­ä½F1åˆ†æ•°ä¸º0.79ã€‚</li>
<li>äººä¸è¯­è¨€æ¨¡å‹çš„è¾“å‡ºå·®å¼‚ä¸»è¦æºäºéšå–»ç†è®ºä¸­çš„ç°è‰²åœ°å¸¦å’Œæ¦‚å¿µæŒ‘æˆ˜ã€‚</li>
<li>è¯­è¨€æ¨¡å‹å¯éƒ¨åˆ†è‡ªåŠ¨è¿›è¡Œéšå–»è¯†åˆ«ã€‚</li>
<li>è¯­è¨€æ¨¡å‹å¯ä½œä¸ºå¼€å‘å’Œä¼˜åŒ–éšå–»è¯†åˆ«åè®®åŠå…¶ç†è®ºåŸºç¡€çš„æµ‹è¯•å¹³å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24866">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-297c0ebf7ae532ac2ac5f5a046044390.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07b957e55b8999436504096385631f1d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LEAF-A-Robust-Expert-Based-Framework-for-Few-Shot-Continual-Event-Detection"><a href="#LEAF-A-Robust-Expert-Based-Framework-for-Few-Shot-Continual-Event-Detection" class="headerlink" title="LEAF: A Robust Expert-Based Framework for Few-Shot Continual Event   Detection"></a>LEAF: A Robust Expert-Based Framework for Few-Shot Continual Event   Detection</h2><p><strong>Authors:Bao-Ngoc Dao, Quang Nguyen, Luyen Ngo Dinh, Minh Le, Linh Ngo Van</strong></p>
<p>Few-shot Continual Event Detection (FCED) poses the dual challenges of learning from limited data and mitigating catastrophic forgetting across sequential tasks. Existing approaches often suffer from severe forgetting due to the full fine-tuning of a shared base model, which leads to knowledge interference between tasks. Moreover, they frequently rely on data augmentation strategies that can introduce unnatural or semantically distorted inputs. To address these limitations, we propose LEAF, a novel and robust expert-based framework for FCED. LEAF integrates a specialized mixture of experts architecture into the base model, where each expert is parameterized with low-rank adaptation (LoRA) matrices. A semantic-aware expert selection mechanism dynamically routes instances to the most relevant experts, enabling expert specialization and reducing knowledge interference. To improve generalization in limited-data settings, LEAF incorporates a contrastive learning objective guided by label descriptions, which capture high-level semantic information about event types. Furthermore, to prevent overfitting on the memory buffer, our framework employs a knowledge distillation strategy that transfers knowledge from previous models to the current one. Extensive experiments on multiple FCED benchmarks demonstrate that LEAF consistently achieves state-of-the-art performance. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬æŒç»­äº‹ä»¶æ£€æµ‹ï¼ˆFCEDï¼‰é¢ä¸´ç€ä»æœ‰é™æ•°æ®ä¸­å­¦ä¹ å’Œç¼“è§£é¡ºåºä»»åŠ¡ä¸­çš„ç¾éš¾æ€§é—å¿˜çš„åŒé‡æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸å› ä¸ºå…±äº«åŸºç¡€æ¨¡å‹çš„å…¨é¢å¾®è°ƒè€Œé­å—ä¸¥é‡çš„é—å¿˜é—®é¢˜ï¼Œè¿™å¯¼è‡´ä»»åŠ¡ä¹‹é—´çš„çŸ¥è¯†å¹²æ‰°ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ç»å¸¸ä¾èµ–æ•°æ®å¢å¼ºç­–ç•¥ï¼Œè¿™å¯èƒ½å¼•å…¥ä¸è‡ªç„¶æˆ–è¯­ä¹‰æ‰­æ›²çš„è¾“å…¥ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†LEAFï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹FCEDçš„æ–°å‹ç¨³å¥çš„ä¸“å®¶åŸºç¡€æ¡†æ¶ã€‚LEAFå°†ä¸“ä¸šåŒ–çš„æ··åˆä¸“å®¶æ¶æ„é›†æˆåˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼Œå…¶ä¸­æ¯ä¸ªä¸“å®¶éƒ½ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰çŸ©é˜µè¿›è¡Œå‚æ•°åŒ–ã€‚è¯­ä¹‰æ„ŸçŸ¥çš„ä¸“å®¶é€‰æ‹©æœºåˆ¶åŠ¨æ€åœ°å°†å®ä¾‹è·¯ç”±åˆ°æœ€ç›¸å…³çš„ä¸“å®¶ï¼Œå®ç°ä¸“å®¶ä¸“ä¸šåŒ–å¹¶å‡å°‘çŸ¥è¯†å¹²æ‰°ã€‚ä¸ºäº†æé«˜æœ‰é™æ•°æ®ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼ŒLEAFé‡‡ç”¨äº†ç”±æ ‡ç­¾æè¿°å¼•å¯¼çš„å¯¹æ¯”å­¦ä¹ ç›®æ ‡ï¼Œæ•è·æœ‰å…³äº‹ä»¶ç±»å‹çš„é«˜çº§è¯­ä¹‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†é˜²æ­¢å¯¹å†…å­˜ç¼“å†²åŒºçš„è¿‡åº¦æ‹Ÿåˆï¼Œæˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨çŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œå°†å…ˆå‰æ¨¡å‹çš„çŸ¥è¯†è½¬ç§»åˆ°å½“å‰æ¨¡å‹ä¸­ã€‚åœ¨å¤šä¸ªFCEDåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLEAFå§‹ç»ˆå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24547v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹Few-Shot Continual Event Detectionï¼ˆFCEDï¼‰ä»»åŠ¡çš„æ–°å‹ç¨³å¥ä¸“å®¶æ¡†æ¶LEAFã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆç‰¹å®šä¸“å®¶æ¨¡å‹å’Œè¯­ä¹‰æ„ŸçŸ¥çš„ä¸“å®¶é€‰æ‹©æœºåˆ¶ï¼Œè§£å†³äº†æœ‰é™æ•°æ®å­¦ä¹ å’Œé¡ºåºä»»åŠ¡ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚æ­¤å¤–ï¼Œå®ƒç»“åˆäº†ä½ç§©é€‚åº”çŸ©é˜µå‚æ•°åŒ–çš„ä¸“å®¶æ¨¡å‹å’Œå¯¹æ¯”å­¦ä¹ æŸå¤±æ¥æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨çŸ¥è¯†è’¸é¦ç­–ç•¥é˜²æ­¢å¯¹å†…å­˜ç¼“å†²åŒºçš„è¿‡åº¦æ‹Ÿåˆã€‚å®éªŒè¯æ˜ï¼ŒLEAFåœ¨å¤šä¸ªFCEDåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LEAFæ˜¯ä¸€ä¸ªé’ˆå¯¹Few-Shot Continual Event Detectionï¼ˆFCEDï¼‰çš„æ–°å‹ä¸“å®¶æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœ‰é™æ•°æ®å­¦ä¹ å’Œä»»åŠ¡è¿ç»­æ€§ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</li>
<li>LEAFé€šè¿‡é›†æˆç‰¹å®šä¸“å®¶æ¨¡å‹å’Œè¯­ä¹‰æ„ŸçŸ¥çš„ä¸“å®¶é€‰æ‹©æœºåˆ¶æ¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¯ä¸ªä¸“å®¶æ¨¡å‹é€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰çŸ©é˜µè¿›è¡Œå‚æ•°åŒ–ï¼Œæœ‰åŠ©äºå‡å°‘çŸ¥è¯†å¹²æ‰°å’Œæé«˜æ¨¡å‹é€‚åº”æ€§ã€‚</li>
<li>LEAFåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±æ¥æé«˜åœ¨æœ‰é™æ•°æ®ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡æ ‡ç­¾æè¿°æ•æ‰äº‹ä»¶ç±»å‹çš„é«˜çº§è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>LEAFé‡‡ç”¨çŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œé˜²æ­¢å¯¹å†…å­˜ç¼“å†²åŒºçš„è¿‡åº¦æ‹Ÿåˆï¼Œå¹¶æå‡æ¨¡å‹çš„æŒç»­å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒLEAFåœ¨å¤šä¸ªFCEDåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24547">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e6e3c53dce0c28b4d53a6595ecb852e.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-1f5cc28e69d01742125642e3b1300e44~resize:0:q75.jpg?source=1f5c5e47&expiration=1759913480&auth_key=1759913480-0-0-64eef39d7eb5fa85fc21d03f5c042fee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-6a85eb8d8de501438ecf6ce1fc93d30d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EVLF-FM-Explainable-Vision-Language-Foundation-Model-for-Medicine"><a href="#EVLF-FM-Explainable-Vision-Language-Foundation-Model-for-Medicine" class="headerlink" title="EVLF-FM: Explainable Vision Language Foundation Model for Medicine"></a>EVLF-FM: Explainable Vision Language Foundation Model for Medicine</h2><p><strong>Authors:Yang Bai, Haoran Cheng, Yang Zhou, Jun Zhou, Arun Thirunavukarasu, Yuhe Ke, Jie Yao, Kanae Fukutsu, Chrystie Wan Ning Quek, Ashley Hong, Laura Gutierrez, Zhen Ling Teo, Darren Shu Jeng Ting, Brian T. Soetikno, Christopher S. Nielsen, Tobias Elze, Zengxiang Li, Linh Le Dinh, Hiok Hong Chan, Victor Koh, Marcus Tan, Kelvin Z. Li, Leonard Yip, Ching Yu Cheng, Yih Chung Tham, Gavin Siew Wei Tan, Leopold Schmetterer, Marcus Ang, Rahat Hussain, Jod Mehta, Tin Aung, Lionel Tim-Ee Cheng, Tran Nguyen Tuan Anh, Chee Leong Cheng, Tien Yin Wong, Nan Liu, Iain Beehuat Tan, Soon Thye Lim, Eyal Klang, Tony Kiat Hon Lim, Rick Siow Mong Goh, Yong Liu, Daniel Shu Wei Ting</strong></p>
<p>Despite the promise of foundation models in medical AI, current systems remain limited - they are modality-specific and lack transparent reasoning processes, hindering clinical adoption. To address this gap, we present EVLF-FM, a multimodal vision-language foundation model (VLM) designed to unify broad diagnostic capability with fine-grain explainability. The development and testing of EVLF-FM encompassed over 1.3 million total samples from 23 global datasets across eleven imaging modalities related to six clinical specialties: dermatology, hepatology, ophthalmology, pathology, pulmonology, and radiology. External validation employed 8,884 independent test samples from 10 additional datasets across five imaging modalities. Technically, EVLF-FM is developed to assist with multiple disease diagnosis and visual question answering with pixel-level visual grounding and reasoning capabilities. In internal validation for disease diagnostics, EVLF-FM achieved the highest average accuracy (0.858) and F1-score (0.797), outperforming leading generalist and specialist models. In medical visual grounding, EVLF-FM also achieved stellar performance across nine modalities with average mIOU of 0.743 and <a href="mailto:&#65;&#99;&#x63;&#64;&#x30;&#x2e;&#53;">&#65;&#99;&#x63;&#64;&#x30;&#x2e;&#53;</a> of 0.837. External validations further confirmed strong zero-shot and few-shot performance, with competitive F1-scores despite a smaller model size. Through a hybrid training strategy combining supervised and visual reinforcement fine-tuning, EVLF-FM not only achieves state-of-the-art accuracy but also exhibits step-by-step reasoning, aligning outputs with visual evidence. EVLF-FM is an early multi-disease VLM model with explainability and reasoning capabilities that could advance adoption of and trust in foundation models for real-world clinical deployment. </p>
<blockquote>
<p>å°½ç®¡åŒ»ç–—äººå·¥æ™ºèƒ½ä¸­çš„åŸºç¡€æ¨¡å‹å‰æ™¯å¹¿é˜”ï¼Œä½†å½“å‰ç³»ç»Ÿä»å­˜åœ¨å±€é™æ€§ï¼Œå®ƒä»¬å…·æœ‰ç‰¹å®šçš„æ¨¡æ€ï¼Œç¼ºä¹é€æ˜çš„æ¨ç†è¿‡ç¨‹ï¼Œé˜»ç¢äº†å…¶åœ¨ä¸´åºŠä¸Šçš„é‡‡ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EVLF-FMï¼Œè¿™æ˜¯ä¸€æ¬¾å¤šæ¨¡æ€è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œæ—¨åœ¨å°†å¹¿æ³›çš„è¯Šæ–­èƒ½åŠ›ä¸ç²¾ç»†çš„é¢—ç²’è§£é‡Šæ€§ç»“åˆèµ·æ¥ã€‚EVLF-FMçš„å¼€å‘å’Œæµ‹è¯•æ¶µç›–äº†æ¥è‡ªå…¨çƒ23ä¸ªæ•°æ®é›†çš„è¶…è¿‡130ä¸‡æ ·æœ¬ï¼Œæ¶‰åŠä¸å…­ä¸ªä¸´åºŠä¸“ä¸šç›¸å…³çš„åä¸€ç§æˆåƒæ¨¡æ€ï¼ŒåŒ…æ‹¬çš®è‚¤ç§‘ã€è‚ç—…ç§‘ã€çœ¼ç§‘ã€ç—…ç†å­¦ã€è‚ºç—…å­¦å’Œæ”¾å°„å­¦ã€‚å¤–éƒ¨éªŒè¯é‡‡ç”¨äº†æ¥è‡ªäº”ä¸ªæˆåƒæ¨¡æ€çš„å¦å¤–åä¸ªæ•°æ®é›†çš„8884ä¸ªç‹¬ç«‹æµ‹è¯•æ ·æœ¬ã€‚åœ¨æŠ€æœ¯ä¸Šï¼ŒEVLF-FMçš„å¼€å‘æ—¨åœ¨ååŠ©å…·æœ‰åƒç´ çº§è§†è§‰å®šä½å’Œæ¨ç†èƒ½åŠ›çš„å¤šç§ç–¾ç—…è¯Šæ–­å’Œè§†è§‰é—®ç­”ã€‚åœ¨å†…éƒ¨éªŒè¯ç–¾ç—…è¯Šæ–­ä¸­ï¼ŒEVLF-FMè·å¾—äº†æœ€é«˜çš„å¹³å‡å‡†ç¡®åº¦ï¼ˆ0.858ï¼‰å’ŒF1åˆ†æ•°ï¼ˆ0.797ï¼‰ï¼Œä¼˜äºé¢†å…ˆçš„ç»¼åˆæ¨¡å‹å’Œä¸“å®¶æ¨¡å‹ã€‚åœ¨åŒ»ç–—è§†è§‰å®šä½ä¸­ï¼ŒEVLF-FMåœ¨ä¹ä¸ªæ¨¡æ€ä¸Šçš„å¹³å‡mIOUä¸º0.743ï¼Œ<a href="mailto:&#x41;&#99;&#99;&#64;&#48;&#x2e;&#53;">&#x41;&#99;&#99;&#64;&#48;&#x2e;&#53;</a>ä¸º0.837ï¼Œè¡¨ç°å“è¶Šã€‚å¤–éƒ¨éªŒè¯è¿›ä¸€æ­¥è¯å®äº†å…¶é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬çš„å¼ºå¤§æ€§èƒ½ï¼Œå°½ç®¡æ¨¡å‹è§„æ¨¡è¾ƒå°ï¼Œä½†F1åˆ†æ•°å…·æœ‰ç«äº‰åŠ›ã€‚é€šè¿‡ç»“åˆæœ‰ç›‘ç£å­¦ä¹ å’Œè§†è§‰å¼ºåŒ–ç²¾ç»†è°ƒæ•´çš„æ··åˆè®­ç»ƒç­–ç•¥ï¼ŒEVLF-FMä¸ä»…å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œè€Œä¸”å±•ç°å‡ºé€æ­¥æ¨ç†çš„èƒ½åŠ›ï¼Œä½¿è¾“å‡ºä¸è§†è§‰è¯æ®ä¿æŒä¸€è‡´ã€‚EVLF-FMæ˜¯ä¸€æ¬¾æ—©æœŸå…·æœ‰è§£é‡Šæ€§å’Œæ¨ç†èƒ½åŠ›çš„å¤šç–¾ç—…VLMæ¨¡å‹ï¼Œå¯ä»¥ä¿ƒè¿›åŸºç¡€æ¨¡å‹åœ¨å®é™…ä¸´åºŠéƒ¨ç½²ä¸­çš„åº”ç”¨å’Œä¿¡ä»»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24231v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»ç–—AIé¢†åŸŸçš„åŸºç¡€æ¨¡å‹å­˜åœ¨çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆEVLF-FMï¼‰ã€‚è¯¥æ¨¡å‹èåˆäº†å¹¿æ³›çš„è¯Šæ–­èƒ½åŠ›ä¸ç²¾ç»†çš„è§£é‡Šæ€§ï¼Œé€‚ç”¨äºå¤šç§ç–¾ç—…è¯Šæ–­ä¸è§†è§‰é—®ç­”ä»»åŠ¡ã€‚ç»è¿‡å…¨çƒ23ä¸ªæ•°æ®é›†çš„è®­ç»ƒä¸æµ‹è¯•ï¼Œå¹¶åœ¨ç‹¬ç«‹æµ‹è¯•æ ·æœ¬ä¸­éªŒè¯å…¶æ€§èƒ½ã€‚EVLF-FMåœ¨ç–¾ç—…è¯Šæ–­ä¸åŒ»ç–—è§†è§‰å®šä½ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œé€šè¿‡æ··åˆè®­ç»ƒç­–ç•¥å®ç°é«˜çº§å‡†ç¡®æ€§å’Œé€æ­¥æ¨ç†ï¼Œæœ‰åŠ©äºæ¨åŠ¨åŸºç¡€æ¨¡å‹åœ¨ä¸´åºŠå®é™…åº”ç”¨ä¸­çš„æ™®åŠä¸ä¿¡ä»»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰åŒ»ç–—AIåŸºç¡€æ¨¡å‹å­˜åœ¨å±€é™æ€§ï¼Œç¼ºä¹è·¨æ¨¡æ€çš„é€šç”¨æ€§å’Œé€æ˜æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>EVLF-FMæ˜¯ä¸€ç§å¤šæ¨¡æ€è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œæ—¨åœ¨ç»Ÿä¸€å¹¿æ³›çš„è¯Šæ–­èƒ½åŠ›å’Œç²¾ç»†çš„è§£é‡Šæ€§ã€‚</li>
<li>EVLF-FMç»è¿‡å…¨çƒ23ä¸ªæ•°æ®é›†çš„è®­ç»ƒå’Œæµ‹è¯•ï¼Œæ¶µç›–å…­ç§ä¸´åºŠä¸“ä¸šå’Œåä¸€ç§æˆåƒæ¨¡æ€ã€‚</li>
<li>åœ¨ç–¾ç—…è¯Šæ–­å’ŒåŒ»ç–—è§†è§‰å®šä½ä»»åŠ¡ä¸­ï¼ŒEVLF-FMæ€§èƒ½å“è¶Šï¼Œå¹³å‡å‡†ç¡®ç‡å’ŒF1åˆ†æ•°é¢†å…ˆã€‚</li>
<li>EVLF-FMé€šè¿‡æ··åˆè®­ç»ƒç­–ç•¥å®ç°é«˜çº§å‡†ç¡®æ€§å’Œé€æ­¥æ¨ç†ï¼Œå¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>EVLF-FMåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æƒ…å†µä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24231">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-769ddd39e423a88c28f07b6b48d0c713.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ASTROCO-Self-Supervised-Conformer-Style-Transformers-for-Light-Curve-Embeddings"><a href="#ASTROCO-Self-Supervised-Conformer-Style-Transformers-for-Light-Curve-Embeddings" class="headerlink" title="ASTROCO: Self-Supervised Conformer-Style Transformers for Light-Curve   Embeddings"></a>ASTROCO: Self-Supervised Conformer-Style Transformers for Light-Curve   Embeddings</h2><p><strong>Authors:Antony Tan, Pavlos Protopapas, Martina CÃ¡diz-Leyton, Guillermo Cabrera-Vives, Cristobal Donoso-Oliva, Ignacio Becker</strong></p>
<p>We present AstroCo, a Conformer-style encoder for irregular stellar light curves. By combining attention with depthwise convolutions and gating, AstroCo captures both global dependencies and local features. On MACHO R-band, AstroCo outperforms Astromer v1 and v2, yielding 70 percent and 61 percent lower error respectively and a relative macro-F1 gain of about 7 percent, while producing embeddings that transfer effectively to few-shot classification. These results highlight AstroCoâ€™s potential as a strong and label-efficient foundation for time-domain astronomy. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†AstroCoï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä¸è§„åˆ™æ’æ˜Ÿå…‰æ›²çº¿çš„Conformeré£æ ¼ç¼–ç å™¨ã€‚é€šè¿‡ç»“åˆæ³¨æ„åŠ›ã€æ·±åº¦å·ç§¯å’Œé—¨æ§æœºåˆ¶ï¼ŒAstroCoèƒ½å¤Ÿæ•æ‰å…¨å±€ä¾èµ–å…³ç³»å’Œå±€éƒ¨ç‰¹å¾ã€‚åœ¨MACHO Ræ³¢æ®µä¸Šï¼ŒAstroCoçš„è¡¨ç°ä¼˜äºAstromer v1å’Œv2ï¼Œåˆ†åˆ«é™ä½äº†70%å’Œ61%çš„é”™è¯¯ç‡ï¼Œç›¸å¯¹å®è§‚F1å¾—åˆ†æé«˜äº†çº¦7%ï¼ŒåŒæ—¶äº§ç”Ÿäº†æœ‰æ•ˆè¿ç§»è‡³å°æ ·æœ¬åˆ†ç±»çš„åµŒå…¥ã€‚è¿™äº›ç»“æœçªå‡ºäº†AstroCoä½œä¸ºæ—¶é—´åŸŸå¤©æ–‡å­¦å¼ºå¤§ä¸”æ ‡ç­¾æ•ˆç‡é«˜çš„åŸºç¡€çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24134v1">PDF</a> Accepted at the NeurIPS 2025 Workshop on Machine Learning and the   Physical Sciences (ML4PS), camera-ready version in progress</p>
<p><strong>Summary</strong></p>
<p>AstroCoæ˜¯ä¸€ç§åŸºäºConformerç¼–ç å™¨çš„æ’æ˜Ÿä¸è§„åˆ™å…‰æ›²çº¿æ¨¡å‹ã€‚é€šè¿‡ç»“åˆæ³¨æ„åŠ›æœºåˆ¶ã€æ·±åº¦å·ç§¯å’Œé—¨æ§æœºåˆ¶ï¼ŒAstroCoèƒ½å¤Ÿæ•æ‰å…¨å±€ä¾èµ–å…³ç³»å’Œå±€éƒ¨ç‰¹å¾ã€‚åœ¨MACHO Ræ³¢æ®µä¸Šï¼ŒAstroCoçš„è¡¨ç°ä¼˜äºAstromer v1å’Œv2ï¼Œåˆ†åˆ«é™ä½äº†70%å’Œ61%çš„é”™è¯¯ç‡ï¼Œç›¸å¯¹å®è§‚F1å¾—åˆ†æé«˜äº†çº¦7%ï¼ŒåŒæ—¶äº§ç”Ÿçš„åµŒå…¥èƒ½å¤Ÿæœ‰æ•ˆåœ°åº”ç”¨äºå°æ ·åˆ†ç±»ã€‚è¿™äº›ç»“æœçªæ˜¾äº†AstroCoä½œä¸ºæ—¶é—´åŸŸå¤©æ–‡å­¦å¼ºå¤§ä¸”æ ‡ç­¾æ•ˆç‡é«˜çš„åŸºç¡€çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AstroCoæ˜¯ä¸€ä¸ªç”¨äºä¸è§„åˆ™æ’æ˜Ÿå…‰æ›²çº¿çš„Conformeré£æ ¼ç¼–ç å™¨ã€‚</li>
<li>AstroCoç»“åˆæ³¨æ„åŠ›æœºåˆ¶ã€æ·±åº¦å·ç§¯å’Œé—¨æ§æœºåˆ¶ï¼Œæ•æ‰å…¨å±€ä¾èµ–å’Œå±€éƒ¨ç‰¹å¾ã€‚</li>
<li>åœ¨MACHO Ræ³¢æ®µä¸Šï¼ŒAstroCoè¡¨ç°ä¼˜äºAstromer v1å’Œv2ã€‚</li>
<li>AstroCoé™ä½äº†é”™è¯¯ç‡ï¼Œç›¸å¯¹å®è§‚F1å¾—åˆ†æœ‰æ‰€æé«˜ã€‚</li>
<li>AstroCoäº§ç”Ÿçš„åµŒå…¥é€‚ç”¨äºå°æ ·åˆ†ç±»ã€‚</li>
<li>AstroCoå…·æœ‰æ½œåŠ›æˆä¸ºæ—¶é—´åŸŸå¤©æ–‡å­¦å¼ºå¤§ä¸”æ ‡ç­¾æ•ˆç‡é«˜çš„åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-91cc5eb1ca8f38cfbd5da96c729e430e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759913502&auth_key=1759913502-0-0-61b1ba6ca3b6fa73ce31c399c7e71f85&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b378b5f2ad0cbeaf14fb97ba9cf04c9d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759913509&auth_key=1759913509-0-0-928f34cb2c53baa5720e383c6b90ec50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e4cfc6877a68d87a891ed09359df3999~resize:0:q75.jpg?source=1f5c5e47&expiration=1759913516&auth_key=1759913516-0-0-3f25892331cb94d24de74c3bdc4f0fb5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-deedba9d780a7a8de6de31b45b467d86~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927461&auth_key=1759927461-0-0-ccce1ef6ff96a71e9598c532e168e1d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-79d79a1de57b8d107536ca4a8dabe28c.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-36ca059a90063057a0062b29123ce291~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927474&auth_key=1759927474-0-0-86575f7d2e76b8bcebc4d7fdc0a2d7b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CE-FAM-Concept-Based-Explanation-via-Fusion-of-Activation-Maps"><a href="#CE-FAM-Concept-Based-Explanation-via-Fusion-of-Activation-Maps" class="headerlink" title="CE-FAM: Concept-Based Explanation via Fusion of Activation Maps"></a>CE-FAM: Concept-Based Explanation via Fusion of Activation Maps</h2><p><strong>Authors:Michihiro Kuroki, Toshihiko Yamasaki</strong></p>
<p>Although saliency maps can highlight important regions to explain the reasoning behind image classification in artificial intelligence (AI), the meaning of these regions is left to the userâ€™s interpretation. In contrast, conceptbased explanations decompose AI predictions into humanunderstandable concepts, clarifying their contributions. However, few methods can simultaneously reveal what concepts an image classifier learns, which regions are associated with them, and how they contribute to predictions. We propose a novel concept-based explanation method, Concept-based Explanation via Fusion of Activation Maps (CE-FAM). It employs a branched network that shares activation maps with an image classifier and learns to mimic the embeddings of a Vision and Language Model (VLM). The branch network predicts concepts in an image, and their corresponding regions are represented by a weighted sum of activation maps, with weights given by the gradients of the concept prediction scores. Their contributions are quantified based on their impact on the image classification score. Our method provides a general framework for identifying the concept regions and their contributions while leveraging VLM knowledge to handle arbitrary concepts without requiring an annotated dataset. Furthermore, we introduce a novel evaluation metric to assess the accuracy of the concept regions. Our qualitative and quantitative evaluations demonstrate our method outperforms existing approaches and excels in zero-shot inference for unseen concepts. </p>
<blockquote>
<p>è™½ç„¶æ˜¾è‘—æ€§å›¾å¯ä»¥çªå‡ºå¼ºè°ƒäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å›¾åƒåˆ†ç±»èƒŒåçš„é‡è¦åŒºåŸŸä»¥è§£é‡Šå…¶æ¨ç†è¿‡ç¨‹ï¼Œä½†è¿™äº›åŒºåŸŸçš„å«ä¹‰ä»ç•™ç»™ç”¨æˆ·è‡ªè¡Œè§£è¯»ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºæ¦‚å¿µçš„è§£é‡Šå°†AIé¢„æµ‹åˆ†è§£æˆäººç±»å¯ç†è§£çš„æ¦‚å¿µï¼Œæ¾„æ¸…äº†å®ƒä»¬çš„è´¡çŒ®ã€‚ç„¶è€Œï¼Œå¾ˆå°‘æœ‰æ–¹æ³•èƒ½åŒæ—¶æ­ç¤ºå›¾åƒåˆ†ç±»å™¨å­¦ä¹ äº†å“ªäº›æ¦‚å¿µï¼Œå“ªäº›åŒºåŸŸä¸ä¹‹ç›¸å…³ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•å¯¹é¢„æµ‹åšå‡ºè´¡çŒ®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¦‚å¿µè§£é‡Šæ–¹æ³•â€”â€”åŸºäºæ¿€æ´»å›¾çš„èåˆæ¦‚å¿µè§£é‡Šæ³•ï¼ˆCE-FAMï¼‰ã€‚å®ƒé‡‡ç”¨åˆ†æ”¯ç½‘ç»œï¼Œè¯¥ç½‘ç»œä¸å›¾åƒåˆ†ç±»å™¨å…±äº«æ¿€æ´»å›¾ï¼Œå¹¶å­¦ä¹ æ¨¡ä»¿è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„åµŒå…¥ã€‚åˆ†æ”¯ç½‘ç»œé¢„æµ‹å›¾åƒä¸­çš„æ¦‚å¿µï¼Œå…¶ç›¸åº”åŒºåŸŸç”±æ¿€æ´»å›¾çš„åŠ æƒå’Œè¡¨ç¤ºï¼Œæƒé‡ç”±æ¦‚å¿µé¢„æµ‹åˆ†æ•°çš„æ¢¯åº¦ç»™å‡ºã€‚å®ƒä»¬çš„è´¡çŒ®æ˜¯åŸºäºå¯¹å›¾åƒåˆ†ç±»åˆ†æ•°çš„å½±å“è¿›è¡Œé‡åŒ–çš„ã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶ï¼Œç”¨äºè¯†åˆ«æ¦‚å¿µåŒºåŸŸåŠå…¶è´¡çŒ®ï¼ŒåŒæ—¶åˆ©ç”¨VLMçš„çŸ¥è¯†æ¥å¤„ç†ä»»æ„æ¦‚å¿µï¼Œè€Œæ— éœ€æ ‡æ³¨æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥è¯„ä¼°æ¦‚å¿µåŒºåŸŸçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„å®šæ€§å’Œå®šé‡è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœªè§è¿‡çš„æ¦‚å¿µçš„é›¶æ ·æœ¬æ¨ç†æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23849v1">PDF</a> This paper has been accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¦‚å¿µçš„è§£é‡Šæ–¹æ³•CE-FAMï¼Œç”¨äºæ­ç¤ºå›¾åƒåˆ†ç±»å™¨å­¦ä¹ çš„æ¦‚å¿µã€å›¾åƒä¸­çš„ç›¸å…³åŒºåŸŸä»¥åŠè¿™äº›æ¦‚å¿µå¯¹é¢„æµ‹çš„è´¡çŒ®ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åˆ†æ”¯ç½‘ç»œå…±äº«æ¿€æ´»å›¾ï¼Œå¹¶æ¨¡ä»¿è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„åµŒå…¥ï¼Œé¢„æµ‹å›¾åƒä¸­çš„æ¦‚å¿µåŠå…¶å¯¹åº”åŒºåŸŸã€‚åˆ†æ”¯ç½‘ç»œé€šè¿‡åŠ æƒå’Œæ¿€æ´»å›¾è¡¨ç¤ºè¿™äº›åŒºåŸŸï¼Œæƒé‡ç”±æ¦‚å¿µé¢„æµ‹åˆ†æ•°çš„æ¢¯åº¦ç»™å‡ºã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡æ¥è¯„ä¼°æ¦‚å¿µåŒºåŸŸçš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åœ¨æœªè§è¿‡çš„æ¦‚å¿µä¸Šå®ç°äº†é›¶æ ·æœ¬æ¨ç†çš„ä¼˜å¼‚è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CE-FAMæ–¹æ³•ç»“åˆäº†æ¿€æ´»å›¾å’Œæ¦‚å¿µé¢„æµ‹æ¥è§£é‡Šå›¾åƒåˆ†ç±»å™¨çš„å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡åˆ†æ”¯ç½‘ç»œé¢„æµ‹å›¾åƒä¸­çš„æ¦‚å¿µåŠå…¶å¯¹åº”åŒºåŸŸï¼Œå¹¶åˆ©ç”¨æ¿€æ´»å›¾çš„åŠ æƒå’Œè¡¨ç¤ºè¿™äº›åŒºåŸŸã€‚</li>
<li>CE-FAMåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„çŸ¥è¯†æ¥å¤„ç†ä»»æ„æ¦‚å¿µï¼Œæ— éœ€æ ‡æ³¨æ•°æ®é›†ã€‚</li>
<li>æ–¹æ³•æä¾›äº†ä¸€ç§é€šç”¨æ¡†æ¶æ¥è¯†åˆ«æ¦‚å¿µåŒºåŸŸåŠå…¶è´¡çŒ®ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡æ¥è¯„ä¼°æ¦‚å¿µåŒºåŸŸçš„å‡†ç¡®æ€§ã€‚</li>
<li>CE-FAMåœ¨é›¶æ ·æœ¬æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨æœªè§è¿‡çš„æ¦‚å¿µä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e0176e36010b651f6dbf3f4f1d48d31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa551ed850f7b2db3d5be847f8ee5d26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9a708011f970512667762ffac485171.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-d4884e23c85984c1f527cab9dea0bd23~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927501&auth_key=1759927501-0-0-e76d7ac0e1c55dfa5fd7f59e9c7ebec5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-2bf204412f60886937d458092eb3b085.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Online-Specific-Emitter-Identification-via-Collision-Alleviated-Signal-Hash"><a href="#Online-Specific-Emitter-Identification-via-Collision-Alleviated-Signal-Hash" class="headerlink" title="Online Specific Emitter Identification via Collision-Alleviated Signal   Hash"></a>Online Specific Emitter Identification via Collision-Alleviated Signal   Hash</h2><p><strong>Authors:Hongyu Wang, Wenjia Xu, Guangzuo Li, Siyuan Wan, Yaohua Sun, Jiuniu Wang, Mugen Peng</strong></p>
<p>Specific Emitter Identification (SEI) has been widely studied, aiming to distinguish signals from different emitters given training samples from those emitters. However, real-world scenarios often require identifying signals from novel emitters previously unseen. Since these novel emitters only have a few or no prior samples, existing models struggle to identify signals from novel emitters online and tend to bias toward the distribution of seen emitters. To address these challenges, we propose the Online Specific Emitter Identification (OSEI) task, comprising both online \revise{few-shot and generalized zero-shot} learning tasks. It requires constructing models using signal samples from seen emitters and then identifying new samples from seen and novel emitters online during inference. We propose a novel hash-based model, Collision-Alleviated Signal Hash (CASH), providing a unified approach for addressing the OSEI task. The CASH operates in two steps: in the seen emitters identifying step, a signal encoder and a seen emitters identifier determine whether the signal sample is from seen emitters, mitigating the model from biasing toward seen emitters distribution. In the signal hash coding step, an online signal hasher assigns a hash code to each signal sample, identifying its specific emitter. Experimental results on real-world signal datasets (i.e., ADSB and ORACLE) demonstrate that our method accurately identifies signals from both seen and novel emitters online. This model outperforms existing methods by a minimum of 6.08% and 8.55% in accuracy for the few-shot and \revise{generalized zero-shot learning }tasks, respectively. The code will be open-sourced at \href{<a target="_blank" rel="noopener" href="https://github.com/IntelliSensing/OSEI-CASH%7D%7Bhttps://github.com/IntelliSensing/OSEI-CASH%7D">https://github.com/IntelliSensing/OSEI-CASH}{https://github.com/IntelliSensing/OSEI-CASH}</a>. </p>
<blockquote>
<p>ç‰¹å®šå‘å°„å™¨è¯†åˆ«ï¼ˆSEIï¼‰å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œå…¶ç›®æ ‡æ˜¯æ ¹æ®æ¥è‡ªè¿™äº›å‘å°„å™¨çš„è®­ç»ƒæ ·æœ¬åŒºåˆ†æ¥è‡ªä¸åŒå‘å°„å™¨çš„ä¿¡å·ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„æƒ…å†µé€šå¸¸éœ€è¦è¯†åˆ«æ¥è‡ªä»¥å‰æœªè§è¿‡çš„æ–°å‹å‘å°„å™¨çš„ä¿¡å·ã€‚ç”±äºè¿™äº›æ–°å‹å‘å°„å™¨åªæœ‰å°‘é‡æˆ–æ²¡æœ‰å…ˆéªŒæ ·æœ¬ï¼Œç°æœ‰æ¨¡å‹åœ¨åœ¨çº¿è¯†åˆ«æ–°å‹å‘å°„å™¨çš„ä¿¡å·æ—¶é‡åˆ°å›°éš¾ï¼Œå¹¶å¾€å¾€åå‘äºå·²è§å‘å°„å™¨çš„åˆ†å¸ƒã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åœ¨çº¿ç‰¹å®šå‘å°„å™¨è¯†åˆ«ï¼ˆOSEIï¼‰ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ¶µç›–äº†åœ¨çº¿çš„\revise{å°æ ·æœ¬å­¦ä¹ å’Œå¹¿ä¹‰é›¶æ ·æœ¬å­¦ä¹ }ã€‚å®ƒéœ€è¦ä½¿ç”¨æ¥è‡ªå·²è§å‘å°„å™¨çš„ä¿¡å·æ ·æœ¬æ„å»ºæ¨¡å‹ï¼Œç„¶ååœ¨æ¨æ–­æœŸé—´åœ¨çº¿è¯†åˆ«æ¥è‡ªå·²çŸ¥å’Œæ–°å‹å‘å°„å™¨çš„æ–°æ ·æœ¬ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå“ˆå¸Œçš„æ–°å‹æ¨¡å‹ï¼Œå³ç¢°æ’ç¼“è§£ä¿¡å·å“ˆå¸Œï¼ˆCASHï¼‰ï¼Œä¸ºOSEIä»»åŠ¡æä¾›äº†ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•ã€‚CASHåˆ†ä¸¤æ­¥æ“ä½œï¼šåœ¨å·²è§å‘å°„å™¨è¯†åˆ«æ­¥éª¤ä¸­ï¼Œä¿¡å·ç¼–ç å™¨å’Œå·²è§å‘å°„å™¨è¯†åˆ«å™¨ç¡®å®šä¿¡å·æ ·æœ¬æ˜¯å¦æ¥è‡ªå·²è§å‘å°„å™¨ï¼Œå‡è½»æ¨¡å‹åå‘äºå·²è§å‘å°„å™¨åˆ†å¸ƒçš„é—®é¢˜ã€‚åœ¨ä¿¡å·å“ˆå¸Œç¼–ç æ­¥éª¤ä¸­ï¼Œåœ¨çº¿ä¿¡å·å“ˆå¸Œå™¨ä¸ºæ¯ä¸ªä¿¡å·æ ·æœ¬åˆ†é…ä¸€ä¸ªå“ˆå¸Œä»£ç ï¼Œä»¥è¯†åˆ«å…¶ç‰¹å®šå‘å°„å™¨ã€‚åœ¨çœŸå®ä¸–ç•Œä¿¡å·æ•°æ®é›†ï¼ˆä¾‹å¦‚ADSBå’ŒORACLEï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡†ç¡®åœ°åœ¨çº¿è¯†åˆ«äº†æ¥è‡ªå·²çŸ¥å’Œæ–°å‹å‘å°„å™¨çš„ä¿¡å·ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨å°æ ·æœ¬å­¦ä¹ å’Œ\revise{å¹¿ä¹‰é›¶æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†è‡³å°‘6.08ï¼…å’Œ8.55ï¼…}ã€‚ä»£ç å°†åœ¨\href{<a target="_blank" rel="noopener" href="https://github.com/IntelliSensing/OSEI-CASH%7D%7Bhttps://github.com/IntelliSensing/OSEI-CASH%7D%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/IntelliSensing/OSEI-CASH}{https://github.com/IntelliSensing/OSEI-CASH}ä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23807v1">PDF</a> This paper has been accepted by IEEE Transactions on Vehicular   Technology</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åœ¨çº¿ç‰¹å®šå‘å°„å™¨è¯†åˆ«ï¼ˆOSEIï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³ä»å·²çŸ¥å’ŒæœªçŸ¥å‘å°„å™¨è¯†åˆ«ä¿¡å·çš„é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹åœ¨è¯†åˆ«æ¥è‡ªæ–°å‹å‘å°„å™¨çš„ä¿¡å·æ—¶å­˜åœ¨çš„åè§é—®é¢˜ï¼Œæå‡ºäº†åŸºäºå“ˆå¸Œçš„ç¢°æ’ç¼“è§£ä¿¡å·å“ˆå¸Œï¼ˆCASHï¼‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸¤ä¸ªæ­¥éª¤è¿›è¡Œå·¥ä½œï¼šé¦–å…ˆç¡®å®šä¿¡å·æ˜¯å¦æ¥è‡ªå·²çŸ¥å‘å°„å™¨ï¼Œç„¶åä¸ºæ¯ä¸ªä¿¡å·æ ·æœ¬åˆ†é…å“ˆå¸Œç ä»¥è¯†åˆ«å…¶ç‰¹å®šå‘å°„å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®ä¿¡å·æ•°æ®é›†ä¸Šèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«æ¥è‡ªå·²çŸ¥å’ŒæœªçŸ¥å‘å°„å™¨çš„ä¿¡å·ï¼Œå¹¶åœ¨å°‘æ ·æœ¬å’Œå¹¿ä¹‰é›¶æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸Šåˆ†åˆ«ä¼˜äºç°æœ‰æ–¹æ³•è‡³å°‘6.08%å’Œ8.55%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»‹ç»äº†åœ¨çº¿ç‰¹å®šå‘å°„å™¨è¯†åˆ«ï¼ˆOSEIï¼‰ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ—¨åœ¨è¯†åˆ«æ¥è‡ªå·²çŸ¥å’ŒæœªçŸ¥å‘å°„å™¨çš„ä¿¡å·ã€‚</li>
<li>æå‡ºäº†ç¢°æ’ç¼“è§£ä¿¡å·å“ˆå¸Œï¼ˆCASHï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡ä¸¤ä¸ªæ­¥éª¤è¿›è¡Œå·¥ä½œï¼šç¡®å®šä¿¡å·æ˜¯å¦æ¥è‡ªå·²çŸ¥å‘å°„å™¨å’Œä¸ºæ¯ä¸ªä¿¡å·æ ·æœ¬åˆ†é…å“ˆå¸Œç ä»¥è¯†åˆ«å…¶ç‰¹å®šå‘å°„å™¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCASHæ¨¡å‹åœ¨çœŸå®ä¿¡å·æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«æ¥è‡ªå·²çŸ¥å’ŒæœªçŸ¥å‘å°„å™¨çš„ä¿¡å·ã€‚</li>
<li>CASHæ¨¡å‹åœ¨å°‘æ ·æœ¬å’Œå¹¿ä¹‰é›¶æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡é«˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>CASHæ¨¡å‹çš„ç¼–ç å°†å¼€æºï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨ã€‚</li>
<li>è¯¥æ¨¡å‹æœ‰åŠ©äºè§£å†³ç°æœ‰æ¨¡å‹å¯¹å·²çŸ¥å‘å°„å™¨åˆ†å¸ƒçš„åè§é—®é¢˜ã€‚</li>
<li>CASHæ¨¡å‹é€šè¿‡ç»Ÿä¸€çš„æ–¹æ³•å¤„ç†OSEIä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23807">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ed04430e201123effcfcb23d2feb5747~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927515&auth_key=1759927515-0-0-a7a9fecaddccafd0ec014215695f999f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5bee77b29af412ef1a0734b717555796~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927522&auth_key=1759927522-0-0-67cffb4463bad841d65dbcd960d41e56&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-db7346319bed38b389c5237f15708408~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927529&auth_key=1759927529-0-0-6b4e3a3c83c896530c6a2b5c1f200398&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="How-to-Make-Large-Language-Models-Generate-100-Valid-Molecules"><a href="#How-to-Make-Large-Language-Models-Generate-100-Valid-Molecules" class="headerlink" title="How to Make Large Language Models Generate 100% Valid Molecules?"></a>How to Make Large Language Models Generate 100% Valid Molecules?</h2><p><strong>Authors:Wen Tao, Jing Tang, Alvin Chan, Bryan Hooi, Baolong Bi, Nanyun Peng, Yuansheng Liu, Yiwei Wang</strong></p>
<p>Molecule generation is key to drug discovery and materials science, enabling the design of novel compounds with specific properties. Large language models (LLMs) can learn to perform a wide range of tasks from just a few examples. However, generating valid molecules using representations like SMILES is challenging for LLMs in few-shot settings. In this work, we explore how LLMs can generate 100% valid molecules. We evaluate whether LLMs can use SELFIES, a representation where every string corresponds to a valid molecule, for valid molecule generation but find that LLMs perform worse with SELFIES than with SMILES. We then examine LLMsâ€™ ability to correct invalid SMILES and find their capacity limited. Finally, we introduce SmiSelf, a cross-chemical language framework for invalid SMILES correction. SmiSelf converts invalid SMILES to SELFIES using grammatical rules, leveraging SELFIESâ€™ mechanisms to correct the invalid SMILES. Experiments show that SmiSelf ensures 100% validity while preserving molecular characteristics and maintaining or even enhancing performance on other metrics. SmiSelf helps expand LLMsâ€™ practical applications in biomedicine and is compatible with all SMILES-based generative models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/wentao228/SmiSelf">https://github.com/wentao228/SmiSelf</a>. </p>
<blockquote>
<p>åˆ†å­ç”Ÿæˆå¯¹äºè¯ç‰©å‘ç°å’Œææ–™ç§‘å­¦è‡³å…³é‡è¦ï¼Œå®ƒèƒ½å¤Ÿä½¿äººä»¬è®¾è®¡å‡ºå…·æœ‰ç‰¹å®šå±æ€§çš„æ–°å‹åŒ–åˆç‰©ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åªéœ€è¦ä»å°‘é‡æ ·æœ¬ä¸­å­¦ä¹ ä¾¿å¯ä»¥æ‰§è¡Œå„ç§ä»»åŠ¡ã€‚ç„¶è€Œï¼Œåœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨SMILESç­‰è¡¨ç¤ºæ³•ç”Ÿæˆæœ‰æ•ˆçš„åˆ†å­å¯¹LLMsæ¥è¯´å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†LLMså¦‚ä½•ç”Ÿæˆ100%æœ‰æ•ˆçš„åˆ†å­ã€‚æˆ‘ä»¬è¯„ä¼°äº†LLMsæ˜¯å¦å¯ä»¥ä½¿ç”¨SELFIESï¼ˆä¸€ç§æ¯ä¸ªå­—ç¬¦ä¸²éƒ½å¯¹åº”ä¸€ä¸ªæœ‰æ•ˆåˆ†å­çš„è¡¨ç¤ºæ³•ï¼‰æ¥è¿›è¡Œæœ‰æ•ˆåˆ†å­ç”Ÿæˆï¼Œä½†å‘ç°LLMsåœ¨SELFIESä¸Šçš„è¡¨ç°ä¸å¦‚SMILESã€‚ç„¶åæˆ‘ä»¬ç ”ç©¶äº†LLMsçº æ­£æ— æ•ˆSMILESçš„èƒ½åŠ›ï¼Œå¹¶å‘ç°å…¶èƒ½åŠ›æœ‰é™ã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å‡ºäº†SmiSelfï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨åŒ–å­¦è¯­è¨€æ¡†æ¶ï¼Œç”¨äºçº æ­£æ— æ•ˆçš„SMILESã€‚SmiSelfä½¿ç”¨è¯­æ³•è§„åˆ™å°†æ— æ•ˆçš„SMILESè½¬æ¢ä¸ºSELFIESï¼Œåˆ©ç”¨SELFIESçš„æœºåˆ¶çº æ­£æ— æ•ˆçš„SMILESã€‚å®éªŒè¡¨æ˜ï¼ŒSmiSelfåœ¨ç¡®ä¿100%æœ‰æ•ˆæ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒåˆ†å­ç‰¹æ€§ï¼Œå¹¶åœ¨å…¶ä»–æŒ‡æ ‡ä¸Šç”šè‡³æé«˜æ€§èƒ½ã€‚SmiSelfæœ‰åŠ©äºæ‰©å¤§LLMsåœ¨ç”Ÿç‰©åŒ»å­¦ä¸­çš„å®é™…åº”ç”¨ï¼Œå¹¶ä¸”ä¸æ‰€æœ‰åŸºäºSMILESçš„ç”Ÿæˆæ¨¡å‹å…¼å®¹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wentao228/SmiSelf%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wentao228/SmiSelfæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23099v1">PDF</a> EMNLP 2025 Main</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åˆ†å­ç”Ÿæˆä¸­å…·æœ‰æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯ç‰©å‘ç°å’Œææ–™ç§‘å­¦é¢†åŸŸã€‚ç„¶è€Œï¼ŒLLMåœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸‹ç”ŸæˆSMILESè¡¨ç¤ºçš„æœ‰æ•ˆåˆ†å­å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†LLMå¦‚ä½•ç”Ÿæˆç™¾åˆ†ä¹‹ç™¾æœ‰æ•ˆåˆ†å­çš„é—®é¢˜ã€‚å°½ç®¡å°è¯•ä½¿ç”¨SELFIESè¡¨ç¤ºæ–¹æ³•ï¼Œä½†å‘ç°LLMçš„è¡¨ç°ä¸å¦‚SMILESã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†SmiSelfæ¡†æ¶ï¼Œç”¨äºæ ¡æ­£æ— æ•ˆçš„SMILESå¹¶å°†å…¶è½¬æ¢ä¸ºSELFIESè¡¨ç¤ºå½¢å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒSmiSelfç¡®ä¿äº†ç™¾åˆ†ä¹‹ç™¾çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä¿æŒäº†åˆ†å­ç‰¹æ€§ï¼Œå¹¶åœ¨å…¶ä»–æŒ‡æ ‡ä¸Šç»´æŒæˆ–æé«˜äº†æ€§èƒ½ã€‚è¿™ä¸ºLLMåœ¨ç”Ÿç‰©åŒ»å­¦ä¸­çš„å®é™…åº”ç”¨æä¾›äº†å¸®åŠ©ï¼Œå¹¶ä¸æ‰€æœ‰åŸºäºSMILESçš„ç”Ÿæˆæ¨¡å‹å…¼å®¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯ç‰©å‘ç°å’Œææ–™ç§‘å­¦ä¸­çš„åˆ†å­ç”Ÿæˆæ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>åœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸‹ï¼ŒLLMç”ŸæˆSMILESè¡¨ç¤ºçš„æœ‰æ•ˆåˆ†å­å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>å°è¯•ä½¿ç”¨SELFIESè¡¨ç¤ºæ–¹æ³•ç”Ÿæˆåˆ†å­ï¼Œä½†LLMè¡¨ç°ä¸ä½³ã€‚</li>
<li>å¼•å…¥SmiSelfæ¡†æ¶ç”¨äºæ ¡æ­£æ— æ•ˆçš„SMILESè¡¨ç¤ºå¹¶å°†å…¶è½¬æ¢ä¸ºSELFIESè¡¨ç¤ºå½¢å¼ã€‚</li>
<li>SmiSelfç¡®ä¿äº†ç™¾åˆ†ä¹‹ç™¾çš„æœ‰æ•ˆæ€§å¹¶ä¿è¯åˆ†å­ç‰¹æ€§å¾—åˆ°ä¿æŒã€‚</li>
<li>SmiSelfç»´æŒæˆ–æé«˜äº†åœ¨å…¶ä»–æŒ‡æ ‡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23099">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2a31db2e609c96ac40753645d9cfce1b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927537&auth_key=1759927537-0-0-c2cdd7090e7f05f7a6aee366b9406a6a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-ba1a875afccef58573be9fb653c41f95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e66215511f4361285da06c6cafed1392.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-fc39faf451d22c5198348e05fbcf45f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927558&auth_key=1759927558-0-0-930bbad7591728cf32095328ffb82473&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-01a6d50edf71ad8c28bc5bafc62a0a5c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927564&auth_key=1759927564-0-0-eb905b99aa0042f7e76bdaed3c5261d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a26d3baa7e53229e23c037345e06921a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927570&auth_key=1759927570-0-0-4297470ca916b39954df9bba7d22fe28&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="What-Matters-More-For-In-Context-Learning-under-Matched-Compute-Budgets-Pretraining-on-Natural-Text-or-Incorporating-Targeted-Synthetic-Examples"><a href="#What-Matters-More-For-In-Context-Learning-under-Matched-Compute-Budgets-Pretraining-on-Natural-Text-or-Incorporating-Targeted-Synthetic-Examples" class="headerlink" title="What Matters More For In-Context Learning under Matched Compute Budgets:   Pretraining on Natural Text or Incorporating Targeted Synthetic Examples?"></a>What Matters More For In-Context Learning under Matched Compute Budgets:   Pretraining on Natural Text or Incorporating Targeted Synthetic Examples?</h2><p><strong>Authors:Mohammed Sabry, Anya Belz</strong></p>
<p>Does explicitly exercising the induction circuit during pretraining improve in-context learning (ICL), or is natural text sufficient when compute is held constant (iso-FLOPs)? To test whether targeted synthetic data can accelerate induction-head emergence and enhance ICL, we introduce Bi-Induct, a lightweight curriculum that injects forward-copy (Induction), backward-copy (Anti), or a balanced mix into the pretraining stream. We train models from 0.13B to 1B parameters under iso-FLOPs, evaluating (i) few-shot ICL benchmarks, (ii) head-level telemetry, and (iii) held-out language modeling perplexity. Our findings challenge the assumption that early induction circuit activation directly improves ICL. While Bi-Induct accelerates induction-head emergence at small scales, this does not consistently yield stronger generalization. On standard LM benchmarks, Bi-Induct matches natural-only training; on function-style ICL probes, the 1B natural-only performs best. Stress tests (e.g., label permutation, HITS@1 vs. HITS@3, 1 vs. 10 shots) preserve these trends. Telemetry shows larger natural-only models develop broader, earlier induction heads without explicit induction patterns. Anti-induction data fails to elicit meaningful activation. Perplexity penalties from synthetic data shrink with scale, suggesting larger models can absorb non-natural patterns with minimal cost. Crucially, ablating the top 2% of induction heads degrades ICL more than random ablations, especially for natural-only models, indicating more centralized, load-bearing circuits. Bi-Induct variants exhibit more redundant induction activity, implying different circuit utilization. Overall, inducing activation is not sufficient: ICL gains depend on these circuits becoming functionally necessary. These results underscore mechanism-aware pretraining diagnostics and data mixtures that foster load-bearing, not merely present, structure. </p>
<blockquote>
<p>åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æ˜ç¡®é”»ç‚¼å½’çº³ç”µè·¯æ˜¯å¦ä¼šæ”¹å–„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼Œæˆ–è€…åœ¨ä¿æŒè®¡ç®—é‡ä¸å˜ï¼ˆiso-FLOPsï¼‰æ—¶è‡ªç„¶æ–‡æœ¬æ˜¯å¦è¶³å¤Ÿï¼Ÿä¸ºäº†æµ‹è¯•æœ‰é’ˆå¯¹æ€§çš„åˆæˆæ•°æ®æ˜¯å¦å¯ä»¥åŠ é€Ÿå½’çº³å¤´ï¼ˆinduction-headï¼‰çš„å‡ºç°å¹¶å¢å¼ºICLï¼Œæˆ‘ä»¬å¼•å…¥äº†Bi-Inductï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„è¯¾ç¨‹å­¦ä¹ æ³•ï¼Œå°†æ­£å‘å¤åˆ¶ï¼ˆå½’çº³ï¼‰ã€åå‘å¤åˆ¶ï¼ˆAntiï¼‰æˆ–å¹³è¡¡æ··åˆæ³¨å…¥é¢„è®­ç»ƒæµä¸­ã€‚æˆ‘ä»¬åœ¨iso-FLOPsæ¡ä»¶ä¸‹ï¼Œå¯¹ä»0.13Båˆ°1Bå‚æ•°çš„æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œè¯„ä¼°ï¼ˆiï¼‰å°‘é‡ICLåŸºå‡†æµ‹è¯•ï¼Œï¼ˆiiï¼‰å¤´éƒ¨çº§åˆ«é¥æµ‹ï¼Œä»¥åŠï¼ˆiiiï¼‰ä¿æŒçš„è¯­è¨€å»ºæ¨¡å›°æƒ‘åº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ—©æœŸå½’çº³ç”µè·¯æ¿€æ´»ç›´æ¥æ”¹å–„ICLçš„å‡è®¾ä¸æˆç«‹ã€‚è™½ç„¶Bi-Inductåœ¨å°å‹è§„æ¨¡ä¸ŠåŠ é€Ÿäº†å½’çº³å¤´çš„å‡ºç°ï¼Œä½†è¿™å¹¶ä¸ä¸€å®šä¼šäº§ç”Ÿæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è‡ªç„¶è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒBi-Inductçš„è¡¨ç°ä¸ä»…è‡ªç„¶è®­ç»ƒç›¸åŒ¹é…ï¼›åœ¨åŠŸèƒ½å‹ICLæ¢æµ‹ä¸­ï¼Œè§„æ¨¡ä¸º1Bçš„ä»…è‡ªç„¶è®­ç»ƒè¡¨ç°æœ€ä½³ã€‚åœ¨å‹åŠ›æµ‹è¯•ï¼ˆå¦‚æ ‡ç­¾ç½®æ¢ã€HITS@1ä¸HITS@3ã€1ä¸10æ¬¡å°„å‡»ï¼‰ä¸­ï¼Œè¿™äº›è¶‹åŠ¿ä¾æ—§ä¿æŒä¸å˜ã€‚é¥æµ‹æ˜¾ç¤ºï¼Œè§„æ¨¡è¾ƒå¤§çš„è‡ªç„¶æ¨¡å‹èƒ½å¤Ÿå¼€å‘å‡ºæ›´å¹¿æ³›ã€æ›´æ—©çš„å½’çº³å¤´è€Œæ²¡æœ‰æ˜æ˜¾çš„å½’çº³æ¨¡å¼ã€‚åå‘å½’çº³æ•°æ®æœªèƒ½å¼•å‘æœ‰æ„ä¹‰çš„æ¿€æ´»ã€‚æ¥è‡ªåˆæˆæ•°æ®çš„å›°æƒ‘åº¦æƒ©ç½šéšç€è§„æ¨¡çš„æ‰©å¤§è€Œç¼©å°ï¼Œè¿™è¡¨æ˜å¤§å‹æ¨¡å‹å¯ä»¥å¸æ”¶éè‡ªç„¶æ¨¡å¼è€Œä»£ä»·æå°ã€‚å…³é”®çš„æ˜¯ï¼Œæ¶ˆé™¤å‰2%çš„å½’çº³å¤´å¯¹ICLçš„ç ´åç¨‹åº¦è¶…è¿‡éšæœºæ¶ˆé™¤ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä»…è‡ªç„¶çš„æ¨¡å‹ï¼Œè¿™è¡¨æ˜æ›´ä¸ºé›†ä¸­ã€è´Ÿè½½æ‰¿è½½çš„ç”µè·¯ã€‚Bi-Inductå˜ä½“è¡¨ç°å‡ºæ›´å¤šçš„å†—ä½™å½’çº³æ´»åŠ¨ï¼Œæš—ç¤ºä¸åŒçš„ç”µè·¯åˆ©ç”¨æ–¹å¼ã€‚æ€»çš„æ¥è¯´ï¼Œè¯±å‘æ¿€æ´»å¹¶ä¸è¶³ä»¥ä¾èµ–ï¼šICLçš„å¢ç›Šå–å†³äºè¿™äº›ç”µè·¯æ˜¯å¦æˆä¸ºåŠŸèƒ½ä¸Šçš„å¿…éœ€å“ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†æœºåˆ¶æ„ŸçŸ¥çš„é¢„è®­ç»ƒè¯Šæ–­å’Œæ•°æ®æ··åˆç‰©ï¼Œå®ƒä»¬ä¿ƒè¿›äº†è´Ÿè½½æ‰¿è½½ï¼Œè€Œä¸ä»…ä»…æ˜¯ç°æœ‰çš„ç»“æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22947v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯å¦éœ€è¦é€šè¿‡æ˜ç¡®è®­ç»ƒå½’çº³ç”µè·¯æ¥æ”¹å–„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€‚é€šè¿‡å¼•å…¥Bi-Inductè¯¾ç¨‹ï¼Œå°†å½’çº³ï¼ˆInductionï¼‰å’Œåå‘å½’çº³ï¼ˆAntiï¼‰çš„å¹³è¡¡æ··åˆæ³¨å…¥é¢„è®­ç»ƒæµä¸­ï¼Œå¯¹æ¨¡å‹è¿›è¡Œæµ‹è¯•ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶Bi-Inductå¯ä»¥åŠ é€Ÿå°è§„æ¨¡å½’çº³å¤´çš„å‡ºç°ï¼Œä½†å¹¶ä¸æ€»æ˜¯å¯¼è‡´æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚å¯¹äºæ ‡å‡†LMåŸºå‡†æµ‹è¯•ï¼ŒBi-Inductä¸ä»…è‡ªç„¶è®­ç»ƒç›¸åŒ¹é…ï¼›è€Œåœ¨åŠŸèƒ½æ ·å¼çš„ICLæµ‹è¯•ä¸­ï¼Œè§„æ¨¡ä¸º1Bçš„è‡ªç„¶æ¨¡å‹è¡¨ç°æœ€ä½³ã€‚è¿™è¡¨æ˜ä»…ä»…æ¿€æ´»å½’çº³ç”µè·¯å¹¶ä¸è¶³ä»¥æé«˜ICLçš„æ•ˆæœï¼Œè€Œæ˜¯éœ€è¦è¿™äº›ç”µè·¯æˆä¸ºåŠŸèƒ½ä¸Šçš„å¿…è¦ç»„æˆéƒ¨åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Bi-Inductè¯¾ç¨‹æ—¨åœ¨é€šè¿‡æ³¨å…¥å½’çº³å’Œåå‘å½’çº³æ•°æ®æ¥åŠ é€Ÿå½’çº³ç”µè·¯çš„æ¿€æ´»ã€‚</li>
<li>åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ˜ç¡®è®­ç»ƒå½’çº³ç”µè·¯å¹¶ä¸æ€»æ˜¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¯¹äºæ ‡å‡†è¯­è¨€å»ºæ¨¡åŸºå‡†æµ‹è¯•ï¼ŒBi-Inductçš„è¡¨ç°ä¸ä»…ä½¿ç”¨è‡ªç„¶è®­ç»ƒæ•°æ®ç›¸å½“ã€‚</li>
<li>åœ¨ç‰¹å®šåŠŸèƒ½æ ·å¼çš„ICLæµ‹è¯•ä¸­ï¼Œè§„æ¨¡ä¸º1Bçš„è‡ªç„¶æ¨¡å‹è¡¨ç°æœ€ä½³ã€‚</li>
<li>å½’çº³ç”µè·¯çš„ä½œç”¨ä¸ä»…ä»…æ˜¯å‘ˆç°ç»“æ„ï¼Œè€Œæ˜¯éœ€è¦æˆä¸ºåŠŸèƒ½ä¸Šçš„å¿…è¦ç»„æˆéƒ¨åˆ†ã€‚</li>
<li>è¾ƒå¤§è§„æ¨¡çš„è‡ªç„¶æ¨¡å‹å¯ä»¥å¸æ”¶éè‡ªç„¶æ¨¡å¼ï¼Œä¸”ä»£ä»·è¾ƒå°ã€‚</li>
<li>åºŸé™¤å½’çº³å¤´å¯¹ICLçš„å½±å“æ›´å¤§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè‡ªç„¶æ¨¡å‹ï¼Œè¡¨æ˜ç”µè·¯åˆ©ç”¨æ›´åŠ é›†ä¸­å’Œæ‰¿è½½è´Ÿè½½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-13f56feef7fbb44933fab73900360f3f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927577&auth_key=1759927577-0-0-1f1d6bf1fbd53516d2f742b691849822&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-32074c9c10f3d2a144948b58a27a2d8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e80375c9d0c5fa3fe8c1927b822a8a82.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-ac587a8f38ea29bd95e87b66272d2456~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927598&auth_key=1759927598-0-0-0239dd603418db51dfa86b32f9809de9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-Generalizable-Implicit-In-Context-Learning-with-Attention-Routing"><a href="#Towards-Generalizable-Implicit-In-Context-Learning-with-Attention-Routing" class="headerlink" title="Towards Generalizable Implicit In-Context Learning with Attention   Routing"></a>Towards Generalizable Implicit In-Context Learning with Attention   Routing</h2><p><strong>Authors:Jiaqian Li, Yanshu Li, Ligong Han, Ruixiang Tang, Wenya Wang</strong></p>
<p>Implicit in-context learning (ICL) has newly emerged as a promising paradigm that simulates ICL behaviors in the representation space of Large Language Models (LLMs), aiming to attain few-shot performance at zero-shot cost. However, existing approaches largely rely on injecting shift vectors into residual flows, which are typically constructed from labeled demonstrations or task-specific alignment. Such designs fall short of utilizing the structural mechanisms underlying ICL and suffer from limited generalizability. To address this, we propose In-Context Routing (ICR), a novel implicit ICL method that internalizes generalizable ICL patterns at the attention logits level. It extracts reusable structural directions that emerge during ICL and employs a learnable input-conditioned router to modulate attention logits accordingly, enabling a train-once-and-reuse framework. We evaluate ICR on 12 real-world datasets spanning diverse domains and multiple LLMs. The results show that ICR consistently outperforms prior implicit ICL methods that require task-specific retrieval or training, while demonstrating robust generalization to out-of-domain tasks where existing methods struggle. These findings position ICR to push the boundary of ICLâ€™s practical value. </p>
<blockquote>
<p>éšå¼ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä½œä¸ºä¸€ç§æ–°å…´çš„æœ‰å‰é€”çš„èŒƒå¼ï¼Œæ¨¡æ‹Ÿäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¡¨ç¤ºç©ºé—´ä¸­çš„ICLè¡Œä¸ºï¼Œæ—¨åœ¨ä»¥é›¶æˆæœ¬è¾¾åˆ°å°æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–äºå‘æ®‹å·®æµæ³¨å…¥è¿ç§»å‘é‡ï¼Œè¿™é€šå¸¸æ˜¯ç”±å¸¦æ ‡ç­¾çš„æ¼”ç¤ºæˆ–ä»»åŠ¡ç‰¹å®šçš„å¯¹é½æ„å»ºçš„ã€‚è¿™ç§è®¾è®¡æœªèƒ½å……åˆ†åˆ©ç”¨ICLçš„å†…åœ¨ç»“æ„æœºåˆ¶ï¼Œä¸”å…¶é€šç”¨æ€§æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡è·¯ç”±ï¼ˆICRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„éšå¼ICLæ–¹æ³•ï¼Œåœ¨æ³¨æ„åŠ›å¾—åˆ†å±‚é¢å†…éƒ¨åŒ–å¯æ³›åŒ–çš„ICLæ¨¡å¼ã€‚å®ƒæå–äº†åœ¨ICLè¿‡ç¨‹ä¸­å‡ºç°çš„å¯é‡å¤ä½¿ç”¨çš„ç»“æ„æ–¹å‘ï¼Œå¹¶é‡‡ç”¨å¯å­¦ä¹ çš„è¾“å…¥æ¡ä»¶è·¯ç”±å™¨æ¥ç›¸åº”åœ°è°ƒæ•´æ³¨æ„åŠ›å¾—åˆ†ï¼Œä»è€Œå®ç°äº†ä¸€æ¬¡è®­ç»ƒã€å¤šæ¬¡å¤ç”¨çš„æ¡†æ¶ã€‚æˆ‘ä»¬åœ¨æ¶µç›–å¤šä¸ªé¢†åŸŸå’Œå¤šç§å¤§å‹è¯­è¨€æ¨¡å‹çš„12ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¯„ä¼°äº†ICRã€‚ç»“æœè¡¨æ˜ï¼ŒICRæŒç»­ä¼˜äºå…ˆå‰éœ€è¦ä»»åŠ¡ç‰¹å®šæ£€ç´¢æˆ–è®­ç»ƒçš„éšå¼ICLæ–¹æ³•ï¼ŒåŒæ—¶åœ¨ç°æœ‰æ–¹æ³•è¡¨ç°æŒ£æ‰çš„è·¨åŸŸä»»åŠ¡ä¸­è¡¨ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°ç¡®ç«‹äº†ICRåœ¨æ¨åŠ¨ICLå®ç”¨ä»·å€¼æ–¹é¢çš„åœ°ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22854v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšå¼ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¨¡æ‹Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¡¨ç¤ºç©ºé—´ä¸­çš„ICLè¡Œä¸ºï¼Œæ—¨åœ¨ä»¥é›¶æˆæœ¬è¾¾åˆ°å°‘é‡æ ·æœ¬çš„è¡¨ç°ã€‚ä½†ç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–äºå°†åç§»å‘é‡æ³¨å…¥æ®‹å·®æµä¸­ï¼Œè¿™äº›é€šå¸¸æ˜¯ç”±æ ‡ç­¾æ¼”ç¤ºæˆ–ä»»åŠ¡ç‰¹å®šå¯¹é½æ„å»ºçš„ã€‚è¿™æ ·çš„è®¾è®¡æœªèƒ½å……åˆ†åˆ©ç”¨ICLçš„å†…åœ¨ç»“æ„æœºåˆ¶ï¼Œä¸”é€šç”¨æ€§æœ‰é™ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºIn-Context Routingï¼ˆICRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„éšå¼ICLæ–¹æ³•ï¼Œåœ¨æ³¨æ„åŠ›æ—¥å¿—çº§åˆ«å†…åŒ–å¯æ³›åŒ–çš„ICLæ¨¡å¼ã€‚å®ƒæå–äº†ICLæœŸé—´å‡ºç°çš„å¯é‡å¤ä½¿ç”¨çš„ç»“æ„æ–¹å‘ï¼Œå¹¶ä½¿ç”¨å¯å­¦ä¹ çš„è¾“å…¥æ¡ä»¶è·¯ç”±å™¨ç›¸åº”åœ°è°ƒæ•´æ³¨æ„åŠ›æ—¥å¿—ï¼Œä»è€Œå®ç°äº†ä¸€æ¬¡è®­ç»ƒå¤šæ¬¡ä½¿ç”¨çš„æ¡†æ¶ã€‚æˆ‘ä»¬åœ¨è·¨è¶Šä¸åŒé¢†åŸŸçš„12ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†å’Œå¤šä¸ªLLMä¸Šè¯„ä¼°äº†ICRã€‚ç»“æœè¡¨æ˜ï¼ŒICRåœ¨ä¸éœ€è¦ç‰¹å®šä»»åŠ¡æ£€ç´¢æˆ–è®­ç»ƒçš„éšå¼ICLæ–¹æ³•ä¸Šè¡¨ç°ä¸€è‡´ï¼ŒåŒæ—¶åœ¨ç°æœ‰æ–¹æ³•éš¾ä»¥åº”å¯¹çš„åŸŸå¤–ä»»åŠ¡ä¸Šå±•ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°ä½¿ICRåœ¨æ¨åŠ¨ICLçš„å®é™…ä»·å€¼æ–¹é¢å–å¾—çªç ´æ€§è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšå¼ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ—¨åœ¨æ¨¡æ‹Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸Šä¸‹æ–‡è¡Œä¸ºï¼Œå®ç°é›¶æˆæœ¬ä¸‹çš„å°‘é‡æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡æ³¨å…¥åç§»å‘é‡åˆ°æ®‹å·®æµæ¥å®ç°ï¼Œä½†è¿™ç§è®¾è®¡é™åˆ¶äº†å…¶é€šç”¨æ€§ä¸”æœªèƒ½å……åˆ†åˆ©ç”¨ç»“æ„æœºåˆ¶ã€‚</li>
<li>In-Context Routingï¼ˆICRï¼‰æ˜¯ä¸€ç§æ–°çš„éšå¼ICLæ–¹æ³•ï¼Œå®ƒé€šè¿‡å†…åŒ–æ³¨æ„åŠ›æ—¥å¿—çº§åˆ«çš„å¯æ³›åŒ–çš„ICLæ¨¡å¼æ¥æ”¹è¿›ç°æœ‰æ–¹æ³•ã€‚</li>
<li>ICRèƒ½å¤Ÿä»ICLè¿‡ç¨‹ä¸­æå–å¯é‡å¤ä½¿ç”¨çš„ç»“æ„æ–¹å‘ã€‚</li>
<li>ICRä½¿ç”¨ä¸€ä¸ªå¯å­¦ä¹ çš„è¾“å…¥æ¡ä»¶è·¯ç”±å™¨æ¥è°ƒæ•´æ³¨æ„åŠ›æ—¥å¿—ï¼Œå®ç°ä¸€æ¬¡è®­ç»ƒå¤šæ¬¡ä½¿ç”¨çš„æ¡†æ¶ã€‚</li>
<li>ICRåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„éšå¼ICLæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ä»»åŠ¡ç‰¹å®šæ£€ç´¢æˆ–è®­ç»ƒçš„æƒ…å¢ƒä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3940068da5f5d9f6f821e177618be979~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927605&auth_key=1759927605-0-0-d15c7e7ed2c8f1f0f2f7eca0c9870756&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-23ec72053299f4c4769fa45e1b1721e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927612&auth_key=1759927612-0-0-7b811e1eb560fff5e0069fb1373185ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-a4dc13e0ba7e6d0762d4d8da3d83a909.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0ebcb269de325e84a8e8b8b12b81f0d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="HealthSLM-Bench-Benchmarking-Small-Language-Models-for-Mobile-and-Wearable-Healthcare-Monitoring"><a href="#HealthSLM-Bench-Benchmarking-Small-Language-Models-for-Mobile-and-Wearable-Healthcare-Monitoring" class="headerlink" title="HealthSLM-Bench: Benchmarking Small Language Models for Mobile and   Wearable Healthcare Monitoring"></a>HealthSLM-Bench: Benchmarking Small Language Models for Mobile and   Wearable Healthcare Monitoring</h2><p><strong>Authors:Xin Wang, Ting Dang, Xinyu Zhang, Vassilis Kostakos, Michael J. Witbrock, Hong Jia</strong></p>
<p>Mobile and wearable healthcare monitoring play a vital role in facilitating timely interventions, managing chronic health conditions, and ultimately improving individualsâ€™ quality of life. Previous studies on large language models (LLMs) have highlighted their impressive generalization abilities and effectiveness in healthcare prediction tasks. However, most LLM-based healthcare solutions are cloud-based, which raises significant privacy concerns and results in increased memory usage and latency. To address these challenges, there is growing interest in compact models, Small Language Models (SLMs), which are lightweight and designed to run locally and efficiently on mobile and wearable devices. Nevertheless, how well these models perform in healthcare prediction remains largely unexplored. We systematically evaluated SLMs on health prediction tasks using zero-shot, few-shot, and instruction fine-tuning approaches, and deployed the best performing fine-tuned SLMs on mobile devices to evaluate their real-world efficiency and predictive performance in practical healthcare scenarios. Our results show that SLMs can achieve performance comparable to LLMs while offering substantial gains in efficiency and privacy. However, challenges remain, particularly in handling class imbalance and few-shot scenarios. These findings highlight SLMs, though imperfect in their current form, as a promising solution for next-generation, privacy-preserving healthcare monitoring. </p>
<blockquote>
<p>ç§»åŠ¨å’Œå¯ç©¿æˆ´å¼å¥åº·ç›‘æµ‹åœ¨ä¿ƒè¿›åŠæ—¶å¹²é¢„ã€ç®¡ç†æ…¢æ€§å¥åº·çŠ¶å†µä»¥åŠæœ€ç»ˆæé«˜ä¸ªäººç”Ÿæ´»è´¨é‡æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä»¥å¾€å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç ”ç©¶å·²ç»çªå‡ºäº†å…¶åœ¨åŒ»ç–—ä¿å¥é¢„æµ‹ä»»åŠ¡ä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›å’Œæ•ˆæœã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºäºLLMçš„åŒ»ç–—ä¿å¥è§£å†³æ–¹æ¡ˆéƒ½æ˜¯åŸºäºäº‘çš„ï¼Œè¿™å¼•å‘äº†å…³äºéšç§çš„é‡è¦æ‹…å¿§ï¼Œå¹¶å¯¼è‡´å†…å­˜ä½¿ç”¨å’Œå»¶è¿Ÿå¢åŠ ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œäººä»¬å¯¹å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„å…´è¶£æ—¥ç›Šæµ“åšï¼Œè¿™äº›æ¨¡å‹è½»ä¾¿ã€è®¾è®¡ç”¨äºåœ¨ç§»åŠ¨å’Œå¯ç©¿æˆ´è®¾å¤‡ä¸Šæœ¬åœ°è¿è¡Œä¸”æ•ˆç‡é«˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨åŒ»ç–—ä¿å¥é¢„æµ‹æ–¹é¢çš„è¡¨ç°å¦‚ä½•ä»ç„¶çŸ¥ä¹‹ç”šå°‘ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†SLMåœ¨å¥åº·é¢„æµ‹ä»»åŠ¡ä¸­çš„é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’ŒæŒ‡ä»¤å¾®è°ƒæ–¹æ³•ï¼Œå¹¶åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šéƒ¨ç½²äº†è¡¨ç°æœ€ä½³çš„å¾®è°ƒSLMï¼Œä»¥è¯„ä¼°å…¶åœ¨ç°å®ä¸–ç•Œçš„æ•ˆç‡å’Œé¢„æµ‹æ€§èƒ½åœ¨çœŸå®åŒ»ç–—ä¿å¥åœºæ™¯ä¸­çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼ŒSLMå¯ä»¥åœ¨å®ç°ä¸LLMç›¸å½“çš„æ€§èƒ½çš„åŒæ—¶ï¼Œåœ¨æ•ˆç‡å’Œéšç§æ–¹é¢æä¾›äº†å®è´¨æ€§çš„æå‡ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡å’Œå°‘æ ·æœ¬åœºæ™¯æ–¹é¢ã€‚è¿™äº›ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å½“å‰çš„SLMå¹¶ä¸å®Œç¾ï¼Œä½†å®ƒä»¬ä½œä¸ºä¸‹ä¸€ä»£éšç§ä¿æŠ¤å¥åº·ç›‘æµ‹çš„æ½œåœ¨è§£å†³æ–¹æ¡ˆå…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07260v3">PDF</a> 9 pages, 6 tables, 6 figures. Accepted at NeurIPS 2025 Workshop on   GenAI4Health</p>
<p><strong>Summary</strong></p>
<p>å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰åœ¨ç§»åŠ¨å’Œå¯ç©¿æˆ´è®¾å¤‡ä¸Šçš„å¥åº·ç›‘æµ‹ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚ç›¸è¾ƒäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ŒSLMså…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œéšç§ä¿æŠ¤æ€§èƒ½ã€‚æœ¬æ–‡é€šè¿‡ç³»ç»Ÿè¯„ä¼°ï¼ŒéªŒè¯äº†SLMsåœ¨å¥åº·é¢„æµ‹ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶å‘ç°å…¶åœ¨å¤„ç†ä¸å¹³è¡¡æ•°æ®å’Œå°‘é‡æ ·æœ¬æ—¶çš„æŒ‘æˆ˜ã€‚å°½ç®¡å­˜åœ¨ä¸è¶³ï¼Œä½†SLMsæœ‰æœ›æˆä¸ºä¸‹ä¸€ä»£éšç§ä¿æŠ¤å¥åº·ç›‘æµ‹çš„å¯è¡Œè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§»åŠ¨å’Œå¯ç©¿æˆ´è®¾å¤‡åœ¨åŒ»ç–—ä¿å¥ç›‘æµ‹ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œæœ‰åŠ©äºåŠæ—¶å¹²é¢„ã€ç®¡ç†æ…¢æ€§ç–¾ç—…å¹¶æ”¹å–„ä¸ªäººç”Ÿæ´»è´¨é‡ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—ä¿å¥é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆæœã€‚</li>
<li>LLMsä¸»è¦ä½œä¸ºäº‘è§£å†³æ–¹æ¡ˆï¼Œå­˜åœ¨éšç§é—®é¢˜å’Œèµ„æºæ¶ˆè€—å¤§çš„æŒ‘æˆ˜ã€‚</li>
<li>å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰æ˜¯ä¸€ç§è½»é‡çº§çš„è§£å†³æ–¹æ¡ˆï¼Œå¯æœ¬åœ°è¿è¡Œï¼Œæ•ˆç‡é«˜ï¼Œé€‚ç”¨äºç§»åŠ¨å’Œå¯ç©¿æˆ´è®¾å¤‡ã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿè¯„ä¼°ï¼Œå‘ç°SLMsåœ¨å¥åº·é¢„æµ‹ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸LLMsç›¸å½“ã€‚</li>
<li>SLMsåœ¨å¤„ç†ä¸å¹³è¡¡æ•°æ®å’Œå°‘é‡æ ·æœ¬æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8d1cd53594577f09c396b5e7b7a576fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927632&auth_key=1759927632-0-0-d9aca2f80d0fbd71b15d4d8918028832&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-507a8b677549a961e88b8e19155d265e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927639&auth_key=1759927639-0-0-41967d9ab0939f38a07a5e1c09e76d75&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-b4ad3614c9d6f13457a6c02ce29d7d01.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f563b6f54bc2df1d0b0e7d003676e52.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CC-Time-Cross-Model-and-Cross-Modality-Time-Series-Forecasting"><a href="#CC-Time-Cross-Model-and-Cross-Modality-Time-Series-Forecasting" class="headerlink" title="CC-Time: Cross-Model and Cross-Modality Time Series Forecasting"></a>CC-Time: Cross-Model and Cross-Modality Time Series Forecasting</h2><p><strong>Authors:Peng Chen, Yihang Wang, Yang Shu, Yunyao Cheng, Kai Zhao, Zhongwen Rao, Lujia Pan, Bin Yang, Chenjuan Guo</strong></p>
<p>With the success of pre-trained language models (PLMs) in various application fields beyond natural language processing, language models have raised emerging attention in the field of time series forecasting (TSF) and have shown great prospects. However, current PLM-based TSF methods still fail to achieve satisfactory prediction accuracy matching the strong sequential modeling power of language models. To address this issue, we propose Cross-Model and Cross-Modality Learning with PLMs for time series forecasting (CC-Time). We explore the potential of PLMs for time series forecasting from two aspects: 1) what time series features could be modeled by PLMs, and 2) whether relying solely on PLMs is sufficient for building time series models. In the first aspect, CC-Time incorporates cross-modality learning to model temporal dependency and channel correlations in the language model from both time series sequences and their corresponding text descriptions. In the second aspect, CC-Time further proposes the cross-model fusion block to adaptively integrate knowledge from the PLMs and time series model to form a more comprehensive modeling of time series patterns. Extensive experiments on nine real-world datasets demonstrate that CC-Time achieves state-of-the-art prediction accuracy in both full-data training and few-shot learning situations. </p>
<blockquote>
<p>éšç€é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»¥å¤–çš„å„ç§åº”ç”¨é¢†åŸŸçš„æˆåŠŸï¼Œè¯­è¨€æ¨¡å‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰é¢†åŸŸå¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ï¼Œå¹¶æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºäºPLMçš„æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•ä»æœªèƒ½å®ç°ä»¤äººæ»¡æ„çš„é¢„æµ‹ç²¾åº¦ï¼Œæ— æ³•åŒ¹é…è¯­è¨€æ¨¡å‹çš„å¼ºå¤§åºåˆ—å»ºæ¨¡èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºPLMçš„è·¨æ¨¡å‹è·¨æ¨¡æ€å­¦ä¹ æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•ï¼ˆCC-Timeï¼‰ã€‚æˆ‘ä»¬ä»ä¸¤ä¸ªæ–¹é¢æ¢è®¨äº†PLMåœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„æ½œåŠ›ï¼š1ï¼‰PLMèƒ½å¤Ÿå»ºæ¨¡å“ªäº›æ—¶é—´åºåˆ—ç‰¹å¾ï¼›2ï¼‰ä»…ä¾é PLMæ˜¯å¦è¶³ä»¥æ„å»ºæ—¶é—´åºåˆ—æ¨¡å‹ã€‚åœ¨ç¬¬ä¸€æ–¹é¢ï¼ŒCC-Timeé€šè¿‡è·¨æ¨¡æ€å­¦ä¹ æ¥å»ºæ¨¡è¯­è¨€æ¨¡å‹ä¸­çš„æ—¶é—´ä¾èµ–æ€§å’Œé€šé“ç›¸å…³æ€§ï¼Œè¿™äº›å…³è”æ¥è‡ªæ—¶é—´åºåˆ—åºåˆ—åŠå…¶ç›¸åº”çš„æ–‡æœ¬æè¿°ã€‚åœ¨ç¬¬äºŒæ–¹é¢ï¼ŒCC-Timeè¿›ä¸€æ­¥æå‡ºäº†è·¨æ¨¡å‹èåˆæ¨¡å—ï¼Œä»¥è‡ªé€‚åº”åœ°æ•´åˆæ¥è‡ªPLMå’Œæ—¶åºæ¨¡å‹çš„çŸ¥è¯†ï¼Œä»¥å®ç°å¯¹æ—¶é—´åºåˆ—æ¨¡å¼çš„æ›´å…¨é¢çš„å»ºæ¨¡ã€‚åœ¨ä¹ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨å…¨æ•°æ®è®­ç»ƒè¿˜æ˜¯å°æ ·å­¦ä¹ æƒ…å†µä¸‹ï¼ŒCC-Timeéƒ½å®ç°äº†æœ€å…ˆè¿›çš„é¢„æµ‹ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12235v3">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸåº”ç”¨ä¸Šçš„æˆåŠŸï¼Œæ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰é¢†åŸŸå¼€å§‹å…³æ³¨è¯­è¨€æ¨¡å‹çš„åº”ç”¨ï¼Œå¹¶å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚é’ˆå¯¹å½“å‰PLM-based TSFæ–¹æ³•é¢„æµ‹å‡†ç¡®åº¦ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºCC-Timeçš„è·¨æ¨¡å‹è·¨æ¨¡æ€å­¦ä¹ æ–¹æ³•ã€‚ä»ä¸¤ä¸ªæ–¹é¢æ¢è®¨äº†PLMsåœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„æ½œåŠ›ï¼Œå¹¶è®¾è®¡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤šç§çœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰åœ¨éè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„åº”ç”¨å—åˆ°å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰é¢†åŸŸã€‚</li>
<li>å½“å‰PLM-based TSFæ–¹æ³•åœ¨é¢„æµ‹å‡†ç¡®åº¦ä¸Šä»æœ‰æå‡ç©ºé—´ã€‚</li>
<li>CC-Timeæ–¹æ³•ä»ä¸¤ä¸ªæ–¹é¢æ¢è®¨PLMsåœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„æ½œåŠ›ï¼šæ—¶é—´åºåˆ—è¡¨å¾å»ºæ¨¡å’Œæ˜¯å¦ä»…ä¾èµ–PLMsæ„å»ºæ—¶é—´åºåˆ—æ¨¡å‹ã€‚</li>
<li>CC-Timeå¼•å…¥è·¨æ¨¡æ€å­¦ä¹ æ¥å»ºæ¨¡è¯­è¨€æ¨¡å‹ä¸­æ—¶é—´åºåˆ—åºåˆ—å’Œæ—¶é—´åºåˆ—æè¿°ä¹‹é—´çš„æ—¶é—´ä¾èµ–æ€§å’Œé€šé“ç›¸å…³æ€§ã€‚</li>
<li>CC-Timeæå‡ºè·¨æ¨¡å‹èåˆå—ï¼Œè‡ªé€‚åº”åœ°æ•´åˆPLMså’Œæ—¶åºæ¨¡å‹çš„çŸ¥è¯†ï¼Œå½¢æˆæ›´å…¨é¢çš„æ—¶é—´åºåˆ—æ¨¡å¼å»ºæ¨¡ã€‚</li>
<li>åœ¨ä¹ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCC-Timeåœ¨å®Œæ•´æ•°æ®è®­ç»ƒå’Œå°‘æ ·æœ¬å­¦ä¹ æƒ…å†µä¸‹å‡å®ç°äº†æœ€å…ˆè¿›çš„é¢„æµ‹å‡†ç¡®åº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12235">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-304cb8ed212a85637cc881b8388dbf37.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-caeeacbf962994b6a6e1fd39e1b54e7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927667&auth_key=1759927667-0-0-848da159d6525c45a833d7d95c36711f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7789b90e2724f53179d6dd0505407b44~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927673&auth_key=1759927673-0-0-2e73adfc5c226691d6d92098e8b704b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="XTransfer-Modality-Agnostic-Few-Shot-Model-Transfer-for-Human-Sensing-at-the-Edge"><a href="#XTransfer-Modality-Agnostic-Few-Shot-Model-Transfer-for-Human-Sensing-at-the-Edge" class="headerlink" title="XTransfer: Modality-Agnostic Few-Shot Model Transfer for Human Sensing   at the Edge"></a>XTransfer: Modality-Agnostic Few-Shot Model Transfer for Human Sensing   at the Edge</h2><p><strong>Authors:Yu Zhang, Xi Zhang, Hualin zhou, Xinyuan Chen, Shang Gao, Hong Jia, Jianfei Yang, Yuankai Qi, Tao Gu</strong></p>
<p>Deep learning for human sensing on edge systems presents significant potential for smart applications. However, its training and development are hindered by the limited availability of sensor data and resource constraints of edge systems. While transferring pre-trained models to different sensing applications is promising, existing methods often require extensive sensor data and computational resources, resulting in high costs and poor adaptability in practice. In this paper, we propose XTransfer, a first-of-its-kind method enabling modality-agnostic, few-shot model transfer with resource-efficient design. XTransfer flexibly uses single or multiple pre-trained models and transfers knowledge across different modalities by (i) model repairing that safely mitigates modality shift by adapting pre-trained layers with only few sensor data, and (ii) layer recombining that efficiently searches and recombines layers of interest from source models in a layer-wise manner to create compact models. We benchmark various baselines across diverse human sensing datasets spanning different modalities. Comprehensive results demonstrate that XTransfer achieves state-of-the-art performance while significantly reducing the costs of sensor data collection, model training, and edge deployment. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ åœ¨è¾¹ç¼˜ç³»ç»Ÿä¸Šè¿›è¡Œäººä½“æ„ŸçŸ¥å…·æœ‰å·¨å¤§çš„æ™ºèƒ½åº”ç”¨æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…¶è®­ç»ƒå’Œå¼€å‘å—åˆ°ä¼ æ„Ÿå™¨æ•°æ®æœ‰é™å’Œè¾¹ç¼˜ç³»ç»Ÿèµ„æºé™åˆ¶çš„é˜»ç¢ã€‚è™½ç„¶å°†é¢„è®­ç»ƒæ¨¡å‹è½¬ç§»åˆ°ä¸åŒçš„æ„ŸçŸ¥åº”ç”¨ä¸­å‰æ™¯å¹¿é˜”ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„ä¼ æ„Ÿå™¨æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œå¯¼è‡´å®è·µä¸­çš„æˆæœ¬é«˜æ˜‚å’Œé€‚åº”æ€§å·®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†XTransferï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°æ¨¡æ€æ— å…³çš„ã€å°‘æ ·æœ¬æ¨¡å‹è½¬ç§»å¹¶å…·æœ‰èµ„æºé«˜æ•ˆçš„è®¾è®¡ã€‚XTransferå¯ä»¥çµæ´»åœ°ä½¿ç”¨å•ä¸ªæˆ–å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶é€šè¿‡ï¼ˆiï¼‰æ¨¡å‹ä¿®å¤æ¥å®‰å…¨åœ°å‡è½»æ¨¡æ€åç§»ï¼Œä»…ä½¿ç”¨å°‘é‡ä¼ æ„Ÿå™¨æ•°æ®é€‚åº”é¢„è®­ç»ƒå±‚ï¼›ï¼ˆiiï¼‰å±‚é‡ç»„æœ‰æ•ˆåœ°é€å±‚æœç´¢å’Œé‡ç»„æºæ¨¡å‹çš„æ„Ÿå…´è¶£å±‚ä»¥åˆ›å»ºç´§å‡‘æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨è·¨è¶Šä¸åŒæ¨¡æ€çš„å¤šæ ·åŒ–äººä½“æ„ŸçŸ¥æ•°æ®é›†ä¸Šå¯¹å„ç§åŸºçº¿è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç»¼åˆç»“æœè¡¨æ˜ï¼ŒXTransferåœ¨è¾¾åˆ°æœ€æ–°æ€§èƒ½çš„åŒæ—¶ï¼Œå¤§å¤§é™ä½äº†ä¼ æ„Ÿå™¨æ•°æ®é‡‡é›†ã€æ¨¡å‹è®­ç»ƒå’Œè¾¹ç¼˜éƒ¨ç½²çš„æˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22726v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ åœ¨è¾¹ç¼˜ç³»ç»Ÿä¸Šè¿›è¡Œäººç±»æ„ŸçŸ¥å±•ç°å‡ºå·¨å¤§çš„æ™ºèƒ½åº”ç”¨æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…¶è®­ç»ƒå’Œå¼€å‘çš„å±€é™æ€§åœ¨äºä¼ æ„Ÿå™¨æ•°æ®çš„æœ‰é™æ€§å’Œè¾¹ç¼˜ç³»ç»Ÿçš„èµ„æºé™åˆ¶ã€‚è™½ç„¶å°†é¢„è®­ç»ƒæ¨¡å‹è½¬ç§»åˆ°ä¸åŒçš„æ„ŸçŸ¥åº”ç”¨ä¸Šå¾ˆæœ‰å‰æ™¯ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€éœ€è¦å¤§é‡çš„ä¼ æ„Ÿå™¨æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œå¯¼è‡´é«˜æˆæœ¬å’Œå®è·µä¸­é€‚åº”æ€§å·®ã€‚æœ¬æ–‡æå‡ºäº†XTransferæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å…¨æ–°çš„æ¨¡æ€æ— å…³ã€å°æ ·æœ¬æ¨¡å‹è½¬ç§»å’Œèµ„æºé«˜æ•ˆè®¾è®¡çš„æ–¹æ³•ã€‚XTransferçµæ´»åœ°ä½¿ç”¨å•ä¸ªæˆ–å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶é€šè¿‡ï¼ˆiï¼‰æ¨¡å‹ä¿®å¤ï¼Œä»…ä½¿ç”¨å°‘é‡ä¼ æ„Ÿå™¨æ•°æ®é€‚åº”é¢„è®­ç»ƒå±‚ï¼Œå®‰å…¨ç¼“è§£æ¨¡æ€è½¬æ¢ï¼›ï¼ˆiiï¼‰é€å±‚æœç´¢å’Œé‡ç»„æºæ¨¡å‹çš„æ„Ÿå…´è¶£å±‚ï¼Œåˆ›å»ºç´§å‡‘æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨ä¸åŒæ¨¡æ€çš„äººç±»æ„ŸçŸ¥æ•°æ®é›†ä¸Šè¯„ä¼°äº†å„ç§åŸºçº¿ã€‚ç»¼åˆç»“æœè¡¨æ˜ï¼ŒXTransferå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œåœ¨æ”¶é›†ä¼ æ„Ÿå™¨æ•°æ®ã€æ¨¡å‹è®­ç»ƒå’Œç¯å¢ƒéƒ¨ç½²æ–¹é¢çš„æˆæœ¬å¤§å¤§é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨è¾¹ç¼˜ç³»ç»Ÿçš„æ™ºèƒ½åº”ç”¨æ½œåŠ›å·¨å¤§ï¼Œä½†å—é™äºä¼ æ„Ÿå™¨æ•°æ®çš„æœ‰é™æ€§å’Œè¾¹ç¼˜ç³»ç»Ÿçš„èµ„æºé™åˆ¶ã€‚</li>
<li>é¢„è®­ç»ƒæ¨¡å‹è½¬ç§»åˆ°ä¸åŒæ„ŸçŸ¥åº”ç”¨ä¸Šæ˜¯ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ï¼Œä½†ç°æœ‰æ–¹æ³•æˆæœ¬é«˜ä¸”é€‚åº”æ€§å·®ã€‚</li>
<li>XTransferæ˜¯ä¸€ç§æ–°çš„æ¨¡æ€æ— å…³ã€å°æ ·æœ¬æ¨¡å‹è½¬ç§»æ–¹æ³•ï¼Œé€šè¿‡æ¨¡å‹ä¿®å¤å’Œå±‚é‡ç»„æ¥é€‚åº”ä¸åŒçš„ä¼ æ„Ÿå™¨æ•°æ®å’Œæ¨¡æ€ã€‚</li>
<li>XTransferçµæ´»ä½¿ç”¨å•ä¸ªæˆ–å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶èƒ½å®‰å…¨åœ°ç¼“è§£æ¨¡æ€è½¬æ¢çš„é—®é¢˜ã€‚</li>
<li>XTransferåœ¨åˆ›å»ºç´§å‡‘æ¨¡å‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œæ˜¾è‘—é™ä½ä¼ æ„Ÿå™¨æ•°æ®æ”¶é›†ã€æ¨¡å‹è®­ç»ƒå’Œç¯å¢ƒéƒ¨ç½²çš„æˆæœ¬ã€‚</li>
<li>åœ¨å¤šç§äººç±»æ„ŸçŸ¥æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜XTransferå…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿å’Œå…ˆè¿›æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d8f89a26a5b8561034919761e6e113c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-971a25ab20cbe8190bfe96e2ee708c45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-781b7d15e24f114f5d274b0417ebde22.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-1dc99cf4930fb99394984e462e89d260~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927701&auth_key=1759927701-0-0-e2f01fff48c66126ad9ec89276de178b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Prior-Reinforce-Mastering-Agile-Tasks-with-Limited-Trials"><a href="#Prior-Reinforce-Mastering-Agile-Tasks-with-Limited-Trials" class="headerlink" title="Prior Reinforce: Mastering Agile Tasks with Limited Trials"></a>Prior Reinforce: Mastering Agile Tasks with Limited Trials</h2><p><strong>Authors:Yihang Hu, Pingyue Sheng, Yuyang Liu, Shengjie Wang, Yang Gao</strong></p>
<p>Embodied robots nowadays can already handle many real-world manipulation tasks. However, certain other real-world tasks involving dynamic processes (e.g., shooting a basketball into a hoop) are highly agile and impose high precision requirements on the outcomes, presenting additional challenges for methods primarily designed for quasi-static manipulations. This leads to increased efforts in costly data collection, laborious reward design, or complex motion planning. Such tasks, however, are far less challenging for humans. Say a novice basketball player typically needs only about 10 attempts to make their first successful shot, by roughly imitating some motion priors and then iteratively adjusting their motion based on the past outcomes. Inspired by this human learning paradigm, we propose Prior Reinforce(P.R.), a simple and scalable approach which first learns a motion pattern from very few demonstrations, then iteratively refines its generated motions based on feedback of a few real-world trials, until reaching a specific goal. Experiments demonstrated that Prior Reinforce can learn and accomplish a wide range of goal-conditioned agile dynamic tasks with human-level precision and efficiency directly in real-world, such as throwing a basketball into the hoop in fewer than 10 trials. Project website:<a target="_blank" rel="noopener" href="https://adap-robotics.github.io/">https://adap-robotics.github.io/</a>. </p>
<blockquote>
<p>å¦‚ä»Šï¼ŒåµŒå…¥å¼æœºå™¨äººå·²ç»èƒ½å¤Ÿå¤„ç†è®¸å¤šç°å®ä¸–ç•Œçš„æ“ä½œä»»åŠ¡ã€‚ç„¶è€Œï¼Œæ¶‰åŠåŠ¨æ€è¿‡ç¨‹çš„å…¶ä»–æŸäº›ç°å®ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œå°†ç¯®çƒæŠ•è¿›ç¯®ç­ï¼‰éœ€è¦é«˜åº¦çš„æ•æ·æ€§å’Œå¯¹ç»“æœçš„é«˜ç²¾åº¦è¦æ±‚ï¼Œè¿™ä¸ºä¸»è¦è®¾è®¡ç”¨äºå‡†é™æ€æ“ä½œçš„æ–¹æ³•å¸¦æ¥äº†é¢å¤–çš„æŒ‘æˆ˜ã€‚è¿™å¯¼è‡´äº†æˆæœ¬é«˜æ˜‚çš„æ•°æ®æ”¶é›†ã€ç¹ççš„å¥–åŠ±è®¾è®¡æˆ–å¤æ‚çš„è¿åŠ¨è§„åˆ’çš„åŠªåŠ›å¢åŠ ã€‚ç„¶è€Œï¼Œå¯¹äºäººç±»æ¥è¯´ï¼Œæ­¤ç±»ä»»åŠ¡è¦å®¹æ˜“å¾—å¤šã€‚æ¯”å¦‚è¯´ï¼Œä¸€ä¸ªæ–°æ‰‹ç¯®çƒè¿åŠ¨å‘˜é€šå¸¸åªéœ€è¦å¤§çº¦10æ¬¡å°è¯•å°±èƒ½æŠ•è¿›ç¬¬ä¸€ä¸ªæˆåŠŸçš„çƒï¼Œé€šè¿‡ç²—ç•¥æ¨¡ä»¿ä¸€äº›åŠ¨ä½œä¼˜å…ˆäº‹é¡¹ï¼Œç„¶åæ ¹æ®è¿‡å»çš„ç»“æœè¿­ä»£è°ƒæ•´ä»–ä»¬çš„åŠ¨ä½œã€‚å—è¿™ç§äººç±»å­¦ä¹ æ¨¡å¼çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†Prior Reinforceï¼ˆPRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„æ–¹æ³•ï¼Œå®ƒé¦–å…ˆé€šè¿‡å¾ˆå°‘çš„æ¼”ç¤ºæ¥å­¦ä¹ ä¸€ç§è¿åŠ¨æ¨¡å¼ï¼Œç„¶åæ ¹æ®å‡ æ¬¡ç°å®ä¸–ç•Œçš„è¯•éªŒåé¦ˆè¿­ä»£åœ°æ”¹è¿›å…¶ç”Ÿæˆçš„åŠ¨ä½œï¼Œç›´åˆ°è¾¾åˆ°ç‰¹å®šçš„ç›®æ ‡ã€‚å®éªŒè¡¨æ˜ï¼ŒPrior Reinforceèƒ½å¤Ÿåœ¨ç°å®ä¸–ç•Œä¸­ç›´æ¥å­¦ä¹ å’Œå®Œæˆå¤šç§ç›®æ ‡æ¡ä»¶ä¸‹çš„æ•æ·åŠ¨æ€ä»»åŠ¡ï¼Œè¾¾åˆ°äººç±»æ°´å¹³çš„ç²¾ç¡®åº¦å’Œæ•ˆç‡ï¼Œå¦‚åœ¨å°‘äº10æ¬¡è¯•éªŒä¸­å°†ç¯®çƒæŠ•å…¥ç¯®ç­ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://adap-robotics.github.io/%E3%80%82">https://adap-robotics.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21916v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å°‘æ ·æœ¬çš„åŠ¨æ€ä»»åŠ¡æ“ä½œï¼šå€ŸåŠ©äººç±»å­¦ä¹ æ¨¡å¼çš„çµæ„Ÿï¼Œæå‡ºäº†ä¸€ç§åä¸ºPrior Reinforceï¼ˆP.R.ï¼‰çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿä»æå°‘æ¼”ç¤ºä¸­å­¦ä¹ åŠ¨ä½œæ¨¡å¼ï¼Œå¹¶æ ¹æ®çœŸå®ä¸–ç•Œè¯•éªŒçš„åé¦ˆè¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œè¾¾åˆ°ç‰¹å®šç›®æ ‡ã€‚è¯¥æ–¹æ³•å¯å®ç°äººç±»æ°´å¹³çš„ç²¾åº¦å’Œæ•ˆç‡ï¼Œç›´æ¥å®Œæˆä¸€ç³»åˆ—ç›®æ ‡å¯¼å‘çš„åŠ¨æ€ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°å®ä¸–ç•Œçš„åŠ¨æ€æ“ä½œä»»åŠ¡éœ€è¦é«˜åº¦çš„æ•æ·æ€§å’Œç²¾ç¡®æ€§ã€‚</li>
<li>ç›®å‰çš„æ–¹æ³•åœ¨åº”å¯¹è¿™ç±»ä»»åŠ¡æ—¶é¢ä¸´æ•°æ®æ”¶é›†æˆæœ¬é«˜ã€å¥–åŠ±è®¾è®¡ç¹çæˆ–è¿åŠ¨è§„åˆ’å¤æ‚ç­‰æŒ‘æˆ˜ã€‚</li>
<li>äººç±»å­¦ä¹ æ­¤ç±»ä»»åŠ¡çš„èƒ½åŠ›è¿œè¶…ç°æœ‰æ–¹æ³•ï¼Œä»…éœ€å°‘é‡å°è¯•å³å¯è¾¾åˆ°é«˜ç²¾åº¦ã€‚</li>
<li>Prior Reinforceæ–¹æ³•ä»å°‘é‡æ¼”ç¤ºä¸­å­¦ä¹ åŠ¨ä½œæ¨¡å¼ï¼Œå¹¶æ ¹æ®çœŸå®ä¸–ç•Œçš„åé¦ˆè¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å°‘äº10æ¬¡è¯•éªŒå†…å®Œæˆå¦‚æŠ•ç¯®ç­‰åŠ¨æ€ä»»åŠ¡ï¼Œè¾¾åˆ°äººç±»æ°´å¹³çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚</li>
<li>Prior Reinforceæ–¹æ³•å¯ç›´æ¥åœ¨çœŸå®ä¸–ç•Œä¸­è¿›è¡Œå­¦ä¹ å¹¶å®Œæˆä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21916">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e0b25307ae324006ef5c0d76c5ed032a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927709&auth_key=1759927709-0-0-0fbc17aabb9be345dfa47cdaf3942c7d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2fbf99e1a8782c441e870fb6fdfc2e8a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927716&auth_key=1759927716-0-0-23e081fa3e7ef023b49ca6cc36e4543d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3654cb1c6948e25917592db01029a0ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927723&auth_key=1759927723-0-0-9aee0adf7ca5f978bab8c6ae3be0b7c3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-94d5b986c92105f86a1910539eba4399.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-730b23f7f7bd27cceedbe3bf89ab2e70~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927736&auth_key=1759927736-0-0-b3c7905f0d358d3b7f3faff64669c49a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-8386321dad5083eb770339a896b2535f.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-82441b8071594ff9732edbbf9e514279~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927749&auth_key=1759927749-0-0-cea7414ee84278f95f7037f50ab620d8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AgentThink-A-Unified-Framework-for-Tool-Augmented-Chain-of-Thought-Reasoning-in-Vision-Language-Models-for-Autonomous-Driving"><a href="#AgentThink-A-Unified-Framework-for-Tool-Augmented-Chain-of-Thought-Reasoning-in-Vision-Language-Models-for-Autonomous-Driving" class="headerlink" title="AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought   Reasoning in Vision-Language Models for Autonomous Driving"></a>AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought   Reasoning in Vision-Language Models for Autonomous Driving</h2><p><strong>Authors:Kangan Qian, Sicong Jiang, Yang Zhong, Ziang Luo, Zilin Huang, Tianze Zhu, Kun Jiang, Mengmeng Yang, Zheng Fu, Jinyu Miao, Yining Shi, He Zhe Lim, Li Liu, Tianbao Zhou, Huang Yu, Yifei Hu, Guang Li, Guang Chen, Hao Ye, Lijun Sun, Diange Yang</strong></p>
<p>Vision-Language Models (VLMs) show promise for autonomous driving, yet their struggle with hallucinations, inefficient reasoning, and limited real-world validation hinders accurate perception and robust step-by-step reasoning. To overcome this, we introduce \textbf{AgentThink}, a pioneering unified framework that integrates Chain-of-Thought (CoT) reasoning with dynamic, agent-style tool invocation for autonomous driving tasks. AgentThinkâ€™s core innovations include: \textbf{(i) Structured Data Generation}, which establishes an autonomous driving tool library to automatically construct structured, self-verified reasoning data explicitly incorporating tool usage for diverse driving scenarios; \textbf{(ii) A Two-stage Training Pipeline}, employing Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO) to equip VLMs with the capability for autonomous tool invocation; and \textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel multi-tool assessment protocol to rigorously evaluate the modelâ€™s tool invocation and utilization. Experiments on the DriveLMM-o1 benchmark demonstrate that AgentThink significantly boosts overall reasoning scores by \textbf{53.91%} and enhances answer accuracy by \textbf{33.54%}, while markedly improving reasoning quality and consistency. Furthermore, ablation studies and robust zero-shot&#x2F;few-shot generalization experiments across various benchmarks underscore its powerful capabilities. These findings highlight a promising trajectory for developing trustworthy and tool-aware autonomous driving models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/curryqka/AgentThink">https://github.com/curryqka/AgentThink</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå…·æœ‰å¹¿é˜”å‰æ™¯ï¼Œç„¶è€Œå®ƒä»¬é¢ä¸´çš„å¹»è§‰ã€æ¨ç†æ•ˆç‡ä½ä¸‹å’Œç°å®ä¸–ç•ŒéªŒè¯æœ‰é™ç­‰é—®é¢˜é˜»ç¢äº†å‡†ç¡®çš„æ„ŸçŸ¥å’Œç¨³å¥çš„é€æ­¥æ¨ç†ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†<strong>AgentThink</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€åˆ›æ€§çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå®ƒå°†æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ä¸åŠ¨æ€ã€ä»£ç†å¼å·¥å…·è°ƒç”¨ç›¸ç»“åˆï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶ä»»åŠ¡ã€‚AgentThinkçš„æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š**(i)ç»“æ„åŒ–æ•°æ®ç”Ÿæˆ<strong>ï¼Œå»ºç«‹è‡ªåŠ¨é©¾é©¶å·¥å…·åº“ï¼Œè‡ªåŠ¨æ„å»ºç»“æ„åŒ–ã€è‡ªæˆ‘éªŒè¯çš„æ¨ç†æ•°æ®ï¼Œæ˜ç¡®èå…¥å„ç§é©¾é©¶åœºæ™¯çš„å·¥å…·ä½¿ç”¨ï¼›</strong>(ii)ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“<strong>ï¼Œé‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä½¿VLMså…·å¤‡è‡ªä¸»å·¥å…·è°ƒç”¨çš„èƒ½åŠ›ï¼›</strong>(iii)ä»£ç†å¼å·¥å…·ä½¿ç”¨è¯„ä¼°<strong>ï¼Œå¼•å…¥ä¸€ç§æ–°çš„å¤šå·¥å…·è¯„ä¼°åè®®ï¼Œä¸¥æ ¼è¯„ä¼°æ¨¡å‹çš„å·¥å…·è°ƒç”¨å’Œä½¿ç”¨æƒ…å†µã€‚åœ¨DriveLMM-o1åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAgentThinkæ•´ä½“æ¨ç†å¾—åˆ†æé«˜äº†</strong>53.91%<strong>ï¼Œç­”æ¡ˆå‡†ç¡®æ€§æé«˜äº†</strong>33.54%**ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†æ¨ç†è´¨é‡å’Œä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæ¶ˆèç ”ç©¶å’Œå„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„ç¨³å¥é›¶æ ·æœ¬&#x2F;å°‘æ ·æœ¬æ³›åŒ–å®éªŒçªå‡ºäº†å…¶å¼ºå¤§çš„èƒ½åŠ›ã€‚è¿™äº›å‘ç°å±•ç¤ºäº†å¼€å‘å¯ä¿¡ä¸”å·¥å…·æ„ŸçŸ¥çš„è‡ªåŠ¨é©¾é©¶æ¨¡å‹çš„å¹¿é˜”å‰æ™¯ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/curryqka/AgentThink%E3%80%82">https://github.com/curryqka/AgentThinkã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15298v4">PDF</a> 19 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¢å‘è‡ªåŠ¨é©¾é©¶çš„å…ˆè¿›æ¡†æ¶AgentThinkï¼Œè¯¥æ¡†æ¶ç»“åˆäº†é“¾å¼æ€ç»´æ¨ç†å’ŒåŠ¨æ€å·¥å…·è°ƒç”¨æŠ€æœ¯ã€‚AgentThinkçš„åˆ›æ–°ç‚¹åŒ…æ‹¬ç»“æ„åŒ–æ•°æ®ç”Ÿæˆã€ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“å’Œå·¥å…·ä½¿ç”¨è¯„ä¼°åè®®ã€‚å®éªŒè¯æ˜ï¼ŒAgentThinkèƒ½å¤Ÿæ˜¾è‘—æé«˜è‡ªåŠ¨é©¾é©¶æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œç­”æ¡ˆå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AgentThinkæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œé›†æˆäº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†å’ŒåŠ¨æ€å·¥å…·è°ƒç”¨ï¼Œç”¨äºè§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çš„è®¤çŸ¥ä»»åŠ¡ã€‚</li>
<li>ç»“æ„åŒ–æ•°æ®ç”Ÿæˆï¼šå»ºç«‹äº†ä¸€ä¸ªè‡ªåŠ¨é©¾é©¶å·¥å…·åº“ï¼Œç”¨äºè‡ªåŠ¨ç”ŸæˆåŒ…å«å·¥å…·ä½¿ç”¨çš„ç»“æ„åŒ–ã€è‡ªéªŒè¯æ¨ç†æ•°æ®ã€‚</li>
<li>ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼šé‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä½¿VLMå…·å¤‡è‡ªä¸»å·¥å…·è°ƒç”¨çš„èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†æ–°å‹çš„å¤šå·¥å…·è¯„ä¼°åè®®ï¼Œç”¨äºä¸¥æ ¼è¯„ä¼°æ¨¡å‹çš„å·¥å…·è°ƒç”¨å’Œä½¿ç”¨æƒ…å†µã€‚</li>
<li>åœ¨DriveLMM-o1åŸºå‡†æµ‹è¯•ä¸Šï¼ŒAgentThinkæ˜¾è‘—æé«˜äº†æ€»ä½“æ¨ç†å¾—åˆ†å’Œç­”æ¡ˆå‡†ç¡®æ€§ã€‚</li>
<li>æ¶ˆèç ”ç©¶å’Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„é›¶æ ·æœ¬&#x2F;å°‘æ ·æœ¬æ³›åŒ–å®éªŒè¯æ˜äº†AgentThinkçš„å¼ºå¤§èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b85619e531f2c155789012fe96eeb561.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-d1af87f52b1bc48bc374a6e55f809a60~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927763&auth_key=1759927763-0-0-f4eb8b158ed45d6239207afcba771e30&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-7a73d8a17e55c2a08f09551420082944.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5361ef7a8e6860020e18feecfe9e7c22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a97ce5d3c4803036b61628789abddc5d.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-599a29349cceb911f8507014c3af8512~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927791&auth_key=1759927791-0-0-66d1903aed288eac9332e97bbba86da1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Mechanistic-Fine-tuning-for-In-context-Learning"><a href="#Mechanistic-Fine-tuning-for-In-context-Learning" class="headerlink" title="Mechanistic Fine-tuning for In-context Learning"></a>Mechanistic Fine-tuning for In-context Learning</h2><p><strong>Authors:Hakaze Cho, Peng Luo, Mariko Kato, Rin Kaenbyou, Naoya Inoue</strong></p>
<p>In-context Learning (ICL) utilizes structured demonstration-query inputs to induce few-shot learning on Language Models (LMs), which are not originally pre-trained on ICL-style data. To bridge the gap between ICL and pre-training, some approaches fine-tune LMs on large ICL-style datasets by an end-to-end paradigm with massive computational costs. To reduce such costs, in this paper, we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous findings on the inner mechanism of ICL, building training objectives on the attention scores instead of the final outputs, to force the attention scores to focus on the correct label tokens presented in the context and mitigate attention scores from the wrong label tokens. Our experiments on 9 modern LMs and 8 datasets empirically find that ABFT outperforms in performance, robustness, unbiasedness, and efficiency, with only around 0.01% data cost compared to the previous methods. Moreover, our subsequent analysis finds that the end-to-end training objective contains the ABFT objective, suggesting the implicit bias of ICL-style data to the emergence of induction heads. Our work demonstrates the possibility of controlling specific module sequences within LMs to improve their behavior, opening up the future application of mechanistic interpretability. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰åˆ©ç”¨ç»“æ„åŒ–æ¼”ç¤ºæŸ¥è¯¢è¾“å…¥ï¼Œä»¥åœ¨è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰ä¸Šå®ç°å°‘é‡å­¦ä¹ ï¼Œè¿™äº›è¯­è¨€æ¨¡å‹æœ€åˆå¹¶æœªåœ¨ICLé£æ ¼çš„æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚ä¸ºäº†å¼¥åˆICLå’Œé¢„è®­ç»ƒä¹‹é—´çš„å·®è·ï¼Œä¸€äº›æ–¹æ³•é‡‡ç”¨ç«¯åˆ°ç«¯èŒƒå¼å¯¹å¤§å‹ICLé£æ ¼æ•°æ®é›†å¯¹LMè¿›è¡Œå¾®è°ƒï¼Œè¿™éœ€è¦å¤§é‡çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†é™ä½è¿™äº›æˆæœ¬ï¼Œæœ¬æ–‡æå‡ºäº†æ³¨æ„åŠ›è¡Œä¸ºå¾®è°ƒï¼ˆABFTï¼‰ï¼Œå®ƒåˆ©ç”¨å¯¹ICLå†…åœ¨æœºåˆ¶çš„å…ˆå‰å‘ç°ï¼Œåœ¨æ³¨æ„åŠ›åˆ†æ•°ä¸Šæ„å»ºè®­ç»ƒç›®æ ‡ï¼Œè€Œä¸æ˜¯æœ€ç»ˆçš„è¾“å‡ºã€‚è¿™æ ·å¯ä»¥è¿«ä½¿æ³¨æ„åŠ›åˆ†æ•°å…³æ³¨ä¸Šä¸‹æ–‡ä¸­å‡ºç°çš„æ­£ç¡®æ ‡ç­¾ä»¤ç‰Œï¼Œå¹¶å‡å°‘æ¥è‡ªé”™è¯¯æ ‡ç­¾ä»¤ç‰Œçš„æ³¨æ„åŠ›åˆ†æ•°ã€‚æˆ‘ä»¬åœ¨9ä¸ªç°ä»£è¯­è¨€æ¨¡å‹å’Œ8ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»éªŒå‘ç°ï¼ŒABFTåœ¨æ€§èƒ½ã€ç¨³å¥æ€§ã€å…¬æ­£æ€§å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œè€Œä¸”åªæœ‰çº¦0.01%çš„æ•°æ®æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åç»­åˆ†æå‘ç°ï¼Œç«¯åˆ°ç«¯çš„è®­ç»ƒç›®æ ‡åŒ…å«ABFTç›®æ ‡ï¼Œè¿™è¡¨æ˜ICLé£æ ¼æ•°æ®å¯¹å½’çº³å¤´å‡ºç°çš„éšå«åè§ã€‚æˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†æ§åˆ¶LMå†…ç‰¹å®šæ¨¡å—åºåˆ—ä»¥æ”¹å–„å…¶è¡Œä¸ºçš„å¯èƒ½æ€§ï¼Œä¸ºæœªæ¥çš„æœºæ¢°è§£é‡Šæ€§åº”ç”¨å¼€è¾Ÿäº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14233v2">PDF</a> 28 pages, 31 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡é’ˆå¯¹ In-context Learningï¼ˆICLï¼‰åœ¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰ä¸Šçš„åº”ç”¨è¿›è¡Œäº†ä¼˜åŒ–ç ”ç©¶ã€‚é’ˆå¯¹ICLä¸é¢„è®­ç»ƒä¹‹é—´çš„é¸¿æ²Ÿé—®é¢˜ï¼Œæå‡ºäº†Attention Behavior Fine-Tuningï¼ˆABFTï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨å¯¹ICLå†…åœ¨æœºåˆ¶çš„ç ”ç©¶ç»“æœæ„å»ºè®­ç»ƒç›®æ ‡ï¼Œé€šè¿‡å…³æ³¨æ³¨æ„åŠ›åˆ†æ•°è€Œéæœ€ç»ˆè¾“å‡ºï¼Œä½¿æ³¨æ„åŠ›èšç„¦äºæ­£ç¡®æ ‡ç­¾ï¼Œå¹¶å‡å°‘é”™è¯¯æ ‡ç­¾çš„å½±å“ã€‚å®éªŒè¯æ˜ï¼ŒABFTåœ¨æ€§èƒ½ã€ç¨³å¥æ€§ã€å…¬å¹³æ€§å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸”æ•°æ®æˆæœ¬æä½ã€‚åŒæ—¶ï¼Œåˆ†æå‘ç°ç«¯åˆ°ç«¯çš„è®­ç»ƒç›®æ ‡åŒ…å«ABFTç›®æ ‡ï¼Œæ­ç¤ºäº†ICLæ•°æ®å¯¹å½’çº³å¤´å‡ºç°çš„éšæ€§åè§ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†æ§åˆ¶LMå†…éƒ¨ç‰¹å®šæ¨¡å—åºåˆ—ä»¥æé«˜å…¶æ€§èƒ½çš„å¯èƒ½æ€§ï¼Œä¸ºæœªæ¥çš„æœºæ¢°è§£é‡Šæ€§åº”ç”¨å¼€è¾Ÿäº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICLåˆ©ç”¨ç»“æ„åŒ–çš„æ¼”ç¤ºæŸ¥è¯¢è¾“å…¥æ¥è¯±å¯¼è¯­è¨€æ¨¡å‹çš„å°‘æ ·æœ¬å­¦ä¹ ã€‚</li>
<li>ABFTæ–¹æ³•æ—¨åœ¨å‡å°‘å¤§è§„æ¨¡è®¡ç®—æˆæœ¬ï¼Œé€šè¿‡åˆ©ç”¨å¯¹ICLå†…åœ¨æœºåˆ¶çš„ç ”ç©¶ç»“æœæ„å»ºè®­ç»ƒç›®æ ‡ã€‚</li>
<li>ABFTæ–¹æ³•å…³æ³¨æ³¨æ„åŠ›åˆ†æ•°ï¼Œä½¿æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ä¸­èšç„¦äºæ­£ç¡®çš„æ ‡ç­¾æ ‡è®°ã€‚</li>
<li>ABFTåœ¨æ€§èƒ½ã€ç¨³å¥æ€§ã€å…¬å¹³æ€§å’Œæ•ˆç‡æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæ•°æ®æˆæœ¬æä½ã€‚</li>
<li>ç«¯åˆ°ç«¯çš„è®­ç»ƒç›®æ ‡åŒ…å«ABFTç›®æ ‡ï¼Œæ˜¾ç¤ºICLæ•°æ®çš„éšæ€§åè§ã€‚</li>
<li>æœ¬ç ”ç©¶å±•ç¤ºäº†æ§åˆ¶LMå†…éƒ¨ç‰¹å®šæ¨¡å—åºåˆ—ä»¥æé«˜æ€§èƒ½çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14233">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-70a28de81d0bab40b9b76d813dcafe18~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927798&auth_key=1759927798-0-0-3475cd2a340279cb5f3eac8dc4e299fe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-03247d4d0660d15f3bb7b220d8652509.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-58c6d3a6ffe6996aad6ff80bb9208859~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927812&auth_key=1759927812-0-0-2a3169d8678dbb37403e43e516c57b39&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-bf40cc35b74a8b114a0730bf6a05de4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9fbde27d1945b50f16958e3f2ef218f3.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Beyond-Synthetic-Replays-Turning-Diffusion-Features-into-Few-Shot-Class-Incremental-Learning-Knowledge"><a href="#Beyond-Synthetic-Replays-Turning-Diffusion-Features-into-Few-Shot-Class-Incremental-Learning-Knowledge" class="headerlink" title="Beyond Synthetic Replays: Turning Diffusion Features into Few-Shot   Class-Incremental Learning Knowledge"></a>Beyond Synthetic Replays: Turning Diffusion Features into Few-Shot   Class-Incremental Learning Knowledge</h2><p><strong>Authors:Junsu Kim, Yunhoe Ku, Dongyoon Han, Seungryul Baek</strong></p>
<p>Few-shot class-incremental learning (FSCIL) is challenging due to extremely limited training data while requiring models to acquire new knowledge without catastrophic forgetting. Recent works have explored generative models, particularly Stable Diffusion (SD), to address these challenges. However, existing approaches use SD mainly as a replay generator, whereas we demonstrate that SDâ€™s rich multi-scale representations can serve as a unified backbone. Motivated by this observation, we introduce Diffusion-FSCIL, which extracts four synergistic feature types from SD by capturing real image characteristics through inversion, providing semantic diversity via class-conditioned synthesis, enhancing generalization through controlled noise injection, and enabling replay without image storage through generative features. Unlike conventional approaches requiring synthetic buffers and separate classification backbones, our unified framework operates entirely in the latent space with only lightweight networks ($\approx$6M parameters). Extensive experiments on CUB-200, miniImageNet, and CIFAR-100 demonstrate state-of-the-art performance, with comprehensive ablations confirming the necessity of each feature type. Furthermore, we confirm that our streamlined variant maintains competitive accuracy while substantially improving efficiency, establishing the viability of generative models as practical and effective backbones for FSCIL. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰ç”±äºè®­ç»ƒæ•°æ®æä¸ºæœ‰é™è€Œå…·æœ‰æŒ‘æˆ˜æ€§ï¼ŒåŒæ—¶è¦æ±‚æ¨¡å‹åœ¨è·å–æ–°çŸ¥è¯†æ—¶é¿å…ç¾éš¾æ€§é—å¿˜ã€‚è¿‘æœŸçš„ç ”ç©¶å¼€å§‹æ¢ç´¢ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯Stable Diffusionï¼ˆSDï¼‰ï¼Œæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å°†SDç”¨ä½œå›æ”¾ç”Ÿæˆå™¨ï¼Œè€Œæˆ‘ä»¬è¯æ˜äº†SDçš„ä¸°å¯Œå¤šå°ºåº¦è¡¨å¾å¯ä»¥ä½œä¸ºç»Ÿä¸€çš„åç«¯æ¶æ„ã€‚å—è¿™ä¸€è§‚å¯Ÿçš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†Diffusion-FSCILï¼Œå®ƒé€šè¿‡åè½¬æ•è·çœŸå®å›¾åƒç‰¹å¾ã€é€šè¿‡ç±»åˆ«æ¡ä»¶åˆæˆæä¾›è¯­ä¹‰å¤šæ ·æ€§ã€é€šè¿‡å—æ§å™ªå£°æ³¨å…¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€ä»¥åŠé€šè¿‡ç”Ÿæˆç‰¹å¾å®ç°æ— éœ€å›¾åƒå­˜å‚¨çš„å›æ”¾ï¼Œä»SDä¸­æå–äº†å››ç§ååŒç‰¹å¾ç±»å‹ã€‚ä¸åŒäºéœ€è¦åˆæˆç¼“å†²å™¨å’Œç‹¬ç«‹åˆ†ç±»åç«¯çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œæˆ‘ä»¬çš„ç»Ÿä¸€æ¡†æ¶ä»…åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œä½¿ç”¨è½»é‡çº§ç½‘ç»œï¼ˆçº¦6Må‚æ•°ï¼‰ã€‚åœ¨CUB-200ã€miniImageNetå’ŒCIFAR-100ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†å…¶å“è¶Šæ€§èƒ½ï¼Œç»¼åˆæ¶ˆèç ”ç©¶è¯å®äº†æ¯ç§ç‰¹å¾ç±»å‹çš„å¿…è¦æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç¡®è®¤ï¼Œæˆ‘ä»¬çš„ç®€åŒ–ç‰ˆæœ¬åœ¨ä¿æŒç«äº‰åŠ›çš„å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå¤§å¤§æé«˜äº†æ•ˆç‡ï¼Œè¯æ˜äº†ç”Ÿæˆæ¨¡å‹ä½œä¸ºFSCILå®ç”¨æœ‰æ•ˆåç«¯çš„å¯è¡Œæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23402v2">PDF</a> pre-print ver 2</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºDiffusionæ¨¡å‹çš„Few-Shotç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨Diffusionæ¨¡å‹ä¸°å¯Œçš„å¤šå°ºåº¦è¡¨ç¤ºä½œä¸ºç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡æå–å››ç§ååŒç‰¹å¾ç±»å‹ï¼Œå®ç°æ— éœ€å­˜å‚¨å›¾åƒçš„å›æ”¾åŠŸèƒ½ï¼Œä¸”æ“ä½œå®Œå…¨åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œï¼Œå…·æœ‰é«˜æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†å‡ ç§æŒ‘æˆ˜åŠå…¶åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜åŒ…æ‹¬æœ‰é™çš„è®­ç»ƒæ•°æ®å’Œæ¨¡å‹éœ€è¦è·å–æ–°çŸ¥è¯†è€Œä¸é—å¿˜æ—§çŸ¥è¯†çš„é—®é¢˜ã€‚</li>
<li>å¼ºè°ƒäº†Diffusionæ¨¡å‹åœ¨è§£å†³è¿™äº›é—®é¢˜æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶æ¢è®¨äº†Diffusionæ¨¡å‹çš„å¤šå°ºåº¦è¡¨ç¤ºçš„ä¼˜åŠ¿ã€‚ä»‹ç»äº†å¦‚ä½•åˆ©ç”¨è¿™ç§è¡¨ç¤ºä½œä¸ºç»Ÿä¸€æ¡†æ¶æ¥å¤„ç†FSCILé—®é¢˜ã€‚</li>
<li>ä»‹ç»äº†Diffusion-FSCILæ–¹æ³•ä¸­çš„å››ç§ååŒç‰¹å¾ç±»å‹ï¼ŒåŒ…æ‹¬é€šè¿‡åè½¬æ•è·çœŸå®å›¾åƒç‰¹å¾ã€é€šè¿‡ç±»åˆ«æ¡ä»¶åˆæˆæä¾›è¯­ä¹‰å¤šæ ·æ€§ã€é€šè¿‡å—æ§å™ªå£°æ³¨å…¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ä»¥åŠåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œå›æ”¾çš„åŠŸèƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23402">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14497a53ffce8eeb94b8b36227eca5d3.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-39927f60318115a5fce0f6910366857f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927841&auth_key=1759927841-0-0-86a99b995110e93e76b6bc96c9cda591&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f803d221864b77bd4006be1373cee3d5~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927848&auth_key=1759927848-0-0-27e608cc8ada61919c94dc5779a3e936&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e3cb6e5ad7be007ce94c59e9399371c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927855&auth_key=1759927855-0-0-53a8c2342a06c6dd312356fbe48e1bdc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a60061158424c061b680c76b867de6b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927861&auth_key=1759927861-0-0-1bf27926b7d5c316d8f2ec73c02b9ee2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Pre-training-Epidemic-Time-Series-Forecasters-with-Compartmental-Prototypes"><a href="#Pre-training-Epidemic-Time-Series-Forecasters-with-Compartmental-Prototypes" class="headerlink" title="Pre-training Epidemic Time Series Forecasters with Compartmental   Prototypes"></a>Pre-training Epidemic Time Series Forecasters with Compartmental   Prototypes</h2><p><strong>Authors:Zewen Liu, Juntong Ni, Max S. Y. Lau, Wei Jin</strong></p>
<p>Accurate epidemic forecasting is crucial for outbreak preparedness, but existing data-driven models are often brittle. Typically trained on a single pathogen, they struggle with data scarcity during new outbreaks and fail under distribution shifts caused by viral evolution or interventions. However, decades of surveillance data from diverse diseases offer an untapped source of transferable knowledge. To leverage the collective lessons from history, we propose CAPE, the first open-source pre-trained model for epidemic forecasting. Unlike existing time series foundation models that overlook epidemiological challenges, CAPE models epidemic dynamics as mixtures of latent population states, termed compartmental prototypes. It discovers a flexible dictionary of compartment prototypes directly from surveillance data, enabling each outbreak to be expressed as a time-varying mixture that links observed infections to latent population states. To promote robust generalization, CAPE combines self-supervised pre-training objectives with lightweight epidemic-aware regularizers that align the learned prototypes with epidemiological semantics. On a comprehensive benchmark spanning 17 diseases and 50+ regions, CAPE significantly outperforms strong baselines in zero-shot, few-shot, and full-shot forecasting. This work represents a principled step toward pre-trained epidemic models that are both transferable and epidemiologically grounded. </p>
<blockquote>
<p>å‡†ç¡®çš„ç–«æƒ…é¢„æµ‹å¯¹äºç–«æƒ…å‡†å¤‡è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„æ•°æ®é©±åŠ¨æ¨¡å‹å¾€å¾€å¾ˆè„†å¼±ã€‚è¿™äº›æ¨¡å‹é€šå¸¸é’ˆå¯¹å•ä¸€ç—…åŸä½“è¿›è¡Œè®­ç»ƒï¼Œåœ¨æ–°ç–«æƒ…æœŸé—´é¢ä¸´æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œå¹¶ä¸”åœ¨ç—…æ¯’è¿›åŒ–æˆ–å¹²é¢„æªæ–½å¼•èµ·çš„åˆ†å¸ƒå˜åŒ–ä¸‹ä¼šå¤±æ•ˆã€‚ç„¶è€Œï¼Œæ¥è‡ªå¤šç§ç–¾ç—…çš„æ•°åå¹´ç›‘æ§æ•°æ®æä¾›äº†å°šæœªå¼€å‘çš„çŸ¥è¯†è½¬ç§»æ¥æºã€‚ä¸ºäº†åˆ©ç”¨å†å²çš„é›†ä½“æ•™è®­ï¼Œæˆ‘ä»¬æå‡ºäº†CAPEï¼Œè¿™æ˜¯ç”¨äºç–«æƒ…é¢„æµ‹çš„ç¬¬ä¸€ä¸ªå¼€æºé¢„è®­ç»ƒæ¨¡å‹ã€‚ä¸å¿½è§†æµè¡Œç—…å­¦æŒ‘æˆ˜çš„ç°æœ‰æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ä¸åŒï¼ŒCAPEå°†ç–«æƒ…åŠ¨æ€å»ºæ¨¡ä¸ºæ½œåœ¨äººå£çŠ¶æ€æ··åˆä½“ï¼Œç§°ä¸ºâ€œéš”å®¤åŸå‹â€ã€‚å®ƒç›´æ¥ä»ç›‘æ§æ•°æ®ä¸­å‘ç°äº†çµæ´»çš„éš”å®¤åŸå‹è¯å…¸ï¼Œä½¿æ¯ä¸ªç–«æƒ…éƒ½èƒ½è¡¨è¾¾ä¸ºéšæ—¶é—´å˜åŒ–æ··åˆä½“ï¼Œå°†è§‚å¯Ÿåˆ°çš„æ„ŸæŸ“ä¸æ½œåœ¨äººå£çŠ¶æ€è”ç³»èµ·æ¥ã€‚ä¸ºäº†ä¿ƒè¿›ç¨³å¥çš„æ³›åŒ–ï¼ŒCAPEç»“åˆäº†è‡ªæˆ‘ç›‘ç£çš„é¢„è®­ç»ƒç›®æ ‡ä¸è½»é‡çº§çš„ç–«æƒ…æ„è¯†è°ƒèŠ‚å™¨ï¼Œä½¿å­¦åˆ°çš„åŸå‹ä¸æµè¡Œç—…å­¦è¯­ä¹‰ç›¸ç¬¦ã€‚åœ¨æ¶µç›–17ç§ç–¾ç—…å’Œ50å¤šä¸ªåœ°åŒºçš„ç»¼åˆåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCAPEåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå…¨æ ·æœ¬é¢„æµ‹æ–¹é¢æ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ã€‚è¿™é¡¹å·¥ä½œæœç€æ—¢å¯è½¬ç§»åˆåŸºäºæµè¡Œç—…å­¦çš„é¢„è®­ç»ƒç–«æƒ…æ¨¡å‹è¿ˆå‡ºäº†æœ‰åŸåˆ™çš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03393v4">PDF</a> version 2.0</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶å¼ºè°ƒäº†ä¼ æŸ“ç—…ç²¾å‡†é¢„æµ‹å¯¹äºåº”å¯¹ç–«æƒ…çˆ†å‘çš„é‡è¦æ€§ï¼Œä½†ç°æœ‰æ•°æ®é©±åŠ¨æ¨¡å‹åœ¨é¢å¯¹æ–°ç–«æƒ…çˆ†å‘æ—¶å¸¸å¸¸å­˜åœ¨å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æ•°åå¹´æ¥çš„å¤šç§ç–¾ç—…ç›‘æµ‹æ•°æ®æå‡ºäº†CAPEæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„é¢„å…ˆè®­ç»ƒå¥½çš„ä¼ æŸ“ç—…é¢„æµ‹æ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åˆ†åŒºåŸå‹æ³•æ¥æ¨¡æ‹Ÿä¼ æŸ“ç—…çš„åŠ¨æ€è¿‡ç¨‹ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†æ–°çš„ä¼ æŸ“ç—…å’Œä»¥å¾€ä¸åŒçš„ç–«æƒ…ã€‚åŒæ—¶ï¼Œä¸ºäº†å¢å¼ºæ¨¡å‹çš„é€šç”¨æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜ç»“åˆäº†è‡ªç›‘ç£é¢„è®­ç»ƒç›®æ ‡å’Œè½»é‡çº§ä¼ æŸ“ç—…ç›¸å…³æ­£åˆ™åŒ–å™¨æ¥å®ç°æ¨¡å‹çš„å­¦ä¹ ä¸ç–¾ç—…çš„æµè¡Œç—…å­¦ç‰¹å¾ä¿æŒä¸€è‡´ã€‚ç»è¿‡å¤§è§„æ¨¡éªŒè¯ï¼ŒCAPEæ¨¡å‹åœ¨å¤šç§ç–¾ç—…å’Œåœ°åŒºçš„é¢„æµ‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºå…¶è·¨ç—…ç§å’ŒåŒºåŸŸçš„é¢„æµ‹èƒ½åŠ›ã€‚è¯¥ç ”ç©¶çš„æˆåŠŸæ ‡å¿—ç€é¢„è®­ç»ƒä¼ æŸ“ç—…æ¨¡å‹çš„é‡å¤§è¿›æ­¥ï¼Œè¿™äº›æ¨¡å‹æ—¢å…·æœ‰å¯è¿ç§»æ€§åˆåŸºäºæµè¡Œç—…å­¦ç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ•°æ®é©±åŠ¨æ¨¡å‹åœ¨ä¼ æŸ“ç—…é¢„æµ‹æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹æ–°ç–«æƒ…çˆ†å‘æ—¶ã€‚</li>
<li>CAPEæ¨¡å‹åˆ©ç”¨å¤šç§ç–¾ç—…çš„ç›‘æµ‹æ•°æ®ä½œä¸ºè®­ç»ƒèµ„æºï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>CAPEæ¨¡å‹é‡‡ç”¨åˆ†åŒºåŸå‹æ³•æ¨¡æ‹Ÿä¼ æŸ“ç—…çš„åŠ¨æ€è¿‡ç¨‹ï¼Œå¯ä»¥æ›´å¥½åœ°å¤„ç†æ–°çš„ä¼ æŸ“ç—…å’Œä»¥å¾€ä¸åŒçš„ç–«æƒ…ã€‚</li>
<li>è‡ªç›‘ç£é¢„è®­ç»ƒç›®æ ‡å’Œè½»é‡çº§ä¼ æŸ“ç—…ç›¸å…³æ­£åˆ™åŒ–å™¨çš„ç»“åˆæœ‰åŠ©äºæé«˜æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§å¹¶å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>CAPEæ¨¡å‹åœ¨ä¸åŒç–¾ç—…å’Œåœ°åŒºçš„é¢„æµ‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºå…¶è·¨ç—…ç§å’ŒåŒºåŸŸçš„é¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>CAPEæ¨¡å‹çš„æå‡ºæ ‡å¿—ç€é¢„è®­ç»ƒä¼ æŸ“ç—…æ¨¡å‹çš„é‡å¤§è¿›æ­¥ï¼Œæ—¢å…·æœ‰å¯è¿ç§»æ€§åˆåŸºäºæµè¡Œç—…å­¦ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03393">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6e4fec82b9e981deab52788010d9c1bb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927869&auth_key=1759927869-0-0-76688e002fc3ebe754510579d317e83b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-d1ecd81e3caf7d0866c0b26faf50115a.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-3ae6520fd87c0e826d55028da9b74e29~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927884&auth_key=1759927884-0-0-8c2dbdad74edfd4dfbdd5f84a48b1b42&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-01/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-c136af892265be20759cfe90474d6a9c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759927891&auth_key=1759927891-0-0-86d73eb6a8f2964a90194a6d121b563d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  ThermalGen Style-Disentangled Flow-Based Generative Models for   RGB-to-Thermal Image Translation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-01/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-93cb44bdbf4381d392e7c2881b18dcd7.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-01  ReasoningBank Scaling Agent Self-Evolving with Reasoning Memory
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31987.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
