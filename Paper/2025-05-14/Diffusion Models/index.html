<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  DanceGRPO Unleashing GRPO on Visual Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2d7626b360820d90ff0b18a4d6db5aa3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-14-æ›´æ–°"><a href="#2025-05-14-æ›´æ–°" class="headerlink" title="2025-05-14 æ›´æ–°"></a>2025-05-14 æ›´æ–°</h1><h2 id="DanceGRPO-Unleashing-GRPO-on-Visual-Generation"><a href="#DanceGRPO-Unleashing-GRPO-on-Visual-Generation" class="headerlink" title="DanceGRPO: Unleashing GRPO on Visual Generation"></a>DanceGRPO: Unleashing GRPO on Visual Generation</h2><p><strong>Authors:Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, Ping Luo</strong></p>
<p>Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image&#x2F;video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released. </p>
<blockquote>
<p>æœ€è¿‘ç”Ÿæˆæ¨¡å‹â€”â€”å°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹å’Œæ ¡æ­£æµâ€”â€”çš„çªç ´ä¸ºè§†è§‰å†…å®¹åˆ›ä½œå¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ï¼Œä½†å°†æ¨¡å‹è¾“å‡ºä¸äººçš„åå¥½å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è§†è§‰ç”Ÿæˆæ–¹æ³•é¢ä¸´é‡è¦å±€é™ï¼šä¸ç°ä»£åŸºäºå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰çš„é‡‡æ ·èŒƒå¼ä¸å…¼å®¹ã€å¤§è§„æ¨¡è®­ç»ƒä¸ç¨³å®šã€ä»¥åŠè§†é¢‘ç”Ÿæˆçš„éªŒè¯ç¼ºä¹ã€‚æœ¬æ–‡ä»‹ç»äº†DanceGRPOï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†Group Relative Policy Optimization (GRPO)é€‚åº”è§†è§‰ç”ŸæˆèŒƒå¼çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé‡Šæ”¾äº†ä¸€ç§ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ¶µç›–ä¸¤ç§ç”ŸæˆèŒƒå¼ï¼ˆæ‰©æ•£æ¨¡å‹å’Œæ ¡æ­£æµï¼‰ã€ä¸‰ç§ä»»åŠ¡ï¼ˆæ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘ã€å›¾åƒåˆ°è§†é¢‘ï¼‰ã€å››ç§åŸºç¡€æ¨¡å‹ï¼ˆStable Diffusionã€HunyuanVideoã€FLUXã€SkyReel-I2Vï¼‰ï¼Œä»¥åŠäº”ç§å¥–åŠ±æ¨¡å‹ï¼ˆå›¾åƒ&#x2F;è§†é¢‘ç¾å­¦ã€æ–‡æœ¬-å›¾åƒå¯¹é½ã€è§†é¢‘è¿åŠ¨è´¨é‡ã€äºŒå…ƒå¥–åŠ±ï¼‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒDanceGRPOæ˜¯ç¬¬ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„ç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„ç”ŸæˆèŒƒå¼ã€ä»»åŠ¡ã€åŸºç¡€æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹ä¹‹é—´è¿›è¡Œæ— ç¼é€‚åº”ã€‚DanceGRPOè¡¨ç°å‡ºäº†æŒç»­è€Œæ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨HPS-v2.1ã€CLIP Scoreã€VideoAlignå’ŒGenEvalç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿é«˜è¾¾181%ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒDanceGRPOä¸ä»…å¯ç¨³å®šå¤æ‚è§†é¢‘ç”Ÿæˆçš„ç­–ç•¥ä¼˜åŒ–ï¼Œè¿˜èƒ½å¤Ÿä½¿ç”Ÿæˆç­–ç•¥æ›´å¥½åœ°æ•æ‰å»å™ªè½¨è¿¹ï¼Œç”¨äºBest-of-Næ¨ç†æ‰©å±•å¹¶ä»ç¨€ç–çš„äºŒå…ƒåé¦ˆä¸­å­¦ä¹ ã€‚æˆ‘ä»¬çš„ç»“æœç¡®ç«‹äº†DanceGRPOåœ¨è§†è§‰ç”Ÿæˆä¸­æ‰©å±•å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä»»åŠ¡çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§è§£å†³æ–¹æ¡ˆï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä¸åˆæˆè§†è§‰çš„å’Œè°èåˆæä¾›äº†æ–°çš„è§è§£ã€‚ä»£ç å°†è¢«å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07818v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://dancegrpo.github.io/">https://dancegrpo.github.io/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹å’Œæ ¡æ­£æµï¼‰çš„çªç ´ä¸ºè§†è§‰å†…å®¹åˆ›ä½œå¸¦æ¥é©å‘½æ€§å˜åŒ–ï¼Œä½†å¦‚ä½•ä½¿æ¨¡å‹è¾“å‡ºç¬¦åˆäººç±»åå¥½ä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è§†è§‰ç”Ÿæˆæ–¹æ³•å­˜åœ¨ä¸ç°ä»£åŸºäºå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEsï¼‰çš„é‡‡æ ·èŒƒå¼ä¸å…¼å®¹ã€å¤§è§„æ¨¡è®­ç»ƒä¸ç¨³å®šä»¥åŠè§†é¢‘ç”ŸæˆéªŒè¯ç¼ºä¹ç­‰é—®é¢˜ã€‚æœ¬æ–‡å¼•å…¥DanceGRPOï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†Group Relative Policy Optimization (GRPO)é€‚åº”äºè§†è§‰ç”ŸæˆèŒƒå¼çš„ç»Ÿä¸€æ¡†æ¶ï¼Œä½¿ç”¨ä¸€ä¸ªç»Ÿä¸€çš„RLç®—æ³•ï¼Œè·¨è¶Šä¸¤ç§ç”ŸæˆèŒƒå¼ï¼ˆæ‰©æ•£æ¨¡å‹å’Œæ ¡æ­£æµï¼‰ï¼Œä¸‰é¡¹ä»»åŠ¡ï¼ˆæ–‡æœ¬è½¬å›¾åƒã€æ–‡æœ¬è½¬è§†é¢‘ã€å›¾åƒè½¬è§†é¢‘ï¼‰ï¼Œå››ç§åŸºç¡€æ¨¡å‹ï¼ˆStable Diffusionã€HunyuanVideoã€FLUXã€SkyReel-I2Vï¼‰å’Œäº”ç§å¥–åŠ±æ¨¡å‹ï¼ˆå›¾åƒ&#x2F;è§†é¢‘ç¾å­¦ã€æ–‡æœ¬-å›¾åƒå¯¹é½ã€è§†é¢‘è¿åŠ¨è´¨é‡ã€äºŒå…ƒå¥–åŠ±ï¼‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒDanceGRPOæ˜¯é¦–ä¸ªèƒ½å¤Ÿåœ¨å¤šç§ç”ŸæˆèŒƒå¼ã€ä»»åŠ¡ã€åŸºç¡€æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹ä¹‹é—´æ— ç¼é€‚åº”çš„RLç»Ÿä¸€æ¡†æ¶ã€‚DanceGRPOåœ¨HPS-v2.1ã€CLIP Scoreã€VideoAlignå’ŒGenEvalç­‰åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æŒç»­ä¸”æ˜¾è‘—çš„æ”¹è¿›ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ€é«˜æå‡äº†181%ã€‚DanceGRPOä¸ä»…èƒ½ç¨³å®šå¤æ‚è§†é¢‘ç”Ÿæˆçš„ç­–ç•¥ä¼˜åŒ–ï¼Œè¿˜èƒ½ä½¿ç”Ÿæˆç­–ç•¥æ›´å¥½åœ°æ•æ‰å»å™ªè½¨è¿¹ï¼Œæ”¯æŒBest-of-Næ¨ç†æ‰©å±•å¹¶ä»ç¨€ç–äºŒå…ƒåé¦ˆä¸­å­¦ä¹ ã€‚ç ”ç©¶ç»“æœç¡®ç«‹äº†DanceGRPOåœ¨è§†è§‰ç”Ÿæˆé¢†åŸŸå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä»»åŠ¡çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä¸åˆæˆè§†è§‰çš„å’Œè°ç»“åˆæä¾›äº†æ–°çš„è§è§£ã€‚ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å’Œæ ¡æ­£æµç­‰ç”Ÿæˆæ¨¡å‹çš„çªç ´ä¸ºè§†è§‰å†…å®¹åˆ›ä½œå¸¦æ¥è¿›æ­¥ï¼Œä½†ä»æœ‰æ¨¡å‹è¾“å‡ºä¸äººç±»åå¥½å¯¹é½çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è§†è§‰ç”Ÿæˆæ–¹æ³•å­˜åœ¨è¯¸å¤šé™åˆ¶ï¼Œå¦‚ä¸ç°ä»£é‡‡æ ·èŒƒå¼ä¸å…¼å®¹ã€è®­ç»ƒä¸ç¨³å®šä»¥åŠè§†é¢‘ç”ŸæˆéªŒè¯ç¼ºä¹ã€‚</li>
<li>DanceGRPOæ˜¯é¦–ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå°†Group Relative Policy Optimization (GRPO)åº”ç”¨äºè§†è§‰ç”Ÿæˆï¼Œæ”¯æŒå¤šç§ç”ŸæˆèŒƒå¼ã€ä»»åŠ¡ã€åŸºç¡€æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹ã€‚</li>
<li>DanceGRPOå®ç°äº†åœ¨å„é¡¹åŸºå‡†æµ‹è¯•ä¸Šçš„æ˜¾è‘—æ”¹è¿›ï¼Œæœ€é«˜æå‡è¾¾181%ã€‚</li>
<li>DanceGRPOèƒ½ç¨³å®šå¤æ‚è§†é¢‘ç”Ÿæˆçš„ç­–ç•¥ä¼˜åŒ–ï¼Œå¹¶æ•æ‰å»å™ªè½¨è¿¹ï¼Œæ”¯æŒBest-of-Næ¨ç†æ‰©å±•ã€‚</li>
<li>DanceGRPOèƒ½ä»ç¨€ç–äºŒå…ƒåé¦ˆä¸­å­¦ä¹ ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä¸è§†è§‰åˆæˆçš„ç»“åˆæä¾›äº†æ–°çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07818">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2fec2de9c2417547490ce2852cf3dec3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fab798b748ba34cb5165f16be7cbbc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cdcefc2c62b6f11a626e04327dd794f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Pixel-Motion-as-Universal-Representation-for-Robot-Control"><a href="#Pixel-Motion-as-Universal-Representation-for-Robot-Control" class="headerlink" title="Pixel Motion as Universal Representation for Robot Control"></a>Pixel Motion as Universal Representation for Robot Control</h2><p><strong>Authors:Kanchana Ranasinghe, Xiang Li, Cristina Mata, Jongwoo Park, Michael S Ryoo</strong></p>
<p>We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. Our high-level System 2, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame to guide robot control. Pixel motion-a universal, interpretable, and motion-centric representation-can be extracted from videos in a self-supervised manner, enabling diffusion model training on web-scale video-caption data. Treating generated pixel motion as learned universal representations, our low level System 1 module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision. System 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals. This hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action. Checkout <a target="_blank" rel="noopener" href="https://kahnchana.github.io/LangToMo">https://kahnchana.github.io/LangToMo</a> for visualizations. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†LangToMoï¼Œè¿™æ˜¯ä¸€ä¸ªè§†è§‰è¯­è¨€è¡ŒåŠ¨æ¡†æ¶ï¼Œé‡‡ç”¨åŒé‡ç³»ç»Ÿæ¶æ„ï¼Œä»¥åƒç´ è¿åŠ¨é¢„æµ‹ä½œä¸ºä¸­é—´è¡¨ç¤ºå½¢å¼ã€‚æˆ‘ä»¬çš„é«˜çº§ç³»ç»Ÿ2æ˜¯ä¸€ä¸ªå›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä»å•å¸§ç”Ÿæˆæ–‡æœ¬æ§åˆ¶çš„åƒç´ è¿åŠ¨åºåˆ—ï¼Œä»¥æŒ‡å¯¼æœºå™¨äººæ§åˆ¶ã€‚åƒç´ è¿åŠ¨æ˜¯ä¸€ç§é€šç”¨ã€å¯è§£é‡Šã€ä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„è¡¨ç°å½¢å¼ï¼Œå¯ä»¥ä»¥è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼ä»è§†é¢‘ä¸­æå–ï¼Œä»è€Œå®ç°æ‰©æ•£æ¨¡å‹åœ¨ç½‘é¡µè§„æ¨¡è§†é¢‘å­—å¹•æ•°æ®ä¸Šçš„è®­ç»ƒã€‚å°†ç”Ÿæˆçš„åƒç´ è¿åŠ¨è§†ä¸ºå­¦ä¹ çš„é€šç”¨è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„ä½çº§ç³»ç»Ÿ1æ¨¡å—é€šè¿‡è¿åŠ¨åˆ°åŠ¨ä½œçš„æ˜ å°„å‡½æ•°å°†è¿™äº›è¡¨ç¤ºè½¬æ¢ä¸ºæœºå™¨äººåŠ¨ä½œï¼Œè¿™äº›æ˜ å°„å‡½æ•°å¯ä»¥æ˜¯æ‰‹å·¥åˆ¶ä½œçš„ï¼Œä¹Ÿå¯ä»¥åœ¨æœ€å°‘çš„ç›‘ç£ä¸‹å­¦ä¹ ã€‚ç³»ç»Ÿ2ä½œä¸ºé«˜çº§ç­–ç•¥ï¼Œåœ¨ç¨€ç–çš„æ—¶é—´é—´éš”å†…è¿è¡Œï¼Œè€Œç³»ç»Ÿ1åˆ™åœ¨å¯†é›†çš„æ—¶é—´é—´éš”å†…ä½œä¸ºä½çº§ç­–ç•¥è¿è¡Œã€‚è¿™ç§å±‚æ¬¡åŒ–çš„è§£è€¦ä½¿æœºå™¨äººåœ¨æ— ç›‘ç£å’Œç›‘ç£çš„ç¯å¢ƒä¸‹éƒ½èƒ½å®ç°çµæ´»ã€å¯æ‰©å±•å’Œé€šç”¨çš„æ§åˆ¶ï¼Œä»è€Œç¼©å°äº†è¯­è¨€ã€åŠ¨ä½œå’Œè¡ŒåŠ¨ä¹‹é—´çš„å·®è·ã€‚æƒ³äº†è§£æ›´å¤šå¯è§†åŒ–å†…å®¹ï¼Œè¯·è®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://kahnchana.github.io/LangToMo/]">https://kahnchana.github.io/LangToMo/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07817v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LangToMoæ˜¯ä¸€ä¸ªç»“åˆè§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œçš„æ¡†æ¶ï¼Œé‡‡ç”¨åŒç³»ç»Ÿæ¶æ„å¹¶åˆ©ç”¨åƒç´ è¿åŠ¨é¢„æµ‹ä½œä¸ºä¸­é—´è¡¨ç¤ºã€‚å…¶é«˜çº§ç³»ç»Ÿ2é‡‡ç”¨å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä»å•å¸§ç”Ÿæˆæ–‡æœ¬è°ƒèŠ‚çš„åƒç´ è¿åŠ¨åºåˆ—ï¼ŒæŒ‡å¯¼æœºå™¨äººæ§åˆ¶ã€‚åƒç´ è¿åŠ¨æ˜¯ä¸€ç§é€šç”¨ã€å¯è§£é‡Šã€ä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„è¡¨ç°å½¢å¼ï¼Œå¯ä»è§†é¢‘ä¸­è‡ªæˆ‘ç›‘ç£æå–ï¼Œä½¿æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿåœ¨ç½‘é¡µè§„æ¨¡çš„è§†é¢‘å­—å¹•æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚å°†ç”Ÿæˆçš„åƒç´ è¿åŠ¨è§†ä¸ºå­¦ä¹ åˆ°çš„é€šç”¨è¡¨ç¤ºï¼Œä½çº§ç³»ç»Ÿ1æ¨¡å—é€šè¿‡è¿åŠ¨åˆ°åŠ¨ä½œçš„æ˜ å°„å‡½æ•°å°†è¿™äº›è¡¨ç¤ºè½¬åŒ–ä¸ºæœºå™¨äººåŠ¨ä½œï¼Œè¿™äº›æ˜ å°„å‡½æ•°å¯ä»¥æ˜¯æ‰‹å·¥åˆ¶ä½œçš„ï¼Œä¹Ÿå¯ä»¥æ˜¯æœ€å°‘ç›‘ç£å­¦ä¹ çš„ã€‚ç³»ç»Ÿ2ä½œä¸ºé«˜çº§ç­–ç•¥åœ¨ç¨€ç–æ—¶é—´é—´éš”å†…è¿è¡Œï¼Œè€Œç³»ç»Ÿ1ä½œä¸ºä½çº§ç­–ç•¥åœ¨å¯†é›†æ—¶é—´é—´éš”å†…è¿è¡Œã€‚è¿™ç§åˆ†å±‚è§£è€¦å®ç°äº†çµæ´»çš„ã€å¯æ‰©å±•çš„å’Œé€šç”¨çš„æœºå™¨äººæ§åˆ¶ï¼Œæ— è®ºæ˜¯åœ¨æ— ç›‘ç£è¿˜æ˜¯ç›‘ç£è®¾ç½®ä¸‹éƒ½èƒ½ç¼©å°è¯­è¨€ã€è¿åŠ¨å’ŒåŠ¨ä½œä¹‹é—´çš„å·®è·ã€‚æ›´å¤šå¯è§†åŒ–å†…å®¹è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://kahnchana.github.io/LangToMo">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LangToMoæ˜¯ä¸€ä¸ªèåˆè§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œçš„æ¡†æ¶ï¼Œå…·æœ‰åŒç³»ç»Ÿæ¶æ„ã€‚</li>
<li>æ¡†æ¶åˆ©ç”¨åƒç´ è¿åŠ¨é¢„æµ‹ä½œä¸ºä¸­é—´è¡¨ç¤ºå½¢å¼ã€‚</li>
<li>é«˜çº§ç³»ç»Ÿ2é‡‡ç”¨å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆæ–‡æœ¬è°ƒèŠ‚çš„åƒç´ è¿åŠ¨åºåˆ—ï¼ŒæŒ‡å¯¼æœºå™¨äººæ§åˆ¶ã€‚</li>
<li>åƒç´ è¿åŠ¨å¯ä»è§†é¢‘ä¸­è‡ªæˆ‘ç›‘ç£æå–ï¼Œå¹¶ç”¨äºè®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>ä½çº§ç³»ç»Ÿ1æ¨¡å—å°†åƒç´ è¿åŠ¨è½¬åŒ–ä¸ºæœºå™¨äººåŠ¨ä½œã€‚</li>
<li>ç³»ç»Ÿ2å’Œç³»ç»Ÿ1åˆ†åˆ«ä½œä¸ºé«˜çº§å’Œä½çº§ç­–ç•¥è¿è¡Œï¼Œå®ç°çµæ´»çš„æœºå™¨äººæ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d11023be20cecd0201fdd7906c09d848.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ad3b5f9870cba040fbbb8c0d51d33e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a790d0112804c367a250230bd155572.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GAN-based-synthetic-FDG-PET-images-from-T1-brain-MRI-can-serve-to-improve-performance-of-deep-unsupervised-anomaly-detection-models"><a href="#GAN-based-synthetic-FDG-PET-images-from-T1-brain-MRI-can-serve-to-improve-performance-of-deep-unsupervised-anomaly-detection-models" class="headerlink" title="GAN-based synthetic FDG PET images from T1 brain MRI can serve to   improve performance of deep unsupervised anomaly detection models"></a>GAN-based synthetic FDG PET images from T1 brain MRI can serve to   improve performance of deep unsupervised anomaly detection models</h2><p><strong>Authors:Daria Zotova, Nicolas Pinon, Robin Trombetta, Romain Bouet, Julien Jung, Carole Lartizien</strong></p>
<p>Background and Objective. Research in the cross-modal medical image translation domain has been very productive over the past few years in tackling the scarce availability of large curated multimodality datasets with the promising performance of GAN-based architectures. However, only a few of these studies assessed task-based related performance of these synthetic data, especially for the training of deep models. Method. We design and compare different GAN-based frameworks for generating synthetic brain [18F]fluorodeoxyglucose (FDG) PET images from T1 weighted MRI data. We first perform standard qualitative and quantitative visual quality evaluation. Then, we explore further impact of using these fake PET data in the training of a deep unsupervised anomaly detection (UAD) model designed to detect subtle epilepsy lesions in T1 MRI and FDG PET images. We introduce novel diagnostic task-oriented quality metrics of the synthetic FDG PET data tailored to our unsupervised detection task, then use these fake data to train a use case UAD model combining a deep representation learning based on siamese autoencoders with a OC-SVM density support estimation model. This model is trained on normal subjects only and allows the detection of any variation from the pattern of the normal population. We compare the detection performance of models trained on 35 paired real MR T1 of normal subjects paired either on 35 true PET images or on 35 synthetic PET images generated from the best performing generative models. Performance analysis is conducted on 17 exams of epilepsy patients undergoing surgery. Results. The best performing GAN-based models allow generating realistic fake PET images of control subject with SSIM and PSNR values around 0.9 and 23.8, respectively and in distribution (ID) with regard to the true control dataset. The best UAD model trained on these synthetic normative PET data allows reaching 74% sensitivity. Conclusion. Our results confirm that GAN-based models are the best suited for MR T1 to FDG PET translation, outperforming transformer or diffusion models. We also demonstrate the diagnostic value of these synthetic data for the training of UAD models and evaluation on clinical exams of epilepsy patients. Our code and the normative image dataset are available. </p>
<blockquote>
<p>èƒŒæ™¯ä¸ç›®çš„ï¼šè¿‘å¹´æ¥ï¼Œåœ¨è§£å†³å¤§å‹ç²¾é€‰å¤šæ¨¡æ€æ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜æ–¹é¢ï¼Œè·¨æ¨¡æ€åŒ»å­¦å›¾åƒç¿»è¯‘é¢†åŸŸçš„ç ”ç©¶å–å¾—äº†æ˜¾è‘—æˆæœï¼ŒåŸºäºGANçš„æ¶æ„è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œåªæœ‰å°‘æ•°ç ”ç©¶è¯„ä¼°äº†è¿™äº›åˆæˆæ•°æ®åœ¨ä»»åŠ¡ç›¸å…³çš„æ€§èƒ½è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨æ·±åº¦æ¨¡å‹çš„è®­ç»ƒæ–¹é¢ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬è®¾è®¡å¹¶æ¯”è¾ƒäº†åŸºäºä¸åŒGANçš„æ¡†æ¶ï¼Œç”¨äºä»T1åŠ æƒMRIæ•°æ®ç”Ÿæˆåˆæˆè„‘[18F]æ°Ÿè„±æ°§è‘¡è„ç³–ï¼ˆFDGï¼‰PETå›¾åƒã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œæ ‡å‡†çš„å®šæ€§å’Œå®šé‡è§†è§‰è´¨é‡è¯„ä¼°ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢è®¨äº†ä½¿ç”¨è¿™äº›å‡PETæ•°æ®åœ¨è®­ç»ƒæ·±åº¦æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼ˆUADï¼‰æ¨¡å‹ä¸­çš„å½±å“ï¼Œè¯¥æ¨¡å‹æ—¨åœ¨æ£€æµ‹T1 MRIå’ŒFDG PETå›¾åƒä¸­çš„å¾®å¦™ç™«ç—«ç—…ç¶ã€‚æˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹æˆ‘ä»¬çš„æ— ç›‘ç£æ£€æµ‹ä»»åŠ¡çš„æ–°å‹è¯Šæ–­ä»»åŠ¡å¯¼å‘å‹è´¨é‡æŒ‡æ ‡ï¼Œç„¶åä½¿ç”¨è¿™äº›å‡æ•°æ®æ¥è®­ç»ƒä¸€ä¸ªç”¨ä¾‹UADæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†åŸºäºå­ªç”Ÿè‡ªç¼–ç å™¨çš„æ·±åº¦è¡¨ç¤ºå­¦ä¹ ä¸OC-SVMå¯†åº¦æ”¯æŒä¼°è®¡æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä»…åœ¨æ­£å¸¸å—è¯•è€…ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯æ£€æµ‹ä»»ä½•ä¸æ­£å¸¸äººç¾¤æ¨¡å¼çš„åå·®ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†åœ¨çœŸå®MR T1å›¾åƒï¼ˆæ­£å¸¸å—è¯•è€…é…å¯¹ï¼‰çš„35å¯¹å›¾åƒä¸Šè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ï¼Œè¿™äº›å›¾åƒè¦ä¹ˆä¸çœŸå®çš„PETå›¾åƒé…å¯¹ï¼Œè¦ä¹ˆä¸ç”±è¡¨ç°æœ€ä½³çš„ç”Ÿæˆæ¨¡å‹ç”Ÿæˆçš„åˆæˆPETå›¾åƒé…å¯¹ã€‚åœ¨æ¥å—æ‰‹æœ¯çš„17ä¾‹ç™«ç—«æ‚£è€…è€ƒè¯•ä¸Šè¿›è¡Œæ€§èƒ½åˆ†æã€‚ç»“æœï¼šè¡¨ç°æœ€ä½³çš„åŸºäºGANçš„æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„å‡PETå›¾åƒï¼Œæ§åˆ¶å¯¹è±¡çš„ç»“æ„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆSSIMï¼‰å’Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å€¼åˆ†åˆ«ä¸ºçº¦0.9å’Œ23.8ï¼Œåœ¨çœŸå®æ§åˆ¶æ•°æ®é›†æ–¹é¢è¡¨ç°å‡ºèº«ä»½åˆ†å¸ƒï¼ˆIDï¼‰ã€‚ä½¿ç”¨è¿™äº›åˆæˆè§„èŒƒæ€§PETæ•°æ®è®­ç»ƒçš„æœ€ä½³UADæ¨¡å‹çš„æ•æ„Ÿæ€§è¾¾åˆ°74%ã€‚ç»“è®ºï¼šæˆ‘ä»¬çš„ç»“æœè¯å®ï¼ŒåŸºäºGANçš„æ¨¡å‹æœ€é€‚åˆäºä»MR T1åˆ°FDG PETçš„ç¿»è¯‘ä»»åŠ¡ï¼Œä¼˜äºè½¬æ¢å™¨æˆ–æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†è¿™äº›åˆæˆæ•°æ®åœ¨è®­ç»ƒUADæ¨¡å‹å’Œè¯„ä¼°ç™«ç—«æ‚£è€…ä¸´åºŠè€ƒè¯•ä¸­çš„è¯Šæ–­ä»·å€¼ã€‚æˆ‘ä»¬çš„ä»£ç å’Œè§„èŒƒå›¾åƒæ•°æ®é›†å¯ä¾›ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07364v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶åˆ©ç”¨GANæ¶æ„ç”Ÿæˆåˆæˆè„‘[18F]æ°Ÿè„±æ°§è‘¡è„ç³–ï¼ˆFDGï¼‰PETå›¾åƒï¼Œä»T1åŠ æƒMRIæ•°æ®ä¸­ç”Ÿæˆã€‚ç ”ç©¶ä¸ä»…è¯„ä¼°äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡ï¼Œè¿˜æ¢ç´¢äº†è¿™äº›åˆæˆæ•°æ®åœ¨è®­ç»ƒæ·±åº¦æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œè¯¥æ¨¡å‹æ—¨åœ¨æ£€æµ‹T1 MRIå’ŒFDG PETå›¾åƒä¸­çš„ç»†å¾®ç™«ç—«ç—…ç¶ã€‚ç ”ç©¶ç¡®è®¤GANæ¨¡å‹æœ€é€‚åˆMR T1åˆ°FDG PETçš„ç¿»è¯‘ï¼Œä¸”åˆæˆæ•°æ®å¯¹è®­ç»ƒæ— ç›‘ç£æ£€æµ‹æ¨¡å‹å…·æœ‰è¯Šæ–­ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GANæ¶æ„è¢«ç”¨äºç”Ÿæˆåˆæˆè„‘FDG PETå›¾åƒï¼Œä»T1åŠ æƒMRIæ•°æ®ä¸­ç”Ÿæˆã€‚</li>
<li>ä¸ä»…è¯„ä¼°äº†å›¾åƒç”Ÿæˆçš„è´¨é‡ï¼Œè¿˜ç ”ç©¶äº†è¿™äº›åˆæˆæ•°æ®åœ¨è®­ç»ƒæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚</li>
<li>æœ€ä½³GANæ¨¡å‹ç”Ÿæˆçš„åˆæˆPETå›¾åƒå…·æœ‰é«˜åº¦çš„ç°å®æ€§ï¼Œä¸çœŸå®æ§åˆ¶æ•°æ®é›†åˆ†å¸ƒä¸€è‡´ã€‚</li>
<li>ä½¿ç”¨åˆæˆè§„èŒƒæ€§PETæ•°æ®è®­ç»ƒçš„æœ€ä½³UADæ¨¡å‹è¾¾åˆ°74%çš„æ•æ„Ÿæ€§ã€‚</li>
<li>GANæ¨¡å‹æœ€é€‚åˆäºMR T1åˆ°FDG PETçš„ç¿»è¯‘ã€‚</li>
<li>åˆæˆæ•°æ®å¯¹è®­ç»ƒæ— ç›‘ç£æ£€æµ‹æ¨¡å‹å…·æœ‰è¯Šæ–­ä»·å€¼ã€‚</li>
<li>ç ”ç©¶ä»£ç å’Œè§„èŒƒå›¾åƒæ•°æ®é›†å¯ä¾›ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07364">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75d8aa298bc71f1dd84f1fb41bb64059.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ad81deaf0ad0909d007a8a007018411.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edee906e9a345f0ac2a02462149c7882.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e31676b64bce418610fd0cae36d7bb02.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Semantic-Guided-Diffusion-Model-for-Single-Step-Image-Super-Resolution"><a href="#Semantic-Guided-Diffusion-Model-for-Single-Step-Image-Super-Resolution" class="headerlink" title="Semantic-Guided Diffusion Model for Single-Step Image Super-Resolution"></a>Semantic-Guided Diffusion Model for Single-Step Image Super-Resolution</h2><p><strong>Authors:Zihang Liu, Zhenyu Zhang, Hao Tang</strong></p>
<p>Diffusion-based image super-resolution (SR) methods have demonstrated remarkable performance. Recent advancements have introduced deterministic sampling processes that reduce inference from 15 iterative steps to a single step, thereby significantly improving the inference speed of existing diffusion models. However, their efficiency remains limited when handling complex semantic regions due to the single-step inference. To address this limitation, we propose SAMSR, a semantic-guided diffusion framework that incorporates semantic segmentation masks into the sampling process. Specifically, we introduce the SAM-Noise Module, which refines Gaussian noise using segmentation masks to preserve spatial and semantic features. Furthermore, we develop a pixel-wise sampling strategy that dynamically adjusts the residual transfer rate and noise strength based on pixel-level semantic weights, prioritizing semantically rich regions during the diffusion process. To enhance model training, we also propose a semantic consistency loss, which aligns pixel-wise semantic weights between predictions and ground truth. Extensive experiments on both real-world and synthetic datasets demonstrate that SAMSR significantly improves perceptual quality and detail recovery, particularly in semantically complex images. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/Liu-Zihang/SAMSR">https://github.com/Liu-Zihang/SAMSR</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ–¹æ³•å·²æ˜¾ç¤ºå‡ºå“è¶Šçš„æ€§èƒ½ã€‚æœ€è¿‘çš„è¿›å±•å¼•å…¥äº†ç¡®å®šæ€§é‡‡æ ·è¿‡ç¨‹ï¼Œå°†æ¨ç†ä»15ä¸ªè¿­ä»£æ­¥éª¤å‡å°‘åˆ°å•ä¸ªæ­¥éª¤ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ç°æœ‰æ‰©æ•£æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚ç„¶è€Œï¼Œç”±äºå•æ­¥æ¨ç†çš„é™åˆ¶ï¼Œå®ƒä»¬åœ¨å¤„ç†å¤æ‚è¯­ä¹‰åŒºåŸŸæ—¶çš„æ•ˆç‡ä»ç„¶æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SAMSRï¼Œä¸€ä¸ªè¯­ä¹‰å¼•å¯¼çš„æ‰©æ•£æ¡†æ¶ï¼Œå°†è¯­ä¹‰åˆ†å‰²æ©è†œèå…¥åˆ°é‡‡æ ·è¿‡ç¨‹ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†SAM-Noiseæ¨¡å—ï¼Œè¯¥æ¨¡å—ä½¿ç”¨åˆ†å‰²æ©è†œå¯¹é«˜æ–¯å™ªå£°è¿›è¡Œç²¾ç‚¼ï¼Œä»¥ä¿ç•™ç©ºé—´å’Œè¯­ä¹‰ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åƒç´ çº§é‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®åƒç´ çº§è¯­ä¹‰æƒé‡åŠ¨æ€è°ƒæ•´æ®‹å·®ä¼ è¾“ç‡å’Œå™ªå£°å¼ºåº¦ï¼Œåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ä¼˜å…ˆå¤„ç†è¯­ä¹‰ä¸°å¯Œçš„åŒºåŸŸã€‚ä¸ºäº†æé«˜æ¨¡å‹è®­ç»ƒæ•ˆæœï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è¯­ä¹‰ä¸€è‡´æ€§æŸå¤±ï¼Œè¯¥æŸå¤±å¯ä»¥å¯¹é½é¢„æµ‹å’ŒçœŸå®å€¼ä¹‹é—´çš„åƒç´ çº§è¯­ä¹‰æƒé‡ã€‚åœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSAMSRæ˜¾è‘—æé«˜äº†æ„ŸçŸ¥è´¨é‡å’Œç»†èŠ‚æ¢å¤ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­ä¹‰å¤æ‚çš„å›¾åƒä¸­ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Liu-Zihang/SAMSR%E3%80%82">https://github.com/Liu-Zihang/SAMSRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07071v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£çš„å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ–¹æ³•çš„æ–°è¿›å±•ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ–°çš„è¯­ä¹‰å¼•å¯¼æ‰©æ•£æ¡†æ¶SAMSRï¼Œç»“åˆè¯­ä¹‰åˆ†å‰²æ©è†œè¿›è¡Œé‡‡æ ·è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶é€šè¿‡SAM-Noiseæ¨¡å—ä½¿ç”¨åˆ†å‰²æ©è†œä¼˜åŒ–é«˜æ–¯å™ªå£°ï¼Œä»¥ä¿ç•™ç©ºé—´è¯­ä¹‰ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†åƒç´ çº§é‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®åƒç´ çº§è¯­ä¹‰æƒé‡åŠ¨æ€è°ƒæ•´æ®‹ç•™ä¼ è¾“ç‡å’Œå™ªå£°å¼ºåº¦ã€‚ä¸ºæ”¹è¿›æ¨¡å‹è®­ç»ƒï¼Œå¼•å…¥è¯­ä¹‰ä¸€è‡´æ€§æŸå¤±ï¼Œå¯¹é½é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾çš„åƒç´ çº§è¯­ä¹‰æƒé‡ã€‚å®éªŒè¡¨æ˜ï¼ŒSAMSRæ˜¾è‘—æé«˜æ„ŸçŸ¥è´¨é‡å’Œç»†èŠ‚æ¢å¤èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­ä¹‰å¤æ‚çš„å›¾åƒä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡é¢†åŸŸå±•ç°å‡ºæ˜¾è‘—æ€§èƒ½ã€‚</li>
<li>è¿‘æœŸå‘å±•å¼•å…¥äº†ç¡®å®šæ€§é‡‡æ ·è¿‡ç¨‹ï¼Œå°†æ¨ç†æ­¥éª¤ä»15æ¬¡è¿­ä»£å‡å°‘åˆ°ä¸€æ¬¡ï¼Œæé«˜äº†ç°æœ‰æ‰©æ•£æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>æå‡ºçš„SAMSRæ¡†æ¶ç»“åˆè¯­ä¹‰åˆ†å‰²æ©è†œè¿›è¡Œé‡‡æ ·ï¼Œæ—¨åœ¨è§£å†³å¤„ç†å¤æ‚è¯­ä¹‰åŒºåŸŸæ—¶çš„æ•ˆç‡é™åˆ¶ã€‚</li>
<li>SAMSRé€šè¿‡SAM-Noiseæ¨¡å—åˆ©ç”¨åˆ†å‰²æ©è†œä¼˜åŒ–é«˜æ–¯å™ªå£°ï¼Œä¿ç•™ç©ºé—´å’Œè¯­ä¹‰ç‰¹å¾ã€‚</li>
<li>åƒç´ çº§é‡‡æ ·ç­–ç•¥æ ¹æ®åƒç´ çº§è¯­ä¹‰æƒé‡è°ƒæ•´æ®‹ç•™ä¼ è¾“ç‡å’Œå™ªå£°å¼ºåº¦ï¼Œä¼˜å…ˆå¤„ç†è¯­ä¹‰ä¸°å¯Œçš„åŒºåŸŸã€‚</li>
<li>å¼•å…¥è¯­ä¹‰ä¸€è‡´æ€§æŸå¤±ä»¥æ”¹è¿›æ¨¡å‹è®­ç»ƒï¼Œå¯¹é½é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾çš„åƒç´ çº§è¯­ä¹‰æƒé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07071">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4d60992a1418e4035087d04749a9693.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b07899f54cb6f394124d22c9c6498112.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45ebb0cbd9860183b58125abfdf2f298.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-68273c3b1f4c7c0059497f15fad77430.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70e4099c701ca4f53781118acf39601c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa39b9f173f02133a376164d65e4ad7e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Replay-Based-Continual-Learning-with-Dual-Layered-Distillation-and-a-Streamlined-U-Net-for-Efficient-Text-to-Image-Generation"><a href="#Replay-Based-Continual-Learning-with-Dual-Layered-Distillation-and-a-Streamlined-U-Net-for-Efficient-Text-to-Image-Generation" class="headerlink" title="Replay-Based Continual Learning with Dual-Layered Distillation and a   Streamlined U-Net for Efficient Text-to-Image Generation"></a>Replay-Based Continual Learning with Dual-Layered Distillation and a   Streamlined U-Net for Efficient Text-to-Image Generation</h2><p><strong>Authors:Md. Naimur Asif Borno, Md Sakib Hossain Shovon, Asmaa Soliman Al-Moisheer, Mohammad Ali Moni</strong></p>
<p>Recent advancements in text-to-image diffusion models are hindered by high computational demands, limiting accessibility and scalability. This paper introduces KDC-Diff, a novel stable diffusion framework that enhances efficiency while maintaining image quality. KDC-Diff features a streamlined U-Net architecture with nearly half the parameters of the original U-Net (482M), significantly reducing model complexity. We propose a dual-layered distillation strategy to ensure high-fidelity generation, transferring semantic and structural insights from a teacher to a compact student model while minimizing quality degradation. Additionally, replay-based continual learning is integrated to mitigate catastrophic forgetting, allowing the model to retain prior knowledge while adapting to new data. Despite operating under extremely low computational resources, KDC-Diff achieves state-of-the-art performance on the Oxford Flowers and Butterflies &amp; Moths 100 Species datasets, demonstrating competitive metrics such as FID, CLIP, and LPIPS. Moreover, it significantly reduces inference time compared to existing models. These results establish KDC-Diff as a highly efficient and adaptable solution for text-to-image generation, particularly in computationally constrained environments. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¿›å±•å—åˆ°é«˜è®¡ç®—éœ€æ±‚çš„é˜»ç¢ï¼Œè¿™é™åˆ¶äº†å…¶å¯è®¿é—®æ€§å’Œå¯æ‰©å±•æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†KDC-Diffï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç¨³å®šçš„æ‰©æ•£æ¡†æ¶ï¼Œæé«˜äº†æ•ˆç‡åŒæ—¶ä¿æŒäº†å›¾åƒè´¨é‡ã€‚KDC-Diffé‡‡ç”¨ç®€åŒ–çš„U-Netæ¶æ„ï¼Œå‚æ•°æ•°é‡å‡ ä¹ä¸ºåŸå§‹U-Netçš„ä¸€åŠï¼ˆ482Mï¼‰ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹å¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒå±‚è’¸é¦ç­–ç•¥ï¼Œä»¥ç¡®ä¿é«˜ä¿çœŸç”Ÿæˆï¼Œå°†æ•™å¸ˆå’Œå­¦ç”Ÿçš„ç´§å‡‘æ¨¡å‹ä¹‹é—´çš„è¯­ä¹‰å’Œç»“æ„è§è§£è¿›è¡Œè½¬ç§»ï¼ŒåŒæ—¶æœ€å°åŒ–è´¨é‡ä¸‹é™ã€‚æ­¤å¤–ï¼Œè¿˜é›†æˆäº†åŸºäºé‡æ”¾çš„æŒç»­å­¦ä¹ ï¼Œä»¥å‡è½»ç¾éš¾æ€§é—å¿˜ï¼Œä½¿æ¨¡å‹åœ¨é€‚åº”æ–°æ•°æ®çš„åŒæ—¶ä¿ç•™å…ˆå‰çŸ¥è¯†ã€‚å°½ç®¡åœ¨æä½çš„è®¡ç®—èµ„æºä¸‹è¿è¡Œï¼ŒKDC-Diffåœ¨Oxford Flowerså’ŒButterflies &amp; Moths 100 Speciesæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå…·æœ‰ç«äº‰åŠ›çš„æŒ‡æ ‡ï¼Œå¦‚FIDã€CLIPå’ŒLPIPSã€‚è€Œä¸”ï¼Œä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒå¤§å¤§å‡å°‘äº†æ¨ç†æ—¶é—´ã€‚è¿™äº›ç»“æœä½¿KDC-Diffæˆä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„é«˜æ•ˆä¸”å¯é€‚åº”çš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—å—é™çš„ç¯å¢ƒä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06995v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆç¨³å®šçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹KDC-Diffï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¿æŒå›¾åƒè´¨é‡çš„åŒæ—¶æé«˜è®¡ç®—æ•ˆç‡ã€‚è¯¥æ¨¡å‹é€šè¿‡ç®€åŒ–U-Netæ¶æ„å’Œå¼•å…¥åŒå±‚æ¬¡è’¸é¦ç­–ç•¥ä»¥åŠå›æ”¾å¼æŒç»­å­¦ä¹ ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹å¤æ‚åº¦å’Œè®¡ç®—éœ€æ±‚ã€‚åœ¨ç‰›æ´¥èŠ±å‰å’Œè´è¶ä¸é£è›¾ç­‰æ•°æ®é›†ä¸Šï¼ŒKDC-Diffå–å¾—äº†å…ˆè¿›çš„è¡¨ç°ï¼Œå±•ç¤ºäº†å…¶é«˜æ•ˆçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KDC-Diffæ˜¯ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹çš„é«˜è®¡ç®—éœ€æ±‚é—®é¢˜ã€‚</li>
<li>KDC-Diffé‡‡ç”¨ç®€åŒ–çš„U-Netæ¶æ„ï¼Œå‚æ•°æ•°é‡æ¥è¿‘åŸå§‹U-Netçš„ä¸€åŠï¼Œé™ä½äº†æ¨¡å‹å¤æ‚åº¦ã€‚</li>
<li>æå‡ºåŒå±‚æ¬¡è’¸é¦ç­–ç•¥ï¼Œç¡®ä¿ä»æ•™å¸ˆæ¨¡å‹åˆ°å­¦ç”Ÿæ¨¡å‹çš„è¯­ä¹‰å’Œç»“æ„çŸ¥è¯†è½¬ç§»ï¼ŒåŒæ—¶æœ€å°åŒ–è´¨é‡æŸå¤±ã€‚</li>
<li>é›†æˆå›æ”¾å¼æŒç»­å­¦ä¹ ï¼Œç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”æ–°æ•°æ®çš„åŒæ—¶ä¿ç•™å…ˆå‰çŸ¥è¯†ã€‚</li>
<li>KDC-Diffåœ¨ç‰›æ´¥èŠ±å‰å’Œè´è¶ä¸é£è›¾ç­‰æ•°æ®é›†ä¸Šå–å¾—äº†å“è¶Šè¡¨ç°ï¼Œæ˜¾ç¤ºå…¶åœ¨FIDã€CLIPå’ŒLPIPSç­‰è¯„ä¼°æŒ‡æ ‡ä¸Šçš„ç«äº‰åŠ›ã€‚</li>
<li>ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒKDC-Diffæ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3738e2eff05188e069cfa6b26f79931.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e2576b840940d8d08f46d4f606de80e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d60aecdf95aa342a5af2c0b253e9f5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc84189fe95e4114be4cf0782cd9cb65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39fce9265555ea53c13e5953953c5c48.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Learning-for-Class-Distribution-Mismatch"><a href="#Unsupervised-Learning-for-Class-Distribution-Mismatch" class="headerlink" title="Unsupervised Learning for Class Distribution Mismatch"></a>Unsupervised Learning for Class Distribution Mismatch</h2><p><strong>Authors:Pan Du, Wangbo Zhao, Xinai Lu, Nian Liu, Zhikai Li, Chaoyu Gong, Suyun Zhao, Hong Chen, Cuiping Li, Kai Wang, Yang You</strong></p>
<p>Class distribution mismatch (CDM) refers to the discrepancy between class distributions in training data and target tasks. Previous methods address this by designing classifiers to categorize classes known during training, while grouping unknown or new classes into an â€œotherâ€ category. However, they focus on semi-supervised scenarios and heavily rely on labeled data, limiting their applicability and performance. To address this, we propose Unsupervised Learning for Class Distribution Mismatch (UCDM), which constructs positive-negative pairs from unlabeled data for classifier training. Our approach randomly samples images and uses a diffusion model to add or erase semantic classes, synthesizing diverse training pairs. Additionally, we introduce a confidence-based labeling mechanism that iteratively assigns pseudo-labels to valuable real-world data and incorporates them into the training process. Extensive experiments on three datasets demonstrate UCDMâ€™s superiority over previous semi-supervised methods. Specifically, with a 60% mismatch proportion on Tiny-ImageNet dataset, our approach, without relying on labeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%, and 72.5% in classifying known, unknown, and new classes. </p>
<blockquote>
<p>ç±»åˆ«åˆ†å¸ƒä¸åŒ¹é…ï¼ˆCDMï¼‰æŒ‡çš„æ˜¯è®­ç»ƒæ•°æ®ä¸ç›®æ ‡ä»»åŠ¡ä¸­ç±»åˆ«åˆ†å¸ƒçš„å·®å¼‚ã€‚ä¹‹å‰çš„æ–¹æ³•æ˜¯é€šè¿‡è®¾è®¡åˆ†ç±»å™¨æ¥åº”å¯¹è¿™ä¸€å·®å¼‚ï¼Œå¯¹è®­ç»ƒæœŸé—´å·²çŸ¥çš„ç±»åˆ«è¿›è¡Œåˆ†ç±»ï¼ŒåŒæ—¶å°†æœªçŸ¥æˆ–æ–°ç±»åˆ«å½’ä¸ºâ€œå…¶ä»–â€ç±»åˆ«ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¸»è¦å…³æ³¨åŠç›‘ç£åœºæ™¯ï¼Œå¹¶ä¸¥é‡ä¾èµ–æœ‰æ ‡ç­¾æ•°æ®ï¼Œä»è€Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§å’Œæ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºç±»åˆ«åˆ†å¸ƒä¸åŒ¹é…çš„æ— ç›‘ç£å­¦ä¹ ï¼ˆUCDMï¼‰ï¼Œå®ƒé€šè¿‡æ— æ ‡ç­¾æ•°æ®æ„å»ºæ­£è´Ÿé¢å¯¹æ¥è¿›è¡Œåˆ†ç±»å™¨è®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•éšæœºé‡‡æ ·å›¾åƒï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹æ·»åŠ æˆ–åˆ é™¤è¯­ä¹‰ç±»åˆ«ï¼Œåˆæˆå¤šæ ·åŒ–çš„è®­ç»ƒå¯¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºç½®ä¿¡åº¦çš„æ ‡è®°æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥è¿­ä»£åœ°å¯¹æœ‰ä»·å€¼çš„çœŸå®æ•°æ®è¿›è¡Œä¼ªæ ‡ç­¾åˆ†é…å¹¶å°†å…¶çº³å…¥è®­ç»ƒè¿‡ç¨‹ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUCDMåœ¨ä¹‹å‰åŠç›‘ç£æ–¹æ³•ä¸Šçš„ä¼˜è¶Šæ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨Tiny-ImageNetæ•°æ®é›†ä¸Šï¼Œåœ¨é«˜è¾¾60%çš„ç±»åˆ«åˆ†å¸ƒä¸åŒ¹é…æ¯”ä¾‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ä¾èµ–æœ‰æ ‡ç­¾æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¯¹å·²çŸ¥ç±»åˆ«ã€æœªçŸ¥ç±»åˆ«å’Œæ–°ç±»åˆ«çš„åˆ†ç±»è¶…è¿‡äº†OpenMatchï¼ˆæ¯ç±»æœ‰40ä¸ªæ ‡ç­¾ï¼‰çš„å‡†ç¡®ç‡åˆ†åˆ«ä¸ºï¼šæå‡äº†35.1%ã€63.7%å’Œ72.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06948v1">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹ç±»åˆ«åˆ†å¸ƒä¸åŒ¹é…ï¼ˆCDMï¼‰é—®é¢˜çš„æ— ç›‘ç£å­¦ä¹ æ–¹æ³•UCDMã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®æ„å»ºæ­£è´Ÿæ ·æœ¬å¯¹è¿›è¡Œåˆ†ç±»å™¨è®­ç»ƒï¼Œé€šè¿‡éšæœºé‡‡æ ·å›¾åƒå¹¶è¿ç”¨æ‰©æ•£æ¨¡å‹å¢åŠ æˆ–åˆ é™¤è¯­ä¹‰ç±»åˆ«ï¼Œåˆæˆå¤šæ ·çš„è®­ç»ƒæ ·æœ¬å¯¹ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§åŸºäºç½®ä¿¡åº¦çš„æ ‡ç­¾æœºåˆ¶ï¼Œè¿­ä»£åœ°ä¸ºæœ‰ä»·å€¼çš„çœŸå®æ•°æ®åˆ†é…ä¼ªæ ‡ç­¾å¹¶å°†å…¶çº³å…¥è®­ç»ƒè¿‡ç¨‹ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUCDMåœ¨ç±»åˆ«åˆ†å¸ƒé«˜åº¦ä¸åŒ¹é…çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„åŠç›‘ç£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç±»åˆ†å¸ƒä¸åŒ¹é…ï¼ˆCDMï¼‰æŒ‡çš„æ˜¯è®­ç»ƒæ•°æ®ä¸ç›®æ ‡ä»»åŠ¡ä¸­ç±»åˆ«åˆ†å¸ƒçš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡è®¾è®¡åˆ†ç±»å™¨æ¥è¯†åˆ«è®­ç»ƒæœŸé—´å·²çŸ¥çš„ç±»åˆ«ï¼Œå°†æœªçŸ¥æˆ–æ–°ç±»åˆ«å½’ä¸ºâ€œå…¶ä»–â€ç±»åˆ«ï¼Œä½†è¿™ç§æ–¹æ³•ä¸»è¦åº”ç”¨äºåŠç›‘ç£åœºæ™¯ï¼Œä¸¥é‡ä¾èµ–æ ‡è®°æ•°æ®ï¼Œé™åˆ¶äº†å…¶åº”ç”¨æ€§å’Œæ€§èƒ½ã€‚</li>
<li>UCDMæ–¹æ³•åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®æ„å»ºæ­£è´Ÿæ ·æœ¬å¯¹è¿›è¡Œè®­ç»ƒï¼Œè§£å†³äº†å¯¹æ ‡è®°æ•°æ®çš„ä¾èµ–é—®é¢˜ã€‚</li>
<li>UCDMé€šè¿‡éšæœºé‡‡æ ·å›¾åƒå¹¶è¿ç”¨æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†è¯­ä¹‰ç±»åˆ«çš„å¢åŠ æˆ–åˆ é™¤ï¼Œåˆæˆäº†å¤šæ ·çš„è®­ç»ƒæ ·æœ¬ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºç½®ä¿¡åº¦çš„æ ‡ç­¾æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿè¿­ä»£åœ°ä¸ºçœŸå®æ•°æ®åˆ†é…ä¼ªæ ‡ç­¾ï¼Œå¹¶å°†å…¶çº³å…¥è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>åœ¨Tiny-ImageNetæ•°æ®é›†ä¸Šï¼Œå½“ç±»åˆ«ä¸åŒ¹é…æ¯”ä¾‹ä¸º60%æ—¶ï¼ŒUCDMæ–¹æ³•åœ¨è¯†åˆ«å·²çŸ¥ã€æœªçŸ¥å’Œæ–°ç±»åˆ«ä¸Šçš„æ€§èƒ½å‡è¶…è¿‡äº†ä¼ ç»Ÿçš„åŠç›‘ç£æ–¹æ³•OpenMatchã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06948">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4f1c053187856336c10d8de023c4cc98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f591785b857ca8b62d42400384c24fe0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1e87f9a381d48b2acb376c60b30a69f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-957364f14565f0eae78c493d5ba97082.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Learning-Graph-Representation-of-Agent-Diffuser"><a href="#Learning-Graph-Representation-of-Agent-Diffuser" class="headerlink" title="Learning Graph Representation of Agent Diffuser"></a>Learning Graph Representation of Agent Diffuser</h2><p><strong>Authors:Youcef Djenouri, Nassim Belmecheri, Tomasz Michalak, Jan DubiÅ„ski, Ahmed Nabil Belbachir, Anis Yazidi</strong></p>
<p>Diffusion-based generative models have significantly advanced text-to-image synthesis, demonstrating impressive text comprehension and zero-shot generalization. These models refine images from random noise based on textual prompts, with initial reliance on text input shifting towards enhanced visual fidelity over time. This transition suggests that static model parameters might not optimally address the distinct phases of generation. We introduce LGR-AD (Learning Graph Representation of Agent Diffusers), a novel multi-agent system designed to improve adaptability in dynamic computer vision tasks. LGR-AD models the generation process as a distributed system of interacting agents, each representing an expert sub-model. These agents dynamically adapt to varying conditions and collaborate through a graph neural network that encodes their relationships and performance metrics. Our approach employs a coordination mechanism based on top-$k$ maximum spanning trees, optimizing the generation process. Each agentâ€™s decision-making is guided by a meta-model that minimizes a novel loss function, balancing accuracy and diversity. Theoretical analysis and extensive empirical evaluations show that LGR-AD outperforms traditional diffusion models across various benchmarks, highlighting its potential for scalable and flexible solutions in complex image generation tasks. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/YousIA/LGR_AD">https://github.com/YousIA/LGR_AD</a> </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œå±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ–‡æœ¬ç†è§£å’Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹æ ¹æ®æ–‡æœ¬æç¤ºä»éšæœºå™ªå£°ä¸­ç»†åŒ–å›¾åƒï¼Œæœ€åˆä¾èµ–æ–‡æœ¬è¾“å…¥ï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œå¯¹è§†è§‰é€¼çœŸåº¦çš„å…³æ³¨é€æ¸å¢å¼ºã€‚è¿™ç§è½¬å˜è¡¨æ˜ï¼Œé™æ€æ¨¡å‹å‚æ•°å¯èƒ½æ— æ³•æœ€ä½³åœ°å¤„ç†ç”Ÿæˆçš„ä¸åŒé˜¶æ®µã€‚æˆ‘ä»¬å¼•å…¥äº†LGR-ADï¼ˆå­¦ä¹ ä»£ç†æ‰©æ•£å›¾è¡¨ç¤ºï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¤šä»£ç†ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜åŠ¨æ€è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„é€‚åº”æ€§ã€‚LGR-ADå°†ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºç›¸äº’ä½œç”¨çš„ä»£ç†çš„åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œæ¯ä¸ªä»£ç†ä»£è¡¨ä¸€ä¸ªä¸“å®¶å­æ¨¡å‹ã€‚è¿™äº›ä»£ç†èƒ½å¤ŸåŠ¨æ€é€‚åº”å„ç§æ¡ä»¶ï¼Œå¹¶é€šè¿‡ç¼–ç å…¶å…³ç³»å’Œæ€§èƒ½æŒ‡æ ‡çš„å›¾ç¥ç»ç½‘ç»œè¿›è¡Œåä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åŸºäºtop-kæœ€å¤§ç”Ÿæˆæ ‘çš„åè°ƒæœºåˆ¶ï¼Œä»¥ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚æ¯ä¸ªä»£ç†çš„å†³ç­–ç”±å…ƒæ¨¡å‹å¼•å¯¼ï¼Œè¯¥å…ƒæ¨¡å‹æœ€å°åŒ–æ–°å‹æŸå¤±å‡½æ•°ï¼Œå¹³è¡¡å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ã€‚ç†è®ºåˆ†æå’Œå¹¿æ³›çš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLGR-ADçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼Œçªæ˜¾å…¶åœ¨å¤æ‚å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å¯æ‰©å±•å’Œçµæ´»è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚ç›¸å…³ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/YousIA/LGR_AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YousIA/LGR_ADæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06761v1">PDF</a> Accepted at AAMAS2025 International Conference on Autonomous Agents   and Multiagent Systems</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ–¹é¢çš„æ˜¾è‘—è¿›å±•ï¼Œä»¥åŠè¿™äº›æ¨¡å‹å¦‚ä½•å®ç°å¯¹æ–‡æœ¬ç†è§£çš„æ˜¾è‘—è¿›æ­¥å’Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›çš„æå‡ã€‚ä¸ºæ”¹å–„åŠ¨æ€è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šä»£ç†ç³»ç»ŸLGR-ADã€‚è¯¥ç³»ç»Ÿå°†ç”Ÿæˆè¿‡ç¨‹æ¨¡æ‹Ÿä¸ºäº¤äº’ä»£ç†çš„åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œæ¯ä¸ªä»£ç†ä»£è¡¨ä¸€ä¸ªä¸“å®¶å­æ¨¡å‹ï¼Œé€šè¿‡å›¾ç¥ç»ç½‘ç»œè¿›è¡Œåä½œä¸é€‚åº”ã€‚LGR-ADé€šè¿‡top-kæœ€å¤§ç”Ÿæˆæ ‘ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ï¼ŒåŒæ—¶å¼•å…¥æ–°å‹æŸå¤±å‡½æ•°å¹³è¡¡ç²¾åº¦ä¸å¤šæ ·æ€§ã€‚ç†è®ºä¸å¤§é‡å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒLGR-ADåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼Œä¸ºå¤æ‚å›¾åƒç”Ÿæˆä»»åŠ¡æä¾›äº†å¯æ‰©å±•ä¸”çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£ç”Ÿæˆæ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„æ–‡æœ¬ç†è§£å’Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>LGR-ADæ˜¯ä¸€ç§æ–°å‹å¤šä»£ç†ç³»ç»Ÿï¼Œæ—¨åœ¨æ”¹å–„åŠ¨æ€è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„é€‚åº”æ€§ã€‚</li>
<li>LGR-ADå°†ç”Ÿæˆè¿‡ç¨‹æ¨¡æ‹Ÿä¸ºäº¤äº’ä»£ç†çš„åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œæ¯ä¸ªä»£ç†ä»£è¡¨ä¸€ä¸ªä¸“å®¶å­æ¨¡å‹ã€‚</li>
<li>é€šè¿‡å›¾ç¥ç»ç½‘ç»œè¿›è¡Œåä½œä¸é€‚åº”ï¼Œå®ç°äº†ä»£ç†é—´çš„åŠ¨æ€è°ƒæ•´å’Œäº’åŠ¨ã€‚</li>
<li>LGR-ADé‡‡ç”¨top-kæœ€å¤§ç”Ÿæˆæ ‘ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥æ–°å‹æŸå¤±å‡½æ•°ä»¥å¹³è¡¡ç”Ÿæˆçš„ç²¾åº¦å’Œå¤šæ ·æ€§ã€‚</li>
<li>ç†è®ºä¸å¤§é‡å®è¯è¯„ä¼°è¯æ˜LGR-ADåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7dbc6a231835cc47324f068ef684738.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-428f003d5fa29766e837326999873613.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6fb096f40e4bb62548185964273e4ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e04501f70bda661fdd33dba4c732937.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea680203ef9b9c0c6c245cd82b80e2ee.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="StableMotion-Repurposing-Diffusion-Based-Image-Priors-for-Motion-Estimation"><a href="#StableMotion-Repurposing-Diffusion-Based-Image-Priors-for-Motion-Estimation" class="headerlink" title="StableMotion: Repurposing Diffusion-Based Image Priors for Motion   Estimation"></a>StableMotion: Repurposing Diffusion-Based Image Priors for Motion   Estimation</h2><p><strong>Authors:Ziyi Wang, Haipeng Li, Lin Sui, Tianhao Zhou, Hai Jiang, Lang Nie, Shuaicheng Liu</strong></p>
<p>We present StableMotion, a novel framework leverages knowledge (geometry and content priors) from pretrained large-scale image diffusion models to perform motion estimation, solving single-image-based image rectification tasks such as Stitched Image Rectangling (SIR) and Rolling Shutter Correction (RSC). Specifically, StableMotion framework takes text-to-image Stable Diffusion (SD) models as backbone and repurposes it into an image-to-motion estimator. To mitigate inconsistent output produced by diffusion models, we propose Adaptive Ensemble Strategy (AES) that consolidates multiple outputs into a cohesive, high-fidelity result. Additionally, we present the concept of Sampling Steps Disaster (SSD), the counterintuitive scenario where increasing the number of sampling steps can lead to poorer outcomes, which enables our framework to achieve one-step inference. StableMotion is verified on two image rectification tasks and delivers state-of-the-art performance in both, as well as showing strong generalizability. Supported by SSD, StableMotion offers a speedup of 200 times compared to previous diffusion model-based methods. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†StableMotionæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§è§„æ¨¡å›¾åƒæ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†ï¼ˆå‡ ä½•å’Œå…ˆéªŒå†…å®¹ï¼‰æ¥è¿›è¡Œè¿åŠ¨ä¼°è®¡ï¼Œè§£å†³åŸºäºå•å›¾åƒçš„å›¾åƒæ ¡æ­£ä»»åŠ¡ï¼Œå¦‚æ‹¼æ¥å›¾åƒçŸ©å½¢åŒ–ï¼ˆSIRï¼‰å’Œæ»šåŠ¨å¿«é—¨æ ¡æ­£ï¼ˆRSCï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒStableMotionæ¡†æ¶ä»¥æ–‡æœ¬åˆ°å›¾åƒçš„Stable Diffusionï¼ˆSDï¼‰æ¨¡å‹ä½œä¸ºéª¨å¹²ï¼Œå¹¶å°†å…¶é‡æ–°å®šä½ä¸ºå›¾åƒåˆ°è¿åŠ¨ä¼°è®¡å™¨ã€‚ä¸ºäº†å‡è½»æ‰©æ•£æ¨¡å‹äº§ç”Ÿçš„è¾“å‡ºä¸ä¸€è‡´é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”é›†æˆç­–ç•¥ï¼ˆAESï¼‰ï¼Œå®ƒå°†å¤šä¸ªè¾“å‡ºåˆå¹¶ä¸ºä¸€ä¸ªè¿è´¯ã€é«˜ä¿çœŸåº¦çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†é‡‡æ ·æ­¥éª¤ç¾éš¾ï¼ˆSSDï¼‰çš„æ¦‚å¿µï¼Œå³å¢åŠ é‡‡æ ·æ­¥éª¤çš„æ•°é‡å¯èƒ½å¯¼è‡´ç»“æœæ›´å·®çš„åç›´è§‰åœºæ™¯ï¼Œè¿™ä½¿æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿå®ç°ä¸€æ­¥æ¨æ–­ã€‚StableMotionåœ¨ä¸¤é¡¹å›¾åƒæ ¡æ­£ä»»åŠ¡ä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œå¹¶åœ¨ä¸¤é¡¹ä»»åŠ¡ä¸Šéƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚å¾—ç›ŠäºSSDï¼ŒStableMotionä¸ä¹‹å‰çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•ç›¸æ¯”å®ç°äº†200å€çš„é€Ÿåº¦æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06668v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¨³å®šè¿åŠ¨æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§è§„æ¨¡å›¾åƒæ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†ï¼Œè¿›è¡Œè¿åŠ¨ä¼°è®¡ï¼Œè§£å†³åŸºäºå•å›¾åƒçš„ä»»åŠ¡ï¼Œå¦‚å›¾åƒæ ¡æ­£å’Œæ»šåŠ¨å¿«é—¨æ ¡æ­£ã€‚å®ƒé‡‡ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„Stable Diffusionæ¨¡å‹ä½œä¸ºéª¨å¹²ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå›¾åƒåˆ°è¿åŠ¨ä¼°è®¡å™¨ã€‚é€šè¿‡è‡ªé€‚åº”é›†æˆç­–ç•¥ï¼Œå‡è½»æ‰©æ•£æ¨¡å‹äº§ç”Ÿçš„ä¸ä¸€è‡´è¾“å‡ºé—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†é‡‡æ ·æ­¥éª¤ç¾éš¾çš„æ¦‚å¿µï¼Œä½¿æ¡†æ¶å®ç°ä¸€æ­¥æ¨æ–­ã€‚StableMotionåœ¨ä¸¤ç§å›¾åƒæ ¡æ­£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ä¹‹å‰çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•ç›¸æ¯”ï¼Œé€Ÿåº¦æé«˜äº†200å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StableMotionæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œè¿åŠ¨ä¼°è®¡ã€‚</li>
<li>å®ƒè§£å†³äº†åŸºäºå•å›¾åƒçš„ä»»åŠ¡ï¼Œå¦‚å›¾åƒæ ¡æ­£å’Œæ»šåŠ¨å¿«é—¨æ ¡æ­£ã€‚</li>
<li>é‡‡ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„Stable Diffusionæ¨¡å‹ä½œä¸ºéª¨å¹²ï¼Œè½¬åŒ–ç”¨äºå›¾åƒåˆ°è¿åŠ¨ä¼°è®¡ã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”é›†æˆç­–ç•¥ï¼ˆAESï¼‰å‡è½»æ‰©æ•£æ¨¡å‹çš„ä¸ä¸€è‡´è¾“å‡ºé—®é¢˜ã€‚</li>
<li>å¼•å…¥é‡‡æ ·æ­¥éª¤ç¾éš¾ï¼ˆSSDï¼‰æ¦‚å¿µï¼Œè¯´æ˜å¢åŠ é‡‡æ ·æ­¥éª¤æ•°é‡å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>StableMotionåœ¨å›¾åƒæ ¡æ­£ä»»åŠ¡ä¸Šå®ç°äº†ä¸€æµæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f0359ead616b7821aac19c655f450f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a06d2850445a2193f9b19c383589c5e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a130ae0305c7b161f96d171439c9036e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42d723a8852e3d5aa1a931206c8d93ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0c8262d063ac30b804e3cdedd5fdb53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89584eb13a3bcb8a343df19c7f7d9a39.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ReplayCAD-Generative-Diffusion-Replay-for-Continual-Anomaly-Detection"><a href="#ReplayCAD-Generative-Diffusion-Replay-for-Continual-Anomaly-Detection" class="headerlink" title="ReplayCAD: Generative Diffusion Replay for Continual Anomaly Detection"></a>ReplayCAD: Generative Diffusion Replay for Continual Anomaly Detection</h2><p><strong>Authors:Lei Hu, Zhiyong Gan, Ling Deng, Jinglin Liang, Lingyu Liang, Shuangping Huang, Tianshui Chen</strong></p>
<p>Continual Anomaly Detection (CAD) enables anomaly detection models in learning new classes while preserving knowledge of historical classes. CAD faces two key challenges: catastrophic forgetting and segmentation of small anomalous regions. Existing CAD methods store image distributions or patch features to mitigate catastrophic forgetting, but they fail to preserve pixel-level detailed features for accurate segmentation. To overcome this limitation, we propose ReplayCAD, a novel diffusion-driven generative replay framework that replay high-quality historical data, thus effectively preserving pixel-level detailed features. Specifically, we compress historical data by searching for a class semantic embedding in the conditional space of the pre-trained diffusion model, which can guide the model to replay data with fine-grained pixel details, thus improving the segmentation performance. However, relying solely on semantic features results in limited spatial diversity. Hence, we further use spatial features to guide data compression, achieving precise control of sample space, thereby generating more diverse data. Our method achieves state-of-the-art performance in both classification and segmentation, with notable improvements in segmentation: 11.5% on VisA and 8.1% on MVTec. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/HULEI7/ReplayCAD">https://github.com/HULEI7/ReplayCAD</a>. </p>
<blockquote>
<p>æŒç»­å¼‚å¸¸æ£€æµ‹ï¼ˆCADï¼‰èƒ½å¤Ÿä½¿å¼‚å¸¸æ£€æµ‹æ¨¡å‹åœ¨å­¦ä¹ æ–°ç±»åˆ«æ—¶ä¿ç•™å¯¹å†å²ç±»åˆ«çš„çŸ¥è¯†ã€‚CADé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç¾éš¾æ€§é—å¿˜å’Œå°åŒºåŸŸå¼‚å¸¸çš„åˆ†å‰²ã€‚ç°æœ‰çš„CADæ–¹æ³•é€šè¿‡å­˜å‚¨å›¾åƒåˆ†å¸ƒæˆ–è¡¥ä¸ç‰¹å¾æ¥ç¼“è§£ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ï¼Œä½†å®ƒä»¬æ— æ³•ä¿ç•™åƒç´ çº§åˆ«çš„è¯¦ç»†ç‰¹å¾ä»¥å®ç°å‡†ç¡®åˆ†å‰²ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ReplayCADï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ‰©æ•£é©±åŠ¨ç”Ÿæˆå›æ”¾æ¡†æ¶ï¼Œå¯ä»¥å›æ”¾é«˜è´¨é‡çš„å†å²æ•°æ®ï¼Œä»è€Œæœ‰æ•ˆåœ°ä¿ç•™åƒç´ çº§åˆ«çš„è¯¦ç»†ç‰¹å¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡æœç´¢é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ç©ºé—´ä¸­çš„ç±»è¯­ä¹‰åµŒå…¥æ¥å‹ç¼©å†å²æ•°æ®ï¼Œè¿™å¯ä»¥å¼•å¯¼æ¨¡å‹ä»¥ç²¾ç»†çš„åƒç´ ç»†èŠ‚å›æ”¾æ•°æ®ï¼Œä»è€Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚ç„¶è€Œï¼Œä»…ä¾èµ–è¯­ä¹‰ç‰¹å¾ä¼šå¯¼è‡´ç©ºé—´å¤šæ ·æ€§æœ‰é™ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨ç©ºé—´ç‰¹å¾æ¥æŒ‡å¯¼æ•°æ®å‹ç¼©ï¼Œå®ç°å¯¹æ ·æœ¬ç©ºé—´çš„ç²¾ç¡®æ§åˆ¶ï¼Œä»è€Œç”Ÿæˆæ›´å¤šæ ·åŒ–çš„æ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ†ç±»å’Œåˆ†å‰²æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨åˆ†å‰²æ–¹é¢çš„æ”¹è¿›å°¤ä¸ºæ˜¾è‘—ï¼šVisAä¸Šæé«˜äº†11.5%ï¼ŒMVTecä¸Šæé«˜äº†8.1%ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HULEI7/ReplayCAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HULEI7/ReplayCADæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06603v1">PDF</a> Accepted by IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Continual Anomaly Detection (CAD)é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¾éš¾æ€§é—å¿˜å’Œå¾®å°å¼‚å¸¸åŒºåŸŸåˆ†å‰²é—®é¢˜ã€‚ç°æœ‰CADæ–¹æ³•é€šè¿‡å­˜å‚¨å›¾åƒåˆ†å¸ƒæˆ–è¡¥ä¸ç‰¹å¾æ¥å‡è½»ç¾éš¾æ€§é—å¿˜ï¼Œä½†æ— æ³•ä¿ç•™åƒç´ çº§è¯¦ç»†ç‰¹å¾ä»¥å®ç°å‡†ç¡®åˆ†å‰²ã€‚é’ˆå¯¹æ­¤é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºReplayCADï¼Œä¸€ç§æ–°å‹æ‰©æ•£é©±åŠ¨ç”Ÿæˆå›æ”¾æ¡†æ¶ï¼Œé€šè¿‡å›æ”¾é«˜è´¨é‡å†å²æ•°æ®æœ‰æ•ˆä¿ç•™åƒç´ çº§è¯¦ç»†ç‰¹å¾ã€‚è¯¥æ–¹æ³•é€šè¿‡æœç´¢é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ç©ºé—´ä¸­çš„ç±»è¯­ä¹‰åµŒå…¥æ¥å‹ç¼©å†å²æ•°æ®ï¼Œå¯æŒ‡å¯¼æ¨¡å‹ä»¥ç²¾ç»†çš„åƒç´ ç»†èŠ‚å›æ”¾æ•°æ®ï¼Œä»è€Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚åŒæ—¶ï¼Œç»“åˆç©ºé—´ç‰¹å¾æŒ‡å¯¼æ•°æ®å‹ç¼©ï¼Œå®ç°æ ·æœ¬ç©ºé—´çš„ç²¾ç¡®æ§åˆ¶ï¼Œç”Ÿæˆæ›´å¤šæ ·åŒ–çš„æ•°æ®ã€‚è¯¥æ–¹æ³•åœ¨åˆ†ç±»å’Œåˆ†å‰²æ–¹é¢éƒ½å–å¾—äº†æœ€æ–°æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†å‰²æ–¹é¢ï¼šVisAä¸Šæé«˜äº†11.5%ï¼ŒMVTecä¸Šæé«˜äº†8.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Continual Anomaly Detection (CAD) é¢ä¸´ç¾éš¾æ€§é—å¿˜å’Œå¾®å°å¼‚å¸¸åŒºåŸŸåˆ†å‰²çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰CADæ–¹æ³•ä¸»è¦ä¾èµ–å­˜å‚¨å›¾åƒåˆ†å¸ƒæˆ–è¡¥ä¸ç‰¹å¾æ¥åº”å¯¹ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œä½†æ— æ³•å®ç°å‡†ç¡®åˆ†å‰²ã€‚</li>
<li>ReplayCADæ˜¯ä¸€ä¸ªæ–°å‹çš„æ‰©æ•£é©±åŠ¨ç”Ÿæˆå›æ”¾æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å›æ”¾é«˜è´¨é‡å†å²æ•°æ®æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ReplayCADé€šè¿‡æœç´¢é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ç©ºé—´ä¸­çš„ç±»è¯­ä¹‰åµŒå…¥æ¥å‹ç¼©å†å²æ•°æ®ã€‚</li>
<li>ReplayCADç»“åˆäº†è¯­ä¹‰ç‰¹å¾ï¼Œæé«˜äº†åƒç´ çº§åˆ«çš„ç²¾ç»†å›æ”¾èƒ½åŠ›ï¼Œè¿›è€Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>é™¤äº†è¯­ä¹‰ç‰¹å¾å¤–ï¼ŒReplayCADè¿˜ä½¿ç”¨ç©ºé—´ç‰¹å¾æ¥æŒ‡å¯¼æ•°æ®å‹ç¼©ï¼Œå®ç°æ ·æœ¬ç©ºé—´çš„ç²¾ç¡®æ§åˆ¶å¹¶ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06603">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-34fdf8579c8d81eea46441f51183e197.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a1a4c2ffe4ef9c1858d9243ef439a71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2b3c415ec9b8213262945235cfab3387.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cba776b7b2a0ab04c09c9b799767657b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d7021ee66bf99f7cab661167e69b519.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c6405c9fbcd7444dfc3ecc994daba97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dd9a5a0c9dfef755570c41a93cfcb54.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DiffLocks-Generating-3D-Hair-from-a-Single-Image-using-Diffusion-Models"><a href="#DiffLocks-Generating-3D-Hair-from-a-Single-Image-using-Diffusion-Models" class="headerlink" title="DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models"></a>DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models</h2><p><strong>Authors:Radu Alexandru Rosu, Keyu Wu, Yao Feng, Youyi Zheng, Michael J. Black</strong></p>
<p>We address the task of generating 3D hair geometry from a single image, which is challenging due to the diversity of hairstyles and the lack of paired image-to-3D hair data. Previous methods are primarily trained on synthetic data and cope with the limited amount of such data by using low-dimensional intermediate representations, such as guide strands and scalp-level embeddings, that require post-processing to decode, upsample, and add realism. These approaches fail to reconstruct detailed hair, struggle with curly hair, or are limited to handling only a few hairstyles. To overcome these limitations, we propose DiffLocks, a novel framework that enables detailed reconstruction of a wide variety of hairstyles directly from a single image. First, we address the lack of 3D hair data by automating the creation of the largest synthetic hair dataset to date, containing 40K hairstyles. Second, we leverage the synthetic hair dataset to learn an image-conditioned diffusion-transfomer model that generates accurate 3D strands from a single frontal image. By using a pretrained image backbone, our method generalizes to in-the-wild images despite being trained only on synthetic data. Our diffusion model predicts a scalp texture map in which any point in the map contains the latent code for an individual hair strand. These codes are directly decoded to 3D strands without post-processing techniques. Representing individual strands, instead of guide strands, enables the transformer to model the detailed spatial structure of complex hairstyles. With this, DiffLocks can recover highly curled hair, like afro hairstyles, from a single image for the first time. Data and code is available at <a target="_blank" rel="noopener" href="https://radualexandru.github.io/difflocks/">https://radualexandru.github.io/difflocks/</a> </p>
<blockquote>
<p>æˆ‘ä»¬è§£å†³äº†ä»å•å¼ å›¾åƒç”Ÿæˆ3Då¤´å‘å‡ ä½•å½¢çŠ¶çš„ä»»åŠ¡ï¼Œè¿™æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå‘å‹å¤šæ ·ï¼Œå¹¶ä¸”ç¼ºä¹æˆå¯¹çš„å›¾åƒåˆ°3Då¤´å‘æ•°æ®ã€‚ä¹‹å‰çš„æ–¹æ³•ä¸»è¦æ˜¯åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡ä½¿ç”¨ä½ç»´ä¸­é—´è¡¨ç¤ºï¼ˆå¦‚å¼•å¯¼å‘ä¸å’Œå¤´çš®çº§åµŒå…¥ï¼‰æ¥å¤„ç†æ­¤ç±»æ•°æ®çš„æœ‰é™é‡ï¼Œè¿™äº›è¡¨ç¤ºéœ€è¦åå¤„ç†æ¥è¿›è¡Œè§£ç ã€ä¸Šé‡‡æ ·å’Œå¢åŠ çœŸå®æ„Ÿã€‚è¿™äº›æ–¹æ³•æ— æ³•é‡å»ºè¯¦ç»†çš„å¤´å‘ï¼Œéš¾ä»¥å¤„ç†å·å‘ï¼Œæˆ–è€…ä»…é™äºå¤„ç†å‡ ç§å‘å‹ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†DiffLocksï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿä»å•å¼ å›¾åƒç›´æ¥é‡å»ºå„ç§å‘å‹ç»†èŠ‚çš„æ–°æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡è‡ªåŠ¨åŒ–åˆ›å»ºè¿„ä»Šä¸ºæ­¢æœ€å¤§çš„åˆæˆå¤´å‘æ•°æ®é›†æ¥è§£å†³3Då¤´å‘æ•°æ®çš„ç¼ºä¹é—®é¢˜ï¼Œè¯¥æ•°æ®é›†åŒ…å«4ä¸‡ç§å‘å‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨åˆæˆå¤´å‘æ•°æ®é›†æ¥å­¦ä¹ ä¸€ä¸ªå—å›¾åƒæ¡ä»¶æ‰©æ•£çš„å˜å‹å™¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä»å•å¼ æ­£é¢å›¾åƒç”Ÿæˆå‡†ç¡®çš„3Då‘ä¸ã€‚é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒä¸»å¹²ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ¨å¹¿åˆ°é‡ç”Ÿå›¾åƒï¼Œå°½ç®¡å®ƒåªåœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„æ‰©æ•£æ¨¡å‹é¢„æµ‹äº†ä¸€ä¸ªå¤´çš®çº¹ç†å›¾ï¼Œè¯¥å›¾ä¸­çš„ä»»ä½•ç‚¹éƒ½åŒ…å«å•ä¸ªå‘ä¸çš„æ½œåœ¨ä»£ç ã€‚è¿™äº›ä»£ç ç›´æ¥è¢«è§£ç ä¸º3Då‘ä¸ï¼Œæ— éœ€åå¤„ç†æŠ€å·§ã€‚é€šè¿‡è¡¨ç¤ºå•ä¸ªå‘ä¸è€Œä¸æ˜¯å¼•å¯¼å‘ä¸ï¼Œå˜å‹å™¨èƒ½å¤Ÿæ¨¡æ‹Ÿå¤æ‚å‘å‹çš„è¯¦ç»†ç©ºé—´ç»“æ„ã€‚å› æ­¤ï¼ŒDiffLocksèƒ½å¤Ÿé¦–æ¬¡ä»å•å¼ å›¾åƒä¸­æ¢å¤é«˜åº¦å·æ›²çš„å¤´å‘ï¼Œå¦‚éæ´²å¼å‘å‹ã€‚æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://radualexandru.github.io/difflocks/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://radualexandru.github.io/difflocks/ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06166v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡è§£å†³äº†ä»å•å¼ å›¾åƒç”Ÿæˆ3Då¤´å‘å‡ ä½•ç»“æ„çš„é—®é¢˜ã€‚é’ˆå¯¹å‘å‹å¤šæ ·æ€§å’Œç¼ºä¹é…å¯¹å›¾åƒåˆ°3Då¤´å‘æ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†DiffLocksæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªåŠ¨åŒ–åˆ›å»ºè¿„ä»Šä¸ºæ­¢æœ€å¤§çš„åˆæˆå¤´å‘æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«4ä¸‡å¤šç§å‘å‹æ¥è§£å†³æ•°æ®ç¼ºä¹çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨åˆæˆå¤´å‘æ•°æ®é›†å­¦ä¹ å›¾åƒæ¡ä»¶æ‰©æ•£è½¬ç§»æ¨¡å‹ï¼Œç›´æ¥ä»å•å¼ æ­£é¢å›¾åƒç”Ÿæˆå‡†ç¡®çš„3Då‘ä¸ã€‚é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒä¸»å¹²ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ¨å¹¿åˆ°é‡å¤–å›¾åƒï¼Œå°½ç®¡åªæ¥å—åˆæˆæ•°æ®è®­ç»ƒã€‚æ‰©æ•£æ¨¡å‹é¢„æµ‹å¤´çš®çº¹ç†å›¾ï¼Œå›¾ä¸­çš„æ¯ä¸ªç‚¹éƒ½åŒ…å«å•ä¸ªå‘ä¸æ½œä»£ç ï¼Œå¯ç›´æ¥è§£ç ä¸º3Då‘ä¸ï¼Œæ— éœ€åå¤„ç†ã€‚è¿™ä»£è¡¨å•ä¸ªå‘ä¸ï¼Œè€Œä¸æ˜¯å¼•å¯¼å‘ä¸ï¼Œä½¿è½¬æ¢å™¨èƒ½å¤Ÿæ¨¡æ‹Ÿå¤æ‚å‘å‹çš„è¯¦ç»†ç©ºé—´ç»“æ„ã€‚DiffLocksèƒ½é¦–æ¬¡ä»å•ä¸€å›¾åƒä¸­æ¢å¤é«˜åº¦å·æ›²çš„å¤´å‘ï¼Œå¦‚éæ´²å‘å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiffLocksæ¡†æ¶è§£å†³äº†ä»å•å¼ å›¾åƒç”Ÿæˆ3Då¤´å‘å‡ ä½•ç»“æ„çš„é—®é¢˜ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§å‘å‹ã€‚</li>
<li>è‡ªåŠ¨åŒ–åˆ›å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„åˆæˆå¤´å‘æ•°æ®é›†ï¼ŒåŒ…å«4ä¸‡å¤šç§å‘å‹ã€‚</li>
<li>åˆ©ç”¨åˆæˆå¤´å‘æ•°æ®é›†å­¦ä¹ å›¾åƒæ¡ä»¶æ‰©æ•£è½¬ç§»æ¨¡å‹ï¼Œç›´æ¥ç”Ÿæˆ3Då‘ä¸ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹é¢„æµ‹å¤´çš®çº¹ç†å›¾ï¼Œæ¯ä¸ªç‚¹åŒ…å«å‘ä¸æ½œä»£ç ï¼Œå¯ç›´æ¥è§£ç ä¸º3Då‘ä¸ã€‚</li>
<li>æ–¹æ³•é€šè¿‡é¢„è®­ç»ƒçš„å›¾åƒä¸»å¹²å®ç°æ¨å¹¿ï¼Œé€‚ç”¨äºé‡å¤–å›¾åƒã€‚</li>
<li>ä»£è¡¨å•ä¸ªå‘ä¸è€Œä¸æ˜¯å¼•å¯¼å‘ä¸ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿå¤æ‚å‘å‹çš„è¯¦ç»†ç©ºé—´ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4a3c05e4195b75f43ca58ef685c73b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2231254861c4255193caeddf115e0a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3c47645e709f0c2a3cd08b8d47e39cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44247661fafc3d82f756e1f49262cb97.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-566892316ef470a1251961262a11b4ac.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Noise-Consistent-Siamese-Diffusion-for-Medical-Image-Synthesis-and-Segmentation"><a href="#Noise-Consistent-Siamese-Diffusion-for-Medical-Image-Synthesis-and-Segmentation" class="headerlink" title="Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and   Segmentation"></a>Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and   Segmentation</h2><p><strong>Authors:Kunpeng Qiu, Zhiqiang Gao, Zhiying Zhou, Mingjie Sun, Yongxin Guo</strong></p>
<p>Deep learning has revolutionized medical image segmentation, yet its full potential remains constrained by the paucity of annotated datasets. While diffusion models have emerged as a promising approach for generating synthetic image-mask pairs to augment these datasets, they paradoxically suffer from the same data scarcity challenges they aim to mitigate. Traditional mask-only models frequently yield low-fidelity images due to their inability to adequately capture morphological intricacies, which can critically compromise the robustness and reliability of segmentation models. To alleviate this limitation, we introduce Siamese-Diffusion, a novel dual-component model comprising Mask-Diffusion and Image-Diffusion. During training, a Noise Consistency Loss is introduced between these components to enhance the morphological fidelity of Mask-Diffusion in the parameter space. During sampling, only Mask-Diffusion is used, ensuring diversity and scalability. Comprehensive experiments demonstrate the superiority of our method. Siamese-Diffusion boosts SANetâ€™s mDice and mIoU by 3.6% and 4.4% on the Polyps, while UNet improves by 1.52% and 1.64% on the ISIC2018. Code is available at GitHub. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å·²ç»å½»åº•æ”¹å˜äº†åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸï¼Œä½†å…¶æ½œåŠ›ä»ç„¶å—åˆ°æ ‡æ³¨æ•°æ®é›†ç¼ºä¹çš„é™åˆ¶ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹ä½œä¸ºä¸€ç§ç”Ÿæˆåˆæˆå›¾åƒ-æ©è†œå¯¹ä»¥å¢å¼ºè¿™äº›æ•°æ®é›†çš„æ–¹æ³•å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å®ƒä»¬å´é¢ä¸´ç€ç›¸åŒçš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ï¼Œå³å®ƒä»¬è¯•å›¾ç¼“è§£çš„é—®é¢˜ã€‚ä¼ ç»Ÿçš„ä»…ä½¿ç”¨æ©è†œæ¨¡å‹ç”±äºæ— æ³•å……åˆ†æ•æ‰å½¢æ€ç»†èŠ‚ï¼Œç»å¸¸äº§ç”Ÿä½ä¿çœŸåº¦çš„å›¾åƒï¼Œè¿™å¯èƒ½ä¼šä¸¥é‡æŸå®³åˆ†å‰²æ¨¡å‹çš„ç¨³å¥æ€§å’Œå¯é æ€§ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†Siamese-Diffusionï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŒç»„åˆ†æ¨¡å‹ï¼ŒåŒ…æ‹¬Mask-Diffusionå’ŒImage-Diffusionã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åœ¨è¿™äº›ç»„ä»¶ä¹‹é—´å¼•å…¥äº†å™ªå£°ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥å¢å¼ºMask-Diffusionåœ¨å‚æ•°ç©ºé—´ä¸­çš„å½¢æ€ä¿çœŸåº¦ã€‚åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œåªä½¿ç”¨Mask-Diffusionï¼Œä»¥ç¡®ä¿å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚ç»¼åˆå®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚Siamese-Diffusionæé«˜äº†Polypsä¸Šçš„SANetçš„mDiceå’ŒmIoUåˆ†åˆ«ä¸º3.6%å’Œ4.4%ï¼Œè€ŒUNetåœ¨ISIC2018ä¸Šçš„æ”¹è¿›åˆ†åˆ«ä¸º1.52%å’Œ1.64%ã€‚ä»£ç å·²åœ¨GitHubä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06068v1">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²é¢†åŸŸå¼•èµ·äº†é©å‘½æ€§çš„å˜é©ï¼Œä½†ä»å—åˆ°æ ‡æ³¨æ•°æ®é›†ç¼ºä¹çš„é™åˆ¶ã€‚æ‰©æ•£æ¨¡å‹ä½œä¸ºä¸€ç§ç”Ÿæˆåˆæˆå›¾åƒ-æ©è†œå¯¹ä»¥å¢å¼ºæ•°æ®é›†çš„æ–¹æ³•æœ‰æ½œåŠ›ï¼Œä½†å®ƒä»¬åŒæ ·é¢ä¸´æ•°æ®ç¼ºä¹çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ä»…æ©è†œæ¨¡å‹å› æ— æ³•å……åˆ†æ•æ‰å½¢æ€ç»†èŠ‚ï¼Œå¸¸äº§ç”Ÿä½è´¨é‡å›¾åƒï¼Œè¿™ä¼šä¸¥é‡å½±å“åˆ†å‰²æ¨¡å‹çš„ç¨³å¥æ€§å’Œå¯é æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Siamese-Diffusionï¼Œä¸€ç§åŒ…å«Mask-Diffusionå’ŒImage-Diffusionçš„æ–°å‹åŒç»„ä»¶æ¨¡å‹ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åœ¨ç»„ä»¶é—´å¼•å…¥å™ªå£°ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥åœ¨å‚æ•°ç©ºé—´ä¸­æå‡Mask-Diffusionçš„å½¢æ€ä¿çœŸåº¦ã€‚é‡‡æ ·æ—¶ä»…ä½¿ç”¨Mask-Diffusionï¼Œä»¥ç¡®ä¿å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å“è¶Šï¼ŒSiamese-Diffusionæå‡äº†Polypsä¸Šçš„SANetçš„mDiceå’ŒmIoUåˆ†åˆ«ä¸º3.6%å’Œ4.4%ï¼Œè€ŒUNetåœ¨ISIC2018ä¸Šçš„æ”¹è¿›åˆ†åˆ«ä¸º1.52%å’Œ1.64%ã€‚ä»£ç å·²ä¸Šä¼ è‡³GitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­åº”ç”¨å¹¿æ³›ï¼Œä½†æ ‡æ³¨æ•°æ®é›†çš„ç¼ºä¹é™åˆ¶äº†å…¶æ½œåŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å¯ç”Ÿæˆåˆæˆå›¾åƒ-æ©è†œå¯¹ä»¥å¢å¼ºæ•°æ®é›†ï¼Œä½†é¢ä¸´æ•°æ®ç¼ºä¹çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿä»…æ©è†œæ¨¡å‹äº§ç”Ÿçš„å›¾åƒè´¨é‡è¾ƒä½ï¼Œå› æ— æ³•å……åˆ†æ•æ‰å½¢æ€ç»†èŠ‚ï¼Œå½±å“æ¨¡å‹ç¨³å¥æ€§å’Œå¯é æ€§ã€‚</li>
<li>æå‡ºçš„Siamese-Diffusionæ¨¡å‹åŒ…å«Mask-Diffusionå’ŒImage-Diffusionä¸¤ä¸ªç»„ä»¶ã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥å™ªå£°ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥æå‡Mask-Diffusionçš„å½¢æ€ä¿çœŸåº¦ã€‚</li>
<li>é‡‡æ ·æ—¶ä»…ä½¿ç”¨Mask-Diffusionï¼Œä»¥å®ç°å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>å®éªŒè¯æ˜Siamese-Diffusionè¡¨ç°å“è¶Šï¼Œå¯¹SANetå’ŒUNetçš„æ€§èƒ½æœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe6c0c678fc098c5283c7480ab238a75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28fb51bed211eace5f80b73f5a5b1071.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f7f40156b3a03df8adc5967cad7e870.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-879cd050456c3a253169968c7b573330.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10cc05b65f851b17c65f0326fc1806fd.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Mogao-An-Omni-Foundation-Model-for-Interleaved-Multi-Modal-Generation"><a href="#Mogao-An-Omni-Foundation-Model-for-Interleaved-Multi-Modal-Generation" class="headerlink" title="Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation"></a>Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation</h2><p><strong>Authors:Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, Weilin Huang</strong></p>
<p>Recent progress in unified models for image understanding and generation has been impressive, yet most approaches remain limited to single-modal generation conditioned on multiple modalities. In this paper, we present Mogao, a unified framework that advances this paradigm by enabling interleaved multi-modal generation through a causal approach. Mogao integrates a set of key technical improvements in architecture design, including a deep-fusion design, dual vision encoders, interleaved rotary position embeddings, and multi-modal classifier-free guidance, which allow it to harness the strengths of both autoregressive models for text generation and diffusion models for high-quality image synthesis. These practical improvements also make Mogao particularly effective to process interleaved sequences of text and images arbitrarily. To further unlock the potential of unified models, we introduce an efficient training strategy on a large-scale, in-house dataset specifically curated for joint text and image generation. Extensive experiments show that Mogao not only achieves state-of-the-art performance in multi-modal understanding and text-to-image generation, but also excels in producing high-quality, coherent interleaved outputs. Its emergent capabilities in zero-shot image editing and compositional generation highlight Mogao as a practical omni-modal foundation model, paving the way for future development and scaling the unified multi-modal systems. </p>
<blockquote>
<p>å…³äºå›¾åƒç†è§£å’Œç”Ÿæˆçš„ç»¼åˆæ¨¡å‹ï¼Œè¿‘æœŸè¿›å±•ä»¤äººå°è±¡æ·±åˆ»ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•ä»ç„¶å±€é™äºå¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„å•æ¨¡æ€ç”Ÿæˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Mogaoï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨è¿›è¿™ä¸€èŒƒå¼çš„ä¸€ä½“åŒ–æ¡†æ¶ï¼Œå®ƒé€šè¿‡å› æœæ–¹æ³•å®ç°äº†äº¤é”™å¤šæ¨¡æ€ç”Ÿæˆã€‚Mogaoåœ¨æ¶æ„è®¾è®¡ä¸Šè¿›è¡Œäº†ä¸€ç³»åˆ—å…³é”®æŠ€æœ¯æ”¹è¿›ï¼ŒåŒ…æ‹¬æ·±åº¦èåˆè®¾è®¡ã€åŒè§†ç¼–ç å™¨ã€äº¤é”™æ—‹è½¬ä½ç½®åµŒå…¥å’Œæ— å¤šæ¨¡æ€åˆ†ç±»å¼•å¯¼ï¼Œä½¿å…¶ç»“åˆäº†è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆæ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜è´¨é‡å›¾åƒåˆæˆçš„ä¼˜åŠ¿ã€‚è¿™äº›å®é™…æ”¹è¿›ä¹Ÿä½¿å¾—Mogaoåœ¨å¤„ç†ä»»æ„æ–‡æœ¬å’Œå›¾åƒçš„äº¤é”™åºåˆ—æ—¶ç‰¹åˆ«æœ‰æ•ˆã€‚ä¸ºäº†å……åˆ†å‘æŒ¥ç»Ÿä¸€æ¨¡å‹çš„æ½œåŠ›ï¼Œæˆ‘ä»¬åœ¨ä¸“é—¨ç”¨äºè”åˆæ–‡æœ¬å’Œå›¾åƒç”Ÿæˆçš„å¤§è§„æ¨¡å†…éƒ¨æ•°æ®é›†ä¸Šå¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMogaoä¸ä»…åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨ç”Ÿæˆé«˜è´¨é‡ã€è¿è´¯çš„äº¤é”™è¾“å‡ºæ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚å…¶åœ¨é›¶æ ·æœ¬å›¾åƒç¼–è¾‘å’Œç»„åˆç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›çªæ˜¾äº†Mogaoä½œä¸ºä¸€ä¸ªå®ç”¨çš„å…¨æ¨¡æ€åŸºç¡€æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œä¸ºæœªæ¥çš„å‘å±•å’Œæ‰©å¤§ç»Ÿä¸€å¤šæ¨¡æ€ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05472v2">PDF</a> Mogao Technical Report</p>
<p><strong>Summary</strong><br>å¤šæ¨¡æ€ç”Ÿæˆæ˜¯å›¾åƒç†è§£å’Œç”Ÿæˆé¢†åŸŸçš„ä¸€ä¸ªçƒ­é—¨è¯é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMogaoçš„ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡ä¸€ç³»åˆ—å…³é”®æŠ€æœ¯æ”¹è¿›ï¼Œå®ç°äº†å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å¼çš„çªç ´ã€‚Mogaoæ¡†æ¶é‡‡ç”¨æ·±åº¦èåˆè®¾è®¡ã€åŒé‡è§†è§‰ç¼–ç å™¨ã€äº¤é”™æ—‹è½¬ä½ç½®åµŒå…¥å’Œæ— åˆ†ç±»å™¨å¤šæ¨¡æ€æŒ‡å¯¼ç­‰æŠ€æœ¯ï¼Œèƒ½å¤Ÿå……åˆ†åˆ©ç”¨è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆæ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿è¿›è¡Œé«˜è´¨é‡å›¾åƒåˆæˆã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§åœ¨å¤§è§„æ¨¡å†…éƒ¨æ•°æ®é›†ä¸Šè®­ç»ƒç»Ÿä¸€æ¨¡å‹çš„ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç‰¹åˆ«é€‚ç”¨äºæ–‡æœ¬å’Œå›¾åƒçš„ä»»æ„äº¤é”™åºåˆ—å¤„ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒMogaoåœ¨å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨é›¶æ ·æœ¬å›¾åƒç¼–è¾‘å’Œç»„åˆç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œæˆä¸ºäº†ä¸€ä¸ªå®ç”¨çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mogaoæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå®ç°äº†å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å¼çš„çªç ´ã€‚</li>
<li>é€šè¿‡æ·±åº¦èåˆè®¾è®¡ç­‰æŠ€æœ¯æ”¹è¿›ï¼ŒMogaoèƒ½å¤Ÿç»¼åˆåˆ©ç”¨è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆæ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ã€‚</li>
<li>Mogaoå…·æœ‰å¤„ç†æ–‡æœ¬å’Œå›¾åƒä»»æ„äº¤é”™åºåˆ—çš„èƒ½åŠ›ã€‚</li>
<li>Mogaoåœ¨å¤§è§„æ¨¡å†…éƒ¨æ•°æ®é›†ä¸Šçš„è®­ç»ƒç­–ç•¥ç‰¹åˆ«æœ‰æ•ˆã€‚</li>
<li>Mogaoåœ¨å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>Mogaoåœ¨é›¶æ ·æœ¬å›¾åƒç¼–è¾‘å’Œç»„åˆç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05472">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3b99f93799d4a77979f540277e0943f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2fa425f14bfc87b9696e4db9ef6de6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e120d180d577e724d91f96249920b3eb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MDE-Edit-Masked-Dual-Editing-for-Multi-Object-Image-Editing-via-Diffusion-Models"><a href="#MDE-Edit-Masked-Dual-Editing-for-Multi-Object-Image-Editing-via-Diffusion-Models" class="headerlink" title="MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via   Diffusion Models"></a>MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via   Diffusion Models</h2><p><strong>Authors:Hongyang Zhu, Haipeng Liu, Bo Fu, Yang Wang</strong></p>
<p>Multi-object editing aims to modify multiple objects or regions in complex scenes while preserving structural coherence. This task faces significant challenges in scenarios involving overlapping or interacting objects: (1) Inaccurate localization of target objects due to attention misalignment, leading to incomplete or misplaced edits; (2) Attribute-object mismatch, where color or texture changes fail to align with intended regions due to cross-attention leakage, creating semantic conflicts (\textit{e.g.}, color bleeding into non-target areas). Existing methods struggle with these challenges: approaches relying on global cross-attention mechanisms suffer from attention dilution and spatial interference between objects, while mask-based methods fail to bind attributes to geometrically accurate regions due to feature entanglement in multi-object scenarios. To address these limitations, we propose a training-free, inference-stage optimization approach that enables precise localized image manipulation in complex multi-object scenes, named MDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via two key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention with segmentation masks for precise object positioning, and Color Consistency Loss (CCL) amplifies target attribute attention within masks while suppressing leakage to adjacent regions. This dual-loss design ensures localized and coherent multi-object edits. Extensive experiments demonstrate that MDE-Edit outperforms state-of-the-art methods in editing accuracy and visual quality, offering a robust solution for complex multi-object image manipulation tasks. </p>
<blockquote>
<p>å¤šå¯¹è±¡ç¼–è¾‘æ—¨åœ¨åœ¨å¤æ‚çš„åœºæ™¯ä¸­ä¿®æ”¹å¤šä¸ªå¯¹è±¡æˆ–åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒç»“æ„è¿è´¯æ€§ã€‚åœ¨æ¶‰åŠé‡å æˆ–äº¤äº’å¯¹è±¡çš„åœºæ™¯ä¸­ï¼Œæ­¤ä»»åŠ¡é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼š(1)ç”±äºæ³¨æ„åŠ›é”™ä½å¯¼è‡´ç›®æ ‡å¯¹è±¡å®šä½ä¸å‡†ç¡®ï¼Œä»è€Œå¯¼è‡´ç¼–è¾‘ä¸å®Œæ•´æˆ–é”™ä½ï¼›(2)å±æ€§å¯¹è±¡ä¸åŒ¹é…ï¼Œç”±äºè·¨æ³¨æ„åŠ›æ³„æ¼ï¼Œé¢œè‰²æˆ–çº¹ç†å˜åŒ–æœªèƒ½ä¸æ„å›¾åŒºåŸŸå¯¹é½ï¼Œä»è€Œäº§ç”Ÿè¯­ä¹‰å†²çªï¼ˆä¾‹å¦‚ï¼Œé¢œè‰²æ¸—å…¥éç›®æ ‡åŒºåŸŸï¼‰ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼šä¾èµ–å…¨å±€è·¨æ³¨æ„åŠ›æœºåˆ¶çš„æ–¹æ³•å—åˆ°æ³¨æ„åŠ›ç¨€é‡Šå’Œå¯¹è±¡é—´ç©ºé—´å¹²æ‰°çš„å½±å“ï¼Œè€ŒåŸºäºæ©è†œçš„æ–¹æ³•ç”±äºåœ¨å¤šå¯¹è±¡åœºæ™¯ä¸­çš„ç‰¹å¾çº ç¼ è€Œæ— æ³•å°†å±æ€§ç»‘å®šåˆ°å‡ ä½•ç²¾ç¡®çš„åŒºåŸŸã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒã€åœ¨æ¨ç†é˜¶æ®µè¿›è¡Œä¼˜åŒ–çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„å¤šå¯¹è±¡åœºæ™¯ä¸­å®ç°ç²¾ç¡®çš„å®šä½å›¾åƒæ“ä½œï¼Œåä¸ºMDE-Editã€‚MDE-Edité€šè¿‡ä¸¤ä¸ªå…³é”®æŸå¤±ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ä¸­çš„å™ªå£°æ½œåœ¨ç‰¹å¾ï¼šå¯¹è±¡å¯¹é½æŸå¤±ï¼ˆOALï¼‰å°†å¤šå±‚è·¨æ³¨æ„åŠ›ä¸åˆ†å‰²æ©è†œå¯¹é½ï¼Œä»¥å®ç°ç²¾ç¡®çš„å¯¹è±¡å®šä½ï¼›é¢œè‰²ä¸€è‡´æ€§æŸå¤±ï¼ˆCCLï¼‰åœ¨æ©è†œå†…æ”¾å¤§ç›®æ ‡å±æ€§æ³¨æ„åŠ›ï¼ŒåŒæ—¶æŠ‘åˆ¶å¯¹ç›¸é‚»åŒºåŸŸçš„æ³„æ¼ã€‚è¿™ç§åŒé‡æŸå¤±è®¾è®¡ç¡®ä¿å±€éƒ¨åŒ–å’Œè¿è´¯çš„å¤šå¯¹è±¡ç¼–è¾‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMDE-Editåœ¨ç¼–è¾‘å‡†ç¡®æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œä¸ºå¤æ‚çš„å¤šå¯¹è±¡å›¾åƒæ“ä½œä»»åŠ¡æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05101v2">PDF</a> 9 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºMDE-Editçš„æ–¹æ³•ï¼Œå®ƒå¯ä»¥åœ¨å¤æ‚çš„å¤šå¯¹è±¡åœºæ™¯ä¸­å®ç°ç²¾ç¡®çš„å±€éƒ¨å›¾åƒæ“ä½œã€‚MDE-Edité€šè¿‡åœ¨æ‰©æ•£æ¨¡å‹ä¸­ä¼˜åŒ–å™ªå£°æ½œåœ¨ç‰¹å¾ï¼Œé‡‡ç”¨ä¸¤ç§å…³é”®æŸå¤±ï¼šå¯¹è±¡å¯¹é½æŸå¤±ï¼ˆOALï¼‰å’Œé¢œè‰²ä¸€è‡´æ€§æŸå¤±ï¼ˆCCLï¼‰ï¼Œå®ç°å¯¹å¤šå¯¹è±¡è¿›è¡Œç²¾ç¡®å®šä½å’Œå¯¹ç›®æ ‡å±æ€§è¿›è¡Œä¸€è‡´ç¼–è¾‘ã€‚è¯¥æ–¹æ³•åœ¨ç¼–è¾‘å‡†ç¡®æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸ºå¤æ‚çš„å¤šå¯¹è±¡å›¾åƒæ“ä½œä»»åŠ¡æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šå¯¹è±¡ç¼–è¾‘é¢ä¸´æŒ‘æˆ˜ï¼šç›®æ ‡å¯¹è±¡å®šä½ä¸å‡†ç¡®ï¼Œå¯¼è‡´ç¼–è¾‘ä¸å®Œæ•´æˆ–é”™ä½ï¼›å±æ€§ä¸å¯¹è±¡ä¸åŒ¹é…ï¼Œç”±äºè·¨æ³¨æ„åŠ›æ³„æ¼å¯¼è‡´è¯­ä¹‰å†²çªã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™äº›æŒ‘æˆ˜æ—¶å­˜åœ¨å›°éš¾ï¼Œå…¨çƒè·¨æ³¨æ„åŠ›æœºåˆ¶å’ŒåŸºäºé®ç½©çš„æ–¹æ³•éƒ½æœ‰å±€é™æ€§ã€‚</li>
<li>MDE-Edité‡‡ç”¨è®­ç»ƒå¤–ã€æ¨ç†é˜¶æ®µä¼˜åŒ–çš„æ–¹æ³•ï¼Œå¯åœ¨å¤æ‚çš„å¤šå¯¹è±¡åœºæ™¯ä¸­å®ç°ç²¾ç¡®å±€éƒ¨å›¾åƒæ“ä½œã€‚</li>
<li>MDE-Edité€šè¿‡ä¸¤ç§å…³é”®æŸå¤±å®ç°ï¼šå¯¹è±¡å¯¹é½æŸå¤±ï¼ˆOALï¼‰ç”¨äºç²¾ç¡®å¯¹è±¡å®šä½ï¼Œé¢œè‰²ä¸€è‡´æ€§æŸå¤±ï¼ˆCCLï¼‰ç¡®ä¿ç›®æ ‡å±æ€§åœ¨é®ç½©å†…ä¸€è‡´ï¼ŒåŒæ—¶æŠ‘åˆ¶æ³„æ¼åˆ°ç›¸é‚»åŒºåŸŸã€‚</li>
<li>MDE-Edité€šè¿‡ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ä¸­çš„å™ªå£°æ½œåœ¨ç‰¹å¾æ¥å®ç°ç²¾ç¡®ç¼–è¾‘ã€‚</li>
<li>MDE-Editåœ¨ç¼–è¾‘å‡†ç¡®æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢è¡¨ç°å‡ºä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f875c81ae13cb2a1f7aaf72f062c980.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1165ef63d018b232d422a25e3fc7b4bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f44aa69c4372bd24d86a31a3ec29e793.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ItDPDM-Information-Theoretic-Discrete-Poisson-Diffusion-Model"><a href="#ItDPDM-Information-Theoretic-Discrete-Poisson-Diffusion-Model" class="headerlink" title="ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model"></a>ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model</h2><p><strong>Authors:Sagnik Bhattacharya, Abhiram Gorle, Ahmed Mohsin, Ahsan Bilal, Connor Ding, Amit Kumar Singh Yadav, Tsachy Weissman</strong></p>
<p>Existing methods for generative modeling of discrete data, such as symbolic music tokens, face two primary challenges: (1) they either embed discrete inputs into continuous state-spaces or (2) rely on variational losses that only approximate the true negative log-likelihood. Previous efforts have individually targeted these limitations. While information-theoretic Gaussian diffusion models alleviate the suboptimality of variational losses, they still perform modeling in continuous domains. In this work, we introduce the Information-Theoretic Discrete Poisson Diffusion Model (ItDPDM), which simultaneously addresses both limitations by directly operating in a discrete state-space via a Poisson diffusion process inspired by photon arrival processes in camera sensors. We introduce a novel Poisson Reconstruction Loss (PRL) and derive an exact relationship between PRL and the true negative log-likelihood, thereby eliminating the need for approximate evidence lower bounds. Experiments conducted on the Lakh MIDI symbolic music dataset and the CIFAR-10 image benchmark demonstrate that ItDPDM delivers significant improvements, reducing test NLL by up to 80% compared to prior baselines, while also achieving faster convergence. </p>
<blockquote>
<p>ç°æœ‰çš„ç¦»æ•£æ•°æ®ç”Ÿæˆæ¨¡å‹æ–¹æ³•ï¼Œå¦‚ç¬¦å·éŸ³ä¹ä»¤ç‰Œï¼Œé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šä¸€æ˜¯å°†ç¦»æ•£è¾“å…¥åµŒå…¥è¿ç»­çŠ¶æ€ç©ºé—´ï¼ŒäºŒæ˜¯ä¾èµ–äºä»…è¿‘ä¼¼çœŸå®è´Ÿå¯¹æ•°ä¼¼ç„¶çš„å˜åŒ–æŸå¤±ã€‚ä¹‹å‰çš„åŠªåŠ›å·²ç»åˆ†åˆ«é’ˆå¯¹è¿™äº›å±€é™æ€§è¿›è¡Œäº†æ”¹è¿›ã€‚è™½ç„¶ä¿¡æ¯ç†è®ºé«˜æ–¯æ‰©æ•£æ¨¡å‹ç¼“è§£äº†å˜åŒ–æŸå¤±çš„æ¬¡ä¼˜æ€§ï¼Œä½†å®ƒä»¬ä»ç„¶åœ¨è¿ç»­åŸŸä¸­è¿›è¡Œå»ºæ¨¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¿¡æ¯ç†è®ºç¦»æ•£æ³Šæ¾æ‰©æ•£æ¨¡å‹ï¼ˆItDPDMï¼‰ï¼Œè¯¥æ¨¡å‹é€šè¿‡å—ç›¸æœºä¼ æ„Ÿå™¨ä¸­å…‰å­åˆ°è¾¾è¿‡ç¨‹å¯å‘çš„æ³Šæ¾æ‰©æ•£è¿‡ç¨‹ï¼Œç›´æ¥åœ¨ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­è§£å†³è¿™ä¸¤ä¸ªå±€é™æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ³Šæ¾é‡å»ºæŸå¤±ï¼ˆPRLï¼‰å¹¶æ¨å¯¼å‡ºPRLä¸çœŸå®è´Ÿå¯¹æ•°ä¼¼ç„¶ä¹‹é—´çš„ç²¾ç¡®å…³ç³»ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹è¿‘ä¼¼è¯æ®ä¸‹é™çš„éœ€æ±‚ã€‚åœ¨Lakh MIDIç¬¦å·éŸ³ä¹æ•°æ®é›†å’ŒCIFAR-10å›¾åƒåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒItDPDMæä¾›äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä¸å…ˆå‰çš„åŸºç¡€ç›¸æ¯”ï¼Œæµ‹è¯•NLLé™ä½äº†é«˜è¾¾80%ï¼ŒåŒæ—¶å®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05082v2">PDF</a> Pre-print</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¿¡æ¯ç†è®ºç¦»æ•£æ³Šæ¾æ‰©æ•£æ¨¡å‹ï¼ˆItDPDMï¼‰ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ³Šæ¾æ‰©æ•£è¿‡ç¨‹ç›´æ¥åœ¨ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œè§£å†³äº†å½“å‰ç”Ÿæˆç¦»æ•£æ•°æ®å»ºæ¨¡çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚ItDPDMå¼•å…¥äº†ä¸€ç§æ–°çš„æ³Šæ¾é‡å»ºæŸå¤±ï¼ˆPRLï¼‰ï¼Œå¹¶ä¸çœŸæ­£çš„è´Ÿå¯¹æ•°ä¼¼ç„¶å»ºç«‹äº†ç²¾ç¡®çš„å…³ç³»ï¼Œä»è€Œä¸éœ€è¦ä½¿ç”¨è¿‘ä¼¼çš„è¯æ®ä¸‹ç•Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒItDPDMåœ¨Lakh MIDIç¬¦å·éŸ³ä¹æ•°æ®é›†å’ŒCIFAR-10å›¾åƒåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œä¸å…ˆå‰çš„åŸºç¡€ç›¸æ¯”ï¼Œæµ‹è¯•NLLé™ä½äº†é«˜è¾¾80%ï¼ŒåŒæ—¶å®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ItDPDMè§£å†³äº†ç°æœ‰ç”Ÿæˆç¦»æ•£æ•°æ®å»ºæ¨¡æ–¹æ³•é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>ItDPDMé€šè¿‡æ³Šæ¾æ‰©æ•£è¿‡ç¨‹ç›´æ¥åœ¨ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­è¿›è¡Œæ“ä½œã€‚</li>
<li>å¼•å…¥äº†æ–°çš„æ³Šæ¾é‡å»ºæŸå¤±ï¼ˆPRLï¼‰ã€‚</li>
<li>PRLä¸çœŸæ­£çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ä¹‹é—´å»ºç«‹äº†ç²¾ç¡®çš„å…³ç³»ã€‚</li>
<li>æ— éœ€ä½¿ç”¨è¿‘ä¼¼çš„è¯æ®ä¸‹ç•Œã€‚</li>
<li>ItDPDMåœ¨ç¬¦å·éŸ³ä¹æ•°æ®é›†å’Œå›¾åƒåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f9c718eff41eb7b5ff326c9339c62e9a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-34bbeea2828f004d0d3f1b69eecc7fed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f55ff440643d436eab530eb25fea48ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c861688623ec2e008878ac47d1d2df6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14e74a7fe39681e308d94dead5c4f8dc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="From-Spaceborne-to-Airborne-SAR-Image-Synthesis-Using-Foundation-Models-for-Multi-Scale-Adaptation"><a href="#From-Spaceborne-to-Airborne-SAR-Image-Synthesis-Using-Foundation-Models-for-Multi-Scale-Adaptation" class="headerlink" title="From Spaceborne to Airborne: SAR Image Synthesis Using Foundation Models   for Multi-Scale Adaptation"></a>From Spaceborne to Airborne: SAR Image Synthesis Using Foundation Models   for Multi-Scale Adaptation</h2><p><strong>Authors:Solene Debuysere, Nicolas Trouve, Nathan Letheule, Olivier Leveque, Elise Colin</strong></p>
<p>The availability of Synthetic Aperture Radar (SAR) satellite imagery has increased considerably in recent years, with datasets commercially available. However, the acquisition of high-resolution SAR images in airborne configurations, remains costly and limited. Thus, the lack of open source, well-labeled, or easily exploitable SAR text-image datasets is a barrier to the use of existing foundation models in remote sensing applications. In this context, synthetic image generation is a promising solution to augment this scarce data, enabling a broader range of applications. Leveraging over 15 years of ONERAâ€™s extensive archival airborn data from acquisition campaigns, we created a comprehensive training dataset of 110 thousands SAR images to exploit a 3.5 billion parameters pre-trained latent diffusion model \cite{Baqu2019SethiR}. In this work, we present a novel approach utilizing spatial conditioning techniques within a foundation model to transform satellite SAR imagery into airborne SAR representations. Additionally, we demonstrate that our pipeline is effective for bridging the realism of simulated images generated by ONERAâ€™s physics-based simulator EMPRISE \cite{empriseem_ai_images}. Our method explores a key application of AI in advancing SAR imaging technology. To the best of our knowledge, we are the first to introduce this approach in the literature. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œåˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å«æ˜Ÿå›¾åƒçš„å¯è·å–æ€§å¤§å¹…å¢åŠ ï¼Œå¹¶ä¸”æ•°æ®é›†å·²å®ç°å•†ä¸šåŒ–ã€‚ç„¶è€Œï¼Œè·å–ç©ºä¸­é…ç½®çš„SARé«˜åˆ†è¾¨ç‡å›¾åƒæˆæœ¬ä»ç„¶é«˜æ˜‚ä¸”èµ„æºæœ‰é™ã€‚å› æ­¤ï¼Œç¼ºä¹å¼€æ”¾æºä»£ç ã€å¸¦æœ‰æ˜ç¡®æ ‡ç­¾æˆ–æ˜“äºå¼€å‘çš„SARæ–‡æœ¬å›¾åƒæ•°æ®é›†æ˜¯é˜»ç¢ç°æœ‰åŸºç¡€æ¨¡å‹åœ¨é¥æ„Ÿåº”ç”¨ä¸­ä½¿ç”¨çš„ä¸€ä¸ªéšœç¢ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåˆæˆå›¾åƒç”Ÿæˆæ˜¯è§£å†³æ•°æ®ç¨€ç¼ºçš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ”¯æŒæ›´å¹¿æ³›çš„åº”ç”¨ã€‚åˆ©ç”¨æ³•å›½èˆªç©ºèˆªå¤©ç ”ç©¶é™¢ï¼ˆONERAï¼‰å¤šå¹´æ¥æ”¶é›†çš„15ä¸‡ä½™å¼ ä»å¤šä¸ªæ”¶é›†ä»»åŠ¡æ”¶é›†çš„æ•°æ®æ„å»ºçš„èˆªç©ºå’Œå›¾åƒæ•°æ®åº“æ•°æ®é›†èµ„æºï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«ç™¾ä¸‡SARå›¾åƒçš„ç»¼åˆè®­ç»ƒæ•°æ®é›†æ¥å¼€å‘æ‹¥æœ‰3.5äº¿å‚æ•°çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼ˆBaquç­‰äººåœ¨å…¶è‘—ä½œã€Šåˆ©ç”¨æ·±åº¦å­¦ä¹ åœ¨è¶…å®½å¸¦è°±åˆæˆå­”å¾„é›·è¾¾ä¸Šè¿›è¡Œç‰¹å¾è¯†åˆ«å’Œå®šé‡æˆåƒåˆ†æã€‹ä¸­æåˆ°ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹ä¸­çš„ç©ºé—´æ¡ä»¶æŠ€æœ¯å°†å«æ˜ŸSARå›¾åƒè½¬æ¢ä¸ºç©ºä¸­SARè¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„ç®¡é“å¯ä»¥æœ‰æ•ˆåœ°å°†ä»¿çœŸå›¾åƒçš„é€¼çœŸåº¦æé«˜åˆ°ç”±ONERAåŸºäºç‰©ç†çš„æ¨¡æ‹Ÿå™¨EMPRISEç”Ÿæˆçš„å›¾åƒæ°´å¹³ï¼ˆempriseemåœ¨å…¶è‘—ä½œä¸­æåˆ°ï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¢ç´¢äº†äººå·¥æ™ºèƒ½åœ¨æ¨åŠ¨SARæˆåƒæŠ€æœ¯æ–¹é¢çš„ä¸€é¡¹å…³é”®åº”ç”¨ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªåœ¨æ–‡çŒ®ä¸­ä»‹ç»è¿™ç§æ–¹æ³•çš„äººã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03844v2">PDF</a> </p>
<p><strong>Summary</strong><br>SARå«æ˜Ÿå›¾åƒæ•°æ®æ—¥ç›Šä¸°å¯Œï¼Œä½†é«˜åˆ†è¾¨ç‡çš„SARå›¾åƒè·å–æˆæœ¬é«˜æ˜‚ä¸”æœ‰é™ã€‚ç¼ºä¹å¼€æºã€æ ‡æ³¨è‰¯å¥½æˆ–æ˜“äºåˆ©ç”¨çš„SARæ–‡æœ¬å›¾åƒæ•°æ®é›†æˆä¸ºä½¿ç”¨ç°æœ‰é¥æ„Ÿåº”ç”¨åŸºç¡€æ¨¡å‹çš„éšœç¢ã€‚åˆ©ç”¨ONERAé•¿è¾¾15å¹´çš„å¤§é‡å­˜æ¡£é£è¡Œæ•°æ®ï¼Œåˆ›å»ºäº†ä¸€ä¸ªåŒ…å«11ä¸‡å¼ SARå›¾åƒçš„ç»¼åˆè®­ç»ƒæ•°æ®é›†ï¼Œåˆ©ç”¨å«æœ‰3.5äº¿å‚æ•°çš„é¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œåˆ©ç”¨ã€‚æœ¬ç ”ç©¶é‡‡ç”¨ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹ä¸­çš„ç©ºé—´æ¡ä»¶æŠ€æœ¯ï¼Œå°†å«æ˜ŸSARå›¾åƒè½¬æ¢ä¸ºé£è¡ŒSARè¡¨ç¤ºå½¢å¼ã€‚æ­¤å¤–ï¼Œå±•ç¤ºäº†è¯¥ç®¡é“èƒ½æœ‰æ•ˆå¼¥åˆç”±ONERAçš„ç‰©ç†æ¨¡æ‹Ÿå™¨EMPRISEç”Ÿæˆçš„æ¨¡æ‹Ÿå›¾åƒçš„ç°å®æ„Ÿå·®è·ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†AIåœ¨æ¨åŠ¨SARæˆåƒæŠ€æœ¯æ–¹é¢çš„é‡è¦åº”ç”¨ã€‚æˆ‘ä»¬æ˜¯æ–‡çŒ®ä¸­é¦–æ¬¡å¼•å…¥è¿™ç§æ–¹æ³•çš„ç ”ç©¶è€…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SARå«æ˜Ÿå›¾åƒæ•°æ®æ—¥ç›Šä¸°å¯Œï¼Œä½†é«˜è´¨é‡æ•°æ®è·å–ä»ç„¶å—é™ä¸”æˆæœ¬é«˜ã€‚</li>
<li>ç¼ºä¹å¼€æºã€æ ‡æ³¨è‰¯å¥½çš„SARæ–‡æœ¬å›¾åƒæ•°æ®é›†é™åˆ¶äº†åŸºç¡€æ¨¡å‹åœ¨é¥æ„Ÿåº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚</li>
<li>åˆ©ç”¨ONERAå­˜æ¡£çš„é£è¡Œæ•°æ®åˆ›å»ºäº†åŒ…å«å¤§é‡SARå›¾åƒçš„ç»¼åˆè®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>é‡‡ç”¨ç©ºé—´æ¡ä»¶æŠ€æœ¯çš„åŸºç¡€æ¨¡å‹èƒ½å°†å«æ˜ŸSARå›¾åƒè½¬åŒ–ä¸ºé£è¡ŒSARè¡¨ç¤ºå½¢å¼ã€‚</li>
<li>æˆåŠŸå¼¥åˆæ¨¡æ‹Ÿå›¾åƒä¸ç°å®ä¹‹é—´çš„å·®è·ï¼Œåˆ©ç”¨ONERAçš„ç‰©ç†æ¨¡æ‹Ÿå™¨EMPRISEç”Ÿæˆçš„æ¨¡æ‹Ÿå›¾åƒè¿›è¡ŒéªŒè¯ã€‚</li>
<li>è¯¥æ–¹æ³•é¦–æ¬¡åœ¨æ–‡çŒ®ä¸­å¼•å…¥ï¼Œå±•ç°äº†AIåœ¨æ¨åŠ¨SARæˆåƒæŠ€æœ¯æ–¹é¢çš„é‡è¦åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b11675b103610f3b90598a7f92e4e7a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f1dc3103008a7ac74e47d0dd1b353e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2f1f5a4c675d8bbcb90f1ff3b943dc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d7626b360820d90ff0b18a4d6db5aa3.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MagicPortrait-Temporally-Consistent-Face-Reenactment-with-3D-Geometric-Guidance"><a href="#MagicPortrait-Temporally-Consistent-Face-Reenactment-with-3D-Geometric-Guidance" class="headerlink" title="MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric   Guidance"></a>MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric   Guidance</h2><p><strong>Authors:Mengting Wei, Yante Li, Tuomas Varanka, Yan Jiang, Guoying Zhao</strong></p>
<p>In this study, we propose a method for video face reenactment that integrates a 3D face parametric model into a latent diffusion framework, aiming to improve shape consistency and motion control in existing video-based face generation approaches. Our approach employs the FLAME (Faces Learned with an Articulated Model and Expressions) model as the 3D face parametric representation, providing a unified framework for modeling face expressions and head pose. This not only enables precise extraction of motion features from driving videos, but also contributes to the faithful preservation of face shape and geometry. Specifically, we enhance the latent diffusion model with rich 3D expression and detailed pose information by incorporating depth maps, normal maps, and rendering maps derived from FLAME sequences. These maps serve as motion guidance and are encoded into the denoising UNet through a specifically designed Geometric Guidance Encoder (GGE). A multi-layer feature fusion module with integrated self-attention mechanisms is used to combine facial appearance and motion latent features within the spatial domain. By utilizing the 3D face parametric model as motion guidance, our method enables parametric alignment of face identity between the reference image and the motion captured from the driving video. Experimental results on benchmark datasets show that our method excels at generating high-quality face animations with precise expression and head pose variation modeling. In addition, it demonstrates strong generalization performance on out-of-domain images. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/weimengting/MagicPortrait">https://github.com/weimengting/MagicPortrait</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å°†3Däººè„¸å‚æ•°æ¨¡å‹é›†æˆåˆ°æ½œåœ¨æ‰©æ•£æ¡†æ¶ä¸­çš„è§†é¢‘äººè„¸å†æ¼”ç»æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ—¨åœ¨æé«˜ç°æœ‰è§†é¢‘äººè„¸ç”Ÿæˆæ–¹æ³•ä¸­çš„å½¢çŠ¶ä¸€è‡´æ€§å’Œè¿åŠ¨æ§åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨FLAMEï¼ˆç”¨å…³èŠ‚æ¨¡å‹å’Œè¡¨æƒ…å­¦ä¹ çš„é¢éƒ¨æ¨¡å‹ï¼‰ä½œä¸º3Däººè„¸å‚æ•°è¡¨ç¤ºï¼Œä¸ºé¢éƒ¨è¡¨è¾¾å’Œå¤´éƒ¨å§¿æ€å»ºæ¨¡æä¾›äº†ç»Ÿä¸€æ¡†æ¶ã€‚è¿™ä¸ä»…èƒ½å¤Ÿå®ç°ä»é©±åŠ¨è§†é¢‘ä¸­ç²¾ç¡®æå–è¿åŠ¨ç‰¹å¾ï¼Œè¿˜æœ‰åŠ©äºä¿ç•™é¢éƒ¨å½¢çŠ¶å’Œå‡ ä½•ç»“æ„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡èå…¥ä»FLAMEåºåˆ—ä¸­å¾—åˆ°çš„æ·±åº¦å›¾ã€æ³•çº¿å›¾ä»¥åŠæ¸²æŸ“å›¾ï¼Œå¢å¼ºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ä¸°å¯Œ3Dè¡¨è¾¾å’Œè¯¦ç»†å§¿æ€ä¿¡æ¯ã€‚è¿™äº›å›¾ä½œä¸ºè¿åŠ¨æŒ‡å¯¼ï¼Œé€šè¿‡ä¸“é—¨è®¾è®¡çš„å‡ ä½•æŒ‡å¯¼ç¼–ç å™¨ï¼ˆGGEï¼‰è¢«ç¼–ç åˆ°å»å™ªUNetä¸­ã€‚åˆ©ç”¨å…·æœ‰é›†æˆè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å¤šå±‚ç‰¹å¾èåˆæ¨¡å—ï¼Œåœ¨ç©ºé—´åŸŸå†…ç»„åˆé¢éƒ¨å¤–è§‚å’Œè¿åŠ¨æ½œåœ¨ç‰¹å¾ã€‚é€šè¿‡åˆ©ç”¨3Däººè„¸å‚æ•°æ¨¡å‹ä½œä¸ºè¿åŠ¨æŒ‡å¯¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°å‚è€ƒå›¾åƒä¸ä»é©±åŠ¨è§†é¢‘ä¸­æ•è·çš„è¿åŠ¨ä¹‹é—´çš„å‚æ•°å¯¹é½ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡äººè„¸åŠ¨ç”»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰ç²¾ç¡®çš„è¡¨æƒ…å’Œå¤´éƒ¨å§¿æ€å˜åŒ–å»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨åŸŸå¤–å›¾åƒä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–æ€§èƒ½ã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/weimengting/MagicPortrait%E3%80%82">https://github.com/weimengting/MagicPortraitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21497v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶å°†3Däººè„¸å‚æ•°æ¨¡å‹èå…¥æ½œåœ¨æ‰©æ•£æ¡†æ¶ï¼Œæå‡ºä¸€ç§è§†é¢‘äººè„¸å†æ¼”ç»æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ç°æœ‰è§†é¢‘äººè„¸ç”Ÿæˆæ–¹æ³•ä¸­çš„å½¢çŠ¶ä¸€è‡´æ€§å’Œè¿åŠ¨æ§åˆ¶æ•ˆæœã€‚ä½¿ç”¨FLAMEæ¨¡å‹ä½œä¸º3Däººè„¸å‚æ•°è¡¨ç¤ºï¼Œä¸ºé¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨å§¿æ€å»ºæ¨¡æä¾›ç»Ÿä¸€æ¡†æ¶ï¼Œå¯ç²¾ç¡®æå–é©±åŠ¨è§†é¢‘ä¸­çš„è¿åŠ¨ç‰¹å¾ï¼Œå¹¶å¿ å®ä¿ç•™äººè„¸å½¢çŠ¶å’Œå‡ ä½•ç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å°†3Däººè„¸å‚æ•°æ¨¡å‹èå…¥æ½œåœ¨æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨æå‡è§†é¢‘äººè„¸ç”Ÿæˆçš„è´¨é‡ã€‚</li>
<li>é‡‡ç”¨FLAMEæ¨¡å‹ä½œä¸º3Däººè„¸å‚æ•°è¡¨ç¤ºï¼Œç»Ÿä¸€å»ºæ¨¡é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨å§¿æ€ã€‚</li>
<li>é€šè¿‡æ·±åº¦å›¾ã€æ³•çº¿å›¾ã€æ¸²æŸ“å›¾ç­‰ï¼Œå¢å¼ºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„3Dè¡¨æƒ…å’Œå§¿æ€ç»†èŠ‚ã€‚</li>
<li>è®¾è®¡çš„Geometric Guidance Encoder (GGE)å°†è¿åŠ¨æŒ‡å¯¼ä¿¡æ¯ç¼–ç è¿›å»å™ªUNetã€‚</li>
<li>åˆ©ç”¨å¤šå±‚ç‰¹å¾èåˆæ¨¡å—ç»“åˆè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨ç©ºåŸŸå†…ç»“åˆé¢éƒ¨å¤–è§‚å’Œè¿åŠ¨æ½œåœ¨ç‰¹å¾ã€‚</li>
<li>ä½¿ç”¨3Däººè„¸å‚æ•°æ¨¡å‹ä½œä¸ºè¿åŠ¨æŒ‡å¯¼ï¼Œå®ç°å‚è€ƒå›¾åƒä¸é©±åŠ¨è§†é¢‘ä¸­æ•æ‰åˆ°çš„è¿åŠ¨çš„å‚æ•°å¯¹é½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½ç”Ÿæˆé«˜è´¨é‡çš„äººè„¸åŠ¨ç”»ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21497">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d5843e81a933188c00c226cc2610100.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ada25ae5bcdb7515b0f34eb4c9c21d6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9cbcbfd7099d30846fb2a50d83a285c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57395529b890dbaad177e7d9afe52ae2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DiffServe-Efficiently-Serving-Text-to-Image-Diffusion-Models-with-Query-Aware-Model-Scaling"><a href="#DiffServe-Efficiently-Serving-Text-to-Image-Diffusion-Models-with-Query-Aware-Model-Scaling" class="headerlink" title="DiffServe: Efficiently Serving Text-to-Image Diffusion Models with   Query-Aware Model Scaling"></a>DiffServe: Efficiently Serving Text-to-Image Diffusion Models with   Query-Aware Model Scaling</h2><p><strong>Authors:Sohaib Ahmad, Qizheng Yang, Haoliang Wang, Ramesh K. Sitaraman, Hui Guan</strong></p>
<p>Text-to-image generation using diffusion models has gained increasing popularity due to their ability to produce high-quality, realistic images based on text prompts. However, efficiently serving these models is challenging due to their computation-intensive nature and the variation in query demands. In this paper, we aim to address both problems simultaneously through query-aware model scaling. The core idea is to construct model cascades so that easy queries can be processed by more lightweight diffusion models without compromising image generation quality. Based on this concept, we develop an end-to-end text-to-image diffusion model serving system, DiffServe, which automatically constructs model cascades from available diffusion model variants and allocates resources dynamically in response to demand fluctuations. Our empirical evaluations demonstrate that DiffServe achieves up to 24% improvement in response quality while maintaining 19-70% lower latency violation rates compared to state-of-the-art model serving systems. </p>
<blockquote>
<p>ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå› èƒ½æ ¹æ®æ–‡æœ¬æç¤ºäº§ç”Ÿé«˜è´¨é‡ã€é€¼çœŸçš„å›¾åƒè€Œè¶Šæ¥è¶Šå—æ¬¢è¿ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›æ¨¡å‹çš„è®¡ç®—å¯†é›†å‹å’ŒæŸ¥è¯¢éœ€æ±‚çš„å¤šæ ·æ€§ï¼Œæœ‰æ•ˆåœ°æä¾›æœåŠ¡æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡æŸ¥è¯¢æ„ŸçŸ¥æ¨¡å‹ç¼©æ”¾åŒæ—¶è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯æ„å»ºæ¨¡å‹çº§è”ï¼Œä½¿ç®€å•æŸ¥è¯¢å¯ä»¥ç”±æ›´è½»é‡çº§çš„æ‰©æ•£æ¨¡å‹å¤„ç†ï¼Œè€Œä¸ä¼šæŸå®³å›¾åƒç”Ÿæˆè´¨é‡ã€‚åŸºäºæ­¤æ¦‚å¿µï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æœåŠ¡ç³»ç»ŸDiffServeï¼Œå®ƒè‡ªåŠ¨æ ¹æ®å¯ç”¨çš„æ‰©æ•£æ¨¡å‹å˜ä½“æ„å»ºæ¨¡å‹çº§è”ï¼Œå¹¶åŠ¨æ€åˆ†é…èµ„æºä»¥åº”å¯¹éœ€æ±‚æ³¢åŠ¨ã€‚æˆ‘ä»¬çš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€æ–°çš„æ¨¡å‹æœåŠ¡ç³»ç»Ÿç›¸æ¯”ï¼ŒDiffServeåœ¨ä¿æŒ19-70%è¾ƒä½å»¶è¿Ÿè¿è§„ç‡çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾24%çš„å“åº”è´¨é‡æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15381v2">PDF</a> 16 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œèƒ½ç”Ÿæˆé«˜è´¨é‡ã€é€¼çœŸçš„å›¾åƒã€‚ç„¶è€Œï¼Œé«˜æ•ˆæœåŠ¡è¿™äº›æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬è®¡ç®—å¯†é›†ä¸”æŸ¥è¯¢éœ€æ±‚å¤šå˜ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡æŸ¥è¯¢æ„ŸçŸ¥æ¨¡å‹ç¼©æ”¾åŒæ—¶è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæ„å»ºæ¨¡å‹çº§è”ï¼Œä½¿ç®€å•æŸ¥è¯¢å¯ä»¥ç”±æ›´è½»é‡çº§çš„æ‰©æ•£æ¨¡å‹å¤„ç†ï¼Œè€Œä¸å½±å“å›¾åƒç”Ÿæˆè´¨é‡ã€‚åŸºäºè¿™ä¸€ç†å¿µï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æœåŠ¡ç³»ç»ŸDiffServeï¼Œå®ƒè‡ªåŠ¨æ„å»ºæ¨¡å‹çº§è”ï¼Œå¹¶æ ¹æ®éœ€æ±‚æ³¢åŠ¨åŠ¨æ€åˆ†é…èµ„æºã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒDiffServeåœ¨æé«˜å“åº”è´¨é‡çš„åŒæ—¶ï¼Œå°†å»¶è¿Ÿè¿è§„ç‡é™ä½äº†19-70%ï¼Œè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€é€¼çœŸçš„æ–‡æœ¬è½¬å›¾åƒã€‚</li>
<li>é«˜æ•ˆæœåŠ¡æ‰©æ•£æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬è®¡ç®—å¯†é›†ä¸”æŸ¥è¯¢éœ€æ±‚å¤šå˜ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡æŸ¥è¯¢æ„ŸçŸ¥æ¨¡å‹ç¼©æ”¾è§£å†³è®¡ç®—å¯†é›†å’ŒæŸ¥è¯¢éœ€æ±‚å¤šå˜çš„é—®é¢˜ã€‚</li>
<li>æ„å»ºæ¨¡å‹çº§è”å¯ä»¥å¤„ç†ç®€å•æŸ¥è¯¢ï¼ŒåŒæ—¶ä¿æŒå›¾åƒç”Ÿæˆè´¨é‡ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æœåŠ¡ç³»ç»ŸDiffServeã€‚</li>
<li>DiffServeè‡ªåŠ¨æ„å»ºæ¨¡å‹çº§è”å¹¶æ ¹æ®éœ€æ±‚æ³¢åŠ¨åŠ¨æ€åˆ†é…èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15381">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20f284c4d7c78674a28506dca66ee396.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c23b556ecac2c7bd5c424e3004515f83.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ca686eb61c4ed015ef4cd170deff800.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c344d69b383b308fbab965080719b04.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance"><a href="#Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance" class="headerlink" title="Enhanced Generative Data Augmentation for Semantic Segmentation via   Stronger Guidance"></a>Enhanced Generative Data Augmentation for Semantic Segmentation via   Stronger Guidance</h2><p><strong>Authors:Quang-Huy Che, Duc-Tri Le, Bich-Nga Pham, Duc-Khai Lam, Vinh-Tiep Nguyen</strong></p>
<p>Data augmentation is crucial for pixel-wise annotation tasks like semantic segmentation, where labeling requires significant effort and intensive labor. Traditional methods, involving simple transformations such as rotations and flips, create new images but often lack diversity along key semantic dimensions and fail to alter high-level semantic properties. To address this issue, generative models have emerged as an effective solution for augmenting data by generating synthetic images. Controllable Generative models offer data augmentation methods for semantic segmentation tasks by using prompts and visual references from the original image. However, these models face challenges in generating synthetic images that accurately reflect the content and structure of the original image due to difficulties in creating effective prompts and visual references. In this work, we introduce an effective data augmentation pipeline for semantic segmentation using Controllable Diffusion model. Our proposed method includes efficient prompt generation using \textit{Class-Prompt Appending} and \textit{Visual Prior Blending} to enhance attention to labeled classes in real images, allowing the pipeline to generate a precise number of augmented images while preserving the structure of segmentation-labeled classes. In addition, we implement a \textit{class balancing algorithm} to ensure a balanced training dataset when merging the synthetic and original images. Evaluation on PASCAL VOC datasets, our pipeline demonstrates its effectiveness in generating high-quality synthetic images for semantic segmentation. Our code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance%7D%7Bthis">https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance}{this</a> https URL}. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºå¯¹äºåƒç´ çº§æ ‡æ³¨ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²ï¼‰è‡³å…³é‡è¦ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦æŠ•å…¥å¤§é‡ç²¾åŠ›å’ŒåŠ³åŠ›è¿›è¡Œæ ‡æ³¨ã€‚ä¼ ç»Ÿçš„æ–¹æ³•è™½ç„¶å¯ä»¥é€šè¿‡æ—‹è½¬ã€ç¿»è½¬ç­‰ç®€å•å˜æ¢æ¥åˆ›å»ºæ–°å›¾åƒï¼Œä½†å®ƒä»¬å¾€å¾€åœ¨å…³é”®çš„è¯­ä¹‰ç»´åº¦ä¸Šç¼ºä¹å¤šæ ·æ€§ï¼Œå¹¶ä¸”æ— æ³•æ”¹å˜é«˜çº§è¯­ä¹‰å±æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç”Ÿæˆæ¨¡å‹ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æ•°æ®å¢å¼ºè§£å†³æ–¹æ¡ˆå·²ç»å‡ºç°ï¼Œé€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæ¥å¢å¼ºæ•°æ®ã€‚å¯æ§ç”Ÿæˆæ¨¡å‹é€šè¿‡ä½¿ç”¨åŸå§‹å›¾åƒçš„æç¤ºå’Œè§†è§‰å‚è€ƒæ¥ä¸ºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡æä¾›æ•°æ®å¢å¼ºæ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆå‡†ç¡®åæ˜ åŸå§‹å›¾åƒå†…å®¹å’Œç»“æ„çš„åˆæˆå›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºåˆ›å»ºæœ‰æ•ˆçš„æç¤ºå’Œè§†è§‰å‚è€ƒå…·æœ‰éš¾åº¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä½¿ç”¨å¯æ§æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆæ•°æ®å¢å¼ºç®¡é“ï¼Œç”¨äºè¯­ä¹‰åˆ†å‰²ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨â€œç±»æç¤ºé™„åŠ â€å’Œâ€œè§†è§‰å…ˆéªŒæ··åˆâ€è¿›è¡Œé«˜æ•ˆæç¤ºç”Ÿæˆï¼Œä»¥æé«˜å¯¹çœŸå®å›¾åƒä¸­æ ‡è®°ç±»çš„æ³¨æ„åŠ›ï¼Œä½¿ç®¡é“èƒ½å¤Ÿåœ¨ä¿ç•™åˆ†å‰²æ ‡è®°ç±»ç»“æ„çš„åŒæ—¶ç”Ÿæˆç²¾ç¡®æ•°é‡çš„å¢å¼ºå›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ç§â€œç±»åˆ«å¹³è¡¡ç®—æ³•â€ï¼Œä»¥ç¡®ä¿åœ¨åˆå¹¶åˆæˆå›¾åƒå’ŒåŸå§‹å›¾åƒæ—¶è·å¾—å¹³è¡¡çš„è®­ç»ƒæ•°æ®é›†ã€‚åœ¨PASCAL VOCæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®¡é“åœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆå›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²æ–¹é¢éå¸¸æœ‰æ•ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance">æ­¤https URL</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06002v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨å¯æ§æ‰©æ•£æ¨¡å‹è¿›è¡Œè¯­ä¹‰åˆ†å‰²çš„æœ‰æ•ˆæ•°æ®å¢å¼ºç®¡é“ã€‚é€šè¿‡é‡‡ç”¨ç±»æç¤ºé™„åŠ å’Œè§†è§‰å…ˆéªŒèåˆç­‰æ–¹æ³•ï¼Œè¯¥ç®¡é“èƒ½å¤Ÿåœ¨ç”Ÿæˆå¢å¼ºå›¾åƒæ—¶é‡ç‚¹å…³æ³¨çœŸå®å›¾åƒä¸­çš„æ ‡è®°ç±»ï¼ŒåŒæ—¶ä¿æŒåˆ†å‰²æ ‡è®°ç±»çš„ç»“æ„ã€‚æ­¤å¤–ï¼Œè¿˜å®ç°äº†ä¸€ç§ç±»å¹³è¡¡ç®—æ³•ï¼Œä»¥ç¡®ä¿åœ¨åˆå¹¶åˆæˆå›¾åƒå’ŒåŸå§‹å›¾åƒæ—¶è®­ç»ƒæ•°æ®é›†å¹³è¡¡ã€‚åœ¨PASCAL VOCæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†è¯¥ç®¡é“åœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆå›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®å¢å¼ºåœ¨åƒç´ çº§æ³¨é‡Šä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²ï¼‰ä¸­éå¸¸é‡è¦ï¼Œå› ä¸ºæ ‡æ³¨éœ€è¦å¤§é‡åŠ³åŠ¨ã€‚</li>
<li>ä¼ ç»Ÿçš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼ˆå¦‚æ—‹è½¬å’Œç¿»è½¬ï¼‰ç¼ºä¹è¯­ä¹‰ç»´åº¦ä¸Šçš„å¤šæ ·æ€§ï¼Œæ— æ³•æ”¹å˜é«˜çº§è¯­ä¹‰å±æ€§ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹æ˜¯è§£å†³æ•°æ®å¢å¼ºé—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•ï¼Œå¯ä»¥é€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæ¥å¢åŠ æ•°æ®å¤šæ ·æ€§ã€‚</li>
<li>å¯æ§ç”Ÿæˆæ¨¡å‹ä½¿ç”¨æç¤ºå’ŒåŸå§‹å›¾åƒçš„è§†è§‰å‚è€ƒï¼Œä¸ºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡æä¾›æ•°æ®å¢å¼ºæ–¹æ³•ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºç®¡é“ï¼ŒåŒ…æ‹¬é«˜æ•ˆçš„æç¤ºç”Ÿæˆå’Œè§†è§‰å…ˆéªŒèåˆï¼Œä»¥åœ¨çœŸå®å›¾åƒä¸­å¢å¼ºå¯¹æ ‡è®°ç±»çš„å…³æ³¨ã€‚</li>
<li>è¯¥ç®¡é“å®æ–½äº†ä¸€ç§ç±»å¹³è¡¡ç®—æ³•ï¼Œä»¥ç¡®ä¿åœ¨åˆå¹¶åˆæˆå’ŒåŸå§‹å›¾åƒæ—¶è®­ç»ƒæ•°æ®é›†çš„å¹³è¡¡ã€‚</li>
<li>åœ¨PASCAL VOCæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†è¯¥ç®¡é“ç”Ÿæˆé«˜è´¨é‡åˆæˆå›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c1716bef1d0d6ad24dfb909fb50fe68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f65bfd9b2b7e788b5f54c9643a7800c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91dd43e3d48806df3e2fa23090a86993.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b3c65fb7936a29984cc349b14779630.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b79cbb1b35476816ef186a87a8ee7de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c298870b43ce121bb270ff9dab12e16e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Speed-accuracy-relations-for-diffusion-models-Wisdom-from-nonequilibrium-thermodynamics-and-optimal-transport"><a href="#Speed-accuracy-relations-for-diffusion-models-Wisdom-from-nonequilibrium-thermodynamics-and-optimal-transport" class="headerlink" title="Speed-accuracy relations for diffusion models: Wisdom from   nonequilibrium thermodynamics and optimal transport"></a>Speed-accuracy relations for diffusion models: Wisdom from   nonequilibrium thermodynamics and optimal transport</h2><p><strong>Authors:Kotaro Ikeda, Tomoya Uda, Daisuke Okanohara, Sosuke Ito</strong></p>
<p>We discuss a connection between a generative model, called the diffusion model, and nonequilibrium thermodynamics for the Fokker-Planck equation, called stochastic thermodynamics. Using techniques from stochastic thermodynamics, we derive the speed-accuracy relations for diffusion models, which are inequalities that relate the accuracy of data generation to the entropy production rate. This relation can be interpreted as the speed of the diffusion dynamics in the absence of the non-conservative force. From a stochastic thermodynamic perspective, our results provide quantitative insight into how best to generate data in diffusion models. The optimal learning protocol is introduced by the geodesic of space of the 2-Wasserstein distance in optimal transport theory. We numerically illustrate the validity of the speed-accuracy relations for diffusion models with different noise schedules and different data. We numerically discuss our results for optimal and suboptimal learning protocols. We also demonstrate the applicability of our results to data generation from the real-world image datasets. </p>
<blockquote>
<p>æˆ‘ä»¬æ¢è®¨äº†ä¸€ç§ç”Ÿæˆæ¨¡å‹â€”â€”æ‰©æ•£æ¨¡å‹ä¸éå¹³è¡¡æ€çƒ­åŠ›å­¦ä¹‹é—´çš„å…³è”ï¼Œå¯¹äºç¦å…‹-æ™®æœ—å…‹æ–¹ç¨‹ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºéšæœºçƒ­åŠ›å­¦ã€‚åˆ©ç”¨éšæœºçƒ­åŠ›å­¦çš„æŠ€æœ¯ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºæ‰©æ•£æ¨¡å‹çš„é€Ÿåº¦-ç²¾åº¦å…³ç³»ï¼Œè¿™äº›å…³ç³»æ˜¯ä¸ç­‰å¼ï¼Œæè¿°äº†æ•°æ®ç”Ÿæˆçš„ç²¾åº¦ä¸ç†µäº§ç”Ÿç‡ä¹‹é—´çš„è”ç³»ã€‚è¿™ç§å…³ç³»å¯ä»¥è¢«è§£é‡Šä¸ºåœ¨æ²¡æœ‰éä¿å®ˆåŠ›çš„æƒ…å†µä¸‹æ‰©æ•£åŠ¨åŠ›å­¦çš„é€Ÿåº¦ã€‚ä»éšæœºçƒ­åŠ›å­¦çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬çš„ç»“æœæä¾›äº†åœ¨æ‰©æ•£æ¨¡å‹ä¸­å¦‚ä½•æœ€ä½³ç”Ÿæˆæ•°æ®çš„å®šé‡è§è§£ã€‚æœ€ä½³å­¦ä¹ åè®®æ˜¯é€šè¿‡æœ€ä¼˜ä¼ è¾“ç†è®ºä¸­çš„2-Wassersteinè·ç¦»çš„ç©ºé—´æµ‹åœ°çº¿å¼•å…¥çš„ã€‚æˆ‘ä»¬é€šè¿‡æ•°å€¼æ–¹æ³•è¯´æ˜äº†ä¸åŒå™ªå£°è®¡åˆ’å’Œä¸åŒæ•°æ®çš„æ‰©æ•£æ¨¡å‹çš„é€Ÿåº¦-ç²¾åº¦å…³ç³»çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æ•°å€¼è®¨è®ºäº†æˆ‘ä»¬é’ˆå¯¹æœ€ä½³å’Œæ¬¡ä¼˜å­¦ä¹ åè®®çš„ç»“æœã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†æˆ‘ä»¬çš„ç»“æœåœ¨ç°å®å›¾åƒæ•°æ®é›†çš„æ•°æ®ç”Ÿæˆä¸­çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.04495v5">PDF</a> 37 pages, 9 figures</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹ä¸Fokker-Planckæ–¹ç¨‹çš„éšæœºçƒ­åŠ›å­¦ä¹‹é—´çš„è”ç³»ã€‚é€šè¿‡éšæœºçƒ­åŠ›å­¦æŠ€æœ¯ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºæ‰©æ•£æ¨¡å‹çš„é€Ÿåº¦-ç²¾åº¦å…³ç³»ï¼Œè¿™äº›å…³ç³»æ˜¯ä¸ç­‰å¼ï¼Œå°†æ•°æ®ç”Ÿæˆçš„ç²¾åº¦ä¸ç†µäº§ç”Ÿç‡ç›¸å…³è”ã€‚ç»“æœä»éšæœºçƒ­åŠ›å­¦çš„è§’åº¦ä¸ºå¦‚ä½•åœ¨æ‰©æ•£æ¨¡å‹ä¸­æœ€ä½³ç”Ÿæˆæ•°æ®æä¾›äº†å®šé‡è§è§£ã€‚å¼•å…¥æœ€ä¼˜å­¦ä¹ åè®®ï¼Œé€šè¿‡æœ€ä¼˜ä¼ è¾“ç†è®ºä¸­çš„2-Wassersteinè·ç¦»çš„ç©ºé—´æµ‹åœ°çº¿å®ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¸Fokker-Planckæ–¹ç¨‹çš„éšæœºçƒ­åŠ›å­¦å­˜åœ¨è”ç³»ã€‚</li>
<li>é€šè¿‡éšæœºçƒ­åŠ›å­¦æŠ€æœ¯ï¼Œæ¨å¯¼å‡ºæ‰©æ•£æ¨¡å‹çš„é€Ÿåº¦-ç²¾åº¦å…³ç³»ã€‚</li>
<li>é€Ÿåº¦-ç²¾åº¦å…³ç³»å¯ä»¥è§£é‡Šä¸ºéä¿å®ˆåŠ›ä¸å­˜åœ¨æƒ…å†µä¸‹æ‰©æ•£åŠ¨åŠ›å­¦çš„é€Ÿåº¦ã€‚</li>
<li>ç»“æœä»éšæœºçƒ­åŠ›å­¦è§’åº¦æä¾›äº†åœ¨æ‰©æ•£æ¨¡å‹ä¸­æœ€ä½³ç”Ÿæˆæ•°æ®çš„å®šé‡è§è§£ã€‚</li>
<li>å¼•å…¥æœ€ä¼˜å­¦ä¹ åè®®ï¼Œè¯¥åè®®é€šè¿‡æœ€ä¼˜ä¼ è¾“ç†è®ºä¸­çš„2-Wassersteinè·ç¦»çš„æµ‹åœ°çº¿å®ç°ã€‚</li>
<li>å¯¹ä¸åŒå™ªå£°å®‰æ’å’Œæ•°æ®é›†çš„æ‰©æ•£æ¨¡å‹çš„é€Ÿåº¦-ç²¾åº¦å…³ç³»è¿›è¡Œäº†æ•°å€¼éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.04495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b3dd02d1d30dcf0cf635dcd827071ce9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c342e62c2bbe11193a5d72fc18718b5b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7cb8718af0c9ab9daa19cf13df17781b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DEFT-Efficient-Fine-Tuning-of-Diffusion-Models-by-Learning-the-Generalised-h-transform"><a href="#DEFT-Efficient-Fine-Tuning-of-Diffusion-Models-by-Learning-the-Generalised-h-transform" class="headerlink" title="DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the   Generalised $h$-transform"></a>DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the   Generalised $h$-transform</h2><p><strong>Authors:Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano, Emile Mathieu, Urszula Julia Komorowska, Pietro Lio</strong></p>
<p>Generative modelling paradigms based on denoising diffusion processes have emerged as a leading candidate for conditional sampling in inverse problems. In many real-world applications, we often have access to large, expensively trained unconditional diffusion models, which we aim to exploit for improving conditional sampling. Most recent approaches are motivated heuristically and lack a unifying framework, obscuring connections between them. Further, they often suffer from issues such as being very sensitive to hyperparameters, being expensive to train or needing access to weights hidden behind a closed API. In this work, we unify conditional training and sampling using the mathematically well-understood Doobâ€™s h-transform. This new perspective allows us to unify many existing methods under a common umbrella. Under this framework, we propose DEFT (Doobâ€™s h-transform Efficient FineTuning), a new approach for conditional generation that simply fine-tunes a very small network to quickly learn the conditional $h$-transform, while keeping the larger unconditional network unchanged. DEFT is much faster than existing baselines while achieving state-of-the-art performance across a variety of linear and non-linear benchmarks. On image reconstruction tasks, we achieve speedups of up to 1.6$\times$, while having the best perceptual quality on natural images and reconstruction performance on medical images. Further, we also provide initial experiments on protein motif scaffolding and outperform reconstruction guidance methods. </p>
<blockquote>
<p>åŸºäºå»å™ªæ‰©æ•£è¿‡ç¨‹çš„ç”Ÿæˆå»ºæ¨¡èŒƒå¼å·²æˆä¸ºé€†é—®é¢˜ä¸­æ¡ä»¶é‡‡æ ·çš„é¢†å…ˆå€™é€‰æ–¹æ³•ã€‚åœ¨è®¸å¤šçœŸå®ä¸–ç•Œåº”ç”¨ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å¯ä»¥è®¿é—®å¤§å‹ä¸”ç»è¿‡æ˜‚è´µè®­ç»ƒçš„æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬æ—¨åœ¨åˆ©ç”¨è¿™äº›æ¨¡å‹æ¥æ”¹å–„æ¡ä»¶é‡‡æ ·ã€‚æœ€è¿‘çš„æ–¹æ³•å¤§å¤šæ˜¯å¯å‘å¼ä¸”ç¼ºä¹ç»Ÿä¸€æ¡†æ¶ï¼Œå¯¼è‡´å®ƒä»¬ä¹‹é—´çš„è”ç³»æ¨¡ç³Šä¸æ¸…ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ç»å¸¸é¢ä¸´è¯¸å¦‚å¯¹è¶…å‚æ•°éå¸¸æ•æ„Ÿã€è®­ç»ƒæˆæœ¬é«˜æ˜‚æˆ–éœ€è¦è®¿é—®å°é—­APIä¸­çš„æƒé‡ç­‰é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ•°å­¦ä¸Šæ˜“äºç†è§£çš„Doobçš„hè½¬æ¢æ¥ç»Ÿä¸€æ¡ä»¶è®­ç»ƒå’Œé‡‡æ ·ã€‚è¿™ä¸ªæ–°è§†è§’ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å…±åŒçš„åŸºç¡€ä¸Šç»Ÿä¸€è®¸å¤šç°æœ‰æ–¹æ³•ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†DEFTï¼ˆDoobçš„hè½¬æ¢é«˜æ•ˆå¾®è°ƒï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¡ä»¶ç”Ÿæˆæ–¹æ³•ï¼Œåªéœ€å¾®è°ƒä¸€ä¸ªéå¸¸å°çš„ç½‘ç»œå³å¯å¿«é€Ÿå­¦ä¹ æ¡ä»¶hè½¬æ¢ï¼ŒåŒæ—¶ä¿æŒè¾ƒå¤§çš„æ— æ¡ä»¶ç½‘ç»œä¸å˜ã€‚DEFTç›¸è¾ƒäºç°æœ‰åŸºå‡†æµ‹è¯•ï¼Œé€Ÿåº¦æ›´å¿«ï¼ŒåŒæ—¶åœ¨å„ç§çº¿æ€§å’Œéçº¿æ€§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬å®ç°äº†é«˜è¾¾1.6å€çš„åŠ é€Ÿï¼ŒåŒæ—¶åœ¨è‡ªç„¶å›¾åƒä¸Šæ‹¥æœ‰æœ€ä½³çš„æ„ŸçŸ¥è´¨é‡å’ŒåŒ»ç–—å›¾åƒçš„é‡å»ºæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹è›‹ç™½è´¨åŸºåºæ”¯æ¶è¿›è¡Œäº†åˆæ­¥å®éªŒï¼Œå¹¶è¶…è¶Šäº†é‡å»ºæŒ‡å¯¼æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01781v4">PDF</a> arXiv admin note: text overlap with arXiv:2312.09236</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå»å™ªæ‰©æ•£è¿‡ç¨‹ç”Ÿæˆå»ºæ¨¡èŒƒå¼å·²æˆä¸ºè§£å†³é€†å‘é—®é¢˜ä¸­æ¡ä»¶é‡‡æ ·çš„é¢†å…ˆå€™é€‰æ–¹æ³•ã€‚æœ¬æ–‡åˆ©ç”¨æ•°å­¦ä¸Šç†è§£è‰¯å¥½çš„Doobçš„h-å˜æ¢æ¥ç»Ÿä¸€æ¡ä»¶è®­ç»ƒå’Œé‡‡æ ·ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºDEFTï¼ˆDoobçš„hå˜æ¢é«˜æ•ˆå¾®è°ƒï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¡ä»¶ç”Ÿæˆæ–¹æ³•ï¼Œåªéœ€å¾®è°ƒä¸€ä¸ªéå¸¸å°çš„ç½‘ç»œå³å¯å¿«é€Ÿå­¦ä¹ æ¡ä»¶hå˜æ¢ï¼ŒåŒæ—¶ä¿æŒæ›´å¤§çš„æ— æ¡ä»¶ç½‘ç»œä¸å˜ã€‚DEFTç›¸æ¯”ç°æœ‰åŸºçº¿æ–¹æ³•æ›´å¿«ï¼ŒåŒæ—¶åœ¨å„ç§çº¿æ€§å’Œéçº¿æ€§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬å®ç°äº†æœ€é«˜è¾¾1.6å€çš„åŠ é€Ÿï¼ŒåŒæ—¶åœ¨è‡ªç„¶å›¾åƒä¸Šå…·æœ‰æœ€ä½³çš„æ„ŸçŸ¥è´¨é‡å’ŒåŒ»ç–—å›¾åƒçš„é‡å»ºæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†è›‹ç™½è´¨åŸºåºæ”¯æ¶çš„åˆæ­¥å®éªŒï¼Œå¹¶è¶…è¶Šäº†é‡å»ºæŒ‡å¯¼æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ¡ä»¶é‡‡æ ·é¢†åŸŸå…·æœ‰é¢†å…ˆåœ°ä½ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³é€†å‘é—®é¢˜ä¸­ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç¼ºä¹ç»Ÿä¸€æ¡†æ¶ï¼Œæœ¬æ–‡åˆ©ç”¨Doobçš„h-å˜æ¢æ¥ç»Ÿä¸€æ¡ä»¶è®­ç»ƒå’Œé‡‡æ ·ã€‚</li>
<li>æå‡ºDEFTæ–¹æ³•ï¼Œé€šè¿‡å¾®è°ƒå°ç½‘ç»œå¿«é€Ÿå­¦ä¹ æ¡ä»¶hå˜æ¢ï¼Œä¿æŒå¤§ç½‘ç»œä¸å˜ã€‚</li>
<li>DEFTåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå›¾åƒé‡å»ºä»»åŠ¡åŠ é€Ÿæ•ˆæœæ˜¾è‘—ã€‚</li>
<li>DEFTåœ¨è‡ªç„¶å›¾åƒå’ŒåŒ»ç–—å›¾åƒä¸Šåˆ†åˆ«è¡¨ç°å‡ºæœ€ä½³æ„ŸçŸ¥è´¨é‡å’Œé‡å»ºæ€§èƒ½ã€‚</li>
<li>åˆæ­¥å®éªŒæ˜¾ç¤ºï¼ŒDEFTåœ¨è›‹ç™½è´¨åŸºåºæ”¯æ¶æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.01781">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a35a35ae04ee635d3ed7ae7f09c5167e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-089cd1947b9623c22b4d8ad4cb1c4339.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-879cd050456c3a253169968c7b573330.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  Anatomical Attention Alignment representation for Radiology Report   Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e31676b64bce418610fd0cae36d7bb02.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  TUM2TWIN Introducing the Large-Scale Multimodal Urban Digital Twin   Benchmark Dataset
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25879.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
