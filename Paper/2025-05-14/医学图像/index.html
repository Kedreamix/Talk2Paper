<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  Anatomical Attention Alignment representation for Radiology Report   Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-879cd050456c3a253169968c7b573330.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-14-æ›´æ–°"><a href="#2025-05-14-æ›´æ–°" class="headerlink" title="2025-05-14 æ›´æ–°"></a>2025-05-14 æ›´æ–°</h1><h2 id="Anatomical-Attention-Alignment-representation-for-Radiology-Report-Generation"><a href="#Anatomical-Attention-Alignment-representation-for-Radiology-Report-Generation" class="headerlink" title="Anatomical Attention Alignment representation for Radiology Report   Generation"></a>Anatomical Attention Alignment representation for Radiology Report   Generation</h2><p><strong>Authors:Quang Vinh Nguyen, Minh Duc Nguyen, Thanh Hoang Son Vo, Hyung-Jeong Yang, Soo-Hyung Kim</strong></p>
<p>Automated Radiology report generation (RRG) aims at producing detailed descriptions of medical images, reducing radiologistsâ€™ workload and improving access to high-quality diagnostic services. Existing encoder-decoder models only rely on visual features extracted from raw input images, which can limit the understanding of spatial structures and semantic relationships, often resulting in suboptimal text generation. To address this, we propose Anatomical Attention Alignment Network (A3Net), a framework that enhance visual-textual understanding by constructing hyper-visual representations. Our approach integrates a knowledge dictionary of anatomical structures with patch-level visual features, enabling the model to effectively associate image regions with their corresponding anatomical entities. This structured representation improves semantic reasoning, interpretability, and cross-modal alignment, ultimately enhancing the accuracy and clinical relevance of generated reports. Experimental results on IU X-Ray and MIMIC-CXR datasets demonstrate that A3Net significantly improves both visual perception and text generation quality. Our code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/Vinh-AI/A3Net%7D%7BGitHub%7D">https://github.com/Vinh-AI/A3Net}{GitHub}</a>. </p>
<blockquote>
<p>è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ—¨åœ¨ç”ŸæˆåŒ»å­¦å›¾åƒçš„è¯¦ç»†æè¿°ï¼Œå‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ï¼Œæ”¹å–„é«˜è´¨é‡è¯Šæ–­æœåŠ¡çš„è·å–é€”å¾„ã€‚ç°æœ‰çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ä»…ä¾èµ–äºä»åŸå§‹è¾“å…¥å›¾åƒä¸­æå–çš„è§†è§‰ç‰¹å¾ï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶å¯¹ç©ºé—´ç»“æ„å’Œè¯­ä¹‰å…³ç³»çš„ç†è§£ï¼Œé€šå¸¸å¯¼è‡´æ–‡æœ¬ç”Ÿæˆæ•ˆæœä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè§£å‰–æ³¨æ„åŠ›å¯¹é½ç½‘ç»œï¼ˆA3Netï¼‰â€ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡æ„å»ºè¶…è§†è§‰è¡¨å¾å¢å¼ºè§†è§‰æ–‡æœ¬ç†è§£çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ•´åˆäº†è§£å‰–ç»“æ„çš„çŸ¥è¯†è¯å…¸å’Œè¡¥ä¸çº§åˆ«çš„è§†è§‰ç‰¹å¾ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†å›¾åƒåŒºåŸŸä¸ç›¸åº”çš„è§£å‰–å®ä½“ç›¸å…³è”ã€‚è¿™ç§ç»“æ„åŒ–è¡¨ç¤ºæé«˜äº†è¯­ä¹‰æ¨ç†ã€è§£é‡Šæ€§å’Œè·¨æ¨¡æ€å¯¹é½ï¼Œæœ€ç»ˆæé«˜äº†ç”ŸæˆæŠ¥å‘Šå‡†ç¡®æ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚åœ¨IU Xå…‰ç‰‡å’ŒMIMIC-CXRæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒA3Netæ˜¾è‘—æé«˜äº†è§†è§‰æ„ŸçŸ¥å’Œæ–‡æœ¬ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨GitHubä¸Šè·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/Vinh-AI/A3Net%E3%80%82">https://github.com/Vinh-AI/A3Netã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07689v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒæŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆï¼ˆRRGï¼‰æ—¨åœ¨å‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡å¹¶æ”¹å–„é«˜è´¨é‡è¯Šæ–­æœåŠ¡çš„è·å–ã€‚ç°æœ‰æ¨¡å‹ä¸»è¦ä¾èµ–ä»åŸå§‹å›¾åƒä¸­æå–çš„è§†è§‰ç‰¹å¾ï¼Œè¿™é™åˆ¶äº†ç©ºé—´ç»“æ„å’Œè¯­ä¹‰å…³ç³»çš„ç†è§£ï¼Œå¯¼è‡´æ–‡æœ¬ç”Ÿæˆæ•ˆæœå¸¸ä¸ç†æƒ³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Anatomical Attention Alignment Networkï¼ˆA3Netï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºè¶…è§†è§‰è¡¨å¾å¢å¼ºè§†è§‰ä¸æ–‡æœ¬çš„ç†è§£ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç»“æ„çŸ¥è¯†å­—å…¸ä¸è¡¥ä¸çº§åˆ«çš„è§†è§‰ç‰¹å¾ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†å›¾åƒåŒºåŸŸä¸ç›¸åº”çš„è§£å‰–å®ä½“ç›¸å…³è”ã€‚è¿™ç§ç»“æ„åŒ–è¡¨ç¤ºæé«˜äº†è¯­ä¹‰æ¨ç†ã€å¯è§£é‡Šæ€§å’Œè·¨æ¨¡æ€å¯¹é½ï¼Œæœ€ç»ˆæé«˜äº†æŠ¥å‘Šç”Ÿæˆçš„å‡†ç¡®æ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚åœ¨IU Xå…‰ç‰‡å’ŒMIMIC-CXRæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒA3Netæ˜¾è‘—æé«˜äº†è§†è§‰æ„ŸçŸ¥å’Œæ–‡æœ¬ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆï¼ˆRRGï¼‰æœ‰åŠ©äºå‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œè´Ÿæ‹…ï¼Œæé«˜é«˜è´¨é‡è¯Šæ–­æœåŠ¡çš„å¯åŠæ€§ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ä¸»è¦ä¾èµ–è§†è§‰ç‰¹å¾ï¼Œé™åˆ¶äº†ç©ºé—´ç»“æ„å’Œè¯­ä¹‰å…³ç³»çš„ç†è§£ã€‚</li>
<li>A3Netæ¡†æ¶é€šè¿‡æ„å»ºè¶…è§†è§‰è¡¨å¾å¢å¼ºè§†è§‰ä¸æ–‡æœ¬çš„ç†è§£ã€‚</li>
<li>A3Netç»“åˆç»“æ„çŸ¥è¯†å­—å…¸ä¸è¡¥ä¸çº§åˆ«çš„è§†è§‰ç‰¹å¾ï¼Œæé«˜æ¨¡å‹å¯¹å›¾åƒåŒºåŸŸä¸è§£å‰–å®ä½“çš„å…³è”èƒ½åŠ›ã€‚</li>
<li>ç»“æ„åŒ–è¡¨ç¤ºæé«˜äº†è¯­ä¹‰æ¨ç†ã€å¯è§£é‡Šæ€§å’Œè·¨æ¨¡æ€å¯¹é½ã€‚</li>
<li>A3Netæ˜¾è‘—æé«˜äº†æŠ¥å‘Šç”Ÿæˆçš„å‡†ç¡®æ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07689">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ffd95d24e346b511c22061ba616eed7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17911107e68e326ec08166edf4d4d6d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de066f57639cf2234e1b430c349253db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c25308b9d9d2d693c1698df91a32cdd7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50d665de8057ef66f22922d15fc22854.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ABS-Mamba-SAM2-Driven-Bidirectional-Spiral-Mamba-Network-for-Medical-Image-Translation"><a href="#ABS-Mamba-SAM2-Driven-Bidirectional-Spiral-Mamba-Network-for-Medical-Image-Translation" class="headerlink" title="ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical   Image Translation"></a>ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical   Image Translation</h2><p><strong>Authors:Feng Yuan, Yifan Gao, Wenbin Wu, Keqing Wu, Xiaotong Guo, Jie Jiang, Xin Gao</strong></p>
<p>Accurate multi-modal medical image translation requires ha-rmonizing global anatomical semantics and local structural fidelity, a challenge complicated by intermodality information loss and structural distortion. We propose ABS-Mamba, a novel architecture integrating the Segment Anything Model 2 (SAM2) for organ-aware semantic representation, specialized convolutional neural networks (CNNs) for preserving modality-specific edge and texture details, and Mambaâ€™s selective state-space modeling for efficient long- and short-range feature dependencies. Structurally, our dual-resolution framework leverages SAM2â€™s image encoder to capture organ-scale semantics from high-resolution inputs, while a parallel CNNs branch extracts fine-grained local features. The Robust Feature Fusion Network (RFFN) integrates these epresentations, and the Bidirectional Mamba Residual Network (BMRN) models spatial dependencies using spiral scanning and bidirectional state-space dynamics. A three-stage skip fusion decoder enhances edge and texture fidelity. We employ Efficient Low-Rank Adaptation (LoRA+) fine-tuning to enable precise domain specialization while maintaining the foundational capabilities of the pre-trained components. Extensive experimental validation on the SynthRAD2023 and BraTS2019 datasets demonstrates that ABS-Mamba outperforms state-of-the-art methods, delivering high-fidelity cross-modal synthesis that preserves anatomical semantics and structural details to enhance diagnostic accuracy in clinical applications. The code is available at <a target="_blank" rel="noopener" href="https://github.com/gatina-yone/ABS-Mamba">https://github.com/gatina-yone/ABS-Mamba</a> </p>
<blockquote>
<p>å‡†ç¡®çš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒç¿»è¯‘éœ€è¦åè°ƒå…¨å±€è§£å‰–è¯­ä¹‰å’Œå±€éƒ¨ç»“æ„å¿ å®æ€§ï¼Œè¿™ä¸€æŒ‘æˆ˜å› æ¨¡æ€é—´ä¿¡æ¯ä¸¢å¤±å’Œç»“æ„å¤±çœŸè€Œå¤æ‚åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ABS-Mambaï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œå®ƒé›†æˆäº†Segment Anything Model 2ï¼ˆSAM2ï¼‰ç”¨äºå™¨å®˜æ„ŸçŸ¥è¯­ä¹‰è¡¨ç¤ºï¼Œä¸“é—¨è®¾è®¡çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç”¨äºä¿ç•™æ¨¡æ€ç‰¹å®šçš„è¾¹ç¼˜å’Œçº¹ç†ç»†èŠ‚ï¼Œä»¥åŠMambaçš„é€‰æ‹©çŠ¶æ€ç©ºé—´å»ºæ¨¡ï¼Œä»¥å®ç°é«˜æ•ˆçš„é•¿çŸ­ç¨‹ç‰¹å¾ä¾èµ–æ€§ã€‚ç»“æ„ä¸Šï¼Œæˆ‘ä»¬çš„åŒåˆ†è¾¨ç‡æ¡†æ¶åˆ©ç”¨SAM2çš„å›¾åƒç¼–ç å™¨ä»é«˜åˆ†è¾¨ç‡è¾“å…¥ä¸­æ•è·å™¨å®˜çº§è¯­ä¹‰ï¼Œè€Œå¹¶è¡ŒCNNåˆ†æ”¯åˆ™æå–ç²¾ç»†çš„å±€éƒ¨ç‰¹å¾ã€‚é²æ£’ç‰¹å¾èåˆç½‘ç»œï¼ˆRFFNï¼‰é›†æˆäº†è¿™äº›è¡¨ç¤ºï¼ŒåŒå‘Mambaæ®‹å·®ç½‘ç»œï¼ˆBMRNï¼‰ä½¿ç”¨èºæ—‹æ‰«æå’ŒåŒå‘çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦å¯¹ç©ºé—´ä¾èµ–æ€§è¿›è¡Œå»ºæ¨¡ã€‚ä¸‰é˜¶æ®µè·³è¿‡èåˆè§£ç å™¨å¢å¼ºäº†è¾¹ç¼˜å’Œçº¹ç†çš„å¿ å®æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨æœ‰æ•ˆçš„ä½ç§©é€‚åº”ï¼ˆLoRA+ï¼‰å¾®è°ƒæ–¹æ³•ï¼Œåœ¨ä¿æŒé¢„è®­ç»ƒç»„ä»¶åŸºç¡€èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°ç²¾ç¡®çš„é¢†åŸŸä¸“ä¸šåŒ–ã€‚åœ¨SynthRAD2023å’ŒBraTS2019æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯è¡¨æ˜ï¼ŒABS-Mambaä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå®ç°äº†é«˜ä¿çœŸè·¨æ¨¡æ€åˆæˆï¼Œä¿ç•™äº†è§£å‰–è¯­ä¹‰å’Œç»“æ„ç»†èŠ‚ï¼Œæé«˜äº†ä¸´åºŠåº”ç”¨ä¸­è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gatina-yone/ABS-Mamba%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/gatina-yone/ABS-Mambaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07687v1">PDF</a> MICCAI 2025(under view)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹åŒ»å­¦å›¾åƒç¿»è¯‘æ¶æ„ABS-Mambaï¼Œè¯¥æ¶æ„èåˆäº†Segment Anything Model 2ï¼ˆSAM2ï¼‰è¿›è¡Œå™¨å®˜æ„ŸçŸ¥è¯­ä¹‰è¡¨ç¤ºï¼Œä¸“ä¸šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ä¿ç•™æ¨¡æ€ç‰¹å®šçš„è¾¹ç¼˜å’Œçº¹ç†ç»†èŠ‚ï¼Œä»¥åŠMambaçš„é€‰æ‹©çŠ¶æ€ç©ºé—´å»ºæ¨¡ï¼Œä»¥é«˜æ•ˆå®ç°é•¿çŸ­ç¨‹ç‰¹å¾ä¾èµ–ã€‚å…¶åŒåˆ†è¾¨ç‡æ¡†æ¶æ•æ‰å™¨å®˜è§„æ¨¡è¯­ä¹‰å’Œé«˜åˆ†è¾¨ç‡è¾“å…¥ï¼ŒåŒæ—¶å¹¶è¡ŒCNNåˆ†æ”¯æå–ç²¾ç»†å±€éƒ¨ç‰¹å¾ã€‚é€šè¿‡Robust Feature Fusion Networkï¼ˆRFFNï¼‰æ•´åˆè¿™äº›è¡¨ç¤ºï¼Œå¹¶é€šè¿‡Bidirectional Mamba Residual Networkï¼ˆBMRNï¼‰è¿›è¡Œç©ºé—´ä¾èµ–æ€§å»ºæ¨¡ã€‚ç»è¿‡åœ¨SynthRAD2023å’ŒBraTS2019æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯ï¼ŒABS-Mambaè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå®ç°äº†é«˜ä¿çœŸè·¨æ¨¡æ€åˆæˆï¼Œä¿ç•™è§£å‰–å­¦è¯­ä¹‰å’Œç»“æ„ç»†èŠ‚ï¼Œæé«˜ä¸´åºŠè¯Šæ–­å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ABS-Mambaæ¶æ„èåˆäº†SAM2è¿›è¡Œå™¨å®˜æ„ŸçŸ¥è¯­ä¹‰è¡¨ç¤ºï¼Œä»¥æ•æ‰å…¨å±€è§£å‰–å­¦è¯­ä¹‰ã€‚</li>
<li>CNNsç”¨äºä¿ç•™æ¨¡æ€ç‰¹å®šçš„è¾¹ç¼˜å’Œçº¹ç†ç»†èŠ‚ï¼Œä»¥ç»´æŒå±€éƒ¨ç»“æ„å®Œæ•´æ€§ã€‚</li>
<li>Mambaçš„é€‰æ‹©çŠ¶æ€ç©ºé—´å»ºæ¨¡å®ç°é«˜æ•ˆé•¿çŸ­ç¨‹ç‰¹å¾ä¾èµ–ã€‚</li>
<li>åŒåˆ†è¾¨ç‡æ¡†æ¶åŒæ—¶æ•æ‰å™¨å®˜è§„æ¨¡è¯­ä¹‰å’Œé«˜åˆ†è¾¨ç‡ä¿¡æ¯ã€‚</li>
<li>RFFNæ•´åˆä¸åŒç‰¹å¾è¡¨ç¤ºï¼ŒBMRNè¿›è¡Œç©ºé—´ä¾èµ–æ€§å»ºæ¨¡ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>é€šè¿‡Efficient Low-Rank Adaptationï¼ˆLoRA+ï¼‰ç²¾ç»†è°ƒæ•´ï¼Œå®ç°ç²¾ç¡®åŸŸä¸“ä¸šåŒ–ï¼ŒåŒæ—¶ä¿æŒé¢„è®­ç»ƒç»„ä»¶çš„åŸºç¡€èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca862168d659c4f3c6023f09f59435d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a19dbac974820340e78f5adf3a5b518.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Breast-Cancer-Classification-in-Deep-Ultraviolet-Fluorescence-Images-Using-a-Patch-Level-Vision-Transformer-Framework"><a href="#Breast-Cancer-Classification-in-Deep-Ultraviolet-Fluorescence-Images-Using-a-Patch-Level-Vision-Transformer-Framework" class="headerlink" title="Breast Cancer Classification in Deep Ultraviolet Fluorescence Images   Using a Patch-Level Vision Transformer Framework"></a>Breast Cancer Classification in Deep Ultraviolet Fluorescence Images   Using a Patch-Level Vision Transformer Framework</h2><p><strong>Authors:Pouya Afshin, David Helminiak, Tongtong Lu, Tina Yen, Julie M. Jorns, Mollie Patton, Bing Yu, Dong Hye Ye</strong></p>
<p>Breast-conserving surgery (BCS) aims to completely remove malignant lesions while maximizing healthy tissue preservation. Intraoperative margin assessment is essential to achieve a balance between thorough cancer resection and tissue conservation. A deep ultraviolet fluorescence scanning microscope (DUV-FSM) enables rapid acquisition of whole surface images (WSIs) for excised tissue, providing contrast between malignant and normal tissues. However, breast cancer classification with DUV WSIs is challenged by high resolutions and complex histopathological features. This study introduces a DUV WSI classification framework using a patch-level vision transformer (ViT) model, capturing local and global features. Grad-CAM++ saliency weighting highlights relevant spatial regions, enhances result interpretability, and improves diagnostic accuracy for benign and malignant tissue classification. A comprehensive 5-fold cross-validation demonstrates the proposed approach significantly outperforms conventional deep learning methods, achieving a classification accuracy of 98.33%. </p>
<blockquote>
<p>ä¿ç•™ä¹³æˆ¿æ‰‹æœ¯ï¼ˆBCSï¼‰æ—¨åœ¨å½»åº•æ¸…é™¤æ¶æ€§ç—…å˜ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°ä¿ç•™å¥åº·ç»„ç»‡ã€‚æœ¯ä¸­è¾¹ç¼˜è¯„ä¼°å¯¹äºåœ¨å½»åº•åˆ‡é™¤ç™Œç—‡ä¸ä¿ç•™ç»„ç»‡ä¹‹é—´å–å¾—å¹³è¡¡è‡³å…³é‡è¦ã€‚æ·±ç´«å¤–çº¿è§å…‰æ‰«ææ˜¾å¾®é•œï¼ˆDUV-FSMï¼‰å¯ä»¥å¿«é€Ÿè·å–å·²åˆ‡é™¤ç»„ç»‡çš„å…¨è¡¨é¢å›¾åƒï¼ˆWSIï¼‰ï¼Œä¸ºæ¶æ€§ç»„ç»‡å’Œæ­£å¸¸ç»„ç»‡æä¾›å¯¹æ¯”ã€‚ç„¶è€Œï¼Œä½¿ç”¨DUV WSIè¿›è¡Œä¹³è…ºç™Œåˆ†ç±»é¢ä¸´ç€é«˜åˆ†è¾¨ç‡å’Œå¤æ‚ç»„ç»‡ç—…ç†ç‰¹å¾çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºè¡¥ä¸çº§è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¨¡å‹çš„DUV WSIåˆ†ç±»æ¡†æ¶ï¼Œèƒ½å¤Ÿæ•æ‰å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚Grad-CAM++æ˜¾è‘—æ€§åŠ æƒèƒ½å¤Ÿçªå‡ºç›¸å…³çš„ç©ºé—´åŒºåŸŸï¼Œæé«˜ç»“æœçš„å¯è§£é‡Šæ€§ï¼Œå¹¶å¢å¼ºå¯¹è‰¯æ€§å’Œæ¶æ€§ç»„ç»‡åˆ†ç±»çš„è¯Šæ–­å‡†ç¡®æ€§ã€‚ç»è¿‡å…¨é¢çš„äº”æŠ˜äº¤å‰éªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•æ˜æ˜¾ä¼˜äºä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œè¾¾åˆ°äº†98.33%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07654v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é‡‡ç”¨æ·±ç´«å¤–çº¿è§å…‰æ‰«ææ˜¾å¾®é•œï¼ˆDUV-FSMï¼‰å¯¹ä¹³è…ºç™Œåˆ‡é™¤ç»„ç»‡è¿›è¡Œæœ¯ä¸­è¾¹ç¼˜è¯„ä¼°çš„æ–¹æ³•ã€‚é€šè¿‡è·å–å…¨è¡¨é¢å›¾åƒï¼ˆWSIï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè¿…é€Ÿè¯†åˆ«æ¶æ€§ä¸æ­£å¸¸ç»„ç»‡ä¹‹é—´çš„å¯¹æ¯”ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºpatchçº§è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¨¡å‹çš„DUV WSIåˆ†ç±»æ¡†æ¶ï¼Œèƒ½å¤Ÿæ•æ‰å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚Grad-CAM++æ˜¾è‘—æ€§åŠ æƒæŠ€æœ¯çªå‡ºäº†å…³é”®çš„ç©ºé—´åŒºåŸŸï¼Œæé«˜äº†åˆ†ç±»ç»“æœçš„è§£é‡Šæ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚ç»è¿‡ç»¼åˆçš„5å€äº¤å‰éªŒè¯ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå®ç°äº†é«˜è¾¾98.33%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä¹³è…ºç™Œæ‰‹æœ¯ä¸­ï¼Œå®Œå…¨åˆ‡é™¤æ¶æ€§ç—…ç¶å¹¶å°½å¯èƒ½ä¿ç•™å¥åº·ç»„ç»‡æ˜¯å…³é”®ã€‚</li>
<li>æ·±ç´«å¤–çº¿è§å…‰æ‰«ææ˜¾å¾®é•œï¼ˆDUV-FSMï¼‰èƒ½å¿«é€Ÿè·å–åˆ‡é™¤ç»„ç»‡çš„å…¨è¡¨é¢å›¾åƒï¼ˆWSIï¼‰ã€‚</li>
<li>é€šè¿‡å¯¹æ¯”æ¶æ€§ä¸æ­£å¸¸ç»„ç»‡ï¼Œæœ‰åŠ©äºè¿›è¡Œæœ¯ä¸­è¾¹ç¼˜è¯„ä¼°ã€‚</li>
<li>é‡‡ç”¨åŸºäºpatchçº§è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¨¡å‹çš„DUV WSIåˆ†ç±»æ¡†æ¶èƒ½å¤Ÿæ•æ‰å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚</li>
<li>Grad-CAM++æŠ€æœ¯èƒ½å¤Ÿçªå‡ºå…³é”®çš„ç©ºé—´åŒºåŸŸï¼Œæé«˜è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸æ¯”ï¼Œæ–°æ–¹æ³•åœ¨åˆ†ç±»å‡†ç¡®ç‡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b312affad5bae96055b263186d795588.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2b813eac54c4735f5c26e60f8e489f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-081d5bd5c4a8949e59c892091b8d625d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a0b0f27a953a70cfccd9689e0aaae3a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Robust-Kidney-Abnormality-Segmentation-A-Validation-Study-of-an-AI-Based-Framework"><a href="#Robust-Kidney-Abnormality-Segmentation-A-Validation-Study-of-an-AI-Based-Framework" class="headerlink" title="Robust Kidney Abnormality Segmentation: A Validation Study of an   AI-Based Framework"></a>Robust Kidney Abnormality Segmentation: A Validation Study of an   AI-Based Framework</h2><p><strong>Authors:Sarah de Boer, Hartmut HÃ¤ntze, Kiran Vaidhya Venkadesh, Myrthe A. D. Buser, Gabriel E. Humpire Mamani, Lina Xu, Lisa C. Adams, Jawed Nawabi, Keno K. Bressem, Bram van Ginneken, Mathias Prokop, Alessa Hering</strong></p>
<p>Kidney abnormality segmentation has important potential to enhance the clinical workflow, especially in settings requiring quantitative assessments. Kidney volume could serve as an important biomarker for renal diseases, with changes in volume correlating directly with kidney function. Currently, clinical practice often relies on subjective visual assessment for evaluating kidney size and abnormalities, including tumors and cysts, which are typically staged based on diameter, volume, and anatomical location. To support a more objective and reproducible approach, this research aims to develop a robust, thoroughly validated kidney abnormality segmentation algorithm, made publicly available for clinical and research use. We employ publicly available training datasets and leverage the state-of-the-art medical image segmentation framework nnU-Net. Validation is conducted using both proprietary and public test datasets, with segmentation performance quantified by Dice coefficient and the 95th percentile Hausdorff distance. Furthermore, we analyze robustness across subgroups based on patient sex, age, CT contrast phases, and tumor histologic subtypes. Our findings demonstrate that our segmentation algorithm, trained exclusively on publicly available data, generalizes effectively to external test sets and outperforms existing state-of-the-art models across all tested datasets. Subgroup analyses reveal consistent high performance, indicating strong robustness and reliability. The developed algorithm and associated code are publicly accessible at <a target="_blank" rel="noopener" href="https://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation">https://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation</a>. </p>
<blockquote>
<p>è‚¾è„å¼‚å¸¸åˆ†å‰²åœ¨ä¸´åºŠå·¥ä½œæµç¨‹ä¸­å…·æœ‰é‡è¦æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦è¿›è¡Œå®šé‡è¯„ä¼°çš„æƒ…å†µä¸‹ã€‚è‚¾è„ä½“ç§¯å¯ä½œä¸ºè‚¾è„ç–¾ç—…çš„é‡è¦ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œä½“ç§¯çš„å˜åŒ–ç›´æ¥ä¸è‚¾è„åŠŸèƒ½ç›¸å…³ã€‚ç›®å‰ï¼Œä¸´åºŠä¸Šé€šå¸¸ä¾èµ–äºä¸»è§‚è§†è§‰è¯„ä¼°æ¥è¯„ä¼°è‚¾è„å¤§å°å’Œå¼‚å¸¸ï¼ŒåŒ…æ‹¬è‚¿ç˜¤å’Œå›Šè‚¿ç­‰ï¼Œè¿™äº›é€šå¸¸åŸºäºç›´å¾„ã€ä½“ç§¯å’Œè§£å‰–ä½ç½®è¿›è¡Œåˆ†æœŸã€‚ä¸ºäº†æ”¯æŒæ›´å®¢è§‚å’Œå¯é‡å¤çš„æ–¹æ³•ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§ç¨³å¥ä¸”ç»è¿‡å……åˆ†éªŒè¯çš„è‚¾è„å¼‚å¸¸åˆ†å‰²ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯å…¬å¼€ç”¨äºä¸´åºŠå’Œç ”ç©¶ã€‚æˆ‘ä»¬é‡‡ç”¨å…¬å¼€å¯ç”¨çš„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨æœ€å…ˆè¿›çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶nnU-Netã€‚éªŒè¯ä½¿ç”¨ä¸“æœ‰å’Œå…¬å…±æµ‹è¯•æ•°æ®é›†è¿›è¡Œï¼Œé€šè¿‡Diceç³»æ•°å’Œ95thç™¾åˆ†ä½Hausdorffè·ç¦»é‡åŒ–åˆ†å‰²æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº†åŸºäºæ‚£è€…æ€§åˆ«ã€å¹´é¾„ã€CTå¯¹æ¯”é˜¶æ®µå’Œè‚¿ç˜¤ç»„ç»‡å­¦äºšå‹çš„å­ç»„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»…ä½¿ç”¨å…¬å¼€æ•°æ®è®­ç»ƒçš„åˆ†å‰²ç®—æ³•æœ‰æ•ˆåœ°æ¨å¹¿åˆ°å¤–éƒ¨æµ‹è¯•é›†ï¼Œå¹¶ä¸”åœ¨æ‰€æœ‰æµ‹è¯•æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚å­ç»„åˆ†ææ˜¾ç¤ºæ€§èƒ½ä¸€è‡´è¾ƒé«˜ï¼Œè¡¨æ˜å…¶å…·æœ‰è¾ƒå¼ºçš„ç¨³å¥æ€§å’Œå¯é æ€§ã€‚æ‰€å¼€å‘çš„ç®—æ³•å’Œç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentationä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07573v1">PDF</a> 35 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºè‚¾è„å¼‚å¸¸åˆ†å‰²çš„ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯æé«˜ä¸´åºŠå·¥ä½œæµç¨‹æ•ˆç‡ï¼Œå°¤å…¶åœ¨éœ€è¦å®šé‡è¯„ä¼°çš„æƒ…å¢ƒä¸‹ã€‚è‚¾è„ä½“ç§¯ä½œä¸ºè‚¾è„ç–¾ç—…çš„é‡è¦ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œå…¶å˜åŒ–å¯ç›´æ¥åæ˜ è‚¾è„åŠŸèƒ½ã€‚ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§ç¨³å¥ä¸”ç»è¿‡å……åˆ†éªŒè¯çš„è‚¾è„å¼‚å¸¸åˆ†å‰²ç®—æ³•ï¼Œç”¨äºä¸´åºŠå’Œç ”ç©¶ä½¿ç”¨ï¼Œå¹¶å…¬å¼€å¯ç”¨ã€‚è¯¥ç ”ç©¶é‡‡ç”¨å…¬å¼€çš„è®­ç»ƒæ•°æ®é›†å’Œæœ€æ–°çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶nnU-Netï¼Œå¹¶åœ¨ä¸“æœ‰å’Œå…¬å…±æµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ï¼Œé€šè¿‡Diceç³»æ•°å’Œ95thç™¾åˆ†ä½Hausdorffè·ç¦»é‡åŒ–åˆ†å‰²æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜åˆ†æäº†ç®—æ³•åœ¨ä¸åŒæ‚£è€…æ€§åˆ«ã€å¹´é¾„ã€CTå¯¹æ¯”é˜¶æ®µå’Œè‚¿ç˜¤ç»„ç»‡å­¦äºšå‹ä¸‹çš„ç¨³å¥æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤–éƒ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œä¸”æ€§èƒ½ç¨³å®šå¯é ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚¾è„å¼‚å¸¸åˆ†å‰²åœ¨ä¸´åºŠå·¥ä½œæµç¨‹ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå°¤å…¶åœ¨éœ€è¦å®šé‡è¯„ä¼°çš„æƒ…å†µä¸‹ã€‚</li>
<li>è‚¾è„ä½“ç§¯æ˜¯åæ˜ è‚¾è„åŠŸèƒ½çš„é‡è¦ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚</li>
<li>ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§ç¨³å¥ã€ç»è¿‡éªŒè¯çš„è‚¾è„å¼‚å¸¸åˆ†å‰²ç®—æ³•ï¼Œç”¨äºä¸´åºŠå’Œç ”ç©¶ä½¿ç”¨ã€‚</li>
<li>è¯¥ç ”ç©¶é‡‡ç”¨å…¬å¼€çš„è®­ç»ƒæ•°æ®é›†å’Œæœ€æ–°çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶nnU-Netã€‚</li>
<li>ç®—æ³•åœ¨ä¸“æœ‰å’Œå…¬å…±æµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>ç®—æ³•åœ¨ä¸åŒæ‚£è€…ç‰¹å¾ï¼ˆå¦‚æ€§åˆ«ã€å¹´é¾„ï¼‰å’Œä¸åŒçš„CTå¯¹æ¯”é˜¶æ®µä¸‹å…·æœ‰ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a796bb33eaf96800e34777e8398fc905.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bb93d34e9b1bf48fb732aa52b2d8545.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e768b4c76b41210fb386bb42f20704a1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Optimized-flux-single-crystal-growth-of-the-quantum-spin-liquid-candidate-NdTa-7-O-19-and-other-rare-earth-heptatantalates-ErTa-7-O-19-and-GdTa-7-O-19"><a href="#Optimized-flux-single-crystal-growth-of-the-quantum-spin-liquid-candidate-NdTa-7-O-19-and-other-rare-earth-heptatantalates-ErTa-7-O-19-and-GdTa-7-O-19" class="headerlink" title="Optimized flux single-crystal growth of the quantum spin liquid   candidate NdTa$7$O${19}$ and other rare-earth heptatantalates,   ErTa$7$O${19}$ and GdTa$7$O${19}$"></a>Optimized flux single-crystal growth of the quantum spin liquid   candidate NdTa$<em>7$O$</em>{19}$ and other rare-earth heptatantalates,   ErTa$<em>7$O$</em>{19}$ and GdTa$<em>7$O$</em>{19}$</h2><p><strong>Authors:Lia Å ibav, Matic LozinÅ¡ek, Zvonko JagliÄiÄ‡, Tina Arh, Panchanana Khuntia, Andrej Zorko, Mirela Dragomir</strong></p>
<p>Single crystals are essential for characterizing a wide range of magnetic states, including exotic ones such as quantum spin liquids. This study reports a flux method for growing single crystals of NdTa$<em>7$O$</em>{19}$, the first quantum spin liquid candidate on a triangular spin lattice with dominant Ising like spin correlations. Purple NdTa$<em>7$O$</em>{19}$ single crystals with hexagonal morphology were successfully grown using a K$_2$Mo$<em>3$O$</em>{10}$-B$_2$O$_3$ flux. With lateral sizes up to 3.5 mm and a thickness up to 2 mm, these are the largest dimensions reported to date. The chemical composition was confirmed by powder and single-crystal X-ray diffraction along with scanning electron microscopy with energy dispersive X-ray spectroscopy. Aiming for an accurate determination of the magnetic anisotropy and its effect on the magnetic properties, NdTa$<em>7$O$</em>{19}$ crystals were additionally analyzed by magnetic susceptibility, revealing a substantial anisotropy without long-range magnetic ordering down to 2 K. Single crystals of two novel rare-earth heptatantalates, ErTa$<em>7$O$</em>{19}$ and GdTa$<em>7$O$</em>{19}$, were also grown and their magnetic properties investigated. The magnetic anisotropy of ErTa$<em>7$O$</em>{19}$ closely resembles that of isostructural NdTa$<em>7$O$</em>{19}$, indicating a possibility of a similar exotic magnetic ground state. In contrast, GdTa$<em>7$O$</em>{19}$ shows paramagnetic behavior, consistent with previous results obtained for polycrystalline samples. </p>
<blockquote>
<p>å•æ™¶å¯¹äºè¡¨å¾å„ç§ç£æ€ï¼ŒåŒ…æ‹¬é‡å­è‡ªæ—‹æ¶²ä½“ç­‰å¥‡å¼‚çŠ¶æ€è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æŠ¥å‘Šäº†ä¸€ç§ç”Ÿé•¿NdTa7O19å•æ™¶çš„é€šé‡æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åœ¨ä¸‰è§’è‡ªæ—‹æ™¶æ ¼ä¸Šå…·æœ‰ä¸»å¯¼ä¼Šè¾›å‹è‡ªæ—‹å…³è”çš„é¦–ä¸ªé‡å­è‡ªæ—‹æ¶²ä½“å€™é€‰ææ–™ã€‚ä½¿ç”¨K2Mo3O10-B2O3é€šé‡æˆåŠŸç”Ÿé•¿äº†å…·æœ‰å…­è¾¹å½¢å½¢æ€çš„ç´«è‰²NdTa7O19å•æ™¶ï¼Œå…¶æ¨ªå‘å°ºå¯¸è¾¾3.5æ¯«ç±³ï¼Œåšåº¦è¾¾2æ¯«ç±³ï¼Œæ˜¯ç›®å‰æŠ¥é“çš„æœ€å¤§å°ºå¯¸ã€‚é€šè¿‡ç²‰æœ«å’Œå•æ™¶Xå°„çº¿è¡å°„ä»¥åŠå¸¦æœ‰èƒ½é‡è‰²æ•£Xå°„çº¿å…‰è°±çš„æ‰«æç”µå­æ˜¾å¾®é•œç¡®è®¤äº†å…¶åŒ–å­¦æˆåˆ†ã€‚ä¸ºäº†å‡†ç¡®ç¡®å®šç£å„å‘å¼‚æ€§åŠå…¶å¯¹ç£æ€§çš„å½±å“ï¼Œè¿˜é€šè¿‡ç£åŒ–ç‡å¯¹NdTa7O19æ™¶ä½“è¿›è¡Œäº†åˆ†æï¼Œå‘ç°å…¶åœ¨ä½è‡³2Kæ—¶æ²¡æœ‰é•¿ç¨‹ç£åºï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„å„å‘å¼‚æ€§ã€‚è¿˜ç”Ÿé•¿äº†ä¸¤ç§æ–°å‹ç¨€åœŸä¸ƒé’½é…¸ç›ErTa7O19å’ŒGdTa7O19çš„å•æ™¶ï¼Œå¹¶ç ”ç©¶äº†å®ƒä»¬çš„ç£æ€§èƒ½ã€‚ErTa7O19çš„ç£å„å‘å¼‚æ€§ä¸å…·æœ‰åŒæ„çš„NdTa7O19éå¸¸ç›¸ä¼¼ï¼Œè¡¨æ˜å¯èƒ½å­˜åœ¨ç±»ä¼¼çš„å¥‡å¼‚ç£åŸºæ€ã€‚ç›¸åï¼ŒGdTa7O19è¡¨ç°å‡ºé¡ºç£æ€§è¡Œä¸ºï¼Œè¿™ä¸å¤šæ™¶æ ·å“çš„å…ˆå‰ç»“æœä¸€è‡´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07475v1">PDF</a> 24 pages, 11 figures, supplementary material</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æŠ¥é“äº†ä¸€ç§ç”Ÿé•¿$NdTa_7O_{19}$å•æ™¶çš„é€šé‡æ–¹æ³•ï¼Œè¯¥ç‰©è´¨æˆä¸ºé¦–ä¸ªåœ¨ä¸‰è§’è‡ªæ—‹æ™¶æ ¼ä¸Šå…·æœ‰ä¸»å¯¼Isingç±»è‡ªæ—‹å…³è”çš„é‡å­è‡ªæ—‹æ¶²ä½“å€™é€‰ææ–™ã€‚æˆåŠŸä½¿ç”¨$K_2Mo_3O_{10}-B_2O_3$é€šé‡ç”Ÿé•¿å‡ºå…·æœ‰å…­è§’å½¢æ€çš„ç´«è‰²$NdTa_7O_{19}$å•æ™¶ï¼Œå…¶æ¨ªå‘å°ºå¯¸æœ€å¤§å¯è¾¾3.5æ¯«ç±³ï¼Œåšåº¦æœ€å¤§ä¸º2æ¯«ç±³ã€‚é€šè¿‡ç²‰æœ«å’Œå•æ™¶Xå°„çº¿è¡å°„ä»¥åŠæ‰«æç”µå­æ˜¾å¾®é•œä¸èƒ½é‡è‰²æ•£Xå°„çº¿å…‰è°±å­¦ç¡®è®¤å…¶åŒ–å­¦æˆåˆ†ã€‚ä¸ºå‡†ç¡®ç¡®å®šç£å„å‘å¼‚æ€§åŠå…¶å¯¹ç£æ€§èƒ½çš„å½±å“ï¼Œè¿˜é€šè¿‡ç£åŒ–ç‡åˆ†æäº†$NdTa_7O_{19}$æ™¶ä½“ï¼Œæ­ç¤ºå‡ºæ˜¾è‘—çš„ç£å„å‘å¼‚æ€§ä»¥åŠåœ¨2Kä»¥ä¸‹æ— è¿œç¨‹ç£åºçš„ç°è±¡ã€‚æ­¤å¤–ï¼Œè¿˜ç”Ÿé•¿äº†ä¸¤ç§æ–°å‹ç¨€åœŸä¸ƒé’½é…¸ç›$ErTa_7O_{19}$å’Œ$GdTa_7O_{19}$çš„å•æ™¶ï¼Œå¹¶ç ”ç©¶äº†å®ƒä»¬çš„ç£æ€§èƒ½ã€‚$ErTa_7O_{19}$çš„ç£å„å‘å¼‚æ€§ç±»ä¼¼äºåŒæ„çš„$NdTa_7O_{19}$ï¼Œå¯èƒ½å…·æœ‰ç±»ä¼¼çš„å¥‡å¼‚ç£åŸºæ€ã€‚è€Œ$GdTa_7O_{19}$åˆ™è¡¨ç°å‡ºä¸å¤šæ™¶æ ·å“ç»“æœä¸€è‡´çš„é¡ºç£æ€§è¡Œä¸ºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æŠ¥é“äº†ä¸€ç§ç”Ÿé•¿$NdTa_7O_{19}$å•æ™¶çš„é€šé‡æ–¹æ³•ï¼Œè¿™ç§ææ–™æ˜¯é¦–ä¸ªåœ¨ä¸‰è§’è‡ªæ—‹æ™¶æ ¼ä¸Šçš„é‡å­è‡ªæ—‹æ¶²ä½“å€™é€‰ã€‚</li>
<li>æˆåŠŸç”Ÿé•¿å‡ºå¤§å°ºå¯¸çš„$NdTa_7O_{19}$å•æ™¶ï¼Œæœ€å¤§æ¨ªå‘å°ºå¯¸è¾¾3.5æ¯«ç±³ï¼Œåšåº¦è¾¾2æ¯«ç±³ã€‚</li>
<li>é€šè¿‡å¤šç§æŠ€æœ¯ç¡®è®¤äº†$NdTa_7O_{19}$ã€$ErTa_7O_{19}$å’Œ$GdTa_7O_{19}$çš„åŒ–å­¦æˆåˆ†ã€‚</li>
<li>$NdTa_7O_{19}$æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ç£å„å‘å¼‚æ€§åŠä½æ¸©ä¸‹æ— è¿œç¨‹ç£åºçš„ç°è±¡ã€‚</li>
<li>$ErTa_7O_{19}$çš„ç£å„å‘å¼‚æ€§ç±»ä¼¼äº$NdTa_7O_{19}$ï¼Œæš—ç¤ºå…¶å¯èƒ½æœ‰ç±»ä¼¼çš„ç‰¹æ®Šç£åŸºæ€ã€‚</li>
<li>$GdTa_7O_{19}$è¡¨ç°å‡ºé¡ºç£æ€§è¡Œä¸ºï¼Œè¿™ä¸ä¹‹å‰çš„å¤šæ™¶æ ·å“ç ”ç©¶ç»“æœä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07475">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-148c11ecf6eac4b16df2536389944852.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-167e46ae3086c8d99e12751003c58b9a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40d020b24727fb191b0708b0fa824e02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd41a34c2496860e83827c3544a33a34.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="GAN-based-synthetic-FDG-PET-images-from-T1-brain-MRI-can-serve-to-improve-performance-of-deep-unsupervised-anomaly-detection-models"><a href="#GAN-based-synthetic-FDG-PET-images-from-T1-brain-MRI-can-serve-to-improve-performance-of-deep-unsupervised-anomaly-detection-models" class="headerlink" title="GAN-based synthetic FDG PET images from T1 brain MRI can serve to   improve performance of deep unsupervised anomaly detection models"></a>GAN-based synthetic FDG PET images from T1 brain MRI can serve to   improve performance of deep unsupervised anomaly detection models</h2><p><strong>Authors:Daria Zotova, Nicolas Pinon, Robin Trombetta, Romain Bouet, Julien Jung, Carole Lartizien</strong></p>
<p>Background and Objective. Research in the cross-modal medical image translation domain has been very productive over the past few years in tackling the scarce availability of large curated multimodality datasets with the promising performance of GAN-based architectures. However, only a few of these studies assessed task-based related performance of these synthetic data, especially for the training of deep models. Method. We design and compare different GAN-based frameworks for generating synthetic brain [18F]fluorodeoxyglucose (FDG) PET images from T1 weighted MRI data. We first perform standard qualitative and quantitative visual quality evaluation. Then, we explore further impact of using these fake PET data in the training of a deep unsupervised anomaly detection (UAD) model designed to detect subtle epilepsy lesions in T1 MRI and FDG PET images. We introduce novel diagnostic task-oriented quality metrics of the synthetic FDG PET data tailored to our unsupervised detection task, then use these fake data to train a use case UAD model combining a deep representation learning based on siamese autoencoders with a OC-SVM density support estimation model. This model is trained on normal subjects only and allows the detection of any variation from the pattern of the normal population. We compare the detection performance of models trained on 35 paired real MR T1 of normal subjects paired either on 35 true PET images or on 35 synthetic PET images generated from the best performing generative models. Performance analysis is conducted on 17 exams of epilepsy patients undergoing surgery. Results. The best performing GAN-based models allow generating realistic fake PET images of control subject with SSIM and PSNR values around 0.9 and 23.8, respectively and in distribution (ID) with regard to the true control dataset. The best UAD model trained on these synthetic normative PET data allows reaching 74% sensitivity. Conclusion. Our results confirm that GAN-based models are the best suited for MR T1 to FDG PET translation, outperforming transformer or diffusion models. We also demonstrate the diagnostic value of these synthetic data for the training of UAD models and evaluation on clinical exams of epilepsy patients. Our code and the normative image dataset are available. </p>
<blockquote>
<p><strong>èƒŒæ™¯ä¸ç›®çš„</strong>ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºGANæ¶æ„çš„è·¨æ¨¡æ€åŒ»å­¦å›¾åƒç¿»è¯‘é¢†åŸŸçš„ç ”ç©¶å·²ç»å–å¾—äº†ä¸°ç¡•çš„æˆæœï¼Œè§£å†³äº†å¤§å‹æ•´ç†å¤šæ¨¡æ€æ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ã€‚ç„¶è€Œï¼Œåªæœ‰å°‘æ•°ç ”ç©¶è¯„ä¼°äº†è¿™äº›åˆæˆæ•°æ®åœ¨ä»»åŠ¡ç›¸å…³æ€§èƒ½æ–¹é¢çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯ç”¨äºè®­ç»ƒæ·±åº¦æ¨¡å‹æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>æ–¹æ³•</strong>ã€‚æˆ‘ä»¬è®¾è®¡å¹¶æ¯”è¾ƒäº†åŸºäºä¸åŒGANçš„æ¡†æ¶ï¼Œç”¨äºä»T1åŠ æƒMRIæ•°æ®ç”Ÿæˆåˆæˆçš„å¤§è„‘[18F]æ°Ÿè„±æ°§è‘¡è„ç³–ï¼ˆFDGï¼‰PETå›¾åƒã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œæ ‡å‡†çš„ä¸»è§‚å’Œå®¢è§‚è§†è§‰è´¨é‡è¯„ä¼°ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢ä½¿ç”¨è¿™äº›å‡PETæ•°æ®åœ¨è®­ç»ƒæ·±åº¦æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼ˆUADï¼‰æ¨¡å‹ä¸­çš„å½±å“ï¼Œè¯¥æ¨¡å‹æ—¨åœ¨æ£€æµ‹T1 MRIå’ŒFDG PETå›¾åƒä¸­çš„ç»†å¾®ç™«ç—«ç—…ç¶ã€‚æˆ‘ä»¬å¼•å…¥é’ˆå¯¹æˆ‘ä»¬çš„æ— ç›‘ç£æ£€æµ‹ä»»åŠ¡çš„æ–°å‹è¯Šæ–­ä»»åŠ¡å¯¼å‘å‹è´¨é‡æŒ‡æ ‡ï¼Œä»¥è¯„ä¼°åˆæˆFDG PETæ•°æ®ï¼Œç„¶åä½¿ç”¨è¿™äº›å‡æ•°æ®æ¥è®­ç»ƒä¸€ä¸ªç”¨ä¾‹UADæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†åŸºäºå­ªç”Ÿè‡ªç¼–ç å™¨çš„æ·±åº¦è¡¨ç¤ºå­¦ä¹ å’ŒOC-SVMå¯†åº¦æ”¯æŒä¼°è®¡æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä»…å¯¹æ­£å¸¸ä¸»ä½“è¿›è¡Œè®­ç»ƒï¼Œå¯æ£€æµ‹å‡ºä»»ä½•ä¸æ­£å¸¸äººç¾¤æ¨¡å¼çš„åå·®ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†åœ¨çœŸå®MR T1å›¾åƒï¼ˆæ¥è‡ª35åæ­£å¸¸å—è¯•è€…ï¼‰é…å¯¹ä¸­ï¼Œä½¿ç”¨çœŸå®PETå›¾åƒæˆ–æœ€ä½³æ€§èƒ½ç”Ÿæˆæ¨¡å‹çš„åˆæˆPETå›¾åƒè®­ç»ƒçš„æ¨¡å‹çš„æ£€æµ‹æ€§èƒ½ã€‚æ€§èƒ½åˆ†ææ˜¯åœ¨æ¥å—æ‰‹æœ¯çš„17åç™«ç—«æ‚£è€…èº«ä¸Šè¿›è¡Œçš„ã€‚</p>
<p><strong>ç»“æœ</strong>ã€‚è¡¨ç°æœ€ä½³çš„åŸºäºGANçš„æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„å‡PETå›¾åƒï¼Œå¯¹ç…§ç»„çš„SSIMå’ŒPSNRå€¼åˆ†åˆ«çº¦ä¸º0.9å’Œ23.8ï¼Œä¸çœŸå®å¯¹ç…§ç»„æ•°æ®åœ¨åˆ†å¸ƒä¸Šç›¸ç¬¦ã€‚ä½¿ç”¨è¿™äº›åˆæˆè§„èŒƒæ€§PETæ•°æ®è®­ç»ƒçš„æœ€ä½³UADæ¨¡å‹çš„æ•æ„Ÿæ€§è¾¾åˆ°74%ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07364v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒè·¨æ¨¡æ€è½¬æ¢é¢†åŸŸçš„ç ”ç©¶åœ¨è¿‡å»å‡ å¹´ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åŸºäºGANçš„æ¶æ„åœ¨ç”Ÿæˆåˆæˆæ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æœ¬ç ”ç©¶è®¾è®¡å¹¶æ¯”è¾ƒäº†ä¸åŒçš„GANæ¡†æ¶ï¼Œç”¨äºä»T1åŠ æƒMRIæ•°æ®ç”Ÿæˆåˆæˆçš„å¤§è„‘[18F]æ°Ÿè„±æ°§è‘¡è„ç³–ï¼ˆFDGï¼‰PETå›¾åƒã€‚ç ”ç©¶å‘ç°ï¼Œæœ€ä½³GANæ¨¡å‹ç”Ÿæˆçš„åˆæˆPETå›¾åƒä¸ç°å®å›¾åƒç›¸ä¼¼åº¦é«˜ï¼Œä¸”å¯¹äºè®­ç»ƒæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ¨¡å‹å…·æœ‰è¯Šæ–­ä»·å€¼ã€‚è¯¥æ¨¡å‹åœ¨ç™«ç—«ç—…ç¶æ£€æµ‹æ–¹é¢çš„æ•æ„Ÿåº¦è¾¾åˆ°74%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºGANçš„æ¶æ„åœ¨åŒ»å­¦å›¾åƒè·¨æ¨¡æ€è½¬æ¢é¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆåˆæˆæ•°æ®æ–¹é¢ã€‚</li>
<li>è®¾è®¡å¹¶æ¯”è¾ƒäº†å¤šç§GANæ¡†æ¶ï¼Œç”¨äºä»T1åŠ æƒMRIæ•°æ®ç”Ÿæˆå¤§è„‘FDG PETå›¾åƒã€‚</li>
<li>æœ€ä½³GANæ¨¡å‹ç”Ÿæˆçš„åˆæˆPETå›¾åƒä¸ç°å®å›¾åƒç›¸ä¼¼åº¦é«˜ï¼Œå…·æœ‰ç°å®æ€§è¯„ä¼°æŒ‡æ ‡å¦‚SSIMå’ŒPSNRå€¼ã€‚</li>
<li>åˆæˆæ•°æ®å¯¹äºè®­ç»ƒæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ¨¡å‹å…·æœ‰è¯Šæ–­ä»·å€¼ã€‚</li>
<li>åœ¨ç™«ç—«ç—…ç¶æ£€æµ‹æ–¹é¢ï¼Œä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹çš„æ•æ„Ÿåº¦è¾¾åˆ°74%ã€‚</li>
<li>æœ€ä½³UADæ¨¡å‹åœ¨æ‰‹æœ¯ä¸­çš„ç™«ç—«æ‚£è€…è€ƒè¯•ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶ä»£ç å’Œæ ‡å‡†åŒ–å›¾åƒæ•°æ®é›†å¯ä¾›ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07364">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-75d8aa298bc71f1dd84f1fb41bb64059.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ad81deaf0ad0909d007a8a007018411.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-edee906e9a345f0ac2a02462149c7882.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e31676b64bce418610fd0cae36d7bb02.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Link-to-the-Past-Temporal-Propagation-for-Fast-3D-Human-Reconstruction-from-Monocular-Video"><a href="#Link-to-the-Past-Temporal-Propagation-for-Fast-3D-Human-Reconstruction-from-Monocular-Video" class="headerlink" title="Link to the Past: Temporal Propagation for Fast 3D Human Reconstruction   from Monocular Video"></a>Link to the Past: Temporal Propagation for Fast 3D Human Reconstruction   from Monocular Video</h2><p><strong>Authors:Matthew Marchellus, Nadhira Noor, In Kyu Park</strong></p>
<p>Fast 3D clothed human reconstruction from monocular video remains a significant challenge in computer vision, particularly in balancing computational efficiency with reconstruction quality. Current approaches are either focused on static image reconstruction but too computationally intensive, or achieve high quality through per-video optimization that requires minutes to hours of processing, making them unsuitable for real-time applications. To this end, we present TemPoFast3D, a novel method that leverages temporal coherency of human appearance to reduce redundant computation while maintaining reconstruction quality. Our approach is a â€œplug-and playâ€ solution that uniquely transforms pixel-aligned reconstruction networks to handle continuous video streams by maintaining and refining a canonical appearance representation through efficient coordinate mapping. Extensive experiments demonstrate that TemPoFast3D matches or exceeds state-of-the-art methods across standard metrics while providing high-quality textured reconstruction across diverse pose and appearance, with a maximum speed of 12 FPS. </p>
<blockquote>
<p>ä»å•ç›®è§†é¢‘ä¸­å¿«é€Ÿè¿›è¡Œä¸‰ç»´æœè£…äººä½“é‡å»ºä»ç„¶æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—æ•ˆç‡å’Œé‡å»ºè´¨é‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å½“å‰çš„æ–¹æ³•è¦ä¹ˆä¸“æ³¨äºé™æ€å›¾åƒçš„é‡å»ºï¼Œä½†è®¡ç®—è¿‡äºå¯†é›†ï¼Œè¦ä¹ˆé€šè¿‡è§†é¢‘ä¼˜åŒ–å®ç°é«˜è´¨é‡é‡å»ºï¼Œè¿™éœ€è¦æ•°åˆ†é’Ÿåˆ°æ•°å°æ—¶çš„å¤„ç†æ—¶é—´ï¼Œä½¿å…¶ä¸é€‚åˆå®æ—¶åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†TemPoFast3Dï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨äººä½“å¤–è§‚çš„æ—¶é—´è¿è´¯æ€§æ¥å‡å°‘å†—ä½™è®¡ç®—å¹¶ä¿æŒé‡å»ºè´¨é‡çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯ä¸€ç§â€œå³æ’å³ç”¨â€çš„è§£å†³æ–¹æ¡ˆï¼Œå®ƒé€šè¿‡ä¿æŒå’Œç»†åŒ–è§„èŒƒå¤–è§‚è¡¨ç¤ºï¼Œå¹¶é€šè¿‡æœ‰æ•ˆçš„åæ ‡æ˜ å°„æ¥è½¬æ¢åƒç´ å¯¹é½é‡å»ºç½‘ç»œï¼Œä»¥å¤„ç†è¿ç»­çš„è§†é¢‘æµã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTemPoFast3Dåœ¨æ ‡å‡†æŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¸ç°æœ‰æŠ€æœ¯æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ï¼Œåœ¨ä¸åŒå§¿æ€å’Œå¤–è§‚ä¸‹éƒ½èƒ½å®ç°é«˜è´¨é‡çº¹ç†é‡å»ºï¼Œæœ€é«˜é€Ÿåº¦ä¸ºæ¯ç§’å¤„ç†åäºŒå¸§å›¾åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07333v1">PDF</a> Accepted in CVPR 2025</p>
<p><strong>Summary</strong><br>    æå‡ºäº†ä¸€ç§åŸºäºæ—¶åºä¸€è‡´æ€§çš„å¿«é€Ÿä¸‰ç»´æœè£…é‡å»ºæ–¹æ³•ï¼Œå®ç°äº†é«˜æ•ˆè®¡ç®—ä¸é«˜è´¨é‡é‡å»ºçš„å¹³è¡¡ã€‚åˆ©ç”¨åƒç´ å¯¹é½é‡å»ºç½‘ç»œå¤„ç†è¿ç»­è§†é¢‘æµï¼Œé€šè¿‡ç»´æŠ¤å¹¶ä¼˜åŒ–æ ‡å‡†å¤–è§‚è¡¨ç¤ºï¼Œå®ç°é«˜è´¨é‡çº¹ç†é‡å»ºã€‚æœ€é«˜å¤„ç†é€Ÿåº¦å¯è¾¾æ¯ç§’12å¸§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TemPoFast3Dæ˜¯ä¸€ç§åˆ©ç”¨äººä½“å¤–è§‚æ—¶åºä¸€è‡´æ€§è¿›è¡Œå¿«é€Ÿä¸‰ç»´æœè£…é‡å»ºçš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†è®¡ç®—æ•ˆç‡å’Œé‡å»ºè´¨é‡çš„å¹³è¡¡ã€‚</li>
<li>TemPoFast3Dé‡‡ç”¨â€œå³æ’å³ç”¨â€çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå¤„ç†è¿ç»­çš„è§†é¢‘æµã€‚</li>
<li>é€šè¿‡ç»´æŠ¤å¹¶ä¼˜åŒ–æ ‡å‡†å¤–è§‚è¡¨ç¤ºï¼Œå®ç°äº†é«˜æ•ˆçš„åæ ‡æ˜ å°„ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§å§¿åŠ¿å’Œå¤–è§‚ä¸‹çš„é‡å»ºè´¨é‡è¾¾åˆ°æˆ–è¶…è¿‡äº†ç°æœ‰æŠ€æœ¯ã€‚</li>
<li>TemPoFast3Då…·æœ‰æ¯ç§’æœ€é«˜å¯è¾¾12å¸§çš„å¤„ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0649a6fcb1eebdfffe8beb604dfec3a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0f6edda818664cb23d74d35aef151e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f88f822d4228ff4ea67fb762f3c7728.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-373c003803cabdb2b7fb196685fe4521.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-635b02c6905f0b936d8c62ba553ced5d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Hallucination-Aware-Multimodal-Benchmark-for-Gastrointestinal-Image-Analysis-with-Large-Vision-Language-Models"><a href="#Hallucination-Aware-Multimodal-Benchmark-for-Gastrointestinal-Image-Analysis-with-Large-Vision-Language-Models" class="headerlink" title="Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image   Analysis with Large Vision-Language Models"></a>Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image   Analysis with Large Vision-Language Models</h2><p><strong>Authors:Bidur Khanal, Sandesh Pokhrel, Sanjay Bhandari, Ramesh Rana, Nikesh Shrestha, Ram Bahadur Gurung, Cristian Linte, Angus Watson, Yash Raj Shrestha, Binod Bhattarai</strong></p>
<p>Vision-Language Models (VLMs) are becoming increasingly popular in the medical domain, bridging the gap between medical images and clinical language. Existing VLMs demonstrate an impressive ability to comprehend medical images and text queries to generate detailed, descriptive diagnostic medical reports. However, hallucinationâ€“the tendency to generate descriptions that are inconsistent with the visual contentâ€“remains a significant issue in VLMs, with particularly severe implications in the medical field. To facilitate VLM research on gastrointestinal (GI) image analysis and study hallucination, we curate a multimodal image-text GI dataset: Gut-VLM. This dataset is created using a two-stage pipeline: first, descriptive medical reports of Kvasir-v2 images are generated using ChatGPT, which introduces some hallucinated or incorrect texts. In the second stage, medical experts systematically review these reports, and identify and correct potential inaccuracies to ensure high-quality, clinically reliable annotations. Unlike traditional datasets that contain only descriptive texts, our dataset also features tags identifying hallucinated sentences and their corresponding corrections. A common approach to reducing hallucination in VLM is to finetune the model on a small-scale, problem-specific dataset. However, we take a different strategy using our dataset. Instead of finetuning the VLM solely for generating textual reports, we finetune it to detect and correct hallucinations, an approach we call hallucination-aware finetuning. Our results show that this approach is better than simply finetuning for descriptive report generation. Additionally, we conduct an extensive evaluation of state-of-the-art VLMs across several metrics, establishing a benchmark. GitHub Repo: <a target="_blank" rel="noopener" href="https://github.com/bhattarailab/Hallucination-Aware-VLM">https://github.com/bhattarailab/Hallucination-Aware-VLM</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»ç–—é¢†åŸŸè¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå®ƒä»¬å¡«è¡¥äº†åŒ»ç–—å›¾åƒå’Œä¸´åºŠè¯­è¨€ä¹‹é—´çš„ç©ºç™½ã€‚ç°æœ‰çš„VLMsè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„ç†è§£åŒ»ç–—å›¾åƒå’Œæ–‡æœ¬æŸ¥è¯¢çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆè¯¦ç»†ã€æè¿°æ€§çš„è¯Šæ–­åŒ»ç–—æŠ¥å‘Šã€‚ç„¶è€Œï¼Œå¹»è§‰â€”â€”ç”Ÿæˆä¸è§†è§‰å†…å®¹ä¸ä¸€è‡´æè¿°çš„å€¾å‘â€”â€”ä»ç„¶æ˜¯VLMsä¸­çš„ä¸€ä¸ªé‡å¤§é—®é¢˜ï¼Œåœ¨åŒ»ç–—é¢†åŸŸå…·æœ‰å°¤å…¶ä¸¥é‡çš„åæœã€‚ä¸ºäº†ä¿ƒè¿›èƒƒè‚ é“ï¼ˆGIï¼‰å›¾åƒåˆ†æå’Œå¹»è§‰ç ”ç©¶çš„VLMç ”ç©¶ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€å›¾åƒæ–‡æœ¬èƒƒè‚ é“æ•°æ®é›†ï¼šè‚ é“VLMã€‚è¯¥æ•°æ®é›†ä½¿ç”¨ä¸¤é˜¶æ®µç®¡é“åˆ›å»ºï¼šé¦–å…ˆï¼Œä½¿ç”¨ChatGPTç”ŸæˆKvasir-v2å›¾åƒçš„æè¿°æ€§åŒ»ç–—æŠ¥å‘Šï¼Œè¿™å¼•å…¥äº†ä¸€äº›å¹»è§‰æˆ–é”™è¯¯çš„æ–‡æœ¬ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒåŒ»å­¦ä¸“å®¶ç³»ç»Ÿåœ°å¯¹è¿™äº›æŠ¥å‘Šè¿›è¡Œå®¡æŸ¥ï¼Œå¹¶è¯†åˆ«å¹¶çº æ­£æ½œåœ¨çš„ä¸å‡†ç¡®ä¹‹å¤„ï¼Œä»¥ç¡®ä¿é«˜è´¨é‡ã€ä¸´åºŠå¯é çš„æ³¨é‡Šã€‚ä¸ä¼ ç»Ÿçš„ä»…åŒ…å«æè¿°æ€§æ–‡æœ¬çš„æ•°æ®é›†ä¸åŒï¼Œæˆ‘ä»¬çš„æ•°æ®é›†è¿˜åŒ…å«æ ‡è¯†å¹»è§‰å¥å­åŠå…¶ç›¸åº”ä¿®æ­£çš„æ ‡ç­¾ã€‚å‡å°‘VLMä¸­å¹»è§‰çš„ä¸€ç§å¸¸è§æ–¹æ³•æ˜¯ä½¿ç”¨å°è§„æ¨¡ã€ç‰¹å®šé—®é¢˜çš„æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸åŒçš„ç­–ç•¥ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®é›†ã€‚æˆ‘ä»¬ä¸æ˜¯ä»…é’ˆå¯¹ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Šå¯¹VLMè¿›è¡Œå¾®è°ƒï¼Œè€Œæ˜¯å¯¹å…¶è¿›è¡Œè°ƒæ•´å’Œä¿®æ­£å¹»è§‰çš„å¾®è°ƒï¼Œæˆ‘ä»¬ç§°è¿™ç§æ–¹æ³•ä¸ºâ€œå¹»è§‰æ„ŸçŸ¥å¾®è°ƒâ€ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•ä¼˜äºä»…ç”¨äºç”Ÿæˆæè¿°æ€§æŠ¥å‘Šçš„å¾®è°ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„VLMsè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå»ºç«‹äº†åŸºå‡†æµ‹è¯•ã€‚GitHubä»“åº“é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/bhattarailab/Hallucination-Aware-VLM%E3%80%82">https://github.com/bhattarailab/Hallucination-Aware-VLMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07001v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»å­¦é¢†åŸŸæ—¥ç›Šæµè¡Œçš„è·¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿåœ¨åŒ»å­¦å›¾åƒå’Œä¸´åºŠè¯­è¨€ä¹‹é—´å»ºç«‹è”ç³»ã€‚ç°æœ‰çš„VLMèƒ½å¤Ÿç”Ÿæˆè¯¦ç»†çš„è¯Šæ–­æŠ¥å‘Šï¼Œä½†å›¾åƒæè¿°ä¸­çš„å¹»è§‰é—®é¢˜ä»ç„¶ä¸¥é‡ã€‚ä¸ºäº†ç ”ç©¶èƒƒè‚ é“å›¾åƒåˆ†æå’Œå¹»è§‰é—®é¢˜ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€å›¾åƒæ–‡æœ¬èƒƒè‚ é“æ•°æ®é›†ï¼šè‚ é“è§†ç•Œæ¨¡å‹ã€‚æ­¤æ•°æ®é›†é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ç”Ÿæˆï¼Œç¬¬ä¸€é˜¶æ®µä½¿ç”¨ChatGPTç”ŸæˆKvasir-v2å›¾åƒçš„åŒ»å­¦æŠ¥å‘Šï¼Œå¯èƒ½åŒ…å«å¹»è§‰æˆ–é”™è¯¯æ–‡æœ¬ã€‚ç¬¬äºŒé˜¶æ®µç”±åŒ»å­¦ä¸“å®¶å®¡æŸ¥å¹¶çº æ­£æ½œåœ¨é”™è¯¯ï¼Œç¡®ä¿é«˜è´¨é‡çš„ä¸´åºŠå¯é æ³¨é‡Šã€‚ä¸åŒäºä»…åŒ…å«æè¿°æ€§æ–‡æœ¬çš„ä¼ ç»Ÿæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†è¿˜åŒ…å«è¯†åˆ«å¹»è§‰å¥å­çš„æ ‡ç­¾åŠå…¶ç›¸åº”æ›´æ­£ã€‚æœ¬æ–‡é‡‡ç”¨äº†ä¸€ç§ä¸åŒäºä¼ ç»Ÿå‡å°‘å¹»è§‰çš„æ–¹æ³•ï¼Œä¸æ˜¯é€šè¿‡å°è§„æ¨¡ç‰¹å®šé—®é¢˜çš„æ•°æ®é›†å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè€Œæ˜¯å¯¹å…¶è¿›è¡Œå¾®è°ƒä»¥æ£€æµ‹å’Œçº æ­£å¹»è§‰ï¼Œç§°ä¸ºå¹»è§‰æ„ŸçŸ¥å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•ä¼˜äºä»…ç”¨äºç”Ÿæˆæè¿°æ€§æŠ¥å‘Šçš„å¾®è°ƒæ–¹æ³•ã€‚æ­¤å¤–ï¼Œå¯¹æœ€å…ˆè¿›çš„VLMè¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œå»ºç«‹äº†åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œèƒ½å¤Ÿåœ¨åŒ»å­¦å›¾åƒå’Œä¸´åºŠæ–‡æœ¬ä¹‹é—´å»ºç«‹è”ç³»ã€‚</li>
<li>ç°æœ‰VLMåœ¨ç”ŸæˆåŒ»å­¦å›¾åƒæè¿°æ—¶å­˜åœ¨å¹»è§‰é—®é¢˜ï¼Œå³ç”Ÿæˆä¸å›¾åƒå†…å®¹ä¸ä¸€è‡´çš„æè¿°ã€‚</li>
<li>ä¸ºäº†ç ”ç©¶å’Œè§£å†³èƒƒè‚ é“å›¾åƒåˆ†æä¸­çš„å¹»è§‰é—®é¢˜ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€å›¾åƒæ–‡æœ¬èƒƒè‚ é“æ•°æ®é›†ï¼šè‚ é“è§†ç•Œæ¨¡å‹ã€‚</li>
<li>è¯¥æ•°æ®é›†åŒ…å«ç”±ChatGPTç”Ÿæˆçš„åŒ»å­¦æŠ¥å‘Šä»¥åŠç”±åŒ»å­¦ä¸“å®¶è¿›è¡Œçš„å®¡æŸ¥å’Œæ›´æ­£ï¼Œä»¥ç¡®ä¿é«˜è´¨é‡å’Œä¸´åºŠå¯é æ€§ã€‚</li>
<li>ä¸ä¼ ç»Ÿå‡å°‘å¹»è§‰çš„æ–¹æ³•ä¸åŒï¼Œé‡‡ç”¨å¹»è§‰æ„ŸçŸ¥å¾®è°ƒçš„æ–¹æ³•å¯¹VLMè¿›è¡Œå¾®è°ƒï¼Œä»¥æ£€æµ‹å’Œçº æ­£å¹»è§‰ã€‚</li>
<li>å¹»è§‰æ„ŸçŸ¥å¾®è°ƒæ–¹æ³•ä¼˜äºä»…ç”¨äºç”Ÿæˆæè¿°æ€§æŠ¥å‘Šçš„å¾®è°ƒæ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07001">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-50ae36d9407067e83da02b6f5cc7028b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01c92bc7e6c1bee3aecba0f2c124482f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed790da902f24164fe37c10df2b070a8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Boosting-Cross-spectral-Unsupervised-Domain-Adaptation-for-Thermal-Semantic-Segmentation"><a href="#Boosting-Cross-spectral-Unsupervised-Domain-Adaptation-for-Thermal-Semantic-Segmentation" class="headerlink" title="Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal   Semantic Segmentation"></a>Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal   Semantic Segmentation</h2><p><strong>Authors:Seokjun Kwon, Jeongmin Shin, Namil Kim, Soonmin Hwang, Yukyung Choi</strong></p>
<p>In autonomous driving, thermal image semantic segmentation has emerged as a critical research area, owing to its ability to provide robust scene understanding under adverse visual conditions. In particular, unsupervised domain adaptation (UDA) for thermal image segmentation can be an efficient solution to address the lack of labeled thermal datasets. Nevertheless, since these methods do not effectively utilize the complementary information between RGB and thermal images, they significantly decrease performance during domain adaptation. In this paper, we present a comprehensive study on cross-spectral UDA for thermal image semantic segmentation. We first propose a novel masked mutual learning strategy that promotes complementary information exchange by selectively transferring results between each spectral model while masking out uncertain regions. Additionally, we introduce a novel prototypical self-supervised loss designed to enhance the performance of the thermal segmentation model in nighttime scenarios. This approach addresses the limitations of RGB pre-trained networks, which cannot effectively transfer knowledge under low illumination due to the inherent constraints of RGB sensors. In experiments, our method achieves higher performance over previous UDA methods and comparable performance to state-of-the-art supervised methods. </p>
<blockquote>
<p>åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œçƒ­å›¾åƒè¯­ä¹‰åˆ†å‰²å·²æˆä¸ºä¸€ä¸ªå…³é”®ç ”ç©¶é¢†åŸŸï¼Œå› ä¸ºå®ƒèƒ½åœ¨æ¶åŠ£çš„è§†è§‰æ¡ä»¶ä¸‹æä¾›ç¨³å¥çš„åœºæ™¯ç†è§£ã€‚ç‰¹åˆ«æ˜¯ï¼Œç”¨äºçƒ­å›¾åƒåˆ†å‰²çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰å¯ä»¥æ˜¯æœ‰æ•ˆè§£å†³ç¼ºä¹æ ‡è®°çƒ­æ•°æ®é›†çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›æ–¹æ³•æ²¡æœ‰æœ‰æ•ˆåœ°åˆ©ç”¨RGBå’Œçƒ­å›¾åƒä¹‹é—´çš„äº’è¡¥ä¿¡æ¯ï¼Œå®ƒä»¬åœ¨åŸŸè‡ªé€‚åº”è¿‡ç¨‹ä¸­ä¼šé™ä½æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹ç”¨äºçƒ­å›¾åƒè¯­ä¹‰åˆ†å‰²çš„è·¨å…‰è°±UDAè¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ©æ¨¡äº’å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡é€‰æ‹©æ€§åœ°åœ¨å„å…‰è°±æ¨¡å‹ä¹‹é—´è½¬ç§»ç»“æœå¹¶å±è”½ä¸ç¡®å®šåŒºåŸŸï¼Œä»¥ä¿ƒè¿›äº’è¡¥ä¿¡æ¯çš„äº¤æ¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸå‹è‡ªç›‘ç£æŸå¤±ï¼Œæ—¨åœ¨æé«˜å¤œé—´åœºæ™¯ä¸­çš„çƒ­åˆ†å‰²æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•è§£å†³äº†RGBé¢„è®­ç»ƒç½‘ç»œçš„å±€é™æ€§ï¼Œç”±äºRGBä¼ æ„Ÿå™¨çš„å›ºæœ‰çº¦æŸï¼Œè¿™äº›ç½‘ç»œåœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹æ— æ³•æœ‰æ•ˆåœ°è½¬ç§»çŸ¥è¯†ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾ƒä¹‹å‰çš„UDAæ–¹æ³•å®ç°äº†æ›´é«˜çš„æ€§èƒ½ï¼Œå¹¶ä¸æœ€æ–°çš„æœ‰ç›‘ç£æ–¹æ³•è¾¾åˆ°äº†ç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06951v1">PDF</a> 7 pages, 4 figures, International Conference on Robotics and   Automation(ICRA) 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œçƒ­å›¾åƒè¯­ä¹‰åˆ†å‰²å·²æˆä¸ºä¸€é¡¹é‡è¦çš„ç ”ç©¶é¢†åŸŸï¼Œå› å…¶èƒ½å¤Ÿåœ¨æ¶åŠ£çš„è§†è§‰æ¡ä»¶ä¸‹æä¾›ç¨³å¥çš„åœºæ™¯ç†è§£ã€‚é’ˆå¯¹çƒ­å›¾åƒåˆ†å‰²çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰æ–¹æ³•å¯ä»¥æœ‰æ•ˆè§£å†³æ ‡è®°çƒ­æ•°æ®é›†ç¼ºä¹çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨RGBå’Œçƒ­å›¾åƒä¹‹é—´çš„äº’è¡¥ä¿¡æ¯ï¼Œå®ƒä»¬åœ¨åŸŸè‡ªé€‚åº”è¿‡ç¨‹ä¸­çš„æ€§èƒ½æœ‰æ‰€é™ä½ã€‚æœ¬æ–‡å…¨é¢ç ”ç©¶äº†ç”¨äºçƒ­å›¾åƒè¯­ä¹‰åˆ†å‰²çš„è·¨å…‰è°±UDAã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ©æ¨¡äº’å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡é€‰æ‹©æ€§åœ°åœ¨å„å…‰è°±æ¨¡å‹ä¹‹é—´è½¬ç§»ç»“æœå¹¶æ©ç›–ä¸ç¡®å®šåŒºåŸŸï¼Œä»¥ä¿ƒè¿›äº’è¡¥ä¿¡æ¯çš„äº¤æ¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸå‹è‡ªç›‘ç£æŸå¤±ï¼Œæ—¨åœ¨æé«˜å¤œé—´åœºæ™¯ä¸­çš„çƒ­åˆ†å‰²æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•è§£å†³äº†RGBé¢„è®­ç»ƒç½‘ç»œåœ¨å¤œé—´æ— æ³•æœ‰æ•ˆè½¬ç§»çŸ¥è¯†çš„å±€é™æ€§ï¼Œè¿™æ˜¯ç”±äºRGBä¼ æ„Ÿå™¨çš„å›ºæœ‰çº¦æŸé€ æˆçš„ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºä¹‹å‰çš„UDAæ–¹æ³•è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ï¼Œå¹¶ä¸æœ€æ–°çš„ç›‘ç£æ–¹æ³•ç›¸å½“ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çƒ­å›¾åƒè¯­ä¹‰åˆ†å‰²åœ¨è‡ªåŠ¨é©¾é©¶ä¸­æ˜¯å…³é”®ç ”ç©¶é¢†åŸŸï¼Œèƒ½åœ¨æ¶åŠ£è§†è§‰æ¡ä»¶ä¸‹æä¾›ç¨³å¥çš„åœºæ™¯ç†è§£ã€‚</li>
<li>æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰æ˜¯è§£å†³çƒ­å›¾åƒåˆ†å‰²ä¸­æ ‡è®°æ•°æ®é›†ç¼ºä¹çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ç°æœ‰UDAæ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨RGBå’Œçƒ­å›¾åƒä¹‹é—´çš„äº’è¡¥ä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æå‡ºäº†æ©æ¨¡äº’å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡é€‰æ‹©æ€§è½¬ç§»ç»“æœå¹¶æ©ç›–ä¸ç¡®å®šåŒºåŸŸï¼Œä¿ƒè¿›äº’è¡¥ä¿¡æ¯äº¤æ¢ã€‚</li>
<li>å¼•å…¥åŸå‹è‡ªç›‘ç£æŸå¤±ï¼Œæé«˜å¤œé—´åœºæ™¯ä¸­çš„çƒ­åˆ†å‰²æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•è§£å†³äº†RGBé¢„è®­ç»ƒç½‘ç»œåœ¨å¤œé—´æ— æ³•æœ‰æ•ˆè½¬ç§»çŸ¥è¯†çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06951">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c61a03816928928670a3c617431cdd44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4152adef618403d9bb552a63b4f9ec8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad6db9e16c0a148ad46ec3436ab7a2d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70df984500050810f0a775e6d9eb074f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-133ca95420ba96f3843c340f5e083168.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b14546b5c65987835a709b175cb0ff17.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Improving-Generalization-of-Medical-Image-Registration-Foundation-Model"><a href="#Improving-Generalization-of-Medical-Image-Registration-Foundation-Model" class="headerlink" title="Improving Generalization of Medical Image Registration Foundation Model"></a>Improving Generalization of Medical Image Registration Foundation Model</h2><p><strong>Authors:Jing Hu, Kaiwei Yu, Hongjiang Xian, Shu Hu, Xin Wang</strong></p>
<p>Deformable registration is a fundamental task in medical image processing, aiming to achieve precise alignment by establishing nonlinear correspondences between images. Traditional methods offer good adaptability and interpretability but are limited by computational efficiency. Although deep learning approaches have significantly improved registration speed and accuracy, they often lack flexibility and generalizability across different datasets and tasks. In recent years, foundation models have emerged as a promising direction, leveraging large and diverse datasets to learn universal features and transformation patterns for image registration, thus demonstrating strong cross-task transferability. However, these models still face challenges in generalization and robustness when encountering novel anatomical structures, varying imaging conditions, or unseen modalities. To address these limitations, this paper incorporates Sharpness-Aware Minimization (SAM) into foundation models to enhance their generalization and robustness in medical image registration. By optimizing the flatness of the loss landscape, SAM improves model stability across diverse data distributions and strengthens its ability to handle complex clinical scenarios. Experimental results show that foundation models integrated with SAM achieve significant improvements in cross-dataset registration performance, offering new insights for the advancement of medical image registration technology. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Promise13/fm_sam%7D%7Bhttps://github.com/Promise13/fm/_sam">https://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\_sam</a>. </p>
<blockquote>
<p>å¯å˜å½¢çš„å›¾åƒé…å‡†æ˜¯ä¸€ä¸ªåŸºæœ¬çš„åŒ»å­¦å›¾åƒå¤„ç†ä»»åŠ¡ï¼Œç›®çš„æ˜¯é€šè¿‡åœ¨ä¸åŒçš„å›¾åƒä¹‹é—´å»ºç«‹éçº¿æ€§å¯¹åº”å…³ç³»æ¥å®ç°ç²¾ç¡®çš„é…å‡†ã€‚ä¼ ç»Ÿçš„æ–¹æ³•å…·æœ‰è‰¯å¥½çš„é€‚åº”æ€§å’Œå¯è§£é‡Šæ€§ï¼Œä½†åœ¨è®¡ç®—æ•ˆç‡ä¸Šæœ‰æ‰€å±€é™ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ æ–¹æ³•å¤§å¤§æé«˜äº†é…å‡†çš„é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬å¾€å¾€åœ¨è·¨è¶Šä¸åŒçš„æ•°æ®é›†å’Œä»»åŠ¡æ—¶ç¼ºä¹çµæ´»æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿‘å¹´æ¥ï¼ŒåŸºç¡€æ¨¡å‹ä½œä¸ºä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘å´­éœ²å¤´è§’ï¼Œå®ƒä»¬åˆ©ç”¨å¤§è§„æ¨¡å’Œå¤šæ ·åŒ–çš„æ•°æ®é›†æ¥å­¦ä¹ å›¾åƒé…å‡†ä¸­çš„é€šç”¨ç‰¹å¾å’Œè½¬æ¢æ¨¡å¼ï¼Œä»è€Œè¡¨ç°å‡ºå¼ºå¤§çš„è·¨ä»»åŠ¡è¿ç§»èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“é‡åˆ°æ–°çš„è§£å‰–ç»“æ„ã€ä¸åŒçš„æˆåƒæ¡ä»¶æˆ–æœªè§çš„æ¨¡æ€æ—¶ï¼Œè¿™äº›æ¨¡å‹åœ¨æ³›åŒ–å’Œç¨³å¥æ€§æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†Sharpness-Aware Minimizationï¼ˆSAMï¼‰æŠ€æœ¯åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼Œä»¥å¢å¼ºå…¶åœ¨åŒ»å­¦å›¾åƒé…å‡†ä¸­çš„æ³›åŒ–å’Œç¨³å¥æ€§ã€‚é€šè¿‡ä¼˜åŒ–æŸå¤±æ™¯è§‚çš„å°–é”åº¦ï¼ŒSAMæé«˜äº†æ¨¡å‹åœ¨ä¸åŒæ•°æ®åˆ†å¸ƒä¸Šçš„ç¨³å®šæ€§ï¼Œå¹¶åŠ å¼ºäº†å…¶å¤„ç†å¤æ‚ä¸´åºŠåœºæ™¯çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé›†æˆSAMçš„åŸºç¡€æ¨¡å‹åœ¨è·¨æ•°æ®é›†é…å‡†æ€§èƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä¸ºåŒ»å­¦å›¾åƒé…å‡†æŠ€æœ¯çš„å‘å±•æä¾›äº†æ–°çš„è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Promise13/fm_sam%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Promise13/fm_samè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06527v1">PDF</a> IJCNN</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŒ»å­¦å›¾åƒå¤„ç†ä¸­çš„å¯å˜å½¢æ³¨å†Œé—®é¢˜ï¼Œä»‹ç»äº†ä¼ ç»Ÿæ–¹æ³•å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è¯¥é¢†åŸŸçš„å±€é™å’ŒæŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæ–‡ç« å°†Sharpness-Aware Minimizationï¼ˆSAMï¼‰æŠ€æœ¯å¼•å…¥åŸºç¡€æ¨¡å‹ï¼Œæé«˜äº†æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒæ³¨å†Œä¸­çš„æ³›åŒ–å’Œç¨³å¥æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“åˆSAMçš„åŸºç¡€æ¨¡å‹åœ¨è·¨æ•°æ®é›†æ³¨å†Œæ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯å˜å½¢æ³¨å†Œæ˜¯åŒ»å­¦å›¾åƒå¤„ç†ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡å›¾åƒé—´çš„éçº¿æ€§å¯¹åº”å…³ç³»å®ç°ç²¾ç¡®å¯¹é½ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å…·æœ‰è‰¯å¥½çš„é€‚åº”æ€§å’Œå¯è§£é‡Šæ€§ï¼Œä½†è®¡ç®—æ•ˆç‡æœ‰é™ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æ³¨å†Œé€Ÿåº¦å’Œå‡†ç¡®æ€§å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œä½†ç¼ºä¹çµæ´»æ€§å’Œè·¨æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åŸºç¡€æ¨¡å‹åˆ©ç”¨å¤§è§„æ¨¡å¤šæ ·æ•°æ®é›†å­¦ä¹ å›¾åƒæ³¨å†Œçš„é€šç”¨ç‰¹å¾å’Œè½¬æ¢æ¨¡å¼ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„è·¨ä»»åŠ¡è¿ç§»èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥Sharpness-Aware Minimizationï¼ˆSAMï¼‰æŠ€æœ¯å¢å¼ºåŸºç¡€æ¨¡å‹çš„æ³›åŒ–å’Œç¨³å¥æ€§ã€‚</li>
<li>SAMé€šè¿‡ä¼˜åŒ–æŸå¤±æ™¯è§‚çš„å¹³å¦åº¦ï¼Œæé«˜äº†æ¨¡å‹åœ¨å¤šæ ·åŒ–æ•°æ®åˆ†å¸ƒä¸‹çš„ç¨³å®šæ€§ï¼Œå¹¶å¢å¼ºäº†å¤„ç†å¤æ‚ä¸´åºŠåœºæ™¯çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3de6c26c04033b0bc61c8acacbf9b20c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4eb7169d373f0871c32782e10079d5d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f40042672261d6987759a190c3647eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4023ed6269cf1d3a4320beeca4a72b06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-990674037a6ff611ee46bb11be9a7dd3.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Text-to-CadQuery-A-New-Paradigm-for-CAD-Generation-with-Scalable-Large-Model-Capabilities"><a href="#Text-to-CadQuery-A-New-Paradigm-for-CAD-Generation-with-Scalable-Large-Model-Capabilities" class="headerlink" title="Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large   Model Capabilities"></a>Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large   Model Capabilities</h2><p><strong>Authors:Haoyang Xie, Feng Ju</strong></p>
<p>Computer-aided design (CAD) is fundamental to modern engineering and manufacturing, but creating CAD models still requires expert knowledge and specialized software. Recent advances in large language models (LLMs) open up the possibility of generative CAD, where natural language is directly translated into parametric 3D models. However, most existing methods generate task-specific command sequences that pretrained models cannot directly handle. These sequences must be converted into CAD representations such as CAD vectors before a 3D model can be produced, which requires training models from scratch and adds unnecessary complexity. To tackle this issue, we propose generating CadQuery code directly from text, leveraging the strengths of pretrained LLMs to produce 3D models without intermediate representations, using this Python-based scripting language. Since LLMs already excel at Python generation and spatial reasoning, fine-tuning them on Text-to-CadQuery data proves highly effective. Given that these capabilities typically improve with scale, we hypothesize that larger models will perform better after fine-tuning. To enable this, we augment the Text2CAD dataset with 170,000 CadQuery annotations. We fine-tune six open-source LLMs of varying sizes and observe consistent improvements. Our best model achieves a top-1 exact match of 69.3%, up from 58.8%, and reduces Chamfer Distance by 48.6%. Project page: <a target="_blank" rel="noopener" href="https://github.com/Text-to-CadQuery/Text-to-CadQuery">https://github.com/Text-to-CadQuery/Text-to-CadQuery</a>. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ˜¯ç°ä»£å·¥ç¨‹å’Œåˆ¶é€ ä¸šçš„åŸºç¡€ï¼Œä½†åˆ›å»ºCADæ¨¡å‹ä»ç„¶éœ€è¦ä¸“ä¸šçŸ¥è¯†å’Œä¸“ç”¨è½¯ä»¶ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºç”Ÿæˆå¼CADæä¾›äº†å¯èƒ½æ€§ï¼Œå…¶ä¸­è‡ªç„¶è¯­è¨€ç›´æ¥ç¿»è¯‘ä¸ºå‚æ•°åŒ–3Dæ¨¡å‹ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ç”Ÿæˆç‰¹å®šä»»åŠ¡çš„å‘½ä»¤åºåˆ—ï¼Œè¿™äº›åºåˆ—ä¸èƒ½ç›´æ¥ç”±é¢„è®­ç»ƒæ¨¡å‹å¤„ç†ã€‚è¿™äº›åºåˆ—å¿…é¡»è½¬æ¢ä¸ºCADè¡¨ç¤ºï¼ˆå¦‚CADçŸ¢é‡ï¼‰ï¼Œç„¶åæ‰èƒ½ç”Ÿæˆ3Dæ¨¡å‹ï¼Œè¿™éœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œå¢åŠ äº†ä¸å¿…è¦çš„å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºç›´æ¥ä»æ–‡æœ¬ç”ŸæˆCadQueryä»£ç ï¼Œåˆ©ç”¨é¢„è®­ç»ƒLLMçš„ä¼˜åŠ¿ï¼Œé€šè¿‡åŸºäºPythonçš„è„šæœ¬è¯­è¨€ç”Ÿæˆ3Dæ¨¡å‹ï¼Œæ— éœ€ä¸­é—´è¡¨ç¤ºã€‚ç”±äºLLMå·²ç»åœ¨Pythonç”Ÿæˆå’Œç©ºé—´æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå› æ­¤åœ¨Text-to-CadQueryæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒè¯æ˜æ˜¯éå¸¸æœ‰æ•ˆçš„ã€‚è€ƒè™‘åˆ°è¿™äº›èƒ½åŠ›é€šå¸¸éšç€è§„æ¨¡è€Œæé«˜ï¼Œæˆ‘ä»¬å‡è®¾æ›´å¤§çš„æ¨¡å‹åœ¨å¾®è°ƒåä¼šæœ‰æ›´å¥½çš„è¡¨ç°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¢åŠ äº†17ä¸‡ä¸ªCadQueryæ³¨é‡Šæ¥å¢å¼ºText2CADæ•°æ®é›†ã€‚æˆ‘ä»¬å¾®è°ƒäº†å…­ä¸ªä¸åŒå¤§å°çš„å¼€æºLLMï¼Œå¹¶è§‚å¯Ÿåˆ°äº†ä¸€è‡´çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹å°†top-1ç²¾ç¡®åŒ¹é…åº¦ä»58.8%æé«˜åˆ°69.3%ï¼Œå¹¶å‡å°‘äº†Chamferè·ç¦»48.6%ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/Text-to-CadQuery/Text-to-CadQuery%E3%80%82">https://github.com/Text-to-CadQuery/Text-to-CadQueryã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06507v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨ç°ä»£å·¥ç¨‹å’Œåˆ¶é€ ä¸­çš„é‡è¦æ€§ï¼Œç ”ç©¶å›¢é˜Ÿå°è¯•å°†è‡ªç„¶è¯­è¨€ç›´æ¥è½¬åŒ–ä¸ºå‚æ•°åŒ–ä¸‰ç»´æ¨¡å‹ï¼Œå®ç°ç”Ÿæˆå¼CADã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éœ€å°†è‡ªç„¶è¯­è¨€è½¬åŒ–ä¸ºç‰¹å®šä»»åŠ¡å‘½ä»¤åºåˆ—ï¼Œå†è½¬æ¢ä¸ºCADè¡¨ç¤ºå½¢å¼ï¼Œå¦‚CADçŸ¢é‡ï¼Œæ‰èƒ½ç”Ÿæˆä¸‰ç»´æ¨¡å‹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºé€šè¿‡æ–‡æœ¬ç›´æ¥ç”ŸæˆCadQueryä»£ç çš„æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹äº§ç”Ÿä¸‰ç»´æ¨¡å‹ï¼Œè·³è¿‡ä¸­é—´è¡¨ç¤ºå½¢å¼ã€‚ç»è¿‡åœ¨Text-to-CadQueryæ•°æ®é›†ä¸Šçš„å¾®è°ƒï¼Œå¤§å‹æ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨ç°ä»£å·¥ç¨‹åˆ¶é€ ä¸­ä»éœ€è¦ä¸“ä¸šçŸ¥è¯†å’Œä¸“é—¨è½¯ä»¶ã€‚</li>
<li>è‡ªç„¶è¯­è¨€ç›´æ¥è½¬åŒ–ä¸ºå‚æ•°åŒ–ä¸‰ç»´æ¨¡å‹æ˜¯ç”Ÿæˆå¼CADçš„æ–°è¶‹åŠ¿ã€‚</li>
<li>å½“å‰æ–¹æ³•éœ€å°†è‡ªç„¶è¯­è¨€è½¬åŒ–ä¸ºä»»åŠ¡ç‰¹å®šå‘½ä»¤åºåˆ—å†ç”ŸæˆCADæ¨¡å‹ï¼Œè¿™å¢åŠ äº†å¤æ‚æ€§å’Œæ—¶é—´æˆæœ¬ã€‚</li>
<li>é€šè¿‡æ–‡æœ¬ç›´æ¥ç”ŸæˆCadQueryä»£ç å¯è·³è¿‡ä¸­é—´è¡¨ç¤ºå½¢å¼ï¼Œç®€åŒ–æµç¨‹ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒå¯æœ‰æ•ˆæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é¡¹ç›®é€šè¿‡ä½¿ç”¨æ›´å¤§è§„æ¨¡æ•°æ®é›†æå‡äº†æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5684077e0af79fc7166121b4eea84bdd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46b4243164a362b4ce3aafe16e588efa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fceafcb5ea1a0ef001ff81c4cd4933d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0dd21c3f41db8f71ac8f16270e9080f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3b1a6718586430c809f06a1d9323fb3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Robust-Precise-Knowledge-Distillation-based-Novel-Context-Aware-Predictor-for-Disease-Detection-in-Brain-and-Gastrointestinal"><a href="#Robust-Precise-Knowledge-Distillation-based-Novel-Context-Aware-Predictor-for-Disease-Detection-in-Brain-and-Gastrointestinal" class="headerlink" title="Robust &amp; Precise Knowledge Distillation-based Novel Context-Aware   Predictor for Disease Detection in Brain and Gastrointestinal"></a>Robust &amp; Precise Knowledge Distillation-based Novel Context-Aware   Predictor for Disease Detection in Brain and Gastrointestinal</h2><p><strong>Authors:Saif Ur Rehman Khan, Muhammad Nabeel Asim, Sebastian Vollmer, Andreas Dengel</strong></p>
<p>Medical disease prediction, particularly through imaging, remains a challenging task due to the complexity and variability of medical data, including noise, ambiguity, and differing image quality. Recent deep learning models, including Knowledge Distillation (KD) methods, have shown promising results in brain tumor image identification but still face limitations in handling uncertainty and generalizing across diverse medical conditions. Traditional KD methods often rely on a context-unaware temperature parameter to soften teacher model predictions, which does not adapt effectively to varying uncertainty levels present in medical images. To address this issue, we propose a novel framework that integrates Ant Colony Optimization (ACO) for optimal teacher-student model selection and a novel context-aware predictor approach for temperature scaling. The proposed context-aware framework adjusts the temperature based on factors such as image quality, disease complexity, and teacher model confidence, allowing for more robust knowledge transfer. Additionally, ACO efficiently selects the most appropriate teacher-student model pair from a set of pre-trained models, outperforming current optimization methods by exploring a broader solution space and better handling complex, non-linear relationships within the data. The proposed framework is evaluated using three publicly available benchmark datasets, each corresponding to a distinct medical imaging task. The results demonstrate that the proposed framework significantly outperforms current state-of-the-art methods, achieving top accuracy rates: 98.01% on the MRI brain tumor (Kaggle) dataset, 92.81% on the Figshare MRI dataset, and 96.20% on the GastroNet dataset. This enhanced performance is further evidenced by the improved results, surpassing existing benchmarks of 97.24% (Kaggle), 91.43% (Figshare), and 95.00% (GastroNet). </p>
<blockquote>
<p>åŒ»ç–—ç–¾ç—…é¢„æµ‹ï¼Œå°¤å…¶æ˜¯é€šè¿‡æˆåƒè¿›è¡Œé¢„æµ‹ï¼Œä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºåŒ»ç–—æ•°æ®å…·æœ‰å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼ŒåŒ…æ‹¬å™ªå£°ã€æ¨¡ç³Šå’Œä¸åŒå›¾åƒè´¨é‡ç­‰é—®é¢˜ã€‚æœ€è¿‘çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æ–¹æ³•ï¼Œåœ¨è„‘è‚¿ç˜¤å›¾åƒè¯†åˆ«æ–¹é¢å·²æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„ç»“æœï¼Œä½†åœ¨å¤„ç†ä¸ç¡®å®šæ€§å’Œè·¨ä¸åŒåŒ»ç–—æ¡ä»¶æ¨å¹¿æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚ä¼ ç»ŸçŸ¥è¯†è’¸é¦æ–¹æ³•é€šå¸¸ä¾èµ–äºä¸Šä¸‹æ–‡æ— å…³çš„æ¸©åº¦å‚æ•°æ¥è½¯åŒ–æ•™å¸ˆæ¨¡å‹çš„é¢„æµ‹ï¼Œè¿™ä¸èƒ½æœ‰æ•ˆåœ°é€‚åº”åŒ»å­¦å›¾åƒä¸­å­˜åœ¨çš„ä¸åŒä¸ç¡®å®šæ€§æ°´å¹³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œèåˆäº†èšç¾¤ä¼˜åŒ–ï¼ˆACOï¼‰æ¥è¿›è¡Œæœ€ä½³æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹é€‰æ‹©ä»¥åŠä¸€ç§æ–°å‹ä¸Šä¸‹æ–‡æ„ŸçŸ¥é¢„æµ‹æ–¹æ³•è¿›è¡Œæ¸©åº¦æ ‡å®šã€‚æ‰€æå‡ºçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¡†æ¶æ ¹æ®å›¾åƒè´¨é‡ã€ç–¾ç—…å¤æ‚æ€§å’Œæ•™å¸ˆæ¨¡å‹ä¿¡å¿ƒç­‰å› ç´ è°ƒæ•´æ¸©åº¦ï¼Œä»è€Œå®ç°æ›´ç¨³å¥çš„çŸ¥è¯†è½¬ç§»ã€‚æ­¤å¤–ï¼Œèšç¾¤ä¼˜åŒ–èƒ½å¤Ÿé«˜æ•ˆåœ°ä»ä¸€ç»„é¢„è®­ç»ƒæ¨¡å‹ä¸­é€‰æ‹©æœ€åˆé€‚çš„æ•™å­¦æ¨¡å‹å¯¹å­¦ç”Ÿæ¨¡å‹è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œé€šè¿‡æ¢ç´¢æ›´å¹¿æ³›çš„è§£å†³æ–¹æ¡ˆç©ºé—´å’Œæ›´å¥½åœ°å¤„ç†æ•°æ®ä¸­çš„å¤æ‚éçº¿æ€§å…³ç³»ï¼Œè¶…è¶Šäº†å½“å‰ä¼˜åŒ–æ–¹æ³•ã€‚è¯¥æ¡†æ¶ä½¿ç”¨ä¸‰ä¸ªå…¬å¼€å¯ç”¨çš„åŸºå‡†æ•°æ®é›†è¿›è¡Œäº†è¯„ä¼°ï¼Œæ¯ä¸ªæ•°æ®é›†éƒ½å¯¹åº”ä¸€ä¸ªç‹¬ç‰¹çš„åŒ»å­¦æˆåƒä»»åŠ¡ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶æ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨MRIè„‘è‚¿ç˜¤ï¼ˆKaggleï¼‰æ•°æ®é›†ä¸Šè¾¾åˆ°98.01%çš„å‡†ç¡®ç‡ï¼Œåœ¨Figshare MRIæ•°æ®é›†ä¸Šè¾¾åˆ°92.8r%çš„å‡†ç¡®ç‡ï¼Œä»¥åŠåœ¨GastroNetæ•°æ®é›†ä¸Šè¾¾åˆ°96.20%çš„å‡†ç¡®ç‡ã€‚è¿™ä¸€æ”¹è¿›çš„æ€§èƒ½è¿›ä¸€æ­¥ä½“ç°åœ¨è¶…è¶Šç°æœ‰åŸºå‡†çš„ç»“æœä¸­ï¼Œåˆ†åˆ«ä¸ºKaggleçš„97.24%ã€Figshareçš„91.43%å’ŒGastroNetçš„95.00%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06381v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦ç–¾ç—…é¢„æµ‹ï¼Œå°¤å…¶æ˜¯é€šè¿‡åŒ»å­¦å½±åƒé¢„æµ‹ï¼Œå› åŒ»å­¦æ•°æ®å¤æ‚æ€§åŠå¤šå˜æ€§ï¼ˆå¦‚å™ªå£°ã€æ¨¡ç³Šã€å›¾åƒè´¨é‡ä¸ä¸€ç­‰ï¼‰è€Œå……æ»¡æŒ‘æˆ˜ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¦‚çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æ–¹æ³•ï¼Œåœ¨è„‘è‚¿ç˜¤å›¾åƒè¯†åˆ«æ–¹é¢å±•ç°å‡ºè‰¯å¥½å‰æ™¯ï¼Œä½†åœ¨å¤„ç†ä¸ç¡®å®šæ€§å’Œè·¨ä¸åŒåŒ»å­¦çŠ¶å†µæ¨å¹¿æ—¶ä»æœ‰é™åˆ¶ã€‚é’ˆå¯¹ä¼ ç»ŸKDæ–¹æ³•åœ¨å¤„ç†åŒ»å­¦å›¾åƒä¸­çš„ä¸åŒä¸ç¡®å®šæ€§æ—¶ä¸å¤Ÿçµæ´»çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§ç»“åˆèšç¾¤ä¼˜åŒ–ï¼ˆACOï¼‰è¿›è¡Œæœ€ä¼˜æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹é€‰æ‹©å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥é¢„æµ‹å™¨è¿›è¡Œæ¸©åº¦è°ƒèŠ‚çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ ¹æ®å›¾åƒè´¨é‡ã€ç–¾ç—…å¤æ‚æ€§å’Œæ•™å¸ˆæ¨¡å‹ä¿¡å¿ƒç­‰å› ç´ è°ƒæ•´æ¸©åº¦ï¼Œå®ç°æ›´ç¨³å¥çš„çŸ¥è¯†è½¬ç§»ã€‚åŒæ—¶ï¼ŒACOèƒ½ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­é«˜æ•ˆé€‰æ‹©æœ€åˆé€‚çš„æ•™å­¦æ¨¡å‹ï¼Œåœ¨æ•°æ®å¤„ç†æ–¹é¢è¡¨ç°æ›´ä¼˜ç§€ã€‚æ–°æ¡†æ¶åœ¨ä¸‰ä¸ªå…¬å¼€åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå…¶æ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†Kaggleçš„MRIè„‘è‚¿ç˜¤æ•°æ®é›†çš„98.01%ã€Figshareçš„MRIæ•°æ®é›†çš„92.81%ï¼Œä»¥åŠGastroNetæ•°æ®é›†çš„96.20%ã€‚ä¸åŸåŸºå‡†ç›¸æ¯”æœ‰æ‰€æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦ç–¾ç—…é¢„æµ‹é€šè¿‡åŒ»å­¦å½±åƒå­˜åœ¨å¤æ‚æ€§åŠæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®å™ªå£°ã€æ¨¡ç³Šå’Œå›¾åƒè´¨é‡ä¸ä¸€ç­‰é—®é¢˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¦‚çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æ–¹æ³•ï¼Œåœ¨è„‘è‚¿ç˜¤å›¾åƒè¯†åˆ«æ–¹é¢æœ‰æ‰€æˆå°±ï¼Œä½†å¤„ç†ä¸ç¡®å®šæ€§å’Œè·¨ä¸åŒåŒ»å­¦çŠ¶å†µæ¨å¹¿æ—¶å­˜åœ¨å±€é™ã€‚</li>
<li>ä¼ ç»ŸKDæ–¹æ³•åœ¨å¤„ç†åŒ»å­¦å›¾åƒä¸­çš„ä¸åŒä¸ç¡®å®šæ€§æ—¶ç¼ºä¹çµæ´»æ€§ã€‚</li>
<li>æå‡ºçš„ç»“åˆèšç¾¤ä¼˜åŒ–ï¼ˆACOï¼‰å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥é¢„æµ‹å™¨çš„æ–°æ¡†æ¶èƒ½å¤Ÿæ›´ç¨³å¥åœ°è¿›è¡ŒçŸ¥è¯†è½¬ç§»ï¼Œå¹¶æ ¹æ®å›¾åƒè´¨é‡ã€ç–¾ç—…å¤æ‚æ€§å’Œæ•™å¸ˆæ¨¡å‹ä¿¡å¿ƒè°ƒæ•´æ¸©åº¦ã€‚</li>
<li>ACOèƒ½å¤Ÿé«˜æ•ˆé€‰æ‹©æœ€ä¼˜æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹ï¼Œåœ¨æ•°æ®å¤„ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ–°æ¡†æ¶åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>æ–°æ¡†æ¶çš„å‡†ç¡®ç‡åœ¨Kaggleçš„MRIè„‘è‚¿ç˜¤æ•°æ®é›†ä¸Šè¾¾åˆ°98.01%ï¼Œåœ¨Figshareçš„MRIæ•°æ®é›†ä¸Šè¾¾åˆ°92.81%ï¼Œåœ¨GastroNetæ•°æ®é›†ä¸Šè¾¾åˆ°96.2%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06381">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-79960c1d498efc76f05af4d79a693c68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f375f2ac9b1785dce2c8c92b5cf9f054.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Noise-Consistent-Siamese-Diffusion-for-Medical-Image-Synthesis-and-Segmentation"><a href="#Noise-Consistent-Siamese-Diffusion-for-Medical-Image-Synthesis-and-Segmentation" class="headerlink" title="Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and   Segmentation"></a>Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and   Segmentation</h2><p><strong>Authors:Kunpeng Qiu, Zhiqiang Gao, Zhiying Zhou, Mingjie Sun, Yongxin Guo</strong></p>
<p>Deep learning has revolutionized medical image segmentation, yet its full potential remains constrained by the paucity of annotated datasets. While diffusion models have emerged as a promising approach for generating synthetic image-mask pairs to augment these datasets, they paradoxically suffer from the same data scarcity challenges they aim to mitigate. Traditional mask-only models frequently yield low-fidelity images due to their inability to adequately capture morphological intricacies, which can critically compromise the robustness and reliability of segmentation models. To alleviate this limitation, we introduce Siamese-Diffusion, a novel dual-component model comprising Mask-Diffusion and Image-Diffusion. During training, a Noise Consistency Loss is introduced between these components to enhance the morphological fidelity of Mask-Diffusion in the parameter space. During sampling, only Mask-Diffusion is used, ensuring diversity and scalability. Comprehensive experiments demonstrate the superiority of our method. Siamese-Diffusion boosts SANetâ€™s mDice and mIoU by 3.6% and 4.4% on the Polyps, while UNet improves by 1.52% and 1.64% on the ISIC2018. Code is available at GitHub. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å·²ç»å½»åº•æ”¹å˜äº†åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸï¼Œä½†å…¶æ½œåŠ›ä»ç„¶å—åˆ°æ ‡æ³¨æ•°æ®é›†ç¼ºä¹çš„é™åˆ¶ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹å·²æˆä¸ºç”Ÿæˆåˆæˆå›¾åƒ-æ©è†œå¯¹ä»¥å¢å¼ºè¿™äº›æ•°æ®é›†çš„å¾ˆæœ‰å‰é€”çš„æ–¹æ³•ï¼Œä½†å®ƒä»¬å´é¢ä¸´ç€è‡ªèº«æƒ³è¦ç¼“è§£çš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ä»…æ©è†œæ¨¡å‹ç»å¸¸äº§ç”Ÿä½ä¿çœŸåº¦çš„å›¾åƒï¼Œå› ä¸ºå®ƒä»¬æ— æ³•å……åˆ†æ•æ‰å½¢æ€ç»†èŠ‚ï¼Œè¿™å¯èƒ½ä¼šä¸¥é‡æŸå®³åˆ†å‰²æ¨¡å‹çš„ç¨³å¥æ€§å’Œå¯é æ€§ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†Siamese-Diffusionï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŒç»„åˆ†æ¨¡å‹ï¼ŒåŒ…æ‹¬Mask-Diffusionå’ŒImage-Diffusionã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåœ¨è¿™ä¸¤ä¸ªç»„ä»¶ä¹‹é—´å¼•å…¥äº†å™ªå£°ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥å¢å¼ºMask-Diffusionåœ¨å‚æ•°ç©ºé—´ä¸­çš„å½¢æ€ä¿çœŸåº¦ã€‚åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œåªä½¿ç”¨Mask-Diffusionï¼Œä»¥ç¡®ä¿å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚ç»¼åˆå®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚Siamese-Diffusionæé«˜äº†Polypsä¸Šçš„SANetçš„mDiceå’ŒmIoUåˆ†åˆ«ä¸º3.6%å’Œ4.4%ï¼Œè€ŒUNetåœ¨ISIC2018ä¸Šçš„æ”¹è¿›åˆ†åˆ«ä¸º1.52%å’Œ1.64%ã€‚ä»£ç å·²åœ¨GitHubä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06068v1">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå¼•å‘äº†é©å‘½æ€§çš„å˜é©ï¼Œä½†å—é™äºæ ‡æ³¨æ•°æ®é›†çš„åŒ®ä¹ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆåˆæˆå›¾åƒ-æ©è†œå¯¹ä»¥æ‰©å……æ•°æ®é›†æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å®ƒä»¬åŒæ ·é¢ä¸´æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿä»…ä½¿ç”¨æ©è†œæ¨¡å‹å› æ— æ³•å……åˆ†æ•æ‰å½¢æ€ç»†èŠ‚ï¼Œå¸¸å¯¼è‡´å›¾åƒä¿çœŸåº¦ä½ï¼Œä»è€Œä¸¥é‡å½±å“åˆ†å‰²æ¨¡å‹çš„ç¨³å¥æ€§å’Œå¯é æ€§ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºSiamese-Diffusionï¼Œä¸€ç§åŒ…å«Mask-Diffusionå’ŒImage-Diffusionçš„åŒç»„åˆ†æ–°å‹æ¨¡å‹ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¼•å…¥å™ªå£°ä¸€è‡´æ€§æŸå¤±ï¼Œå¢å¼ºMask-Diffusionçš„å½¢æ€ä¿çœŸåº¦ã€‚é‡‡æ ·è¿‡ç¨‹ä¸­ä»…ä½¿ç”¨Mask-Diffusionï¼Œç¡®ä¿å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ï¼ŒSiamese-Diffusionæå‡Polypsä¸Šçš„SANetçš„mDiceå’ŒmIoUåˆ†åˆ«ä¸º3.6%å’Œ4.4%ï¼ŒISIC2018ä¸Šçš„UNetåˆ†åˆ«æå‡1.52%å’Œ1.64%ã€‚ä»£ç å·²ä¸Šä¼ è‡³GitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨å—åˆ°æ ‡æ³¨æ•°æ®é›†ç¼ºä¹çš„é™åˆ¶ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§ç”Ÿæˆåˆæˆå›¾åƒ-æ©è†œå¯¹çš„æ½œåŠ›æ–¹æ³•ï¼Œç”¨äºæ‰©å……æ•°æ®é›†ï¼Œä½†ä¹Ÿé¢ä¸´æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿä»…ä½¿ç”¨æ©è†œæ¨¡å‹çš„å›¾åƒä¿çœŸåº¦ä½ï¼Œå› æ— æ³•å……åˆ†æ•æ‰å½¢æ€ç»†èŠ‚ï¼Œå½±å“åˆ†å‰²æ¨¡å‹çš„ç¨³å¥æ€§å’Œå¯é æ€§ã€‚</li>
<li>æå‡ºçš„Siamese-Diffusionæ¨¡å‹åŒ…å«Mask-Diffusionå’ŒImage-DiffusionåŒç»„åˆ†ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥å™ªå£°ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥æé«˜Mask-Diffusionçš„å½¢æ€ä¿çœŸåº¦ã€‚</li>
<li>Siamese-Diffusionæ¨¡å‹åœ¨Polypså’ŒISIC2018æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ï¼Œç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fe6c0c678fc098c5283c7480ab238a75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28fb51bed211eace5f80b73f5a5b1071.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f7f40156b3a03df8adc5967cad7e870.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-879cd050456c3a253169968c7b573330.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10cc05b65f851b17c65f0326fc1806fd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DFEN-Dual-Feature-Equalization-Network-for-Medical-Image-Segmentation"><a href="#DFEN-Dual-Feature-Equalization-Network-for-Medical-Image-Segmentation" class="headerlink" title="DFEN: Dual Feature Equalization Network for Medical Image Segmentation"></a>DFEN: Dual Feature Equalization Network for Medical Image Segmentation</h2><p><strong>Authors:Jianjian Yin, Yi Chen, Chengyu Li, Zhichao Zheng, Yanhui Gu, Junsheng Zhou</strong></p>
<p>Current methods for medical image segmentation primarily focus on extracting contextual feature information from the perspective of the whole image. While these methods have shown effective performance, none of them take into account the fact that pixels at the boundary and regions with a low number of class pixels capture more contextual feature information from other classes, leading to misclassification of pixels by unequal contextual feature information. In this paper, we propose a dual feature equalization network based on the hybrid architecture of Swin Transformer and Convolutional Neural Network, aiming to augment the pixel feature representations by image-level equalization feature information and class-level equalization feature information. Firstly, the image-level feature equalization module is designed to equalize the contextual information of pixels within the image. Secondly, we aggregate regions of the same class to equalize the pixel feature representations of the corresponding class by class-level feature equalization module. Finally, the pixel feature representations are enhanced by learning weights for image-level equalization feature information and class-level equalization feature information. In addition, Swin Transformer is utilized as both the encoder and decoder, thereby bolstering the ability of the model to capture long-range dependencies and spatial correlations. We conducted extensive experiments on Breast Ultrasound Images (BUSI), International Skin Imaging Collaboration (ISIC2017), Automated Cardiac Diagnosis Challenge (ACDC) and PH$^2$ datasets. The experimental results demonstrate that our method have achieved state-of-the-art performance. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/JianJianYin/DFEN">https://github.com/JianJianYin/DFEN</a>. </p>
<blockquote>
<p>å½“å‰åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–¹æ³•ä¸»è¦ä»æ•´ä¸ªå›¾åƒçš„è§’åº¦æå–ä¸Šä¸‹æ–‡ç‰¹å¾ä¿¡æ¯ã€‚è™½ç„¶è¿™äº›æ–¹æ³•å·²ç»å±•ç°å‡ºæœ‰æ•ˆçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬éƒ½æ²¡æœ‰è€ƒè™‘åˆ°è¾¹ç•Œå¤„çš„åƒç´ ä»¥åŠå…·æœ‰å°‘é‡ç±»åˆ«åƒç´ çš„åŒºåŸŸèƒ½å¤Ÿä»å…¶ä»–ç±»åˆ«ä¸­è·å–æ›´å¤šçš„ä¸Šä¸‹æ–‡ç‰¹å¾ä¿¡æ¯ï¼Œä»è€Œå¯¼è‡´åƒç´ è¯¯åˆ†ç±»ï¼Œå› ä¸ºä¸Šä¸‹æ–‡ç‰¹å¾ä¿¡æ¯æ˜¯ä¸å¹³ç­‰çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºSwin Transformerå’Œå·ç§¯ç¥ç»ç½‘ç»œæ··åˆæ¶æ„çš„åŒç‰¹å¾å‡è¡¡ç½‘ç»œï¼Œæ—¨åœ¨é€šè¿‡å›¾åƒçº§å‡è¡¡ç‰¹å¾ä¿¡æ¯å’Œç±»åˆ«çº§å‡è¡¡ç‰¹å¾ä¿¡æ¯æ¥å¢å¼ºåƒç´ ç‰¹å¾è¡¨ç¤ºã€‚é¦–å…ˆï¼Œè®¾è®¡äº†å›¾åƒçº§ç‰¹å¾å‡è¡¡æ¨¡å—ï¼Œä»¥å‡è¡¡å›¾åƒå†…åƒç´ çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡ç±»åˆ«çº§ç‰¹å¾å‡è¡¡æ¨¡å—èšåˆåŒä¸€ç±»åˆ«çš„åŒºåŸŸï¼Œä»¥å‡è¡¡ç›¸åº”ç±»åˆ«çš„åƒç´ ç‰¹å¾è¡¨ç¤ºã€‚æœ€åï¼Œé€šè¿‡å›¾åƒçº§å‡è¡¡ç‰¹å¾ä¿¡æ¯å’Œç±»åˆ«çº§å‡è¡¡ç‰¹å¾ä¿¡æ¯çš„å­¦ä¹ æƒé‡æ¥å¢å¼ºåƒç´ ç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼ŒSwin TransformeråŒæ—¶ä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨ï¼Œå¢å¼ºäº†æ¨¡å‹æ•æ‰é•¿è·ç¦»ä¾èµ–æ€§å’Œç©ºé—´å…³è”æ€§çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¹³è…ºè¶…å£°å›¾åƒï¼ˆBUSIï¼‰ã€å›½é™…çš®è‚¤å½±åƒåä½œï¼ˆISIC2017ï¼‰ã€è‡ªåŠ¨å¿ƒè„è¯Šæ–­æŒ‘æˆ˜ï¼ˆACDCï¼‰å’ŒPHæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JianJianYin/DFEN%E5%85%AC%E5%BC%BA%E5%8F%96%E5%BE%97%E3%80%82">https://github.com/JianJianYin/DFENå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05913v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§åŸºäºSwin Transformerå’Œå·ç§¯ç¥ç»ç½‘ç»œæ··åˆæ¶æ„çš„åŒç‰¹å¾å‡è¡¡ç½‘ç»œï¼Œç”¨äºå¢å¼ºåƒç´ ç‰¹å¾è¡¨ç¤ºã€‚è¯¥ç½‘ç»œé€šè¿‡å›¾åƒçº§ç‰¹å¾å‡è¡¡æ¨¡å—å’Œç±»åˆ«çº§ç‰¹å¾å‡è¡¡æ¨¡å—ï¼Œå‡è¡¡å›¾åƒå†…åƒç´ çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’ŒåŒç±»åƒç´ çš„ç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼ŒSwin TransformeråŒæ—¶ä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨ï¼Œæé«˜äº†æ¨¡å‹æ•æ‰é•¿è·ç¦»ä¾èµ–å’Œç©ºé—´å…³è”çš„èƒ½åŠ›ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ä¸»è¦å…³æ³¨ä»æ•´ä¸ªå›¾åƒçš„è§’åº¦æå–ä¸Šä¸‹æ–‡ç‰¹å¾ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥äº†è¾¹ç•Œåƒç´ å’Œå…·æœ‰è¾ƒå°‘ç±»åˆ«åƒç´ çš„åŒºåŸŸï¼Œè¿™äº›åŒºåŸŸä»å…¶ä»–ç±»åˆ«æ•è·æ›´å¤šçš„ä¸Šä¸‹æ–‡ç‰¹å¾ä¿¡æ¯ï¼Œå¯¼è‡´åƒç´ è¯¯åˆ†ç±»ã€‚</li>
<li>æå‡ºçš„åŒç‰¹å¾å‡è¡¡ç½‘ç»œæ—¨åœ¨é€šè¿‡å›¾åƒçº§å’Œç±»åˆ«çº§çš„ç‰¹å¾å‡è¡¡ä¿¡æ¯æ¥å¢å¼ºåƒç´ ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>ç½‘ç»œè®¾è®¡åŒ…æ‹¬å›¾åƒçº§ç‰¹å¾å‡è¡¡æ¨¡å—ï¼Œç”¨äºå‡è¡¡å›¾åƒå†…åƒç´ çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡ç±»åˆ«çº§ç‰¹å¾å‡è¡¡æ¨¡å—ï¼Œç½‘ç»œèšåˆåŒä¸€ç±»åˆ«çš„åŒºåŸŸï¼Œå‡è¡¡ç›¸åº”ç±»åˆ«çš„åƒç´ ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>Swin Transformerä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨ï¼Œæé«˜äº†æ¨¡å‹æ•æ‰é•¿è·ç¦»ä¾èµ–å’Œç©ºé—´å…³è”çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c817cf7d1cbf325de764f2633cf2a7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9553d8a156f9fdbed5f5a7fb173ed184.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b70c8a1fa1f6e99aa26c77655a99a72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-193e2ea24101c16a3a3a095370d176ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dc7f4ca3a5ec29289e3181d5f005794.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c32f437b6888ff7cdd1c3acd1c7af54e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9743a4bae61248f0febdff4280a02e1a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Describe-Anything-in-Medical-Images"><a href="#Describe-Anything-in-Medical-Images" class="headerlink" title="Describe Anything in Medical Images"></a>Describe Anything in Medical Images</h2><p><strong>Authors:Xi Xiao, Yunbei Zhang, Thanh-Huy Nguyen, Ba-Thinh Lam, Janet Wang, Jihun Hamm, Tianyang Wang, Xingjian Li, Xiao Wang, Hao Xu, Tianming Liu, Min Xu</strong></p>
<p>Localized image captioning has made significant progress with models like the Describe Anything Model (DAM), which can generate detailed region-specific descriptions without explicit region-text supervision. However, such capabilities have yet to be widely applied to specialized domains like medical imaging, where diagnostic interpretation relies on subtle regional findings rather than global understanding. To mitigate this gap, we propose MedDAM, the first comprehensive framework leveraging large vision-language models for region-specific captioning in medical images. MedDAM employs medical expert-designed prompts tailored to specific imaging modalities and establishes a robust evaluation benchmark comprising a customized assessment protocol, data pre-processing pipeline, and specialized QA template library. This benchmark evaluates both MedDAM and other adaptable large vision-language models, focusing on clinical factuality through attribute-level verification tasks, thereby circumventing the absence of ground-truth region-caption pairs in medical datasets. Extensive experiments on the VinDr-CXR, LIDC-IDRI, and SkinCon datasets demonstrate MedDAMâ€™s superiority over leading peers (including GPT-4o, Claude 3.7 Sonnet, LLaMA-3.2 Vision, Qwen2.5-VL, GPT-4Rol, and OMG-LLaVA) in the task, revealing the importance of region-level semantic alignment in medical image understanding and establishing MedDAM as a promising foundation for clinical vision-language integration. </p>
<blockquote>
<p>å±€éƒ¨å›¾åƒæè¿°åœ¨è¯¸å¦‚Describe Anything Modelï¼ˆDAMï¼‰ç­‰æ¨¡å‹çš„æ¨åŠ¨ä¸‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®çš„åŒºåŸŸæ–‡æœ¬ç›‘ç£çš„æƒ…å†µä¸‹ç”Ÿæˆè¯¦ç»†çš„åŒºåŸŸç‰¹å®šæè¿°ã€‚ç„¶è€Œï¼Œè¿™ç§èƒ½åŠ›å°šæœªå¹¿æ³›åº”ç”¨äºåŒ»å­¦æˆåƒç­‰ç‰¹å®šé¢†åŸŸï¼Œè¯Šæ–­è§£è¯»ä¾èµ–äºç»†å¾®çš„åŒºåŸŸå‘ç°è€Œéå…¨å±€ç†è§£ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MedDAMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡ŒåŒ»å­¦å›¾åƒåŒºåŸŸç‰¹å®šæè¿°çš„å…¨é¢æ¡†æ¶ã€‚MedDAMé‡‡ç”¨é’ˆå¯¹ç‰¹å®šæˆåƒæ¨¡å¼è®¾è®¡çš„åŒ»å­¦ä¸“å®¶æç¤ºï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªç¨³å¥çš„è¯„ä¼°åŸºå‡†ï¼ŒåŒ…æ‹¬å®šåˆ¶è¯„ä¼°åè®®ã€æ•°æ®é¢„å¤„ç†ç®¡é“å’Œä¸“ç”¨é—®ç­”æ¨¡æ¿åº“ã€‚è¯¥åŸºå‡†ä¸ä»…è¯„ä¼°MedDAMçš„æ€§èƒ½ï¼Œè¿˜è¯„ä¼°å…¶ä»–å¯é€‚åº”çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å±æ€§çº§åˆ«éªŒè¯ä»»åŠ¡å…³æ³¨ä¸´åºŠäº‹å®æ€§ï¼Œä»è€Œè§„é¿åŒ»å­¦æ•°æ®é›†ä¸­ç¼ºä¹åœ°é¢çœŸå®åŒºåŸŸæ ‡é¢˜å¯¹çš„é—®é¢˜ã€‚åœ¨VinDr-CXRã€LIDC-IDRIå’ŒSkinConæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMedDAMåœ¨ä»»åŠ¡ä¸­ä¼˜äºé¢†å…ˆçš„å¯¹æ ‡æ¨¡å‹ï¼ˆåŒ…æ‹¬GPT-4oã€Claude 3.7 Sonnetã€LLaMA-3.2 Visionã€Qwen2.5-VLã€GPT-4Rolå’ŒOMG-LLaVAï¼‰ï¼Œæ­ç¤ºäº†åŒ»å­¦å›¾åƒç†è§£ä¸­åŒºåŸŸçº§è¯­ä¹‰å¯¹é½çš„é‡è¦æ€§ï¼Œå¹¶ç¡®ç«‹äº†MedDAMä½œä¸ºä¸´åºŠè§†è§‰è¯­è¨€é›†æˆçš„æœ‰å‰é€”çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05804v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒåŒºåŸŸç‰¹å®šçš„æè¿°æ¨¡å‹MedDAMçš„å‘å±•å’Œåº”ç”¨ã€‚MedDAMåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨åŒ»å­¦å›¾åƒä¸­è¿›è¡ŒåŒºåŸŸç‰¹å®šçš„æè¿°ï¼Œå¹¶å»ºç«‹äº†åŒ…å«è‡ªå®šä¹‰è¯„ä¼°åè®®ã€æ•°æ®é¢„å¤„ç†ç®¡é“å’Œä¸“é—¨çš„è´¨é‡ä¿è¯æ¨¡æ¿åº“çš„ç»¼åˆè¯„ä¼°åŸºå‡†ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†MedDAMåœ¨åŒ»å­¦å›¾åƒç†è§£ä¸­çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedDAMæ˜¯é¦–ä¸ªé’ˆå¯¹åŒ»å­¦å›¾åƒåŒºåŸŸç‰¹å®šæè¿°çš„ç»¼åˆæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œæè¿°ã€‚</li>
<li>MedDAMé‡‡ç”¨åŒ»å­¦ä¸“å®¶è®¾è®¡çš„é’ˆå¯¹ç‰¹å®šæˆåƒæ¨¡å¼çš„æç¤ºã€‚</li>
<li>å»ºç«‹äº†åŒ…å«è‡ªå®šä¹‰è¯„ä¼°åè®®ã€æ•°æ®é¢„å¤„ç†ç®¡é“å’Œä¸“é—¨è´¨é‡ä¿è¯æ¨¡æ¿åº“çš„ç»¼åˆè¯„ä¼°åŸºå‡†ã€‚</li>
<li>MedDAMé‡ç‚¹è¯„ä¼°ä¸´åºŠäº‹å®æ€§ï¼Œé€šè¿‡å±æ€§çº§åˆ«çš„éªŒè¯ä»»åŠ¡æ¥å…‹æœåŒ»å­¦æ•°æ®é›†ä¸­ç¼ºä¹çœŸå®åŒºåŸŸæ ‡é¢˜å¯¹çš„é—®é¢˜ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMedDAMåœ¨åŒ»å­¦å›¾åƒç†è§£ä»»åŠ¡ä¸­ä¼˜äºå…¶ä»–å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
<li>MedDAMçš„é‡è¦æ€§åœ¨äºå…¶åœ¨åŒ»å­¦å›¾åƒç†è§£ä¸­çš„åŒºåŸŸçº§è¯­ä¹‰å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-68051635c11c20010486e1ea9b888659.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a502db77b7d81ee4881f06bf758a7ffb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b57a65a1cdcb26fc977632314bcdc3cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-573dd50ad75c09fe59bb4dad392c7bf4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-552ef5d8dd0e3e01e2cc5a6be13fbe2b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e56e4b1ce953556285ec725e7671c95.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="HyperspectralMAE-The-Hyperspectral-Imagery-Classification-Model-using-Fourier-Encoded-Dual-Branch-Masked-Autoencoder"><a href="#HyperspectralMAE-The-Hyperspectral-Imagery-Classification-Model-using-Fourier-Encoded-Dual-Branch-Masked-Autoencoder" class="headerlink" title="HyperspectralMAE: The Hyperspectral Imagery Classification Model using   Fourier-Encoded Dual-Branch Masked Autoencoder"></a>HyperspectralMAE: The Hyperspectral Imagery Classification Model using   Fourier-Encoded Dual-Branch Masked Autoencoder</h2><p><strong>Authors:Wooyoung Jeong, Hyun Jae Park, Seonghun Jeong, Jong Wook Jang, Tae Hoon Lim, Dae Seoung Kim</strong></p>
<p>Hyperspectral imagery provides rich spectral detail but poses unique challenges because of its high dimensionality in both spatial and spectral domains. We propose \textit{HyperspectralMAE}, a Transformer-based foundation model for hyperspectral data that employs a \textit{dual masking} strategy: during pre-training we randomly occlude 50% of spatial patches and 50% of spectral bands. This forces the model to learn representations capable of reconstructing missing information across both dimensions. To encode spectral order, we introduce learnable harmonic Fourier positional embeddings based on wavelength. The reconstruction objective combines mean-squared error (MSE) with the spectral angle mapper (SAM) to balance pixel-level accuracy and spectral-shape fidelity.   The resulting model contains about $1.8\times10^{8}$ parameters and produces 768-dimensional embeddings, giving it sufficient capacity for transfer learning. We pre-trained HyperspectralMAE on two large hyperspectral corpora â€“ NASA EO-1 Hyperion ($\sim$1,600 scenes, $\sim$$3\times10^{11}$ pixel spectra) and DLR EnMAP Level-0 ($\sim$1,300 scenes, $\sim$$3\times10^{11}$ pixel spectra) â€“ and fine-tuned it for land-cover classification on the Indian Pines benchmark. HyperspectralMAE achieves state-of-the-art transfer-learning accuracy on Indian Pines, confirming that masked dual-dimensional pre-training yields robust spectral-spatial representations. These results demonstrate that dual masking and wavelength-aware embeddings advance hyperspectral image reconstruction and downstream analysis. </p>
<blockquote>
<p>é«˜å…‰è°±æˆåƒæä¾›äº†ä¸°å¯Œçš„å…‰è°±ç»†èŠ‚ï¼Œä½†åŒæ—¶ä¹Ÿå› å…¶ç©ºé—´ä¸å…‰è°±åŸŸçš„é«˜ç»´åº¦ç‰¹æ€§å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºTransformerçš„é«˜å…‰è°±æ•°æ®åŸºç¡€æ¨¡å‹â€”â€”HyperspectralMAEã€‚è¯¥æ¨¡å‹é‡‡ç”¨åŒé‡æ©è”½ç­–ç•¥ï¼šåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éšæœºé®æŒ¡50%çš„ç©ºé—´æ–‘å—å’Œ50%çš„å…‰è°±æ³¢æ®µã€‚è¿™è¿«ä½¿æ¨¡å‹å­¦ä¹ èƒ½å¤Ÿåœ¨ä¸¤ä¸ªç»´åº¦ä¸Šé‡å»ºç¼ºå¤±ä¿¡æ¯çš„èƒ½åŠ›ã€‚ä¸ºäº†ç¼–ç å…‰è°±é¡ºåºï¼Œæˆ‘ä»¬åŸºäºæ³¢é•¿å¼•å…¥äº†å¯å­¦ä¹ çš„è°æ³¢å‚…é‡Œå¶ä½ç½®åµŒå…¥ã€‚é‡å»ºç›®æ ‡ç»“åˆäº†å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä¸å…‰è°±è§’åº¦æ˜ å°„å™¨ï¼ˆSAMï¼‰ï¼Œä»¥å¹³è¡¡åƒç´ çº§ç²¾åº¦å’Œå…‰è°±å½¢çŠ¶ä¿çœŸåº¦ã€‚æ‰€å¾—æ¨¡å‹åŒ…å«çº¦1.8Ã—10^8ä¸ªå‚æ•°ï¼Œç”Ÿæˆ768ç»´åµŒå…¥ï¼Œä¸ºè¿ç§»å­¦ä¹ æä¾›äº†è¶³å¤Ÿçš„å®¹é‡ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå¤§å‹é«˜å…‰è°±è¯­æ–™åº“â€”â€”NASA EO-1 Hyperionï¼ˆçº¦1600ä¸ªåœºæ™¯ï¼Œçº¦3Ã—10^11åƒç´ å…‰è°±ï¼‰å’ŒDLR EnMAP Level-0ï¼ˆçº¦1300ä¸ªåœºæ™¯ï¼Œçº¦3Ã—10^11åƒç´ å…‰è°±ï¼‰ä¸Šé¢„è®­ç»ƒäº†HyperspectralMAEæ¨¡å‹ï¼Œå¹¶åœ¨å°åº¦æ¾æ ‘åœŸåœ°è¦†ç›–åˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚HyperspectralMAEåœ¨Indian PinesåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¿ç§»å­¦ä¹ ç²¾åº¦ï¼Œè¯å®äº†æ©è”½åŒé‡ç»´åº¦é¢„è®­ç»ƒèƒ½å¤Ÿäº§ç”Ÿç¨³å¥çš„å…‰è°±ç©ºé—´è¡¨ç¤ºã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒåŒé‡æ©è”½å’Œæ³¢é•¿æ„ŸçŸ¥åµŒå…¥åœ¨æ¨è¿›é«˜å…‰è°±å›¾åƒé‡å»ºå’Œä¸‹æ¸¸åˆ†ææ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05710v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºHyperspectralMAEçš„åŸºäºTransformerçš„ç”¨äºé«˜å…‰è°±æ•°æ®çš„æ¨¡å‹ã€‚å®ƒé‡‡ç”¨åŒé‡æ©ç ç­–ç•¥è¿›è¡Œé¢„è®­ç»ƒï¼Œèƒ½å¤Ÿå­¦ä¹ åœ¨ç©ºé—´å’Œå…‰è°±ä¸¤ä¸ªç»´åº¦ä¸Šé‡å»ºç¼ºå¤±ä¿¡æ¯çš„èƒ½åŠ›ã€‚å¼•å…¥åŸºäºæ³¢é•¿çš„å¯å­¦ä¹ è°æ³¢å‚…é‡Œå¶ä½ç½®åµŒå…¥æ¥ç¼–ç å…‰è°±é¡ºåºã€‚é‡å»ºç›®æ ‡ç»“åˆäº†å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰å’Œå…‰è°±è§’åº¦æ˜ å°„å™¨ï¼ˆSAMï¼‰ï¼Œä»¥å®ç°åƒç´ çº§ç²¾åº¦å’Œå…‰è°±å½¢çŠ¶ä¿çœŸåº¦çš„å¹³è¡¡ã€‚æ¨¡å‹åœ¨å¤§å‹é«˜å…‰è°±æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨å°åº¦æ¾æ ‘åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå®ç°äº†æœ€å…ˆè¿›çš„è¿ç§»å­¦ä¹ ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>æ¨¡å‹å¼•å…¥åŒé‡æ©ç ç­–ç•¥ï¼Œåœ¨ç©ºé—´å’Œå…‰è°±ä¸¤ä¸ªç»´åº¦ä¸Šè®­ç»ƒæ¨¡å‹é‡å»ºç¼ºå¤±ä¿¡æ¯çš„èƒ½åŠ›ã€‚</p>
</li>
<li><p>æ¨¡å‹é‡‡ç”¨å¯å­¦ä¹ è°æ³¢å‚…é‡Œå¶ä½ç½®åµŒå…¥ï¼ŒåŸºäºæ³¢é•¿ç¼–ç å…‰è°±é¡ºåºã€‚</p>
</li>
<li><p>é‡å»ºç›®æ ‡ç»“åˆäº†å‡æ–¹è¯¯å·®å’Œå…‰è°±è§’åº¦æ˜ å°„å™¨ï¼Œå®ç°åƒç´ çº§å’Œå…‰è°±çº§çš„å‡†ç¡®æ€§å¹³è¡¡ã€‚</p>
</li>
<li><p>æ¨¡å‹é¢„è®­ç»ƒåœ¨ä¸¤ä¸ªå¤§å‹é«˜å…‰è°±æ•°æ®é›†ä¸Šè¿›è¡Œï¼ŒåŒ…å«çº¦ä¸€äº¿å…«åƒä¸‡å‚æ•°å’Œ768ç»´åµŒå…¥ï¼Œå…·æœ‰è¶³å¤Ÿçš„è¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚</p>
</li>
<li><p>æ¨¡å‹åœ¨å°åº¦æ¾æ ‘åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è¿ç§»å­¦ä¹ ç²¾åº¦ã€‚</p>
</li>
<li><p>å®éªŒç»“æœè¡¨æ˜åŒé‡æ©ç å’Œæ³¢é•¿æ„ŸçŸ¥åµŒå…¥èƒ½å¤Ÿæ¨è¿›é«˜å…‰è°±å›¾åƒé‡å»ºå’Œä¸‹æ¸¸åˆ†æã€‚</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a10e870177da443ac0bd0e2f9e7ec16f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83a75f761c6db8b4724d39e71d2ebae4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d83398ddb3001b6940ac7602c1de5ce.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="FF-PNet-A-Pyramid-Network-Based-on-Feature-and-Field-for-Brain-Image-Registration"><a href="#FF-PNet-A-Pyramid-Network-Based-on-Feature-and-Field-for-Brain-Image-Registration" class="headerlink" title="FF-PNet: A Pyramid Network Based on Feature and Field for Brain Image   Registration"></a>FF-PNet: A Pyramid Network Based on Feature and Field for Brain Image   Registration</h2><p><strong>Authors:Ying Zhang, Shuai Guo, Chenxi Sun, Yuchen Zhu, Jinhai Xiang</strong></p>
<p>In recent years, deformable medical image registration techniques have made significant progress. However, existing models still lack efficiency in parallel extraction of coarse and fine-grained features. To address this, we construct a new pyramid registration network based on feature and deformation field (FF-PNet). For coarse-grained feature extraction, we design a Residual Feature Fusion Module (RFFM), for fine-grained image deformation, we propose a Residual Deformation Field Fusion Module (RDFFM). Through the parallel operation of these two modules, the model can effectively handle complex image deformations. It is worth emphasizing that the encoding stage of FF-PNet only employs traditional convolutional neural networks without any attention mechanisms or multilayer perceptrons, yet it still achieves remarkable improvements in registration accuracy, fully demonstrating the superior feature decoding capabilities of RFFM and RDFFM. We conducted extensive experiments on the LPBA and OASIS datasets. The results show our network consistently outperforms popular methods in metrics like the Dice Similarity Coefficient. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¯å˜å½¢åŒ»å­¦å›¾åƒé…å‡†æŠ€æœ¯å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨ç²—ç²’åº¦å’Œç»†ç²’åº¦ç‰¹å¾çš„å¹¶è¡Œæå–æ–¹é¢ä»ç¼ºä¹æ•ˆç‡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŸºäºç‰¹å¾å’Œå˜å½¢åœºï¼ˆFF-PNetï¼‰çš„æ–°é‡‘å­—å¡”é…å‡†ç½‘ç»œã€‚å¯¹äºç²—ç²’åº¦ç‰¹å¾æå–ï¼Œæˆ‘ä»¬è®¾è®¡äº†æ®‹å·®ç‰¹å¾èåˆæ¨¡å—ï¼ˆRFFMï¼‰ï¼Œå¯¹äºç»†ç²’åº¦å›¾åƒå˜å½¢ï¼Œæˆ‘ä»¬æå‡ºäº†æ®‹å·®å˜å½¢åœºèåˆæ¨¡å—ï¼ˆRDFFMï¼‰ã€‚è¿™ä¸¤ä¸ªæ¨¡å—çš„å¹¶è¡Œæ“ä½œä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤æ‚çš„å›¾åƒå˜å½¢ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒFF-PNetçš„ç¼–ç é˜¶æ®µä»…é‡‡ç”¨ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œæ²¡æœ‰ä»»ä½•æ³¨æ„åŠ›æœºåˆ¶æˆ–å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œä½†åœ¨é…å‡†ç²¾åº¦ä¸Šä»å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå……åˆ†å±•ç¤ºäº†RFFMå’ŒRDFFMçš„ä¼˜ç§€ç‰¹å¾è§£ç èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨LPBAå’ŒOASISæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç½‘ç»œåœ¨Diceç›¸ä¼¼ç³»æ•°ç­‰æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºæµè¡Œçš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04938v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé…å‡†æŠ€æœ¯è¿‘å¹´æ¥å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ç²—ç²’åº¦å’Œç»†ç²’åº¦ç‰¹å¾å¹¶è¡Œæå–æ•ˆç‡ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†åŸºäºç‰¹å¾å’Œå˜å½¢åœºçš„æ–°å‹é‡‘å­—å¡”é…å‡†ç½‘ç»œï¼ˆFF-PNetï¼‰ã€‚ä¸ºæå–ç²—ç²’åº¦ç‰¹å¾ï¼Œè®¾è®¡äº†Residual Feature Fusion Moduleï¼ˆRFFMï¼‰ï¼›ä¸ºå¤„ç†å›¾åƒç²¾ç»†å˜å½¢ï¼Œæå‡ºäº†Residual Deformation Field Fusion Moduleï¼ˆRDFFMï¼‰ã€‚è¿™ä¸¤ä¸ªæ¨¡å—çš„å¹¶è¡Œæ“ä½œä½¿å¾—æ¨¡å‹èƒ½æ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚çš„å›¾åƒå˜å½¢ã€‚è¯¥ç½‘ç»œåœ¨LPBAå’ŒOASISæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œå³ä½¿åœ¨æ— æ³¨æ„åŠ›æœºåˆ¶æˆ–å¤šå±‚æ„ŸçŸ¥å™¨çš„æƒ…å†µä¸‹ï¼Œä»…ä½¿ç”¨ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬çš„ç½‘ç»œåœ¨é…å‡†ç²¾åº¦ä¸Šä»æœ‰æ˜¾è‘—æå‡ï¼Œå……åˆ†å±•ç¤ºäº†RFFMå’ŒRDFFMçš„ç‰¹å¾è§£ç ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰åŒ»å­¦å›¾åƒé…å‡†æŠ€æœ¯ä»å­˜åœ¨ç²—ç²’åº¦å’Œç»†ç²’åº¦ç‰¹å¾å¹¶è¡Œæå–æ•ˆç‡é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„é‡‘å­—å¡”é…å‡†ç½‘ç»œï¼ˆFF-PNetï¼‰ï¼Œèåˆäº†ç‰¹å¾å’Œå˜å½¢åœºã€‚</li>
<li>è®¾è®¡äº†Residual Feature Fusion Moduleï¼ˆRFFMï¼‰ç”¨äºç²—ç²’åº¦ç‰¹å¾æå–ã€‚</li>
<li>æå‡ºäº†Residual Deformation Field Fusion Moduleï¼ˆRDFFMï¼‰å¤„ç†å›¾åƒç²¾ç»†å˜å½¢ã€‚</li>
<li>FF-PNeté€šè¿‡å¹¶è¡Œæ“ä½œè¿™ä¸¤ä¸ªæ¨¡å—ï¼Œæ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚å›¾åƒå˜å½¢ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒFF-PNetåœ¨é…å‡†ç²¾åº¦ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04938">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e104db23b06c9b05e2aea9ab533803fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b58ccadecce4db9ec0324ccdad7e2683.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-166168e9743b24b2b80c31a76aacdcb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f83bfcdb21e54a5f2a804222d900af8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d6c3d0a0cc6ac1a610e482e677a11b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45334aaa4a00d0a047864c7edb31304c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a313a599c4766740e731284c9990d34c.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MAISY-Motion-Aware-Image-SYnthesis-for-Medical-Image-Motion-Correction"><a href="#MAISY-Motion-Aware-Image-SYnthesis-for-Medical-Image-Motion-Correction" class="headerlink" title="MAISY: Motion-Aware Image SYnthesis for Medical Image Motion Correction"></a>MAISY: Motion-Aware Image SYnthesis for Medical Image Motion Correction</h2><p><strong>Authors:Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim</strong></p>
<p>Patient motion during medical image acquisition causes blurring, ghosting, and distorts organs, which makes image interpretation challenging. Current state-of-the-art algorithms using Generative Adversarial Network (GAN)-based methods with their ability to learn the mappings between corrupted images and their ground truth via Structural Similarity Index Measure (SSIM) loss effectively generate motion-free images. However, we identified the following limitations: (i) they mainly focus on global structural characteristics and therefore overlook localized features that often carry critical pathological information, and (ii) the SSIM loss function struggles to handle images with varying pixel intensities, luminance factors, and variance. In this study, we propose Motion-Aware Image SYnthesis (MAISY) which initially characterize motion and then uses it for correction by: (a) leveraging the foundation model Segment Anything Model (SAM), to dynamically learn spatial patterns along anatomical boundaries where motion artifacts are most pronounced and, (b) introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively emphasizes spatial regions with high pixel variance to preserve essential anatomical details during artifact correction. Experiments on chest and head CT datasets demonstrate that our model outperformed the state-of-the-art counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by 10%, and Dice by 16%. </p>
<blockquote>
<p>æ‚£è€…åœ¨åŒ»å­¦å›¾åƒé‡‡é›†è¿‡ç¨‹ä¸­çš„è¿åŠ¨ä¼šå¯¼è‡´å›¾åƒæ¨¡ç³Šã€å‡ºç°æ®‹å½±å’Œå™¨å®˜æ‰­æ›²ï¼Œè¿™ä½¿å¾—å›¾åƒè§£è¯»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å½“å‰æœ€å…ˆè¿›çš„ç®—æ³•ä½¿ç”¨åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ–¹æ³•ï¼Œå®ƒä»¬èƒ½å¤Ÿé€šè¿‡ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰æŸå¤±æ¥å­¦ä¹ å¤±çœŸå›¾åƒä¸çœŸå®å›¾åƒä¹‹é—´çš„æ˜ å°„ï¼Œä»è€Œæœ‰æ•ˆåœ°ç”Ÿæˆæ— è¿åŠ¨å›¾åƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°äº†ä»¥ä¸‹å±€é™æ€§ï¼šï¼ˆiï¼‰å®ƒä»¬ä¸»è¦å…³æ³¨å…¨å±€ç»“æ„ç‰¹å¾ï¼Œå› æ­¤å¿½ç•¥äº†é€šå¸¸æºå¸¦å…³é”®ç—…ç†ä¿¡æ¯çš„å±€éƒ¨ç‰¹å¾ï¼›ï¼ˆiiï¼‰SSIMæŸå¤±å‡½æ•°åœ¨å¤„ç†åƒç´ å¼ºåº¦ã€äº®åº¦å› ç´ å’Œæ–¹å·®å„å¼‚çš„å›¾åƒæ—¶é‡åˆ°å›°éš¾ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è¿åŠ¨æ„ŸçŸ¥å›¾åƒåˆæˆï¼ˆMAISYï¼‰ï¼Œå®ƒé¦–å…ˆè¡¨å¾è¿åŠ¨ï¼Œç„¶ååˆ©ç”¨è¿åŠ¨è¿›è¡Œä¿®æ­£ï¼šï¼ˆaï¼‰é€šè¿‡åˆ©ç”¨â€œä»»ä½•å†…å®¹åˆ†å‰²æ¨¡å‹â€ï¼ˆSAMï¼‰çš„åŸºç¡€ï¼ŒåŠ¨æ€å­¦ä¹ è§£å‰–è¾¹ç•Œå¤„çš„ç©ºé—´æ¨¡å¼ï¼Œè¿™äº›è¾¹ç•Œæ˜¯è¿åŠ¨ä¼ªå½±æœ€æ˜æ˜¾çš„åŒºåŸŸï¼›ï¼ˆbï¼‰å¼•å…¥æ–¹å·®é€‰æ‹©æ€§SSIMï¼ˆVS-SSIMï¼‰æŸå¤±ï¼Œè¯¥æŸå¤±èƒ½å¤Ÿè‡ªé€‚åº”åœ°å¼ºè°ƒå…·æœ‰é«˜åƒç´ æ–¹å·®çš„åŒºåŸŸï¼Œä»è€Œåœ¨çº æ­£ä¼ªå½±æ—¶ä¿ç•™å…³é”®çš„è§£å‰–ç»†èŠ‚ã€‚å¯¹èƒ¸éƒ¨å’Œå¤´éƒ¨CTæ•°æ®é›†çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æé«˜äº†40%ï¼Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰æé«˜äº†10%ï¼Œè¿ªæ°æ–¯ç‰¹æ‹‰ç³»æ•°æé«˜äº†16%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04105v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŒ»å­¦å›¾åƒè·å–è¿‡ç¨‹ä¸­æ‚£è€…è¿åŠ¨å¯¼è‡´çš„å›¾åƒæ¨¡ç³Šã€é¬¼å½±å’Œå™¨å®˜æ‰­æ›²é—®é¢˜ï¼Œä½¿å¾—å›¾åƒè§£è¯»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰ç®—æ³•è™½å¯ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åŸºäºç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰æŸå¤±ç”Ÿæˆæ— è¿åŠ¨å›¾åƒï¼Œä½†ä»å­˜åœ¨å¿½ç•¥å±€éƒ¨ç‰¹å¾å’ŒSSIMæŸå¤±å‡½æ•°å¤„ç†åƒç´ å¼ºåº¦ã€äº®åº¦å› ç´ å’Œæ–¹å·®å·®å¼‚è¾ƒå¤§çš„å›¾åƒæ—¶è¡¨ç°ä¸ä½³çš„å±€é™æ€§ã€‚æœ¬ç ”ç©¶æå‡ºMotion-Aware Image SYnthesisï¼ˆMAISYï¼‰ï¼Œé¦–å…ˆè¡¨å¾è¿åŠ¨ï¼Œç„¶ååˆ©ç”¨è¿åŠ¨ä¿¡æ¯è¿›è¡Œæ ¡æ­£ï¼šé€šè¿‡åˆ©ç”¨Segment Anything Modelï¼ˆSAMï¼‰åŸºç¡€æ¨¡å‹åŠ¨æ€å­¦ä¹ è§£å‰–è¾¹ç•Œçš„ç©ºé—´æ¨¡å¼æ¥çªå‡ºè¿åŠ¨ä¼ªå½±ï¼Œå¹¶å¼•å…¥Variance-Selective SSIMï¼ˆVS-SSIMï¼‰æŸå¤±ï¼Œä»¥åœ¨ä¼ªå½±æ ¡æ­£è¿‡ç¨‹ä¸­è‡ªé€‚åº”å¼ºè°ƒé«˜åƒç´ æ–¹å·®çš„åŒºåŸŸï¼Œä¿ç•™å…³é”®çš„è§£å‰–ç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨èƒ¸éƒ¨å’Œå¤´éƒ¨CTæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æé«˜40%ï¼Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰æé«˜10%ï¼Œç‹„æ°ç³»æ•°æé«˜16%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‚£è€…è¿åŠ¨åœ¨åŒ»å­¦å›¾åƒè·å–ä¸­å¯¼è‡´å›¾åƒæ¨¡ç³Šã€é¬¼å½±å’Œå™¨å®˜æ‰­æ›²ï¼Œä½¿å¾—è§£è¯»å›°éš¾ã€‚</li>
<li>å½“å‰ç®—æ³•ä½¿ç”¨GANå’ŒSSIMæŸå¤±å¤„ç†è¿åŠ¨ä¼ªå½±ï¼Œä½†ä¸»è¦å…³æ³¨å…¨å±€ç»“æ„ç‰¹å¾ï¼Œå¿½è§†åŒ…å«é‡è¦ç—…ç†ä¿¡æ¯çš„å±€éƒ¨ç‰¹å¾ã€‚</li>
<li>SSIMæŸå¤±åœ¨å¤„ç†åƒç´ å¼ºåº¦ã€äº®åº¦å› ç´ å’Œæ–¹å·®ä¸åŒçš„å›¾åƒæ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>æå‡ºMotion-Aware Image SYnthesisï¼ˆMAISYï¼‰æ¨¡å‹ï¼Œé€šè¿‡Segment Anything Modelï¼ˆSAMï¼‰åŠ¨æ€å­¦ä¹ è§£å‰–è¾¹ç•Œçš„ç©ºé—´æ¨¡å¼æ¥è¯†åˆ«è¿åŠ¨ä¼ªå½±ã€‚</li>
<li>MAISYå¼•å…¥Variance-Selective SSIMï¼ˆVS-SSIMï¼‰æŸå¤±ï¼Œè‡ªé€‚åº”å¼ºè°ƒåƒç´ æ–¹å·®é«˜çš„åŒºåŸŸï¼Œä»¥ä¿ç•™å…³é”®è§£å‰–ç»†èŠ‚ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMAISYåœ¨èƒ¸éƒ¨å’Œå¤´éƒ¨CTæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-980ab98e0b523a5f9b4c0de90a997335.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91dd2baf30826453f3fa093afa4a5c38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aec9bc57c218e1014e2b1609ce0922f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Leveraging-Automatic-CAD-Annotations-for-Supervised-Learning-in-3D-Scene-Understanding"><a href="#Leveraging-Automatic-CAD-Annotations-for-Supervised-Learning-in-3D-Scene-Understanding" class="headerlink" title="Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene   Understanding"></a>Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene   Understanding</h2><p><strong>Authors:Yuchen Rao, Stefan Ainetter, Sinisa Stekovic, Vincent Lepetit, Friedrich Fraundorfer</strong></p>
<p>High-level 3D scene understanding is essential in many applications. However, the challenges of generating accurate 3D annotations make development of deep learning models difficult. We turn to recent advancements in automatic retrieval of synthetic CAD models, and show that data generated by such methods can be used as high-quality ground truth for training supervised deep learning models. More exactly, we employ a pipeline akin to the one previously used to automatically annotate objects in ScanNet scenes with their 9D poses and CAD models. This time, we apply it to the recent ScanNet++ v1 dataset, which previously lacked such annotations. Our findings demonstrate that it is not only possible to train deep learning models on these automatically-obtained annotations but that the resulting models outperform those trained on manually annotated data. We validate this on two distinct tasks: point cloud completion and single-view CAD model retrieval and alignment. Our results underscore the potential of automatic 3D annotations to enhance model performance while significantly reducing annotation costs. To support future research in 3D scene understanding, we will release our annotations, which we call SCANnotate++, along with our trained models. </p>
<blockquote>
<p>é«˜çº§ä¸‰ç»´åœºæ™¯ç†è§£åœ¨è®¸å¤šåº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”Ÿæˆç²¾ç¡®ä¸‰ç»´æ³¨é‡Šçš„æŒ‘æˆ˜ä½¿å¾—æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¼€å‘å˜å¾—å›°éš¾ã€‚æˆ‘ä»¬è½¬å‘æœ€è¿‘è‡ªåŠ¨æ£€ç´¢åˆæˆCADæ¨¡å‹çš„è¿›å±•ï¼Œå¹¶è¯æ˜é€šè¿‡æ­¤ç±»æ–¹æ³•ç”Ÿæˆçš„æ•°æ®å¯ä»¥ç”¨ä½œè®­ç»ƒæœ‰ç›‘ç£æ·±åº¦å­¦ä¹ æ¨¡å‹çš„é«˜è´¨é‡çœŸå®æ ‡æ³¨ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€æ¡ç±»ä¼¼äºä¹‹å‰ç”¨äºScanNetåœºæ™¯ä¸­è‡ªåŠ¨æ ‡æ³¨å¯¹è±¡ä¸å…¶ä¹ç»´å§¿æ€å’ŒCADæ¨¡å‹çš„ç®¡é“ã€‚è¿™æ¬¡ï¼Œæˆ‘ä»¬å°†å…¶åº”ç”¨äºæœ€æ–°çš„ScanNet++ v1æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¹‹å‰ç¼ºä¹æ­¤ç±»æ³¨é‡Šã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸ä»…å¯ä»¥åœ¨è¿™äº›è‡ªåŠ¨è·å¾—çš„æ³¨é‡Šä¸Šè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè€Œä¸”å¾—åˆ°çš„æ¨¡å‹çš„æ€§èƒ½è¶…è¿‡äº†åœ¨æ‰‹åŠ¨æ³¨é‡Šæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ä¸ªç‹¬ç«‹çš„ä»»åŠ¡éªŒè¯äº†è¿™ä¸€ç‚¹ï¼šç‚¹äº‘è¡¥å…¨å’Œå•è§†å›¾CADæ¨¡å‹æ£€ç´¢ä¸å¯¹é½ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†è‡ªåŠ¨ä¸‰ç»´æ³¨é‡Šåœ¨æé«˜æ¨¡å‹æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½æ ‡æ³¨æˆæœ¬çš„æ½œåŠ›ã€‚ä¸ºäº†æ”¯æŒæœªæ¥çš„ä¸‰ç»´åœºæ™¯ç†è§£ç ”ç©¶ï¼Œæˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„æ³¨é‡Šï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºSCANnotate++ï¼‰ï¼Œä»¥åŠæˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13580v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://stefan-ainetter.github.io/SCANnotatepp">https://stefan-ainetter.github.io/SCANnotatepp</a>; CVPRâ€™25   Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ©ç”¨è‡ªåŠ¨æ£€ç´¢åˆæˆCADæ¨¡å‹çš„æ–°æŠ€æœ¯ï¼Œè§£å†³äº†ç”Ÿæˆå‡†ç¡®3Dæ³¨é‡Šçš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚æ–‡ç« é€šè¿‡ç±»ä¼¼ScanNetåœºæ™¯å¯¹è±¡è‡ªåŠ¨æ ‡æ³¨çš„ç®¡é“ï¼Œå¯¹ç¼ºä¹æ­¤ç±»æ ‡æ³¨çš„ScanNet++ v1æ•°æ®é›†è¿›è¡Œæ ‡æ³¨ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ä»…å¯ä»¥åœ¨è¿™äº›è‡ªåŠ¨è·å¾—çš„æ ‡æ³¨ä¸Šè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè€Œä¸”æ‰€å¾—æ¨¡å‹çš„æ€§èƒ½è¿˜ä¼˜äºæ‰‹åŠ¨æ ‡æ³¨æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚è¿™ä¸ºè‡ªåŠ¨3Dæ³¨é‡Šåœ¨å¢å¼ºæ¨¡å‹æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½æ ‡æ³¨æˆæœ¬æä¾›äº†å¯èƒ½ã€‚æ–‡ç« æœ€åå‘å¸ƒäº†åä¸ºSCANnotate++çš„æ ‡æ³¨åŠè®­ç»ƒæ¨¡å‹ï¼Œä»¥æ”¯æŒæœªæ¥çš„3Dåœºæ™¯ç†è§£ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­æ¢è®¨äº†åœ¨è®¸å¤šåº”ç”¨ä¸­ï¼Œé«˜çº§ä¸‰ç»´åœºæ™¯ç†è§£çš„é‡è¦æ€§åŠå…¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆå‡†ç¡®ä¸‰ç»´æ³¨é‡Šæ–¹é¢å­˜åœ¨çš„å›°éš¾å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¼€å‘é€ æˆäº†å½±å“ã€‚</li>
<li>æœ€è¿‘åœ¨è‡ªåŠ¨æ£€ç´¢åˆæˆCADæ¨¡å‹æ–¹é¢çš„è¿›å±•è¢«åº”ç”¨äºæ–‡ç« ä¸­è®¨è®ºçš„é—®é¢˜ï¼Œè¿™äº›æŠ€æœ¯è¿›æ­¥èƒ½è§£å†³æ·±åº¦å­¦ä¹ ä¸­è®­ç»ƒæ ·æœ¬æ³¨é‡Šçš„éš¾é¢˜ã€‚</li>
<li>æ–‡ç« é€šè¿‡ä½¿ç”¨ç±»ä¼¼ScanNetåœºæ™¯çš„ç®¡é“ï¼ŒæˆåŠŸå¯¹ç¼ºä¹æ­¤ç±»æ ‡æ³¨çš„ScanNet++ v1æ•°æ®é›†è¿›è¡Œäº†è‡ªåŠ¨æ ‡æ³¨ã€‚è¿™ä¸€æ–¹æ³•åˆ©ç”¨å…ˆå‰ç”¨äºå¯¹è±¡è‡ªåŠ¨æ ‡æ³¨çš„æŠ€æœ¯ï¼Œä½†è¿™æ¬¡åº”ç”¨åˆ°äº†æ–°çš„æ•°æ®é›†ä¸Šã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œä½¿ç”¨è‡ªåŠ¨è·å–çš„æ³¨é‡Šè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸ä»…å¯è¡Œï¼Œè€Œä¸”ç»“æœæ¨¡å‹çš„è¡¨ç°è¶…è¶Šäº†æ‰‹åŠ¨æ³¨é‡Šæ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚è¿™ä¸€å‘ç°ä½“ç°åœ¨ç‚¹äº‘è¡¥å…¨å’Œå•è§†å›¾CADæ¨¡å‹æ£€ç´¢ä¸å¯¹é½ä¸¤ä¸ªä»»åŠ¡ä¸Šã€‚</li>
<li>è‡ªåŠ¨ä¸‰ç»´æ³¨é‡ŠæŠ€æœ¯å…·æœ‰æé«˜æ¨¡å‹æ€§èƒ½åŒæ—¶æ˜¾è‘—é™ä½æ ‡æ³¨æˆæœ¬çš„æ½œåŠ›ã€‚è¿™ä¸€æŠ€æœ¯çš„å®é™…åº”ç”¨ä»·å€¼å¾—åˆ°äº†éªŒè¯ã€‚</li>
<li>ä¸ºäº†æ”¯æŒæœªæ¥çš„ç ”ç©¶ï¼Œæ–‡ç« å‘å¸ƒäº†åä¸ºSCANnotate++çš„æ³¨é‡Šä»¥åŠè®­ç»ƒå¥½çš„æ¨¡å‹ã€‚è¿™å°†ä¸ºä¸‰ç»´åœºæ™¯ç†è§£ç ”ç©¶æä¾›æ–°çš„èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0851917d7116fe5081af0a81fbfbe157.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63032ce7a7c64077e664f27e6a061354.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-261fc97f44c75d77faa745b096a25866.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe3ea7b428a57cad58f9a68d93a6a9a8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0303bb75b63c6b19cecda0d4ca074201.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Comprehensive-Analysis-of-Relative-Pressure-Estimation-Methods-Utilizing-4D-Flow-MRI"><a href="#Comprehensive-Analysis-of-Relative-Pressure-Estimation-Methods-Utilizing-4D-Flow-MRI" class="headerlink" title="Comprehensive Analysis of Relative Pressure Estimation Methods Utilizing   4D Flow MRI"></a>Comprehensive Analysis of Relative Pressure Estimation Methods Utilizing   4D Flow MRI</h2><p><strong>Authors:Brandon Hardy, Judith Zimmermann, Vincent Lechner, Mia Bonini, Julio A. Sotelo, Nicholas S. Burris, Daniel B. Ennis, David Marlevi, David A. Nordsletten</strong></p>
<p>Magnetic resonance imaging (MRI) can estimate three-dimensional (3D) time-resolved relative pressure fields using 4D-flow MRI, thereby providing rich pressure field information. Clinical alternatives include catheterization and Doppler echocardiography, which only provide one-dimensional pressure drops. The accuracy of one-dimensional pressure drops derived from 4D-flow has been explored previously, but additional work is needed to evaluate the accuracy of 3D relative pressure field estimates. This work presents an analysis of three state-of-the-art relative pressure estimators: virtual Work-Energy Relative Pressure (vWERP), the Pressure Poisson Estimator (PPE), and the Stokes Estimator (STE). The spatiotemporal characteristics and sensitivity to noise were determined in silico. Estimators were then validated using a type B aortic dissection (TBAD) flow phantom with varying tear geometry and twelve catheter pressure measurements. Finally, the performance of each estimator was evaluated across eight patient cases. In silico pressure field errors were lower in STE compared to PPE, although PPE pressures were less noise sensitive. High velocity gradients and low spatial resolution contributed most significantly to local variations in 3D pressure field errors. Low temporal resolution lead to systematic underestimation of highly transient peak pressure events. In the flow phantom analysis, vWERP was the most accurate method, followed by STE and PPE. Each pressure estimator was strongly correlated with ground truth pressure values, despite the tendency to underestimate peak pressures. Patient case results demonstrated that each pressure estimator could be feasibly integrated into a clinical workflow. </p>
<blockquote>
<p>åˆ©ç”¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰çš„4DæµæŠ€æœ¯å¯ä»¥ä¼°è®¡ä¸‰ç»´ï¼ˆ3Dï¼‰æ—¶é—´åˆ†è¾¨çš„ç›¸å¯¹å‹åŠ›åœºï¼Œä»è€Œæä¾›ä¸°å¯Œçš„å‹åŠ›åœºä¿¡æ¯ã€‚ä¸´åºŠä¸Šå¯é€‰æ‹©çš„æ–¹æ³•åŒ…æ‹¬å¯¼ç®¡æ’å…¥æ³•å’Œå¤šæ™®å‹’è¶…å£°å¿ƒåŠ¨å›¾æ³•ï¼Œä½†è¿™ä¸¤è€…ä»…èƒ½æä¾›ä¸€ç»´çš„å‹åŠ›ä¸‹é™ä¿¡æ¯ã€‚ä»¥å‰å·²ç»æ¢ç´¢è¿‡ä»å››ç»´æµä¸­æå–ä¸€ç»´å‹åŠ›ä¸‹é™çš„å‡†ç¡®æ€§ï¼Œä½†è¿˜éœ€è¦æ›´å¤šçš„å·¥ä½œæ¥è¯„ä¼°ä¸‰ç»´ç›¸å¯¹å‹åŠ›åœºä¼°è®¡çš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸‰ç§æœ€å…ˆè¿›çš„ç›¸å¯¹å‹åŠ›ä¼°è®¡å™¨ï¼šè™šæ‹Ÿå·¥ä½œèƒ½é‡ç›¸å¯¹å‹åŠ›ï¼ˆvWERPï¼‰ã€å‹åŠ›æ³Šæ¾ä¼°è®¡å™¨ï¼ˆPPEï¼‰å’Œæ–¯æ‰˜å…‹æ–¯ä¼°è®¡å™¨ï¼ˆSTEï¼‰ã€‚è¿™äº›ä¼°è®¡å™¨çš„æ—¶ç©ºç‰¹æ€§å’Œå¯¹å™ªå£°çš„æ•æ„Ÿæ€§éƒ½æ˜¯é€šè¿‡è®¡ç®—æœºæ¨¡æ‹Ÿç¡®å®šçš„ã€‚ç„¶ååˆ©ç”¨Bå‹ä¸»åŠ¨è„‰å¤¹å±‚ï¼ˆTBADï¼‰æµåŠ¨æ¨¡å‹è¿›è¡ŒéªŒè¯ï¼Œè¯¥æ¨¡å‹å…·æœ‰ä¸åŒçš„æ’•è£‚å‡ ä½•å½¢çŠ¶å’ŒåäºŒæ¬¡å¯¼ç®¡å‹åŠ›æµ‹é‡å€¼ã€‚æœ€åï¼Œå¯¹æ¯ç§ä¼°è®¡å™¨åœ¨å…«ä¾‹æ‚£è€…ä¸­çš„è¡¨ç°è¿›è¡Œäº†è¯„ä¼°ã€‚æ¨¡æ‹Ÿçš„å‹åŠ›åœºè¯¯å·®åœ¨STEä¸­ä½äºPPEï¼Œä½†PPEå¯¹å™ªå£°çš„æ•æ„Ÿæ€§è¾ƒä½ã€‚é«˜é€Ÿæ¢¯åº¦å’Œä½ç©ºé—´åˆ†è¾¨ç‡å¯¹ä¸‰ç»´å‹åŠ›åœºè¯¯å·®çš„å±€éƒ¨å˜åŒ–è´¡çŒ®æœ€å¤§ã€‚ä½æ—¶é—´åˆ†è¾¨ç‡å¯¼è‡´é«˜åº¦ç¬æ€å³°å€¼å‹åŠ›äº‹ä»¶çš„ç³»ç»Ÿæ€§ä½ä¼°ã€‚åœ¨æµåŠ¨æ¨¡å‹åˆ†æä¸­ï¼ŒvWERPæ˜¯æœ€å‡†ç¡®çš„æ–¹æ³•ï¼Œå…¶æ¬¡æ˜¯STEå’ŒPPEã€‚å°½ç®¡æœ‰ä½ä¼°å³°å€¼å‹åŠ›çš„è¶‹åŠ¿ï¼Œä½†æ¯ä¸ªå‹åŠ›ä¼°è®¡å€¼ä¸çœŸå®å‹åŠ›å€¼éƒ½æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ã€‚æ‚£è€…æ¡ˆä¾‹ç»“æœè¡¨æ˜ï¼Œæ¯ä¸ªå‹åŠ›ä¼°è®¡å™¨éƒ½å¯ä»¥åˆç†åœ°é›†æˆåˆ°ä¸´åºŠå·¥ä½œæµç¨‹ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02847v2">PDF</a> 10 pages, 8 figures. Planned submission to IEEE Transactions on   Medical Imaging</p>
<p><strong>Summary</strong><br>     åŸºäºç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰çš„4D-æµMRIæŠ€æœ¯å¯ä¼°è®¡ä¸‰ç»´æ—¶é—´è§£æçš„ç›¸å¯¹å‹åŠ›åœºï¼Œæä¾›ä¸°å¯Œçš„å‹åŠ›åœºä¿¡æ¯ã€‚æœ¬ç ”ç©¶å¯¹ä¸‰ç§å…ˆè¿›çš„ç›¸å¯¹å‹åŠ›ä¼°è®¡å™¨è¿›è¡Œåˆ†æï¼ŒåŒ…æ‹¬è™šæ‹Ÿå·¥ä½œèƒ½é‡ç›¸å¯¹å‹åŠ›ï¼ˆvWERPï¼‰ã€å‹åŠ›æ³Šæ¾ä¼°è®¡å™¨ï¼ˆPPEï¼‰å’Œæ–¯æ‰˜å…‹æ–¯ä¼°è®¡å™¨ï¼ˆSTEï¼‰ã€‚ç ”ç©¶ç¡®å®šäº†è¿™äº›ä¼°è®¡å™¨çš„æ—¶ç©ºç‰¹æ€§å’Œå¯¹å™ªå£°çš„æ•æ„Ÿæ€§ï¼Œå¹¶ä½¿ç”¨Bå‹ä¸»åŠ¨è„‰å¤¹å±‚ï¼ˆTBADï¼‰æµæ¨¡å‹å’Œå¯¼ç®¡å‹åŠ›æµ‹é‡è¿›è¡ŒéªŒè¯ã€‚å°½ç®¡å­˜åœ¨å±€é™æ€§ï¼Œä½†å„ä¼°è®¡å™¨ä¸æ‚£è€…å‹åŠ›å€¼é«˜åº¦ç›¸å…³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>4D-flow MRIå¯ä¼°è®¡ä¸‰ç»´æ—¶é—´è§£æçš„ç›¸å¯¹å‹åŠ›åœºï¼Œæä¾›ä¸°å¯Œçš„å‹åŠ›ä¿¡æ¯ã€‚</li>
<li>ä¸‰ç§ç›¸å¯¹å‹åŠ›ä¼°è®¡å™¨ï¼ˆvWERPã€PPEã€STEï¼‰çš„åˆ†æè¡¨æ˜ï¼Œå®ƒä»¬åœ¨æ¨¡æ‹Ÿå‹åŠ›åœºè¯¯å·®å’Œå™ªå£°æ•æ„Ÿæ€§æ–¹é¢æœ‰æ‰€ä¸åŒã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ¨¡å‹ä¸­ï¼ŒvWERPåœ¨å‹åŠ›ä¼°è®¡æ–¹é¢è¡¨ç°æœ€ä¸ºå‡†ç¡®ã€‚</li>
<li>å‹åŠ›ä¼°è®¡å™¨ä¸çœŸå®å‹åŠ›å€¼é«˜åº¦ç›¸å…³ï¼Œä½†å¯èƒ½ä½ä¼°å³°å€¼å‹åŠ›ã€‚</li>
<li>è¿™äº›å‹åŠ›ä¼°è®¡å™¨å¯ä»¥æ•´åˆåˆ°ä¸´åºŠå·¥ä½œæµç¨‹ä¸­ï¼Œä¸ºä¸´åºŠåº”ç”¨æä¾›æœ‰ä»·å€¼çš„å·¥å…·ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02847">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-790d7114171891d0283967c7ca5b15d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80c63795bee373346e295543e9aafb3b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-38c21217838e4d2829ab7addcca062fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91e23b476a979995e867e6b3defb9932.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2890ae6a8433debb414f99381d3237c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-edfe7e20d0971446acaa7258614a2b2a.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  Lightweight End-to-end Text-to-speech Synthesis for low resource   on-device applications
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2d7626b360820d90ff0b18a4d6db5aa3.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  DanceGRPO Unleashing GRPO on Visual Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23154.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
