<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  DanceGRPO Unleashing GRPO on Visual Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-18c87c0687e277dbbc33eb97e5ec28e3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-14-æ›´æ–°"><a href="#2025-05-14-æ›´æ–°" class="headerlink" title="2025-05-14 æ›´æ–°"></a>2025-05-14 æ›´æ–°</h1><h2 id="DanceGRPO-Unleashing-GRPO-on-Visual-Generation"><a href="#DanceGRPO-Unleashing-GRPO-on-Visual-Generation" class="headerlink" title="DanceGRPO: Unleashing GRPO on Visual Generation"></a>DanceGRPO: Unleashing GRPO on Visual Generation</h2><p><strong>Authors:Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, Ping Luo</strong></p>
<p>Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image&#x2F;video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released. </p>
<blockquote>
<p>è¿‘æœŸç”Ÿæˆæ¨¡å‹ï¼ˆå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹å’Œæ ¡æ­£æµï¼‰çš„çªç ´ä¸ºè§†è§‰å†…å®¹åˆ›ä½œå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œä½†å¦‚ä½•ä½¿æ¨¡å‹è¾“å‡ºä¸äººç±»åå¥½ä¿æŒä¸€è‡´ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è§†è§‰ç”Ÿæˆæ–¹æ³•é¢ä¸´å…³é”®å±€é™æ€§ï¼šä¸ç°ä»£åŸºäºå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰çš„é‡‡æ ·èŒƒå¼ä¸å…¼å®¹ã€å¤§è§„æ¨¡è®­ç»ƒä¸ç¨³å®šã€ä»¥åŠè§†é¢‘ç”Ÿæˆçš„éªŒè¯ç¼ºä¹ç­‰ã€‚æœ¬æ–‡ä»‹ç»äº†DanceGRPOï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé€‚åº”é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„è§†è§‰ç”ŸæˆèŒƒå¼ç»Ÿä¸€æ¡†æ¶ï¼Œé‡Šæ”¾äº†ä¸€ç§ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ¶µç›–äº†ä¸¤ç§ç”ŸæˆèŒƒå¼ï¼ˆæ‰©æ•£æ¨¡å‹å’Œæ ¡æ­£æµï¼‰ã€ä¸‰é¡¹ä»»åŠ¡ï¼ˆæ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘ã€å›¾åƒåˆ°è§†é¢‘ï¼‰ã€å››ç§åŸºç¡€æ¨¡å‹ï¼ˆç¨³å®šæ‰©æ•£ã€ç¯è¿œè§†é¢‘ã€æµé‡ã€SkyReel-I2Vï¼‰å’Œäº”ç§å¥–åŠ±æ¨¡å‹ï¼ˆå›¾åƒ&#x2F;è§†é¢‘ç¾å­¦ã€æ–‡æœ¬-å›¾åƒå¯¹é½ã€è§†é¢‘è¿åŠ¨è´¨é‡ã€äºŒå…ƒå¥–åŠ±ï¼‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒDanceGRPOæ˜¯ç¬¬ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„ç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„ç”ŸæˆèŒƒå¼ã€ä»»åŠ¡ã€åŸºç¡€æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹ä¹‹é—´æ— ç¼é€‚åº”ã€‚DanceGRPOåœ¨HPS-v2.1ã€CLIP Scoreã€VideoAlignå’ŒGenEvalç­‰åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æŒç»­ä¸”æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¶…è¶ŠåŸºçº¿æœ€å¤šè¾¾181%ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒDanceGRPOä¸ä»…èƒ½å¤Ÿç¨³å®šå¤æ‚è§†é¢‘ç”Ÿæˆçš„ç­–ç•¥ä¼˜åŒ–ï¼Œè¿˜èƒ½å¤Ÿä½¿ç”Ÿæˆç­–ç•¥æ›´å¥½åœ°æ•æ‰å»å™ªè½¨è¿¹ä»¥å®ç°æœ€ä½³Næ¨ç†æ‰©å±•ï¼Œå¹¶ä»ç¨€ç–çš„äºŒå…ƒåé¦ˆä¸­å­¦ä¹ ã€‚æˆ‘ä»¬çš„ç»“æœç¡®ç«‹äº†DanceGRPOåœ¨è§†è§‰ç”Ÿæˆä¸­æ‰©å±•å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä»»åŠ¡çš„ç¨³å¥æ€§å’Œå¤šåŠŸèƒ½æ€§è§£å†³æ–¹æ¡ˆï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä¸åˆæˆä¹‹é—´çš„å’Œè°æä¾›äº†æ–°è§è§£ã€‚ä»£ç å°†è¢«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07818v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://dancegrpo.github.io/">https://dancegrpo.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DanceGRPOæ¡†æ¶ï¼Œå®ƒæ˜¯åŸºäºå¼ºåŒ–å­¦ä¹ çš„è§†è§‰ç”Ÿæˆç»Ÿä¸€æ¡†æ¶ï¼Œå°†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰é€‚åº”äºè§†è§‰ç”Ÿæˆæ¨¡å¼ã€‚è¯¥æ¡†æ¶å¯é€‚åº”ä¸åŒçš„ç”Ÿæˆæ¨¡å¼ã€ä»»åŠ¡ã€åŸºç¡€æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹ï¼Œå¯¹å¤æ‚çš„è§†é¢‘ç”Ÿæˆè¿›è¡Œç¨³å®šç­–ç•¥ä¼˜åŒ–ï¼Œå¹¶èƒ½ä»ç¨€ç–çš„äºŒå…ƒåé¦ˆä¸­å­¦ä¹ ã€‚DanceGRPOæ˜¾è‘—æé«˜äº†åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ï¼Œå¦‚HPS-v2.1ã€CLIP Scoreã€VideoAlignå’ŒGenEvalç­‰ï¼Œå±•ç¤ºäº†å…¶åœ¨è§†è§‰ç”Ÿæˆé¢†åŸŸçš„é²æ£’æ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DanceGRPOæ˜¯é¦–ä¸ªç»Ÿä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€‚ç”¨äºè§†è§‰ç”Ÿæˆçš„å¤šç§æ¨¡å¼ã€ä»»åŠ¡å’ŒåŸºç¡€æ¨¡å‹ã€‚</li>
<li>è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰å¼ºåŒ–å­¦ä¹ åœ¨è§†è§‰ç”Ÿæˆä¸­çš„å…³é”®é—®é¢˜ï¼Œå¦‚ä¸ODEsé‡‡æ ·èŒƒå¼çš„ä¸å…¼å®¹æ€§ã€å¤§è§„æ¨¡è®­ç»ƒçš„ä¸ç¨³å®šæ€§ä»¥åŠè§†é¢‘ç”Ÿæˆçš„éªŒè¯ç¼ºä¹ã€‚</li>
<li>DanceGRPOé€šè¿‡ç­–ç•¥ä¼˜åŒ–æé«˜äº†åŸºå‡†æµ‹è¯•æ€§èƒ½ï¼Œæœ€é«˜æå‡äº†181%ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿç¨³å®šå¤æ‚çš„è§†é¢‘ç”Ÿæˆç­–ç•¥ä¼˜åŒ–ï¼Œå¹¶ä»ç¨€ç–çš„äºŒå…ƒåé¦ˆä¸­å­¦ä¹ ã€‚</li>
<li>DanceGRPOèƒ½å¤Ÿæ•æ‰å»å™ªè½¨è¿¹ï¼Œæ”¯æŒBest-of-Næ¨ç†æ‰©å±•ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä»»åŠ¡åœ¨è§†è§‰ç”Ÿæˆæ–¹é¢çš„åº”ç”¨æä¾›äº†ç¨³å¥ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07818">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2fec2de9c2417547490ce2852cf3dec3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fab798b748ba34cb5165f16be7cbbc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cdcefc2c62b6f11a626e04327dd794f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Learning-from-Peers-in-Reasoning-Models"><a href="#Learning-from-Peers-in-Reasoning-Models" class="headerlink" title="Learning from Peers in Reasoning Models"></a>Learning from Peers in Reasoning Models</h2><p><strong>Authors:Tongxu Luo, Wenyu Du, Jiaxi Bi, Stephen Chung, Zhengyang Tang, Hao Yang, Min Zhang, Benyou Wang</strong></p>
<p>Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the â€œPrefix Dominance Trapâ€. Inspired by psychological findings that peer interaction can promote self-correction without negatively impacting already accurate individuals, we propose <strong>Learning from Peers</strong> (LeaP) to address this phenomenon. Specifically, every tokens, each reasoning path summarizes its intermediate reasoning and shares it with others through a routing mechanism, enabling paths to incorporate peer insights during inference. However, we observe that smaller models sometimes fail to follow summarization and reflection instructions effectively. To address this, we fine-tune them into our <strong>LeaP-T</strong> model series. Experiments on AIME 2024, AIME 2025, AIMO 2025, and GPQA Diamond show that LeaP provides substantial improvements. For instance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the baseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks with an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis reveals LeaPâ€™s robust error correction by timely peer insights, showing strong error tolerance and handling varied task difficulty. LeaP marks a milestone by enabling LRMs to collaborate during reasoning. Our code, datasets, and models are available at <a target="_blank" rel="noopener" href="https://learning-from-peers.github.io/">https://learning-from-peers.github.io/</a> . </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å³ä½¿åœ¨æ¨ç†è·¯å¾„ä¸­å‡ºé”™ä¹Ÿæœ‰è‡ªæˆ‘çº æ­£çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå½“æ¨ç†è¿‡ç¨‹ä»ä¸€ä¸ªçŸ­æš‚è€Œç³Ÿç³•çš„å¼€å¤´å¼€å§‹æ—¶ï¼Œæ¨¡å‹å¾ˆéš¾æ¢å¤ã€‚æˆ‘ä»¬å°†è¿™ç§ç°è±¡ç§°ä¸ºâ€œå‰ç¼€ä¸»å¯¼é™·é˜±â€ã€‚å—å¿ƒç†å­¦å‘ç°çš„å¯å‘ï¼Œå³åŒä¼´äº’åŠ¨å¯ä»¥ä¿ƒè¿›è‡ªæˆ‘çº æ­£ï¼Œè€Œä¸ä¼šç»™å·²ç»å‡†ç¡®çš„ä¸ªä½“å¸¦æ¥è´Ÿé¢å½±å“ï¼Œæˆ‘ä»¬æå‡ºäº†â€œä»åŒä¼´å­¦ä¹ â€ï¼ˆLeaPï¼‰æ¥è§£å†³è¿™ä¸€ç°è±¡ã€‚å…·ä½“æ¥è¯´ï¼Œæ¯æ¡æ¨ç†è·¯å¾„éƒ½ä¼šæ±‡æ€»å…¶ä¸­é—´æ¨ç†ï¼Œå¹¶é€šè¿‡è·¯ç”±æœºåˆ¶ä¸å…¶ä»–è·¯å¾„åˆ†äº«ï¼Œä»è€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­èå…¥åŒä¼´çš„è§è§£ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¾ƒå°çš„æ¨¡å‹æœ‰æ—¶ä¸èƒ½æœ‰æ•ˆåœ°éµå¾ªæ€»ç»“å’Œåæ€æŒ‡ä»¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å°†å®ƒä»¬å¾®è°ƒä¸º<strong>LeaP-T</strong>æ¨¡å‹ç³»åˆ—ã€‚åœ¨AIME 2024ã€AIME 2025ã€AIMO 2025å’ŒGPQA Diamondä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLeaPæä¾›äº†å®è´¨æ€§çš„æ”¹è¿›ã€‚ä¾‹å¦‚ï¼Œå¸¦æœ‰LeaPçš„QwQ-32Båœ¨å¹³å‡å¾—åˆ†ä¸Šæ¯”åŸºçº¿é«˜å‡ºè¿‘5åˆ†ï¼Œå¹¶åœ¨ä¸‰ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†DeepSeek-R1-671Bï¼Œå¹³å‡æé«˜äº†3.3åˆ†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å¾®è°ƒçš„LeaP-T-7Båœ¨AIME 2024ä¸Šçš„æ€§èƒ½ä¸DeepSeek-R1-Distill-Qwen-14Bç›¸åŒ¹é…ã€‚æ·±å…¥åˆ†ææ­ç¤ºäº†LeaPé€šè¿‡åŠæ—¶çš„åŒä¼´è§è§£è¿›è¡Œç¨³å¥çš„é”™è¯¯æ ¡æ­£ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„é”™è¯¯å®¹å¿èƒ½åŠ›å’Œå¤„ç†å„ç§ä»»åŠ¡éš¾åº¦çš„èƒ½åŠ›ã€‚LeaPæ ‡å¿—ç€LRMsåœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œåä½œçš„ä¸€ä¸ªé‡Œç¨‹ç¢‘ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://learning-from-peers.github.io/%E6%89%BE%E5%88%B0%E3%80%82">https://learning-from-peers.github.io/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07787v1">PDF</a> 29 pages, 32 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å…·å¤‡è‡ªæˆ‘çº é”™èƒ½åŠ›ï¼Œä½†å½“æ¨ç†è¿‡ç¨‹èµ·å§‹äºçŸ­å°çš„é”™è¯¯å¼€å¤´æ—¶ï¼Œæ¨¡å‹éš¾ä»¥æ¢å¤ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºâ€œå‰ç¼€ä¸»å¯¼é™·é˜±â€ã€‚æœ¬ç ”ç©¶å—å¿ƒç†ç ”ç©¶çš„å¯å‘ï¼Œæå‡ºäº†â€œLearning from Peersâ€ï¼ˆLeaPï¼‰æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡è·¯ç”±æœºåˆ¶ä½¿æ¯æ¡æ¨ç†è·¯å¾„æ±‡æ€»å…¶ä¸­é—´æ¨ç†å¹¶ä¸å…¶ä»–è·¯å¾„åˆ†äº«ï¼Œä»è€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­èå…¥åŒä¼´è§è§£ã€‚é’ˆå¯¹å°å‹æ¨¡å‹åœ¨æ‰§è¡Œæ€»ç»“ä¸åæ€æŒ‡ä»¤æ—¶çš„ä¸è¶³ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¾®è°ƒï¼Œå½¢æˆäº†<strong>LeaP-T</strong>ç³»åˆ—æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒLeaPåœ¨AIME 2024ã€AIME 2025ã€AIMO 2025å’ŒGPQA Diamondä¸Šçš„è¡¨ç°æœ‰æ˜¾è‘—æå‡ã€‚ä¾‹å¦‚ï¼ŒQwQ-32Båœ¨ä½¿ç”¨LeaPåå¹³å‡æ¯”åŸºçº¿é«˜å‡ºè¿‘5ä¸ªç»å¯¹ç‚¹ï¼Œå¹¶åœ¨ä¸‰ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šå¹³å‡è¶…è¶Šäº†DeepSeek-R1-671B 3.3ç‚¹ã€‚å°¤å…¶ç»è¿‡è°ƒæ ¡çš„LeaP-T-7Båœ¨AIME 0ä¸Šçš„è¡¨ç°ä¸DeepSeek-R1-Distill-Qwen-14Bç›¸åŒ¹é…ã€‚æ·±å…¥åˆ†ææ˜¾ç¤ºï¼ŒLeaPé€šè¿‡åŠæ—¶çš„åŒä¼´è§è§£å®ç°äº†ç¨³å¥çš„é”™è¯¯çº æ­£ï¼Œå±•ç°å‡ºå¼ºå¤§çš„é”™è¯¯å®¹å¿èƒ½åŠ›å’Œå¤„ç†ä¸åŒä»»åŠ¡éš¾åº¦çš„èƒ½åŠ›ã€‚LeaPæ ‡å¿—ç€LRMsåœ¨åä½œæ¨ç†æ–¹é¢çš„é‡Œç¨‹ç¢‘å¼è¿›å±•ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://learning-from-peers.github.io/%E8%8E%B7%E5%8F%96%E3%80%82">https://learning-from-peers.github.io/è·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å…·æœ‰è‡ªæˆ‘çº é”™èƒ½åŠ›ï¼Œä½†åœ¨é”™è¯¯å¼€å¤´åéš¾ä»¥æ¢å¤ï¼Œç§°ä¸ºâ€œå‰ç¼€ä¸»å¯¼é™·é˜±â€ã€‚</li>
<li>æå‡ºâ€œLearning from Peersâ€ï¼ˆLeaPï¼‰æ–¹æ³•ï¼Œé€šè¿‡åŒä¼´é—´çš„äº¤äº’ä¿ƒè¿›è‡ªæˆ‘çº é”™ã€‚</li>
<li>LeaPæ–¹æ³•ä½¿æ¯æ¡æ¨ç†è·¯å¾„èƒ½å¤Ÿæ±‡æ€»å¹¶åˆ†äº«ä¸­é—´æ¨ç†ï¼Œä»è€Œèå…¥åŒä¼´è§è§£ã€‚</li>
<li>é’ˆå¯¹å°å‹æ¨¡å‹çš„ä¸è¶³ï¼Œè¿›è¡Œäº†å¾®è°ƒå½¢æˆLeaP-Tç³»åˆ—ã€‚</li>
<li>LeaPåœ¨å¤šä¸ªå®éªŒä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¦‚QwQ-32Bå’ŒDeepSeek-R1-671Bçš„æ¯”è¾ƒã€‚</li>
<li>LeaP-T-7Bæ¨¡å‹ç»è¿‡è°ƒæ ¡åï¼Œåœ¨AIME 2024ä¸Šçš„è¡¨ç°ä¸é«˜çº§æ¨¡å‹ç›¸åŒ¹æ•Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07787">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a50c4e2e6df89cb8e53e5cbed35f63e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd50090325447a85fe985d67fbaff1a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-135d9f85bed1391f236df1f21b816910.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-003e99494c347341aedd71f5872c09ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-367ec4f2f42e619a9a16baf15858baa9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5662cc26228dddf5819163cf3deb4ed3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Agent-RL-Scaling-Law-Agent-RL-with-Spontaneous-Code-Execution-for-Mathematical-Problem-Solving"><a href="#Agent-RL-Scaling-Law-Agent-RL-with-Spontaneous-Code-Execution-for-Mathematical-Problem-Solving" class="headerlink" title="Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for   Mathematical Problem Solving"></a>Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for   Mathematical Problem Solving</h2><p><strong>Authors:Xinji Mai, Haotian Xu, Xing W, Weinong Wang, Yingying Zhang, Wenqiang Zhang</strong></p>
<p>Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \href{<a target="_blank" rel="noopener" href="https://github.com/Anonymize-Author/AgentRL%7D%7Bhttps://github.com/Anonymize-Author/AgentRL%7D">https://github.com/Anonymize-Author/AgentRL}{https://github.com/Anonymize-Author/AgentRL}</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿›è¡Œéœ€è¦ç²¾ç¡®ã€å¯éªŒè¯è®¡ç®—çš„æ•°å­¦æ¨ç†ä»»åŠ¡æ—¶ç»å¸¸é‡åˆ°å›°éš¾ã€‚è™½ç„¶åŸºäºç»“æœå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºäº†æ–‡æœ¬æ¨ç†èƒ½åŠ›ï¼Œä½†äº†è§£æ™ºèƒ½ä½“å¦‚ä½•è‡ªä¸»å­¦ä¹ åˆ©ç”¨å¦‚ä»£ç æ‰§è¡Œç­‰å¤–éƒ¨å·¥å…·ä»ç„¶è‡³å…³é‡è¦ã€‚æˆ‘ä»¬ç ”ç©¶äº†åŸºäºç»“æœå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ åœ¨å·¥å…·é›†æˆæ¨ç†ï¼ˆTool-Integrated Reasoningï¼‰ä¸­çš„åº”ç”¨ï¼Œå³ZeroTIRã€‚æˆ‘ä»¬è®­ç»ƒåŸºç¡€LLMï¼Œä½¿å…¶èƒ½å¤Ÿé’ˆå¯¹æ•°å­¦é—®é¢˜è‡ªå‘åœ°ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç ï¼Œæ— éœ€ç›‘ç£å·¥å…·ä½¿ç”¨ç¤ºä¾‹ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®åœ¨äºè¯æ˜éšç€RLè®­ç»ƒçš„è¿›è¡Œï¼Œå…³é”®æŒ‡æ ‡çš„å¯é¢„æµ‹æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å¼ºçƒˆçš„æ­£ç›¸å…³å…³ç³»ï¼Œå…¶ä¸­è®­ç»ƒæ­¥éª¤çš„å¢åŠ å¯¼è‡´è‡ªå‘ä»£ç æ‰§è¡Œé¢‘ç‡ã€å¹³å‡å“åº”é•¿åº¦å’Œæœ€ç»ˆä»»åŠ¡å‡†ç¡®åº¦çš„æé«˜ã€‚è¿™è¡¨æ˜åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æŠ•å…¥çš„è®¡ç®—åŠªåŠ›ä¸æœ‰æ•ˆå·¥å…·å¢å¼ºæ¨ç†ç­–ç•¥çš„å‡ºç°ä¹‹é—´å­˜åœ¨å¯é‡åŒ–çš„å…³ç³»ã€‚æˆ‘ä»¬å®æ–½äº†ä¸€ä¸ªç¨³å¥çš„æ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªè§£è€¦çš„ä»£ç æ‰§è¡Œç¯å¢ƒï¼Œå¹¶åœ¨æ ‡å‡†çš„RLç®—æ³•å’Œæ¡†æ¶ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ã€‚å®éªŒè¡¨æ˜ï¼ŒZeroTIRåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—è¶…è¶Šäº†éå·¥å…·ZeroRLçš„åŸºå‡†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºè‡ªä¸»å·¥å…·è·å–å’Œæ™ºèƒ½ä½“RLå†…çš„æ‰©å±•æä¾›äº†åŸºæœ¬ç†è§£ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¯å¤åˆ¶çš„æ ‡å‡†ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Anonymize-Author/AgentRL%E3%80%82">https://github.com/Anonymize-Author/AgentRLã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07773v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°æ¬ ä½³ï¼Œéœ€è¦ç²¾ç¡®ã€å¯éªŒè¯çš„è®¡ç®—ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»ç»“æœå¯¼å‘å¥–åŠ±å‡ºå‘ï¼Œæå‡å·¥å…·é›†æˆæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºZeroTIRçš„æ–¹æ³•ï¼Œè®­ç»ƒåŸºç¡€LLMè‡ªä¸»ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç è§£å†³æ•°å­¦é—®é¢˜ï¼Œæ— éœ€ç›‘ç£å·¥å…·ä½¿ç”¨ç¤ºä¾‹ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€RLè®­ç»ƒçš„è¿›è¡Œï¼Œå…³é”®æŒ‡æ ‡å¯é¢„æµ‹åœ°å¢é•¿ã€‚å¦‚è®­ç»ƒæ­¥éª¤çš„å¢åŠ å¯¼è‡´è‡ªä¸»ä»£ç æ‰§è¡Œé¢‘ç‡ã€å¹³å‡å“åº”é•¿åº¦å’Œä»»åŠ¡å‡†ç¡®åº¦çš„æé«˜ã€‚è¿™æ˜¾ç¤ºäº†è®­ç»ƒæŠ•å…¥çš„è®¡ç®—åŠªåŠ›ä¸å‡ºç°æœ‰æ•ˆå·¥å…·å¢å¼ºæ¨ç†ç­–ç•¥ä¹‹é—´çš„é‡åŒ–å…³ç³»ã€‚å®éªŒè¯æ˜ï¼ŒZeroTIRåœ¨éå·¥å…·ZeroRLåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—è¶…è¶Šäº†å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºè‡ªä¸»å·¥å…·è·å–å’ŒAgent RLå†…çš„æ‰©å±•æä¾›äº†åŸºç¡€ç†è§£ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å¯å¤ç°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsé¢ä¸´æ•°å­¦æ¨ç†ä»»åŠ¡æŒ‘æˆ˜ï¼Œéœ€è¦å¢å¼ºè®¡ç®—ç²¾ç¡®åº¦ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ (RL)èƒ½æé«˜å·¥å…·é›†æˆæ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åŸºäºç»“æœå¯¼å‘å¥–åŠ±çš„æ–¹æ³•ã€‚</li>
<li>ZeroTIRæ–¹æ³•ä½¿LLMsèƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç æ¥è§£å†³æ•°å­¦é—®é¢˜ï¼Œæ— éœ€å·¥å…·ä½¿ç”¨ç¤ºä¾‹ã€‚</li>
<li>éšç€RLè®­ç»ƒçš„è¿›è¡Œï¼Œå…³é”®æ€§èƒ½æŒ‡æ ‡å¦‚è‡ªä¸»ä»£ç æ‰§è¡Œé¢‘ç‡ã€å“åº”é•¿åº¦å’Œä»»åŠ¡å‡†ç¡®åº¦å‘ˆç°æ­£å‘å¢é•¿è¶‹åŠ¿ã€‚</li>
<li>æŠ•å…¥çš„è®¡ç®—åŠªåŠ›ä¸å‡ºç°æœ‰æ•ˆçš„å·¥å…·å¢å¼ºæ¨ç†ç­–ç•¥ä¹‹é—´å­˜åœ¨é‡åŒ–å…³ç³»ã€‚</li>
<li>ZeroTIRæ–¹æ³•åœ¨æ ‡å‡†RLç®—æ³•å’Œæ¡†æ¶ä¸Šè¡¨ç°ç¨³å¥ï¼Œæ˜¾è‘—è¶…è¶Šäº†éå·¥å…·ZeroRLåŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4bead99dc01c1c0a2564e544a77b289f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f2c8423fa863c4270331b0d5e445251.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38dca8f3d7b54fe586e911c87832c5ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2a125a26e4ebf81a82a980542b6bb06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76ef8dc29a2e40231341a5008d2b5793.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="The-Pitfalls-of-Benchmarking-in-Algorithm-Selection-What-We-Are-Getting-Wrong"><a href="#The-Pitfalls-of-Benchmarking-in-Algorithm-Selection-What-We-Are-Getting-Wrong" class="headerlink" title="The Pitfalls of Benchmarking in Algorithm Selection: What We Are Getting   Wrong"></a>The Pitfalls of Benchmarking in Algorithm Selection: What We Are Getting   Wrong</h2><p><strong>Authors:GaÅ¡per Petelin, Gjorgjina Cenikj</strong></p>
<p>Algorithm selection, aiming to identify the best algorithm for a given problem, plays a pivotal role in continuous black-box optimization. A common approach involves representing optimization functions using a set of features, which are then used to train a machine learning meta-model for selecting suitable algorithms. Various approaches have demonstrated the effectiveness of these algorithm selection meta-models. However, not all evaluation approaches are equally valid for assessing the performance of meta-models. We highlight methodological issues that frequently occur in the community and should be addressed when evaluating algorithm selection approaches. First, we identify flaws with the â€œleave-instance-outâ€ evaluation technique. We show that non-informative features and meta-models can achieve high accuracy, which should not be the case with a well-designed evaluation framework. Second, we demonstrate that measuring the performance of optimization algorithms with metrics sensitive to the scale of the objective function requires careful consideration of how this impacts the construction of the meta-model, its predictions, and the modelâ€™s error. Such metrics can falsely present overly optimistic performance assessments of the meta-models. This paper emphasizes the importance of careful evaluation, as loosely defined methodologies can mislead researchers, divert efforts, and introduce noise into the field </p>
<blockquote>
<p>ç®—æ³•é€‰æ‹©å¯¹äºæŒç»­çš„é»‘ç›’ä¼˜åŒ–èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå…¶ç›®æ ‡æ˜¯ä¸ºç»™å®šé—®é¢˜æ‰¾åˆ°æœ€ä½³ç®—æ³•ã€‚ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯é€šè¿‡ä¸€ç³»åˆ—ç‰¹å¾æ¥è¡¨ç¤ºä¼˜åŒ–å‡½æ•°ï¼Œç„¶åä½¿ç”¨è¿™äº›ç‰¹å¾æ¥è®­ç»ƒç”¨äºé€‰æ‹©é€‚åˆç®—æ³•çš„æœºå™¨å­¦ä¹ å…ƒæ¨¡å‹ã€‚å„ç§æ–¹æ³•å·²ç»è¯æ˜äº†ç®—æ³•é€‰æ‹©å…ƒæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå¹¶éæ‰€æœ‰çš„è¯„ä¼°æ–¹æ³•éƒ½èƒ½æœ‰æ•ˆåœ°è¯„ä¼°å…ƒæ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¼ºè°ƒäº†ç¤¾åŒºä¸­ç»å¸¸å‘ç”Ÿçš„æ–¹æ³•è®ºé—®é¢˜ï¼Œå¹¶åœ¨è¯„ä¼°ç®—æ³•é€‰æ‹©æ–¹æ³•æ—¶åº”è§£å†³è¿™äº›é—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å‘ç°äº†â€œç•™ä¸€å®ä¾‹â€è¯„ä¼°æŠ€æœ¯çš„ç¼ºé™·ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œéä¿¡æ¯ç‰¹å¾å’Œå…ƒæ¨¡å‹å¯ä»¥è¾¾åˆ°å¾ˆé«˜çš„å‡†ç¡®æ€§ï¼Œè¿™åœ¨è®¾è®¡è‰¯å¥½çš„è¯„ä¼°æ¡†æ¶ä¸‹æ˜¯ä¸åº”å‡ºç°çš„ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œä½¿ç”¨å¯¹ç›®æ ‡å‡½æ•°è§„æ¨¡æ•æ„Ÿçš„æŒ‡æ ‡æ¥è¡¡é‡ä¼˜åŒ–ç®—æ³•çš„æ€§èƒ½éœ€è¦ä»”ç»†è€ƒè™‘è¿™ä¸€ç‚¹å¯¹å…ƒæ¨¡å‹çš„æ„å»ºã€é¢„æµ‹å’Œæ¨¡å‹è¯¯å·®çš„å½±å“ã€‚è¿™æ ·çš„æŒ‡æ ‡å¯èƒ½ä¼šé”™è¯¯åœ°è¡¨ç°å‡ºè¿‡äºä¹è§‚çš„å…ƒæ¨¡å‹æ€§èƒ½è¯„ä¼°ã€‚æœ¬æ–‡å¼ºè°ƒäº†ä»”ç»†è¯„ä¼°çš„é‡è¦æ€§ï¼Œå› ä¸ºå®šä¹‰æ¾æ•£çš„æ–¹æ³•å¯èƒ½ä¼šè¯¯å¯¼ç ”ç©¶äººå‘˜ã€åˆ†æ•£ç²¾åŠ›å¹¶ä¸ºè¯¥é¢†åŸŸå¼•å…¥å™ªéŸ³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07750v1">PDF</a> </p>
<p><strong>Summary</strong><br>ç®—æ³•é€‰æ‹©åœ¨è¿ç»­é»‘ç®±ä¼˜åŒ–ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œç›®çš„æ˜¯ä¸ºç»™å®šé—®é¢˜é€‰æ‹©æœ€ä½³ç®—æ³•ã€‚æ–‡ä¸­æŒ‡å‡ºè¯„ä¼°ç®—æ³•é€‰æ‹©å…ƒæ¨¡å‹æ€§èƒ½æ—¶å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚â€œç•™å‡ºå®ä¾‹â€è¯„ä¼°æŠ€æœ¯çš„ç¼ºé™·ã€ä½¿ç”¨éä¿¡æ¯ç‰¹å¾å’Œå…ƒæ¨¡å‹çš„é«˜ç²¾åº¦é—®é¢˜ä»¥åŠç›®æ ‡å‡½æ•°è§„æ¨¡å¯¹ç®—æ³•æ€§èƒ½è¯„ä¼°æŒ‡æ ‡çš„å½±å“ç­‰ã€‚å¼ºè°ƒäº†éœ€è¦è®¤çœŸå¯¹å¾…è¯„ä¼°é—®é¢˜ï¼Œä»¥é¿å…è¯¯å¯¼ç ”ç©¶è€…ã€æµªè´¹åŠªåŠ›ä»¥åŠä¸ºé¢†åŸŸå¼•å…¥å™ªå£°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç®—æ³•é€‰æ‹©åœ¨è¿ç»­é»‘ç®±ä¼˜åŒ–ä¸­çš„é‡è¦æ€§ï¼Œæ—¨åœ¨é’ˆå¯¹ç»™å®šé—®é¢˜é€‰æ‹©æœ€ä½³ç®—æ³•ã€‚</li>
<li>ä½¿ç”¨ç‰¹å¾è¡¨ç¤ºä¼˜åŒ–å‡½æ•°æ¥è®­ç»ƒæœºå™¨å­¦ä¹ å…ƒæ¨¡å‹è¿›è¡Œç®—æ³•é€‰æ‹©çš„å¸¸è§æ–¹æ³•åŠå…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>â€œç•™å‡ºå®ä¾‹â€è¯„ä¼°æŠ€æœ¯çš„ç¼ºé™·ï¼Œéä¿¡æ¯ç‰¹å¾å’Œå…ƒæ¨¡å‹å¯èƒ½å®ç°é«˜ç²¾åº¦ï¼Œè¿™ä¸åº”æ˜¯ä¸€ä¸ªå¥½çš„è¯„ä¼°æ¡†æ¶çš„æƒ…å†µã€‚</li>
<li>ä½¿ç”¨å¯¹ç›®æ ‡å‡½æ•°è§„æ¨¡æ•æ„Ÿçš„æŒ‡æ ‡æ¥è¯„ä¼°ä¼˜åŒ–ç®—æ³•çš„æ€§èƒ½çš„æ³¨æ„äº‹é¡¹ï¼ŒåŠå…¶å¯¹å…ƒæ¨¡å‹çš„æ„å»ºã€é¢„æµ‹å’Œæ¨¡å‹è¯¯å·®çš„å½±å“ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07750">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bae460786e014589f85136ec0242e1ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f943d7a68b0fd69cba3a6c21ec72482b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31f214dc5066a4319e2602aefd424023.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="S-GRPO-Early-Exit-via-Reinforcement-Learning-in-Reasoning-Models"><a href="#S-GRPO-Early-Exit-via-Reinforcement-Learning-in-Reasoning-Models" class="headerlink" title="S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models"></a>S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models</h2><p><strong>Authors:Muzhi Dai, Chenxu Yang, Qingyi Si</strong></p>
<p>As Test-Time Scaling emerges as an active research focus in the large language model community, advanced post-training methods increasingly emphasize extending chain-of-thought (CoT) generation length, thereby enhancing reasoning capabilities to approach Deepseek R1-like reasoning models. However, recent studies reveal that reasoning models (even Qwen3) consistently exhibit excessive thought redundancy in CoT generation. This overthinking problem stems from conventional outcome-reward reinforcement learningâ€™s systematic neglect in regulating intermediate reasoning steps. This paper proposes Serial-Group Decaying-Reward Policy Optimization (namely S-GRPO), a novel reinforcement learning method that empowers models with the capability to determine the sufficiency of reasoning steps, subsequently triggering early exit of CoT generation. Specifically, unlike GRPO, which samples multiple possible completions (parallel group) in parallel, we select multiple temporal positions in the generation of one CoT to allow the model to exit thinking and instead generate answers (serial group), respectively. For the correct answers in a serial group, we assign rewards that decay according to positions, with lower rewards towards the later ones, thereby reinforcing the modelâ€™s behavior to generate higher-quality answers at earlier phases with earlier exits of thinking. Empirical evaluations demonstrate compatibility with state-of-the-art reasoning models, including Qwen3 and Deepseek-distill models, achieving 35.4% ~ 61.1% sequence length reduction with 0.72% ~ 6.08% accuracy improvements across GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond benchmarks. </p>
<blockquote>
<p>éšç€æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTest-Time Scalingï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ç¤¾åŒºä¸­æˆä¸ºä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶ç„¦ç‚¹ï¼Œå…ˆè¿›çš„åè®­ç»ƒæ–¹æ³•è¶Šæ¥è¶Šå¼ºè°ƒå»¶é•¿æ€ç»´é“¾ï¼ˆCoTï¼‰ç”Ÿæˆé•¿åº¦ï¼Œä»è€Œæé«˜æ¨ç†èƒ½åŠ›ï¼Œä»¥è¾¾åˆ°Deepseek R1ç­‰æ¨ç†æ¨¡å‹çš„æ°´å¹³ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿åœ¨å…ˆè¿›çš„æ¨ç†æ¨¡å‹ï¼ˆå¦‚Qwen3ï¼‰ä¸­ï¼Œæ€ç»´é“¾ç”Ÿæˆä¹Ÿå­˜åœ¨è¿‡å¤šçš„æ€ç»´å†—ä½™ã€‚è¿™ç§è¿‡åº¦æ€è€ƒçš„é—®é¢˜æºäºä¼ ç»Ÿç»“æœå¥–åŠ±å¼ºåŒ–å­¦ä¹ åœ¨è°ƒèŠ‚ä¸­é—´æ¨ç†æ­¥éª¤æ—¶çš„ç³»ç»Ÿæ€§å¿½è§†ã€‚æœ¬æ–‡æå‡ºäº†ä¸²è¡Œç»„è¡°å‡å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆç®€ç§°S-GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä½¿æ¨¡å‹å…·å¤‡åˆ¤æ–­æ¨ç†æ­¥éª¤æ˜¯å¦å……è¶³çš„èƒ½åŠ›ï¼Œä»è€Œè§¦å‘æ€ç»´é“¾ç”Ÿæˆçš„æ—©æœŸé€€å‡ºã€‚å…·ä½“æ¥è¯´ï¼Œä¸GRPOä¸åŒï¼ŒGRPOä¼šå¹¶è¡Œé‡‡æ ·å¤šä¸ªå¯èƒ½çš„å®Œæˆç»“æœï¼ˆå¹¶è¡Œç»„ï¼‰ï¼Œè€Œæˆ‘ä»¬é€‰æ‹©åœ¨ä¸€ä¸ªæ€ç»´é“¾ç”Ÿæˆä¸­çš„å¤šä¸ªæ—¶é—´ä½ç½®ï¼Œå…è®¸æ¨¡å‹åœæ­¢æ€è€ƒå¹¶è½¬è€Œç”Ÿæˆç­”æ¡ˆï¼ˆä¸²è¡Œç»„ï¼‰ã€‚å¯¹äºä¸²è¡Œç»„ä¸­çš„æ­£ç¡®ç­”æ¡ˆï¼Œæˆ‘ä»¬æ ¹æ®ä½ç½®èµ‹äºˆè¡°å‡çš„å¥–åŠ±ï¼Œåé¢çš„ç­”æ¡ˆå¥–åŠ±è¾ƒä½ï¼Œä»è€Œå¼ºåŒ–æ¨¡å‹åœ¨è¾ƒæ—©é˜¶æ®µç”Ÿæˆé«˜è´¨é‡ç­”æ¡ˆçš„è¡Œä¸ºï¼Œå¹¶å°½æ—©é€€å‡ºæ€è€ƒã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸åŒ…æ‹¬Qwen3å’ŒDeepseek-distillæ¨¡å‹åœ¨å†…çš„æœ€æ–°æ¨ç†æ¨¡å‹å…¼å®¹ï¼Œåœ¨GSM8Kã€AIME 2024ã€AMC 2023ã€MATH-500å’ŒGPQA Diamondç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†35.4% ~ 61.1%çš„åºåˆ—é•¿åº¦å‡å°‘ï¼ŒåŒæ—¶å‡†ç¡®ç‡æé«˜äº†0.72% ~ 6.08%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07686v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTest-Time Scalingï¼‰ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ç¤¾åŒºä¸­çš„æ´»è·ƒç ”ç©¶é¢†åŸŸï¼Œå¼ºè°ƒåœ¨è®­ç»ƒåæ‰©å±•æ€ç»´é“¾ç”Ÿæˆé•¿åº¦çš„é‡è¦æ€§ï¼Œä»¥å¢å¼ºæ¨ç†èƒ½åŠ›å¹¶æ¥è¿‘Deepseek R1ç±»æ¨ç†æ¨¡å‹ã€‚ç„¶è€Œï¼Œç ”ç©¶å‘ç°ç°æœ‰æ¨ç†æ¨¡å‹åœ¨æ€ç»´é“¾ç”Ÿæˆä¸­å­˜åœ¨è¿‡åº¦å†—ä½™çš„é—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥â€”â€”ä¸²è¡Œåˆ†ç»„è¡°å‡å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆS-GRPOï¼‰ï¼Œä½¿æ¨¡å‹å…·å¤‡åˆ¤æ–­æ¨ç†æ­¥éª¤å……åˆ†æ€§çš„èƒ½åŠ›ï¼Œä»è€Œå®ç°åœ¨æ€ç»´é“¾ç”Ÿæˆçš„æ—©æœŸé€€å‡ºã€‚S-GRPOåœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºå…¶å¯ä»¥å…¼å®¹æœ€æ–°çš„æ¨ç†æ¨¡å‹å¹¶å‡å°‘åºåˆ—é•¿åº¦ï¼ŒåŒæ—¶æé«˜å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶ç¼©æ”¾æ˜¯å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ç¤¾åŒºçš„ç ”ç©¶é‡ç‚¹ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰æ¨ç†æ¨¡å‹å­˜åœ¨æ€ç»´é“¾ç”Ÿæˆè¿‡åº¦å†—ä½™çš„é—®é¢˜ã€‚</li>
<li>S-GRPOæ˜¯ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œç”¨äºä¼˜åŒ–æ¨ç†æ¨¡å‹çš„æ€ç»´é“¾ç”Ÿæˆã€‚</li>
<li>S-GRPOé€šè¿‡åˆ¤æ–­æ¨ç†æ­¥éª¤çš„å……åˆ†æ€§æ¥å®ç°æ—©æœŸé€€å‡ºæ€ç»´é“¾ç”Ÿæˆã€‚</li>
<li>S-GRPOä¸ç°æœ‰æ¨ç†æ¨¡å‹å…¼å®¹ï¼Œå¦‚Qwen3å’ŒDeepseek-distillæ¨¡å‹ã€‚</li>
<li>S-GRPOå¯ä»¥åœ¨ä¸åŒæ•°æ®é›†ä¸Šå®ç°åºåˆ—é•¿åº¦å‡å°‘å’Œå‡†ç¡®æ€§æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d7f40771965649538497229ca4a29368.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b71e12bf3985904a562115a48086722.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfe1fc41a5cceab18ea9390010126360.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0467d2942739e8e70fab02761eead7f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MiMo-Unlocking-the-Reasoning-Potential-of-Language-Model-â€“-From-Pretraining-to-Posttraining"><a href="#MiMo-Unlocking-the-Reasoning-Potential-of-Language-Model-â€“-From-Pretraining-to-Posttraining" class="headerlink" title="MiMo: Unlocking the Reasoning Potential of Language Model â€“ From   Pretraining to Posttraining"></a>MiMo: Unlocking the Reasoning Potential of Language Model â€“ From   Pretraining to Posttraining</h2><p><strong>Authors:Xiaomi LLM-Core Team,  :, Bingquan Xia, Bowen Shen,  Cici, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, Liang Zhao, Peidian Li, Peng Wang, Shihua Yu, Shimao Chen, Weikun Wang, Wenhan Ma, Xiangwei Deng, Yi Huang, Yifan Song, Zihan Jiang, Bowen Ye, Can Cai, Chenhong He, Dong Zhang, Duo Zhang, Guoan Wang, Hao Tian, Haochen Zhao, Heng Qu, Hongshen Xu, Jun Shi, Kainan Bao, QingKai Fang, Kang Zhou, Kangyang Zhou, Lei Li, Menghang Zhu, Nuo Chen, Qiantong Wang, Shaohui Liu, Shicheng Li, Shuhao Gu, Shuhuai Ren, Shuo Liu, Sirui Deng, Weiji Zhuang, Weiwei Lv, Wenyu Yang, Xin Zhang, Xing Yong, Xing Zhang, Xingchen Song, Xinzhe Xu, Xu Wang, Yihan Yan, Yu Tu, Yuanyuan Tian, Yudong Wang, Yue Yu, Zhenru Lin, Zhichao Song, Zihao Yue</strong></p>
<p>We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base modelâ€™s reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/xiaomimimo/MiMo">https://github.com/xiaomimimo/MiMo</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†MiMo-7Bï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“ä¸ºæ¨ç†ä»»åŠ¡è®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨é¢„è®­ç»ƒå’Œåè®­ç»ƒé˜¶æ®µéƒ½è¿›è¡Œäº†ä¼˜åŒ–ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬å¢å¼ºäº†æ•°æ®é¢„å¤„ç†æµç¨‹ï¼Œå¹¶é‡‡ç”¨äº†ä¸‰é˜¶æ®µæ•°æ®æ··åˆç­–ç•¥ï¼Œä»¥åŠ å¼ºåŸºç¡€æ¨¡å‹çš„æ¨ç†æ½œåŠ›ã€‚MiMo-7B-Baseåœ¨25ä¸‡äº¿ä¸ªä»¤ç‰Œä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶å¢åŠ äº†å¤šä»¤ç‰Œé¢„æµ‹ç›®æ ‡ï¼Œä»¥æé«˜æ€§èƒ½å’ŒåŠ é€Ÿæ¨ç†é€Ÿåº¦ã€‚åœ¨åè®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªåŒ…å«13ä¸‡ä¸ªå¯éªŒè¯çš„æ•°å­¦å’Œç¼–ç¨‹é—®é¢˜çš„æ•°æ®é›†ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬é‡‡ç”¨æµ‹è¯•éš¾åº¦é©±åŠ¨çš„ä»£ç å¥–åŠ±æ–¹æ¡ˆæ¥ç¼“è§£ç¨€ç–å¥–åŠ±é—®é¢˜ï¼Œå¹¶é‡‡ç”¨æˆ˜ç•¥æ•°æ®é‡é‡‡æ ·æ¥ç¨³å®šè®­ç»ƒã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒMiMo-7B-Baseå…·æœ‰å‡ºè‰²çš„æ¨ç†æ½œåŠ›ï¼Œç”šè‡³è¶…è¿‡äº†è®¸å¤šæ›´å¤§çš„32Bæ¨¡å‹ã€‚æœ€ç»ˆçš„RLè°ƒä¼˜æ¨¡å‹MiMo-7B-RLåœ¨æ•°å­¦ã€ä»£ç å’Œä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†OpenAI o1-miniçš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹çš„æ£€æŸ¥ç‚¹ä½äº <a target="_blank" rel="noopener" href="https://github.com/xiaomimimo/MiMo%E3%80%82">https://github.com/xiaomimimo/MiMoã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07608v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MiMo-7Bæ˜¯ä¸ºæ¨ç†ä»»åŠ¡è®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…¶åœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µè¿›è¡Œäº†ä¼˜åŒ–ã€‚é¢„è®­ç»ƒé˜¶æ®µå¢å¼ºäº†æ•°æ®é¢„å¤„ç†æµç¨‹ï¼Œå¹¶é‡‡ç”¨äº†ä¸‰é˜¶æ®µæ•°æ®æ··åˆç­–ç•¥æ¥æå‡åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚MiMo-7B-Baseåœ¨25ä¸‡äº¿ä¸ªä»¤ç‰Œä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶å¢åŠ äº†å¤šä»¤ç‰Œé¢„æµ‹ç›®æ ‡ä»¥æé«˜æ€§èƒ½å’ŒåŠ é€Ÿæ¨ç†é€Ÿåº¦ã€‚å¾®è°ƒé˜¶æ®µåˆ™ä½¿ç”¨æ•°æ®é›†æ•´åˆå¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡æµ‹è¯•éš¾åº¦é©±åŠ¨çš„å¥–åŠ±æ–¹æ¡ˆå’Œç­–ç•¥æ•°æ®é‡é‡‡æ ·ï¼Œå®ç°è®­ç»ƒçš„ç¨³å®šåŒ–ã€‚è¯„ä»·ç»“æœæ˜¾ç¤ºï¼ŒMiMo-7Bç³»åˆ—æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦ã€ä»£ç å’Œé€šç”¨æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†å…¶ä»–å¤§å‹æ¨¡å‹ã€‚æ¨¡å‹çš„æ£€æŸ¥ç‚¹å¯ä»¥åœ¨ç›¸å…³é“¾æ¥æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MiMo-7Bæ˜¯ä¸ºæ¨ç†ä»»åŠ¡è®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é¢„è®­ç»ƒé˜¶æ®µå¢å¼ºäº†æ•°æ®é¢„å¤„ç†æµç¨‹å¹¶é‡‡ç”¨ä¸‰é˜¶æ®µæ•°æ®æ··åˆç­–ç•¥æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MiMo-7B-Baseåœ¨å¤§é‡æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¢åŠ å¤šä»¤ç‰Œé¢„æµ‹ç›®æ ‡ä»¥æå‡æ€§èƒ½ã€‚</li>
<li>è°ƒå’Œé˜¶æ®µé€šè¿‡æ•´åˆå¼ºåŒ–å­¦ä¹ æ¥æå‡æ¨¡å‹åœ¨æ•°å­¦å’Œç¼–ç¨‹é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨æµ‹è¯•éš¾åº¦é©±åŠ¨çš„å¥–åŠ±æ–¹æ¡ˆæ¥è§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜ï¼Œå¹¶ä½¿ç”¨æˆ˜ç•¥æ•°æ®é‡é‡‡æ ·æ¥ç¨³å®šè®­ç»ƒã€‚</li>
<li>MiMo-7Bç³»åˆ—æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†æ½œåŠ›ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šè¶…è¶Šäº†æ›´å¤§çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c2c34cf44bf3b2eb99d6ebe8fc6fa81.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f21d754da642b6a55fe784e29a164f6b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Reinforced-Internal-External-Knowledge-Synergistic-Reasoning-for-Efficient-Adaptive-Search-Agent"><a href="#Reinforced-Internal-External-Knowledge-Synergistic-Reasoning-for-Efficient-Adaptive-Search-Agent" class="headerlink" title="Reinforced Internal-External Knowledge Synergistic Reasoning for   Efficient Adaptive Search Agent"></a>Reinforced Internal-External Knowledge Synergistic Reasoning for   Efficient Adaptive Search Agent</h2><p><strong>Authors:Ziyang Huang, Xiaowei Yuan, Yiming Ju, Jun Zhao, Kang Liu</strong></p>
<p>Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å‡ºç°å¹»è§‰çš„å¸¸è§ç­–ç•¥ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥ä½¿LLMé€šè¿‡æ¿€æ´»æ£€ç´¢åŠŸèƒ½æ¥å……å½“æœç´¢ä»£ç†ï¼Œä½†ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ é€šå¸¸æœªèƒ½å……åˆ†åˆ©ç”¨å…¶å†…éƒ¨çŸ¥è¯†ã€‚è¿™å¯èƒ½å¯¼è‡´å†—ä½™çš„æ£€ç´¢ã€æ½œåœ¨çš„æœ‰å®³çŸ¥è¯†å†²çªå’Œå¢åŠ çš„æ¨ç†å»¶è¿Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œè¿«åˆ‡éœ€è¦ä¸€ç§é«˜æ•ˆä¸”è‡ªé€‚åº”çš„æœç´¢ä»£ç†ï¼Œè¯¥ä»£ç†èƒ½å¤Ÿè¾¨åˆ«æœ€ä½³çš„æ£€ç´¢æ—¶é—´ï¼Œå¹¶ååŒæ•´åˆå‚æ•°åŒ–ï¼ˆå†…éƒ¨ï¼‰å’Œæ£€ç´¢ï¼ˆå¤–éƒ¨ï¼‰çŸ¥è¯†ã€‚æœ¬æ–‡ä»‹ç»äº†å¼ºåŒ–å†…éƒ¨ä¸å¤–éƒ¨çŸ¥è¯†ååŒæ¨ç†ä»£ç†ï¼ˆIKEAï¼‰ï¼Œè¯¥ä»£ç†èƒ½å¤Ÿè¯†åˆ«å…¶è‡ªèº«çš„çŸ¥è¯†è¾¹ç•Œï¼Œå¹¶ä¼˜å…ˆåˆ©ç”¨å†…éƒ¨çŸ¥è¯†ï¼Œä»…åœ¨è®¤ä¸ºå†…éƒ¨çŸ¥è¯†ä¸è¶³æ—¶æ‰æ±‚åŠ©äºå¤–éƒ¨æœç´¢ã€‚è¿™æ˜¯é€šè¿‡ä½¿ç”¨æ–°å‹çš„çŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥å¥–åŠ±å‡½æ•°å’ŒçŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥è®­ç»ƒæ•°æ®é›†æ¥å®ç°çš„ï¼Œè¿™äº›æ˜¯ä¸ºäº†é¢å‘å†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†ååŒçš„RLè®¾è®¡çš„ï¼Œæ¿€åŠ±æ¨¡å‹æä¾›å‡†ç¡®ç­”æ¡ˆï¼Œå‡å°‘ä¸å¿…è¦çš„æ£€ç´¢ï¼Œå¹¶åœ¨è‡ªèº«çŸ¥è¯†ä¸è¶³æ—¶é¼“åŠ±é€‚å½“çš„å¤–éƒ¨æœç´¢ã€‚åœ¨å¤šä¸ªçŸ¥è¯†æ¨ç†ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒIKEAæ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¤§å¤§é™ä½äº†æ£€ç´¢é¢‘ç‡ï¼Œå¹¶è¡¨ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07596v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æå‡ºäº†å¢å¼ºå‹å†…éƒ¨å¤–éƒ¨çŸ¥è¯†ååŒæ¨ç†æ¨¡å‹ï¼ˆIKEAï¼‰ï¼Œèƒ½æœ‰æ•ˆå‡å°‘å†—ä½™æ£€ç´¢å¹¶åº”å¯¹æ½œåœ¨çš„å¤–éƒ¨çŸ¥è¯†å†²çªã€‚è¯¥æ¨¡å‹é‡‡ç”¨çŸ¥è¯†è¾¹ç•Œæ„è¯†å¥–åŠ±å‡½æ•°å’Œè®­ç»ƒæ•°æ®é›†è®¾è®¡ï¼Œä½¿æ¨¡å‹èƒ½ç²¾å‡†åŒºåˆ†æœ€ä¼˜æ£€ç´¢æ—¶æœºï¼Œå¹¶ååŒåˆ©ç”¨å†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†ã€‚ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼ŒIKEAåœ¨å¤šä¸ªçŸ¥è¯†æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—å‡å°‘äº†æ£€ç´¢é¢‘ç‡å¹¶å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>IKEAæ¨¡å‹èƒ½æœ‰æ•ˆè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¸¸è§çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­–ç•¥çš„å±€é™æ€§é—®é¢˜ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æŠ€æœ¯ï¼Œä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿä½œä¸ºæœç´¢ä»£ç†æ¥æ¿€æ´»æ£€ç´¢èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ¨¡å‹é€šå¸¸å¿½è§†å†…éƒ¨çŸ¥è¯†çš„åˆ©ç”¨ï¼Œå¯èƒ½å¯¼è‡´å†—ä½™æ£€ç´¢å’ŒçŸ¥è¯†å†²çªã€‚</li>
<li>IKEAèƒ½å¤Ÿç¡®å®šè‡ªèº«çŸ¥è¯†è¾¹ç•Œï¼Œä¼˜å…ˆåˆ©ç”¨å†…éƒ¨çŸ¥è¯†ï¼Œåªåœ¨å¿…è¦æ—¶è¿›è¡Œå¤–éƒ¨æœç´¢ã€‚</li>
<li>æ¨¡å‹é€šè¿‡çŸ¥è¯†è¾¹ç•Œæ„è¯†å¥–åŠ±å‡½æ•°å’Œè®­ç»ƒæ•°æ®é›†è®¾è®¡å®ç°å†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†çš„ååŒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIKEAåœ¨å¤šä¸ªçŸ¥è¯†æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œèƒ½æ˜¾è‘—æé«˜ç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œå‡å°‘ä¸å¿…è¦çš„æ£€ç´¢å¹¶ä¿ƒè¿›é€‚å½“çš„å¤–éƒ¨æœç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f525fd7e0acc8dde1a54f042d94fd7f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-89f62d0d02a821092597f0b834777f77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6aae2f3aabffad3fc2e583d26436a86b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Discrete-Visual-Tokens-of-Autoregression-by-Diffusion-and-for-Reasoning"><a href="#Discrete-Visual-Tokens-of-Autoregression-by-Diffusion-and-for-Reasoning" class="headerlink" title="Discrete Visual Tokens of Autoregression, by Diffusion, and for   Reasoning"></a>Discrete Visual Tokens of Autoregression, by Diffusion, and for   Reasoning</h2><p><strong>Authors:Bohan Wang, Zhongqi Yue, Fengda Zhang, Shuo Chen, Liâ€™an Bi, Junzhe Zhang, Xue Song, Kennard Yanting Chan, Jiachun Pan, Weijia Wu, Mingze Zhou, Wang Lin, Kaihang Pan, Saining Zhang, Liyu Jia, Wentao Hu, Wei Zhao, Hanwang Zhang</strong></p>
<p>We completely discard the conventional spatial prior in image representation and introduce a novel discrete visual tokenizer: Self-consistency Tokenizer (Selftok). At its design core, we compose an autoregressive (AR) prior â€“ mirroring the causal structure of language â€“ into visual tokens by using the reverse diffusion process of image generation. The AR property makes Selftok fundamentally distinct from traditional spatial tokens in the following two key ways: - Selftok offers an elegant and minimalist approach to unify diffusion and AR for vision-language models (VLMs): By representing images with Selftok tokens, we can train a VLM using a purely discrete autoregressive architecture â€“ like that in LLMs â€“ without requiring additional modules or training objectives. - We theoretically show that the AR prior satisfies the Bellman equation, whereas the spatial prior does not. Therefore, Selftok supports reinforcement learning (RL) for visual generation with effectiveness comparable to that achieved in LLMs. Besides the AR property, Selftok is also a SoTA tokenizer that achieves a favorable trade-off between high-quality reconstruction and compression rate. We use Selftok to build a pure AR VLM for both visual comprehension and generation tasks. Impressively, without using any text-image training pairs, a simple policy gradient RL working in the visual tokens can significantly boost the visual generation benchmark, surpassing all the existing models by a large margin. Therefore, we believe that Selftok effectively addresses the long-standing challenge that visual tokens cannot support effective RL. When combined with the well-established strengths of RL in LLMs, this brings us one step closer to realizing a truly multimodal LLM. Project Page: <a target="_blank" rel="noopener" href="https://selftok-team.github.io/report/">https://selftok-team.github.io/report/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬å®Œå…¨æ‘’å¼ƒäº†ä¼ ç»Ÿçš„ç©ºé—´å…ˆéªŒå›¾åƒè¡¨ç¤ºæ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹ç¦»æ•£è§†è§‰åˆ†è¯å™¨ï¼šè‡ªæ´½åˆ†è¯å™¨ï¼ˆSelftokï¼‰ã€‚åœ¨è®¾è®¡æ ¸å¿ƒï¼Œæˆ‘ä»¬é€šè¿‡å›¾åƒç”Ÿæˆçš„é€†å‘æ‰©æ•£è¿‡ç¨‹ï¼Œå°†è‡ªå›å½’ï¼ˆARï¼‰å…ˆéªŒâ€”â€”åæ˜ è¯­è¨€çš„å› æœç»“æ„â€”â€”èå…¥è§†è§‰æ ‡è®°ä¸­ã€‚ARç‰¹æ€§ä½¿å¾—Selftokåœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ä¸ä¼ ç»Ÿçš„ç©ºé—´æ ‡è®°æœ‰ç€æ ¹æœ¬çš„åŒºåˆ«ï¼šé¦–å…ˆï¼ŒSelftokä¸ºæ•´åˆæ‰©æ•£å’ŒARçš„è§†è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æä¾›äº†ä¼˜é›…ä¸”æç®€çš„æ–¹æ³•ï¼šé€šè¿‡Selftokä»¤ç‰Œè¡¨ç¤ºå›¾åƒï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨çº¯ç¦»æ•£è‡ªå›å½’æ¶æ„æ¥è®­ç»ƒVLMï¼Œç±»ä¼¼äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ— éœ€é¢å¤–çš„æ¨¡å—æˆ–è®­ç»ƒç›®æ ‡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†ARå…ˆéªŒæ»¡è¶³è´å°”æ›¼æ–¹ç¨‹ï¼Œè€Œç©ºé—´å…ˆéªŒåˆ™ä¸æ»¡è¶³ã€‚å› æ­¤ï¼ŒSelftokæ”¯æŒç”¨äºè§†è§‰ç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œå…¶æ•ˆæœå¯ä¸LLMæ‰€å®ç°çš„æ•ˆæœç›¸å½“ã€‚é™¤äº†ARç‰¹æ€§å¤–ï¼ŒSelftokè¿˜æ˜¯ä¸€ç§å…ˆè¿›çš„åˆ†è¯å™¨ï¼Œå®ç°äº†é«˜è´¨é‡é‡å»ºå’Œå‹ç¼©ç‡ä¹‹é—´çš„æœ‰åˆ©æƒè¡¡ã€‚æˆ‘ä»¬ä½¿ç”¨Selftokä¸ºè§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡æ„å»ºçº¯ARçš„VLMã€‚ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼Œä¸ä½¿ç”¨ä»»ä½•æ–‡æœ¬-å›¾åƒè®­ç»ƒå¯¹ï¼Œä¸€ä¸ªç®€å•çš„ç­–ç•¥æ¢¯åº¦RLåœ¨è§†è§‰ä»¤ç‰Œä¸­å¯ä»¥æ˜¾è‘—æé«˜è§†è§‰ç”ŸæˆåŸºå‡†æµ‹è¯•æˆç»©ï¼Œå¤§å¤§è¶…è¶Šç°æœ‰æ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¤ä¸ºSelftokæœ‰æ•ˆåœ°è§£å†³äº†é•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ï¼Œå³è§†è§‰ä»¤ç‰Œæ— æ³•æ”¯æŒæœ‰æ•ˆçš„RLã€‚å½“ä¸RLåœ¨LLMä¸­çš„æ—¢å®šä¼˜åŠ¿ç›¸ç»“åˆæ—¶ï¼Œè¿™ä½¿æˆ‘ä»¬ç¦»å®ç°çœŸæ­£çš„å¤šæ¨¡æ€LLMåˆè¿‘äº†ä¸€æ­¥ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://selftok-team.github.io/report/%E3%80%82">https://selftok-team.github.io/report/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07538v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ‘’å¼ƒäº†ä¼ ç»Ÿçš„ç©ºé—´å…ˆéªŒå›¾åƒè¡¨ç¤ºæ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹ç¦»æ•£è§†è§‰ä»¤ç‰Œå™¨ï¼šè‡ªæ´½ä»¤ç‰Œå™¨ï¼ˆSelftokï¼‰ã€‚å…¶è®¾è®¡æ ¸å¿ƒæ˜¯é€šè¿‡å›¾åƒç”Ÿæˆçš„é€†å‘æ‰©æ•£è¿‡ç¨‹ï¼Œä½¿ç”¨è‡ªå›å½’ï¼ˆARï¼‰å…ˆéªŒæ¥ç”Ÿæˆè§†è§‰ä»¤ç‰Œï¼Œè¿™åæ˜ äº†è¯­è¨€çš„å› æœç»“æ„ã€‚Selftokåœ¨ä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢ä½¿ä¼ ç»Ÿç©ºé—´ä»¤ç‰Œç›¸å½¢è§ç»Œï¼šä¸€ã€å®ƒä¸ºèåˆæ‰©æ•£å’ŒARçš„è§†å¬è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æä¾›äº†ä¸€ç§ä¼˜é›…è€Œç®€æ´çš„æ–¹æ³•ï¼›äºŒã€ç†è®ºè¯æ˜ARå…ˆéªŒæ»¡è¶³è´å°”æ›¼æ–¹ç¨‹ï¼Œè€Œç©ºé—´å…ˆéªŒåˆ™ä¸æ»¡è¶³ã€‚Selftokæ”¯æŒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è§†è§‰ç”Ÿæˆæ–¹é¢çš„åº”ç”¨ï¼Œæ•ˆæœä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸å½“ã€‚æ­¤å¤–ï¼ŒSelftokè¿˜æ˜¯ä¸€ä¸ªå…ˆè¿›çš„ä»¤ç‰ŒåŒ–å™¨ï¼Œå®ç°äº†é«˜è´¨é‡é‡å»ºä¸å‹ç¼©ç‡ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ã€‚æˆ‘ä»¬åˆ©ç”¨Selftokå»ºç«‹äº†ä¸€ä¸ªçº¯ARçš„VLMï¼Œç”¨äºè§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚åœ¨ä¸ä½¿ç”¨ä»»ä½•æ–‡æœ¬-å›¾åƒè®­ç»ƒå¯¹çš„æƒ…å†µä¸‹ï¼Œç®€å•çš„åŸºäºè§†è§‰ä»¤ç‰Œçš„ç­–ç•¥æ¢¯åº¦RLå¯ä»¥æ˜¾è‘—æé«˜è§†è§‰ç”ŸæˆåŸºå‡†æµ‹è¯•çš„æ€§èƒ½ï¼Œå¤§å¤§è¶…è¶Šç°æœ‰æ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç›¸ä¿¡Selftokæœ‰æ•ˆåœ°è§£å†³äº†é•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ï¼Œå³è§†è§‰ä»¤ç‰Œæ— æ³•æ”¯æŒæœ‰æ•ˆçš„RLã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹ç¦»æ•£è§†è§‰ä»¤ç‰Œå™¨â€”â€”è‡ªæ´½ä»¤ç‰Œå™¨ï¼ˆSelftokï¼‰ï¼Œæ‘’å¼ƒäº†ä¼ ç»Ÿç©ºé—´å…ˆéªŒå›¾åƒè¡¨ç¤ºæ–¹æ³•ã€‚</li>
<li>Selftokçš„æ ¸å¿ƒè®¾è®¡æ˜¯è¿ç”¨è‡ªå›å½’ï¼ˆARï¼‰å…ˆéªŒç”Ÿæˆè§†è§‰ä»¤ç‰Œï¼Œè¿™åæ˜ äº†è¯­è¨€çš„å› æœç»“æ„ã€‚</li>
<li>Selftokåœ¨è§†è§‰ä»¤ç‰ŒåŒ–æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†é«˜è´¨é‡é‡å»ºä¸å‹ç¼©ç‡ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>Selftokä¸ºè§†å¬è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æä¾›äº†ä¸€ç§èåˆæ‰©æ•£å’ŒARçš„ç®€æ´æ–¹æ³•ï¼Œå¹¶èƒ½åœ¨ä¸ä¾èµ–é¢å¤–æ¨¡å—æˆ–è®­ç»ƒç›®æ ‡çš„æƒ…å†µä¸‹è®­ç»ƒVLMã€‚</li>
<li>ARå…ˆéªŒæ»¡è¶³è´å°”æ›¼æ–¹ç¨‹ï¼Œé€‚åˆç”¨äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è§†è§‰ç”Ÿæˆæ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>åœ¨ä¸ä½¿ç”¨æ–‡æœ¬-å›¾åƒè®­ç»ƒå¯¹çš„æƒ…å†µä¸‹ï¼ŒåŸºäºSelftokçš„è§†è§‰ä»¤ç‰Œèƒ½é€šè¿‡ç®€å•çš„ç­–ç•¥æ¢¯åº¦RLæ˜¾è‘—æé«˜è§†è§‰ç”Ÿæˆæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07538">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7fa70c38c9573e26f9846530740d55e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99776ae9d2fa1f0e3b6eb1253ce183ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f89e83716081479e66860615d5af133f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b29fd1e5f2f8925dc311e40012b05c1b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Kalman-Filter-Enhanced-GRPO-for-Reinforcement-Learning-Based-Language-Model-Reasoning"><a href="#Kalman-Filter-Enhanced-GRPO-for-Reinforcement-Learning-Based-Language-Model-Reasoning" class="headerlink" title="Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language   Model Reasoning"></a>Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language   Model Reasoning</h2><p><strong>Authors:Hu Wang, Congbo Ma, Ian Reid, Mohammad Yaqub</strong></p>
<p>Reward baseline is important for Reinforcement Learning (RL) algorithms to reduce variance in policy gradient estimates. Recently, for language modeling, Group Relative Policy Optimization (GRPO) is proposed to compute the advantage for each output by subtracting the mean reward, as the baseline, for all outputs in the group. However, it can lead to inaccurate advantage estimates in environments with highly noisy rewards, potentially introducing bias. In this work, we propose a model, called Kalman Filter Enhanced Group Relative Policy Optimization (KRPO), by using lightweight Kalman filtering to dynamically estimate the latent reward mean and variance. This filtering technique replaces the naive batch mean baseline, enabling more adaptive advantage normalization. Our method does not require additional learned parameters over GRPO. This approach offers a simple yet effective way to incorporate multiple outputs of GRPO into advantage estimation, improving policy optimization in settings where highly dynamic reward signals are difficult to model for language models. Through experiments and analyses, we show that using a more adaptive advantage estimation model, KRPO can improve the stability and performance of GRPO. The code is available at <a target="_blank" rel="noopener" href="https://github.com/billhhh/KRPO_LLMs_RL">https://github.com/billhhh/KRPO_LLMs_RL</a> </p>
<blockquote>
<p>å¥–åŠ±åŸºçº¿åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•ä¸­éå¸¸é‡è¦ï¼Œå¯ä»¥å‡å°‘ç­–ç•¥æ¢¯åº¦ä¼°è®¡ä¸­çš„æ–¹å·®ã€‚æœ€è¿‘ï¼Œå¯¹äºè¯­è¨€å»ºæ¨¡ï¼Œæå‡ºäº†é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œé€šè¿‡å‡å»æ‰€æœ‰è¾“å‡ºç»„çš„å¹³å‡å¥–åŠ±ä½œä¸ºåŸºçº¿ï¼Œä¸ºæ¯ä¸ªè¾“å‡ºè®¡ç®—ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œåœ¨å¥–åŠ±é«˜åº¦å˜ˆæ‚çš„ç¯å¢ƒä¸­ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¼˜åŠ¿ä¼°è®¡ä¸å‡†ç¡®ï¼Œä»è€Œå¼•å…¥åè§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡å‹ï¼Œç§°ä¸ºå¡å°”æ›¼æ»¤æ³¢å¢å¼ºå‹é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆKRPOï¼‰ï¼Œé€šè¿‡ä½¿ç”¨è½»é‡çº§çš„å¡å°”æ›¼æ»¤æ³¢æ¥åŠ¨æ€ä¼°è®¡æ½œåœ¨å¥–åŠ±çš„å¹³å‡å€¼å’Œæ–¹å·®ã€‚è¿™ç§æ»¤æ³¢æŠ€æœ¯æ›¿ä»£äº†ç®€å•çš„æ‰¹é‡å‡å€¼åŸºçº¿ï¼Œä½¿ä¼˜åŠ¿å½’ä¸€åŒ–æ›´åŠ è‡ªé€‚åº”ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦åœ¨GRPOä¹‹ä¸Šå­¦ä¹ é¢å¤–çš„å‚æ•°ã€‚è¿™ç§æ–¹æ³•ä¸ºå°†GRPOçš„å¤šä¸ªè¾“å‡ºçº³å…¥ä¼˜åŠ¿ä¼°è®¡æä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨éš¾ä»¥å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå»ºæ¨¡çš„é«˜åº¦åŠ¨æ€å¥–åŠ±ä¿¡å·ç¯å¢ƒä¸­ï¼Œå¯ä»¥æ”¹å–„ç­–ç•¥ä¼˜åŒ–ã€‚é€šè¿‡å®éªŒå’Œåˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨æ›´è‡ªé€‚åº”çš„ä¼˜åŠ¿ä¼°è®¡æ¨¡å‹KRPOå¯ä»¥æé«˜GRPOçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/billhhh/KRPO_LLMs_RL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/billhhh/KRPO_LLMs_RLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07527v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±åŸºçº¿å¯¹äºå‡å°‘ç­–ç•¥æ¢¯åº¦ä¼°è®¡çš„æ–¹å·®è‡³å…³é‡è¦ã€‚é’ˆå¯¹è¯­è¨€å»ºæ¨¡ï¼Œæå‡ºäº†Group Relative Policy Optimization (GRPO)ï¼Œé€šè¿‡å‡å»æ‰€æœ‰è¾“å‡ºç»„çš„å¹³å‡å¥–åŠ±ä½œä¸ºåŸºçº¿æ¥è®¡ç®—æ¯ä¸ªè¾“å‡ºçš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œåœ¨å¥–åŠ±é«˜åº¦å˜ˆæ‚çš„ç¯å¢ƒä¸­ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¼˜åŠ¿ä¼°è®¡ä¸å‡†ç¡®ï¼Œå¹¶å¯èƒ½å¼•å…¥åè§ã€‚æœ¬ç ”ç©¶æå‡ºäº†Kalman Filter Enhanced Group Relative Policy Optimization (KRPO)ï¼Œåˆ©ç”¨è½»é‡çº§Kalmanæ»¤æ³¢åŠ¨æ€ä¼°è®¡æ½œåœ¨å¥–åŠ±å‡å€¼å’Œæ–¹å·®ï¼Œæ›¿ä»£äº†åŸºäºæ‰¹é‡å‡å€¼åŸºçº¿çš„ç®€å•ç­–ç•¥ã€‚è¯¥æ–¹æ³•å…è®¸æ›´å¥½åœ°è¿›è¡Œä¼˜åŠ¿å½’ä¸€åŒ–ï¼Œè€Œæ— éœ€åœ¨GRPOä¹‹ä¸Šå¼•å…¥é¢å¤–çš„å­¦ä¹ å‚æ•°ã€‚è¿™ç§æ–¹æ³•æä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥å°†GRPOçš„å¤šä¸ªè¾“å‡ºçº³å…¥ä¼˜åŠ¿ä¼°è®¡ä¸­ï¼Œå¹¶åœ¨é«˜åº¦åŠ¨æ€çš„å¥–åŠ±ä¿¡å·éš¾ä»¥å»ºæ¨¡çš„æƒ…å†µä¸‹æ”¹è¿›ç­–ç•¥ä¼˜åŒ–ã€‚å®éªŒå’Œåˆ†æè¡¨æ˜ï¼Œä½¿ç”¨æ›´è‡ªé€‚åº”çš„ä¼˜åŠ¿ä¼°è®¡æ¨¡å‹KRPOå¯ä»¥æé«˜GRPOçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±åŸºçº¿å¯¹äºå‡å°‘ç­–ç•¥æ¢¯åº¦ä¼°è®¡çš„æ–¹å·®è‡³å…³é‡è¦ã€‚</li>
<li>Group Relative Policy Optimization (GRPO) æå‡ºé€šè¿‡å‡å»ç»„å†…çš„å¹³å‡å¥–åŠ±ä½œä¸ºåŸºçº¿æ¥è®¡ç®—æ¯ä¸ªè¾“å‡ºçš„ä¼˜åŠ¿ã€‚</li>
<li>åœ¨é«˜å™ªéŸ³å¥–åŠ±ç¯å¢ƒä¸‹ï¼ŒGRPOå¯èƒ½å¯¼è‡´ä¼˜åŠ¿ä¼°è®¡ä¸å‡†ç¡®ã€‚</li>
<li>Kalman Filter Enhanced Group Relative Policy Optimization (KRPO) åˆ©ç”¨Kalmanæ»¤æ³¢åŠ¨æ€ä¼°è®¡å¥–åŠ±å‡å€¼å’Œæ–¹å·®ï¼Œæ”¹è¿›äº†ä¼˜åŠ¿ä¼°è®¡ã€‚</li>
<li>KRPOæ–¹æ³•æé«˜äº†GRPOçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚</li>
<li>KRPOæ–¹æ³•æ— éœ€åœ¨GRPOåŸºç¡€ä¸Šå¼•å…¥é¢å¤–çš„å­¦ä¹ å‚æ•°ã€‚</li>
<li>å®éªŒå’Œåˆ†æéªŒè¯äº†KRPOåœ¨åŠ¨æ€å¥–åŠ±ä¿¡å·ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0ea496de5792cacff16cb06fdf168b6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8067c42155a91d3ce45d3c6f0fd2d6f2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Learning-to-Reason-and-Navigate-Parameter-Efficient-Action-Planning-with-Large-Language-Models"><a href="#Learning-to-Reason-and-Navigate-Parameter-Efficient-Action-Planning-with-Large-Language-Models" class="headerlink" title="Learning to Reason and Navigate: Parameter Efficient Action Planning   with Large Language Models"></a>Learning to Reason and Navigate: Parameter Efficient Action Planning   with Large Language Models</h2><p><strong>Authors:Bahram Mohammadi, Ehsan Abbasnejad, Yuankai Qi, Qi Wu, Anton Van Den Hengel, Javen Qinfeng Shi</strong></p>
<p>The remote embodied referring expression (REVERIE) task requires an agent to navigate through complex indoor environments and localize a remote object specified by high-level instructions, such as â€œbring me a spoonâ€, without pre-exploration. Hence, an efficient navigation plan is essential for the final success. This paper proposes a novel parameter-efficient action planner using large language models (PEAP-LLM) to generate a single-step instruction at each location. The proposed model consists of two modules, LLM goal planner (LGP) and LoRA action planner (LAP). Initially, LGP extracts the goal-oriented plan from REVERIE instructions, including the target object and room. Then, LAP generates a single-step instruction with the goal-oriented plan, high-level instruction, and current visual observation as input. PEAP-LLM enables the embodied agent to interact with LAP as the path planner on the fly. A simple direct application of LLMs hardly achieves good performance. Also, existing hard-prompt-based methods are error-prone in complicated scenarios and need human intervention. To address these issues and prevent the LLM from generating hallucinations and biased information, we propose a novel two-stage method for fine-tuning the LLM, consisting of supervised fine-tuning (STF) and direct preference optimization (DPO). SFT improves the quality of generated instructions, while DPO utilizes environmental feedback. Experimental results show the superiority of our proposed model on REVERIE compared to the previous state-of-the-art. </p>
<blockquote>
<p>è¿œç¨‹å®ä½“å‚ç…§è¡¨è¾¾ï¼ˆREVERIEï¼‰ä»»åŠ¡è¦æ±‚æ™ºèƒ½ä½“åœ¨å¤æ‚çš„å®¤å†…ç¯å¢ƒä¸­è¿›è¡Œå¯¼èˆªï¼Œå¹¶å®šä½ç”±é«˜çº§æŒ‡ä»¤æŒ‡å®šçš„è¿œç¨‹å¯¹è±¡ï¼Œä¾‹å¦‚â€œç»™æˆ‘æ‹¿ä¸€ä¸ªå‹ºå­â€ï¼Œæ— éœ€é¢„å…ˆæ¢ç´¢ã€‚å› æ­¤ï¼Œæœ‰æ•ˆçš„å¯¼èˆªè®¡åˆ’å¯¹äºæœ€ç»ˆçš„æˆåŠŸè‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å‚æ•°é«˜æ•ˆåŠ¨ä½œè§„åˆ’å™¨ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆPEAP-LLMï¼‰ï¼Œåœ¨æ¯ä¸ªä½ç½®ç”Ÿæˆå•æ­¥æŒ‡ä»¤ã€‚æ‰€æå‡ºçš„æ¨¡å‹ç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼Œå³LLMç›®æ ‡è§„åˆ’å™¨ï¼ˆLGPï¼‰å’ŒLoRAåŠ¨ä½œè§„åˆ’å™¨ï¼ˆLAPï¼‰ã€‚é¦–å…ˆï¼ŒLGPä»REVERIEæŒ‡ä»¤ä¸­æå–ç›®æ ‡å¯¼å‘è®¡åˆ’ï¼ŒåŒ…æ‹¬ç›®æ ‡å¯¹è±¡å’Œæˆ¿é—´ã€‚ç„¶åï¼ŒLAPä»¥ç›®æ ‡å¯¼å‘è®¡åˆ’ã€é«˜çº§æŒ‡ä»¤å’Œå½“å‰è§†è§‰è§‚å¯Ÿä¸ºè¾“å…¥ï¼Œç”Ÿæˆå•æ­¥æŒ‡ä»¤ã€‚PEAP-LLMä½¿å®ä½“æ™ºèƒ½ä½“èƒ½å¤Ÿå³æ—¶ä¸LAPä½œä¸ºè·¯å¾„è§„åˆ’å™¨è¿›è¡Œäº¤äº’ã€‚å•çº¯åœ°å°†å¤§å‹è¯­è¨€æ¨¡å‹ç›´æ¥åº”ç”¨å¾ˆéš¾å–å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºäºç¡¬æç¤ºçš„æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸­å®¹æ˜“å‡ºç°é”™è¯¯ï¼Œéœ€è¦äººå·¥å¹²é¢„ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œé˜²æ­¢LLMäº§ç”Ÿå¹»è§‰å’Œåè§ä¿¡æ¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µå¾®è°ƒLLMçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSTFï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€‚STFæé«˜äº†ç”ŸæˆæŒ‡ä»¤çš„è´¨é‡ï¼Œè€ŒDPOåˆ™åˆ©ç”¨äº†ç¯å¢ƒåé¦ˆã€‚å®éªŒç»“æœè¯æ˜ï¼Œä¸å…ˆå‰çš„æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨REVERIEä¸Šçš„æ¨¡å‹å…·æœ‰ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07500v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¿œç¨‹å®ä½“æŒ‡ä»£è¡¨è¾¾ï¼ˆREVERIEï¼‰ä»»åŠ¡ä¸­ï¼Œä¸€ä¸ªæ™ºèƒ½ä½“å¦‚ä½•åœ¨å¤æ‚çš„å®¤å†…ç¯å¢ƒä¸­å¯¼èˆªå¹¶å®šä½ç”±é«˜çº§æŒ‡ä»¤æŒ‡å®šçš„è¿œç¨‹å¯¹è±¡ã€‚ä¸ºæé«˜æ•ˆç‡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆåŠ¨ä½œè§„åˆ’å™¨ï¼ˆPEAP-LLMï¼‰ã€‚è¯¥æ¨¡å‹åŒ…å«LLMç›®æ ‡è§„åˆ’å™¨ï¼ˆLGPï¼‰å’ŒLoRAåŠ¨ä½œè§„åˆ’å™¨ï¼ˆLAPï¼‰ã€‚LGPä»REVERIEæŒ‡ä»¤ä¸­æå–ç›®æ ‡å¯¼å‘çš„è®¡åˆ’ï¼ŒåŒ…æ‹¬ç›®æ ‡å¯¹è±¡å’Œæˆ¿é—´ã€‚LAPåˆ™æ ¹æ®ç›®æ ‡å¯¼å‘çš„è®¡åˆ’ã€é«˜çº§æŒ‡ä»¤å’Œå½“å‰è§†è§‰è§‚å¯Ÿç”Ÿæˆå•æ­¥æŒ‡ä»¤ã€‚PEAP-LLMä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿä½œä¸ºè·¯å¾„è§„åˆ’å™¨å³æ—¶ä¸LAPäº’åŠ¨ã€‚ä¸ºè§£å†³ç›´æ¥ä½¿ç”¨LLMçš„é—®é¢˜å’Œé˜²æ­¢LLMäº§ç”Ÿå¹»è§‰å’Œåè§ä¿¡æ¯ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µå¾®è°ƒLLMæ–¹æ³•ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSTFï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€‚STFæé«˜äº†ç”ŸæˆæŒ‡ä»¤çš„è´¨é‡ï¼Œè€ŒDPOåˆ™åˆ©ç”¨ç¯å¢ƒåé¦ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¹‹å‰çš„å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„æ¨¡å‹åœ¨REVERIEä»»åŠ¡ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>REVERIEä»»åŠ¡è¦æ±‚æ™ºèƒ½ä½“åœ¨å¤æ‚çš„å®¤å†…ç¯å¢ƒä¸­å¯¼èˆªå¹¶å®šä½ç”±é«˜çº§æŒ‡ä»¤æŒ‡å®šçš„è¿œç¨‹å¯¹è±¡ã€‚</li>
<li>PEAP-LLMæ˜¯ä¸€ç§ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆåŠ¨ä½œè§„åˆ’å™¨ï¼ŒåŒ…å«LLMç›®æ ‡è§„åˆ’å™¨ï¼ˆLGPï¼‰å’ŒLoRAåŠ¨ä½œè§„åˆ’å™¨ï¼ˆLAPï¼‰ã€‚</li>
<li>LGPä»REVERIEæŒ‡ä»¤ä¸­æå–ç›®æ ‡å¯¼å‘çš„è®¡åˆ’ï¼ŒåŒ…æ‹¬ç›®æ ‡å¯¹è±¡å’Œæˆ¿é—´ï¼›LAPåˆ™ç”Ÿæˆå•æ­¥æŒ‡ä»¤ã€‚</li>
<li>PEAP-LLMä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿä½œä¸ºè·¯å¾„è§„åˆ’å™¨å³æ—¶ä¸LAPäº’åŠ¨ï¼Œæé«˜å¯¼èˆªæ•ˆç‡ã€‚</li>
<li>ç›´æ¥åº”ç”¨LLMå­˜åœ¨é—®é¢˜å’Œåè§ä¿¡æ¯é£é™©ï¼Œå› æ­¤æå‡ºäº†ç›‘ç£å¾®è°ƒï¼ˆSTFï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„ä¸¤é˜¶æ®µæ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>STFæé«˜äº†ç”ŸæˆæŒ‡ä»¤çš„è´¨é‡ï¼Œè€ŒDPOåˆ©ç”¨ç¯å¢ƒåé¦ˆæ¥ä¼˜åŒ–æ¨¡å‹è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07500">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-669d4f762fdf146cffffdf8993a5285f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fc5ac78ffdb1dfcf79791a6ba7bf703.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79145462fbb0e19f27964f4d539049f4.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-Multi-Agent-Reasoning-Systems-for-Collaborative-Expertise-Delegation-An-Exploratory-Design-Study"><a href="#Towards-Multi-Agent-Reasoning-Systems-for-Collaborative-Expertise-Delegation-An-Exploratory-Design-Study" class="headerlink" title="Towards Multi-Agent Reasoning Systems for Collaborative Expertise   Delegation: An Exploratory Design Study"></a>Towards Multi-Agent Reasoning Systems for Collaborative Expertise   Delegation: An Exploratory Design Study</h2><p><strong>Authors:Baixuan Xu, Chunyang Li, Weiqi Wang, Wei Fan, Tianshi Zheng, Haochen Shi, Tao Fan, Yangqiu Song, Qiang Yang</strong></p>
<p>Designing effective collaboration structure for multi-agent LLM systems to enhance collective reasoning is crucial yet remains under-explored. In this paper, we systematically investigate how collaborative reasoning performance is affected by three key design dimensions: (1) Expertise-Domain Alignment, (2) Collaboration Paradigm (structured workflow vs. diversity-driven integration), and (3) System Scale. Our findings reveal that expertise alignment benefits are highly domain-contingent, proving most effective for contextual reasoning tasks. Furthermore, collaboration focused on integrating diverse knowledge consistently outperforms rigid task decomposition. Finally, we empirically explore the impact of scaling the multi-agent system with expertise specialization and study the computational trade off, highlighting the need for more efficient communication protocol design. This work provides concrete guidelines for configuring specialized multi-agent system and identifies critical architectural trade-offs and bottlenecks for scalable multi-agent reasoning. The code will be made available upon acceptance. </p>
<blockquote>
<p>åœ¨å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿä¸­è®¾è®¡æœ‰æ•ˆçš„åä½œç»“æ„ä»¥æå‡é›†ä½“æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œä½†è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†ä¸‰ä¸ªå…³é”®è®¾è®¡ç»´åº¦å¦‚ä½•å½±å“åä½œæ¨ç†æ€§èƒ½ï¼šï¼ˆ1ï¼‰ä¸“ä¸šé¢†åŸŸçŸ¥è¯†ä¸æŠ€èƒ½çš„åŒ¹é…åº¦ï¼›ï¼ˆ2ï¼‰åä½œèŒƒå¼ï¼ˆç»“æ„åŒ–å·¥ä½œæµç¨‹ä¸å¤šæ ·æ€§é©±åŠ¨é›†æˆï¼‰ï¼›ä»¥åŠï¼ˆ3ï¼‰ç³»ç»Ÿè§„æ¨¡ã€‚æˆ‘ä»¬å‘ç°ä¸“ä¸šæŠ€èƒ½åŒ¹é…çš„ä¼˜åŠ¿é«˜åº¦ä¾èµ–äºç‰¹å®šé¢†åŸŸï¼Œå¯¹äºè¯­å¢ƒæ¨ç†ä»»åŠ¡æœ€ä¸ºæœ‰æ•ˆã€‚æ­¤å¤–ï¼Œèšç„¦äºæ•´åˆå¤šæ ·åŒ–çŸ¥è¯†çš„åä½œæ–¹å¼æŒç»­ä¼˜äºåƒµåŒ–çš„ä»»åŠ¡åˆ†è§£æ–¹å¼ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å®è¯æ¢ç´¢äº†é€šè¿‡ä¸“ä¸šæŠ€èƒ½ä¸“ä¸šåŒ–æ‰©å±•å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å½±å“ï¼Œå¹¶ç ”ç©¶äº†è®¡ç®—æŠ˜è¡·æ–¹æ¡ˆï¼Œå¼ºè°ƒäº†éœ€è¦è®¾è®¡æ›´é«˜æ•ˆçš„é€šä¿¡åè®®ã€‚è¿™é¡¹å·¥ä½œä¸ºé…ç½®ä¸“ä¸šåŒ–çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†å…·ä½“æŒ‡å¯¼ï¼Œå¹¶ç¡®å®šäº†å¯æ‰©å±•çš„å¤šæ™ºèƒ½ä½“æ¨ç†çš„å…³é”®æ¶æ„æŠ˜è¡·æ–¹æ¡ˆå’Œç“¶é¢ˆã€‚ä»£ç å°†åœ¨æ¥å—åæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07313v1">PDF</a> 18 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç³»ç»Ÿæ¢è®¨äº†å¦‚ä½•è®¾è®¡å¤šæ™ºèƒ½ä½“å¤§å‹è¯­è¨€æ¨¡å‹ç³»ç»Ÿçš„åä½œç»“æ„ä»¥æå‡é›†ä½“æ¨ç†æ•ˆèƒ½ã€‚ç ”ç©¶åˆ†æäº†ä¸‰ä¸ªå…³é”®è®¾è®¡ç»´åº¦â€”â€”ä¸“ä¸šçŸ¥è¯†çš„é¢†åŸŸä¸€è‡´æ€§ã€åä½œèŒƒå¼å’Œç³»ç»Ÿè§„æ¨¡çš„å½±å“ã€‚ç ”ç©¶è¡¨æ˜ä¸“ä¸šçŸ¥è¯†çš„ä¸€è‡´æ€§åœ¨ä¸åŒé¢†åŸŸä¸­è¡¨ç°å‡ºè‰¯å¥½çš„ä¸Šä¸‹æ–‡æ¨ç†ä»»åŠ¡æ•ˆç›Šï¼Œé€šè¿‡é›†æˆå¤šå…ƒåŒ–çŸ¥è¯†ä¸ºåŸºç¡€çš„åä½œæŒç»­ä¼˜äºç¡¬æ€§ä»»åŠ¡åˆ†è§£æ–¹å¼ã€‚åŒæ—¶ï¼Œè®ºæ–‡å®è¯æ¢ç´¢äº†ä¸“ä¸šæŠ€èƒ½åˆ†å·¥å¯¹äºè§„æ¨¡åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å½±å“ï¼Œå¼ºè°ƒäº†æ›´é«˜æ•ˆæ²Ÿé€šåè®®è®¾è®¡çš„å¿…è¦æ€§ã€‚è¯¥ç ”ç©¶ä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„é…ç½®æä¾›äº†å®é™…æŒ‡å¯¼ï¼Œå¹¶æŒ‡å‡ºäº†å¯è§„æ¨¡åŒ–å¤šæ™ºèƒ½ä½“æ¨ç†çš„å…³é”®æ¶æ„æƒè¡¡å’Œç“¶é¢ˆã€‚ä»£ç å°†åœ¨æ¥å—åå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“å¤§å‹è¯­è¨€æ¨¡å‹ç³»ç»Ÿçš„åä½œç»“æ„è®¾è®¡å¯¹äºæå‡é›†ä½“æ¨ç†æ•ˆèƒ½è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶åˆ†æäº†ä¸“ä¸šçŸ¥è¯†çš„é¢†åŸŸä¸€è‡´æ€§ã€åä½œèŒƒå¼å’Œç³»ç»Ÿè§„æ¨¡ä¸‰ä¸ªå…³é”®è®¾è®¡ç»´åº¦ã€‚</li>
<li>ä¸“ä¸šçŸ¥è¯†çš„ä¸€è‡´æ€§åœ¨ä¸Šä¸‹æ–‡æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆç›Šï¼Œä¸”æ•ˆç›Šé«˜åº¦ä¾èµ–äºé¢†åŸŸã€‚</li>
<li>ä»¥é›†æˆå¤šå…ƒåŒ–çŸ¥è¯†ä¸ºåŸºç¡€çš„åä½œæŒç»­ä¼˜äºç¡¬æ€§ä»»åŠ¡åˆ†è§£æ–¹å¼ã€‚</li>
<li>è®ºæ–‡å®è¯æ¢ç´¢äº†ä¸“ä¸šæŠ€èƒ½åˆ†å·¥å¯¹äºè§„æ¨¡åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å½±å“ã€‚</li>
<li>æ›´é«˜æ•ˆçš„æ²Ÿé€šåè®®è®¾è®¡å¯¹äºå®ç°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åä½œå’Œè§„æ¨¡åŒ–è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d4ac57b50e61d13e64807510b295387a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a55bc0d30fa27f7151ce15cbcc1017a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1f04fa792c064896cef9241a394b3ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af63df485d37ac2d376b5344f9ec4b1d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="INTELLECT-2-A-Reasoning-Model-Trained-Through-Globally-Decentralized-Reinforcement-Learning"><a href="#INTELLECT-2-A-Reasoning-Model-Trained-Through-Globally-Decentralized-Reinforcement-Learning" class="headerlink" title="INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized   Reinforcement Learning"></a>INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized   Reinforcement Learning</h2><p><strong>Authors: Prime Intellect Team, Sami Jaghouar, Justus Mattern, Jack Min Ong, Jannik Straube, Manveer Basra, Aaron Pazdera, Kushal Thaman, Matthew Di Ferrante, Felix Gabriel, Fares Obeid, Kemal Erdem, Michael Keiblinger, Johannes Hagemann</strong></p>
<p>We introduce INTELLECT-2, the first globally distributed reinforcement learning (RL) training run of a 32 billion parameter language model. Unlike traditional centralized training efforts, INTELLECT-2 trains a reasoning model using fully asynchronous RL across a dynamic, heterogeneous swarm of permissionless compute contributors.   To enable a training run with this unique infrastructure, we built various components from scratch: we introduce PRIME-RL, our training framework purpose-built for distributed asynchronous reinforcement learning, based on top of novel components such as TOPLOC, which verifies rollouts from untrusted inference workers, and SHARDCAST, which efficiently broadcasts policy weights from training nodes to inference workers.   Beyond infrastructure components, we propose modifications to the standard GRPO training recipe and data filtering techniques that were crucial to achieve training stability and ensure that our model successfully learned its training objective, thus improving upon QwQ-32B, the state of the art reasoning model in the 32B parameter range.   We open-source INTELLECT-2 along with all of our code and data, hoping to encourage and enable more open research in the field of decentralized training. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†INTELLECT-2ï¼Œè¿™æ˜¯é¦–ä¸ªå…¨çƒåˆ†å¸ƒçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒè¿è¡Œçš„32äº¿å‚æ•°è¯­è¨€æ¨¡å‹ã€‚ä¸åŒäºä¼ ç»Ÿçš„é›†ä¸­åŒ–è®­ç»ƒåŠªåŠ›ï¼ŒINTELLECT-2ä½¿ç”¨å®Œå…¨å¼‚æ­¥çš„å¼ºåŒ–å­¦ä¹ åœ¨ä¸€ä¸ªåŠ¨æ€ã€å¼‚æ„çš„æ— éœ€è®¸å¯çš„è®¡ç®—è´¡çŒ®è€…é›†ç¾¤ä¸Šè®­ç»ƒæ¨ç†æ¨¡å‹ã€‚ä¸ºäº†åœ¨è¿™ç§ç‹¬ç‰¹çš„æ¶æ„ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬ä»é›¶å¼€å§‹æ„å»ºäº†å„ç§ç»„ä»¶ï¼šæˆ‘ä»¬æ¨å‡ºäº†PRIME-RLï¼Œè¿™æ˜¯ä¸“ä¸ºåˆ†å¸ƒå¼å¼‚æ­¥å¼ºåŒ–å­¦ä¹ æ„å»ºçš„è®­ç»ƒæ¡†æ¶ï¼ŒåŸºäºæ–°å‹ç»„ä»¶ï¼Œå¦‚TOPLOCï¼ˆç”¨äºéªŒè¯æ¥è‡ªä¸å—ä¿¡ä»»æ¨ç†å·¥ä½œè€…çš„è¿è¡Œç»“æœï¼‰å’ŒSHARDCASTï¼ˆç”¨äºæœ‰æ•ˆåœ°å°†ç­–ç•¥æƒé‡ä»è®­ç»ƒèŠ‚ç‚¹å¹¿æ’­åˆ°æ¨ç†å·¥ä½œè€…ï¼‰ã€‚é™¤äº†æ¶æ„ç»„ä»¶ä¹‹å¤–ï¼Œæˆ‘ä»¬å¯¹æ ‡å‡†GRPOè®­ç»ƒé…æ–¹å’Œæ•°æ®è¿‡æ»¤æŠ€æœ¯è¿›è¡Œäº†ä¿®æ”¹ï¼Œè¿™å¯¹äºå®ç°è®­ç»ƒç¨³å®šæ€§å’Œç¡®ä¿æˆ‘ä»¬çš„æ¨¡å‹æˆåŠŸå­¦ä¹ å…¶è®­ç»ƒç›®æ ‡æ˜¯è‡³å…³é‡è¦çš„ï¼Œä»è€Œåœ¨32Bå‚æ•°èŒƒå›´å†…çš„æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹QwQ-32Bçš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ã€‚æˆ‘ä»¬å…¬å¼€äº†INTELLECT-2ä»¥åŠæˆ‘ä»¬æ‰€æœ‰çš„ä»£ç å’Œæ•°æ®ï¼Œå¸Œæœ›é¼“åŠ±å’Œä¿ƒè¿›åˆ†å¸ƒå¼è®­ç»ƒé¢†åŸŸçš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07291v1">PDF</a> 26 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç« ä»‹ç»äº†é¦–ä¸ªå…¨çƒåˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿è¡Œçš„â€œINTELLECT-2â€ã€‚è¿™æ˜¯ä¸€é¡¹é‡‡ç”¨å¼‚æ­¥å¼ºåŒ–å­¦ä¹ åœ¨åŠ¨æ€ã€å¼‚æ„çš„åˆ†å¸ƒå¼è®¡ç®—é›†ç¾¤ä¸Šè¿›è¡Œçš„è®­ç»ƒæ¨ç†æ¨¡å‹çš„å·¥ä½œã€‚æ–‡ç« è¯¦ç»†æè¿°äº†ä¸ºå®ç°è¿™ä¸€è®­ç»ƒæ‰€å¼€å‘çš„è®­ç»ƒæ¡†æ¶ã€åŸºç¡€è®¾æ–½ç»„ä»¶å’ŒæŠ€æœ¯æ”¹è¿›ï¼ŒåŒ…æ‹¬éªŒè¯æ¥è‡ªä¸å¯ä¿¡æ¨ç†å·¥ä½œè€…çš„æ¨ç†ç»“æœçš„TOPLOCã€å¹¿æ’­è®­ç»ƒèŠ‚ç‚¹ç­–ç•¥æƒé‡çš„SHARDCASTä»¥åŠå¯¹æ ‡å‡†GRPOè®­ç»ƒé£Ÿè°±å’Œæ•°æ®è¿‡æ»¤æŠ€æœ¯çš„æ”¹è¿›ã€‚æœ€åï¼Œä½œè€…å¼€æºäº†â€œINTELLECT-2â€å’Œç›¸å…³ä»£ç æ•°æ®ï¼Œä»¥ä¿ƒè¿›åˆ†å¸ƒå¼è®­ç»ƒé¢†åŸŸçš„ç ”ç©¶å¼€æ”¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>â€œINTELLECT-2â€æ˜¯å…¨çƒé¦–ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„åˆ†å¸ƒå¼è®­ç»ƒæ¨ç†æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨å¼‚æ­¥å¼ºåŒ–å­¦ä¹ åœ¨åŠ¨æ€ã€å¼‚æ„çš„åˆ†å¸ƒå¼è®¡ç®—é›†ç¾¤ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>æ–‡ç« ä»‹ç»äº†ä¸ºå®ç°è¿™ä¸€è®­ç»ƒæ‰€å¼€å‘çš„è®­ç»ƒæ¡†æ¶PRIME-RLã€‚</li>
<li>æ–‡ç« è¯¦ç»†æè¿°äº†ç”¨äºéªŒè¯æ¨ç†ç»“æœçš„TOPLOCæŠ€æœ¯å’Œå¹¿æ’­ç­–ç•¥æƒé‡çš„SHARDCASTæŠ€æœ¯ã€‚</li>
<li>è¯¥æ¨¡å‹æ”¹è¿›äº†GRPOè®­ç»ƒæ–¹æ³•å’Œæ•°æ®è¿‡æ»¤æŠ€æœ¯ï¼Œå®ç°äº†è®­ç»ƒç¨³å®šæ€§å’Œå­¦ä¹ æˆæœçš„æå‡ã€‚</li>
<li>è¯¥æ¨¡å‹è¶…è¶Šäº†ç°æœ‰çš„32Bå‚æ•°èŒƒå›´å†…çš„æœ€ä½³æ¨¡å‹QwQ-32Bã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07291">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-764032ef5666413bee925f2bb85690b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cac6cbede241746c44d85af645828aff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe8b3b221677d318a8d02f585aa81af1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Skywork-VL-Reward-An-Effective-Reward-Model-for-Multimodal-Understanding-and-Reasoning"><a href="#Skywork-VL-Reward-An-Effective-Reward-Model-for-Multimodal-Understanding-and-Reasoning" class="headerlink" title="Skywork-VL Reward: An Effective Reward Model for Multimodal   Understanding and Reasoning"></a>Skywork-VL Reward: An Effective Reward Model for Multimodal   Understanding and Reasoning</h2><p><strong>Authors:Xiaokun Wang,  Chris, Jiangbo Pei, Wei Shen, Yi Peng, Yunzhuo Hao, Weijie Qiu, Ai Jian, Tianyidan Xie, Xuchen Song, Yang Liu, Yahui Zhou</strong></p>
<p>We propose Skywork-VL Reward, a multimodal reward model that provides reward signals for both multimodal understanding and reasoning tasks. Our technical approach comprises two key components: First, we construct a large-scale multimodal preference dataset that covers a wide range of tasks and scenarios, with responses collected from both standard vision-language models (VLMs) and advanced VLM reasoners. Second, we design a reward model architecture based on Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage fine-tuning using pairwise ranking loss on pairwise preference data. Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art results on multimodal VL-RewardBench and exhibits competitive performance on the text-only RewardBench benchmark. Furthermore, preference data constructed based on our Skywork-VL Reward proves highly effective for training Mixed Preference Optimization (MPO), leading to significant improvements in multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as a significant advancement toward general-purpose, reliable reward models for multimodal alignment. Our model has been publicly released to promote transparency and reproducibility. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Skywork-VL Rewardè¿™ä¸€å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸ºç†è§£å’Œæ¨ç†ä»»åŠ¡æä¾›å¥–åŠ±ä¿¡å·ã€‚æˆ‘ä»¬çš„æŠ€æœ¯æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªå…³é”®éƒ¨åˆ†ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€åå¥½æ•°æ®é›†ï¼Œæ¶µç›–å¹¿æ³›çš„ä»»åŠ¡å’Œåœºæ™¯ï¼Œæ”¶é›†çš„å“åº”æ—¢æ¥è‡ªæ ‡å‡†çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œä¹ŸåŒ…æ‹¬å…ˆè¿›çš„VLMæ¨ç†å™¨ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä»¥Qwen2.5-VL-7B-Instructä¸ºåŸºç¡€è®¾è®¡å¥–åŠ±æ¨¡å‹æ¶æ„ï¼Œæ•´åˆå¥–åŠ±å¤´éƒ¨ï¼Œåˆ©ç”¨æˆå¯¹çš„åå¥½æ•°æ®ä½¿ç”¨æˆå¯¹æ’åæŸå¤±è¿›è¡Œå¤šé˜¶æ®µå¾®è°ƒã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒSkywork-VL Rewardåœ¨å¤šæ¨¡æ€VL-RewardBenchä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå¹¶åœ¨åªæœ‰æ–‡æœ¬çš„RewardBenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºäº†ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼ŒåŸºäºæˆ‘ä»¬Skywork-VL Rewardæ„å»ºçš„åå¥½æ•°æ®è¢«è¯æ˜å¯¹è®­ç»ƒæ··åˆåå¥½ä¼˜åŒ–ï¼ˆMPOï¼‰é«˜åº¦æœ‰æ•ˆï¼Œæå¤§åœ°æé«˜äº†å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒSkywork-VL Rewardåœ¨å¤šæ¨¡æ€å¯¹é½çš„é€šç”¨å¯é å¥–åŠ±æ¨¡å‹æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚æˆ‘ä»¬çš„æ¨¡å‹å·²ç»å…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›é€æ˜åº¦å’Œå¯é‡å¤æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07263v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Skywork-VL Rewardæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼Œç”¨äºç†è§£å’Œæ¨ç†ä»»åŠ¡ã€‚å…¶æŠ€æœ¯æ–¹æ³•åŒ…æ‹¬æ„å»ºå¤§è§„æ¨¡å¤šæ¨¡æ€åå¥½æ•°æ®é›†å’ŒåŸºäºQwen2.5-VL-7B-Instructè®¾è®¡å¥–åŠ±æ¨¡å‹æ¶æ„ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒSkywork-VL Rewardåœ¨å¤šæ¨¡æ€VL-RewardBenchä¸Šå–å¾—æœ€ä½³ç»“æœï¼Œå¹¶åœ¨çº¯æ–‡æœ¬RewardBenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚æ­¤å¤–ï¼ŒåŸºäºSkywork-VL Rewardæ„å»ºçš„åå¥½æ•°æ®å¯¹Mixed Preference Optimization (MPO)çš„è®­ç»ƒéå¸¸æœ‰æ•ˆï¼Œèƒ½æ˜¾è‘—æé«˜å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚æ­¤æ¨¡å‹å·²å…¬å¼€ï¼Œä»¥ä¿ƒè¿›é€æ˜åº¦å’Œå¯é‡å¤æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Skywork-VL Rewardæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼Œæ—¨åœ¨æä¾›ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­çš„å¥–åŠ±ä¿¡å·ã€‚</li>
<li>æŠ€æœ¯æ–¹æ³•åŒ…æ‹¬æ„å»ºå¤§è§„æ¨¡å¤šæ¨¡æ€åå¥½æ•°æ®é›†ï¼Œæ¶µç›–å¹¿æ³›çš„ä»»åŠ¡å’Œåœºæ™¯ã€‚</li>
<li>å¥–åŠ±æ¨¡å‹æ¶æ„åŸºäºQwen2.5-VL-7B-Instructè®¾è®¡ï¼Œå¹¶é›†æˆäº†å¥–åŠ±å¤´ã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒSkywork-VL Rewardåœ¨å¤šæ¨¡æ€VL-RewardBenchä¸Šå–å¾—æœ€ä½³ç»“æœã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨çº¯æ–‡æœ¬RewardBenchåŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>åŸºäºSkywork-VL Rewardçš„åå¥½æ•°æ®å¯¹MPOçš„è®­ç»ƒéå¸¸æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07263">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-233c3263299db8885eecc0d11a63c21e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14b5098c009a03436c14ced2f1535321.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-214cc83659b929e9d57319ae52f3bf84.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a93fa163fbe83a7863420174e29bf362.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DynamicRAG-Leveraging-Outputs-of-Large-Language-Model-as-Feedback-for-Dynamic-Reranking-in-Retrieval-Augmented-Generation"><a href="#DynamicRAG-Leveraging-Outputs-of-Large-Language-Model-as-Feedback-for-Dynamic-Reranking-in-Retrieval-Augmented-Generation" class="headerlink" title="DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for   Dynamic Reranking in Retrieval-Augmented Generation"></a>DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for   Dynamic Reranking in Retrieval-Augmented Generation</h2><p><strong>Authors:Jiashuo Sun, Xianrui Zhong, Sizhe Zhou, Jiawei Han</strong></p>
<p>Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker, which refines retrieved documents to enhance generation quality and explainability. The challenge of selecting the optimal number of documents (k) remains unsolved: too few may omit critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. In this paper, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. We model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results. The model, data and code are available at <a target="_blank" rel="noopener" href="https://github.com/GasolSun36/DynamicRAG">https://github.com/GasolSun36/DynamicRAG</a> </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤–éƒ¨çŸ¥è¯†æ£€ç´¢ï¼Œä½¿å…¶æˆä¸ºçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„é«˜æ•ˆå·¥å…·ã€‚è¿™äº›ç³»ç»Ÿä¸­çš„å…³é”®ä½†ç»å¸¸è¢«å¿½è§†çš„éƒ¨åˆ†æ˜¯é‡æ’å™¨ï¼Œå®ƒä¼šå¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œç²¾ç‚¼ï¼Œä»¥æé«˜ç”Ÿæˆè´¨é‡å’Œå¯è§£é‡Šæ€§ã€‚é€‰æ‹©æœ€ä½³æ–‡æ¡£æ•°é‡ï¼ˆkï¼‰çš„æŒ‘æˆ˜ä»æœªè§£å†³ï¼šå¤ªå°‘å¯èƒ½ä¼šé—æ¼å…³é”®ä¿¡æ¯ï¼Œè€Œå¤ªå¤šåˆ™ä¼šå¼•å…¥å™ªéŸ³å’Œæ•ˆç‡é—®é¢˜ã€‚å°½ç®¡æœ€è¿‘çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†åŸºäºLLMçš„é‡æ’å™¨ï¼Œä½†å®ƒä»¬ä¸»è¦åˆ©ç”¨å†…éƒ¨æ¨¡å‹çŸ¥è¯†ï¼Œå¹¶å¿½è§†äº†LLMå¯ä»¥æä¾›çš„ä¸°å¯Œç›‘ç£ä¿¡å·ï¼Œä¾‹å¦‚ä½¿ç”¨å“åº”è´¨é‡ä½œä¸ºä¼˜åŒ–é‡æ’å†³ç­–çš„åé¦ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DynamicRAGï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹RAGæ¡†æ¶ï¼Œå…¶ä¸­é‡æ’å™¨ä¼šæ ¹æ®æŸ¥è¯¢åŠ¨æ€è°ƒæ•´æ£€ç´¢åˆ°çš„æ–‡æ¡£çš„æ’åºå’Œæ•°é‡ã€‚æˆ‘ä»¬å°†é‡æ’å™¨å»ºæ¨¡ä¸ºé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–çš„ä»£ç†ï¼Œä½¿ç”¨ä»LLMè¾“å‡ºè´¨é‡ä¸­è·å¾—çš„å¥–åŠ±ã€‚åœ¨ä¸ƒä¸ªçŸ¥è¯†å¯†é›†å‹æ•°æ®é›†ä¸Šï¼ŒDynamicRAGè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†æœ€æ–°çš„ç»“æœã€‚æ¨¡å‹ã€æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/GasolSun36/DynamicRAG">é“¾æ¥åœ°å€</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07233v1">PDF</a> 24 pages, 6 figures, 15 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ£€ç´¢å¢å¼ºçš„ç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤–éƒ¨çŸ¥è¯†æ£€ç´¢ï¼Œå¯¹äºçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡éå¸¸æœ‰æ•ˆã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†å…¶ä¸­çš„å…³é”®ç»„ä»¶â€”â€”é‡æ’å™¨ï¼ˆrerankerï¼‰ï¼Œå®ƒèƒ½å¤Ÿä¼˜åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œæé«˜ç”Ÿæˆè´¨é‡å’Œå¯è§£é‡Šæ€§ã€‚å°½ç®¡é€‰æ‹©æœ€ä½³æ–‡æ¡£æ•°é‡ï¼ˆkï¼‰çš„é—®é¢˜å°šæœªè§£å†³ï¼Œä½†æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„RAGæ¡†æ¶â€”â€”DynamicRAGï¼Œå…¶ä¸­é‡æ’å™¨èƒ½å¤Ÿæ ¹æ®æŸ¥è¯¢åŠ¨æ€è°ƒæ•´æ£€ç´¢åˆ°çš„æ–‡æ¡£çš„é¡ºåºå’Œæ•°é‡ã€‚DynamicRAGä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–é‡æ’å™¨ï¼Œå¹¶ä»LLMè¾“å‡ºè´¨é‡ä¸­è·å–å¥–åŠ±ã€‚åœ¨ä¸ƒä¸ªçŸ¥è¯†å¯†é›†å‹æ•°æ®é›†ä¸Šï¼ŒDynamicRAGè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤–éƒ¨çŸ¥è¯†æ£€ç´¢ï¼Œé€‚ç”¨äºçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ã€‚</li>
<li>é‡æ’å™¨ï¼ˆrerankerï¼‰æ˜¯RAGç³»ç»Ÿä¸­çš„ä¸€ä¸ªå…³é”®ç»„ä»¶ï¼Œèƒ½ä¼˜åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œæé«˜ç”Ÿæˆè´¨é‡å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>é€‰æ‹©æœ€ä½³æ–‡æ¡£æ•°é‡ï¼ˆkï¼‰æ˜¯RAGç³»ç»Ÿé¢ä¸´çš„æŒ‘æˆ˜ï¼Œéœ€è¦å¹³è¡¡è·å–ä¿¡æ¯çš„åŒæ—¶é¿å…å¼•å…¥å™ªå£°å’Œæ•ˆç‡ä½ä¸‹ã€‚</li>
<li>æœ€è¿‘çš„ç ”ç©¶ä¸»è¦åˆ©ç”¨å†…éƒ¨æ¨¡å‹çŸ¥è¯†æ¥æ„å»ºLLMé‡æ’å™¨ï¼Œä½†å¿½ç•¥äº†LLMæä¾›çš„ä¸°å¯Œç›‘ç£ä¿¡å·ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†DynamicRAGæ¡†æ¶ï¼Œé‡æ’å™¨å¯æ ¹æ®æŸ¥è¯¢åŠ¨æ€è°ƒæ•´æ£€ç´¢åˆ°çš„æ–‡æ¡£çš„é¡ºåºå’Œæ•°é‡ã€‚</li>
<li>DynamicRAGä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–é‡æ’å™¨ï¼Œä»LLMè¾“å‡ºè´¨é‡ä¸­è·å–å¥–åŠ±ã€‚</li>
<li>åœ¨å¤šä¸ªçŸ¥è¯†å¯†é›†å‹æ•°æ®é›†ä¸Šï¼ŒDynamicRAGè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07233">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bb29ccebf54083599f0c3b1150ebd2a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da4e32b86258eb9284ebfc9d2316d56d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29e287e463465d73deb41dae2b9f7dea.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Measuring-General-Intelligence-with-Generated-Games"><a href="#Measuring-General-Intelligence-with-Generated-Games" class="headerlink" title="Measuring General Intelligence with Generated Games"></a>Measuring General Intelligence with Generated Games</h2><p><strong>Authors:Vivek Verma, David Huang, William Chen, Dan Klein, Nicholas Tomlin</strong></p>
<p>We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºgg-benchï¼Œè¿™æ˜¯ä¸€ç³»åˆ—ç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹ä¸­é€šç”¨æ¨ç†èƒ½åŠ›çš„æ¸¸æˆç¯å¢ƒé›†åˆã€‚ä¸åŒäºå¤§å¤šæ•°é™æ€åŸºå‡†æµ‹è¯•ï¼Œgg-benchæ˜¯ä¸€ä¸ªæ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼Œå¯ä»¥æŒ‰éœ€ç”Ÿæˆæ–°çš„è¯„ä¼°å®ä¾‹ã€‚å…·ä½“è€Œè¨€ï¼Œgg-benchæ˜¯é€šè¿‡ä»¥ä¸‹æ–¹å¼åˆæˆç”Ÿæˆçš„ï¼šï¼ˆ1ï¼‰ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆæ–°å‹æ¸¸æˆçš„è‡ªç„¶è¯­è¨€æè¿°ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨LLMå°†æ¯ä¸ªæ¸¸æˆç¼–ç ä¸ºGymç¯å¢ƒï¼›ï¼ˆ3ï¼‰é€šè¿‡åœ¨ç”Ÿæˆçš„æ¸¸æˆä¸Šè¿›è¡Œè‡ªæˆ‘å¯¹æŠ—è®­ç»ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†ã€‚æˆ‘ä»¬é€šè¿‡æç¤ºæ¨¡å‹æ¸¸æˆæè¿°ã€å½“å‰æ£‹ç›˜çŠ¶æ€å’Œæœ‰æ•ˆåŠ¨ä½œåˆ—è¡¨æ¥è¯„ä¼°è¯­è¨€æ¨¡å‹ï¼Œæ¨¡å‹ä¼šè¾“å‡ºå®ƒä»¬æƒ³é‡‡å–çš„è¡ŒåŠ¨ã€‚gg-benchå…·æœ‰æŒ‘æˆ˜æ€§ï¼šæœ€å…ˆè¿›çš„LLMï¼Œå¦‚GPT-4oå’ŒClaude 3.7 Sonnetï¼Œåœ¨gg-benchä¸Šä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œèƒœç‡ä¸º7-9%ï¼Œè€Œæ¨ç†æ¨¡å‹å¦‚o1ã€o3-miniå’ŒDeepSeek-R1çš„å¹³å‡èƒœç‡ä¸º31-36%ã€‚æˆ‘ä»¬å‘å¸ƒç”Ÿæˆçš„æ¸¸æˆã€æ•°æ®ç”Ÿæˆè¿‡ç¨‹å’Œè¯„ä¼°ä»£ç ï¼Œä»¥æ”¯æŒæœªæ¥çš„å»ºæ¨¡å·¥ä½œå’Œæˆ‘ä»¬åŸºå‡†æµ‹è¯•çš„æ‰©å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07215v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>gg-benchæ˜¯ä¸€å¥—ç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹é€šç”¨æ¨ç†èƒ½åŠ›çš„æ¸¸æˆç¯å¢ƒé›†åˆã€‚å®ƒé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°çš„æ–°æ¸¸æˆï¼Œå°†æ¸¸æˆè½¬åŒ–ä¸ºGymç¯å¢ƒï¼Œå¹¶é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆçš„å¼ºåŒ–å­¦ä¹ ä»£ç†è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡æ ¹æ®æ¸¸æˆæè¿°ã€å½“å‰æ£‹ç›˜çŠ¶æ€å’Œæœ‰æ•ˆè¡ŒåŠ¨åˆ—è¡¨æç¤ºæ¨¡å‹ï¼Œè¯„ä¼°è¯­è¨€æ¨¡å‹çš„èƒœç‡ã€‚gg-benchå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰çš„è¯­è¨€æ¨¡å‹åœ¨æ­¤ç¯å¢ƒä¸­çš„èƒœç‡è¾ƒä½ï¼Œè€Œä¸€äº›æ¨ç†æ¨¡å‹çš„èƒœç‡è¾ƒé«˜ã€‚æˆ‘ä»¬å‘å¸ƒäº†ç”Ÿæˆçš„æ¸¸æˆã€æ•°æ®ç”Ÿæˆè¿‡ç¨‹å’Œè¯„ä¼°ä»£ç ï¼Œä»¥æ”¯æŒæœªæ¥çš„å»ºæ¨¡å·¥ä½œå’ŒåŸºå‡†æµ‹è¯•çš„æ‰©å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>gg-benchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹é€šç”¨æ¨ç†èƒ½åŠ›çš„åŠ¨æ€åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å®ƒé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè‡ªç„¶è¯­è¨€çš„æè¿°æ¥åˆ›å»ºæ–°çš„æ¸¸æˆç¯å¢ƒã€‚</li>
<li>gg-benchçš„æ¸¸æˆç¯å¢ƒè¢«è½¬åŒ–ä¸ºGymç¯å¢ƒï¼Œä»¥ä¾¿è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚</li>
<li>é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆçš„å¼ºåŒ–å­¦ä¹ ä»£ç†æ¥è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚</li>
<li>è¯­è¨€æ¨¡å‹åœ¨gg-benchä¸Šçš„èƒœç‡è¾ƒä½ï¼Œè€Œä¸€äº›æ¨ç†æ¨¡å‹çš„èƒœç‡è¾ƒé«˜ã€‚</li>
<li>è¯¥ç ”ç©¶å‘å¸ƒäº†ç”Ÿæˆçš„æ¸¸æˆã€æ•°æ®ç”Ÿæˆè¿‡ç¨‹å’Œè¯„ä¼°ä»£ç ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶å’ŒåŸºå‡†æµ‹è¯•çš„æ‰©å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07215">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3ac4d150ac230a67b306a1726b298bc7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cec528de59c6dbfab35a6bd2d850ccb0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c7508b978f423cd04d3d4d7b8e0cd6f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42c10228695536daef124cf71750221d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe426c46e9b97facc8874f27527e422c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Seed1-5-VL-Technical-Report"><a href="#Seed1-5-VL-Technical-Report" class="headerlink" title="Seed1.5-VL Technical Report"></a>Seed1.5-VL Technical Report</h2><p><strong>Authors:Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei Liu, Qinghao Ye, Rui Qian, Shen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Sijin Wu, Tianheng Cheng, Weiwei Liu, Wenqian Wang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan Ding, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang, Xuehan Xiong, Yanghua Peng, Yangrui Chen, Yanwei Li, Yanxu Hu, Yi Lin, Yiyuan Hu, Yiyuan Zhang, Youbin Wu, Yu Li, Yudong Liu, Yue Ling, Yujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Bencheng Liao, Can Huang, Can Zhang, Chaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei Lou, Chengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang, Gang Wu, Guodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang Hao, Hui Shen, Huixia Li, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen, Jianhui Duan, Jihao Liu, Jin Zeng, Jingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng, Junfeng Zhan, Junjie Fang, Junting Lu, Kai Hua, Kai Liu, Kai Shen, Kaiyuan Zhang, Ke Shen, Ke Wang, Keyu Pan, Kun Zhang, Kunchang Li, Lanxin Li, Lei Li, Lei Shi, Li Han, Liang Xiang, Liangqiang Chen, Lin Chen, Lin Li, Lin Yan, Liying Chi, Longxiang Liu, Mengfei Du, Mingxuan Wang, Ningxin Pan, Peibin Chen, Pengfei Chen, Pengfei Wu, Qingqing Yuan, Qingyao Shuai, Qiuyan Tao, Renjie Zheng, Renrui Zhang, Ru Zhang, Rui Wang, Rui Yang, Rui Zhao, Shaoqiang Xu, Shihao Liang, Shipeng Yan, Shu Zhong, Shuaishuai Cao, Shuangzhi Wu, Shufan Liu, Shuhan Chang, Songhua Cai, Tenglong Ao, Tianhao Yang, Tingting Zhang, Wanjun Zhong, Wei Jia, Wei Weng, Weihao Yu, Wenhao Huang, Wenjia Zhu, Wenli Yang, Wenzhi Wang, Xiang Long, XiangRui Yin, Xiao Li, Xiaolei Zhu, Xiaoying Jia, Xijin Zhang, Xin Liu, Xinchen Zhang, Xinyu Yang, Xiongcai Luo, Xiuli Chen, Xuantong Zhong, Xuefeng Xiao, Xujing Li, Yan Wu, Yawei Wen, Yifan Du, Yihao Zhang, Yining Ye, Yonghui Wu, Yu Liu, Yu Yue, Yufeng Zhou, Yufeng Yuan, Yuhang Xu, Yuhong Yang, Yun Zhang, Yunhao Fang, Yuntao Li, Yurui Ren, Yuwen Xiong, Zehua Hong, Zehua Wang, Zewei Sun, Zeyu Wang, Zhao Cai, Zhaoyue Zha, Zhecheng An, Zhehui Zhao, Zhengzhuo Xu, Zhipeng Chen, Zhiyong Wu, Zhuofan Zheng, Zihao Wang, Zilong Huang, Ziyu Zhu, Zuquan Song</strong></p>
<p>We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at <a target="_blank" rel="noopener" href="https://www.volcengine.com/">https://www.volcengine.com/</a> (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428) </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºSeed1.5-VLï¼Œè¿™æ˜¯ä¸€æ¬¾æ—¨åœ¨æ¨è¿›é€šç”¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†çš„è§†å¬è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚Seed1.5-VLç”±å¸¦æœ‰5.32äº¿å‚æ•°ï¼ˆæ³¨ï¼šåº”æŒ‡çš„æ˜¯äº¿å­—ç¬¦å‚æ•°æ•°é‡ï¼‰çš„è§†è§‰ç¼–ç å™¨å’Œå¸¦æœ‰2ä¸‡äº¿ï¼ˆæ´»è·ƒå‚æ•°ï¼‰æ··åˆä¸“å®¶çŸ¥è¯†ï¼ˆMoEï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ç»„æˆã€‚å°½ç®¡å…¶ç»“æ„ç›¸å¯¹ç´§å‡‘ï¼Œä½†å®ƒåœ¨å¹¿æ³›çš„å…¬å…±è§†è§‰è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•å’Œå†…éƒ¨è¯„ä¼°å¥—ä»¶ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­çš„å‰å…­åé¡¹åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…¶ä¸­ä¸‰åå…«ä¸ªçš„æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚æ­¤å¤–ï¼Œåœ¨é¢å‘ä»£ç†çš„ä»»åŠ¡ï¼ˆå¦‚GUIæ§åˆ¶å’Œæ¸¸æˆï¼‰ä¸­ï¼ŒSeed1.5-VLåœ¨åŒ…æ‹¬OpenAI CUAå’ŒClaude 3.7åœ¨å†…çš„é¢†å…ˆçš„å¤šæ¨¡æ€ç³»ç»Ÿä¸­è¡¨ç°å‡ºè‰²ã€‚é™¤äº†è§†è§‰å’Œè§†é¢‘ç†è§£ä¹‹å¤–ï¼Œå®ƒè¿˜å±•ç¤ºäº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œå¯¹äºè§†è§‰è°œé¢˜ç­‰å¤šæ¨¡æ€æ¨ç†æŒ‘æˆ˜ç‰¹åˆ«æœ‰æ•ˆã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™äº›åŠŸèƒ½å°†åœ¨å„ç§ä»»åŠ¡ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚æœ¬æŠ¥å‘Šä¸»è¦å…¨é¢å›é¡¾äº†æˆ‘ä»¬åœ¨æ„å»ºSeed1.5-VLæ¨¡å‹è®¾è®¡ã€æ•°æ®æ„å»ºå’Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç»éªŒï¼Œå¸Œæœ›è¿™ä»½æŠ¥å‘Šèƒ½æ¿€å‘è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚ç°åœ¨å¯ä»¥é€šè¿‡è®¿é—®<a target="_blank" rel="noopener" href="https://www.volcengine.com/%EF%BC%88%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E6%A8%A1%E5%9E%8BID%EF%BC%9Adoubao-1-5-thinking-vision-pro-250428%EF%BC%89%E6%9D%A5%E4%BA%86%E8%A7%A3%E6%9B%B4%E5%A4%9A%E5%85%B3%E4%BA%8ESeed1.5-VL%E7%9A%84%E4%BF%A1%E6%81%AF%E3%80%82">https://www.volcengine.com/ï¼ˆç«å±±å¼•æ“æ¨¡å‹IDï¼šdoubao-1-5-thinking-vision-pro-250428ï¼‰æ¥äº†è§£æ›´å¤šå…³äºSeed1.5-VLçš„ä¿¡æ¯ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07062v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Seed1.5-VLæ˜¯ä¸€æ¬¾ç”¨äºæ¨è¿›é€šç”¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†çš„è·¨è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚å®ƒåŒ…å«ä¸€ä¸ª532Må‚æ•°çš„è§†è§‰ç¼–ç å™¨å’Œå«æœ‰20Bæ´»è·ƒå‚æ•°çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å¤§å‹è¯­è¨€æ¨¡å‹ã€‚å°½ç®¡æ¶æ„ç›¸å¯¹ç´§å‡‘ï¼Œä½†åœ¨å¹¿æ³›çš„å…¬å¼€VLMåŸºå‡†æµ‹è¯•å’Œå†…éƒ¨è¯„ä¼°å¥—ä»¶ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…¶ä¸­åœ¨38ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æ­¤å¤–ï¼Œåœ¨ä»£ç†ä¸­å¿ƒä»»åŠ¡ï¼ˆå¦‚GUIæ§åˆ¶å’Œæ¸¸æˆï¼‰å’Œå¤šæ¨¡æ€æ¨ç†æŒ‘æˆ˜ï¼ˆå¦‚è§†è§‰è°œé¢˜ï¼‰ä¸­è¡¨ç°ä¼˜ç§€ã€‚è¯¥æ¨¡å‹æœ‰åŠ©äºå„ç§ä»»åŠ¡çš„å¹¿æ³›åº”ç”¨ã€‚è®¿é—®åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://www.volcengine.com/">https://www.volcengine.com/</a>ï¼ˆç«å±±å¼•æ“æ¨¡å‹IDï¼šdoubao-1-5-thinking-vision-pro-250428ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Seed1.5-VLæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜é€šç”¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®ƒç»“åˆäº†è§†è§‰ç¼–ç å™¨å’Œæ··åˆä¸“å®¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰ç´§å‡‘çš„æ¶æ„å’Œé«˜æ•ˆçš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¹¿æ³›çš„å…¬å…±åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSeed1.5-VLè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>åœ¨ä»£ç†ä¸­å¿ƒä»»åŠ¡å’Œå¤šæ¨¡æ€æ¨ç†æŒ‘æˆ˜ä¸­ï¼ŒSeed1.5-VLå…·æœ‰å‡ºè‰²çš„è¡¨ç°ã€‚</li>
<li>è¯¥æ¨¡å‹é€‚ç”¨äºå„ç§ä»»åŠ¡åº”ç”¨ï¼Œå¹¶å¯é€šè¿‡è®¿é—®ç‰¹å®šç½‘ç«™è·å–è¯¦ç»†ä¿¡æ¯ã€‚</li>
<li>Seed1.5-VLçš„è®¾è®¡ã€æ•°æ®æ„å»ºå’Œè®­ç»ƒå„é˜¶æ®µéƒ½æœ‰è¯¦ç»†çš„ç»éªŒåˆ†äº«ï¼Œä»¥æ¿€å‘è¿›ä¸€æ­¥çš„ç ”ç©¶çµæ„Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07062">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0c0d85aa4e89643f0be4ff3e6a987125.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a42aa2face17b7403f28d5053d75350.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b80b97494cd7babc0cda73d0dabf57cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5e857ace28f9d11c6b6e2f0191d6ff6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03c9ab184909bcb00b6e4bedb05be47e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DialogueReason-Rule-Based-RL-Sparks-Dialogue-Reasoning-in-LLMs"><a href="#DialogueReason-Rule-Based-RL-Sparks-Dialogue-Reasoning-in-LLMs" class="headerlink" title="DialogueReason: Rule-Based RL Sparks Dialogue Reasoning in LLMs"></a>DialogueReason: Rule-Based RL Sparks Dialogue Reasoning in LLMs</h2><p><strong>Authors:Yubo Shu, Zhewei Huang, Xin Wu, Chen Hu, Shuchang Zhou, Daxin Jiang</strong></p>
<p>We propose DialogueReason, a reasoning paradigm that uncovers the lost roles in monologue-style reasoning models, aiming to boost diversity and coherency of the reasoning process. Recent advances in RL-based large reasoning models have led to impressive long CoT capabilities and high performance on math and science benchmarks. However, these reasoning models rely mainly on monologue-style reasoning, which often limits reasoning diversity and coherency, frequently recycling fixed strategies or exhibiting unnecessary shifts in attention. Our work consists of an analysis of monologue reasoning patterns and the development of a dialogue-based reasoning approach. We first introduce the Compound-QA task, which concatenates multiple problems into a single prompt to assess both diversity and coherency of reasoning. Our analysis shows that Compound-QA exposes weaknesses in monologue reasoning, evidenced by both quantitative metrics and qualitative reasoning traces. Building on the analysis, we propose a dialogue-based reasoning, named DialogueReason, structured around agents, environment, and interactions. Using PPO with rule-based rewards, we train open-source LLMs (Qwen-QWQ and Qwen-Base) to adopt dialogue reasoning. We evaluate trained models on MATH, AIME, and GPQA datasets, showing that the dialogue reasoning model outperforms monologue models under more complex compound questions. Additionally, we discuss how dialogue-based reasoning helps enhance interpretability, facilitate more intuitive human interaction, and inspire advances in multi-agent system design. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†DialogueReasonï¼Œè¿™æ˜¯ä¸€ç§æ­ç¤ºç‹¬ç™½å¼æ¨ç†æ¨¡å‹ä¸­ç¼ºå¤±è§’è‰²çš„æ¨ç†èŒƒå¼ï¼Œæ—¨åœ¨æé«˜æ¨ç†è¿‡ç¨‹çš„å¤šæ ·æ€§å’Œè¿è´¯æ€§ã€‚åŸºäºRLçš„å¤§å‹æ¨ç†æ¨¡å‹çš„æœ€æ–°è¿›å±•è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„é•¿æœŸæ¨ç†èƒ½åŠ›å’Œåœ¨æ•°å­¦ä¸ç§‘å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨ç†æ¨¡å‹ä¸»è¦ä¾èµ–äºç‹¬ç™½å¼æ¨ç†ï¼Œè¿™å¾€å¾€é™åˆ¶äº†æ¨ç†çš„å¤šæ ·æ€§å’Œè¿è´¯æ€§ï¼Œç»å¸¸é‡å¤ä½¿ç”¨å›ºå®šç­–ç•¥æˆ–è¡¨ç°å‡ºä¸å¿…è¦çš„æ³¨æ„åŠ›è½¬ç§»ã€‚æˆ‘ä»¬çš„å·¥ä½œåŒ…æ‹¬å¯¹ç‹¬ç™½æ¨ç†æ¨¡å¼çš„åˆ†æå’ŒåŸºäºå¯¹è¯çš„æ¨ç†æ–¹æ³•çš„å‘å±•ã€‚æˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†Compound-QAä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡å°†å¤šä¸ªé—®é¢˜è¿æ¥æˆä¸€ä¸ªå•ä¸€çš„æç¤ºï¼Œä»¥è¯„ä¼°æ¨ç†çš„å¤šæ ·æ€§å’Œè¿è´¯æ€§ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒCompound-QAæš´éœ²äº†ç‹¬ç™½æ¨ç†çš„å¼±ç‚¹ï¼Œè¿™ç”±å®šé‡æŒ‡æ ‡å’Œå®šæ€§æ¨ç†è½¨è¿¹è¯æ˜ã€‚åœ¨åˆ†æçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¯¹è¯çš„æ¨ç†æ–¹æ³•ï¼Œåä¸ºDialogueReasonï¼Œä»¥ä»£ç†ã€ç¯å¢ƒå’Œäº¤äº’ä¸ºä¸­å¿ƒæ„å»ºã€‚ä½¿ç”¨å¸¦æœ‰åŸºäºè§„åˆ™å¥–åŠ±çš„PPOç®—æ³•ï¼Œæˆ‘ä»¬è®­ç»ƒå¼€æºLLMï¼ˆQwen-QWQå’ŒQwen-Baseï¼‰é‡‡ç”¨å¯¹è¯å¼æ¨ç†ã€‚æˆ‘ä»¬åœ¨MATHã€AIMEå’ŒGPQAæ•°æ®é›†ä¸Šè¯„ä¼°äº†è®­ç»ƒæ¨¡å‹çš„è¡¨ç°ï¼Œç»“æœè¡¨æ˜å¯¹è¯å¼æ¨ç†æ¨¡å‹åœ¨æ›´å¤æ‚çš„é—®é¢˜ç»„åˆä¸Šä¼˜äºç‹¬ç™½æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†åŸºäºå¯¹è¯çš„æ¨ç†å¦‚ä½•å¸®åŠ©å¢å¼ºå¯è§£é‡Šæ€§ã€ä¿ƒè¿›æ›´ç›´è§‚çš„äººç±»äº’åŠ¨ï¼Œå¹¶æ¿€å‘å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡çš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07049v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†DialogueReasonè¿™ä¸€å¯¹è¯æ¨ç†èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³ç‹¬ç™½å¼æ¨ç†æ¨¡å‹åœ¨å¤šæ ·æ€§å’Œè¿è´¯æ€§æ–¹é¢çš„ä¸è¶³ã€‚æ–‡ç« åˆ†æäº†å½“å‰åŸºäºRLçš„å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤„ç†å¤åˆé—®é¢˜æ—¶å‡ºç°çš„å±€é™æ€§ï¼Œå¹¶ä»‹ç»äº†æ–°çš„å¯¹è¯å¼æ¨ç†æ–¹æ³•DialogueReasonã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å¤šä»£ç†äº¤äº’ï¼Œæé«˜äº†æ¨¡å‹çš„æ¨ç†å¤šæ ·æ€§å’Œè¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤æ‚å¤åˆé—®é¢˜ä¸Šï¼Œå¯¹è¯æ¨ç†æ¨¡å‹ä¼˜äºç‹¬ç™½æ¨¡å‹ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹è¿˜å¢å¼ºäº†å¯è§£é‡Šæ€§ï¼Œä¾¿äºäººç±»ç›´è§‚äº¤äº’ï¼Œå¹¶ä¸ºå¤šä»£ç†ç³»ç»Ÿè®¾è®¡æä¾›äº†å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DialogueReasonæ—¨åœ¨è§£å†³ç‹¬ç™½å¼æ¨ç†æ¨¡å‹åœ¨å¤šæ ·æ€§å’Œè¿è´¯æ€§æ–¹é¢çš„ä¸è¶³ã€‚</li>
<li>å¯¹è¯å¼æ¨ç†æ–¹æ³•é€šè¿‡å¼•å…¥å¤šä»£ç†äº¤äº’æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Compound-QAä»»åŠ¡ç”¨äºè¯„ä¼°æ¨¡å‹çš„å¤šæ ·æ€§å’Œè¿è´¯æ€§ã€‚</li>
<li>åŸºäºPPOå’Œè§„åˆ™å¥–åŠ±è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Qwen-QWQå’ŒQwen-Baseï¼‰èƒ½å¤Ÿé‡‡ç”¨å¯¹è¯å¼æ¨ç†ã€‚</li>
<li>åœ¨å¤æ‚å¤åˆé—®é¢˜ä¸Šï¼Œå¯¹è¯æ¨ç†æ¨¡å‹ä¼˜äºç‹¬ç™½æ¨¡å‹ã€‚</li>
<li>å¯¹è¯å¼æ¨ç†æ¨¡å‹å¢å¼ºäº†æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œä¾¿äºäººç±»ç›´è§‚äº¤äº’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed26ad46e20df6bd3875b1904dcc246a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb0bf3f6b220a358f51ecb0b91d2c46b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48d53d214c1185f2f91f97619e543d72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4cb594ac97adbee73009c871f959fa02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fb7999910978888055a44af19cf0d3c.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="From-Knowledge-to-Reasoning-Evaluating-LLMs-for-Ionic-Liquids-Research-in-Chemical-and-Biological-Engineering"><a href="#From-Knowledge-to-Reasoning-Evaluating-LLMs-for-Ionic-Liquids-Research-in-Chemical-and-Biological-Engineering" class="headerlink" title="From Knowledge to Reasoning: Evaluating LLMs for Ionic Liquids Research   in Chemical and Biological Engineering"></a>From Knowledge to Reasoning: Evaluating LLMs for Ionic Liquids Research   in Chemical and Biological Engineering</h2><p><strong>Authors:Gaurab Sarkar, Sougata Saha</strong></p>
<p>Although Large Language Models (LLMs) have achieved remarkable performance in diverse general knowledge and reasoning tasks, their utility in the scientific domain of Chemical and Biological Engineering (CBE) is unclear. Hence, it necessitates challenging evaluation benchmarks that can measure LLM performance in knowledge- and reasoning-based tasks, which is lacking. As a foundational step, we empirically measure the reasoning capabilities of LLMs in CBE. We construct and share an expert-curated dataset of 5,920 examples for benchmarking LLMsâ€™ reasoning capabilities in the niche domain of Ionic Liquids (ILs) for carbon sequestration, an emergent solution to reducing global warming. The dataset presents different difficulty levels by varying along the dimensions of linguistic and domain-specific knowledge. Benchmarking three less than 10B parameter open-source LLMs on the dataset suggests that while smaller general-purpose LLMs are knowledgeable about ILs, they lack domain-specific reasoning capabilities. Based on our results, we further discuss considerations for leveraging LLMs for carbon capture research using ILs. Since LLMs have a high carbon footprint, gearing them for IL research can symbiotically benefit both fields and help reach the ambitious carbon neutrality target by 2050. Dataset link: <a target="_blank" rel="noopener" href="https://github.com/sougata-ub/llms_for_ionic_liquids">https://github.com/sougata-ub/llms_for_ionic_liquids</a> </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ ·åŒ–å’Œé€šç”¨çš„çŸ¥è¯†å’Œæ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œå®ƒä»¬åœ¨åŒ–å­¦å’Œç”Ÿç‰©å·¥ç¨‹ï¼ˆCBEï¼‰ç§‘å­¦é¢†åŸŸçš„åº”ç”¨å°šä¸æ¸…æ¥šã€‚å› æ­¤ï¼Œéœ€è¦å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯„ä¼°åŸºå‡†ï¼Œèƒ½å¤Ÿè¡¡é‡LLMåœ¨åŸºäºçŸ¥è¯†å’Œæ¨ç†çš„ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè€Œè¿™ä¸€ç‚¹æ­£æ˜¯æˆ‘ä»¬æ‰€ç¼ºä¹çš„ã€‚ä½œä¸ºåŸºç¡€æ­¥éª¤ï¼Œæˆ‘ä»¬é€šè¿‡å®è¯ç ”ç©¶æµ‹é‡LLMåœ¨CBEä¸­çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºå¹¶å…±äº«äº†ä¸€ä¸ªåŒ…å«5920ä¸ªæ ·æœ¬çš„ä¸“å®¶çº§æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°ç¦»å­æ¶²ä½“ï¼ˆILsï¼‰è¿™ä¸€æ–°å…´ç¢³æ•è·è§£å†³æ–¹æ¡ˆé¢†åŸŸä¸­çš„LLMæ¨ç†èƒ½åŠ›ã€‚æ•°æ®é›†é€šè¿‡è¯­è¨€å’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†çš„ç»´åº¦å±•ç°ä¸åŒçš„éš¾åº¦çº§åˆ«ã€‚åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°ä¸‰ä¸ªå°‘äº10Bå‚æ•°çš„å¼€æºLLMï¼Œç»“æœè¡¨æ˜ï¼Œè™½ç„¶è¾ƒå°çš„é€šç”¨LLMå¯¹ç¦»å­æ¶²ä½“æœ‰ä¸€å®šçš„äº†è§£ï¼Œä½†å®ƒä»¬ç¼ºä¹ç‰¹å®šé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚åŸºäºæˆ‘ä»¬çš„ç ”ç©¶ç»“æœï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¨è®ºäº†åˆ©ç”¨ç¦»å­æ¶²ä½“è¿›è¡Œç¢³æ•è·ç ”ç©¶ä¸­åˆ©ç”¨LLMçš„æ³¨æ„äº‹é¡¹ã€‚ç”±äºLLMçš„ç¢³è¶³è¿¹è¾ƒé«˜ï¼Œå°†å…¶ç”¨äºç¦»å­æ¶²ä½“ç ”ç©¶å¯ä»¥äº’åˆ©å…±èµ¢ï¼Œæœ‰åŠ©äºå®ç°åˆ°2050å¹´å®ç°é›„å¿ƒå‹ƒå‹ƒçš„ç¢³ä¸­å’Œç›®æ ‡ã€‚æ•°æ®é›†é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/sougata-ub/llms_for_ionic_liquids">https://github.com/sougata-ub/llms_for_ionic_liquids</a></p>
</blockquote>
<p><strong>Translation into Simplified Chinese</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06964v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ–å­¦ä¸ç”Ÿç‰©å·¥ç¨‹ï¼ˆCBEï¼‰é¢†åŸŸçš„å®ç”¨æ€§å°šä¸æ¸…æ¥šï¼Œå› æ­¤éœ€è¦å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯„ä¼°åŸºå‡†æ¥è¡¡é‡å…¶åœ¨çŸ¥è¯†ä¸æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å¯¹LLMsåœ¨ç¦»å­æ¶²ä½“ï¼ˆILsï¼‰é¢†åŸŸçš„æ¨ç†èƒ½åŠ›è¿›è¡Œäº†å®è¯æµ‹é‡ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªä¸“å®¶å®¡æ ¸çš„æ•°æ®åº“ç”¨äºè¯„ä¼°LLMsåœ¨ç¢³æ•è·é¢†åŸŸçš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå°å‹é€šç”¨LLMsè™½ç„¶äº†è§£ç¦»å­æ¶²ä½“ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸæ¨ç†èƒ½åŠ›ä¸Šæœ‰æ‰€æ¬ ç¼ºã€‚è€ƒè™‘åˆ°LLMsçš„é«˜ç¢³æ’æ”¾å’Œå¯¹ç¦»å­æ¶²ä½“ç ”ç©¶çš„åº”ç”¨æ½œåŠ›ï¼Œéœ€è¦æ›´ç²¾ç»†åœ°åˆ©ç”¨å®ƒä»¬æ¥æ¨åŠ¨ç¢³æ•è·ç ”ç©¶ï¼ŒåŠ©åŠ›å®ç°ç¢³ä¸­å’Œç›®æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ–å­¦ä¸ç”Ÿç‰©å·¥ç¨‹ï¼ˆCBEï¼‰é¢†åŸŸçš„å®ç”¨æ€§å°šå¾…æ˜ç¡®ã€‚</li>
<li>éœ€è¦å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯„ä¼°åŸºå‡†æ¥è¡¡é‡LLMsåœ¨çŸ¥è¯†ä¸æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
<li>ç ”ç©¶å®è¯æµ‹é‡äº†LLMsåœ¨ç¦»å­æ¶²ä½“ï¼ˆILsï¼‰é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªä¸“å®¶å®¡æ ¸çš„æ•°æ®åº“ç”¨äºè¯„ä¼°LLMsåœ¨ç¢³æ•è·é¢†åŸŸçš„è¡¨ç°ã€‚</li>
<li>å°å‹é€šç”¨LLMsäº†è§£ç¦»å­æ¶²ä½“ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸæ¨ç†èƒ½åŠ›ä¸Šæœ‰æ‰€æ¬ ç¼ºã€‚</li>
<li>LLMsåœ¨ç¢³æ•è·ç ”ç©¶ä¸­æœ‰æ½œåŠ›ï¼Œä½†éœ€è¦æ›´ç²¾ç»†çš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06964">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d7fc98a31cd7f10dd7d0377560db5e23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aba506d203268d71904e82bc2e57fc77.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d4b242baa415a9690cf374c834e25e7.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="LineFlow-A-Framework-to-Learn-Active-Control-of-Production-Lines"><a href="#LineFlow-A-Framework-to-Learn-Active-Control-of-Production-Lines" class="headerlink" title="LineFlow: A Framework to Learn Active Control of Production Lines"></a>LineFlow: A Framework to Learn Active Control of Production Lines</h2><p><strong>Authors:Kai MÃ¼ller, Martin Wenzel, Tobias Windisch</strong></p>
<p>Many production lines require active control mechanisms, such as adaptive routing, worker reallocation, and rescheduling, to maintain optimal performance. However, designing these control systems is challenging for various reasons, and while reinforcement learning (RL) has shown promise in addressing these challenges, a standardized and general framework is still lacking. In this work, we introduce LineFlow, an extensible, open-source Python framework for simulating production lines of arbitrary complexity and training RL agents to control them. To demonstrate the capabilities and to validate the underlying theoretical assumptions of LineFlow, we formulate core subproblems of active line control in ways that facilitate mathematical analysis. For each problem, we provide optimal solutions for comparison. We benchmark state-of-the-art RL algorithms and show that the learned policies approach optimal performance in well-understood scenarios. However, for more complex, industrial-scale production lines, RL still faces significant challenges, highlighting the need for further research in areas such as reward shaping, curriculum learning, and hierarchical control. </p>
<blockquote>
<p>è®¸å¤šç”Ÿäº§çº¿éœ€è¦ä¸»åŠ¨æ§åˆ¶æœºåˆ¶ï¼Œå¦‚è‡ªé€‚åº”è·¯ç”±ã€å·¥äººé‡æ–°åˆ†é…å’Œé‡æ–°è°ƒåº¦ï¼Œä»¥ç»´æŒæœ€ä½³æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºå„ç§åŸå› ï¼Œè®¾è®¡è¿™äº›æ§åˆ¶ç³»ç»Ÿå…·æœ‰æŒ‘æˆ˜æ€§ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨åº”å¯¹è¿™äº›æŒ‘æˆ˜æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ä»ç¼ºä¹æ ‡å‡†åŒ–å’Œé€šç”¨çš„æ¡†æ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LineFlowï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å¼€æºPythonæ¡†æ¶ï¼Œç”¨äºæ¨¡æ‹Ÿä»»æ„å¤æ‚åº¦çš„ç”Ÿäº§çº¿å¹¶è®­ç»ƒRLä»£ç†æ¥æ§åˆ¶å®ƒä»¬ã€‚ä¸ºäº†å±•ç¤ºLineFlowçš„èƒ½åŠ›å¹¶éªŒè¯å…¶åŸºç¡€ç†è®ºå‡è®¾ï¼Œæˆ‘ä»¬ä»¥æœ‰åˆ©äºæ•°å­¦åˆ†æçš„æ–¹å¼ä¸»åŠ¨åˆ¶å®šäº†ç”Ÿäº§çº¿æ§åˆ¶çš„æ ¸å¿ƒå­é—®é¢˜ã€‚å¯¹äºæ¯ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éƒ½æä¾›äº†æœ€ä½³è§£å†³æ–¹æ¡ˆè¿›è¡Œå¯¹æ¯”ã€‚æˆ‘ä»¬å¯¹æœ€æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶è¯æ˜åœ¨æ˜ç¡®ç†è§£çš„åœºæ™¯ä¸­ï¼Œå­¦ä¹ åˆ°çš„ç­–ç•¥æ¥è¿‘æœ€ä½³æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºæ›´å¤æ‚ã€å·¥ä¸šè§„æ¨¡çš„ç”Ÿäº§çº¿ï¼ŒRLä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œè¿™çªæ˜¾äº†éœ€è¦åœ¨å¥–åŠ±å¡‘é€ ã€è¯¾ç¨‹å­¦ä¹ å’Œåˆ†å±‚æ§åˆ¶ç­‰é¢†åŸŸè¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06744v1">PDF</a> Accepted at ICML 2025</p>
<p><strong>Summary</strong><br>è‡ªåŠ¨åŒ–ç”Ÿäº§çº¿éœ€è¦ä¸»åŠ¨æ§åˆ¶æœºåˆ¶æ¥ç»´æŒæœ€ä½³æ€§èƒ½ï¼Œå¦‚è‡ªé€‚åº”è·¯ç”±ã€å·¥äººé‡æ–°åˆ†é…å’Œé‡æ–°è°ƒåº¦ç­‰ã€‚åœ¨è®¾è®¡è¿™äº›æ§åˆ¶ç³»ç»Ÿæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œè™½ç„¶å¼ºåŒ–å­¦ä¹ å·²å±•ç°å‡ºè§£å†³è¿™äº›æŒ‘æˆ˜çš„å¸Œæœ›ï¼Œä½†ä»ç¼ºä¹æ ‡å‡†åŒ–å’Œé€šç”¨çš„æ¡†æ¶ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºLineFlowï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å¼€æºPythonæ¡†æ¶ï¼Œå¯æ¨¡æ‹Ÿä»»æ„å¤æ‚åº¦çš„ç”Ÿäº§çº¿å¹¶è®­ç»ƒRLä»£ç†æ¥æ§åˆ¶å®ƒä»¬ã€‚ä¸ºè¯æ˜LineFlowçš„èƒ½åŠ›å’ŒéªŒè¯å…¶ç†è®ºå‡è®¾ï¼Œæˆ‘ä»¬ä»¥ä¾¿äºæ•°å­¦åˆ†æçš„æ–¹å¼åˆ¶å®šäº†ä¸»åŠ¨çº¿è·¯æ§åˆ¶çš„æ ¸å¿ƒå­é—®é¢˜ï¼Œå¹¶é’ˆå¯¹æ¯ä¸ªé—®é¢˜æä¾›äº†æœ€ä¼˜è§£è¿›è¡Œå¯¹æ¯”ã€‚æˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„RLç®—æ³•ï¼Œå¹¶å±•ç¤ºäº†åœ¨å·²çŸ¥åœºæ™¯ä¸­ï¼Œå­¦ä¹ åˆ°çš„ç­–ç•¥æ¥è¿‘æœ€ä¼˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºæ›´å¤æ‚ã€å·¥ä¸šè§„æ¨¡çš„ç”Ÿäº§çº¿ï¼ŒRLä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†å¯¹å¥–åŠ±å¡‘é€ ã€è¯¾ç¨‹å­¦ä¹ å’Œåˆ†å±‚æ§åˆ¶ç­‰é¢†åŸŸè¿›ä¸€æ­¥ç ”ç©¶çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿäº§çº¿çš„ä¸»åŠ¨æ§åˆ¶æœºåˆ¶å¯¹äºç»´æŒæœ€ä½³æ€§èƒ½è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬è‡ªé€‚åº”è·¯ç”±ã€å·¥äººé‡æ–°åˆ†é…å’Œé‡æ–°è°ƒåº¦ç­‰ã€‚</li>
<li>è®¾è®¡ç”Ÿäº§æ§åˆ¶ç³»ç»Ÿä¸­å­˜åœ¨æŒ‘æˆ˜ï¼Œç¼ºä¹æ ‡å‡†åŒ–å’Œé€šç”¨çš„æ¡†æ¶æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>LineFlowæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å¼€æºPythonæ¡†æ¶ï¼Œç”¨äºæ¨¡æ‹Ÿä»»æ„å¤æ‚åº¦çš„ç”Ÿäº§çº¿å¹¶è®­ç»ƒRLä»£ç†è¿›è¡Œæ§åˆ¶ã€‚</li>
<li>LineFlowé€šè¿‡åˆ¶å®šæ ¸å¿ƒå­é—®é¢˜å¹¶ä¸ºå…¶æä¾›æœ€ä¼˜è§£è¿›è¡Œå¯¹æ¯”æ¥éªŒè¯å…¶èƒ½åŠ›å’Œç†è®ºå‡è®¾ã€‚</li>
<li>æœ€å…ˆè¿›çš„RLç®—æ³•å·²è¢«è¯„ä¼°ï¼Œå¹¶åœ¨æŸäº›åœºæ™¯ä¸­è¡¨ç°å‡ºæ¥è¿‘æœ€ä¼˜æ€§èƒ½çš„è¡¨ç°ã€‚</li>
<li>åœ¨æ›´å¤æ‚ã€å·¥ä¸šè§„æ¨¡çš„ç”Ÿäº§çº¿åº”ç”¨ä¸­ï¼ŒRLä»é¢ä¸´å¥–åŠ±å¡‘é€ ã€è¯¾ç¨‹å­¦ä¹ å’Œåˆ†å±‚æ§åˆ¶ç­‰å…³é”®é¢†åŸŸçš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06744">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f538d1a77df4161139f195ef85ee551.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3354662d96674ab9bd2d5f0fc9e24ef5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-96e706439efab507bbea3c0a18d8cf9d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c72d1a11e2c7c18beeb6957dd128778.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="From-Rankings-to-Insights-Evaluation-Should-Shift-Focus-from-Leaderboard-to-Feedback"><a href="#From-Rankings-to-Insights-Evaluation-Should-Shift-Focus-from-Leaderboard-to-Feedback" class="headerlink" title="From Rankings to Insights: Evaluation Should Shift Focus from   Leaderboard to Feedback"></a>From Rankings to Insights: Evaluation Should Shift Focus from   Leaderboard to Feedback</h2><p><strong>Authors:Zongqi Wang, Tianle Gu, Chen Gong, Xin Tian, Siqi Bao, Yujiu Yang</strong></p>
<p>Automatic evaluation benchmarks such as MT-Bench, Arena-Hard, and Auto-Arena are seeing growing adoption for the evaluation of Large Language Models (LLMs). Existing research has primarily focused on approximating human-based model rankings using limited data and LLM-as-a-Judge. However, the fundamental premise of these studies, which attempts to replicate human rankings, is flawed. Specifically, these benchmarks typically offer only overall scores, limiting their utility to leaderboard rankings, rather than providing feedback that can guide model optimization and support model profiling. Therefore, we advocate for an evaluation paradigm shift from approximating human-based model rankings to providing feedback with analytical value. To this end, we introduce Feedbacker, an evaluation framework that provides comprehensive and fine-grained results, thereby enabling thorough identification of a modelâ€™s specific strengths and weaknesses. Such feedback not only supports the targeted optimization of the model but also enhances the understanding of its behavior. Feedbacker comprises three key components: an extensible tree-based query taxonomy builder, an automated query synthesis scheme, and a suite of visualization and analysis tools. Furthermore, we propose a novel LLM-as-a-Judge method: PC2 (Pre-Comparison-derived Criteria) pointwise evaluation. This method derives evaluation criteria by pre-comparing the differences between several auxiliary responses, achieving the accuracy of pairwise evaluation while maintaining the time complexity of pointwise evaluation. Finally, leveraging the evaluation results of 17 mainstream LLMs, we demonstrate the usage of Feedbacker and highlight its effectiveness and potential. Our homepage project is available at <a target="_blank" rel="noopener" href="https://liudan193.github.io/Feedbacker">https://liudan193.github.io/Feedbacker</a>. </p>
<blockquote>
<p>è‡ªåŠ¨è¯„ä¼°åŸºå‡†ï¼ˆå¦‚MT-Benchã€Arena-Hardå’ŒAuto-Arenaï¼‰åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢æ­£è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚ç›®å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ©ç”¨æœ‰é™æ•°æ®æ¨¡æ‹ŸåŸºäºäººç±»çš„æ¨¡å‹æ’åä»¥åŠä»¥LLMä½œä¸ºæ³•å®˜çš„æ–¹æ³•ä¸Šã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶è¯•å›¾å¤åˆ¶äººç±»æ’åçš„åŸºæœ¬å‰ææ˜¯æœ‰ç¼ºé™·çš„ã€‚å…·ä½“æ¥è¯´ï¼Œè¿™äº›åŸºå‡†é€šå¸¸åªæä¾›æ€»ä½“å¾—åˆ†ï¼Œä½¿å…¶ä»…é™äºæ’è¡Œæ¦œæ’åï¼Œè€Œæ— æ³•æä¾›å¯ä»¥æŒ‡å¯¼æ¨¡å‹ä¼˜åŒ–å’Œæ¨¡å‹åˆ†æçš„æœ‰ä»·å€¼åé¦ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸»å¼ ä»æ¨¡æ‹ŸåŸºäºäººç±»çš„æ¨¡å‹æ’åè½¬å‘æä¾›å…·æœ‰åˆ†æä»·å€¼çš„åé¦ˆæ„è§çš„è¯„ä»·èŒƒå¼è½¬å˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†Feedbackerè¯„ä¼°æ¡†æ¶ï¼Œå®ƒæä¾›å…¨é¢è€Œç²¾ç»†çš„ç»“æœï¼Œä»è€Œèƒ½å¤Ÿå½»åº•è¯†åˆ«æ¨¡å‹çš„ç‰¹å®šä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚è¿™ç§åé¦ˆä¸ä»…æ”¯æŒæ¨¡å‹çš„é’ˆå¯¹æ€§ä¼˜åŒ–ï¼Œè¿˜æé«˜äº†å¯¹æ¨¡å‹è¡Œä¸ºçš„ç†è§£ã€‚FeedbackeråŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå¯æ‰©å±•çš„æ ‘å½¢æŸ¥è¯¢åˆ†ç±»æ„å»ºå™¨ã€è‡ªåŠ¨åŒ–æŸ¥è¯¢åˆæˆæ–¹æ¡ˆä»¥åŠä¸€å¥—å¯è§†åŒ–å·¥å…·å’Œæ•°æ®åˆ†æå·¥å…·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä»¥LLMä½œä¸ºæ³•å®˜çš„æ–¹æ³•ï¼šPC2ï¼ˆåŸºäºé¢„æ¯”è¾ƒæ ‡å‡†ï¼‰çš„ç‚¹çº§è¯„ä¼°ã€‚è¿™ç§æ–¹æ³•é€šè¿‡é¢„å…ˆæ¯”è¾ƒå¤šä¸ªè¾…åŠ©å“åº”ä¹‹é—´çš„å·®å¼‚æ¥æ¨å¯¼è¯„ä¼°æ ‡å‡†ï¼Œå®ç°äº†é…å¯¹è¯„ä¼°çš„å‡†ç¡®æ€§åŒæ—¶ä¿æŒç‚¹çº§è¯„ä¼°çš„æ—¶é—´å¤æ‚åº¦ã€‚æœ€åï¼Œé€šè¿‡å¯¹17ç§ä¸»æµLLMçš„è¯„ä¼°ç»“æœåŠ ä»¥åˆ©ç”¨ï¼Œæˆ‘ä»¬å±•ç¤ºäº†Feedbackerçš„ä½¿ç”¨æƒ…å†µå¹¶å¼ºè°ƒäº†å…¶æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚æˆ‘ä»¬çš„ä¸»é¡µé¡¹ç›®å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://liudan193.github.io/Feedbacker%E8%AE%BF%E9%97%AE%E3%80%82">https://liudan193.github.io/Feedbackerè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06698v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¿™æ˜¯ä¸€é¡¹å…³äºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°çš„ç ”ç©¶ã€‚ç ”ç©¶å›¢é˜ŸæŒ‡å‡ºäº†å½“å‰è‡ªåŠ¨è¯„ä¼°åŸºå‡†ï¼ˆå¦‚MT-Benchã€Arena-Hardå’ŒAuto-Arenaï¼‰å­˜åœ¨çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåä¸ºFeedbackerçš„æ–°è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨æä¾›å…·æœ‰åˆ†æä»·å€¼çš„åé¦ˆï¼ŒåŒ…æ‹¬ä¸€ä¸ªå¯æ‰©å±•çš„æ ‘å½¢æŸ¥è¯¢åˆ†ç±»æ„å»ºå™¨ã€è‡ªåŠ¨åŒ–æŸ¥è¯¢åˆæˆæ–¹æ¡ˆå’Œå¯è§†åŒ–åˆ†æå·¥å…·å¥—ä»¶ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§æ–°å‹çš„LLM-as-a-Judgeæ–¹æ³•ï¼šPC2ï¼ˆåŸºäºé¢„æ¯”è¾ƒæ ‡å‡†é€ç‚¹è¯„ä¼°ï¼‰ï¼Œå¯æ›´æœ‰æ•ˆåœ°è¿›è¡Œè¯­è¨€æ¨¡å‹è¯„ä¼°ã€‚è¯¥é¡¹ç›®å·²ç»å¯¹æ‰€è·å¾—çš„è¯„ä¼°ç»“æœè¿›è¡Œåˆæ­¥ä½¿ç”¨å±•ç¤ºå…¶æ•ˆæœå’Œæ½œåŠ›ï¼Œè¯¦æƒ…å¯åœ¨<a target="_blank" rel="noopener" href="https://liudan193.github.io/Feedbacker%E4%B8%AD%E6%9F%A5%E9%98%85%E3%80%82%E8%BF%99%E9%A1%B9%E7%A0%94%E7%A9%B6%E7%9A%84%E9%87%8D%E7%82%B9%E6%98%AF%E6%8F%90%E4%BE%9B%E6%9B%B4%E5%87%86%E7%A1%AE%E7%9A%84%E8%AF%84%E4%BC%B0%E7%BB%93%E6%9E%9C%E4%BB%A5%E4%BC%98%E5%8C%96%E5%92%8C%E6%94%B9%E8%BF%9B%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E3%80%82">https://liudan193.github.io/Feedbackerä¸­æŸ¥é˜…ã€‚è¿™é¡¹ç ”ç©¶çš„é‡ç‚¹æ˜¯æä¾›æ›´å‡†ç¡®çš„è¯„ä¼°ç»“æœä»¥ä¼˜åŒ–å’Œæ”¹è¿›æ¨¡å‹æ€§èƒ½ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºè¯¥æ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>å½“å‰è‡ªåŠ¨è¯„ä¼°åŸºå‡†ä¸»è¦å…³æ³¨äºä½¿ç”¨æœ‰é™æ•°æ®è¿‘ä¼¼äººç±»æ¨¡å‹æ’åï¼Œä½†å­˜åœ¨ç¼ºé™·ã€‚è¿™äº›åŸºå‡†å¾€å¾€ä»…æä¾›æ€»ä½“å¾—åˆ†ï¼Œä»…é™äºæ’åæ¦œå•çš„å±•ç¤ºã€‚ä¸ºæ­¤éœ€è¦è¯„ä»·èŒƒå¼ä»è¿‘ä¼¼äººç±»æ¨¡å‹æ’åè½¬å‘æä¾›å…·æœ‰åˆ†æä»·å€¼çš„åé¦ˆã€‚</li>
<li>Feedbackeræ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æä¾›å…¨é¢ä¸”ç²¾ç»†çš„ç»“æœï¼Œæœ‰åŠ©äºè¯†åˆ«æ¨¡å‹çš„ç‰¹å®šä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚è¿™ä¸ä»…èƒ½å¤Ÿæ”¯æŒæ¨¡å‹çš„é’ˆå¯¹æ€§ä¼˜åŒ–ï¼Œè¿˜èƒ½å¢å¼ºå¯¹æ¨¡å‹è¡Œä¸ºçš„ç†è§£ã€‚</li>
<li>Feedbackeræ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå¯æ‰©å±•çš„æ ‘å½¢æŸ¥è¯¢åˆ†ç±»æ„å»ºå™¨ã€è‡ªåŠ¨åŒ–æŸ¥è¯¢åˆæˆæ–¹æ¡ˆå’Œå¯è§†åŒ–åˆ†æå·¥å…·å¥—ä»¶ã€‚æ­¤å¤–è¿˜æå‡ºäº†ä¸€ç§æ–°å‹çš„LLMè¯„ä¼°æ–¹æ³•ï¼šPC2ç‚¹å¯¹ç‚¹è¯„ä»·æ³•ã€‚æ­¤æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨é¢„å…ˆæ¯”è¾ƒçš„å·®åˆ«ç”Ÿæˆæ–°çš„è¯„ä»·å‡†åˆ™ï¼Œå¹¶åœ¨ç»´æŒç‚¹å¯¹ç‚¹è¯„ä»·çš„æ—¶é—´å¤æ‚åº¦çš„åŒæ—¶æé«˜é…å¯¹è¯„ä»·çš„å‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e0070c53676aa357830c443385ca267.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94943f15699e490df7a4ce3a0a51d6ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18c87c0687e277dbbc33eb97e5ec28e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a30d5605278f6127abf72cedd6e6fac6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cb67005b2f6bae16707d658a5fc7d3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34b0bdcea481ce7097d3e0dc4087ccdb.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-70239aae6790064485258e7c927cd93f.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  Learning Dynamics in Continual Pre-Training for Large Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-11/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-89a1f849a29e37ab509edc0e1ef09684.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-11  Communicating Activations Between Language Model Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23901.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
