<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  Learning Dynamics in Continual Pre-Training for Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-70239aae6790064485258e7c927cd93f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-14-æ›´æ–°"><a href="#2025-05-14-æ›´æ–°" class="headerlink" title="2025-05-14 æ›´æ–°"></a>2025-05-14 æ›´æ–°</h1><h2 id="Learning-Dynamics-in-Continual-Pre-Training-for-Large-Language-Models"><a href="#Learning-Dynamics-in-Continual-Pre-Training-for-Large-Language-Models" class="headerlink" title="Learning Dynamics in Continual Pre-Training for Large Language Models"></a>Learning Dynamics in Continual Pre-Training for Large Language Models</h2><p><strong>Authors:Xingjin Wang, Howe Tissue, Lu Wang, Linjing Li, Daniel Dajun Zeng</strong></p>
<p>Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters. </p>
<blockquote>
<p>æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰å·²æˆä¸ºå°†å¼ºå¤§çš„åŸºç¡€æ¨¡å‹åº”ç”¨äºç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„ä¸€ç§æµè¡Œä¸”æœ‰æ•ˆçš„æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„CPTè¿‡ç¨‹ä¸­çš„å­¦ä¹ åŠ¨æ€ã€‚æˆ‘ä»¬ç‰¹åˆ«å…³æ³¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­é€šç”¨é¢†åŸŸå’Œä¸‹æ¸¸é¢†åŸŸçš„æ€§èƒ½å¦‚ä½•å‘å±•ï¼Œå¹¶é€šè¿‡éªŒè¯æŸå¤±æ¥è¡¡é‡é¢†åŸŸæ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°CPTæŸå¤±æ›²çº¿æ ¹æœ¬ä¸Šæ˜¯æè¿°ä»ä¸€ä¸ªæ›²çº¿åˆ°å¦ä¸€ä¸ªéšè—æ›²çº¿çš„è½¬å˜ï¼Œå¯ä»¥é€šè¿‡è§£è€¦åˆ†å¸ƒç§»ä½å’Œå­¦ä¹ ç‡é€€ç«çš„å½±å“æ¥æè¿°ã€‚æˆ‘ä»¬æ¨å¯¼å‡ºäº†ä¸€ä¸ªç»“åˆè¿™ä¸¤ä¸ªå› ç´ çš„CPTå®šæ ‡å¾‹ï¼Œèƒ½å¤Ÿé¢„æµ‹ä»»ä½•ï¼ˆæŒç»­ï¼‰è®­ç»ƒæ­¥éª¤çš„æŸå¤±ä»¥åŠåœ¨CPTä¸­çš„å­¦ä¹ ç‡è®¡åˆ’ï¼ˆLRSï¼‰ã€‚æˆ‘ä»¬çš„å…¬å¼å…¨é¢åœ°ç†è§£äº†å‡ ç§CPTä¸­çš„å…³é”®å› ç´ ï¼ŒåŒ…æ‹¬æ½œåœ¨æŸå¤±ã€å³°å€¼å­¦ä¹ ç‡ã€è®­ç»ƒæ­¥éª¤ã€å›æ”¾æ¯”ç‡ç­‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ ¹æ®ä¸åŒçš„CPTç›®æ ‡è°ƒæ•´è®­ç»ƒè¶…å‚æ•°ï¼Œå¦‚å¹³è¡¡é€šç”¨å’Œç‰¹å®šé¢†åŸŸçš„æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å®šæ ‡å¾‹é€‚ç”¨äºå„ç§CPTæ•°æ®é›†å’ŒåŸ¹è®­è¶…å‚æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07796v1">PDF</a> Accepted to ICML2025 (spotlight)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰æ–¹æ³•æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œæœ¬ç ”ç©¶æ¢ç´¢äº†CPTè¿‡ç¨‹ä¸­çš„å­¦ä¹ åŠ¨æ€ã€‚ç ”ç©¶é‡ç‚¹åœ¨äºé€šç”¨å’Œä¸‹æ¸¸é¢†åŸŸçš„æ€§èƒ½å¦‚ä½•éšè®­ç»ƒæ­¥éª¤è€Œæ¼”å˜ï¼Œé€šè¿‡éªŒè¯æŸå¤±æ¥è¡¡é‡é¢†åŸŸæ€§èƒ½ã€‚è§‚å¯Ÿåˆ°CPTæŸå¤±æ›²çº¿æœ¬è´¨ä¸Šåæ˜ äº†ä¸€ç§æ›²çº¿åˆ°å¦ä¸€ç§éšè—æ›²çº¿çš„è½¬å˜ï¼Œå¯é€šè¿‡åˆ†ç¦»åˆ†å¸ƒå˜åŒ–å’Œè¡°å‡å­¦ä¹ é€Ÿç‡çš„å½±å“æ¥æè¿°ã€‚æœ¬ç ”ç©¶æ¨å¯¼å‡ºç»“åˆè¿™ä¸¤ä¸ªå› ç´ çš„CPTç¼©æ”¾å®šå¾‹ï¼Œå¯é¢„æµ‹ä»»ä½•è¿ç»­è®­ç»ƒæ­¥éª¤å’ŒCPTä¸­çš„å­¦ä¹ é€Ÿç‡è®¡åˆ’ï¼ˆLRSï¼‰çš„æŸå¤±ã€‚è¯¥å…¬å¼å…¨é¢ç†è§£äº†CPTä¸­çš„å‡ ä¸ªå…³é”®å› ç´ ï¼ŒåŒ…æ‹¬æ½œåœ¨æŸå¤±ã€å³°å€¼å­¦ä¹ ç‡ã€è®­ç»ƒæ­¥éª¤ã€å›æ”¾æ¯”ç‡ç­‰ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯é€‚åº”ä¸åŒçš„CPTç›®æ ‡ï¼Œè°ƒæ•´è®­ç»ƒè¶…å‚æ•°ä»¥å¹³è¡¡é€šç”¨å’Œé¢†åŸŸç‰¹å®šæ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥ç¼©æ”¾å®šå¾‹åœ¨å¤šç§CPTæ•°æ®é›†å’ŒåŸ¹è®­è¶…å‚æ•°ä¸­å‡æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰æ˜¯åº”ç”¨äºç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ç ”ç©¶é‡ç‚¹åŒ…æ‹¬CPTè¿‡ç¨‹ä¸­çš„å­¦ä¹ åŠ¨æ€ï¼Œç‰¹åˆ«æ˜¯é€šç”¨å’Œä¸‹æ¸¸é¢†åŸŸæ€§èƒ½çš„æ¼”å˜ã€‚</li>
<li>CPTæŸå¤±æ›²çº¿åæ˜ äº†ä»ä¸€ç§æ›²çº¿åˆ°å¦ä¸€ç§éšè—æ›²çº¿çš„è½¬å˜ã€‚</li>
<li>é€šè¿‡åˆ†ç¦»åˆ†å¸ƒå˜åŒ–å’Œè¡°å‡å­¦ä¹ é€Ÿç‡çš„å½±å“æ¥æè¿°è¿™ä¸€è½¬å˜ã€‚</li>
<li>æ¨å¯¼å‡ºçš„CPTç¼©æ”¾å®šå¾‹å¯é¢„æµ‹ä»»ä½•è®­ç»ƒæ­¥éª¤å’ŒCPTä¸­çš„å­¦ä¹ é€Ÿç‡è®¡åˆ’çš„æŸå¤±ã€‚</li>
<li>è¯¥å…¬å¼æä¾›äº†å¯¹CPTä¸­å…³é”®å› ç´ çš„å…¨é¢ç†è§£ï¼ŒåŒ…æ‹¬æ½œåœ¨æŸå¤±ã€å³°å€¼å­¦ä¹ ç‡ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07796">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-04f5acca528b4c1c1f6cc135a2f4bba4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70239aae6790064485258e7c927cd93f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-398c3a8f742bcf3015fdbb1d35c151db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ec974eda5b6bce7b81be610fc907e68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b01432e554b2cfcdc9cb15d76a21161.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bd1bb0cde92857db0709b3f3914ee5e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Overflow-Prevention-Enhances-Long-Context-Recurrent-LLMs"><a href="#Overflow-Prevention-Enhances-Long-Context-Recurrent-LLMs" class="headerlink" title="Overflow Prevention Enhances Long-Context Recurrent LLMs"></a>Overflow Prevention Enhances Long-Context Recurrent LLMs</h2><p><strong>Authors:Assaf Ben-Kish, Itamar Zimerman, M. Jehanzeb Mirza, James Glass, Leonid Karlinsky, Raja Giryes</strong></p>
<p>A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations. </p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢†åŸŸçš„ä¸€ä¸ªæœ€æ–°è¶‹åŠ¿æ˜¯å¼€å‘é€’å½’å­äºŒæ¬¡æ¨¡å‹ï¼Œä»¥æé«˜é•¿ä¸Šä¸‹æ–‡å¤„ç†æ•ˆç‡ã€‚æˆ‘ä»¬ç ”ç©¶äº†é¢†å…ˆçš„å¤§å‹é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ï¼Œé‡ç‚¹ç ”ç©¶äº†å®ƒä»¬çš„å›ºå®šå¤§å°é€’å½’å†…å­˜å¯¹æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå³ä½¿è¿™äº›æ¨¡å‹ç»è¿‡é’ˆå¯¹æ‰©å±•ä¸Šä¸‹æ–‡çš„è®­ç»ƒï¼Œå®ƒä»¬å¯¹é•¿ä¸Šä¸‹æ–‡çš„åˆ©ç”¨ä»ç„¶ä¸è¶³ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯æ˜äº†åŸºäºåˆ†å—çš„æ¨ç†è¿‡ç¨‹ï¼ˆä»…è¯†åˆ«å’Œå¤„ç†è¾“å…¥ä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†ï¼‰å¯ä»¥ç¼“è§£é€’å½’å†…å­˜æ•…éšœï¼Œå¹¶æœ‰æ•ˆé€‚ç”¨äºè®¸å¤šé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ï¼šåœ¨LongBenchä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†Falcon3-Mamba-Inst-7Bçš„æ€»ä½“æ€§èƒ½æé«˜äº†1 4%ï¼Œå°†Falcon-Mamba-Inst-7Bæé«˜äº†2 8%ï¼Œå°†RecurrentGemma-IT-9Bæé«˜äº†5 0%ï¼Œå°†RWKV6-Finch-7Bæé«˜äº†5 1%ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè¿™ç§ç®€å•çš„æ–¹æ³•è¿˜åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LongBench v2åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œæ˜¾ç¤ºå‡ºä¸åŒç­‰è§„æ¨¡çš„Transformerç›¸å½“çš„ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼•å‘äº†ä¸€ä¸ªé—®é¢˜ï¼Œå³é€’å½’æ¨¡å‹æ˜¯å¦çœŸçš„åˆ©ç”¨é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œå› ä¸ºæˆ‘ä»¬çš„å•å—ç­–ç•¥å³ä½¿åœ¨ç†è®ºä¸Šéœ€è¦è·¨ä¸Šä¸‹æ–‡å…³ç³»çš„ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºæ›´å¼ºçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07793v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é•¿æ–‡æœ¬æ¨¡å‹ï¼ˆLLMï¼‰é¢†åŸŸçš„æ–°è¶‹åŠ¿æ˜¯å¼€å‘å…·æœ‰äºšäºŒæ¬¡æ–¹ç‰¹æ€§çš„é€’å½’æ¨¡å‹ï¼Œä»¥æé«˜å¤„ç†é•¿æ–‡æœ¬çš„æ•ˆç‡ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸»æµçš„é•¿æ–‡æœ¬æ¨¡å‹ï¼Œå¹¶é‡ç‚¹å…³æ³¨å…¶å›ºå®šå¤§å°çš„é€’å½’å†…å­˜å¯¹æ€§èƒ½çš„å½±å“ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿è¿™äº›æ¨¡å‹ç»è¿‡é•¿æ–‡æœ¬è®­ç»ƒï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»æœªèƒ½å……åˆ†åˆ©ç”¨é•¿æ–‡æœ¬çš„ç‰¹æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡åŸºäºå—çš„æ¨ç†è¿‡ç¨‹ï¼ˆä»…è¯†åˆ«å’Œå¤„ç†è¾“å…¥ä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†ï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆç¼“è§£é€’å½’å†…å­˜å¤±æ•ˆé—®é¢˜ï¼Œå¹¶å¯¹è®¸å¤šé•¿æ–‡æœ¬ä»»åŠ¡äº§ç”Ÿç§¯æå½±å“ã€‚æ­¤æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹ä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå¹¶åœ¨LongBenchä¸Šè¾¾åˆ°äº†æœ€å‰æ²¿æ°´å¹³ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œé€’å½’æ¨¡å‹æ˜¯å¦çœŸæ­£åˆ©ç”¨é•¿è·ç¦»ä¾èµ–å…³ç³»å€¼å¾—è¿›ä¸€æ­¥æ¢è®¨ï¼Œå› ä¸ºå•å—ç­–ç•¥åœ¨æŸäº›ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•¿æ–‡æœ¬æ¨¡å‹ï¼ˆLLMï¼‰æ­£å‘å±•å‡ºå…·æœ‰äºšäºŒæ¬¡æ–¹ç‰¹æ€§çš„é€’å½’æ¨¡å‹ï¼Œä»¥æé«˜å¤„ç†é•¿æ–‡æœ¬çš„æ•ˆç‡ã€‚</li>
<li>ä¸»æµé•¿æ–‡æœ¬æ¨¡å‹çš„å›ºå®šå¤§å°é€’å½’å†…å­˜å½±å“å…¶æ€§èƒ½ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨é•¿æ–‡æœ¬ç‰¹æ€§ã€‚</li>
<li>åŸºäºå—çš„æ¨ç†è¿‡ç¨‹èƒ½ç¼“è§£é€’å½’å†…å­˜å¤±æ•ˆé—®é¢˜ï¼Œå¯¹é•¿æ–‡æœ¬ä»»åŠ¡äº§ç”Ÿç§¯æå½±å“ã€‚</li>
<li>æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹ä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå¹¶åœ¨LongBenchä¸Šè¾¾åˆ°æœ€å‰æ²¿æ°´å¹³ã€‚</li>
<li>è¯¥æ–¹æ³•æå‡æ€§èƒ½çš„åŒæ—¶ç®€åŒ–äº†æ“ä½œï¼Œè¡¨ç°å‡ºç«äº‰æ€§çš„ç»“æœã€‚</li>
<li>é€’å½’æ¨¡å‹æ˜¯å¦çœŸæ­£åˆ©ç”¨é•¿è·ç¦»ä¾èµ–å…³ç³»å€¼å¾—è¿›ä¸€æ­¥æ¢è®¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57658723124ef0f09256534debccb27e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82b10455dc32c32f159a25dc15024286.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f8cfb89fb5f8c659bf21f4887292e55.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Agent-RL-Scaling-Law-Agent-RL-with-Spontaneous-Code-Execution-for-Mathematical-Problem-Solving"><a href="#Agent-RL-Scaling-Law-Agent-RL-with-Spontaneous-Code-Execution-for-Mathematical-Problem-Solving" class="headerlink" title="Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for   Mathematical Problem Solving"></a>Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for   Mathematical Problem Solving</h2><p><strong>Authors:Xinji Mai, Haotian Xu, Xing W, Weinong Wang, Yingying Zhang, Wenqiang Zhang</strong></p>
<p>Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \href{<a target="_blank" rel="noopener" href="https://github.com/Anonymize-Author/AgentRL%7D%7Bhttps://github.com/Anonymize-Author/AgentRL%7D">https://github.com/Anonymize-Author/AgentRL}{https://github.com/Anonymize-Author/AgentRL}</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†éœ€è¦è¿›è¡Œç²¾ç¡®ã€å¯éªŒè¯è®¡ç®—çš„æ•°å­¦æ¨ç†ä»»åŠ¡æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚è™½ç„¶åŸºäºç»“æœå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜äº†æ–‡æœ¬æ¨ç†èƒ½åŠ›ï¼Œä½†äº†è§£æ™ºèƒ½ä½“å¦‚ä½•è‡ªä¸»å­¦ä¹ åˆ©ç”¨ä»£ç æ‰§è¡Œç­‰å¤–éƒ¨å·¥å…·ä»ç„¶è‡³å…³é‡è¦ã€‚æˆ‘ä»¬ç ”ç©¶äº†åŸºäºç»“æœå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ åœ¨å·¥å…·é›†æˆæ¨ç†ï¼ˆTool-Integrated Reasoningï¼‰ä¸­çš„åº”ç”¨ï¼Œå³ZeroTIRã€‚æˆ‘ä»¬è®­ç»ƒåŸºç¡€LLMï¼Œä½¿å…¶èƒ½å¤Ÿé’ˆå¯¹æ•°å­¦é—®é¢˜è‡ªå‘ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç ï¼Œè€Œæ— éœ€ç›‘ç£çš„å·¥å…·ä½¿ç”¨ç¤ºä¾‹ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†éšç€å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è¿›è¡Œï¼Œå…³é”®æŒ‡æ ‡çš„å¯é¢„æµ‹æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å¼ºçƒˆçš„æ­£ç›¸å…³å…³ç³»ï¼Œå³è®­ç»ƒæ­¥éª¤çš„å¢åŠ å¯¼è‡´è‡ªå‘ä»£ç æ‰§è¡Œé¢‘ç‡ã€å¹³å‡å“åº”é•¿åº¦å’Œæœ€ç»ˆä»»åŠ¡å‡†ç¡®åº¦çš„æé«˜ã€‚è¿™è¡¨æ˜åœ¨è®­ç»ƒä¸­æ‰€æŠ•å…¥çš„è®¡ç®—åŠªåŠ›ä¸æœ‰æ•ˆå·¥å…·å¢å¼ºæ¨ç†ç­–ç•¥çš„å‡ºç°ä¹‹é—´å­˜åœ¨å¯é‡åŒ–çš„å…³ç³»ã€‚æˆ‘ä»¬å®ç°äº†ä¸€ä¸ªç¨³å¥çš„æ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªè§£è€¦çš„ä»£ç æ‰§è¡Œç¯å¢ƒï¼Œå¹¶å¯¹æ ‡å‡†å¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œæ¡†æ¶éªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ã€‚å®éªŒè¡¨æ˜ï¼ŒZeroTIRåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—è¶…è¶Šäº†éå·¥å…·ZeroRLçš„åŸºå‡†çº¿ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºè‡ªä¸»å·¥å…·è·å–å’Œæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ å†…çš„æ‰©å±•æä¾›äº†åŸºæœ¬ç†è§£ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¯å¤åˆ¶çš„åŸºå‡†ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Anonymize-Author/AgentRL">https://github.com/Anonymize-Author/AgentRL</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07773v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†éœ€è¦ç²¾ç¡®å¯éªŒè¯è®¡ç®—çš„æ•°å­¦æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°æ¬ ä½³ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»ç»“æœå¯¼å‘çš„å¥–åŠ±å‡ºå‘ï¼Œæ¢ç´¢å·¥å…·é›†æˆæ¨ç†ï¼ˆTool-Integrated Reasoningï¼‰ä¸­çš„ZeroTIRæ–¹æ³•ã€‚è¯¥æ–¹æ³•è®­ç»ƒåŸºç¡€LLMï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªå‘ä¸ºæ•°å­¦é—®é¢˜ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç ï¼Œæ— éœ€ç›‘ç£å·¥å…·ä½¿ç”¨ç¤ºä¾‹ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€RLè®­ç»ƒçš„è¿›è¡Œï¼Œå…³é”®æŒ‡æ ‡å‘ˆç°å¯é¢„æµ‹çš„è§„æ¨¡åŒ–å¢é•¿ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç§¯æçš„æ­£ç›¸å…³å…³ç³»ï¼šè®­ç»ƒæ­¥éª¤çš„å¢åŠ å¯¼è‡´è‡ªå‘ä»£ç æ‰§è¡Œé¢‘ç‡ã€å¹³å‡å“åº”é•¿åº¦å’Œä»»åŠ¡å‡†ç¡®åº¦çš„æå‡ã€‚è¿™è¡¨æ˜è®­ç»ƒä¸­çš„è®¡ç®—æŠ•å…¥ä¸æœ‰æ•ˆå·¥å…·è¾…åŠ©æ¨ç†ç­–ç•¥çš„å‡ºç°ä¹‹é—´å­˜åœ¨å¯é‡åŒ–çš„å…³ç³»ã€‚å®éªŒè¡¨æ˜ï¼ŒZeroTIRåœ¨éå·¥å…·ZeroRLåŸºå‡†çº¿ä¸Šæ˜¾è‘—è¶…è¶Šäº†å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦åŸºå‡†æµ‹è¯•ã€‚æœ¬ç ”ç©¶ä¸ºè‡ªä¸»å·¥å…·è·å–å’ŒAgent RLå†…çš„æ‰©å±•æä¾›äº†åŸºç¡€ç†è§£ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å¯å¤åˆ¶çš„æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤„ç†éœ€è¦ç²¾ç¡®è®¡ç®—çš„æ•°å­¦æ¨ç†ä»»åŠ¡æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»ç»“æœå¯¼å‘çš„å¥–åŠ±å‡ºå‘ï¼Œæœ‰åŠ©äºæå‡æ–‡æœ¬æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ZeroTIRæ–¹æ³•ä½¿LLMèƒ½å¤Ÿè‡ªå‘ä¸ºæ•°å­¦é—®é¢˜ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç ã€‚</li>
<li>éšç€RLè®­ç»ƒçš„è¿›è¡Œï¼Œå…³é”®æŒ‡æ ‡å¦‚è‡ªå‘ä»£ç æ‰§è¡Œé¢‘ç‡ã€å¹³å‡å“åº”é•¿åº¦å’Œä»»åŠ¡å‡†ç¡®åº¦å‘ˆç°å¯é¢„æµ‹çš„å¢é•¿ã€‚</li>
<li>è®­ç»ƒä¸­çš„è®¡ç®—æŠ•å…¥ä¸æœ‰æ•ˆå·¥å…·è¾…åŠ©æ¨ç†ç­–ç•¥çš„å‡ºç°å­˜åœ¨å¯é‡åŒ–çš„å…³ç³»ã€‚</li>
<li>ZeroTIRåœ¨åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†éå·¥å…·ZeroRLæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4bead99dc01c1c0a2564e544a77b289f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f2c8423fa863c4270331b0d5e445251.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38dca8f3d7b54fe586e911c87832c5ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2a125a26e4ebf81a82a980542b6bb06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76ef8dc29a2e40231341a5008d2b5793.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Enhancing-Code-Generation-via-Bidirectional-Comment-Level-Mutual-Grounding"><a href="#Enhancing-Code-Generation-via-Bidirectional-Comment-Level-Mutual-Grounding" class="headerlink" title="Enhancing Code Generation via Bidirectional Comment-Level Mutual   Grounding"></a>Enhancing Code Generation via Bidirectional Comment-Level Mutual   Grounding</h2><p><strong>Authors:Yifeng Di, Tianyi Zhang</strong></p>
<p>Large Language Models (LLMs) have demonstrated unprecedented capability in code generation. However, LLM-generated code is still plagued with a wide range of functional errors, especially for complex programming tasks that LLMs have not seen before. Recent studies have shown that developers often struggle with inspecting and fixing incorrect code generated by LLMs, diminishing their productivity and trust in LLM-based code generation. Inspired by the mutual grounding theory in communication, we propose an interactive approach that leverages code comments as a medium for developers and LLMs to establish a shared understanding. Our approach facilitates iterative grounding by interleaving code generation, inline comment generation, and contextualized user feedback through editable comments to align generated code with developer intent. We evaluated our approach on two popular benchmarks and demonstrated that our approach significantly improved multiple state-of-the-art LLMs, e.g., 17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we conducted a user study with 12 participants in comparison to two baselines: (1) interacting with GitHub Copilot, and (2) interacting with a multi-step code generation paradigm called Multi-Turn Program Synthesis. Participants completed the given programming tasks 16.7% faster and with 10.5% improvement in task success rate when using our approach. Both results show that interactively refining code comments enables the collaborative establishment of mutual grounding, leading to more accurate code generation and higher developer confidence. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºäº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMç”Ÿæˆçš„ä»£ç ä»ç„¶å­˜åœ¨ç€å¹¿æ³›çš„åŠŸèƒ½é”™è¯¯ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†LLMæœªæ›¾é‡åˆ°è¿‡çš„å¤æ‚ç¼–ç¨‹ä»»åŠ¡æ—¶ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¼€å‘è€…åœ¨æ£€æŸ¥å’Œä¿®å¤LLMç”Ÿæˆçš„é”™è¯¯ä»£ç æ—¶ç»å¸¸é‡åˆ°å›°éš¾ï¼Œè¿™é™ä½äº†ä»–ä»¬çš„ç”Ÿäº§åŠ›å’Œå¯¹LLMä»£ç ç”Ÿæˆçš„å¯ä¿¡åº¦ã€‚å—é€šä¿¡ä¸­çš„ç›¸äº’åŸºç¡€ç†è®ºçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§äº¤äº’æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä»£ç æ³¨é‡Šä½œä¸ºå¼€å‘è€…å’ŒLLMä¹‹é—´çš„åª’ä»‹ï¼Œä»¥å»ºç«‹å…±äº«ç†è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡äº¤æ›¿è¿›è¡Œä»£ç ç”Ÿæˆã€å†…è”æ³¨é‡Šç”Ÿæˆå’Œé€šè¿‡å¯ç¼–è¾‘æ³¨é‡Šçš„ä¸Šä¸‹æ–‡ç”¨æˆ·åé¦ˆï¼Œæ¥ä¿ƒè¿›ç”Ÿæˆçš„ä»£ç ä¸å¼€å‘è€…æ„å›¾çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæµè¡Œçš„åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æ”¹è¿›äº†å¤šä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚åœ¨äººç±»è¯„ä¼°ï¼ˆHumanEvalï¼‰ä¸Šï¼Œå¯¹code-davinci-002çš„pass@1æå‡äº†17.1%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ä¸¤ä¸ªåŸºå‡†è¿›è¡Œäº†ç”¨æˆ·ç ”ç©¶ï¼šä¸€æ˜¯ä¸GitHub Copilotäº’åŠ¨ï¼ŒäºŒæ˜¯ä¸å¤šæ­¥ä»£ç ç”Ÿæˆæ¨¡å¼â€”â€”å¤šè½®ç¨‹åºåˆæˆäº’åŠ¨ã€‚åœ¨ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•æ—¶ï¼Œå‚ä¸è€…å®Œæˆç»™å®šç¼–ç¨‹ä»»åŠ¡çš„é€Ÿåº¦æé«˜äº†16.7%ï¼Œä»»åŠ¡æˆåŠŸç‡æé«˜äº†10.5%ã€‚è¿™ä¸¤ä¸ªç»“æœéƒ½è¡¨æ˜ï¼Œäº¤äº’åœ°ä¿®æ”¹ä»£ç æ³¨é‡Šæœ‰åŠ©äºå»ºç«‹ç›¸äº’çš„åŸºç¡€ï¼Œä»è€Œå¯¼è‡´æ›´å‡†ç¡®çš„ä»£ç ç”Ÿæˆå’Œæ›´é«˜çš„å¼€å‘è€…ä¿¡å¿ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07768v1">PDF</a> Accepted to ICSE 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å±•ç°å‡ºå‰æ‰€æœªæœ‰çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMç”Ÿæˆçš„ä»£ç ä»å­˜åœ¨å¹¿æ³›çš„åŠŸèƒ½é”™è¯¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚ä¸”å‰æ‰€æœªæœ‰çš„ç¼–ç¨‹ä»»åŠ¡æ—¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¼€å‘è€…åœ¨æ£€æŸ¥ä¸ä¿®å¤LLMç”Ÿæˆçš„é”™è¯¯ä»£ç æ—¶ï¼Œå¾€å¾€é¢ä¸´æŒ‘æˆ˜ï¼Œè¿›è€Œå½±å“å…¶ç”Ÿäº§æ•ˆç‡å’Œå¯¹äºLLMä»£ç ç”ŸæˆåŠŸèƒ½çš„ä¿¡ä»»åº¦ã€‚æœ¬ç ”ç©¶å—æ²Ÿé€šä¸­çš„ç›¸äº’å®šä½ç†è®ºå¯å‘ï¼Œæå‡ºä¸€ç§äº¤äº’æ–¹æ³•ï¼Œåˆ©ç”¨ä»£ç æ³¨é‡Šä½œä¸ºå¼€å‘è€…å’ŒLLMå»ºç«‹å…±äº«ç†è§£çš„åª’ä»‹ã€‚è¯¥æ–¹æ³•é€šè¿‡äº¤æ›¿è¿›è¡Œä»£ç ç”Ÿæˆã€å†…è”æ³¨é‡Šç”Ÿæˆå’ŒåŸºäºä¸Šä¸‹æ–‡çš„å¯ç¼–è¾‘ç”¨æˆ·åé¦ˆï¼Œä½¿ç”Ÿæˆçš„ä»£ç ç¬¦åˆå¼€å‘è€…çš„æ„å›¾ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šå¯¹è¯¥æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶è¯æ˜å…¶æ˜¾è‘—æ”¹è¿›äº†å¤šé¡¹æœ€å‰æ²¿çš„LLMæ€§èƒ½ï¼Œä¾‹å¦‚åœ¨HumanEvalä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿code-davinci-002çš„pass@1æå‡äº†17.1%ã€‚æ­¤å¤–ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•â€”â€”ä¸GitHub Copilotäº’åŠ¨åŠå¤šå›åˆä»£ç ç”Ÿæˆæ¨¡å¼ï¼ˆMulti-Turn Program Synthesisï¼‰â€”â€”æˆ‘ä»¬å¼€å±•äº†åŒ…å«12åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå‚ä¸è€…åœ¨ç»™å®šçš„ç¼–ç¨‹ä»»åŠ¡ä¸Šå®Œæˆé€Ÿåº¦æé«˜äº†16.7%ï¼Œä»»åŠ¡æˆåŠŸç‡æé«˜äº†10.5%ã€‚ä¸¤é¡¹ç»“æœå‡è¡¨æ˜ï¼Œé€šè¿‡äº¤äº’æ–¹å¼ä¿®æ­£ä»£ç æ³¨é‡Šæœ‰åŠ©äºå…±åŒå»ºç«‹ç›¸äº’å®šä½ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®çš„ä»£ç å¹¶æå‡å¼€å‘è€…çš„ä¿¡å¿ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMåœ¨ä»£ç ç”Ÿæˆæ–¹é¢å­˜åœ¨åŠŸèƒ½é”™è¯¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚ä¸”å‰æ‰€æœªæœ‰çš„ç¼–ç¨‹ä»»åŠ¡æ—¶ã€‚</li>
<li>å¼€å‘è€…åœ¨æ£€æŸ¥ä¸ä¿®å¤LLMç”Ÿæˆçš„é”™è¯¯ä»£ç æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå½±å“ç”Ÿäº§æ•ˆç‡ä¸å¯¹LLMçš„ä¿¡ä»»ã€‚</li>
<li>æå‡ºä¸€ç§äº¤äº’æ–¹æ³•ï¼Œåˆ©ç”¨ä»£ç æ³¨é‡Šä½œä¸ºåª’ä»‹ï¼Œä¿ƒè¿›å¼€å‘è€…å’ŒLLMå»ºç«‹å…±äº«ç†è§£ã€‚</li>
<li>é€šè¿‡äº¤æ›¿è¿›è¡Œä»£ç ç”Ÿæˆã€å†…è”æ³¨é‡Šç”Ÿæˆå’ŒåŸºäºä¸Šä¸‹æ–‡çš„ç”¨æˆ·åé¦ˆï¼Œä½¿ç”Ÿæˆçš„ä»£ç ç¬¦åˆå¼€å‘è€…æ„å›¾ã€‚</li>
<li>åœ¨ä¸¤ä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†LLMçš„æ€§èƒ½ã€‚</li>
<li>ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œä½¿ç”¨æ­¤æ–¹æ³•çš„å‚ä¸è€…åœ¨ç¼–ç¨‹ä»»åŠ¡ä¸Šçš„å®Œæˆé€Ÿåº¦å’Œæé«˜ä»»åŠ¡æˆåŠŸç‡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>äº¤äº’æ–¹å¼ä¿®æ­£ä»£ç æ³¨é‡Šæœ‰åŠ©äºå…±åŒå»ºç«‹ç›¸äº’å®šä½ï¼Œæå‡ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œå¼€å‘è€…çš„ä¿¡å¿ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2af59580429e368915048332143a413c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-927e0823d3d99ca9c705a4b570073009.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75ca65a9317013d55be1433c038c0c17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f83228372fac4a53f97ac8aaefd9540.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32151192b8838a03634019185220853b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af8a55c649dce50eef9ff2924c127485.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc0b350c2fb3b5d283e4704ba461587e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="S-GRPO-Early-Exit-via-Reinforcement-Learning-in-Reasoning-Models"><a href="#S-GRPO-Early-Exit-via-Reinforcement-Learning-in-Reasoning-Models" class="headerlink" title="S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models"></a>S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models</h2><p><strong>Authors:Muzhi Dai, Chenxu Yang, Qingyi Si</strong></p>
<p>As Test-Time Scaling emerges as an active research focus in the large language model community, advanced post-training methods increasingly emphasize extending chain-of-thought (CoT) generation length, thereby enhancing reasoning capabilities to approach Deepseek R1-like reasoning models. However, recent studies reveal that reasoning models (even Qwen3) consistently exhibit excessive thought redundancy in CoT generation. This overthinking problem stems from conventional outcome-reward reinforcement learningâ€™s systematic neglect in regulating intermediate reasoning steps. This paper proposes Serial-Group Decaying-Reward Policy Optimization (namely S-GRPO), a novel reinforcement learning method that empowers models with the capability to determine the sufficiency of reasoning steps, subsequently triggering early exit of CoT generation. Specifically, unlike GRPO, which samples multiple possible completions (parallel group) in parallel, we select multiple temporal positions in the generation of one CoT to allow the model to exit thinking and instead generate answers (serial group), respectively. For the correct answers in a serial group, we assign rewards that decay according to positions, with lower rewards towards the later ones, thereby reinforcing the modelâ€™s behavior to generate higher-quality answers at earlier phases with earlier exits of thinking. Empirical evaluations demonstrate compatibility with state-of-the-art reasoning models, including Qwen3 and Deepseek-distill models, achieving 35.4% ~ 61.1% sequence length reduction with 0.72% ~ 6.08% accuracy improvements across GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond benchmarks. </p>
<blockquote>
<p>éšç€æµ‹è¯•æ—¶é—´ç¼©æ”¾ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ç¤¾åŒºä¸­çš„æ´»è·ƒç ”ç©¶ç„¦ç‚¹ï¼Œå…ˆè¿›çš„åè®­ç»ƒæ–¹æ³•è¶Šæ¥è¶Šå¼ºè°ƒå»¶é•¿æ€ç»´é“¾ï¼ˆCoTï¼‰ç”Ÿæˆé•¿åº¦ï¼Œä»è€Œæé«˜æ¨ç†èƒ½åŠ›ï¼Œä»¥æ¥è¿‘Deepseek R1ç­‰æ¨ç†æ¨¡å‹ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œæ¨ç†æ¨¡å‹ï¼ˆå³ä½¿æ˜¯Qwen3ï¼‰åœ¨æ€ç»´é“¾ç”Ÿæˆä¸­å§‹ç»ˆè¡¨ç°å‡ºè¿‡å¤šçš„æ€ç»´å†—ä½™ã€‚è¿™ç§è¿‡åº¦æ€è€ƒçš„é—®é¢˜æºäºä¼ ç»Ÿç»“æœå¥–åŠ±å¼ºåŒ–å­¦ä¹ åœ¨è°ƒèŠ‚ä¸­é—´æ¨ç†æ­¥éª¤ä¸­çš„ç³»ç»Ÿæ€§å¿½è§†ã€‚æœ¬æ–‡æå‡ºäº†ä¸²è¡Œç»„è¡°å‡å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆç®€ç§°S-GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä½¿æ¨¡å‹å…·å¤‡åˆ¤æ–­æ¨ç†æ­¥éª¤æ˜¯å¦å……è¶³çš„èƒ½åŠ›ï¼Œä»è€Œè§¦å‘æ€ç»´é“¾ç”Ÿæˆçš„æ—©æœŸé€€å‡ºã€‚å…·ä½“æ¥è¯´ï¼Œä¸GRPOä¸åŒï¼ŒGRPOä¼šå¹¶è¡Œé‡‡æ ·å¤šä¸ªå¯èƒ½çš„å®Œæˆï¼ˆå¹¶è¡Œç»„ï¼‰ï¼Œè€Œæˆ‘ä»¬é€‰æ‹©åœ¨ä¸€æ¡æ€ç»´é“¾çš„ç”Ÿæˆè¿‡ç¨‹ä¸­é€‰æ‹©å¤šä¸ªæ—¶é—´ä½ç½®ï¼Œè®©æ¨¡å‹åœæ­¢æ€è€ƒå¹¶å¼€å§‹ç”Ÿæˆç­”æ¡ˆï¼ˆä¸²è¡Œç»„ï¼‰ã€‚å¯¹äºä¸²è¡Œç»„ä¸­çš„æ­£ç¡®ç­”æ¡ˆï¼Œæˆ‘ä»¬æ ¹æ®ä½ç½®è¡°å‡åˆ†é…å¥–åŠ±ï¼ŒåæœŸç­”æ¡ˆçš„å¥–åŠ±è¾ƒä½ï¼Œä»è€Œå¼ºåŒ–æ¨¡å‹åœ¨è¾ƒæ—©é˜¶æ®µç”Ÿæˆé«˜è´¨é‡ç­”æ¡ˆçš„è¡Œä¸ºï¼Œå¹¶å°½æ—©é€€å‡ºæ€è€ƒã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸åŒ…æ‹¬Qwen3å’ŒDeepseekè’¸é¦æ¨¡å‹åœ¨å†…çš„æœ€æ–°æ¨ç†æ¨¡å‹å…¼å®¹ï¼Œåœ¨GSM8Kã€AIME 2024ã€AMC 2023ã€MATH-500å’ŒGPQA Diamondç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†35.4%~61.1%çš„åºåˆ—é•¿åº¦ç¼©å‡ï¼ŒåŒæ—¶å‡†ç¡®ç‡æé«˜äº†0.72%~6.08%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07686v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹é¢†åŸŸä¸­æ–°å…´çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æŠ€æœ¯ï¼Œå¼ºè°ƒåœ¨è®­ç»ƒåæ‰©å±•æ€ç»´é“¾ç”Ÿæˆé•¿åº¦çš„é‡è¦æ€§ï¼Œä»¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæ¥è¿‘Deepseek R1çº§åˆ«çš„æ¨ç†æ¨¡å‹ã€‚ç„¶è€Œï¼Œç ”ç©¶å‘ç°æ¨ç†æ¨¡å‹ï¼ˆå¦‚Qwen3ï¼‰åœ¨æ€ç»´é“¾ç”Ÿæˆä¸­å­˜åœ¨è¿‡åº¦å†—ä½™çš„é—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºS-GRPOçš„æ–°å‹å¼ºåŒ–å­¦ä¹ æ”¿ç­–ä¼˜åŒ–æ–¹æ³•ï¼Œå³ä¸²è¡Œç¾¤ç»„è¡°å‡å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼Œè¯¥æ–¹æ³•ä½¿æ¨¡å‹å…·å¤‡åˆ¤æ–­æ¨ç†æ­¥éª¤æ˜¯å¦å……è¶³çš„èƒ½åŠ›ï¼Œå¹¶æ®æ­¤æå‰ç»ˆæ­¢æ€ç»´é“¾çš„ç”Ÿæˆã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒS-GRPOä¸æœ€æ–°æ¨ç†æ¨¡å‹å…¼å®¹ï¼ŒåŒ…æ‹¬Qwen3å’ŒDeepseekè’¸é¦æ¨¡å‹ï¼Œåœ¨GSM8Kã€AIME 2024ã€AMC 2023ã€MATH-500å’ŒGPQA Diamondç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ç°äº†åºåˆ—é•¿åº¦å‡å°‘35.4%~61.1%ï¼ŒåŒæ—¶ç²¾åº¦æé«˜0.72%~6.08%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´ç¼©æ”¾æŠ€æœ¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹é¢†åŸŸæ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶ç„¦ç‚¹ã€‚</li>
<li>å…ˆè¿›çš„è®­ç»ƒåæ–¹æ³•å¼ºè°ƒæ‰©å±•æ€ç»´é“¾ç”Ÿæˆé•¿åº¦ä»¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ¨ç†æ¨¡å‹å­˜åœ¨è¿‡åº¦å†—ä½™çš„é—®é¢˜ã€‚</li>
<li>S-GRPOæ˜¯ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ”¿ç­–ä¼˜åŒ–æ–¹æ³•ï¼Œè§£å†³äº†è¿‡åº¦å†—ä½™çš„é—®é¢˜ã€‚</li>
<li>S-GRPOé€šè¿‡åœ¨æ€ç»´é“¾ç”Ÿæˆè¿‡ç¨‹ä¸­é€‰æ‹©å¤šä¸ªæ—¶é—´ä½ç½®æ¥å®ç°æ—©æœŸé€€å‡ºã€‚</li>
<li>S-GRPOå¯¹æ­£ç¡®ç­”æ¡ˆé‡‡ç”¨è¡°å‡å¥–åŠ±ç­–ç•¥ï¼Œé¼“åŠ±æ¨¡å‹åœ¨è¾ƒæ—©é˜¶æ®µç”Ÿæˆé«˜è´¨é‡ç­”æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d7f40771965649538497229ca4a29368.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b71e12bf3985904a562115a48086722.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfe1fc41a5cceab18ea9390010126360.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0467d2942739e8e70fab02761eead7f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Retrieval-Augmented-Generation-for-Chemistry"><a href="#Benchmarking-Retrieval-Augmented-Generation-for-Chemistry" class="headerlink" title="Benchmarking Retrieval-Augmented Generation for Chemistry"></a>Benchmarking Retrieval-Augmented Generation for Chemistry</h2><p><strong>Authors:Xianrui Zhong, Bowen Jin, Siru Ouyang, Yanzhen Shen, Qiao Jin, Yin Fang, Zhiyong Lu, Jiawei Han</strong></p>
<p>Retrieval-augmented generation (RAG) has emerged as a powerful framework for enhancing large language models (LLMs) with external knowledge, particularly in scientific domains that demand specialized and dynamic information. Despite its promise, the application of RAG in the chemistry domain remains underexplored, primarily due to the lack of high-quality, domain-specific corpora and well-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a comprehensive benchmark designed to systematically assess the effectiveness of RAG across a diverse set of chemistry-related tasks. The accompanying chemistry corpus integrates heterogeneous knowledge sources, including scientific literature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia entries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG toolkit that supports five retrieval algorithms and eight LLMs. Using ChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain â€“ achieving an average relative improvement of 17.4% over direct inference methods. We further conduct in-depth analyses on retriever architectures, corpus selection, and the number of retrieved passages, culminating in practical recommendations to guide future research and deployment of RAG systems in the chemistry domain. The code and data is available at <a target="_blank" rel="noopener" href="https://chemrag.github.io/">https://chemrag.github.io</a>. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä½œä¸ºä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡å¤–éƒ¨çŸ¥è¯†å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠŸèƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ä¸“ä¸šåŒ–å’ŒåŠ¨æ€ä¿¡æ¯çš„ç§‘å­¦é¢†åŸŸè¡¨ç°å‡ºå…¶æ½œåŠ›ã€‚å°½ç®¡å‰æ™¯å¹¿é˜”ï¼Œä½†RAGåœ¨åŒ–å­¦é¢†åŸŸçš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹é«˜è´¨é‡ã€ç‰¹å®šé¢†åŸŸçš„è¯­æ–™åº“å’Œç²¾å¿ƒç­–åˆ’çš„è¯„ä¼°åŸºå‡†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ChemRAG-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°RAGåœ¨å¤šç§åŒ–å­¦ç›¸å…³ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¼´éšçš„åŒ–å­¦è¯­æ–™åº“èåˆäº†å„ç§å¼‚è´¨çŸ¥è¯†æ¥æºï¼ŒåŒ…æ‹¬ç§‘å­¦æ–‡çŒ®ã€PubChemæ•°æ®åº“ã€PubMedæ‘˜è¦ã€æ•™ç§‘ä¹¦å’ŒWikipediaæ¡ç›®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†ChemRAG-Toolkitï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„RAGå·¥å…·åŒ…ï¼Œæ”¯æŒäº”ç§æ£€ç´¢ç®—æ³•å’Œå…«ç§LLMã€‚ä½¿ç”¨ChemRAG-Toolkitï¼Œæˆ‘ä»¬è¯æ˜äº†RAGäº§ç”Ÿäº†æ˜¾è‘—çš„æ€§èƒ½æå‡â€”â€”ç›¸å¯¹äºç›´æ¥æ¨ç†æ–¹æ³•ï¼Œå¹³å‡ç›¸å¯¹æé«˜äº†17.4%ã€‚æˆ‘ä»¬è¿˜å¯¹æ£€ç´¢å™¨æ¶æ„ã€è¯­æ–™åº“é€‰æ‹©å’Œæ£€ç´¢æ®µè½æ•°é‡è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¹¶æ®æ­¤æå‡ºäº†å®ç”¨å»ºè®®ï¼Œä»¥æŒ‡å¯¼æœªæ¥RAGç³»ç»Ÿåœ¨åŒ–å­¦é¢†åŸŸçš„ç ”ç©¶å’Œéƒ¨ç½²ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://chemrag.github.ioè·å¾—./">https://chemrag.github.ioè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07671v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¢å¼ºå‹æ£€ç´¢ç”Ÿæˆæ¡†æ¶ï¼ˆRAGï¼‰é€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ä¸“ä¸šåŒ–å’ŒåŠ¨æ€ä¿¡æ¯çš„ç§‘å­¦é¢†åŸŸã€‚é’ˆå¯¹åŒ–å­¦é¢†åŸŸçš„RAGåº”ç”¨ä»ç„¶ç¼ºä¹æ¢ç´¢ï¼Œä¸»è¦ç”±äºç¼ºä¹é«˜è´¨é‡ã€ç‰¹å®šé¢†åŸŸçš„è¯­æ–™åº“å’Œè¯„ä¼°åŸºå‡†ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ChemRAG-BenchåŸºå‡†ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°åŒ–å­¦ç›¸å…³ä»»åŠ¡ä¸­RAGçš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶æä¾›åŒ–å­¦è¯­æ–™åº“æ”¯æŒå¤šæ¥æºçŸ¥è¯†æ•´åˆã€‚æ­¤å¤–ï¼ŒChemRAG-Toolkitä½œä¸ºæ¨¡å—åŒ–å¯æ‰©å±•çš„RAGå·¥å…·åŒ…ï¼Œæ”¯æŒäº”ç§æ£€ç´¢ç®—æ³•å’Œå…«ç§LLMã€‚é€šè¿‡ChemRAG-ToolkitéªŒè¯ï¼ŒRAGç›¸æ¯”ç›´æ¥æ¨ç†æ–¹æ³•æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œå¹³å‡ç›¸å¯¹æé«˜ç‡ä¸º17.4%ã€‚æˆ‘ä»¬å¯¹æ£€ç´¢å™¨æ¶æ„ã€è¯­æ–™åº“é€‰æ‹©å’Œæ£€ç´¢ç¯‡ç« æ•°é‡è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œä¸ºæœªæ¥ç ”ç©¶å’Œéƒ¨ç½²RAGç³»ç»Ÿåœ¨åŒ–å­¦é¢†åŸŸæä¾›äº†å®è·µå»ºè®®ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://chemrag.github.ioè·å–./">https://chemrag.github.ioè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RAGæ¡†æ¶å¢å¼ºäº†LLMåœ¨åŒ–å­¦é¢†åŸŸçš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>ChemRAG-Benchæä¾›äº†å…¨é¢çš„åŸºå‡†ä»¥è¯„ä¼°åŒ–å­¦ç›¸å…³ä»»åŠ¡ä¸­RAGçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æä¾›çš„åŒ–å­¦è¯­æ–™åº“èåˆäº†å¤šæºçŸ¥è¯†ï¼ŒåŒ…æ‹¬ç§‘å­¦æ–‡çŒ®ã€PubChemæ•°æ®åº“ç­‰ã€‚</li>
<li>ChemRAG-Toolkitæ”¯æŒå¤šç§æ£€ç´¢ç®—æ³•å’ŒLLMï¼Œæ¨¡å—åŒ–è®¾è®¡ä¾¿äºæ‰©å±•ã€‚</li>
<li>RAGç›¸æ¯”ç›´æ¥æ¨ç†æ–¹æ³•å¹³å‡æ€§èƒ½æå‡17.4%ã€‚</li>
<li>ç ”ç©¶æ·±å…¥åˆ†æäº†æ£€ç´¢å™¨æ¶æ„ã€è¯­æ–™åº“é€‰æ‹©å’Œæ£€ç´¢ç¯‡ç« æ•°é‡å¯¹æ€§èƒ½çš„å½±å“ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3a79a20a64414b066a32832427f5cd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb19a1a4747b10332445209ebe2e02e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d05c6155ad4a2e29018640e4aa4c9093.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4770cf7b0ee0a58dc49d57fdd70e545.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Concept-Level-Explainability-for-Auditing-Steering-LLM-Responses"><a href="#Concept-Level-Explainability-for-Auditing-Steering-LLM-Responses" class="headerlink" title="Concept-Level Explainability for Auditing &amp; Steering LLM Responses"></a>Concept-Level Explainability for Auditing &amp; Steering LLM Responses</h2><p><strong>Authors:Kenza Amara, Rita Sevastjanova, Mennatallah El-Assady</strong></p>
<p>As large language models (LLMs) become widely deployed, concerns about their safety and alignment grow. An approach to steer LLM behavior, such as mitigating biases or defending against jailbreaks, is to identify which parts of a prompt influence specific aspects of the modelâ€™s output. Token-level attribution methods offer a promising solution, but still struggle in text generation, explaining the presence of each token in the output separately, rather than the underlying semantics of the entire LLM response. We introduce ConceptX, a model-agnostic, concept-level explainability method that identifies the concepts, i.e., semantically rich tokens in the prompt, and assigns them importance based on the outputsâ€™ semantic similarity. Unlike current token-level methods, ConceptX also offers to preserve context integrity through in-place token replacements and supports flexible explanation goals, e.g., gender bias. ConceptX enables both auditing, by uncovering sources of bias, and steering, by modifying prompts to shift the sentiment or reduce the harmfulness of LLM responses, without requiring retraining. Across three LLMs, ConceptX outperforms token-level methods like TokenSHAP in both faithfulness and human alignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for random edits and lower attack success rates from 0.463 to 0.242, outperforming attribution and paraphrasing baselines. While prompt engineering and self-explaining methods sometimes yield safer responses, ConceptX offers a transparent and faithful alternative for improving LLM safety and alignment, demonstrating the practical value of attribution-based explainability in guiding LLM behavior. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œå¯¹å…¶å®‰å…¨æ€§å’Œå¯¹é½æ€§çš„æ‹…å¿§ä¹Ÿåœ¨å¢é•¿ã€‚å¼•å¯¼LLMè¡Œä¸ºçš„æ–¹æ³•ï¼Œä¾‹å¦‚ç¼“è§£åè§æˆ–é˜²èŒƒè¶Šç‹±ï¼Œæ˜¯ç¡®å®šå“ªäº›æç¤ºéƒ¨åˆ†å½±å“æ¨¡å‹è¾“å‡ºçš„ç‰¹å®šæ–¹é¢ã€‚ä»¤ç‰Œçº§å½’å› æ–¹æ³•æä¾›äº†å¾ˆæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†åœ¨æ–‡æœ¬ç”Ÿæˆä¸­ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦è§£é‡Šè¾“å‡ºä¸­æ¯ä¸ªä»¤ç‰Œçš„å­˜åœ¨ï¼Œè€Œä¸æ˜¯æ•´ä¸ªLLMå“åº”çš„æ½œåœ¨è¯­ä¹‰ã€‚æˆ‘ä»¬æ¨å‡ºäº†ConceptXï¼Œè¿™æ˜¯ä¸€ç§æ¨¡å‹æ— å…³çš„æ¦‚å¿µçº§è§£é‡Šæ–¹æ³•ï¼Œå¯ä»¥è¯†åˆ«æç¤ºä¸­çš„æ¦‚å¿µï¼Œå³è¯­ä¹‰ä¸°å¯Œçš„ä»¤ç‰Œï¼Œå¹¶æ ¹æ®è¾“å‡ºè¯­ä¹‰ç›¸ä¼¼æ€§åˆ†é…å®ƒä»¬çš„é‡è¦æ€§ã€‚ä¸å½“å‰çš„ä»¤ç‰Œçº§æ–¹æ³•ä¸åŒï¼ŒConceptXè¿˜é€šè¿‡å°±åœ°ä»¤ç‰Œæ›¿æ¢ä¿ç•™äº†ä¸Šä¸‹æ–‡å®Œæ•´æ€§ï¼Œå¹¶æ”¯æŒçµæ´»çš„è§£é‡Šç›®æ ‡ï¼Œä¾‹å¦‚æ€§åˆ«åè§ã€‚ConceptXèƒ½å¤Ÿé€šè¿‡æ­ç¤ºåè§çš„æ¥æºæ¥è¿›è¡Œå®¡è®¡ï¼Œå¹¶é€šè¿‡ä¿®æ”¹æç¤ºæ¥æ”¹å˜æƒ…ç»ªæˆ–å‡å°‘LLMå“åº”çš„æœ‰å®³æ€§ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚åœ¨ä¸‰ä¸ªLLMä¸­ï¼ŒConceptXåœ¨å¿ è¯šåº¦å’Œäººç±»å¯¹é½æ–¹é¢éƒ½ä¼˜äºTokenSHAPç­‰ä»¤ç‰Œçº§æ–¹æ³•ã€‚ä¸éšæœºç¼–è¾‘ç›¸æ¯”ï¼Œé©¾é©¶ä»»åŠ¡å°†æƒ…ç»ªè½¬ç§»æé«˜äº†0.252è‡³0.131ï¼Œæ”»å‡»æˆåŠŸç‡ä»0.463é™è‡³0.242ï¼Œè¶…è¿‡äº†å½’å±å’Œæ”¹è¿°åŸºçº¿ã€‚è™½ç„¶æç¤ºå·¥ç¨‹å’Œè‡ªè§£é‡Šæ–¹æ³•æœ‰æ—¶ä¼šç”Ÿæˆæ›´å®‰å…¨çš„å“åº”ï¼Œä½†ConceptXæä¾›äº†ä¸€ç§é€æ˜å’Œå¿ è¯šçš„æ›¿ä»£æ–¹æ¡ˆï¼Œç”¨äºæé«˜LLMçš„å®‰å…¨æ€§å’Œå¯¹é½æ€§ï¼Œè¯æ˜äº†åŸºäºå½’å±çš„è§£é‡Šåœ¨å®é™…æŒ‡å¯¼LLMè¡Œä¸ºä¸­çš„å®ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07610v1">PDF</a> 9 pages, 7 figures, Submission to Neurips 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹¿æ³›åº”ç”¨å¼•å‘äº†å…³äºå…¶å®‰å…¨æ€§å’Œå¯¹é½æ€§çš„æ‹…å¿§ã€‚ConceptXæ˜¯ä¸€ç§æ¨¡å‹æ— å…³çš„æ¦‚å¿µçº§è§£é‡Šæ€§æ–¹æ³•ï¼Œå®ƒèƒ½è¯†åˆ«æç¤ºä¸­çš„æ¦‚å¿µï¼Œå¹¶æ ¹æ®è¾“å‡ºè¯­ä¹‰ç›¸ä¼¼æ€§ä¸ºå®ƒä»¬åˆ†é…é‡è¦æ€§ã€‚ä¸ç°æœ‰çš„ä»¤ç‰Œçº§æ–¹æ³•ç›¸æ¯”ï¼ŒConceptXé€šè¿‡åŸåœ°ä»¤ç‰Œæ›¿æ¢ä¿ç•™äº†ä¸Šä¸‹æ–‡å®Œæ•´æ€§ï¼Œå¹¶æ”¯æŒçµæ´»çš„è§£é‡Šç›®æ ‡ï¼Œå¦‚æ€§åˆ«åè§ã€‚ConceptXæ—¢å¯ç”¨äºé€šè¿‡æ­éœ²åè§æ¥æºè¿›è¡Œå®¡è®¡ï¼Œåˆå¯é€šè¿‡ä¿®æ”¹æç¤ºæ¥æ”¹å˜æƒ…ç»ªæˆ–å‡å°‘LLMå“åº”çš„å±å®³æ€§ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ConceptXåœ¨ä¿¡ä»°æ€§å’Œäººç±»ä¸€è‡´æ€§æ–¹é¢å‡ä¼˜äºTokenSHAPç­‰ä»¤ç‰Œçº§æ–¹æ³•ã€‚å¼•å¯¼ä»»åŠ¡ä½¿æƒ…ç»ªè½¬ç§»æå‡äº†0.252ï¼Œç›¸å¯¹äºéšæœºç¼–è¾‘çš„0.131ï¼Œæ”»å‡»æˆåŠŸç‡ä»0.463é™è‡³0.242ã€‚è™½ç„¶æç¤ºå·¥ç¨‹å’Œè‡ªè§£é‡Šæ–¹æ³•æœ‰æ—¶ä¼šç”Ÿæˆæ›´å®‰å…¨çš„å“åº”ï¼Œä½†ConceptXæä¾›äº†ä¸€ä¸ªé€æ˜å’Œå¿ è¯šçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä»¥æ”¹å–„LLMçš„å®‰å…¨æ€§å’Œå¯¹é½æ€§ï¼Œå±•ç¤ºäº†åŸºäºå½’å› è§£é‡Šçš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®‰å…¨æ€§å’Œå¯¹é½æ€§é—®é¢˜å¼•å‘å…³æ³¨ã€‚</li>
<li>ConceptXæ˜¯ä¸€ç§æ¦‚å¿µçº§çš„è§£é‡Šæ–¹æ³•ï¼Œèƒ½è¯†åˆ«æç¤ºä¸­çš„å…³é”®æ¦‚å¿µå¹¶æ ¹æ®è¾“å‡ºè¯­ä¹‰ç›¸ä¼¼æ€§åˆ†é…é‡è¦æ€§ã€‚</li>
<li>ConceptXé€šè¿‡ä¿ç•™ä¸Šä¸‹æ–‡å®Œæ•´æ€§å’Œæ”¯æŒçµæ´»çš„è§£é‡Šç›®æ ‡ï¼Œå¦‚æ€§åˆ«åè§ç­‰ï¼Œä¼˜äºç°æœ‰çš„ä»¤ç‰Œçº§æ–¹æ³•ã€‚</li>
<li>ConceptXæ—¢å¯ç”¨äºå®¡è®¡ï¼ˆæ­éœ²åè§æ¥æºï¼‰ï¼Œä¹Ÿå¯ç”¨äºå¼•å¯¼LLMè¡Œä¸ºï¼Œæ”¹å˜æƒ…ç»ªæˆ–å‡å°‘å“åº”çš„å±å®³æ€§ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>åœ¨ä¿¡ä»°æ€§å’Œäººç±»ä¸€è‡´æ€§æ–¹é¢ï¼ŒConceptXä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>å¼•å¯¼ä»»åŠ¡æ˜¾ç¤ºConceptXåœ¨æƒ…ç»ªè½¬ç§»å’Œé™ä½æ”»å‡»æˆåŠŸç‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07610">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-255843d9c2fc9e717eeb7c87499df0fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a444332c34f2a19739682ee59a49fa95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e7419c2c345e69403db87f5e3830fdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49c2381c23357341339dc14eae1966ee.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MiMo-Unlocking-the-Reasoning-Potential-of-Language-Model-â€“-From-Pretraining-to-Posttraining"><a href="#MiMo-Unlocking-the-Reasoning-Potential-of-Language-Model-â€“-From-Pretraining-to-Posttraining" class="headerlink" title="MiMo: Unlocking the Reasoning Potential of Language Model â€“ From   Pretraining to Posttraining"></a>MiMo: Unlocking the Reasoning Potential of Language Model â€“ From   Pretraining to Posttraining</h2><p><strong>Authors:Xiaomi LLM-Core Team,  :, Bingquan Xia, Bowen Shen,  Cici, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, Liang Zhao, Peidian Li, Peng Wang, Shihua Yu, Shimao Chen, Weikun Wang, Wenhan Ma, Xiangwei Deng, Yi Huang, Yifan Song, Zihan Jiang, Bowen Ye, Can Cai, Chenhong He, Dong Zhang, Duo Zhang, Guoan Wang, Hao Tian, Haochen Zhao, Heng Qu, Hongshen Xu, Jun Shi, Kainan Bao, QingKai Fang, Kang Zhou, Kangyang Zhou, Lei Li, Menghang Zhu, Nuo Chen, Qiantong Wang, Shaohui Liu, Shicheng Li, Shuhao Gu, Shuhuai Ren, Shuo Liu, Sirui Deng, Weiji Zhuang, Weiwei Lv, Wenyu Yang, Xin Zhang, Xing Yong, Xing Zhang, Xingchen Song, Xinzhe Xu, Xu Wang, Yihan Yan, Yu Tu, Yuanyuan Tian, Yudong Wang, Yue Yu, Zhenru Lin, Zhichao Song, Zihao Yue</strong></p>
<p>We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base modelâ€™s reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/xiaomimimo/MiMo">https://github.com/xiaomimimo/MiMo</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†MiMo-7Bï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“ä¸ºæ¨ç†ä»»åŠ¡è®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µéƒ½è¿›è¡Œäº†ä¼˜åŒ–ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬å¢å¼ºäº†æ•°æ®é¢„å¤„ç†ç®¡é“ï¼Œé‡‡ç”¨äº†ä¸‰é˜¶æ®µæ•°æ®æ··åˆç­–ç•¥ï¼Œä»¥æé«˜åŸºç¡€æ¨¡å‹çš„æ¨ç†æ½œåŠ›ã€‚MiMo-7BåŸºç¡€ç‰ˆåœ¨25ä¸‡äº¿ä¸ªä»¤ç‰Œä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶å¢åŠ äº†å¤šä»¤ç‰Œé¢„æµ‹ç›®æ ‡ï¼Œä»¥æé«˜æ€§èƒ½å’ŒåŠ é€Ÿæ¨ç†é€Ÿåº¦ã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªåŒ…å«13ä¸‡ä¸ªå¯éªŒè¯çš„æ•°å­¦å’Œç¼–ç¨‹é—®é¢˜çš„æ•°æ®é›†ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ï¼Œé‡‡ç”¨æµ‹è¯•éš¾åº¦é©±åŠ¨çš„ä»£ç å¥–åŠ±æ–¹æ¡ˆæ¥ç¼“è§£ç¨€ç–å¥–åŠ±é—®é¢˜ï¼Œå¹¶é‡‡ç”¨æˆ˜ç•¥æ•°æ®é‡é‡‡æ ·æ¥ç¨³å®šè®­ç»ƒã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒMiMo-7BåŸºç¡€ç‰ˆå…·æœ‰å‡ºè‰²çš„æ¨ç†æ½œåŠ›ï¼Œç”šè‡³è¶…è¿‡äº†è®¸å¤šæ›´å¤§çš„32Bæ¨¡å‹ã€‚æœ€ç»ˆçš„å¼ºåŒ–å­¦ä¹ è°ƒä¼˜æ¨¡å‹MiMo-7B-RLåœ¨æ•°å­¦ã€ä»£ç å’Œé€šç”¨æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†OpenAI o1-miniçš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹çš„æ£€æŸ¥ç‚¹ä½äº<a target="_blank" rel="noopener" href="https://github.com/xiaomimimo/MiMo">https://github.com/xiaomimimo/MiMo</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07608v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MiMo-7Bæ˜¯ä¸€æ¬¾é’ˆå¯¹æ¨ç†ä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä¼˜åŒ–é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µå¢å¼ºå…¶æ€§èƒ½ã€‚å®ƒé‡‡ç”¨ä¸‰é˜¶æ®µæ•°æ®æ··åˆç­–ç•¥è¿›è¡Œé¢„è®­ç»ƒï¼Œå¢åŠ Multi-Token Predictionç›®æ ‡æå‡æ€§èƒ½åŠæ¨ç†é€Ÿåº¦ã€‚åŒæ—¶ï¼Œå¯¹å¯éªŒè¯çš„æ•°å­¦ä¸ç¼–ç¨‹é—®é¢˜é›†è¿›è¡Œå¾®è°ƒè®­ç»ƒï¼Œå¹¶é‡‡ç”¨æµ‹è¯•éš¾åº¦é©±åŠ¨çš„ä»£ç å¥–åŠ±æ–¹æ¡ˆåŠæˆ˜ç•¥æ•°æ®é‡é‡‡æ ·æŠ€æœ¯è§£å†³è®­ç»ƒé—®é¢˜ã€‚MiMo-7Bæ¨¡å‹è¡¨ç°å“è¶Šï¼Œè¶…è¶Šè®¸å¤šæ›´å¤§çš„æ¨¡å‹ã€‚æ¨¡å‹æ£€æŸ¥ç‚¹å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MiMo-7Bæ˜¯ä¸€ä¸ªç”¨äºæ¨ç†ä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡å¢å¼ºæ•°æ®é¢„å¤„ç†ç®¡é“å’Œä¸‰é˜¶æ®µæ•°æ®æ··åˆç­–ç•¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>MiMo-7Båœ¨é¢„è®­ç»ƒä¸­åŠ å…¥Multi-Token Predictionç›®æ ‡ä»¥æå‡æ€§èƒ½å’Œæ¨ç†é€Ÿåº¦ã€‚</li>
<li>åœ¨å¾®è°ƒé˜¶æ®µï¼Œä½¿ç”¨æ•°å­¦å’Œç¼–ç¨‹é—®é¢˜çš„æ•°æ®é›†è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚</li>
<li>é‡‡ç”¨æµ‹è¯•éš¾åº¦é©±åŠ¨çš„ä»£ç å¥–åŠ±æ–¹æ¡ˆä»¥åŠæˆ˜ç•¥æ•°æ®é‡é‡‡æ ·æŠ€æœ¯ä»¥è§£å†³è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨€ç–å¥–åŠ±é—®é¢˜ã€‚</li>
<li>MiMo-7Bæ¨¡å‹å±•ç°å‡ºå“è¶Šæ¨ç†èƒ½åŠ›ï¼Œè¶…è¶Šè®¸å¤šæ›´å¤§çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c2c34cf44bf3b2eb99d6ebe8fc6fa81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f21d754da642b6a55fe784e29a164f6b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Ophora-A-Large-Scale-Data-Driven-Text-Guided-Ophthalmic-Surgical-Video-Generation-Model"><a href="#Ophora-A-Large-Scale-Data-Driven-Text-Guided-Ophthalmic-Surgical-Video-Generation-Model" class="headerlink" title="Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video   Generation Model"></a>Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video   Generation Model</h2><p><strong>Authors:Wei Li, Ming Hu, Guoan Wang, Lihao Liu, Kaijin Zhou, Junzhi Ning, Xin Guo, Zongyuan Ge, Lixu Gu, Junjun He</strong></p>
<p>In ophthalmic surgery, developing an AI system capable of interpreting surgical videos and predicting subsequent operations requires numerous ophthalmic surgical videos with high-quality annotations, which are difficult to collect due to privacy concerns and labor consumption. Text-guided video generation (T2V) emerges as a promising solution to overcome this issue by generating ophthalmic surgical videos based on surgeon instructions. In this paper, we present Ophora, a pioneering model that can generate ophthalmic surgical videos following natural language instructions. To construct Ophora, we first propose a Comprehensive Data Curation pipeline to convert narrative ophthalmic surgical videos into a large-scale, high-quality dataset comprising over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge from a T2V model pre-trained on natural video-text datasets for privacy-preserved ophthalmic surgical video generation based on Ophora-160K. Experiments on video quality evaluation via quantitative analysis and ophthalmologist feedback demonstrate that Ophora can generate realistic and reliable ophthalmic surgical videos based on surgeon instructions. We also validate the capability of Ophora for empowering downstream tasks of ophthalmic surgical workflow understanding. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mar-cry/Ophora">https://github.com/mar-cry/Ophora</a>. </p>
<blockquote>
<p>åœ¨çœ¼ç§‘æ‰‹æœ¯ä¸­ï¼Œå¼€å‘ä¸€ä¸ªèƒ½å¤Ÿè§£è¯»æ‰‹æœ¯è§†é¢‘å¹¶é¢„æµ‹åç»­æ“ä½œçš„AIç³»ç»Ÿï¼Œéœ€è¦å¤§é‡çš„å¸¦æœ‰é«˜è´¨é‡æ³¨é‡Šçš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚ç”±äºéšç§é—®é¢˜å’ŒåŠ³åŠ¨æ¶ˆè€—ï¼Œè¿™äº›è§†é¢‘çš„æ”¶é›†éå¸¸å›°éš¾ã€‚æ–‡æœ¬å¼•å¯¼çš„è§†é¢‘ç”Ÿæˆï¼ˆT2Vï¼‰ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆåº”è¿è€Œç”Ÿï¼Œå®ƒå¯ä»¥æ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Ophoraï¼Œä¸€ä¸ªå¯ä»¥æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘çš„å¼€åˆ›æ€§æ¨¡å‹ã€‚ä¸ºäº†æ„å»ºOphoraï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®æ•´ç†ç®¡é“ï¼Œå°†å™è¿°æ€§çœ¼ç§‘æ‰‹æœ¯è§†é¢‘è½¬åŒ–ä¸ºå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡16ä¸‡å¯¹è§†é¢‘æŒ‡ä»¤å¯¹ï¼Œå‘½åä¸ºOphora-160Kã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†æ¸è¿›å¼è§†é¢‘æŒ‡ä»¤è°ƒæ•´æ–¹æ¡ˆï¼Œä»¥ä»T2Væ¨¡å‹é¢„è®­ç»ƒçš„è‡ªç„¶è§†é¢‘æ–‡æœ¬æ•°æ®é›†ä¸­è½¬ç§»ä¸°å¯Œçš„æ—¶ç©ºçŸ¥è¯†ï¼ŒåŸºäºOphora-160Kè¿›è¡Œéšç§ä¿æŠ¤çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ç”Ÿæˆã€‚é€šè¿‡å¯¹è§†é¢‘è´¨é‡çš„å®šé‡åˆ†æå’Œçœ¼ç§‘åŒ»ç”Ÿçš„åé¦ˆè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒOphoraå¯ä»¥æ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”Ÿæˆç°å®å’Œå¯é çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†Ophoraåœ¨æ‰§è¡Œçœ¼ç§‘æ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£ç­‰ä¸‹æ¸¸ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/mar-cry/Ophora%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/mar-cry/Ophoraè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07449v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºOphoraçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚ä¸ºè§£å†³é«˜è´¨é‡çœ¼ç§‘æ‰‹æœ¯è§†é¢‘æ•°æ®éš¾ä»¥æ”¶é›†çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†å¤§è§„æ¨¡æ•°æ®é›†Ophora-160Kï¼Œå¹¶æå‡ºäº†ä¸€ç§å…¨é¢çš„æ•°æ®æ•´ç†æµç¨‹ã€‚æ­¤å¤–ï¼Œå›¢é˜Ÿè¿˜æå‡ºäº†ä¸€ç§æ¸è¿›çš„è§†é¢‘æŒ‡ä»¤å¾®è°ƒæ–¹æ¡ˆï¼Œå°†ä¸°å¯Œçš„æ—¶ç©ºçŸ¥è¯†ä»é¢„è®­ç»ƒäºè‡ªç„¶è§†é¢‘æ–‡æœ¬æ•°æ®é›†çš„T2Væ¨¡å‹è¿ç§»åˆ°Ophoraä¸­ã€‚å®éªŒè¡¨æ˜ï¼ŒOphoraå¯ä»¥æ ¹æ®åŒ»å¸ˆæŒ‡ä»¤ç”ŸæˆçœŸå®å¯é çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ï¼Œå¹¶èƒ½èµ‹èƒ½çœ¼ç§‘æ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£ç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Ophoraæ¨¡å‹èƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ï¼Œè§£å†³äº†é«˜è´¨é‡çœ¼ç§‘æ‰‹æœ¯è§†é¢‘æ•°æ®éš¾ä»¥æ”¶é›†çš„é—®é¢˜ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†Ophora-160Kï¼Œç”¨äºè®­ç»ƒå’ŒéªŒè¯Ophoraæ¨¡å‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å…¨é¢çš„æ•°æ®æ•´ç†æµç¨‹ï¼Œç”¨äºä»å™äº‹çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ä¸­ç”Ÿæˆé«˜è´¨é‡çš„æ•°æ®é›†ã€‚</li>
<li>é‡‡ç”¨æ¸è¿›çš„è§†é¢‘æŒ‡ä»¤å¾®è°ƒæ–¹æ¡ˆï¼Œå°†é¢„è®­ç»ƒçš„T2Væ¨¡å‹çŸ¥è¯†è¿ç§»åˆ°Ophoraæ¨¡å‹ä¸­ã€‚</li>
<li>å®éªŒè¯æ˜äº†Ophoraåœ¨ç”ŸæˆçœŸå®å¯é çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>Ophoraæœ‰èƒ½åŠ›èµ‹èƒ½çœ¼ç§‘æ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£ç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d296e6dd7064fc361f5299766edb5eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4ec3b71a8c06c8f6d824bb3db7290c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cca6f327a9e0a5dece7366cde8a1f565.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LEAD-Iterative-Data-Selection-for-Efficient-LLM-Instruction-Tuning"><a href="#LEAD-Iterative-Data-Selection-for-Efficient-LLM-Instruction-Tuning" class="headerlink" title="LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning"></a>LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning</h2><p><strong>Authors:Xiaotian Lin, Yanlin Qi, Yizhang Zhu, Themis Palpanas, Chengliang Chai, Nan Tang, Yuyu Luo</strong></p>
<p>Instruction tuning has emerged as a critical paradigm for improving the capabilities and alignment of large language models (LLMs). However, existing iterative model-aware data selection methods incur significant computational overhead, as they rely on repeatedly performing full-dataset model inference to estimate sample utility for subsequent training iterations, creating a fundamental efficiency bottleneck. In this paper, we propose LEAD, an efficient iterative data selection framework that accurately estimates sample utility entirely within the standard training loop, eliminating the need for costly additional model inference. At its core, LEAD introduces Instance-Level Dynamic Uncertainty (IDU), a theoretically grounded utility function combining instantaneous training loss, gradient-based approximation of loss changes, and exponential smoothing of historical loss signals. To further scale efficiently to large datasets, LEAD employs a two-stage, coarse-to-fine selection strategy, adaptively prioritizing informative clusters through a multi-armed bandit mechanism, followed by precise fine-grained selection of high-utility samples using IDU. Extensive experiments across four diverse benchmarks show that LEAD significantly outperforms state-of-the-art methods, improving average model performance by 6.1%-10.8% while using only 2.5% of the training data and reducing overall training time by 5-10x. </p>
<blockquote>
<p>æŒ‡ä»¤è°ƒæ•´å·²æˆä¸ºä¸€ä¸ªæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½åŠ›å’Œå¯¹é½çš„å…³é”®èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¿­ä»£æ¨¡å‹æ„ŸçŸ¥æ•°æ®é€‰æ‹©æ–¹æ³•ä¼šäº§ç”Ÿå·¨å¤§çš„è®¡ç®—å¼€é”€ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºåå¤æ‰§è¡Œå…¨æ•°æ®é›†æ¨¡å‹æ¨ç†æ¥ä¼°è®¡æ ·æœ¬æ•ˆç”¨ï¼Œä»¥ä¾›åç»­è®­ç»ƒè¿­ä»£ä½¿ç”¨ï¼Œè¿™åˆ›å»ºäº†ä¸€ä¸ªåŸºæœ¬çš„æ•ˆç‡ç“¶é¢ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LEADï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆè¿­ä»£æ•°æ®é€‰æ‹©æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨æ ‡å‡†è®­ç»ƒå¾ªç¯å†…å‡†ç¡®ä¼°è®¡æ ·æœ¬æ•ˆç”¨ï¼Œæ— éœ€è¿›è¡Œæ˜‚è´µçš„é¢å¤–æ¨¡å‹æ¨ç†ã€‚LEADçš„æ ¸å¿ƒæ˜¯å¼•å…¥å®ä¾‹çº§åŠ¨æ€ä¸ç¡®å®šæ€§ï¼ˆIDUï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆå³æ—¶è®­ç»ƒæŸå¤±ã€æŸå¤±å˜åŒ–çš„æ¢¯åº¦è¿‘ä¼¼å’Œå†å²æŸå¤±ä¿¡å·çš„æŒ‡æ•°å¹³æ»‘çš„ç†è®ºåŸºç¡€æ•ˆç”¨å‡½æ•°ã€‚ä¸ºäº†è¿›ä¸€æ­¥æœ‰æ•ˆåœ°æ‰©å±•åˆ°å¤§å‹æ•°æ®é›†ï¼ŒLEADé‡‡ç”¨ä»ç²—åˆ°ç»†çš„ä¸¤é˜¶æ®µé€‰æ‹©ç­–ç•¥ï¼Œé€šè¿‡å¤šè‡‚è€è™æœºæœºåˆ¶è‡ªé€‚åº”åœ°ä¼˜å…ˆå¤„ç†ä¿¡æ¯ä¸°å¯Œçš„é›†ç¾¤ï¼Œç„¶åä½¿ç”¨IDUè¿›è¡Œç²¾ç¡®çš„é«˜æ•ˆç”¨æ ·æœ¬çš„ç²¾ç»†ç²’åº¦é€‰æ‹©ã€‚åœ¨å››ä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLEADæ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œåœ¨ä»…ä½¿ç”¨2.5%çš„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¹³å‡æ¨¡å‹æ€§èƒ½æé«˜6.1%-10.8%ï¼ŒåŒæ—¶æ€»ä½“è®­ç»ƒæ—¶é—´å‡å°‘5-10å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07437v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›å’Œå¯¹é½é—®é¢˜ï¼ŒæŒ‡ä»¤å¾®è°ƒå·²æˆä¸ºä¸€ç§å…³é”®èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¿­ä»£æ¨¡å‹æ„ŸçŸ¥æ•°æ®é€‰æ‹©æ–¹æ³•äº§ç”Ÿäº†æ˜¾è‘—çš„è®¡ç®—å¼€é”€ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºè¿›è¡Œå…¨æ•°æ®é›†æ¨¡å‹æ¨ç†æ¥ä¼°è®¡æ ·æœ¬æ•ˆç”¨ï¼Œä»è€Œä¸ºåç»­çš„è¿­ä»£è®­ç»ƒåˆ›å»ºæ•ˆç‡ç“¶é¢ˆã€‚æœ¬æ–‡æå‡ºäº†LEADï¼Œä¸€ç§é«˜æ•ˆè¿­ä»£æ•°æ®é€‰æ‹©æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨æ ‡å‡†è®­ç»ƒå¾ªç¯å†…å‡†ç¡®ä¼°è®¡æ ·æœ¬æ•ˆç”¨ï¼Œæ— éœ€æ˜‚è´µçš„é¢å¤–æ¨¡å‹æ¨ç†ã€‚å…¶æ ¸å¿ƒåœ¨äºå¼•å…¥åŸºäºå®ä¾‹çš„åŠ¨æ€ä¸ç¡®å®šæ€§ï¼ˆIDUï¼‰ï¼Œä¸€ä¸ªç»“åˆå³æ—¶è®­ç»ƒæŸå¤±ã€åŸºäºæ¢¯åº¦çš„æŸå¤±å˜åŒ–è¿‘ä¼¼å’Œå†å²æŸå¤±ä¿¡å·çš„æŒ‡æ•°å¹³æ»‘çš„ç†è®ºåŸºç¡€æ•ˆç”¨å‡½æ•°ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„æ•ˆç‡ï¼ŒLEADé‡‡ç”¨ä»ç²—åˆ°ç»†çš„äºŒçº§é€‰æ‹©ç­–ç•¥ï¼Œé€šè¿‡å¤šè‡‚è€è™æœºæœºåˆ¶è‡ªé€‚åº”åœ°ä¼˜å…ˆè€ƒè™‘ä¿¡æ¯ä¸°å¯Œçš„èšç±»ï¼Œç„¶åä½¿ç”¨IDUè¿›è¡Œé«˜æ•ˆç”¨æ ·æœ¬çš„ç²¾ç»†ç²’åº¦é€‰æ‹©ã€‚å®éªŒè¯æ˜ï¼ŒLEADæ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œå¹³å‡æ¨¡å‹æ€§èƒ½æå‡6.1%-10.8%ï¼ŒåŒæ—¶ä½¿ç”¨çš„æ•°æ®é‡ä»…å 2.5%ï¼Œæ•´ä½“è®­ç»ƒæ—¶é—´å‡å°‘5-10å€ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æŒ‡ä»¤å¾®è°ƒå·²æˆä¸ºæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹èƒ½åŠ›å’Œå¯¹é½çš„å…³é”®èŒƒå¼ã€‚</li>
<li>ç°æœ‰æ•°æ®é€‰æ‹©æ–¹æ³•å­˜åœ¨è®¡ç®—å¼€é”€å¤§ï¼Œå› ä¸ºéœ€è¦åå¤è¿›è¡Œå…¨æ•°æ®é›†æ¨¡å‹æ¨ç†ã€‚</li>
<li>LEADæ¡†æ¶æå‡ºä¸€ç§é«˜æ•ˆè¿­ä»£æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œå¯åœ¨æ ‡å‡†è®­ç»ƒå¾ªç¯å†…å‡†ç¡®ä¼°è®¡æ ·æœ¬æ•ˆç”¨ã€‚</li>
<li>LEADå¼•å…¥åŸºäºå®ä¾‹çš„åŠ¨æ€ä¸ç¡®å®šæ€§ï¼ˆIDUï¼‰ä½œä¸ºæ•ˆç”¨å‡½æ•°ï¼Œç»“åˆå³æ—¶è®­ç»ƒæŸå¤±ã€æ¢¯åº¦å˜åŒ–å’ŒæŸå¤±ä¿¡å·å¹³æ»‘ã€‚</li>
<li>LEADé‡‡ç”¨ä»ç²—åˆ°ç»†çš„äºŒçº§é€‰æ‹©ç­–ç•¥ï¼Œå…ˆè€ƒè™‘ä¿¡æ¯ä¸°å¯Œçš„èšç±»ï¼Œå†ç²¾ç»†é€‰æ‹©é«˜æ•ˆç”¨æ ·æœ¬ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºLEADæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæé«˜æ¨¡å‹æ€§èƒ½å¹¶å¤§å¹…å‡å°‘æ•°æ®å’Œæ—¶é—´æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07437">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0423072057ea3b86709698937ad3d209.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-66d9d2e1c5c2dfd9df45440d7f47b4b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a8b86cb29a81d545390dd4275db1ebb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-167253f31b5f68db5065ff70459f53c1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="L-SWAG-Layer-Sample-Wise-Activation-with-Gradients-information-for-Zero-Shot-NAS-on-Vision-Transformers"><a href="#L-SWAG-Layer-Sample-Wise-Activation-with-Gradients-information-for-Zero-Shot-NAS-on-Vision-Transformers" class="headerlink" title="L-SWAG: Layer-Sample Wise Activation with Gradients information for   Zero-Shot NAS on Vision Transformers"></a>L-SWAG: Layer-Sample Wise Activation with Gradients information for   Zero-Shot NAS on Vision Transformers</h2><p><strong>Authors:Sofia Casarin, Sergio Escalera, Oswald Lanz</strong></p>
<p>Training-free Neural Architecture Search (NAS) efficiently identifies high-performing neural networks using zero-cost (ZC) proxies. Unlike multi-shot and one-shot NAS approaches, ZC-NAS is both (i) time-efficient, eliminating the need for model training, and (ii) interpretable, with proxy designs often theoretically grounded. Despite rapid developments in the field, current SOTA ZC proxies are typically constrained to well-established convolutional search spaces. With the rise of Large Language Models shaping the future of deep learning, this work extends ZC proxy applicability to Vision Transformers (ViTs). We present a new benchmark using the Autoformer search space evaluated on 6 distinct tasks and propose Layer-Sample Wise Activation with Gradients information (L-SWAG), a novel, generalizable metric that characterizes both convolutional and transformer architectures across 14 tasks. Additionally, previous works highlighted how different proxies contain complementary information, motivating the need for a ML model to identify useful combinations. To further enhance ZC-NAS, we therefore introduce LIBRA-NAS (Low Information gain and Bias Re-Alignment), a method that strategically combines proxies to best represent a specific benchmark. Integrated into the NAS search, LIBRA-NAS outperforms evolution and gradient-based NAS techniques by identifying an architecture with a 17.0% test error on ImageNet1k in just 0.1 GPU days. </p>
<blockquote>
<p>æ— è®­ç»ƒç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰åˆ©ç”¨é›¶æˆæœ¬ï¼ˆZCï¼‰ä»£ç†æœ‰æ•ˆåœ°è¯†åˆ«é«˜æ€§èƒ½ç¥ç»ç½‘ç»œã€‚ä¸å¤šé•œå¤´å’Œå•é•œå¤´NASæ–¹æ³•ä¸åŒï¼ŒZC-NASæ—¢ï¼ˆiï¼‰çœæ—¶ï¼Œæ— éœ€æ¨¡å‹è®­ç»ƒï¼Œåˆï¼ˆiiï¼‰å…·æœ‰å¯è§£é‡Šæ€§ï¼Œä»£ç†è®¾è®¡é€šå¸¸ç†è®ºæ‰å®ã€‚å°½ç®¡è¯¥é¢†åŸŸå‘å±•è¿…é€Ÿï¼Œä½†å½“å‰çš„SOTA ZCä»£ç†é€šå¸¸ä»…é™äºæˆç†Ÿçš„å·ç§¯æœç´¢ç©ºé—´ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹å¡‘é€ æ·±åº¦å­¦ä¹ æœªæ¥ï¼Œè¿™é¡¹å·¥ä½œå°†ZCä»£ç†çš„åº”ç”¨æ‰©å±•åˆ°è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ã€‚æˆ‘ä»¬åˆ©ç”¨Autoformeræœç´¢ç©ºé—´åœ¨6ä¸ªä¸åŒä»»åŠ¡ä¸Šå»ºç«‹äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶æå‡ºäº†å¸¦æœ‰æ¢¯åº¦ä¿¡æ¯çš„å±‚é‡‡æ ·æ¿€æ´»ï¼ˆL-SWAGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ã€å¯æ¨å¹¿çš„æŒ‡æ ‡ï¼Œå¯ä»¥è¡¨å¾14ä¸ªä»»åŠ¡çš„å·ç§¯å’Œè½¬æ¢å™¨æ¶æ„ã€‚æ­¤å¤–ï¼Œå…ˆå‰çš„ç ”ç©¶å¼ºè°ƒäº†ä¸åŒä»£ç†åŒ…å«äº’è¡¥ä¿¡æ¯ï¼Œè¿™çªå‡ºäº†éœ€è¦ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹æ¥è¯†åˆ«æœ‰ç”¨ç»„åˆçš„éœ€è¦ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹è¿›ZC-NASï¼Œæˆ‘ä»¬å› æ­¤å¼•å…¥äº†LIBRA-NASï¼ˆä½ä¿¡æ¯å¢ç›Šå’Œåå·®é‡æ–°å¯¹é½ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç­–ç•¥æ€§åœ°ç»“åˆä»£ç†ä»¥æœ€å¥½åœ°ä»£è¡¨ç‰¹å®šåŸºå‡†æµ‹è¯•çš„æ–¹æ³•ã€‚é›†æˆåˆ°NASæœç´¢ä¸­ï¼ŒLIBRA-NASåœ¨ImageNet1kä¸Šçš„æµ‹è¯•é”™è¯¯ç‡ä¸º17.0%ï¼Œåœ¨ä»…0.1ä¸ªGPUå¤©æ•°çš„æœç´¢ä¸­è¶…è¿‡äº†è¿›åŒ–ç­–ç•¥å’ŒåŸºäºæ¢¯åº¦çš„NASæŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07300v1">PDF</a> accepted at CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>é«˜æ•ˆã€æ— éœ€è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰æ–¹æ³•é€šè¿‡é›¶æˆæœ¬ï¼ˆZCï¼‰ä»£ç†æœ‰æ•ˆåœ°è¯†åˆ«é«˜æ€§èƒ½ç¥ç»ç½‘ç»œã€‚æœ¬æ–‡å°†ZC-NASæ‰©å±•åˆ°ç”¨äºè¯†åˆ«æœªæ¥æ·±åº¦å­¦ä¹ ä¸­å¤§å‹è¯­è¨€æ¨¡å‹æ‰€å¹¿æ³›ä½¿ç”¨çš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•æ–¹æ³•Autoformeræœç´¢ç©ºé—´ï¼Œå¹¶åœ¨å…­ä¸ªä¸åŒä»»åŠ¡ä¸Šè¯„ä¼°ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„é€šç”¨åº¦é‡æ–¹æ³•Layer-Sample Wise Activation with Gradientsï¼ˆL-SWAGï¼‰ï¼Œç”¨äºåˆ»ç”»å·ç§¯å±‚å’ŒTransformeræ¶æ„ã€‚ä¸ºè¿›ä¸€æ­¥æå‡ZC-NASæ€§èƒ½ï¼Œå¼•å…¥äº†ä¸€ç§åä¸ºLIBRA-NASçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡ç­–ç•¥æ€§ç»“åˆä»£ç†ä»¥æœ€ä½³æ–¹å¼ä»£è¡¨ç‰¹å®šåŸºå‡†æµ‹è¯•ã€‚è¯¥æ–¹æ³•åœ¨NASæœç´¢ä¸­é›†æˆäº†LIBRA-NASï¼Œåœ¨ImageNet1kä¸Šçš„æµ‹è¯•è¯¯å·®è¾¾åˆ°17%ï¼Œä»…ä½¿ç”¨0.1 GPUå¤©çš„æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZC-NASé€šè¿‡é›¶æˆæœ¬ä»£ç†é«˜æ•ˆè¯†åˆ«é«˜æ€§èƒ½ç¥ç»ç½‘ç»œï¼Œæ— éœ€è®­ç»ƒæ¨¡å‹ï¼Œæé«˜äº†æ—¶é—´æ•ˆç‡ã€‚</li>
<li>ZCä»£ç†å¯ä»¥æ‰©å±•åˆ°è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼Œä»¥é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨ã€‚</li>
<li>æå‡ºæ–°çš„åŸºå‡†æµ‹è¯•æ–¹æ³•å’ŒAutoformeræœç´¢ç©ºé—´ï¼Œç”¨äºè¯„ä¼°ä¸åŒä»»åŠ¡ä¸Šçš„è§†è§‰è½¬æ¢å™¨æ€§èƒ½ã€‚</li>
<li>å¼•å…¥Layer-Sample Wise Activation with Gradientsï¼ˆL-SWAGï¼‰ä½œä¸ºæ–°çš„é€šç”¨åº¦é‡æ ‡å‡†ï¼Œå¯ä»¥åˆ»ç”»å·ç§¯å’ŒTransformeræ¶æ„çš„ç‰¹æ€§ã€‚</li>
<li>LIBRA-NASæ˜¯ä¸€ç§æ–°çš„ç»“åˆä»£ç†çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ€ä½³åœ°ä»£è¡¨ç‰¹å®šåŸºå‡†æµ‹è¯•ã€‚</li>
<li>LIBRA-NASåœ¨NASæœç´¢ä¸­é›†æˆäº†è¿›åŒ–ç®—æ³•å’ŒåŸºäºæ¢¯åº¦çš„æŠ€æœ¯ï¼Œåœ¨ImageNet1kä¸Šçš„æµ‹è¯•é”™è¯¯ç‡ä¸ºä»…ä½¿ç”¨å¾ˆå°‘è®¡ç®—èµ„æºçš„æ¡ä»¶ä¸‹çš„æœ€ä½³è¡¨ç°ä¹‹ä¸€ã€‚è¿™è¡¨æ˜å…¶åœ¨æå‡ç¥ç»ç½‘ç»œæ¶æ„æœç´¢æ•ˆç‡æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07300">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-59ae9106582d5e7da1f43e16b0e481d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c11f8dde969cce2fac4f85ad43eeabb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-232987c936fdc9c9870d5249e8650381.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="EmoVLM-KD-Fusing-Distilled-Expertise-with-Vision-Language-Models-for-Visual-Emotion-Analysis"><a href="#EmoVLM-KD-Fusing-Distilled-Expertise-with-Vision-Language-Models-for-Visual-Emotion-Analysis" class="headerlink" title="EmoVLM-KD: Fusing Distilled Expertise with Vision-Language Models for   Visual Emotion Analysis"></a>EmoVLM-KD: Fusing Distilled Expertise with Vision-Language Models for   Visual Emotion Analysis</h2><p><strong>Authors:SangEun Lee, Yubeen Lee, Eunil Park</strong></p>
<p>Visual emotion analysis, which has gained considerable attention in the field of affective computing, aims to predict the dominant emotions conveyed by an image. Despite advancements in visual emotion analysis with the emergence of vision-language models, we observed that instruction-tuned vision-language models and conventional vision models exhibit complementary strengths in visual emotion analysis, as vision-language models excel in certain cases, whereas vision models perform better in others. This finding highlights the need to integrate these capabilities to enhance the performance of visual emotion analysis. To bridge this gap, we propose EmoVLM-KD, an instruction-tuned vision-language model augmented with a lightweight module distilled from conventional vision models. Instead of deploying both models simultaneously, which incurs high computational costs, we transfer the predictive patterns of a conventional vision model into the vision-language model using a knowledge distillation framework. Our approach first fine-tunes a vision-language model on emotion-specific instruction data and then attaches a distilled module to its visual encoder while keeping the vision-language model frozen. Predictions from the vision language model and the distillation module are effectively balanced by a gate module, which subsequently generates the final outcome. Extensive experiments show that EmoVLM-KD achieves state-of-the-art performance on multiple visual emotion analysis benchmark datasets, outperforming the existing methods while maintaining computational efficiency. The code is available in <a target="_blank" rel="noopener" href="https://github.com/sange1104/EmoVLM-KD">https://github.com/sange1104/EmoVLM-KD</a>. </p>
<blockquote>
<p>è§†è§‰æƒ…æ„Ÿåˆ†ææ˜¯æƒ…æ„Ÿè®¡ç®—é¢†åŸŸå¤‡å—å…³æ³¨çš„ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨é¢„æµ‹å›¾åƒæ‰€ä¼ è¾¾çš„ä¸»å¯¼æƒ…æ„Ÿã€‚éšç€è§†è§‰è¯­è¨€æ¨¡å‹çš„å…´èµ·ï¼Œè§†è§‰æƒ…æ„Ÿåˆ†æé¢†åŸŸå–å¾—äº†è¿›å±•ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼ŒæŒ‡ä»¤è°ƒæ•´å‹è§†è§‰è¯­è¨€æ¨¡å‹å’Œä¼ ç»Ÿè§†è§‰æ¨¡å‹åœ¨è§†è§‰æƒ…æ„Ÿåˆ†æä¸­è¡¨ç°å‡ºäº’è¡¥çš„ä¼˜åŠ¿ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œè€Œè§†è§‰æ¨¡å‹åœ¨å¦ä¸€äº›æƒ…å†µä¸‹è¡¨ç°æ›´å¥½ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†æ•´åˆè¿™äº›èƒ½åŠ›ä»¥æé«˜è§†è§‰æƒ…æ„Ÿåˆ†ææ€§èƒ½çš„é‡è¦æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†EmoVLM-KDï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†ä¼ ç»Ÿè§†è§‰æ¨¡å‹è’¸é¦å‡ºçš„è½»é‡åŒ–æ¨¡å—çš„æŒ‡ä»¤è°ƒæ•´å‹è§†è§‰è¯­è¨€æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„åŒæ—¶éƒ¨ç½²ä¸¤ç§æ¨¡å‹çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬é‡‡ç”¨çŸ¥è¯†è’¸é¦æ¡†æ¶å°†ä¼ ç»Ÿè§†è§‰æ¨¡å‹çš„é¢„æµ‹æ¨¡å¼è½¬ç§»åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ï¼Œè¿™æ ·å¯ä»¥é¿å…é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡æƒ…æ„Ÿç‰¹å®šæŒ‡ä»¤æ•°æ®å¯¹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç„¶ååœ¨ä¿æŒè§†è§‰è¯­è¨€æ¨¡å‹ä¸å˜çš„åŒæ—¶ï¼Œå°†å…¶ä¸è’¸é¦æ¨¡å—é™„åŠ åˆ°è§†è§‰ç¼–ç å™¨ä¸Šã€‚æ¥è‡ªè§†è§‰è¯­è¨€æ¨¡å‹å’Œè’¸é¦æ¨¡å—çš„é¢„æµ‹é€šè¿‡ä¸€ä¸ªé—¨æ¨¡å—è¿›è¡Œæœ‰æ•ˆçš„å¹³è¡¡ï¼Œä»è€Œç”Ÿæˆæœ€ç»ˆçš„ç»“æœã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEmoVLM-KDåœ¨å¤šä¸ªè§†è§‰æƒ…æ„Ÿåˆ†æåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œä¸ä»…åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¹Ÿä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/sange1104/EmoVLM-KD%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/sange1104/EmoVLM-KDè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07164v1">PDF</a> Accepted at Workshop and Competition on Affective &amp; Behavior Analysis   in-the-wild (ABAW), CVPR 2025, 10 pages, 4 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰æƒ…æ„Ÿåˆ†ææ—¨åœ¨é¢„æµ‹å›¾åƒæ‰€ä¼ è¾¾çš„ä¸»å¯¼æƒ…æ„Ÿï¼Œåœ¨æƒ…æ„Ÿè®¡ç®—é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç ”ç©¶å‘ç°ï¼ŒæŒ‡ä»¤è°ƒæ•´å‹è§†è§‰è¯­è¨€æ¨¡å‹å’Œä¼ ç»Ÿè§†è§‰æ¨¡å‹åœ¨è§†è§‰æƒ…æ„Ÿåˆ†æä¸­å…·æœ‰äº’è¡¥ä¼˜åŠ¿ã€‚ä¸ºæ­¤ï¼Œæå‡ºEmoVLM-KDæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æŒ‡ä»¤è°ƒæ•´å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦æ¡†æ¶èå…¥ä¼ ç»Ÿè§†è§‰æ¨¡å‹çš„é¢„æµ‹æ¨¡å¼ã€‚EmoVLM-KDé€šè¿‡ç²¾ç»†åŒ–è°ƒæ•´æƒ…æ„Ÿç‰¹å®šæŒ‡ä»¤æ•°æ®å¯¹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå¹¶æ·»åŠ ä¸€ä¸ªè’¸é¦æ¨¡å—åˆ°å…¶è§†è§‰ç¼–ç å™¨ä¸Šã€‚è¯¥æ¨¡å‹çš„é¢„æµ‹ç»“æœç”±é—¨æ¨¡å—æœ‰æ•ˆå¹³è¡¡ï¼Œç”Ÿæˆæœ€ç»ˆè¾“å‡ºã€‚å®éªŒè¡¨æ˜ï¼ŒEmoVLM-KDåœ¨å¤šä¸ªè§†è§‰æƒ…æ„Ÿåˆ†æåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨è®¡ç®—æ•ˆç‡ä¸Šä¿æŒä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æƒ…æ„Ÿåˆ†ææ˜¯æƒ…æ„Ÿè®¡ç®—é¢†åŸŸçš„ä¸€ä¸ªé‡è¦æ–¹å‘ï¼Œæ—¨åœ¨é¢„æµ‹å›¾åƒä¸­çš„ä¸»å¯¼æƒ…æ„Ÿã€‚</li>
<li>æŒ‡ä»¤è°ƒæ•´å‹è§†è§‰è¯­è¨€æ¨¡å‹å’Œä¼ ç»Ÿè§†è§‰æ¨¡å‹åœ¨è§†è§‰æƒ…æ„Ÿåˆ†æä¸­å…·æœ‰å„è‡ªçš„ä¼˜åŠ¿ï¼Œéœ€è¦æ•´åˆä»¥æå‡æ€§èƒ½ã€‚</li>
<li>EmoVLM-KDæ¨¡å‹ç»“åˆäº†æŒ‡ä»¤è°ƒæ•´å‹è§†è§‰è¯­è¨€æ¨¡å‹å’Œä¼ ç»Ÿè§†è§‰æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦å®ç°é«˜æ•ˆçš„æ€§èƒ½æå‡ã€‚</li>
<li>EmoVLM-KDæ¨¡å‹åŒ…å«æƒ…æ„Ÿç‰¹å®šæŒ‡ä»¤æ•°æ®çš„ç²¾ç»†åŒ–è°ƒæ•´ã€è’¸é¦æ¨¡å—å’Œé—¨æ¨¡å—ä¸‰ä¸ªå…³é”®éƒ¨åˆ†ã€‚</li>
<li>EmoVLM-KDåœ¨å¤šä¸ªè§†è§‰æƒ…æ„Ÿåˆ†æåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>EmoVLM-KDæ¨¡å‹çš„ä»£ç å·²å…¬å¼€å¯è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6dcf79e54a53d5dbf4667f17de0ec969.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47f879e0d22eb6f6d4cd0ad69c9df3dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd99fede146353ee6b956807f82bffc2.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Visual-Instruction-Tuning-with-Chain-of-Region-of-Interest"><a href="#Visual-Instruction-Tuning-with-Chain-of-Region-of-Interest" class="headerlink" title="Visual Instruction Tuning with Chain of Region-of-Interest"></a>Visual Instruction Tuning with Chain of Region-of-Interest</h2><p><strong>Authors:Yixin Chen, Shuai Zhang, Boran Han, Bernie Wang</strong></p>
<p>High-resolution (HR) images are pivotal for enhancing the recognition and understanding capabilities of multimodal large language models (MLLMs). However, directly increasing image resolution can significantly escalate computational demands. In this study, we propose a method called Chain of Region-of-Interest (CoRoI) for Visual Instruction Tuning, aimed at alleviating the computational burden associated with high-resolution images for MLLMs. Drawing inspiration from the selective nature of the human visual system, we recognize that not all regions within high-resolution images carry equal importance. CoRoI seeks to identify and prioritize the most informative regions, thereby enhancing multimodal visual comprehension and recognition while circumventing the need for processing lengthy HR image tokens. Through extensive experiments on 11 benchmarks, we validate the efficacy of CoRoI across varying sizes, ranging from 7B to 34B in parameters. Our models consistently demonstrate superior performance across diverse multimodal benchmarks and tasks. Notably, our method outperforms LLaVA-NeXT on almost all benchmarks and our finetuned 34B model surpasses proprietary methods like Gemini Pro 1.0 on six benchmarks, as well as outperforming GPT-4V on MMB, SEED-I, and MME. </p>
<blockquote>
<p>é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰å›¾åƒå¯¹äºæé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¯†åˆ«å’Œç†è§£èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç›´æ¥å¢åŠ å›¾åƒåˆ†è¾¨ç‡ä¼šå¤§å¹…å¢åŠ è®¡ç®—éœ€æ±‚ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºâ€œåŸºäºæ„Ÿå…´è¶£åŒºåŸŸçš„é“¾â€ï¼ˆCoRoIï¼‰çš„è§†è§‰æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨ç¼“è§£å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒçš„è®¡ç®—è´Ÿæ‹…ã€‚æˆ‘ä»¬ä»äººç±»è§†è§‰ç³»ç»Ÿçš„é€‰æ‹©æ€§ç‰¹ç‚¹ä¸­æ±²å–çµæ„Ÿï¼Œè®¤è¯†åˆ°é«˜åˆ†è¾¨ç‡å›¾åƒä¸­çš„å„ä¸ªåŒºåŸŸå¹¶éåŒç­‰é‡è¦ã€‚CoRoIæ—¨åœ¨è¯†åˆ«å¹¶ä¼˜å…ˆå¤„ç†æœ€å…·ä¿¡æ¯é‡çš„åŒºåŸŸï¼Œä»è€Œæé«˜å¤šæ¨¡æ€è§†è§‰çš„ç†è§£å’Œè¯†åˆ«èƒ½åŠ›ï¼ŒåŒæ—¶é¿å…å¤„ç†å†—é•¿çš„HRå›¾åƒæ ‡è®°çš„éœ€è¦ã€‚åœ¨æ¶µç›–å¤šç§è§„æ¨¡å’Œä»»åŠ¡çš„å…±åä¸€é¡¹åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬éªŒè¯äº†CoRoIçš„æœ‰æ•ˆæ€§ï¼Œæ¨¡å‹å‚æ•°ä»æ•°åäº¿åˆ°æ•°åäº¿ä¸ç­‰ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å„ç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•å’Œä»»åŠ¡ä¸­å§‹ç»ˆè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡ ä¹åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­éƒ½ä¼˜äºLLaVA-NeXTï¼Œæˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†Gemini Pro 1.0ç­‰ä¸“æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨MMBã€SEED-Iå’ŒMMEä¸Šè¶…è¿‡äº†GPT-4Vã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06840v1">PDF</a> N&#x2F;A</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºCoRoIçš„è§†è§‰æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶é¢ä¸´çš„è®¡ç®—è´Ÿæ‹…é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡è¯†åˆ«å¹¶ä¼˜å…ˆå¤„ç†é«˜åˆ†è¾¨å›¾åƒä¸­æœ€å…·ä¿¡æ¯é‡çš„åŒºåŸŸï¼Œå¢å¼ºå¤šæ¨¡æ€è§†è§‰ç†è§£å’Œè¯†åˆ«èƒ½åŠ›ï¼Œè€Œæ— éœ€å¤„ç†å†—é•¿çš„å›¾åƒæ ‡è®°ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€è§†è§‰é¢†åŸŸçš„ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•çš„æ•ˆç‡å¾—åˆ°äº†æœ‰æ•ˆéªŒè¯ï¼Œå¤§å¹…æé«˜äº†å›¾åƒè¯†åˆ«çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œç›¸è¾ƒäºå…¶ä»–é¡¶å°–æ¨¡å‹å¦‚LLaVA-NeXTå’ŒGPT-4Vç­‰ï¼Œè¯¥æ–¹æ³•çš„ä¼˜åŠ¿åœ¨äºæå‡äº†åœ¨å¤šæ¨¡æ€ä»»åŠ¡çš„æ³›åŒ–æ€§èƒ½ï¼Œåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸­éƒ½æœ‰ç¨³å®šçš„è¡¨ç°ã€‚éšç€æ¨¡å‹çš„è§„æ¨¡å¢å¤§ï¼Œè¿™ç§ä¼˜åŠ¿ä¼šæ›´åŠ æ˜æ˜¾ã€‚è¯¥ç ”ç©¶çš„å‘ç°æœ‰æœ›æ¨åŠ¨å¤šæ¨¡æ€è§†è§‰ç†è§£æŠ€æœ¯çš„è¿›ä¸€æ­¥åº”ç”¨å’Œå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CoRoIæ–¹æ³•æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶çš„è®¡ç®—è´Ÿæ‹…é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•è¯†åˆ«å¹¶ä¼˜å…ˆå¤„ç†æœ€å…·ä¿¡æ¯é‡çš„å›¾åƒåŒºåŸŸï¼Œä»¥å®ç°å¯¹å›¾åƒçš„ç†è§£å’Œè¯†åˆ«çš„é«˜æ•ˆè¿‡ç¨‹ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºäº†å‡ºè‰²çš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06840">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3e515741946085de45d18e3dd9441312.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29cb5faa764e061a5286922031048b8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4eae2388eddadffccaa4c7b4eef7858.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a088f287d1dc26774a59d110191011e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cdf507ba06df1b0e6a71085c655b546.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8fbae38c06e8c650ded05b3dd7a6f6b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Attention-Is-Not-All-You-Need-The-Importance-of-Feedforward-Networks-in-Transformer-Models"><a href="#Attention-Is-Not-All-You-Need-The-Importance-of-Feedforward-Networks-in-Transformer-Models" class="headerlink" title="Attention Is Not All You Need: The Importance of Feedforward Networks in   Transformer Models"></a>Attention Is Not All You Need: The Importance of Feedforward Networks in   Transformer Models</h2><p><strong>Authors:Isaac Gerber</strong></p>
<p>Decoder-only transformer networks have become incredibly popular for language modeling tasks. State-of-the-art models can have over a hundred transformer blocks, containing billions of trainable parameters, and are trained on trillions of tokens of text. Each transformer block typically consists of a multi-head attention (MHA) mechanism and a two-layer fully connected feedforward network (FFN). In this paper, we examine the importance of the FFN during the model pre-training process through a series of experiments, confirming that the FFN is important to model performance. Furthermore, we show that models using a transformer block configuration with three-layer FFNs with fewer such blocks outperform the standard two-layer configuration delivering lower training loss with fewer total parameters in less time. </p>
<blockquote>
<p>è§£ç å™¨ä»…çš„è½¬æ¢å™¨ç½‘ç»œåœ¨å»ºæ¨¡ä»»åŠ¡ä¸­å˜å¾—éå¸¸å—æ¬¢è¿ã€‚æœ€å…ˆè¿›çš„æ¨¡å‹å¯ä»¥æ‹¥æœ‰è¶…è¿‡ä¸€ç™¾ä¸ªè½¬æ¢å™¨å—ï¼ŒåŒ…å«æ•°åäº¿çš„å¯è®­ç»ƒå‚æ•°ï¼Œå¹¶åœ¨ä¸‡äº¿ä¸ªæ–‡æœ¬æ ‡è®°ä¸Šè¿›è¡Œè®­ç»ƒã€‚æ¯ä¸ªè½¬æ¢å™¨å—é€šå¸¸åŒ…å«ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰æœºåˆ¶å’Œä¸¤å±‚å…¨è¿æ¥çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ç³»åˆ—å®éªŒæ¥æ£€éªŒå‰é¦ˆç½‘ç»œåœ¨æ¨¡å‹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„é‡è¦æ€§ï¼Œè¯å®äº†å‰é¦ˆç½‘ç»œå¯¹æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä½¿ç”¨ä¸‰å±‚å‰é¦ˆç½‘ç»œçš„è½¬æ¢å™¨å—é…ç½®æ¯”æ ‡å‡†çš„ä¸¤å±‚é…ç½®æ›´ä¼˜ç§€ï¼Œå®ƒèƒ½åœ¨æ›´çŸ­çš„æ—¶é—´å†…ä»¥æ›´å°‘çš„æ€»å‚æ•°å®ç°æ›´ä½çš„è®­ç»ƒæŸå¤±ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06633v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè§£ç å™¨ä»…çš„è½¬æ¢å™¨ç½‘ç»œå·²æˆä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„çƒ­é—¨é€‰æ‹©ã€‚æœ€å…ˆè¿›çš„æ¨¡å‹å¯èƒ½åŒ…å«è¶…è¿‡ä¸€ç™¾ä¸ªè½¬æ¢å™¨å—ï¼Œæ•°åäº¿çš„å¯è®­ç»ƒå‚æ•°ï¼Œå¹¶åœ¨ä¸‡äº¿çº§åˆ«çš„æ–‡æœ¬æ ‡è®°ä¸Šè¿›è¡Œè®­ç»ƒã€‚æœ¬æ–‡é€šè¿‡ä¸€ç³»åˆ—å®éªŒéªŒè¯äº†å…¨è¿æ¥å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰åœ¨æ¨¡å‹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„é‡è¦æ€§ï¼Œå¹¶è¡¨æ˜é‡‡ç”¨ä¸‰å±‚FFNçš„è½¬æ¢å™¨å—é…ç½®èƒ½å¤Ÿåœ¨æ›´å°‘çš„æ—¶é—´å†…å®ç°æ›´ä½è®­ç»ƒæŸå¤±å’Œæ›´å°‘æ€»å‚æ•°çš„æ€§èƒ½è¶…è¶Šæ ‡å‡†ä¸¤å±‚é…ç½®ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è§£ç å™¨ä»…çš„è½¬æ¢å™¨ç½‘ç»œå·²æˆä¸ºæµè¡Œçš„è¯­è¨€å»ºæ¨¡å·¥å…·ã€‚</li>
<li>æœ€å…ˆè¿›çš„æ¨¡å‹åŒ…å«ä¼—å¤šè½¬æ¢å™¨å—å’Œæ•°åäº¿å¯è®­ç»ƒå‚æ•°ã€‚</li>
<li>æ¨¡å‹åœ¨å¤§é‡æ–‡æœ¬æ ‡è®°ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>å®éªŒéªŒè¯äº†å…¨è¿æ¥å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰åœ¨æ¨¡å‹é¢„è®­ç»ƒä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ä¸‰å±‚FFNçš„è½¬æ¢å™¨å—é…ç½®å¯ä»¥è¶…è¶Šæ ‡å‡†ä¸¤å±‚é…ç½®çš„æ€§èƒ½ã€‚</li>
<li>ä¼˜åŒ–é…ç½®èƒ½åœ¨æ›´å°‘çš„æ—¶é—´å†…å®ç°æ›´ä½è®­ç»ƒæŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7661a4ac95427f5a2817748e97199d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bd6d72eefc6df1e846914dcd77f0a6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bdb5397475492dfe3ca5fc0ddb1873c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9034e1874c9153e343fc7e746407e3cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c48c4f415c1c4dd2e8c53318f00113af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f84c4571efa927da25dcad9e0de5f34a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Q-Heart-ECG-Question-Answering-via-Knowledge-Informed-Multimodal-LLMs"><a href="#Q-Heart-ECG-Question-Answering-via-Knowledge-Informed-Multimodal-LLMs" class="headerlink" title="Q-Heart: ECG Question Answering via Knowledge-Informed Multimodal LLMs"></a>Q-Heart: ECG Question Answering via Knowledge-Informed Multimodal LLMs</h2><p><strong>Authors:Hung Manh Pham, Jialu Tang, Aaqib Saeed, Dong Ma</strong></p>
<p>Electrocardiography (ECG) offers critical cardiovascular insights, such as identifying arrhythmias and myocardial ischemia, but enabling automated systems to answer complex clinical questions directly from ECG signals (ECG-QA) remains a significant challenge. Current approaches often lack robust multimodal reasoning capabilities or rely on generic architectures ill-suited for the nuances of physiological signals. We introduce Q-Heart, a novel multimodal framework designed to bridge this gap. Q-Heart leverages a powerful, adapted ECG encoder and integrates its representations with textual information via a specialized ECG-aware transformer-based mapping layer. Furthermore, Q-Heart leverages dynamic prompting and retrieval of relevant historical clinical reports to guide tuning the language model toward knowledge-aware ECG reasoning. Extensive evaluations on the benchmark ECG-QA dataset show Q-Heart achieves state-of-the-art performance, outperforming existing methods by a 4% improvement in exact match accuracy. Our work demonstrates the effectiveness of combining domain-specific architectural adaptations with knowledge-augmented LLM instruction tuning for complex physiological ECG analysis, paving the way for more capable and potentially interpretable clinical patient care systems. </p>
<blockquote>
<p>å¿ƒç”µå›¾ï¼ˆECGï¼‰æä¾›äº†å…³äºå¿ƒè¡€ç®¡çš„å…³é”®æ´å¯Ÿï¼Œå¦‚è¯†åˆ«å¿ƒå¾‹å¤±å¸¸å’Œå¿ƒè‚Œç¼ºè¡€ï¼Œä½†æ˜¯ä½¿è‡ªåŠ¨åŒ–ç³»ç»Ÿç›´æ¥ä»å¿ƒç”µå›¾ä¿¡å·ï¼ˆECG-QAï¼‰å›ç­”å¤æ‚çš„ä¸´åºŠé—®é¢˜ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚å½“å‰çš„æ–¹æ³•é€šå¸¸ç¼ºä¹ç¨³å¥çš„å¤šæ¨¡å¼æ¨ç†èƒ½åŠ›ï¼Œæˆ–è€…ä¾èµ–äºå¯¹ç”Ÿç†ä¿¡å·ç»†å¾®å·®åˆ«é€‚åº”èƒ½åŠ›è¾ƒå·®çš„é€šç”¨æ¶æ„ã€‚æˆ‘ä»¬å¼•å…¥äº†Q-Heartï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å¤šæ¨¡å¼æ¡†æ¶ï¼Œæ—¨åœ¨å¼¥åˆè¿™ä¸€é¸¿æ²Ÿã€‚Q-Heartåˆ©ç”¨å¼ºå¤§çš„è‡ªé€‚åº”å¿ƒç”µå›¾ç¼–ç å™¨å’Œé€šè¿‡åŸºäºå˜å‹å™¨çš„ç‰¹æ®Šå¿ƒç”µå›¾æ„ŸçŸ¥æ˜ å°„å±‚å°†æ–‡æœ¬ä¿¡æ¯ä¸å¿ƒç”µå›¾è¡¨ç¤ºç›¸ç»“åˆã€‚æ­¤å¤–ï¼ŒQ-Heartåˆ©ç”¨åŠ¨æ€æç¤ºå’Œæ£€ç´¢ç›¸å…³çš„å†å²ä¸´åºŠæŠ¥å‘Šæ¥æŒ‡å¯¼è¯­è¨€æ¨¡å‹æœå‘çŸ¥è¯†æ„ŸçŸ¥çš„å¿ƒç”µå›¾æ¨ç†è°ƒæ•´ã€‚åœ¨ECG-QAæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼ŒQ-Heartè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œåœ¨ç²¾ç¡®åŒ¹é…å‡†ç¡®æ€§æ–¹é¢æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†4%ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜äº†ç»“åˆç‰¹å®šé¢†åŸŸçš„æ¶æ„é€‚åº”æ€§ä¸çŸ¥è¯†å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡ä»¤è°ƒæ•´å¯¹äºå¤æ‚ç”Ÿç†å¿ƒç”µå›¾åˆ†æçš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ›´å¼ºå¤§å’Œæ½œåœ¨å¯è§£é‡Šçš„ä¸´åºŠæ‚£è€…æŠ¤ç†ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06296v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¿ƒç”µå›¾ï¼ˆECGï¼‰ä¸ºå¿ƒè¡€ç®¡å¥åº·æä¾›äº†å…³é”®çš„è§è§£ï¼Œå¦‚è¯†åˆ«å¿ƒå¾‹å¤±å¸¸å’Œå¿ƒè‚Œç¼ºè¡€ç­‰ã€‚ç„¶è€Œï¼Œå¦‚ä½•è®©è‡ªåŠ¨åŒ–ç³»ç»Ÿç›´æ¥ä»å¿ƒç”µå›¾ä¿¡å·ä¸­å›ç­”å¤æ‚çš„ä¸´åºŠé—®é¢˜ï¼ˆECG-QAï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚å½“å‰çš„æ–¹æ³•è¦ä¹ˆç¼ºä¹ç¨³å¥çš„å¤šæ¨¡å¼æ¨ç†èƒ½åŠ›ï¼Œè¦ä¹ˆä¾èµ–äºä¸é€‚åˆç”Ÿç†ä¿¡å·ç»†å¾®å·®åˆ«çš„é€šç”¨æ¶æ„ã€‚æˆ‘ä»¬å¼•å…¥äº†Q-Heartï¼Œä¸€ä¸ªæ—¨åœ¨å¼¥åˆè¿™ä¸€é¸¿æ²Ÿçš„æ–°å‹å¤šæ¨¡å¼æ¡†æ¶ã€‚Q-Heartåˆ©ç”¨å¼ºå¤§çš„å¿ƒç”µå›¾ç¼–ç å™¨ï¼Œå¹¶é€šè¿‡ä¸“ä¸šçš„åŸºäºå˜å‹å™¨çš„å¿ƒç”µå›¾æ„ŸçŸ¥æ˜ å°„å±‚å°†å¿ƒç”µå›¾è¡¨ç¤ºä¸æ–‡æœ¬ä¿¡æ¯é›†æˆåœ¨ä¸€èµ·ã€‚æ­¤å¤–ï¼ŒQ-Heartåˆ©ç”¨åŠ¨æ€æç¤ºå’Œæ£€ç´¢ç›¸å…³çš„å†å²ä¸´åºŠæŠ¥å‘Šæ¥å¼•å¯¼è¯­è¨€æ¨¡å‹æœç€çŸ¥è¯†æ„ŸçŸ¥çš„å¿ƒç”µå›¾æ¨ç†æ–¹å‘è°ƒæ•´ã€‚åœ¨å¿ƒç”µå›¾é—®ç­”æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒQ-Heartè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œåœ¨ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡æ–¹é¢æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†4%ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜äº†å°†é¢†åŸŸç‰¹å®šçš„æ¶æ„é€‚åº”ä¸çŸ¥è¯†å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡ä»¤è°ƒæ•´ç›¸ç»“åˆï¼Œå¯¹äºå¤æ‚ç”Ÿç†å¿ƒç”µå›¾åˆ†æçš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ›´å¼ºå¤§å’Œå¯è§£é‡Šçš„ä¸´åºŠæ‚£è€…æŠ¤ç†ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ECGåœ¨è¯Šæ–­å¿ƒè¡€ç®¡ç–¾ç—…å¦‚å¿ƒå¾‹å¤±å¸¸å’Œå¿ƒè‚Œç¼ºè¡€æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>è‡ªåŠ¨åŒ–çš„ECGé—®ç­”ç³»ç»Ÿï¼ˆECG-QAï¼‰ä»å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡å¼æ¨ç†å’Œç”Ÿç†ä¿¡å·ç»†å¾®å·®åˆ«å¤„ç†æ–¹é¢ã€‚</li>
<li>Q-Heartæ˜¯ä¸€ä¸ªæ–°å‹å¤šæ¨¡å¼æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¿ƒç”µå›¾ç¼–ç å™¨å’ŒåŸºäºå˜å‹å™¨çš„æ˜ å°„å±‚è§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>Q-Heartåˆ©ç”¨åŠ¨æ€æç¤ºå’Œå†å²ä¸´åºŠæŠ¥å‘Šçš„æ£€ç´¢ï¼Œå¢å¼ºäº†è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>åœ¨åŸºå‡†ECG-QAæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒQ-Heartåœ¨ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡æ–¹é¢å®ç°äº†æ˜¾è‘—çš„æå‡ã€‚</li>
<li>ç ”ç©¶è¯æ˜äº†ç»“åˆç‰¹å®šé¢†åŸŸæ¶æ„è°ƒæ•´å’ŒçŸ¥è¯†å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹å¯¹äºå¤æ‚ç”Ÿç†å¿ƒç”µå›¾åˆ†æçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06296">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18f58c018c47724508de143759f7c6c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f06bf1ab25490bc5f6a850ce46a91021.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f741fe1631fb84a892694c0e33be7d06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd0c468e08c5407f39e4ce0bb3ccf964.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2129d8230ee9b220c4b16417b43d347.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8474907d26ee87ddb14b0167ecb13eae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc393d4a719af5bc938b040904c1c708.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Tell-Me-Who-Your-Students-Are-GPT-Can-Generate-Valid-Multiple-Choice-Questions-When-Studentsâ€™-Mis-Understanding-Is-Hinted"><a href="#Tell-Me-Who-Your-Students-Are-GPT-Can-Generate-Valid-Multiple-Choice-Questions-When-Studentsâ€™-Mis-Understanding-Is-Hinted" class="headerlink" title="Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice   Questions When Studentsâ€™ (Mis)Understanding Is Hinted"></a>Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice   Questions When Studentsâ€™ (Mis)Understanding Is Hinted</h2><p><strong>Authors:Machi Shimmei, Masaki Uto, Yuichiroh Matsubayashi, Kentaro Inui, Aditi Mallavarapu, Noboru Matsuda</strong></p>
<p>The primary goal of this study is to develop and evaluate an innovative prompting technique, AnaQuest, for generating multiple-choice questions (MCQs) using a pre-trained large language model. In AnaQuest, the choice items are sentence-level assertions about complex concepts. The technique integrates formative and summative assessments. In the formative phase, students answer open-ended questions for target concepts in free text. For summative assessment, AnaQuest analyzes these responses to generate both correct and incorrect assertions. To evaluate the validity of the generated MCQs, Item Response Theory (IRT) was applied to compare item characteristics between MCQs generated by AnaQuest, a baseline ChatGPT prompt, and human-crafted items. An empirical study found that expert instructors rated MCQs generated by both AI models to be as valid as those created by human instructors. However, IRT-based analysis revealed that AnaQuest-generated questions - particularly those with incorrect assertions (foils) - more closely resembled human-crafted items in terms of difficulty and discrimination than those produced by ChatGPT. </p>
<blockquote>
<p>æœ¬ç ”ç©¶çš„ä¸»è¦ç›®æ ‡æ˜¯å¼€å‘å¹¶è¯„ä¼°ä¸€ç§åˆ›æ–°æ€§çš„æç¤ºæŠ€æœ¯â€”â€”AnaQuestï¼Œè¯¥æŠ€æœ¯ä½¿ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆå¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰ã€‚åœ¨AnaQuestä¸­ï¼Œé€‰æ‹©é¡¹æ˜¯å…³äºå¤æ‚æ¦‚å¿µçš„å¥å­çº§æ–­è¨€ã€‚è¯¥æŠ€æœ¯ç»“åˆäº†å½¢æˆæ€§è¯„ä¼°å’Œç»ˆç»“æ€§è¯„ä¼°ã€‚åœ¨å½¢æˆæ€§é˜¶æ®µï¼Œå­¦ç”Ÿä»¥è‡ªç”±æ–‡æœ¬çš„å½¢å¼å›ç­”å…³äºç›®æ ‡æ¦‚å¿µçš„é—®é¢˜ã€‚åœ¨ç»ˆç»“æ€§è¯„ä¼°ä¸­ï¼ŒAnaQueståˆ†æè¿™äº›å›åº”æ¥ç”Ÿæˆæ­£ç¡®çš„å’Œé”™è¯¯çš„æ–­è¨€ã€‚ä¸ºäº†è¯„ä¼°ç”Ÿæˆçš„MCQsçš„æœ‰æ•ˆæ€§ï¼Œåº”ç”¨é¡¹ç›®ååº”ç†è®ºï¼ˆIRTï¼‰æ¥æ¯”è¾ƒAnaQuestç”Ÿæˆçš„å¤šé¡¹é€‰æ‹©é¢˜ã€åŸºçº¿ChatGPTæç¤ºå’Œäººç±»åˆ¶ä½œçš„é¡¹ç›®ä¹‹é—´çš„é¡¹ç›®ç‰¹å¾ã€‚å®è¯ç ”ç©¶å‘ç°åœ¨ä¸“å®¶è¯„ä¼°ä¸­ï¼Œäººå·¥æ™ºèƒ½æ¨¡å‹ç”Ÿæˆçš„MCQsä¸äººç±»æ•™å¸ˆåˆ›å»ºçš„é¢˜ç›®ä¸€æ ·æœ‰æ•ˆã€‚ç„¶è€Œï¼ŒåŸºäºIRTçš„åˆ†ææ˜¾ç¤ºï¼ŒAnaQuestç”Ÿæˆçš„é—®é¢˜â€”â€”ç‰¹åˆ«æ˜¯é‚£äº›å¸¦æœ‰é”™è¯¯æ–­è¨€ï¼ˆä¼ªè£…ï¼‰çš„é—®é¢˜â€”â€”åœ¨éš¾åº¦å’ŒåŒºåˆ†åº¦æ–¹é¢æ›´æ¥è¿‘äºäººç±»åˆ¶ä½œçš„é¢˜ç›®ï¼Œè€Œä¸æ˜¯ChatGPTç”Ÿæˆçš„é¢˜ç›®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05815v1">PDF</a> This is a pre-print version of a paper to appear in AIED2025</p>
<p><strong>Summary</strong></p>
<p>AnaQuestæ˜¯ä¸€é¡¹åŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ›æ–°æŠ€æœ¯ï¼Œç”¨äºç”Ÿæˆå¤šé‡é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰ã€‚å®ƒé€šè¿‡å­¦ç”Ÿåœ¨æ–‡æœ¬ä¸­å›ç­”é—®é¢˜å¹¶è‡ªåŠ¨åˆ†æå­¦ç”Ÿçš„å›ç­”æ¥ç”Ÿæˆæ­£ç¡®å’Œé”™è¯¯çš„æ–­è¨€ã€‚ç ”ç©¶è¡¨æ˜ï¼Œäººå·¥æ™ºèƒ½ç”Ÿæˆçš„MCQsä¸ä¸“å®¶æ•™å¸ˆç”Ÿæˆçš„é¢˜ç›®åœ¨æœ‰æ•ˆæ€§ä¸Šç›¸å½“ã€‚ç„¶è€Œï¼ŒåŸºäºItem Response Theoryï¼ˆIRTï¼‰çš„åˆ†ææ˜¾ç¤ºï¼ŒAnaQuestç”Ÿæˆçš„é¢˜ç›®åœ¨éš¾åº¦å’ŒåŒºåˆ†åº¦ä¸Šæ›´æ¥è¿‘äººç±»åˆ›ä½œçš„é¢˜ç›®ã€‚ç‰¹åˆ«æ˜¯æœ‰é”™è¯¯æ–­è¨€çš„é¢˜ç›®æ›´æ˜¯å¦‚æ­¤ã€‚æ€»ä½“è€Œè¨€ï¼ŒAnaQuestæ˜¯ä¸€ç§é«˜æ•ˆçš„ç”Ÿæˆæœ‰æ•ˆå¤šé‡é€‰æ‹©é¢˜çš„æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AnaQuestæ˜¯ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šé‡é€‰æ‹©é¢˜çš„åˆ›æ–°æŠ€æœ¯ã€‚</li>
<li>è¯¥æŠ€æœ¯ç»“åˆäº†å½¢æˆæ€§å’Œæ€»ç»“æ€§è¯„ä¼°ï¼Œé€šè¿‡å­¦ç”Ÿå›ç­”å¼€æ”¾æ€§é—®é¢˜å¹¶è‡ªåŠ¨åˆ†æç”Ÿæˆæ­£ç¡®å’Œé”™è¯¯çš„æ–­è¨€æ¥å·¥ä½œã€‚</li>
<li>AnaQuestç”Ÿæˆçš„é¢˜ç›®åŒ…æ‹¬å…³äºå¤æ‚æ¦‚å¿µçš„å¥å­çº§æ–­è¨€ã€‚</li>
<li>å®è¯ç ”ç©¶è¡¨æ˜ï¼ŒAIç”Ÿæˆçš„MCQsä¸ä¸“å®¶æ•™å¸ˆç”Ÿæˆçš„é¢˜ç›®åœ¨æœ‰æ•ˆæ€§ä¸Šç›¸å½“ã€‚</li>
<li>Item Response Theoryï¼ˆIRTï¼‰åˆ†ææ˜¾ç¤ºï¼ŒAnaQuestç”Ÿæˆçš„é¢˜ç›®åœ¨éš¾åº¦å’ŒåŒºåˆ†åº¦ä¸Šæ›´æ¥è¿‘äººç±»åˆ›ä½œçš„é¢˜ç›®ã€‚ç‰¹åˆ«æ˜¯åŒ…å«é”™è¯¯æ–­è¨€çš„é¢˜ç›®æ›´æ˜¯å¦‚æ­¤ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05815">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-796127144b71c2dde79b5d1ee4cdc67e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b78ae34db3d2b5dab6b10719dc61c94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8da321da25cc845e4bb45e4abb6999ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53b52141e6f246a3f96284c333732c78.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Image-Recognition-with-Online-Lightweight-Vision-Transformer-A-Survey"><a href="#Image-Recognition-with-Online-Lightweight-Vision-Transformer-A-Survey" class="headerlink" title="Image Recognition with Online Lightweight Vision Transformer: A Survey"></a>Image Recognition with Online Lightweight Vision Transformer: A Survey</h2><p><strong>Authors:Zherui Zhang, Rongtao Xu, Jie Zhou, Changwei Wang, Xingtian Pei, Wenhao Xu, Jiguang Zhang, Li Guo, Longxiang Gao, Wenbo Xu, Shibiao Xu</strong></p>
<p>The Transformer architecture has achieved significant success in natural language processing, motivating its adaptation to computer vision tasks. Unlike convolutional neural networks, vision transformers inherently capture long-range dependencies and enable parallel processing, yet lack inductive biases and efficiency benefits, facing significant computational and memory challenges that limit its real-world applicability. This paper surveys various online strategies for generating lightweight vision transformers for image recognition, focusing on three key areas: Efficient Component Design, Dynamic Network, and Knowledge Distillation. We evaluate the relevant exploration for each topic on the ImageNet-1K benchmark, analyzing trade-offs among precision, parameters, throughput, and more to highlight their respective advantages, disadvantages, and flexibility. Finally, we propose future research directions and potential challenges in the lightweighting of vision transformers with the aim of inspiring further exploration and providing practical guidance for the community. Project Page: <a target="_blank" rel="noopener" href="https://github.com/ajxklo/Lightweight-VIT">https://github.com/ajxklo/Lightweight-VIT</a> </p>
<blockquote>
<p>Transformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œè¿™ä¿ƒä½¿äººä»¬å°†å…¶é€‚åº”äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚ä¸å·ç§¯ç¥ç»ç½‘ç»œä¸åŒï¼Œè§†è§‰Transformeræœ¬è´¨ä¸Šæ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»å¹¶å…è®¸å¹¶è¡Œå¤„ç†ï¼Œä½†ç¼ºä¹å½’çº³åè§å’Œæ•ˆç‡ä¼˜åŠ¿ï¼Œé¢ä¸´è®¡ç®—é‡å’Œå†…å­˜æ–¹é¢çš„æŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨æ€§ã€‚æœ¬æ–‡ç»¼è¿°äº†åœ¨çº¿ç”Ÿæˆè½»é‡çº§è§†è§‰Transformerç”¨äºå›¾åƒè¯†åˆ«çš„å„ç§ç­–ç•¥ï¼Œé‡ç‚¹å…³æ³¨ä¸‰ä¸ªå…³é”®é¢†åŸŸï¼šé«˜æ•ˆç»„ä»¶è®¾è®¡ã€åŠ¨æ€ç½‘ç»œå’ŒçŸ¥è¯†è’¸é¦ã€‚æˆ‘ä»¬åœ¨ImageNet-1KåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æ¯ä¸ªè¯é¢˜çš„ç›¸å…³æ¢ç´¢ï¼Œåˆ†æäº†ç²¾ç¡®åº¦ã€å‚æ•°ã€ååé‡ç­‰æ–¹é¢çš„æƒè¡¡ï¼Œä»¥çªå‡ºå„è‡ªçš„ä¼˜åŠ¿ã€åŠ£åŠ¿å’Œçµæ´»æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†è½»é‡çº§è§†è§‰Transformerçš„æœªæ¥ç ”ç©¶æ–¹å‘å’Œæ½œåœ¨æŒ‘æˆ˜ï¼Œæ—¨åœ¨æ¿€å‘è¿›ä¸€æ­¥çš„æ¢ç´¢ï¼Œä¸ºç¤¾åŒºæä¾›å®é™…æŒ‡å¯¼ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/ajxklo/Lightweight-VIT">https://github.com/ajxklo/Lightweight-VIT</a></p>
</blockquote>
<hr>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03113v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°†Transformeræ¶æ„åº”ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ç­–ç•¥ï¼Œå¹¶æ¦‚è¿°äº†åœ¨çº¿ç­–ç•¥æ¥ç”Ÿæˆè½»é‡çº§è§†è§‰Transformerç”¨äºå›¾åƒè¯†åˆ«ã€‚æ–‡ç« é‡ç‚¹å…³æ³¨äº†Efficient Component Designã€Dynamic Networkå’ŒKnowledge Distillationä¸‰ä¸ªå…³é”®é¢†åŸŸï¼Œå¹¶åœ¨ImageNet-1KåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†ç›¸å…³æ¢ç´¢ã€‚æ–‡ç« åˆ†æäº†ç²¾ç¡®åº¦ã€å‚æ•°ã€ååé‡å’Œçµæ´»æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æŒ‡å‡ºäº†å„è‡ªçš„ä¼˜ç¼ºç‚¹ã€‚æœ€åï¼Œæœ¬æ–‡æå‡ºäº†è½»é‡çº§è§†è§‰Transformerçš„æœªæ¥ç ”ç©¶æ–¹å‘å’Œæ½œåœ¨æŒ‘æˆ˜ï¼Œæ—¨åœ¨ä¸ºç¤¾åŒºæä¾›å®é™…æŒ‡å¯¼å’Œè¿›ä¸€æ­¥æ¢ç´¢çš„çµæ„Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¶æ„åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„æˆåŠŸåº”ç”¨ã€‚</li>
<li>è½»é‡çº§è§†è§‰Transformerçš„ç”Ÿæˆç­–ç•¥ã€‚</li>
<li>Efficient Component Designã€Dynamic Networkå’ŒKnowledge Distillationä¸‰ä¸ªå…³é”®é¢†åŸŸçš„é‡ç‚¹ç ”ç©¶ã€‚</li>
<li>åœ¨ImageNet-1KåŸºå‡†æµ‹è¯•ä¸Šå¯¹ç›¸å…³æ¢ç´¢çš„è¯„ä¼°ã€‚</li>
<li>ç²¾ç¡®åº¦ã€å‚æ•°ã€ååé‡å’Œçµæ´»æ€§ä¹‹é—´çš„æƒè¡¡åˆ†æã€‚</li>
<li>å„é¢†åŸŸæ¢ç´¢çš„ä¼˜ç¼ºç‚¹åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8d68b5a4e31e5bfd85b909a6e0d6a94e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92ca69cc9de51d3773f2d7bb86baa851.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70040e8ba5772d36233cca82eef78b9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef94e7aeac77304479c3539d58d6a41a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90b28395b9db504dbc2e491947606c55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ab60760317aa345952e1c315f1d2547.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ReplaceMe-Network-Simplification-via-Layer-Pruning-and-Linear-Transformations"><a href="#ReplaceMe-Network-Simplification-via-Layer-Pruning-and-Linear-Transformations" class="headerlink" title="ReplaceMe: Network Simplification via Layer Pruning and Linear   Transformations"></a>ReplaceMe: Network Simplification via Layer Pruning and Linear   Transformations</h2><p><strong>Authors:Dmitriy Shopkhoev, Ammar Ali, Magauiya Zhussip, Valentin Malykh, Stamatios Lefkimmiatis, Nikos Komodakis, Sergey Zagoruyko</strong></p>
<p>We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation to approximate the pruned blocks. This estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining&#x2F;fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original modelâ€™s performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ReplaceMeï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„æ— è®­ç»ƒæ·±åº¦å‰ªææ–¹æ³•ï¼Œå®ƒå¯ä»¥é€šè¿‡çº¿æ€§è¿ç®—æœ‰æ•ˆåœ°æ›¿æ¢transformerå—ï¼ŒåŒæ—¶åœ¨ä½å‹ç¼©æ¯”çš„æƒ…å†µä¸‹ä¿æŒé«˜æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„éœ€è¦é¢å¤–è®­ç»ƒæˆ–å¾®è°ƒçš„å‰ªææ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åªéœ€è¦ä¸€ä¸ªå°å‹æ ¡å‡†æ•°æ®é›†ï¼Œç”¨äºä¼°è®¡çº¿æ€§å˜æ¢ä»¥è¿‘ä¼¼å‰ªæå—ã€‚ä¼°è®¡çš„çº¿æ€§æ˜ å°„å¯ä»¥æ— ç¼åœ°ä¸å…¶ä»–transformerå—åˆå¹¶ï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„ç½‘ç»œå‚æ•°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒReplaceMeå§‹ç»ˆä¼˜äºå…¶ä»–æ— è®­ç»ƒæ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ¶‰åŠå¤§é‡é‡æ–°è®­ç»ƒ&#x2F;å¾®è°ƒå’Œç»“æ„ä¿®æ”¹çš„å…ˆè¿›å‰ªææ–¹æ³•ä¸­è¡¨ç°å‡ºé«˜åº¦ç«äº‰åŠ›ã€‚åº”ç”¨äºå¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ï¼ŒReplaceMeå¯å®ç°é«˜è¾¾25%çš„å‰ªæç‡ï¼ŒåŒæ—¶åœ¨å¼€æ”¾åŸºå‡†æµ‹è¯•ä¸­ä¿ç•™åŸå§‹æ¨¡å‹çº¦90%çš„æ€§èƒ½â€”â€”æ— éœ€ä»»ä½•è®­ç»ƒæˆ–ä¿®å¤æ­¥éª¤ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€æœ€å°ï¼ˆè§å›¾1ï¼‰ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¼€æºåº“æ¥å®ç°ReplaceMeä»¥åŠå‡ ç§å…ˆè¿›çš„æ·±åº¦å‰ªææŠ€æœ¯ï¼Œå¯åœ¨æœ¬ä»“åº“ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02819v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æå‡ºä¸€ç§åä¸ºReplaceMeçš„é€šç”¨è®­ç»ƒæ— å…³æ·±åº¦å‰ªææ–¹æ³•ï¼Œé€šè¿‡çº¿æ€§æ“ä½œæœ‰æ•ˆæ›¿æ¢transformerå—ï¼Œåœ¨ä½å‹ç¼©æ¯”ä¸‹ä¿æŒé«˜æ€§èƒ½ã€‚ä¸å…¶ä»–éœ€è¦é¢å¤–è®­ç»ƒæˆ–å¾®è°ƒçš„ä¼ ç»Ÿå‰ªææ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ä»…éœ€è¦ä¸€ä¸ªå°çš„æ ¡å‡†æ•°æ®é›†æ¥ä¼°è®¡çº¿æ€§å˜æ¢ä»¥è¿‘ä¼¼å‰ªæå—ã€‚ä¼°è®¡çš„çº¿æ€§æ˜ å°„å¯ä»¥æ— ç¼åœ°èå…¥åˆ°å‰©ä½™çš„transformerå—ä¸­ï¼Œæ— éœ€æ·»åŠ ä»»ä½•ç½‘ç»œå‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒReplaceMeæŒç»­è¶…è¶Šå…¶ä»–è®­ç»ƒæ— å…³çš„æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ¶‰åŠå¤§é‡é‡æ–°è®­ç»ƒæˆ–ç²¾ç»†è°ƒæ•´çš„é¡¶å°–å‰ªææ–¹æ³•ä¸­è¡¨ç°æå…·ç«äº‰åŠ›ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸Šåº”ç”¨æ—¶ï¼Œå¯åœ¨æ— éœ€ä»»ä½•è®­ç»ƒæˆ–ä¿®å¤æ­¥éª¤çš„æƒ…å†µä¸‹å®ç°é«˜è¾¾25%çš„å‰ªæï¼ŒåŒæ—¶ä¿ç•™åŸå§‹æ¨¡å‹åœ¨å…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šçº¦90%çš„æ€§èƒ½ï¼Œå¹¶ä¸”è®¡ç®—å¼€é”€æœ€å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReplaceMeæ˜¯ä¸€ç§è®­ç»ƒæ— å…³çš„é€šç”¨æ·±åº¦å‰ªææ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡çº¿æ€§æ“ä½œæ›¿æ¢transformerå—ä»¥å®ç°é«˜æ€§èƒ½å’Œä½å‹ç¼©æ¯”ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ä»…éœ€è¦ä¸€ä¸ªå°çš„æ ¡å‡†æ•°æ®é›†æ¥ä¼°è®¡çº¿æ€§æ˜ å°„ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–å¾®è°ƒã€‚</li>
<li>ä¼°è®¡çš„çº¿æ€§æ˜ å°„å¯ä»¥ä¸ç°æœ‰çš„transformerå—æ— ç¼é›†æˆï¼Œæ— éœ€æ·»åŠ ä»»ä½•æ–°çš„ç½‘ç»œå‚æ•°ã€‚</li>
<li>ReplaceMeçš„å®éªŒè¡¨ç°ä¼˜äºå…¶ä»–è®­ç»ƒæ— å…³çš„æ–¹æ³•ï¼ŒåŒæ—¶åœ¨ä¸é¡¶å°–å‰ªææ–¹æ³•çš„å¯¹æ¯”ä¸­ä¹Ÿè¡¨ç°ä¸ä¿—ã€‚</li>
<li>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šåº”ç”¨æ—¶ï¼ŒReplaceMeå¯ä»¥å®ç°é«˜è¾¾25%çš„å‰ªæï¼ŒåŒæ—¶ä¿ç•™åŸå§‹æ¨¡å‹çº¦90%çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€ä»»ä½•è®­ç»ƒæˆ–ä¿®å¤æ­¥éª¤ï¼Œå¹¶ä¸”è®¡ç®—å¼€é”€è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-08276d4482adcce0108a43edf62e4146.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98d9c0b650bedd3eb3804aab8f3d8180.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a601ec0368bd50049922e2a2ab8cf411.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be722ea3beba03af9aa58f692f4e574f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ConTextual-Improving-Clinical-Text-Summarization-in-LLMs-with-Context-preserving-Token-Filtering-and-Knowledge-Graphs"><a href="#ConTextual-Improving-Clinical-Text-Summarization-in-LLMs-with-Context-preserving-Token-Filtering-and-Knowledge-Graphs" class="headerlink" title="ConTextual: Improving Clinical Text Summarization in LLMs with   Context-preserving Token Filtering and Knowledge Graphs"></a>ConTextual: Improving Clinical Text Summarization in LLMs with   Context-preserving Token Filtering and Knowledge Graphs</h2><p><strong>Authors:Fahmida Liza Piya, Rahmatollah Beheshti</strong></p>
<p>Unstructured clinical data can serve as a unique and rich source of information that can meaningfully inform clinical practice. Extracting the most pertinent context from such data is critical for exploiting its true potential toward optimal and timely decision-making in patient care. While prior research has explored various methods for clinical text summarization, most prior studies either process all input tokens uniformly or rely on heuristic-based filters, which can overlook nuanced clinical cues and fail to prioritize information critical for decision-making. In this study, we propose Contextual, a novel framework that integrates a Context-Preserving Token Filtering method with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By preserving context-specific important tokens and enriching them with structured knowledge, ConTextual improves both linguistic coherence and clinical fidelity. Our extensive empirical evaluations on two public benchmark datasets demonstrate that ConTextual consistently outperforms other baselines. Our proposed approach highlights the complementary role of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity, as well as offering a scalable solution for improving precision in clinical text generation. </p>
<blockquote>
<p>éç»“æ„åŒ–ä¸´åºŠæ•°æ®å¯ä»¥ä½œä¸ºç‹¬ç‰¹ä¸”ä¸°å¯Œçš„ä¿¡æ¯æ¥æºï¼Œå¯¹ä¸´åºŠå®è·µäº§ç”Ÿæœ‰æ„ä¹‰çš„å½±å“ã€‚ä»è¿™ç±»æ•°æ®ä¸­æå–æœ€å…³é”®ä¸Šä¸‹æ–‡å¯¹äºå®ç°å…¶åœ¨æ‚£è€…æŠ¤ç†ä¸­çš„æœ€ä½³å’ŒåŠæ—¶å†³ç­–çš„çœŸæ­£æ½œåŠ›è‡³å…³é‡è¦ã€‚è™½ç„¶å…ˆå‰çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†å„ç§ä¸´åºŠæ–‡æœ¬æ‘˜è¦æ–¹æ³•ï¼Œä½†å¤§å¤šæ•°æ—©æœŸç ”ç©¶è¦ä¹ˆå¯¹æ‰€æœ‰è¾“å…¥æ ‡è®°è¿›è¡Œç»Ÿä¸€å¤„ç†ï¼Œè¦ä¹ˆä¾èµ–äºåŸºäºå¯å‘å¼è¿‡æ»¤å™¨ï¼Œè¿™å¯èƒ½ä¼šå¿½ç•¥å¾®å¦™çš„ä¸´åºŠçº¿ç´¢å¹¶ä¸”æ— æ³•ä¼˜å…ˆå¤„ç†å¯¹å†³ç­–è‡³å…³é‡è¦çš„ä¿¡æ¯ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºContextualçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ä¸Šä¸‹æ–‡ä¿ç•™æ ‡è®°è¿‡æ»¤æ–¹æ³•ä¸ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰è¿›è¡Œä¸Šä¸‹æ–‡æ‰©å……ã€‚é€šè¿‡ä¿ç•™ç‰¹å®šä¸Šä¸‹æ–‡çš„æ ‡è®°å¹¶ç”¨ç»“æ„åŒ–çŸ¥è¯†ä¸°å¯Œå®ƒä»¬ï¼ŒContextualæé«˜äº†è¯­è¨€è¿è´¯æ€§å’Œä¸´åºŠçœŸå®æ€§ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒContextualå§‹ç»ˆä¼˜äºå…¶ä»–åŸºçº¿ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çªå‡ºäº†æ ‡è®°çº§è¿‡æ»¤å’Œç»“æ„åŒ–æ£€ç´¢åœ¨å¢å¼ºè¯­è¨€å’Œä¸´åºŠå®Œæ•´æ€§æ–¹é¢çš„äº’è¡¥ä½œç”¨ï¼Œå¹¶æä¾›äº†æé«˜ä¸´åºŠæ–‡æœ¬ç”Ÿæˆç²¾åº¦çš„å¯æ‰©å±•è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16394v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸´åºŠéç»“æ„åŒ–æ•°æ®æ˜¯ç‹¬ç‰¹ä¸”ä¸°å¯Œçš„ä¿¡æ¯æ¥æºï¼Œå¯¹ä¸´åºŠå®è·µå…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚ä»è¿™äº›æ•°æ®ä¸­æå–æœ€å…³é”®çš„å†…å®¹å¯¹äºå……åˆ†åˆ©ç”¨å…¶æ½œåŠ›ä»¥åšå‡ºæœ€ä½³ä¸”åŠæ—¶çš„åŒ»ç–—å†³ç­–è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€œContextualâ€ï¼Œå®ƒç»“åˆäº†â€œContext-Preserving Token Filteringâ€æ–¹æ³•ä¸é¢†åŸŸç‰¹å®šçš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰è¿›è¡Œä¸Šä¸‹æ–‡æ‰©å……ã€‚é€šè¿‡ä¿ç•™é‡è¦çš„ä¸Šä¸‹æ–‡ç‰¹å®šæ ‡è®°å¹¶ç”¨ç»“æ„åŒ–çš„çŸ¥è¯†ä¸°å¯Œå®ƒä»¬ï¼ŒContextualæé«˜äº†è¯­è¨€è¿è´¯æ€§å’Œä¸´åºŠå‡†ç¡®æ€§ã€‚åœ¨å…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒContextualæŒç»­ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚æœ¬ç ”ç©¶çš„æ–¹æ³•çªæ˜¾äº†æ ‡è®°çº§è¿‡æ»¤å’Œç»“æ„åŒ–æ£€ç´¢åœ¨å¢å¼ºè¯­è¨€å’Œä¸´åºŠå®Œæ•´æ€§æ–¹é¢çš„äº’è¡¥ä½œç”¨ï¼Œå¹¶ä¸ºæé«˜ä¸´åºŠæ–‡æœ¬ç”Ÿæˆçš„ç²¾åº¦æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸´åºŠéç»“æ„åŒ–æ•°æ®æ˜¯ç‹¬ç‰¹ä¸”ä¸°å¯Œçš„ä¿¡æ¯æ¥æºï¼Œå¯¹ä¸´åºŠå®è·µå…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚</li>
<li>æå–ä¸´åºŠæ–‡æœ¬ä¸­çš„å…³é”®ä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹äºåšå‡ºæœ€ä½³åŒ»ç–—å†³ç­–è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç ”ç©¶åœ¨å¤„ç†æ–¹æ³•ä¸Šæœ‰æ‰€ä¸è¶³ï¼Œè¦ä¹ˆå¯¹æ‰€æœ‰è¾“å…¥æ ‡è®°è¿›è¡Œç»Ÿä¸€å¤„ç†ï¼Œè¦ä¹ˆä¾èµ–åŸºäºå¯å‘å¼çš„è¿‡æ»¤å™¨ï¼Œå¯èƒ½ä¼šå¿½ç•¥å¾®å¦™çš„ä¸´åºŠçº¿ç´¢å’Œå…³é”®å†³ç­–ä¿¡æ¯ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€œContextualâ€ï¼Œç»“åˆäº†ä¸Šä¸‹æ–‡ä¿ç•™æ ‡è®°è¿‡æ»¤æ–¹æ³•å’Œé¢†åŸŸç‰¹å®šçš„çŸ¥è¯†å›¾è°±è¿›è¡Œä¸Šä¸‹æ–‡æ‰©å……ã€‚</li>
<li>Contextualé€šè¿‡ä¿ç•™é‡è¦çš„ä¸Šä¸‹æ–‡ç‰¹å®šæ ‡è®°å¹¶ç”¨ç»“æ„åŒ–çš„çŸ¥è¯†ä¸°å¯Œå®ƒä»¬ï¼Œæé«˜äº†è¯­è¨€è¿è´¯æ€§å’Œä¸´åºŠå‡†ç¡®æ€§ã€‚</li>
<li>åœ¨å…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒContextualä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16394">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3eb51f0248f635d59a6382c1dd5896f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5d9a5be89d57e0acd81b5019a2245278.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5891bc27eafa5255502dd8dddadc7e69.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="The-First-Prompt-Counts-the-Most-An-Evaluation-of-Large-Language-Models-on-Iterative-Example-Based-Code-Generation"><a href="#The-First-Prompt-Counts-the-Most-An-Evaluation-of-Large-Language-Models-on-Iterative-Example-Based-Code-Generation" class="headerlink" title="The First Prompt Counts the Most! An Evaluation of Large Language Models   on Iterative Example-Based Code Generation"></a>The First Prompt Counts the Most! An Evaluation of Large Language Models   on Iterative Example-Based Code Generation</h2><p><strong>Authors:Yingjie Fu, Bozhou Li, Linyi Li, Wentao Zhang, Tao Xie</strong></p>
<p>The capabilities of Large Language Models (LLMs) in code generation have been extensively studied, particularly for implementing target functionalities from natural-language descriptions. Alternatively, input-output (I&#x2F;O) examples provide an accessible, unambiguous, and flexible way to describe functionalities. However, their inherent diversity, opaqueness, and incompleteness impose greater challenges for understanding and implementing the target requirements. Therefore, generating code from I&#x2F;O examples (i.e., example-based code generation) provides a new perspective, allowing us to additionally evaluate LLMsâ€™ capability to infer target functionalities from limited information and to process new-form requirements. However, related research about LLMs in example-based code generation remains largely unexplored. To fill this gap, this paper presents the first comprehensive study on example-based code generation using LLMs. We adopt an iterative evaluation framework and formalize the objective of example-based code generation as two sequential sub-objectives: generating code conforming to the given examples and generating code that successfully implements the target functionalities from (iteratively) given examples. We assess six state-of-the-art LLMs using a new benchmark of 172 diverse target functionalities. The results demonstrate that when requirements are described using iterative I&#x2F;O examples rather than natural language, the LLMsâ€™ score decreases by over 60%, and the vast majority (even over 95%) of successfully implemented functionalities are achieved in the first round of the iterations. Furthermore, we also find that combining I&#x2F;O examples with even imprecise and fragmental natural language descriptions greatly improves LLM performance, and the selection of initial I&#x2F;O examples can also influence the score, suggesting opportunities for prompt optimization. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°å®ç°ç›®æ ‡åŠŸèƒ½æ–¹é¢ã€‚å¦å¤–ï¼Œè¾“å…¥&#x2F;è¾“å‡ºï¼ˆI&#x2F;Oï¼‰ç¤ºä¾‹æä¾›äº†ä¸€ç§æ˜“äºè®¿é—®ã€æ˜ç¡®å’Œçµæ´»çš„æ–¹å¼æ¥æè¿°åŠŸèƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å†…åœ¨å¤šæ ·æ€§ã€æ¨¡ç³Šæ€§å’Œä¸å®Œå…¨æ€§ç»™ç†è§£å’Œå®ç°ç›®æ ‡è¦æ±‚å¸¦æ¥äº†æ›´å¤§çš„æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œä»I&#x2F;Oç¤ºä¾‹ç”Ÿæˆä»£ç ï¼ˆå³åŸºäºç¤ºä¾‹çš„ä»£ç ç”Ÿæˆï¼‰æä¾›äº†æ–°çš„è§†è§’ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿé¢å¤–è¯„ä¼°LLMä»æœ‰é™ä¿¡æ¯ä¸­æ¨æ–­ç›®æ ‡åŠŸèƒ½çš„èƒ½åŠ›ï¼Œä»¥åŠå¤„ç†æ–°å½¢å¼è¦æ±‚çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…³äºLLMåœ¨åŸºäºç¤ºä¾‹çš„ä»£ç ç”Ÿæˆæ–¹é¢çš„ç›¸å…³ç ”ç©¶ä»ç„¶å¾ˆå°‘ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡é¦–æ¬¡å¯¹åŸºäºLLMçš„ç¤ºä¾‹ä»£ç ç”Ÿæˆè¿›è¡Œäº†å…¨é¢ç ”ç©¶ã€‚æˆ‘ä»¬é‡‡ç”¨è¿­ä»£è¯„ä¼°æ¡†æ¶ï¼Œå°†åŸºäºç¤ºä¾‹çš„ä»£ç ç”Ÿæˆçš„ç›®æ ‡æ­£å¼å®šä¹‰ä¸ºä¸¤ä¸ªé¡ºåºçš„å­ç›®æ ‡ï¼šç”Ÿæˆç¬¦åˆç»™å®šç¤ºä¾‹çš„ä»£ç ï¼Œä»¥åŠç”Ÿæˆèƒ½å¤ŸæˆåŠŸå®ç°ç›®æ ‡åŠŸèƒ½çš„ä»£ç ï¼ˆé€šè¿‡è¿­ä»£ç»™å‡ºçš„ç¤ºä¾‹ï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«172ä¸ªä¸åŒç›®æ ‡åŠŸèƒ½çš„æ–°åŸºå‡†æµ‹è¯•è¯„ä¼°äº†å…­ç§æœ€æ–°LLMã€‚ç»“æœè¡¨æ˜ï¼Œå½“è¦æ±‚ä½¿ç”¨è¿­ä»£I&#x2F;Oç¤ºä¾‹è€Œä¸æ˜¯è‡ªç„¶è¯­è¨€æè¿°æ—¶ï¼ŒLLMçš„å¾—åˆ†ä¸‹é™äº†60%ä»¥ä¸Šï¼Œç»å¤§å¤šæ•°ï¼ˆè¶…è¿‡95%ï¼‰æˆåŠŸå®ç°çš„åŠŸèƒ½éƒ½æ˜¯åœ¨ç¬¬ä¸€è½®è¿­ä»£ä¸­å®ç°çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°ï¼Œå°†I&#x2F;Oç¤ºä¾‹ä¸å³ä½¿æ˜¯ä¸ç²¾ç¡®å’Œç‰‡æ®µåŒ–çš„è‡ªç„¶è¯­è¨€æè¿°ç›¸ç»“åˆï¼Œå¯ä»¥å¤§å¤§æé«˜LLMçš„æ€§èƒ½ï¼Œåˆå§‹I&#x2F;Oç¤ºä¾‹çš„é€‰æ‹©ä¹Ÿä¼šå½±å“å¾—åˆ†ï¼Œè¿™æç¤ºæˆ‘ä»¬æœ‰æœºä¼šè¿›è¡Œæç¤ºä¼˜åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.06774v2">PDF</a> Accepted by ISSTA 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¿­ä»£è¾“å…¥&#x2F;è¾“å‡ºï¼ˆI&#x2F;Oï¼‰ä¾‹å­çš„ä»£ç ç”Ÿæˆæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€ä¸ªæ–°è§†è§’ã€‚æœ¬æ–‡é¦–æ¬¡å¯¹æ­¤è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼Œå‘ç°ä½¿ç”¨è¿­ä»£I&#x2F;Oä¾‹å­æè¿°ç›®æ ‡åŠŸèƒ½æ—¶ï¼ŒLLMçš„æ€§èƒ½ä¸‹é™è¶…è¿‡60%ï¼Œä½†ç»“åˆä¸ç²¾ç¡®å’Œç‰‡æ®µåŒ–çš„è‡ªç„¶è¯­è¨€æè¿°å¯ä»¥æ˜¾è‘—æé«˜LLMæ€§èƒ½ã€‚åˆå§‹I&#x2F;Oä¾‹å­çš„é€‰æ‹©ä¹Ÿä¼šå½±å“å¾—åˆ†ï¼Œæç¤ºæœ‰ä¼˜åŒ–æç¤ºçš„æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŸºäºè¾“å…¥&#x2F;è¾“å‡ºï¼ˆI&#x2F;Oï¼‰ä¾‹å­çš„ä»£ç ç”Ÿæˆæ–¹é¢æä¾›äº†æ–°çš„è§†è§’ã€‚</li>
<li>ä½¿ç”¨è¿­ä»£I&#x2F;Oä¾‹å­æè¿°ç›®æ ‡åŠŸèƒ½æ—¶ï¼ŒLLMæ€§èƒ½ä¸‹é™è¶…è¿‡60%ã€‚</li>
<li>ç»“åˆä¸ç²¾ç¡®å’Œç‰‡æ®µåŒ–çš„è‡ªç„¶è¯­è¨€æè¿°å¯ä»¥æ˜¾è‘—æé«˜LLMåœ¨åŸºäºI&#x2F;Oä¾‹å­çš„ä»£ç ç”Ÿæˆä¸­çš„æ€§èƒ½ã€‚</li>
<li>åˆå§‹I&#x2F;Oä¾‹å­çš„é€‰æ‹©å½±å“LLMçš„æ€§èƒ½å¾—åˆ†ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨è¿­ä»£è¯„ä¼°æ¡†æ¶ï¼Œå°†åŸºäºI&#x2F;Oä¾‹å­çš„ä»£ç ç”Ÿæˆåˆ†ä¸ºä¸¤ä¸ªè¿ç»­çš„å­ç›®æ ‡ï¼šç”Ÿæˆç¬¦åˆä¾‹å­çš„ä»£ç å’ŒæˆåŠŸå®ç°ç›®æ ‡åŠŸèƒ½çš„ä»£ç ã€‚</li>
<li>åœ¨ç¬¬ä¸€è½®è¿­ä»£ä¸­ï¼ŒæˆåŠŸå®ç°çš„åŠŸèƒ½è¶…è¿‡95%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.06774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb267a6cc1853ddd3432a18aa650d700.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1b4a15269d2172e86cc59079e14c24a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c1835aea526d9ebd4c022d856e380ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-658a4cafafe117572a0178c74b74eeb0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e1f04fa792c064896cef9241a394b3ec.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  Multi-Agent Path Finding via Finite-Horizon Hierarchical Factorization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-18c87c0687e277dbbc33eb97e5ec28e3.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  DanceGRPO Unleashing GRPO on Visual Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25219.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
