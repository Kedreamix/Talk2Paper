<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-05-14  Pixel Motion as Universal Representation for Robot Control">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-edee906e9a345f0ac2a02462149c7882.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    26 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-14-更新"><a href="#2025-05-14-更新" class="headerlink" title="2025-05-14 更新"></a>2025-05-14 更新</h1><h2 id="Pixel-Motion-as-Universal-Representation-for-Robot-Control"><a href="#Pixel-Motion-as-Universal-Representation-for-Robot-Control" class="headerlink" title="Pixel Motion as Universal Representation for Robot Control"></a>Pixel Motion as Universal Representation for Robot Control</h2><p><strong>Authors:Kanchana Ranasinghe, Xiang Li, Cristina Mata, Jongwoo Park, Michael S Ryoo</strong></p>
<p>We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. Our high-level System 2, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame to guide robot control. Pixel motion-a universal, interpretable, and motion-centric representation-can be extracted from videos in a self-supervised manner, enabling diffusion model training on web-scale video-caption data. Treating generated pixel motion as learned universal representations, our low level System 1 module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision. System 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals. This hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action. Checkout <a target="_blank" rel="noopener" href="https://kahnchana.github.io/LangToMo">https://kahnchana.github.io/LangToMo</a> for visualizations. </p>
<blockquote>
<p>我们推出了LangToMo，这是一个视觉-语言-行动框架，采用双系统架构，利用像素运动预测作为中间表示。我们的高级系统2是一个图像扩散模型，从单帧生成文本调节的像素运动序列，以指导机器人控制。像素运动是一种通用、可解释、以运动为中心的表示，可以以自我监督的方式从视频中提取，使得扩散模型能够在网页规模的视频字幕数据上进行训练。将生成的像素运动视为学习的通用表示，我们的低级系统1模块通过运动到动作的映射功能将这些表示转换为机器人动作，这些映射功能可以是手工制作的，或者通过最小的监督进行学习。系统2作为高级策略，在稀疏的时间间隔内应用，而系统1作为低级策略在密集的时间间隔内行动。这种分层解耦使得在无人监督和有人监督的环境下，机器人控制更加灵活、可扩展和通用，在语言、运动和动作之间搭建了桥梁。想了解更多可视化内容，请访问：<a target="_blank" rel="noopener" href="https://kahnchana.github.io/LangToMo%E3%80%82">https://kahnchana.github.io/LangToMo。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07817v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>LangToMo是一个视觉语言动作框架，采用双系统架构，以像素运动预测作为中间表示形式。其高级系统2采用图像扩散模型，通过单一帧生成文本控制的像素运动序列，指导机器人控制。该框架通过自我监督的方式从视频中提取像素运动，使扩散模型能够在网页规模的视频描述数据上进行训练。将生成的像素运动视为学习的通用表示，其低级系统1模块通过运动到动作的映射函数将这些表示转换为机器人动作，这些映射函数可以是手工制作的，也可以以最少的监督进行学习。系统2作为高级策略在稀疏时间间隔内运行，而系统1作为低级策略在密集时间间隔内运行。这种分层解耦实现了灵活的、可扩展的和通用的机器人控制，无论是在无监督还是监督设置下都能缩小语言、动作和行为之间的差距。更多可视化内容请访问：<a target="_blank" rel="noopener" href="https://kahnchana.github.io/LangToMo">网站链接</a>。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LangToMo是一个视觉语言动作框架，具有双系统架构。</li>
<li>该框架利用像素运动预测作为中间表示形式。</li>
<li>高级系统2采用图像扩散模型，生成文本控制的像素运动序列来指导机器人控制。</li>
<li>像素运动可以通过自我监督的方式从视频中提取。</li>
<li>低级系统1模块将像素运动转换为机器人动作。</li>
<li>该框架实现了灵活的、可扩展的和通用的机器人控制。</li>
<li>该框架缩小了语言、动作和行为之间的差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07817">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d11023be20cecd0201fdd7906c09d848.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ad3b5f9870cba040fbbb8c0d51d33e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a790d0112804c367a250230bd155572.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ABS-Mamba-SAM2-Driven-Bidirectional-Spiral-Mamba-Network-for-Medical-Image-Translation"><a href="#ABS-Mamba-SAM2-Driven-Bidirectional-Spiral-Mamba-Network-for-Medical-Image-Translation" class="headerlink" title="ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical   Image Translation"></a>ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical   Image Translation</h2><p><strong>Authors:Feng Yuan, Yifan Gao, Wenbin Wu, Keqing Wu, Xiaotong Guo, Jie Jiang, Xin Gao</strong></p>
<p>Accurate multi-modal medical image translation requires ha-rmonizing global anatomical semantics and local structural fidelity, a challenge complicated by intermodality information loss and structural distortion. We propose ABS-Mamba, a novel architecture integrating the Segment Anything Model 2 (SAM2) for organ-aware semantic representation, specialized convolutional neural networks (CNNs) for preserving modality-specific edge and texture details, and Mamba’s selective state-space modeling for efficient long- and short-range feature dependencies. Structurally, our dual-resolution framework leverages SAM2’s image encoder to capture organ-scale semantics from high-resolution inputs, while a parallel CNNs branch extracts fine-grained local features. The Robust Feature Fusion Network (RFFN) integrates these epresentations, and the Bidirectional Mamba Residual Network (BMRN) models spatial dependencies using spiral scanning and bidirectional state-space dynamics. A three-stage skip fusion decoder enhances edge and texture fidelity. We employ Efficient Low-Rank Adaptation (LoRA+) fine-tuning to enable precise domain specialization while maintaining the foundational capabilities of the pre-trained components. Extensive experimental validation on the SynthRAD2023 and BraTS2019 datasets demonstrates that ABS-Mamba outperforms state-of-the-art methods, delivering high-fidelity cross-modal synthesis that preserves anatomical semantics and structural details to enhance diagnostic accuracy in clinical applications. The code is available at <a target="_blank" rel="noopener" href="https://github.com/gatina-yone/ABS-Mamba">https://github.com/gatina-yone/ABS-Mamba</a> </p>
<blockquote>
<p>精确的多模态医学图像翻译需要协调全局解剖语义和局部结构保真度，这一挑战因模态间信息丢失和结构失真而复杂化。我们提出了ABS-Mamba，这是一种新型架构，它集成了Segment Anything Model 2（SAM2）用于器官感知语义表示，专业化的卷积神经网络（CNN）用于保留模态特定的边缘和纹理细节，以及Mamba的选择性状态空间建模，以实现高效的长短程特征依赖性。结构上，我们的双分辨率框架利用SAM2的图像编码器捕获来自高分辨率输入的组织规模语义，而并行CNN分支则提取精细的局部特征。鲁棒的特征融合网络（RFFN）集成了这些表示，双向Mamba残差网络（BMRN）使用螺旋扫描和双向状态空间动力学对空间依赖性进行建模。三阶段跳过融合解码器增强了边缘和纹理的保真度。我们采用有效的低秩适应（LoRA+）微调方法，以实现精确的领域专业化，同时保持预训练组件的基本功能。在SynthRAD2023和BraTS2019数据集上的广泛实验验证表明，ABS-Mamba优于最新方法，实现了高保真度的跨模态合成，保留了解剖语义和结构细节，提高了临床应用中诊断的准确性。代码可用在<a target="_blank" rel="noopener" href="https://github.com/gatina-yone/ABS-Mamba%E3%80%82">https://github.com/gatina-yone/ABS-Mamba。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07687v1">PDF</a> MICCAI 2025(under view)</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为ABS-Mamba的新型多模态医学影像翻译架构，它融合了Segment Anything Model 2（SAM2）进行器官感知语义表示、卷积神经网络（CNN）保留模态边缘和纹理细节，以及Mamba的选择状态空间建模进行长短距离特征依赖。通过高效融合网络（RFFN）整合表示，采用双向Mamba残差网络（BMRN）进行空间依赖性建模。该架构在SynthRAD2023和BraTS2019数据集上的实验验证显示，ABS-Mamba在保持解剖语义和结构细节的同时，实现了高保真跨模态合成，提高了临床应用的诊断准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ABS-Mamba架构融合了SAM2进行器官感知语义表示，以捕捉全局解剖学语义。</li>
<li>专用CNN用于保留模态特定的边缘和纹理细节。</li>
<li>Mamba的选择状态空间建模实现长短距离特征依赖的有效平衡。</li>
<li>RFFN整合了不同特征表示，而BMRN通过螺旋扫描和双向状态空间动力学进行空间依赖性建模。</li>
<li>三阶段跳过融合解码器增强了边缘和纹理的保真度。</li>
<li>采用Efficient Low-Rank Adaptation（LoRA+）微调方法，可在精确领域专业化的同时保持预训练组件的基础能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07687">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ca862168d659c4f3c6023f09f59435d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a19dbac974820340e78f5adf3a5b518.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GAN-based-synthetic-FDG-PET-images-from-T1-brain-MRI-can-serve-to-improve-performance-of-deep-unsupervised-anomaly-detection-models"><a href="#GAN-based-synthetic-FDG-PET-images-from-T1-brain-MRI-can-serve-to-improve-performance-of-deep-unsupervised-anomaly-detection-models" class="headerlink" title="GAN-based synthetic FDG PET images from T1 brain MRI can serve to   improve performance of deep unsupervised anomaly detection models"></a>GAN-based synthetic FDG PET images from T1 brain MRI can serve to   improve performance of deep unsupervised anomaly detection models</h2><p><strong>Authors:Daria Zotova, Nicolas Pinon, Robin Trombetta, Romain Bouet, Julien Jung, Carole Lartizien</strong></p>
<p>Background and Objective. Research in the cross-modal medical image translation domain has been very productive over the past few years in tackling the scarce availability of large curated multimodality datasets with the promising performance of GAN-based architectures. However, only a few of these studies assessed task-based related performance of these synthetic data, especially for the training of deep models. Method. We design and compare different GAN-based frameworks for generating synthetic brain [18F]fluorodeoxyglucose (FDG) PET images from T1 weighted MRI data. We first perform standard qualitative and quantitative visual quality evaluation. Then, we explore further impact of using these fake PET data in the training of a deep unsupervised anomaly detection (UAD) model designed to detect subtle epilepsy lesions in T1 MRI and FDG PET images. We introduce novel diagnostic task-oriented quality metrics of the synthetic FDG PET data tailored to our unsupervised detection task, then use these fake data to train a use case UAD model combining a deep representation learning based on siamese autoencoders with a OC-SVM density support estimation model. This model is trained on normal subjects only and allows the detection of any variation from the pattern of the normal population. We compare the detection performance of models trained on 35 paired real MR T1 of normal subjects paired either on 35 true PET images or on 35 synthetic PET images generated from the best performing generative models. Performance analysis is conducted on 17 exams of epilepsy patients undergoing surgery. Results. The best performing GAN-based models allow generating realistic fake PET images of control subject with SSIM and PSNR values around 0.9 and 23.8, respectively and in distribution (ID) with regard to the true control dataset. The best UAD model trained on these synthetic normative PET data allows reaching 74% sensitivity. Conclusion. Our results confirm that GAN-based models are the best suited for MR T1 to FDG PET translation, outperforming transformer or diffusion models. We also demonstrate the diagnostic value of these synthetic data for the training of UAD models and evaluation on clinical exams of epilepsy patients. Our code and the normative image dataset are available. </p>
<blockquote>
<p><strong>背景与目的</strong>：近年来，关于跨模态医学图像翻译领域的研究在解决大型整理多模态数据集稀缺的问题方面取得了丰硕的成果，基于GAN的架构表现出良好的性能。然而，只有少数研究评估了这些合成数据在任务相关方面的性能，尤其是用于训练深度学习模型方面。<br><strong>方法</strong>：我们设计和比较了基于不同GAN的框架，用于从T1加权MRI数据生成合成的大脑氟代脱氧葡萄糖（FDG）PET图像。我们首先进行标准的定性和定量视觉质量评估。然后，我们进一步探索使用这些假PET数据在训练深度无监督异常检测（UAD）模型中的影响，该模型旨在检测T1 MRI和FDG PET图像中的微妙癫痫病灶。我们引入了针对我们的无监督检测任务的新型诊断任务导向型质量指标，用于评估合成FDG PET数据的质量。然后，我们使用这些假数据来训练一个用例UAD模型，该模型结合了基于孪生自编码器的深度表示学习与OC-SVM密度支持估计模型。该模型仅对正常主体进行训练，并允许检测任何与正常人群模式的偏差。我们比较了在35对真实MR T1图像（正常主体）与35张真实PET图像或来自表现最佳的生成模型的35张合成PET图像上训练的模型的检测性能。性能分析是在接受手术的17名癫痫患者身上进行的。<br><strong>结果</strong>：表现最佳的基于GAN的模型能够生成逼真的假PET图像，控制对象的结构相似性度量（SSIM）和峰值信噪比（PSNR）值分别约为0.9和23.8，且在身份分布上与真实控制数据集相符。使用这些合成规范性PET数据训练的最佳UAD模型的敏感性达到74%。<br><strong>结论</strong>：我们的结果证实，基于GAN的模型在MR T1到FDG PET翻译方面最适合，优于Transformer或扩散模型。我们还证明了这些合成数据对于训练UAD模型和评估癫痫患者的临床考试中的诊断价值。我们的代码和规范图像数据集可供使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07364v1">PDF</a> </p>
<p><strong>Summary</strong>:<br>本文探讨了基于GAN架构的跨模态医学图像翻译在生成合成脑[¹⁸F]氟脱氧葡萄糖（FDG）PET图像方面的应用。研究设计并比较了不同GAN框架生成合成图像的效果，特别是在训练用于检测细微癫痫病灶的深度学习模型中的应用。结果表明，GAN模型最适合于生成与真实数据相近的合成PET图像，使用这些合成数据训练的模型在临床癫痫患者检查中显示出诊断价值。代码及数据集可供公开获取。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>GAN模型适用于生成医学图像合成数据，尤其是用于跨模态医学图像翻译领域。</li>
<li>研究通过设计不同GAN框架生成合成脑FDG PET图像，从MR T1数据出发。</li>
<li>通过对合成图像进行视觉质量评估和定量评估，确定最佳性能的GAN模型。</li>
<li>研究探索了使用合成PET数据训练深度学习模型进行癫痫病灶检测的应用。</li>
<li>引入针对特定检测任务的新型诊断质量指标来评估合成数据的适用性。</li>
<li>使用最佳合成数据训练的UAD模型在癫痫患者检测中达到74%的敏感性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07364">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-75d8aa298bc71f1dd84f1fb41bb64059.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ad81deaf0ad0909d007a8a007018411.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edee906e9a345f0ac2a02462149c7882.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e31676b64bce418610fd0cae36d7bb02.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Language-Driven-Dual-Style-Mixing-for-Single-Domain-Generalized-Object-Detection"><a href="#Language-Driven-Dual-Style-Mixing-for-Single-Domain-Generalized-Object-Detection" class="headerlink" title="Language-Driven Dual Style Mixing for Single-Domain Generalized Object   Detection"></a>Language-Driven Dual Style Mixing for Single-Domain Generalized Object   Detection</h2><p><strong>Authors:Hongda Qin, Xiao Lu, Zhiyong Wei, Yihong Cao, Kailun Yang, Ningjiang Chen</strong></p>
<p>Generalizing an object detector trained on a single domain to multiple unseen domains is a challenging task. Existing methods typically introduce image or feature augmentation to diversify the source domain to raise the robustness of the detector. Vision-Language Model (VLM)-based augmentation techniques have been proven to be effective, but they require that the detector’s backbone has the same structure as the image encoder of VLM, limiting the detector framework selection. To address this problem, we propose Language-Driven Dual Style Mixing (LDDS) for single-domain generalization, which diversifies the source domain by fully utilizing the semantic information of the VLM. Specifically, we first construct prompts to transfer style semantics embedded in the VLM to an image translation network. This facilitates the generation of style diversified images with explicit semantic information. Then, we propose image-level style mixing between the diversified images and source domain images. This effectively mines the semantic information for image augmentation without relying on specific augmentation selections. Finally, we propose feature-level style mixing in a double-pipeline manner, allowing feature augmentation to be model-agnostic and can work seamlessly with the mainstream detector frameworks, including the one-stage, two-stage, and transformer-based detectors. Extensive experiments demonstrate the effectiveness of our approach across various benchmark datasets, including real to cartoon and normal to adverse weather tasks. The source code and pre-trained models will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/qinhongda8/LDDS">https://github.com/qinhongda8/LDDS</a>. </p>
<blockquote>
<p>将单一领域训练的物体检测器推广到多个未见领域是一项具有挑战性的任务。现有方法通常通过图像或特征增强来多样化源领域，以提高检测器的稳健性。基于视觉语言模型（VLM）的增强技术已被证明是有效的，但它们要求检测器的骨干结构与VLM的图像编码器相同，从而限制了检测器框架的选择。为了解决此问题，我们提出了用于单一领域推广的语言驱动双风格混合（LDDS）方法，该方法通过充分利用VLM中的语义信息来多样化源领域。具体来说，我们首先构建提示，将嵌入在VLM中的风格语义转移到图像翻译网络中。这有助于生成具有明确语义信息的风格多样化图像。然后，我们提出在多样化图像和源域图像之间进行图像级别的风格混合。这有效地挖掘了用于图像增强的语义信息，而无需依赖特定的增强选择。最后，我们采用双管道方式进行特征级别的风格混合，使特征增强成为模型无关，并能无缝地与主流检测器框架配合使用，包括单阶段、两阶段和基于变压器的检测器。大量实验证明，我们的方法在各种基准数据集上均有效，包括从现实到卡通和从正常到恶劣天气任务。源代码和预训练模型将在<a target="_blank" rel="noopener" href="https://github.com/qinhongda8/LDDS%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/qinhongda8/LDDS上公开提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07219v1">PDF</a> The source code and pre-trained models will be publicly available at   <a target="_blank" rel="noopener" href="https://github.com/qinhongda8/LDDS">https://github.com/qinhongda8/LDDS</a></p>
<p><strong>Summary</strong><br>基于视觉语言模型（VLM）的单域泛化挑战可通过语言驱动双风格混合（LDDS）技术来解决。该方法充分利用VLM中的风格语义信息生成多样化图像，通过图像级风格混合和特征级风格混合，提高了检测器对未见域的泛化能力。实验证明，该方法在不同基准数据集上均有效。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LDDS方法解决了将单一域训练的物体检测器推广到多个未见域的挑战。</li>
<li>LDDS利用VLM中的风格语义信息生成多样化图像，提高检测器的泛化能力。</li>
<li>LDDS通过图像级风格混合挖掘语义信息，不依赖于特定增强选择。</li>
<li>LDDS采用双管道方式实现特征级风格混合，可与主流检测器框架无缝协作。</li>
<li>LDDS方法在各种基准数据集上进行广泛实验验证，包括真实到卡通和正常到恶劣天气任务。</li>
<li>LDDS的源代码和预训练模型将公开提供，便于其他研究者使用和改进。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07219">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0a168f79ee222bff299c40b92d6f64b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80f140c1e28d7068e733a50877d55330.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0438606acbea2776ffbf533713c5d631.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f8329c4c921b3f07241a2c1da4b5fda6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b41952cf94a4cf4614c78f4e5d2d3914.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9eab95fc0d4912a1022736762fbc3c6f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TopicVD-A-Topic-Based-Dataset-of-Video-Guided-Multimodal-Machine-Translation-for-Documentaries"><a href="#TopicVD-A-Topic-Based-Dataset-of-Video-Guided-Multimodal-Machine-Translation-for-Documentaries" class="headerlink" title="TopicVD: A Topic-Based Dataset of Video-Guided Multimodal Machine   Translation for Documentaries"></a>TopicVD: A Topic-Based Dataset of Video-Guided Multimodal Machine   Translation for Documentaries</h2><p><strong>Authors:Jinze Lv, Jian Chen, Zi Long, Xianghua Fu, Yin Chen</strong></p>
<p>Most existing multimodal machine translation (MMT) datasets are predominantly composed of static images or short video clips, lacking extensive video data across diverse domains and topics. As a result, they fail to meet the demands of real-world MMT tasks, such as documentary translation. In this study, we developed TopicVD, a topic-based dataset for video-supported multimodal machine translation of documentaries, aiming to advance research in this field. We collected video-subtitle pairs from documentaries and categorized them into eight topics, such as economy and nature, to facilitate research on domain adaptation in video-guided MMT. Additionally, we preserved their contextual information to support research on leveraging the global context of documentaries in video-guided MMT. To better capture the shared semantics between text and video, we propose an MMT model based on a cross-modal bidirectional attention module. Extensive experiments on the TopicVD dataset demonstrate that visual information consistently improves the performance of the NMT model in documentary translation. However, the MMT model’s performance significantly declines in out-of-domain scenarios, highlighting the need for effective domain adaptation methods. Additionally, experiments demonstrate that global context can effectively improve translation performance. % Dataset and our implementations are available at <a target="_blank" rel="noopener" href="https://github.com/JinzeLv/TopicVD">https://github.com/JinzeLv/TopicVD</a> </p>
<blockquote>
<p>目前大多数多模态机器翻译（MMT）数据集主要由静态图像或短视频片段组成，缺乏跨不同领域和主题的大量视频数据。因此，它们无法满足现实世界中的MMT任务需求，如纪录片翻译。本研究中，我们开发了TopicVD，这是一个基于视频支持的多模态机器翻译纪录片数据集，旨在推动该领域的研究。我们从纪录片中收集了视频字幕对，并将其分为经济、自然等八个主题，以促进视频指导的MMT中的领域自适应研究。此外，我们还保留了其上下文信息，以支持在视频指导的MMT中利用纪录片的全球语境的研究。为了更好地捕捉文本和视频之间的共享语义，我们提出了一种基于跨模态双向注意力模块的多模态翻译模型。在TopicVD数据集上的大量实验表明，视觉信息始终提高了神经机器翻译模型在纪录片翻译方面的性能。然而，在多模态翻译模型处理跨领域场景时性能显著下降，这凸显了有效领域自适应方法的必要性。此外，实验表明全局上下文可以有效地提高翻译性能。数据集及我们的实现可访问 <a target="_blank" rel="noopener" href="https://github.com/JinzeLv/TopicVD">https://github.com/JinzeLv/TopicVD</a> 了解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05714v1">PDF</a> NLDB 2025</p>
<p><strong>Summary</strong>：<br>针对当前多媒体机器翻译（MMT）数据集缺乏涵盖广泛主题的视频数据的问题，本文提出TopicVD数据集，旨在推进视频辅助多媒体机器翻译领域的研究。通过收集纪录片视频字幕对，并按主题分类，同时保留上下文信息，研究视频与文本之间的共享语义。此外，引入基于跨模态双向注意力模块的多媒体机器翻译模型。实验表明，视频信息可提高翻译性能，但跨领域场景下性能下降，显示需要有效的领域适应方法。全球语境可有效提升翻译性能。数据集及相关实现已公开在GitHub上提供。</p>
<p><strong>Key Takeaways</strong>：</p>
<ul>
<li>TopicVD数据集针对多媒体机器翻译设计，尤其适用于纪录片翻译研究。</li>
<li>TopicVD包含多种主题的视频数据，有利于领域适应研究。</li>
<li>上下文信息得以保留以支持相关研究。</li>
<li>提出基于跨模态双向注意力模块的多媒体机器翻译模型。</li>
<li>视频信息能改善翻译性能。</li>
<li>在跨领域场景下，多媒体机器翻译模型性能显著下降，显示对有效领域适应方法的需求。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05714">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6465962a9fc7fc4e460e36656ac311e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2cb13f0e8e6169d11abb374d0d4a37a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efd0f5086df20df87fb7104f2c77019d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Realization-of-a-Pre-Sample-Photonic-based-Free-Electron-Modulator-in-Ultrafast-Transmission-Electron-Microscopes"><a href="#Realization-of-a-Pre-Sample-Photonic-based-Free-Electron-Modulator-in-Ultrafast-Transmission-Electron-Microscopes" class="headerlink" title="Realization of a Pre-Sample Photonic-based Free-Electron Modulator in   Ultrafast Transmission Electron Microscopes"></a>Realization of a Pre-Sample Photonic-based Free-Electron Modulator in   Ultrafast Transmission Electron Microscopes</h2><p><strong>Authors:Beatrice Matilde Ferrari, Cameron James Richard Duncan, Michael Yannai, Raphael Dahan, Paolo Rosi, Irene Ostroman, Maria Giulia Bravi, Arthur Niedermayr, Tom Lenkiewicz Abudi, Yuval Adiv, Tal Fishman, Sang Tae Park, Dan Masiel, Thomas Lagrange, Fabrizio Carbone, Vincenzo Grillo, F. Javier García de Abajo, Ido Kaminer, Giovanni Maria Vanacore</strong></p>
<p>Spatial and temporal light modulation is a well-established technology that enables dynamic shaping of the phase and amplitude of optical fields, significantly enhancing the resolution and sensitivity of imaging methods. Translating this capability to electron beams is highly desirable within the framework of a transmission electron microscope (TEM) to benefit from the nanometer spatial resolution of these instruments. In this work, we report on the experimental realization of a photonic-based free-electron modulator integrated into the column of two ultrafast TEMs for pre-sample electron-beam shaping. Electron-photon interaction is employed to coherently modulate both the transverse and longitudinal components of the electron wave function, while leveraging dynamically controlled optical fields and tailored design of electron-laser-sample interaction geometry. Using energy- and momentum-resolved electron detection, we successfully reconstruct the shaped electron wave function at the TEM sample plane. These results demonstrate the ability to manipulate the electron wave function before probing the sample, paving the way for the future development of innovative imaging methods in ultrafast electron microscopy. </p>
<blockquote>
<p>空间和时间光调制是一项成熟的技术，能够动态地改变光场的相位和振幅，从而极大地提高成像方法的分辨率和灵敏度。在透射电子显微镜（TEM）的框架下，将这一能力应用到电子束上是非常理想的，以利用这些仪器的纳米空间分辨率。在这项工作中，我们报告了将光子基自由电子调制器集成到两台超快透射电子显微镜的支柱中，用于样品前的电子束整形。利用电子-光子相互作用，相干地调制电子波函数的横向和纵向分量，同时利用动态控制的光场和定制的电子-激光-样品相互作用几何结构。通过能量和动量解析的电子检测，我们成功地在透射电子显微镜样品平面上重建了整形后的电子波函数。这些结果证明了在探测样品之前操作电子波函数的能力，为超快电子显微镜中创新成像方法的未来发展铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11313v2">PDF</a> 14 pages, 5, figures, includes supplementary information, journal   paper</p>
<p><strong>Summary</strong></p>
<p>空间和时间光调制技术已广泛应用于光学领域，实现了对光场相位和振幅的动态调控，大大提高了成像方法的分辨率和灵敏度。本文将此技术应用于透射电子显微镜（TEM）中，实现了电子束的预样本调控。该研究利用电子光子相互作用，对电子波函数的横向和纵向成分进行相干调控，同时采用动态控制的光场和定制的电子激光样品相互作用几何设计。通过能量和动量解析的电子检测，成功重建了样品平面上的电子波函数。这一成果展示了在探测样品之前操控电子波函数的能力，为超快电子显微镜中创新成像方法的发展铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>空间和时间光调制技术已广泛应用于光学领域，实现对光场相位和振幅的动态调控。</li>
<li>技术被成功应用于透射电子显微镜（TEM）中，实现了电子束的预样本调控。</li>
<li>通过电子光子相互作用对电子波函数的横向和纵向成分进行相干调控。</li>
<li>利用动态控制的光场和定制的几何设计实现电子激光样品相互作用。</li>
<li>通过能量和动量解析的电子检测成功重建了样品平面上的电子波函数。</li>
<li>该技术展示了在探测样品前操控电子波函数的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11313">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0df5ad72e4140d00f93db2c59f952cff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e368e6362eb39ee15c917b96d85b1a50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6eb9e66a07727ccf656071e82a6eb5a9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-7c03550ec50e2ce6f1ca34536b68e039.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-05-14  Breast Cancer Classification in Deep Ultraviolet Fluorescence Images   Using a Patch-Level Vision Transformer Framework
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a7232ea14da8cf4d1a5ec0344649ce78.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-05-14  Beyond CLIP Generalization Against Forward&Backward Forgetting Adapter   for Continual Learning of Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18884.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
