<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  Breast Cancer Classification in Deep Ultraviolet Fluorescence Images   Using a Patch-Level Vision Transformer Framework">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-7c03550ec50e2ce6f1ca34536b68e039.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    42 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-14-æ›´æ–°"><a href="#2025-05-14-æ›´æ–°" class="headerlink" title="2025-05-14 æ›´æ–°"></a>2025-05-14 æ›´æ–°</h1><h2 id="Breast-Cancer-Classification-in-Deep-Ultraviolet-Fluorescence-Images-Using-a-Patch-Level-Vision-Transformer-Framework"><a href="#Breast-Cancer-Classification-in-Deep-Ultraviolet-Fluorescence-Images-Using-a-Patch-Level-Vision-Transformer-Framework" class="headerlink" title="Breast Cancer Classification in Deep Ultraviolet Fluorescence Images   Using a Patch-Level Vision Transformer Framework"></a>Breast Cancer Classification in Deep Ultraviolet Fluorescence Images   Using a Patch-Level Vision Transformer Framework</h2><p><strong>Authors:Pouya Afshin, David Helminiak, Tongtong Lu, Tina Yen, Julie M. Jorns, Mollie Patton, Bing Yu, Dong Hye Ye</strong></p>
<p>Breast-conserving surgery (BCS) aims to completely remove malignant lesions while maximizing healthy tissue preservation. Intraoperative margin assessment is essential to achieve a balance between thorough cancer resection and tissue conservation. A deep ultraviolet fluorescence scanning microscope (DUV-FSM) enables rapid acquisition of whole surface images (WSIs) for excised tissue, providing contrast between malignant and normal tissues. However, breast cancer classification with DUV WSIs is challenged by high resolutions and complex histopathological features. This study introduces a DUV WSI classification framework using a patch-level vision transformer (ViT) model, capturing local and global features. Grad-CAM++ saliency weighting highlights relevant spatial regions, enhances result interpretability, and improves diagnostic accuracy for benign and malignant tissue classification. A comprehensive 5-fold cross-validation demonstrates the proposed approach significantly outperforms conventional deep learning methods, achieving a classification accuracy of 98.33%. </p>
<blockquote>
<p>ä¹³è…ºä¿ä¹³æ‰‹æœ¯ï¼ˆBCSï¼‰æ—¨åœ¨å½»åº•åˆ‡é™¤æ¶æ€§ç—…å˜ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°ä¿ç•™å¥åº·ç»„ç»‡ã€‚æœ¯ä¸­è¾¹ç¼˜è¯„ä¼°å¯¹äºåœ¨å½»åº•åˆ‡é™¤ç™Œç—‡å’Œä¿ç•™ç»„ç»‡ä¹‹é—´å–å¾—å¹³è¡¡è‡³å…³é‡è¦ã€‚æ·±ç´«å¤–è§å…‰æ‰«ææ˜¾å¾®é•œï¼ˆDUV-FSMï¼‰å¯ä»¥å¿«é€Ÿè·å–åˆ‡é™¤ç»„ç»‡çš„å…¨è¡¨é¢å›¾åƒï¼ˆWSIï¼‰ï¼Œä¸ºæ¶æ€§ç»„ç»‡å’Œæ­£å¸¸ç»„ç»‡æä¾›å¯¹æ¯”ã€‚ç„¶è€Œï¼Œç”±äºé«˜è§£æåº¦å’Œå¤æ‚çš„ç»„ç»‡ç—…ç†å­¦ç‰¹å¾ï¼Œä½¿ç”¨DUV WSIè¿›è¡Œä¹³è…ºç™Œåˆ†ç±»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºåŒºåŸŸçº§åˆ«çš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¨¡å‹çš„DUV WSIåˆ†ç±»æ¡†æ¶ï¼Œèƒ½å¤Ÿæ•æ‰å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚Grad-CAM++æ˜¾è‘—æ€§æƒé‡èƒ½å¤Ÿçªå‡ºç›¸å…³çš„ç©ºé—´åŒºåŸŸï¼Œæé«˜ç»“æœçš„å¯è§£é‡Šæ€§ï¼Œå¹¶æ”¹å–„è‰¯æ€§å’Œæ¶æ€§ç»„ç»‡åˆ†ç±»çš„è¯Šæ–­å‡†ç¡®æ€§ã€‚é€šè¿‡å…¨é¢çš„äº”æŠ˜äº¤å‰éªŒè¯æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œè¾¾åˆ°äº†98.33%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07654v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é«˜åˆ†è¾¨ç‡ä¸å¤æ‚çš„ç»„ç»‡ç—…ç†å­¦ç‰¹å¾ä¸ºä½¿ç”¨æ·±ç´«å¤–çº¿è§å…‰æ‰«ææ˜¾å¾®é•œï¼ˆDUV-FSMï¼‰åœ¨åˆ‡é™¤ç»„ç»‡ä¸­è¿›è¡Œä¹³è…ºç™Œåˆ†ç±»å¸¦æ¥æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºè¡¥ä¸çº§åˆ«çš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¨¡å‹çš„DUVå®½è§†é‡å›¾åƒï¼ˆWSIï¼‰åˆ†ç±»æ¡†æ¶ï¼Œæ•æ‰å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚Grad-CAM++æ˜¾è‘—åŠ æƒæ–¹æ³•å‡¸æ˜¾å…³é”®çš„ç©ºé—´åŒºåŸŸï¼Œæé«˜äº†ç»“æœçš„å¯è§£é‡Šæ€§å’Œå¯¹è‰¯æ¶æ€§ç»„ç»‡åˆ†ç±»çš„è¯Šæ–­å‡†ç¡®æ€§ã€‚ç»è¿‡å…¨é¢çš„äº”æŠ˜äº¤å‰éªŒè¯ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œåˆ†ç±»å‡†ç¡®ç‡è¾¾åˆ°äº†98.33%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™Œä¿ä¹³æ‰‹æœ¯æ—¨åœ¨å½»åº•åˆ‡é™¤æ¶æ€§ç—…å˜å¹¶æœ€å¤§ç¨‹åº¦åœ°ä¿ç•™å¥åº·ç»„ç»‡ã€‚</li>
<li>æœ¯ä¸­è¾¹ç•Œè¯„ä¼°æ˜¯å½»åº•åˆ‡é™¤è‚¿ç˜¤å’Œä¿ç•™ç»„ç»‡ä¹‹é—´çš„å¹³è¡¡çš„å…³é”®ã€‚</li>
<li>æ·±ç´«å¤–çº¿è§å…‰æ‰«ææ˜¾å¾®é•œï¼ˆDUV-FSMï¼‰èƒ½å¤Ÿè¿…é€Ÿè·å–åˆ‡é™¤ç»„ç»‡çš„å…¨è¡¨é¢å›¾åƒï¼ˆWSIï¼‰ï¼ŒåŒºåˆ†æ¶æ€§ä¸æ­£å¸¸ç»„ç»‡ã€‚</li>
<li>DUV WSIåˆ†ç±»é¢ä¸´é«˜åˆ†è¾¨å’Œå¤æ‚ç»„ç»‡ç—…ç†å­¦ç‰¹å¾çš„æŒ‘æˆ˜ã€‚</li>
<li>è¡¥ä¸çº§åˆ«çš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¨¡å‹ç”¨äºæ•æ‰å±€éƒ¨å’Œå…¨å±€ç‰¹å¾è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>Grad-CAM++æ˜¾è‘—åŠ æƒæé«˜äº†åˆ†ç±»ç»“æœçš„è¯Šæ–­å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b312affad5bae96055b263186d795588.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2b813eac54c4735f5c26e60f8e489f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-081d5bd5c4a8949e59c892091b8d625d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a0b0f27a953a70cfccd9689e0aaae3a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MAIS-Memory-Attention-for-Interactive-Segmentation"><a href="#MAIS-Memory-Attention-for-Interactive-Segmentation" class="headerlink" title="MAIS: Memory-Attention for Interactive Segmentation"></a>MAIS: Memory-Attention for Interactive Segmentation</h2><p><strong>Authors:Mauricio Orbes-Arteaga, Oeslle Lucena, Sabastien Ourselin, M. Jorge Cardoso</strong></p>
<p>Interactive medical segmentation reduces annotation effort by refining predictions through user feedback. Vision Transformer (ViT)-based models, such as the Segment Anything Model (SAM), achieve state-of-the-art performance using user clicks and prior masks as prompts. However, existing methods treat interactions as independent events, leading to redundant corrections and limited refinement gains. We address this by introducing MAIS, a Memory-Attention mechanism for Interactive Segmentation that stores past user inputs and segmentation states, enabling temporal context integration. Our approach enhances ViT-based segmentation across diverse imaging modalities, achieving more efficient and accurate refinements. </p>
<blockquote>
<p>äº¤äº’å¼åŒ»å­¦åˆ†å‰²é€šè¿‡ç”¨æˆ·åé¦ˆæ¥ä¼˜åŒ–é¢„æµ‹ï¼Œå‡å°‘äº†æ ‡æ³¨å·¥ä½œé‡ã€‚ä¾‹å¦‚ï¼ŒåŸºäºVision Transformerï¼ˆViTï¼‰çš„æ¨¡å‹ï¼Œå¦‚Segment Anything Modelï¼ˆSAMï¼‰ï¼Œé€šè¿‡ä½¿ç”¨ç”¨æˆ·ç‚¹å‡»å’Œå…ˆéªŒæ©è†œä½œä¸ºæç¤ºæ¥å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å°†äº¤äº’è§†ä¸ºç‹¬ç«‹äº‹ä»¶ï¼Œå¯¼è‡´å†—ä½™æ ¡æ­£å’Œæœ‰é™çš„ä¼˜åŒ–æ”¶ç›Šã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥MAISæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºäº¤äº’å¼åˆ†å‰²çš„è®°å¿†æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥å­˜å‚¨è¿‡å»çš„ç”¨æˆ·è¾“å…¥å’Œåˆ†å‰²çŠ¶æ€ï¼Œå®ç°æ—¶é—´ä¸Šä¸‹æ–‡çš„æ•´åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§æˆåƒæ¨¡å¼ä¸‹å¢å¼ºäº†åŸºäºViTçš„åˆ†å‰²ï¼Œå®ç°äº†æ›´é«˜æ•ˆå’Œå‡†ç¡®çš„ä¼˜åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07511v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦äº’åŠ¨åˆ†å‰²é€šè¿‡ç”¨æˆ·åé¦ˆä¼˜åŒ–é¢„æµ‹ï¼Œå‡å°‘äº†æ ‡æ³¨å·¥ä½œé‡ã€‚é‡‡ç”¨Vision Transformerï¼ˆViTï¼‰æ¨¡å‹çš„Segment Anything Modelï¼ˆSAMï¼‰åˆ©ç”¨ç”¨æˆ·ç‚¹å‡»å’Œå…ˆéªŒæ©è†œä½œä¸ºæç¤ºï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å°†äº’åŠ¨è§†ä¸ºç‹¬ç«‹äº‹ä»¶ï¼Œå¯¼è‡´å†—ä½™ä¿®æ­£å’Œæœ‰é™çš„ä¼˜åŒ–æ”¶ç›Šã€‚æˆ‘ä»¬æå‡ºé€šè¿‡å¼•å…¥MAISï¼ˆä¸€ç§ç”¨äºäº¤äº’å¼åˆ†å‰²çš„è®°å¿†æ³¨æ„åŠ›æœºåˆ¶ï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ƒå­˜å‚¨è¿‡å»ç”¨æˆ·è¾“å…¥å’Œåˆ†å‰²çŠ¶æ€ï¼Œå®ç°æ—¶é—´ä¸Šä¸‹æ–‡çš„æ•´åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†åŸºäºViTçš„è·¨å¤šç§æˆåƒæ¨¡æ€çš„åˆ†å‰²æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº’åŠ¨åŒ»å­¦åˆ†å‰²é€šè¿‡ç”¨æˆ·åé¦ˆå‡å°‘æ ‡æ³¨å·¥ä½œé‡ã€‚</li>
<li>Vision Transformerï¼ˆViTï¼‰æ¨¡å‹ï¼Œå¦‚Segment Anything Modelï¼ˆSAMï¼‰ï¼Œåˆ©ç”¨ç”¨æˆ·ç‚¹å‡»å’Œå…ˆéªŒæ©è†œè¾¾åˆ°é¡¶å°–æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å°†äº’åŠ¨è§†ä¸ºç‹¬ç«‹äº‹ä»¶ï¼Œé€ æˆå†—ä½™ä¿®æ­£å’Œæœ‰é™çš„ä¼˜åŒ–æ”¶ç›Šã€‚</li>
<li>å¼•å…¥MAISï¼ˆè®°å¿†æ³¨æ„åŠ›æœºåˆ¶ç”¨äºäº¤äº’å¼åˆ†å‰²ï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>MAISèƒ½å­˜å‚¨è¿‡å»ç”¨æˆ·è¾“å…¥å’Œåˆ†å‰²çŠ¶æ€ï¼Œå®ç°æ—¶é—´ä¸Šä¸‹æ–‡çš„æ•´åˆã€‚</li>
<li>æ–¹æ³•æé«˜äº†åŸºäºViTçš„è·¨å¤šç§æˆåƒæ¨¡æ€çš„åˆ†å‰²æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8461953008e2843822edbd42083cdf13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-157be76457fc34887c0dd9ed25922a91.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Learning-Graph-Representation-of-Agent-Diffuser"><a href="#Learning-Graph-Representation-of-Agent-Diffuser" class="headerlink" title="Learning Graph Representation of Agent Diffuser"></a>Learning Graph Representation of Agent Diffuser</h2><p><strong>Authors:Youcef Djenouri, Nassim Belmecheri, Tomasz Michalak, Jan DubiÅ„ski, Ahmed Nabil Belbachir, Anis Yazidi</strong></p>
<p>Diffusion-based generative models have significantly advanced text-to-image synthesis, demonstrating impressive text comprehension and zero-shot generalization. These models refine images from random noise based on textual prompts, with initial reliance on text input shifting towards enhanced visual fidelity over time. This transition suggests that static model parameters might not optimally address the distinct phases of generation. We introduce LGR-AD (Learning Graph Representation of Agent Diffusers), a novel multi-agent system designed to improve adaptability in dynamic computer vision tasks. LGR-AD models the generation process as a distributed system of interacting agents, each representing an expert sub-model. These agents dynamically adapt to varying conditions and collaborate through a graph neural network that encodes their relationships and performance metrics. Our approach employs a coordination mechanism based on top-$k$ maximum spanning trees, optimizing the generation process. Each agentâ€™s decision-making is guided by a meta-model that minimizes a novel loss function, balancing accuracy and diversity. Theoretical analysis and extensive empirical evaluations show that LGR-AD outperforms traditional diffusion models across various benchmarks, highlighting its potential for scalable and flexible solutions in complex image generation tasks. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/YousIA/LGR_AD">https://github.com/YousIA/LGR_AD</a> </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œå±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ–‡æœ¬ç†è§£å’Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹æ ¹æ®æ–‡æœ¬æç¤ºä»éšæœºå™ªå£°ä¸­ç»†åŒ–å›¾åƒï¼Œæœ€åˆä¾èµ–æ–‡æœ¬è¾“å…¥ï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œå¯¹è§†è§‰ä¿çœŸåº¦çš„å…³æ³¨é€æ¸å¢å¼ºã€‚è¿™ç§è½¬å˜è¡¨æ˜ï¼Œé™æ€æ¨¡å‹å‚æ•°å¯èƒ½æ— æ³•æœ€ä½³åœ°åº”å¯¹ç”Ÿæˆçš„ä¸åŒé˜¶æ®µã€‚æˆ‘ä»¬å¼•å…¥äº†LGR-ADï¼ˆå­¦ä¹ Agentæ‰©æ•£çš„å›¾è¡¨ç¤ºï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å¤šAgentç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜åŠ¨æ€è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„é€‚åº”æ€§ã€‚LGR-ADå°†ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºäº¤äº’Agentçš„åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œæ¯ä¸ªAgentä»£è¡¨ä¸€ä¸ªä¸“å®¶å­æ¨¡å‹ã€‚è¿™äº›Agentå¯ä»¥åŠ¨æ€é€‚åº”å„ç§æ¡ä»¶ï¼Œå¹¶é€šè¿‡ç¼–ç å…¶å…³ç³»å’Œæ€§èƒ½æŒ‡æ ‡çš„å›¾ç¥ç»ç½‘ç»œè¿›è¡Œåä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åŸºäºtop-kæœ€å¤§ç”Ÿæˆæ ‘çš„åè°ƒæœºåˆ¶ï¼Œä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚æ¯ä¸ªAgentçš„å†³ç­–ç”±å…ƒæ¨¡å‹å¼•å¯¼ï¼Œè¯¥æ¨¡å‹æœ€å°åŒ–æ–°å‹æŸå¤±å‡½æ•°ï¼Œä»¥å¹³è¡¡ç²¾åº¦å’Œå¤šæ ·æ€§ã€‚ç†è®ºåˆ†æå’Œå¹¿æ³›çš„å®è¯ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLGR-ADçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼Œçªæ˜¾å…¶åœ¨å¤æ‚å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å¯æ‰©å±•å’Œçµæ´»è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/YousIA/LGR_AD">https://github.com/YousIA/LGR_AD</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06761v1">PDF</a> Accepted at AAMAS2025 International Conference on Autonomous Agents   and Multiagent Systems</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå¼æ¨¡å‹åœ¨æ–‡æœ¬é©±åŠ¨å›¾åƒåˆæˆæ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œè¿™äº›æ¨¡å‹å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ–‡æœ¬ç†è§£èƒ½åŠ›å’Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æå‡ºä¸€ç§æ–°å‹å¤šä»£ç†ç³»ç»ŸLGR-ADï¼Œé€šè¿‡å›¾ç¥ç»ç½‘ç»œåŠ¨æ€é€‚åº”ä¸åŒæ¡ä»¶å¹¶åä½œå®Œæˆè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚æ¯ä¸ªä»£ç†ä»£è¡¨ä¸€ä¸ªä¸“å®¶å­æ¨¡å‹ï¼Œé€šè¿‡åŸºäºtop-kæœ€å¤§ç”Ÿæˆæ ‘çš„åè°ƒæœºåˆ¶ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚ç†è®ºåˆ†æå’Œå¤§é‡å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒLGR-ADåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼Œåœ¨å¤æ‚å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å…·æœ‰å¯æ‰©å±•å’Œçµæ´»è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬é©±åŠ¨å›¾åƒåˆæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œå±•ç°æ–‡æœ¬ç†è§£å’Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>LGR-ADæ˜¯ä¸€ç§æ–°å‹å¤šä»£ç†ç³»ç»Ÿï¼Œç”¨äºæ”¹è¿›åŠ¨æ€è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„é€‚åº”æ€§ã€‚</li>
<li>LGR-ADæ¨¡å‹é‡‡ç”¨å›¾ç¥ç»ç½‘ç»œç¼–ç ä»£ç†é—´å…³ç³»å’Œæ€§èƒ½åº¦é‡ã€‚</li>
<li>LGR-ADé€šè¿‡åŸºäºtop-kæœ€å¤§ç”Ÿæˆæ ‘çš„åè°ƒæœºåˆ¶ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>æ¯ä¸ªä»£ç†çš„å†³ç­–åˆ¶å®šç”±å…ƒæ¨¡å‹å¼•å¯¼ï¼Œé‡‡ç”¨æ–°å‹æŸå¤±å‡½æ•°å¹³è¡¡å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>ç†è®ºåˆ†æå’Œå¤§é‡å®è¯è¯„ä¼°è¡¨æ˜LGR-ADåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>LGR-ADå…·æœ‰å¯æ‰©å±•å’Œçµæ´»è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ï¼Œå°¤å…¶åœ¨å¤æ‚å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c7dbc6a231835cc47324f068ef684738.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-428f003d5fa29766e837326999873613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6fb096f40e4bb62548185964273e4ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e04501f70bda661fdd33dba4c732937.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea680203ef9b9c0c6c245cd82b80e2ee.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Towards-Better-Cephalometric-Landmark-Detection-with-Diffusion-Data-Generation"><a href="#Towards-Better-Cephalometric-Landmark-Detection-with-Diffusion-Data-Generation" class="headerlink" title="Towards Better Cephalometric Landmark Detection with Diffusion Data   Generation"></a>Towards Better Cephalometric Landmark Detection with Diffusion Data   Generation</h2><p><strong>Authors:Dongqian Guo, Wencheng Han, Pang Lyu, Yuxi Zhou, Jianbing Shen</strong></p>
<p>Cephalometric landmark detection is essential for orthodontic diagnostics and treatment planning. Nevertheless, the scarcity of samples in data collection and the extensive effort required for manual annotation have significantly impeded the availability of diverse datasets. This limitation has restricted the effectiveness of deep learning-based detection methods, particularly those based on large-scale vision models. To address these challenges, we have developed an innovative data generation method capable of producing diverse cephalometric X-ray images along with corresponding annotations without human intervention. To achieve this, our approach initiates by constructing new cephalometric landmark annotations using anatomical priors. Then, we employ a diffusion-based generator to create realistic X-ray images that correspond closely with these annotations. To achieve precise control in producing samples with different attributes, we introduce a novel prompt cephalometric X-ray image dataset. This dataset includes real cephalometric X-ray images and detailed medical text prompts describing the images. By leveraging these detailed prompts, our method improves the generation process to control different styles and attributes. Facilitated by the large, diverse generated data, we introduce large-scale vision detection models into the cephalometric landmark detection task to improve accuracy. Experimental results demonstrate that training with the generated data substantially enhances the performance. Compared to methods without using the generated data, our approach improves the Success Detection Rate (SDR) by 6.5%, attaining a notable 82.2%. All code and data are available at: <a target="_blank" rel="noopener" href="https://um-lab.github.io/cepha-generation">https://um-lab.github.io/cepha-generation</a> </p>
<blockquote>
<p>å¤´å½±æµ‹é‡æ ‡å¿—æ£€æµ‹åœ¨æ­£ç•¸è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ•°æ®é‡‡é›†ä¸­çš„æ ·æœ¬ç¨€ç¼ºä»¥åŠæ‰‹åŠ¨æ ‡æ³¨æ‰€éœ€çš„å¤§é‡åŠªåŠ›æå¤§åœ°é˜»ç¢äº†å¤šæ ·åŒ–æ•°æ®é›†çš„å¯ç”¨æ€§ã€‚è¿™ä¸€å±€é™æ€§é™åˆ¶äº†åŸºäºæ·±åº¦å­¦ä¹ æ£€æµ‹æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åŸºäºå¤§è§„æ¨¡è§†è§‰æ¨¡å‹çš„æ–¹æ³•ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åˆ›æ–°çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿæ— éœ€äººå·¥å¹²é¢„åœ°ç”Ÿæˆå¤šæ ·åŒ–çš„å¤´å½±æµ‹é‡Xå°„çº¿å›¾åƒä»¥åŠç›¸åº”çš„æ ‡æ³¨ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåˆ©ç”¨è§£å‰–å…ˆéªŒæ„å»ºæ–°çš„å¤´å½±æµ‹é‡æ ‡å¿—æ ‡æ³¨ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºæ‰©æ•£çš„ç”Ÿæˆå™¨æ¥åˆ›å»ºä¸è¿™äº›æ ‡æ³¨ç´§å¯†å¯¹åº”çš„é€¼çœŸçš„Xå°„çº¿å›¾åƒã€‚ä¸ºäº†å®ç°å…·æœ‰ä¸åŒå±æ€§çš„æ ·æœ¬ç”Ÿæˆçš„ç²¾ç¡®æ§åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æç¤ºå¤´å½±æµ‹é‡Xå°„çº¿å›¾åƒæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«çœŸå®çš„å¤´å½±æµ‹é‡Xå°„çº¿å›¾åƒå’Œè¯¦ç»†çš„åŒ»å­¦æ–‡æœ¬æç¤ºï¼Œæè¿°è¿™äº›å›¾åƒã€‚é€šè¿‡åˆ©ç”¨è¿™äº›è¯¦ç»†çš„æç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ”¹è¿›äº†ç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥æ§åˆ¶ä¸åŒçš„é£æ ¼å’Œå±æ€§ã€‚é€šè¿‡å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„ç”Ÿæˆæ•°æ®ï¼Œæˆ‘ä»¬å°†å¤§è§„æ¨¡è§†è§‰æ£€æµ‹æ¨¡å‹å¼•å…¥åˆ°å¤´å½±æµ‹é‡æ ‡å¿—æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œä»¥æé«˜å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç”Ÿæˆæ•°æ®è¿›è¡Œè®­ç»ƒèƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚ä¸ä¸ä½¿ç”¨ç”Ÿæˆæ•°æ®çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸæ£€æµ‹ç‡ï¼ˆSDRï¼‰æé«˜äº†6.5%ï¼Œè¾¾åˆ°äº†82.2%ï¼Œè¿™ä¸€æˆæœå€¼å¾—æ³¨æ„ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®éƒ½å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://um-lab.github.io/cepha-generation%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://um-lab.github.io/cepha-generationä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06055v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ›æ–°çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œç”¨äºè§£å†³åœ¨æ­£ç•¸è¯Šæ–­ä¸æ²»ç–—è®¡åˆ’ä¸­å¤´å½±æµ‹é‡æ ‡å¿—æ£€æµ‹æ ·æœ¬ç¨€ç¼ºçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆè§£å‰–å­¦å…ˆéªŒçŸ¥è¯†æ„å»ºæ–°çš„å¤´å½±æµ‹é‡æ ‡å¿—æ³¨é‡Šï¼Œå¹¶é‡‡ç”¨åŸºäºæ‰©æ•£çš„ç”Ÿæˆå™¨åˆ›å»ºä¸è¿™äº›æ³¨é‡Šç›¸å¯¹åº”çš„é€¼çœŸçš„Xå°„çº¿å›¾åƒï¼Œå®ç°äº†æ— éœ€äººå·¥å¹²é¢„çš„å¤šæ ·åŒ–æ•°æ®ç”Ÿæˆã€‚é€šè¿‡å¼•å…¥å¸¦æœ‰è¯¦ç»†æè¿°åŒ»ç–—æ–‡æœ¬æç¤ºçš„æç¤ºå¤´å½±Xå°„çº¿å›¾åƒæ•°æ®é›†ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ§åˆ¶ä¸åŒæ ·å¼å’Œå±æ€§ï¼Œæé«˜ç”Ÿæˆè¿‡ç¨‹çš„ç²¾åº¦ã€‚åˆ©ç”¨å¤§è§„æ¨¡ç”Ÿæˆçš„å¤šæ ·æ•°æ®ï¼Œå°†å¤§å‹è§†è§‰æ£€æµ‹æ¨¡å‹å¼•å…¥å¤´å½±æµ‹é‡æ ‡å¿—æ£€æµ‹ä»»åŠ¡ï¼Œæé«˜äº†æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç”Ÿæˆæ•°æ®è¿›è¡Œè®­ç»ƒå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œä¸ä¸ä½¿ç”¨ç”Ÿæˆæ•°æ®çš„æ–¹æ³•ç›¸æ¯”ï¼ŒæˆåŠŸç‡æ£€æµ‹ç‡ï¼ˆSDRï¼‰æé«˜äº†6.5%ï¼Œè¾¾åˆ°äº†82.2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§£å†³æ ·æœ¬ç¨€ç¼ºå’Œæ‰‹åŠ¨æ ‡æ³¨è´¹åŠ›çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§åˆ›æ–°çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨è§£å‰–å­¦å…ˆéªŒçŸ¥è¯†æ„å»ºæ–°çš„å¤´å½±æµ‹é‡æ ‡å¿—æ³¨é‡Šã€‚</li>
<li>é‡‡ç”¨åŸºäºæ‰©æ•£çš„ç”Ÿæˆå™¨åˆ›å»ºä¸æ³¨é‡Šç›¸å¯¹åº”çš„é€¼çœŸçš„Xå°„çº¿å›¾åƒã€‚</li>
<li>å¼•å…¥å¸¦æœ‰åŒ»ç–—æ–‡æœ¬æç¤ºçš„æç¤ºå¤´å½±Xå°„çº¿å›¾åƒæ•°æ®é›†ï¼Œæ§åˆ¶ä¸åŒæ ·å¼å’Œå±æ€§ã€‚</li>
<li>åˆ©ç”¨å¤§è§„æ¨¡ç”Ÿæˆçš„å¤šæ ·æ•°æ®ï¼Œå¼•å…¥å¤§å‹è§†è§‰æ£€æµ‹æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨ç”Ÿæˆæ•°æ®è¿›è¡Œè®­ç»ƒå¯ä»¥æ˜¾è‘—æé«˜å¤´å½±æµ‹é‡æ ‡å¿—æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-14\./crop_Vision Transformer/2505.06055v1/page_0_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9ec0f44762a73dd41794af7ad17c971.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8f6f5f35d16a8fb48a791e28a6c1af7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5e9eaaa5c659dd360daacfbe609f3aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a72277af3a8b416b49bcb8f86d7816b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdc26fb6ac01279cd7f59bfdf9e5dc9c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Towards-Facial-Image-Compression-with-Consistency-Preserving-Diffusion-Prior"><a href="#Towards-Facial-Image-Compression-with-Consistency-Preserving-Diffusion-Prior" class="headerlink" title="Towards Facial Image Compression with Consistency Preserving Diffusion   Prior"></a>Towards Facial Image Compression with Consistency Preserving Diffusion   Prior</h2><p><strong>Authors:Yimin Zhou, Yichong Xia, Bin Chen, Baoyi An, Haoqian Wang, Zhi Wang, Yaowei Wang, Zikun Zhou</strong></p>
<p>With the widespread application of facial image data across various domains, the efficient storage and transmission of facial images has garnered significant attention. However, the existing learned face image compression methods often produce unsatisfactory reconstructed image quality at low bit rates. Simply adapting diffusion-based compression methods to facial compression tasks results in reconstructed images that perform poorly in downstream applications due to insufficient preservation of high-frequency information. To further explore the diffusion prior in facial image compression, we propose Facial Image Compression with a Stable Diffusion Prior (FaSDiff), a method that preserves consistency through frequency enhancement. FaSDiff employs a high-frequency-sensitive compressor in an end-to-end framework to capture fine image details and produce robust visual prompts. Additionally, we introduce a hybrid low-frequency enhancement module that disentangles low-frequency facial semantics and stably modulates the diffusion prior alongside visual prompts. The proposed modules allow FaSDiff to leverage diffusion priors for superior human visual perception while minimizing performance loss in machine vision due to semantic inconsistency. Extensive experiments show that FaSDiff outperforms state-of-the-art methods in balancing human visual quality and machine vision accuracy. The code will be released after the paper is accepted. </p>
<blockquote>
<p>éšç€é¢éƒ¨å›¾åƒæ•°æ®åœ¨å„ä¸ªé¢†åŸŸåº”ç”¨çš„æ™®åŠï¼Œé¢éƒ¨å›¾åƒçš„å­˜å‚¨å’Œä¼ è¾“æ•ˆç‡é—®é¢˜å¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å­¦ä¹ é¢éƒ¨å›¾åƒå‹ç¼©æ–¹æ³•åœ¨ä½æ¯”ç‰¹ç‡ä¸‹äº§ç”Ÿçš„é‡å»ºå›¾åƒè´¨é‡å¾€å¾€ä¸å°½äººæ„ã€‚ç®€å•åœ°å°†åŸºäºæ‰©æ•£çš„å‹ç¼©æ–¹æ³•åº”ç”¨äºé¢éƒ¨å‹ç¼©ä»»åŠ¡ä¼šå¯¼è‡´é‡å»ºå›¾åƒåœ¨ä¸‹æ¸¸åº”ç”¨ä¸­è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºé«˜é¢‘ä¿¡æ¯çš„ä¿ç•™ä¸è¶³ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç ”ç©¶é¢éƒ¨å›¾åƒå‹ç¼©ä¸­çš„æ‰©æ•£å…ˆéªŒçŸ¥è¯†ï¼Œæˆ‘ä»¬æå‡ºäº†å¸¦æœ‰ç¨³å®šæ‰©æ•£å…ˆéªŒçš„é¢éƒ¨å›¾åƒå‹ç¼©ï¼ˆFaSDiffï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡é¢‘ç‡å¢å¼ºä¿æŒä¸€è‡´æ€§ã€‚FaSDiffé‡‡ç”¨ä¸€ç§é«˜é¢‘æ•æ„Ÿå‹ç¼©æœºï¼Œåœ¨ç«¯åˆ°ç«¯æ¡†æ¶ä¸­æ•æ‰å›¾åƒç»†èŠ‚ï¼Œç”Ÿæˆç¨³å¥çš„è§†è§‰æç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ··åˆä½é¢‘å¢å¼ºæ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿåˆ†ç¦»ä½é¢‘é¢éƒ¨è¯­ä¹‰ï¼Œå¹¶ç¨³å®šåœ°è°ƒèŠ‚æ‰©æ•£å…ˆéªŒä¸è§†è§‰æç¤ºã€‚æ‰€æå‡ºçš„æ¨¡å—ä½¿FaSDiffèƒ½å¤Ÿåˆ©ç”¨æ‰©æ•£å…ˆéªŒæ¥æé«˜äººç±»è§†è§‰æ„ŸçŸ¥ï¼ŒåŒæ—¶æœ€å°åŒ–ç”±äºè¯­ä¹‰ä¸ä¸€è‡´è€Œå¯¼è‡´çš„æœºå™¨è§†è§‰æ€§èƒ½æŸå¤±ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFaSDiffåœ¨å¹³è¡¡äººç±»è§†è§‰è´¨é‡å’Œæœºå™¨è§†è§‰å‡†ç¡®æ€§æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è®ºæ–‡è¢«æ¥å—åï¼Œä»£ç å°†å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05870v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹é¢éƒ¨å›¾åƒæ•°æ®åœ¨å„ä¸ªé¢†åŸŸå¹¿æ³›åº”ç”¨çš„éœ€æ±‚ï¼Œæå‡ºä¸€ç§å¸¦æœ‰ç¨³å®šæ‰©æ•£å…ˆéªŒçš„é¢éƒ¨å›¾åƒå‹ç¼©æ–¹æ³•ï¼ˆFaSDiffï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡é¢‘ç‡å¢å¼ºä¿æŒä¸€è‡´æ€§ï¼Œé‡‡ç”¨ç«¯å¯¹ç«¯æ¡†æ¶ä¸­çš„é«˜é¢‘æ•æ„Ÿå‹ç¼©æœºæ¥æ•æ‰å›¾åƒç»†èŠ‚å¹¶äº§ç”Ÿç¨³å¥çš„è§†è§‰æç¤ºã€‚å¼•å…¥æ··åˆä½é¢‘å¢å¼ºæ¨¡å—ï¼Œåˆ†ç¦»ä½é¢‘é¢éƒ¨è¯­ä¹‰ï¼Œç¨³å®šè°ƒèŠ‚æ‰©æ•£å…ˆéªŒä¸è§†è§‰æç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒFaSDiffåœ¨å¹³è¡¡äººç±»è§†è§‰è´¨é‡å’Œæœºå™¨è§†è§‰å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é¢éƒ¨å›¾åƒæ•°æ®çš„å¹¿æ³›åº”ç”¨ä¿ƒä½¿äº†å¯¹å…¶é«˜æ•ˆå­˜å‚¨å’Œä¼ è¾“çš„éœ€æ±‚ã€‚</li>
<li>ç°æœ‰å­¦ä¹ é¢éƒ¨å›¾åƒå‹ç¼©æ–¹æ³•åœ¨ä½ä½ç‡ä¸‹é‡å»ºå›¾åƒè´¨é‡ä¸ä½³ã€‚</li>
<li>æ‰©æ•£å…ˆéªŒåœ¨é¢éƒ¨å›¾åƒå‹ç¼©ä¸­çš„åº”ç”¨å¯ä»¥æé«˜é‡å»ºå›¾åƒçš„è´¨é‡ã€‚</li>
<li>FaSDiffæ–¹æ³•é€šè¿‡é¢‘ç‡å¢å¼ºä¿æŒä¸€è‡´æ€§ï¼Œé‡‡ç”¨é«˜é¢‘æ•æ„Ÿå‹ç¼©æœºæ•æ‰å›¾åƒç»†èŠ‚ã€‚</li>
<li>æ··åˆä½é¢‘å¢å¼ºæ¨¡å—ç”¨äºåˆ†ç¦»ä½é¢‘é¢éƒ¨è¯­ä¹‰ï¼Œå¹¶ä¸æ‰©æ•£å…ˆéªŒå’Œè§†è§‰æç¤ºç›¸ç»“åˆã€‚</li>
<li>FaSDiffåœ¨å¹³è¡¡äººç±»è§†è§‰è´¨é‡å’Œæœºå™¨è§†è§‰å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
<li>ä»£ç å°†åœ¨è®ºæ–‡è¢«æ¥å—åå‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05870">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-288de31456c3cf2ee5c793fed9f947c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-75b86a4ec5fe6bc70a1351e61df24de5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52ba6189578118bab3efed8ad9f96224.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69035760be2ac5b80825f1f1f30faa10.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Image-Recognition-with-Online-Lightweight-Vision-Transformer-A-Survey"><a href="#Image-Recognition-with-Online-Lightweight-Vision-Transformer-A-Survey" class="headerlink" title="Image Recognition with Online Lightweight Vision Transformer: A Survey"></a>Image Recognition with Online Lightweight Vision Transformer: A Survey</h2><p><strong>Authors:Zherui Zhang, Rongtao Xu, Jie Zhou, Changwei Wang, Xingtian Pei, Wenhao Xu, Jiguang Zhang, Li Guo, Longxiang Gao, Wenbo Xu, Shibiao Xu</strong></p>
<p>The Transformer architecture has achieved significant success in natural language processing, motivating its adaptation to computer vision tasks. Unlike convolutional neural networks, vision transformers inherently capture long-range dependencies and enable parallel processing, yet lack inductive biases and efficiency benefits, facing significant computational and memory challenges that limit its real-world applicability. This paper surveys various online strategies for generating lightweight vision transformers for image recognition, focusing on three key areas: Efficient Component Design, Dynamic Network, and Knowledge Distillation. We evaluate the relevant exploration for each topic on the ImageNet-1K benchmark, analyzing trade-offs among precision, parameters, throughput, and more to highlight their respective advantages, disadvantages, and flexibility. Finally, we propose future research directions and potential challenges in the lightweighting of vision transformers with the aim of inspiring further exploration and providing practical guidance for the community. Project Page: <a target="_blank" rel="noopener" href="https://github.com/ajxklo/Lightweight-VIT">https://github.com/ajxklo/Lightweight-VIT</a> </p>
<blockquote>
<p>Transformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œè¿™ä¿ƒä½¿å…¶é€‚åº”è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚ä¸å·ç§¯ç¥ç»ç½‘ç»œä¸åŒï¼Œè§†è§‰Transformerå¤©ç”Ÿå°±èƒ½æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»å¹¶å®ç°å¹¶è¡Œå¤„ç†ï¼Œç„¶è€Œç”±äºç¼ºä¹å½’çº³åè§å’Œæ•ˆç‡ä¼˜åŠ¿ï¼Œå®ƒä»¬é¢ä¸´ç€è®¡ç®—å’Œå†…å­˜æ–¹é¢çš„æŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡ç»¼è¿°äº†ä¸ºå›¾åƒè¯†åˆ«ç”Ÿæˆè½»é‡åŒ–è§†è§‰Transformerçš„å„ç§åœ¨çº¿ç­–ç•¥ï¼Œé‡ç‚¹å…³æ³¨ä¸‰ä¸ªå…³é”®é¢†åŸŸï¼šé«˜æ•ˆç»„ä»¶è®¾è®¡ã€åŠ¨æ€ç½‘ç»œå’ŒçŸ¥è¯†è’¸é¦ã€‚æˆ‘ä»¬åœ¨ImageNet-1KåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æ¯ä¸ªè¯é¢˜çš„ç›¸å…³æ¢ç´¢ï¼Œåˆ†æäº†ç²¾åº¦ã€å‚æ•°ã€ååé‡ç­‰ä¹‹é—´çš„æƒè¡¡ï¼Œä»¥çªå‡ºå„è‡ªçš„ä¼˜åŠ¿ã€åŠ£åŠ¿å’Œçµæ´»æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰Transformerè½»é‡åŒ–çš„æœªæ¥ç ”ç©¶æ–¹å‘å’Œæ½œåœ¨æŒ‘æˆ˜ï¼Œæ—¨åœ¨æ¿€å‘è¿›ä¸€æ­¥æ¢ç´¢ï¼Œä¸ºç¤¾åŒºæä¾›å®ç”¨æŒ‡å¯¼ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/ajxklo/Lightweight-VIT">https://github.com/ajxklo/Lightweight-VIT</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03113v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†å°†Transformeræ¶æ„åº”ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒè¯†åˆ«æ–¹é¢çš„åº”ç”¨ã€‚æœ¬æ–‡é‡ç‚¹å…³æ³¨äº†ä¸‰ä¸ªå…³é”®é¢†åŸŸï¼šé«˜æ•ˆç»„ä»¶è®¾è®¡ã€åŠ¨æ€ç½‘ç»œå’ŒçŸ¥è¯†è’¸é¦ã€‚æ–‡ç« è¯„ä»·äº†è¿™ä¸‰ä¸ªä¸»é¢˜åœ¨ImageNet-1KåŸºå‡†æµ‹è¯•ä¸Šçš„ç›¸å…³ç ”ç©¶ï¼Œåˆ†æäº†ç²¾ç¡®åº¦ã€å‚æ•°ã€ååé‡å’Œçµæ´»æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æŒ‡å‡ºäº†å„è‡ªçš„ä¼˜ç¼ºç‚¹ã€‚æœ€åï¼Œæœ¬æ–‡æå‡ºäº†è½»é‡çº§è§†è§‰å˜å‹å™¨çš„æœªæ¥ç ”ç©¶æ–¹å‘å’Œæ½œåœ¨æŒ‘æˆ˜ï¼Œæ—¨åœ¨ä¸ºç¤¾åŒºæä¾›å®é™…æŒ‡å¯¼å’Œæ¿€åŠ±è¿›ä¸€æ­¥çš„æ¢ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¶æ„åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„æˆåŠŸåº”ç”¨å¼•äººæ³¨ç›®ã€‚</li>
<li>è§†è§‰å˜å‹å™¨èƒ½å¤Ÿæ•è·é•¿è·ç¦»ä¾èµ–æ€§å’Œæ”¯æŒå¹¶è¡Œå¤„ç†ã€‚</li>
<li>è§†è§‰å˜å‹å™¨é¢ä¸´è®¡ç®—èµ„æºå’Œå†…å­˜çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚</li>
<li>æ–‡ç« æ¢è®¨äº†ç”Ÿæˆè½»é‡çº§è§†è§‰å˜å‹å™¨çš„åœ¨çº¿ç­–ç•¥ï¼Œä¸»è¦é›†ä¸­åœ¨é«˜æ•ˆç»„ä»¶è®¾è®¡ã€åŠ¨æ€ç½‘ç»œå’ŒçŸ¥è¯†è’¸é¦æ–¹é¢ã€‚</li>
<li>åœ¨ImageNet-1KåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†ç›¸å…³ç ”ç©¶çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ç²¾ç¡®åº¦ã€å‚æ•°ã€ååé‡å’Œçµæ´»æ€§ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºäº†æ¯ä¸ªé¢†åŸŸçš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d68b5a4e31e5bfd85b909a6e0d6a94e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92ca69cc9de51d3773f2d7bb86baa851.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70040e8ba5772d36233cca82eef78b9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef94e7aeac77304479c3539d58d6a41a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90b28395b9db504dbc2e491947606c55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ab60760317aa345952e1c315f1d2547.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VladVA-Discriminative-Fine-tuning-of-LVLMs"><a href="#VladVA-Discriminative-Fine-tuning-of-LVLMs" class="headerlink" title="VladVA: Discriminative Fine-tuning of LVLMs"></a>VladVA: Discriminative Fine-tuning of LVLMs</h2><p><strong>Authors:Yassine Ouali, Adrian Bulat, Alexandros Xenos, Anestis Zaganidis, Ioannis Maniadis Metaxas, Brais Martinez, Georgios Tzimiropoulos</strong></p>
<p>Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a â€œbag of wordsâ€ behavior. At the same time, Large Vision-Language Models (LVLMs), which combine vision encoders with LLMs, have been shown to be capable of detailed vision-language reasoning, yet their autoregressive nature renders them less suitable for discriminative tasks.   In this work, we propose to combine â€œthe best of both worldsâ€: a new training approach for discriminative fine-tuning of LVLMs that results in strong discriminative and compositional capabilities. Essentially, our approach converts a generative LVLM into a discriminative one, unlocking its capability for powerful image-text discrimination combined with enhanced language understanding.   Our contributions include (1) a carefully designed training&#x2F;optimization framework that utilizes image-text pairs of variable length and granularity for training the model with both contrastive and next-token prediction losses. This is accompanied by ablation studies that justify the necessity of our frameworkâ€™s components; (2) a parameter-efficient adaptation method using a combination of soft prompting and LoRA adapters; (3) significant improvements over state-of-the-art CLIP-like models of similar size, including standard image-text retrieval benchmarks and notable gains in compositionality. </p>
<blockquote>
<p>å¯¹æ¯”è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰å·²æˆä¸ºåˆ¤åˆ«å¼è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ çš„å®é™…æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„è¯­è¨€ç†è§£èƒ½åŠ›æœ‰é™ï¼Œé€šå¸¸è¡¨ç°å‡ºâ€œè¯è¢‹â€è¡Œä¸ºã€‚åŒæ—¶ï¼Œç»“åˆäº†è§†è§‰ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰å·²æ˜¾ç¤ºå‡ºè¯¦ç»†çš„è§†è§‰è¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶è‡ªå›å½’æ€§è´¨ä½¿å¾—å®ƒä»¬ä¸å¤ªé€‚åˆåˆ¤åˆ«ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç»“åˆä¸¤è€…ä¼˜ç‚¹çš„æ–¹æ¡ˆï¼šä¸€ç§ç”¨äºLVLMåˆ¤åˆ«å¾®è°ƒçš„æ–°è®­ç»ƒæ–¹æ³•ï¼Œå…·æœ‰å¾ˆå¼ºçš„åˆ¤åˆ«å’Œç»„åˆèƒ½åŠ›ã€‚æœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†ç”Ÿæˆå¼LVLMè½¬æ¢ä¸ºåˆ¤åˆ«å¼ï¼Œè§£é”å…¶å¼ºå¤§çš„å›¾åƒæ–‡æœ¬åˆ¤åˆ«èƒ½åŠ›ï¼ŒåŒæ—¶å¢å¼ºè¯­è¨€ç†è§£ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„è®­ç»ƒ&#x2F;ä¼˜åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¯å˜é•¿åº¦å’Œç²’åº¦çš„å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨å¯¹æ¯”å’Œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æŸå¤±æ¥è®­ç»ƒæ¨¡å‹ã€‚è¿™ä¼´éšä¸€äº›æ¶ˆèç ”ç©¶ï¼Œä»¥è¯æ˜æˆ‘ä»¬æ¡†æ¶ç»„ä»¶çš„å¿…è¦æ€§ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨è½¯æç¤ºå’ŒLoRAé€‚é…å™¨ç›¸ç»“åˆçš„æ–¹æ³•å®ç°å‚æ•°é«˜æ•ˆé€‚åº”ï¼›ï¼ˆ3ï¼‰åœ¨ç±»ä¼¼è§„æ¨¡çš„å…ˆè¿›CLIPæ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒ…æ‹¬æ ‡å‡†å›¾åƒæ–‡æœ¬æ£€ç´¢åŸºå‡†æµ‹è¯•å’Œç»„åˆæ€§çš„æ˜¾è‘—æ”¶ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04378v3">PDF</a> Published at CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºCLIPç­‰å¯¹æ¯”è®­ç»ƒçš„å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åˆ¤åˆ«å¼è§†è§‰è¯­è¨€è¡¨å¾å­¦ä¹ ä¸Šè¡¨ç°çªå‡ºï¼Œä½†å­˜åœ¨è¯­è¨€ç†è§£æœ‰é™çš„é—®é¢˜ã€‚å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ç»“åˆäº†è§†è§‰ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå±•ç°å‡ºè¯¦ç»†çš„è§†è§‰è¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶è‡ªå›å½’ç‰¹æ€§ä½¿å¾—å®ƒä»¬åœ¨åˆ¤åˆ«ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚æœ¬ç ”ç©¶æ—¨åœ¨ç»“åˆä¸¤è€…çš„ä¼˜ç‚¹ï¼Œæå‡ºä¸€ç§é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„åˆ¤åˆ«å¾®è°ƒæ–°è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä½¿æ¨¡å‹å…¼å…·å¼ºå¤§çš„åˆ¤åˆ«èƒ½åŠ›å’Œç»„åˆèƒ½åŠ›ã€‚ç ”ç©¶è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰åˆ©ç”¨å¯å˜é•¿åº¦å’Œç²’åº¦çš„å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œè®­ç»ƒ&#x2F;ä¼˜åŒ–çš„æ¡†æ¶ï¼Œç»“åˆå¯¹æ¯”å’Œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æŸå¤±ï¼›ï¼ˆ2ï¼‰é‡‡ç”¨ç»“åˆè½¯æç¤ºå’ŒLoRAé€‚é…å™¨çš„å‚æ•°é«˜æ•ˆé€‚é…æ–¹æ³•ï¼›ï¼ˆ3ï¼‰æ˜¾è‘—æ”¹è¿›äº†ç±»ä¼¼è§„æ¨¡çš„CLIPæ¨¡å‹ï¼ŒåŒ…æ‹¬å›¾åƒæ–‡æœ¬æ£€ç´¢åŸºå‡†æµ‹è¯•æˆç»©å’Œç»„åˆæ€§çš„æ˜æ˜¾æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”è®­ç»ƒçš„å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨åˆ¤åˆ«å¼è§†è§‰è¯­è¨€è¡¨å¾å­¦ä¹ ä¸Šå…·æœ‰ä¼˜åŠ¿ï¼Œä½†å­˜åœ¨è¯­è¨€ç†è§£å±€é™ã€‚</li>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å…·å¤‡è¯¦ç»†çš„è§†è§‰è¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œä½†è‡ªå›å½’ç‰¹æ€§ä½¿å…¶ä¸é€‚ç”¨äºåˆ¤åˆ«ä»»åŠ¡ã€‚</li>
<li>æœ¬ç ”ç©¶æ—¨åœ¨ç»“åˆä¸¤è€…çš„ä¼˜ç‚¹ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹LVLMsçš„åˆ¤åˆ«å¾®è°ƒæ–°è®­ç»ƒç­–ç•¥ã€‚</li>
<li>è®­ç»ƒ&#x2F;ä¼˜åŒ–æ¡†æ¶åˆ©ç”¨å¯å˜é•¿åº¦å’Œç²’åº¦çš„å›¾åƒæ–‡æœ¬å¯¹ï¼Œç»“åˆå¯¹æ¯”å’Œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æŸå¤±ã€‚</li>
<li>é‡‡ç”¨ç»“åˆè½¯æç¤ºå’ŒLoRAé€‚é…å™¨çš„å‚æ•°é«˜æ•ˆé€‚é…æ–¹æ³•ã€‚</li>
<li>ä¸ç±»ä¼¼è§„æ¨¡çš„CLIPæ¨¡å‹ç›¸æ¯”ï¼Œè¯¥ç­–ç•¥åœ¨å›¾åƒæ–‡æœ¬æ£€ç´¢ç­‰æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c61946fe29b04e59650501403bd08810.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23e0db73c7c041af71fa3b879db3fc5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5f36edc784e6168cd6a5d63fd8beee3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38a7cc54fe749be62414da913370e462.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a44605d800cfef4ce73ec52b8910267a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MambaNUT-Nighttime-UAV-Tracking-via-Mamba-based-Adaptive-Curriculum-Learning"><a href="#MambaNUT-Nighttime-UAV-Tracking-via-Mamba-based-Adaptive-Curriculum-Learning" class="headerlink" title="MambaNUT: Nighttime UAV Tracking via Mamba-based Adaptive Curriculum   Learning"></a>MambaNUT: Nighttime UAV Tracking via Mamba-based Adaptive Curriculum   Learning</h2><p><strong>Authors:You Wu, Xiangyang Yang, Xucheng Wang, Hengzhou Ye, Dan Zeng, Shuiwang Li</strong></p>
<p>Harnessing low-light enhancement and domain adaptation, nighttime UAV tracking has made substantial strides. However, over-reliance on image enhancement, limited high-quality nighttime data, and a lack of integration between daytime and nighttime trackers hinder the development of an end-to-end trainable framework. Additionally, current ViT-based trackers demand heavy computational resources due to their reliance on the self-attention mechanism. In this paper, we propose a novel pure Mamba-based tracking framework (MambaNUT) that employs a state space model with linear complexity as its backbone, incorporating a single-stream architecture that integrates feature learning and template-search coupling within Vision Mamba. We introduce an adaptive curriculum learning (ACL) approach that dynamically adjusts sampling strategies and loss weights, thereby improving the modelâ€™s ability of generalization. Our ACL is composed of two levels of curriculum schedulers: (1) sampling scheduler that transforms the data distribution from imbalanced to balanced, as well as from easier (daytime) to harder (nighttime) samples; (2) loss scheduler that dynamically assigns weights based on the size of the training set and IoU of individual instances. Exhaustive experiments on multiple nighttime UAV tracking benchmarks demonstrate that the proposed MambaNUT achieves state-of-the-art performance while requiring lower computational costs. The code will be available at <a target="_blank" rel="noopener" href="https://github.com/wuyou3474/MambaNUT">https://github.com/wuyou3474/MambaNUT</a>. </p>
<blockquote>
<p>åˆ©ç”¨ä½å…‰å¢å¼ºå’Œé¢†åŸŸè‡ªé€‚åº”æŠ€æœ¯ï¼Œå¤œé—´æ— äººæœºè·Ÿè¸ªå·²ç»å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œè¿‡äºä¾èµ–å›¾åƒå¢å¼ºã€é«˜è´¨é‡å¤œé—´æ•°æ®çš„æœ‰é™æ€§ä»¥åŠç™½å¤©å’Œå¤œé—´è·Ÿè¸ªå™¨ä¹‹é—´ç¼ºä¹æ•´åˆï¼Œé˜»ç¢äº†ç«¯åˆ°ç«¯å¯è®­ç»ƒæ¡†æ¶çš„å‘å±•ã€‚æ­¤å¤–ï¼Œå½“å‰çš„åŸºäºViTçš„è·Ÿè¸ªå™¨ç”±äºä¾èµ–äºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºçº¯Mambaçš„è·Ÿè¸ªæ¡†æ¶ï¼ˆMambaNUTï¼‰ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨çº¿æ€§å¤æ‚åº¦çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ä½œä¸ºéª¨å¹²ï¼Œé‡‡ç”¨å•æµæ¶æ„ï¼Œåœ¨Vision Mambaå†…æ•´åˆç‰¹å¾å­¦ä¹ å’Œæ¨¡æ¿æœç´¢è€¦åˆã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ ï¼ˆACLï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥åŠ¨æ€è°ƒæ•´é‡‡æ ·ç­–ç•¥å’ŒæŸå¤±æƒé‡ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ACLç”±ä¸¤ä¸ªå±‚æ¬¡çš„è¯¾ç¨‹è°ƒåº¦å™¨ç»„æˆï¼šï¼ˆ1ï¼‰é‡‡æ ·è°ƒåº¦å™¨ï¼Œå®ƒå°†æ•°æ®åˆ†å¸ƒä»ä¸å¹³è¡¡è½¬å˜ä¸ºå¹³è¡¡ï¼ŒåŒæ—¶å°†æ ·æœ¬ä»è¾ƒç®€å•çš„ï¼ˆç™½å¤©ï¼‰è½¬å˜ä¸ºè¾ƒéš¾çš„ï¼ˆå¤œé—´ï¼‰ï¼›ï¼ˆ2ï¼‰æŸå¤±è°ƒåº¦å™¨æ ¹æ®è®­ç»ƒé›†çš„å¤§å°å’Œä¸ªä½“çš„IoUåŠ¨æ€åˆ†é…æƒé‡ã€‚åœ¨å¤šä¸ªå¤œé—´æ— äººæœºè·Ÿè¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MambaNUTè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬è¾ƒä½ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/wuyou3474/MambaNUT%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/wuyou3474/MambaNUTä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00626v3">PDF</a> </p>
<p><strong>Summary</strong><br>æ— äººæœºå¤œé—´è¿½è¸ªæŠ€æœ¯å–å¾—æ˜¾è‘—è¿›å±•ï¼Œç»“åˆä½å…‰å¢å¼ºå’Œé¢†åŸŸè‡ªé€‚åº”ï¼Œä½†å­˜åœ¨è¿‡åº¦ä¾èµ–å›¾åƒå¢å¼ºã€é«˜è´¨é‡å¤œé—´æ•°æ®æœ‰é™åŠæ˜¼å¤œè¿½è¸ªå™¨æœªæ•´åˆç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºåŸºäºçº¯Mambaçš„è·Ÿè¸ªæ¡†æ¶MambaNUTï¼Œé‡‡ç”¨çº¿æ€§å¤æ‚åº¦çŠ¶æ€ç©ºé—´æ¨¡å‹ä½œä¸ºä¸»å¹²ï¼Œå¼•å…¥è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼ŒåŠ¨æ€è°ƒæ•´é‡‡æ ·ç­–ç•¥å’ŒæŸå¤±æƒé‡ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒMambaNUTåœ¨å¤œé—´æ— äººæœºè¿½è¸ªæ–¹é¢è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— äººæœºå¤œé—´è¿½è¸ªæŠ€æœ¯å·²æœ‰å¤šé¡¹è¿›å±•ï¼Œæ•´åˆä½å…‰å¢å¼ºå’Œé¢†åŸŸè‡ªé€‚åº”ç­‰æŠ€æœ¯æ¥æå‡æ€§èƒ½ã€‚</li>
<li>å½“å‰æŠ€æœ¯é¢ä¸´è¿‡åº¦ä¾èµ–å›¾åƒå¢å¼ºã€é«˜è´¨é‡å¤œé—´æ•°æ®æœ‰é™åŠæ˜¼å¤œè¿½è¸ªå™¨æ•´åˆé—®é¢˜ã€‚</li>
<li>æå‡ºçš„MambaNUTæ¡†æ¶åŸºäºçº¯Mambaè·Ÿè¸ªæŠ€æœ¯ï¼Œé‡‡ç”¨çº¿æ€§å¤æ‚åº¦çŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚</li>
<li>MambaNUTå¼•å…¥å•æµæ¶æ„ï¼Œé›†æˆç‰¹å¾å­¦ä¹ å’Œæ¨¡æ¿æœç´¢è€¦åˆã€‚</li>
<li>è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ ç­–ç•¥åŠ¨æ€è°ƒæ•´é‡‡æ ·ç­–ç•¥å’ŒæŸå¤±æƒé‡ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MambaNUTåœ¨å¤œé—´æ— äººæœºè¿½è¸ªæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-59445be950c76558c12bb65f1a3a3348.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d1e3aa283c1e6091e0b81c4b3b07db80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1a27b609b7ea3e2e91163de2c08734b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b37629b0d5437c966aab83fafb97a7e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14c307eca7ff56a567e04c62f5670b76.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GeoGround-A-Unified-Large-Vision-Language-Model-for-Remote-Sensing-Visual-Grounding"><a href="#GeoGround-A-Unified-Large-Vision-Language-Model-for-Remote-Sensing-Visual-Grounding" class="headerlink" title="GeoGround: A Unified Large Vision-Language Model for Remote Sensing   Visual Grounding"></a>GeoGround: A Unified Large Vision-Language Model for Remote Sensing   Visual Grounding</h2><p><strong>Authors:Yue Zhou, Mengcheng Lan, Xiang Li, Litong Feng, Yiping Ke, Xue Jiang, Qingyun Li, Xue Yang, Wayne Zhang</strong></p>
<p>Remote sensing (RS) visual grounding aims to use natural language expression to locate specific objects (in the form of the bounding box or segmentation mask) in RS images, enhancing human interaction with intelligent RS interpretation systems. Early research in this area was primarily based on horizontal bounding boxes (HBBs), but as more diverse RS datasets have become available, tasks involving oriented bounding boxes (OBBs) and segmentation masks have emerged. In practical applications, different targets require different grounding types: HBB can localize an objectâ€™s position, OBB provides its orientation, and mask depicts its shape. However, existing specialized methods are typically tailored to a single type of RS visual grounding task and are hard to generalize across tasks. In contrast, large vision-language models (VLMs) exhibit powerful multi-task learning capabilities but struggle to handle dense prediction tasks like segmentation. This paper proposes GeoGround, a novel framework that unifies support for HBB, OBB, and mask RS visual grounding tasks, allowing flexible output selection. Rather than customizing the architecture of VLM, our work aims to elegantly support pixel-level visual grounding output through the Text-Mask technique. We define prompt-assisted and geometry-guided learning to enhance consistency across different signals. Experimental results show that GeoGround demonstrates strong performance across four RS visual grounding tasks, matching the performance of specialized methods on multiple benchmarks. Code available at <a target="_blank" rel="noopener" href="https://github.com/zytx121/GeoGround">https://github.com/zytx121/GeoGround</a> </p>
<blockquote>
<p>é¥æ„Ÿï¼ˆRSï¼‰è§†è§‰å®šä½æ—¨åœ¨åˆ©ç”¨è‡ªç„¶è¯­è¨€è¡¨è¾¾å¼åœ¨é¥æ„Ÿå›¾åƒä¸­å®šä½ç‰¹å®šå¯¹è±¡ï¼ˆä»¥è¾¹ç•Œæ¡†æˆ–åˆ†å‰²æ©ç çš„å½¢å¼ï¼‰ï¼Œå¢å¼ºäººç±»ä¸æ™ºèƒ½é¥æ„Ÿè§£é‡Šç³»ç»Ÿçš„äº¤äº’ã€‚æ—©æœŸçš„ç ”ç©¶ä¸»è¦åŸºäºæ°´å¹³è¾¹ç•Œæ¡†ï¼ˆHBBï¼‰ï¼Œä½†éšç€æ›´å¤šé¥æ„Ÿæ•°æ®é›†çš„å¯ç”¨ï¼Œæ¶‰åŠå®šå‘è¾¹ç•Œæ¡†ï¼ˆOBBï¼‰å’Œåˆ†å‰²æ©ç çš„ä»»åŠ¡å·²ç»å‡ºç°ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä¸åŒçš„ç›®æ ‡éœ€è¦ä¸åŒçš„å®šä½ç±»å‹ï¼šHBBå¯ä»¥å®šä½å¯¹è±¡çš„ä½ç½®ï¼ŒOBBæä¾›å…¶æ–¹å‘ï¼Œè€Œæ©ç æç»˜å…¶å½¢çŠ¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸“ä¸šæ–¹æ³•é€šå¸¸é’ˆå¯¹å•ä¸€çš„é¥æ„Ÿè§†è§‰å®šä½ä»»åŠ¡ï¼Œéš¾ä»¥è·¨ä»»åŠ¡æ¨å¹¿ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å±•ç°å‡ºå¼ºå¤§çš„å¤šä»»åŠ¡å­¦ä¹ èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†è¯¸å¦‚åˆ†å‰²ä¹‹ç±»çš„å¯†é›†é¢„æµ‹ä»»åŠ¡æ—¶å´è¡¨ç°æŒ£æ‰ã€‚æœ¬æ–‡æå‡ºäº†GeoGroundï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ”¯æŒHBBã€OBBå’Œæ©ç é¥æ„Ÿè§†è§‰å®šä½ä»»åŠ¡ï¼Œå…è®¸çµæ´»è¾“å‡ºé€‰æ‹©ã€‚æˆ‘ä»¬çš„å·¥ä½œæ—¨åœ¨é€šè¿‡æ–‡æœ¬æ©ç æŠ€æœ¯å·§å¦™åœ°æ”¯æŒåƒç´ çº§è§†è§‰å®šä½è¾“å‡ºï¼Œè€Œä¸æ˜¯è‡ªå®šä¹‰VLMæ¶æ„ã€‚æˆ‘ä»¬å®šä¹‰äº†æç¤ºè¾…åŠ©å’Œå‡ ä½•å¼•å¯¼å­¦ä¹ ä»¥å¢å¼ºä¸åŒä¿¡å·ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGeoGroundåœ¨å››ä¸ªé¥æ„Ÿè§†è§‰å®šä½ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸ä¸“ç”¨æ–¹æ³•çš„æ€§èƒ½ç›¸åŒ¹é…ã€‚ä»£ç å¯ç”¨<a target="_blank" rel="noopener" href="https://github.com/zytx121/GeoGround%E3%80%82">https://github.com/zytx121/GeoGroundã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11904v3">PDF</a> 9 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¥æ„Ÿè§†è§‰å®šä½æŠ€æœ¯ä¸­çš„æ–°æ¡†æ¶GeoGroundã€‚è¯¥æ¡†æ¶æ—¨åœ¨ç»Ÿä¸€æ”¯æŒæ°´å¹³è¾¹ç•Œæ¡†ï¼ˆHBBï¼‰ã€å®šå‘è¾¹ç•Œæ¡†ï¼ˆOBBï¼‰å’Œæ©è†œä¸‰ç§é¥æ„Ÿè§†è§‰å®šä½ä»»åŠ¡ï¼Œå®ç°çµæ´»çš„è¾“å‡ºé€‰æ‹©ã€‚GeoGroundé€šè¿‡æ–‡æœ¬æ©è†œæŠ€æœ¯ä¼˜é›…åœ°æ”¯æŒåƒç´ çº§è§†è§‰å®šä½è¾“å‡ºï¼Œå¹¶é€šè¿‡æç¤ºè¾…åŠ©å’Œå‡ ä½•å¼•å¯¼å­¦ä¹ å¢å¼ºä¸åŒä¿¡å·ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGeoGroundåœ¨å››é¡¹é¥æ„Ÿè§†è§‰å®šä½ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸ä¸“ä¸šæ–¹æ³•çš„æ€§èƒ½ç›¸åŒ¹é…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿè§†è§‰å®šä½æŠ€æœ¯ä½¿ç”¨è‡ªç„¶è¯­è¨€è¡¨è¾¾å¼åœ¨é¥æ„Ÿå›¾åƒä¸­å®šä½ç‰¹å®šå¯¹è±¡ã€‚</li>
<li>æ—©æœŸç ”ç©¶ä¸»è¦åŸºäºæ°´å¹³è¾¹ç•Œæ¡†ï¼ˆHBBï¼‰ï¼Œç°åœ¨å‡ºç°äº†å®šå‘è¾¹ç•Œæ¡†ï¼ˆOBBï¼‰å’Œæ©è†œä»»åŠ¡ã€‚</li>
<li>ä¸åŒç›®æ ‡éœ€è¦ä¸åŒç±»å‹çš„å®šä½ï¼šHBBå¯å®šä½å¯¹è±¡ä½ç½®ï¼ŒOBBæä¾›æ–¹å‘ä¿¡æ¯ï¼Œæ©è†œæç»˜å¯¹è±¡å½¢çŠ¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸é’ˆå¯¹å•ä¸€é¥æ„Ÿè§†è§‰å®šä½ä»»åŠ¡ï¼Œéš¾ä»¥è·¨ä»»åŠ¡æ¨å¹¿ã€‚</li>
<li>GeoGroundæ¡†æ¶ç»Ÿä¸€æ”¯æŒHBBã€OBBå’Œæ©è†œé¥æ„Ÿè§†è§‰å®šä½ä»»åŠ¡ï¼Œå®ç°çµæ´»è¾“å‡ºé€‰æ‹©ã€‚</li>
<li>GeoGroundé€šè¿‡æ–‡æœ¬æ©è†œæŠ€æœ¯å®ç°åƒç´ çº§è§†è§‰å®šä½è¾“å‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11904">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ebfeffbea8baddee70a107f801cf3117.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-914a7019f1976b00b4358a0058bb1a74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d09823444ebc41cd79533912f20d717e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-046e19bc7d03b8257bd6b3196518fc00.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8689856d31858aaf96abdcbf2cceb43.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="How-to-build-the-best-medical-image-segmentation-algorithm-using-foundation-models-a-comprehensive-empirical-study-with-Segment-Anything-Model"><a href="#How-to-build-the-best-medical-image-segmentation-algorithm-using-foundation-models-a-comprehensive-empirical-study-with-Segment-Anything-Model" class="headerlink" title="How to build the best medical image segmentation algorithm using   foundation models: a comprehensive empirical study with Segment Anything   Model"></a>How to build the best medical image segmentation algorithm using   foundation models: a comprehensive empirical study with Segment Anything   Model</h2><p><strong>Authors:Hanxue Gu, Haoyu Dong, Jichen Yang, Maciej A. Mazurowski</strong></p>
<p>Automated segmentation is a fundamental medical image analysis task, which enjoys significant advances due to the advent of deep learning. While foundation models have been useful in natural language processing and some vision tasks for some time, the foundation model developed with image segmentation in mind - Segment Anything Model (SAM) - has been developed only recently and has shown similar promise. However, there are still no systematic analyses or â€œbest-practiceâ€ guidelines for optimal fine-tuning of SAM for medical image segmentation. This work summarizes existing fine-tuning strategies with various backbone architectures, model components, and fine-tuning algorithms across 18 combinations, and evaluates them on 17 datasets covering all common radiology modalities. Our study reveals that (1) fine-tuning SAM leads to slightly better performance than previous segmentation methods, (2) fine-tuning strategies that use parameter-efficient learning in both the encoder and decoder are superior to other strategies, (3) network architecture has a small impact on final performance, (4) further training SAM with self-supervised learning can improve final model performance. We also demonstrate the ineffectiveness of some methods popular in the literature and further expand our experiments into few-shot and prompt-based settings. Lastly, we released our code and MRI-specific fine-tuned weights, which consistently obtained superior performance over the original SAM, at <a target="_blank" rel="noopener" href="https://github.com/mazurowski-lab/finetune-SAM">https://github.com/mazurowski-lab/finetune-SAM</a>. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–åˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œéšç€æ·±åº¦å­¦ä¹ çš„å‡ºç°ï¼Œè¯¥ä»»åŠ¡å–å¾—äº†é‡å¤§è¿›å±•ã€‚è™½ç„¶åŸºç¡€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’ŒæŸäº›è§†è§‰ä»»åŠ¡ä¸­å·²æœ‰ä¸€æ®µæ—¶é—´çš„åº”ç”¨ï¼Œä½†é’ˆå¯¹å›¾åƒåˆ†å‰²è€Œå¼€å‘çš„åŸºç¡€æ¨¡å‹â€”â€”ä»»æ„åˆ†å‰²æ¨¡å‹ï¼ˆSAMï¼‰æ˜¯æœ€è¿‘æ‰å¼€å‘çš„ï¼Œå¹¶æ˜¾ç¤ºå‡ºåŒæ ·çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç›®å‰å°šæ— é’ˆå¯¹SAMåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æœ€ä¼˜å¾®è°ƒçš„ç³»ç»Ÿæ€§åˆ†ææˆ–â€œæœ€ä½³å®è·µâ€æŒ‡å—ã€‚è¿™é¡¹å·¥ä½œæ€»ç»“äº†ä½¿ç”¨ä¸åŒä¸»å¹²æ¶æ„ã€æ¨¡å‹ç»„ä»¶å’Œå¾®è°ƒç®—æ³•çš„ç°æœ‰å¾®è°ƒç­–ç•¥ï¼Œå¹¶åœ¨æ¶µç›–æ‰€æœ‰å¸¸è§æ”¾å°„å­¦æ¨¡æ€çš„17ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼šï¼ˆ1ï¼‰å¾®è°ƒSAMçš„æ€§èƒ½ç•¥ä¼˜äºä»¥å‰çš„åˆ†å‰²æ–¹æ³•ï¼›ï¼ˆ2ï¼‰åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¸­éƒ½ä½¿ç”¨å‚æ•°æœ‰æ•ˆå­¦ä¹ çš„å¾®è°ƒç­–ç•¥ä¼˜äºå…¶ä»–ç­–ç•¥ï¼›ï¼ˆ3ï¼‰ç½‘ç»œæ¶æ„å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“è¾ƒå°ï¼›ï¼ˆ4ï¼‰ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ è¿›ä¸€æ­¥è®­ç»ƒSAMå¯ä»¥æé«˜æœ€ç»ˆæ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†æ–‡çŒ®ä¸­ä¸€äº›æµè¡Œæ–¹æ³•çš„æœ‰æ•ˆæ€§ä¸è¶³ï¼Œå¹¶å°†æˆ‘ä»¬çš„å®éªŒè¿›ä¸€æ­¥æ‰©å±•åˆ°å°æ ·æœ¬å’ŒåŸºäºæç¤ºçš„è®¾ç½®ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/mazurowski-lab/finetune-SAM%E5%8F%91%E5%B8%83%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E9%92%88%E5%AF%B9MRI%E7%9A%84%E7%89%B9%E5%AE%9A%E5%BE%AE%E8%B0%83%E6%9D%83%E9%87%8D%EF%BC%8C%E8%BF%99%E4%BA%9B%E6%9D%83%E9%87%8D%E5%9C%A8%E5%8E%9F%E5%A7%8BSAM%E4%B8%8A%E5%A7%8B%E7%BB%88%E8%8E%B7%E5%BE%97%E6%9B%B4%E5%A5%BD%E7%9A%84%E6%80%A7%E8%83%BD%E3%80%82">https://github.com/mazurowski-lab/finetune-SAMå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç å’Œé’ˆå¯¹MRIçš„ç‰¹å®šå¾®è°ƒæƒé‡ï¼Œè¿™äº›æƒé‡åœ¨åŸå§‹SAMä¸Šå§‹ç»ˆè·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.09957v3">PDF</a> Accepted for publication at the Journal of Machine Learning for   Biomedical Imaging (MELBA)</p>
<p><strong>Summary</strong><br>     è¿‘æœŸå‘å±•çš„ç”¨äºå›¾åƒåˆ†å‰²çš„foundationæ¨¡å‹Segment Anything Modelï¼ˆSAMï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸæ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œé’ˆå¯¹SAMåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡çš„æœ€ä¼˜å¾®è°ƒå°šæœªæœ‰ç³»ç»Ÿæ€§åˆ†ææˆ–â€œæœ€ä½³å®è·µâ€æŒ‡å—ã€‚æœ¬ç ”ç©¶æ€»ç»“äº†ç°æœ‰çš„ä½¿ç”¨ä¸åŒéª¨å¹²æ¶æ„ã€æ¨¡å‹ç»„ä»¶å’Œå¾®è°ƒç®—æ³•çš„å¾®è°ƒç­–ç•¥ï¼Œå¹¶åœ¨æ¶µç›–æ‰€æœ‰å¸¸è§æ”¾å°„å­¦æ¨¡æ€çš„17ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œå¾®è°ƒSAMç•¥ä¼˜äºä¹‹å‰çš„æ–¹æ³•ï¼Œå‚æ•°é«˜æ•ˆå­¦ä¹ åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¸­çš„ä½¿ç”¨ç­–ç•¥ä¼˜äºå…¶ä»–ç­–ç•¥ï¼Œç½‘ç»œæ¶æ„å¯¹æœ€ç»ˆæ€§èƒ½å½±å“è¾ƒå°ï¼Œä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ è¿›ä¸€æ­¥è®­ç»ƒSAMå¯æé«˜æ¨¡å‹æ€§èƒ½ã€‚åŒæ—¶ï¼Œæœ¬ç ”ç©¶è¿˜å±•ç¤ºäº†æ–‡çŒ®ä¸­ä¸€äº›æµè¡Œæ–¹æ³•çš„æœ‰æ•ˆæ€§ä¸è¶³ï¼Œå¹¶æ‰©å±•äº†å®éªŒåˆ°å°æ ·æœ¬å’ŒåŸºäºæç¤ºçš„è®¾ç½®ã€‚æœ€åï¼Œæˆ‘ä»¬å…¬å¼€äº†ä»£ç å’Œé’ˆå¯¹MRIçš„ç‰¹å®šå¾®è°ƒæƒé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Segment Anything Model (SAM)æ˜¯ä¸€ä¸ªä¸“ä¸ºå›¾åƒåˆ†å‰²è®¾è®¡çš„foundationæ¨¡å‹ï¼Œå®ƒåœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸæ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å°šæœªå¯¹SAMè¿›è¡Œé’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡çš„æœ€ä¼˜å¾®è°ƒè¿›è¡Œç³»ç»Ÿæ€§çš„åˆ†ææˆ–æä¾›â€œæœ€ä½³å®è·µâ€æŒ‡å—ã€‚</li>
<li>ç ”ç©¶æ€»ç»“å‡ºå¤šç§å¾®è°ƒç­–ç•¥å¹¶ä½¿ç”¨å¤šç§éª¨å¹²æ¶æ„ã€æ¨¡å‹ç»„ä»¶å’Œå¾®è°ƒç®—æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>é€šè¿‡å¯¹å¤šç§æ•°æ®é›†çš„ç ”ç©¶å‘ç°ï¼Œå¾®è°ƒSAMçš„æ€§èƒ½ç•¥ä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚</li>
<li>å‚æ•°é«˜æ•ˆå­¦ä¹ åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¸­çš„ä½¿ç”¨ç­–ç•¥è¡¨ç°æœ€ä½³ï¼Œç½‘ç»œæ¶æ„å¯¹æœ€ç»ˆæ€§èƒ½å½±å“è¾ƒå°ã€‚</li>
<li>ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ è¿›ä¸€æ­¥è®­ç»ƒSAMå¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.09957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ef979f362d30c91768edb3cb9cfcbb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0395a23f624f16d17e20aa8bcbbf93a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7b10e1ba15649dc14ded6d0704408d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56e928361467f47b8e5e5155b445b95b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Leveraging-Habitat-Information-for-Fine-grained-Bird-Identification"><a href="#Leveraging-Habitat-Information-for-Fine-grained-Bird-Identification" class="headerlink" title="Leveraging Habitat Information for Fine-grained Bird Identification"></a>Leveraging Habitat Information for Fine-grained Bird Identification</h2><p><strong>Authors:Tin Nguyen, Peijie Chen, Anh Totti Nguyen</strong></p>
<p>Traditional bird classifiers mostly rely on the visual characteristics of birds. Some prior works even train classifiers to be invariant to the background, completely discarding the living environment of birds. Instead, we are the first to explore integrating habitat information, one of the four major cues for identifying birds by ornithologists, into modern bird classifiers. We focus on two leading model types: (1) CNNs and ViTs trained on the downstream bird datasets; and (2) original, multi-modal CLIP. Training CNNs and ViTs with habitat-augmented data results in an improvement of up to +0.83 and +0.23 points on NABirds and CUB-200, respectively. Similarly, adding habitat descriptors to the prompts for CLIP yields a substantial accuracy boost of up to +0.99 and +1.1 points on NABirds and CUB-200, respectively. We find consistent accuracy improvement after integrating habitat features into the image augmentation process and into the textual descriptors of vision-language CLIP classifiers. Code is available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/reasoning-8B7E/">https://anonymous.4open.science/r/reasoning-8B7E/</a>. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„é¸Ÿç±»åˆ†ç±»å™¨å¤§å¤šä¾èµ–äºé¸Ÿç±»çš„è§†è§‰ç‰¹å¾ã€‚ä¸€äº›æ—©æœŸä½œå“ç”šè‡³è®­ç»ƒåˆ†ç±»å™¨å¯¹èƒŒæ™¯ä¿æŒä¸å˜ï¼Œå®Œå…¨å¿½ç•¥é¸Ÿç±»çš„ç”Ÿæ´»ç¯å¢ƒã€‚ç›¸åï¼Œæˆ‘ä»¬æ˜¯é¦–æ‰¹æ¢ç´¢å°†æ –æ¯åœ°ä¿¡æ¯ï¼ˆé¸Ÿç±»å­¦å®¶è¯†åˆ«é¸Ÿç±»çš„å››å¤§çº¿ç´¢ä¹‹ä¸€ï¼‰èå…¥ç°ä»£é¸Ÿç±»åˆ†ç±»å™¨çš„å›¢é˜Ÿã€‚æˆ‘ä»¬å…³æ³¨çš„æ¨¡å‹ç±»å‹ä¸»è¦æœ‰ä¸¤ç§ï¼šï¼ˆ1ï¼‰åœ¨ä¸‹æ¸¸é¸Ÿç±»æ•°æ®é›†ä¸Šè®­ç»ƒçš„CNNå’ŒViTï¼›ï¼ˆ2ï¼‰åŸå§‹çš„è·¨æ¨¡æ€CLIPæ¨¡å‹ã€‚ä½¿ç”¨æ –æ¯åœ°å¢å¼ºçš„æ•°æ®è®­ç»ƒCNNå’ŒViTï¼Œåœ¨NABirdså’ŒCUB-200ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†+0.83ç‚¹å’Œ+0.23ç‚¹ã€‚åŒæ ·åœ°ï¼Œå°†æ –æ¯åœ°æè¿°ç¬¦æ·»åŠ åˆ°CLIPçš„æç¤ºä¸­ï¼Œåœ¨NABirdså’ŒCUB-200ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«å¤§å¹…æé«˜è‡³+0.99ç‚¹å’Œ+1.1ç‚¹ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨å›¾åƒå¢å¼ºè¿‡ç¨‹å’Œè§†è§‰è¯­è¨€CLIPåˆ†ç±»å™¨çš„æ–‡æœ¬æè¿°ç¬¦ä¸­é›†æˆæ –æ¯åœ°ç‰¹å¾åï¼Œå‡†ç¡®ç‡å¾—åˆ°äº†ä¸€è‡´æ€§çš„æé«˜ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/reasoning-8B7E/">åŒ¿åé“¾æ¥</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.14999v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¼ ç»Ÿé¸Ÿç±»åˆ†ç±»å™¨ä¸»è¦ä¾èµ–é¸Ÿç±»çš„è§†è§‰ç‰¹å¾ï¼Œè€Œå¿½è§†é¸Ÿç±»çš„ç”Ÿæ´»ç¯å¢ƒä¿¡æ¯ã€‚è¯¥ç ”ç©¶é¦–æ¬¡æ¢ç´¢å°†æ –æ¯åœ°ä¿¡æ¯è¿™ä¸€é¸Ÿç±»è¯†åˆ«çš„å››å¤§çº¿ç´¢ä¹‹ä¸€èå…¥ç°ä»£é¸Ÿç±»åˆ†ç±»å™¨ã€‚è¯¥ç ”ç©¶å…³æ³¨äº†ä¸¤ç§é¢†å…ˆçš„æ¨¡å‹ç±»å‹ï¼Œå¹¶é€šè¿‡æ·»åŠ æ –æ¯åœ°æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå‘ç°åœ¨NABirdså’ŒCUB-200æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†+0.83å’Œ+0.23ä¸ªç‚¹ã€‚æ­¤å¤–ï¼Œå°†æ –æ¯åœ°æè¿°ç¬¦æ·»åŠ åˆ°CLIPçš„æç¤ºä¸­ä¹Ÿæ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡ï¼Œåœ¨NABirdså’ŒCUB-200ä¸Šåˆ†åˆ«æé«˜äº†+0.99å’Œ+1.1ä¸ªç‚¹ã€‚ç ”ç©¶å‘ç°ï¼Œå°†æ –æ¯åœ°ç‰¹å¾æ•´åˆåˆ°å›¾åƒå¢å¼ºè¿‡ç¨‹å’Œè§†è§‰è¯­è¨€CLIPåˆ†ç±»å™¨çš„æ–‡æœ¬æè¿°ç¬¦ä¸­ï¼Œå¯ä»¥æŒç»­æé«˜å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿé¸Ÿç±»åˆ†ç±»å™¨ä¸»è¦ä¾èµ–é¸Ÿç±»çš„è§†è§‰ç‰¹å¾ï¼Œå¿½ç•¥ç”Ÿæ´»ç¯å¢ƒä¿¡æ¯ã€‚</li>
<li>é¦–æ¬¡å°è¯•å°†æ –æ¯åœ°ä¿¡æ¯èå…¥ç°ä»£é¸Ÿç±»åˆ†ç±»å™¨ã€‚</li>
<li>åœ¨NABirdså’ŒCUB-200æ•°æ®é›†ä¸Šï¼Œé€šè¿‡æ·»åŠ æ –æ¯åœ°æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œå‡†ç¡®ç‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>CLIPæ¨¡å‹ç»“åˆæ –æ¯åœ°æè¿°ç¬¦çš„æç¤ºä¹Ÿæ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡ã€‚</li>
<li>æ•´åˆæ –æ¯åœ°ç‰¹å¾åˆ°å›¾åƒå¢å¼ºè¿‡ç¨‹å’Œæ–‡æœ¬æè¿°ç¬¦ä¸­ï¼Œå¯è¿›ä¸€æ­¥æé«˜å‡†ç¡®ç‡ã€‚</li>
<li>è¯¥ç ”ç©¶å…³æ³¨ä¸¤ç§é¢†å…ˆçš„æ¨¡å‹ç±»å‹ï¼ŒåŒ…æ‹¬CNNå’ŒViTã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.14999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7c03550ec50e2ce6f1ca34536b68e039.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8edc27aabf7834941721ceb8a5bf472.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0bafbf63fc12cfe8e4081236d48c3bc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eab15d61ada084727e37e5011ca54d23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c13a7c66c91082465f072fb7ad8ea0fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d41832642ee733f6773c691c2f6c17f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b2ad4a4c6d8d2a89363df8ef35e6012.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f1c559c8a348c20c413ca5108440254.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-14\./crop_Vision Transformer/2312.14999v2/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-14\./crop_Vision Transformer/2312.14999v2/page_5_2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4656fa795d076aea34f6e55c4fc8abad.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  DepthFusion Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D   Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-edee906e9a345f0ac2a02462149c7882.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  Pixel Motion as Universal Representation for Robot Control
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17862.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
