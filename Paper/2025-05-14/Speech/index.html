<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-05-14  A Comparative Analysis of Static Word Embeddings for Hungarian">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0f36701794ff88abe07da6e61077bca2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-14-更新"><a href="#2025-05-14-更新" class="headerlink" title="2025-05-14 更新"></a>2025-05-14 更新</h1><h2 id="A-Comparative-Analysis-of-Static-Word-Embeddings-for-Hungarian"><a href="#A-Comparative-Analysis-of-Static-Word-Embeddings-for-Hungarian" class="headerlink" title="A Comparative Analysis of Static Word Embeddings for Hungarian"></a>A Comparative Analysis of Static Word Embeddings for Hungarian</h2><p><strong>Authors:Máté Gedeon</strong></p>
<p>This paper presents a comprehensive analysis of various static word embeddings for Hungarian, including traditional models such as Word2Vec, FastText, as well as static embeddings derived from BERT-based models using different extraction methods. We evaluate these embeddings on both intrinsic and extrinsic tasks to provide a holistic view of their performance. For intrinsic evaluation, we employ a word analogy task, which assesses the embeddings ability to capture semantic and syntactic relationships. Our results indicate that traditional static embeddings, particularly FastText, excel in this task, achieving high accuracy and mean reciprocal rank (MRR) scores. Among the BERT-based models, the X2Static method for extracting static embeddings demonstrates superior performance compared to decontextualized and aggregate methods, approaching the effectiveness of traditional static embeddings. For extrinsic evaluation, we utilize a bidirectional LSTM model to perform Named Entity Recognition (NER) and Part-of-Speech (POS) tagging tasks. The results reveal that embeddings derived from dynamic models, especially those extracted using the X2Static method, outperform purely static embeddings. Notably, ELMo embeddings achieve the highest accuracy in both NER and POS tagging tasks, underscoring the benefits of contextualized representations even when used in a static form. Our findings highlight the continued relevance of static word embeddings in NLP applications and the potential of advanced extraction methods to enhance the utility of BERT-based models. This piece of research contributes to the understanding of embedding performance in the Hungarian language and provides valuable insights for future developments in the field. The training scripts, evaluation codes, restricted vocabulary, and extracted embeddings will be made publicly available to support further research and reproducibility. </p>
<blockquote>
<p>本文全面分析了匈牙利语的多种静态词嵌入方法，包括传统的Word2Vec、FastText模型，以及使用不同提取方法从BERT模型派生的静态嵌入。我们对这些嵌入进行了内在和外在任务的评估，以全面了解它们的性能。内在评估中，我们采用了词汇类比任务，该任务评估嵌入捕获语义和句法关系的能力。结果表明，传统静态嵌入，特别是FastText，在此任务中表现出色，准确度和平均倒数排名（MRR）得分很高。在BERT模型中，用于提取静态嵌入的X2Static方法相对于去上下文化和聚合方法显示出卓越的性能，接近传统静态嵌入的有效性。在外在评估中，我们使用双向LSTM模型执行命名实体识别（NER）和词性标注（POS）任务。结果表明，来自动态模型的嵌入，尤其是使用X2Static方法提取的嵌入，优于纯静态嵌入。值得注意的是，ELMo嵌入在NER和POS标注任务中的准确性最高，突显了即使在静态形式中使用上下文表示的好处。我们的研究结果表明静态词嵌入在NLP应用程序中的持续重要性，以及先进提取方法增强基于BERT的模型的潜力。该研究有助于理解匈牙利语中的嵌入性能，并为该领域的未来发展提供了宝贵的见解。为了支持进一步的研究和可重复性，我们将公开提供训练脚本、评估代码、受限词汇和提取的嵌入。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07809v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文全面分析了匈牙利语的多种静态词嵌入方法，包括传统的Word2Vec、FastText模型以及基于BERT的不同提取方法得到的静态嵌入。通过内在和外在任务评估这些嵌入的性能，为它们的表现提供了全面的视角。在内在评估中，采用词类比任务评估嵌入捕捉语义和句法关系的能力，发现FastText等传统静态嵌入表现优秀，准确率和高平均倒数排名（MRR）得分较高。在基于BERT的模型中，X2Static方法提取的静态嵌入表现出卓越性能，接近传统静态嵌入的效果。外在评估中，使用双向LSTM模型进行命名实体识别（NER）和词性标注（POS）任务，发现来自动态模型的嵌入，特别是使用X2Static方法提取的嵌入，优于纯静态嵌入。特别是ELMo嵌入在NER和POS标注任务中的准确性最高，突显了上下文表示的优势，即使以静态形式使用也是如此。研究发现静态词嵌入在自然语言处理应用中持续具有重要意义，先进提取方法有可能增强基于BERT的模型的实用性。该研究为匈牙利语中的嵌入性能提供了理解，并为该领域的未来发展提供了宝贵见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文对匈牙利语的多种静态词嵌入方法进行了全面分析。</li>
<li>内在评估中，传统静态嵌入模型如FastText在词类比任务中表现优秀。</li>
<li>在基于BERT的模型中，X2Static方法提取的静态嵌入表现出卓越性能。</li>
<li>外在评估中，动态模型的嵌入，特别是在使用X2Static方法时，优于纯静态嵌入。</li>
<li>ELMo嵌入在命名实体识别（NER）和词性标注（POS）任务中的准确性最高。</li>
<li>论文强调了静态词嵌入在自然语言处理应用中的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07809">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fbe824291f94cbfc7a4ce817aaf3e97a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b25c6b49ffb1c8249e041679d0ce28ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc5222f54708b6d96cba0a26b05f1a5e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4540ef6bb3643b5f126ff8ca4565ad3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ef839b2e84db3b4650dbf8db32c3446.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0378041ff9f80ed9b440cdb07ce15890.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VTutor-An-Animated-Pedagogical-Agent-SDK-that-Provide-Real-Time-Multi-Model-Feedback"><a href="#VTutor-An-Animated-Pedagogical-Agent-SDK-that-Provide-Real-Time-Multi-Model-Feedback" class="headerlink" title="VTutor: An Animated Pedagogical Agent SDK that Provide Real Time   Multi-Model Feedback"></a>VTutor: An Animated Pedagogical Agent SDK that Provide Real Time   Multi-Model Feedback</h2><p><strong>Authors:Eason Chen, Chenyu Lin, Yu-Kai Huang, Xinyi Tang, Aprille Xi, Jionghao Lin, Kenneth Koedinger</strong></p>
<p>Pedagogical Agents (PAs) show significant potential for boosting student engagement and learning outcomes by providing adaptive, on-demand support in educational contexts. However, existing PA solutions are often hampered by pre-scripted dialogue, unnatural animations, uncanny visual realism, and high development costs. To address these gaps, we introduce VTutor, an open-source SDK leveraging lightweight WebGL, Unity, and JavaScript frameworks. VTutor receives text outputs from a large language model (LLM), converts them into audio via text-to-speech, and then renders a real-time, lip-synced pedagogical agent (PA) for immediate, large-scale deployment on web-based learning platforms. By providing on-demand, personalized feedback, VTutor strengthens students’ motivation and deepens their engagement with instructional material. Using an anime-like aesthetic, VTutor alleviates the uncanny valley effect, allowing learners to engage with expressive yet comfortably stylized characters. Our evaluation with 50 participants revealed that VTutor significantly outperforms the existing talking-head approaches (e.g., SadTalker) on perceived synchronization accuracy, naturalness, emotional expressiveness, and overall preference. As an open-source project, VTutor welcomes community-driven contributions - from novel character designs to specialized showcases of pedagogical agent applications - that fuel ongoing innovation in AI-enhanced education. By providing an accessible, customizable, and learner-centered PA solution, VTutor aims to elevate human-AI interaction experience in education fields, ultimately broadening the impact of AI in learning contexts. The demo link to VTutor is at <a target="_blank" rel="noopener" href="https://vtutor-aied25.vercel.app/">https://vtutor-aied25.vercel.app</a>. </p>
<blockquote>
<p>教学代理（PAs）通过在教育环境中提供自适应、即时支持，显示出巨大的潜力，可以提高学生的学习参与度和学习效果。然而，现有的PA解决方案通常受到预设对话、不自然动画、过度真实的视觉和高昂的开发成本等限制。为了解决这些差距，我们推出了VTutor，一个利用轻量级WebGL、Unity和JavaScript框架的开源SDK。VTutor从大型语言模型（LLM）接收文本输出，通过文本到语音将其转换为音频，然后渲染一个实时、唇同步的教学代理（PA），可在基于网络的学习平台上进行即时、大规模部署。通过提供即时个性化的反馈，VTutor增强了学生的动力，并加深了他们与教材的联系。VTutor采用动漫式的审美风格，减轻了“非真实谷”效应，让学习者能够与表达丰富但风格舒适的角色互动。我们对50名参与者的评估表明，VTutor在感知到的同步精度、自然度、情感表达力和整体偏好上显著优于现有的谈话头方式（如SadTalker）。作为一个开源项目，VTutor欢迎社区驱动的贡献——从新颖的角色设计到教学代理应用程序的专门展示——这些贡献为AI增强教育的持续创新提供了动力。通过提供可访问、可定制、以学生为中心PA解决方案，VTutor旨在提升教育领域中的人机交互体验，最终扩大AI在学习环境中的影响力。VTutor的演示链接为：<a target="_blank" rel="noopener" href="https://vtutor-aied25.vercel.app./">https://vtutor-aied25.vercel.app。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06676v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Pedagogical Agents（PAs）在教育领域中的潜力，通过提供自适应、即时支持来提高学生参与度和学习效果。针对现有PA解决方案的局限性，如预编程对话、不自然的动画和高开发成本等，本文引入了一个名为VTutor的开源SDK。VTutor利用轻量级WebGL、Unity和JavaScript框架，从大型语言模型接收文本输出，通过文本转语音转换为音频，并实时渲染一个与语音同步的PA，可在基于网络的学习平台上大规模部署。VTutor通过提供个性化即时反馈来增强学生的学习动力并加深他们对教学材料的理解。VTutor采用动漫式的美学设计，缓解了人与虚拟角色之间的情感落差，让学习者可以与具有表现力的角色舒适互动。评估结果显示，VTutor在感知同步准确性、自然度、情感表达力和整体偏好等方面显著优于现有的谈话头模式。作为开源项目，VTutor欢迎社区贡献者提供新的角色设计和专门的PA应用程序展示，共同推动AI增强教育的创新。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PAs在教育领域具有提高学生学习参与度和效果的潜力。</li>
<li>现有PA解决方案受到预编程对话、不自然动画和高开发成本的限制。</li>
<li>VTutor是一个开源SDK，旨在解决这些问题，提供自适应、即时支持。</li>
<li>VTutor利用大型语言模型生成文本输出，转换为音频并渲染成实时PA。</li>
<li>VTutor可大规模部署在基于网络的学习平台上，提供个性化反馈。</li>
<li>VTutor采用动漫美学设计，增强学习者与角色之间的互动体验。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06676">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3ea2409087d5718127ad5c723c1929ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-48335079e6b74037ebe49a3bf5913af5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edfe7e20d0971446acaa7258614a2b2a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Gap-An-Intermediate-Language-for-Enhanced-and-Cost-Effective-Grapheme-to-Phoneme-Conversion-with-Homographs-with-Multiple-Pronunciations-Disambiguation"><a href="#Bridging-the-Gap-An-Intermediate-Language-for-Enhanced-and-Cost-Effective-Grapheme-to-Phoneme-Conversion-with-Homographs-with-Multiple-Pronunciations-Disambiguation" class="headerlink" title="Bridging the Gap: An Intermediate Language for Enhanced and   Cost-Effective Grapheme-to-Phoneme Conversion with Homographs with Multiple   Pronunciations Disambiguation"></a>Bridging the Gap: An Intermediate Language for Enhanced and   Cost-Effective Grapheme-to-Phoneme Conversion with Homographs with Multiple   Pronunciations Disambiguation</h2><p><strong>Authors:Abbas Bertina, Shahab Beirami, Hossein Biniazian, Elham Esmaeilnia, Soheil Shahi, Mahdi Pirnia</strong></p>
<p>Grapheme-to-phoneme (G2P) conversion for Persian presents unique challenges due to its complex phonological features, particularly homographs and Ezafe, which exist in formal and informal language contexts. This paper introduces an intermediate language specifically designed for Persian language processing that addresses these challenges through a multi-faceted approach. Our methodology combines two key components: Large Language Model (LLM) prompting techniques and a specialized sequence-to-sequence machine transliteration architecture. We developed and implemented a systematic approach for constructing a comprehensive lexical database for homographs with multiple pronunciations disambiguation often termed polyphones, utilizing formal concept analysis for semantic differentiation. We train our model using two distinct datasets: the LLM-generated dataset for formal and informal Persian and the B-Plus podcasts for informal language variants. The experimental results demonstrate superior performance compared to existing state-of-the-art approaches, particularly in handling the complexities of Persian phoneme conversion. Our model significantly improves Phoneme Error Rate (PER) metrics, establishing a new benchmark for Persian G2P conversion accuracy. This work contributes to the growing research in low-resource language processing and provides a robust solution for Persian text-to-speech systems and demonstrating its applicability beyond Persian. Specifically, the approach can extend to languages with rich homographic phenomena such as Chinese and Arabic </p>
<blockquote>
<p>针对波斯语的字母发音（G2P）转换由于其复杂的语音特征而面临独特的挑战，特别是在正式和非正式语言环境中的同音字和易泽（Ezafe）。本文介绍了一种专门为波斯语处理设计的中间语言，通过多元方法解决这些挑战。我们的方法结合了两个关键组成部分：大型语言模型（LLM）提示技术和专门的序列到序列机器转译架构。我们开发并实施了一种系统的方法来构建包含多音发音歧义的同音词的全面词汇数据库，通常被称为多音字，并利用形式概念分析进行语义区分。我们使用两个独特的数据集来训练我们的模型：用于正式和非正式波斯语的LLM生成数据集和用于非正式语言变体的B-Plus播客。实验结果表明，与现有的最先进方法相比，我们的模型具有卓越的性能，特别是在处理波斯语音素转换的复杂性方面。我们的模型显著提高了音素错误率（PER）指标，为波斯语的G2P转换精度建立了新的基准。这项工作为低资源语言处理的研究增长做出了贡献，并为波斯语音转文本系统提供了稳健的解决方案，并展示了其超越波斯的适用性。特别是，该方法可以扩展到具有丰富同音字现象的语言，如中文和阿拉伯文。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06599v1">PDF</a> pdf, 8 pages, 4 figures, 4 tables</p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了针对波斯语Grapeme-to-phoneme（G2P）转换的挑战，提出了一种中间语言处理方法。该方法结合了大型语言模型（LLM）提示技术和序列到序列机器转换架构，建立了一个全面的词汇数据库，用于处理具有多种发音的同名字（Polyphones）。使用正式概念分析进行语义区分，并在正式和非正式波斯语数据集以及B-Plus非正式语言变体数据集上进行训练。实验结果表明，该方法在波斯语音素转换的复杂性处理上表现出卓越性能，显著提高了音素错误率（PER）指标，为波斯语G2P转换准确性建立了新基准。该研究为低资源语言处理研究做出了贡献，并为波斯语音转系统提供了稳健解决方案，同时展示了其在其他语言如汉语和阿拉伯语中的适用性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>波斯语Grapeme-to-phoneme（G2P）转换面临复杂挑战，包括同形字和Ezafe等复杂语音特征。</li>
<li>引入了一种中间语言处理方法，结合大型语言模型（LLM）提示技术和序列到序列机器转换架构，以应对这些挑战。</li>
<li>建立了一个全面的词汇数据库，用于处理具有多种发音的同名字（Polyphones），利用正式概念分析进行语义区分。</li>
<li>在正式和非正式波斯语数据集以及B-Plus非正式语言变体数据集上进行训练。</li>
<li>实验结果相比现有技术展现了优越性，特别是在处理波斯语音素转换的复杂性方面。</li>
<li>显著提高了音素错误率（PER）指标，为波斯语G2P转换准确性建立了新基准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06599">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b0161b4e946528529a833df603fc508c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b90ffef1c4bb974f382d2a09f3cf047.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-926f793674e90cd5b9453fd237e62d4c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d5162a93a160c1f0a50dee310ec2c67.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5191ff9e2559b61cbdd441a97dc80982.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2adda02c3db5f3cb226e79b141d8fac4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-687d05bc0db46c0f252086a77beb8bee.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Blind-Speech-Separation-with-a-Diffusion-Prior"><a href="#Unsupervised-Blind-Speech-Separation-with-a-Diffusion-Prior" class="headerlink" title="Unsupervised Blind Speech Separation with a Diffusion Prior"></a>Unsupervised Blind Speech Separation with a Diffusion Prior</h2><p><strong>Authors:Zhongweiyang Xu, Xulin Fan, Zhong-Qiu Wang, Xilin Jiang, Romit Roy Choudhury</strong></p>
<p>Blind Speech Separation (BSS) aims to separate multiple speech sources from audio mixtures recorded by a microphone array. The problem is challenging because it is a blind inverse problem, i.e., the microphone array geometry, the room impulse response (RIR), and the speech sources, are all unknown. We propose ArrayDPS to solve the BSS problem in an unsupervised, array-agnostic, and generative manner. The core idea builds on diffusion posterior sampling (DPS), but unlike DPS where the likelihood is tractable, ArrayDPS must approximate the likelihood by formulating a separate optimization problem. The solution to the optimization approximates room acoustics and the relative transfer functions between microphones. These approximations, along with the diffusion priors, iterate through the ArrayDPS sampling process and ultimately yield separated voice sources. We only need a simple single-speaker speech diffusion model as a prior along with the mixtures recorded at the microphones; no microphone array information is necessary. Evaluation results show that ArrayDPS outperforms all baseline unsupervised methods while being comparable to supervised methods in terms of SDR. Audio demos are provided at: <a target="_blank" rel="noopener" href="https://arraydps.github.io/ArrayDPSDemo/">https://arraydps.github.io/ArrayDPSDemo/</a>. </p>
<blockquote>
<p>盲语音分离（BSS）旨在从麦克风阵列记录的音频混合中分离出多个语音源。这个问题具有挑战性，因为它是一个盲逆问题，即麦克风阵列的几何形状、房间冲击响应（RIR）和语音源都是未知的。我们提出ArrayDPS以无监督、阵列无关和生成的方式解决BSS问题。核心理念建立在扩散后采样（DPS）的基础上，但不同于DPS中可能性是可追踪的，ArrayDPS必须通过制定一个单独的优化问题来近似可能性。优化的解决方案近似于房间声学以及麦克风之间的相对传递函数。这些近似值，连同扩散先验，在ArrayDPS采样过程中进行迭代，并最终产生分离的语音源。我们只需要一个简单的单讲者语音扩散模型作为先验，以及麦克风记录的混音；不需要麦克风阵列的信息。评估结果表明，ArrayDPS在SDR方面优于所有基线无监督方法，同时与有监督的方法相当。音频演示请访问：<a target="_blank" rel="noopener" href="https://arraydps.github.io/ArrayDPSDemo/%E3%80%82">https://arraydps.github.io/ArrayDPSDemo/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05657v1">PDF</a> Paper Accepted at ICML2025 Demo:   <a target="_blank" rel="noopener" href="https://arraydps.github.io/ArrayDPSDemo/">https://arraydps.github.io/ArrayDPSDemo/</a> Code:   <a target="_blank" rel="noopener" href="https://github.com/ArrayDPS/ArrayDPS">https://github.com/ArrayDPS/ArrayDPS</a></p>
<p><strong>总结</strong></p>
<p>盲语音分离（BSS）旨在从麦克风阵列录制的音频混合中分离多个语音源。这是一个具有挑战性的逆向问题，因为麦克风阵列的几何形状、房间脉冲响应（RIR）和语音源都是未知的。我们提出ArrayDPS以无监督、阵列无关和生成的方式解决BSS问题。其核心思想建立在扩散后采样（DPS）的基础上，但不同于DPS的是，ArrayDPS必须通过制定单独的优化问题来近似可能性。优化问题的解决方案近似于房间声学以及麦克风之间的相对传输函数。这些近似值与扩散先验值一起，在ArrayDPS采样过程中进行迭代，并最终产生分离的语音源。我们只需要一个简单的单说话人语音扩散模型作为先验值，以及麦克风录制的混合声音；无需知道麦克风阵列的信息。评估结果表明，ArrayDPS在无人监督的方法中表现最佳，同时在有监督的方法的信号干扰比（SDR）方面表现良好。音频演示请访问：<a target="_blank" rel="noopener" href="https://arraydps.github.io/ArrayDPSDemo/%E3%80%82">https://arraydps.github.io/ArrayDPSDemo/。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>Blind Speech Separation (BSS)旨在从麦克风阵列录制的音频中分离多个未知的语音源。</li>
<li>ArrayDPS是一种解决BSS问题的无监督、阵列无关和生成的方法。</li>
<li>ArrayDPS的核心建立在扩散后采样（DPS）的基础上，但需要单独优化来近似可能性。</li>
<li>该优化过程模拟了房间声学及麦克风间的相对传输函数。</li>
<li>ArrayDPS结合了扩散先验值在采样过程中进行迭代，最终产生分离的语音源。</li>
<li>仅需要单说话人语音扩散模型作为先验值，以及麦克风录制的混合声音，无需知道麦克风阵列的具体信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05657">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f3f619f9592f7fdf2125c4f9d9be0eaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-792f126be9364cb206b8e63564a8891c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9d3c70b32a33862eba0211bdeacd57d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-57da60f5e871e72ab306d4133bd89a44.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PAHA-Parts-Aware-Audio-Driven-Human-Animation-with-Diffusion-Model"><a href="#PAHA-Parts-Aware-Audio-Driven-Human-Animation-with-Diffusion-Model" class="headerlink" title="PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model"></a>PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model</h2><p><strong>Authors:S. Z. Zhou, Y. B. Wang, J. F. Wu, T. Hu, J. N. Zhang, Z. J. Li, Y. Liu</strong></p>
<p>Audio-driven human animation technology is widely used in human-computer interaction, and the emergence of diffusion models has further advanced its development. Currently, most methods rely on multi-stage generation and intermediate representations, resulting in long inference time and issues with generation quality in specific foreground regions and audio-motion consistency. These shortcomings are primarily due to the lack of localized fine-grained supervised guidance. To address above challenges, we propose PAHA, an end-to-end audio-driven upper-body human animation framework with diffusion model. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts Consistency Enhancement (PCE). PAR dynamically adjusts regional training loss weights based on pose confidence scores, effectively improving visual quality. PCE constructs and trains diffusion-based regional audio-visual classifiers to improve the consistency of motion and co-speech audio. Afterwards, we design two novel inference guidance methods for the foregoing classifiers, Sequential Guidance (SG) and Differential Guidance (DG), to balance efficiency and quality respectively. Additionally, we build CNAS, the first public Chinese News Anchor Speech dataset, to advance research and validation in this field. Extensive experimental results and user studies demonstrate that PAHA significantly outperforms existing methods in audio-motion alignment and video-related evaluations. The codes and CNAS dataset will be released upon acceptance. </p>
<blockquote>
<p>音频驱动的人类动画技术广泛应用于人机交互领域，扩散模型的出现进一步推动了其发展。目前，大多数方法依赖于多阶段生成和中间表示，导致推理时间长，特定前景区域生成质量和音画同步问题。这些缺点主要是由于缺乏局部精细监督指导。为了解决上述挑战，我们提出了PAHA，这是一个基于扩散模型的端到端音频驱动人体上半身动画框架。我们介绍了两种关键方法：零件感知重加权（PAR）和零件一致性增强（PCE）。PAR根据姿势置信度得分动态调整区域训练损失权重，有效提高视觉效果。PCE构建并训练基于扩散的区域音视频分类器，提高动作和音乐音频的一致性。之后，我们为前述分类器设计了两种新型推理指导方法，即顺序指导（SG）和差分指导（DG），以平衡效率和质量。此外，我们构建了CNAS，即首个公开的中文新闻主播语音数据集，以促进该领域的研究和验证。广泛的实验和用户研究结果表明，PAHA在音频运动对齐和视频相关评估方面显著优于现有方法。代码和CNAS数据集将在接受后发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03603v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于扩散模型的音频驱动人体动画技术面临多阶段生成带来的问题，如推理时间长、前景区域生成质量及音画同步性不佳。为此，提出PAHA框架及PAR和PCE两大方法，通过动态调整区域训练损失权重和提升姿态置信度来改善视觉质量，并构建扩散模型区域音视频分类器提升动作与音频的一致性。此外，推出两种新型推理引导方法SG和DG，平衡效率与质量。同时建立首个中文新闻主播语音数据集CNAS，推动相关研究验证。实验和用户研究证明PAHA在音画同步和视频评价上显著超越现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频驱动人体动画技术在人机交互中广泛应用，扩散模型的出现进一步推动了其发展。</li>
<li>当前方法存在多阶段生成导致的推理时间长、特定前景区域生成质量及音画同步性问题。</li>
<li>缺乏局部精细监督指导是上述挑战的主要原因。</li>
<li>PAHA框架通过引入PAR和PCE两大方法，有效改善视觉质量和音画一致性。</li>
<li>SG和DG两种新型推理引导方法的提出，旨在平衡效率与质量。</li>
<li>建立了首个中文新闻主播语音数据集CNAS，为相关研究提供验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03603">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6889f9b3860ecf99c5922556433bebef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d8f966dc8284a9b15a27770d20fbfb3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-89d3a4c95fa931d1bd6af93bf478182e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7db20fb7d8db03a4ec414596b385621.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DGSNA-prompt-based-Dynamic-Generative-Scene-based-Noise-Addition-method"><a href="#DGSNA-prompt-based-Dynamic-Generative-Scene-based-Noise-Addition-method" class="headerlink" title="DGSNA: prompt-based Dynamic Generative Scene-based Noise Addition method"></a>DGSNA: prompt-based Dynamic Generative Scene-based Noise Addition method</h2><p><strong>Authors:Zihao Chen, Zhentao Lin, Bi Zeng, Linyi Huang, Zhi Li, Jia Cai</strong></p>
<p>To ensure the reliable operation of speech systems across diverse environments, noise addition methods have emerged as the prevailing solution. However, existing methods offer limited coverage of real-world noisy scenes and depend on pre-existing scene-based information and noise. This paper presents prompt-based Dynamic Generative Scene-based Noise Addition (DGSNA), a novel noise addition methodology that integrates Dynamic Generation of Scene-based Information (DGSI) with Scene-based Noise Addition for Speech (SNAS). This integration facilitates automated scene-based noise addition by transforming clean speech into various noise environments, thereby providing a more comprehensive and realistic simulation of diverse noise conditions. Experimental results demonstrate that DGSNA significantly enhances the robustness of speech recognition and keyword spotting models across various noise conditions, achieving a relative improvement of up to 11.21%. Furthermore, DGSNA can be effectively integrated with other noise addition methods to enhance performance. Our implementation and demonstrations are available at <a target="_blank" rel="noopener" href="https://dgsna.github.io/">https://dgsna.github.io</a>. </p>
<blockquote>
<p>为了确保语音系统在不同环境中的可靠运行，噪声添加方法已成为主流的解决方案。然而，现有方法对于真实世界噪声场景覆盖有限，并依赖于基于场景的预先存在信息和噪声。本文提出了基于提示的动态生成场景噪声添加（DGSNA），这是一种新型噪声添加方法，它将基于场景的动态信息生成（DGSI）与基于场景的语音噪声添加（SNAS）相结合。这种结合通过转换干净语音为各种噪声环境，实现了基于场景的自动噪声添加，从而提供了对多种噪声条件的更全面和现实的模拟。实验结果表明，DGSNA显著提高了语音识别和关键词点模型在各种噪声条件下的稳健性，相对改进率最高达11.21%。此外，DGSNA可以与其他噪声添加方法有效结合以提高性能。我们的实现和演示可在<a target="_blank" rel="noopener" href="https://dgsna.github.io查看./">https://dgsna.github.io查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12363v4">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本研究提出一种基于提示的动态生成场景噪声添加方法（DGSNA），集成动态生成场景信息（DGSI）与场景噪声添加技术（SNAS）。该方法可自动模拟多种噪声环境下的语音信号，提高语音识别和关键词识别模型在各种噪声条件下的稳健性，相对改进率高达11.21%。此外，DGSNA可与其他噪声添加方法结合使用，进一步提高性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>DGSNA是一种新型的噪声添加方法，结合了DGSI和SNAS技术。</li>
<li>该方法能够自动模拟多种噪声环境下的语音信号。</li>
<li>DGSNA提高了语音识别和关键词识别模型在各种噪声条件下的稳健性。</li>
<li>DGSNA相对改进率高达11.21%。</li>
<li>DGSNA可与其他噪声添加方法结合，进一步提高性能。</li>
<li>该方法的实施和演示可在网上找到。</li>
<li>DGSNA有助于确保语音系统在不同环境中的可靠运行。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.12363">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-97ad1331196a5539b210db04b0572d0a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="GTSinger-A-Global-Multi-Technique-Singing-Corpus-with-Realistic-Music-Scores-for-All-Singing-Tasks"><a href="#GTSinger-A-Global-Multi-Technique-Singing-Corpus-with-Realistic-Music-Scores-for-All-Singing-Tasks" class="headerlink" title="GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music   Scores for All Singing Tasks"></a>GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music   Scores for All Singing Tasks</h2><p><strong>Authors:Yu Zhang, Changhao Pan, Wenxiang Guo, Ruiqi Li, Zhiyuan Zhu, Jialei Wang, Wenhao Xu, Jingyu Lu, Zhiqing Hong, Chuxin Wang, LiChao Zhang, Jinzheng He, Ziyue Jiang, Yuxin Chen, Chen Yang, Jiecheng Zhou, Xinyu Cheng, Zhou Zhao</strong></p>
<p>The scarcity of high-quality and multi-task singing datasets significantly hinders the development of diverse controllable and personalized singing tasks, as existing singing datasets suffer from low quality, limited diversity of languages and singers, absence of multi-technique information and realistic music scores, and poor task suitability. To tackle these problems, we present GTSinger, a large global, multi-technique, free-to-use, high-quality singing corpus with realistic music scores, designed for all singing tasks, along with its benchmarks. Particularly, (1) we collect 80.59 hours of high-quality singing voices, forming the largest recorded singing dataset; (2) 20 professional singers across nine widely spoken languages offer diverse timbres and styles; (3) we provide controlled comparison and phoneme-level annotations of six commonly used singing techniques, helping technique modeling and control; (4) GTSinger offers realistic music scores, assisting real-world musical composition; (5) singing voices are accompanied by manual phoneme-to-audio alignments, global style labels, and 16.16 hours of paired speech for various singing tasks. Moreover, to facilitate the use of GTSinger, we conduct four benchmark experiments: technique-controllable singing voice synthesis, technique recognition, style transfer, and speech-to-singing conversion. The corpus and demos can be found at <a target="_blank" rel="noopener" href="http://aaronz345.github.io/GTSingerDemo/">http://aaronz345.github.io/GTSingerDemo/</a>. We provide the dataset and the code for processing data and conducting benchmarks at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/AaronZ345/GTSinger">https://huggingface.co/datasets/AaronZ345/GTSinger</a> and <a target="_blank" rel="noopener" href="https://github.com/AaronZ345/GTSinger">https://github.com/AaronZ345/GTSinger</a>. </p>
<blockquote>
<p>高质量多任务演唱数据集稀缺的问题显著阻碍了多样可控和个性化演唱任务的发展，因为现有的演唱数据集存在质量低、语言及歌手多样性有限、缺乏多技术信息和现实音乐乐谱以及任务适用性差等问题。为了解决这些问题，我们推出了GTSinger，这是一个大型的全球性、多技术、免费使用的高质量演唱语料库，具有现实音乐乐谱，适用于所有演唱任务，以及相应的基准测试。具体来说，（1）我们收集了80.59小时的高质量演唱声音，形成了最大的录制演唱数据集；（2）20名专业歌手跨越九种广泛使用的语言，展现了多样的音色和风格；（3）我们提供了六种常用演唱技术的受控比较和音素级注释，有助于技术建模和控制；（4）GTSinger提供了现实的乐谱，有助于现实音乐创作；（5）演唱声音配有手动音素到音频对齐、全局风格标签，以及用于各种演唱任务的16.16小时配套语音。此外，为了使用GTSinger，我们进行了四项基准测试：技术可控的演唱声音合成、技术识别、风格转换和语音到演唱的转换。语料库和演示可在<a target="_blank" rel="noopener" href="http://aaronz345.github.io/GTSingerDemo/%E6%89%BE%E5%88%B0%E3%80%82%E6%88%91%E4%BB%AC%E5%9C%A8https://huggingface.co/datasets/AaronZ345/GTSinger%E5%92%8Chttps://github.com/AaronZ345/GTSinger%E6%8F%90%E4%BE%9B%E4%BA%86%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%8F%8A%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">http://aaronz345.github.io/GTSingerDemo/找到。我们在https://huggingface.co/datasets/AaronZ345/GTSinger和https://github.com/AaronZ345/GTSinger提供了数据集和数据处理及基准测试的代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13832v6">PDF</a> Accepted by NeurIPS 2024 (Spotlight)</p>
<p><strong>摘要</strong></p>
<p>本文介绍了GTSinger这一全球大型、多技术、免费高质量歌唱数据集。该数据集拥有现实音乐曲目，适用于所有歌唱任务。数据集特点包括：收集80.59小时的高质量歌声，涉及九种广泛使用的语言；包含六种常用歌唱技术的对比和音素级注释；提供现实音乐曲目，辅助现实音乐创作；此外，还有手动音素到音频的对齐、全球风格标签和16.16小时的配对语音，用于各种歌唱任务。同时，为便于使用GTSinger，进行了四项基准实验：技术可控的歌唱声音合成、技术识别、风格转换和语音转歌唱。数据集和相关代码可在指定网站下载。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>GTSinger是一个全球性的大型歌唱数据集，旨在解决现有数据集在质量和多样性方面存在的问题。</li>
<li>数据集包含80.59小时的高质量歌声录音，是目前最大的已记录歌唱数据集。</li>
<li>数据集涵盖九种广泛使用的语言，提供了多样化的音色和风格。</li>
<li>数据集中包含六种常用歌唱技术的对比和音素级注释，有助于技术建模和控制。</li>
<li>GTSinger提供现实音乐曲目，支持真实音乐创作。</li>
<li>数据集包含手动音素到音频的对齐、全球风格标签和配对语音，适用于多种歌唱任务。</li>
<li>为更好地利用数据集，提供了四项基准实验，包括技术可控的歌唱声音合成等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.13832">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-78b2c0a548389ecb0386af8b0c67fec8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0a829871655320f45719954820b9f50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f36701794ff88abe07da6e61077bca2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad95cdd657d117546409a35f0b263aff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0f1a2777fc489b76f96c568fb6e77be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b8c04cc16af9c2620048c99092a7307.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ee1e7f017c3f53d3af04c4f984bce33.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-987d3d4d1f9dc684eec1d1f422388e82.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-05-14  GAN-based synthetic FDG PET images from T1 brain MRI can serve to   improve performance of deep unsupervised anomaly detection models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5b70c8a1fa1f6e99aa26c77655a99a72.jpg" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-05-14  DFEN Dual Feature Equalization Network for Medical Image Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23539.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
