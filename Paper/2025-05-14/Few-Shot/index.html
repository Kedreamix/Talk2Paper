<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  Beyond CLIP Generalization Against Forward&amp;Backward Forgetting Adapter   for Continual Learning of Vision-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a7232ea14da8cf4d1a5ec0344649ce78.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    46 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-14-æ›´æ–°"><a href="#2025-05-14-æ›´æ–°" class="headerlink" title="2025-05-14 æ›´æ–°"></a>2025-05-14 æ›´æ–°</h1><h2 id="Beyond-CLIP-Generalization-Against-Forward-Backward-Forgetting-Adapter-for-Continual-Learning-of-Vision-Language-Models"><a href="#Beyond-CLIP-Generalization-Against-Forward-Backward-Forgetting-Adapter-for-Continual-Learning-of-Vision-Language-Models" class="headerlink" title="Beyond CLIP Generalization: Against Forward&amp;Backward Forgetting Adapter   for Continual Learning of Vision-Language Models"></a>Beyond CLIP Generalization: Against Forward&amp;Backward Forgetting Adapter   for Continual Learning of Vision-Language Models</h2><p><strong>Authors:Songlin Dong, Chenhao Ding, Jiangyang Li, Jizhou Han, Qiang Wang, Yuhang He, Yihong Gong</strong></p>
<p>This study aims to address the problem of multi-domain task incremental learning<del>(MTIL), which requires that vision-language models</del>(VLMs) continuously acquire new knowledge while maintaining their inherent zero-shot recognition capability. Existing paradigms delegate the testing of unseen-domain samples to the original CLIP, which only prevents the degradation of the modelâ€™s zero-shot capability but fails to enhance the generalization of the VLM further. To this end, we propose a novel MTIL framework, named AFA, which comprises two core modules: (1) an against forward-forgetting adapter that learns task-invariant information for each dataset in the incremental tasks to enhance the zero-shot recognition ability of VLMs; (2) an against backward-forgetting adapter that strengthens the few-shot learning capability of VLMs while supporting incremental learning. Extensive experiments demonstrate that the AFA method significantly outperforms existing state-of-the-art approaches, especially in few-shot MTIL tasks, and surpasses the inherent zero-shot performance of CLIP in terms of transferability. The code is provided in the Supplementary Material. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤šåŸŸä»»åŠ¡å¢é‡å­¦ä¹ ï¼ˆMTILï¼‰çš„é—®é¢˜ï¼Œè¿™è¦æ±‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æŒç»­è·å–æ–°çŸ¥è¯†çš„åŒæ—¶ï¼Œä¿æŒå…¶å›ºæœ‰çš„é›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ã€‚ç°æœ‰èŒƒå¼å°†æœªè§åŸŸæ ·æœ¬çš„æµ‹è¯•å§”æ‰˜ç»™åŸå§‹CLIPï¼Œè¿™åªèƒ½é˜²æ­¢æ¨¡å‹é›¶æ ·æœ¬èƒ½åŠ›çš„é€€åŒ–ï¼Œä½†æœªèƒ½è¿›ä¸€æ­¥æé«˜VLMçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„MTILæ¡†æ¶ï¼Œåä¸ºAFAï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šï¼ˆ1ï¼‰ä¸€ç§å¯¹æŠ—å‰å‘é—å¿˜é€‚é…å™¨ï¼Œç”¨äºå­¦ä¹ å¢é‡ä»»åŠ¡ä¸­æ¯ä¸ªæ•°æ®é›†çš„ä»»åŠ¡ä¸å˜ä¿¡æ¯ï¼Œä»¥æé«˜VLMçš„é›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰ä¸€ç§å¯¹æŠ—åå‘é—å¿˜é€‚é…å™¨ï¼Œæ—¨åœ¨åŠ å¼ºVLMçš„å°æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼ŒåŒæ—¶æ”¯æŒå¢é‡å­¦ä¹ ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAFAæ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå°¤å…¶åœ¨å°‘æ ·æœ¬MTILä»»åŠ¡ä¸­è¡¨ç°çªå‡ºï¼Œå¹¶ä¸”åœ¨å¯è¿ç§»æ€§æ–¹é¢è¶…è¶Šäº†CLIPçš„å›ºæœ‰é›¶æ ·æœ¬æ€§èƒ½ã€‚ä»£ç å·²ä½œä¸ºè¡¥å……ææ–™æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07690v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šåŸŸä»»åŠ¡å¢é‡å­¦ä¹ ï¼ˆMTILï¼‰é—®é¢˜è¦æ±‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æŒç»­è·å–æ–°çŸ¥è¯†çš„åŒæ—¶ä¿æŒå…¶å›ºæœ‰çš„é›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ã€‚ç°æœ‰æ¨¡å¼å°†æœªè§åŸŸæ ·æœ¬çš„æµ‹è¯•å§”æ‰˜ç»™åŸå§‹CLIPï¼Œè¿™åªèƒ½é˜²æ­¢æ¨¡å‹é›¶æ ·æœ¬èƒ½åŠ›çš„é€€åŒ–ï¼Œä½†æœªèƒ½è¿›ä¸€æ­¥æé«˜VLMçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„MTILæ¡†æ¶ï¼Œåä¸ºAFAï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šï¼ˆ1ï¼‰ä¸€ç§é˜²æ­¢çŸ¥è¯†é—å¿˜çš„é€‚é…å™¨ï¼Œç”¨äºå­¦ä¹ å¢é‡ä»»åŠ¡ä¸­æ¯ä¸ªæ•°æ®é›†çš„ä»»åŠ¡ä¸å˜ä¿¡æ¯ï¼Œä»¥æé«˜VLMçš„é›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰å¦ä¸€ç§å¼ºåŒ–VLMçš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›å¹¶æ”¯æŒå¢é‡å­¦ä¹ çš„é˜²æ­¢åå‘é—å¿˜çš„é€‚é…å™¨ã€‚å®éªŒè¡¨æ˜ï¼ŒAFAæ–¹æ³•åœ¨å°‘æ ·æœ¬MTILä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œå¹¶ä¸”åœ¨å¯è¿ç§»æ€§æ–¹é¢è¶…è¶Šäº†CLIPçš„å›ºæœ‰é›¶æ ·æœ¬æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶å…³æ³¨å¤šåŸŸä»»åŠ¡å¢é‡å­¦ä¹ ï¼ˆMTILï¼‰ï¼Œæ—¨åœ¨è®©è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æŒç»­å­¦ä¹ æ–°çŸ¥è¯†çš„åŒæ—¶ä¿æŒé›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ¨¡å¼æµ‹è¯•æœªè§åŸŸæ ·æœ¬æ—¶å­˜åœ¨å±€é™æ€§ï¼Œä»…é˜²æ­¢æ¨¡å‹é›¶æ ·æœ¬èƒ½åŠ›é€€åŒ–ï¼Œæœªè¿›ä¸€æ­¥æé«˜VLMæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºçš„AFAæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šé˜²æ­¢æ­£å‘é—å¿˜çš„é€‚é…å™¨å’Œé˜²æ­¢åå‘é—å¿˜çš„é€‚é…å™¨ã€‚</li>
<li>é˜²æ­¢æ­£å‘é—å¿˜çš„é€‚é…å™¨é€šè¿‡å­¦ä¹ ä»»åŠ¡ä¸å˜ä¿¡æ¯æ¥æé«˜VLMçš„é›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>é˜²æ­¢åå‘é—å¿˜çš„é€‚é…å™¨å¼ºåŒ–VLMçš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶æ”¯æŒå¢é‡å­¦ä¹ ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒAFAæ–¹æ³•åœ¨å°‘æ ·æœ¬MTILä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d864b75af5f37950ed870a3d0a23c807.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e485d146906c8391353d4acd893ee42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88a5c5d03ba8f4dbb67ef08de6db4082.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfd0d44962ea6c428195d0c8971e8686.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ShotAdapter-Text-to-Multi-Shot-Video-Generation-with-Diffusion-Models"><a href="#ShotAdapter-Text-to-Multi-Shot-Video-Generation-with-Diffusion-Models" class="headerlink" title="ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models"></a>ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models</h2><p><strong>Authors:Ozgur Kara, Krishna Kumar Singh, Feng Liu, Duygu Ceylan, James M. Rehg, Tobias Hinz</strong></p>
<p>Current diffusion-based text-to-video methods are limited to producing short video clips of a single shot and lack the capability to generate multi-shot videos with discrete transitions where the same character performs distinct activities across the same or different backgrounds. To address this limitation we propose a framework that includes a dataset collection pipeline and architectural extensions to video diffusion models to enable text-to-multi-shot video generation. Our approach enables generation of multi-shot videos as a single video with full attention across all frames of all shots, ensuring character and background consistency, and allows users to control the number, duration, and content of shots through shot-specific conditioning. This is achieved by incorporating a transition token into the text-to-video model to control at which frames a new shot begins and a local attention masking strategy which controls the transition tokenâ€™s effect and allows shot-specific prompting. To obtain training data we propose a novel data collection pipeline to construct a multi-shot video dataset from existing single-shot video datasets. Extensive experiments demonstrate that fine-tuning a pre-trained text-to-video model for a few thousand iterations is enough for the model to subsequently be able to generate multi-shot videos with shot-specific control, outperforming the baselines. You can find more details in <a target="_blank" rel="noopener" href="https://shotadapter.github.io/">https://shotadapter.github.io/</a> </p>
<blockquote>
<p>å½“å‰åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è§†é¢‘è½¬æ¢æ–¹æ³•ä»…é™äºç”Ÿæˆå•é•œå¤´çš„ç®€çŸ­è§†é¢‘ç‰‡æ®µï¼Œç¼ºä¹ç”Ÿæˆå…·æœ‰ç¦»æ•£è¿‡æ¸¡çš„å¤šé•œå¤´è§†é¢‘çš„èƒ½åŠ›ï¼Œå…¶ä¸­åŒä¸€è§’è‰²åœ¨åŒä¸€æˆ–ä¸åŒèƒŒæ™¯ä¸‹æ‰§è¡Œä¸åŒçš„æ´»åŠ¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼ŒåŒ…æ‹¬æ•°æ®é›†æ”¶é›†ç®¡é“å’Œè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ¶æ„æ‰©å±•ï¼Œä»¥å®ç°æ–‡æœ¬åˆ°å¤šé•œå¤´è§†é¢‘çš„ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå°†å¤šé•œå¤´è§†é¢‘ç”Ÿæˆä¸ºä¸€ä¸ªå•ä¸€è§†é¢‘ï¼Œå…¨é¢å…³æ³¨æ‰€æœ‰é•œå¤´çš„æ‰€æœ‰å¸§ï¼Œç¡®ä¿è§’è‰²å’ŒèƒŒæ™¯çš„ä¸€è‡´æ€§ï¼Œå¹¶å…è®¸ç”¨æˆ·é€šè¿‡é•œå¤´ç‰¹å®šçš„æ¡ä»¶æ§åˆ¶é•œå¤´çš„æ•°é‡ã€æŒç»­æ—¶é—´å’Œå†…å®¹ã€‚è¿™æ˜¯é€šè¿‡å‘æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸­èå…¥è¿‡æ¸¡ä»¤ç‰Œæ¥å®ç°çš„ï¼Œè¯¥ä»¤ç‰Œæ§åˆ¶æ–°é•œå¤´åœ¨å“ªäº›å¸§å¼€å§‹ï¼Œä»¥åŠå±€éƒ¨æ³¨æ„åŠ›æ©ç ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ§åˆ¶è¿‡æ¸¡ä»¤ç‰Œçš„å½±å“ï¼Œå¹¶å…è®¸é’ˆå¯¹é•œå¤´è¿›è¡Œç‰¹å®šæç¤ºã€‚ä¸ºäº†è·å–è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®æ”¶é›†ç®¡é“ï¼Œç”¨äºä»ç°æœ‰çš„å•é•œå¤´è§†é¢‘æ•°æ®é›†ä¸­æ„å»ºå¤šé•œå¤´è§†é¢‘æ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå¯¹é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹è¿›è¡Œæ•°åƒæ¬¡è¿­ä»£çš„å¾®è°ƒè¶³ä»¥ä½¿æ¨¡å‹éšåèƒ½å¤Ÿå…·æœ‰é’ˆå¯¹é•œå¤´çš„æ§åˆ¶ç”Ÿæˆå¤šé•œå¤´è§†é¢‘ï¼Œå¹¶ä¸”è¡¨ç°ä¼˜äºåŸºçº¿ã€‚æ›´å¤šç»†èŠ‚å¯åœ¨<a target="_blank" rel="noopener" href="https://shotadapter.github.io/">https://shotadapter.github.io/</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07652v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§èƒ½å¤Ÿç”Ÿæˆæ–‡æœ¬åˆ°å¤šé•œå¤´è§†é¢‘çš„æ¡†æ¶ï¼ŒåŒ…æ‹¬æ•°æ®é›†æ”¶é›†ç®¡é“å’Œæ¶æ„æ‰©å±•çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ–‡æœ¬è½¬è§†é¢‘æ–¹æ³•ä»…é™äºç”Ÿæˆå•é•œå¤´çŸ­è§†é¢‘çš„é—®é¢˜ï¼Œå¹¶å®ç°äº†åœ¨å¤šé•œå¤´è§†é¢‘ç”Ÿæˆä¸­è·¨è¶Šç›¸åŒæˆ–ä¸åŒèƒŒæ™¯çš„ç›¸åŒè§’è‰²æ‰§è¡Œä¸åŒæ´»åŠ¨çš„åŠŸèƒ½ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡é•œå¤´ç‰¹å®šçš„æ¡ä»¶æ§åˆ¶é•œå¤´çš„æ•°é‡ã€æŒç»­æ—¶é—´å’Œå†…å®¹ã€‚é€šè¿‡å¼•å…¥è¿‡æ¸¡ä»¤ç‰Œæ¥æ§åˆ¶æ–°é•œå¤´å¼€å§‹çš„ä½ç½®å’Œå±€éƒ¨æ³¨æ„åŠ›æ©ç ç­–ç•¥æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ä¸ºäº†è·å–è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®æ”¶é›†ç®¡é“ï¼Œä»ç°æœ‰çš„å•é•œå¤´è§†é¢‘æ•°æ®é›†ä¸­æ„å»ºå¤šé•œå¤´è§†é¢‘æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼Œå¯¹é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹è¿›è¡Œæ•°åƒæ¬¡è¿­ä»£çš„å¾®è°ƒè¶³ä»¥ä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå…·æœ‰é•œå¤´ç‰¹å®šæ§åˆ¶çš„å¤šé•œå¤´è§†é¢‘ï¼Œè¶…è¶Šäº†åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ä¸ªèƒ½å¤Ÿç”Ÿæˆæ–‡æœ¬åˆ°å¤šé•œå¤´è§†é¢‘çš„æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä»…é™äºå•é•œå¤´è§†é¢‘çš„å±€é™æ€§ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬æ•°æ®é›†æ”¶é›†ç®¡é“å’Œè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ¶æ„æ‰©å±•ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥é€šè¿‡é•œå¤´ç‰¹å®šçš„æ¡ä»¶æ§åˆ¶é•œå¤´çš„æ•°é‡ã€æŒç»­æ—¶é—´å’Œå†…å®¹ã€‚</li>
<li>é€šè¿‡å¼•å…¥è¿‡æ¸¡ä»¤ç‰Œå’Œå±€éƒ¨æ³¨æ„åŠ›æ©ç ç­–ç•¥å®ç°å¤šé•œå¤´è§†é¢‘çš„ç”Ÿæˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®æ”¶é›†ç®¡é“ï¼Œç”¨äºä»ç°æœ‰çš„å•é•œå¤´è§†é¢‘æ•°æ®é›†ä¸­æ„å»ºå¤šé•œå¤´è§†é¢‘æ•°æ®é›†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œå¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒèƒ½å¤Ÿç”Ÿæˆå…·æœ‰é•œå¤´ç‰¹å®šæ§åˆ¶çš„å¤šé•œå¤´è§†é¢‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f42e8d4a727df297b4b36dee74ef479f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b025acb03d316dc295d0eb37237dc441.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d52407d9d1d790be43b38748cf2d2a3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b676f46e61f0420f1720e2fe25f143b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dcbaae99f4558c682cdd536c5b95323.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ReinboT-Amplifying-Robot-Visual-Language-Manipulation-with-Reinforcement-Learning"><a href="#ReinboT-Amplifying-Robot-Visual-Language-Manipulation-with-Reinforcement-Learning" class="headerlink" title="ReinboT: Amplifying Robot Visual-Language Manipulation with   Reinforcement Learning"></a>ReinboT: Amplifying Robot Visual-Language Manipulation with   Reinforcement Learning</h2><p><strong>Authors:Hongyin Zhang, Zifeng Zhuang, Han Zhao, Pengxiang Ding, Hongchao Lu, Donglin Wang</strong></p>
<p>Vision-Language-Action (VLA) models have shown great potential in general robotic decision-making tasks via imitation learning. However, the variable quality of training data often constrains the performance of these models. On the other hand, offline Reinforcement Learning (RL) excels at learning robust policy models from mixed-quality data. In this paper, we introduce Reinforced robot GPT (ReinboT), a novel end-to-end VLA model that integrates the RL principle of maximizing cumulative reward. ReinboT achieves a deeper understanding of the data quality distribution by predicting dense returns that capture the nuances of manipulation tasks. The dense return prediction capability enables the robot to generate more robust decision-making actions, oriented towards maximizing future benefits. Extensive experiments show that ReinboT achieves state-of-the-art performance on the CALVIN mixed-quality dataset and exhibits superior few-shot learning and out-of-distribution generalization capabilities in real-world tasks. </p>
<blockquote>
<p>è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨é€šè¿‡æ¨¡ä»¿å­¦ä¹ è¿›è¡Œä¸€èˆ¬çš„æœºå™¨äººå†³ç­–ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè®­ç»ƒæ•°æ®è´¨é‡çš„ä¸ç¨³å®šç»å¸¸é™åˆ¶è¿™äº›æ¨¡å‹çš„æ€§èƒ½ã€‚å¦ä¸€æ–¹é¢ï¼Œç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ“…é•¿ä»æ··åˆè´¨é‡æ•°æ®ä¸­å­¦ä¹ ç¨³å¥çš„ç­–ç•¥æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¼ºåŒ–æœºå™¨äººGPTï¼ˆReinboTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯çš„VLAæ¨¡å‹ï¼Œé›†æˆäº†å¼ºåŒ–å­¦ä¹ çš„æœ€å¤§åŒ–ç´¯ç§¯å›æŠ¥åŸåˆ™ã€‚ReinboTé€šè¿‡é¢„æµ‹å¯†é›†å›æŠ¥æ¥å®ç°å¯¹æ•°æ®è´¨é‡åˆ†å¸ƒçš„æ·±å…¥ç†è§£ï¼Œè¿™äº›å¯†é›†å›æŠ¥æ•æ‰åˆ°äº†æ“ä½œä»»åŠ¡çš„ç»†å¾®å·®åˆ«ã€‚å¯†é›†å›æŠ¥é¢„æµ‹èƒ½åŠ›ä½¿æœºå™¨äººèƒ½å¤Ÿç”Ÿæˆæ›´ç¨³å¥çš„å†³ç­–è¡ŒåŠ¨ï¼Œè‡´åŠ›äºæœ€å¤§åŒ–æœªæ¥æ”¶ç›Šã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒReinboTåœ¨CALVINæ··åˆè´¨é‡æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼Œå¹¶åœ¨ç°å®ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„å°æ ·æœ¬å­¦ä¹ å’Œè·¨åˆ†å¸ƒæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07395v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œçš„VLAæ¨¡å‹åœ¨æœºå™¨äººå†³ç­–ä»»åŠ¡ä¸­å±•ç°å‡ºäº†å·¨å¤§æ½œåŠ›ï¼Œä½†å®ƒä»¬å¸¸å¸¸å—é™äºè®­ç»ƒæ•°æ®çš„è´¨é‡é—®é¢˜ã€‚è€Œç¦»çº¿å¼ºåŒ–å­¦ä¹ æ“…é•¿ä»æ··åˆè´¨é‡æ•°æ®ä¸­å­¦ä¹ ç¨³å¥çš„ç­–ç•¥æ¨¡å‹ã€‚æœ¬æ–‡ä»‹ç»äº†èåˆå¼ºåŒ–å­¦ä¹ åŸç†çš„æœºå™¨äººGPTï¼ˆReinboTï¼‰ï¼Œä¸€ç§æ–°å‹ç«¯åˆ°ç«¯çš„VLAæ¨¡å‹ã€‚ReinboTé€šè¿‡é¢„æµ‹å¯†é›†å›æŠ¥æ¥ç†è§£æ•°æ®è´¨é‡åˆ†å¸ƒï¼Œä»è€Œå®ç°åœ¨æ“ä½œä»»åŠ¡ä¸­çš„ç»†å¾®å·®åˆ«ã€‚è¿™ç§å¯†é›†å›æŠ¥é¢„æµ‹èƒ½åŠ›ä½¿æœºå™¨äººèƒ½å¤Ÿç”Ÿæˆæ›´ç¨³å¥çš„å†³ç­–è¡ŒåŠ¨ï¼Œè‡´åŠ›äºæœ€å¤§åŒ–æœªæ¥æ”¶ç›Šã€‚å®éªŒè¡¨æ˜ï¼ŒReinboTåœ¨CALVINæ··åˆè´¨é‡æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨ç°å®ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„å°æ ·æœ¬å­¦ä¹ å’Œè·¨åˆ†å¸ƒæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLAæ¨¡å‹åœ¨æœºå™¨äººå†³ç­–ä»»åŠ¡ä¸­æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å—é™äºè®­ç»ƒæ•°æ®è´¨é‡ã€‚</li>
<li>ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ“…é•¿ä»æ··åˆè´¨é‡æ•°æ®ä¸­å­¦ä¹ ç¨³å¥ç­–ç•¥æ¨¡å‹ã€‚</li>
<li>ReinboTæ˜¯ç»“åˆå¼ºåŒ–å­¦ä¹ åŸç†çš„æ–°å‹VLAæ¨¡å‹ï¼Œèƒ½é¢„æµ‹å¯†é›†å›æŠ¥ä»¥ç†è§£æ•°æ®è´¨é‡åˆ†å¸ƒã€‚</li>
<li>å¯†é›†å›æŠ¥é¢„æµ‹èƒ½åŠ›æœ‰åŠ©äºæœºå™¨äººç”Ÿæˆæ›´ç¨³å¥çš„å†³ç­–è¡ŒåŠ¨ï¼Œè‡´åŠ›äºæœ€å¤§åŒ–æœªæ¥æ”¶ç›Šã€‚</li>
<li>ReinboTåœ¨CALVINæ··åˆè´¨é‡æ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>ReinboTå±•ç°å‡ºå“è¶Šçš„å°æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-be18356cc1b47e9962356e7df6ab1b1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97c1df44ac36e5d3b9a10d34a84f0320.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cda1c4ec6f1c728866b2b8b9e451e22.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ReCDAP-Relation-Based-Conditional-Diffusion-with-Attention-Pooling-for-Few-Shot-Knowledge-Graph-Completion"><a href="#ReCDAP-Relation-Based-Conditional-Diffusion-with-Attention-Pooling-for-Few-Shot-Knowledge-Graph-Completion" class="headerlink" title="ReCDAP: Relation-Based Conditional Diffusion with Attention Pooling for   Few-Shot Knowledge Graph Completion"></a>ReCDAP: Relation-Based Conditional Diffusion with Attention Pooling for   Few-Shot Knowledge Graph Completion</h2><p><strong>Authors:Jeongho Kim, Chanyeong Heo, Jaehee Jung</strong></p>
<p>Knowledge Graphs (KGs), composed of triples in the form of (head, relation, tail) and consisting of entities and relations, play a key role in information retrieval systems such as question answering, entity search, and recommendation. In real-world KGs, although many entities exist, the relations exhibit a long-tail distribution, which can hinder information retrieval performance. Previous few-shot knowledge graph completion studies focused exclusively on the positive triple information that exists in the graph or, when negative triples were incorporated, used them merely as a signal to indicate incorrect triples. To overcome this limitation, we propose Relation-Based Conditional Diffusion with Attention Pooling (ReCDAP). First, negative triples are generated by randomly replacing the tail entity in the support set. By conditionally incorporating positive information in the KG and non-existent negative information into the diffusion process, the model separately estimates the latent distributions for positive and negative relations. Moreover, including an attention pooler enables the model to leverage the differences between positive and negative cases explicitly. Experiments on two widely used datasets demonstrate that our method outperforms existing approaches, achieving state-of-the-art performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/hou27/ReCDAP-FKGC">https://github.com/hou27/ReCDAP-FKGC</a>. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ç”±(å¤´å®ä½“ï¼Œå…³ç³»ï¼Œå°¾å®ä½“)å½¢å¼çš„ä¸‰å…ƒç»„æ„æˆï¼ŒåŒ…å«å®ä½“å’Œå…³ç³»ï¼Œåœ¨é—®ç­”ã€å®ä½“æœç´¢å’Œæ¨èç­‰ä¿¡æ¯ç³»ç»Ÿæ£€ç´¢ä¸­æ‰®æ¼”ç€å…³é”®è§’è‰²ã€‚åœ¨ç°å®ä¸–ç•Œçš„çŸ¥è¯†å›¾è°±ä¸­ï¼Œå°½ç®¡å­˜åœ¨è®¸å¤šå®ä½“ï¼Œä½†å…³ç³»å‘ˆç°å‡ºé•¿å°¾åˆ†å¸ƒï¼Œå¯èƒ½ä¼šé˜»ç¢ä¿¡æ¯æ£€ç´¢æ€§èƒ½ã€‚ä»¥å‰å…³äºçŸ¥è¯†å›¾è°±è¡¥å…¨çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å›¾ä¸­å­˜åœ¨çš„æ­£ä¸‰å…ƒç»„ä¿¡æ¯ä¸Šï¼Œæˆ–è€…å½“å¼•å…¥è´Ÿä¸‰å…ƒç»„æ—¶ï¼Œä»…ä»…å°†å®ƒä»¬ä½œä¸ºè¡¨ç¤ºé”™è¯¯ä¸‰å…ƒç»„çš„ä¿¡å·ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå…³ç³»çš„æ¡ä»¶æ‰©æ•£ä¸æ³¨æ„åŠ›æ± åŒ–ï¼ˆReCDAPï¼‰ã€‚é¦–å…ˆï¼Œé€šè¿‡éšæœºæ›¿æ¢æ”¯æŒé›†ä¸­çš„å°¾å®ä½“æ¥ç”Ÿæˆè´Ÿä¸‰å…ƒç»„ã€‚é€šè¿‡æœ‰æ¡ä»¶åœ°èå…¥çŸ¥è¯†å›¾è°±ä¸­çš„æ­£ä¿¡æ¯å’Œä¸å­˜åœ¨çš„è´Ÿä¿¡æ¯åˆ°æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œè¯¥æ¨¡å‹åˆ†åˆ«ä¼°è®¡æ­£å…³ç³»å’Œè´Ÿå…³ç³»çš„æ½œåœ¨åˆ†å¸ƒã€‚æ­¤å¤–ï¼ŒåŠ å…¥æ³¨æ„åŠ›æ± åŒ–å™¨ä½¿æ¨¡å‹èƒ½å¤Ÿæ˜ç¡®åˆ©ç”¨æ­£è´Ÿæ¡ˆä¾‹ä¹‹é—´çš„å·®å¼‚ã€‚åœ¨ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/hou27/ReCDAP-FKGC%E3%80%82">https://github.com/hou27/ReCDAP-FKGCã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07171v1">PDF</a> Accepted by SIGIR 2025, 5 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†çŸ¥è¯†å›¾è°±åœ¨ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿä¸­çš„ä½œç”¨ï¼Œå¦‚é—®ç­”ã€å®ä½“æœç´¢å’Œæ¨èç­‰ã€‚é’ˆå¯¹çŸ¥è¯†å›¾è°±è¡¥å…¨ä¸­çš„é•¿å°¾åˆ†å¸ƒé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå…³ç³»æ¡ä»¶æ‰©æ•£ä¸æ³¨æ„åŠ›æ± åŒ–çš„æ–¹æ³•ï¼ˆReCDAPï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆè´Ÿä¸‰å…ƒç»„ï¼Œç»“åˆæ­£ä¸‰å…ƒç»„ä¿¡æ¯ï¼Œåˆ†åˆ«ä¼°è®¡æ­£è´Ÿå…³ç³»çš„æ½œåœ¨åˆ†å¸ƒã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¸¸ç”¨æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†å›¾è°±åœ¨ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œå¦‚é—®ç­”ã€å®ä½“æœç´¢å’Œæ¨èã€‚</li>
<li>ç°å®ä¸–ç•Œä¸­çŸ¥è¯†å›¾è°±çš„å®ä½“å…³ç³»å‘ˆç°é•¿å°¾åˆ†å¸ƒï¼Œå½±å“ä¿¡æ¯æ£€ç´¢æ€§èƒ½ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æ­£ä¸‰å…ƒç»„ä¿¡æ¯ï¼Œå¿½è§†äº†è´Ÿä¸‰å…ƒç»„åœ¨ä¿¡æ¯æ£€ç´¢ä¸­çš„ä½œç”¨ã€‚</li>
<li>ReCDAPæ–¹æ³•é€šè¿‡ç”Ÿæˆè´Ÿä¸‰å…ƒç»„ï¼Œç»“åˆæ­£ä¸‰å…ƒç»„ä¿¡æ¯ï¼Œè¿›è¡ŒçŸ¥è¯†å›¾è°±è¡¥å…¨ã€‚</li>
<li>ReCDAPæ–¹æ³•ä¼°è®¡æ­£è´Ÿå…³ç³»çš„æ½œåœ¨åˆ†å¸ƒï¼Œæé«˜çŸ¥è¯†å›¾è°±è¡¥å…¨çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒReCDAPæ–¹æ³•åœ¨å¸¸ç”¨æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07171">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4fc2e28f8e7dd0cfb73400b1a9fea471.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0690f6f90a0b07c1b6a2d51c6e8aaa86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6a8c6dd8c8ce6d4c69d4655c4fbdb7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83542ee56278089fa58dec6555e5f020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-741ddfdcdef757d302d76d11148b1be4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dadfc69e1fcd1366721e38450046ea75.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Vision-Language-Foundation-Model-for-Leaf-Disease-Identification"><a href="#A-Vision-Language-Foundation-Model-for-Leaf-Disease-Identification" class="headerlink" title="A Vision-Language Foundation Model for Leaf Disease Identification"></a>A Vision-Language Foundation Model for Leaf Disease Identification</h2><p><strong>Authors:Khang Nguyen Quoc, Lan Le Thi Thu, Luyl-Da Quach</strong></p>
<p>Leaf disease identification plays a pivotal role in smart agriculture. However, many existing studies still struggle to integrate image and textual modalities to compensate for each otherâ€™s limitations. Furthermore, many of these approaches rely on pretraining with constrained datasets such as ImageNet, which lack domain-specific information. We propose SCOLD (Soft-target COntrastive learning for Leaf Disease identification), a context-aware vision-language foundation model tailored to address these challenges for agricultural tasks. SCOLD is developed using a diverse corpus of plant leaf images and corresponding symptom descriptions, comprising over 186,000 image-caption pairs aligned with 97 unique concepts. Through task-agnostic pretraining, SCOLD leverages contextual soft targets to mitigate overconfidence in contrastive learning by smoothing labels, thereby improving model generalization and robustness on fine-grained classification tasks. Experimental results demonstrate that SCOLD outperforms existing vision-language models such as OpenAI-CLIP-L, BioCLIP, and SigLIP2 across several benchmarks, including zero-shot and few-shot classification, image-text retrieval, and image classification, while maintaining a competitive parameter footprint. Ablation studies further highlight SCOLDâ€™s effectiveness in contrast to its counterparts. The proposed approach significantly advances the agricultural vision-language foundation model, offering strong performance with minimal or no supervised fine-tuning. This work lays a solid groundwork for future research on models trained with long-form and simplified contexts, tasks involving class ambiguity, and multi-modal systems for intelligent plant disease diagnostics. The code for this study is available at <a target="_blank" rel="noopener" href="https://huggingface.co/enalis/scold">https://huggingface.co/enalis/scold</a> </p>
<blockquote>
<p>å¶ç‰‡ç–¾ç—…è¯†åˆ«åœ¨æ™ºèƒ½å†œä¸šä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰çš„ç ”ç©¶ä»éš¾ä»¥æ•´åˆå›¾åƒå’Œæ–‡æœ¬æ¨¡æ€ä»¥ç›¸äº’å¼¥è¡¥å½¼æ­¤çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•ä¸­è®¸å¤šä¾èµ–äºä½¿ç”¨å—é™æ•°æ®é›†ï¼ˆå¦‚ImageNetï¼‰è¿›è¡Œé¢„è®­ç»ƒï¼Œè¿™äº›æ•°æ®é›†ç¼ºä¹ç‰¹å®šé¢†åŸŸçš„è¯¦ç»†ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†SCOLDï¼ˆé’ˆå¯¹å¶ç‰‡ç–¾ç—…è¯†åˆ«çš„è½¯ç›®æ ‡å¯¹æ¯”å­¦ä¹ æ³•ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é¢å‘å†œä¸šä»»åŠ¡æŒ‘æˆ˜çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚SCOLDä½¿ç”¨åŒ…å«è¶…è¿‡18.6ä¸‡å¼ ä¸97ä¸ªç‹¬ç‰¹æ¦‚å¿µå¯¹é½çš„å›¾åƒæ ‡é¢˜å¯¹æ„æˆçš„å¤šæ ·åŒ–æ¤ç‰©å¶ç‰‡å›¾åƒå’Œç›¸åº”çš„ç—‡çŠ¶æè¿°æ¥å¼€å‘ã€‚é€šè¿‡ä»»åŠ¡æ— å…³çš„é¢„è®­ç»ƒï¼ŒSCOLDåˆ©ç”¨ä¸Šä¸‹æ–‡è½¯ç›®æ ‡æ¥ç¼“è§£å¯¹æ¯”å­¦ä¹ ä¸­æ ‡ç­¾å¹³æ»‘å¯¼è‡´çš„è¿‡åº¦è‡ªä¿¡é—®é¢˜ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨ç»†ç²’åº¦åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ³›åŒ–å’Œç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSCOLDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¦‚OpenAI-CLIP-Lã€BioCLIPå’ŒSigLIP2ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åˆ†ç±»ã€å›¾åƒæ–‡æœ¬æ£€ç´¢å’Œå›¾åƒåˆ†ç±»ç­‰ä»»åŠ¡ï¼ŒåŒæ—¶ä¿æŒç«äº‰æ€§çš„å‚æ•°å ç”¨ç©ºé—´ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥çªå‡ºäº†SCOLDä¸ç«äº‰å¯¹æ‰‹çš„æœ‰æ•ˆæ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•æ˜¾è‘—åœ°æ¨è¿›äº†å†œä¸šè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„å‘å±•ï¼Œä»¥æœ€å°çš„æˆ–æ— éœ€ç›‘ç£å¾®è°ƒå°±èƒ½å®ç°å¼ºåŠ²çš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œä¸ºæœªæ¥åœ¨æ¨¡å‹ä½¿ç”¨é•¿å½¢å¼ç®€åŒ–è¯­å¢ƒã€æ¶‰åŠç±»åˆ«æ¨¡ç³Šæ€§çš„ä»»åŠ¡å’Œå¤šæ¨¡æ€ç³»ç»Ÿæ™ºèƒ½æ¤ç‰©ç–¾ç—…è¯Šæ–­ç­‰é¢†åŸŸçš„ç ”ç©¶å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚è¿™é¡¹ç ”ç©¶çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/enalis/scold">https://huggingface.co/enalis/scold</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07019v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹å†œä¸šä»»åŠ¡ä¸­çš„å¶ç—…è¯†åˆ«æŒ‘æˆ˜çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å¼€å‘ä¸€ç§åä¸ºSCOLDçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿèåˆå›¾åƒå’Œæ–‡æœ¬æ¨¡æ€ï¼Œå…‹æœå•ä¸€æ¨¡æ€çš„å±€é™æ€§ã€‚SCOLDæ¨¡å‹ä½¿ç”¨åŒ…å«è¶…è¿‡18.6ä¸‡å¼ æ¤ç‰©å¶ç‰‡å›¾åƒå’Œç›¸åº”ç—‡çŠ¶æè¿°çš„å¤šæ ·è¯­æ–™åº“è¿›è¡Œè®­ç»ƒï¼Œæ¶µç›–97ä¸ªç‹¬ç‰¹æ¦‚å¿µã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSCOLDåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åˆ†ç±»ã€å›¾åƒæ–‡æœ¬æ£€ç´¢å’Œå›¾åƒåˆ†ç±»ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¦‚OpenAI-CLIP-Lã€BioCLIPå’ŒSigLIP2ï¼ŒåŒæ—¶ä¿æŒç«äº‰æ€§çš„å‚æ•°å ç”¨ã€‚è¯¥æ¨¡å‹ä¸ºæé«˜å†œä¸šè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¶ç—…è¯†åˆ«åœ¨æ™ºèƒ½å†œä¸šä¸­è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰ç ”ç©¶åœ¨æ•´åˆå›¾åƒå’Œæ–‡æœ¬æ¨¡æ€æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>SCOLDæ¨¡å‹æ˜¯ä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œç‰¹åˆ«é’ˆå¯¹å†œä¸šä»»åŠ¡ã€‚</li>
<li>SCOLDæ¨¡å‹ä½¿ç”¨åŒ…å«å¤§é‡æ¤ç‰©å¶ç‰‡å›¾åƒå’Œç—‡çŠ¶æè¿°çš„è¯­æ–™åº“è¿›è¡Œè®­ç»ƒï¼Œæ¶µç›–å¤šç§æ¦‚å¿µå’Œä»»åŠ¡ã€‚</li>
<li>SCOLDé€šè¿‡ä»»åŠ¡æ— å…³é¢„è®­ç»ƒå’Œä¸Šä¸‹æ–‡è½¯ç›®æ ‡ï¼Œåˆ©ç”¨æ ‡ç­¾å¹³æ»‘æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–å’Œé²æ£’æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSCOLDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå…¶ä»–è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
<li>SCOLDæ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åˆ†ç±»ã€å›¾åƒæ–‡æœ¬æ£€ç´¢å’Œå›¾åƒåˆ†ç±»ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-64dbeff0a9f62d097d03f1479a1c1870.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c922887ff792db98d39fc3c760871f53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d2636453da2551e77ebcc02ccb3cf51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-489a06f94d885f69651dea5b0ddb9218.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Adaptive-Wiping-Adaptive-contact-rich-manipulation-through-few-shot-imitation-learning-with-Force-Torque-feedback-and-pre-trained-object-representations"><a href="#Adaptive-Wiping-Adaptive-contact-rich-manipulation-through-few-shot-imitation-learning-with-Force-Torque-feedback-and-pre-trained-object-representations" class="headerlink" title="Adaptive Wiping: Adaptive contact-rich manipulation through few-shot   imitation learning with Force-Torque feedback and pre-trained object   representations"></a>Adaptive Wiping: Adaptive contact-rich manipulation through few-shot   imitation learning with Force-Torque feedback and pre-trained object   representations</h2><p><strong>Authors:Chikaha Tsuji, Enrique Coronado, Pablo Osorio, Gentiane Venture</strong></p>
<p>Imitation learning offers a pathway for robots to perform repetitive tasks, allowing humans to focus on more engaging and meaningful activities. However, challenges arise from the need for extensive demonstrations and the disparity between training and real-world environments. This paper focuses on contact-rich tasks like wiping with soft and deformable objects, requiring adaptive force control to handle variations in wiping surface height and the spongeâ€™s physical properties. To address these challenges, we propose a novel method that integrates real-time force-torque (FT) feedback with pre-trained object representations. This approach allows robots to dynamically adjust to previously unseen changes in surface heights and spongesâ€™ physical properties. In real-world experiments, our method achieved 96% accuracy in applying reference forces, significantly outperforming the previous method that lacked an FT feedback loop, which only achieved 4% accuracy. To evaluate the adaptability of our approach, we conducted experiments under different conditions from the training setup, involving 40 scenarios using 10 sponges with varying physical properties and 4 types of wiping surface heights, demonstrating significant improvements in the robotâ€™s adaptability by analyzing force trajectories. The video of our work is available at: <a target="_blank" rel="noopener" href="https://sites.google.com/view/adaptive-wiping">https://sites.google.com/view/adaptive-wiping</a> </p>
<blockquote>
<p>æ¨¡ä»¿å­¦ä¹ ä¸ºæœºå™¨äººæ‰§è¡Œé‡å¤æ€§ä»»åŠ¡æä¾›äº†é€”å¾„ï¼Œä½¿äººç±»èƒ½å¤Ÿä¸“æ³¨äºæ›´å…·å¸å¼•åŠ›å’Œæœ‰æ„ä¹‰çš„æ´»åŠ¨ã€‚ç„¶è€Œï¼ŒæŒ‘æˆ˜æ¥è‡ªäºéœ€è¦å¤§é‡çš„æ¼”ç¤ºä»¥åŠè®­ç»ƒç¯å¢ƒä¸çœŸå®ç¯å¢ƒä¹‹é—´çš„å·®å¼‚ã€‚æœ¬æ–‡èšç„¦äºæ“¦æ‹­ç±»ä»»åŠ¡ï¼Œå¦‚ä½¿ç”¨æŸ”è½¯ä¸”å¯å˜å½¢ç‰©ä½“çš„æ“¦æ‹­æ“ä½œï¼Œéœ€è¦è‡ªé€‚åº”åŠ›æ§åˆ¶æ¥å¤„ç†æ“¦æ‹­è¡¨é¢é«˜åº¦å’Œæµ·ç»µçš„ç‰©ç†å±æ€§å˜åŒ–ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†å®æ—¶åŠ›æ‰­çŸ©ï¼ˆFTï¼‰åé¦ˆä¸é¢„è®­ç»ƒçš„å¯¹è±¡è¡¨ç¤ºç›¸ç»“åˆã€‚è¿™ç§æ–¹æ³•ä½¿æœºå™¨äººèƒ½å¤ŸåŠ¨æ€é€‚åº”ä»¥å‰æœªè§è¿‡çš„è¡¨é¢é«˜åº¦å’Œæµ·ç»µç‰©ç†å±æ€§çš„å˜åŒ–ã€‚åœ¨çœŸå®ä¸–ç•Œçš„å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åº”ç”¨å‚è€ƒåŠ›æ–¹é¢è¾¾åˆ°äº†96%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºç¼ºä¹FTåé¦ˆå›è·¯çš„æ–¹æ³•ï¼Œåè€…ä»…è¾¾åˆ°4%çš„å‡†ç¡®ç‡ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬æ–¹æ³•çš„é€‚åº”æ€§ï¼Œæˆ‘ä»¬åœ¨ä¸åŒäºè®­ç»ƒç¯å¢ƒçš„æ¡ä»¶ä¸‹è¿›è¡Œäº†å®éªŒï¼Œæ¶‰åŠä½¿ç”¨å…·æœ‰ä¸åŒç‰©ç†å±æ€§çš„10ç§æµ·ç»µå’Œ4ç§æ“¦æ‹­è¡¨é¢é«˜åº¦çš„40ä¸ªåœºæ™¯ï¼Œé€šè¿‡åˆ†æåŠ›è½¨è¿¹ï¼Œæ˜¾ç¤ºå‡ºæœºå™¨äººåœ¨é€‚åº”æ€§æ–¹é¢çš„æ˜¾è‘—æ”¹å–„ã€‚æˆ‘ä»¬å·¥ä½œçš„è§†é¢‘å¯åœ¨ä»¥ä¸‹ç½‘å€è§‚çœ‹ï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/adaptive%2dwiping">https://sites.google.com/view/adaptive-wiping</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06451v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœºå™¨äººé€šè¿‡æ¨¡ä»¿å­¦ä¹ æ‰§è¡Œé‡å¤ä»»åŠ¡ï¼Œä½¿äººç±»èƒ½å¤Ÿä¸“æ³¨äºæ›´æœ‰æ„ä¹‰å’Œå¸å¼•åŠ›çš„æ´»åŠ¨ã€‚ç„¶è€Œï¼Œè¯¥æ–¹æ³•é¢ä¸´éœ€è¦æµ·é‡æ¼”ç¤ºæ•°æ®å’Œè®­ç»ƒç¯å¢ƒä¸çœŸå®ç¯å¢ƒå­˜åœ¨å·®å¼‚çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹æ“¦æ‹­ç­‰æ¥è§¦ä¸°å¯Œçš„ä»»åŠ¡ï¼Œæœ¬æ–‡æå‡ºä¸€ç§ç»“åˆå®æ—¶åŠ›æ‰­çŸ©åé¦ˆä¸é¢„è®­ç»ƒç‰©ä½“è¡¨å¾çš„æ–°æ–¹æ³•ï¼Œæœºå™¨äººå¯æ ¹æ®è¡¨é¢é«˜åº¦å’Œæµ·ç»µç‰©ç†å±æ€§çš„å˜åŒ–åŠ¨æ€è°ƒæ•´ã€‚åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œæ–°æ–¹æ³•åœ¨æ–½åŠ å‚è€ƒåŠ›æ–¹é¢å®ç°äº†96%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºç¼ºä¹åŠ›æ‰­çŸ©åé¦ˆçš„ä»¥å¾€æ–¹æ³•ï¼ˆä»…4%çš„å‡†ç¡®ç‡ï¼‰ã€‚é€šè¿‡åœ¨ä¸åŒæ¡ä»¶ä¸‹è¿›è¡Œå®éªŒï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨ä¸åŒæµ·ç»µç‰©ç†å±æ€§å’Œè¡¨é¢é«˜åº¦ä¸‹çš„é€‚åº”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡ä»¿å­¦ä¹ å…è®¸æœºå™¨äººæ‰§è¡Œé‡å¤ä»»åŠ¡ï¼Œä½¿äººç±»ä¸“æ³¨äºæ›´æœ‰æ„ä¹‰çš„æ´»åŠ¨ã€‚</li>
<li>æ¨¡ä»¿å­¦ä¹ é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬éœ€è¦å¤§é‡æ¼”ç¤ºæ•°æ®å’Œè®­ç»ƒç¯å¢ƒä¸çœŸå®ç¯å¢ƒçš„å·®å¼‚ã€‚</li>
<li>é’ˆå¯¹æ“¦æ‹­ç­‰æ¥è§¦ä¸°å¯Œçš„ä»»åŠ¡ï¼Œéœ€è¦é€‚åº”åŠ›æ§åˆ¶æ¥å¤„ç†è¡¨é¢é«˜åº¦å’Œæµ·ç»µç‰©ç†å±æ€§çš„å˜åŒ–ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆå®æ—¶åŠ›æ‰­çŸ©åé¦ˆä¸é¢„è®­ç»ƒç‰©ä½“è¡¨å¾çš„æ–°æ–¹æ³•ï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>æ–°æ–¹æ³•åœ¨çœŸå®åœºæ™¯ä¸­å®ç°äº†é«˜å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºä»¥å¾€æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†æ–¹æ³•åœ¨ä¸åŒæµ·ç»µç‰©ç†å±æ€§å’Œè¡¨é¢é«˜åº¦ä¸‹çš„é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06451">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31eded18068d3c258398f514fbd0e04b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f44f5a8a317a4fedc99bf9801781e932.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a90226cf3acbc48c031fa9eba912be9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88acafcfba20fb1cd7304917a4f79554.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e55c43994b27ae255293a951cc09c996.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d75ed33fee3031668a9637d9976206d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f76bd347254d3a27aed805749d3b5ae0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31c6c77a54adbacae71955d34bef1459.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TREND-Tri-teaching-for-Robust-Preference-based-Reinforcement-Learning-with-Demonstrations"><a href="#TREND-Tri-teaching-for-Robust-Preference-based-Reinforcement-Learning-with-Demonstrations" class="headerlink" title="TREND: Tri-teaching for Robust Preference-based Reinforcement Learning   with Demonstrations"></a>TREND: Tri-teaching for Robust Preference-based Reinforcement Learning   with Demonstrations</h2><p><strong>Authors:Shuaiyi Huang, Mara Levy, Anubhav Gupta, Daniel Ekpo, Ruijie Zheng, Abhinav Shrivastava</strong></p>
<p>Preference feedback collected by human or VLM annotators is often noisy, presenting a significant challenge for preference-based reinforcement learning that relies on accurate preference labels. To address this challenge, we propose TREND, a novel framework that integrates few-shot expert demonstrations with a tri-teaching strategy for effective noise mitigation. Our method trains three reward models simultaneously, where each model views its small-loss preference pairs as useful knowledge and teaches such useful pairs to its peer network for updating the parameters. Remarkably, our approach requires as few as one to three expert demonstrations to achieve high performance. We evaluate TREND on various robotic manipulation tasks, achieving up to 90% success rates even with noise levels as high as 40%, highlighting its effective robustness in handling noisy preference feedback. Project page: <a target="_blank" rel="noopener" href="https://shuaiyihuang.github.io/publications/TREND">https://shuaiyihuang.github.io/publications/TREND</a>. </p>
<blockquote>
<p>é€šè¿‡äººç±»æˆ–VLMæ³¨é‡Šå™¨æ”¶é›†çš„åå¥½åé¦ˆå¾€å¾€å­˜åœ¨å™ªå£°ï¼Œè¿™ä¸ºåŸºäºå‡†ç¡®åå¥½æ ‡ç­¾çš„åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TRENDï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å°‘é‡ä¸“å®¶æ¼”ç¤ºä¸ä¸‰æ•™å­¦ä¹ ç­–ç•¥ç›¸ç»“åˆçš„æ–°å‹æ¡†æ¶ï¼Œä»¥å®ç°æœ‰æ•ˆçš„å™ªå£°ç¼“è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒæ—¶è®­ç»ƒä¸‰ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹å°†å…¶ä½æŸå¤±åå¥½å¯¹è§†ä¸ºæœ‰ç”¨çŸ¥è¯†ï¼Œå¹¶å°†å…¶æ•™ç»™åŒè¡Œç½‘ç»œä»¥æ›´æ–°å‚æ•°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åªéœ€è¦ä¸€åˆ°ä¸‰ä¸ªä¸“å®¶æ¼”ç¤ºå³å¯å®ç°é«˜æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šè¯„ä¼°TRENDï¼Œå³ä½¿åœ¨é«˜è¾¾40%çš„å™ªå£°æ°´å¹³ä¸‹ï¼Œä¹Ÿèƒ½è¾¾åˆ°é«˜è¾¾90%çš„æˆåŠŸç‡ï¼Œä»è€Œçªå‡ºäº†å…¶åœ¨å¤„ç†å™ªå£°åå¥½åé¦ˆæ–¹é¢çš„æœ‰æ•ˆç¨³å¥æ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://shuaiyihuang.github.io/publications/TREND">https://shuaiyihuang.github.io/publications/TREND</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06079v1">PDF</a> ICRA 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºTRENDçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå°‘é‡ä¸“å®¶æ¼”ç¤ºå’Œä¸‰é‡æ•™å­¦ç­–ç•¥æ¥è§£å†³åŸºäºå™ªå£°åå¥½çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒæ—¶è®­ç»ƒä¸‰ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹å°†å…¶ä½æŸå¤±åå¥½å¯¹è§†ä¸ºæœ‰ç”¨çŸ¥è¯†å¹¶ä¼ æˆç»™åŒè¡Œç½‘ç»œè¿›è¡Œå‚æ•°æ›´æ–°ã€‚è¯¥æ–¹æ³•åªéœ€è¦ä¸€åˆ°ä¸‰æ¬¡ä¸“å®¶æ¼”ç¤ºå³å¯å®ç°é«˜æ€§èƒ½ï¼Œåœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šå–å¾—é«˜è¾¾90%çš„æˆåŠŸç‡ï¼Œå³ä½¿åœ¨é«˜è¾¾40%çš„å™ªå£°æ°´å¹³ä¸‹ä¹Ÿèƒ½æœ‰æ•ˆåº”å¯¹å™ªå£°åå¥½åé¦ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TRENDæ¡†æ¶è§£å†³äº†åŸºäºå™ªå£°åå¥½çš„å¼ºåŒ–å­¦ä¹ æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡æ•´åˆå°‘é‡ä¸“å®¶æ¼”ç¤ºå’Œä¸‰é‡æ•™å­¦ç­–ç•¥å®ç°æœ‰æ•ˆå™ªå£°ç¼“è§£ã€‚</li>
<li>åŒæ—¶è®­ç»ƒä¸‰ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œåˆ©ç”¨ä½æŸå¤±åå¥½å¯¹æ›´æ–°å‚æ•°ã€‚</li>
<li>ä»…éœ€ä¸€è‡³ä¸‰æ¬¡ä¸“å®¶æ¼”ç¤ºå³å¯å®ç°é«˜æ€§èƒ½ã€‚</li>
<li>åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šå–å¾—äº†é«˜è¾¾90%çš„æˆåŠŸç‡ã€‚</li>
<li>åœ¨é«˜å™ªå£°æ°´å¹³ä¸‹ä»èƒ½ä¿æŒç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9382a21b0916821344edeb949dc6a085.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0dbca607ebb284a3ba96abca04b82c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aec0b5ae1fa5f11c00e2ff7d91ad0304.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d746e5a9c0a1cf27205e7419dcb44ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3850718e47e065ea7ac41ac10f6f4b02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c493230ce2a50533520988c5750ff0c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Task-Adapter-Task-specific-Adaptation-with-Order-aware-Alignment-for-Few-shot-Action-Recognition"><a href="#Task-Adapter-Task-specific-Adaptation-with-Order-aware-Alignment-for-Few-shot-Action-Recognition" class="headerlink" title="Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for   Few-shot Action Recognition"></a>Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for   Few-shot Action Recognition</h2><p><strong>Authors:Congqi Cao, Peiheng Han, Yueran zhang, Yating Yu, Qinyi Lv, Lingtong Min, Yanning zhang</strong></p>
<p>Large-scale pre-trained models have achieved remarkable success in language and image tasks, leading an increasing number of studies to explore the application of pre-trained image models, such as CLIP, in the domain of few-shot action recognition (FSAR). However, current methods generally suffer from several problems: 1) Direct fine-tuning often undermines the generalization capability of the pre-trained model; 2) The exploration of task-specific information is insufficient in the visual tasks; 3) The semantic order information is typically overlooked during text modeling; 4) Existing cross-modal alignment techniques ignore the temporal coupling of multimodal information. To address these, we propose Task-Adapter++, a parameter-efficient dual adaptation method for both image and text encoders. Specifically, to make full use of the variations across different few-shot learning tasks, we design a task-specific adaptation for the image encoder so that the most discriminative information can be well noticed during feature extraction. Furthermore, we leverage large language models (LLMs) to generate detailed sequential sub-action descriptions for each action class, and introduce semantic order adapters into the text encoder to effectively model the sequential relationships between these sub-actions. Finally, we develop an innovative fine-grained cross-modal alignment strategy that actively maps visual features to reside in the same temporal stage as semantic descriptions. Extensive experiments fully demonstrate the effectiveness and superiority of the proposed method, which achieves state-of-the-art performance on 5 benchmarks consistently. The code is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/Jaulin-Bage/Task-Adapter-pp">https://github.com/Jaulin-Bage/Task-Adapter-pp</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹åœ¨è¯­è¨€å’Œå›¾åƒä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå¼•å‘äº†è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶æ¢ç´¢é¢„è®­ç»ƒå›¾åƒæ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰é¢†åŸŸçš„åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•æ™®éå­˜åœ¨ä¸€äº›é—®é¢˜ï¼š1ï¼‰ç›´æ¥å¾®è°ƒå¾€å¾€ä¼šå‰Šå¼±é¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›2ï¼‰è§†è§‰ä»»åŠ¡ä¸­ç‰¹å®šä»»åŠ¡ä¿¡æ¯çš„æ¢ç´¢ä¸è¶³ï¼›3ï¼‰æ–‡æœ¬å»ºæ¨¡æ—¶å¿½ç•¥äº†è¯­ä¹‰é¡ºåºä¿¡æ¯ï¼›4ï¼‰ç°æœ‰çš„è·¨æ¨¡æ€å¯¹é½æŠ€æœ¯å¿½ç•¥äº†å¤šæ¨¡æ€ä¿¡æ¯çš„æ—¶åºè€¦åˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Task-Adapter++ï¼Œè¿™æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨çš„åŒé‡é€‚åº”æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å……åˆ†åˆ©ç”¨ä¸åŒå°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¹‹é—´çš„å·®å¼‚ï¼Œæˆ‘ä»¬ä¸ºå›¾åƒç¼–ç å™¨è®¾è®¡äº†ç‰¹å®šä»»åŠ¡çš„é€‚åº”æ–¹æ³•ï¼Œä»¥ä¾¿åœ¨ç‰¹å¾æå–è¿‡ç¨‹ä¸­èƒ½å¤Ÿå……åˆ†æ³¨æ„åˆ°æœ€å…·åŒºåˆ†æ€§çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ¯ä¸ªåŠ¨ä½œç±»åˆ«ç”Ÿæˆè¯¦ç»†çš„åºåˆ—å­åŠ¨ä½œæè¿°ï¼Œå¹¶åœ¨æ–‡æœ¬ç¼–ç å™¨ä¸­å¼•å…¥è¯­ä¹‰é¡ºåºé€‚é…å™¨ï¼Œä»¥æœ‰æ•ˆåœ°å»ºæ¨¡è¿™äº›å­åŠ¨ä½œä¹‹é—´çš„åºåˆ—å…³ç³»ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åˆ›æ–°çš„ç²¾ç»†è·¨æ¨¡æ€å¯¹é½ç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½ç§¯æåœ°å°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°ä¸è¯­ä¹‰æè¿°ç›¸åŒçš„æ—¶åºé˜¶æ®µã€‚å¤§é‡çš„å®éªŒå……åˆ†è¯æ˜äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ï¼Œè¯¥æ–¹æ³•åœ¨5ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå‡è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ä»£ç å·²å¼€æºåœ¨<a target="_blank" rel="noopener" href="https://github.com/Jaulin-Bage/Task-Adapter-pp">https://github.com/Jaulin-Bage/Task-Adapter-pp</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06002v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2408.00249</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹åœ¨è¯­è¨€å’Œå›¾åƒä»»åŠ¡ä¸­çš„å‡ºè‰²è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰é¢†åŸŸçš„åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚ç›´æ¥å¾®è°ƒå½±å“æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€è§†è§‰ä»»åŠ¡ä¸­ä»»åŠ¡ç‰¹å®šä¿¡æ¯æ¢ç´¢ä¸è¶³ã€æ–‡æœ¬å»ºæ¨¡ä¸­è¯­ä¹‰é¡ºåºä¿¡æ¯è¢«å¿½è§†ä»¥åŠç°æœ‰è·¨æ¨¡æ€å¯¹é½æŠ€æœ¯å¿½ç•¥å¤šæ¨¡æ€ä¿¡æ¯çš„æ—¶åºè€¦åˆç­‰ï¼Œæœ¬æ–‡æå‡ºäº†Task-Adapter++ï¼Œä¸€ç§å‚æ•°é«˜æ•ˆçš„å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨çš„åŒé‡é€‚åº”æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è®¾è®¡ä»»åŠ¡ç‰¹å®šé€‚åº”çš„å›¾åƒç¼–ç å™¨ã€åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè¯¦ç»†é¡ºåºå­åŠ¨ä½œæè¿°ä»¥åŠå¼•å…¥è¯­ä¹‰é¡ºåºé€‚é…å™¨åˆ°æ–‡æœ¬ç¼–ç å™¨å’Œç²¾ç»†è·¨æ¨¡æ€å¯¹é½ç­–ç•¥ï¼Œå®ç°äº†åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å“è¶Šè¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹åœ¨è¯­è¨€å’Œå›¾åƒä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰é¢†åŸŸã€‚</li>
<li>å½“å‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜åŒ…æ‹¬ç›´æ¥å¾®è°ƒå½±å“æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€è§†è§‰ä»»åŠ¡ä¸­ä»»åŠ¡ç‰¹å®šä¿¡æ¯æ¢ç´¢ä¸è¶³ç­‰ã€‚</li>
<li>Task-Adapter++æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨çš„åŒé‡é€‚åº”æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡è®¾è®¡ä»»åŠ¡ç‰¹å®šé€‚åº”çš„å›¾åƒç¼–ç å™¨ä»¥åˆ©ç”¨ä¸åŒå°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­çš„å˜åŒ–ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè¯¦ç»†é¡ºåºå­åŠ¨ä½œæè¿°ï¼Œå¹¶å¼•å…¥è¯­ä¹‰é¡ºåºé€‚é…å™¨åˆ°æ–‡æœ¬ç¼–ç å™¨ã€‚</li>
<li>å¼€å‘å‡ºä¸€ç§åˆ›æ–°çš„ç²¾ç»†è·¨æ¨¡æ€å¯¹é½ç­–ç•¥ï¼Œç§¯æå°†è§†è§‰ç‰¹å¾ä¸è¯­ä¹‰æè¿°æ˜ å°„åˆ°åŒä¸€æ—¶é—´é˜¶æ®µã€‚</li>
<li>Task-Adapter++åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å“è¶Šè¡¨ç°ï¼Œå¹¶å…¬å¼€äº†æºä»£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-de3a9d8f0e1dab06c7baecff972f4d18.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d494af3d66420ac9121c01ddef57de1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f375b02140efa50b1058cf2688b59755.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2ae2dee0230e30222a035d566a50b79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93e07735f9c55613254800d5f4614e16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f75efd5adc6d13fcda819c537c78b97d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcd27552fb4adb3ad2a1808bd2e689a3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-â€“-An-evaluation-on-urological-doctorsâ€™-notes"><a href="#Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-â€“-An-evaluation-on-urological-doctorsâ€™-notes" class="headerlink" title="Can open source large language models be used for tumor documentation in   Germany? â€“ An evaluation on urological doctorsâ€™ notes"></a>Can open source large language models be used for tumor documentation in   Germany? â€“ An evaluation on urological doctorsâ€™ notes</h2><p><strong>Authors:Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Meike Ressing, Torsten Panholzer</strong></p>
<p>Tumor documentation in Germany is largely done manually, requiring reading patient records and entering data into structured databases. Large language models (LLMs) could potentially enhance this process by improving efficiency and reliability. This evaluation tests eleven different open source LLMs with sizes ranging from 1-70 billion model parameters on three basic tasks of the tumor documentation process: identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis. For evaluating the LLMs on these tasks, a dataset of annotated text snippets based on anonymized doctorsâ€™ notes from urology was prepared. Different prompting strategies were used to investigate the effect of the number of examples in few-shot prompting and to explore the capabilities of the LLMs in general. The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks. Models with less extensive training data or having fewer than 7 billion parameters showed notably lower performance, while larger models did not display performance gains. Examples from a different medical domain than urology could also improve the outcome in few-shot prompting, which demonstrates the ability of LLMs to handle tasks needed for tumor documentation. Open source LLMs show a strong potential for automating tumor documentation. Models from 7-12 billion parameters could offer an optimal balance between performance and resource efficiency. With tailored fine-tuning and well-designed prompting, these models might become important tools for clinical documentation in the future. The code for the evaluation is available from <a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval">https://github.com/stefan-m-lenz/UroLlmEval</a>. We also release the dataset as a new valuable resource that addresses the shortage of authentic and easily accessible benchmarks in German-language medical NLP. </p>
<blockquote>
<p>åœ¨å¾·å›½ï¼Œè‚¿ç˜¤è®°å½•å·¥ä½œå¤§å¤šä»¥æ‰‹åŠ¨æ–¹å¼è¿›è¡Œï¼Œéœ€è¦é˜…è¯»æ‚£è€…ç—…å†å¹¶å°†æ•°æ®å½•å…¥ç»“æ„åŒ–æ•°æ®åº“ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ‰æ½œåŠ›é€šè¿‡æé«˜æ•ˆç‡å’Œå¯é æ€§æ¥æ”¹è¿›è¿™ä¸€è¿‡ç¨‹ã€‚æœ¬æ¬¡è¯„ä¼°å¯¹ä¸‰ç§è‚¿ç˜¤è®°å½•åŸºæœ¬ä»»åŠ¡ï¼ˆè¯†åˆ«è‚¿ç˜¤è¯Šæ–­ã€åˆ†é…ICD-10ä»£ç å’Œæå–é¦–æ¬¡è¯Šæ–­æ—¥æœŸï¼‰ä¸Šï¼Œæµ‹è¯•äº†è§„æ¨¡ä»1äº¿è‡³70äº¿æ¨¡å‹å‚æ•°çš„11ä¸ªä¸åŒå¼€æºLLMã€‚ä¸ºäº†è¯„ä¼°è¿™äº›ä»»åŠ¡ä¸Šçš„LLMæ€§èƒ½ï¼Œå‡†å¤‡äº†ä¸€ä¸ªåŸºäºæ³Œå°¿å¤–ç§‘åŒ¿ååŒ»ç”Ÿç¬”è®°çš„æ ‡æ³¨æ–‡æœ¬ç‰‡æ®µæ•°æ®é›†ã€‚ä½¿ç”¨äº†ä¸åŒçš„æç¤ºç­–ç•¥æ¥ç ”ç©¶å°‘é‡ç¤ºä¾‹æç¤ºä¸­ç¤ºä¾‹æ•°é‡å¯¹LLMæ€§èƒ½çš„å½±å“ï¼Œå¹¶æ¢ç´¢LLMçš„ä¸€èˆ¬èƒ½åŠ›ã€‚æ¨¡å‹Llama 3.1 8Bã€Mistral 7Bå’ŒMistral NeMo 12Båœ¨ä»»åŠ¡ä¸­è¡¨ç°ç›¸å½“å‡ºè‰²ã€‚æ‹¥æœ‰è¾ƒå°‘è®­ç»ƒæ•°æ®æˆ–å‚æ•°å°‘äº7äº¿çš„æ¨¡å‹è¡¨ç°æ˜æ˜¾è¾ƒå·®ï¼Œè€Œæ›´å¤§çš„æ¨¡å‹å¹¶æ²¡æœ‰æ˜¾ç¤ºå‡ºæ€§èƒ½æå‡ã€‚æ¥è‡ªæ³Œå°¿å­¦ä»¥å¤–çš„åŒ»å­¦é¢†åŸŸçš„ä¾‹å­ä¹Ÿå¯ä»¥åœ¨å°‘é‡æç¤ºä¸­æ”¹å–„ç»“æœï¼Œè¿™è¯æ˜äº†LLMå¤„ç†è‚¿ç˜¤è®°å½•æ‰€éœ€ä»»åŠ¡çš„èƒ½åŠ›ã€‚å¼€æºLLMåœ¨è‡ªåŠ¨è¿›è¡Œè‚¿ç˜¤è®°å½•æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚å‚æ•°åœ¨7äº¿è‡³12äº¿ä¹‹é—´çš„æ¨¡å‹å¯èƒ½åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒä»¥åŠç²¾å¿ƒè®¾è®¡æç¤ºï¼Œè¿™äº›æ¨¡å‹å¯èƒ½ä¼šæˆä¸ºæœªæ¥ä¸´åºŠè®°å½•çš„é‡è¦å·¥å…·ã€‚è¯„ä¼°çš„ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval%E8%8E%B7%E5%8F%96%E3%80%82%E6%88%91%E4%BB%AC%E8%BF%98%E5%85%AC%E5%BC%80%E4%BA%86%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E4%BD%9C%E4%B8%BA%E8%A7%A3%E5%86%B3%E5%BE%B7%E5%9B%BD%E5%8C%BB%E5%AD%A6NLP%E4%B8%AD%E7%9C%9F%E5%AE%9E%E3%80%81%E6%98%93%E4%BA%8E%E8%AE%BF%E9%97%AE%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%B5%84%E6%BA%90%E7%9F%AD%E7%BC%BA%E9%97%AE%E9%A2%98%E7%9A%84%E4%B8%80%E4%B8%AA%E6%96%B0%E6%9C%89%E4%BB%B7%E5%80%BC%E8%B5%84%E6%BA%90%E3%80%82">https://github.com/stefan-m-lenz/UroLlmEvalè·å–ã€‚æˆ‘ä»¬è¿˜å…¬å¼€äº†æ•°æ®é›†ï¼Œä½œä¸ºè§£å†³å¾·å›½åŒ»å­¦NLPä¸­çœŸå®ã€æ˜“äºè®¿é—®çš„åŸºå‡†æµ‹è¯•èµ„æºçŸ­ç¼ºé—®é¢˜çš„ä¸€ä¸ªæ–°æœ‰ä»·å€¼èµ„æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12106v3">PDF</a> 53 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‚¿ç˜¤æ–‡æ¡£å¤„ç†ä¸­å±•ç°å‡ºæ½œåœ¨çš„åº”ç”¨ä»·å€¼ï¼Œèƒ½æé«˜æ•ˆç‡å’Œå¯é æ€§ã€‚é€šè¿‡å¯¹11ç§ä¸åŒè§„æ¨¡ï¼ˆä»1äº¿åˆ°70äº¿å‚æ•°ï¼‰çš„å¼€æºLLMsè¿›è¡Œè¯„ä»·ï¼Œå‘ç°å®ƒä»¬åœ¨è‚¿ç˜¤è¯Šæ–­ç¡®è®¤ã€ICD-10ä»£ç åˆ†é…å’Œé¦–æ¬¡è¯Šæ–­æ—¥æœŸæå–ç­‰ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚æ¨¡å‹æ€§èƒ½éšå‚æ•°å¢åŠ è€Œæé«˜ï¼Œä½†ä¸æ˜¯çº¿æ€§å…³ç³»ã€‚ç‰¹å®šåŒ»ç–—é¢†åŸŸçš„æ ·æœ¬åœ¨few-shot promptingä¸‹èƒ½å¤Ÿæé«˜è¡¨ç°ï¼Œè¯´æ˜LLMsæœ‰èƒ½åŠ›å¤„ç†è‚¿ç˜¤æ–‡æ¡£ç›¸å…³çš„ä»»åŠ¡ã€‚æ¨èæ¨¡å‹å‚æ•°åœ¨7-12äº¿èŒƒå›´å†…ä»¥è¾¾åˆ°æ€§èƒ½å’Œèµ„æºæ•ˆç‡çš„å¹³è¡¡ã€‚é€‚å½“å¾®è°ƒå¹¶è®¾è®¡æç¤ºå¯ä½¿å…¶åœ¨æœªæ¥æˆä¸ºä¸´åºŠæ–‡æ¡£çš„é‡è¦å·¥å…·ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å·²å‘å¸ƒä¾›ç ”ç©¶ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯åº”ç”¨äºè‚¿ç˜¤æ–‡æ¡£å¤„ç†ï¼Œæé«˜æ•ˆç‡å’Œå¯é æ€§ã€‚</li>
<li>é€šè¿‡è¯„ä»·11ç§ä¸åŒè§„æ¨¡å’Œç±»å‹çš„LLMsåœ¨ä¸‰ä¸ªåŸºæœ¬ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‘ç°è‰¯å¥½æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½ä¸å‚æ•°è§„æ¨¡æœ‰å…³ï¼Œä½†ä¸æ˜¯çº¿æ€§å…³ç³»ï¼Œå‚æ•°è§„æ¨¡è¿‡å¤§ä¸ä¸€å®šå¸¦æ¥æ€§èƒ½æå‡ã€‚</li>
<li>Few-shot promptingæ•ˆæœå—æ ·æœ¬é¢†åŸŸå½±å“ï¼Œè¡¨æ˜LLMså…·å¤‡å¤„ç†ä¸åŒåŒ»ç–—é¢†åŸŸä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>æ¨èæ¨¡å‹å‚æ•°èŒƒå›´åœ¨7-12äº¿ï¼Œä»¥å®ç°æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>é€‚å½“å¾®è°ƒå’Œä¼˜åŒ–æç¤ºè®¾è®¡å¯ä»¥ä½¿è¿™äº›æ¨¡å‹æˆä¸ºæœªæ¥ä¸´åºŠæ–‡æ¡£å¤„ç†çš„é‡è¦å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f5b2171a7c546253d3782efe165a317.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec1b13367880e573c14047c7e811034b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MedualTime-A-Dual-Adapter-Language-Model-for-Medical-Time-Series-Text-Multimodal-Learning"><a href="#MedualTime-A-Dual-Adapter-Language-Model-for-Medical-Time-Series-Text-Multimodal-Learning" class="headerlink" title="MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text   Multimodal Learning"></a>MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text   Multimodal Learning</h2><p><strong>Authors:Jiexia Ye, Weiqi Zhang, Ziyue Li, Jia Li, Meng Zhao, Fugee Tsung</strong></p>
<p>The recent rapid advancements in language models (LMs) have garnered attention in medical time series-text multimodal learning. However, existing contrastive learning-based and prompt-based LM approaches tend to be biased, often assigning a primary role to time series modality while treating text modality as secondary. We classify these approaches under a temporal-primary paradigm, which may overlook the unique and critical task-relevant information embedded in text modality like clinical reports, thus failing to fully leverage mutual benefits and complementarity of different modalities. To fill this gap, we propose a novel textual-temporal multimodal learning paradigm that enables either modality to serve as the primary while being enhanced by the other, thereby effectively capturing modality-specific information and fostering cross-modal interaction. In specific, we design MedualTime, a language model composed of dual adapters to implement temporal-primary and textual-primary modeling simultaneously. Within each adapter, lightweight adaptation tokens are injected into the top layers of LM to encourage high-level modality fusion. The shared LM pipeline by dual adapters not only achieves adapter alignment but also enables efficient fine-tuning, reducing computational resources. Empirically, MedualTime demonstrates superior performance on medical data, achieving notable improvements of 8% accuracy and 12% F1 in supervised settings. Furthermore, MedualTimeâ€™s transferability is validated by few-shot label transfer experiments from coarse-grained to fine-grained medical data. <a target="_blank" rel="noopener" href="https://github.com/start2020/MedualTime">https://github.com/start2020/MedualTime</a> </p>
<blockquote>
<p>è¿‘æœŸçš„è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰è¿…é€Ÿè¿›å±•å¼•èµ·äº†åŒ»å­¦æ—¶é—´åºåˆ—æ–‡æœ¬å¤šæ¨¡æ€å­¦ä¹ çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºå¯¹æ¯”å­¦ä¹ å’ŒåŸºäºæç¤ºçš„è¯­è¨€æ¨¡å‹æ–¹æ³•å¾€å¾€å­˜åœ¨åè§ï¼Œé€šå¸¸å°†æ—¶é—´åºåˆ—æ¨¡æ€è§†ä¸ºä¸»è¦æ¨¡æ€ï¼Œè€Œå°†æ–‡æœ¬æ¨¡æ€è§†ä¸ºæ¬¡è¦æ¨¡æ€ã€‚æˆ‘ä»¬å°†è¿™äº›æ–¹æ³•å½’ç±»ä¸ºæ—¶é—´ä¸»å¯¼èŒƒå¼ï¼Œè¿™å¯èƒ½ä¼šå¿½è§†æ–‡æœ¬æ¨¡æ€ä¸­åµŒå…¥çš„ç‹¬ç‰¹ä¸”å…³é”®çš„ä»»åŠ¡ç›¸å…³ä¿¡æ¯ï¼Œå¦‚ä¸´åºŠæŠ¥å‘Šï¼Œä»è€Œæœªèƒ½å……åˆ†åˆ©ç”¨ä¸åŒæ¨¡æ€çš„ç›¸äº’ä¼˜åŠ¿å’Œäº’è¡¥æ€§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬æ—¶é—´å¤šæ¨¡æ€å­¦ä¹ èŒƒå¼ï¼Œä½¿ä»»ä¸€æ¨¡æ€éƒ½å¯ä»¥ä½œä¸ºä¸»è¦æ¨¡æ€ï¼ŒåŒæ—¶å—åˆ°å¦ä¸€æ¨¡æ€çš„å¢å¼ºï¼Œä»è€Œæœ‰æ•ˆåœ°æ•è·ç‰¹å®šäºæ¨¡æ€çš„ä¿¡æ¯å¹¶ä¿ƒè¿›è·¨æ¨¡æ€äº¤äº’ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†MedualTimeï¼Œä¸€ä¸ªç”±åŒé€‚é…å™¨ç»„æˆçš„è¯­è¨€æ¨¡å‹ï¼Œå¯åŒæ—¶å®ç°æ—¶é—´ä¸»å¯¼å’Œæ–‡æœ¬ä¸»å¯¼å»ºæ¨¡ã€‚åœ¨æ¯ä¸ªé€‚é…å™¨ä¸­ï¼Œå°†è½»é‡çº§é€‚é…ä»¤ç‰Œæ³¨å…¥è¯­è¨€æ¨¡å‹é¡¶å±‚ï¼Œä»¥é¼“åŠ±é«˜çº§æ¨¡æ€èåˆã€‚åŒé€‚é…å™¨å…±äº«çš„LMç®¡é“ä¸ä»…å®ç°äº†é€‚é…å™¨å¯¹é½ï¼Œè¿˜å®ç°äº†é«˜æ•ˆçš„å¾®è°ƒï¼Œå‡å°‘äº†è®¡ç®—èµ„æºã€‚å®é™…ä¸Šï¼ŒMedualTimeåœ¨åŒ»ç–—æ•°æ®ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨ç›‘ç£è®¾ç½®ä¸­çš„å‡†ç¡®ç‡å’ŒF1å¾—åˆ†åˆ†åˆ«æé«˜äº†8%å’Œ12%ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä»ç²—ç²’åº¦åˆ°ç»†ç²’åº¦çš„åŒ»å­¦æ•°æ®çš„å°‘é‡æ ‡ç­¾è½¬ç§»å®éªŒï¼ŒéªŒè¯äº†MedualTimeçš„å¯è¿ç§»æ€§ã€‚<a target="_blank" rel="noopener" href="https://github.com/start2020/MedualTime">https://github.com/start2020/MedualTime</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06620v3">PDF</a> 9 pages, 6 figure, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦æ—¶é—´åºåˆ—æ–‡æœ¬å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æœ€æ–°è¿›å±•ã€‚é’ˆå¯¹ç°æœ‰å¯¹æ¯”å­¦ä¹ å’Œæç¤ºè¯­è¨€æ¨¡å‹ä¸­çš„æ—¶é—´åºåˆ—ä¸ºä¸»è¦æ¨¡æ€ã€æ–‡æœ¬ä¸ºæ¬¡è¦æ¨¡æ€çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬æ—¶åºå¤šæ¨¡æ€å­¦ä¹ èŒƒå¼ã€‚é€šè¿‡è®¾è®¡MedualTimeæ¨¡å‹ï¼Œå®ç°äº†æ—¶åºä¸ºä¸»å’Œæ–‡æœ¬ä¸ºä¸»çš„å»ºæ¨¡åŒæ—¶è¿›è¡Œã€‚è¯¥æ¨¡å‹é€šè¿‡è½»é‡çº§é€‚é…å™¨æ³¨å…¥é«˜å±‚æ¨¡æ€èåˆä¿¡æ¯ï¼Œè¾¾åˆ°ä¼˜å¼‚æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨åŒ»å­¦æ•°æ®ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨ç²—ç²’åº¦åˆ°ç»†ç²’åº¦çš„åŒ»å­¦æ•°æ®è¿ç§»å®éªŒä¸­éªŒè¯äº†å…¶è¿ç§»èƒ½åŠ›ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®GitHubä»“åº“ä»¥è·å–æ›´å¤šèµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦æ—¶é—´åºåˆ—æ–‡æœ¬å¤šæ¨¡æ€å­¦ä¹ ä¸­å–å¾—è¿›å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å€¾å‘äºå°†æ—¶é—´åºåˆ—ä½œä¸ºä¸»æ¨¡æ€ï¼Œè€Œæ–‡æœ¬ä¸ºæ¬¡è¦æ¨¡æ€ï¼Œè¿™å¯èƒ½å¯¼è‡´ä»»åŠ¡ç›¸å…³çš„å…³é”®ä¿¡æ¯è¢«å¿½è§†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬æ—¶åºå¤šæ¨¡æ€å­¦ä¹ èŒƒå¼ï¼Œä½¿ä»»ä¸€æ¨¡æ€éƒ½å¯ä»¥ä½œä¸ºä¸»è¦æ¨¡æ€ï¼ŒåŒæ—¶å¾—åˆ°å¦ä¸€æ¨¡æ€çš„å¢å¼ºã€‚</li>
<li>MedualTimeæ¨¡å‹å®ç°äº†æ—¶åºä¸ºä¸»å’Œæ–‡æœ¬ä¸ºä¸»çš„å»ºæ¨¡åŒæ—¶è¿›è¡Œï¼Œé€šè¿‡åŒé€‚é…å™¨å®ç°æ¨¡æ€å¯¹é½å’Œé«˜æ•ˆå¾®è°ƒã€‚</li>
<li>MedualTimeæ¨¡å‹åœ¨åŒ»å­¦æ•°æ®ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç›‘ç£è®¾ç½®ä¸‹å‡†ç¡®ç‡å’ŒF1å¾—åˆ†æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06620">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dae07751c109f7fabb3f3cffde80a21b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-821b65a18689fdc2dd711ea5ac4fe4e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d562861c4885eca6756845947e00e4f0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a7232ea14da8cf4d1a5ec0344649ce78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae088beb08fce003d6ed74cd92879424.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38c54ddce1ea173e05d588e42e56ba99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02f2cec5d72fcbdf4678e49df78f1298.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="How-to-build-the-best-medical-image-segmentation-algorithm-using-foundation-models-a-comprehensive-empirical-study-with-Segment-Anything-Model"><a href="#How-to-build-the-best-medical-image-segmentation-algorithm-using-foundation-models-a-comprehensive-empirical-study-with-Segment-Anything-Model" class="headerlink" title="How to build the best medical image segmentation algorithm using   foundation models: a comprehensive empirical study with Segment Anything   Model"></a>How to build the best medical image segmentation algorithm using   foundation models: a comprehensive empirical study with Segment Anything   Model</h2><p><strong>Authors:Hanxue Gu, Haoyu Dong, Jichen Yang, Maciej A. Mazurowski</strong></p>
<p>Automated segmentation is a fundamental medical image analysis task, which enjoys significant advances due to the advent of deep learning. While foundation models have been useful in natural language processing and some vision tasks for some time, the foundation model developed with image segmentation in mind - Segment Anything Model (SAM) - has been developed only recently and has shown similar promise. However, there are still no systematic analyses or â€œbest-practiceâ€ guidelines for optimal fine-tuning of SAM for medical image segmentation. This work summarizes existing fine-tuning strategies with various backbone architectures, model components, and fine-tuning algorithms across 18 combinations, and evaluates them on 17 datasets covering all common radiology modalities. Our study reveals that (1) fine-tuning SAM leads to slightly better performance than previous segmentation methods, (2) fine-tuning strategies that use parameter-efficient learning in both the encoder and decoder are superior to other strategies, (3) network architecture has a small impact on final performance, (4) further training SAM with self-supervised learning can improve final model performance. We also demonstrate the ineffectiveness of some methods popular in the literature and further expand our experiments into few-shot and prompt-based settings. Lastly, we released our code and MRI-specific fine-tuned weights, which consistently obtained superior performance over the original SAM, at <a target="_blank" rel="noopener" href="https://github.com/mazurowski-lab/finetune-SAM">https://github.com/mazurowski-lab/finetune-SAM</a>. </p>
<blockquote>
<p>è‡ªåŠ¨åˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œå¾—ç›Šäºæ·±åº¦å­¦ä¹ çš„è¿›æ­¥ï¼Œè¯¥ä»»åŠ¡å–å¾—äº†é‡å¤§è¿›å±•ã€‚è™½ç„¶åŸºç¡€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’ŒæŸäº›è§†è§‰ä»»åŠ¡ä¸­å·²æœ‰ä¸€æ®µæ—¶é—´çš„åº”ç”¨ï¼Œä½†ä»¥å›¾åƒåˆ†å‰²ä¸ºä¸­å¿ƒå¼€å‘çš„åŸºç¡€æ¨¡å‹â€”â€”ä»»æ„åˆ†å‰²æ¨¡å‹ï¼ˆSAMï¼‰â€”â€”æ˜¯æœ€è¿‘æ‰å¼€å‘çš„ï¼Œå¹¶æ˜¾ç¤ºå‡ºåŒæ ·çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºSAMåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æœ€ä½³å¾®è°ƒï¼Œä»ç„¶æ²¡æœ‰ç³»ç»Ÿçš„åˆ†ææˆ–â€œæœ€ä½³å®è·µâ€æŒ‡å—ã€‚è¿™é¡¹å·¥ä½œæ€»ç»“äº†ä½¿ç”¨å„ç§ä¸»å¹²æ¶æ„ã€æ¨¡å‹ç»„ä»¶å’Œå¾®è°ƒç®—æ³•çš„ç°æœ‰å¾®è°ƒç­–ç•¥ï¼Œè·¨è¶Šäº†18ç§ç»„åˆï¼Œå¹¶åœ¨æ¶µç›–æ‰€æœ‰å¸¸è§æ”¾å°„å­¦æ¨¡æ€çš„17ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼šï¼ˆ1ï¼‰å¾®è°ƒSAMç•¥å¾®ä¼˜äºä»¥å‰çš„åˆ†å‰²æ–¹æ³•ï¼›ï¼ˆ2ï¼‰åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¸­éƒ½ä½¿ç”¨å‚æ•°æœ‰æ•ˆå­¦ä¹ çš„å¾®è°ƒç­–ç•¥ä¼˜äºå…¶ä»–ç­–ç•¥ï¼›ï¼ˆ3ï¼‰ç½‘ç»œæ¶æ„å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“è¾ƒå°ï¼›ï¼ˆ4ï¼‰ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ è¿›ä¸€æ­¥è®­ç»ƒSAMå¯ä»¥æé«˜æœ€ç»ˆæ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†æ–‡çŒ®ä¸­ä¸€äº›æµè¡Œæ–¹æ³•çš„æ— æ•ˆæ€§ï¼Œå¹¶å°†æˆ‘ä»¬çš„å®éªŒè¿›ä¸€æ­¥æ‰©å±•åˆ°å°æ ·æœ¬å’ŒåŸºäºæç¤ºçš„è®¾ç½®ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/mazurowski-lab/finetune-SAM%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E9%92%88%E5%AF%B9MRI%E7%9A%84%E7%89%B9%E5%AE%9A%E5%BE%AE%E8%B0%83%E6%9D%83%E9%87%8D%EF%BC%8C%E8%BF%99%E4%BA%9B%E6%9D%83%E9%87%8D%E5%9C%A8%E5%8E%9F%E5%A7%8BSAM%E4%B8%8A%E5%A7%8B%E7%BB%88%E8%8E%B7%E5%BE%97%E4%BA%86%E6%9B%B4%E5%A5%BD%E7%9A%84%E6%80%A7%E8%83%BD%E3%80%82">https://github.com/mazurowski-lab/finetune-SAMä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç å’Œé’ˆå¯¹MRIçš„ç‰¹å®šå¾®è°ƒæƒé‡ï¼Œè¿™äº›æƒé‡åœ¨åŸå§‹SAMä¸Šå§‹ç»ˆè·å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.09957v3">PDF</a> Accepted for publication at the Journal of Machine Learning for   Biomedical Imaging (MELBA)</p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ çš„å‘å±•æ¨åŠ¨äº†åŒ»å­¦å›¾åƒåˆ†æä¸­çš„è‡ªåŠ¨åŒ–åˆ†å‰²ä»»åŠ¡å–å¾—æ˜¾è‘—è¿›å±•ã€‚æœ€è¿‘å¼€å‘çš„ä»¥å›¾åƒåˆ†å‰²ä¸ºé‡ç‚¹çš„Segment Anything Modelï¼ˆSAMï¼‰æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…³äºå¦‚ä½•å¯¹SAMè¿›è¡Œæœ€ä½³å¾®è°ƒä»¥ä¼˜åŒ–åŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œå°šæ— ç³»ç»Ÿåˆ†æå’Œâ€œæœ€ä½³å®è·µâ€æŒ‡å—ã€‚æœ¬ç ”ç©¶æ€»ç»“äº†ä½¿ç”¨ä¸åŒä¸»å¹²æ¶æ„ã€æ¨¡å‹ç»„ä»¶å’Œå¾®è°ƒç®—æ³•çš„ç°æœ‰å¾®è°ƒç­–ç•¥ï¼Œå¹¶åœ¨æ¶µç›–æ‰€æœ‰å¸¸è§æ”¾å°„å­¦æ¨¡æ€çš„17ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œå¾®è°ƒSAMç•¥ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œä½¿ç”¨å‚æ•°é«˜æ•ˆå­¦ä¹ çš„å¾®è°ƒç­–ç•¥ä¼˜äºå…¶ä»–ç­–ç•¥ï¼Œç½‘ç»œæ¶æ„å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“è¾ƒå°ï¼Œä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ è¿›ä¸€æ­¥è®­ç»ƒSAMå¯ä»¥æé«˜æœ€ç»ˆæ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜å±•ç¤ºäº†æ–‡çŒ®ä¸­ä¸€äº›æµè¡Œæ–¹æ³•çš„æœ‰æ•ˆæ€§ä¸è¶³ï¼Œå¹¶å°†å®éªŒè¿›ä¸€æ­¥æ‰©å±•åˆ°å°æ ·æœ¬å’ŒåŸºäºæç¤ºçš„ç¯å¢ƒä¸­ã€‚æœ€åï¼Œå‘å¸ƒäº†ä»£ç å’Œé’ˆå¯¹MRIçš„å¾®è°ƒæƒé‡ï¼Œæ€§èƒ½ä¼˜äºåŸå§‹SAMæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–åˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„åŸºæœ¬ä»»åŠ¡ï¼Œæ·±åº¦å­¦ä¹ çš„å‘å±•æ˜¾è‘—æ¨åŠ¨äº†å…¶è¿›å±•ã€‚</li>
<li>Segment Anything Model (SAM)æ˜¯é’ˆå¯¹å›¾åƒåˆ†å‰²è€Œå¼€å‘çš„ï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç›®å‰é’ˆå¯¹SAMçš„æœ€ä½³å¾®è°ƒç­–ç•¥å°šæ— ç³»ç»Ÿåˆ†æå’ŒæŒ‡å—ã€‚</li>
<li>ç ”ç©¶æ€»ç»“äº†å¤šç§å¾®è°ƒç­–ç•¥ï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¸åŒçš„ä¸»å¹²æ¶æ„ã€æ¨¡å‹ç»„ä»¶å’Œå¾®è°ƒç®—æ³•ã€‚</li>
<li>å¾®è°ƒSAMçš„æ€§èƒ½ç•¥ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨å‚æ•°é«˜æ•ˆå­¦ä¹ çš„å¾®è°ƒç­–ç•¥åœ¨ç½‘ç»œæ¶æ„å¯¹æœ€ç»ˆæ€§èƒ½å½±å“è¾ƒå°çš„æƒ…å†µä¸‹è¡¨ç°æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.09957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ef979f362d30c91768edb3cb9cfcbb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0395a23f624f16d17e20aa8bcbbf93a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7b10e1ba15649dc14ded6d0704408d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56e928361467f47b8e5e5155b445b95b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-edee906e9a345f0ac2a02462149c7882.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  Pixel Motion as Universal Representation for Robot Control
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2cb13f0e8e6169d11abb374d0d4a37a1.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  TopicVD A Topic-Based Dataset of Video-Guided Multimodal Machine   Translation for Documentaries
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26522.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
