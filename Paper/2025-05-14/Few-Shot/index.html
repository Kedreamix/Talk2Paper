<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-05-14  Beyond CLIP Generalization Against Forward&amp;Backward Forgetting Adapter   for Continual Learning of Vision-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a7232ea14da8cf4d1a5ec0344649ce78.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    46 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-14-更新"><a href="#2025-05-14-更新" class="headerlink" title="2025-05-14 更新"></a>2025-05-14 更新</h1><h2 id="Beyond-CLIP-Generalization-Against-Forward-Backward-Forgetting-Adapter-for-Continual-Learning-of-Vision-Language-Models"><a href="#Beyond-CLIP-Generalization-Against-Forward-Backward-Forgetting-Adapter-for-Continual-Learning-of-Vision-Language-Models" class="headerlink" title="Beyond CLIP Generalization: Against Forward&amp;Backward Forgetting Adapter   for Continual Learning of Vision-Language Models"></a>Beyond CLIP Generalization: Against Forward&amp;Backward Forgetting Adapter   for Continual Learning of Vision-Language Models</h2><p><strong>Authors:Songlin Dong, Chenhao Ding, Jiangyang Li, Jizhou Han, Qiang Wang, Yuhang He, Yihong Gong</strong></p>
<p>This study aims to address the problem of multi-domain task incremental learning<del>(MTIL), which requires that vision-language models</del>(VLMs) continuously acquire new knowledge while maintaining their inherent zero-shot recognition capability. Existing paradigms delegate the testing of unseen-domain samples to the original CLIP, which only prevents the degradation of the model’s zero-shot capability but fails to enhance the generalization of the VLM further. To this end, we propose a novel MTIL framework, named AFA, which comprises two core modules: (1) an against forward-forgetting adapter that learns task-invariant information for each dataset in the incremental tasks to enhance the zero-shot recognition ability of VLMs; (2) an against backward-forgetting adapter that strengthens the few-shot learning capability of VLMs while supporting incremental learning. Extensive experiments demonstrate that the AFA method significantly outperforms existing state-of-the-art approaches, especially in few-shot MTIL tasks, and surpasses the inherent zero-shot performance of CLIP in terms of transferability. The code is provided in the Supplementary Material. </p>
<blockquote>
<p>本研究旨在解决多域任务增量学习（MTIL）的问题，这要求视觉语言模型（VLM）在持续获取新知识的同时，保持其固有的零样本识别能力。现有范式将未见域样本的测试委托给原始CLIP，这只能防止模型零样本能力的退化，但未能进一步提高VLM的泛化能力。为此，我们提出了一种新型的MTIL框架，名为AFA，它包括两个核心模块：（1）一种对抗前向遗忘适配器，用于学习增量任务中每个数据集的任务不变信息，以提高VLM的零样本识别能力；（2）一种对抗后向遗忘适配器，旨在加强VLM的小样本学习能力，同时支持增量学习。大量实验表明，AFA方法显著优于现有最先进的方法，尤其在少样本MTIL任务中表现突出，并且在可迁移性方面超越了CLIP的固有零样本性能。代码已作为补充材料提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07690v1">PDF</a> </p>
<p><strong>Summary</strong><br>多域任务增量学习（MTIL）问题要求视觉语言模型（VLM）在持续获取新知识的同时保持其固有的零样本识别能力。现有模式将未见域样本的测试委托给原始CLIP，这只能防止模型零样本能力的退化，但未能进一步提高VLM的泛化能力。为此，我们提出了一种新的MTIL框架，名为AFA，它包括两个核心模块：（1）一种防止知识遗忘的适配器，用于学习增量任务中每个数据集的任务不变信息，以提高VLM的零样本识别能力；（2）另一种强化VLM的少样本学习能力并支持增量学习的防止反向遗忘的适配器。实验表明，AFA方法在少样本MTIL任务上显著优于现有先进技术，并且在可迁移性方面超越了CLIP的固有零样本性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该研究关注多域任务增量学习（MTIL），旨在让视觉语言模型（VLM）在持续学习新知识的同时保持零样本识别能力。</li>
<li>现有模式测试未见域样本时存在局限性，仅防止模型零样本能力退化，未进一步提高VLM泛化能力。</li>
<li>提出的AFA框架包含两个核心模块：防止正向遗忘的适配器和防止反向遗忘的适配器。</li>
<li>防止正向遗忘的适配器通过学习任务不变信息来提高VLM的零样本识别能力。</li>
<li>防止反向遗忘的适配器强化VLM的少样本学习能力，并支持增量学习。</li>
<li>实验表明，AFA方法在少样本MTIL任务上表现优异，超越现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07690">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d864b75af5f37950ed870a3d0a23c807.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e485d146906c8391353d4acd893ee42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88a5c5d03ba8f4dbb67ef08de6db4082.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfd0d44962ea6c428195d0c8971e8686.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ShotAdapter-Text-to-Multi-Shot-Video-Generation-with-Diffusion-Models"><a href="#ShotAdapter-Text-to-Multi-Shot-Video-Generation-with-Diffusion-Models" class="headerlink" title="ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models"></a>ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models</h2><p><strong>Authors:Ozgur Kara, Krishna Kumar Singh, Feng Liu, Duygu Ceylan, James M. Rehg, Tobias Hinz</strong></p>
<p>Current diffusion-based text-to-video methods are limited to producing short video clips of a single shot and lack the capability to generate multi-shot videos with discrete transitions where the same character performs distinct activities across the same or different backgrounds. To address this limitation we propose a framework that includes a dataset collection pipeline and architectural extensions to video diffusion models to enable text-to-multi-shot video generation. Our approach enables generation of multi-shot videos as a single video with full attention across all frames of all shots, ensuring character and background consistency, and allows users to control the number, duration, and content of shots through shot-specific conditioning. This is achieved by incorporating a transition token into the text-to-video model to control at which frames a new shot begins and a local attention masking strategy which controls the transition token’s effect and allows shot-specific prompting. To obtain training data we propose a novel data collection pipeline to construct a multi-shot video dataset from existing single-shot video datasets. Extensive experiments demonstrate that fine-tuning a pre-trained text-to-video model for a few thousand iterations is enough for the model to subsequently be able to generate multi-shot videos with shot-specific control, outperforming the baselines. You can find more details in <a target="_blank" rel="noopener" href="https://shotadapter.github.io/">https://shotadapter.github.io/</a> </p>
<blockquote>
<p>当前基于扩散的文本到视频转换方法仅限于生成单镜头的简短视频片段，缺乏生成具有离散过渡的多镜头视频的能力，其中同一角色在同一或不同背景下执行不同的活动。为了解决这个问题，我们提出了一种框架，包括数据集收集管道和视频扩散模型的架构扩展，以实现文本到多镜头视频的生成。我们的方法能够将多镜头视频生成为一个单一视频，全面关注所有镜头的所有帧，确保角色和背景的一致性，并允许用户通过镜头特定的条件控制镜头的数量、持续时间和内容。这是通过向文本到视频模型中融入过渡令牌来实现的，该令牌控制新镜头在哪些帧开始，以及局部注意力掩码策略，该策略控制过渡令牌的影响，并允许针对镜头进行特定提示。为了获取训练数据，我们提出了一种新的数据收集管道，用于从现有的单镜头视频数据集中构建多镜头视频数据集。大量实验表明，对预训练的文本到视频模型进行数千次迭代的微调足以使模型随后能够具有针对镜头的控制生成多镜头视频，并且表现优于基线。更多细节可在<a target="_blank" rel="noopener" href="https://shotadapter.github.io/">https://shotadapter.github.io/</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07652v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种能够生成文本到多镜头视频的框架，包括数据集收集管道和架构扩展的视频扩散模型。该框架解决了现有文本转视频方法仅限于生成单镜头短视频的问题，并实现了在多镜头视频生成中跨越相同或不同背景的相同角色执行不同活动的功能。用户可以通过镜头特定的条件控制镜头的数量、持续时间和内容。通过引入过渡令牌来控制新镜头开始的位置和局部注意力掩码策略来实现这一点。为了获取训练数据，我们提出了一种新的数据收集管道，从现有的单镜头视频数据集中构建多镜头视频数据集。实验表明，对预训练的文本到视频模型进行数千次迭代的微调足以使模型能够生成具有镜头特定控制的多镜头视频，超越了基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一个能够生成文本到多镜头视频的框架，解决了现有方法仅限于单镜头视频的局限性。</li>
<li>框架包括数据集收集管道和视频扩散模型的架构扩展。</li>
<li>用户可以通过镜头特定的条件控制镜头的数量、持续时间和内容。</li>
<li>通过引入过渡令牌和局部注意力掩码策略实现多镜头视频的生成。</li>
<li>提出了一种新的数据收集管道，用于从现有的单镜头视频数据集中构建多镜头视频数据集。</li>
<li>实验表明，对预训练模型进行微调能够生成具有镜头特定控制的多镜头视频。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07652">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f42e8d4a727df297b4b36dee74ef479f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b025acb03d316dc295d0eb37237dc441.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d52407d9d1d790be43b38748cf2d2a3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b676f46e61f0420f1720e2fe25f143b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dcbaae99f4558c682cdd536c5b95323.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ReinboT-Amplifying-Robot-Visual-Language-Manipulation-with-Reinforcement-Learning"><a href="#ReinboT-Amplifying-Robot-Visual-Language-Manipulation-with-Reinforcement-Learning" class="headerlink" title="ReinboT: Amplifying Robot Visual-Language Manipulation with   Reinforcement Learning"></a>ReinboT: Amplifying Robot Visual-Language Manipulation with   Reinforcement Learning</h2><p><strong>Authors:Hongyin Zhang, Zifeng Zhuang, Han Zhao, Pengxiang Ding, Hongchao Lu, Donglin Wang</strong></p>
<p>Vision-Language-Action (VLA) models have shown great potential in general robotic decision-making tasks via imitation learning. However, the variable quality of training data often constrains the performance of these models. On the other hand, offline Reinforcement Learning (RL) excels at learning robust policy models from mixed-quality data. In this paper, we introduce Reinforced robot GPT (ReinboT), a novel end-to-end VLA model that integrates the RL principle of maximizing cumulative reward. ReinboT achieves a deeper understanding of the data quality distribution by predicting dense returns that capture the nuances of manipulation tasks. The dense return prediction capability enables the robot to generate more robust decision-making actions, oriented towards maximizing future benefits. Extensive experiments show that ReinboT achieves state-of-the-art performance on the CALVIN mixed-quality dataset and exhibits superior few-shot learning and out-of-distribution generalization capabilities in real-world tasks. </p>
<blockquote>
<p>视觉-语言-动作（VLA）模型在通过模仿学习进行一般的机器人决策任务中表现出了巨大的潜力。然而，训练数据质量的不稳定经常限制这些模型的性能。另一方面，离线强化学习（RL）擅长从混合质量数据中学习稳健的策略模型。在本文中，我们介绍了强化机器人GPT（ReinboT），这是一种新型端到端的VLA模型，集成了强化学习的最大化累积回报原则。ReinboT通过预测密集回报来实现对数据质量分布的深入理解，这些密集回报捕捉到了操作任务的细微差别。密集回报预测能力使机器人能够生成更稳健的决策行动，致力于最大化未来收益。大量实验表明，ReinboT在CALVIN混合质量数据集上达到了最新性能，并在现实任务中表现出了卓越的小样本学习和跨分布泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07395v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于视觉、语言和动作的VLA模型在机器人决策任务中展现出了巨大潜力，但它们常常受限于训练数据的质量问题。而离线强化学习擅长从混合质量数据中学习稳健的策略模型。本文介绍了融合强化学习原理的机器人GPT（ReinboT），一种新型端到端的VLA模型。ReinboT通过预测密集回报来理解数据质量分布，从而实现在操作任务中的细微差别。这种密集回报预测能力使机器人能够生成更稳健的决策行动，致力于最大化未来收益。实验表明，ReinboT在CALVIN混合质量数据集上达到了最新技术水平，并在现实任务中展现出卓越的小样本学习和跨分布泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLA模型在机器人决策任务中有巨大潜力，但受限于训练数据质量。</li>
<li>离线强化学习擅长从混合质量数据中学习稳健策略模型。</li>
<li>ReinboT是结合强化学习原理的新型VLA模型，能预测密集回报以理解数据质量分布。</li>
<li>密集回报预测能力有助于机器人生成更稳健的决策行动，致力于最大化未来收益。</li>
<li>ReinboT在CALVIN混合质量数据集上实现了最新技术水平。</li>
<li>ReinboT展现出卓越的小样本学习能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07395">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-be18356cc1b47e9962356e7df6ab1b1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97c1df44ac36e5d3b9a10d34a84f0320.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cda1c4ec6f1c728866b2b8b9e451e22.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ReCDAP-Relation-Based-Conditional-Diffusion-with-Attention-Pooling-for-Few-Shot-Knowledge-Graph-Completion"><a href="#ReCDAP-Relation-Based-Conditional-Diffusion-with-Attention-Pooling-for-Few-Shot-Knowledge-Graph-Completion" class="headerlink" title="ReCDAP: Relation-Based Conditional Diffusion with Attention Pooling for   Few-Shot Knowledge Graph Completion"></a>ReCDAP: Relation-Based Conditional Diffusion with Attention Pooling for   Few-Shot Knowledge Graph Completion</h2><p><strong>Authors:Jeongho Kim, Chanyeong Heo, Jaehee Jung</strong></p>
<p>Knowledge Graphs (KGs), composed of triples in the form of (head, relation, tail) and consisting of entities and relations, play a key role in information retrieval systems such as question answering, entity search, and recommendation. In real-world KGs, although many entities exist, the relations exhibit a long-tail distribution, which can hinder information retrieval performance. Previous few-shot knowledge graph completion studies focused exclusively on the positive triple information that exists in the graph or, when negative triples were incorporated, used them merely as a signal to indicate incorrect triples. To overcome this limitation, we propose Relation-Based Conditional Diffusion with Attention Pooling (ReCDAP). First, negative triples are generated by randomly replacing the tail entity in the support set. By conditionally incorporating positive information in the KG and non-existent negative information into the diffusion process, the model separately estimates the latent distributions for positive and negative relations. Moreover, including an attention pooler enables the model to leverage the differences between positive and negative cases explicitly. Experiments on two widely used datasets demonstrate that our method outperforms existing approaches, achieving state-of-the-art performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/hou27/ReCDAP-FKGC">https://github.com/hou27/ReCDAP-FKGC</a>. </p>
<blockquote>
<p>知识图谱（KGs）由(头实体，关系，尾实体)形式的三元组构成，包含实体和关系，在问答、实体搜索和推荐等信息系统检索中扮演着关键角色。在现实世界的知识图谱中，尽管存在许多实体，但关系呈现出长尾分布，可能会阻碍信息检索性能。以前关于知识图谱补全的研究主要集中在图中存在的正三元组信息上，或者当引入负三元组时，仅仅将它们作为表示错误三元组的信号。为了克服这一局限性，我们提出了基于关系的条件扩散与注意力池化（ReCDAP）。首先，通过随机替换支持集中的尾实体来生成负三元组。通过有条件地融入知识图谱中的正信息和不存在的负信息到扩散过程中，该模型分别估计正关系和负关系的潜在分布。此外，加入注意力池化器使模型能够明确利用正负案例之间的差异。在两个广泛使用的数据集上的实验表明，我们的方法优于现有方法，达到了最先进的性能。代码可访问<a target="_blank" rel="noopener" href="https://github.com/hou27/ReCDAP-FKGC%E3%80%82">https://github.com/hou27/ReCDAP-FKGC。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07171v1">PDF</a> Accepted by SIGIR 2025, 5 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>本文介绍了知识图谱在信息检索系统中的作用，如问答、实体搜索和推荐等。针对知识图谱补全中的长尾分布问题，提出了一种基于关系条件扩散与注意力池化的方法（ReCDAP）。该方法通过生成负三元组，结合正三元组信息，分别估计正负关系的潜在分布。实验表明，该方法在常用数据集上取得了最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>知识图谱在信息检索系统中扮演重要角色，如问答、实体搜索和推荐。</li>
<li>现实世界中知识图谱的实体关系呈现长尾分布，影响信息检索性能。</li>
<li>现有研究主要关注正三元组信息，忽视了负三元组在信息检索中的作用。</li>
<li>ReCDAP方法通过生成负三元组，结合正三元组信息，进行知识图谱补全。</li>
<li>ReCDAP方法估计正负关系的潜在分布，提高知识图谱补全的性能。</li>
<li>实验表明，ReCDAP方法在常用数据集上取得了最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07171">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4fc2e28f8e7dd0cfb73400b1a9fea471.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0690f6f90a0b07c1b6a2d51c6e8aaa86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6a8c6dd8c8ce6d4c69d4655c4fbdb7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83542ee56278089fa58dec6555e5f020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-741ddfdcdef757d302d76d11148b1be4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dadfc69e1fcd1366721e38450046ea75.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Vision-Language-Foundation-Model-for-Leaf-Disease-Identification"><a href="#A-Vision-Language-Foundation-Model-for-Leaf-Disease-Identification" class="headerlink" title="A Vision-Language Foundation Model for Leaf Disease Identification"></a>A Vision-Language Foundation Model for Leaf Disease Identification</h2><p><strong>Authors:Khang Nguyen Quoc, Lan Le Thi Thu, Luyl-Da Quach</strong></p>
<p>Leaf disease identification plays a pivotal role in smart agriculture. However, many existing studies still struggle to integrate image and textual modalities to compensate for each other’s limitations. Furthermore, many of these approaches rely on pretraining with constrained datasets such as ImageNet, which lack domain-specific information. We propose SCOLD (Soft-target COntrastive learning for Leaf Disease identification), a context-aware vision-language foundation model tailored to address these challenges for agricultural tasks. SCOLD is developed using a diverse corpus of plant leaf images and corresponding symptom descriptions, comprising over 186,000 image-caption pairs aligned with 97 unique concepts. Through task-agnostic pretraining, SCOLD leverages contextual soft targets to mitigate overconfidence in contrastive learning by smoothing labels, thereby improving model generalization and robustness on fine-grained classification tasks. Experimental results demonstrate that SCOLD outperforms existing vision-language models such as OpenAI-CLIP-L, BioCLIP, and SigLIP2 across several benchmarks, including zero-shot and few-shot classification, image-text retrieval, and image classification, while maintaining a competitive parameter footprint. Ablation studies further highlight SCOLD’s effectiveness in contrast to its counterparts. The proposed approach significantly advances the agricultural vision-language foundation model, offering strong performance with minimal or no supervised fine-tuning. This work lays a solid groundwork for future research on models trained with long-form and simplified contexts, tasks involving class ambiguity, and multi-modal systems for intelligent plant disease diagnostics. The code for this study is available at <a target="_blank" rel="noopener" href="https://huggingface.co/enalis/scold">https://huggingface.co/enalis/scold</a> </p>
<blockquote>
<p>叶片疾病识别在智能农业中扮演着至关重要的角色。然而，许多现有的研究仍难以整合图像和文本模态以相互弥补彼此的局限性。此外，这些方法中许多依赖于使用受限数据集（如ImageNet）进行预训练，这些数据集缺乏特定领域的详细信息。我们提出了SCOLD（针对叶片疾病识别的软目标对比学习法），这是一种面向农业任务挑战的上下文感知视觉语言基础模型。SCOLD使用包含超过18.6万张与97个独特概念对齐的图像标题对构成的多样化植物叶片图像和相应的症状描述来开发。通过任务无关的预训练，SCOLD利用上下文软目标来缓解对比学习中标签平滑导致的过度自信问题，从而提高模型在细粒度分类任务上的泛化和稳健性。实验结果表明，SCOLD在多个基准测试中均优于现有的视觉语言模型，如OpenAI-CLIP-L、BioCLIP和SigLIP2，包括零样本和少样本分类、图像文本检索和图像分类等任务，同时保持竞争性的参数占用空间。消融研究进一步突出了SCOLD与竞争对手的有效性。所提出的方法显著地推进了农业视觉语言基础模型的发展，以最小的或无需监督微调就能实现强劲的性能。这项工作为未来在模型使用长形式简化语境、涉及类别模糊性的任务和多模态系统智能植物疾病诊断等领域的研究奠定了坚实的基础。这项研究的代码可在<a target="_blank" rel="noopener" href="https://huggingface.co/enalis/scold">https://huggingface.co/enalis/scold</a> 获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07019v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了针对农业任务中的叶病识别挑战的解决方案。通过开发一种名为SCOLD的上下文感知视觉语言基础模型，该模型能够融合图像和文本模态，克服单一模态的局限性。SCOLD模型使用包含超过18.6万张植物叶片图像和相应症状描述的多样语料库进行训练，涵盖97个独特概念。实验结果表明，SCOLD在零样本和少样本分类、图像文本检索和图像分类等多个基准测试中优于现有视觉语言模型，如OpenAI-CLIP-L、BioCLIP和SigLIP2，同时保持竞争性的参数占用。该模型为提高农业视觉语言基础模型的性能提供了新的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>叶病识别在智能农业中至关重要，但现有研究在整合图像和文本模态方面仍存在挑战。</li>
<li>SCOLD模型是一种上下文感知的视觉语言基础模型，旨在解决这些挑战，特别针对农业任务。</li>
<li>SCOLD模型使用包含大量植物叶片图像和症状描述的语料库进行训练，涵盖多种概念和任务。</li>
<li>SCOLD通过任务无关预训练和上下文软目标，利用标签平滑来提高模型的泛化和鲁棒性。</li>
<li>实验结果表明，SCOLD在多个基准测试中优于其他视觉语言模型。</li>
<li>SCOLD模型在零样本和少样本分类、图像文本检索和图像分类等任务上表现出色。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07019">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-64dbeff0a9f62d097d03f1479a1c1870.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c922887ff792db98d39fc3c760871f53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d2636453da2551e77ebcc02ccb3cf51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-489a06f94d885f69651dea5b0ddb9218.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Adaptive-Wiping-Adaptive-contact-rich-manipulation-through-few-shot-imitation-learning-with-Force-Torque-feedback-and-pre-trained-object-representations"><a href="#Adaptive-Wiping-Adaptive-contact-rich-manipulation-through-few-shot-imitation-learning-with-Force-Torque-feedback-and-pre-trained-object-representations" class="headerlink" title="Adaptive Wiping: Adaptive contact-rich manipulation through few-shot   imitation learning with Force-Torque feedback and pre-trained object   representations"></a>Adaptive Wiping: Adaptive contact-rich manipulation through few-shot   imitation learning with Force-Torque feedback and pre-trained object   representations</h2><p><strong>Authors:Chikaha Tsuji, Enrique Coronado, Pablo Osorio, Gentiane Venture</strong></p>
<p>Imitation learning offers a pathway for robots to perform repetitive tasks, allowing humans to focus on more engaging and meaningful activities. However, challenges arise from the need for extensive demonstrations and the disparity between training and real-world environments. This paper focuses on contact-rich tasks like wiping with soft and deformable objects, requiring adaptive force control to handle variations in wiping surface height and the sponge’s physical properties. To address these challenges, we propose a novel method that integrates real-time force-torque (FT) feedback with pre-trained object representations. This approach allows robots to dynamically adjust to previously unseen changes in surface heights and sponges’ physical properties. In real-world experiments, our method achieved 96% accuracy in applying reference forces, significantly outperforming the previous method that lacked an FT feedback loop, which only achieved 4% accuracy. To evaluate the adaptability of our approach, we conducted experiments under different conditions from the training setup, involving 40 scenarios using 10 sponges with varying physical properties and 4 types of wiping surface heights, demonstrating significant improvements in the robot’s adaptability by analyzing force trajectories. The video of our work is available at: <a target="_blank" rel="noopener" href="https://sites.google.com/view/adaptive-wiping">https://sites.google.com/view/adaptive-wiping</a> </p>
<blockquote>
<p>模仿学习为机器人执行重复性任务提供了途径，使人类能够专注于更具吸引力和有意义的活动。然而，挑战来自于需要大量的演示以及训练环境与真实环境之间的差异。本文聚焦于擦拭类任务，如使用柔软且可变形物体的擦拭操作，需要自适应力控制来处理擦拭表面高度和海绵的物理属性变化。为了应对这些挑战，我们提出了一种新方法，该方法将实时力扭矩（FT）反馈与预训练的对象表示相结合。这种方法使机器人能够动态适应以前未见过的表面高度和海绵物理属性的变化。在真实世界的实验中，我们的方法在应用参考力方面达到了96%的准确率，显著优于缺乏FT反馈回路的方法，后者仅达到4%的准确率。为了评估我们方法的适应性，我们在不同于训练环境的条件下进行了实验，涉及使用具有不同物理属性的10种海绵和4种擦拭表面高度的40个场景，通过分析力轨迹，显示出机器人在适应性方面的显著改善。我们工作的视频可在以下网址观看：<a target="_blank" rel="noopener" href="https://sites.google.com/view/adaptive%2dwiping">https://sites.google.com/view/adaptive-wiping</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06451v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>机器人通过模仿学习执行重复任务，使人类能够专注于更有意义和吸引力的活动。然而，该方法面临需要海量演示数据和训练环境与真实环境存在差异的挑战。针对擦拭等接触丰富的任务，本文提出一种结合实时力扭矩反馈与预训练物体表征的新方法，机器人可根据表面高度和海绵物理属性的变化动态调整。在真实场景中，新方法在施加参考力方面实现了96%的准确率，显著优于缺乏力扭矩反馈的以往方法（仅4%的准确率）。通过在不同条件下进行实验，验证了方法在不同海绵物理属性和表面高度下的适应性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>模仿学习允许机器人执行重复任务，使人类专注于更有意义的活动。</li>
<li>模仿学习面临的挑战包括需要大量演示数据和训练环境与真实环境的差异。</li>
<li>针对擦拭等接触丰富的任务，需要适应力控制来处理表面高度和海绵物理属性的变化。</li>
<li>提出了一种结合实时力扭矩反馈与预训练物体表征的新方法，以应对这些挑战。</li>
<li>新方法在真实场景中实现了高准确率，显著优于以往方法。</li>
<li>通过实验验证了方法在不同海绵物理属性和表面高度下的适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06451">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-31eded18068d3c258398f514fbd0e04b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f44f5a8a317a4fedc99bf9801781e932.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a90226cf3acbc48c031fa9eba912be9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88acafcfba20fb1cd7304917a4f79554.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e55c43994b27ae255293a951cc09c996.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d75ed33fee3031668a9637d9976206d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f76bd347254d3a27aed805749d3b5ae0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31c6c77a54adbacae71955d34bef1459.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TREND-Tri-teaching-for-Robust-Preference-based-Reinforcement-Learning-with-Demonstrations"><a href="#TREND-Tri-teaching-for-Robust-Preference-based-Reinforcement-Learning-with-Demonstrations" class="headerlink" title="TREND: Tri-teaching for Robust Preference-based Reinforcement Learning   with Demonstrations"></a>TREND: Tri-teaching for Robust Preference-based Reinforcement Learning   with Demonstrations</h2><p><strong>Authors:Shuaiyi Huang, Mara Levy, Anubhav Gupta, Daniel Ekpo, Ruijie Zheng, Abhinav Shrivastava</strong></p>
<p>Preference feedback collected by human or VLM annotators is often noisy, presenting a significant challenge for preference-based reinforcement learning that relies on accurate preference labels. To address this challenge, we propose TREND, a novel framework that integrates few-shot expert demonstrations with a tri-teaching strategy for effective noise mitigation. Our method trains three reward models simultaneously, where each model views its small-loss preference pairs as useful knowledge and teaches such useful pairs to its peer network for updating the parameters. Remarkably, our approach requires as few as one to three expert demonstrations to achieve high performance. We evaluate TREND on various robotic manipulation tasks, achieving up to 90% success rates even with noise levels as high as 40%, highlighting its effective robustness in handling noisy preference feedback. Project page: <a target="_blank" rel="noopener" href="https://shuaiyihuang.github.io/publications/TREND">https://shuaiyihuang.github.io/publications/TREND</a>. </p>
<blockquote>
<p>通过人类或VLM注释器收集的偏好反馈往往存在噪声，这为基于准确偏好标签的基于偏好的强化学习带来了重大挑战。为了应对这一挑战，我们提出了TREND，这是一个将少量专家演示与三教学习策略相结合的新型框架，以实现有效的噪声缓解。我们的方法同时训练三个奖励模型，每个模型将其低损失偏好对视为有用知识，并将其教给同行网络以更新参数。值得注意的是，我们的方法只需要一到三个专家演示即可实现高性能。我们在各种机器人操作任务上评估TREND，即使在高达40%的噪声水平下，也能达到高达90%的成功率，从而突出了其在处理噪声偏好反馈方面的有效稳健性。项目页面：<a target="_blank" rel="noopener" href="https://shuaiyihuang.github.io/publications/TREND">https://shuaiyihuang.github.io/publications/TREND</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06079v1">PDF</a> ICRA 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为TREND的新框架，通过整合少量专家演示和三重教学策略来解决基于噪声偏好的强化学习问题。该框架同时训练三个奖励模型，每个模型将其低损失偏好对视为有用知识并传授给同行网络进行参数更新。该方法只需要一到三次专家演示即可实现高性能，在机器人操作任务上取得高达90%的成功率，即使在高达40%的噪声水平下也能有效应对噪声偏好反馈。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TREND框架解决了基于噪声偏好的强化学习挑战。</li>
<li>通过整合少量专家演示和三重教学策略实现有效噪声缓解。</li>
<li>同时训练三个奖励模型，利用低损失偏好对更新参数。</li>
<li>仅需一至三次专家演示即可实现高性能。</li>
<li>在机器人操作任务上取得了高达90%的成功率。</li>
<li>在高噪声水平下仍能保持稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06079">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9382a21b0916821344edeb949dc6a085.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0dbca607ebb284a3ba96abca04b82c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aec0b5ae1fa5f11c00e2ff7d91ad0304.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d746e5a9c0a1cf27205e7419dcb44ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3850718e47e065ea7ac41ac10f6f4b02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c493230ce2a50533520988c5750ff0c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Task-Adapter-Task-specific-Adaptation-with-Order-aware-Alignment-for-Few-shot-Action-Recognition"><a href="#Task-Adapter-Task-specific-Adaptation-with-Order-aware-Alignment-for-Few-shot-Action-Recognition" class="headerlink" title="Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for   Few-shot Action Recognition"></a>Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for   Few-shot Action Recognition</h2><p><strong>Authors:Congqi Cao, Peiheng Han, Yueran zhang, Yating Yu, Qinyi Lv, Lingtong Min, Yanning zhang</strong></p>
<p>Large-scale pre-trained models have achieved remarkable success in language and image tasks, leading an increasing number of studies to explore the application of pre-trained image models, such as CLIP, in the domain of few-shot action recognition (FSAR). However, current methods generally suffer from several problems: 1) Direct fine-tuning often undermines the generalization capability of the pre-trained model; 2) The exploration of task-specific information is insufficient in the visual tasks; 3) The semantic order information is typically overlooked during text modeling; 4) Existing cross-modal alignment techniques ignore the temporal coupling of multimodal information. To address these, we propose Task-Adapter++, a parameter-efficient dual adaptation method for both image and text encoders. Specifically, to make full use of the variations across different few-shot learning tasks, we design a task-specific adaptation for the image encoder so that the most discriminative information can be well noticed during feature extraction. Furthermore, we leverage large language models (LLMs) to generate detailed sequential sub-action descriptions for each action class, and introduce semantic order adapters into the text encoder to effectively model the sequential relationships between these sub-actions. Finally, we develop an innovative fine-grained cross-modal alignment strategy that actively maps visual features to reside in the same temporal stage as semantic descriptions. Extensive experiments fully demonstrate the effectiveness and superiority of the proposed method, which achieves state-of-the-art performance on 5 benchmarks consistently. The code is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/Jaulin-Bage/Task-Adapter-pp">https://github.com/Jaulin-Bage/Task-Adapter-pp</a>. </p>
<blockquote>
<p>大规模预训练模型在语言和图像任务中取得了显著的成功，引发了越来越多的研究探索预训练图像模型（如CLIP）在少样本动作识别（FSAR）领域的应用。然而，当前的方法普遍存在一些问题：1）直接微调往往会削弱预训练模型的泛化能力；2）视觉任务中特定任务信息的探索不足；3）文本建模时忽略了语义顺序信息；4）现有的跨模态对齐技术忽略了多模态信息的时序耦合。为了解决这些问题，我们提出了Task-Adapter++，这是一种参数高效的图像和文本编码器的双重适应方法。具体来说，为了充分利用不同少样本学习任务之间的差异，我们为图像编码器设计了特定任务的适应方法，以便在特征提取过程中能够充分注意到最具区分性的信息。此外，我们利用大型语言模型（LLM）为每个动作类别生成详细的序列子动作描述，并在文本编码器中引入语义顺序适配器，以有效地建模这些子动作之间的序列关系。最后，我们开发了一种创新的精细跨模态对齐策略，该策略能积极地将视觉特征映射到与语义描述相同的时序阶段。大量的实验充分证明了所提方法的有效性和优越性，该方法在5个基准测试上均达到了最新技术水平。代码已开源在<a target="_blank" rel="noopener" href="https://github.com/Jaulin-Bage/Task-Adapter-pp">https://github.com/Jaulin-Bage/Task-Adapter-pp</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06002v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2408.00249</p>
<p><strong>Summary</strong></p>
<p>本文探讨了大规模预训练模型在语言和图像任务中的出色表现，特别是在少样本动作识别（FSAR）领域的应用。针对现有方法存在的问题，如直接微调影响模型泛化能力、视觉任务中任务特定信息探索不足、文本建模中语义顺序信息被忽视以及现有跨模态对齐技术忽略多模态信息的时序耦合等，本文提出了Task-Adapter++，一种参数高效的图像和文本编码器的双重适应方法。该方法通过设计任务特定适应的图像编码器、利用大型语言模型生成详细顺序子动作描述以及引入语义顺序适配器到文本编码器和精细跨模态对齐策略，实现了在五个基准测试上的卓越表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模预训练模型在语言和图像任务中表现卓越，特别是在少样本动作识别（FSAR）领域。</li>
<li>当前方法存在的问题包括直接微调影响模型泛化能力、视觉任务中任务特定信息探索不足等。</li>
<li>Task-Adapter++是一种参数高效的图像和文本编码器的双重适应方法，旨在解决上述问题。</li>
<li>该方法通过设计任务特定适应的图像编码器以利用不同少样本学习任务中的变化。</li>
<li>利用大型语言模型生成详细顺序子动作描述，并引入语义顺序适配器到文本编码器。</li>
<li>开发出一种创新的精细跨模态对齐策略，积极将视觉特征与语义描述映射到同一时间阶段。</li>
<li>Task-Adapter++在五个基准测试上实现了卓越表现，并公开了源代码。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06002">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-de3a9d8f0e1dab06c7baecff972f4d18.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d494af3d66420ac9121c01ddef57de1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f375b02140efa50b1058cf2688b59755.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2ae2dee0230e30222a035d566a50b79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93e07735f9c55613254800d5f4614e16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f75efd5adc6d13fcda819c537c78b97d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcd27552fb4adb3ad2a1808bd2e689a3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-–-An-evaluation-on-urological-doctors’-notes"><a href="#Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-–-An-evaluation-on-urological-doctors’-notes" class="headerlink" title="Can open source large language models be used for tumor documentation in   Germany? – An evaluation on urological doctors’ notes"></a>Can open source large language models be used for tumor documentation in   Germany? – An evaluation on urological doctors’ notes</h2><p><strong>Authors:Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Meike Ressing, Torsten Panholzer</strong></p>
<p>Tumor documentation in Germany is largely done manually, requiring reading patient records and entering data into structured databases. Large language models (LLMs) could potentially enhance this process by improving efficiency and reliability. This evaluation tests eleven different open source LLMs with sizes ranging from 1-70 billion model parameters on three basic tasks of the tumor documentation process: identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis. For evaluating the LLMs on these tasks, a dataset of annotated text snippets based on anonymized doctors’ notes from urology was prepared. Different prompting strategies were used to investigate the effect of the number of examples in few-shot prompting and to explore the capabilities of the LLMs in general. The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks. Models with less extensive training data or having fewer than 7 billion parameters showed notably lower performance, while larger models did not display performance gains. Examples from a different medical domain than urology could also improve the outcome in few-shot prompting, which demonstrates the ability of LLMs to handle tasks needed for tumor documentation. Open source LLMs show a strong potential for automating tumor documentation. Models from 7-12 billion parameters could offer an optimal balance between performance and resource efficiency. With tailored fine-tuning and well-designed prompting, these models might become important tools for clinical documentation in the future. The code for the evaluation is available from <a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval">https://github.com/stefan-m-lenz/UroLlmEval</a>. We also release the dataset as a new valuable resource that addresses the shortage of authentic and easily accessible benchmarks in German-language medical NLP. </p>
<blockquote>
<p>在德国，肿瘤记录工作大多以手动方式进行，需要阅读患者病历并将数据录入结构化数据库。大型语言模型（LLM）有潜力通过提高效率和可靠性来改进这一过程。本次评估对三种肿瘤记录基本任务（识别肿瘤诊断、分配ICD-10代码和提取首次诊断日期）上，测试了规模从1亿至70亿模型参数的11个不同开源LLM。为了评估这些任务上的LLM性能，准备了一个基于泌尿外科匿名医生笔记的标注文本片段数据集。使用了不同的提示策略来研究少量示例提示中示例数量对LLM性能的影响，并探索LLM的一般能力。模型Llama 3.1 8B、Mistral 7B和Mistral NeMo 12B在任务中表现相当出色。拥有较少训练数据或参数少于7亿的模型表现明显较差，而更大的模型并没有显示出性能提升。来自泌尿学以外的医学领域的例子也可以在少量提示中改善结果，这证明了LLM处理肿瘤记录所需任务的能力。开源LLM在自动进行肿瘤记录方面显示出巨大潜力。参数在7亿至12亿之间的模型可能在性能和资源效率之间达到最佳平衡。通过有针对性的微调以及精心设计提示，这些模型可能会成为未来临床记录的重要工具。评估的代码可从<a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval%E8%8E%B7%E5%8F%96%E3%80%82%E6%88%91%E4%BB%AC%E8%BF%98%E5%85%AC%E5%BC%80%E4%BA%86%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E4%BD%9C%E4%B8%BA%E8%A7%A3%E5%86%B3%E5%BE%B7%E5%9B%BD%E5%8C%BB%E5%AD%A6NLP%E4%B8%AD%E7%9C%9F%E5%AE%9E%E3%80%81%E6%98%93%E4%BA%8E%E8%AE%BF%E9%97%AE%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%B5%84%E6%BA%90%E7%9F%AD%E7%BC%BA%E9%97%AE%E9%A2%98%E7%9A%84%E4%B8%80%E4%B8%AA%E6%96%B0%E6%9C%89%E4%BB%B7%E5%80%BC%E8%B5%84%E6%BA%90%E3%80%82">https://github.com/stefan-m-lenz/UroLlmEval获取。我们还公开了数据集，作为解决德国医学NLP中真实、易于访问的基准测试资源短缺问题的一个新有价值资源。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12106v3">PDF</a> 53 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在肿瘤文档处理中展现出潜在的应用价值，能提高效率和可靠性。通过对11种不同规模（从1亿到70亿参数）的开源LLMs进行评价，发现它们在肿瘤诊断确认、ICD-10代码分配和首次诊断日期提取等任务中表现良好。模型性能随参数增加而提高，但不是线性关系。特定医疗领域的样本在few-shot prompting下能够提高表现，说明LLMs有能力处理肿瘤文档相关的任务。推荐模型参数在7-12亿范围内以达到性能和资源效率的平衡。适当微调并设计提示可使其在未来成为临床文档的重要工具。相关代码和数据集已发布供研究使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）可应用于肿瘤文档处理，提高效率和可靠性。</li>
<li>通过评价11种不同规模和类型的LLMs在三个基本任务上的表现，发现良好性能。</li>
<li>模型性能与参数规模有关，但不是线性关系，参数规模过大不一定带来性能提升。</li>
<li>Few-shot prompting效果受样本领域影响，表明LLMs具备处理不同医疗领域任务的能力。</li>
<li>推荐模型参数范围在7-12亿，以实现性能和资源效率之间的平衡。</li>
<li>适当微调和优化提示设计可以使这些模型成为未来临床文档处理的重要工具。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12106">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6f5b2171a7c546253d3782efe165a317.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec1b13367880e573c14047c7e811034b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MedualTime-A-Dual-Adapter-Language-Model-for-Medical-Time-Series-Text-Multimodal-Learning"><a href="#MedualTime-A-Dual-Adapter-Language-Model-for-Medical-Time-Series-Text-Multimodal-Learning" class="headerlink" title="MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text   Multimodal Learning"></a>MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text   Multimodal Learning</h2><p><strong>Authors:Jiexia Ye, Weiqi Zhang, Ziyue Li, Jia Li, Meng Zhao, Fugee Tsung</strong></p>
<p>The recent rapid advancements in language models (LMs) have garnered attention in medical time series-text multimodal learning. However, existing contrastive learning-based and prompt-based LM approaches tend to be biased, often assigning a primary role to time series modality while treating text modality as secondary. We classify these approaches under a temporal-primary paradigm, which may overlook the unique and critical task-relevant information embedded in text modality like clinical reports, thus failing to fully leverage mutual benefits and complementarity of different modalities. To fill this gap, we propose a novel textual-temporal multimodal learning paradigm that enables either modality to serve as the primary while being enhanced by the other, thereby effectively capturing modality-specific information and fostering cross-modal interaction. In specific, we design MedualTime, a language model composed of dual adapters to implement temporal-primary and textual-primary modeling simultaneously. Within each adapter, lightweight adaptation tokens are injected into the top layers of LM to encourage high-level modality fusion. The shared LM pipeline by dual adapters not only achieves adapter alignment but also enables efficient fine-tuning, reducing computational resources. Empirically, MedualTime demonstrates superior performance on medical data, achieving notable improvements of 8% accuracy and 12% F1 in supervised settings. Furthermore, MedualTime’s transferability is validated by few-shot label transfer experiments from coarse-grained to fine-grained medical data. <a target="_blank" rel="noopener" href="https://github.com/start2020/MedualTime">https://github.com/start2020/MedualTime</a> </p>
<blockquote>
<p>近期的语言模型（LMs）迅速进展引起了医学时间序列文本多模态学习的关注。然而，现有的基于对比学习和基于提示的语言模型方法往往存在偏见，通常将时间序列模态视为主要模态，而将文本模态视为次要模态。我们将这些方法归类为时间主导范式，这可能会忽视文本模态中嵌入的独特且关键的任务相关信息，如临床报告，从而未能充分利用不同模态的相互优势和互补性。为了填补这一空白，我们提出了一种新的文本时间多模态学习范式，使任一模态都可以作为主要模态，同时受到另一模态的增强，从而有效地捕获特定于模态的信息并促进跨模态交互。具体来说，我们设计了MedualTime，一个由双适配器组成的语言模型，可同时实现时间主导和文本主导建模。在每个适配器中，将轻量级适配令牌注入语言模型顶层，以鼓励高级模态融合。双适配器共享的LM管道不仅实现了适配器对齐，还实现了高效的微调，减少了计算资源。实际上，MedualTime在医疗数据上表现出卓越的性能，在监督设置中的准确率和F1得分分别提高了8%和12%。此外，通过从粗粒度到细粒度的医学数据的少量标签转移实验，验证了MedualTime的可迁移性。<a target="_blank" rel="noopener" href="https://github.com/start2020/MedualTime">https://github.com/start2020/MedualTime</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06620v3">PDF</a> 9 pages, 6 figure, 3 tables</p>
<p><strong>Summary</strong></p>
<p>本文关注语言模型在医学时间序列文本多模态学习中的最新进展。针对现有对比学习和提示语言模型中的时间序列为主要模态、文本为次要模态的问题，提出了一种新的文本时序多模态学习范式。通过设计MedualTime模型，实现了时序为主和文本为主的建模同时进行。该模型通过轻量级适配器注入高层模态融合信息，达到优异性能。此外，该模型在医学数据上表现出卓越性能，并在粗粒度到细粒度的医学数据迁移实验中验证了其迁移能力。有关更多详细信息，请访问GitHub仓库以获取更多资源。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>语言模型在医学时间序列文本多模态学习中取得进展。</li>
<li>现有方法倾向于将时间序列作为主模态，而文本为次要模态，这可能导致任务相关的关键信息被忽视。</li>
<li>提出了一种新的文本时序多模态学习范式，使任一模态都可以作为主要模态，同时得到另一模态的增强。</li>
<li>MedualTime模型实现了时序为主和文本为主的建模同时进行，通过双适配器实现模态对齐和高效微调。</li>
<li>MedualTime模型在医学数据上表现优越，监督设置下准确率和F1得分有显著提高。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06620">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dae07751c109f7fabb3f3cffde80a21b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-821b65a18689fdc2dd711ea5ac4fe4e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d562861c4885eca6756845947e00e4f0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a7232ea14da8cf4d1a5ec0344649ce78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae088beb08fce003d6ed74cd92879424.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38c54ddce1ea173e05d588e42e56ba99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02f2cec5d72fcbdf4678e49df78f1298.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="How-to-build-the-best-medical-image-segmentation-algorithm-using-foundation-models-a-comprehensive-empirical-study-with-Segment-Anything-Model"><a href="#How-to-build-the-best-medical-image-segmentation-algorithm-using-foundation-models-a-comprehensive-empirical-study-with-Segment-Anything-Model" class="headerlink" title="How to build the best medical image segmentation algorithm using   foundation models: a comprehensive empirical study with Segment Anything   Model"></a>How to build the best medical image segmentation algorithm using   foundation models: a comprehensive empirical study with Segment Anything   Model</h2><p><strong>Authors:Hanxue Gu, Haoyu Dong, Jichen Yang, Maciej A. Mazurowski</strong></p>
<p>Automated segmentation is a fundamental medical image analysis task, which enjoys significant advances due to the advent of deep learning. While foundation models have been useful in natural language processing and some vision tasks for some time, the foundation model developed with image segmentation in mind - Segment Anything Model (SAM) - has been developed only recently and has shown similar promise. However, there are still no systematic analyses or “best-practice” guidelines for optimal fine-tuning of SAM for medical image segmentation. This work summarizes existing fine-tuning strategies with various backbone architectures, model components, and fine-tuning algorithms across 18 combinations, and evaluates them on 17 datasets covering all common radiology modalities. Our study reveals that (1) fine-tuning SAM leads to slightly better performance than previous segmentation methods, (2) fine-tuning strategies that use parameter-efficient learning in both the encoder and decoder are superior to other strategies, (3) network architecture has a small impact on final performance, (4) further training SAM with self-supervised learning can improve final model performance. We also demonstrate the ineffectiveness of some methods popular in the literature and further expand our experiments into few-shot and prompt-based settings. Lastly, we released our code and MRI-specific fine-tuned weights, which consistently obtained superior performance over the original SAM, at <a target="_blank" rel="noopener" href="https://github.com/mazurowski-lab/finetune-SAM">https://github.com/mazurowski-lab/finetune-SAM</a>. </p>
<blockquote>
<p>自动分割是医学图像分析中的一项基本任务，得益于深度学习的进步，该任务取得了重大进展。虽然基础模型在自然语言处理和某些视觉任务中已有一段时间的应用，但以图像分割为中心开发的基础模型——任意分割模型（SAM）——是最近才开发的，并显示出同样的潜力。然而，对于SAM在医学图像分割中的最佳微调，仍然没有系统的分析或“最佳实践”指南。这项工作总结了使用各种主干架构、模型组件和微调算法的现有微调策略，跨越了18种组合，并在涵盖所有常见放射学模态的17个数据集上进行了评估。我们的研究表明：（1）微调SAM略微优于以前的分割方法；（2）在编码器和解码器中都使用参数有效学习的微调策略优于其他策略；（3）网络架构对最终性能的影响较小；（4）使用自监督学习进一步训练SAM可以提高最终模型性能。我们还证明了文献中一些流行方法的无效性，并将我们的实验进一步扩展到小样本和基于提示的设置中。最后，我们在<a target="_blank" rel="noopener" href="https://github.com/mazurowski-lab/finetune-SAM%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E9%92%88%E5%AF%B9MRI%E7%9A%84%E7%89%B9%E5%AE%9A%E5%BE%AE%E8%B0%83%E6%9D%83%E9%87%8D%EF%BC%8C%E8%BF%99%E4%BA%9B%E6%9D%83%E9%87%8D%E5%9C%A8%E5%8E%9F%E5%A7%8BSAM%E4%B8%8A%E5%A7%8B%E7%BB%88%E8%8E%B7%E5%BE%97%E4%BA%86%E6%9B%B4%E5%A5%BD%E7%9A%84%E6%80%A7%E8%83%BD%E3%80%82">https://github.com/mazurowski-lab/finetune-SAM上发布了我们的代码和针对MRI的特定微调权重，这些权重在原始SAM上始终获得了更好的性能。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.09957v3">PDF</a> Accepted for publication at the Journal of Machine Learning for   Biomedical Imaging (MELBA)</p>
<p><strong>Summary</strong><br>     深度学习的发展推动了医学图像分析中的自动化分割任务取得显著进展。最近开发的以图像分割为重点的Segment Anything Model（SAM）显示出巨大的潜力。然而，关于如何对SAM进行最佳微调以优化医学图像分割，尚无系统分析和“最佳实践”指南。本研究总结了使用不同主干架构、模型组件和微调算法的现有微调策略，并在涵盖所有常见放射学模态的17个数据集上进行了评估。研究发现，微调SAM略优于以前的方法，使用参数高效学习的微调策略优于其他策略，网络架构对最终性能的影响较小，使用自监督学习进一步训练SAM可以提高最终模型性能。此外，还展示了文献中一些流行方法的有效性不足，并将实验进一步扩展到小样本和基于提示的环境中。最后，发布了代码和针对MRI的微调权重，性能优于原始SAM模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动化分割是医学图像分析中的基本任务，深度学习的发展显著推动了其进展。</li>
<li>Segment Anything Model (SAM)是针对图像分割而开发的，显示出巨大潜力。</li>
<li>目前针对SAM的最佳微调策略尚无系统分析和指南。</li>
<li>研究总结了多种微调策略，包括使用不同的主干架构、模型组件和微调算法。</li>
<li>微调SAM的性能略优于以前的方法。</li>
<li>使用参数高效学习的微调策略在网络架构对最终性能影响较小的情况下表现更佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.09957">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0ef979f362d30c91768edb3cb9cfcbb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0395a23f624f16d17e20aa8bcbbf93a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7b10e1ba15649dc14ded6d0704408d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56e928361467f47b8e5e5155b445b95b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-edee906e9a345f0ac2a02462149c7882.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-05-14  Pixel Motion as Universal Representation for Robot Control
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2cb13f0e8e6169d11abb374d0d4a37a1.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-05-14  TopicVD A Topic-Based Dataset of Video-Guided Multimodal Machine   Translation for Documentaries
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26522.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
