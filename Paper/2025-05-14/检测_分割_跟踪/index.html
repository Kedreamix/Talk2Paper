<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-05-14  DepthFusion Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D   Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4656fa795d076aea34f6e55c4fc8abad.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-14-更新"><a href="#2025-05-14-更新" class="headerlink" title="2025-05-14 更新"></a>2025-05-14 更新</h1><h2 id="DepthFusion-Depth-Aware-Hybrid-Feature-Fusion-for-LiDAR-Camera-3D-Object-Detection"><a href="#DepthFusion-Depth-Aware-Hybrid-Feature-Fusion-for-LiDAR-Camera-3D-Object-Detection" class="headerlink" title="DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D   Object Detection"></a>DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D   Object Detection</h2><p><strong>Authors:Mingqian Ji, Jian Yang, Shanshan Zhang</strong></p>
<p>State-of-the-art LiDAR-camera 3D object detectors usually focus on feature fusion. However, they neglect the factor of depth while designing the fusion strategy. In this work, we are the first to observe that different modalities play different roles as depth varies via statistical analysis and visualization. Based on this finding, we propose a Depth-Aware Hybrid Feature Fusion (DepthFusion) strategy that guides the weights of point cloud and RGB image modalities by introducing depth encoding at both global and local levels. Specifically, the Depth-GFusion module adaptively adjusts the weights of image Bird’s-Eye-View (BEV) features in multi-modal global features via depth encoding. Furthermore, to compensate for the information lost when transferring raw features to the BEV space, we propose a Depth-LFusion module, which adaptively adjusts the weights of original voxel features and multi-view image features in multi-modal local features via depth encoding. Extensive experiments on the nuScenes and KITTI datasets demonstrate that our DepthFusion method surpasses previous state-of-the-art methods. Moreover, our DepthFusion is more robust to various kinds of corruptions, outperforming previous methods on the nuScenes-C dataset. </p>
<blockquote>
<p>前沿的激光雷达相机3D目标检测器通常关注特征融合。然而，在设计融合策略时，它们忽略了深度因素。在这项工作中，我们首次通过统计分析和可视化观察到，随着深度的变化，不同模式起着不同的作用。基于这一发现，我们提出了一种深度感知混合特征融合（DepthFusion）策略，通过在全球和局部层面引入深度编码，来指导点云和RGB图像模式的权重。具体来说，Depth-GFusion模块通过深度编码自适应调整多模态全局特征中图像鸟瞰图（BEV）特征的权重。此外，为了弥补将原始特征转移到BEV空间时丢失的信息，我们提出了Depth-LFusion模块，它通过深度编码自适应调整多模态局部特征中原始体素特征和多视角图像特征的权重。在nuScenes和KITTI数据集上的大量实验表明，我们的DepthFusion方法超越了之前的最先进方法。此外，我们的DepthFusion对各种腐败现象具有更强的鲁棒性，在nuScenes-C数据集上的表现优于以前的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07398v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于激光雷达和摄像头的三维物体检测中，现有技术主要聚焦于特征融合，但忽略了深度因素在设计融合策略中的重要性。本文通过统计分析和可视化发现不同模态在不同深度下扮演着不同的角色。因此，本文提出了一种深度感知混合特征融合策略（DepthFusion），通过引入深度编码，在全局和局部层面指导点云和RGB图像模态的权重。实验证明，本文提出的DepthFusion方法超越了现有技术，对各类失真具有更强的鲁棒性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前先进的激光雷达-摄像头3D物体检测主要关注特征融合，但忽略了深度因素。</li>
<li>本文首次通过统计分析和可视化发现不同模态在不同深度下的不同作用。</li>
<li>提出了一种新的深度感知混合特征融合策略（DepthFusion）。</li>
<li>DepthFusion通过引入深度编码，在全局和局部层面调整点云和RGB图像模态的权重。</li>
<li>Depth-GFusion模块自适应调整图像鸟瞰图特征的权重。</li>
<li>Depth-LFusion模块用于补偿特征从原始空间转移到鸟瞰图空间时的信息损失。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07398">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-59f61378b0b3b0975dfc0908d5d0fa17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2854af5c2e99acce5cdbb7329861c798.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4528bc586188a0f17eae5f03dd14ff6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92a9c42e8d1288b592941d17802673e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18004127418ca426eb4692cae2d5532e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a75681f9a39d62be6178db9ccf4a048.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Language-Driven-Dual-Style-Mixing-for-Single-Domain-Generalized-Object-Detection"><a href="#Language-Driven-Dual-Style-Mixing-for-Single-Domain-Generalized-Object-Detection" class="headerlink" title="Language-Driven Dual Style Mixing for Single-Domain Generalized Object   Detection"></a>Language-Driven Dual Style Mixing for Single-Domain Generalized Object   Detection</h2><p><strong>Authors:Hongda Qin, Xiao Lu, Zhiyong Wei, Yihong Cao, Kailun Yang, Ningjiang Chen</strong></p>
<p>Generalizing an object detector trained on a single domain to multiple unseen domains is a challenging task. Existing methods typically introduce image or feature augmentation to diversify the source domain to raise the robustness of the detector. Vision-Language Model (VLM)-based augmentation techniques have been proven to be effective, but they require that the detector’s backbone has the same structure as the image encoder of VLM, limiting the detector framework selection. To address this problem, we propose Language-Driven Dual Style Mixing (LDDS) for single-domain generalization, which diversifies the source domain by fully utilizing the semantic information of the VLM. Specifically, we first construct prompts to transfer style semantics embedded in the VLM to an image translation network. This facilitates the generation of style diversified images with explicit semantic information. Then, we propose image-level style mixing between the diversified images and source domain images. This effectively mines the semantic information for image augmentation without relying on specific augmentation selections. Finally, we propose feature-level style mixing in a double-pipeline manner, allowing feature augmentation to be model-agnostic and can work seamlessly with the mainstream detector frameworks, including the one-stage, two-stage, and transformer-based detectors. Extensive experiments demonstrate the effectiveness of our approach across various benchmark datasets, including real to cartoon and normal to adverse weather tasks. The source code and pre-trained models will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/qinhongda8/LDDS">https://github.com/qinhongda8/LDDS</a>. </p>
<blockquote>
<p>将单一领域训练的物体检测器推广到多个未见领域是一项具有挑战性的任务。现有方法通常通过图像或特征增强来使源领域多样化，以提高检测器的稳健性。基于视觉语言模型（VLM）的增强技术已被证明是有效的，但它们要求检测器的骨干结构与VLM的图像编码器相同，从而限制了检测器框架的选择。为了解决这一问题，我们提出了用于单一领域推广的语言驱动双重风格混合（LDDS）方法，该方法通过充分利用VLM的语义信息来使源领域多样化。具体来说，我们首先构建提示，将嵌入在VLM中的风格语义信息转移到图像翻译网络中。这有助于生成具有明确语义信息的风格多样化图像。然后，我们提出了多样化图像和源域图像之间的图像级风格混合。这有效地挖掘了用于图像增强的语义信息，而不依赖于特定的增强选择。最后，我们采用双管道方式提出了特征级风格混合，使特征增强成为模型无关，并能无缝地与主流检测器框架协同工作，包括一阶、二阶和基于变压器的检测器。大量实验表明，我们的方法在各种基准数据集上都是有效的，包括从现实到卡通和从正常到恶劣天气任务。源代码和预训练模型将在<a target="_blank" rel="noopener" href="https://github.com/qinhongda8/LDDS">https://github.com/qinhongda8/LDDS</a>上公开提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07219v1">PDF</a> The source code and pre-trained models will be publicly available at   <a target="_blank" rel="noopener" href="https://github.com/qinhongda8/LDDS">https://github.com/qinhongda8/LDDS</a></p>
<p><strong>摘要</strong></p>
<p>利用视觉语言模型（VLM）的语义信息提升单域目标检测器在多域中的通用化性能是一大挑战。当前方法主要通过图像或特征增强技术丰富源域数据来提升检测器鲁棒性。尽管基于VLM的增强技术已证明有效，但受限于检测器架构必须与图像编码器的结构相同。为解决此问题，我们提出语言驱动的双风格混合（LDDS）方法用于单域通用化，通过充分利用VLM的语义信息丰富源域数据。首先构建提示以将VLM中的风格语义转移到图像翻译网络中，生成具有明确语义信息的风格多样化图像。接着提出图像级别的风格混合技术，有效挖掘多样化图像的语义信息，无需特定增强选择。最后，采用双管道方式进行特征级别的风格混合，使特征增强具有模型无关性，并能无缝配合主流检测器框架。大量实验证明我们的方法在各种基准数据集上的有效性，包括真实到卡通和正常到恶劣天气任务。相关源代码和预训练模型将在<a target="_blank" rel="noopener" href="https://github.com/qinhongda8/LDDS%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/qinhongda8/LDDS公开提供。</a></p>
<p><strong>关键发现</strong></p>
<ol>
<li>提出了一种新的单域通用化方法——语言驱动的双风格混合（LDDS），以解决将单一域训练的检测器推广到多个未见域的挑战。</li>
<li>通过利用视觉语言模型（VLM）的语义信息丰富源域数据，提高了检测器的鲁棒性。</li>
<li>提出构建提示以转移VLM中的风格语义到图像翻译网络，生成具有明确语义信息的风格多样化图像。</li>
<li>引入图像级别的风格混合技术，有效挖掘多样化图像的语义信息，无需特定增强选择。</li>
<li>采用双管道方式进行特征级别的风格混合，使得特征增强具有模型无关性，并适应主流检测器框架。</li>
<li>在多个基准数据集上进行了广泛实验，证明了方法的有效性，包括真实到卡通和正常到恶劣天气任务的应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07219">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0a168f79ee222bff299c40b92d6f64b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80f140c1e28d7068e733a50877d55330.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0438606acbea2776ffbf533713c5d631.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8329c4c921b3f07241a2c1da4b5fda6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b41952cf94a4cf4614c78f4e5d2d3914.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9eab95fc0d4912a1022736762fbc3c6f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Boosting-Cross-spectral-Unsupervised-Domain-Adaptation-for-Thermal-Semantic-Segmentation"><a href="#Boosting-Cross-spectral-Unsupervised-Domain-Adaptation-for-Thermal-Semantic-Segmentation" class="headerlink" title="Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal   Semantic Segmentation"></a>Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal   Semantic Segmentation</h2><p><strong>Authors:Seokjun Kwon, Jeongmin Shin, Namil Kim, Soonmin Hwang, Yukyung Choi</strong></p>
<p>In autonomous driving, thermal image semantic segmentation has emerged as a critical research area, owing to its ability to provide robust scene understanding under adverse visual conditions. In particular, unsupervised domain adaptation (UDA) for thermal image segmentation can be an efficient solution to address the lack of labeled thermal datasets. Nevertheless, since these methods do not effectively utilize the complementary information between RGB and thermal images, they significantly decrease performance during domain adaptation. In this paper, we present a comprehensive study on cross-spectral UDA for thermal image semantic segmentation. We first propose a novel masked mutual learning strategy that promotes complementary information exchange by selectively transferring results between each spectral model while masking out uncertain regions. Additionally, we introduce a novel prototypical self-supervised loss designed to enhance the performance of the thermal segmentation model in nighttime scenarios. This approach addresses the limitations of RGB pre-trained networks, which cannot effectively transfer knowledge under low illumination due to the inherent constraints of RGB sensors. In experiments, our method achieves higher performance over previous UDA methods and comparable performance to state-of-the-art supervised methods. </p>
<blockquote>
<p>在自动驾驶领域，热图像语义分割已成为一个关键的研究方向，因为它能在恶劣的视觉条件下提供稳健的场景理解。特别是，针对热图像分割的无监督域自适应（UDA）可以作为一种有效的解决方案来解决标记热数据集缺乏的问题。然而，由于这些方法不能有效地利用RGB和热图像之间的互补信息，它们在域自适应过程中会降低性能。在本文中，我们对跨光谱UDA在热图像语义分割方面进行了深入研究。首先，我们提出了一种新颖的掩模互学习策略，通过有选择地转移每个光谱模型之间的结果并掩盖不确定区域，从而促进互补信息的交换。此外，我们还引入了一种新型的原型自监督损失，旨在提高夜间场景中的热分割模型性能。该方法解决了RGB预训练网络的局限性，由于RGB传感器的固有约束，这些网络在低光照条件下无法有效地转移知识。在实验中，我们的方法取得了超过以前UDA方法的高性能和与最新监督方法的相当性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06951v1">PDF</a> 7 pages, 4 figures, International Conference on Robotics and   Automation(ICRA) 2025</p>
<p><strong>Summary</strong></p>
<p>基于无监督域自适应方法的优势，本研究探讨交叉光谱自适应语义分割技术对于热图像自主驾驶技术的影响。本研究提出一种新颖的掩膜互学习机制，促进RGB与热图像之间的互补信息交换，同时引入原型自监督损失以增强夜间场景下的热图像分割性能。该方法解决了基于RGB的预训练网络在低光照环境下的知识迁移难题。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>研究聚焦于无监督域自适应（UDA）在热图像语义分割中的应用。</li>
<li>提出一种新颖的掩膜互学习机制，用于选择性地在光谱模型之间转移结果，同时掩盖不确定区域。这种机制促进RGB和热力图像的互补信息交换。</li>
<li>为提高夜间场景下的热图像分割性能，引入原型自监督损失。这一损失旨在增强模型在低光照环境下的性能表现。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06951">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c61a03816928928670a3c617431cdd44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4152adef618403d9bb552a63b4f9ec8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad6db9e16c0a148ad46ec3436ab7a2d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70df984500050810f0a775e6d9eb074f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-133ca95420ba96f3843c340f5e083168.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b14546b5c65987835a709b175cb0ff17.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Underwater-object-detection-in-sonar-imagery-with-detection-transformer-and-Zero-shot-neural-architecture-search"><a href="#Underwater-object-detection-in-sonar-imagery-with-detection-transformer-and-Zero-shot-neural-architecture-search" class="headerlink" title="Underwater object detection in sonar imagery with detection transformer   and Zero-shot neural architecture search"></a>Underwater object detection in sonar imagery with detection transformer   and Zero-shot neural architecture search</h2><p><strong>Authors:XiaoTong Gu, Shengyu Tang, Yiming Cao, Changdong Yu</strong></p>
<p>Underwater object detection using sonar imagery has become a critical and rapidly evolving research domain within marine technology. However, sonar images are characterized by lower resolution and sparser features compared to optical images, which seriously degrades the performance of object detection.To address these challenges, we specifically propose a Detection Transformer (DETR) architecture optimized with a Neural Architecture Search (NAS) approach called NAS-DETR for object detection in sonar images. First, an improved Zero-shot Neural Architecture Search (NAS) method based on the maximum entropy principle is proposed to identify a real-time, high-representational-capacity CNN-Transformer backbone for sonar image detection. This method enables the efficient discovery of high-performance network architectures with low computational and time overhead. Subsequently, the backbone is combined with a Feature Pyramid Network (FPN) and a deformable attention-based Transformer decoder to construct a complete network architecture. This architecture integrates various advanced components and training schemes to enhance overall performance. Extensive experiments demonstrate that this architecture achieves state-of-the-art performance on two Representative datasets, while maintaining minimal overhead in real-time efficiency and computational complexity. Furthermore, correlation analysis between the key parameters and differential entropy-based fitness function is performed to enhance the interpretability of the proposed framework. To the best of our knowledge, this is the first work in the field of sonar object detection to integrate the DETR architecture with a NAS search mechanism. </p>
<blockquote>
<p>使用声纳图像进行水下目标检测已成为海洋技术中一个关键且快速发展的研究领域。然而，与光学图像相比，声纳图像的特征分辨率较低、特征较稀疏，这严重降低了目标检测的性能。为了解决这些挑战，我们特地提出了一种使用神经网络架构搜索（NAS）方法优化的检测转换器（DETR）架构，称为NAS-DETR，用于声纳图像中的目标检测。首先，我们提出了一种基于最大熵原理的改进零镜头神经网络架构搜索（NAS）方法，用于识别适用于声纳图像检测的实时、高表征容量CNN-Transformer骨干网。该方法能够高效地发现具有低计算和时间开销的高性能网络架构。随后，将骨干网与特征金字塔网络（FPN）和基于可变形注意力的Transformer解码器相结合，构建完整的网络架构。该架构集成了各种先进组件和训练方案，以提高整体性能。大量实验表明，该架构在两个代表性数据集上实现了最先进的性能，同时保持了实时效率和计算复杂度的最低开销。此外，还对关键参数与基于差异熵的适应度函数进行了相关性分析，以提高所提出框架的可解释性。据我们所知，这是声纳目标检测领域首次将DETR架构与NAS搜索机制相结合的工作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06694v1">PDF</a> </p>
<p><strong>Summary</strong><br>     针对水下物体检测的挑战，提出一种基于神经网络架构搜索（NAS）的优化检测转换器（DETR）架构，称为NAS-DETR。它采用改进的零镜头NAS方法，结合特征金字塔网络（FPN）和可变形注意力基础的转换器解码器，构建完整网络架构。在两项代表性数据集上实现最先进的性能，同时保持实时效率和计算复杂度的最小化。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>水下物体检测是海洋技术中关键且快速演化的研究领域。</li>
<li>声纳图像具有较低分辨率和稀疏特征，给物体检测带来挑战。</li>
<li>提出基于神经网络架构搜索（NAS）的优化检测转换器（DETR）架构，称为NAS-DETR，用于声纳图像中的物体检测。</li>
<li>改进的零镜头NAS方法基于最大熵原则，能高效发现高性能网络架构，具有低计算和时间开销。</li>
<li>结合FPN和可变形注意力基础的转换器解码器，构建完整网络架构，提高整体性能。</li>
<li>在两个代表性数据集上实现最先进的性能，同时保持实时效率和计算复杂度的平衡。</li>
<li>通过关键参数与差异熵基于的适应度函数之间的相关性分析，提高框架的可解释性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06694">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a5df4b6a62a885e348e2c5c5bddac6e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86e0721420416b88265ded0a05b3bdfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a52f341e3095158e08d9cff2ca22549.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89de8f4333c0b3dd12e7a8e82067e560.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MonoCoP-Chain-of-Prediction-for-Monocular-3D-Object-Detection"><a href="#MonoCoP-Chain-of-Prediction-for-Monocular-3D-Object-Detection" class="headerlink" title="MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection"></a>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</h2><p><strong>Authors:Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu</strong></p>
<p>Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets. </p>
<blockquote>
<p>准确预测3D属性对于单目3D对象检测（Mono3D）至关重要，深度估计由于将2D图像映射到3D空间时固有的模糊性而面临最大挑战。尽管现有方法利用多种深度线索（例如估计深度不确定性、建模深度误差）来提高深度准确性，但它们忽略了准确的深度预测需要依赖于其他3D属性，因为这些属性通过3D到2D的投影本质上相互关联，这最终限制了总体准确性和稳定性。本文受到大型语言模型（LLM）中的思维链（Chain-of-Thought，CoT）的启发，提出了一种利用预测链（Chain-of-Prediction，CoP）进行属性顺序预测和条件预测的方法MonoCoP，其通过三个关键设计实现：首先，它为每个3D属性采用轻量级的AttributeNet（AN）来学习特定于属性的特征；其次，MonoCoP构建了一个明确的链条来传播从一个属性学到的特征到下一个属性；最后，MonoCoP使用残差连接来沿着链条聚合每个属性的特征，确保后续的属性预测依赖于所有先前处理的属性，同时不会忘记早期的属性特征。实验结果表明，我们的MonoCoP在KITTI排行榜上达到了最新技术水平，并且无需额外数据，在Waymo和nuScenes正面数据集上也超越了现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04594v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于Chain-of-Prediction（CoP）的单眼三维物体检测新方法MonoCoP。该方法通过构建预测链，顺序且条件地预测三维属性。它通过为每个三维属性设计轻量级的AttributeNet（AN）来学习特定属性特征，并通过显式链传播这些特征。此外，MonoCoP采用残差连接来汇聚各属性的特征，确保后续属性预测建立在所有先前处理过的属性之上。实验结果显示，MonoCoP在KITTI排行榜上达到最新技术水平，且在Waymo和nuScenes正面数据集上超越现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MonoCoP利用Chain-of-Prediction（CoP）方法顺序且条件地预测三维属性，这与传统方法不同。</li>
<li>通过AttributeNet（AN）为每个三维属性学习特定特征。</li>
<li>通过显式预测链传播特征，增强属性间的关联性。</li>
<li>残差连接确保后续属性预测建立在所有先前处理过的属性之上，避免遗忘早期特征。</li>
<li>MonoCoP在KITTI、Waymo和nuScenes数据集上实现先进性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04594">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ad2fd13cc5a4dbf34f8ec0a97ea9988b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4656fa795d076aea34f6e55c4fc8abad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d6169567c1dc4abeac209b7307d9785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4fbd105d5b65f5ad6dbead5d93a91e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4e1fb2b9073d0dc3996c41cd4d610d7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance"><a href="#Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance" class="headerlink" title="Enhanced Generative Data Augmentation for Semantic Segmentation via   Stronger Guidance"></a>Enhanced Generative Data Augmentation for Semantic Segmentation via   Stronger Guidance</h2><p><strong>Authors:Quang-Huy Che, Duc-Tri Le, Bich-Nga Pham, Duc-Khai Lam, Vinh-Tiep Nguyen</strong></p>
<p>Data augmentation is crucial for pixel-wise annotation tasks like semantic segmentation, where labeling requires significant effort and intensive labor. Traditional methods, involving simple transformations such as rotations and flips, create new images but often lack diversity along key semantic dimensions and fail to alter high-level semantic properties. To address this issue, generative models have emerged as an effective solution for augmenting data by generating synthetic images. Controllable Generative models offer data augmentation methods for semantic segmentation tasks by using prompts and visual references from the original image. However, these models face challenges in generating synthetic images that accurately reflect the content and structure of the original image due to difficulties in creating effective prompts and visual references. In this work, we introduce an effective data augmentation pipeline for semantic segmentation using Controllable Diffusion model. Our proposed method includes efficient prompt generation using \textit{Class-Prompt Appending} and \textit{Visual Prior Blending} to enhance attention to labeled classes in real images, allowing the pipeline to generate a precise number of augmented images while preserving the structure of segmentation-labeled classes. In addition, we implement a \textit{class balancing algorithm} to ensure a balanced training dataset when merging the synthetic and original images. Evaluation on PASCAL VOC datasets, our pipeline demonstrates its effectiveness in generating high-quality synthetic images for semantic segmentation. Our code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance%7D%7Bthis">https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance}{this</a> https URL}. </p>
<blockquote>
<p>数据增强对于像素级注释任务（如语义分割）至关重要，这些任务需要大量劳动和努力来进行标注。传统的方法，涉及简单的转换（如旋转和翻转），可以创建新的图像，但往往在关键的语义维度上缺乏多样性，并且无法改变高级语义属性。为了解决这个问题，生成模型已经出现为通过生成合成图像来增强数据的有效解决方案。可控生成模型通过使用原始图像的提示和视觉参考来为语义分割任务提供数据增强方法。然而，这些模型在生成准确反映原始图像内容和结构的合成图像时面临挑战，因为创建有效的提示和视觉参考存在困难。在这项工作中，我们引入了使用可控扩散模型对语义分割进行有效数据增强的管道。我们提出的方法包括使用“类提示追加”和“视觉先验混合”进行有效的提示生成，以提高对真实图像中标记类的关注，使管道能够在保留分割标记类结构的同时生成精确数量的增强图像。此外，我们实现了“类别平衡算法”，以确保在合并合成图像和原始图像时获得平衡的训练数据集。在PASCAL VOC数据集上的评估表明，我们的管道在生成用于语义分割的高质量合成图像方面非常有效。我们的代码可以在这个URL的网址找到：<a target="_blank" rel="noopener" href="https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance">https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06002v4">PDF</a> </p>
<p><strong>Summary</strong><br>数据增强对于像素级标注任务如语义分割至关重要，传统方法缺乏多样性。生成模型通过生成合成图像有效解决了数据增强问题，但可控生成模型面临准确反映原始图像内容和结构的挑战。本文引入了一种使用可控扩散模型的语义分割数据增强管道，通过高效提示生成和视觉先验融合，在真实图像中增强对标记类的注意力，同时保证合成图像的数量和质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据增强在像素级标注任务中非常重要，尤其对于语义分割，能减少标注的劳动成本。</li>
<li>传统数据增强方法如旋转和翻转虽然能创建新图像，但缺乏关键语义维度上的多样性。</li>
<li>生成模型，特别是可控生成模型，已被证明是数据增强的有效解决方案。</li>
<li>可控生成模型面临准确反映原始图像内容和结构的挑战，需要有效的提示和视觉参考。</li>
<li>本文介绍了一种新的数据增强管道，结合类提示添加和视觉先验融合，用于语义分割任务。</li>
<li>该管道在PASCAL VOC数据集上的评估结果证明了其生成高质量合成图像的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06002">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3c1716bef1d0d6ad24dfb909fb50fe68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f65bfd9b2b7e788b5f54c9643a7800c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91dd43e3d48806df3e2fa23090a86993.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b3c65fb7936a29984cc349b14779630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b79cbb1b35476816ef186a87a8ee7de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c298870b43ce121bb270ff9dab12e16e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="BihoT-A-Large-Scale-Dataset-and-Benchmark-for-Hyperspectral-Camouflaged-Object-Tracking"><a href="#BihoT-A-Large-Scale-Dataset-and-Benchmark-for-Hyperspectral-Camouflaged-Object-Tracking" class="headerlink" title="BihoT: A Large-Scale Dataset and Benchmark for Hyperspectral Camouflaged   Object Tracking"></a>BihoT: A Large-Scale Dataset and Benchmark for Hyperspectral Camouflaged   Object Tracking</h2><p><strong>Authors:Hanzheng Wang, Wei Li, Xiang-Gen Xia, Qian Du</strong></p>
<p>Hyperspectral object tracking (HOT) has exhibited potential in various applications, particularly in scenes where objects are camouflaged. Existing trackers can effectively retrieve objects via band regrouping because of the bias in existing HOT datasets, where most objects tend to have distinguishing visual appearances rather than spectral characteristics. This bias allows the tracker to directly use the visual features obtained from the false-color images generated by hyperspectral images without the need to extract spectral features. To tackle this bias, we find that the tracker should focus on the spectral information when object appearance is unreliable. Thus, we provide a new task called hyperspectral camouflaged object tracking (HCOT) and meticulously construct a large-scale HCOT dataset, termed BihoT, which consists of 41,912 hyperspectral images covering 49 video sequences. The dataset covers various artificial camouflage scenes where objects have similar appearances, diverse spectrums, and frequent occlusion, making it a very challenging dataset for HCOT. Besides, a simple but effective baseline model, named spectral prompt-based distractor-aware network (SPDAN), is proposed, comprising a spectral embedding network (SEN), a spectral prompt-based backbone network (SPBN), and a distractor-aware module (DAM). Specifically, the SEN extracts spectral-spatial features via 3-D and 2-D convolutions. Then, the SPBN fine-tunes powerful RGB trackers with spectral prompts and alleviates the insufficiency of training samples. Moreover, the DAM utilizes a novel statistic to capture the distractor caused by occlusion from objects and background. Extensive experiments demonstrate that our proposed SPDAN achieves state-of-the-art performance on the proposed BihoT and other HOT datasets. </p>
<blockquote>
<p>高光谱目标跟踪（HOT）在各种应用中已显示出潜力，特别是在目标伪装场景中。现有的跟踪器可以通过波段重组有效地检索目标，这是因为现有HOT数据集存在的偏见，大多数目标往往具有区分度的是视觉外观而不是光谱特征。这种偏见使得跟踪器能够直接使用从高光谱图像生成的假彩色图像获得的视觉特征，而无需提取光谱特征。为了解决这种偏见，我们发现当目标外观不可靠时，跟踪器应专注于光谱信息。因此，我们提供了一个新的任务，称为高光谱伪装目标跟踪（HCOT），并精心构建了一个大规模的HCOT数据集，名为BihoT，由覆盖49个视频序列的41912个高光谱图像组成。该数据集涵盖了各种人工伪装场景，其中目标具有相似的外观、多样的光谱和频繁遮挡，使其成为HCOT非常有挑战性的数据集。此外，提出了一个简单有效的基线模型，称为基于光谱提示的分心网络（SPDAN），它由光谱嵌入网络（SEN）、基于光谱提示的主干网络（SPBN）和分心模块（DAM）组成。具体而言，SEN通过3D和2D卷积提取光谱空间特征。然后，SPBN利用光谱提示微调功能强大的RGB跟踪器并缓解训练样本不足的问题。此外，DAM利用一种新型统计方法来捕捉由目标和背景遮挡造成的分心物。大量实验表明，我们提出的SPDAN在提出的BihoT和其他HOT数据集上达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12232v2">PDF</a> IEEE Transactions on Neural Networks and Learning Systems, 2025</p>
<p><strong>摘要</strong><br>    本文介绍了超光谱伪装目标跟踪（HCOT）的新任务及其挑战。针对现有超光谱目标跟踪（HOT）数据集的偏见问题，提出了大型HCOT数据集BihoT的构建方法，包含各种伪装场景下的复杂变化目标。同时，提出了一种简单有效的基线模型SPDAN，能够在数据集上取得一流性能。SPDAN包含光谱嵌入网络、光谱提示背景网络和干扰识别模块。其中，光谱嵌入网络通过三维和二维卷积提取光谱空间特征；光谱提示背景网络通过引入光谱提示优化RGB跟踪器；干扰识别模块能够捕捉遮挡引起的干扰。</p>
<p><strong>关键发现</strong></p>
<ul>
<li>现有超光谱目标跟踪（HOT）数据集存在偏见，更关注目标视觉特征而非光谱特性。</li>
<li>提出新的任务：超光谱伪装目标跟踪（HCOT），以解决现有偏见问题。</li>
<li>构建大型HCOT数据集BihoT，包含复杂伪装场景下的目标图像和视频序列。</li>
<li>SPDAN基线模型包括光谱嵌入网络（SEN）、光谱提示背景网络（SPBN）和干扰识别模块（DAM）。</li>
<li>SEN通过三维和二维卷积提取光谱空间特征。</li>
<li>SPBN通过引入光谱提示优化RGB跟踪器，缓解训练样本不足的问题。</li>
<li>DAM能够捕捉遮挡引起的干扰，提高跟踪性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12232">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b819918bdedf8a15155d141ec965d9e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a1ad3ba1681d06ea90149c8a1e40400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d30d485ecfc40368c3635b28ea57ab6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc592dea092816c09f7a5fd822a94cd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edd911bf81474959d2371e7e519307a0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4bbca98f559e0b07fdb1fa04b135015d.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-05-14  My Emotion on your face The use of Facial Keypoint Detection to   preserve Emotions in Latent Space Editing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-7c03550ec50e2ce6f1ca34536b68e039.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-05-14  Breast Cancer Classification in Deep Ultraviolet Fluorescence Images   Using a Patch-Level Vision Transformer Framework
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27768.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
