<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  DepthFusion Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D   Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4656fa795d076aea34f6e55c4fc8abad.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    30 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-14-æ›´æ–°"><a href="#2025-05-14-æ›´æ–°" class="headerlink" title="2025-05-14 æ›´æ–°"></a>2025-05-14 æ›´æ–°</h1><h2 id="DepthFusion-Depth-Aware-Hybrid-Feature-Fusion-for-LiDAR-Camera-3D-Object-Detection"><a href="#DepthFusion-Depth-Aware-Hybrid-Feature-Fusion-for-LiDAR-Camera-3D-Object-Detection" class="headerlink" title="DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D   Object Detection"></a>DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D   Object Detection</h2><p><strong>Authors:Mingqian Ji, Jian Yang, Shanshan Zhang</strong></p>
<p>State-of-the-art LiDAR-camera 3D object detectors usually focus on feature fusion. However, they neglect the factor of depth while designing the fusion strategy. In this work, we are the first to observe that different modalities play different roles as depth varies via statistical analysis and visualization. Based on this finding, we propose a Depth-Aware Hybrid Feature Fusion (DepthFusion) strategy that guides the weights of point cloud and RGB image modalities by introducing depth encoding at both global and local levels. Specifically, the Depth-GFusion module adaptively adjusts the weights of image Birdâ€™s-Eye-View (BEV) features in multi-modal global features via depth encoding. Furthermore, to compensate for the information lost when transferring raw features to the BEV space, we propose a Depth-LFusion module, which adaptively adjusts the weights of original voxel features and multi-view image features in multi-modal local features via depth encoding. Extensive experiments on the nuScenes and KITTI datasets demonstrate that our DepthFusion method surpasses previous state-of-the-art methods. Moreover, our DepthFusion is more robust to various kinds of corruptions, outperforming previous methods on the nuScenes-C dataset. </p>
<blockquote>
<p>å‰æ²¿çš„æ¿€å…‰é›·è¾¾ç›¸æœº3Dç›®æ ‡æ£€æµ‹å™¨é€šå¸¸å…³æ³¨ç‰¹å¾èåˆã€‚ç„¶è€Œï¼Œåœ¨è®¾è®¡èåˆç­–ç•¥æ—¶ï¼Œå®ƒä»¬å¿½ç•¥äº†æ·±åº¦å› ç´ ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡é€šè¿‡ç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ–è§‚å¯Ÿåˆ°ï¼Œéšç€æ·±åº¦çš„å˜åŒ–ï¼Œä¸åŒæ¨¡å¼èµ·ç€ä¸åŒçš„ä½œç”¨ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ·±åº¦æ„ŸçŸ¥æ··åˆç‰¹å¾èåˆï¼ˆDepthFusionï¼‰ç­–ç•¥ï¼Œé€šè¿‡åœ¨å…¨çƒå’Œå±€éƒ¨å±‚é¢å¼•å…¥æ·±åº¦ç¼–ç ï¼Œæ¥æŒ‡å¯¼ç‚¹äº‘å’ŒRGBå›¾åƒæ¨¡å¼çš„æƒé‡ã€‚å…·ä½“æ¥è¯´ï¼ŒDepth-GFusionæ¨¡å—é€šè¿‡æ·±åº¦ç¼–ç è‡ªé€‚åº”è°ƒæ•´å¤šæ¨¡æ€å…¨å±€ç‰¹å¾ä¸­å›¾åƒé¸Ÿç°å›¾ï¼ˆBEVï¼‰ç‰¹å¾çš„æƒé‡ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¼¥è¡¥å°†åŸå§‹ç‰¹å¾è½¬ç§»åˆ°BEVç©ºé—´æ—¶ä¸¢å¤±çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬æå‡ºäº†Depth-LFusionæ¨¡å—ï¼Œå®ƒé€šè¿‡æ·±åº¦ç¼–ç è‡ªé€‚åº”è°ƒæ•´å¤šæ¨¡æ€å±€éƒ¨ç‰¹å¾ä¸­åŸå§‹ä½“ç´ ç‰¹å¾å’Œå¤šè§†è§’å›¾åƒç‰¹å¾çš„æƒé‡ã€‚åœ¨nuSceneså’ŒKITTIæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DepthFusionæ–¹æ³•è¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„DepthFusionå¯¹å„ç§è…è´¥ç°è±¡å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ï¼Œåœ¨nuScenes-Cæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07398v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ¿€å…‰é›·è¾¾å’Œæ‘„åƒå¤´çš„ä¸‰ç»´ç‰©ä½“æ£€æµ‹ä¸­ï¼Œç°æœ‰æŠ€æœ¯ä¸»è¦èšç„¦äºç‰¹å¾èåˆï¼Œä½†å¿½ç•¥äº†æ·±åº¦å› ç´ åœ¨è®¾è®¡èåˆç­–ç•¥ä¸­çš„é‡è¦æ€§ã€‚æœ¬æ–‡é€šè¿‡ç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ–å‘ç°ä¸åŒæ¨¡æ€åœ¨ä¸åŒæ·±åº¦ä¸‹æ‰®æ¼”ç€ä¸åŒçš„è§’è‰²ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ·±åº¦æ„ŸçŸ¥æ··åˆç‰¹å¾èåˆç­–ç•¥ï¼ˆDepthFusionï¼‰ï¼Œé€šè¿‡å¼•å…¥æ·±åº¦ç¼–ç ï¼Œåœ¨å…¨å±€å’Œå±€éƒ¨å±‚é¢æŒ‡å¯¼ç‚¹äº‘å’ŒRGBå›¾åƒæ¨¡æ€çš„æƒé‡ã€‚å®éªŒè¯æ˜ï¼Œæœ¬æ–‡æå‡ºçš„DepthFusionæ–¹æ³•è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œå¯¹å„ç±»å¤±çœŸå…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å…ˆè¿›çš„æ¿€å…‰é›·è¾¾-æ‘„åƒå¤´3Dç‰©ä½“æ£€æµ‹ä¸»è¦å…³æ³¨ç‰¹å¾èåˆï¼Œä½†å¿½ç•¥äº†æ·±åº¦å› ç´ ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡é€šè¿‡ç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ–å‘ç°ä¸åŒæ¨¡æ€åœ¨ä¸åŒæ·±åº¦ä¸‹çš„ä¸åŒä½œç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦æ„ŸçŸ¥æ··åˆç‰¹å¾èåˆç­–ç•¥ï¼ˆDepthFusionï¼‰ã€‚</li>
<li>DepthFusioné€šè¿‡å¼•å…¥æ·±åº¦ç¼–ç ï¼Œåœ¨å…¨å±€å’Œå±€éƒ¨å±‚é¢è°ƒæ•´ç‚¹äº‘å’ŒRGBå›¾åƒæ¨¡æ€çš„æƒé‡ã€‚</li>
<li>Depth-GFusionæ¨¡å—è‡ªé€‚åº”è°ƒæ•´å›¾åƒé¸Ÿç°å›¾ç‰¹å¾çš„æƒé‡ã€‚</li>
<li>Depth-LFusionæ¨¡å—ç”¨äºè¡¥å¿ç‰¹å¾ä»åŸå§‹ç©ºé—´è½¬ç§»åˆ°é¸Ÿç°å›¾ç©ºé—´æ—¶çš„ä¿¡æ¯æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07398">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-59f61378b0b3b0975dfc0908d5d0fa17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2854af5c2e99acce5cdbb7329861c798.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4528bc586188a0f17eae5f03dd14ff6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92a9c42e8d1288b592941d17802673e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18004127418ca426eb4692cae2d5532e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a75681f9a39d62be6178db9ccf4a048.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Language-Driven-Dual-Style-Mixing-for-Single-Domain-Generalized-Object-Detection"><a href="#Language-Driven-Dual-Style-Mixing-for-Single-Domain-Generalized-Object-Detection" class="headerlink" title="Language-Driven Dual Style Mixing for Single-Domain Generalized Object   Detection"></a>Language-Driven Dual Style Mixing for Single-Domain Generalized Object   Detection</h2><p><strong>Authors:Hongda Qin, Xiao Lu, Zhiyong Wei, Yihong Cao, Kailun Yang, Ningjiang Chen</strong></p>
<p>Generalizing an object detector trained on a single domain to multiple unseen domains is a challenging task. Existing methods typically introduce image or feature augmentation to diversify the source domain to raise the robustness of the detector. Vision-Language Model (VLM)-based augmentation techniques have been proven to be effective, but they require that the detectorâ€™s backbone has the same structure as the image encoder of VLM, limiting the detector framework selection. To address this problem, we propose Language-Driven Dual Style Mixing (LDDS) for single-domain generalization, which diversifies the source domain by fully utilizing the semantic information of the VLM. Specifically, we first construct prompts to transfer style semantics embedded in the VLM to an image translation network. This facilitates the generation of style diversified images with explicit semantic information. Then, we propose image-level style mixing between the diversified images and source domain images. This effectively mines the semantic information for image augmentation without relying on specific augmentation selections. Finally, we propose feature-level style mixing in a double-pipeline manner, allowing feature augmentation to be model-agnostic and can work seamlessly with the mainstream detector frameworks, including the one-stage, two-stage, and transformer-based detectors. Extensive experiments demonstrate the effectiveness of our approach across various benchmark datasets, including real to cartoon and normal to adverse weather tasks. The source code and pre-trained models will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/qinhongda8/LDDS">https://github.com/qinhongda8/LDDS</a>. </p>
<blockquote>
<p>å°†å•ä¸€é¢†åŸŸè®­ç»ƒçš„ç‰©ä½“æ£€æµ‹å™¨æ¨å¹¿åˆ°å¤šä¸ªæœªè§é¢†åŸŸæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡å›¾åƒæˆ–ç‰¹å¾å¢å¼ºæ¥ä½¿æºé¢†åŸŸå¤šæ ·åŒ–ï¼Œä»¥æé«˜æ£€æµ‹å™¨çš„ç¨³å¥æ€§ã€‚åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¢å¼ºæŠ€æœ¯å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å®ƒä»¬è¦æ±‚æ£€æµ‹å™¨çš„éª¨å¹²ç»“æ„ä¸VLMçš„å›¾åƒç¼–ç å™¨ç›¸åŒï¼Œä»è€Œé™åˆ¶äº†æ£€æµ‹å™¨æ¡†æ¶çš„é€‰æ‹©ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºå•ä¸€é¢†åŸŸæ¨å¹¿çš„è¯­è¨€é©±åŠ¨åŒé‡é£æ ¼æ··åˆï¼ˆLDDSï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å……åˆ†åˆ©ç”¨VLMçš„è¯­ä¹‰ä¿¡æ¯æ¥ä½¿æºé¢†åŸŸå¤šæ ·åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºæç¤ºï¼Œå°†åµŒå…¥åœ¨VLMä¸­çš„é£æ ¼è¯­ä¹‰ä¿¡æ¯è½¬ç§»åˆ°å›¾åƒç¿»è¯‘ç½‘ç»œä¸­ã€‚è¿™æœ‰åŠ©äºç”Ÿæˆå…·æœ‰æ˜ç¡®è¯­ä¹‰ä¿¡æ¯çš„é£æ ¼å¤šæ ·åŒ–å›¾åƒã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ ·åŒ–å›¾åƒå’ŒæºåŸŸå›¾åƒä¹‹é—´çš„å›¾åƒçº§é£æ ¼æ··åˆã€‚è¿™æœ‰æ•ˆåœ°æŒ–æ˜äº†ç”¨äºå›¾åƒå¢å¼ºçš„è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œä¸ä¾èµ–äºç‰¹å®šçš„å¢å¼ºé€‰æ‹©ã€‚æœ€åï¼Œæˆ‘ä»¬é‡‡ç”¨åŒç®¡é“æ–¹å¼æå‡ºäº†ç‰¹å¾çº§é£æ ¼æ··åˆï¼Œä½¿ç‰¹å¾å¢å¼ºæˆä¸ºæ¨¡å‹æ— å…³ï¼Œå¹¶èƒ½æ— ç¼åœ°ä¸ä¸»æµæ£€æµ‹å™¨æ¡†æ¶ååŒå·¥ä½œï¼ŒåŒ…æ‹¬ä¸€é˜¶ã€äºŒé˜¶å’ŒåŸºäºå˜å‹å™¨çš„æ£€æµ‹å™¨ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åŸºå‡†æ•°æ®é›†ä¸Šéƒ½æ˜¯æœ‰æ•ˆçš„ï¼ŒåŒ…æ‹¬ä»ç°å®åˆ°å¡é€šå’Œä»æ­£å¸¸åˆ°æ¶åŠ£å¤©æ°”ä»»åŠ¡ã€‚æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/qinhongda8/LDDS">https://github.com/qinhongda8/LDDS</a>ä¸Šå…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07219v1">PDF</a> The source code and pre-trained models will be publicly available at   <a target="_blank" rel="noopener" href="https://github.com/qinhongda8/LDDS">https://github.com/qinhongda8/LDDS</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è¯­ä¹‰ä¿¡æ¯æå‡å•åŸŸç›®æ ‡æ£€æµ‹å™¨åœ¨å¤šåŸŸä¸­çš„é€šç”¨åŒ–æ€§èƒ½æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚å½“å‰æ–¹æ³•ä¸»è¦é€šè¿‡å›¾åƒæˆ–ç‰¹å¾å¢å¼ºæŠ€æœ¯ä¸°å¯ŒæºåŸŸæ•°æ®æ¥æå‡æ£€æµ‹å™¨é²æ£’æ€§ã€‚å°½ç®¡åŸºäºVLMçš„å¢å¼ºæŠ€æœ¯å·²è¯æ˜æœ‰æ•ˆï¼Œä½†å—é™äºæ£€æµ‹å™¨æ¶æ„å¿…é¡»ä¸å›¾åƒç¼–ç å™¨çš„ç»“æ„ç›¸åŒã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºè¯­è¨€é©±åŠ¨çš„åŒé£æ ¼æ··åˆï¼ˆLDDSï¼‰æ–¹æ³•ç”¨äºå•åŸŸé€šç”¨åŒ–ï¼Œé€šè¿‡å……åˆ†åˆ©ç”¨VLMçš„è¯­ä¹‰ä¿¡æ¯ä¸°å¯ŒæºåŸŸæ•°æ®ã€‚é¦–å…ˆæ„å»ºæç¤ºä»¥å°†VLMä¸­çš„é£æ ¼è¯­ä¹‰è½¬ç§»åˆ°å›¾åƒç¿»è¯‘ç½‘ç»œä¸­ï¼Œç”Ÿæˆå…·æœ‰æ˜ç¡®è¯­ä¹‰ä¿¡æ¯çš„é£æ ¼å¤šæ ·åŒ–å›¾åƒã€‚æ¥ç€æå‡ºå›¾åƒçº§åˆ«çš„é£æ ¼æ··åˆæŠ€æœ¯ï¼Œæœ‰æ•ˆæŒ–æ˜å¤šæ ·åŒ–å›¾åƒçš„è¯­ä¹‰ä¿¡æ¯ï¼Œæ— éœ€ç‰¹å®šå¢å¼ºé€‰æ‹©ã€‚æœ€åï¼Œé‡‡ç”¨åŒç®¡é“æ–¹å¼è¿›è¡Œç‰¹å¾çº§åˆ«çš„é£æ ¼æ··åˆï¼Œä½¿ç‰¹å¾å¢å¼ºå…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œå¹¶èƒ½æ— ç¼é…åˆä¸»æµæ£€æµ‹å™¨æ¡†æ¶ã€‚å¤§é‡å®éªŒè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åŸºå‡†æ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬çœŸå®åˆ°å¡é€šå’Œæ­£å¸¸åˆ°æ¶åŠ£å¤©æ°”ä»»åŠ¡ã€‚ç›¸å…³æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/qinhongda8/LDDS%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/qinhongda8/LDDSå…¬å¼€æä¾›ã€‚</a></p>
<p><strong>å…³é”®å‘ç°</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å•åŸŸé€šç”¨åŒ–æ–¹æ³•â€”â€”è¯­è¨€é©±åŠ¨çš„åŒé£æ ¼æ··åˆï¼ˆLDDSï¼‰ï¼Œä»¥è§£å†³å°†å•ä¸€åŸŸè®­ç»ƒçš„æ£€æµ‹å™¨æ¨å¹¿åˆ°å¤šä¸ªæœªè§åŸŸçš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è¯­ä¹‰ä¿¡æ¯ä¸°å¯ŒæºåŸŸæ•°æ®ï¼Œæé«˜äº†æ£€æµ‹å™¨çš„é²æ£’æ€§ã€‚</li>
<li>æå‡ºæ„å»ºæç¤ºä»¥è½¬ç§»VLMä¸­çš„é£æ ¼è¯­ä¹‰åˆ°å›¾åƒç¿»è¯‘ç½‘ç»œï¼Œç”Ÿæˆå…·æœ‰æ˜ç¡®è¯­ä¹‰ä¿¡æ¯çš„é£æ ¼å¤šæ ·åŒ–å›¾åƒã€‚</li>
<li>å¼•å…¥å›¾åƒçº§åˆ«çš„é£æ ¼æ··åˆæŠ€æœ¯ï¼Œæœ‰æ•ˆæŒ–æ˜å¤šæ ·åŒ–å›¾åƒçš„è¯­ä¹‰ä¿¡æ¯ï¼Œæ— éœ€ç‰¹å®šå¢å¼ºé€‰æ‹©ã€‚</li>
<li>é‡‡ç”¨åŒç®¡é“æ–¹å¼è¿›è¡Œç‰¹å¾çº§åˆ«çš„é£æ ¼æ··åˆï¼Œä½¿å¾—ç‰¹å¾å¢å¼ºå…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œå¹¶é€‚åº”ä¸»æµæ£€æµ‹å™¨æ¡†æ¶ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬çœŸå®åˆ°å¡é€šå’Œæ­£å¸¸åˆ°æ¶åŠ£å¤©æ°”ä»»åŠ¡çš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a168f79ee222bff299c40b92d6f64b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80f140c1e28d7068e733a50877d55330.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0438606acbea2776ffbf533713c5d631.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8329c4c921b3f07241a2c1da4b5fda6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b41952cf94a4cf4614c78f4e5d2d3914.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9eab95fc0d4912a1022736762fbc3c6f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Boosting-Cross-spectral-Unsupervised-Domain-Adaptation-for-Thermal-Semantic-Segmentation"><a href="#Boosting-Cross-spectral-Unsupervised-Domain-Adaptation-for-Thermal-Semantic-Segmentation" class="headerlink" title="Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal   Semantic Segmentation"></a>Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal   Semantic Segmentation</h2><p><strong>Authors:Seokjun Kwon, Jeongmin Shin, Namil Kim, Soonmin Hwang, Yukyung Choi</strong></p>
<p>In autonomous driving, thermal image semantic segmentation has emerged as a critical research area, owing to its ability to provide robust scene understanding under adverse visual conditions. In particular, unsupervised domain adaptation (UDA) for thermal image segmentation can be an efficient solution to address the lack of labeled thermal datasets. Nevertheless, since these methods do not effectively utilize the complementary information between RGB and thermal images, they significantly decrease performance during domain adaptation. In this paper, we present a comprehensive study on cross-spectral UDA for thermal image semantic segmentation. We first propose a novel masked mutual learning strategy that promotes complementary information exchange by selectively transferring results between each spectral model while masking out uncertain regions. Additionally, we introduce a novel prototypical self-supervised loss designed to enhance the performance of the thermal segmentation model in nighttime scenarios. This approach addresses the limitations of RGB pre-trained networks, which cannot effectively transfer knowledge under low illumination due to the inherent constraints of RGB sensors. In experiments, our method achieves higher performance over previous UDA methods and comparable performance to state-of-the-art supervised methods. </p>
<blockquote>
<p>åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œçƒ­å›¾åƒè¯­ä¹‰åˆ†å‰²å·²æˆä¸ºä¸€ä¸ªå…³é”®çš„ç ”ç©¶æ–¹å‘ï¼Œå› ä¸ºå®ƒèƒ½åœ¨æ¶åŠ£çš„è§†è§‰æ¡ä»¶ä¸‹æä¾›ç¨³å¥çš„åœºæ™¯ç†è§£ã€‚ç‰¹åˆ«æ˜¯ï¼Œé’ˆå¯¹çƒ­å›¾åƒåˆ†å‰²çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰å¯ä»¥ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆæ¥è§£å†³æ ‡è®°çƒ­æ•°æ®é›†ç¼ºä¹çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›æ–¹æ³•ä¸èƒ½æœ‰æ•ˆåœ°åˆ©ç”¨RGBå’Œçƒ­å›¾åƒä¹‹é—´çš„äº’è¡¥ä¿¡æ¯ï¼Œå®ƒä»¬åœ¨åŸŸè‡ªé€‚åº”è¿‡ç¨‹ä¸­ä¼šé™ä½æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹è·¨å…‰è°±UDAåœ¨çƒ­å›¾åƒè¯­ä¹‰åˆ†å‰²æ–¹é¢è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ©æ¨¡äº’å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡æœ‰é€‰æ‹©åœ°è½¬ç§»æ¯ä¸ªå…‰è°±æ¨¡å‹ä¹‹é—´çš„ç»“æœå¹¶æ©ç›–ä¸ç¡®å®šåŒºåŸŸï¼Œä»è€Œä¿ƒè¿›äº’è¡¥ä¿¡æ¯çš„äº¤æ¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸå‹è‡ªç›‘ç£æŸå¤±ï¼Œæ—¨åœ¨æé«˜å¤œé—´åœºæ™¯ä¸­çš„çƒ­åˆ†å‰²æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•è§£å†³äº†RGBé¢„è®­ç»ƒç½‘ç»œçš„å±€é™æ€§ï¼Œç”±äºRGBä¼ æ„Ÿå™¨çš„å›ºæœ‰çº¦æŸï¼Œè¿™äº›ç½‘ç»œåœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹æ— æ³•æœ‰æ•ˆåœ°è½¬ç§»çŸ¥è¯†ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†è¶…è¿‡ä»¥å‰UDAæ–¹æ³•çš„é«˜æ€§èƒ½å’Œä¸æœ€æ–°ç›‘ç£æ–¹æ³•çš„ç›¸å½“æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06951v1">PDF</a> 7 pages, 4 figures, International Conference on Robotics and   Automation(ICRA) 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ— ç›‘ç£åŸŸè‡ªé€‚åº”æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œæœ¬ç ”ç©¶æ¢è®¨äº¤å‰å…‰è°±è‡ªé€‚åº”è¯­ä¹‰åˆ†å‰²æŠ€æœ¯å¯¹äºçƒ­å›¾åƒè‡ªä¸»é©¾é©¶æŠ€æœ¯çš„å½±å“ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°é¢–çš„æ©è†œäº’å­¦ä¹ æœºåˆ¶ï¼Œä¿ƒè¿›RGBä¸çƒ­å›¾åƒä¹‹é—´çš„äº’è¡¥ä¿¡æ¯äº¤æ¢ï¼ŒåŒæ—¶å¼•å…¥åŸå‹è‡ªç›‘ç£æŸå¤±ä»¥å¢å¼ºå¤œé—´åœºæ™¯ä¸‹çš„çƒ­å›¾åƒåˆ†å‰²æ€§èƒ½ã€‚è¯¥æ–¹æ³•è§£å†³äº†åŸºäºRGBçš„é¢„è®­ç»ƒç½‘ç»œåœ¨ä½å…‰ç…§ç¯å¢ƒä¸‹çš„çŸ¥è¯†è¿ç§»éš¾é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶èšç„¦äºæ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰åœ¨çƒ­å›¾åƒè¯­ä¹‰åˆ†å‰²ä¸­çš„åº”ç”¨ã€‚</li>
<li>æå‡ºä¸€ç§æ–°é¢–çš„æ©è†œäº’å­¦ä¹ æœºåˆ¶ï¼Œç”¨äºé€‰æ‹©æ€§åœ°åœ¨å…‰è°±æ¨¡å‹ä¹‹é—´è½¬ç§»ç»“æœï¼ŒåŒæ—¶æ©ç›–ä¸ç¡®å®šåŒºåŸŸã€‚è¿™ç§æœºåˆ¶ä¿ƒè¿›RGBå’Œçƒ­åŠ›å›¾åƒçš„äº’è¡¥ä¿¡æ¯äº¤æ¢ã€‚</li>
<li>ä¸ºæé«˜å¤œé—´åœºæ™¯ä¸‹çš„çƒ­å›¾åƒåˆ†å‰²æ€§èƒ½ï¼Œå¼•å…¥åŸå‹è‡ªç›‘ç£æŸå¤±ã€‚è¿™ä¸€æŸå¤±æ—¨åœ¨å¢å¼ºæ¨¡å‹åœ¨ä½å…‰ç…§ç¯å¢ƒä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06951">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c61a03816928928670a3c617431cdd44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4152adef618403d9bb552a63b4f9ec8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad6db9e16c0a148ad46ec3436ab7a2d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70df984500050810f0a775e6d9eb074f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-133ca95420ba96f3843c340f5e083168.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b14546b5c65987835a709b175cb0ff17.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Underwater-object-detection-in-sonar-imagery-with-detection-transformer-and-Zero-shot-neural-architecture-search"><a href="#Underwater-object-detection-in-sonar-imagery-with-detection-transformer-and-Zero-shot-neural-architecture-search" class="headerlink" title="Underwater object detection in sonar imagery with detection transformer   and Zero-shot neural architecture search"></a>Underwater object detection in sonar imagery with detection transformer   and Zero-shot neural architecture search</h2><p><strong>Authors:XiaoTong Gu, Shengyu Tang, Yiming Cao, Changdong Yu</strong></p>
<p>Underwater object detection using sonar imagery has become a critical and rapidly evolving research domain within marine technology. However, sonar images are characterized by lower resolution and sparser features compared to optical images, which seriously degrades the performance of object detection.To address these challenges, we specifically propose a Detection Transformer (DETR) architecture optimized with a Neural Architecture Search (NAS) approach called NAS-DETR for object detection in sonar images. First, an improved Zero-shot Neural Architecture Search (NAS) method based on the maximum entropy principle is proposed to identify a real-time, high-representational-capacity CNN-Transformer backbone for sonar image detection. This method enables the efficient discovery of high-performance network architectures with low computational and time overhead. Subsequently, the backbone is combined with a Feature Pyramid Network (FPN) and a deformable attention-based Transformer decoder to construct a complete network architecture. This architecture integrates various advanced components and training schemes to enhance overall performance. Extensive experiments demonstrate that this architecture achieves state-of-the-art performance on two Representative datasets, while maintaining minimal overhead in real-time efficiency and computational complexity. Furthermore, correlation analysis between the key parameters and differential entropy-based fitness function is performed to enhance the interpretability of the proposed framework. To the best of our knowledge, this is the first work in the field of sonar object detection to integrate the DETR architecture with a NAS search mechanism. </p>
<blockquote>
<p>ä½¿ç”¨å£°çº³å›¾åƒè¿›è¡Œæ°´ä¸‹ç›®æ ‡æ£€æµ‹å·²æˆä¸ºæµ·æ´‹æŠ€æœ¯ä¸­ä¸€ä¸ªå…³é”®ä¸”å¿«é€Ÿå‘å±•çš„ç ”ç©¶é¢†åŸŸã€‚ç„¶è€Œï¼Œä¸å…‰å­¦å›¾åƒç›¸æ¯”ï¼Œå£°çº³å›¾åƒçš„ç‰¹å¾åˆ†è¾¨ç‡è¾ƒä½ã€ç‰¹å¾è¾ƒç¨€ç–ï¼Œè¿™ä¸¥é‡é™ä½äº†ç›®æ ‡æ£€æµ‹çš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç‰¹åœ°æå‡ºäº†ä¸€ç§ä½¿ç”¨ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰æ–¹æ³•ä¼˜åŒ–çš„æ£€æµ‹è½¬æ¢å™¨ï¼ˆDETRï¼‰æ¶æ„ï¼Œç§°ä¸ºNAS-DETRï¼Œç”¨äºå£°çº³å›¾åƒä¸­çš„ç›®æ ‡æ£€æµ‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæœ€å¤§ç†µåŸç†çš„æ”¹è¿›é›¶é•œå¤´ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«é€‚ç”¨äºå£°çº³å›¾åƒæ£€æµ‹çš„å®æ—¶ã€é«˜è¡¨å¾å®¹é‡CNN-Transformeréª¨å¹²ç½‘ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆåœ°å‘ç°å…·æœ‰ä½è®¡ç®—å’Œæ—¶é—´å¼€é”€çš„é«˜æ€§èƒ½ç½‘ç»œæ¶æ„ã€‚éšåï¼Œå°†éª¨å¹²ç½‘ä¸ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ˆFPNï¼‰å’ŒåŸºäºå¯å˜å½¢æ³¨æ„åŠ›çš„Transformerè§£ç å™¨ç›¸ç»“åˆï¼Œæ„å»ºå®Œæ•´çš„ç½‘ç»œæ¶æ„ã€‚è¯¥æ¶æ„é›†æˆäº†å„ç§å…ˆè¿›ç»„ä»¶å’Œè®­ç»ƒæ–¹æ¡ˆï¼Œä»¥æé«˜æ•´ä½“æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ¶æ„åœ¨ä¸¤ä¸ªä»£è¡¨æ€§æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶æ•ˆç‡å’Œè®¡ç®—å¤æ‚åº¦çš„æœ€ä½å¼€é”€ã€‚æ­¤å¤–ï¼Œè¿˜å¯¹å…³é”®å‚æ•°ä¸åŸºäºå·®å¼‚ç†µçš„é€‚åº”åº¦å‡½æ•°è¿›è¡Œäº†ç›¸å…³æ€§åˆ†æï¼Œä»¥æé«˜æ‰€æå‡ºæ¡†æ¶çš„å¯è§£é‡Šæ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯å£°çº³ç›®æ ‡æ£€æµ‹é¢†åŸŸé¦–æ¬¡å°†DETRæ¶æ„ä¸NASæœç´¢æœºåˆ¶ç›¸ç»“åˆçš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06694v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ°´ä¸‹ç‰©ä½“æ£€æµ‹çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§åŸºäºç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰çš„ä¼˜åŒ–æ£€æµ‹è½¬æ¢å™¨ï¼ˆDETRï¼‰æ¶æ„ï¼Œç§°ä¸ºNAS-DETRã€‚å®ƒé‡‡ç”¨æ”¹è¿›çš„é›¶é•œå¤´NASæ–¹æ³•ï¼Œç»“åˆç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ˆFPNï¼‰å’Œå¯å˜å½¢æ³¨æ„åŠ›åŸºç¡€çš„è½¬æ¢å™¨è§£ç å™¨ï¼Œæ„å»ºå®Œæ•´ç½‘ç»œæ¶æ„ã€‚åœ¨ä¸¤é¡¹ä»£è¡¨æ€§æ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒå®æ—¶æ•ˆç‡å’Œè®¡ç®—å¤æ‚åº¦çš„æœ€å°åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹ç‰©ä½“æ£€æµ‹æ˜¯æµ·æ´‹æŠ€æœ¯ä¸­å…³é”®ä¸”å¿«é€Ÿæ¼”åŒ–çš„ç ”ç©¶é¢†åŸŸã€‚</li>
<li>å£°çº³å›¾åƒå…·æœ‰è¾ƒä½åˆ†è¾¨ç‡å’Œç¨€ç–ç‰¹å¾ï¼Œç»™ç‰©ä½“æ£€æµ‹å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºåŸºäºç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰çš„ä¼˜åŒ–æ£€æµ‹è½¬æ¢å™¨ï¼ˆDETRï¼‰æ¶æ„ï¼Œç§°ä¸ºNAS-DETRï¼Œç”¨äºå£°çº³å›¾åƒä¸­çš„ç‰©ä½“æ£€æµ‹ã€‚</li>
<li>æ”¹è¿›çš„é›¶é•œå¤´NASæ–¹æ³•åŸºäºæœ€å¤§ç†µåŸåˆ™ï¼Œèƒ½é«˜æ•ˆå‘ç°é«˜æ€§èƒ½ç½‘ç»œæ¶æ„ï¼Œå…·æœ‰ä½è®¡ç®—å’Œæ—¶é—´å¼€é”€ã€‚</li>
<li>ç»“åˆFPNå’Œå¯å˜å½¢æ³¨æ„åŠ›åŸºç¡€çš„è½¬æ¢å™¨è§£ç å™¨ï¼Œæ„å»ºå®Œæ•´ç½‘ç»œæ¶æ„ï¼Œæé«˜æ•´ä½“æ€§èƒ½ã€‚</li>
<li>åœ¨ä¸¤ä¸ªä»£è¡¨æ€§æ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒå®æ—¶æ•ˆç‡å’Œè®¡ç®—å¤æ‚åº¦çš„å¹³è¡¡ã€‚</li>
<li>é€šè¿‡å…³é”®å‚æ•°ä¸å·®å¼‚ç†µåŸºäºçš„é€‚åº”åº¦å‡½æ•°ä¹‹é—´çš„ç›¸å…³æ€§åˆ†æï¼Œæé«˜æ¡†æ¶çš„å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06694">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5df4b6a62a885e348e2c5c5bddac6e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86e0721420416b88265ded0a05b3bdfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a52f341e3095158e08d9cff2ca22549.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89de8f4333c0b3dd12e7a8e82067e560.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MonoCoP-Chain-of-Prediction-for-Monocular-3D-Object-Detection"><a href="#MonoCoP-Chain-of-Prediction-for-Monocular-3D-Object-Detection" class="headerlink" title="MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection"></a>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</h2><p><strong>Authors:Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu</strong></p>
<p>Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets. </p>
<blockquote>
<p>å‡†ç¡®é¢„æµ‹3Då±æ€§å¯¹äºå•ç›®3Då¯¹è±¡æ£€æµ‹ï¼ˆMono3Dï¼‰è‡³å…³é‡è¦ï¼Œæ·±åº¦ä¼°è®¡ç”±äºå°†2Då›¾åƒæ˜ å°„åˆ°3Dç©ºé—´æ—¶å›ºæœ‰çš„æ¨¡ç³Šæ€§è€Œé¢ä¸´æœ€å¤§æŒ‘æˆ˜ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•åˆ©ç”¨å¤šç§æ·±åº¦çº¿ç´¢ï¼ˆä¾‹å¦‚ä¼°è®¡æ·±åº¦ä¸ç¡®å®šæ€§ã€å»ºæ¨¡æ·±åº¦è¯¯å·®ï¼‰æ¥æé«˜æ·±åº¦å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†å‡†ç¡®çš„æ·±åº¦é¢„æµ‹éœ€è¦ä¾èµ–äºå…¶ä»–3Då±æ€§ï¼Œå› ä¸ºè¿™äº›å±æ€§é€šè¿‡3Dåˆ°2Dçš„æŠ•å½±æœ¬è´¨ä¸Šç›¸äº’å…³è”ï¼Œè¿™æœ€ç»ˆé™åˆ¶äº†æ€»ä½“å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚æœ¬æ–‡å—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨é¢„æµ‹é“¾ï¼ˆChain-of-Predictionï¼ŒCoPï¼‰è¿›è¡Œå±æ€§é¡ºåºé¢„æµ‹å’Œæ¡ä»¶é¢„æµ‹çš„æ–¹æ³•MonoCoPï¼Œå…¶é€šè¿‡ä¸‰ä¸ªå…³é”®è®¾è®¡å®ç°ï¼šé¦–å…ˆï¼Œå®ƒä¸ºæ¯ä¸ª3Då±æ€§é‡‡ç”¨è½»é‡çº§çš„AttributeNetï¼ˆANï¼‰æ¥å­¦ä¹ ç‰¹å®šäºå±æ€§çš„ç‰¹å¾ï¼›å…¶æ¬¡ï¼ŒMonoCoPæ„å»ºäº†ä¸€ä¸ªæ˜ç¡®çš„é“¾æ¡æ¥ä¼ æ’­ä»ä¸€ä¸ªå±æ€§å­¦åˆ°çš„ç‰¹å¾åˆ°ä¸‹ä¸€ä¸ªå±æ€§ï¼›æœ€åï¼ŒMonoCoPä½¿ç”¨æ®‹å·®è¿æ¥æ¥æ²¿ç€é“¾æ¡èšåˆæ¯ä¸ªå±æ€§çš„ç‰¹å¾ï¼Œç¡®ä¿åç»­çš„å±æ€§é¢„æµ‹ä¾èµ–äºæ‰€æœ‰å…ˆå‰å¤„ç†çš„å±æ€§ï¼ŒåŒæ—¶ä¸ä¼šå¿˜è®°æ—©æœŸçš„å±æ€§ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MonoCoPåœ¨KITTIæ’è¡Œæ¦œä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶ä¸”æ— éœ€é¢å¤–æ•°æ®ï¼Œåœ¨Waymoå’ŒnuScenesæ­£é¢æ•°æ®é›†ä¸Šä¹Ÿè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04594v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºChain-of-Predictionï¼ˆCoPï¼‰çš„å•çœ¼ä¸‰ç»´ç‰©ä½“æ£€æµ‹æ–°æ–¹æ³•MonoCoPã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºé¢„æµ‹é“¾ï¼Œé¡ºåºä¸”æ¡ä»¶åœ°é¢„æµ‹ä¸‰ç»´å±æ€§ã€‚å®ƒé€šè¿‡ä¸ºæ¯ä¸ªä¸‰ç»´å±æ€§è®¾è®¡è½»é‡çº§çš„AttributeNetï¼ˆANï¼‰æ¥å­¦ä¹ ç‰¹å®šå±æ€§ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ˜¾å¼é“¾ä¼ æ’­è¿™äº›ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒMonoCoPé‡‡ç”¨æ®‹å·®è¿æ¥æ¥æ±‡èšå„å±æ€§çš„ç‰¹å¾ï¼Œç¡®ä¿åç»­å±æ€§é¢„æµ‹å»ºç«‹åœ¨æ‰€æœ‰å…ˆå‰å¤„ç†è¿‡çš„å±æ€§ä¹‹ä¸Šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMonoCoPåœ¨KITTIæ’è¡Œæ¦œä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œä¸”åœ¨Waymoå’ŒnuScenesæ­£é¢æ•°æ®é›†ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MonoCoPåˆ©ç”¨Chain-of-Predictionï¼ˆCoPï¼‰æ–¹æ³•é¡ºåºä¸”æ¡ä»¶åœ°é¢„æµ‹ä¸‰ç»´å±æ€§ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒã€‚</li>
<li>é€šè¿‡AttributeNetï¼ˆANï¼‰ä¸ºæ¯ä¸ªä¸‰ç»´å±æ€§å­¦ä¹ ç‰¹å®šç‰¹å¾ã€‚</li>
<li>é€šè¿‡æ˜¾å¼é¢„æµ‹é“¾ä¼ æ’­ç‰¹å¾ï¼Œå¢å¼ºå±æ€§é—´çš„å…³è”æ€§ã€‚</li>
<li>æ®‹å·®è¿æ¥ç¡®ä¿åç»­å±æ€§é¢„æµ‹å»ºç«‹åœ¨æ‰€æœ‰å…ˆå‰å¤„ç†è¿‡çš„å±æ€§ä¹‹ä¸Šï¼Œé¿å…é—å¿˜æ—©æœŸç‰¹å¾ã€‚</li>
<li>MonoCoPåœ¨KITTIã€Waymoå’ŒnuScenesæ•°æ®é›†ä¸Šå®ç°å…ˆè¿›æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04594">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad2fd13cc5a4dbf34f8ec0a97ea9988b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4656fa795d076aea34f6e55c4fc8abad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d6169567c1dc4abeac209b7307d9785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4fbd105d5b65f5ad6dbead5d93a91e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4e1fb2b9073d0dc3996c41cd4d610d7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance"><a href="#Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance" class="headerlink" title="Enhanced Generative Data Augmentation for Semantic Segmentation via   Stronger Guidance"></a>Enhanced Generative Data Augmentation for Semantic Segmentation via   Stronger Guidance</h2><p><strong>Authors:Quang-Huy Che, Duc-Tri Le, Bich-Nga Pham, Duc-Khai Lam, Vinh-Tiep Nguyen</strong></p>
<p>Data augmentation is crucial for pixel-wise annotation tasks like semantic segmentation, where labeling requires significant effort and intensive labor. Traditional methods, involving simple transformations such as rotations and flips, create new images but often lack diversity along key semantic dimensions and fail to alter high-level semantic properties. To address this issue, generative models have emerged as an effective solution for augmenting data by generating synthetic images. Controllable Generative models offer data augmentation methods for semantic segmentation tasks by using prompts and visual references from the original image. However, these models face challenges in generating synthetic images that accurately reflect the content and structure of the original image due to difficulties in creating effective prompts and visual references. In this work, we introduce an effective data augmentation pipeline for semantic segmentation using Controllable Diffusion model. Our proposed method includes efficient prompt generation using \textit{Class-Prompt Appending} and \textit{Visual Prior Blending} to enhance attention to labeled classes in real images, allowing the pipeline to generate a precise number of augmented images while preserving the structure of segmentation-labeled classes. In addition, we implement a \textit{class balancing algorithm} to ensure a balanced training dataset when merging the synthetic and original images. Evaluation on PASCAL VOC datasets, our pipeline demonstrates its effectiveness in generating high-quality synthetic images for semantic segmentation. Our code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance%7D%7Bthis">https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance}{this</a> https URL}. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºå¯¹äºåƒç´ çº§æ³¨é‡Šä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²ï¼‰è‡³å…³é‡è¦ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦å¤§é‡åŠ³åŠ¨å’ŒåŠªåŠ›æ¥è¿›è¡Œæ ‡æ³¨ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ï¼Œæ¶‰åŠç®€å•çš„è½¬æ¢ï¼ˆå¦‚æ—‹è½¬å’Œç¿»è½¬ï¼‰ï¼Œå¯ä»¥åˆ›å»ºæ–°çš„å›¾åƒï¼Œä½†å¾€å¾€åœ¨å…³é”®çš„è¯­ä¹‰ç»´åº¦ä¸Šç¼ºä¹å¤šæ ·æ€§ï¼Œå¹¶ä¸”æ— æ³•æ”¹å˜é«˜çº§è¯­ä¹‰å±æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç”Ÿæˆæ¨¡å‹å·²ç»å‡ºç°ä¸ºé€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæ¥å¢å¼ºæ•°æ®çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚å¯æ§ç”Ÿæˆæ¨¡å‹é€šè¿‡ä½¿ç”¨åŸå§‹å›¾åƒçš„æç¤ºå’Œè§†è§‰å‚è€ƒæ¥ä¸ºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡æä¾›æ•°æ®å¢å¼ºæ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆå‡†ç¡®åæ˜ åŸå§‹å›¾åƒå†…å®¹å’Œç»“æ„çš„åˆæˆå›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºåˆ›å»ºæœ‰æ•ˆçš„æç¤ºå’Œè§†è§‰å‚è€ƒå­˜åœ¨å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä½¿ç”¨å¯æ§æ‰©æ•£æ¨¡å‹å¯¹è¯­ä¹‰åˆ†å‰²è¿›è¡Œæœ‰æ•ˆæ•°æ®å¢å¼ºçš„ç®¡é“ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨â€œç±»æç¤ºè¿½åŠ â€å’Œâ€œè§†è§‰å…ˆéªŒæ··åˆâ€è¿›è¡Œæœ‰æ•ˆçš„æç¤ºç”Ÿæˆï¼Œä»¥æé«˜å¯¹çœŸå®å›¾åƒä¸­æ ‡è®°ç±»çš„å…³æ³¨ï¼Œä½¿ç®¡é“èƒ½å¤Ÿåœ¨ä¿ç•™åˆ†å‰²æ ‡è®°ç±»ç»“æ„çš„åŒæ—¶ç”Ÿæˆç²¾ç¡®æ•°é‡çš„å¢å¼ºå›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†â€œç±»åˆ«å¹³è¡¡ç®—æ³•â€ï¼Œä»¥ç¡®ä¿åœ¨åˆå¹¶åˆæˆå›¾åƒå’ŒåŸå§‹å›¾åƒæ—¶è·å¾—å¹³è¡¡çš„è®­ç»ƒæ•°æ®é›†ã€‚åœ¨PASCAL VOCæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®¡é“åœ¨ç”Ÿæˆç”¨äºè¯­ä¹‰åˆ†å‰²çš„é«˜è´¨é‡åˆæˆå›¾åƒæ–¹é¢éå¸¸æœ‰æ•ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨è¿™ä¸ªURLçš„ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance">https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06002v4">PDF</a> </p>
<p><strong>Summary</strong><br>æ•°æ®å¢å¼ºå¯¹äºåƒç´ çº§æ ‡æ³¨ä»»åŠ¡å¦‚è¯­ä¹‰åˆ†å‰²è‡³å…³é‡è¦ï¼Œä¼ ç»Ÿæ–¹æ³•ç¼ºä¹å¤šæ ·æ€§ã€‚ç”Ÿæˆæ¨¡å‹é€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæœ‰æ•ˆè§£å†³äº†æ•°æ®å¢å¼ºé—®é¢˜ï¼Œä½†å¯æ§ç”Ÿæˆæ¨¡å‹é¢ä¸´å‡†ç¡®åæ˜ åŸå§‹å›¾åƒå†…å®¹å’Œç»“æ„çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§ä½¿ç”¨å¯æ§æ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰åˆ†å‰²æ•°æ®å¢å¼ºç®¡é“ï¼Œé€šè¿‡é«˜æ•ˆæç¤ºç”Ÿæˆå’Œè§†è§‰å…ˆéªŒèåˆï¼Œåœ¨çœŸå®å›¾åƒä¸­å¢å¼ºå¯¹æ ‡è®°ç±»çš„æ³¨æ„åŠ›ï¼ŒåŒæ—¶ä¿è¯åˆæˆå›¾åƒçš„æ•°é‡å’Œè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®å¢å¼ºåœ¨åƒç´ çº§æ ‡æ³¨ä»»åŠ¡ä¸­éå¸¸é‡è¦ï¼Œå°¤å…¶å¯¹äºè¯­ä¹‰åˆ†å‰²ï¼Œèƒ½å‡å°‘æ ‡æ³¨çš„åŠ³åŠ¨æˆæœ¬ã€‚</li>
<li>ä¼ ç»Ÿæ•°æ®å¢å¼ºæ–¹æ³•å¦‚æ—‹è½¬å’Œç¿»è½¬è™½ç„¶èƒ½åˆ›å»ºæ–°å›¾åƒï¼Œä½†ç¼ºä¹å…³é”®è¯­ä¹‰ç»´åº¦ä¸Šçš„å¤šæ ·æ€§ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å¯æ§ç”Ÿæˆæ¨¡å‹ï¼Œå·²è¢«è¯æ˜æ˜¯æ•°æ®å¢å¼ºçš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
<li>å¯æ§ç”Ÿæˆæ¨¡å‹é¢ä¸´å‡†ç¡®åæ˜ åŸå§‹å›¾åƒå†…å®¹å’Œç»“æ„çš„æŒ‘æˆ˜ï¼Œéœ€è¦æœ‰æ•ˆçš„æç¤ºå’Œè§†è§‰å‚è€ƒã€‚</li>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºç®¡é“ï¼Œç»“åˆç±»æç¤ºæ·»åŠ å’Œè§†è§‰å…ˆéªŒèåˆï¼Œç”¨äºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>è¯¥ç®¡é“åœ¨PASCAL VOCæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¯æ˜äº†å…¶ç”Ÿæˆé«˜è´¨é‡åˆæˆå›¾åƒçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3c1716bef1d0d6ad24dfb909fb50fe68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f65bfd9b2b7e788b5f54c9643a7800c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91dd43e3d48806df3e2fa23090a86993.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b3c65fb7936a29984cc349b14779630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b79cbb1b35476816ef186a87a8ee7de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c298870b43ce121bb270ff9dab12e16e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="BihoT-A-Large-Scale-Dataset-and-Benchmark-for-Hyperspectral-Camouflaged-Object-Tracking"><a href="#BihoT-A-Large-Scale-Dataset-and-Benchmark-for-Hyperspectral-Camouflaged-Object-Tracking" class="headerlink" title="BihoT: A Large-Scale Dataset and Benchmark for Hyperspectral Camouflaged   Object Tracking"></a>BihoT: A Large-Scale Dataset and Benchmark for Hyperspectral Camouflaged   Object Tracking</h2><p><strong>Authors:Hanzheng Wang, Wei Li, Xiang-Gen Xia, Qian Du</strong></p>
<p>Hyperspectral object tracking (HOT) has exhibited potential in various applications, particularly in scenes where objects are camouflaged. Existing trackers can effectively retrieve objects via band regrouping because of the bias in existing HOT datasets, where most objects tend to have distinguishing visual appearances rather than spectral characteristics. This bias allows the tracker to directly use the visual features obtained from the false-color images generated by hyperspectral images without the need to extract spectral features. To tackle this bias, we find that the tracker should focus on the spectral information when object appearance is unreliable. Thus, we provide a new task called hyperspectral camouflaged object tracking (HCOT) and meticulously construct a large-scale HCOT dataset, termed BihoT, which consists of 41,912 hyperspectral images covering 49 video sequences. The dataset covers various artificial camouflage scenes where objects have similar appearances, diverse spectrums, and frequent occlusion, making it a very challenging dataset for HCOT. Besides, a simple but effective baseline model, named spectral prompt-based distractor-aware network (SPDAN), is proposed, comprising a spectral embedding network (SEN), a spectral prompt-based backbone network (SPBN), and a distractor-aware module (DAM). Specifically, the SEN extracts spectral-spatial features via 3-D and 2-D convolutions. Then, the SPBN fine-tunes powerful RGB trackers with spectral prompts and alleviates the insufficiency of training samples. Moreover, the DAM utilizes a novel statistic to capture the distractor caused by occlusion from objects and background. Extensive experiments demonstrate that our proposed SPDAN achieves state-of-the-art performance on the proposed BihoT and other HOT datasets. </p>
<blockquote>
<p>é«˜å…‰è°±ç›®æ ‡è·Ÿè¸ªï¼ˆHOTï¼‰åœ¨å„ç§åº”ç”¨ä¸­å·²æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›®æ ‡ä¼ªè£…åœºæ™¯ä¸­ã€‚ç°æœ‰çš„è·Ÿè¸ªå™¨å¯ä»¥é€šè¿‡æ³¢æ®µé‡ç»„æœ‰æ•ˆåœ°æ£€ç´¢ç›®æ ‡ï¼Œè¿™æ˜¯å› ä¸ºç°æœ‰HOTæ•°æ®é›†å­˜åœ¨çš„åè§ï¼Œå¤§å¤šæ•°ç›®æ ‡å¾€å¾€å…·æœ‰åŒºåˆ†åº¦çš„æ˜¯è§†è§‰å¤–è§‚è€Œä¸æ˜¯å…‰è°±ç‰¹å¾ã€‚è¿™ç§åè§ä½¿å¾—è·Ÿè¸ªå™¨èƒ½å¤Ÿç›´æ¥ä½¿ç”¨ä»é«˜å…‰è°±å›¾åƒç”Ÿæˆçš„å‡å½©è‰²å›¾åƒè·å¾—çš„è§†è§‰ç‰¹å¾ï¼Œè€Œæ— éœ€æå–å…‰è°±ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™ç§åè§ï¼Œæˆ‘ä»¬å‘ç°å½“ç›®æ ‡å¤–è§‚ä¸å¯é æ—¶ï¼Œè·Ÿè¸ªå™¨åº”ä¸“æ³¨äºå…‰è°±ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡ï¼Œç§°ä¸ºé«˜å…‰è°±ä¼ªè£…ç›®æ ‡è·Ÿè¸ªï¼ˆHCOTï¼‰ï¼Œå¹¶ç²¾å¿ƒæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„HCOTæ•°æ®é›†ï¼Œåä¸ºBihoTï¼Œç”±è¦†ç›–49ä¸ªè§†é¢‘åºåˆ—çš„41912ä¸ªé«˜å…‰è°±å›¾åƒç»„æˆã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å„ç§äººå·¥ä¼ªè£…åœºæ™¯ï¼Œå…¶ä¸­ç›®æ ‡å…·æœ‰ç›¸ä¼¼çš„å¤–è§‚ã€å¤šæ ·çš„å…‰è°±å’Œé¢‘ç¹é®æŒ¡ï¼Œä½¿å…¶æˆä¸ºHCOTéå¸¸æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„åŸºçº¿æ¨¡å‹ï¼Œç§°ä¸ºåŸºäºå…‰è°±æç¤ºçš„åˆ†å¿ƒç½‘ç»œï¼ˆSPDANï¼‰ï¼Œå®ƒç”±å…‰è°±åµŒå…¥ç½‘ç»œï¼ˆSENï¼‰ã€åŸºäºå…‰è°±æç¤ºçš„ä¸»å¹²ç½‘ç»œï¼ˆSPBNï¼‰å’Œåˆ†å¿ƒæ¨¡å—ï¼ˆDAMï¼‰ç»„æˆã€‚å…·ä½“è€Œè¨€ï¼ŒSENé€šè¿‡3Då’Œ2Då·ç§¯æå–å…‰è°±ç©ºé—´ç‰¹å¾ã€‚ç„¶åï¼ŒSPBNåˆ©ç”¨å…‰è°±æç¤ºå¾®è°ƒåŠŸèƒ½å¼ºå¤§çš„RGBè·Ÿè¸ªå™¨å¹¶ç¼“è§£è®­ç»ƒæ ·æœ¬ä¸è¶³çš„é—®é¢˜ã€‚æ­¤å¤–ï¼ŒDAMåˆ©ç”¨ä¸€ç§æ–°å‹ç»Ÿè®¡æ–¹æ³•æ¥æ•æ‰ç”±ç›®æ ‡å’ŒèƒŒæ™¯é®æŒ¡é€ æˆçš„åˆ†å¿ƒç‰©ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„SPDANåœ¨æå‡ºçš„BihoTå’Œå…¶ä»–HOTæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12232v2">PDF</a> IEEE Transactions on Neural Networks and Learning Systems, 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä»‹ç»äº†è¶…å…‰è°±ä¼ªè£…ç›®æ ‡è·Ÿè¸ªï¼ˆHCOTï¼‰çš„æ–°ä»»åŠ¡åŠå…¶æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰è¶…å…‰è°±ç›®æ ‡è·Ÿè¸ªï¼ˆHOTï¼‰æ•°æ®é›†çš„åè§é—®é¢˜ï¼Œæå‡ºäº†å¤§å‹HCOTæ•°æ®é›†BihoTçš„æ„å»ºæ–¹æ³•ï¼ŒåŒ…å«å„ç§ä¼ªè£…åœºæ™¯ä¸‹çš„å¤æ‚å˜åŒ–ç›®æ ‡ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºçº¿æ¨¡å‹SPDANï¼Œèƒ½å¤Ÿåœ¨æ•°æ®é›†ä¸Šå–å¾—ä¸€æµæ€§èƒ½ã€‚SPDANåŒ…å«å…‰è°±åµŒå…¥ç½‘ç»œã€å…‰è°±æç¤ºèƒŒæ™¯ç½‘ç»œå’Œå¹²æ‰°è¯†åˆ«æ¨¡å—ã€‚å…¶ä¸­ï¼Œå…‰è°±åµŒå…¥ç½‘ç»œé€šè¿‡ä¸‰ç»´å’ŒäºŒç»´å·ç§¯æå–å…‰è°±ç©ºé—´ç‰¹å¾ï¼›å…‰è°±æç¤ºèƒŒæ™¯ç½‘ç»œé€šè¿‡å¼•å…¥å…‰è°±æç¤ºä¼˜åŒ–RGBè·Ÿè¸ªå™¨ï¼›å¹²æ‰°è¯†åˆ«æ¨¡å—èƒ½å¤Ÿæ•æ‰é®æŒ¡å¼•èµ·çš„å¹²æ‰°ã€‚</p>
<p><strong>å…³é”®å‘ç°</strong></p>
<ul>
<li>ç°æœ‰è¶…å…‰è°±ç›®æ ‡è·Ÿè¸ªï¼ˆHOTï¼‰æ•°æ®é›†å­˜åœ¨åè§ï¼Œæ›´å…³æ³¨ç›®æ ‡è§†è§‰ç‰¹å¾è€Œéå…‰è°±ç‰¹æ€§ã€‚</li>
<li>æå‡ºæ–°çš„ä»»åŠ¡ï¼šè¶…å…‰è°±ä¼ªè£…ç›®æ ‡è·Ÿè¸ªï¼ˆHCOTï¼‰ï¼Œä»¥è§£å†³ç°æœ‰åè§é—®é¢˜ã€‚</li>
<li>æ„å»ºå¤§å‹HCOTæ•°æ®é›†BihoTï¼ŒåŒ…å«å¤æ‚ä¼ªè£…åœºæ™¯ä¸‹çš„ç›®æ ‡å›¾åƒå’Œè§†é¢‘åºåˆ—ã€‚</li>
<li>SPDANåŸºçº¿æ¨¡å‹åŒ…æ‹¬å…‰è°±åµŒå…¥ç½‘ç»œï¼ˆSENï¼‰ã€å…‰è°±æç¤ºèƒŒæ™¯ç½‘ç»œï¼ˆSPBNï¼‰å’Œå¹²æ‰°è¯†åˆ«æ¨¡å—ï¼ˆDAMï¼‰ã€‚</li>
<li>SENé€šè¿‡ä¸‰ç»´å’ŒäºŒç»´å·ç§¯æå–å…‰è°±ç©ºé—´ç‰¹å¾ã€‚</li>
<li>SPBNé€šè¿‡å¼•å…¥å…‰è°±æç¤ºä¼˜åŒ–RGBè·Ÿè¸ªå™¨ï¼Œç¼“è§£è®­ç»ƒæ ·æœ¬ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>DAMèƒ½å¤Ÿæ•æ‰é®æŒ¡å¼•èµ·çš„å¹²æ‰°ï¼Œæé«˜è·Ÿè¸ªæ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b819918bdedf8a15155d141ec965d9e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a1ad3ba1681d06ea90149c8a1e40400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d30d485ecfc40368c3635b28ea57ab6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc592dea092816c09f7a5fd822a94cd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edd911bf81474959d2371e7e519307a0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-14/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4bbca98f559e0b07fdb1fa04b135015d.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  My Emotion on your face The use of Facial Keypoint Detection to   preserve Emotions in Latent Space Editing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-7c03550ec50e2ce6f1ca34536b68e039.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  Breast Cancer Classification in Deep Ultraviolet Fluorescence Images   Using a Patch-Level Vision Transformer Framework
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27768.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
