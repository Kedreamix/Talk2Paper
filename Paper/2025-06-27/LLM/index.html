<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-27  The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-6f3c1ed44efb0cbfb794708947c3304d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-27-æ›´æ–°"><a href="#2025-06-27-æ›´æ–°" class="headerlink" title="2025-06-27 æ›´æ–°"></a>2025-06-27 æ›´æ–°</h1><h2 id="The-Decrypto-Benchmark-for-Multi-Agent-Reasoning-and-Theory-of-Mind"><a href="#The-Decrypto-Benchmark-for-Multi-Agent-Reasoning-and-Theory-of-Mind" class="headerlink" title="The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind"></a>The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind</h2><p><strong>Authors:Andrei Lupu, Timon Willi, Jakob Foerster</strong></p>
<p>As Large Language Models (LLMs) gain agentic abilities, they will have to navigate complex multi-agent scenarios, interacting with human users and other agents in cooperative and competitive settings. This will require new reasoning skills, chief amongst them being theory of mind (ToM), or the ability to reason about the â€œmentalâ€ states of other agents. However, ToM and other multi-agent abilities in LLMs are poorly understood, since existing benchmarks suffer from narrow scope, data leakage, saturation, and lack of interactivity. We thus propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM drawing inspiration from cognitive science, computational pragmatics and multi-agent reinforcement learning. It is designed to be as easy as possible in all other dimensions, eliminating confounding factors commonly found in other benchmarks. To our knowledge, it is also the first platform for designing interactive ToM experiments.   We validate the benchmark design through comprehensive empirical evaluations of frontier LLMs, robustness studies, and human-AI cross-play experiments. We find that LLM game-playing abilities lag behind humans and simple word-embedding baselines. We then create variants of two classic cognitive science experiments within Decrypto to evaluate three key ToM abilities. Surprisingly, we find that state-of-the-art reasoning models are significantly worse at those tasks than their older counterparts. This demonstrates that Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and paves the path towards better artificial agents. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è·å¾—ä»£ç†èƒ½åŠ›ï¼Œå®ƒä»¬å°†éœ€è¦åº”å¯¹å¤æ‚çš„å¤šä»£ç†åœºæ™¯ï¼Œåœ¨åˆä½œå’Œç«äº‰ç¯å¢ƒä¸­ä¸äººç±»ç”¨æˆ·å’Œå…¶ä»–ä»£ç†è¿›è¡Œäº¤äº’ã€‚è¿™å°†éœ€è¦æ–°çš„æ¨ç†èƒ½åŠ›ï¼Œå…¶ä¸­æœ€ä¸»è¦çš„æ˜¯å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰èƒ½åŠ›ï¼Œå³æ¨ç†å…¶ä»–ä»£ç†çš„â€œç²¾ç¥â€çŠ¶æ€çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒToMä»¥åŠLLMä¸­çš„å…¶ä»–å¤šä»£ç†èƒ½åŠ›éƒ½äº†è§£ç”šå°‘ï¼Œå› ä¸ºç°æœ‰çš„åŸºå‡†æµ‹è¯•å­˜åœ¨èŒƒå›´ç‹­çª„ã€æ•°æ®æ³„éœ²ã€é¥±å’Œå’Œç¼ºä¹äº¤äº’æ€§çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Decryptoï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ¸¸æˆçš„å¤šä»£ç†æ¨ç†å’Œå¿ƒæ™ºç†è®ºåŸºå‡†æµ‹è¯•ï¼Œå®ƒå€Ÿé‰´äº†è®¤çŸ¥ç§‘å­¦ã€è®¡ç®—è¯­ç”¨å­¦å’Œå¤šä»£ç†å¼ºåŒ–å­¦ä¹ ã€‚å…¶è®¾è®¡æ—¨åœ¨å°½å¯èƒ½åœ¨å…¶ä»–æ‰€æœ‰æ–¹é¢éƒ½å°½å¯èƒ½ç®€å•ï¼Œæ¶ˆé™¤åœ¨å…¶ä»–åŸºå‡†æµ‹è¯•ä¸­é€šå¸¸å­˜åœ¨çš„æ··æ·†å› ç´ ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™ä¹Ÿæ˜¯è®¾è®¡äº¤äº’å¼å¿ƒæ™ºç†è®ºå®éªŒçš„ç¬¬ä¸€ä¸ªå¹³å°ã€‚æˆ‘ä»¬é€šè¿‡å…¨é¢è¯„ä¼°å‰æ²¿çš„LLMã€ç¨³å¥æ€§ç ”ç©¶å’Œäººæœºäº¤å‰å®éªŒæ¥éªŒè¯åŸºå‡†æµ‹è¯•çš„è®¾è®¡ã€‚æˆ‘ä»¬å‘ç°LLMçš„æ¸¸æˆèƒ½åŠ›ä¸äººç±»å’Œç®€å•çš„è¯åµŒå…¥åŸºçº¿ç›¸æ¯”æœ‰æ‰€ä¸è¶³ã€‚ç„¶åæˆ‘ä»¬åœ¨Decryptoä¸­åˆ›å»ºäº†ä¸¤ç§ç»å…¸è®¤çŸ¥ç§‘å­¦å®éªŒçš„å˜ä½“ï¼Œä»¥è¯„ä¼°ä¸‰ç§å…³é”®çš„ToMèƒ½åŠ›ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°æœ€æ–°ä¸€ä»£çš„æ¨ç†æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—è½åäºæ—§æ¨¡å‹ã€‚è¿™è¡¨æ˜Decryptoè§£å†³äº†å½“å‰æ¨ç†å’Œå¿ƒæ™ºç†è®ºè¯„ä¼°ä¸­çš„å…³é”®å·®è·ï¼Œå¹¶ä¸ºæ›´å¥½çš„äººå·¥æ™ºèƒ½ä»£ç†çš„å‘å±•é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20664v1">PDF</a> 41 pages, 19 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è·å¾—ä»£ç†èƒ½åŠ›åï¼Œéœ€è¦åº”å¯¹å¤æ‚çš„å¤šä»£ç†åœºæ™¯ï¼Œä¸å…¶ä»–ä»£ç†å’Œäººç±»ç¤¾ä¼šåˆä½œä¸ç«äº‰ã€‚è¿™éœ€è¦æ–°çš„æ¨ç†èƒ½åŠ›ï¼Œå…¶ä¸­æœ€é‡è¦çš„æ˜¯å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰ï¼Œå³æ¨ç†å…¶ä»–ä»£ç†çš„â€œå¿ƒç†â€çŠ¶æ€çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒToMå’ŒLLMä¸­çš„å…¶ä»–å¤šä»£ç†èƒ½åŠ›å°šä¸æ¸…æ¥šï¼Œå› ä¸ºç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨èŒƒå›´ç‹­çª„ã€æ•°æ®æ³„éœ²ã€é¥±å’Œå’Œç¼ºä¹äº¤äº’ç­‰é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ¸¸æˆçš„åŸºå‡†æµ‹è¯•Decryptoï¼Œç”¨äºå¤šä»£ç†æ¨ç†å’ŒToMï¼Œçµæ„Ÿæ¥è‡ªè®¤çŸ¥ç§‘å­¦ã€è®¡ç®—è¯­ç”¨å­¦å’Œå¤šä»£ç†å¼ºåŒ–å­¦ä¹ ã€‚å…¶æ—¨åœ¨å°½å¯èƒ½æ¶ˆé™¤å…¶ä»–ç»´åº¦ä¸­çš„å¹²æ‰°å› ç´ ï¼Œå¹¶æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ˜¯è®¾è®¡äº¤äº’å¼ToMå®éªŒçš„ç¬¬ä¸€ä¸ªå¹³å°ã€‚æˆ‘ä»¬é€šè¿‡å‰æ²¿LLMçš„ç»¼åˆå®è¯è¯„ä¼°ã€ç¨³å¥æ€§ç ”ç©¶å’Œäººæœºäº¤å‰å®éªŒéªŒè¯äº†åŸºå‡†æµ‹è¯•è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘ç°LLMçš„æ¸¸æˆèƒ½åŠ›è½åäºäººç±»å’Œç®€å•çš„è¯åµŒå…¥åŸºçº¿ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åœ¨Decryptoä¸­åˆ›å»ºäº†ä¸¤ä¸ªç»å…¸è®¤çŸ¥ç§‘å­¦å®éªŒçš„å˜ç§ï¼Œä»¥è¯„ä¼°ä¸‰é¡¹å…³é”®çš„ToMèƒ½åŠ›ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°è¿œä¸å¦‚æ—§æ¨¡å‹ã€‚è¿™è¡¨æ˜Decryptoè§£å†³äº†å½“å‰æ¨ç†å’ŒToMè¯„ä¼°ä¸­çš„å…³é”®ç©ºç™½ï¼Œå¹¶ä¸ºæ›´å¥½çš„äººå·¥æ™ºèƒ½ä»£ç†é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€åº”å¯¹å¤šä»£ç†åœºæ™¯ï¼Œè¿™è¦æ±‚å…·å¤‡æ–°çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨å¤šç§é—®é¢˜ï¼Œå¦‚èŒƒå›´ç‹­çª„ã€æ•°æ®æ³„éœ²å’Œç¼ºä¹äº¤äº’æ€§ã€‚</li>
<li>æå‡ºäº†åŸºäºæ¸¸æˆçš„åŸºå‡†æµ‹è¯•Decryptoï¼Œç”¨äºå¤šä»£ç†æ¨ç†å’ŒToMè¯„ä¼°ã€‚</li>
<li>Decryptoå¹³å°è®¾è®¡æ—¨åœ¨æ¶ˆé™¤å¹²æ‰°å› ç´ ï¼Œæ˜¯é¦–ä¸ªäº¤äº’å¼ToMå®éªŒå¹³å°ã€‚</li>
<li>LLMåœ¨æ¸¸æˆèƒ½åŠ›ä¸Šè½åäºäººç±»å’Œè¯åµŒå…¥åŸºçº¿ã€‚</li>
<li>Decryptoé€šè¿‡ç»å…¸è®¤çŸ¥ç§‘å­¦å®éªŒçš„å˜ç§è¯„ä¼°äº†LLMçš„å…³é”®ToMèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2edcf303d1122d08144611c632f2f348.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f3c1ed44efb0cbfb794708947c3304d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4618ab1b41a9c4225ff843cb752c5c28.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Towards-Community-Driven-Agents-for-Machine-Learning-Engineering"><a href="#Towards-Community-Driven-Agents-for-Machine-Learning-Engineering" class="headerlink" title="Towards Community-Driven Agents for Machine Learning Engineering"></a>Towards Community-Driven Agents for Machine Learning Engineering</h2><p><strong>Authors:Sijie Li, Weiwei Sun, Shanda Li, Ameet Talwalkar, Yiming Yang</strong></p>
<p>Large language model-based machine learning (ML) agents have shown great promise in automating ML research. However, existing agents typically operate in isolation on a given research problem, without engaging with the broader research community, where human researchers often gain insights and contribute by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live evaluation framework designed to assess an agentâ€™s ability to communicate with and leverage collective knowledge from a simulated Kaggle research community. Building on this framework, we propose CoMind, a novel agent that excels at exchanging insights and developing novel solutions within a community context. CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2% human competitors on average across four ongoing Kaggle competitions. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/comind-ml/CoMind">https://github.com/comind-ml/CoMind</a>. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä»£ç†åœ¨è‡ªåŠ¨åŒ–MLç ”ç©¶æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä»£ç†é€šå¸¸å­¤ç«‹åœ°å¤„ç†ç»™å®šç ”ç©¶é—®é¢˜ï¼Œè€Œæ²¡æœ‰ä¸æ›´å¹¿æ³›çš„ç ”ç©¶ç¤¾åŒºäº’åŠ¨ï¼Œè€Œäººç±»ç ”ç©¶è€…é€šå¸¸é€šè¿‡åˆ†äº«çŸ¥è¯†æ¥è·å¾—æ´å¯ŸåŠ›å¹¶åšå‡ºè´¡çŒ®ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†MLE-Liveï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°ä»£ç†ä¸æ¨¡æ‹ŸKaggleç ”ç©¶ç¤¾åŒºè¿›è¡Œäº¤æµå¹¶åˆ©ç”¨é›†ä½“çŸ¥è¯†çš„èƒ½åŠ›çš„å®æ—¶è¯„ä¼°æ¡†æ¶ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†CoMindï¼Œè¿™æ˜¯ä¸€ä¸ªæ“…é•¿åœ¨ç¤¾åŒºç¯å¢ƒä¸­äº¤æµè§è§£å¹¶å¼€å‘æ–°è§£å†³æ–¹æ¡ˆçš„æ–°å‹ä»£ç†ã€‚CoMindåœ¨MLE-Liveä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å››ä¸ªæ­£åœ¨è¿›è¡Œçš„Kaggleç«èµ›ä¸­å¹³å‡å‡»è´¥äº†79.2%çš„äººç±»ç«äº‰å¯¹æ‰‹ã€‚æˆ‘ä»¬çš„ä»£ç å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/comind-ml/CoMind%E3%80%82">https://github.com/comind-ml/CoMindã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20640v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æœºå™¨å­¦ä¹ ä»£ç†åœ¨è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ç ”ç©¶æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç°æœ‰ä»£ç†é€šå¸¸å­¤ç«‹åœ°è§£å†³ç‰¹å®šé—®é¢˜ï¼Œç¼ºä¹ä¸å¹¿æ³›çš„ç ”ç©¶ç¤¾åŒºäº¤æµäº’åŠ¨ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºMLE-Liveè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°ä»£ç†ä¸æ¨¡æ‹ŸKaggleç ”ç©¶ç¤¾åŒºçš„æ²Ÿé€šèƒ½åŠ›ä»¥åŠåˆ©ç”¨é›†ä½“çŸ¥è¯†çš„èƒ½åŠ›ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬æ¨å‡ºCoMindä»£ç†ï¼Œæ“…é•¿åœ¨ç¤¾åŒºç¯å¢ƒä¸­äº¤æµè§è§£å¹¶å¼€å‘æ–°è§£å†³æ–¹æ¡ˆã€‚CoMindåœ¨MLE-Liveä¸Šè¡¨ç°å“è¶Šï¼Œå¹³å‡åœ¨å››ä¸ªè¿›è¡Œçš„Kaggleç«èµ›ä¸­è¶…è¶Šäº†79.2%çš„äººç±»ç«äº‰å¯¹æ‰‹ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/comind-ml/CoMind">ç½‘å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æœºå™¨å­¦ä¹ ä»£ç†åœ¨è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ç ”ç©¶æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰ä»£ç†å¤§å¤šå­¤ç«‹è¿è¡Œï¼Œç¼ºä¹ä¸å¹¿æ³›ç ”ç©¶ç¤¾åŒºçš„äº’åŠ¨ã€‚</li>
<li>MLE-Liveè¯„ä¼°æ¡†æ¶æ—¨åœ¨è¯„ä¼°ä»£ç†ä¸æ¨¡æ‹Ÿç ”ç©¶ç¤¾åŒºçš„äº¤äº’èƒ½åŠ›ã€‚</li>
<li>CoMindä»£ç†åœ¨MLE-Liveä¸Šè¡¨ç°å“è¶Šï¼Œå…·å¤‡åœ¨æ¨¡æ‹Ÿç¤¾åŒºç¯å¢ƒä¸­äº¤æµè§è§£å’Œå¼€å‘æ–°è§£å†³æ–¹æ¡ˆçš„èƒ½åŠ›ã€‚</li>
<li>CoMindåœ¨å¤šä¸ªKaggleç«èµ›ä¸­å¹³å‡è¶…è¶Šäº†å¤§éƒ¨åˆ†äººç±»ç«äº‰å¯¹æ‰‹ã€‚</li>
<li>CoMindä»£ç†çš„ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä¾›å…¬ä¼—æŸ¥é˜…å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20640">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa54024434c0ed81ac5ec7aee270d998.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-903826177fc7ccba3c3d21d1685e3579.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b80b5fe056a8daa761053ec93500776e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DiffuCoder-Understanding-and-Improving-Masked-Diffusion-Models-for-Code-Generation"><a href="#DiffuCoder-Understanding-and-Improving-Masked-Diffusion-Models-for-Code-Generation" class="headerlink" title="DiffuCoder: Understanding and Improving Masked Diffusion Models for Code   Generation"></a>DiffuCoder: Understanding and Improving Masked Diffusion Models for Code   Generation</h2><p><strong>Authors:Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, Yizhe Zhang</strong></p>
<p>Diffusion large language models (dLLMs) are compelling alternatives to autoregressive (AR) models because their denoising models operate over the entire sequence. The global planning and iterative refinement features of dLLMs are particularly useful for code generation. However, current training and inference mechanisms for dLLMs in coding are still under-explored. To demystify the decoding behavior of dLLMs and unlock their potential for coding, we systematically investigate their denoising processes and reinforcement learning (RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code. Using this model as a testbed, we analyze its decoding behavior, revealing how it differs from that of AR models: (1) dLLMs can decide how causal their generation should be without relying on semi-AR decoding, and (2) increasing the sampling temperature diversifies not only token choices but also their generation order. This diversity creates a rich search space for RL rollouts. For RL training, to reduce the variance of token log-likelihood estimates and maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel sampling scheme that constructs complementary mask noise for completions used in training. In our experiments, coupled-GRPO significantly improves DiffuCoderâ€™s performance on code generation benchmarks (+4.4% on EvalPlus) and reduces reliance on AR causal during decoding. Our work provides deeper insight into the machinery of dLLM generation and offers an effective, diffusion-native RL training framework. <a target="_blank" rel="noopener" href="https://github.com/apple/ml-diffucoder">https://github.com/apple/ml-diffucoder</a>. </p>
<blockquote>
<p>æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä½œä¸ºè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹çš„å¸å¼•äººçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå› ä¸ºå®ƒä»¬å»å™ªæ¨¡å‹åœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿è¡Œã€‚dLLMsçš„å…¨å±€è§„åˆ’å’Œè¿­ä»£ä¼˜åŒ–åŠŸèƒ½å¯¹äºä»£ç ç”Ÿæˆç‰¹åˆ«æœ‰ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰é’ˆå¯¹dLLMsåœ¨ç¼–ç æ–¹é¢çš„è®­ç»ƒå’Œæ¨ç†æœºåˆ¶ä»ç„¶ç¼ºä¹ç ”ç©¶ã€‚ä¸ºäº†æ­å¼€dLLMsè§£ç è¡Œä¸ºçš„å¥¥ç§˜å¹¶è§£é”å…¶åœ¨ç¼–ç æ–¹é¢çš„æ½œåŠ›ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†å®ƒä»¬çš„å»å™ªè¿‡ç¨‹å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨åŒ…å«ä»£ç çš„è¶…è¿‡ å·¨å¤§å­—èŠ‚çš„æ•°æ®é›†ä¸Šè®­ç»ƒäº†ä¸€ä¸ªåŒ…å«æœ‰åºå¤§å‚æ•°æ•°çš„é€šç”¨ç‰ˆä»£ç æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œåç§°ä¸ºâ€œDiffuCoderâ€ã€‚é€šè¿‡è¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬åˆ†æäº†å…¶è§£ç è¡Œä¸ºï¼Œå¹¶æ­ç¤ºäº†å…¶ä¸ARæ¨¡å‹çš„ä¸åŒä¹‹å¤„ï¼šé¦–å…ˆæ˜¯è‡ªå›å½’æ¨¡å‹ä¸­æ— å…³çš„æ½œåœ¨ç¼ºé™·æ‰€é©±åŠ¨çš„é©¬å°”å¯å¤«ç”Ÿæˆé—®é¢˜æ— æ³•å½»åº•å¾—åˆ°è§£å†³ï¼›ï¼ˆå…·ä½“åˆ°è¯¥é—®é¢˜å†…éƒ¨æœºåˆ¶é‡Œï¼Œï¼‰å°½ç®¡æˆ‘ä»¬å¯ä»¥é€šè¿‡è‡ªä¸»å†³å®šçš„å»¶è¿Ÿæ¥è§£å†³ç­–ç•¥å…ˆè¡Œæˆ–è€…å›æŠ¥åˆ©ç”¨è¿‡å¤šçš„é—®é¢˜ã€‚å…¶æ¬¡ï¼Œæé«˜é‡‡æ ·æ¸©åº¦ä¸ä»…ä½¿ä»¤ç‰Œé€‰æ‹©å¤šæ ·åŒ–ï¼Œè€Œä¸”æ”¹å˜äº†ä»¤ç‰Œçš„ç”Ÿæˆé¡ºåºã€‚è¿™ç§å¤šæ ·æ€§ä¸ºRLè®­ç»ƒæä¾›äº†ä¸°å¯Œçš„æœç´¢ç©ºé—´ã€‚å¯¹äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒè€Œè¨€ï¼Œä¸ºäº†å‡å°‘ä»¤ç‰Œå¯¹æ•°ä¼¼ç„¶ä¼°è®¡çš„æ–¹å·®å¹¶ä¿æŒè®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œè€¦åˆ-GRPOâ€çš„æ–°å‹é‡‡æ ·æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆé’ˆå¯¹è®­ç»ƒä¸­æ‰€ä½¿ç”¨çš„å®Œæˆé¡¹æ„å»ºäº’è¡¥æ©ç å™ªå£°ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œâ€œè€¦åˆ-GRPOâ€æ˜¾è‘—æé«˜äº†DiffuCoderåœ¨ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼ˆ+EvalPlusæå‡ç‡ä¸º4.4%ï¼‰ï¼Œå¹¶å‡å°‘äº†è§£ç è¿‡ç¨‹ä¸­å¯¹äºè‡ªå›å½’å› æœæ€§çš„ä¾èµ–ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºdLLMç”Ÿæˆæœºåˆ¶æä¾›äº†æ›´æ·±å…¥çš„äº†è§£ï¼Œå¹¶æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„ã€åŸºäºæ‰©æ•£çš„RLè®­ç»ƒæ¡†æ¶ã€‚å¦‚éœ€äº†è§£æ›´å¤šè¯¦æƒ…ï¼Œè¯·è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/apple/ml-diffucoder">https://github.com/apple/ml-diffucoder</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20639v1">PDF</a> preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Diffusion Large Language Modelsï¼ˆdLLMsï¼‰åœ¨ç¼–ç¨‹ä»£ç ç”Ÿæˆæ–¹é¢çš„ä¼˜åŠ¿å’Œåº”ç”¨ã€‚é€šè¿‡ç³»ç»Ÿåœ°ç ”ç©¶å…¶é™å™ªè¿‡ç¨‹å’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä½œè€…è®­ç»ƒäº†ä¸€ä¸ªåä¸ºDiffuCoderçš„7B dLLMæ¨¡å‹ï¼Œå¹¶åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚dLLMsèƒ½å¤Ÿåœ¨ä¸ä½¿ç”¨åŠè‡ªå›å½’è§£ç çš„æƒ…å†µä¸‹å†³å®šç”Ÿæˆçš„å› æœæ€§ï¼Œå¢åŠ é‡‡æ ·æ¸©åº¦å¯ä»¥å¤šæ ·åŒ–ç”Ÿæˆçš„æ ‡è®°å’Œé¡ºåºã€‚ä¸ºäº†æ›´æœ‰æ•ˆåœ°è®­ç»ƒRLï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„é‡‡æ ·æ–¹æ¡ˆâ€”â€”è€¦åˆGRPOï¼Œè¯¥æ–¹æ¡ˆä¸ºè®­ç»ƒä¸­çš„å®Œæˆéƒ¨åˆ†æ„å»ºäº’è¡¥æ©ç å™ªå£°ã€‚å®éªŒè¡¨æ˜ï¼Œè€¦åˆGRPOèƒ½æ˜¾è‘—æé«˜DiffuCoderåœ¨ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼Œå¹¶å‡å°‘è§£ç è¿‡ç¨‹ä¸­çš„ARå› æœä¾èµ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>dLLMsé€šè¿‡åœ¨æ•´ä¸ªåºåˆ—ä¸Šæ“ä½œçš„é™å™ªæ¨¡å‹æˆä¸ºå¸å¼•äººçš„è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£ç ç”Ÿæˆæ–¹é¢ã€‚</li>
<li>dLLMsçš„è§£ç è¡Œä¸ºä¸ARæ¨¡å‹ä¸åŒï¼Œå¯ä»¥åœ¨ä¸ä¾èµ–åŠè‡ªå›å½’è§£ç çš„æƒ…å†µä¸‹å†³å®šç”Ÿæˆçš„å› æœæ€§ã€‚</li>
<li>å¢åŠ é‡‡æ ·æ¸©åº¦å¯ä»¥å¤šæ ·åŒ–æ ‡è®°çš„é€‰æ‹©å’Œç”Ÿæˆé¡ºåºï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒåˆ›å»ºä¸€ä¸ªä¸°å¯Œçš„æœç´¢ç©ºé—´ã€‚</li>
<li>ä½œè€…æå‡ºäº†ä¸€ä¸ªæ–°çš„é‡‡æ ·æ–¹æ¡ˆâ€”â€”è€¦åˆGRPOï¼Œä»¥æé«˜RLè®­ç»ƒçš„æ•ˆç‡å¹¶å‡å°‘æ ‡è®°æ—¥å¿—ä¼¼ç„¶ä¼°è®¡çš„æ–¹å·®ã€‚</li>
<li>åä¸ºDiffuCoderçš„dLLMæ¨¡å‹åœ¨ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
<li>DiffuCoderçš„æ€§èƒ½æå‡å¾—ç›Šäºå…¶æ–°çš„è§£ç æ–¹æ³•å’ŒRLè®­ç»ƒæ¡†æ¶ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºdLLMç”Ÿæˆæœºåˆ¶æä¾›äº†æ·±å…¥è§è§£ï¼Œå¹¶ä¸ºæ‰©æ•£åŸç”ŸRLè®­ç»ƒæ¡†æ¶æä¾›äº†æœ‰æ•ˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8a5b0b51e74200edd6980f3bfbdb579.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fab6f03b8677a7805ff29cfa9802755.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-894b6e6328ece7c23bc3d00fd9739b61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc7edce831c554d0a6d32308f871a1e8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Video-Perception-Models-for-3D-Scene-Synthesis"><a href="#Video-Perception-Models-for-3D-Scene-Synthesis" class="headerlink" title="Video Perception Models for 3D Scene Synthesis"></a>Video Perception Models for 3D Scene Synthesis</h2><p><strong>Authors:Rui Huang, Guangyao Zhai, Zuria Bauer, Marc Pollefeys, Federico Tombari, Leonidas Guibas, Gao Huang, Francis Engelmann</strong></p>
<p>Traditionally, 3D scene synthesis requires expert knowledge and significant manual effort. Automating this process could greatly benefit fields such as architectural design, robotics simulation, virtual reality, and gaming. Recent approaches to 3D scene synthesis often rely on the commonsense reasoning of large language models (LLMs) or strong visual priors of modern image generation models. However, current LLMs demonstrate limited 3D spatial reasoning ability, which restricts their ability to generate realistic and coherent 3D scenes. Meanwhile, image generation-based methods often suffer from constraints in viewpoint selection and multi-view inconsistencies. In this work, we present Video Perception models for 3D Scene synthesis (VIPScene), a novel framework that exploits the encoded commonsense knowledge of the 3D physical world in video generation models to ensure coherent scene layouts and consistent object placements across views. VIPScene accepts both text and image prompts and seamlessly integrates video generation, feedforward 3D reconstruction, and open-vocabulary perception models to semantically and geometrically analyze each object in a scene. This enables flexible scene synthesis with high realism and structural consistency. For more precise analysis, we further introduce First-Person View Score (FPVScore) for coherence and plausibility evaluation, utilizing continuous first-person perspective to capitalize on the reasoning ability of multimodal large language models. Extensive experiments show that VIPScene significantly outperforms existing methods and generalizes well across diverse scenarios. The code will be released. </p>
<blockquote>
<p>ä¼ ç»Ÿä¸Šï¼Œ3Dåœºæ™¯åˆæˆéœ€è¦ä¸“ä¸šçŸ¥è¯†å¹¶éœ€è¦å¤§é‡æ‰‹åŠ¨æ“ä½œã€‚è‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹å¯èƒ½å¯¹å»ºç­‘è®¾è®¡ã€æœºå™¨äººä»¿çœŸã€è™šæ‹Ÿç°å®å’Œæ¸¸æˆç­‰é¢†åŸŸå¤§æœ‰è£¨ç›Šã€‚æœ€è¿‘çš„3Dåœºæ™¯åˆæˆæ–¹æ³•å¸¸å¸¸ä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¸¸è¯†æ¨ç†æˆ–ç°ä»£å›¾åƒç”Ÿæˆæ¨¡å‹çš„å¼ºå¤§è§†è§‰å…ˆéªŒã€‚ç„¶è€Œï¼Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºæœ‰é™çš„3Dç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬ç”ŸæˆçœŸå®å’Œè¿è´¯çš„3Dåœºæ™¯çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼ŒåŸºäºå›¾åƒç”Ÿæˆçš„æ–¹æ³•å¸¸å¸¸å—åˆ°è§†ç‚¹é€‰æ‹©å’Œå¤šè§†è§’ä¸ä¸€è‡´æ€§çš„çº¦æŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20601v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºè§†é¢‘æ„ŸçŸ¥æ¨¡å‹çš„3Dåœºæ™¯åˆæˆï¼ˆVIPSceneï¼‰åˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­çš„å¸¸è¯†çŸ¥è¯†ï¼Œç¡®ä¿åœºæ™¯å¸ƒå±€è¿è´¯ï¼Œä¸åŒè§†è§’çš„å¯¹è±¡æ”¾ç½®ä¸€è‡´ã€‚è¯¥æ¡†æ¶æ¥å—æ–‡æœ¬å’Œå›¾åƒæç¤ºï¼Œæ— ç¼é›†æˆè§†é¢‘ç”Ÿæˆã€å‰é¦ˆ3Dé‡å»ºå’Œå¼€æ”¾è¯æ±‡æ„ŸçŸ¥æ¨¡å‹ï¼Œå¯¹åœºæ™¯ä¸­çš„æ¯ä¸ªå¯¹è±¡è¿›è¡Œè¯­ä¹‰å’Œå‡ ä½•åˆ†æã€‚è¿™ä¸ºé«˜ç°å®æ€§ã€ç»“æ„ä¸€è‡´æ€§çš„çµæ´»åœºæ™¯åˆæˆæä¾›äº†å¯èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VIPSceneåˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­çš„å¸¸è¯†çŸ¥è¯†æ¥ç¡®ä¿è¿è´¯çš„åœºæ™¯å¸ƒå±€å’Œä¸€è‡´çš„è·¨è§†è§’å¯¹è±¡æ”¾ç½®ã€‚</li>
<li>è¯¥æ¡†æ¶å¯ä»¥æ¥å—æ–‡æœ¬å’Œå›¾åƒæç¤ºæ¥è¿›è¡Œåœºæ™¯åˆæˆã€‚</li>
<li>VIPSceneç»“åˆäº†è§†é¢‘ç”Ÿæˆã€å‰é¦ˆ3Dé‡å»ºå’Œæ„ŸçŸ¥æ¨¡å‹ã€‚</li>
<li>å®ƒèƒ½å¤Ÿå®ç°çµæ´»çš„åœºæ™¯åˆæˆï¼Œå¹¶ç¡®ä¿é«˜ç°å®æ€§å’Œç»“æ„ä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥ç¬¬ä¸€äººç§°è§†è§’å¾—åˆ†ï¼ˆFPVScoreï¼‰ç”¨äºè¯„ä¼°åœºæ™¯çš„è¿è´¯æ€§å’Œåˆç†æ€§ã€‚</li>
<li>VIPSceneåœ¨å®éªŒä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œå¹¶èƒ½åœ¨ä¸åŒçš„åœºæ™¯ä¸­å¾ˆå¥½åœ°æ¨å¹¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5fdd369ebe958ca3dee71570684739a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e376b160c3d4577103790f2e6ce08135.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-444ecfe291803ea1ade0e301005fc635.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5236392d5fadd6b1a9c1a6b0b3434d9c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="WattsOnAI-Measuring-Analyzing-and-Visualizing-Energy-and-Carbon-Footprint-of-AI-Workloads"><a href="#WattsOnAI-Measuring-Analyzing-and-Visualizing-Energy-and-Carbon-Footprint-of-AI-Workloads" class="headerlink" title="WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon   Footprint of AI Workloads"></a>WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon   Footprint of AI Workloads</h2><p><strong>Authors:Hongzhen Huang, Kunming Zhang, Hanlong Liao, Kui Wu, Guoming Tang</strong></p>
<p>The rapid advancement of AI, particularly large language models (LLMs), has raised significant concerns about the energy use and carbon emissions associated with model training and inference. However, existing tools for measuring and reporting such impacts are often fragmented, lacking systematic metric integration and offering limited support for correlation analysis among them. This paper presents WattsOnAI, a comprehensive software toolkit for the measurement, analysis, and visualization of energy use, power draw, hardware performance, and carbon emissions across AI workloads. By seamlessly integrating with existing AI frameworks, WattsOnAI offers standardized reports and exports fine-grained time-series data to support benchmarking and reproducibility in a lightweight manner. It further enables in-depth correlation analysis between hardware metrics and model performance and thus facilitates bottleneck identification and performance enhancement. By addressing critical limitations in existing tools, WattsOnAI encourages the research community to weigh environmental impact alongside raw performance of AI workloads and advances the shift toward more sustainable â€œGreen AIâ€ practices. The code is available at <a target="_blank" rel="noopener" href="https://github.com/SusCom-Lab/WattsOnAI">https://github.com/SusCom-Lab/WattsOnAI</a>. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å°¤å…¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å¼•å‘äº†äººä»¬å¯¹æ¨¡å‹è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­èƒ½è€—å’Œç¢³æ’æ”¾çš„æ‹…å¿§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç”¨äºæµ‹é‡å’ŒæŠ¥å‘Šæ­¤ç±»å½±å“çš„å·¥å…·é€šå¸¸å¾ˆåˆ†æ•£ï¼Œç¼ºä¹ç³»ç»Ÿçš„æŒ‡æ ‡æ•´åˆï¼Œå¹¶ä¸”åœ¨å®ƒä»¬ä¹‹é—´æä¾›å…³è”åˆ†æçš„æ”¯æŒæœ‰é™ã€‚æœ¬æ–‡ä»‹ç»äº†WattsOnAIï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è½¯ä»¶å·¥å…·åŒ…ï¼Œç”¨äºæµ‹é‡ã€åˆ†æå’Œå¯è§†åŒ–äººå·¥æ™ºèƒ½å·¥ä½œè´Ÿè½½çš„èƒ½è€—ã€åŠŸç‡æ¶ˆè€—ã€ç¡¬ä»¶æ€§èƒ½å’Œç¢³æ’æ”¾ã€‚WattsOnAIæ— ç¼é›†æˆç°æœ‰çš„AIæ¡†æ¶ï¼Œæä¾›æ ‡å‡†åŒ–æŠ¥å‘Šå¹¶å¯¼å‡ºç²¾ç»†çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œä»¥æ”¯æŒåŸºå‡†æµ‹è¯•å’Œè½»é‡çº§çš„å¯é‡å¤æ€§ã€‚å®ƒè¿˜å¯ä»¥è¿›è¡Œç¡¬ä»¶æŒ‡æ ‡ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„æ·±å…¥åˆ†æï¼Œä»è€Œæœ‰åŠ©äºè¯†åˆ«ç“¶é¢ˆå’Œæé«˜æ€§èƒ½ã€‚é€šè¿‡è§£å†³ç°æœ‰å·¥å…·çš„å…³é”®å±€é™æ€§ï¼ŒWattsOnAIé¼“åŠ±ç ”ç©¶ç¤¾åŒºæƒè¡¡äººå·¥æ™ºèƒ½å·¥ä½œè´Ÿè½½çš„åŸå§‹æ€§èƒ½ä¸ç¯å¢ƒå½±å“ï¼Œæ¨åŠ¨å‘æ›´å¯æŒç»­çš„â€œç»¿è‰²AIâ€å®è·µè½¬å˜ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SusCom-Lab/WattsOnAI%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SusCom-Lab/WattsOnAIæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20535v1">PDF</a> 11 pages, 7 figures and 5 tables</p>
<p><strong>Summary</strong></p>
<p>éšç€äººå·¥æ™ºèƒ½å°¤å…¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå…¶ç›¸å…³çš„èƒ½è€—ä¸ç¢³æ’æ”¾é—®é¢˜å¤‡å—å…³æ³¨ã€‚ç°æœ‰å·¥å…·å­˜åœ¨ç¼ºä¹ç»Ÿä¸€çš„è¡¡é‡æ ‡å‡†å’Œå…¨é¢çš„åˆ†æèƒ½åŠ›çš„é—®é¢˜ã€‚è€Œæœ¬è®ºæ–‡æ¨å‡ºçš„WattsOnAIè½¯ä»¶å·¥å…·å¥—ä»¶ä¸ºAIå·¥ä½œè´Ÿè½½çš„èƒ½è€—æµ‹é‡ä¸ç¢³åˆ†ææä¾›äº†ä¸€ä¸ªç»¼åˆå¹³å°ã€‚å®ƒå¯ä»¥æ— ç¼å¯¹æ¥ç°æœ‰çš„AIæ¡†æ¶ï¼Œå®ç°èƒ½æºä½¿ç”¨ã€ç”µåŠ›æ¶ˆè€—ã€ç¡¬ä»¶æ€§èƒ½åŠç¢³æ’æ”¾æ•°æ®çš„ç»¼åˆåº¦é‡ä¸å¯è§†åŒ–åˆ†æã€‚å®ƒæä¾›æ ‡å‡†åŒ–çš„æŠ¥å‘ŠåŠç»†ç²’åº¦çš„æ—¶é—´åºåˆ—æ•°æ®å¯¼å‡ºï¼Œä¸ºæ€§èƒ½è¯„ä¼°æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚åŒæ—¶ï¼Œé€šè¿‡æ·±åº¦åˆ†æç¡¬ä»¶æŒ‡æ ‡ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„å…³è”ï¼Œå®ƒèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ‰¾å‡ºç“¶é¢ˆå¹¶è¿›è¡Œæ€§èƒ½ä¼˜åŒ–ã€‚è¿™æ¬¾å·¥å…·çš„æ¨å‡ºï¼Œä¿ƒè¿›äº†AIç ”ç©¶é¢†åŸŸå¯¹å·¥ä½œé‡åŸå§‹æ€§èƒ½ä¸ç¯å¢ƒå½±å“å¹¶é‡è¡¡é‡ç ”ç©¶çš„å…´èµ·ï¼Œä¹Ÿæ ‡å¿—ç€AIèµ°å‘æ›´ç»¿è‰²å¯æŒç»­å‘å±•æ–¹å‘çš„æ–°å°è¯•ã€‚è·å–è¯¥å·¥å…·ä»£ç çš„ç½‘å€ä¸ºï¼š[é“¾æ¥åœ°å€]ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/SusCom-Lab/WattsOnAI%EF%BC%89%E3%80%82">https://github.com/SusCom-Lab/WattsOnAIï¼‰ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AIçš„å¿«é€Ÿè¿›æ­¥å¼•èµ·äº†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹èƒ½è€—ä¸ç¢³æ’æ”¾çš„å¹¿æ³›æ‹…å¿§ã€‚</li>
<li>ç›®å‰ç›¸å…³çš„èƒ½è€—è¡¡é‡å’Œåˆ†æå·¥å…·å­˜åœ¨æ ‡å‡†åŒ–é—®é¢˜ä¸”æ— æ³•å……åˆ†åæ˜ å…³è”æ€§åˆ†æã€‚</li>
<li>WattsOnAIå·¥å…·å¥—ä»¶æä¾›äº†å…¨é¢çš„èƒ½æºä½¿ç”¨ã€ç”µåŠ›æ¶ˆè€—ã€ç¡¬ä»¶æ€§èƒ½å’Œç¢³æ’æ”¾çš„æµ‹é‡ä¸åˆ†æåŠŸèƒ½ã€‚</li>
<li>WattsOnAIèƒ½å¤Ÿæ— ç¼å¯¹æ¥ç°æœ‰AIæ¡†æ¶ï¼Œå®ç°æ•°æ®å¯è§†åŒ–å¹¶æä¾›æ ‡å‡†åŒ–æŠ¥å‘Šè¾“å‡ºã€‚</li>
<li>å®ƒå…·å¤‡ç»†ç²’åº¦æ—¶é—´åºåˆ—æ•°æ®çš„å¯¼å‡ºåŠŸèƒ½ï¼Œä¸ºæ€§èƒ½è¯„ä¼°æä¾›äº†é‡è¦ä¾æ®ã€‚</li>
<li>é€šè¿‡æ·±åº¦åˆ†æç¡¬ä»¶æŒ‡æ ‡ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„å…³è”ï¼Œå¸®åŠ©è¯†åˆ«ç“¶é¢ˆå¹¶ä¼˜åŒ–æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20535">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a8953bd933a1a78901ca4f201b4bcc5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c22e0845b061e1ac5a50305275acd062.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdeedb11d9564e36f8f57202f5e0d642.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f0006bf473bc84337d611648894da49.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cbb1929cf743b885d85ae1eb1de69655.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e577be7730ee796909b747856c0cca81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2579808bc389b506f9d0cd97cf47c694.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4e52efc2b51e90e3f315815aaab322d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Case-based-Reasoning-Augmented-Large-Language-Model-Framework-for-Decision-Making-in-Realistic-Safety-Critical-Driving-Scenarios"><a href="#Case-based-Reasoning-Augmented-Large-Language-Model-Framework-for-Decision-Making-in-Realistic-Safety-Critical-Driving-Scenarios" class="headerlink" title="Case-based Reasoning Augmented Large Language Model Framework for   Decision Making in Realistic Safety-Critical Driving Scenarios"></a>Case-based Reasoning Augmented Large Language Model Framework for   Decision Making in Realistic Safety-Critical Driving Scenarios</h2><p><strong>Authors:Wenbin Gan, Minh-Son Dao, Koji Zettsu</strong></p>
<p>Driving in safety-critical scenarios requires quick, context-aware decision-making grounded in both situational understanding and experiential reasoning. Large Language Models (LLMs), with their powerful general-purpose reasoning capabilities, offer a promising foundation for such decision-making. However, their direct application to autonomous driving remains limited due to challenges in domain adaptation, contextual grounding, and the lack of experiential knowledge needed to make reliable and interpretable decisions in dynamic, high-risk environments. To address this gap, this paper presents a Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for evasive maneuver decision-making in complex risk scenarios. Our approach integrates semantic scene understanding from dashcam video inputs with the retrieval of relevant past driving cases, enabling LLMs to generate maneuver recommendations that are both context-sensitive and human-aligned. Experiments across multiple open-source LLMs show that our framework improves decision accuracy, justification quality, and alignment with human expert behavior. Risk-aware prompting strategies further enhance performance across diverse risk types, while similarity-based case retrieval consistently outperforms random sampling in guiding in-context learning. Case studies further demonstrate the frameworkâ€™s robustness in challenging real-world conditions, underscoring its potential as an adaptive and trustworthy decision-support tool for intelligent driving systems. </p>
<blockquote>
<p>åœ¨å…³é”®å®‰å…¨åœºæ™¯ä¸­é©¾é©¶éœ€è¦å¿«é€Ÿã€åŸºäºæƒ…å¢ƒçš„å†³ç­–åˆ¶å®šï¼Œè¿™æ—¢éœ€è¦ç†è§£æƒ…å¢ƒï¼Œä¹Ÿéœ€è¦ç»éªŒæ¨ç†ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡­å€Ÿå…¶å¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä¸ºè¿™ç§å†³ç­–åˆ¶å®šæä¾›äº†æœ‰å‰æ™¯çš„åŸºç¡€ã€‚ç„¶è€Œï¼Œå°†å…¶ç›´æ¥åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºåœ¨é¢†åŸŸé€‚åº”ã€æƒ…å¢ƒåŸºç¡€åŒ–ä»¥åŠåœ¨åŠ¨æ€ã€é«˜é£é™©ç¯å¢ƒä¸­åšå‡ºå¯é å’Œå¯è§£é‡Šå†³ç­–æ‰€éœ€ç»éªŒçŸ¥è¯†çš„ç¼ºä¹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¡ˆä¾‹çš„æ¨ç†å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆCBR-LLMï¼‰æ¡†æ¶ï¼Œç”¨äºå¤æ‚é£é™©åœºæ™¯ä¸­çš„é¿è®©å†³ç­–åˆ¶å®šã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä»è¡Œè½¦è®°å½•ä»ªè§†é¢‘è¾“å…¥ä¸­çš„è¯­ä¹‰åœºæ™¯ç†è§£ä»¥åŠä¸ç›¸å…³è¿‡å»é©¾é©¶æ¡ˆä¾‹çš„æ£€ç´¢ï¼Œä½¿LLMèƒ½å¤Ÿç”Ÿæˆæ—¢æ•æ„Ÿäºä¸Šä¸‹æ–‡åˆç¬¦åˆäººç±»è¡Œä¸ºçš„æœºåŠ¨å»ºè®®ã€‚è·¨å¤šä¸ªå¼€æºLLMçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æé«˜äº†å†³ç­–å‡†ç¡®æ€§ã€è§£é‡Šè´¨é‡å’Œä¸äººç±»ä¸“å®¶è¡Œä¸ºçš„ä¸€è‡´æ€§ã€‚é£é™©æ„ŸçŸ¥æç¤ºç­–ç•¥è¿›ä¸€æ­¥å¢å¼ºäº†ä¸åŒç±»å‹é£é™©çš„æ€§èƒ½ï¼Œè€ŒåŸºäºç›¸ä¼¼æ€§çš„æ¡ˆä¾‹æ£€ç´¢åœ¨æŒ‡å¯¼ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹é¢å§‹ç»ˆä¼˜äºéšæœºæŠ½æ ·ã€‚æ¡ˆä¾‹ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†è¯¥æ¡†æ¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ï¼Œçªæ˜¾äº†å…¶åœ¨æ™ºèƒ½é©¾é©¶ç³»ç»Ÿä¸­ä½œä¸ºè‡ªé€‚åº”å’Œå¯é çš„å†³ç­–æ”¯æŒå·¥å…·çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20531v1">PDF</a> 12 pages, 10 figures, under-review conference</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨é©¾é©¶å®‰å…¨å†³ç­–ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹è‡ªåŠ¨é©¾é©¶åœ¨å¤æ‚é£é™©åœºæ™¯ä¸­çš„å†³ç­–æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ¡ˆä¾‹æ¨ç†å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆCBR-LLMï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†é©¾é©¶åœºæ™¯ç†è§£å’Œç›¸å…³é©¾é©¶æ¡ˆä¾‹æ£€ç´¢ï¼Œä½¿LLMèƒ½å¤Ÿç”Ÿæˆæ—¢ç¬¦åˆä¸Šä¸‹æ–‡åˆç¬¦åˆäººç±»å†³ç­–çš„æœºåŠ¨å»ºè®®ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æé«˜äº†å†³ç­–å‡†ç¡®æ€§ã€è§£é‡Šè´¨é‡ä»¥åŠä¸äººç±»ä¸“å®¶è¡Œä¸ºçš„å¥‘åˆåº¦ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶å®‰å…¨å†³ç­–æä¾›äº†æœ‰å‰é€”çš„åŸºç¡€ã€‚</li>
<li>ç›´æ¥åº”ç”¨LLMåˆ°è‡ªåŠ¨é©¾é©¶é¢ä¸´é¢†åŸŸé€‚åº”ã€ä¸Šä¸‹æ–‡å®šä½å’Œç»éªŒçŸ¥è¯†ç¼ºä¹çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„CBR-LLMæ¡†æ¶ç»“åˆäº†é©¾é©¶åœºæ™¯ç†è§£å’Œç›¸å…³é©¾é©¶æ¡ˆä¾‹æ£€ç´¢ï¼Œç”¨äºå¤æ‚é£é™©åœºæ™¯çš„æœºåŠ¨å†³ç­–ã€‚</li>
<li>CBR-LLMæ¡†æ¶æé«˜äº†å†³ç­–å‡†ç¡®æ€§ã€è§£é‡Šè´¨é‡ï¼Œæ›´å¥½åœ°å¥‘åˆäººç±»ä¸“å®¶è¡Œä¸ºã€‚</li>
<li>é£é™©æ„ŸçŸ¥æç¤ºç­–ç•¥è¿›ä¸€æ­¥æé«˜äº†åœ¨ä¸åŒé£é™©ç±»å‹ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>ç›¸ä¼¼åº¦åŸºç¡€ä¸Šçš„æ¡ˆä¾‹æ£€ç´¢åœ¨æŒ‡å¯¼ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹é¢ä¼˜äºéšæœºé‡‡æ ·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-99506f51adfb1599392df2d24eed2111.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8808f0e46c899b6c379d1c73736e07df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76b8c4c785e59e0621cf3d253c7c2a38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16039e115844d8790958561292ac8ae5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35f0ba200717aeef6838a1f239680f08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb27286930aa02cfabafdf41d3fe3046.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b42b5d524dfc51a829d87f365534559.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Asymmetric-REINFORCE-for-off-Policy-Reinforcement-Learning-Balancing-positive-and-negative-rewards"><a href="#Asymmetric-REINFORCE-for-off-Policy-Reinforcement-Learning-Balancing-positive-and-negative-rewards" class="headerlink" title="Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing   positive and negative rewards"></a>Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing   positive and negative rewards</h2><p><strong>Authors:Charles Arnal, GaÃ«tan Narozniak, Vivien Cabannes, Yunhao Tang, Julia Kempe, Remi Munos</strong></p>
<p>Reinforcement learning (RL) is increasingly used to align large language models (LLMs). Off-policy methods offer greater implementation simplicity and data efficiency than on-policy techniques, but often result in suboptimal performance. In this work, we study the intermediate range of algorithms between off-policy RL and supervised fine-tuning by analyzing a simple off-policy REINFORCE algorithm, where the advantage is defined as $A&#x3D;r-V$, with $r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$ emphasizes high-reward samples, while raising it penalizes low-reward ones more heavily. We first provide a theoretical analysis of this off-policy REINFORCE algorithm, showing that when the baseline $V$ lower-bounds the expected reward, the algorithm enjoys a policy improvement guarantee. Our analysis reveals that while on-policy updates can safely leverage both positive and negative signals, off-policy updates benefit from focusing more on positive rewards than on negative ones. We validate our findings experimentally in a controlled stochastic bandit setting and through fine-tuning state-of-the-art LLMs on reasoning tasks. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ä¸åŸºäºç­–ç•¥çš„on-policyæ–¹æ³•ç›¸æ¯”ï¼Œoff-policyæ–¹æ³•å…·æœ‰æ›´å¤§çš„å®ç°ç®€å•æ€§å’Œæ•°æ®æ•ˆç‡ä¼˜åŠ¿ï¼Œä½†é€šå¸¸ä¼šå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æä¸€ä¸ªç®€å•çš„off-policy REINFORCEç®—æ³•æ¥ç ”ç©¶off-policy RLå’ŒåŸºäºç›‘ç£çš„å¾®è°ƒä¹‹é—´çš„ç®—æ³•ä¸­é—´èŒƒå›´ã€‚åœ¨è¿™ä¸ªç®—æ³•ä¸­ï¼Œä¼˜åŠ¿è¢«å®šä¹‰ä¸ºA&#x3D;r-Vï¼Œå…¶ä¸­ræ˜¯å¥–åŠ±ï¼ŒVæ˜¯å¯è°ƒèŠ‚çš„åŸºçº¿ã€‚ç›´è§‚åœ°çœ‹ï¼Œé™ä½Vä¼šå¼ºè°ƒé«˜å¥–åŠ±æ ·æœ¬ï¼Œè€Œæé«˜Våˆ™ä¼šæ›´åŠ ä¸¥å‰åœ°æƒ©ç½šä½å¥–åŠ±æ ·æœ¬ã€‚æˆ‘ä»¬é¦–å…ˆå¯¹è¿™ç§off-policy REINFORCEç®—æ³•è¿›è¡Œç†è®ºåˆ†æï¼Œè¡¨æ˜å½“åŸºçº¿Vé¢„æœŸå¥–åŠ±çš„ä¸‹ç•Œæ—¶ï¼Œè¯¥ç®—æ³•å…·æœ‰ç­–ç•¥æ”¹è¿›ä¿è¯ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå°½ç®¡åŸºäºç­–ç•¥çš„æ›´æ–°å¯ä»¥å®‰å…¨åœ°ä½¿ç”¨æ­£å‘å’Œè´Ÿå‘ä¿¡å·ï¼Œä½†åŸºäºç­–ç•¥çš„æ›´æ–°æ›´å¤šä¾§é‡äºæ­£å‘å¥–åŠ±è€Œä¸æ˜¯è´Ÿå‘å¥–åŠ±å—ç›Šæ›´å¤šã€‚æˆ‘ä»¬åœ¨å—æ§çš„éšæœºbanditç¯å¢ƒå’Œé€šè¿‡å¾®è°ƒæœ€æ–°LLMè¿›è¡Œæ¨ç†ä»»åŠ¡å®éªŒä¸­éªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20520v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨è¶Šæ¥è¶Šå—åˆ°é‡è§†ã€‚ç›¸æ¯”äºé‡‡ç”¨ç­–ç•¥ç±»æ–¹æ³•ï¼Œé‡‡ç”¨éç­–ç•¥ç±»æ–¹æ³•æ›´ä¸ºç®€å•ä¸”é«˜æ•ˆï¼Œä½†å¾€å¾€æ•ˆæœæ¬ ä½³ã€‚æœ¬æ–‡ç ”ç©¶äº†éç­–ç•¥å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸ç›‘ç£å¾®è°ƒä¹‹é—´çš„ä¸­é—´åœ°å¸¦ï¼Œé€šè¿‡è§£æä¸€ä¸ªç®€å•çš„éç­–ç•¥REINFORCEç®—æ³•ï¼Œè¯¥ç®—æ³•çš„ä¼˜åŠ¿åœ¨äºå¥–åŠ±ä¸åŸºå‡†å€¼ä¹‹å·®ã€‚ç†è®ºä¸Šåˆ†æè¡¨æ˜ï¼Œå½“åŸºçº¿å€¼å¯¹æœŸæœ›å¥–åŠ±è¿›è¡Œä¸‹ç•Œä¼°è®¡æ—¶ï¼Œç®—æ³•èƒ½å¤Ÿä¿è¯ç­–ç•¥æ”¹è¿›ã€‚åˆ†æå‘ç°ï¼Œç›¸æ¯”äºè´Ÿé¢å¥–åŠ±ä¿¡å·ï¼Œéç­–ç•¥æ›´æ–°æ›´ä¾§é‡äºåˆ©ç”¨æ­£é¢å¥–åŠ±ä¿¡å·ã€‚å®éªŒéªŒè¯åœ¨éšæœºæ§åˆ¶ç¯å¢ƒå’Œå¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒä»»åŠ¡ä¸­å‡æœ‰æ•ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä½¿ç”¨æ­£æˆä¸ºçƒ­é—¨é¢†åŸŸï¼Œå°¤å…¶æ˜¯ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½å’Œè°ƒä¼˜ã€‚</li>
<li>éç­–ç•¥å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç®€åŒ–å’Œæé«˜æ•°æ®æ•ˆç‡æ–¹é¢ä¼˜äºç­–ç•¥æ–¹æ³•ï¼Œä½†æ€§èƒ½å¯èƒ½ä¸å¦‚é¢„æœŸç†æƒ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20520">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-139861d0888a0e03cd190b3794c9577b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="BotHash-Efficient-and-Training-Free-Bot-Detection-Through-Approximate-Nearest-Neighbor"><a href="#BotHash-Efficient-and-Training-Free-Bot-Detection-Through-Approximate-Nearest-Neighbor" class="headerlink" title="BotHash: Efficient and Training-Free Bot Detection Through Approximate   Nearest Neighbor"></a>BotHash: Efficient and Training-Free Bot Detection Through Approximate   Nearest Neighbor</h2><p><strong>Authors:Edoardo Di Paolo, Fabio De Gaspari, Angelo Spognardi</strong></p>
<p>Online Social Networks (OSNs) are a cornerstone in modern society, serving as platforms for diverse content consumption by millions of users each day. However, the challenge of ensuring the accuracy of information shared on these platforms remains significant, especially with the widespread dissemination of disinformation. Social bots â€“ automated accounts designed to mimic human behavior, frequently spreading misinformation â€“ represent one of the critical problems of OSNs. The advent of Large Language Models (LLMs) has further complicated bot behaviors, making detection increasingly difficult. This paper presents BotHash, an innovative, training-free approach to social bot detection. BotHash leverages a simplified user representation that enables approximate nearest-neighbor search to detect bots, avoiding the complexities of Deep-Learning model training and large dataset creation. We demonstrate that BotHash effectively differentiates between human and bot accounts, even when state-of-the-art LLMs are employed to generate postsâ€™ content. BotHash offers several advantages over existing methods, including its independence from a training phase, robust performance with minimal ground-truth data, and early detection capabilities, showing promising results across various datasets. </p>
<blockquote>
<p>åœ¨çº¿ç¤¾äº¤ç½‘ç»œï¼ˆOSNsï¼‰æ˜¯ç°ä»£ç¤¾ä¼šçš„åŸºçŸ³ï¼Œæ¯å¤©ä¸ºæ•°ç™¾ä¸‡ç”¨æˆ·æä¾›å¤šæ ·åŒ–çš„å†…å®¹æ¶ˆè´¹å¹³å°ã€‚ç„¶è€Œï¼Œç¡®ä¿è¿™äº›å¹³å°ä¸Šå…±äº«ä¿¡æ¯çš„å‡†ç¡®æ€§ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å‡æ¶ˆæ¯çš„å¹¿æ³›ä¼ æ’­ã€‚ç¤¾äº¤æœºå™¨äººï¼ˆæ¨¡ä»¿äººç±»è¡Œä¸ºçš„è‡ªåŠ¨åŒ–è´¦æˆ·ï¼Œç»å¸¸ä¼ æ’­é”™è¯¯æ¶ˆæ¯ï¼‰æ˜¯åœ¨çº¿ç¤¾äº¤ç½‘ç»œçš„å…³é”®é—®é¢˜ä¹‹ä¸€ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°è¿›ä¸€æ­¥åŠ å‰§äº†æœºå™¨äººè¡Œä¸ºå¤æ‚æ€§ï¼Œä½¿æ£€æµ‹å˜å¾—è¶Šæ¥è¶Šå›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†BotHashï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„æ— éœ€è®­ç»ƒçš„ç¤¾ä¼šæœºå™¨äººæ£€æµ‹æ–¹æ³•ã€‚BotHashåˆ©ç”¨ç®€åŒ–çš„ç”¨æˆ·è¡¨ç¤ºï¼Œå¯ä»¥é€šè¿‡è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢æ¥æ£€æµ‹æœºå™¨äººï¼Œé¿å…äº†æ·±åº¦æ¨¡å‹è®­ç»ƒå’Œå¤§å‹æ•°æ®é›†åˆ›å»ºçš„å¤æ‚æ€§ã€‚æˆ‘ä»¬è¯æ˜äº†BotHashå¯ä»¥æœ‰æ•ˆåœ°åŒºåˆ†äººç±»å’Œæœºå™¨äººè´¦æˆ·ï¼Œå³ä½¿åœ¨æœ€å…ˆè¿›çš„LLMç”¨äºç”Ÿæˆå¸–å­å†…å®¹çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒBotHashå…·æœ‰è®¸å¤šä¼˜åŠ¿ï¼ŒåŒ…æ‹¬ç‹¬ç«‹äºè®­ç»ƒé˜¶æ®µã€ä½¿ç”¨æœ€å°‘çš„çœŸå®æ•°æ®å³å¯å®ç°ç¨³å¥æ€§èƒ½ã€å…·å¤‡æ—©æœŸæ£€æµ‹èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨å„ç§æ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20503v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¤¾äº¤ç½‘ç»œåœ¨ç°ä»£ç¤¾ä¼šä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œæ¯å¤©ä¸ºæ•°ç™¾ä¸‡ç”¨æˆ·æä¾›å¤šæ ·åŒ–çš„å†…å®¹æ¶ˆè´¹å¹³å°ã€‚ç„¶è€Œï¼Œç¡®ä¿å¹³å°ä¸Šä¿¡æ¯çš„å‡†ç¡®æ€§ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯éšç€å‡ä¿¡æ¯çš„å¹¿æ³›ä¼ æ’­ã€‚ç¤¾äº¤æœºå™¨äººï¼ˆæ¨¡ä»¿äººç±»è¡Œä¸ºçš„è‡ªåŠ¨åŒ–è´¦æˆ·ï¼Œç»å¸¸ä¼ æ’­é”™è¯¯ä¿¡æ¯ï¼‰æ˜¯ç¤¾äº¤ç½‘ç»œçš„ä¸»è¦éš¾é¢˜ä¹‹ä¸€ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°è¿›ä¸€æ­¥åŠ å‰§äº†æœºå™¨äººè¡Œä¸ºçš„é—®é¢˜ï¼Œä½¿å¾—æ£€æµ‹å˜å¾—æ›´åŠ å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ— éœ€è®­ç»ƒçš„ç¤¾ä¼šæœºå™¨äººæ£€æµ‹æ³•â€”â€”BotHashã€‚BotHashé€šè¿‡ç®€åŒ–çš„ç”¨æˆ·è¡¨ç¤ºï¼Œå®ç°äº†è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢æ¥æ£€æµ‹æœºå™¨äººï¼Œé¿å…äº†æ·±åº¦æ¨¡å‹è®­ç»ƒå’Œå¤§å‹æ•°æ®é›†åˆ›å»ºçš„å¤æ‚æ€§ã€‚æˆ‘ä»¬è¯æ˜äº†BotHashå¯ä»¥æœ‰æ•ˆåœ°åŒºåˆ†äººç±»å’Œæœºå™¨äººè´¦æˆ·ï¼Œå³ä½¿åœ¨æœ€å…ˆè¿›çš„LLMè¢«ç”¨æ¥ç”Ÿæˆå¸–å­çš„å†…å®¹æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚BotHashç›¸æ¯”ç°æœ‰æ–¹æ³•å…·æœ‰å‡ ä¸ªä¼˜åŠ¿ï¼ŒåŒ…æ‹¬ç‹¬ç«‹äºè®­ç»ƒé˜¶æ®µã€åœ¨å°‘é‡çœŸå®æ•°æ®æƒ…å†µä¸‹è¡¨ç°ç¨³å¥ã€æ—©æœŸæ£€æµ‹èƒ½åŠ›å¼ºç­‰ï¼Œåœ¨å„ç§æ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨çº¿ç¤¾äº¤ç½‘ç»œçš„å‡†ç¡®æ€§é—®é¢˜æ—¥ç›Šå‡¸æ˜¾ï¼Œç¤¾äº¤æœºå™¨äººä¼ æ’­å‡ä¿¡æ¯æˆä¸ºä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ºç°ä½¿å¾—æ£€æµ‹ç¤¾äº¤æœºå™¨äººæ›´åŠ å›°éš¾ã€‚</li>
<li>BotHashæ˜¯ä¸€ç§åˆ›æ–°çš„æ— éœ€è®­ç»ƒçš„ç¤¾ä¼šæœºå™¨äººæ£€æµ‹æ³•ã€‚</li>
<li>BotHashé€šè¿‡ç®€åŒ–ç”¨æˆ·è¡¨ç¤ºå’Œè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢æ¥æ£€æµ‹æœºå™¨äººã€‚</li>
<li>BotHashé¿å…äº†å¤æ‚çš„æ·±åº¦æ¨¡å‹è®­ç»ƒå’Œå¤§å‹æ•°æ®é›†åˆ›å»ºçš„éœ€æ±‚ã€‚</li>
<li>BotHashèƒ½æœ‰æ•ˆåŒºåˆ†äººç±»å’Œæœºå™¨äººè´¦æˆ·ï¼Œå³ä½¿åœ¨æœ‰LLMç”Ÿæˆå†…å®¹çš„æƒ…å†µä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20503">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-edfa804dedd4ddb9b9821527c3dd6737.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a5c4632f407c52908ea061ac3a82e7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd5b662654324a5eb0d43307e1beb64a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d8adba73dcd97976e4f4ca9641cc466.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c43fdf32d45a1b161df55190a72bb410.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ReCode-Updating-Code-API-Knowledge-with-Reinforcement-Learning"><a href="#ReCode-Updating-Code-API-Knowledge-with-Reinforcement-Learning" class="headerlink" title="ReCode: Updating Code API Knowledge with Reinforcement Learning"></a>ReCode: Updating Code API Knowledge with Reinforcement Learning</h2><p><strong>Authors:Haoze Wu, Yunzhi Yao, Wenhao Yu, Huajun Chen, Ningyu Zhang</strong></p>
<p>Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMsâ€™ code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMsâ€™ general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zjunlp/ReCode">https://github.com/zjunlp/ReCode</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œä½†åœ¨é€‚åº”å¤–éƒ¨åº“APIçš„é¢‘ç¹æ›´æ–°æ—¶å´è¡¨ç°ä¸ä½³ã€‚è¿™ä¸€å…³é”®å±€é™æ€§æºäºå¯¹è®­ç»ƒæ•°æ®ä¸­è¿‡æ—¶APIçŸ¥è¯†çš„ä¾èµ–ï¼Œå³ä½¿æœ‰è®¿é—®å½“å‰æ–‡æ¡£ï¼Œä¹Ÿé˜»ç¢äº†åŠ¨æ€ç¯å¢ƒä¸­çš„å¯é ä»£ç ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReCodeï¼ˆåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ä»£ç æ›´æ–°ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡ä»¿ç¨‹åºå‘˜é€‚åº”APIå˜åŒ–çš„æ–°å‹æ¡†æ¶ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤§çº¦2000ä¸ªæ•°æ®æ¡ç›®æ•°æ®é›†æ¥è®­ç»ƒLLMï¼Œä»¥æ ¹æ®æ›´æ–°ä¿¡æ¯è¿›è¡Œç‰ˆæœ¬è¿ç§»ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¿®æ”¹åçš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦åº¦é‡æ ‡å‡†ï¼Œä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±æ¥è¿›è¡Œä»£ç è¯„ä¼°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒReCodeåœ¨åŠ¨æ€APIåœºæ™¯ä¸‹å¤§å¤§æé«˜äº†LLMçš„ä»£ç ç”Ÿæˆæ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æœªè§è¿‡çš„CodeUpdateArenaä»»åŠ¡ä¸­ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œä¸ç›‘ç£å¾®è°ƒç›¸æ¯”ï¼ŒReCodeå¯¹LLMçš„é€šç”¨ä»£ç ç”Ÿæˆèƒ½åŠ›çš„å½±å“è¾ƒå°ã€‚æˆ‘ä»¬åœ¨å„ç§LLMå’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆGRPOå’ŒDAPOï¼‰ä¸Šåº”ç”¨äº†ReCodeï¼Œå‡å®ç°äº†æŒç»­æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç»è¿‡è®­ç»ƒåï¼ŒQwen2.5-Coder-7Bçš„è¡¨ç°è¶…è¿‡äº†32Bå‚æ•°ä»£ç æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä»¥åŠç›¸åŒæ¶æ„çš„æ¨ç†æ¨¡å‹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zjunlp/ReCode%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zjunlp/ReCodeæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20495v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é€‚åº”å¤–éƒ¨åº“APIçš„é¢‘ç¹æ›´æ–°æ—¶é‡åˆ°å›°éš¾ã€‚è¯¥é—®é¢˜æºäºå…¶è®­ç»ƒæ•°æ®çš„è¿‡æ—¶APIçŸ¥è¯†ï¼Œå³ä½¿æœ‰å½“å‰æ–‡æ¡£ä¹Ÿæ— æ³•å¯é åœ°åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œä»£ç ç”Ÿæˆã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReCodeæ¡†æ¶ï¼Œé€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ è¿›è¡Œä»£ç æ›´æ–°ï¼Œæ¨¡æ‹Ÿç¨‹åºå‘˜å¯¹APIæ›´æ”¹çš„é€‚åº”ã€‚å®éªŒè¡¨æ˜ï¼ŒReCodeæ˜¾è‘—æé«˜äº†LLMåœ¨åŠ¨æ€APIåœºæ™¯ä¸­çš„ä»£ç ç”Ÿæˆæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§è¿‡çš„CodeUpdateArenaä»»åŠ¡ä¸Šã€‚ReCodeå¯¹LLMçš„ä¸€èˆ¬ä»£ç ç”Ÿæˆèƒ½åŠ›å½±å“è¾ƒå°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é€‚åº”APIé¢‘ç¹æ›´æ–°æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>LLMä¾èµ–è®­ç»ƒæ•°æ®ä¸­çš„è¿‡æ—¶APIçŸ¥è¯†ï¼Œé™åˆ¶äº†å…¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ReCodeæ¡†æ¶é€šè¿‡ç»“åˆè§„åˆ™å¼ºåŒ–å­¦ä¹ æ¨¡ä»¿äººç±»ç¨‹åºå‘˜é€‚åº”APIæ›´æ”¹ã€‚</li>
<li>ReCodeæé«˜äº†LLMåœ¨åŠ¨æ€APIåœºæ™¯ä¸­çš„ä»£ç ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>ReCodeå¯¹LLMçš„ä¸€èˆ¬ä»£ç ç”Ÿæˆèƒ½åŠ›å½±å“è¾ƒå°ã€‚</li>
<li>ReCodeåœ¨å¤šç§LLMså’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸Šå‡å®ç°äº†ä¸€è‡´æ€§çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dd9e66065bbf8428e33d2da7113c54df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5cceb681fede92d1147e826604bf89b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07ace76844054edeab6cde0a4c32eb44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-392cd8b6e733cdb84e634470f036f74b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d21925700dbf40a39c62cdafac483c65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5c727c232a9c7e176f040eee15e49e1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GPTailor-Large-Language-Model-Pruning-Through-Layer-Cutting-and-Stitching"><a href="#GPTailor-Large-Language-Model-Pruning-Through-Layer-Cutting-and-Stitching" class="headerlink" title="GPTailor: Large Language Model Pruning Through Layer Cutting and   Stitching"></a>GPTailor: Large Language Model Pruning Through Layer Cutting and   Stitching</h2><p><strong>Authors:Guinan Su, Li Shen, Lu Yin, Shiwei Liu, Yanwu Yang, Jonas Geiping</strong></p>
<p>Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in deployment and inference. While structured pruning of model parameters offers a promising way to reduce computational costs at deployment time, current methods primarily focus on single model pruning. In this work, we develop a novel strategy to compress models by strategically combining or merging layers from finetuned model variants, which preserves the original modelâ€™s abilities by aggregating capabilities accentuated in different finetunes. We pose the optimal tailoring of these LLMs as a zero-order optimization problem, adopting a search space that supports three different operations: (1) Layer removal, (2) Layer selection from different candidate models, and (3) Layer merging. Our experiments demonstrate that this approach leads to competitive model pruning, for example, for the Llama2-13B model families, our compressed models maintain approximately 97.3% of the original performance while removing $\sim25%$ of parameters, significantly outperforming previous state-of-the-art methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Guinan-Su/auto-merge-llm">https://github.com/Guinan-Su/auto-merge-llm</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£å’Œç”Ÿæˆè¯­è¨€æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™æ ·çš„å°è±¡æ·±åˆ»çš„èƒ½åŠ›é€šå¸¸ä¼´éšç€å·¨å¤§çš„æ¨¡å‹ä½“ç§¯ï¼Œè¿™åœ¨éƒ¨ç½²å’Œæ¨ç†è¿‡ç¨‹ä¸­å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶æ¨¡å‹å‚æ•°çš„ç»“æ„åŒ–å‰ªæä¸ºé™ä½éƒ¨ç½²æ—¶çš„è®¡ç®—æˆæœ¬æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œä½†å½“å‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•ä¸ªæ¨¡å‹çš„å‰ªæä¸Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é€šè¿‡æˆ˜ç•¥æ€§åœ°ç»„åˆæˆ–åˆå¹¶å¾®è°ƒæ¨¡å‹å˜ä½“ä¸­çš„å±‚æ¥å‹ç¼©æ¨¡å‹çš„æ–°ç­–ç•¥ï¼Œé€šè¿‡èšåˆä¸åŒå¾®è°ƒä¸­æ‰€å¼ºè°ƒçš„èƒ½åŠ›ï¼Œä»è€Œä¿ç•™åŸå§‹æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†è¿™äº›LLMçš„æœ€ä½³å®šåˆ¶è§†ä¸ºé›¶é˜¶ä¼˜åŒ–é—®é¢˜ï¼Œé‡‡ç”¨æ”¯æŒä¸‰ç§ä¸åŒæ“ä½œçš„æœç´¢ç©ºé—´ï¼šï¼ˆ1ï¼‰å»é™¤å±‚ï¼Œï¼ˆ2ï¼‰ä»ä¸åŒå€™é€‰æ¨¡å‹ä¸­é€‰å–å±‚ï¼Œä»¥åŠï¼ˆ3ï¼‰åˆå¹¶å±‚ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ¨¡å‹å‰ªæã€‚ä¾‹å¦‚ï¼Œå¯¹äºLlama2-13Bæ¨¡å‹å®¶æ—ï¼Œæˆ‘ä»¬çš„å‹ç¼©æ¨¡å‹åœ¨å»é™¤çº¦25%å‚æ•°çš„åŒæ—¶ï¼Œä¿æŒäº†åŸå§‹æ€§èƒ½çš„çº¦97.3%ï¼Œæ˜¾è‘—ä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Guinan-Su/auto-merge-llm%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Guinan-Su/auto-merge-llmä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20480v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£å’Œç”Ÿæˆè¯­è¨€æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†é€šå¸¸éœ€è¦åºå¤§çš„æ¨¡å‹è§„æ¨¡ï¼Œè¿™ç»™éƒ¨ç½²å’Œæ¨ç†å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ç°æœ‰å‚æ•°ä¿®å‰ªæ–¹æ³•ä¸»è¦å…³æ³¨å•ä¸€æ¨¡å‹çš„ä¿®å‰ªã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ï¼Œé€šè¿‡ç»“åˆå¾®è°ƒæ¨¡å‹å˜ä½“å±‚æ¥å‹ç¼©æ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹æ¨¡å‹çš„å¤šç§èƒ½åŠ›ã€‚æˆ‘ä»¬å°†å…¶ä½œä¸ºé›¶é˜¶ä¼˜åŒ–é—®é¢˜ï¼Œé‡‡ç”¨æ”¯æŒä¸‰ç§æ“ä½œï¼ˆå±‚ç§»é™¤ã€ä»å€™é€‰æ¨¡å‹ä¸­é€‰å–å±‚å’Œå±‚åˆå¹¶ï¼‰çš„æœç´¢ç©ºé—´ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ‰æ•ˆçš„æ¨¡å‹ä¿®å‰ªï¼Œä¾‹å¦‚å¯¹äºLlama2-13Bæ¨¡å‹å®¶æ—ï¼Œæˆ‘ä»¬çš„å‹ç¼©æ¨¡å‹åœ¨å»é™¤çº¦25%å‚æ•°çš„åŒæ—¶ä¿æŒäº†åŸå§‹æ€§èƒ½çš„çº¦97.3%ï¼Œæ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚ä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/Guinan-Su/auto-merge-llm%EF%BC%89%E3%80%82">https://github.com/Guinan-Su/auto-merge-llmï¼‰ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å‡ºè‰²çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä½†æ¨¡å‹è§„æ¨¡è¾ƒå¤§ï¼Œéƒ¨ç½²å’Œæ¨ç†é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰ä¸»è¦çš„æ¨¡å‹ä¿®å‰ªæ–¹æ³•é›†ä¸­åœ¨å•ä¸€æ¨¡å‹çš„ä¿®å‰ªä¸Šï¼Œè€Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆå¤šä¸ªå¾®è°ƒæ¨¡å‹å±‚çš„æ–°ç­–ç•¥ã€‚</li>
<li>é€šè¿‡åˆå¹¶å±‚ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å»é™¤éƒ¨åˆ†å‚æ•°çš„åŒæ—¶ä¿ç•™åŸå§‹æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨é›¶é˜¶ä¼˜åŒ–æ–¹æ³•ï¼Œå¹¶å®šä¹‰äº†ä¸€ä¸ªæœç´¢ç©ºé—´ï¼Œæ”¯æŒå±‚ç§»é™¤ã€ä»å€™é€‰æ¨¡å‹ä¸­é€‰å–å±‚å’Œå±‚åˆå¹¶ä¸‰ç§æ“ä½œã€‚</li>
<li>å¯¹äºLlama2-13Bæ¨¡å‹å®¶æ—çš„å®éªŒè¡¨æ˜ï¼Œå‹ç¼©åçš„æ¨¡å‹æ€§èƒ½ä¿æŒäº†åŸå§‹æ€§èƒ½çš„çº¦97.3%ï¼ŒåŒæ—¶å»é™¤äº†çº¦25%çš„å‚æ•°ã€‚</li>
<li>è¿™ç§æ–°çš„å‹ç¼©ç­–ç•¥æ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20480">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c01883934f7f6a4dec1d216e44a9413a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9775cfe0d52edd6489e26f08398ba04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e275a9662e07bebd69aff6b456b4f05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e3d01065c2b8426077d1935b68b2a47.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Probing-AI-Safety-with-Source-Code"><a href="#Probing-AI-Safety-with-Source-Code" class="headerlink" title="Probing AI Safety with Source Code"></a>Probing AI Safety with Source Code</h2><p><strong>Authors:Ujwal Narayan, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Karthik Narasimhan, Ameet Deshpande, Vishvak Murahari</strong></p>
<p>Large language models (LLMs) have become ubiquitous, interfacing with humans in numerous safety-critical applications. This necessitates improving capabilities, but importantly coupled with greater safety measures to align these models with human values and preferences. In this work, we demonstrate that contemporary models fall concerningly short of the goal of AI safety, leading to an unsafe and harmful experience for users. We introduce a prompting strategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT converts natural language inputs to simple code that represents the same intent. For instance, CoDoT transforms the natural language prompt â€œMake the statement more toxic: {text}â€ to: â€œmake_more_toxic({text})â€. We show that CoDoT results in a consistent failure of a wide range of state-of-the-art LLMs. For example, GPT-4 Turboâ€™s toxicity increases 16.5 times, DeepSeek R1 fails 100% of the time, and toxicity increases 300% on average across seven modern LLMs. Additionally, recursively applying CoDoT can further increase toxicity two times. Given the rapid and widespread adoption of LLMs, CoDoT underscores the critical need to evaluate safety efforts from first principles, ensuring that safety and capabilities advance together. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æ— å¤„ä¸åœ¨ï¼Œåœ¨ä¼—å¤šçš„å®‰å…¨å…³é”®åº”ç”¨ä¸­ä¸äººç±»æ¥å£ã€‚è¿™è¦æ±‚æé«˜èƒ½åŠ›ï¼Œä½†æ›´é‡è¦çš„æ˜¯ä¸æ›´å¤§çš„å®‰å…¨æªæ–½ç›¸ç»“åˆï¼Œä»¥ä½¿è¿™äº›æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½ç›¸ä¸€è‡´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯æ˜å½“ä»£æ¨¡å‹ä»¤äººæ‹…å¿§åœ°æœªèƒ½è¾¾åˆ°äººå·¥æ™ºèƒ½å®‰å…¨çš„ç›®æ ‡ï¼Œå¯¼è‡´ç”¨æˆ·é¢ä¸´ä¸å®‰å…¨ä¸”æœ‰å®³çš„ä½“éªŒã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºâ€œæ€ç»´ç¼–ç â€ï¼ˆCoDoTï¼‰çš„æç¤ºç­–ç•¥æ¥è¯„ä¼°LLMçš„å®‰å…¨æ€§ã€‚CoDoTå°†è‡ªç„¶è¯­è¨€è¾“å…¥è½¬æ¢ä¸ºç®€å•ä»£ç ï¼Œä»£è¡¨ç›¸åŒçš„æ„å›¾ã€‚ä¾‹å¦‚ï¼ŒCoDoTå°†è‡ªç„¶è¯­è¨€æç¤ºâ€œä½¿é™ˆè¿°æ›´å…·æ¯’æ€§ï¼š{æ–‡æœ¬}â€è½¬æ¢ä¸ºâ€œmake_more_toxic({text})â€ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒCoDoTå¯¼è‡´ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„LLMçš„ä¸€è‡´å¤±è´¥ã€‚ä¾‹å¦‚ï¼ŒGPT-4 Turboçš„æ¯’æ€§å¢åŠ äº†16.5å€ï¼ŒDeepSeek R1çš„å¤±è´¥ç‡ä¸º100%ï¼Œåœ¨ä¸ƒä¸ªç°ä»£LLMä¸­ï¼Œæ¯’æ€§å¹³å‡å¢åŠ äº†300%ã€‚æ­¤å¤–ï¼Œé€’å½’åº”ç”¨CoDoTå¯ä»¥è¿›ä¸€æ­¥ä½¿æ¯’æ€§å¢åŠ ä¸¤å€ã€‚é‰´äºLLMçš„å¿«é€Ÿå’Œå¹¿æ³›é‡‡ç”¨ï¼ŒCoDoTå¼ºè°ƒäº†ä»ç¬¬ä¸€åŸåˆ™è¯„ä¼°å®‰å…¨å·¥ä½œçš„è¿«åˆ‡éœ€æ±‚ï¼Œç¡®ä¿å®‰å…¨å’Œèƒ½åŠ›çš„å…±åŒè¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20471v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¼—å¤šå®‰å…¨å…³é”®åº”ç”¨ä¸­ä¸äººç±»äº¤äº’ï¼Œä½†å½“å‰æ¨¡å‹åœ¨å®‰å…¨æ–¹é¢å­˜åœ¨ç¼ºé™·ï¼Œå¯¹ç”¨æˆ·æ„æˆå®‰å…¨éšæ‚£ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºCode of Thoughtï¼ˆCoDoTï¼‰çš„æç¤ºç­–ç•¥ï¼Œç”¨äºè¯„ä¼°LLMçš„å®‰å…¨æ€§ã€‚CoDoTå°†è‡ªç„¶è¯­è¨€è¾“å…¥è½¬æ¢ä¸ºç®€å•ä»£ç ï¼Œä»£è¡¨ç›¸åŒçš„æ„å›¾ã€‚ä¾‹å¦‚ï¼ŒCoDoTå°†è‡ªç„¶è¯­è¨€æç¤ºâ€œä½¿é™ˆè¿°æ›´å…·æ¯’æ€§ï¼š{æ–‡æœ¬}â€è½¬æ¢ä¸ºâ€œmake_more_toxic({text})â€ã€‚ç ”ç©¶æ˜¾ç¤ºï¼ŒCoDoTå¯¼è‡´ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„LLMé¢‘ç¹å¤±è´¥ã€‚ä¾‹å¦‚ï¼ŒGPT-4 Turboçš„æ¯’æ€§å¢åŠ 16.5å€ï¼ŒDeepSeek R1ç™¾åˆ†ä¹‹ç™¾å¤±è´¥ï¼Œåœ¨ä¸ƒä¸ªç°ä»£LLMä¸Šçš„æ¯’æ€§å¹³å‡å¢åŠ 300%ã€‚é€’å½’åº”ç”¨CoDoTå¯èƒ½è¿›ä¸€æ­¥ä½¿æ¯’æ€§å¢åŠ ä¸¤å€ã€‚å› æ­¤ï¼ŒCoDoTå¼ºè°ƒä»ç¬¬ä¸€åŸåˆ™è¯„ä¼°å®‰å…¨æ€§çš„å¿…è¦ï¼Œç¡®ä¿å®‰å…¨ä¸èƒ½åŠ›çš„åŒæ­¥å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­å­˜åœ¨å®‰å…¨éšæ‚£ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºCode of Thoughtï¼ˆCoDoTï¼‰çš„æç¤ºç­–ç•¥æ¥è¯„ä¼°LLMçš„å®‰å…¨æ€§ã€‚</li>
<li>CoDoTèƒ½å°†è‡ªç„¶è¯­è¨€è¾“å…¥è½¬æ¢ä¸ºç®€å•ä»£ç ï¼Œåæ˜ ç›¸åŒæ„å›¾ã€‚</li>
<li>CoDoTæ­ç¤ºå½“ä»£LLMåœ¨å®‰å…¨æ€§æ–¹é¢çš„æ˜¾è‘—ç¼ºé™·ã€‚</li>
<li>å¤šç§LLMåœ¨CoDoTæç¤ºä¸‹è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½é—®é¢˜ï¼Œå¦‚GPT-4 Turboçš„æ¯’æ€§å¢åŠ 16.5å€ã€‚</li>
<li>é€’å½’åº”ç”¨CoDoTå¯èƒ½è¿›ä¸€æ­¥åŠ å‰§è¿™äº›é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef1aefad39755fe0e96a9e33a2a03571.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0a34730fdf20ccf30ee5847e7dbd476.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce8f078ad25a8836e0e82f799e8a2f06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a750712a553e018068cbd8537b70c808.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b914d78b7fb52319eb6e26751427033f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Med-Art-Diffusion-Transformer-for-2D-Medical-Text-to-Image-Generation"><a href="#Med-Art-Diffusion-Transformer-for-2D-Medical-Text-to-Image-Generation" class="headerlink" title="Med-Art: Diffusion Transformer for 2D Medical Text-to-Image Generation"></a>Med-Art: Diffusion Transformer for 2D Medical Text-to-Image Generation</h2><p><strong>Authors:Changlu Guo, Anders Nymark Christensen, Morten Rieger Hannemose</strong></p>
<p>Text-to-image generative models have achieved remarkable breakthroughs in recent years. However, their application in medical image generation still faces significant challenges, including small dataset sizes, and scarcity of medical textual data. To address these challenges, we propose Med-Art, a framework specifically designed for medical image generation with limited data. Med-Art leverages vision-language models to generate visual descriptions of medical images which overcomes the scarcity of applicable medical textual data. Med-Art adapts a large-scale pre-trained text-to-image model, PixArt-$\alpha$, based on the Diffusion Transformer (DiT), achieving high performance under limited data. Furthermore, we propose an innovative Hybrid-Level Diffusion Fine-tuning (HLDF) method, which enables pixel-level losses, effectively addressing issues such as overly saturated colors. We achieve state-of-the-art performance on two medical image datasets, measured by FID, KID, and downstream classification performance. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„çªç ´ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨åŒ»ç–—å›¾åƒç”Ÿæˆæ–¹é¢çš„åº”ç”¨ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®é›†è¾ƒå°å’ŒåŒ»ç–—æ–‡æœ¬æ•°æ®ç¨€ç¼ºã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Med-Artï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºæœ‰é™æ•°æ®è®¾è®¡çš„åŒ»ç–—å›¾åƒç”Ÿæˆæ¡†æ¶ã€‚Med-Artåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”ŸæˆåŒ»ç–—å›¾åƒçš„è§†è§‰æè¿°ï¼Œå…‹æœäº†å¯ç”¨åŒ»ç–—æ–‡æœ¬æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚Med-Arté‡‡ç”¨åŸºäºDiffusion Transformerï¼ˆDiTï¼‰çš„å¤§å‹é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹PixArt-Î±ï¼Œåœ¨æœ‰é™æ•°æ®çš„æƒ…å†µä¸‹å®ç°äº†é«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ··åˆçº§åˆ«æ‰©æ•£å¾®è°ƒï¼ˆHLDFï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°åƒç´ çº§åˆ«çš„æŸå¤±ï¼Œæœ‰æ•ˆè§£å†³é¢œè‰²è¿‡äºé¥±å’Œç­‰é—®é¢˜ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªåŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œé€šè¿‡FIDã€KIDå’Œä¸‹æ¸¸åˆ†ç±»æ€§èƒ½æ¥è¡¡é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20449v1">PDF</a> The project is available at \url{<a target="_blank" rel="noopener" href="https://medart-ai.github.io}/">https://medart-ai.github.io}</a></p>
<p><strong>Summary</strong>ï¼šè¿‘å¹´æ¥ï¼Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨åŒ»ç–—å›¾åƒç”Ÿæˆæ–¹é¢ï¼Œç”±äºæ•°æ®é›†è¾ƒå°å’ŒåŒ»ç–—æ–‡æœ¬æ•°æ®ç¨€ç¼ºï¼Œä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Med-Artæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸“é—¨ç”¨äºæœ‰é™æ•°æ®çš„åŒ»ç–—å›¾åƒç”Ÿæˆã€‚Med-Artåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”ŸæˆåŒ»å­¦å›¾åƒçš„è§†è§‰æè¿°ï¼Œå…‹æœäº†é€‚ç”¨çš„åŒ»å­¦æ–‡æœ¬æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚Med-Arté€‚åº”åŸºäºDiffusion Transformerï¼ˆDiTï¼‰çš„å¤§å‹é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹PixArt-$\alpha$ï¼Œåœ¨æœ‰é™æ•°æ®ä¸‹å®ç°é«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åˆ›æ–°çš„Hybrid-Level Diffusion Fine-tuningï¼ˆHLDFï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°åƒç´ çº§æŸå¤±ï¼Œæœ‰æ•ˆè§£å†³é¢œè‰²è¿‡äºé¥±å’Œç­‰é—®é¢˜ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªåŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ï¼Œé€šè¿‡FIDã€KIDå’Œä¸‹æ¸¸åˆ†ç±»æ€§èƒ½æ¥è¡¡é‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨åŒ»ç–—å›¾åƒç”Ÿæˆé¢†åŸŸé¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®é›†è¾ƒå°å’ŒåŒ»ç–—æ–‡æœ¬æ•°æ®ç¨€ç¼ºã€‚</li>
<li>Med-Artæ¡†æ¶ä¸“é—¨è®¾è®¡ç”¨äºæœ‰é™æ•°æ®çš„åŒ»ç–—å›¾åƒç”Ÿæˆã€‚</li>
<li>Med-Artåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å…‹æœåŒ»ç–—æ–‡æœ¬æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚</li>
<li>Med-Arté€‚åº”å¤§å‹é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹PixArt-$\alpha$ï¼Œå¹¶ç»“åˆDiffusion Transformerï¼ˆDiTï¼‰å®ç°é«˜æ€§èƒ½ã€‚</li>
<li>Hybrid-Level Diffusion Fine-tuningï¼ˆHLDFï¼‰æ–¹æ³•èƒ½å¤Ÿå®ç°åƒç´ çº§æŸå¤±ï¼Œè§£å†³é¢œè‰²è¿‡äºé¥±å’Œç­‰é—®é¢˜ã€‚</li>
<li>Med-Artåœ¨åŒ»ç–—å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æœ€æ–°æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f28091660d69b43802d1147460c47d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea443dc26dc1150647886fc4ed313b65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02213f7d74cf9fdb4305829a77041c55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc2f5e8417a7cb41871ddb9d7d5ea07c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Any-Order-GPT-as-Masked-Diffusion-Model-Decoupling-Formulation-and-Architecture"><a href="#Any-Order-GPT-as-Masked-Diffusion-Model-Decoupling-Formulation-and-Architecture" class="headerlink" title="Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and   Architecture"></a>Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and   Architecture</h2><p><strong>Authors:Shuchen Xue, Tianyu Xie, Tianyang Hu, Zijin Feng, Jiacheng Sun, Kenji Kawaguchi, Zhenguo Li, Zhi-Ming Ma</strong></p>
<p>Large language models (LLMs) predominantly use autoregressive (AR) approaches, but masked diffusion models (MDMs) are emerging as viable alternatives. A key challenge in comparing AR and MDM paradigms is their typical architectural difference: AR models are often decoder-only, while MDMs have largely been encoder-only. This practice of changing both the modeling paradigm and architecture simultaneously makes direct comparisons unfair, as itâ€™s hard to distinguish whether observed differences stem from the paradigm itself or the architectural shift. This research evaluates MDMs within a decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or AO-AR) and standard AR paradigms. Our investigation suggests that the standard AO-AR objective, which averages over all token permutations, may benefit from refinement, as many permutations appear less informative compared to the languageâ€™s inherent left-to-right structure. (2) Investigate architectural influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that while encoder-only MDMs model a simpler conditional probability space, decoder-only MDMs can achieve dramatic generation speedups ($\sim25\times$) and comparable perplexity with temperature annealing despite modeling a vastly larger space, highlighting key trade-offs. This work thus decouples core paradigm differences from architectural influences, offering insights for future model design. Code is available at <a target="_blank" rel="noopener" href="https://github.com/scxue/AO-GPT-MDM">https://github.com/scxue/AO-GPT-MDM</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸»è¦ä½¿ç”¨è‡ªå›å½’ï¼ˆARï¼‰æ–¹æ³•ï¼Œä½†æ©ç æ‰©æ•£æ¨¡å‹ï¼ˆMDMï¼‰æ­£é€æ¸æˆä¸ºå¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆã€‚æ¯”è¾ƒARå’ŒMDMèŒƒå¼çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜åœ¨äºå®ƒä»¬å…¸å‹çš„æ¶æ„å·®å¼‚ï¼šARæ¨¡å‹é€šå¸¸æ˜¯ä»…è§£ç å™¨ï¼Œè€ŒMDMsä¸»è¦æ˜¯ä»…ç¼–ç å™¨ã€‚åŒæ—¶æ”¹å˜å»ºæ¨¡èŒƒå¼å’Œæ¶æ„çš„åšæ³•ä½¿å¾—ç›´æ¥æ¯”è¾ƒä¸å…¬å¹³ï¼Œå› ä¸ºå¾ˆéš¾åŒºåˆ†è§‚å¯Ÿåˆ°çš„å·®å¼‚æ˜¯æ¥è‡ªäºèŒƒå¼æœ¬èº«è¿˜æ˜¯æ¶æ„çš„è½¬å˜ã€‚æœ¬ç ”ç©¶åœ¨ä»…è§£ç å™¨æ¡†æ¶å†…è¯„ä¼°MDMsï¼Œæ—¨åœ¨ï¼šï¼ˆ1ï¼‰å…¬å¹³åœ°æ¯”è¾ƒMDMï¼ˆä½œä¸ºä»»æ„é¡ºåºARæˆ–AO-ARï¼‰å’Œæ ‡å‡†ARèŒƒå¼ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥è¡¨æ˜ï¼Œæ ‡å‡†AO-ARç›®æ ‡é€šè¿‡å¯¹æ‰€æœ‰ä»¤ç‰Œæ’åˆ—è¿›è¡Œå¹³å‡ï¼Œå¯èƒ½ä¼šä»æ”¹è¿›ä¸­å—ç›Šï¼Œå› ä¸ºè®¸å¤šæ’åˆ—ä¸è¯­è¨€çš„å†…åœ¨ä»å·¦åˆ°å³ç»“æ„ç›¸æ¯”æ˜¾å¾—ä¿¡æ¯é‡ä¸è¶³ã€‚ï¼ˆ2ï¼‰ç ”ç©¶MDMsä¸­çš„æ¶æ„å½±å“ï¼ˆä»…è§£ç å™¨ä¸ä»…ç¼–ç å™¨ï¼‰ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè™½ç„¶ä»…ç¼–ç å™¨MDMsæ¨¡æ‹Ÿäº†ä¸€ä¸ªæ›´ç®€å•çš„æ¡ä»¶æ¦‚ç‡ç©ºé—´ï¼Œä½†ä»…è§£ç å™¨MDMså¯ä»¥å®ç°æ˜¾è‘—çš„äº§ç”Ÿå¼é€Ÿåº¦æå‡ï¼ˆ~25å€ï¼‰ï¼Œå¹¶ä¸”å°½ç®¡æ¨¡æ‹Ÿäº†ä¸€ä¸ªæ›´å¤§çš„ç©ºé—´ï¼Œä½†é€šè¿‡æ¸©åº¦é€€ç«æ³•å¯ä»¥è¾¾åˆ°ç›¸ä¼¼çš„å›°æƒ‘åº¦ï¼Œè¿™çªå‡ºäº†å…³é”®çš„æƒè¡¡ã€‚å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œå°†æ ¸å¿ƒèŒƒå¼å·®å¼‚ä¸æ¶æ„å½±å“åˆ†å¼€ï¼Œä¸ºæœªæ¥æ¨¡å‹è®¾è®¡æä¾›äº†è§è§£ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/scxue/AO-GPT-MDM%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/scxue/AO-GPT-MDMä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19935v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸»è¦ä½¿ç”¨è‡ªå›å½’ï¼ˆARï¼‰æ–¹æ³•ï¼Œä½†æ©ç æ‰©æ•£æ¨¡å‹ï¼ˆMDMï¼‰æ­£æˆä¸ºå¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆã€‚æ¯”è¾ƒARå’ŒMDMèŒƒå¼æ—¶é¢ä¸´çš„å…³é”®æŒ‘æˆ˜åœ¨äºå®ƒä»¬å…¸å‹çš„æ¶æ„å·®å¼‚ï¼šARæ¨¡å‹é€šå¸¸ä¸ºè§£ç å™¨ä»…æ¨¡å¼ï¼Œè€ŒMDMä¸»è¦æ˜¯ç¼–ç å™¨ä»…æ¨¡å¼ã€‚åŒæ—¶æ”¹å˜å»ºæ¨¡èŒƒå¼å’Œæ¶æ„çš„åšæ³•ä½¿å¾—ç›´æ¥æ¯”è¾ƒå˜å¾—ä¸å…¬å¹³ï¼Œå› ä¸ºå¾ˆéš¾åŒºåˆ†è§‚å¯Ÿåˆ°çš„å·®å¼‚æ˜¯æ¥è‡ªäºèŒƒå¼æœ¬èº«è¿˜æ˜¯æ¶æ„çš„è½¬å˜ã€‚æœ¬ç ”ç©¶åœ¨ä»…è§£ç å™¨æ¡†æ¶å†…è¯„ä¼°MDMï¼Œæ—¨åœ¨å…¬å¹³æ¯”è¾ƒMDMï¼ˆä½œä¸ºä»»æ„é¡ºåºARæˆ–AO-ARï¼‰å’Œæ ‡å‡†ARèŒƒå¼ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥è¡¨æ˜ï¼Œæ ‡å‡†AO-ARç›®æ ‡é€šè¿‡å¯¹æ‰€æœ‰ä»¤ç‰Œæ’åˆ—è¿›è¡Œå¹³å‡å¯èƒ½ä¼šå—ç›Šäºæ”¹è¿›ï¼Œå› ä¸ºè®¸å¤šæ’åˆ—ä¸è¯­è¨€çš„å†…åœ¨ä»å·¦åˆ°å³ç»“æ„ç›¸æ¯”æ˜¾å¾—ä¿¡æ¯é‡ä¸è¶³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢è®¨äº†MDMå†…çš„æ¶æ„å½±å“ï¼ˆä»…è§£ç å™¨ä¸ä»…ç¼–ç å™¨ï¼‰ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè™½ç„¶ä»…ç¼–ç å™¨MDMå¯¹ç®€å•æ¡ä»¶æ¦‚ç‡ç©ºé—´è¿›è¡Œå»ºæ¨¡ï¼Œä½†ä»…è§£ç å™¨MDMå¯ä»¥å®ç°æ˜¾ç€çš„ç”Ÿæˆé€Ÿåº¦æå‡ï¼ˆçº¦25å€ï¼‰ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡æ¸©åº¦é€€ç«å®ç°ç±»ä¼¼çš„å›°æƒ‘åº¦ï¼Œå°½ç®¡å®ƒä»¬å¯¹æ›´å¤§çš„ç©ºé—´è¿›è¡Œå»ºæ¨¡ï¼Œè¿™å‡¸æ˜¾äº†å…³é”®æƒè¡¡ã€‚å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œå°†æ ¸å¿ƒèŒƒå¼å·®å¼‚ä¸æ¶æ„å½±å“åˆ†ç¦»å¼€æ¥ï¼Œä¸ºæœªæ¥çš„æ¨¡å‹è®¾è®¡æä¾›äº†è§è§£ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶åœ¨ä»…è§£ç å™¨æ¡†æ¶å†…è¯„ä¼°æ©ç æ‰©æ•£æ¨¡å‹ï¼ˆMDMï¼‰ï¼Œä»¥å…¬å¹³åœ°æ¯”è¾ƒå…¶ä¸æ ‡å‡†è‡ªå›å½’ï¼ˆARï¼‰èŒƒå¼çš„æ€§èƒ½ã€‚</li>
<li>å‘ç°æ ‡å‡†AO-ARç›®æ ‡é€šè¿‡å¹³å‡æ‰€æœ‰ä»¤ç‰Œæ’åˆ—å¯èƒ½éœ€è¦æ”¹è¿›ï¼Œå› ä¸ºè®¸å¤šæ’åˆ—åœ¨ä¿¡æ¯é‡æ–¹é¢æ˜¾å¾—ä¸è¶³ã€‚</li>
<li>å¯¹æ¯”äº†ä»…è§£ç å™¨MDMå’Œä»…ç¼–ç å™¨MDMä¹‹é—´çš„æ€§èƒ½å·®å¼‚ï¼Œå‘ç°å‰è€…èƒ½å¤Ÿå®ç°æ˜¾è‘—çš„ç”Ÿæˆé€Ÿåº¦æå‡ï¼Œå¹¶èƒ½å¤Ÿåœ¨æ›´å¤§çš„ç©ºé—´å†…å»ºæ¨¡ã€‚</li>
<li>æŒ‡å‡ºæ¶æ„å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œè¡¨æ˜ä¸åŒæ¶æ„ä¹‹é—´å­˜åœ¨æƒè¡¡å…³ç³»ã€‚</li>
<li>é€šè¿‡å®éªŒè¯æ˜æ¸©åº¦é€€ç«æŠ€æœ¯å¯¹äºæé«˜æ¨¡å‹æ€§èƒ½çš„é‡è¦æ€§ã€‚</li>
<li>ç ”ç©¶å·¥ä½œä¸ºæœªæ¥çš„æ¨¡å‹è®¾è®¡æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå°†æ ¸å¿ƒèŒƒå¼å·®å¼‚ä¸æ¶æ„å½±å“åˆ†å¼€è€ƒè™‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6eb87442f941726b06ab528d714b2bce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cb55996a8fb61c99fbb9ae9f6cc37f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd3e7904473d835ac12b07eb931289d3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Adaptive-Request-Scheduling-for-CodeLLM-Serving-with-SLA-Guarantees"><a href="#Adaptive-Request-Scheduling-for-CodeLLM-Serving-with-SLA-Guarantees" class="headerlink" title="Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees"></a>Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees</h2><p><strong>Authors:Shi Chang, Boyuan Chen, Kishanthan Thangarajah, Hanan Lutfiyya, Ahmed E. Hassan</strong></p>
<p>Code Large Language Models (CodeLLMs) are increasingly integrated into modern software development workflows, yet efficiently serving them in resource-constrained, self-hosted environments remains a significant challenge. Existing LLM serving systems employs Continuous Batching for throughput improvement. However, they rely on static batch size configurations that cannot adapt to fluctuating request rates or heterogeneous workloads, leading to frequent SLA (Service Level Agreement) violations and unstable performance. In this study, We propose SABER, a dynamic batching strategy that predicts per-request SLA feasibility and adjusts decisions in real time. SABER improves goodput by up to 26% over the best static configurations and reduces latency variability by up to 45%, all without manual tuning or service restarts. Our results demonstrate that SLA-aware, adaptive scheduling is key to robust, high-performance CodeLLM serving. </p>
<blockquote>
<p>ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆCodeLLMsï¼‰åœ¨ç°ä»£è½¯ä»¶å¼€å‘æµç¨‹ä¸­çš„é›†æˆåº¦è¶Šæ¥è¶Šé«˜ï¼Œä½†åœ¨èµ„æºå—é™ã€è‡ªä¸»æ‰˜ç®¡çš„ç¯å¢ƒä¸­æœ‰æ•ˆåœ°æä¾›æœåŠ¡ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„LLMæœåŠ¡ç³»ç»Ÿä¸ºæé«˜ååé‡è€Œé‡‡ç”¨è¿ç»­æ‰¹å¤„ç†ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–äºé™æ€çš„æ‰¹å¤„ç†å¤§å°é…ç½®ï¼Œæ— æ³•é€‚åº”æ³¢åŠ¨çš„è¯·æ±‚ç‡æˆ–å¼‚æ„çš„å·¥ä½œé‡ï¼Œå¯¼è‡´æœåŠ¡çº§åˆ«åè®®ï¼ˆSLAï¼‰é¢‘ç¹è¿è§„å’Œæ€§èƒ½ä¸ç¨³å®šã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºSABERï¼Œè¿™æ˜¯ä¸€ç§åŠ¨æ€æ‰¹å¤„ç†ç­–ç•¥ï¼Œå¯ä»¥é¢„æµ‹æ¯ä¸ªè¯·æ±‚çš„SLAå¯è¡Œæ€§å¹¶å®æ—¶è°ƒæ•´å†³ç­–ã€‚SABERåœ¨æœ€ä½³é™æ€é…ç½®çš„åŸºç¡€ä¸Šå°†goodputæé«˜äº†é«˜è¾¾26%ï¼Œå¹¶å°†å»¶è¿Ÿå˜åŒ–å‡å°‘äº†é«˜è¾¾45%ï¼Œä¸”æ— éœ€æ‰‹åŠ¨è°ƒæ•´æˆ–æœåŠ¡é‡å¯ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œäº†è§£SLAçš„é€‚åº”æ€§è°ƒåº¦æ˜¯å®ç°ç¨³å¥ã€é«˜æ€§èƒ½çš„CodeLLMæœåŠ¡çš„å…³é”®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19677v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°ä»£è½¯ä»¶å¼€å‘æµç¨‹ä¸­çš„é›†æˆåº¦è¶Šæ¥è¶Šé«˜ï¼Œä½†åœ¨èµ„æºå—é™çš„è‡ªæ‰˜ç®¡ç¯å¢ƒä¸­æœ‰æ•ˆåœ°æä¾›æœåŠ¡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰LLMæœåŠ¡ç³»ç»Ÿé‡‡ç”¨è¿ç»­æ‰¹å¤„ç†æ¥æé«˜ååé‡ï¼Œä½†å®ƒä»¬ä¾èµ–äºæ— æ³•é€‚åº”æ³¢åŠ¨è¯·æ±‚ç‡æˆ–å¼‚æ„å·¥ä½œè´Ÿè½½çš„é™æ€æ‰¹å¤„ç†å¤§å°é…ç½®ï¼Œå¯¼è‡´æœåŠ¡ç­‰çº§åè®®ï¼ˆSLAï¼‰é¢‘ç¹è¿è§„å’Œæ€§èƒ½ä¸ç¨³å®šã€‚æœ¬ç ”ç©¶æå‡ºSABERï¼Œä¸€ç§åŠ¨æ€æ‰¹å¤„ç†ç­–ç•¥ï¼Œå¯ä»¥é¢„æµ‹æ¯ä¸ªè¯·æ±‚çš„SLAå¯è¡Œæ€§å¹¶å®æ—¶è°ƒæ•´å†³ç­–ã€‚SABERåœ¨ä¸è¿›è¡Œæ‰‹åŠ¨è°ƒæ•´æˆ–æœåŠ¡é‡å¯çš„æƒ…å†µä¸‹ï¼Œæœ€ä½³é™æ€é…ç½®å¯æé«˜é«˜è¾¾26%çš„ååç‡ï¼Œå¹¶é™ä½é«˜è¾¾45%çš„å»¶è¿Ÿå˜åŒ–ç‡ã€‚ç»“æœè¡¨æ˜ï¼ŒSLAæ„ŸçŸ¥çš„è‡ªé€‚åº”è°ƒåº¦å¯¹äºç¨³å¥çš„é«˜æ€§èƒ½CodeLLMæœåŠ¡è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CodeLLMsåœ¨ç°ä»£è½¯ä»¶å¼€å‘ä¸­çš„åº”ç”¨æå‡äº†å¯¹æ€§èƒ½çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰LLMæœåŠ¡ç³»ç»Ÿä½¿ç”¨è¿ç»­æ‰¹å¤„ç†ä½†å­˜åœ¨æ€§èƒ½ä¸ç¨³å®šå’ŒSLAè¿è§„çš„é—®é¢˜ã€‚</li>
<li>SABERæ˜¯ä¸€ç§åŠ¨æ€æ‰¹å¤„ç†ç­–ç•¥ï¼Œå¯é¢„æµ‹æ¯ä¸ªè¯·æ±‚çš„SLAå¯è¡Œæ€§å¹¶å®æ—¶è°ƒæ•´å†³ç­–ã€‚</li>
<li>SABERç›¸è¾ƒäºæœ€ä½³é™æ€é…ç½®å¯æé«˜ååç‡é«˜è¾¾26%ã€‚</li>
<li>SABERå¯é™ä½å»¶è¿Ÿå˜åŒ–ç‡é«˜è¾¾45%ã€‚</li>
<li>SABERå®ç°äº†é«˜æ€§èƒ½ä¸”æ— éœ€æ‰‹åŠ¨è°ƒæ•´æˆ–æœåŠ¡é‡å¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6577559bb1dc27b4672a09b8758373cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2e8d1d8ba95bc1d58185c0e19250fcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d4499c693a1d2d5d2beecbd1d45e6a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e5c013ac1100039a38d52a49d36a527.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59ec51acba1f18fbfb2f0b2a1a041ccf.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Recycling-the-Web-A-Method-to-Enhance-Pre-training-Data-Quality-and-Quantity-for-Language-Models"><a href="#Recycling-the-Web-A-Method-to-Enhance-Pre-training-Data-Quality-and-Quantity-for-Language-Models" class="headerlink" title="Recycling the Web: A Method to Enhance Pre-training Data Quality and   Quantity for Language Models"></a>Recycling the Web: A Method to Enhance Pre-training Data Quality and   Quantity for Language Models</h2><p><strong>Authors:Thao Nguyen, Yang Li, Olga Golovneva, Luke Zettlemoyer, Sewoong Oh, Ludwig Schmidt, Xian Li</strong></p>
<p>Scaling laws predict that the performance of large language models improves with increasing model size and data size. In practice, pre-training has been relying on massive web crawls, using almost all data sources publicly available on the internet so far. However, this pool of natural data does not grow at the same rate as the compute supply. Furthermore, the availability of high-quality texts is even more limited: data filtering pipelines often remove up to 99% of the initial web scrapes to achieve state-of-the-art. To address the â€œdata wallâ€ of pre-training scaling, our work explores ways to transform and recycle data discarded in existing filtering processes. We propose REWIRE, REcycling the Web with guIded REwrite, a method to enrich low-quality documents so that they could become useful for training. This in turn allows us to increase the representation of synthetic data in the final pre-training set. Experiments at 1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw texts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points improvement respectively across 22 diverse tasks, compared to training on only filtered web data. Training on the raw-synthetic data mix is also more effective than having access to 2x web data. Through further analysis, we demonstrate that about 82% of the mixed in texts come from transforming lower-quality documents that would otherwise be discarded. REWIRE also outperforms related approaches of generating synthetic data, including Wikipedia-style paraphrasing, question-answer synthesizing and knowledge extraction. These results suggest that recycling web texts holds the potential for being a simple and effective approach for scaling pre-training data. </p>
<blockquote>
<p>è§„æ¨¡å®šå¾‹é¢„æµ‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ä¼šéšç€æ¨¡å‹è§„æ¨¡å’Œæ•°æ®è§„æ¨¡çš„å¢åŠ è€Œæé«˜ã€‚å®é™…ä¸Šï¼Œé¢„è®­ç»ƒä¸€ç›´ä¾èµ–äºå¤§é‡çš„ç½‘ç»œçˆ¬è™«ï¼Œè¿„ä»Šä¸ºæ­¢å‡ ä¹ä½¿ç”¨äº†äº’è”ç½‘ä¸Šæ‰€æœ‰å¯ç”¨çš„æ•°æ®æºã€‚ç„¶è€Œï¼Œè¿™ä¸ªè‡ªç„¶æ•°æ®çš„æ± å¹¶ä¸ä¼šä»¥ä¸è®¡ç®—ä¾›åº”ç›¸åŒçš„é€Ÿåº¦å¢é•¿ã€‚æ­¤å¤–ï¼Œé«˜è´¨é‡æ–‡æœ¬çš„å¯è·å¾—æ€§æ›´åŠ æœ‰é™ï¼šæ•°æ®è¿‡æ»¤ç®¡é“é€šå¸¸ä¼šå»é™¤é«˜è¾¾99%çš„åˆå§‹ç½‘ç»œæŠ“å–æ•°æ®ï¼Œä»¥å®ç°æœ€æ–°æŠ€æœ¯ã€‚ä¸ºäº†è§£å†³é¢„è®­ç»ƒè§„æ¨¡æ‰©å±•çš„â€œæ•°æ®å£å’â€é—®é¢˜ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ¢ç´¢äº†å°†ç°æœ‰è¿‡æ»¤è¿‡ç¨‹ä¸­ä¸¢å¼ƒçš„æ•°æ®è¿›è¡Œè½¬æ¢å’Œå›æ”¶çš„æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†REWIREï¼Œå³ä½¿ç”¨æŒ‡å¯¼é‡å†™æ¥å›æ”¶ç½‘ç»œæ•°æ®ï¼Œè¿™æ˜¯ä¸€ç§ä¸°å¯Œä½è´¨é‡æ–‡æ¡£çš„æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¯¹è®­ç»ƒäº§ç”Ÿä½œç”¨ã€‚è¿™åè¿‡æ¥åˆä½¿æˆ‘ä»¬èƒ½å¤Ÿå¢åŠ åˆæˆæ•°æ®åœ¨æœ€ç»ˆé¢„è®­ç»ƒé›†ä¸­çš„è¡¨ç¤ºã€‚åœ¨DCLMåŸºå‡†æµ‹è¯•çš„1Bã€3Bå’Œ7Bè§„æ¨¡ä¸‹çš„å®éªŒè¡¨æ˜ï¼Œä¸ä»…ä½¿ç”¨è¿‡æ»¤åçš„ç½‘ç»œæ•°æ®è®­ç»ƒç›¸æ¯”ï¼Œæ··åˆé«˜è´¨é‡åŸå§‹æ–‡æœ¬å’Œæˆ‘ä»¬çš„é‡å†™æ–‡æœ¬åœ¨22ä¸ªä¸åŒä»»åŠ¡ä¸Šåˆ†åˆ«æé«˜äº†1.0ã€1.3å’Œ2.5ä¸ªç™¾åˆ†ç‚¹ã€‚å¯¹åŸå§‹åˆæˆæ•°æ®æ··åˆçš„è®­ç»ƒä¹Ÿæ¯”è®¿é—®ä¸¤å€çš„ç½‘ç»œæ•°æ®æ›´æœ‰æ•ˆã€‚é€šè¿‡è¿›ä¸€æ­¥åˆ†æï¼Œæˆ‘ä»¬è¯æ˜å¤§çº¦82%çš„æ··åˆæ–‡æœ¬æ¥è‡ªè½¬æ¢åŸæœ¬ä¼šè¢«ä¸¢å¼ƒçš„ä½è´¨é‡æ–‡æ¡£ã€‚REWIREä¹Ÿä¼˜äºå…¶ä»–ç”Ÿæˆåˆæˆæ•°æ®çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä»¥Wikipediaä¸ºé£æ ¼çš„æ”¹è¿°ã€é—®é¢˜ç­”æ¡ˆåˆæˆå’ŒçŸ¥è¯†æå–ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå›æ”¶ç½‘ç»œæ–‡æœ¬å…·æœ‰æ½œåŠ›ï¼Œå¯èƒ½æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ‰©å±•é¢„è®­ç»ƒæ•°æ®çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04689v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®é—®é¢˜ã€‚éšç€æ¨¡å‹å’Œæ•°æ®è§„æ¨¡çš„å¢åŠ ï¼Œé¢„è®­ç»ƒä¾èµ–äºå¤§é‡çš„ç½‘ç»œçˆ¬è™«æ•°æ®ã€‚ç„¶è€Œï¼Œè‡ªç„¶æ•°æ®çš„å¢é•¿é€Ÿåº¦å¹¶ä¸ä¸è®¡ç®—ä¾›åº”çš„é€Ÿåº¦ç›¸åŒ¹é…ï¼Œé«˜è´¨é‡æ–‡æœ¬çš„å¯è·å¾—æ€§æ›´åŠ æœ‰é™ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºREWIREçš„æ–¹æ³•ï¼Œé€šè¿‡è½¬åŒ–å’Œå›æ”¶è¢«ç°æœ‰è¿‡æ»¤è¿‡ç¨‹ä¸¢å¼ƒçš„æ•°æ®ï¼Œä½¿å…¶å¯ç”¨äºè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œæ··åˆé«˜è´¨é‡åŸå§‹æ–‡æœ¬å’Œé‡å†™æ–‡æœ¬åœ¨DCLMåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚è¿›ä¸€æ­¥åˆ†ææ˜¾ç¤ºï¼Œçº¦82%çš„æ··åˆæ–‡æœ¬æ¥è‡ªäºåŸæœ¬ä¼šè¢«ä¸¢å¼ƒçš„ä½è´¨é‡æ–‡æ¡£çš„è½¬åŒ–ã€‚REWIREæ–¹æ³•åœ¨å…¶ä»–ç”Ÿæˆåˆæˆæ•°æ®çš„æ–¹æ³•ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œè¡¨æ˜å›æ”¶ç½‘ç»œæ–‡æœ¬å¯èƒ½æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ‰©å¤§é¢„è®­ç»ƒæ•°æ®è§„æ¨¡çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æå‡ä¾èµ–äºæ¨¡å‹å’Œæ•°æ®è§„æ¨¡çš„å¢åŠ ï¼Œä½†è‡ªç„¶æ•°æ®çš„å¢é•¿ä¸è®¡ç®—ä¾›åº”ä¸åŒ¹é…ã€‚</li>
<li>é«˜è´¨é‡æ–‡æœ¬åœ¨é¢„è®­ç»ƒä¸­çš„å¯è·å¾—æ€§æœ‰é™ï¼Œæ•°æ®è¿‡æ»¤ç®¡é“ä¼šå»é™¤å¤§é‡åˆå§‹ç½‘ç»œæŠ“å–çš„æ•°æ®ã€‚</li>
<li>REWIREæ–¹æ³•æ—¨åœ¨è½¬åŒ–å’Œå›æ”¶è¢«ä¸¢å¼ƒçš„æ•°æ®ï¼Œä½¿å…¶å¯ç”¨äºè®­ç»ƒï¼Œå¢åŠ åˆæˆæ•°æ®åœ¨é¢„è®­ç»ƒé›†ä¸­çš„ä»£è¡¨æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œæ··åˆé«˜è´¨é‡åŸå§‹æ–‡æœ¬å’Œé‡å†™æ–‡æœ¬åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>çº¦82%çš„æ··åˆæ–‡æœ¬æ¥è‡ªäºåŸæœ¬ä¼šè¢«ä¸¢å¼ƒçš„ä½è´¨é‡æ–‡æ¡£çš„è½¬åŒ–ã€‚</li>
<li>REWIREæ–¹æ³•åœ¨ç”Ÿæˆåˆæˆæ•°æ®çš„å…¶ä»–æ–¹æ³•ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04689">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8344d3a782f54d4e616be76c0814f141.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e410182d0db0c4048a7b8f044455068a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9350255ea85d554f39c71155fcf4a35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc3ff41ce1de00684b794b2918b05627.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Training-Plug-n-Play-Knowledge-Modules-with-Deep-Context-Distillation"><a href="#Training-Plug-n-Play-Knowledge-Modules-with-Deep-Context-Distillation" class="headerlink" title="Training Plug-n-Play Knowledge Modules with Deep Context Distillation"></a>Training Plug-n-Play Knowledge Modules with Deep Context Distillation</h2><p><strong>Authors:Lucas Caccia, Alan Ansell, Edoardo Ponti, Ivan VuliÄ‡, Alessandro Sordoni</strong></p>
<p>Dynamically integrating new or rapidly evolving information after (Large) Language Model pre-training remains challenging, particularly in low-data scenarios or when dealing with private and specialized documents. In-context learning and retrieval-augmented generation (RAG) face limitations, including their high inference costs and their inability to capture global document information. In this paper, we propose a way of modularizing knowledge by training document-level Knowledge Modules (KMs). KMs are lightweight components implemented as parameter-efficient LoRA modules, which are trained to store information about new documents and can be easily plugged into models on demand. We show that next-token prediction performs poorly as the training objective for KMs. We instead propose Deep Context Distillation: we learn KMs parameters such as to simulate hidden states and logits of a teacher that takes the document in context. Our method outperforms standard next-token prediction and pre-instruction training techniques, across two datasets. Finally, we highlight synergies between KMs and RAG. </p>
<blockquote>
<p>åœ¨ï¼ˆå¤§å‹ï¼‰è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒåï¼ŒåŠ¨æ€æ•´åˆæ–°ä¿¡æ¯æˆ–è¿…é€Ÿå‘å±•çš„ä¿¡æ¯ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„åœºæ™¯æˆ–å¤„ç†ç§æœ‰å’Œä¸“ä¸šæ–‡æ¡£æ—¶ã€‚ä¸Šä¸‹æ–‡å­¦ä¹ å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é¢ä¸´åŒ…æ‹¬é«˜æ¨ç†æˆæœ¬å’Œæ— æ³•æ•è·å…¨å±€æ–‡æ¡£ä¿¡æ¯åœ¨å†…çš„å±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡è®­ç»ƒæ–‡æ¡£çº§çŸ¥è¯†æ¨¡å—ï¼ˆKMï¼‰æ¥å®ç°çŸ¥è¯†æ¨¡å—åŒ–åŒ–çš„æ–¹æ³•ã€‚çŸ¥è¯†æ¨¡å—è¢«å®ç°ä¸ºå‚æ•°é«˜æ•ˆçš„LoRAæ¨¡å—ï¼Œèƒ½å¤Ÿå­˜å‚¨å…³äºæ–°æ–‡æ¡£çš„ä¿¡æ¯ï¼Œå¹¶å¯æŒ‰éœ€è½»æ¾æ’å…¥æ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°ï¼Œä½œä¸ºçŸ¥è¯†æ¨¡å—çš„è®­ç»ƒç›®æ ‡ï¼Œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹è¡¨ç°ä¸ä½³ã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†æ·±åº¦ä¸Šä¸‹æ–‡è’¸é¦ï¼šæˆ‘ä»¬å­¦ä¹ çŸ¥è¯†æ¨¡å—çš„å‚æ•°ï¼Œä»¥æ¨¡æ‹Ÿæ•™å¸ˆåœ¨ä¸Šä¸‹æ–‡ä¸­çš„æ–‡æ¡£äº§ç”Ÿçš„éšè—çŠ¶æ€å’Œé€»è¾‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæ ‡å‡†çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹å’Œé¢„å…ˆæŒ‡ä»¤è®­ç»ƒæŠ€æœ¯ã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†çŸ¥è¯†æ¨¡å—å’ŒRAGä¹‹é—´çš„ååŒä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08727v3">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è®­ç»ƒæ–‡æ¡£çº§åˆ«çš„çŸ¥è¯†æ¨¡å—ï¼ˆKMsï¼‰æ¥æ¨¡å—åŒ–çŸ¥è¯†çš„æ–¹æ³•ã€‚KMsä½œä¸ºå‚æ•°é«˜æ•ˆçš„LoRAæ¨¡å—å®ç°ï¼Œèƒ½å¤Ÿå­˜å‚¨æ–°æ–‡æ¡£çš„ä¿¡æ¯ï¼Œå¹¶å¯æ ¹æ®éœ€æ±‚è½»æ¾æ’å…¥æ¨¡å‹ä¸­ã€‚æ–‡ç« æŒ‡å‡ºï¼Œä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œä¸ºç›®æ ‡å¯¹KMsè¿›è¡Œè®­ç»ƒè¡¨ç°ä¸ä½³ï¼Œå¹¶æå‡ºäº†æ·±åº¦ä¸Šä¸‹æ–‡è’¸é¦æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å­¦ä¹ KMçš„å‚æ•°ä»¥æ¨¡æ‹Ÿæ•™å¸ˆæ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ä¸­çš„éšè—çŠ¶æ€å’Œé€»è¾‘ã€‚æ­¤æ–¹æ³•åœ¨ä¸¤ç§æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºæ ‡å‡†çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹å’Œé¢„å…ˆæŒ‡ä»¤è®­ç»ƒæŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å¼ºè°ƒäº†çŸ¥è¯†æ¨¡å—ä¸RAGä¹‹é—´çš„ååŒä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨ä½æ•°æ®åœºæ™¯æˆ–å¤„ç†ç§æœ‰å’Œç‰¹æ®Šæ–‡æ¡£æ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒååŠ¨æ€é›†æˆæ–°ä¿¡æ¯æˆ–å¿«é€Ÿæ¼”åŒ–çš„ä¿¡æ¯å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>å½“å‰çš„æ–¹æ³•å¦‚ä¸Šä¸‹æ–‡å­¦ä¹ å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å­˜åœ¨å±€é™æ€§ï¼ŒåŒ…æ‹¬é«˜æ¨ç†æˆæœ¬å’Œæ— æ³•æ•è·å…¨å±€æ–‡æ¡£ä¿¡æ¯ã€‚</li>
<li>æå‡ºäº†çŸ¥è¯†æ¨¡å—ï¼ˆKMsï¼‰çš„æ¦‚å¿µï¼Œä½œä¸ºå‚æ•°é«˜æ•ˆçš„æ¨¡å—æ¥å­˜å‚¨æ–°æ–‡æ¡£çš„ä¿¡æ¯ï¼Œå¹¶å¯ä»¥è½»æ˜“åœ°æŒ‰éœ€æ’å…¥åˆ°æ¨¡å‹ä¸­ã€‚</li>
<li>æŒ‡å‡ºä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œä¸ºç›®æ ‡çš„è®­ç»ƒæ–¹æ³•è¡¨ç°ä¸ä½³ã€‚</li>
<li>æå‡ºäº†æ·±åº¦ä¸Šä¸‹æ–‡è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡æ¨¡æ‹Ÿæ•™å¸ˆåœ¨ä¸Šä¸‹æ–‡ä¸­çš„éšè—çŠ¶æ€å’Œé€»è¾‘æ¥å­¦ä¹ KMçš„å‚æ•°ã€‚</li>
<li>æ·±åº¦ä¸Šä¸‹æ–‡è’¸é¦æ–¹æ³•åœ¨ä¸¤ç§æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºä¼ ç»Ÿçš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹å’Œé¢„å…ˆæŒ‡ä»¤è®­ç»ƒæŠ€æœ¯ã€‚</li>
<li>çŸ¥è¯†æ¨¡å—ä¸RAGä¹‹é—´å­˜åœ¨ååŒä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-83e9a574443a026d97bb7e6efe429ee1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28f1aa5d00f06e937e561164f70423b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b29eef7c10a6b81ca332e41d88f9ee45.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Adversarial-Reasoning-at-Jailbreaking-Time"><a href="#Adversarial-Reasoning-at-Jailbreaking-Time" class="headerlink" title="Adversarial Reasoning at Jailbreaking Time"></a>Adversarial Reasoning at Jailbreaking Time</h2><p><strong>Authors:Mahdi Sabbaghi, Paul Kassianik, George Pappas, Yaron Singer, Amin Karbasi, Hamed Hassani</strong></p>
<p>As large language models (LLMs) are becoming more capable and widespread, the study of their failure cases is becoming increasingly important. Recent advances in standardizing, measuring, and scaling test-time compute suggest new methodologies for optimizing models to achieve high performance on hard tasks. In this paper, we apply these advances to the task of model jailbreaking: eliciting harmful responses from aligned LLMs. We develop an adversarial reasoning approach to automatic jailbreaking that leverages a loss signal to guide the test-time compute, achieving SOTA attack success rates against many aligned LLMs, even those that aim to trade inference-time compute for adversarial robustness. Our approach introduces a new paradigm in understanding LLM vulnerabilities, laying the foundation for the development of more robust and trustworthy AI systems. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›è¶Šæ¥è¶Šå¼ºï¼Œåº”ç”¨èŒƒå›´è¶Šæ¥è¶Šå¹¿ï¼Œå¯¹å…¶å¤±è´¥æ¡ˆä¾‹çš„ç ”ç©¶ä¹Ÿå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ€è¿‘åœ¨æ ‡å‡†åŒ–ã€æµ‹é‡å’Œæ‰©å±•æµ‹è¯•æ—¶é—´è®¡ç®—æ–¹é¢çš„è¿›å±•ï¼Œä¸ºä¼˜åŒ–æ¨¡å‹ä»¥åœ¨å›°éš¾ä»»åŠ¡ä¸Šå®ç°é«˜æ€§èƒ½æä¾›äº†æ–°çš„æ–¹æ³•è®ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†è¿™äº›è¿›å±•åº”ç”¨äºæ¨¡å‹è¶Šç‹±ä»»åŠ¡ï¼šä»å¯¹é½çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¼•å‡ºæœ‰å®³å“åº”ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åˆ©ç”¨æŸå¤±ä¿¡å·å¼•å¯¼æµ‹è¯•æ—¶é—´è®¡ç®—çš„å¯¹æŠ—æ€§æ¨ç†è‡ªåŠ¨è¶Šç‹±æ–¹æ³•ï¼Œé’ˆå¯¹è®¸å¤šå¯¹é½çš„å¤§å‹è¯­è¨€æ¨¡å‹å®ç°äº†æœ€å…ˆè¿›çš„æ”»å‡»æˆåŠŸç‡ï¼Œç”šè‡³é’ˆå¯¹é‚£äº›æ—¨åœ¨ç”¨å¯¹æŠ—æ€§ç¨³å¥æ€§æ¢å–æ¨ç†æ—¶é—´è®¡ç®—çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¼æ´å¼•å…¥äº†ä¸€ç§æ–°çš„èŒƒå¼ï¼Œä¸ºå¼€å‘æ›´ç¨³å¥å’Œå¯ä¿¡èµ–çš„AIç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01633v2">PDF</a> Accepted to the 42nd International Conference on Machine Learning   (ICML 2025)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤±è´¥æ¡ˆä¾‹ç ”ç©¶å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬æ–‡åº”ç”¨æœ€è¿‘çš„æµ‹è¯•æ—¶é—´è®¡ç®—æ ‡å‡†åŒ–ã€æµ‹é‡å’Œè§„æ¨¡åŒ–çš„è¿›å±•æ¥å¯¹æ¨¡å‹è¶Šç‹±ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚å¼€å‘äº†ä¸€ç§åˆ©ç”¨æŸå¤±ä¿¡å·å¼•å¯¼æµ‹è¯•æ—¶é—´è®¡ç®—çš„å¯¹é½LLMè‡ªåŠ¨è¶Šç‹±çš„å¯¹æŠ—æ¨ç†æ–¹æ³•ï¼Œå®ç°å¯¹è®¸å¤šå¯¹é½LLMçš„é«˜æˆåŠŸç‡æ”»å‡»ï¼Œç”šè‡³é’ˆå¯¹é‚£äº›æ—¨åœ¨ä»¥å¯¹æŠ—æ€§ç¨³å¥æ€§æ¢å–æ¨ç†æ—¶é—´è®¡ç®—çš„ç³»ç»Ÿã€‚è¿™ä¸ºç†è§£LLMçš„æ¼æ´å¼•å…¥äº†ä¸€ç§æ–°èŒƒå¼ï¼Œä¸ºå¼€å‘æ›´ç¨³å¥å’Œå¯ä¿¡èµ–çš„AIç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤±è´¥æ¡ˆä¾‹ç ”ç©¶çš„é‡è¦æ€§éšç€æ¨¡å‹èƒ½åŠ›çš„æé«˜å’Œæ™®åŠè€Œå¢åŠ ã€‚</li>
<li>æœ¬æ–‡åˆ©ç”¨æœ€æ–°çš„æµ‹è¯•æ—¶é—´è®¡ç®—æ ‡å‡†åŒ–ã€æµ‹é‡å’Œè§„æ¨¡åŒ–çš„è¿›å±•æ¥ä¼˜åŒ–æ¨¡å‹è¶Šç‹±ä»»åŠ¡ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§å¯¹æŠ—æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨æŸå¤±ä¿¡å·æ¥å¼•å¯¼æµ‹è¯•æ—¶é—´è®¡ç®—ï¼Œå®ç°å¯¹é½LLMçš„è‡ªåŠ¨è¶Šç‹±ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†å¯¹è®¸å¤šå¯¹é½LLMçš„é«˜æˆåŠŸç‡æ”»å‡»ï¼ŒåŒ…æ‹¬é‚£äº›ä»¥å¯¹æŠ—æ€§ç¨³å¥æ€§æ¢å–æ¨ç†æ—¶é—´è®¡ç®—çš„ç³»ç»Ÿã€‚</li>
<li>æœ¬æ–‡å¼•å…¥äº†ä¸€ç§ç†è§£LLMæ¼æ´çš„æ–°èŒƒå¼ï¼Œæœ‰åŠ©äºæ·±å…¥äº†è§£æ¨¡å‹çš„å¼±ç‚¹ã€‚</li>
<li>è¿™ç§æ–°æ–¹æ³•ä¸ºå¼€å‘æ›´ç¨³å¥å’Œå¯ä¿¡èµ–çš„AIç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-22199abf49672224150759b3577b5662.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba0cb133a7c050e6f7ac12c1d527546b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52c8d758d0811f9e82e94440e5dfec1b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Integrating-Various-Software-Artifacts-for-Better-LLM-based-Bug-Localization-and-Program-Repair"><a href="#Integrating-Various-Software-Artifacts-for-Better-LLM-based-Bug-Localization-and-Program-Repair" class="headerlink" title="Integrating Various Software Artifacts for Better LLM-based Bug   Localization and Program Repair"></a>Integrating Various Software Artifacts for Better LLM-based Bug   Localization and Program Repair</h2><p><strong>Authors:Qiong Feng, Xiaotian Ma, Jiayi Sheng, Ziyuan Feng, Wei Song, Peng Liang</strong></p>
<p>LLMs have garnered considerable attention for their potential to streamline Automated Program Repair (APR). LLM-based approaches can either insert the correct code or directly generate patches when provided with buggy methods. However, most of LLM-based APR methods rely on a single type of software information, without fully leveraging different software artifacts. Despite this, many LLM-based approaches do not explore which specific types of information best assist in APR. Addressing this gap is crucial for advancing LLM-based APR techniques. We propose DEVLoRe to use issue content (description and message) and stack error traces to localize buggy methods, then rely on debug information in buggy methods and issue content and stack error to localize buggy lines and generate plausible patches which can pass all unit tests. The results show that while issue content is particularly effective in assisting LLMs with fault localization and program repair, different types of software artifacts complement each other. By incorporating different artifacts, DEVLoRe successfully locates 49.3% and 47.6% of single and non-single buggy methods and generates 56.0% and 14.5% plausible patches for the Defects4J v2.0 dataset, respectively. This outperforms current state-of-the-art APR methods. Furthermore, we re-implemented and evaluated our framework, demonstrating its effectiveness in its effectiveness in resolving 9 unique issues compared to other state-of-the-art frameworks using the same or more advanced models on SWE-bench Lite.We also discussed whether a leading framework for Python code can be directly applied to Java code, or vice versa. The source code and experimental results of this work for replication are available at <a target="_blank" rel="noopener" href="https://github.com/XYZboom/DEVLoRe">https://github.com/XYZboom/DEVLoRe</a>. </p>
<blockquote>
<p>LLMsï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰å› å…¶è‡ªåŠ¨ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰çš„æ½œåŠ›è€Œå¤‡å—å…³æ³¨ã€‚åŸºäºLLMçš„æ–¹æ³•å¯ä»¥åœ¨æä¾›æœ‰ç¼ºé™·çš„æ–¹æ³•æ—¶æ’å…¥æ­£ç¡®çš„ä»£ç æˆ–ç›´æ¥ç”Ÿæˆè¡¥ä¸ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºäºLLMçš„APRæ–¹æ³•ä»…ä¾èµ–ä¸€ç§è½¯ä»¶ä¿¡æ¯ï¼Œå¹¶æ²¡æœ‰å……åˆ†åˆ©ç”¨å„ç§è½¯ä»¶å·¥ä»¶ã€‚å°½ç®¡è®¸å¤šåŸºäºLLMçš„æ–¹æ³•å¹¶æ²¡æœ‰æ¢ç´¢å“ªç§ç‰¹å®šç±»å‹çš„ä¿¡æ¯æœ€èƒ½è¾…åŠ©APRã€‚å¡«è¡¥è¿™ä¸€ç©ºç™½å¯¹äºæ¨åŠ¨åŸºäºLLMçš„APRæŠ€æœ¯è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºDEVLoReï¼Œä½¿ç”¨é—®é¢˜å†…å®¹ï¼ˆæè¿°å’Œä¿¡æ¯ï¼‰å’Œå †æ ˆé”™è¯¯è·Ÿè¸ªæ¥å®šä½æœ‰ç¼ºé™·çš„æ–¹æ³•ï¼Œç„¶åä¾èµ–æœ‰ç¼ºé™·æ–¹æ³•çš„è°ƒè¯•ä¿¡æ¯ã€é—®é¢˜å†…å®¹å’Œå †æ ˆé”™è¯¯æ¥å®šä½æœ‰ç¼ºé™·çš„è¡Œå¹¶ç”Ÿæˆå¯é€šè¿‡æ‰€æœ‰å•å…ƒæµ‹è¯•çš„åˆç†è§£é‡Šè¡¥ä¸ã€‚ç»“æœè¡¨æ˜ï¼Œé—®é¢˜å†…å®¹åœ¨å¸®åŠ©LLMè¿›è¡Œæ•…éšœå®šä½å’Œç¨‹åºä¿®å¤æ–¹é¢ç‰¹åˆ«æœ‰æ•ˆï¼Œä¸åŒç±»å‹çš„è½¯ä»¶å·¥ä»¶å¯ä»¥ç›¸äº’è¡¥å……ã€‚é€šè¿‡èå…¥ä¸åŒçš„å·¥ä»¶ï¼ŒDEVLoReæˆåŠŸå®šä½äº†Defects4J v2.0æ•°æ®é›†çš„49.3%å’Œ47.6%çš„å•bugå’Œéå•bugæ–¹æ³•ï¼Œå¹¶ä¸ºè¯¥æ•°æ®é›†ç”Ÿæˆäº†56.0%å’Œ14.5%çš„åˆç†è§£é‡Šè¡¥ä¸ã€‚è¿™è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„APRæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡æ–°å®ç°å¹¶è¯„ä¼°äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œè¯æ˜å…¶åœ¨è§£å†³ä¸SWE-bench Liteä¸Šå…¶ä»–æœ€å…ˆè¿›æ¡†æ¶ç›¸æ¯”çš„9ä¸ªç‹¬ç‰¹é—®é¢˜ä¸Šæ›´æœ‰æ•ˆã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†é¢†å…ˆçš„Pythonä»£ç æ¡†æ¶æ˜¯å¦å¯ä»¥ç›´æ¥åº”ç”¨äºJavaä»£ç ï¼Œåä¹‹äº¦ç„¶ã€‚æœ¬å·¥ä½œçš„æºä»£ç å’Œå®éªŒç»“æœå¯ä¾›å¤åˆ¶ï¼Œè®¿é—®åœ°å€ä¸º<a target="_blank" rel="noopener" href="https://github.com/XYZboom/DEVLoRe%E3%80%82">https://github.com/XYZboom/DEVLoReã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03905v3">PDF</a> 25 pages, 12 images, 10 tables, Manuscript revision submitted to a   journal (2025)</p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰ä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿé€šè¿‡æ’å…¥æ­£ç¡®çš„ä»£ç æˆ–ç›´æ¥ç”Ÿæˆè¡¥ä¸æ¥ä¿®å¤bugã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°LLM-basedçš„APRæ–¹æ³•ä»…ä¾èµ–ä¸€ç§è½¯ä»¶ä¿¡æ¯ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨ä¸åŒçš„è½¯ä»¶å·¥ä»¶ã€‚æœ¬æ–‡æå‡ºçš„DEVLoReæ–¹æ³•ç»“åˆé—®é¢˜å†…å®¹ï¼ˆæè¿°å’Œä¿¡æ¯ï¼‰ã€å †æ ˆé”™è¯¯è·Ÿè¸ªæ¥å®šä½é”™è¯¯æ–¹æ³•å’Œè¡Œï¼Œå¹¶ç”Ÿæˆå¯é€šè¿‡æ‰€æœ‰å•å…ƒæµ‹è¯•çš„è¡¥ä¸ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé—®é¢˜å†…å®¹ç‰¹åˆ«æœ‰åŠ©äºLLMè¿›è¡Œæ•…éšœå®šä½å’Œç¨‹åºä¿®å¤ï¼Œä¸åŒç±»å‹çš„è½¯ä»¶å·¥ä»¶å¯ä»¥ç›¸äº’è¡¥å……ã€‚è¯¥æ–¹æ³•åœ¨Defects4J v2.0æ•°æ®é›†ä¸Šå®šä½äº†49.3%å’Œ47.6%çš„å•bugå’Œéå•bugæ–¹æ³•ï¼Œå¹¶åˆ†åˆ«ç”Ÿæˆäº†56.0%å’Œ14.5%çš„å¯è¡Œè¡¥ä¸ï¼Œä¼˜äºå½“å‰æœ€å…ˆè¿›çš„APRæ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†Pythonä»£ç æ¡†æ¶æ˜¯å¦å¯ä»¥ç›´æ¥åº”ç”¨äºJavaä»£ç çš„é—®é¢˜ã€‚æºä»£ç å’Œå®éªŒç»“æœå¯åœ¨XYZboom&#x2F;DEVLoReè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤é¢†åŸŸæœ‰æ½œåŠ›ï¼Œå¯ä¿®å¤bugçš„æ–¹æ³•åŒ…æ‹¬æ’å…¥æ­£ç¡®ä»£ç æˆ–ç›´æ¥ç”Ÿæˆè¡¥ä¸ã€‚</li>
<li>å¤§å¤šæ•°LLM-basedçš„APRæ–¹æ³•ä»…ä¾èµ–å•ä¸€è½¯ä»¶ä¿¡æ¯ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å…¶ä»–è½¯ä»¶å·¥ä»¶ã€‚</li>
<li>DEVLoReç»“åˆé—®é¢˜å†…å®¹ï¼ˆæè¿°å’Œä¿¡æ¯ï¼‰ã€å †æ ˆé”™è¯¯è·Ÿè¸ªæ¥å®šä½é”™è¯¯æ–¹æ³•å’Œè¡Œã€‚</li>
<li>é—®é¢˜å†…å®¹ç‰¹åˆ«æœ‰åŠ©äºLLMè¿›è¡Œæ•…éšœå®šä½å’Œç¨‹åºä¿®å¤ã€‚</li>
<li>ä¸åŒç±»å‹çš„è½¯ä»¶å·¥ä»¶å¯ä»¥ç›¸äº’è¡¥å……ï¼Œæå‡APRæ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>åœ¨Defects4J v2.0æ•°æ®é›†ä¸Šï¼ŒDEVLoReå®šä½äº†æ¥è¿‘ä¸€åŠçš„é”™è¯¯æ–¹æ³•å¹¶ç”Ÿæˆäº†ä¸€å®šæ•°é‡çš„å¯è¡Œè¡¥ä¸ï¼Œè¡¨ç°ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„APRæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03905">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2fb9118819f282aa707ffa04c5b53f44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b86674d2c346f42d7fc9ccceebd6d1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-880e06eb159994acade943a4bf7f65be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-436d2f7693cb378304226ba6a2be9da0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53d11cb67d8d6f56ae63d0c7197463ae.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Separating-Tongue-from-Thought-Activation-Patching-Reveals-Language-Agnostic-Concept-Representations-in-Transformers"><a href="#Separating-Tongue-from-Thought-Activation-Patching-Reveals-Language-Agnostic-Concept-Representations-in-Transformers" class="headerlink" title="Separating Tongue from Thought: Activation Patching Reveals   Language-Agnostic Concept Representations in Transformers"></a>Separating Tongue from Thought: Activation Patching Reveals   Language-Agnostic Concept Representations in Transformers</h2><p><strong>Authors:ClÃ©ment Dumas, Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West</strong></p>
<p>A central question in multilingual language modeling is whether large language models (LLMs) develop a universal concept representation, disentangled from specific languages. In this paper, we address this question by analyzing latent representations (latents) during a word-translation task in transformer-based LLMs. We strategically extract latents from a source translation prompt and insert them into the forward pass on a target translation prompt. By doing so, we find that the output language is encoded in the latent at an earlier layer than the concept to be translated. Building on this insight, we conduct two key experiments. First, we demonstrate that we can change the concept without changing the language and vice versa through activation patching alone. Second, we show that patching with the mean representation of a concept across different languages does not affect the modelsâ€™ ability to translate it, but instead improves it. Finally, we generalize to multi-token generation and demonstrate that the model can generate natural language description of those mean representations. Our results provide evidence for the existence of language-agnostic concept representations within the investigated models. </p>
<blockquote>
<p>åœ¨å¤šè¯­è¨€è¯­è¨€å»ºæ¨¡ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦å‘å±•å‡ºä¸ç‰¹å®šè¯­è¨€æ— å…³çš„æ¦‚å¿µè¡¨ç¤ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æåŸºäºtransformerçš„LLMåœ¨å•è¯ç¿»è¯‘ä»»åŠ¡ä¸­çš„æ½œåœ¨è¡¨ç¤ºï¼ˆæ½œå˜é‡ï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬ç­–ç•¥æ€§åœ°ä»æºç¿»è¯‘æç¤ºä¸­æå–æ½œå˜é‡ï¼Œå¹¶å°†å…¶æ’å…¥åˆ°ç›®æ ‡ç¿»è¯‘æç¤ºçš„å‰å‘ä¼ é€’ä¸­ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬å‘ç°è¾“å‡ºè¯­è¨€æ¯”è¦ç¿»è¯‘çš„æ¦‚å¿µåœ¨æ›´æ—©æœŸçš„å±‚æ¬¡ä¸Šè¢«ç¼–ç åœ¨æ½œåœ¨å˜é‡ä¸­ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸¤é¡¹å…³é”®å®éªŒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬å¯ä»¥é€šè¿‡æ¿€æ´»è¡¥ä¸å•ç‹¬æ”¹å˜æ¦‚å¿µè€Œä¸æ”¹å˜è¯­è¨€ï¼Œåä¹‹äº¦ç„¶ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œä½¿ç”¨ä¸åŒè¯­è¨€çš„æ¦‚å¿µå¹³å‡è¡¨ç¤ºçš„è¡¥ä¸å¹¶ä¸ä¼šå½±å“æ¨¡å‹çš„ç¿»è¯‘èƒ½åŠ›ï¼Œåè€Œä¼šæœ‰æ‰€æé«˜ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ç»“æœæ¨å¹¿åˆ°å¤šä»¤ç‰Œç”Ÿæˆï¼Œå¹¶è¯æ˜è¯¥æ¨¡å‹å¯ä»¥ç”Ÿæˆè¿™äº›å¹³å‡è¡¨ç¤ºçš„è‡ªç„¶è¯­è¨€æè¿°ã€‚æˆ‘ä»¬çš„ç»“æœæä¾›äº†æ‰€ç ”ç©¶æ¨¡å‹ä¸­å­˜åœ¨ä¸è¯­è¨€æ— å…³çš„æ¦‚å¿µè¡¨ç¤ºçš„è¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.08745v4">PDF</a> 20 pages, 14 figures, previous version published under the title â€œHow   Do Llamas Process Multilingual Text? A Latent Exploration through Activation   Patchingâ€ at the ICML 2024 mechanistic interpretability workshop at   <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=0ku2hIm4BS">https://openreview.net/forum?id=0ku2hIm4BS</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯ç¿»è¯‘ä»»åŠ¡ä¸­çš„æ½œåœ¨è¡¨ç¤ºç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹å†…å­˜åœ¨ä¸ç‰¹å®šè¯­è¨€æ— å…³çš„æ¦‚å¿µè¡¨ç¤ºã€‚é€šè¿‡åˆ†æç¿»è¯‘è¿‡ç¨‹ä¸­çš„æ½œä¼è¡¨ç¤ºï¼Œå‘ç°è¾“å‡ºè¯­è¨€æ¯”å¾…ç¿»è¯‘çš„æ¦‚å¿µæ›´æ—©åœ°ç¼–ç åœ¨æ½œä¼ä¸­ã€‚é€šè¿‡æ¿€æ´»è¡¥ä¸ï¼Œå¯ä»¥æ”¹å˜æ¦‚å¿µè€Œä¸æ”¹å˜è¯­è¨€ï¼Œåä¹‹äº¦ç„¶ã€‚ä½¿ç”¨ä¸åŒè¯­è¨€çš„æ¦‚å¿µå¹³å‡è¡¨ç¤ºè¿›è¡Œè¡¥ä¸å¤„ç†å¹¶ä¸å½±å“æ¨¡å‹çš„ç¿»è¯‘èƒ½åŠ›ï¼Œåè€Œæœ‰æ‰€æå‡ã€‚æœ€ç»ˆï¼Œæ¨¡å‹å¯ä»¥ç”Ÿæˆè¿™äº›å¹³å‡è¡¨ç¤ºçš„è‡ªç„¶è¯­è¨€æè¿°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è¯ç¿»è¯‘ä»»åŠ¡ä¸­çš„æ½œåœ¨è¡¨ç¤ºåˆ†ææ˜¯æ¢ç´¢è¯­è¨€æ¨¡å‹å†…éƒ¨æœºåˆ¶çš„é‡è¦é€”å¾„ã€‚</li>
<li>è¾“å‡ºè¯­è¨€åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­æ¯”å¾…ç¿»è¯‘çš„æ¦‚å¿µæ›´æ—©åœ°ç¼–ç åœ¨æ½œä¼è¡¨ç¤ºä¸­ã€‚</li>
<li>é€šè¿‡æ¿€æ´»è¡¥ä¸ï¼Œå¯ä»¥åœ¨ä¸æ”¹å˜è¯­è¨€çš„æƒ…å†µä¸‹æ”¹å˜æ¦‚å¿µï¼Œè¡¨æ˜LLMå­˜åœ¨ä¸è¯­è¨€æ— å…³çš„æ¦‚å¿µè¡¨ç¤ºã€‚</li>
<li>ä½¿ç”¨ä¸åŒè¯­è¨€çš„æ¦‚å¿µå¹³å‡è¡¨ç¤ºè¿›è¡Œè¡¥ä¸å¤„ç†ä¸ä¼šæŸå®³æ¨¡å‹çš„ç¿»è¯‘èƒ½åŠ›ï¼Œåè€Œå¯èƒ½æå‡æ€§èƒ½ã€‚</li>
<li>LLMèƒ½å¤Ÿç”Ÿæˆè¡¨ç¤ºæ¦‚å¿µå¹³å‡å€¼çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚</li>
<li>ç ”ç©¶ç»“æœä¸ºLLMå†…éƒ¨å­˜åœ¨è¯­è¨€æ— å…³çš„æ¦‚å¿µè¡¨ç¤ºæä¾›äº†è¯æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.08745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5c89a0df2eb8894ddf9492d289eed259.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c671b742606a81db08976fa78e3f81b2.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Learning-interpretable-positional-encodings-in-transformers-depends-on-initialization"><a href="#Learning-interpretable-positional-encodings-in-transformers-depends-on-initialization" class="headerlink" title="Learning interpretable positional encodings in transformers depends on   initialization"></a>Learning interpretable positional encodings in transformers depends on   initialization</h2><p><strong>Authors:Takuya Ito, Luca Cocchi, Tim Klinger, Parikshit Ram, Murray Campbell, Luke Hearne</strong></p>
<p>In transformers, the positional encoding (PE) provides essential information that distinguishes the position and order amongst tokens in a sequence. Most prior investigations of PE effects on generalization were tailored to 1D input sequences, such as those presented in natural language, where adjacent tokens (e.g., words) are highly related. In contrast, many real world tasks involve datasets with highly non-trivial positional arrangements, such as datasets organized in multiple spatial dimensions, or datasets for which ground truth positions are not known. Here we find that the choice of initialization of a learnable PE greatly influences its ability to learn interpretable PEs that lead to enhanced generalization. We empirically demonstrate our findings in three experiments: 1) A 2D relational reasoning task; 2) A nonlinear stochastic network simulation; 3) A real world 3D neuroscience dataset, applying interpretability analyses to verify the learning of accurate PEs. Overall, we find that a learned PE initialized from a small-norm distribution can 1) uncover interpretable PEs that mirror ground truth positions in multiple dimensions, and 2) lead to improved generalization. These results illustrate the feasibility of learning identifiable and interpretable PEs for enhanced generalization. </p>
<blockquote>
<p>åœ¨è½¬æ¢å™¨ä¸­ï¼Œä½ç½®ç¼–ç ï¼ˆPEï¼‰æä¾›äº†åŒºåˆ†åºåˆ—ä¸­ä»¤ç‰Œä½ç½®å’Œé¡ºåºçš„é‡è¦ä¿¡æ¯ã€‚ä¹‹å‰å…³äºPEå¯¹æ³›åŒ–å½±å“çš„å¤§å¤šæ•°è°ƒæŸ¥éƒ½æ˜¯é’ˆå¯¹ä¸€ç»´è¾“å…¥åºåˆ—çš„ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€ä¸­å‘ˆç°çš„åºåˆ—ï¼Œå…¶ä¸­ç›¸é‚»çš„ä»¤ç‰Œï¼ˆä¾‹å¦‚å•è¯ï¼‰é«˜åº¦ç›¸å…³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè®¸å¤šç°å®ä¸–ç•Œçš„ä»»åŠ¡æ¶‰åŠå…·æœ‰é«˜åº¦éå¹³å‡¡ä½ç½®å®‰æ’çš„æ•°æ®é›†ï¼Œä¾‹å¦‚ä»¥å¤šä¸ªç©ºé—´ç»´åº¦ç»„ç»‡çš„æ•°æ®é›†ï¼Œæˆ–å¯¹äºå…¶çœŸå®ä½ç½®æœªçŸ¥çš„æ•°æ®é›†ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å‘ç°å¯å­¦ä¹ PEçš„åˆå§‹åŒ–é€‰æ‹©å¯¹å…¶å­¦ä¹ å¯è§£é‡Šçš„PEä»¥æ”¹å–„æ³›åŒ–çš„èƒ½åŠ›æœ‰å¾ˆå¤§å½±å“ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰é¡¹å®éªŒå®è¯è¯æ˜äº†æˆ‘ä»¬çš„å‘ç°ï¼š1ï¼‰äºŒç»´å…³ç³»æ¨ç†ä»»åŠ¡ï¼›2ï¼‰éçº¿æ€§éšæœºç½‘ç»œæ¨¡æ‹Ÿï¼›3ï¼‰ç°å®ä¸–ç•Œä¸­çš„ä¸‰ç»´ç¥ç»ç§‘å­¦æ•°æ®é›†ï¼Œé€šè¿‡å¯è§£é‡Šæ€§åˆ†ææ¥éªŒè¯å‡†ç¡®çš„PEå­¦ä¹ ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å‘ç°ä»èŒƒæ•°è¾ƒå°çš„åˆ†å¸ƒåˆå§‹åŒ–çš„å­¦ä¹ åˆ°çš„PEå¯ä»¥1ï¼‰æ­ç¤ºå¯è§£é‡Šçš„ä½ç½®ç¼–ç ï¼Œè¿™äº›ç¼–ç å¯ä»¥åæ˜ å¤šä¸ªç»´åº¦ä¸­çš„çœŸå®ä½ç½®ï¼Œå¹¶2ï¼‰å¯¼è‡´æ³›åŒ–æ€§èƒ½çš„æé«˜ã€‚è¿™äº›ç»“æœè¯´æ˜äº†å­¦ä¹ å¯è¯†åˆ«å’Œå¯è§£é‡Šçš„ä½ç½®ç¼–ç ä»¥æé«˜æ³›åŒ–çš„å¯è¡Œæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.08272v4">PDF</a> ICML 2025, Workshop on Actionable Interpretability</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥è®ºæ–‡ç ”ç©¶äº†è½¬æ¢å™¨çš„ä½ç½®ç¼–ç ï¼ˆPEï¼‰å¦‚ä½•å½±å“å¯¹å¤šç»´æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ã€‚è®ºæ–‡å‘ç°åˆå§‹åŒ–å­¦ä¹ ä½ç½®ç¼–ç çš„é€‰æ‹©å¯¹è®­ç»ƒå‡ºå¯è§£é‡Šçš„ä½ç½®ç¼–ç è‡³å…³é‡è¦ï¼Œè¿™ç§ä½ç½®ç¼–ç èƒ½å¤Ÿæå‡æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å®éªŒéªŒè¯ï¼Œåœ¨äºŒç»´å…³ç³»æ¨ç†ä»»åŠ¡ã€éçº¿æ€§éšæœºç½‘ç»œæ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œä¸‰ç»´ç¥ç»ç§‘å­¦æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨å°èŒƒæ•°åˆ†å¸ƒåˆå§‹åŒ–å­¦ä¹ ä½ç½®ç¼–ç èƒ½å¤Ÿæé«˜æ³›åŒ–æ€§èƒ½ã€‚è¿™è¡¨æ˜å­¦ä¹ å¯è¾¨è¯†å’Œå¯è§£é‡Šçš„ä½ç½®ç¼–ç å¯¹äºå¢å¼ºæ³›åŒ–èƒ½åŠ›æ˜¯å¯è¡Œçš„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä½ç½®ç¼–ç åœ¨åŒºåˆ†åºåˆ—ä¸­çš„ä»¤ç‰Œä½ç½®å’Œé¡ºåºæ–¹é¢èµ·ç€é‡è¦ä½œç”¨ã€‚</li>
<li>åˆå§‹åŒ–å­¦ä¹ ä½ç½®ç¼–ç çš„é€‰æ‹©å¯¹å…¶å­¦ä¹ å¯è§£é‡Šçš„ä½ç½®ç¼–ç å¹¶æå‡æ³›åŒ–èƒ½åŠ›æœ‰é‡è¦å½±å“ã€‚</li>
<li>ä½¿ç”¨å°èŒƒæ•°åˆ†å¸ƒåˆå§‹åŒ–å­¦ä¹ ä½ç½®ç¼–ç èƒ½å¤Ÿæ­ç¤ºå¯è§£é‡Šçš„ä½ç½®ç¼–ç ï¼Œè¯¥ç¼–ç èƒ½å¤Ÿåæ˜ å¤šç»´æ•°æ®é›†ä¸­çš„çœŸå®ä½ç½®ã€‚</li>
<li>åœ¨äºŒç»´å…³ç³»æ¨ç†ä»»åŠ¡ã€éçº¿æ€§éšæœºç½‘ç»œæ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œä¸‰ç»´ç¥ç»ç§‘å­¦æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨å­¦ä¹ ä½ç½®ç¼–ç èƒ½å¤Ÿæé«˜æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>å­¦ä¹ ä½ç½®ç¼–ç èƒ½å¤Ÿå¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚å¤šç»´æ•°æ®é›†æ—¶ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºè®¾è®¡æ›´æœ‰æ•ˆçš„ä½ç½®ç¼–ç æä¾›äº†æ–°è§†è§’ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†çœŸå®ä¸–ç•Œæ•°æ®é›†æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.08272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-45f477a1f5e1bdba5a76947fb011ea3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fb7c289b4653192cc00b432304ab223.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e25129db053887a4f55a7ed62224a6e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fcf93d6d778a115e8ac797e6eaf9d4cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f12a6d50601da6927e4b4f39bf1a8d8f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-27/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-27/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-27/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2a61de16f2ef80616c80ec4b7c1f2407.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-27  The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-27/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b5c727c232a9c7e176f040eee15e49e1.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-27  MMSearch-R1 Incentivizing LMMs to Search
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
