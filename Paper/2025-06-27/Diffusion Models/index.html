<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-27  Shape2Animal Creative Animal Generation from Natural Silhouettes">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-77d0cdd4f4d6244df74e879fff65c6d9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    49 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-27-æ›´æ–°"><a href="#2025-06-27-æ›´æ–°" class="headerlink" title="2025-06-27 æ›´æ–°"></a>2025-06-27 æ›´æ–°</h1><h2 id="Shape2Animal-Creative-Animal-Generation-from-Natural-Silhouettes"><a href="#Shape2Animal-Creative-Animal-Generation-from-Natural-Silhouettes" class="headerlink" title="Shape2Animal: Creative Animal Generation from Natural Silhouettes"></a>Shape2Animal: Creative Animal Generation from Natural Silhouettes</h2><p><strong>Authors:Quoc-Duy Tran, Anh-Tuan Vo, Dinh-Khoi Vo, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le</strong></p>
<p>Humans possess a unique ability to perceive meaningful patterns in ambiguous stimuli, a cognitive phenomenon known as pareidolia. This paper introduces Shape2Animal framework to mimics this imaginative capacity by reinterpreting natural object silhouettes, such as clouds, stones, or flames, as plausible animal forms. Our automated framework first performs open-vocabulary segmentation to extract object silhouette and interprets semantically appropriate animal concepts using vision-language models. It then synthesizes an animal image that conforms to the input shape, leveraging text-to-image diffusion model and seamlessly blends it into the original scene to generate visually coherent and spatially consistent compositions. We evaluated Shape2Animal on a diverse set of real-world inputs, demonstrating its robustness and creative potential. Our Shape2Animal can offer new opportunities for visual storytelling, educational content, digital art, and interactive media design. Our project page is here: <a target="_blank" rel="noopener" href="https://shape2image.github.io/">https://shape2image.github.io</a> </p>
<blockquote>
<p>äººç±»æ‹¥æœ‰åœ¨æ¨¡ç³Šåˆºæ¿€ä¸­æ„ŸçŸ¥æœ‰æ„ä¹‰æ¨¡å¼çš„èƒ½åŠ›ï¼Œè¿™æ˜¯ä¸€ç§ç§°ä¸ºâ€œå¸•é‡Œå¤šåˆ©ç°è±¡â€çš„è®¤çŸ¥ç°è±¡ã€‚æœ¬æ–‡ä»‹ç»äº†Shape2Animalæ¡†æ¶ï¼Œå®ƒé€šè¿‡é‡æ–°è§£é‡Šè‡ªç„¶ç‰©ä½“çš„è½®å»“ï¼ˆå¦‚äº‘å½©ã€çŸ³å¤´æˆ–ç«ç„°ï¼‰æ¥æ¨¡ä»¿è¿™ç§æƒ³è±¡åŠ›èƒ½åŠ›ï¼Œå°†è¿™äº›è½®å»“è½¬åŒ–ä¸ºåˆç†çš„åŠ¨ç‰©å½¢æ€ã€‚æˆ‘ä»¬çš„è‡ªåŠ¨åŒ–æ¡†æ¶é¦–å…ˆæ‰§è¡Œå¼€æ”¾è¯æ±‡åˆ†å‰²æŠ€æœ¯ï¼Œä»¥æå–ç‰©ä½“è½®å»“å¹¶ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è§£é‡Šè¯­ä¹‰ä¸Šé€‚å½“çš„åŠ¨ç‰©æ¦‚å¿µã€‚ç„¶åï¼Œå®ƒåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åˆæˆç¬¦åˆè¾“å…¥å½¢çŠ¶çš„åŠ¨ç‰©å›¾åƒï¼Œå¹¶å°†å…¶æ— ç¼èåˆåˆ°åŸå§‹åœºæ™¯ä¸­ï¼Œç”Ÿæˆè§†è§‰è¿è´¯ä¸”ç©ºé—´ä¸€è‡´çš„æ„å›¾ã€‚æˆ‘ä»¬åœ¨å„ç§çœŸå®ä¸–ç•Œçš„è¾“å…¥ä¸Šå¯¹Shape2Animalè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶ç¨³å¥æ€§å’Œåˆ›é€ åŠ›æ½œåŠ›ã€‚Shape2Animalä¸ºè§†è§‰æ•…äº‹å™è¿°ã€æ•™è‚²å†…å®¹ã€æ•°å­—è‰ºæœ¯å’Œäº¤äº’å¼åª’ä½“è®¾è®¡æä¾›äº†æ–°çš„æœºä¼šã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ä½äºï¼š<a target="_blank" rel="noopener" href="https://shape2image.github.io/">https://shape2image.github.io</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20616v1">PDF</a> </p>
<p><strong>Summary</strong><br>     äººç±»æ‹¥æœ‰åœ¨æ¨¡ç³Šåˆºæ¿€ä¸­æ„ŸçŸ¥æœ‰æ„ä¹‰æ¨¡å¼çš„èƒ½åŠ›ï¼Œç§°ä¸ºå¸•é‡Œå¤šåˆ©ç°è±¡ã€‚æœ¬æ–‡ä»‹ç»çš„Shape2Animalæ¡†æ¶æ¨¡æ‹Ÿäº†è¿™ä¸€æƒ³è±¡åŠ›ï¼Œé‡æ–°è§£é‡Šè‡ªç„¶ç‰©ä½“çš„è½®å»“ï¼Œå¦‚äº‘å½©ã€çŸ³å¤´æˆ–ç«ç„°ï¼Œä½œä¸ºåˆç†çš„åŠ¨ç‰©å½¢æ€ã€‚è¯¥è‡ªåŠ¨åŒ–æ¡†æ¶é¦–å…ˆè¿›è¡Œå¼€æ”¾è¯æ±‡åˆ†å‰²ä»¥æå–ç‰©ä½“è½®å»“ï¼Œå¹¶ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è§£é‡Šè¯­ä¹‰ä¸Šé€‚å½“çš„åŠ¨ç‰©æ¦‚å¿µã€‚ç„¶åï¼Œå®ƒåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åˆæˆç¬¦åˆè¾“å…¥å½¢çŠ¶çš„åŠ¨ç‰©å›¾åƒï¼Œå¹¶å°†å…¶æ— ç¼èåˆåˆ°åŸå§‹åœºæ™¯ä¸­ï¼Œä»¥äº§ç”Ÿè§†è§‰è¿è´¯ä¸”ç©ºé—´ä¸€è‡´çš„æ„å›¾ã€‚åœ¨å¤šç§çœŸå®ä¸–ç•Œè¾“å…¥ä¸Šè¯„ä¼°äº†Shape2Animalçš„ç¨³å¥æ€§å’Œåˆ›é€ æ€§æ½œåŠ›ã€‚Shape2Animalå¯ä¸ºè§†è§‰å™äº‹ã€æ•™è‚²å†…å®¹ã€æ•°å­—è‰ºæœ¯å’Œäº¤äº’å¼åª’ä½“è®¾è®¡æä¾›æ–°æœºä¼šã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ä½äºï¼š<a target="_blank" rel="noopener" href="https://shape2image.github.io/">https://shape2image.github.io</a>ã€‚ç®€è€Œè¨€ä¹‹ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥Shape2Animalæ¡†æ¶å±•ç¤ºäººé€ æ™ºèƒ½å¯¹äººç±»è§†è§‰åˆ›æ„çš„æ¨¡ä»¿èƒ½åŠ›çš„ç ”ç©¶ã€‚æ¡†æ¶å¯ä»¥å°†è‡ªç„¶ç‰©ä½“çš„è½®å»“è½¬åŒ–ä¸ºåŠ¨ç‰©å½¢æ€ï¼Œå…·æœ‰å¼ºå¤§çš„ç¨³å¥æ€§å’Œåˆ›é€ æ€§æ½œåŠ›ã€‚è¯¥æŠ€æœ¯åœ¨è§†è§‰å™äº‹ã€æ•™è‚²ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Shape2Animalæ¡†æ¶æ¨¡æ‹Ÿäººç±»çš„å¸•é‡Œå¤šåˆ©ç°è±¡èƒ½åŠ›ï¼Œå³æŠŠè‡ªç„¶ç‰©ä½“çš„è½®å»“è§£é‡Šä¸ºåŠ¨ç‰©å½¢æ€çš„èƒ½åŠ›ã€‚</li>
<li>æ¡†æ¶åŒ…å«å¼€æ”¾è¯æ±‡åˆ†å‰²æŠ€æœ¯ä»¥æå–ç‰©ä½“è½®å»“å’Œè§†è§‰è¯­è¨€æ¨¡å‹è§£é‡ŠåŠ¨ç‰©æ¦‚å¿µã€‚</li>
<li>åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åˆæˆç¬¦åˆè¾“å…¥å½¢çŠ¶çš„åŠ¨ç‰©å›¾åƒï¼Œå¹¶ä¸åŸå§‹åœºæ™¯æ— ç¼èåˆã€‚</li>
<li>Shape2Animalåœ¨å¤šç§çœŸå®ä¸–ç•Œè¾“å…¥ä¸Šè¡¨ç°å‡ºç¨³å¥æ€§å’Œåˆ›é€ æ€§æ½œåŠ›ã€‚</li>
<li>Shape2Animalæ¡†æ¶åœ¨è§†è§‰å™äº‹ã€æ•™è‚²å†…å®¹ã€æ•°å­—è‰ºæœ¯å’Œäº¤äº’å¼åª’ä½“è®¾è®¡ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚</li>
<li>é¡¹ç›®é¡µé¢æä¾›äº†å…³äºShape2Animalæ¡†æ¶çš„è¯¦ç»†ä¿¡æ¯å’ŒæŠ€æœ¯å®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20616">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a6f90dca69a3d6453e1a7e916a71b48c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f657c6ffd29fbde129a74c78f6743250.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad86e1efaf7809a6709a846327e9bafa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e4f13dd8f7e6bd3028b97b6039824d4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-066c922674f048c9cd476d6246b23766.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Pay-Less-Attention-to-Deceptive-Artifacts-Robust-Detection-of-Compressed-Deepfakes-on-Online-Social-Networks"><a href="#Pay-Less-Attention-to-Deceptive-Artifacts-Robust-Detection-of-Compressed-Deepfakes-on-Online-Social-Networks" class="headerlink" title="Pay Less Attention to Deceptive Artifacts: Robust Detection of   Compressed Deepfakes on Online Social Networks"></a>Pay Less Attention to Deceptive Artifacts: Robust Detection of   Compressed Deepfakes on Online Social Networks</h2><p><strong>Authors:Manyi Li, Renshuai Tao, Yufan Liu, Chuangchuang Tan, Haotong Qin, Bing Li, Yunchao Wei, Yao Zhao</strong></p>
<p>With the rapid advancement of deep learning, particularly through generative adversarial networks (GANs) and diffusion models (DMs), AI-generated images, or <code>deepfakes&quot;, have become nearly indistinguishable from real ones. These images are widely shared across Online Social Networks (OSNs), raising concerns about their misuse. Existing deepfake detection methods overlook the </code>block effectsâ€ introduced by compression in OSNs, which obscure deepfake artifacts, and primarily focus on raw images, rarely encountered in real-world scenarios. To address these challenges, we propose PLADA (Pay Less Attention to Deceptive Artifacts), a novel framework designed to tackle the lack of paired data and the ineffective use of compressed images. PLADA consists of two core modules: Block Effect Eraser (B2E), which uses a dual-stage attention mechanism to handle block effects, and Open Data Aggregation (ODA), which processes both paired and unpaired data to improve detection. Extensive experiments across 26 datasets demonstrate that PLADA achieves a remarkable balance in deepfake detection, outperforming SoTA methods in detecting deepfakes on OSNs, even with limited paired data and compression. More importantly, this work introduces the &#96;&#96;block effectâ€ as a critical factor in deepfake detection, providing a robust solution for open-world scenarios. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ManyiLee/PLADA">https://github.com/ManyiLee/PLADA</a>. </p>
<blockquote>
<p>éšç€æ·±åº¦å­¦ä¹ çš„å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ï¼ŒAIç”Ÿæˆçš„å›¾åƒï¼Œæˆ–ç§°ä¸ºâ€œæ·±åº¦ä¼ªé€ â€ï¼Œå·²ç»å˜å¾—å‡ ä¹ä¸çœŸå®çš„å›¾åƒæ— æ³•åŒºåˆ†ã€‚è¿™äº›å›¾åƒåœ¨åœ¨çº¿ç¤¾äº¤ç½‘ç»œï¼ˆOSNsï¼‰ä¸Šè¢«å¹¿æ³›å…±äº«ï¼Œå¼•å‘äº†å…³äºå…¶è¯¯ç”¨çš„æ‹…å¿§ã€‚ç°æœ‰çš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•å¿½ç•¥äº†OSNä¸­å‹ç¼©å¼•å…¥çš„â€œå—æ•ˆåº”â€ï¼Œè¿™æ©ç›–äº†æ·±åº¦ä¼ªé€ çš„ç—•è¿¹ï¼Œå¹¶ä¸”ä¸»è¦å…³æ³¨åŸå§‹å›¾åƒï¼Œåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­å¾ˆå°‘é‡åˆ°ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PLADAï¼ˆå°‘å…³æ³¨æ¬ºéª—æ€§äººå·¥åˆ¶å“ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é…å¯¹æ•°æ®çš„ç¼ºä¹ä»¥åŠå‹ç¼©å›¾åƒçš„æœ‰æ•ˆä½¿ç”¨é—®é¢˜ã€‚PLADAç”±ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ç»„æˆï¼šå—æ•ˆåº”æ¶ˆé™¤å™¨ï¼ˆB2Eï¼‰ï¼Œå®ƒä½¿ç”¨ä¸¤é˜¶æ®µæ³¨æ„åŠ›æœºåˆ¶æ¥å¤„ç†å—æ•ˆåº”ï¼›å¼€æ”¾æ•°æ®èšåˆï¼ˆODAï¼‰ï¼Œå®ƒå¤„ç†é…å¯¹å’Œéé…å¯¹æ•°æ®ä»¥æé«˜æ£€æµ‹èƒ½åŠ›ã€‚åœ¨26ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPLADAåœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹é¢å–å¾—äº†ä»¤äººç©ç›®çš„å¹³è¡¡ï¼Œå³ä½¿åœ¨æœ‰é™çš„é…å¯¹æ•°æ®å’Œå‹ç¼©æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½åœ¨OSNä¸Šæ£€æµ‹æ·±åº¦ä¼ªé€ æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¿™é¡¹å·¥ä½œå°†â€œå—æ•ˆåº”â€ä½œä¸ºæ·±åº¦ä¼ªé€ æ£€æµ‹çš„å…³é”®å› ç´ ï¼Œä¸ºå¼€æ”¾ä¸–ç•Œåœºæ™¯æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ManyiLee/PLADA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ManyiLee/PLADAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20548v1">PDF</a> 20 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>éšç€æ·±åº¦å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼ŒAIç”Ÿæˆçš„å›¾åƒï¼Œå³æ‰€è°“çš„â€œæ·±åº¦ä¼ªé€ â€ï¼ˆdeepfakesï¼‰å›¾åƒï¼Œå·²ç»å˜å¾—ä¸çœŸå®å›¾åƒéš¾ä»¥åŒºåˆ†ã€‚è¿™äº›å›¾åƒåœ¨åœ¨çº¿ç¤¾äº¤ç½‘ç»œï¼ˆOSNsï¼‰ä¸Šè¢«å¹¿æ³›å…±äº«ï¼Œå¼•å‘äº†å…³äºå…¶è¯¯ç”¨çš„æ‹…å¿§ã€‚ç°æœ‰çš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•å¿½è§†äº†OSNä¸­çš„å‹ç¼©å¼•å…¥çš„â€œå—æ•ˆåº”â€ï¼Œè¯¥æ•ˆåº”æ©ç›–äº†æ·±åº¦ä¼ªé€ çš„ç—•è¿¹ï¼Œå¹¶ä¸”ä¸»è¦å…³æ³¨åŸå§‹å›¾åƒï¼Œè¿™åœ¨ç°å®åœºæ™¯ä¸­å¾ˆå°‘è§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PLADAï¼ˆå°‘å…³æ³¨æ¬ºéª—æ€§äººå·¥åˆ¶å“ï¼‰ï¼Œä¸€ä¸ªæ–°é¢–çš„è®¾è®¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é…å¯¹æ•°æ®ç¼ºä¹å’Œå‹ç¼©å›¾åƒä½¿ç”¨æ— æ•ˆçš„é—®é¢˜ã€‚PLADAç”±ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ç»„æˆï¼šå—æ•ˆåº”æ¶ˆé™¤å™¨ï¼ˆB2Eï¼‰ï¼Œä½¿ç”¨ä¸¤é˜¶æ®µæ³¨æ„åŠ›æœºåˆ¶å¤„ç†å—æ•ˆåº”ï¼›å¼€æ”¾æ•°æ®èšåˆï¼ˆODAï¼‰ï¼Œå¤„ç†é…å¯¹å’Œéé…å¯¹æ•°æ®ä»¥æé«˜æ£€æµ‹èƒ½åŠ›ã€‚åœ¨26ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPLADAåœ¨OSNä¸Šçš„æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­å®ç°äº†å‡ºè‰²çš„å¹³è¡¡ï¼Œå³ä½¿åœ¨é…å¯¹æ•°æ®æœ‰é™å’Œå‹ç¼©çš„æƒ…å†µä¸‹ä¹Ÿä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¿™é¡¹å·¥ä½œå°†â€œå—æ•ˆåº”â€ä½œä¸ºæ·±åº¦ä¼ªé€ æ£€æµ‹çš„å…³é”®å› ç´ ï¼Œä¸ºå¼€æ”¾ä¸–ç•Œåœºæ™¯æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”Ÿæˆçš„å›¾åƒï¼ˆæ·±åº¦ä¼ªé€ ï¼‰ä¸çœŸå®å›¾åƒéš¾ä»¥åŒºåˆ†ï¼Œå·²åœ¨åœ¨çº¿ç¤¾äº¤ç½‘ç»œï¼ˆOSNï¼‰ä¸Šå¹¿æ³›å…±äº«ï¼Œå¼•å‘è¯¯ç”¨æ‹…å¿§ã€‚</li>
<li>ç°æœ‰æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•ä¸»è¦å…³æ³¨åŸå§‹å›¾åƒï¼Œå¿½è§†OSNä¸­çš„å›¾åƒå‹ç¼©æ‰€å¼•å…¥çš„â€œå—æ•ˆåº”â€ã€‚</li>
<li>PLADAæ¡†æ¶æå‡ºè§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼ŒåŒ…å«å¤„ç†å—æ•ˆåº”çš„æ ¸å¿ƒæ¨¡å—B2Eå’Œå¤„ç†é…å¯¹åŠéé…å¯¹æ•°æ®çš„ODAã€‚</li>
<li>PLADAé€šè¿‡å¹¿æ³›å®éªŒè¯æ˜å…¶åœ¨OSNä¸Šçš„æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„å‡ºè‰²æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨é…å¯¹æ•°æ®æœ‰é™å’Œå›¾åƒå‹ç¼©çš„æƒ…å†µä¸‹ã€‚</li>
<li>PLADAå¼•å…¥â€œå—æ•ˆåº”â€ä½œä¸ºæ·±åº¦ä¼ªé€ æ£€æµ‹çš„å…³é”®å› ç´ ï¼Œä¸ºç°å®åœºæ™¯æä¾›ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</li>
<li>PLADAçš„ä»£ç å·²å…¬å¼€å¯ä¾›ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b5e547b14770657a032485602254ec79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a979624bbe1b401e59a30688a3e4d52a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7a659a6918a26aca9a92ac7a34f90e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb1beab7c5c59d21ab2b96ef025e3f74.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HiWave-Training-Free-High-Resolution-Image-Generation-via-Wavelet-Based-Diffusion-Sampling"><a href="#HiWave-Training-Free-High-Resolution-Image-Generation-via-Wavelet-Based-Diffusion-Sampling" class="headerlink" title="HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based   Diffusion Sampling"></a>HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based   Diffusion Sampling</h2><p><strong>Authors:Tobias Vontobel, Seyedmorteza Sadat, Farnood Salehi, Romann M. Weber</strong></p>
<p>Diffusion models have emerged as the leading approach for image synthesis, demonstrating exceptional photorealism and diversity. However, training diffusion models at high resolutions remains computationally prohibitive, and existing zero-shot generation techniques for synthesizing images beyond training resolutions often produce artifacts, including object duplication and spatial incoherence. In this paper, we introduce HiWave, a training-free, zero-shot approach that substantially enhances visual fidelity and structural coherence in ultra-high-resolution image synthesis using pretrained diffusion models. Our method employs a two-stage pipeline: generating a base image from the pretrained model followed by a patch-wise DDIM inversion step and a novel wavelet-based detail enhancer module. Specifically, we first utilize inversion methods to derive initial noise vectors that preserve global coherence from the base image. Subsequently, during sampling, our wavelet-domain detail enhancer retains low-frequency components from the base image to ensure structural consistency, while selectively guiding high-frequency components to enrich fine details and textures. Extensive evaluations using Stable Diffusion XL demonstrate that HiWave effectively mitigates common visual artifacts seen in prior methods, achieving superior perceptual quality. A user study confirmed HiWaveâ€™s performance, where it was preferred over the state-of-the-art alternative in more than 80% of comparisons, highlighting its effectiveness for high-quality, ultra-high-resolution image synthesis without requiring retraining or architectural modifications. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»æˆä¸ºå›¾åƒåˆæˆçš„é¢†å…ˆæ–¹æ³•ï¼Œè¡¨ç°å‡ºäº†éå‡¡çš„é€¼çœŸåº¦å’Œå¤šæ ·æ€§ã€‚ç„¶è€Œï¼Œåœ¨é«˜åˆ†è¾¨ç‡ä¸‹è®­ç»ƒæ‰©æ•£æ¨¡å‹ä»ç„¶è®¡ç®—ä¸Šæ˜¯ä¸ç°å®çš„ï¼Œå¹¶ä¸”ç°æœ‰çš„é›¶å°„å‡»ç”ŸæˆæŠ€æœ¯å¯¹äºåˆæˆè¶…å‡ºè®­ç»ƒåˆ†è¾¨ç‡çš„å›¾åƒé€šå¸¸ä¼šäº§ç”Ÿä¼ªå½±ï¼ŒåŒ…æ‹¬å¯¹è±¡é‡å¤å’Œç©ºé—´ä¸ä¸€è‡´æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†HiWaveï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒã€é›¶å°„å‡»çš„æ–¹æ³•ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œè¶…é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆï¼Œæå¤§åœ°æé«˜äº†è§†è§‰ä¿çœŸåº¦å’Œç»“æ„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“ï¼šé¦–å…ˆä»é¢„è®­ç»ƒæ¨¡å‹ç”ŸæˆåŸºç¡€å›¾åƒï¼Œç„¶åè¿›è¡Œåˆ†å—DDIMåè½¬å’ŒåŸºäºæ–°å‹å°æ³¢çš„ç»†èŠ‚å¢å¼ºæ¨¡å—ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨åè½¬æ–¹æ³•æ¨å¯¼å‡ºåˆå§‹å™ªå£°å‘é‡ï¼Œä»åŸºç¡€å›¾åƒä¸­ä¿ç•™å…¨å±€ä¸€è‡´æ€§ã€‚éšåï¼Œåœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„å°æ³¢åŸŸç»†èŠ‚å¢å¼ºå™¨ä¿ç•™åŸºç¡€å›¾åƒçš„ä½é¢‘æˆåˆ†ä»¥ç¡®ä¿ç»“æ„ä¸€è‡´æ€§ï¼ŒåŒæ—¶æœ‰é€‰æ‹©åœ°å¼•å¯¼é«˜é¢‘æˆåˆ†ä»¥ä¸°å¯Œç»†èŠ‚å’Œçº¹ç†ã€‚ä½¿ç”¨Stable Diffusion XLçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒHiWaveæœ‰æ•ˆå‡è½»äº†å…ˆå‰æ–¹æ³•ä¸­å¸¸è§çš„è§†è§‰ä¼ªå½±ï¼Œå®ç°äº†æ›´é«˜çš„æ„ŸçŸ¥è´¨é‡ã€‚ç”¨æˆ·ç ”ç©¶è¯å®äº†HiWaveçš„æ€§èƒ½ï¼Œåœ¨è¶…è¿‡80%çš„æ¯”è¾ƒä¸­ï¼Œå®ƒæ¯”æœ€å…ˆè¿›çš„æ›¿ä»£å“æ›´å—æ¬¢è¿ï¼Œè¿™çªå‡ºäº†å…¶åœ¨æ— éœ€é‡æ–°è®­ç»ƒæˆ–ä¿®æ”¹æ¶æ„çš„æƒ…å†µä¸‹è¿›è¡Œé«˜è´¨é‡ã€è¶…é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20452v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†HiWaveï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒã€é›¶å°„å‡»çš„æ–¹æ³•ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œè¶…é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆï¼Œæ˜¾è‘—æé«˜äº†è§†è§‰ä¿çœŸåº¦å’Œç»“æ„è¿è´¯æ€§ã€‚HiWaveé‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“ï¼Œé¦–å…ˆç”ŸæˆåŸºç¡€å›¾åƒï¼Œç„¶åè¿›è¡Œæ–‘ç‰‡çŠ¶DDIMåè½¬å’ŒåŸºäºå°æ³¢çš„ç»†èŠ‚å¢å¼ºæ¨¡å—ã€‚å®ƒé€šè¿‡ä¿ç•™åŸºç¡€å›¾åƒçš„ä½é¢‘æˆåˆ†ç¡®ä¿ç»“æ„ä¸€è‡´æ€§ï¼ŒåŒæ—¶é€‰æ‹©æ€§å¼•å¯¼é«˜é¢‘æˆåˆ†ä¸°å¯Œç»†èŠ‚å’Œçº¹ç†ã€‚ä½¿ç”¨Stable Diffusion XLçš„å¹¿æ³›è¯„ä¼°è¯æ˜ï¼ŒHiWaveæœ‰æ•ˆå‡è½»äº†å…ˆå‰æ–¹æ³•ä¸­å¸¸è§çš„è§†è§‰ä¼ªå½±ï¼Œå®ç°äº†ä¼˜è¶Šçš„æ„ŸçŸ¥è´¨é‡ã€‚ç”¨æˆ·ç ”ç©¶è¯å®ï¼ŒHiWaveåœ¨è¶…è¿‡80%çš„æ¯”è¾ƒä¸­ä¼˜äºæœ€æ–°æŠ€æœ¯æ›¿ä»£æ–¹æ¡ˆï¼Œçªæ˜¾å…¶åœ¨æ— éœ€é‡æ–°è®­ç»ƒæˆ–ä¿®æ”¹æ¶æ„çš„æƒ…å†µä¸‹å®ç°é«˜è´¨é‡ã€è¶…é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºå›¾åƒåˆæˆçš„é¢†å…ˆæ–¹æ³•ï¼Œå±•ç°å‡ºå“è¶Šçš„é€¼çœŸåº¦å’Œå¤šæ ·æ€§ã€‚</li>
<li>è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆè®¡ç®—é‡å¤§ï¼Œç°æœ‰é›¶å°„å‡»ç”ŸæˆæŠ€æœ¯åœ¨åˆæˆè¶…è¿‡è®­ç»ƒåˆ†è¾¨ç‡çš„å›¾åƒæ—¶ä¼šäº§ç”Ÿä¼ªå½±ã€‚</li>
<li>HiWaveæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„é›¶å°„å‡»æ–¹æ³•ï¼Œç”¨äºè¶…é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆï¼Œæé«˜äº†è§†è§‰ä¿çœŸåº¦å’Œç»“æ„è¿è´¯æ€§ã€‚</li>
<li>HiWaveé‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“ï¼šç”ŸæˆåŸºç¡€å›¾åƒï¼Œç„¶åè¿›è¡Œæ–‘ç‰‡çŠ¶DDIMåè½¬å’ŒåŸºäºå°æ³¢çš„ç»†èŠ‚å¢å¼ºã€‚</li>
<li>HiWaveé€šè¿‡ä¿ç•™åŸºç¡€å›¾åƒçš„ä½é¢‘æˆåˆ†æ¥ç¡®ä¿ç»“æ„ä¸€è‡´æ€§ï¼ŒåŒæ—¶å¼•å¯¼é«˜é¢‘æˆåˆ†ä¸°å¯Œç»†èŠ‚å’Œçº¹ç†ã€‚</li>
<li>ä½¿ç”¨Stable Diffusion XLçš„å¹¿æ³›è¯„ä¼°è¯æ˜HiWaveæ–¹æ³•æœ‰æ•ˆå‡è½»ä¼ªå½±ï¼Œå®ç°ä¼˜è¶Šæ„ŸçŸ¥è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20452">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e1ea70ffa3e90c8827fdd568b20df59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79e0568f5aed82b65b8aec99ae412c7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b97ea2a1879bb28c9d6b0c74c77729f7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Recognizing-Surgical-Phases-Anywhere-Few-Shot-Test-time-Adaptation-and-Task-graph-Guided-Refinement"><a href="#Recognizing-Surgical-Phases-Anywhere-Few-Shot-Test-time-Adaptation-and-Task-graph-Guided-Refinement" class="headerlink" title="Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and   Task-graph Guided Refinement"></a>Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and   Task-graph Guided Refinement</h2><p><strong>Authors:Kun Yuan, Tingxuan Chen, Shi Li, Joel L. Lavanchy, Christian Heiliger, Ege Ã–zsoy, Yiming Huang, Long Bai, Nassir Navab, Vinkle Srivastav, Hongliang Ren, Nicolas Padoy</strong></p>
<p>The complexity and diversity of surgical workflows, driven by heterogeneous operating room settings, institutional protocols, and anatomical variability, present a significant challenge in developing generalizable models for cross-institutional and cross-procedural surgical understanding. While recent surgical foundation models pretrained on large-scale vision-language data offer promising transferability, their zero-shot performance remains constrained by domain shifts, limiting their utility in unseen surgical environments. To address this, we introduce Surgical Phase Anywhere (SPA), a lightweight framework for versatile surgical workflow understanding that adapts foundation models to institutional settings with minimal annotation. SPA leverages few-shot spatial adaptation to align multi-modal embeddings with institution-specific surgical scenes and phases. It also ensures temporal consistency through diffusion modeling, which encodes task-graph priors derived from institutional procedure protocols. Finally, SPA employs dynamic test-time adaptation, exploiting the mutual agreement between multi-modal phase prediction streams to adapt the model to a given test video in a self-supervised manner, enhancing the reliability under test-time distribution shifts. SPA is a lightweight adaptation framework, allowing hospitals to rapidly customize phase recognition models by defining phases in natural language text, annotating a few images with the phase labels, and providing a task graph defining phase transitions. The experimental results show that the SPA framework achieves state-of-the-art performance in few-shot surgical phase recognition across multiple institutions and procedures, even outperforming full-shot models with 32-shot labeled data. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SPA">https://github.com/CAMMA-public/SPA</a> </p>
<blockquote>
<p>æ‰‹æœ¯å·¥ä½œæµçš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼Œå—åˆ°æ‰‹æœ¯å®¤è®¾ç½®ã€æœºæ„åè®®å’Œè§£å‰–ç»“æ„å·®å¼‚çš„å½±å“ï¼Œä¸ºå¼€å‘ç”¨äºè·¨æœºæ„å’Œè·¨ç¨‹åºæ‰‹æœ¯ç†è§£çš„é€šç”¨æ¨¡å‹å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶æœ€è¿‘åŸºäºå¤§è§„æ¨¡è§†è§‰è¯­è¨€æ•°æ®çš„é¢„è®­ç»ƒæ‰‹æœ¯åŸºç¡€æ¨¡å‹æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„è¿ç§»æ€§ï¼Œä½†å®ƒä»¬çš„é›¶æ ·æœ¬æ€§èƒ½ä»ç„¶å—åˆ°é¢†åŸŸåç§»çš„é™åˆ¶ï¼Œåœ¨æœªè§è¿‡çš„æ‰‹æœ¯ç¯å¢ƒä¸­æ•ˆç”¨æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ‰‹æœ¯é˜¶æ®µæ— å¤„ä¸åœ¨ï¼ˆSPAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£æ¡†æ¶ï¼Œèƒ½å¤Ÿé€‚åº”æœºæ„è®¾ç½®ï¼Œå¹¶ä¸”éœ€è¦çš„æ ‡æ³¨å¾ˆå°‘ã€‚SPAåˆ©ç”¨å°‘æ•°ç©ºé—´é€‚åº”æŠ€æœ¯ï¼Œä½¿å¤šæ¨¡å¼åµŒå…¥ä¸æœºæ„ç‰¹å®šçš„æ‰‹æœ¯åœºæ™¯å’Œé˜¶æ®µä¿æŒä¸€è‡´ã€‚å®ƒè¿˜é€šè¿‡æ‰©æ•£æ¨¡å‹ä¿è¯æ—¶é—´ä¸€è‡´æ€§ï¼Œè¯¥æ¨¡å‹ç¼–ç æ¥è‡ªæœºæ„ç¨‹åºåè®®çš„ä»»åŠ¡å›¾å…ˆéªŒã€‚æœ€åï¼ŒSPAé‡‡ç”¨åŠ¨æ€æµ‹è¯•æ—¶é—´é€‚åº”ï¼Œåˆ©ç”¨å¤šæ¨¡å¼é˜¶æ®µé¢„æµ‹æµä¹‹é—´çš„ç›¸äº’ä¸€è‡´æ€§ï¼Œä»¥è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼é€‚åº”ç»™å®šçš„æµ‹è¯•è§†é¢‘ï¼Œå¢å¼ºæµ‹è¯•æ—¶åˆ†å¸ƒå˜åŒ–çš„å¯é æ€§ã€‚SPAæ˜¯ä¸€ä¸ªè½»é‡çº§çš„é€‚åº”æ¡†æ¶ï¼Œå…è®¸åŒ»é™¢é€šè¿‡ç”¨è‡ªç„¶è¯­è¨€æ–‡æœ¬å®šä¹‰é˜¶æ®µã€å¯¹å°‘æ•°å›¾åƒè¿›è¡Œé˜¶æ®µæ ‡ç­¾æ³¨é‡Šä»¥åŠæä¾›å®šä¹‰é˜¶æ®µè½¬æ¢çš„ä»»åŠ¡å›¾æ¥å¿«é€Ÿå®šåˆ¶é˜¶æ®µè¯†åˆ«æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPAæ¡†æ¶åœ¨å¤šæœºæ„å’Œç¨‹åºä¸­çš„å°‘æ•°æ‰‹æœ¯é˜¶æ®µè¯†åˆ«ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œç”šè‡³åœ¨32æ¬¡æ ‡æ³¨æ•°æ®çš„å…¨é•œå¤´æ¨¡å‹ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SPA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CAMMA-public/SPAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20254v1">PDF</a> Accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ‰‹æœ¯æµç¨‹ç†è§£çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è½»é‡çº§æ¡†æ¶â€”â€”Surgical Phase Anywhere (SPA)ã€‚SPAåˆ©ç”¨å¤§å‹è·¨è§†å›¾è¯­è¨€æ•°æ®çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œé€šè¿‡å°‘æ ·æœ¬ç©ºé—´é€‚åº”ã€æ‰©æ•£å»ºæ¨¡å’ŒåŠ¨æ€æµ‹è¯•æ—¶é—´é€‚åº”ç­‰æŠ€æœ¯ï¼Œå®ç°äº†å¯¹ç‰¹å®šæœºæ„æ‰‹æœ¯ç¯å¢ƒçš„å¿«é€Ÿè‡ªå®šä¹‰æ¨¡å‹é€‚åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPAæ¡†æ¶åœ¨å¤šä¸ªæœºæ„å’Œæ‰‹æœ¯è¿‡ç¨‹ä¸­çš„å°‘æ ·æœ¬æ‰‹æœ¯é˜¶æ®µè¯†åˆ«ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œç”šè‡³åœ¨æ ‡æ³¨æ•°æ®åªæœ‰å…¨é‡çš„ä¸‰åäºŒåˆ†ä¹‹ä¸€çš„æƒ…å†µä¸‹ä¹Ÿè¡¨ç°ä¼˜ç§€ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰‹æœ¯æµç¨‹çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ç»™è·¨æœºæ„å’Œè·¨ç¨‹åºçš„æ‰‹æœ¯ç†è§£æ¨¡å‹å¼€å‘å¸¦æ¥äº†æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹è™½ç„¶å…·æœ‰è‰¯å¥½çš„è¿ç§»æ€§ï¼Œä½†åœ¨æœªè§è¿‡çš„æ‰‹æœ¯ç¯å¢ƒä¸­çš„é›¶æ ·æœ¬æ€§èƒ½ä»ç„¶å—é™ã€‚</li>
<li>SPAæ¡†æ¶é€šè¿‡å°‘æ ·æœ¬ç©ºé—´é€‚åº”æŠ€æœ¯ï¼Œä½¿å¤šæ¨¡æ€åµŒå…¥ä¸ç‰¹å®šæœºæ„çš„æ‰‹æœ¯åœºæ™¯å’Œé˜¶æ®µå¯¹é½ã€‚</li>
<li>æ‰©æ•£å»ºæ¨¡æŠ€æœ¯ç¡®ä¿äº†SPAçš„æ—¶é—´ä¸€è‡´æ€§ï¼Œå¹¶ç¼–ç äº†æ¥è‡ªæœºæ„ç¨‹åºåè®®çš„ä»»åŠ¡å›¾å…ˆéªŒã€‚</li>
<li>SPAé‡‡ç”¨åŠ¨æ€æµ‹è¯•æ—¶é—´é€‚åº”ï¼Œé€šè¿‡å¤šæ¨¡æ€é˜¶æ®µé¢„æµ‹æµçš„ç›¸äº’ä¸€è‡´æ€§ï¼Œä»¥è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼é€‚åº”ç»™å®šçš„æµ‹è¯•è§†é¢‘ï¼Œæé«˜äº†æµ‹è¯•æ—¶åˆ†å¸ƒå˜åŒ–çš„å¯é æ€§ã€‚</li>
<li>SPAæ¡†æ¶å…è®¸åŒ»é™¢é€šè¿‡è‡ªç„¶è¯­è¨€æ–‡æœ¬å®šä¹‰é˜¶æ®µã€å¯¹å°‘é‡å›¾åƒè¿›è¡Œé˜¶æ®µæ ‡ç­¾æ³¨é‡Šä»¥åŠæä¾›å®šä¹‰é˜¶æ®µè½¬æ¢çš„ä»»åŠ¡å›¾ï¼Œæ¥å¿«é€Ÿå®šåˆ¶é˜¶æ®µè¯†åˆ«æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-35e04133fadb4575800e13da8c9669e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5427f48e938568f12f1f0ca2cbb706aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c550a003ea26a00a27a124e341e5fa00.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Towards-Efficient-Exemplar-Based-Image-Editing-with-Multimodal-VLMs"><a href="#Towards-Efficient-Exemplar-Based-Image-Editing-with-Multimodal-VLMs" class="headerlink" title="Towards Efficient Exemplar Based Image Editing with Multimodal VLMs"></a>Towards Efficient Exemplar Based Image Editing with Multimodal VLMs</h2><p><strong>Authors:Avadhoot Jadhav, Ashutosh Srivastava, Abhinav Java, Silky Singh, Tarun Ram Menta, Surgan Jandial, Balaji Krishnamurthy</strong></p>
<p>Text-to-Image Diffusion models have enabled a wide array of image editing applications. However, capturing all types of edits through text alone can be challenging and cumbersome. The ambiguous nature of certain image edits is better expressed through an exemplar pair, i.e., a pair of images depicting an image before and after an edit respectively. In this work, we tackle exemplar-based image editing â€“ the task of transferring an edit from an exemplar pair to a content image(s), by leveraging pretrained text-to-image diffusion models and multimodal VLMs. Even though our end-to-end pipeline is optimization-free, our experiments demonstrate that it still outperforms baselines on multiple types of edits while being ~4x faster. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å·²ç»å¯ç”¨äº†å¤šç§å›¾åƒç¼–è¾‘åº”ç”¨ç¨‹åºã€‚ç„¶è€Œï¼Œä»…é€šè¿‡æ–‡æœ¬æ•æ‰æ‰€æœ‰ç±»å‹çš„ç¼–è¾‘å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§å’Œç¹çæ€§ã€‚æŸäº›å›¾åƒç¼–è¾‘çš„æ¨¡ç³Šæ€§è´¨é€šè¿‡ç¤ºä¾‹å¯¹ï¼ˆå³åˆ†åˆ«æç»˜ç¼–è¾‘å‰å’Œç¼–è¾‘åçš„å›¾åƒå¯¹ï¼‰æ¥è¡¨è¾¾æ›´å¥½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†åŸºäºç¤ºä¾‹çš„å›¾åƒç¼–è¾‘é—®é¢˜â€”â€”å°†ä»ç¤ºä¾‹å¯¹åˆ°å†…å®¹å›¾åƒï¼ˆä»¬ï¼‰çš„ç¼–è¾‘ä»»åŠ¡è½¬ç§»ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å’Œè·¨æ¨¡æ€VLMsã€‚å°½ç®¡æˆ‘ä»¬çš„ç«¯åˆ°ç«¯ç®¡é“æ— éœ€ä¼˜åŒ–ï¼Œä½†å®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨å¤šç§ç±»å‹çš„ç¼–è¾‘æ–¹é¢ä»ä¼˜äºåŸºçº¿ï¼ŒåŒæ—¶é€Ÿåº¦æé«˜äº†çº¦4å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20155v1">PDF</a> Accepted at ECCV 2024 (AI4VA Workshop)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆText-to-Image Diffusion modelsï¼‰çš„èŒƒä¾‹å›¾åƒç¼–è¾‘æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å’Œå¤šåª’ä½“è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆmultimodal VLMsï¼‰ï¼Œå®ç°äº†ä»èŒƒä¾‹å›¾åƒå¯¹åˆ°å†…å®¹å›¾åƒï¼ˆsï¼‰çš„ç¼–è¾‘è½¬ç§»ã€‚å°½ç®¡è¯¥ç«¯åˆ°ç«¯çš„ç®¡é“æ— éœ€ä¼˜åŒ–ï¼Œå®éªŒè¯æ˜å…¶åœ¨å¤šç§ç±»å‹çš„ç¼–è¾‘ä¸Šä»ä¼˜äºåŸºçº¿ï¼Œä¸”é€Ÿåº¦æé«˜äº†çº¦4å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å·²ç»å¯ç”¨äº†å¹¿æ³›çš„å›¾åƒç¼–è¾‘åº”ç”¨ã€‚</li>
<li>ä»…é€šè¿‡æ–‡æœ¬æ•è·æ‰€æœ‰ç±»å‹çš„ç¼–è¾‘å…·æœ‰æŒ‘æˆ˜æ€§å’Œç¹çæ€§ã€‚</li>
<li>æŸäº›å›¾åƒç¼–è¾‘çš„æ¨¡ç³Šæ€§æ›´é€‚åˆé€šè¿‡ç¤ºä¾‹å›¾åƒå¯¹æ¥è¡¨è¾¾ï¼Œå³å±•ç¤ºå›¾åƒç¼–è¾‘å‰åçš„å¯¹æ¯”ã€‚</li>
<li>æœ¬æ–‡è§£å†³äº†åŸºäºèŒƒä¾‹çš„å›¾åƒç¼–è¾‘é—®é¢˜ï¼Œå³é€šè¿‡èŒƒä¾‹å›¾åƒå¯¹å°†ç¼–è¾‘è½¬ç§»åˆ°å†…å®¹å›¾åƒä¸Šã€‚</li>
<li>åˆ©ç”¨äº†é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å’Œå¤šåª’ä½“è§†è§‰è¯­è¨€æ¨¡å‹æ¥å®ç°è¿™ä¸€æŠ€æœ¯ã€‚</li>
<li>è¯¥ç«¯åˆ°ç«¯çš„èŒƒä¾‹å›¾åƒç¼–è¾‘ç®¡é“æ— éœ€ä¼˜åŒ–ï¼Œä½†å®éªŒè¯æ˜å…¶æ€§èƒ½ä¼˜äºå¤šç§åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20155">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a39cc898ecffa18c0203fa05c39fd58a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88f6cb5cf61cc25c9e27d7ded8c94225.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7081e7ce6c1e5324593a8e1840c0d4d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7ec74fe079f812ba953085508a9e5b2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Morse-Dual-Sampling-for-Lossless-Acceleration-of-Diffusion-Models"><a href="#Morse-Dual-Sampling-for-Lossless-Acceleration-of-Diffusion-Models" class="headerlink" title="Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models"></a>Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models</h2><p><strong>Authors:Chao Li, Jiawei Fan, Anbang Yao</strong></p>
<p>In this paper, we present Morse, a simple dual-sampling framework for accelerating diffusion models losslessly. The key insight of Morse is to reformulate the iterative generation (from noise to data) process via taking advantage of fast jump sampling and adaptive residual feedback strategies. Specifically, Morse involves two models called Dash and Dot that interact with each other. The Dash model is just the pre-trained diffusion model of any type, but operates in a jump sampling regime, creating sufficient space for sampling efficiency improvement. The Dot model is significantly faster than the Dash model, which is learnt to generate residual feedback conditioned on the observations at the current jump sampling point on the trajectory of the Dash model, lifting the noise estimate to easily match the next-step estimate of the Dash model without jump sampling. By chaining the outputs of the Dash and Dot models run in a time-interleaved fashion, Morse exhibits the merit of flexibly attaining desired image generation performance while improving overall runtime efficiency. With our proposed weight sharing strategy between the Dash and Dot models, Morse is efficient for training and inference. Our method shows a lossless speedup of 1.78X to 3.31X on average over a wide range of sampling step budgets relative to 9 baseline diffusion models on 6 image generation tasks. Furthermore, we show that our method can be also generalized to improve the Latent Consistency Model (LCM-SDXL, which is already accelerated with consistency distillation technique) tailored for few-step text-to-image synthesis. The code and models are available at <a target="_blank" rel="noopener" href="https://github.com/deep-optimization/Morse">https://github.com/deep-optimization/Morse</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Morseï¼Œä¸€ä¸ªç®€å•ç”¨äºæ— æŸåŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„åŒé‡é‡‡æ ·æ¡†æ¶ã€‚Morseçš„å…³é”®è§è§£åœ¨äºé€šè¿‡åˆ©ç”¨å¿«é€Ÿè·³è·ƒé‡‡æ ·å’Œè‡ªé€‚åº”æ®‹å·®åé¦ˆç­–ç•¥æ¥é‡æ–°åˆ¶å®šè¿­ä»£ç”Ÿæˆï¼ˆä»å™ªå£°åˆ°æ•°æ®ï¼‰è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼ŒMorseåŒ…å«ä¸¤ä¸ªç›¸äº’ä½œç”¨çš„æ¨¡å‹ï¼Œç§°ä¸ºDashå’ŒDotã€‚Dashæ¨¡å‹åªæ˜¯ä»»ä½•ç±»å‹çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä½†åœ¨è·³è·ƒé‡‡æ ·æœºåˆ¶ä¸‹è¿è¡Œï¼Œä¸ºé‡‡æ ·æ•ˆç‡æå‡åˆ›é€ äº†è¶³å¤Ÿç©ºé—´ã€‚Dotæ¨¡å‹æ˜¾è‘—å¿«äºDashæ¨¡å‹ï¼Œå®ƒå­¦ä¼šåœ¨Dashæ¨¡å‹çš„è½¨è¿¹å½“å‰è·³è·ƒé‡‡æ ·ç‚¹å¤„ç”ŸæˆåŸºäºè§‚å¯Ÿçš„æ®‹å·®åé¦ˆï¼Œå°†å™ªå£°ä¼°è®¡æå‡åˆ°è½»æ¾åŒ¹é…Dashæ¨¡å‹ä¸‹ä¸€æ­¥ä¼°è®¡è€Œæ— éœ€è·³è·ƒé‡‡æ ·ã€‚é€šè¿‡ä»¥æ—¶é—´äº¤é”™æ–¹å¼è¿è¡ŒDashå’ŒDotæ¨¡å‹çš„è¾“å‡ºé“¾æ¥ï¼ŒMorseåœ¨çµæ´»å®ç°æ‰€éœ€çš„å›¾åƒç”Ÿæˆæ€§èƒ½çš„åŒæ—¶æé«˜äº†æ€»ä½“è¿è¡Œæ•ˆç‡ã€‚é€šè¿‡æˆ‘ä»¬åœ¨Dashå’ŒDotæ¨¡å‹ä¹‹é—´æå‡ºçš„æƒé‡å…±äº«ç­–ç•¥ï¼ŒMorseåœ¨è®­ç»ƒå’Œæ¨ç†æ–¹é¢éƒ½éå¸¸é«˜æ•ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨6é¡¹å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šç›¸å¯¹äº9ä¸ªåŸºçº¿æ‰©æ•£æ¨¡å‹åœ¨å¹¿æ³›çš„é‡‡æ ·æ­¥éª¤é¢„ç®—èŒƒå›´å†…å¹³å‡å®ç°äº†æ— æŸåŠ é€Ÿ1.78å€è‡³3.31å€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°æ”¹è¿›é’ˆå¯¹å°‘æ­¥éª¤æ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„Latent Consistency Modelï¼ˆLCM-SDXLï¼Œå·²ä½¿ç”¨ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯åŠ é€Ÿï¼‰ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/deep-optimization/Morse%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/deep-optimization/Morseè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18251v2">PDF</a> Fixed a prompt typo in Figure 18 of the Appendix. This work is   accepted to ICML 2025. The project page:   <a target="_blank" rel="noopener" href="https://github.com/deep-optimization/Morse">https://github.com/deep-optimization/Morse</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åä¸ºMorseçš„ç®€å•åŒé‡‡æ ·æ¡†æ¶ï¼Œç”¨äºåŠ é€Ÿæ‰©æ•£æ¨¡å‹è€Œæ— æŸã€‚å…¶å…³é”®è§è§£æ˜¯é€šè¿‡åˆ©ç”¨å¿«é€Ÿè·³è·ƒé‡‡æ ·å’Œè‡ªé€‚åº”æ®‹å·®åé¦ˆç­–ç•¥æ¥æ”¹é©è¿­ä»£ç”Ÿæˆè¿‡ç¨‹ã€‚MorseåŒ…å«ä¸¤ä¸ªç›¸äº’ä½œç”¨çš„æ¨¡å‹ï¼šDashå’ŒDotã€‚Dashæ¨¡å‹æ˜¯ä»»ä½•ç±»å‹çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä½†åœ¨è·³è·ƒé‡‡æ ·ä½“åˆ¶ä¸‹è¿è¡Œï¼Œä¸ºé‡‡æ ·æ•ˆç‡æ”¹è¿›æä¾›äº†å……è¶³çš„ç©ºé—´ã€‚Dotæ¨¡å‹æ¯”Dashæ¨¡å‹æ›´å¿«ï¼Œå®ƒå­¦ä¹ åœ¨Dashæ¨¡å‹çš„è½¨è¿¹å½“å‰è·³è·ƒé‡‡æ ·ç‚¹ç”Ÿæˆæ®‹å·®åé¦ˆï¼Œå°†å™ªå£°ä¼°è®¡æå‡åˆ°ä¸Dashæ¨¡å‹çš„ä¸‹ä¸€æ­¥ä¼°è®¡ç›¸åŒ¹é…ï¼Œæ— éœ€è·³è·ƒé‡‡æ ·ã€‚é€šè¿‡ä»¥æ—¶é—´äº¤é”™æ–¹å¼è¿è¡ŒDashå’ŒDotæ¨¡å‹çš„è¾“å‡ºé“¾ï¼ŒMorseèƒ½å¤Ÿåœ¨æé«˜è¿è¡Œæ•ˆç‡çš„åŒæ—¶çµæ´»åœ°å®ç°æ‰€éœ€çš„å›¾åƒç”Ÿæˆæ€§èƒ½ã€‚é€šè¿‡æå‡ºDashå’ŒDotæ¨¡å‹ä¹‹é—´çš„æƒé‡å…±äº«ç­–ç•¥ï¼ŒMorseåœ¨è®­ç»ƒå’Œæ¨ç†æ–¹é¢éƒ½æ˜¯é«˜æ•ˆçš„ã€‚è¯¥æ–¹æ³•ç›¸å¯¹äº9ç§åŸºçº¿æ‰©æ•£æ¨¡å‹åœ¨6ç§å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå¹³å‡å®ç°äº†æ— æŸåŠ é€Ÿæ¯”ï¼Œä¸ºå‹ç¼©æ­¥æ•°è¾ƒå°‘çš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆæä¾›äº†è‰¯å¥½çš„æ”¹è¿›æ–¹å‘ã€‚å…¶ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€åœ¨GitHubä¸Šåˆ†äº«ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Morseæ˜¯ä¸€ä¸ªç”¨äºåŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„ç®€å•åŒé‡‡æ ·æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å›¾åƒç”Ÿæˆçš„æ•ˆç‡ã€‚</li>
<li>MorseåŒ…å«ä¸¤ä¸ªæ¨¡å‹ï¼šDashå’ŒDotï¼Œå®ƒä»¬ç›¸äº’åä½œä»¥æ”¹è¿›é‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>Dashæ¨¡å‹é‡‡ç”¨è·³è·ƒé‡‡æ ·æœºåˆ¶ï¼Œä¸ºé‡‡æ ·æ•ˆç‡æä¾›äº†æå‡ç©ºé—´ã€‚</li>
<li>Dotæ¨¡å‹å¿«é€Ÿç”Ÿæˆæ®‹å·®åé¦ˆï¼Œä»¥æ”¹å–„åŸºäºDashæ¨¡å‹çš„å™ªå£°ä¼°è®¡ã€‚</li>
<li>é€šè¿‡ç»“åˆDashå’ŒDotæ¨¡å‹çš„è¾“å‡ºï¼ŒMorseå®ç°äº†çµæ´»çš„å›¾åƒç”Ÿæˆæ€§èƒ½ï¼ŒåŒæ—¶æé«˜äº†è¿è¡Œæ•ˆç‡ã€‚</li>
<li>Morseé€šè¿‡æƒé‡å…±äº«ç­–ç•¥åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å±•ç°å‡ºé«˜æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18251">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-86abeb4e62ed5ce5cb9add9b5b8d0ce0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05708ab567db0ad895ccfcbee4651323.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dd16ba0285d3520134e85faa72004be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-474832705962235fd7685e55a9e41d9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-558f34ecd7ec39ab0e0fbb70c269d726.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Temporal-Differential-Fields-for-4D-Motion-Modeling-via-Image-to-Video-Synthesis"><a href="#Temporal-Differential-Fields-for-4D-Motion-Modeling-via-Image-to-Video-Synthesis" class="headerlink" title="Temporal Differential Fields for 4D Motion Modeling via Image-to-Video   Synthesis"></a>Temporal Differential Fields for 4D Motion Modeling via Image-to-Video   Synthesis</h2><p><strong>Authors:Xin You, Minghui Zhang, Hanxiao Zhang, Jie Yang, Nassir Navab</strong></p>
<p>Temporal modeling on regular respiration-induced motions is crucial to image-guided clinical applications. Existing methods cannot simulate temporal motions unless high-dose imaging scans including starting and ending frames exist simultaneously. However, in the preoperative data acquisition stage, the slight movement of patients may result in dynamic backgrounds between the first and last frames in a respiratory period. This additional deviation can hardly be removed by image registration, thus affecting the temporal modeling. To address that limitation, we pioneeringly simulate the regular motion process via the image-to-video (I2V) synthesis framework, which animates with the first frame to forecast future frames of a given length. Besides, to promote the temporal consistency of animated videos, we devise the Temporal Differential Diffusion Model to generate temporal differential fields, which measure the relative differential representations between adjacent frames. The prompt attention layer is devised for fine-grained differential fields, and the field augmented layer is adopted to better interact these fields with the I2V framework, promoting more accurate temporal variation of synthesized videos. Extensive results on ACDC cardiac and 4D Lung datasets reveal that our approach simulates 4D videos along the intrinsic motion trajectory, rivaling other competitive methods on perceptual similarity and temporal consistency. Codes will be available soon. </p>
<blockquote>
<p>å¯¹å¸¸è§„å‘¼å¸å¼•èµ·çš„è¿åŠ¨è¿›è¡Œæ—¶é—´å»ºæ¨¡å¯¹å›¾åƒå¼•å¯¼çš„ä¸´åºŠåº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æ¨¡æ‹Ÿæ—¶é—´è¿åŠ¨ï¼Œé™¤éåŒæ—¶å­˜åœ¨åŒ…æ‹¬èµ·å§‹å¸§å’Œç»“æŸå¸§åœ¨å†…çš„é«˜å‰‚é‡æˆåƒæ‰«æã€‚ç„¶è€Œï¼Œåœ¨æœ¯å‰æ•°æ®é‡‡é›†é˜¶æ®µï¼Œæ‚£è€…çš„è½»å¾®ç§»åŠ¨å¯èƒ½ä¼šå¯¼è‡´ä¸€ä¸ªå‘¼å¸å‘¨æœŸå†…ç¬¬ä¸€å¸§å’Œæœ€åä¸€å¸§ä¹‹é—´çš„åŠ¨æ€èƒŒæ™¯ã€‚è¿™ç§é¢å¤–çš„åå·®å‡ ä¹æ— æ³•é€šè¿‡å›¾åƒæ³¨å†Œå»é™¤ï¼Œä»è€Œå½±å“æ—¶é—´å»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–åˆ›æ€§åœ°é€šè¿‡å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰åˆæˆæ¡†æ¶æ¨¡æ‹Ÿå¸¸è§„è¿åŠ¨è¿‡ç¨‹ï¼Œè¯¥æ¡†æ¶ä»¥ç¬¬ä¸€å¸§ä¸ºåŠ¨ç”»é¢„æµ‹ç»™å®šé•¿åº¦çš„æœªæ¥å¸§ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜åŠ¨ç”»è§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†æ—¶é—´å·®åˆ†æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆæ—¶é—´å·®åˆ†åœºï¼Œè¯¥æ¨¡å‹æµ‹é‡ç›¸é‚»å¸§ä¹‹é—´çš„ç›¸å¯¹å·®åˆ†è¡¨ç¤ºã€‚è®¾è®¡äº†å³æ—¶æ³¨æ„å±‚ç”¨äºç²¾ç»†å·®åˆ†åœºï¼Œå¹¶é‡‡ç”¨åœºå¢å¼ºå±‚æ¥æ›´å¥½åœ°å°†è¿™äº›å­—æ®µä¸I2Væ¡†æ¶è¿›è¡Œäº¤äº’ï¼Œä¿ƒè¿›åˆæˆè§†é¢‘çš„æ—¶é—´å˜åŒ–æ›´åŠ å‡†ç¡®ã€‚åœ¨ACDCå¿ƒè„å’Œ4Dè‚ºéƒ¨æ•°æ®é›†çš„å¤§é‡ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¨¡æ‹Ÿäº†æ²¿å†…åœ¨è¿åŠ¨è½¨è¿¹çš„4Dè§†é¢‘ï¼Œåœ¨æ„ŸçŸ¥ç›¸ä¼¼æ€§å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢ä¸å…¶ä»–ç«äº‰æ–¹æ³•ç›¸å½“ã€‚ä»£ç å¾ˆå¿«å°±ä¼šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17333v2">PDF</a> early accepted by MICCAI</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³äº†åœ¨å›¾åƒå¼•å¯¼çš„ä¸´åºŠåº”ç”¨ä¸­ï¼Œå…³äºè§„å¾‹å‘¼å¸è¿åŠ¨çš„æ—¶é—´å»ºæ¨¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éœ€è¦åœ¨å¼€å§‹å’Œç»“æŸå¸§åŒæ—¶å­˜åœ¨çš„æ¡ä»¶ä¸‹æ‰èƒ½æ¨¡æ‹Ÿæ—¶åºè¿åŠ¨ã€‚ä½†æ‚£è€…æœ¯å‰æ•°æ®è·å–é˜¶æ®µçš„è½»å¾®ç§»åŠ¨å¯èƒ½å¯¼è‡´å‘¼å¸å‘¨æœŸå†…é¦–å°¾å¸§ä¹‹é—´çš„åŠ¨æ€èƒŒæ™¯å·®å¼‚ï¼Œéš¾ä»¥é€šè¿‡å›¾åƒæ³¨å†ŒæŠ€æœ¯æ¶ˆé™¤ï¼Œä»è€Œå½±å“æ—¶åºå»ºæ¨¡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡é¦–æ¬¡é€šè¿‡å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰åˆæˆæ¡†æ¶æ¨¡æ‹Ÿå¸¸è§„è¿åŠ¨è¿‡ç¨‹ï¼Œä»é¦–å¸§ç”Ÿæˆç»™å®šé•¿åº¦çš„æœªæ¥å¸§åŠ¨ç”»ã€‚ä¸ºæé«˜åŠ¨ç”»è§†é¢‘çš„æ—¶åºä¸€è‡´æ€§ï¼Œæœ¬æ–‡æå‡ºäº†æ—¶åºå·®åˆ†æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆæ—¶åºå·®åˆ†åœºæ¥è¡¡é‡ç›¸é‚»å¸§é—´çš„ç›¸å¯¹å·®å¼‚è¡¨ç¤ºã€‚è®¾è®¡äº†å³æ—¶æ³¨æ„å±‚ç”¨äºç²¾ç»†å·®åˆ†åœºï¼Œå¹¶é‡‡ç”¨åœºå¢å¼ºå±‚æ”¹å–„å…¶ä¸I2Væ¡†æ¶çš„äº¤äº’ï¼Œæé«˜åˆæˆè§†é¢‘çš„å‡†ç¡®æ—¶åºå˜åŒ–ã€‚åœ¨ACDCå¿ƒè„å’Œ4Dè‚ºéƒ¨æ•°æ®é›†ä¸Šçš„å¤§é‡ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ¨¡æ‹Ÿçš„4Dè§†é¢‘æ²¿å†…åœ¨è¿åŠ¨è½¨è¿¹è¿›è¡Œï¼Œåœ¨æ„ŸçŸ¥ç›¸ä¼¼æ€§å’Œæ—¶åºä¸€è‡´æ€§æ–¹é¢ä¸å…¶ä»–æ–¹æ³•ç«äº‰å¹¶å±•ç°å‡ºä¼˜åŠ¿ã€‚ä»£ç å³å°†å¼€æ”¾ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§£å†³ç°æœ‰æ–¹æ³•æ— æ³•æ¨¡æ‹Ÿæ‚£è€…å‘¼å¸è¿åŠ¨æ—¶åºå˜åŒ–çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ¯å‰æ•°æ®è·å–é˜¶æ®µå­˜åœ¨çš„åŠ¨æ€èƒŒæ™¯å·®å¼‚é—®é¢˜ã€‚</li>
<li>æå‡ºå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰åˆæˆæ¡†æ¶ï¼Œæ¨¡æ‹Ÿè§„å¾‹è¿åŠ¨è¿‡ç¨‹ï¼Œä»é¦–å¸§é¢„æµ‹æœªæ¥å¸§ã€‚</li>
<li>å¼•å…¥æ—¶åºå·®åˆ†æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆæ—¶åºå·®åˆ†åœºä»¥æé«˜åŠ¨ç”»è§†é¢‘çš„æ—¶åºä¸€è‡´æ€§ã€‚</li>
<li>è®¾è®¡å³æ—¶æ³¨æ„å±‚å’Œåœºå¢å¼ºå±‚ï¼Œæ”¹å–„æ¨¡å‹æ€§èƒ½ï¼Œæé«˜åˆæˆè§†é¢‘çš„å‡†ç¡®åº¦å’Œè´¨é‡ã€‚</li>
<li>åœ¨ACDCå¿ƒè„å’Œ4Dè‚ºéƒ¨æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥ç›¸ä¼¼æ€§å’Œæ—¶åºä¸€è‡´æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä»£ç å³å°†å¼€æ”¾ä½¿ç”¨ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6af52a55648faa98312538e984e5bb7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bee09bc37e1b88a1130e6604c0bfd205.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-774cbf78098487d114a0dfbb92fe815d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73b1a2a7d9a5884b44a6ef11aab50faa.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Diffusion-Models-Through-a-Global-Lens-Are-They-Culturally-Inclusive"><a href="#Diffusion-Models-Through-a-Global-Lens-Are-They-Culturally-Inclusive" class="headerlink" title="Diffusion Models Through a Global Lens: Are They Culturally Inclusive?"></a>Diffusion Models Through a Global Lens: Are They Culturally Inclusive?</h2><p><strong>Authors:Zahra Bayramli, Ayhan Suleymanzade, Na Min An, Huzama Ahmad, Eunsu Kim, Junyeong Park, James Thorne, Alice Oh</strong></p>
<p>Text-to-image diffusion models have recently enabled the creation of visually compelling, detailed images from textual prompts. However, their ability to accurately represent various cultural nuances remains an open question. In our work, we introduce CultDiff benchmark, evaluating state-of-the-art diffusion models whether they can generate culturally specific images spanning ten countries. We show that these models often fail to generate cultural artifacts in architecture, clothing, and food, especially for underrepresented country regions, by conducting a fine-grained analysis of different similarity aspects, revealing significant disparities in cultural relevance, description fidelity, and realism compared to real-world reference images. With the collected human evaluations, we develop a neural-based image-image similarity metric, namely, CultDiff-S, to predict human judgment on real and generated images with cultural artifacts. Our work highlights the need for more inclusive generative AI systems and equitable dataset representation over a wide range of cultures. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹æœ€è¿‘èƒ½å¤Ÿä»æ–‡æœ¬æç¤ºä¸­ç”Ÿæˆè§†è§‰å¸å¼•åŠ›å¼ºã€ç»†èŠ‚ä¸°å¯Œçš„å›¾åƒã€‚ç„¶è€Œï¼Œå®ƒä»¬å‡†ç¡®è¡¨ç°å„ç§æ–‡åŒ–ç»†å¾®å·®åˆ«çš„èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†CultDiffåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹æ˜¯å¦èƒ½ç”Ÿæˆæ¶µç›–åä¸ªå›½å®¶çš„æ–‡åŒ–ç‰¹å®šå›¾åƒã€‚æˆ‘ä»¬é€šè¿‡å¯¹ä¸åŒç›¸ä¼¼æ–¹é¢çš„ç²¾ç»†åˆ†æå‘ç°ï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆå»ºç­‘ã€æœè£…å’Œé£Ÿå“ç­‰æ–‡åŒ–æ–‡ç‰©æ—¶ç»å¸¸å¤±è´¥ï¼Œå°¤å…¶æ˜¯å¯¹ä»£è¡¨æ€§ä¸è¶³çš„å›½å®¶åœ°åŒºæ›´æ˜¯å¦‚æ­¤ã€‚ä¸çœŸå®ä¸–ç•Œå‚è€ƒå›¾åƒç›¸æ¯”ï¼Œåœ¨æ–‡åŒ–çš„ç›¸å…³æ€§ã€æè¿°çš„ä¿çœŸåº¦å’Œé€¼çœŸåº¦æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚é€šè¿‡æ”¶é›†çš„äººç±»è¯„ä¼°æ•°æ®ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºç¥ç»çš„å›¾åƒå›¾åƒç›¸ä¼¼æ€§åº¦é‡æŒ‡æ ‡ï¼Œå³CultDiff-Sï¼Œç”¨äºé¢„æµ‹äººç±»å¯¹å¸¦æœ‰æ–‡åŒ–æ–‡ç‰©çš„çœŸå®å’Œç”Ÿæˆå›¾åƒçš„åˆ¤æ–­ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†éœ€è¦æ›´å¤šåŒ…å®¹æ€§çš„ç”Ÿæˆäººå·¥æ™ºèƒ½ç³»ç»Ÿå’Œåœ¨å¹¿æ³›æ–‡åŒ–èƒŒæ™¯ä¸‹å‡è¡¡æ•°æ®é›†è¡¨ç¤ºçš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08914v2">PDF</a> 17 pages, 17 figures, 3 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿä»æ–‡æœ¬æç¤ºç”Ÿæˆè§†è§‰ä¸Šå¸å¼•äººçš„ã€è¯¦ç»†çš„å›¾åƒã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å‡†ç¡®è¡¨ç°ä¸åŒæ–‡åŒ–ç»†å¾®å·®åˆ«æ–¹é¢çš„èƒ½åŠ›ä»å­˜åœ¨ç–‘é—®ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæ¨å‡ºäº†CultDiffåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°å…ˆè¿›æ‰©æ•£æ¨¡å‹æ˜¯å¦èƒ½ç”Ÿæˆæ¶µç›–åä¸ªå›½å®¶çš„æ–‡åŒ–ç‰¹å®šå›¾åƒã€‚æˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆå»ºç­‘ã€æœé¥°å’Œé£Ÿç‰©ç­‰æ–‡åŒ–äº§ç‰©æ–¹é¢å¸¸å¸¸å¤±è´¥ï¼Œå°¤å…¶æ˜¯å¯¹ä»£è¡¨æ€§ä¸è¶³çš„å›½å®¶åœ°åŒºã€‚é€šè¿‡å¯¹ä¸åŒç›¸ä¼¼æ€§çš„ç²¾ç»†åˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†ä¸æ–‡åŒ–ç›¸å…³æ€§ã€æè¿°ä¿çœŸåº¦å’Œç°å®æ„Ÿä¸çœŸå®ä¸–ç•Œå‚è€ƒå›¾åƒç›¸æ¯”å­˜åœ¨çš„æ˜¾è‘—å·®è·ã€‚ç»“åˆæ”¶é›†çš„äººç±»è¯„ä¼°æ•°æ®ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºç¥ç»çš„å›¾åƒ-å›¾åƒç›¸ä¼¼æ€§åº¦é‡æ ‡å‡†CultDiff-Sï¼Œä»¥é¢„æµ‹åŒ…å«æ–‡åŒ–äº§ç‰©çš„çœŸå®å’Œç”Ÿæˆå›¾åƒçš„é€¼çœŸåº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†æ›´åŒ…å®¹çš„ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ç³»ç»Ÿå’Œå¹¿æ³›æ–‡åŒ–å‡è¡¡æ•°æ®é›†è¡¨ç¤ºçš„å¿…è¦æ€§ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹èƒ½ç”Ÿæˆè¯¦ç»†çš„å›¾åƒï¼Œä½†å…¶åœ¨è¡¨ç°æ–‡åŒ–ç»†å¾®å·®åˆ«æ–¹é¢çš„èƒ½åŠ›æœ‰å¾…æå‡ã€‚</li>
<li>CultDiffåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆæ¶µç›–å¤šå›½æ–‡åŒ–ç‰¹å®šå›¾åƒæ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨ç”Ÿæˆå»ºç­‘ã€æœé¥°å’Œé£Ÿç‰©ç­‰æ–‡åŒ–äº§ç‰©æ–¹é¢å­˜åœ¨ç¼ºé™·ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£è¡¨æ€§ä¸è¶³çš„å›½å®¶åœ°åŒºã€‚</li>
<li>æ¨¡å‹åœ¨æ–‡åŒ–ç›¸å…³æ€§ã€æè¿°ä¿çœŸåº¦å’Œç°å®æ„Ÿæ–¹é¢ä¸çœŸå®å›¾åƒå­˜åœ¨å·®è·ã€‚</li>
<li>é€šè¿‡äººç±»è¯„ä¼°æ•°æ®ï¼Œå¼€å‘å‡ºåŸºäºç¥ç»çš„å›¾åƒ-å›¾åƒç›¸ä¼¼æ€§åº¦é‡æ ‡å‡†CultDiff-Sã€‚</li>
<li>CultDiff-Sèƒ½æœ‰æ•ˆé¢„æµ‹åŒ…å«æ–‡åŒ–äº§ç‰©çš„çœŸå®å’Œç”Ÿæˆå›¾åƒçš„é€¼çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08914">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77d0cdd4f4d6244df74e879fff65c6d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93db845ad1dd4f9fd16edb25b11db9c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e42649492b30a8f487d7a1ea18fd726.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09d09083d03a086737c7f8abe7aa0b80.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MatSwap-Light-aware-material-transfers-in-images"><a href="#MatSwap-Light-aware-material-transfers-in-images" class="headerlink" title="MatSwap: Light-aware material transfers in images"></a>MatSwap: Light-aware material transfers in images</h2><p><strong>Authors:Ivan Lopes, Valentin Deschaintre, Yannick Hold-Geoffroy, Raoul de Charette</strong></p>
<p>We present MatSwap, a method to transfer materials to designated surfaces in an image photorealistically. Such a task is non-trivial due to the large entanglement of material appearance, geometry, and lighting in a photograph. In the literature, material editing methods typically rely on either cumbersome text engineering or extensive manual annotations requiring artist knowledge and 3D scene properties that are impractical to obtain. In contrast, we propose to directly learn the relationship between the input material â€“ as observed on a flat surface â€“ and its appearance within the scene, without the need for explicit UV mapping. To achieve this, we rely on a custom light- and geometry-aware diffusion model. We fine-tune a large-scale pre-trained text-to-image model for material transfer using our synthetic dataset, preserving its strong priors to ensure effective generalization to real images. As a result, our method seamlessly integrates a desired material into the target location in the photograph while retaining the identity of the scene. We evaluate our method on synthetic and real images and show that it compares favorably to recent work both qualitatively and quantitatively. We release our code and data on <a target="_blank" rel="noopener" href="https://github.com/astra-vision/MatSwap">https://github.com/astra-vision/MatSwap</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†MatSwapæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä»¥æ‘„å½±å†™å®çš„æ–¹å¼å°†ææ–™è½¬ç§»åˆ°å›¾åƒæŒ‡å®šè¡¨é¢çš„æ–¹æ³•ã€‚ç”±äºç…§ç‰‡ä¸­ææ–™å¤–è§‚ã€å‡ ä½•å½¢çŠ¶å’Œå…‰ç…§çš„å¤æ‚çº ç¼ ï¼Œè¿™ä¸€ä»»åŠ¡éå¸¸å›°éš¾ã€‚åœ¨æ–‡çŒ®ä¸­ï¼Œææ–™ç¼–è¾‘æ–¹æ³•é€šå¸¸ä¾èµ–äºç¹ççš„æ–‡æœ¬å·¥ç¨‹æˆ–éœ€è¦å¤§é‡æ‰‹åŠ¨æ³¨é‡Šï¼Œè¿™éœ€è¦è‰ºæœ¯å®¶çš„çŸ¥è¯†å’Œä¸åˆ‡å®é™…çš„3Dåœºæ™¯å±æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬æå‡ºç›´æ¥åœ¨å¹³é¢è¡¨é¢ä¸Šè§‚å¯Ÿåˆ°çš„è¾“å…¥ææ–™ä¸å…¶åœ¨åœºæ™¯ä¸­çš„å¤–è§‚ä¹‹é—´å»ºç«‹å…³ç³»ï¼Œè€Œæ— éœ€æ˜ç¡®çš„UVæ˜ å°„ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä¾èµ–äºä¸€ä¸ªè‡ªå®šä¹‰çš„è½»é‡çº§å’Œå‡ ä½•æ„ŸçŸ¥çš„æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨åˆæˆæ•°æ®é›†å¯¹å¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥è¿›è¡Œææ–™è½¬ç§»ï¼ŒåŒæ—¶ä¿ç•™å…¶å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»¥ç¡®ä¿æœ‰æ•ˆæ³›åŒ–åˆ°çœŸå®å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ— ç¼åœ°å°†æ‰€éœ€ææ–™é›†æˆåˆ°ç…§ç‰‡çš„ç›®æ ‡ä½ç½®ï¼ŒåŒæ—¶ä¿ç•™åœºæ™¯çš„èº«ä»½ã€‚æˆ‘ä»¬åœ¨åˆæˆå›¾åƒå’ŒçœŸå®å›¾åƒä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶å®šæ€§å®šé‡åœ°å±•ç¤ºäº†å®ƒä¸æœ€æ–°å·¥ä½œçš„æ¯”è¾ƒç»“æœã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/astra-vision/MatSwap%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E3%80%82">https://github.com/astra-vision/MatSwapä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07784v2">PDF</a> Accepted to EGSR, journal track to appear in Computer Graphics Forum</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æˆ‘ä»¬æå‡ºäº†MatSwapæ–¹æ³•ï¼Œå¯ä»¥åœ¨å›¾åƒä¸­ä»¥é€¼çœŸçš„æ–¹å¼å°†ææ–™è½¬ç§»åˆ°æŒ‡å®šè¡¨é¢ã€‚ç”±äºç…§ç‰‡ä¸­ææ–™å¤–è§‚ã€å‡ ä½•å½¢çŠ¶å’Œå…‰ç…§ä¹‹é—´çš„å¤æ‚çº ç¼ ï¼Œè¿™ä¸€ä»»åŠ¡éå¸¸å›°éš¾ã€‚ç°æœ‰çš„ææ–™ç¼–è¾‘æ–¹æ³•é€šå¸¸ä¾èµ–äºç¹ççš„æ–‡æœ¬å·¥ç¨‹æˆ–éœ€è¦å¤§é‡æ‰‹åŠ¨æ³¨é‡Šï¼Œè¿™éœ€è¦è‰ºæœ¯å®¶çš„çŸ¥è¯†å’Œä¸åˆ‡å®é™…çš„3Dåœºæ™¯å±æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬æå‡ºç›´æ¥å­¦ä¹ è¾“å…¥ææ–™ï¼ˆåœ¨å¹³é¢è¡¨é¢ä¸Šè§‚å¯Ÿåˆ°çš„ï¼‰ä¸å…¶åœ¨åœºæ™¯ä¸­çš„å¤–è§‚ä¹‹é—´çš„å…³ç³»ï¼Œæ— éœ€æ˜ç¡®çš„UVæ˜ å°„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä¾èµ–äºä¸€ä¸ªè‡ªå®šä¹‰çš„è½»é‡çº§å’Œå‡ ä½•æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨åˆæˆæ•°æ®é›†å¾®è°ƒå¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œç”¨äºææ–™è½¬ç§»ï¼Œä¿ç•™å…¶å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»¥ç¡®ä¿æœ‰æ•ˆåœ°æ¨å¹¿åˆ°çœŸå®å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ— ç¼åœ°å°†æ‰€éœ€ææ–™é›†æˆåˆ°ç…§ç‰‡çš„ç›®æ ‡ä½ç½®ï¼ŒåŒæ—¶ä¿ç•™åœºæ™¯çš„æ ‡è¯†ã€‚æˆ‘ä»¬åœ¨åˆæˆå›¾åƒå’ŒçœŸå®å›¾åƒä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†ä¸æœ€æ–°å·¥ä½œçš„æ¯”è¾ƒç»“æœæ—¢æœ‰å®šæ€§ä¹Ÿæœ‰å®šé‡ã€‚æˆ‘ä»¬å°†ä»£ç å’Œæ•°æ®å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/astra-vision/MatSwap%E3%80%82">https://github.com/astra-vision/MatSwapã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†MatSwapæ–¹æ³•ï¼Œå¯åœ¨å›¾åƒä¸­é€¼çœŸåœ°å°†ææ–™è½¬ç§»åˆ°æŒ‡å®šè¡¨é¢ã€‚</li>
<li>è§£å†³äº†ææ–™å¤–è§‚ã€å‡ ä½•å½¢çŠ¶å’Œå…‰ç…§ä¹‹é—´çš„å¤æ‚çº ç¼ é—®é¢˜ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæ— éœ€ç¹ççš„æ–‡æœ¬å·¥ç¨‹æˆ–å¤§é‡æ‰‹åŠ¨æ³¨é‡Šã€‚</li>
<li>åˆ©ç”¨è‡ªå®šä¹‰çš„è½»é‡çº§å’Œå‡ ä½•æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹å®ç°ææ–™è½¬ç§»ã€‚</li>
<li>é€šè¿‡åˆæˆæ•°æ®é›†å¾®è°ƒå¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œç¡®ä¿æœ‰æ•ˆæ¨å¹¿åˆ°çœŸå®å›¾åƒã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿæ— ç¼é›†æˆæ‰€éœ€ææ–™åˆ°ç…§ç‰‡çš„ç›®æ ‡ä½ç½®ï¼ŒåŒæ—¶ä¿ç•™åœºæ™¯çš„æ ‡è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f061db6f57f4b0cc494b30ab2b8896f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44fedafe96aa8fcfa29d0eed5494761c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa1ecf6e08ff35e3662ceb63af796ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1d70ff3c036a30f6b6a82b72ca749c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a0c3cd9a56b962ccb5102922c6de79a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Solving-Linear-Gaussian-Bayesian-Inverse-Problems-with-Decoupled-Diffusion-Sequential-Monte-Carlo"><a href="#Solving-Linear-Gaussian-Bayesian-Inverse-Problems-with-Decoupled-Diffusion-Sequential-Monte-Carlo" class="headerlink" title="Solving Linear-Gaussian Bayesian Inverse Problems with Decoupled   Diffusion Sequential Monte Carlo"></a>Solving Linear-Gaussian Bayesian Inverse Problems with Decoupled   Diffusion Sequential Monte Carlo</h2><p><strong>Authors:Filip EkstrÃ¶m Kelvinius, Zheng Zhao, Fredrik Lindsten</strong></p>
<p>A recent line of research has exploited pre-trained generative diffusion models as priors for solving Bayesian inverse problems. We contribute to this research direction by designing a sequential Monte Carlo method for linear-Gaussian inverse problems which builds on â€œdecoupled diffusionâ€, where the generative process is designed such that larger updates to the sample are possible. The method is asymptotically exact and we demonstrate the effectiveness of our Decoupled Diffusion Sequential Monte Carlo (DDSMC) algorithm on both synthetic as well as protein and image data. Further, we demonstrate how the approach can be extended to discrete data. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶åˆ©ç”¨é¢„è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ä½œä¸ºè§£å†³è´å¶æ–¯åé—®é¢˜çš„å…ˆéªŒã€‚æˆ‘ä»¬ä¸ºè¿™ä¸€ç ”ç©¶æ–¹å‘è®¾è®¡äº†ä¸€ç§åŸºäºâ€œè§£è€¦æ‰©æ•£â€çš„çº¿æ€§é«˜æ–¯åé—®é¢˜çš„åºè´¯è’™ç‰¹å¡ç½—æ–¹æ³•åšå‡ºè´¡çŒ®ï¼Œå…¶ä¸­ç”Ÿæˆè¿‡ç¨‹çš„è®¾è®¡ä½¿å¾—æ ·æœ¬èƒ½å¤Ÿè¿›è¡Œè¾ƒå¤§çš„æ›´æ–°ã€‚è¯¥æ–¹æ³•å…·æœ‰æ¸è¿‘ç²¾ç¡®æ€§ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨åˆæˆæ•°æ®ä»¥åŠè›‹ç™½è´¨å’Œå›¾åƒæ•°æ®ä¸Šå±•ç¤ºæˆ‘ä»¬çš„è§£è€¦æ‰©æ•£åºè´¯è’™ç‰¹å¡ç½—ï¼ˆDDSMCï¼‰ç®—æ³•çš„æœ‰æ•ˆæ€§æ¥è¯æ˜è¿™ä¸€ç‚¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†å¦‚ä½•å°†è¯¥æ–¹æ³•æ‰©å±•åˆ°ç¦»æ•£æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06379v2">PDF</a> Accepted to ICML 2025, to appear in PMLR 267. Code available at   <a target="_blank" rel="noopener" href="https://github.com/filipekstrm/ddsmc">https://github.com/filipekstrm/ddsmc</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶åˆ©ç”¨é¢„è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒæ¥è§£å†³è´å¶æ–¯åé—®é¢˜ã€‚è®¾è®¡äº†ä¸€ç§åŸºäºâ€œè§£è€¦æ‰©æ•£â€çš„åºè´¯è’™ç‰¹å¡æ´›æ–¹æ³•ï¼Œç”¨äºçº¿æ€§é«˜æ–¯åé—®é¢˜ã€‚ç”Ÿæˆè¿‡ç¨‹çš„è®¾è®¡å…è®¸æ›´å¤§çš„æ ·æœ¬æ›´æ–°ã€‚è¯¥æ–¹æ³•åœ¨åˆæˆæ•°æ®ä»¥åŠè›‹ç™½è´¨å’Œå›¾åƒæ•°æ®ä¸Šéƒ½è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•å°†å…¶æ‰©å±•åˆ°ç¦»æ•£æ•°æ®çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶åˆ©ç”¨é¢„è®­ç»ƒç”Ÿæˆæ‰©æ•£æ¨¡å‹ä½œä¸ºè§£å†³è´å¶æ–¯åé—®é¢˜çš„å…ˆéªŒã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§åŸºäºè§£è€¦æ‰©æ•£çš„åºè´¯è’™ç‰¹å¡æ´›æ–¹æ³•ç”¨äºçº¿æ€§é«˜æ–¯åé—®é¢˜ã€‚</li>
<li>ç”Ÿæˆè¿‡ç¨‹å…è®¸æ›´å¤§çš„æ ·æœ¬æ›´æ–°ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨åˆæˆæ•°æ®ä¸Šçš„æœ‰æ•ˆæ€§å¾—åˆ°éªŒè¯ã€‚</li>
<li>æ–¹æ³•åœ¨è›‹ç™½è´¨å’Œå›¾åƒæ•°æ®ä¸Šçš„è¡¨ç°å¾—åˆ°è¯æ˜ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°ç¦»æ•£æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d49a0c907622849fe64bd4845e302a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7608b097428140e2d8c87ef0d6c95e0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4527de6b27157ff84f8c75dc4fdca9c9.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="VICCA-Visual-Interpretation-and-Comprehension-of-Chest-X-ray-Anomalies-in-Generated-Report-Without-Human-Feedback"><a href="#VICCA-Visual-Interpretation-and-Comprehension-of-Chest-X-ray-Anomalies-in-Generated-Report-Without-Human-Feedback" class="headerlink" title="VICCA: Visual Interpretation and Comprehension of Chest X-ray Anomalies   in Generated Report Without Human Feedback"></a>VICCA: Visual Interpretation and Comprehension of Chest X-ray Anomalies   in Generated Report Without Human Feedback</h2><p><strong>Authors:Sayeh Gholipour Picha, Dawood Al Chanti, Alice Caplier</strong></p>
<p>As artificial intelligence (AI) becomes increasingly central to healthcare, the demand for explainable and trustworthy models is paramount. Current report generation systems for chest X-rays (CXR) often lack mechanisms for validating outputs without expert oversight, raising concerns about reliability and interpretability. To address these challenges, we propose a novel multimodal framework designed to enhance the semantic alignment and localization accuracy of AI-generated medical reports. Our framework integrates two key modules: a Phrase Grounding Model, which identifies and localizes pathologies in CXR images based on textual prompts, and a Text-to-Image Diffusion Module, which generates synthetic CXR images from prompts while preserving anatomical fidelity. By comparing features between the original and generated images, we introduce a dual-scoring system: one score quantifies localization accuracy, while the other evaluates semantic consistency. This approach significantly outperforms existing methods, achieving state-of-the-art results in pathology localization and text-to-image alignment. The integration of phrase grounding with diffusion models, coupled with the dual-scoring evaluation system, provides a robust mechanism for validating report quality, paving the way for more trustworthy and transparent AI in medical imaging. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„ä½œç”¨è¶Šæ¥è¶Šé‡è¦ï¼Œå¯¹å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦çš„æ¨¡å‹éœ€æ±‚ä¹Ÿè¶Šæ¥è¶Šè¿«åˆ‡ã€‚ç›®å‰ç”¨äºç”Ÿæˆèƒ¸éƒ¨Xå…‰ç‰‡æŠ¥å‘Šçš„ç°è¡Œç³»ç»Ÿç¼ºä¹åœ¨ç¼ºä¹ä¸“å®¶ç›‘ç£çš„æƒ…å†µä¸‹éªŒè¯è¾“å‡ºçš„æœºåˆ¶ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹å¯é æ€§å’Œå¯è§£é‡Šæ€§çš„æ‹…å¿§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜äººå·¥æ™ºèƒ½ç”Ÿæˆçš„åŒ»ç–—æŠ¥å‘Šçš„è¯­ä¹‰å¯¹é½å’Œå®šä½å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶é›†æˆäº†ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šçŸ­è¯­å®šä½æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºè¯†åˆ«å¹¶å®šä½èƒ¸éƒ¨Xå…‰ç‰‡å›¾åƒä¸­çš„ç—…ç†æƒ…å†µï¼›æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å—èƒ½å¤Ÿæ ¹æ®æç¤ºç”Ÿæˆåˆæˆèƒ¸éƒ¨Xå…‰ç‰‡å›¾åƒï¼ŒåŒæ—¶ä¿ç•™è§£å‰–ç»“æ„çš„ä¿çœŸåº¦ã€‚é€šè¿‡æ¯”è¾ƒåŸå§‹å›¾åƒå’Œç”Ÿæˆå›¾åƒçš„ç‰¹å¾ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŒé‡è¯„åˆ†ç³»ç»Ÿï¼šä¸€ä¸ªè¯„åˆ†é‡åŒ–å®šä½å‡†ç¡®æ€§ï¼Œå¦ä¸€ä¸ªè¯„åˆ†è¯„ä¼°è¯­ä¹‰ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ç—…ç†å®šä½ä»¥åŠæ–‡æœ¬åˆ°å›¾åƒçš„å¯¹é½æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚çŸ­è¯­å®šä½ä¸æ‰©æ•£æ¨¡å‹çš„é›†æˆï¼ŒåŠ ä¸ŠåŒé‡è¯„åˆ†è¯„ä¼°ç³»ç»Ÿï¼Œä¸ºéªŒè¯æŠ¥å‘Šè´¨é‡æä¾›äº†ç¨³å¥çš„æœºåˆ¶ï¼Œä¸ºåŒ»å­¦å½±åƒé¢†åŸŸæ›´å¯é ã€æ›´é€æ˜çš„AIé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17726v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—ä¿å¥ä¸­çš„æ ¸å¿ƒä½œç”¨æ—¥ç›Šå¢å¼ºï¼Œå¯¹å¯è§£é‡Šæ€§å’Œå¯é æ€§çš„æ¨¡å‹éœ€æ±‚å˜å¾—è‡³å…³é‡è¦ã€‚é’ˆå¯¹å½“å‰èƒ¸éƒ¨Xå…‰æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿç¼ºä¹éªŒè¯æœºåˆ¶çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜AIç”Ÿæˆçš„åŒ»ç–—æŠ¥å‘Šçš„è¯­ä¹‰å¯¹é½å’Œå®šä½å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶é›†æˆäº†çŸ­è¯­å®šä½æ¨¡å‹å’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å—ï¼Œé€šè¿‡æ¯”è¾ƒåŸå§‹å’Œç”Ÿæˆå›¾åƒçš„ç‰¹å¾ï¼Œå¼•å…¥åŒé‡è¯„åˆ†ç³»ç»Ÿï¼Œåˆ†åˆ«è¯„ä¼°å®šä½å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ç—…ç†å®šä½å’Œæ–‡æœ¬åˆ°å›¾åƒå¯¹é½æ–¹é¢è¾¾åˆ°æœ€æ–°ç»“æœã€‚çŸ­è¯­å®šä½ä¸æ‰©æ•£æ¨¡å‹çš„é›†æˆä»¥åŠåŒé‡è¯„åˆ†è¯„ä¼°ç³»ç»Ÿç›¸ç»“åˆï¼Œä¸ºæŠ¥å‘Šè´¨é‡éªŒè¯æä¾›äº†ç¨³å¥æœºåˆ¶ï¼Œä¸ºåŒ»ç–—æˆåƒé¢†åŸŸæ›´å¯é ã€é€æ˜çš„AIåº”ç”¨é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„é‡è¦æ€§æ—¥ç›Šå¢å¼ºï¼Œå¯¹å¯è§£é‡Šæ€§å’Œå¯é æ€§çš„æ¨¡å‹éœ€æ±‚è¿«åˆ‡ã€‚</li>
<li>å½“å‰èƒ¸éƒ¨Xå…‰æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿå­˜åœ¨ç¼ºä¹éªŒè¯æœºåˆ¶çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜AIç”Ÿæˆçš„åŒ»ç–—æŠ¥å‘Šçš„è¯­ä¹‰å¯¹é½å’Œå®šä½å‡†ç¡®æ€§ã€‚</li>
<li>æ¡†æ¶é›†æˆäº†çŸ­è¯­å®šä½æ¨¡å‹å’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å—ã€‚</li>
<li>é€šè¿‡æ¯”è¾ƒåŸå§‹å’Œç”Ÿæˆå›¾åƒçš„ç‰¹å¾ï¼Œå¼•å…¥åŒé‡è¯„åˆ†ç³»ç»Ÿè¯„ä¼°å®šä½å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ç—…ç†å®šä½å’Œæ–‡æœ¬åˆ°å›¾åƒå¯¹é½æ–¹é¢è¾¾åˆ°æœ€æ–°ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-065b3ca3ca9136c1ed7f70ab937643ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ceb4e1750a2153c2f42efe68b6ef3f35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-342ae454f3a989555f3394911d455226.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40014d89b47ab1d002ccc659a60e296d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ReconX-Reconstruct-Any-Scene-from-Sparse-Views-with-Video-Diffusion-Model"><a href="#ReconX-Reconstruct-Any-Scene-from-Sparse-Views-with-Video-Diffusion-Model" class="headerlink" title="ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion   Model"></a>ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion   Model</h2><p><strong>Authors:Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, Yueqi Duan</strong></p>
<p>Advancements in 3D scene reconstruction have transformed 2D images from the real world into 3D models, producing realistic 3D results from hundreds of input photos. Despite great success in dense-view reconstruction scenarios, rendering a detailed scene from insufficient captured views is still an ill-posed optimization problem, often resulting in artifacts and distortions in unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction paradigm that reframes the ambiguous reconstruction challenge as a temporal generation task. The key insight is to unleash the strong generative prior of large pre-trained video diffusion models for sparse-view reconstruction. However, 3D view consistency struggles to be accurately preserved in directly generated video frames from pre-trained models. To address this, given limited input views, the proposed ReconX first constructs a global point cloud and encodes it into a contextual space as the 3D structure condition. Guided by the condition, the video diffusion model then synthesizes video frames that are both detail-preserved and exhibit a high degree of 3D consistency, ensuring the coherence of the scene from various perspectives. Finally, we recover the 3D scene from the generated video through a confidence-aware 3D Gaussian Splatting optimization scheme. Extensive experiments on various real-world datasets show the superiority of our ReconX over state-of-the-art methods in terms of quality and generalizability. </p>
<blockquote>
<p>éšç€ä¸‰ç»´åœºæ™¯é‡å»ºæŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œèƒ½å¤Ÿå°†æ¥è‡ªç°å®ä¸–ç•Œçš„äºŒç»´å›¾åƒè½¬åŒ–ä¸ºä¸‰ç»´æ¨¡å‹ï¼Œåˆ©ç”¨æ•°ç™¾å¼ è¾“å…¥ç…§ç‰‡ç”Ÿæˆé€¼çœŸçš„ä¸‰ç»´ç»“æœã€‚è™½ç„¶åœ¨å¯†é›†è§†å›¾é‡å»ºåœºæ™¯ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†ä»æ•è·çš„æœ‰é™è§†è§’å¯¹åœºæ™¯è¿›è¡Œè¯¦ç»†æ¸²æŸ“ä»ç„¶æ˜¯ä¸€ä¸ªä¸é€‚å®šçš„ä¼˜åŒ–é—®é¢˜ï¼Œè¿™å¸¸å¸¸å¯¼è‡´æœªçŸ¥åŒºåŸŸå‡ºç°ä¼ªå½±å’Œå¤±çœŸã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºReconXçš„æ–°å‹ä¸‰ç»´åœºæ™¯é‡å»ºæ–¹æ³•ï¼Œå®ƒå°†æ¨¡ç³Šçš„é‡å»ºæŒ‘æˆ˜é‡æ–°å®šä¹‰ä¸ºä¸€ç§æ—¶é—´ç”Ÿæˆä»»åŠ¡ï¼Œå…¶å…³é”®è§è§£åœ¨äºåˆ©ç”¨å¤§å‹é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆå…ˆéªŒæ¥è¿›è¡Œç¨€ç–è§†å›¾é‡å»ºã€‚ç„¶è€Œï¼Œç›´æ¥ä»é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆçš„è§†é¢‘å¸§ä¸­å‡†ç¡®ä¿æŒä¸‰ç»´è§†å›¾çš„ä¸€è‡´æ€§æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç»™å®šæœ‰é™çš„è¾“å…¥è§†è§’ï¼Œæ‰€æå‡ºçš„ReconXé¦–å…ˆæ„å»ºå…¨å±€ç‚¹äº‘å¹¶å°†å…¶ç¼–ç ä¸ºä¸Šä¸‹æ–‡ç©ºé—´ä½œä¸ºä¸‰ç»´ç»“æ„æ¡ä»¶ã€‚åœ¨æ¡ä»¶çš„å¼•å¯¼ä¸‹ï¼Œè§†é¢‘æ‰©æ•£æ¨¡å‹éšååˆæˆçš„è§†é¢‘å¸§æ—¢ä¿ç•™äº†ç»†èŠ‚åˆè¡¨ç°å‡ºé«˜åº¦çš„ä¸‰ç»´ä¸€è‡´æ€§ï¼Œä»è€Œç¡®ä¿äº†ä»ä¸åŒè§’åº¦è§‚çœ‹åœºæ™¯çš„è¿è´¯æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ç½®ä¿¡åº¦æ„ŸçŸ¥çš„ä¸‰ç»´é«˜æ–¯å¹³é“ºä¼˜åŒ–æ–¹æ¡ˆä»ç”Ÿæˆçš„è§†é¢‘ä¸­æ¢å¤ä¸‰ç»´åœºæ™¯ã€‚åœ¨å¤šç§çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ReconXåœ¨è´¨é‡å’Œé€šç”¨æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16767v4">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://liuff19.github.io/ReconX">https://liuff19.github.io/ReconX</a></p>
<p><strong>Summary</strong><br>     æå‡ºäº†ä¸€ç§åä¸ºReconXçš„æ–°å‹3Dåœºæ™¯é‡å»ºæ–¹æ³•ï¼Œå°†ç¨€ç–è§†è§’é‡å»ºçš„æŒ‘æˆ˜é‡æ–°å®šä½ä¸ºæ—¶é—´ç”Ÿæˆä»»åŠ¡ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆå…ˆéªŒã€‚é€šè¿‡æ„å»ºå…¨å±€ç‚¹äº‘å¹¶å°†å…¶ç¼–ç ä¸ºä¸‰ç»´ç»“æ„æ¡ä»¶ï¼ŒæŒ‡å¯¼è§†é¢‘æ‰©æ•£æ¨¡å‹åˆæˆç»†èŠ‚ä¸°å¯Œä¸”é«˜åº¦ä¸€è‡´çš„3Dè§†é¢‘å¸§ã€‚é‡‡ç”¨ç½®ä¿¡åº¦æ„ŸçŸ¥çš„ä¸‰ç»´é«˜æ–¯å±•å¼€ä¼˜åŒ–æ–¹æ¡ˆä»ç”Ÿæˆçš„è§†é¢‘ä¸­æ¢å¤ä¸‰ç»´åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReconXå°†ç¨€ç–è§†è§’çš„3Dåœºæ™¯é‡å»ºé—®é¢˜é‡æ–°å®šä½ä¸ºæ—¶é—´ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆå…ˆéªŒæ¥è§£å†³é‡å»ºé—®é¢˜ã€‚</li>
<li>é€šè¿‡æ„å»ºå…¨å±€ç‚¹äº‘å¹¶å°†å…¶ç¼–ç ä¸ºä¸‰ç»´ç»“æ„æ¡ä»¶ï¼Œè§£å†³ç›´æ¥ä»é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆè§†é¢‘å¸§æ—¶å‡ºç°çš„ä¸‰ç»´è§†è§’ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>åˆæˆç»†èŠ‚ä¸°å¯Œä¸”é«˜åº¦ä¸€è‡´çš„3Dè§†é¢‘å¸§ï¼Œç¡®ä¿ä»ä¸åŒè§’åº¦è§‚çœ‹åœºæ™¯æ—¶çš„è¿è´¯æ€§ã€‚</li>
<li>é‡‡ç”¨ç½®ä¿¡åº¦æ„ŸçŸ¥çš„ä¸‰ç»´é«˜æ–¯å±•å¼€ä¼˜åŒ–æ–¹æ¡ˆä»ç”Ÿæˆçš„è§†é¢‘ä¸­æ¢å¤ä¸‰ç»´åœºæ™¯ã€‚</li>
<li>åœ¨å„ç§çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒReconXåœ¨è´¨é‡å’Œæ³›åŒ–æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>ReconXå¯¹äºä»ç°å®ä¸–ç•Œä¸­çš„äºŒç»´å›¾åƒåˆ›å»ºé€¼çœŸçš„ä¸‰ç»´æ¨¡å‹å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.16767">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b7e9b400377a322440477ce8ef422ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73ab4d17415ddae8d6a44b1f709ce86c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3446048336245ce217c90c61105a9a85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5acf6435c1553fdebcecaa20b1dd498.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac51fc81edf20af2945cf8fd06482003.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-27/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-27/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-27/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ff1f4e721b6d7230872b5f5f1d75a157.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-27  Weighted Mean Frequencies a handcraft Fourier feature for 4D Flow MRI   segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-27/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-aa6991d1b1035b20fb1962bd9425a8d0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-27  Joint attitude estimation and 3D neural reconstruction of   non-cooperative space objects
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26024.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
