<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-27  Weighted Mean Frequencies a handcraft Fourier feature for 4D Flow MRI   segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ff1f4e721b6d7230872b5f5f1d75a157.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-27-æ›´æ–°"><a href="#2025-06-27-æ›´æ–°" class="headerlink" title="2025-06-27 æ›´æ–°"></a>2025-06-27 æ›´æ–°</h1><h2 id="Weighted-Mean-Frequencies-a-handcraft-Fourier-feature-for-4D-Flow-MRI-segmentation"><a href="#Weighted-Mean-Frequencies-a-handcraft-Fourier-feature-for-4D-Flow-MRI-segmentation" class="headerlink" title="Weighted Mean Frequencies: a handcraft Fourier feature for 4D Flow MRI   segmentation"></a>Weighted Mean Frequencies: a handcraft Fourier feature for 4D Flow MRI   segmentation</h2><p><strong>Authors:Simon Perrin, SÃ©bastien Levilly, Huajun Sun, Harold MouchÃ¨re, Jean-Michel Serfaty</strong></p>
<p>In recent decades, the use of 4D Flow MRI images has enabled the quantification of velocity fields within a volume of interest and along the cardiac cycle. However, the lack of resolution and the presence of noise in these biomarkers are significant issues. As indicated by recent studies, it appears that biomarkers such as wall shear stress are particularly impacted by the poor resolution of vessel segmentation. The Phase Contrast Magnetic Resonance Angiography (PC-MRA) is the state-of-the-art method to facilitate segmentation. The objective of this work is to introduce a new handcraft feature that provides a novel visualisation of 4D Flow MRI images, which is useful in the segmentation task. This feature, termed Weighted Mean Frequencies (WMF), is capable of revealing the region in three dimensions where a voxel has been passed by pulsatile flow. Indeed, this feature is representative of the hull of all pulsatile velocity voxels. The value of the feature under discussion is illustrated by two experiments. The experiments involved segmenting 4D Flow MRI images using optimal thresholding and deep learning methods. The results obtained demonstrate a substantial enhancement in terms of IoU and Dice, with a respective increase of 0.12 and 0.13 in comparison with the PC-MRA feature, as evidenced by the deep learning task. This feature has the potential to yield valuable insights that could inform future segmentation processes in other vascular regions, such as the heart or the brain. </p>
<blockquote>
<p>è¿‘å‡ åå¹´æ¥ï¼Œå››ç»´æµç£å…±æŒ¯æˆåƒï¼ˆ4D Flow MRIï¼‰å›¾åƒçš„ä½¿ç”¨ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿé‡åŒ–æ„Ÿå…´è¶£ä½“ç§¯å†…çš„é€Ÿåº¦åœºä»¥åŠå¿ƒè„å‘¨æœŸä¸­çš„é€Ÿåº¦åœºã€‚ç„¶è€Œï¼Œè¿™äº›ç”Ÿç‰©æ ‡å¿—ç‰©åˆ†è¾¨ç‡ä½å’Œå­˜åœ¨å™ªå£°æ˜¯é‡è¦çš„é—®é¢˜ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå£å‰ªåˆ‡åº”åŠ›ç­‰ç”Ÿç‰©æ ‡å¿—ç‰©å—åˆ°è¡€ç®¡åˆ†å‰²åˆ†è¾¨ç‡å·®çš„å½±å“å°¤ä¸ºæ˜æ˜¾ã€‚ç›¸ä½å¯¹æ¯”ç£å…±æŒ¯è¡€ç®¡é€ å½±ï¼ˆPC-MRAï¼‰æ˜¯ç›®å‰æœ€å…ˆè¿›çš„ä¿ƒè¿›åˆ†å‰²çš„æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œçš„ç›®æ ‡æ˜¯å¼•å…¥ä¸€ç§æ–°çš„æ‰‹å·¥ç‰¹å¾ï¼Œä¸ºå››ç»´æµç£å…±æŒ¯æˆåƒå›¾åƒæä¾›ä¸€ç§æ–°çš„å¯è§†åŒ–æ–¹æ³•ï¼Œè¿™åœ¨åˆ†å‰²ä»»åŠ¡ä¸­å¾ˆæœ‰ç”¨ã€‚è¿™ç§ç‰¹å¾è¢«ç§°ä¸ºåŠ æƒå¹³å‡é¢‘ç‡ï¼ˆWMFï¼‰ï¼Œå®ƒèƒ½å¤Ÿæ­ç¤ºåœ¨ä¸‰ç»´åŒºåŸŸä¸­ï¼Œå“ªäº›ä½“ç´ è¢«è„‰åŠ¨æµæ‰€ç»è¿‡ã€‚å®é™…ä¸Šï¼Œè¿™ä¸€ç‰¹å¾ä»£è¡¨äº†æ‰€æœ‰è„‰åŠ¨é€Ÿåº¦ä½“ç´ çš„å¤–å£³ã€‚æ‰€è®¨è®ºçš„ç‰¹å¾é€šè¿‡ä¸¤ä¸ªå®éªŒæ¥è¯´æ˜ã€‚è¿™äº›å®éªŒåŒ…æ‹¬ä½¿ç”¨æœ€ä½³é˜ˆå€¼æ³•å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•å¯¹å››ç»´æµç£å…±æŒ¯æˆåƒå›¾åƒè¿›è¡Œåˆ†å‰²ã€‚è·å¾—çš„ç»“æœåœ¨IoUå’ŒDiceæ–¹é¢æœ‰äº†æ˜¾è‘—çš„æå‡ï¼Œä¸PC-MRAç‰¹å¾ç›¸æ¯”ï¼Œåˆ†åˆ«å¢åŠ äº†0.12å’Œ0.13ï¼Œæ·±åº¦å­¦ä¹ ä»»åŠ¡è¯æ˜äº†è¿™ä¸€ç‚¹ã€‚è¿™ä¸€ç‰¹å¾æœ‰å¯èƒ½äº§ç”Ÿæœ‰ä»·å€¼çš„è§è§£ï¼Œå¯ä»¥ä¸ºå…¶ä»–è¡€ç®¡åŒºåŸŸï¼ˆå¦‚å¿ƒè„æˆ–å¤§è„‘ï¼‰çš„åˆ†å‰²è¿‡ç¨‹æä¾›ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20614v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨4DæµMRIå›¾åƒçš„ä¸€ç§æ–°æ‰‹å·¥ç‰¹å¾â€”â€”åŠ æƒå¹³å‡é¢‘ç‡ï¼ˆWMFï¼‰ï¼Œç”¨äºå¯è§†åŒ–è¡€ç®¡åŒºåŸŸè„‰åŠ¨æµæƒ…å†µï¼Œå¹¶åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­æœ‰æ˜¾è‘—æé«˜æ•ˆæœçš„è¡¨ç°ã€‚è¿™ç§ç‰¹å¾å¯ä»¥é€šè¿‡æœ€ä½³é˜ˆå€¼å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•è¿›è¡Œåˆ†å‰²å®éªŒï¼Œæ˜¾ç¤ºå‡ºç›¸æ¯”PC-MRAç‰¹å¾åœ¨IoUå’ŒDiceæŒ‡æ ‡ä¸Šçš„ä¼˜åŠ¿ã€‚è¯¥ç‰¹å¾æœ‰æœ›åœ¨å¿ƒè¡€ç®¡å’Œå…¶ä»–è¡€ç®¡åŒºåŸŸçš„åˆ†å‰²è¿‡ç¨‹ä¸­æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>4D Flow MRIå›¾åƒå¯é‡åŒ–ä½“ç§¯å†…çš„é€Ÿåº¦åœºä»¥åŠå¿ƒè„å‘¨æœŸä¸­çš„å˜åŒ–ã€‚</li>
<li>å½“å‰ä½¿ç”¨ä¸­å­˜åœ¨åˆ†è¾¨ç‡ä½å’Œå™ªå£°çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯å¯¹äºç”Ÿç‰©æ ‡å¿—ç‰©å¦‚å£å‰ªåˆ‡åº”åŠ›å½±å“è¾ƒå¤§ã€‚</li>
<li>PC-MRAæ˜¯ç›®å‰æœ€å…ˆè¿›çš„ä¿ƒè¿›åˆ†å‰²çš„æ–¹æ³•ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ‰‹å·¥ç‰¹å¾â€”â€”åŠ æƒå¹³å‡é¢‘ç‡ï¼ˆWMFï¼‰ï¼Œç”¨äºå¯è§†åŒ–4DæµMRIå›¾åƒã€‚</li>
<li>WMFç‰¹å¾èƒ½æ­ç¤ºè„‰åŠ¨æµç»è¿‡çš„çš„ä¸‰ç»´åŒºåŸŸï¼Œä»£è¡¨æ‰€æœ‰è„‰åŠ¨é€Ÿåº¦ä½“ç´ çš„åŒ…ç»œã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨WMFç‰¹å¾è¿›è¡Œåˆ†å‰²ä»»åŠ¡æ—¶ï¼ŒIoUå’ŒDiceæŒ‡æ ‡ç›¸æ¯”PC-MRAç‰¹å¾æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20614">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c57bb718b7c8a5b9f5482a69ed31b9a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dd7e2133bcdab824dfbac2b53a8c1ccd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68778cb157d16a20fa35c3619457416a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AdvMIM-Adversarial-Masked-Image-Modeling-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#AdvMIM-Adversarial-Masked-Image-Modeling-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical   Image Segmentation"></a>AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical   Image Segmentation</h2><p><strong>Authors:Lei Zhu, Jun Zhou, Rick Siow Mong Goh, Yong Liu</strong></p>
<p>Vision Transformer has recently gained tremendous popularity in medical image segmentation task due to its superior capability in capturing long-range dependencies. However, transformer requires a large amount of labeled data to be effective, which hinders its applicability in annotation scarce semi-supervised learning scenario where only limited labeled data is available. State-of-the-art semi-supervised learning methods propose combinatorial CNN-Transformer learning to cross teach a transformer with a convolutional neural network, which achieves promising results. However, it remains a challenging task to effectively train the transformer with limited labeled data. In this paper, we propose an adversarial masked image modeling method to fully unleash the potential of transformer for semi-supervised medical image segmentation. The key challenge in semi-supervised learning with transformer lies in the lack of sufficient supervision signal. To this end, we propose to construct an auxiliary masked domain from original domain with masked image modeling and train the transformer to predict the entire segmentation mask with masked inputs to increase supervision signal. We leverage the original labels from labeled data and pseudo-labels from unlabeled data to learn the masked domain. To further benefit the original domain from masked domain, we provide a theoretical analysis of our method from a multi-domain learning perspective and devise a novel adversarial training loss to reduce the domain gap between the original and masked domain, which boosts semi-supervised learning performance. We also extend adversarial masked image modeling to CNN network. Extensive experiments on three public medical image segmentation datasets demonstrate the effectiveness of our method, where our method outperforms existing methods significantly. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/zlheui/AdvMIM">https://github.com/zlheui/AdvMIM</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œç”±äºè§†è§‰Transformeråœ¨æ•æ‰é•¿è·ç¦»ä¾èµ–æ–¹é¢å…·æœ‰å‡ºè‰²çš„èƒ½åŠ›ï¼Œå®ƒåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­è·å¾—äº†å·¨å¤§çš„å…³æ³¨ã€‚ç„¶è€Œï¼ŒTransformeréœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®æ‰èƒ½å‘æŒ¥å…¶ä½œç”¨ï¼Œè¿™åœ¨æ ‡æ³¨ç¨€ç¼ºçš„åŠç›‘ç£å­¦ä¹ åœºæ™¯ä¸­é™åˆ¶äº†å…¶é€‚ç”¨æ€§ï¼Œå› ä¸ºè¿™é‡Œåªæœ‰æœ‰é™çš„æ ‡æ³¨æ•°æ®å¯ç”¨ã€‚æœ€å…ˆè¿›çš„åŠç›‘ç£å­¦ä¹ æ–¹æ³•æå‡ºäº†ç»„åˆCNN-Transformerå­¦ä¹ ï¼Œé€šè¿‡äº¤å‰æ•™å­¦çš„æ–¹å¼å°†Transformerä¸å·ç§¯ç¥ç»ç½‘ç»œç›¸ç»“åˆï¼Œå–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼Œåœ¨æœ‰é™çš„æ ‡æ³¨æ•°æ®ä¸‹æœ‰æ•ˆåœ°è®­ç»ƒTransformerä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯¹æŠ—æ€§æ©æ¨¡å›¾åƒå»ºæ¨¡æ–¹æ³•ï¼Œä»¥å……åˆ†é‡Šæ”¾Transformeråœ¨åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ½œåŠ›ã€‚åœ¨åŠç›‘ç£å­¦ä¹ ä¸­ä½¿ç”¨Transformerçš„ä¸»è¦æŒ‘æˆ˜åœ¨äºç¼ºä¹è¶³å¤Ÿçš„ç›‘ç£ä¿¡å·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡æ©æ¨¡å›¾åƒå»ºæ¨¡ä»åŸå§‹é¢†åŸŸæ„å»ºè¾…åŠ©æ©æ¨¡é¢†åŸŸï¼Œå¹¶è®­ç»ƒTransformerä½¿ç”¨æ©æ¨¡è¾“å…¥é¢„æµ‹æ•´ä¸ªåˆ†å‰²æ©æ¨¡ï¼Œä»¥å¢åŠ ç›‘ç£ä¿¡å·ã€‚æˆ‘ä»¬åˆ©ç”¨æ¥è‡ªæ ‡æ³¨æ•°æ®çš„åŸå§‹æ ‡ç­¾å’Œæ¥è‡ªæœªæ ‡æ³¨æ•°æ®çš„ä¼ªæ ‡ç­¾æ¥å­¦ä¹ æ©æ¨¡é¢†åŸŸã€‚ä¸ºäº†è¿›ä¸€æ­¥ä»æ©æ¨¡é¢†åŸŸå—ç›Šï¼Œæˆ‘ä»¬ä»å¤šé¢†åŸŸå­¦ä¹ çš„è§’åº¦å¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ–°å‹å¯¹æŠ—è®­ç»ƒæŸå¤±æ¥å‡å°‘åŸå§‹é¢†åŸŸå’Œæ©æ¨¡é¢†åŸŸä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œè¿™æé«˜äº†åŠç›‘ç£å­¦ä¹ çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜æ‰©å±•äº†å¯¹æŠ—æ€§æ©æ¨¡å›¾åƒå»ºæ¨¡åˆ°CNNç½‘ç»œã€‚åœ¨ä¸‰ä¸ªå…¬å…±åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç°æœ‰æ–¹æ³•ä¸Šè¡¨ç°ä¼˜è¶Šã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/zlheui/AdvMIM%E3%80%82">https://github.com/zlheui/AdvMIMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20563v1">PDF</a> Accepted to MICCAI 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§å¯¹æŠ—æ€§æ©è†œå›¾åƒå»ºæ¨¡æ–¹æ³•ï¼Œç”¨äºåœ¨æœ‰é™æ ‡æ³¨æ•°æ®çš„åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å……åˆ†å‘æŒ¥Transformerçš„æ½œåŠ›ã€‚é€šè¿‡æ„å»ºè¾…åŠ©æ©è†œåŸŸå¹¶ä½¿ç”¨æ©è†œè¾“å…¥é¢„æµ‹æ•´ä¸ªåˆ†å‰²æ©è†œæ¥å¢åŠ ç›‘ç£ä¿¡å·ï¼ŒåŒæ—¶ä½¿ç”¨æ ‡æ³¨æ•°æ®å’Œæœªæ ‡æ³¨æ•°æ®çš„åŸå§‹æ ‡ç­¾å’Œä¼ªæ ‡ç­¾æ¥å­¦ä¹ æ©è†œåŸŸã€‚æ­¤å¤–ï¼Œä»å¤šåŸŸå­¦ä¹ è§’åº¦è¿›è¡Œç†è®ºåˆ†æï¼Œå¹¶è®¾è®¡æ–°å‹å¯¹æŠ—è®­ç»ƒæŸå¤±æ¥ç¼©å°åŸå§‹åŸŸå’Œæ©è†œåŸŸä¹‹é—´çš„åŸŸå·®è·ï¼Œä»è€Œæé«˜åŠç›‘ç£å­¦ä¹ æ€§èƒ½ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformerå› æ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»çš„èƒ½åŠ›åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>ç°æœ‰åŠç›‘ç£å­¦ä¹ æ–¹æ³•å°è¯•ç»“åˆCNNå’ŒTransformerè¿›è¡Œäº¤å‰æ•™å­¦ï¼Œä½†ä»é¢ä¸´åœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹æœ‰æ•ˆè®­ç»ƒTransformerçš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºå¯¹æŠ—æ€§æ©è†œå›¾åƒå»ºæ¨¡æ–¹æ³•ï¼Œæ—¨åœ¨é‡Šæ”¾Transformeråœ¨åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ½œåŠ›ã€‚</li>
<li>é€šè¿‡æ„å»ºæ©è†œåŸŸå¹¶ä½¿ç”¨æ©è†œè¾“å…¥é¢„æµ‹åˆ†å‰²æ©è†œæ¥å¢åŠ ç›‘ç£ä¿¡å·ï¼Œåˆ©ç”¨æ ‡æ³¨å’Œæœªæ ‡æ³¨æ•°æ®çš„æ ‡ç­¾è¿›è¡Œå­¦ä¹ ã€‚</li>
<li>ä»å¤šåŸŸå­¦ä¹ è§’åº¦è¿›è¡Œç†è®ºåˆ†æï¼Œå¹¶è®¾è®¡å¯¹æŠ—è®­ç»ƒæŸå¤±æ¥ç¼©å°åŸå§‹åŸŸå’Œæ©è†œåŸŸä¹‹é—´çš„å·®è·ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…é€‚ç”¨äºTransformerï¼Œè¿˜æ‰©å±•è‡³CNNç½‘ç»œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20563">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-771263cec4e15657066eec84da5c4bb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06f29fb82783d7d11f3431168be90c88.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="The-Jet-Origin-of-the-Mid-infrared-Excess-in-the-Black-Hole-V404-Cygni-in-Quiescence"><a href="#The-Jet-Origin-of-the-Mid-infrared-Excess-in-the-Black-Hole-V404-Cygni-in-Quiescence" class="headerlink" title="The Jet Origin of the Mid-infrared Excess in the Black Hole V404 Cygni   in Quiescence"></a>The Jet Origin of the Mid-infrared Excess in the Black Hole V404 Cygni   in Quiescence</h2><p><strong>Authors:E. S. Borowski, R. I. Hynes, Q. Hunt, A. J. Tetarenko, R. M. Plotkin, T. Shahbaz, P. Gandhi, T. J. Maccarone, J. C. A. Miller-Jones, C. O. Heinke, A. W. Shaw, T. D. Russell, G. R. Sivakoff, P. A. Charles, E. V. Palaiologou, P. Reig</strong></p>
<p>Observations of some quiescent black hole X-ray binaries have revealed an excess of mid-infrared (MIR) emission above that expected from their donor stars. In one system, V404 Cygni, this excess has been variously suggested to arise from the accretion disk, circumbinary material, or a compact relativistic jet. Here we present simultaneous James Webb Space Telescope (JWST), Atacama Large Millimeter&#x2F;submillimeter Array (ALMA), and complementary multi-wavelength observations undertaken to resolve this uncertainty. We observed large-amplitude 21 $\mu$m variability on short timescales with JWST, particularly a dramatic flare which swiftly rose to $\approx 2.4$ mJy, over 10 times the lowest observed MIR flux density. Similar variability was simultaneously observed from radio to X-ray wavelengths with other facilities throughout the campaign. This variability and the flat radio&#x2F;mm&#x2F;MIR spectral index ($\alpha &#x3D; 0.04 \pm 0.01$) suggest that the MIR excess in V404 Cyg does not arise from the accretion disk or circumbinary material but is instead dominated by synchrotron radiation from a jet which persists into quiescence. This result reinforces the ubiquity of the disk-jet connection in accreting black holes across a range of masses and accretion rates. </p>
<blockquote>
<p>å¯¹ä¸€äº›é™æ­¢é»‘æ´Xå°„çº¿åŒæ˜Ÿçš„è§‚æµ‹æ­ç¤ºäº†åœ¨ä¾›æ˜Ÿä½“çš„é¢„æœŸä¹‹ä¸Šçš„ä¸­åº¦çº¢å¤–ï¼ˆMIRï¼‰å‘å°„è¿‡å‰©ã€‚åœ¨ä¸€ä¸ªç³»ç»ŸV404 Cygniä¸­ï¼Œè¿™ç§è¿‡å‰©è¢«è®¤ä¸ºå¯èƒ½æ¥æºäºå¸ç§¯ç›˜ã€åŒæ˜Ÿå‘¨å›´ç‰©è´¨æˆ–ç´§å‡‘ç›¸å¯¹è®ºå–·æµã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æä¾›äº†è©¹å§†æ–¯Â·éŸ¦ä¼¯å¤ªç©ºæœ›è¿œé•œï¼ˆJWSTï¼‰ã€é˜¿å¡”å¡é©¬å¤§å‹æ¯«ç±³&#x2F;äºšæ¯«ç±³é˜µåˆ—ï¼ˆALMAï¼‰ä»¥åŠè¡¥å……çš„å¤šæ³¢é•¿è§‚æµ‹æ•°æ®ï¼Œä»¥è§£å†³è¿™ä¸€ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬ç”¨JWSTè§‚å¯Ÿåˆ°äº†å¤§èŒƒå›´çš„21å¾®ç±³çŸ­æ³¢æ—¶æ®µçš„å¤§å¹…åº¦å˜åŒ–ï¼Œç‰¹åˆ«æ˜¯å‡ºç°äº†æ€¥å‰§çš„è€€æ–‘ï¼Œè¿…é€Ÿä¸Šå‡è‡³çº¦2.4æ¯«å‰ç„¦è€³ï¼ˆmJyï¼‰ï¼Œè¿™æ˜¯è§‚å¯Ÿåˆ°çš„æœ€ä½ä¸­åº¦çº¢å¤–é€šé‡å¯†åº¦çš„åå€ä»¥ä¸Šã€‚åœ¨æ­¤æ¬¡æ´»åŠ¨ä¸­ï¼Œå…¶ä»–è®¾æ–½ä»æ— çº¿ç”µåˆ°Xå°„çº¿çš„æ³¢é•¿ä¹ŸåŒæ­¥è§‚å¯Ÿåˆ°ç±»ä¼¼çš„å˜åŠ¨ã€‚è¿™ç§å˜åŠ¨å’Œæ— çº¿ç”µ&#x2F;æ¯«ç±³&#x2F;ä¸­åº¦çº¢å¤–å…‰è°±æŒ‡æ•°å¹³å¦ï¼ˆÎ±&#x3D;0.04Â±0.01ï¼‰è¡¨æ˜ï¼ŒV404 Cygnusä¸­çš„ä¸­åº¦çº¢å¤–è¿‡å‰©å¹¶éæ¥è‡ªå¸ç§¯ç›˜æˆ–åŒæ˜Ÿå‘¨å›´ç‰©è´¨ï¼Œè€Œæ˜¯ä¸»è¦ç”±å–·æµäº§ç”Ÿçš„åŒæ­¥è¾å°„ä¸»å¯¼ï¼Œä¸”åœ¨é™æ­¢æœŸä»ç„¶å­˜åœ¨ã€‚è¿™ä¸€ç»“æœå†æ¬¡è¯å®äº†æ˜Ÿç›˜è¿æ¥åœ¨å¤šç§è´¨é‡å’Œå¸ç§¯ç‡ä¸‹çš„å¸ç§¯é»‘æ´ä¸­çš„æ™®éå­˜åœ¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20536v1">PDF</a> Submitted June 25, 2025. Comments welcome</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡è§‚æµ‹äº†é™æ­¢é»‘æ´Xå°„çº¿åŒæ˜Ÿä¸­çš„V404 Cygniç³»ç»Ÿï¼Œå‘ç°å…¶ä¸­å­˜åœ¨è¶…å‡ºé¢„æœŸçš„çº¢å¤–è¾å°„ã€‚é€šè¿‡è©¹å§†æ–¯éŸ¦ä¼¯å¤ªç©ºæœ›è¿œé•œï¼ˆJWSTï¼‰ã€é˜¿å¡”å¡é©¬å¤§å‹æ¯«ç±³æ³¢&#x2F;äºšæ¯«ç±³æ³¢é˜µåˆ—ï¼ˆALMAï¼‰å’Œå…¶ä»–è®¾æ–½çš„åŒæ­¥å¤šæ³¢é•¿è§‚æµ‹ï¼Œå‘ç°å…¶å­˜åœ¨å¤§å¹…åº¦çš„ä¸­çº¢å¤–æ³¢æ®µï¼ˆMIRï¼‰å¿«é€Ÿå˜åŒ–ç°è±¡ï¼Œå¦‚å‰§çƒˆé—ªå…‰ç­‰ã€‚ç»“åˆå…¶é¢‘è°±æŒ‡æ•°åˆ†æï¼Œè®¤ä¸ºçº¢å¤–è¾å°„è¶…å‡ºéƒ¨åˆ†å¹¶éæ¥è‡ªå¸ç§¯ç›˜æˆ–ç¯ç»•åŒæ˜Ÿçš„ç‰©è´¨ï¼Œè€Œæ˜¯ç”±é™æ­¢çŠ¶æ€ä¸‹æŒç»­å­˜åœ¨çš„å–·å°„æµäº§ç”Ÿçš„åŒæ­¥è¾å°„ä¸»å¯¼ã€‚è¿™ä¸€ç»“æœå†æ¬¡è¯å®äº†åœ¨ä¸åŒè´¨é‡å’Œå¸ç§¯ç‡çš„é»‘æ´ä¸­ï¼Œç›˜-å–·æµè¿æ¥çš„æ™®éæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>V404 Cygniç³»ç»Ÿçš„é™æ­¢é»‘æ´Xå°„çº¿åŒæ˜Ÿè§‚æµ‹ä¸­å‘ç°äº†è¶…å‡ºé¢„æœŸçš„çº¢å¤–è¾å°„ã€‚</li>
<li>é€šè¿‡JWSTå’ŒALMAçš„åŒæ­¥è§‚æµ‹ï¼Œå‘ç°ç³»ç»Ÿå­˜åœ¨å¤§å¹…åº¦ä¸­çº¢å¤–æ³¢æ®µå¿«é€Ÿå˜åŒ–ç°è±¡ã€‚</li>
<li>çº¢å¤–è¾å°„è¶…å‡ºéƒ¨åˆ†å¹¶éæ¥è‡ªå¸ç§¯ç›˜æˆ–ç¯ç»•åŒæ˜Ÿçš„ç‰©è´¨ã€‚</li>
<li>çº¢å¤–è¾å°„è¶…å‡ºéƒ¨åˆ†ä¸»è¦ç”±é™æ­¢çŠ¶æ€ä¸‹æŒç»­å­˜åœ¨çš„å–·å°„æµäº§ç”Ÿçš„åŒæ­¥è¾å°„ä¸»å¯¼ã€‚</li>
<li>è¿™ä¸€ç»“æœæ­ç¤ºäº†ç›˜-å–·æµè¿æ¥çš„æ™®éæ€§ï¼Œå¯¹ç†è§£é»‘æ´çš„ç‰©ç†ç‰¹æ€§æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>åŒæ—¶è§‚å¯Ÿåˆ°ä»å°„ç”µåˆ°Xå°„çº¿çš„å¤šæ³¢é•¿å˜åŒ–ï¼Œè¡¨æ˜è¿™ç§å˜åŒ–å¯èƒ½ä¸å–·æµæ´»åŠ¨æœ‰å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20536">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7490f209142e0703fbeb8f579aac0a79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9925fe7d954d722599af1ab69ccb50c5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Fusing-Radiomic-Features-with-Deep-Representations-for-Gestational-Age-Estimation-in-Fetal-Ultrasound-Images"><a href="#Fusing-Radiomic-Features-with-Deep-Representations-for-Gestational-Age-Estimation-in-Fetal-Ultrasound-Images" class="headerlink" title="Fusing Radiomic Features with Deep Representations for Gestational Age   Estimation in Fetal Ultrasound Images"></a>Fusing Radiomic Features with Deep Representations for Gestational Age   Estimation in Fetal Ultrasound Images</h2><p><strong>Authors:Fangyijie Wang, Yuan Liang, Sourav Bhattacharjee, Abey Campbell, Kathleen M. Curran, GuÃ©nolÃ© Silvestre</strong></p>
<p>Accurate gestational age (GA) estimation, ideally through fetal ultrasound measurement, is a crucial aspect of providing excellent antenatal care. However, deriving GA from manual fetal biometric measurements depends on the operator and is time-consuming. Hence, automatic computer-assisted methods are demanded in clinical practice. In this paper, we present a novel feature fusion framework to estimate GA using fetal ultrasound images without any measurement information. We adopt a deep learning model to extract deep representations from ultrasound images. We extract radiomic features to reveal patterns and characteristics of fetal brain growth. To harness the interpretability of radiomics in medical imaging analysis, we estimate GA by fusing radiomic features and deep representations. Our framework estimates GA with a mean absolute error of 8.0 days across three trimesters, outperforming current machine learning-based methods at these gestational ages. Experimental results demonstrate the robustness of our framework across different populations in diverse geographical regions. Our code is publicly available on \href{<a target="_blank" rel="noopener" href="https://github.com/13204942/RadiomicsImageFusion_FetalUS%7D%7BGitHub%7D">https://github.com/13204942/RadiomicsImageFusion_FetalUS}{GitHub}</a>. </p>
<blockquote>
<p>å‡†ç¡®ä¼°è®¡èƒé¾„ï¼ˆGAï¼‰æ˜¯æä¾›ä¼˜ç§€äº§å‰æŠ¤ç†çš„å…³é”®æ–¹é¢ï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œå¯é€šè¿‡èƒå„¿è¶…å£°æ³¢æµ‹é‡æ¥å®ç°ã€‚ç„¶è€Œï¼Œä»æ‰‹åŠ¨èƒå„¿ç”Ÿç‰©æµ‹é‡ä¸­å¾—å‡ºèƒé¾„å–å†³äºæ“ä½œäººå‘˜ï¼Œå¹¶ä¸”å¾ˆè€—æ—¶ã€‚å› æ­¤ï¼Œä¸´åºŠå®è·µè¦æ±‚ä½¿ç”¨è®¡ç®—æœºè¾…åŠ©çš„è‡ªåŠ¨æ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç‰¹å¾èåˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨èƒå„¿è¶…å£°å›¾åƒæ— éœ€ä»»ä½•æµ‹é‡ä¿¡æ¯å³å¯ä¼°è®¡èƒé¾„ã€‚æˆ‘ä»¬é‡‡ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä»è¶…å£°å›¾åƒä¸­æå–æ·±åº¦è¡¨ç¤ºã€‚æˆ‘ä»¬æå–æ”¾å°„å­¦ç‰¹å¾ä»¥æ­ç¤ºèƒå„¿å¤§è„‘ç”Ÿé•¿çš„æ¨¡å¼å’Œç‰¹å¾ã€‚ä¸ºäº†åˆ©ç”¨æ”¾å°„ç»„å­¦åœ¨åŒ»å­¦æˆåƒåˆ†æä¸­çš„è§£é‡Šæ€§ï¼Œæˆ‘ä»¬é€šè¿‡èåˆæ”¾å°„å­¦ç‰¹å¾å’Œæ·±åº¦è¡¨ç¤ºæ¥ä¼°è®¡èƒé¾„ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¼°è®¡èƒé¾„çš„å¹³å‡ç»å¯¹è¯¯å·®ä¸º8å¤©ï¼Œè·¨è¶Šä¸‰ä¸ªå­•æœŸã€‚ä¸å½“å‰åŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨è¿™äº›èƒé¾„æ–¹é¢çš„è¡¨ç°æ›´èƒœä¸€ç­¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸åŒåœ°ç†åŒºåŸŸçš„ä¸åŒäººç¾¤ä¸­å…·æœ‰å¾ˆå¼ºçš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20407v1">PDF</a> Accepted at MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ–°å‹ç‰¹å¾èåˆæ¡†æ¶ï¼Œç”¨äºä»èƒå„¿è¶…å£°å›¾åƒä¼°è®¡å­•é¾„ï¼ˆGAï¼‰ï¼Œæ— éœ€æ‰‹åŠ¨æµ‹é‡ä¿¡æ¯ã€‚ç ”ç©¶é€šè¿‡èåˆè¶…å£°å›¾åƒçš„æ·±åº¦ç‰¹å¾å’Œæ”¾å°„å­¦ç‰¹å¾ï¼ˆradiomic featuresï¼‰ä¼°è®¡GAï¼Œæé«˜äº†ä¼°è®¡çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚è¯¥æ¡†æ¶åœ¨ä¸åŒå­•æœŸå’Œä¸åŒåœ°ç†åŒºåŸŸçš„äººç¾¤ä¸­å‡è¡¨ç°å‡ºç¨³å¥æ€§ï¼Œå¹³å‡ç»å¯¹è¯¯å·®ä¸º8å¤©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹èƒå„¿è¶…å£°å›¾åƒè¿›è¡Œè‡ªåŠ¨åˆ†æä»¥ä¼°ç®—å­•é¾„ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ç‰¹å¾èåˆæ¡†æ¶ï¼Œç»“åˆäº†è¶…å£°å›¾åƒçš„æ·±åº¦ç‰¹å¾å’Œæ”¾å°„å­¦ç‰¹å¾ã€‚</li>
<li>é€šè¿‡èåˆè¿™ä¸¤ç§ç‰¹å¾ï¼Œç ”ç©¶æé«˜äº†ä¼°è®¡å­•é¾„çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å¤šä¸ªå­•æœŸå’Œä¸åŒåœ°ç†åŒºåŸŸçš„äººç¾¤ä¸­è¿›è¡Œäº†æµ‹è¯•ï¼Œè¡¨ç°å‡ºäº†ç¨³å¥æ€§ã€‚</li>
<li>ä¸ç°æœ‰çš„æœºå™¨å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨ä¼°ç®—å­•é¾„æ–¹é¢è¡¨ç°å‡ºäº†æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶ç»“æœé€šè¿‡å…¬å…±ä»£ç åº“ï¼ˆGitHubï¼‰å…±äº«ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20407">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4083bcf22fa429ff95b0be9454970caf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7de49953eee026a2e40f4263ca18bb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bd5b57ccdcf822353aafb6c913ce3a3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="In-flight-calibration-of-the-Lobster-Eye-Imager-for-Astronomy"><a href="#In-flight-calibration-of-the-Lobster-Eye-Imager-for-Astronomy" class="headerlink" title="In-flight calibration of the Lobster Eye Imager for Astronomy"></a>In-flight calibration of the Lobster Eye Imager for Astronomy</h2><p><strong>Authors:Huaqing Cheng, Hai-Wu Pan, Yuan Liu, Jingwei Hu, Haonan Yang, Donghua Zhao, Zhixing Ling, He-Yang Liu, Yifan Chen, Xiaojin Sun, Longhui Li, Ge Jin, Chen Zhang, Shuang-Nan Zhang, Weimin Yuan</strong></p>
<p>The Lobster Eye Imager for Astronomy (LEIA), as a pathfinder of the Wide-field X-ray Telescope (WXT) onboard the Einstein Probe (EP) satellite, is the first lobster-eye focusing X-ray telescope with a considerably large field-of-view (FoV) ever flown. During the two and half years of operations, a series of calibration observations were performed, to fully characterize its performance and calibrate the instrumental properties. In this paper, we present the results of the in-flight calibration campaign of LEIA, focusing on the properties of the PSF, source positional accuracy, effective area, energy response and the instrumental background. The calibration sources used are the Crab nebula, Sco X-1 and Cassiopeia A supernova remnant. Specifically, it is found that the spatial resolution remains almost unchanged compared to the pre-launch values, ranging from 3.6â€™-9.3â€™ with a median of 5.9â€™. The post-calibration source positional accuracy is found to be ~2â€™ (at the 90% C.L.). The Crab spectra can be well reproduced by the absorbed power-law model with the best-fit parameters in large agreement with the literature values, indicating that the in-orbit effective area is overall consistent with the model predictions and ground measurements. The effective area exhibits a systematic of $\lesssim10%$ (at the 68% C.L.), and a mild deterioration of ~15% at the lower energy end after one year of operation. The Cas A spectral analysis shows that the energy scale and spectral resolution of the detectors are generally consistent with ground values. The instrumental background is found to be largely consistent among the four detectors, with strong modulations by the geomagnetic activity and the spectrum qualitatively consistent with our previous simulations. These instrumental performances well meet the design requirements. This work paves the way for the in-orbit calibration of the EP-WXT. </p>
<blockquote>
<p>é¾™è™¾çœ¼å¤©æ–‡å­¦æˆåƒä»ªï¼ˆLEIAï¼‰ä½œä¸ºæ­è½½åœ¨çˆ±å› æ–¯å¦æ¢æµ‹å™¨ï¼ˆEPï¼‰å«æ˜Ÿä¸Šçš„å¹¿è§’Xå°„çº¿æœ›è¿œé•œï¼ˆWXTï¼‰çš„æ¢è·¯è€…ï¼Œæ˜¯è¿„ä»Šä¸ºæ­¢é£è¡Œè¿‡çš„å…·æœ‰ç›¸å½“å¤§è§†åœºï¼ˆFoVï¼‰çš„é¦–ä¸ªé¾™è™¾çœ¼èšç„¦Xå°„çº¿æœ›è¿œé•œã€‚åœ¨ä¸¤å¹´åŠçš„è¿è¡Œè¿‡ç¨‹ä¸­ï¼Œè¿›è¡Œäº†ä¸€ç³»åˆ—æ ¡å‡†è§‚æµ‹ï¼Œä»¥å……åˆ†è¡¨å¾å…¶æ€§èƒ½å¹¶æ ¡å‡†ä»ªå™¨å±æ€§ã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»äº†LEIAçš„é£è¡Œæ ¡å‡†æ´»åŠ¨ç»“æœï¼Œä¸»è¦é›†ä¸­åœ¨ç‚¹æ‰©æ•£å‡½æ•°ï¼ˆPSFï¼‰çš„ç‰¹æ€§ã€æºå®šä½ç²¾åº¦ã€æœ‰æ•ˆé¢ç§¯ã€èƒ½é‡å“åº”å’Œä»ªå™¨èƒŒæ™¯ç­‰æ–¹é¢ã€‚ä½¿ç”¨çš„æ ¡å‡†æºåŒ…æ‹¬èŸ¹çŠ¶æ˜Ÿäº‘ã€å¤©èåº§X-1å’Œä»™ååº§Aè¶…æ–°æ˜Ÿé—è¿¹ã€‚å…·ä½“æ¥è¯´ï¼Œä¸å‘å°„å‰çš„å€¼ç›¸æ¯”ï¼Œç©ºé—´åˆ†è¾¨ç‡å‡ ä¹æ²¡æœ‰å˜åŒ–ï¼ŒèŒƒå›´ä»3.6â€™-9.3â€™ï¼Œä¸­ä½æ•°ä¸º5.9â€™ã€‚ç»æ ¡å‡†åçš„æºå®šä½ç²¾åº¦çº¦ä¸º2â€™ï¼ˆåœ¨90%ç½®ä¿¡æ°´å¹³ä¸‹ï¼‰ã€‚èŸ¹çŠ¶æ˜Ÿäº‘å…‰è°±èƒ½å¤Ÿè¢«å¸æ”¶åŠŸç‡æ³•æ¨¡å‹å¾ˆå¥½åœ°å¤ç°ï¼Œæœ€ä½³æ‹Ÿåˆå‚æ•°ä¸æ–‡çŒ®å€¼é«˜åº¦ä¸€è‡´ï¼Œè¡¨æ˜åœ¨è½¨æœ‰æ•ˆé¢ç§¯æ€»ä½“ä¸Šä¸æ¨¡å‹é¢„æµ‹å’Œåœ°é¢æµ‹é‡ç»“æœä¸€è‡´ã€‚æœ‰æ•ˆé¢ç§¯çš„ç³»ç»Ÿæ€§å°äºç­‰äº10%ï¼ˆåœ¨68%ç½®ä¿¡æ°´å¹³ä¸‹ï¼‰ï¼Œè¿è¡Œä¸€å¹´ååœ¨è¾ƒä½èƒ½é‡ç«¯æœ‰çº¦15%çš„è½»å¾®æ¶åŒ–ã€‚Cas Aå…‰è°±åˆ†æè¡¨æ˜ï¼Œæ¢æµ‹å™¨çš„èƒ½é‡å°ºåº¦å’Œå…‰è°±åˆ†è¾¨ç‡ä¸åœ°é¢å€¼å¤§ä½“ä¸€è‡´ã€‚ä»ªå™¨èƒŒæ™¯åœ¨å››ä¸ªæ¢æµ‹å™¨ä¹‹é—´å¤§ä½“ä¸€è‡´ï¼Œå—åœ°ç£æ´»åŠ¨çš„å¼ºçƒˆè°ƒåˆ¶ï¼Œä¸”å…‰è°±ä¸æˆ‘ä»¬ä¹‹å‰çš„æ¨¡æ‹Ÿå®šæ€§ä¸€è‡´ã€‚è¿™äº›ä»ªå™¨æ€§èƒ½å‡ç¬¦åˆè®¾è®¡è¦æ±‚ã€‚è¿™é¡¹å·¥ä½œä¸ºEP-WXTçš„é£è¡Œæ ¡å‡†é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20369v1">PDF</a> 14 pages, 14 figures, 2 tables. Submitted to Astronomy &amp; Astrophysics</p>
<p><strong>æ‘˜è¦</strong></p>
<p>LEIAä½œä¸ºæ­è½½åœ¨çˆ±å› æ–¯å¦æ¢æµ‹å™¨å«æ˜Ÿä¸Šçš„å®½è§†åœºXå°„çº¿æœ›è¿œé•œï¼ˆWXTï¼‰çš„æ¢è·¯è€…ï¼Œæ˜¯é¦–æ¬¾å…·æœ‰è¶…å¤§è§†åœºï¼ˆFoVï¼‰çš„é¾™è™¾çœ¼èšç„¦Xå°„çº¿æœ›è¿œé•œã€‚ç»è¿‡ä¸¤å¹´åŠçš„è¿è¡Œï¼Œè¿›è¡Œäº†ä¸€ç³»åˆ—æ ¡å‡†è§‚æµ‹ï¼Œä»¥å……åˆ†è¡¨å¾å…¶æ€§èƒ½å¹¶æ ¡å‡†ä»ªå™¨å±æ€§ã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»LEIAçš„é£è¡Œæ ¡å‡†ç»“æœï¼Œæ¶‰åŠç‚¹æ‰©æ•£å‡½æ•°æ€§è´¨ã€æºå®šä½ç²¾åº¦ã€æœ‰æ•ˆé¢ç§¯ã€èƒ½é‡å“åº”å’Œä»ªå™¨èƒŒæ™¯ç­‰å±æ€§ã€‚ä½¿ç”¨èŸ¹çŠ¶æ˜Ÿäº‘ã€Sco X-1å’Œä»™ååº§Aè¶…æ–°æ˜Ÿé—è¿¹ä½œä¸ºæ ¡å‡†æºã€‚ç‰¹åˆ«æ˜¯ï¼Œå‘ç°ç©ºé—´åˆ†è¾¨ç‡ä¸å‘å°„å‰çš„æ•°å€¼å‡ ä¹ä¿æŒä¸€è‡´ï¼ŒèŒƒå›´åœ¨3.6â€™-9.3â€™ï¼Œä¸­ä½æ•°ä¸º5.9â€™ã€‚æºå®šä½ç²¾åº¦çº¦ä¸º2â€™ï¼ˆåœ¨90%ç½®ä¿¡æ°´å¹³ä¸‹ï¼‰ã€‚èŸ¹è°±å¯ä»¥é€šè¿‡å¸æ”¶å¹‚å¾‹æ¨¡å‹å¾ˆå¥½åœ°é‡ç°ï¼Œæœ€ä½³æ‹Ÿåˆå‚æ•°ä¸æ–‡çŒ®å€¼é«˜åº¦ä¸€è‡´ï¼Œè¡¨æ˜è½¨é“æœ‰æ•ˆé¢ç§¯ä¸æ¨¡å‹é¢„æµ‹å’Œåœ°é¢æµ‹é‡ç»“æœæ€»ä½“ä¸€è‡´ã€‚æœ‰æ•ˆåŒºåŸŸè¡¨ç°å‡ºç³»ç»Ÿè¯¯å·®å°äºç­‰äºçš„ç™¾åˆ†æ¯”ï¼ˆåœ¨68%ç½®ä¿¡æ°´å¹³ä¸‹ï¼‰ï¼Œç»è¿‡ä¸€å¹´çš„è¿è¡Œåï¼Œåœ¨è¾ƒä½èƒ½é‡ç«¯å‡ºç°çº¦15%çš„è½»å¾®æ¶åŒ–ã€‚Cas Aå…‰è°±åˆ†æè¡¨æ˜ï¼Œæ¢æµ‹å™¨çš„èƒ½é‡å°ºåº¦å’Œå…‰è°±åˆ†è¾¨ç‡ä¸åœ°é¢å€¼åŸºæœ¬ä¸€è‡´ã€‚ä»ªå™¨èƒŒæ™¯åœ¨å››ä¸ªæ¢æµ‹å™¨ä¹‹é—´å¤§ä½“ä¸€è‡´ï¼Œå—åœ°ç£æ´»åŠ¨çš„å½±å“æœ‰å¼ºè°ƒåˆ¶ä½œç”¨ï¼Œå¹¶ä¸”å…‰è°±ä¸å‰æ¨¡æ‹Ÿå®šæ€§ä¸€è‡´ã€‚è¿™äº›ä»ªå™¨æ€§èƒ½å‡è¾¾åˆ°è®¾è®¡è¦æ±‚ã€‚æ­¤é¡¹å·¥ä½œä¸ºåç»­çˆ±å› æ–¯å¦æ¢æµ‹å™¨å«æ˜Ÿçš„WXTçš„åœ¨è½¨æ ¡å‡†é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>LEIAä½œä¸ºæ­è½½åœ¨çˆ±å› æ–¯å¦æ¢æµ‹å™¨å«æ˜Ÿä¸Šçš„å®½è§†åœºXå°„çº¿æœ›è¿œé•œçš„æ¢è·¯è€…ï¼Œæ˜¯é¦–æ¬¾å…·æœ‰è¶…å¤§è§†åœºçš„é¾™è™¾çœ¼èšç„¦Xå°„çº¿æœ›è¿œé•œã€‚</li>
<li>åœ¨ä¸¤å¹´åŠçš„è¿è¡ŒæœŸé—´ï¼Œè¿›è¡Œäº†æ¶µç›–å¤šä¸ªå±æ€§çš„é£è¡Œæ ¡å‡†è§‚æµ‹ã€‚</li>
<li>ç©ºé—´åˆ†è¾¨ç‡ä¸å‘å°„å‰æ•°å€¼ä¿æŒä¸€è‡´ã€‚</li>
<li>æºå®šä½ç²¾åº¦çº¦ä¸º2â€™ã€‚</li>
<li>èŸ¹è°±å¯ä»¥é€šè¿‡å¸æ”¶å¹‚å¾‹æ¨¡å‹å¾ˆå¥½åœ°é‡ç°ï¼Œæœ‰æ•ˆé¢ç§¯ä¸æ¨¡å‹é¢„æµ‹å’Œåœ°é¢æµ‹é‡ç»“æœä¸€è‡´ã€‚</li>
<li>æœ‰æ•ˆåŒºåŸŸå­˜åœ¨ç³»ç»Ÿè¯¯å·®å°äºç­‰äºçš„ç™¾åˆ†æ¯”å˜åŒ–ï¼Œä¸”åœ¨è¾ƒä½èƒ½é‡ç«¯å­˜åœ¨è½»å¾®æ¶åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b2ca9fa7ee82ed76f7a91b4e632b6752.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-219c05619e30bbf70bd0d8384ee64df2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3078ad55244430506694e3b57265adc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52896d5d57798a1c4435ebcba557fb0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fb0a9033ae414a835ff8f3d54e457cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e987930fa8af68ab33c6c0eeda50c244.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EAGLE-An-Efficient-Global-Attention-Lesion-Segmentation-Model-for-Hepatic-Echinococcosis"><a href="#EAGLE-An-Efficient-Global-Attention-Lesion-Segmentation-Model-for-Hepatic-Echinococcosis" class="headerlink" title="EAGLE: An Efficient Global Attention Lesion Segmentation Model for   Hepatic Echinococcosis"></a>EAGLE: An Efficient Global Attention Lesion Segmentation Model for   Hepatic Echinococcosis</h2><p><strong>Authors:Jiayan Chen, Kai Li, Yulu Zhao, Jianqiang Huang, Zhan Wang</strong></p>
<p>Hepatic echinococcosis (HE) is a widespread parasitic disease in underdeveloped pastoral areas with limited medical resources. While CNN-based and Transformer-based models have been widely applied to medical image segmentation, CNNs lack global context modeling due to local receptive fields, and Transformers, though capable of capturing long-range dependencies, are computationally expensive. Recently, state space models (SSMs), such as Mamba, have gained attention for their ability to model long sequences with linear complexity. In this paper, we propose EAGLE, a U-shaped network composed of a Progressive Visual State Space (PVSS) encoder and a Hybrid Visual State Space (HVSS) decoder that work collaboratively to achieve efficient and accurate segmentation of hepatic echinococcosis (HE) lesions. The proposed Convolutional Vision State Space Block (CVSSB) module is designed to fuse local and global features, while the Haar Wavelet Transformation Block (HWTB) module compresses spatial information into the channel dimension to enable lossless downsampling. Due to the lack of publicly available HE datasets, we collected CT slices from 260 patients at a local hospital. Experimental results show that EAGLE achieves state-of-the-art performance with a Dice Similarity Coefficient (DSC) of 89.76%, surpassing MSVM-UNet by 1.61%. </p>
<blockquote>
<p>è‚æ£˜çƒè™«ç—…ï¼ˆHEï¼‰æ˜¯ä¸€ç§åœ¨åŒ»ç–—èµ„æºæœ‰é™çš„æ¬ å‘è¾¾ç‰§åŒºå¹¿æ³›æµè¡Œçš„å¯„ç”Ÿè™«ç—…ã€‚è™½ç„¶åŸºäºCNNçš„æ¨¡å‹å’ŒåŸºäºTransformerçš„æ¨¡å‹å·²å¹¿æ³›åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œä½†CNNç”±äºå±€éƒ¨æ„Ÿå—é‡è€Œç¼ºä¹å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œè€ŒTransformerè™½ç„¶èƒ½å¤Ÿæ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚æœ€è¿‘ï¼ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ï¼Œå¦‚Mambaç­‰ï¼Œå› å…¶ä»¥çº¿æ€§å¤æ‚åº¦å¯¹é•¿åºåˆ—è¿›è¡Œå»ºæ¨¡çš„èƒ½åŠ›è€Œå—åˆ°å…³æ³¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºEAGLEçš„Uå½¢ç½‘ç»œï¼Œå®ƒç”±æ¸è¿›è§†è§‰çŠ¶æ€ç©ºé—´ï¼ˆPVSSï¼‰ç¼–ç å™¨å’Œæ··åˆè§†è§‰çŠ¶æ€ç©ºé—´ï¼ˆHVSSï¼‰è§£ç å™¨ç»„æˆï¼Œå®ƒä»¬ååŒå·¥ä½œï¼Œå®ç°å¯¹è‚æ£˜çƒè™«ç—…ï¼ˆHEï¼‰ç—…å˜çš„é«˜æ•ˆä¸”å‡†ç¡®åˆ†å‰²ã€‚æ‰€æå‡ºçš„å·ç§¯è§†è§‰çŠ¶æ€ç©ºé—´å—ï¼ˆCVSSBï¼‰æ¨¡å—æ—¨åœ¨èåˆå±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œè€ŒHaarå°æ³¢å˜æ¢å—ï¼ˆHWTBï¼‰æ¨¡å—å°†ç©ºé—´ä¿¡æ¯å‹ç¼©åˆ°é€šé“ç»´åº¦ï¼Œä»¥å®ç°æ— æŸä¸‹é‡‡æ ·ã€‚ç”±äºç¼ºä¹å…¬å¼€çš„HEæ•°æ®é›†ï¼Œæˆ‘ä»¬ä»å½“åœ°åŒ»é™¢æ”¶é›†äº†260åæ‚£è€…çš„CTåˆ‡ç‰‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEAGLEè¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½ï¼ŒDiceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ä¸º89.76%ï¼Œæ¯”MSVM-UNeté«˜å‡º1.61%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20333v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰çš„Uå‹ç½‘ç»œç»“æ„EAGLEï¼Œç”¨äºé«˜æ•ˆä¸”å‡†ç¡®åœ°åˆ†å‰²è‚æ£˜çƒèš´ç—…ï¼ˆHEï¼‰ç—…ç¶ã€‚è¯¥ç½‘ç»œç»“åˆäº†æ¸è¿›è§†è§‰çŠ¶æ€ç©ºé—´ï¼ˆPVSSï¼‰ç¼–ç å™¨å’Œæ··åˆè§†è§‰çŠ¶æ€ç©ºé—´ï¼ˆHVSSï¼‰è§£ç å™¨ï¼Œå¹¶è®¾è®¡äº†å·ç§¯è§†è§‰çŠ¶æ€ç©ºé—´å—ï¼ˆCVSSBï¼‰å’ŒHaarå°æ³¢å˜æ¢å—ï¼ˆHWTBï¼‰ï¼Œä»¥èåˆå±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼ŒåŒæ—¶å®ç°æ— æŸé™é‡‡æ ·ã€‚åœ¨ç¼ºä¹å…¬å¼€å¯ç”¨çš„HEæ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œå®éªŒç»“æœæ˜¾ç¤ºEAGLEè¾¾åˆ°äº†å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼ŒDiceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ä¸º89.76%ï¼Œè¶…è¶Šäº†MSVM-UNetã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚æ£˜çƒèš´ç—…ï¼ˆHEï¼‰æ˜¯ä¸€ç§åœ¨åŒ»ç–—èµ„æºæœ‰é™çš„æ¬ å‘è¾¾ç‰§åŒºå¹¿æ³›æµè¡Œçš„å¯„ç”Ÿè™«ç—…ã€‚</li>
<li>CNNå’ŒTransformeræ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†CNNç¼ºä¹å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œè€ŒTransformerè™½ç„¶èƒ½æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰å¦‚Mambaå› å…¶å¯¹é•¿åºåˆ—çš„çº¿æ€§å¤æ‚åº¦å»ºæ¨¡èƒ½åŠ›è€Œå—åˆ°å…³æ³¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†EAGLEç½‘ç»œï¼Œç»“åˆPVSSç¼–ç å™¨å’ŒHVSSè§£ç å™¨å®ç°é«˜æ•ˆå’Œå‡†ç¡®çš„è‚æ£˜çƒèš´ç—…ï¼ˆHEï¼‰ç—…ç¶åˆ†å‰²ã€‚</li>
<li>æå‡ºçš„CVSSBæ¨¡å—æ—¨åœ¨èåˆå±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œè€ŒHWTBæ¨¡å—åˆ™èƒ½å°†ç©ºé—´ä¿¡æ¯å‹ç¼©åˆ°é€šé“ç»´åº¦ä»¥å®ç°æ— æŸé™é‡‡æ ·ã€‚</li>
<li>æ”¶é›†äº†æ¥è‡ªå½“åœ°åŒ»é™¢çš„260åæ‚£è€…çš„CTåˆ‡ç‰‡ä½œä¸ºæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e8305f5029655167a21994ee5b9fa0ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca2d755b26424ae700bf627e8f709720.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14e6cd32fd593e66ff2b4f2fe68a99f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffbe0b3abea0135f45f0d44bea159d72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27d45dbfd00963668846178a11a3ade6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Radiomic-fingerprints-for-knee-MR-images-assessment"><a href="#Radiomic-fingerprints-for-knee-MR-images-assessment" class="headerlink" title="Radiomic fingerprints for knee MR images assessment"></a>Radiomic fingerprints for knee MR images assessment</h2><p><strong>Authors:Yaxi Chen, Simin Ni, Shaheer U. Saeed, Aleksandra Ivanova, Rikin Hargunani, Jie Huang, Chaozong Liu, Yipeng Hu</strong></p>
<p>Accurate interpretation of knee MRI scans relies on expert clinical judgment, often with high variability and limited scalability. Existing radiomic approaches use a fixed set of radiomic features (the signature), selected at the population level and applied uniformly to all patients. While interpretable, these signatures are often too constrained to represent individual pathological variations. As a result, conventional radiomic-based approaches are found to be limited in performance, compared with recent end-to-end deep learning (DL) alternatives without using interpretable radiomic features. We argue that the individual-agnostic nature in current radiomic selection is not central to its intepretability, but is responsible for the poor generalization in our application. Here, we propose a novel radiomic fingerprint framework, in which a radiomic feature set (the fingerprint) is dynamically constructed for each patient, selected by a DL model. Unlike the existing radiomic signatures, our fingerprints are derived on a per-patient basis by predicting the feature relevance in a large radiomic feature pool, and selecting only those that are predictive of clinical conditions for individual patients. The radiomic-selecting model is trained simultaneously with a low-dimensional (considered relatively explainable) logistic regression for downstream classification. We validate our methods across multiple diagnostic tasks including general knee abnormalities, anterior cruciate ligament (ACL) tears, and meniscus tears, demonstrating comparable or superior diagnostic accuracy relative to state-of-the-art end-to-end DL models. More importantly, we show that the interpretability inherent in our approach facilitates meaningful clinical insights and potential biomarker discovery, with detailed discussion, quantitative and qualitative analysis of real-world clinical cases to evidence these advantages. </p>
<blockquote>
<p>å‡†ç¡®çš„è†å…³èŠ‚MRIæ‰«æè§£è¯»ä¾èµ–äºä¸“ä¸šåŒ»ç”Ÿçš„ä¸´åºŠåˆ¤æ–­ï¼Œä½†å¾€å¾€å­˜åœ¨è¾ƒå¤§çš„å·®å¼‚æ€§å’Œæœ‰é™çš„æ‰©å±•æ€§ã€‚ç°æœ‰çš„æ”¾å°„ç»„å­¦æ–¹æ³•ä½¿ç”¨ä¸€ç»„å›ºå®šçš„æ”¾å°„ç»„å­¦ç‰¹å¾ï¼ˆç­¾åï¼‰ï¼Œåœ¨äººç¾¤å±‚é¢è¿›è¡Œé€‰æ‹©å¹¶ç»Ÿä¸€åº”ç”¨äºæ‰€æœ‰æ‚£è€…ã€‚è™½ç„¶è¿™äº›ç­¾åæ˜¯å¯è§£é‡Šçš„ï¼Œä½†å®ƒä»¬é€šå¸¸è¿‡äºå—é™ï¼Œæ— æ³•ä»£è¡¨ä¸ªä½“çš„ç—…ç†å˜åŒ–ã€‚å› æ­¤ï¼Œä¸ä¼ ç»Ÿçš„åŸºäºæ”¾å°„ç»„å­¦çš„æ–¹æ³•ç›¸æ¯”ï¼Œæœ€è¿‘å‡ºç°çš„ç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ›¿ä»£æ–¹æ¡ˆåœ¨ä¸ä½¿ç”¨å¯è§£é‡Šçš„æ”¾å°„ç»„å­¦ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå½“å‰æ”¾å°„ç»„å­¦é€‰æ‹©ä¸­çš„ä¸ªä½“æ— å…³æ€§å¹¶ä¸æ˜¯å…¶è§£é‡Šæ€§çš„å…³é”®ï¼Œè€Œæ˜¯æˆ‘ä»¬åº”ç”¨ä¸­æ¨å¹¿èƒ½åŠ›å·®çš„åŸå› ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ”¾å°„ç»„å­¦æŒ‡çº¹æ¡†æ¶ï¼Œå…¶ä¸­é’ˆå¯¹æ¯ä¸ªæ‚£è€…åŠ¨æ€æ„å»ºæ”¾å°„ç»„å­¦ç‰¹å¾é›†ï¼ˆæŒ‡çº¹ï¼‰ï¼Œå¹¶ç”±æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œé€‰æ‹©ã€‚ä¸ä¼ ç»Ÿçš„æ”¾å°„ç»„å­¦ç­¾åä¸åŒï¼Œæˆ‘ä»¬çš„æŒ‡çº¹æ˜¯åŸºäºæ¯ä¸ªæ‚£è€…çš„åŸºç¡€æ´¾ç”Ÿçš„ï¼Œé€šè¿‡åœ¨å¤§è§„æ¨¡çš„æ”¾å°„ç»„å­¦ç‰¹å¾æ± ä¸­é¢„æµ‹ç‰¹å¾ç›¸å…³æ€§ï¼Œå¹¶ä»…é€‰æ‹©é‚£äº›èƒ½å¤Ÿé¢„æµ‹ä¸ªä½“æ‚£è€…çš„ä¸´åºŠçŠ¶å†µçš„ç‰¹å¾ã€‚æ”¾å°„ç»„å­¦é€‰æ‹©æ¨¡å‹æ˜¯ä¸ä½ç»´åº¦ï¼ˆè¢«è®¤ä¸ºæ˜¯ç›¸å¯¹å¯è§£é‡Šçš„ï¼‰é€»è¾‘å›å½’åŒæ—¶è®­ç»ƒçš„ï¼Œç”¨äºä¸‹æ¸¸åˆ†ç±»ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªè¯Šæ–­ä»»åŠ¡ä¸­è¿›è¡Œäº†éªŒè¯ï¼ŒåŒ…æ‹¬è†å…³èŠ‚å¼‚å¸¸ã€å‰äº¤å‰éŸ§å¸¦æ’•è£‚å’ŒåŠæœˆæ¿æ’•è£‚ç­‰ã€‚ä¸æœ€å…ˆè¿›çš„ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„è¯Šæ–­å‡†ç¡®æ€§ç›¸å½“æˆ–æ›´é«˜ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•æ‰€å›ºæœ‰çš„å¯è§£é‡Šæ€§ï¼Œæœ‰åŠ©äºè·å¾—æœ‰æ„ä¹‰çš„ä¸´åºŠè§è§£å’Œæ½œåœ¨çš„ç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°ã€‚é€šè¿‡å¯¹çœŸå®ä¸–ç•Œç—…ä¾‹çš„è¯¦ç»†è®¨è®ºã€å®šé‡å’Œå®šæ€§åˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™äº›ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20306v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è†ç›–MRIæ‰«æçš„å‡†ç¡®è§£è¯»ä¾èµ–äºä¸“å®¶ä¸´åºŠåˆ¤æ–­ï¼Œä½†å­˜åœ¨é«˜åº¦å¯å˜æ€§å’Œæœ‰é™çš„å¯æ‰©å±•æ€§ã€‚ç°æœ‰æ”¾å°„ç»„å­¦æ–¹æ³•ä½¿ç”¨å›ºå®šçš„ä¸€ç»„æ”¾å°„ç»„å­¦ç‰¹å¾ï¼ˆç­¾åï¼‰ï¼Œåœ¨äººç¾¤å±‚é¢é€‰æ‹©å¹¶ç»Ÿä¸€åº”ç”¨äºæ‰€æœ‰æ‚£è€…ã€‚å°½ç®¡è¿™äº›ç­¾åå…·æœ‰å¯è§£é‡Šæ€§ï¼Œä½†å®ƒä»¬é€šå¸¸è¿‡äºå—é™ï¼Œæ— æ³•ä»£è¡¨ä¸ªäººçš„ç—…ç†å˜åŒ–ã€‚å› æ­¤ï¼Œä¸æœ€è¿‘çš„ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ›¿ä»£æ–¹æ¡ˆç›¸æ¯”ï¼Œä¼ ç»ŸåŸºäºæ”¾å°„ç»„å­¦çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¡¨ç°æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ”¾å°„ç»„å­¦æŒ‡çº¹æ¡†æ¶ï¼Œå…¶ä¸­é’ˆå¯¹æ¯ä¸ªæ‚£è€…åŠ¨æ€æ„å»ºæ”¾å°„ç»„å­¦ç‰¹å¾é›†ï¼ˆæŒ‡çº¹ï¼‰ï¼Œå¹¶ç”±DLæ¨¡å‹é€‰æ‹©ã€‚ä¸ä¼ ç»Ÿçš„æ”¾å°„ç»„å­¦ç­¾åä¸åŒï¼Œæˆ‘ä»¬çš„æŒ‡çº¹æ˜¯æ ¹æ®æ‚£è€…ä¸ªäººæƒ…å†µä»å¤§é‡çš„æ”¾å°„ç»„å­¦ç‰¹å¾æ± ä¸­é¢„æµ‹ç‰¹å¾ç›¸å…³æ€§è€Œå¾—å‡ºçš„ï¼Œä»…é€‰æ‹©é‚£äº›å¯¹ä¸´åºŠçŠ¶å†µå…·æœ‰é¢„æµ‹æ€§çš„ç‰¹å¾ã€‚æ”¾å°„ç»„å­¦é€‰æ‹©æ¨¡å‹ä¸ä½ç»´åº¦ï¼ˆç›¸å¯¹å¯è§£é‡Šï¼‰é€»è¾‘å›å½’åŒæ—¶è¿›è¡Œè®­ç»ƒï¼Œç”¨äºä¸‹æ¸¸åˆ†ç±»ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªè¯Šæ–­ä»»åŠ¡ä¸­éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸€èˆ¬è†ç›–å¼‚å¸¸ã€å‰äº¤å‰éŸ§å¸¦æ’•è£‚å’ŒåŠæœˆæ¿æ’•è£‚ç­‰ï¼Œè¡¨ç°å‡ºä¸æœ€å…ˆè¿›çš„ç«¯åˆ°ç«¯DLæ¨¡å‹ç›¸å½“çš„æˆ–æ›´é«˜çš„è¯Šæ–­å‡†ç¡®æ€§ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ–¹æ³•æ‰€å›ºæœ‰çš„å¯è§£é‡Šæ€§ï¼Œæœ‰åŠ©äºæœ‰æ„ä¹‰çš„ä¸´åºŠè§è§£å’Œæ½œåœ¨çš„ç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰è†ç›–MRIè§£è¯»ä¾èµ–ä¸“å®¶ä¸´åºŠåˆ¤æ–­ï¼Œå­˜åœ¨é«˜å˜æ€§å’Œæœ‰é™å¯æ‰©å±•æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ”¾å°„ç»„å­¦æ–¹æ³•ä½¿ç”¨å›ºå®šç‰¹å¾é›†ï¼Œéš¾ä»¥ä»£è¡¨ä¸ªä½“ç—…ç†å˜åŒ–ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨è†ç›–MRIè§£è¯»ä¸­æœ‰æ½œåŠ›ï¼Œä½†ç¼ºä¹å¯è§£é‡Šæ€§ã€‚</li>
<li>æå‡ºæ–°çš„æ”¾å°„ç»„å­¦æŒ‡çº¹æ¡†æ¶ï¼Œé’ˆå¯¹æ¯ä¸ªæ‚£è€…åŠ¨æ€æ„å»ºç‰¹å¾é›†ã€‚</li>
<li>é€šè¿‡é¢„æµ‹ç‰¹å¾ç›¸å…³æ€§é€‰æ‹©æ”¾å°„ç»„å­¦æŒ‡çº¹ï¼Œæé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>ç»“åˆä½ç»´åº¦é€»è¾‘å›å½’è¿›è¡Œä¸‹æ¸¸åˆ†ç±»ï¼Œå¢åŠ æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>åœ¨å¤šä¸ªè¯Šæ–­ä»»åŠ¡ä¸­éªŒè¯æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬è†ç›–å¼‚å¸¸ã€ACLå’ŒåŠæœˆæ¿æ’•è£‚ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20306">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e01167116c1482cdf5fd9024d1f9a7e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9cc2910ed68a0b1f65b5c51fb329067.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Opportunistic-Osteoporosis-Diagnosis-via-Texture-Preserving-Self-Supervision-Mixture-of-Experts-and-Multi-Task-Integration"><a href="#Opportunistic-Osteoporosis-Diagnosis-via-Texture-Preserving-Self-Supervision-Mixture-of-Experts-and-Multi-Task-Integration" class="headerlink" title="Opportunistic Osteoporosis Diagnosis via Texture-Preserving   Self-Supervision, Mixture of Experts and Multi-Task Integration"></a>Opportunistic Osteoporosis Diagnosis via Texture-Preserving   Self-Supervision, Mixture of Experts and Multi-Task Integration</h2><p><strong>Authors:Jiaxing Huang, Heng Guo, Le Lu, Fan Yang, Minfeng Xu, Ge Yang, Wei Luo</strong></p>
<p>Osteoporosis, characterized by reduced bone mineral density (BMD) and compromised bone microstructure, increases fracture risk in aging populations. While dual-energy X-ray absorptiometry (DXA) is the clinical standard for BMD assessment, its limited accessibility hinders diagnosis in resource-limited regions. Opportunistic computed tomography (CT) analysis has emerged as a promising alternative for osteoporosis diagnosis using existing imaging data. Current approaches, however, face three limitations: (1) underutilization of unlabeled vertebral data, (2) systematic bias from device-specific DXA discrepancies, and (3) insufficient integration of clinical knowledge such as spatial BMD distribution patterns. To address these, we propose a unified deep learning framework with three innovations. First, a self-supervised learning method using radiomic representations to leverage unlabeled CT data and preserve bone texture. Second, a Mixture of Experts (MoE) architecture with learned gating mechanisms to enhance cross-device adaptability. Third, a multi-task learning framework integrating osteoporosis diagnosis, BMD regression, and vertebra location prediction. Validated across three clinical sites and an external hospital, our approach demonstrates superior generalizability and accuracy over existing methods for opportunistic osteoporosis screening and diagnosis. </p>
<blockquote>
<p>éª¨è´¨ç–æ¾ç—‡çš„ç‰¹ç‚¹æ˜¯éª¨çŸ¿ç‰©è´¨å¯†åº¦ï¼ˆBMDï¼‰é™ä½å’Œéª¨å¾®è§‚ç»“æ„å—æŸï¼Œå¢åŠ äº†è€å¹´äººç¾¤éª¨æŠ˜çš„é£é™©ã€‚è™½ç„¶åŒèƒ½Xå°„çº¿å¸æ”¶æ³•ï¼ˆDXAï¼‰æ˜¯BMDè¯„ä¼°çš„ä¸´åºŠæ ‡å‡†ï¼Œä½†å…¶æœ‰é™çš„å¯åŠæ€§é˜»ç¢äº†èµ„æºæœ‰é™åœ°åŒºçš„è¯Šæ–­ã€‚åˆ©ç”¨ç°æœ‰æˆåƒæ•°æ®è¿›è¡Œè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰åˆ†æçš„æœºé‡æ€§CTåˆ†æå·²æˆä¸ºè¯Šæ–­éª¨è´¨ç–æ¾ç—‡çš„é¢‡å…·å‰æ™¯çš„æ›¿ä»£æ–¹æ³•ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•é¢ä¸´ä¸‰ä¸ªå±€é™æ€§ï¼šï¼ˆ1ï¼‰æœªå……åˆ†åˆ©ç”¨æœªæ ‡è®°çš„æ¤ä½“æ•°æ®ï¼Œï¼ˆ2ï¼‰ç‰¹å®šè®¾å¤‡DXAå·®å¼‚å¯¼è‡´çš„ç³»ç»Ÿæ€§åè§ï¼Œï¼ˆ3ï¼‰å¯¹ä¸´åºŠçŸ¥è¯†çš„æ•´åˆä¸è¶³ï¼Œå¦‚ç©ºé—´BMDåˆ†å¸ƒæ¨¡å¼ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå…¶ä¸­åŒ…å«ä¸‰é¡¹åˆ›æ–°ã€‚é¦–å…ˆï¼Œä½¿ç”¨æ”¾å°„å­¦è¡¨ç°è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œä»¥åˆ©ç”¨æœªæ ‡è®°çš„CTæ•°æ®å¹¶ä¿æŒéª¨çº¹ç†ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨å¸¦æœ‰å­¦ä¹ é—¨æ§æœºåˆ¶çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„ï¼Œä»¥æé«˜è·¨è®¾å¤‡é€‚åº”æ€§ã€‚ç¬¬ä¸‰ï¼Œä¸€ä¸ªå¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œå°†éª¨è´¨ç–æ¾ç—‡è¯Šæ–­ã€BMDå›å½’å’Œæ¤ä½“ä½ç½®é¢„æµ‹ç›¸ç»“åˆã€‚ç»è¿‡ä¸‰ä¸ªä¸´åºŠç«™ç‚¹å’Œå¤–éƒ¨åŒ»é™¢çš„éªŒè¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœºä¼šæ€§éª¨è´¨ç–æ¾ç—‡ç­›æŸ¥å’Œè¯Šæ–­æ–¹é¢è¡¨ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„é€šç”¨æ€§å’Œå‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20282v1">PDF</a> Accepted by MICCAI 2025</p>
<p><strong>Summary</strong><br>    åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„éª¨è´¨ç–æ¾è¯Šæ–­æ–¹æ³•ï¼Œé‡‡ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ å’Œæ··åˆä¸“å®¶æ¶æ„ï¼Œåˆ©ç”¨CTå½±åƒæ•°æ®è¿›è¡Œåˆ†æï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ï¼Œæé«˜äº†è¯Šæ–­çš„å‡†ç¡®æ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éª¨è´¨ç–æ¾ä»¥éª¨çŸ¿ç‰©è´¨å¯†åº¦ï¼ˆBMDï¼‰é™ä½å’Œéª¨å¾®è§‚ç»“æ„å—æŸä¸ºç‰¹å¾ï¼Œå¢åŠ äº†è€å¹´äººç¾¤éª¨æŠ˜çš„é£é™©ã€‚</li>
<li>ç›®å‰ä¸´åºŠæ ‡å‡†çš„BMDè¯„ä¼°æ–¹æ³•æ˜¯åŒèƒ½Xå°„çº¿å¸æ”¶æ³•ï¼ˆDXAï¼‰ï¼Œä½†å…¶æœ‰é™çš„å¯è®¿é—®æ€§é˜»ç¢äº†èµ„æºæœ‰é™åœ°åŒºçš„è¯Šæ–­ã€‚</li>
<li>æœºä¼šæ€§è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰åˆ†æå·²æˆä¸ºåˆ©ç”¨ç°æœ‰æˆåƒæ•°æ®è¿›è¡Œéª¨è´¨ç–æ¾è¯Šæ–­çš„æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ³•ã€‚</li>
<li>å½“å‰æ–¹æ³•é¢ä¸´ä¸‰ä¸ªå±€é™æ€§ï¼šæœªå……åˆ†åˆ©ç”¨æœªæ ‡è®°çš„æ¤ä½“æ•°æ®ã€è®¾å¤‡ç‰¹å®šçš„DXAå·®å¼‚å¯¼è‡´çš„ç³»ç»Ÿåè§ä»¥åŠæœªèƒ½å……åˆ†ç»“åˆä¸´åºŠçŸ¥è¯†ï¼Œå¦‚ç©ºé—´BMDåˆ†å¸ƒæ¨¡å¼ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸‰é¡¹åˆ›æ–°ï¼šä½¿ç”¨æ”¾å°„ç»„å­¦è¡¨ç¤ºçš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ–¹æ³•ã€åˆ©ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„å¢å¼ºè·¨è®¾å¤‡é€‚åº”æ€§å’Œå¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œæ•´åˆéª¨è´¨ç–æ¾è¯Šæ–­ã€BMDå›å½’å’Œæ¤ä½“ä½ç½®é¢„æµ‹ã€‚</li>
<li>è¯¥æ–¹æ³•å·²åœ¨ä¸‰ä¸ªä¸´åºŠæœºæ„å’Œå¤–éƒ¨åŒ»é™¢è¿›è¡ŒéªŒè¯ï¼Œè¡¨æ˜åœ¨æœºä¼šæ€§éª¨è´¨ç–æ¾ç­›æŸ¥å’Œè¯Šæ–­æ–¹é¢ï¼Œå…¶å‡†ç¡®æ€§å’Œé€šç”¨æ€§ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b797b885735e8aa1c6f0d7397bc286a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d790d46285d3376568049b330eaaf94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1390c2529882fed047e06482a2f65bdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dca3e9ac68b44ccfd1dce6c6bd8811a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc7e86d4b0975ba6270ec8fdb1773e6d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MS-IQA-A-Multi-Scale-Feature-Fusion-Network-for-PET-CT-Image-Quality-Assessment"><a href="#MS-IQA-A-Multi-Scale-Feature-Fusion-Network-for-PET-CT-Image-Quality-Assessment" class="headerlink" title="MS-IQA: A Multi-Scale Feature Fusion Network for PET&#x2F;CT Image Quality   Assessment"></a>MS-IQA: A Multi-Scale Feature Fusion Network for PET&#x2F;CT Image Quality   Assessment</h2><p><strong>Authors:Siqiao Li, Chen Hui, Wei Zhang, Rui Liang, Chenyue Song, Feng Jiang, Haiqi Zhu, Zhixuan Li, Hong Huang, Xiang Li</strong></p>
<p>Positron Emission Tomography &#x2F; Computed Tomography (PET&#x2F;CT) plays a critical role in medical imaging, combining functional and anatomical information to aid in accurate diagnosis. However, image quality degradation due to noise, compression and other factors could potentially lead to diagnostic uncertainty and increase the risk of misdiagnosis. When evaluating the quality of a PET&#x2F;CT image, both low-level features like distortions and high-level features like organ anatomical structures affect the diagnostic value of the image. However, existing medical image quality assessment (IQA) methods are unable to account for both feature types simultaneously. In this work, we propose MS-IQA, a novel multi-scale feature fusion network for PET&#x2F;CT IQA, which utilizes multi-scale features from various intermediate layers of ResNet and Swin Transformer, enhancing its ability of perceiving both local and global information. In addition, a multi-scale feature fusion module is also introduced to effectively combine high-level and low-level information through a dynamically weighted channel attention mechanism. Finally, to fill the blank of PET&#x2F;CT IQA dataset, we construct PET-CT-IQA-DS, a dataset containing 2,700 varying-quality PET&#x2F;CT images with quality scores assigned by radiologists. Experiments on our dataset and the publicly available LDCTIQAC2023 dataset demonstrate that our proposed model has achieved superior performance against existing state-of-the-art methods in various IQA metrics. This work provides an accurate and efficient IQA method for PET&#x2F;CT. Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/MS-IQA/MS-IQA/">https://github.com/MS-IQA/MS-IQA/</a>. </p>
<blockquote>
<p>æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æ&#x2F;è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆPET&#x2F;CTï¼‰åœ¨åŒ»å­¦æˆåƒä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œå®ƒç»“åˆäº†åŠŸèƒ½å’Œè§£å‰–ä¿¡æ¯ï¼Œæœ‰åŠ©äºå‡†ç¡®è¯Šæ–­ã€‚ç„¶è€Œï¼Œç”±äºå™ªå£°ã€å‹ç¼©å’Œå…¶ä»–å› ç´ å¯¼è‡´çš„å›¾åƒè´¨é‡ä¸‹é™å¯èƒ½å¯¼è‡´è¯Šæ–­ä¸ç¡®å®šæ€§å¢åŠ å’Œè¯¯è¯Šé£é™©ã€‚è¯„ä¼°PET&#x2F;CTå›¾åƒè´¨é‡æ—¶ï¼Œä½çº§åˆ«ç‰¹å¾ï¼ˆå¦‚å¤±çœŸï¼‰å’Œé«˜çº§åˆ«ç‰¹å¾ï¼ˆå¦‚å™¨å®˜è§£å‰–ç»“æ„ï¼‰éƒ½ä¼šå½±å“å›¾åƒçš„è¯Šæ–­ä»·å€¼ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŒ»å­¦å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰æ–¹æ³•æ— æ³•åŒæ—¶å…¼é¡¾è¿™ä¸¤ç§ç‰¹å¾ç±»å‹ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MS-IQAï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„PET&#x2F;CT IQAå¤šå°ºåº¦ç‰¹å¾èåˆç½‘ç»œï¼Œå®ƒåˆ©ç”¨ResNetå’ŒSwin Transformerçš„ä¸­é—´å±‚çš„å¤šå°ºåº¦ç‰¹å¾ï¼Œå¢å¼ºäº†æ„ŸçŸ¥å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªå¤šå°ºåº¦ç‰¹å¾èåˆæ¨¡å—ï¼Œé€šè¿‡åŠ¨æ€åŠ æƒé€šé“æ³¨æ„åŠ›æœºåˆ¶æœ‰æ•ˆåœ°ç»“åˆäº†é«˜çº§å’Œä½çº§ä¿¡æ¯ã€‚æœ€åï¼Œä¸ºäº†å¡«è¡¥PET&#x2F;CT IQAæ•°æ®é›†çš„ç©ºç™½ï¼Œæˆ‘ä»¬æ„å»ºäº†PET-CT-IQA-DSæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«2700å¼ è´¨é‡å„å¼‚çš„PET&#x2F;CTå›¾åƒï¼Œå¹¶ç”±æ”¾å°„ç§‘åŒ»ç”Ÿåˆ†é…äº†è´¨é‡è¯„åˆ†ã€‚åœ¨æˆ‘ä»¬è‡ªå·±çš„æ•°æ®é›†å’Œå…¬å¼€å¯ç”¨çš„LDCTIQAC2023æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„IQAæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å„ç§IQAæŒ‡æ ‡ä¸Šå–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œä¸ºPET&#x2F;CTæä¾›äº†ä¸€ç§å‡†ç¡®é«˜æ•ˆçš„IQAæ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MS-IQA/MS-IQA/%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MS-IQA/MS-IQA/è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20200v1">PDF</a> Accepted to MICCAI 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMS-IQAçš„å¤šå°ºåº¦ç‰¹å¾èåˆç½‘ç»œï¼Œç”¨äºPET&#x2F;CTåŒ»å­¦å›¾åƒè´¨é‡è¯„ä¼°ã€‚è¯¥ç½‘ç»œç»“åˆResNetå’ŒSwin Transformerçš„ä¸­é—´å±‚å¤šå°ºåº¦ç‰¹å¾ï¼Œé€šè¿‡åŠ¨æ€åŠ æƒé€šé“æ³¨æ„åŠ›æœºåˆ¶èåˆé«˜ä½çº§ä¿¡æ¯ã€‚ä¸ºå¡«è¡¥PET&#x2F;CTå›¾åƒè´¨é‡è¯„ä¼°æ•°æ®é›†çš„ç©ºç™½ï¼Œæ„å»ºäº†PET-CT-IQA-DSæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨IQAæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PET&#x2F;CTåŒ»å­¦å›¾åƒè´¨é‡è¯„ä¼°è‡³å…³é‡è¦ï¼Œå½±å“è¯Šæ–­å‡†ç¡®æ€§å’Œé£é™©ã€‚</li>
<li>ç°æœ‰åŒ»å­¦å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰æ–¹æ³•æ— æ³•åŒæ—¶è€ƒè™‘é«˜ä½çº§ç‰¹å¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šå°ºåº¦ç‰¹å¾èåˆç½‘ç»œMS-IQAï¼Œç»“åˆResNetå’ŒSwin Transformerçš„ä¸­é—´å±‚ç‰¹å¾ã€‚</li>
<li>å¼•å…¥å¤šå°ºåº¦ç‰¹å¾èåˆæ¨¡å—ï¼Œé€šè¿‡åŠ¨æ€åŠ æƒé€šé“æ³¨æ„åŠ›æœºåˆ¶èåˆé«˜ä½çº§ä¿¡æ¯ã€‚</li>
<li>æ„å»ºäº†PET-CT-IQA-DSæ•°æ®é›†ï¼ŒåŒ…å«ç”±æ”¾å°„ç§‘åŒ»ç”Ÿè¯„åˆ†çš„è´¨é‡ä¸åŒçš„PET&#x2F;CTå›¾åƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMS-IQAæ¨¡å‹åœ¨IQAæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20200">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b5e7aa74f768cb526c589e24857773ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89c126dc5662f9d242165009d478755c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c8044b1a66695397ddb99add8e7057c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="VoxelOpt-Voxel-Adaptive-Message-Passing-for-Discrete-Optimization-in-Deformable-Abdominal-CT-Registration"><a href="#VoxelOpt-Voxel-Adaptive-Message-Passing-for-Discrete-Optimization-in-Deformable-Abdominal-CT-Registration" class="headerlink" title="VoxelOpt: Voxel-Adaptive Message Passing for Discrete Optimization in   Deformable Abdominal CT Registration"></a>VoxelOpt: Voxel-Adaptive Message Passing for Discrete Optimization in   Deformable Abdominal CT Registration</h2><p><strong>Authors:Hang Zhang, Yuxi Zhang, Jiazheng Wang, Xiang Chen, Renjiu Hu, Xin Tian, Gaolei Li, Min Liu</strong></p>
<p>Recent developments in neural networks have improved deformable image registration (DIR) by amortizing iterative optimization, enabling fast and accurate DIR results. However, learning-based methods often face challenges with limited training data, large deformations, and tend to underperform compared to iterative approaches when label supervision is unavailable. While iterative methods can achieve higher accuracy in such scenarios, they are considerably slower than learning-based methods. To address these limitations, we propose VoxelOpt, a discrete optimization-based DIR framework that combines the strengths of learning-based and iterative methods to achieve a better balance between registration accuracy and runtime. VoxelOpt uses displacement entropy from local cost volumes to measure displacement signal strength at each voxel, which differs from earlier approaches in three key aspects. First, it introduces voxel-wise adaptive message passing, where voxels with lower entropy receives less influence from their neighbors. Second, it employs a multi-level image pyramid with 27-neighbor cost volumes at each level, avoiding exponential complexity growth. Third, it replaces hand-crafted features or contrastive learning with a pretrained foundational segmentation model for feature extraction. In abdominal CT registration, these changes allow VoxelOpt to outperform leading iterative in both efficiency and accuracy, while matching state-of-the-art learning-based methods trained with label supervision. The source code will be available at <a target="_blank" rel="noopener" href="https://github.com/tinymilky/VoxelOpt">https://github.com/tinymilky/VoxelOpt</a> </p>
<blockquote>
<p>è¿‘æœŸç¥ç»ç½‘ç»œçš„å‘å±•é€šè¿‡æ‘Šé”€è¿­ä»£ä¼˜åŒ–æ”¹å–„äº†å¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDIRï¼‰ï¼Œå®ç°äº†å¿«é€Ÿä¸”å‡†ç¡®çš„DIRç»“æœã€‚ç„¶è€Œï¼ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•å¸¸å¸¸é¢ä¸´è®­ç»ƒæ•°æ®æœ‰é™ã€å¤§å˜å½¢ä»¥åŠåœ¨æ²¡æœ‰æ ‡ç­¾ç›‘ç£çš„æƒ…å†µä¸‹ç›¸æ¯”è¿­ä»£æ–¹æ³•è¡¨ç°è¾ƒå·®çš„æŒ‘æˆ˜ã€‚è™½ç„¶è¿­ä»£æ–¹æ³•åœ¨è¿™ç§åœºæ™¯ä¸‹å¯ä»¥è¾¾åˆ°æ›´é«˜çš„å‡†ç¡®åº¦ï¼Œä½†å®ƒä»¬çš„é€Ÿåº¦æ˜æ˜¾æ…¢äºåŸºäºå­¦ä¹ çš„æ–¹æ³•ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†VoxelOptï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºç¦»æ•£ä¼˜åŒ–çš„DIRæ¡†æ¶ï¼Œå®ƒç»“åˆäº†åŸºäºå­¦ä¹ å’Œè¿­ä»£æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œå®ç°äº†é…å‡†ç²¾åº¦å’Œè¿è¡Œæ—¶ä¹‹é—´çš„æ›´å¥½å¹³è¡¡ã€‚VoxelOptä½¿ç”¨å±€éƒ¨æˆæœ¬ä½“ç§¯çš„ä½ç§»ç†µæ¥æµ‹é‡æ¯ä¸ªä½“ç´ çš„ä½ç§»ä¿¡å·å¼ºåº¦ï¼Œè¿™åœ¨ä¸‰ä¸ªæ–¹é¢ä¸æ—©æœŸçš„æ–¹æ³•ä¸åŒã€‚é¦–å…ˆï¼Œå®ƒå¼•å…¥äº†ä½“ç´ çº§çš„è‡ªé€‚åº”æ¶ˆæ¯ä¼ é€’ï¼Œå…¶ä¸­ä½ç†µçš„ä½“ç´ å—åˆ°æ¥è‡ªé‚»å±…çš„å½±å“è¾ƒå°ã€‚å…¶æ¬¡ï¼Œå®ƒé‡‡ç”¨å¤šçº§åˆ«å›¾åƒé‡‘å­—å¡”ï¼Œæ¯ä¸€çº§åˆ«ä½¿ç”¨27ä¸ªé‚»å±…æˆæœ¬ä½“ç§¯ï¼Œé¿å…äº†æŒ‡æ•°çº§çš„å¤æ‚æ€§å¢é•¿ã€‚ç¬¬ä¸‰ï¼Œå®ƒç”¨é¢„è®­ç»ƒçš„åŸºç¡€åˆ†å‰²æ¨¡å‹æ›¿æ¢æ‰‹å·¥ç‰¹å¾æˆ–å¯¹æ¯”å­¦ä¹ æ¥è¿›è¡Œç‰¹å¾æå–ã€‚åœ¨è…¹éƒ¨CTé…å‡†ä¸­ï¼Œè¿™äº›æ”¹å˜ä½¿VoxelOptåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†é¢†å…ˆçš„è¿­ä»£æ–¹æ³•ï¼ŒåŒæ—¶åŒ¹é…äº†ä½¿ç”¨æ ‡ç­¾ç›‘ç£è®­ç»ƒçš„æœ€æ–°å­¦ä¹ æ–¹æ³•çš„æ°´å¹³ã€‚æºä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/tinymilky/VoxelOpt">https://github.com/tinymilky/VoxelOpt</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19975v1">PDF</a> Accepted for publication at MICCAI 2025</p>
<p><strong>Summary</strong><br>     ç¥ç»ç½‘ç»œæœ€æ–°å‘å±•é€šè¿‡æ‘Šé”€è¿­ä»£ä¼˜åŒ–æ”¹å–„äº†å¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDIRï¼‰ï¼Œå®ç°äº†å¿«é€Ÿä¸”å‡†ç¡®çš„DIRç»“æœã€‚VoxelOptæ¡†æ¶ç»“åˆå­¦ä¹ æ–¹æ³•å’Œè¿­ä»£æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œåœ¨é…å‡†å‡†ç¡®æ€§å’Œè¿è¡Œæ—¶é—´ä¹‹é—´å–å¾—æ›´å¥½å¹³è¡¡ã€‚å®ƒé‡‡ç”¨å±€éƒ¨æˆæœ¬ä½“ç§¯çš„ä½ç§»ç†µæ¥æµ‹é‡æ¯ä¸ªä½“ç´ çš„ä½ç§»ä¿¡å·å¼ºåº¦ï¼Œå…·æœ‰ä¸‰ä¸ªå…³é”®ç‰¹ç‚¹ã€‚åœ¨è…¹éƒ¨CTé…å‡†ä¸­ï¼ŒVoxelOptæé«˜äº†æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œä¸æœ‰æ ‡ç­¾ç›‘ç£è®­ç»ƒçš„é¡¶å°–å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½ç›¸åŒ¹é…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œæ”¹å–„å¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDIRï¼‰ï¼Œå®ç°å¿«é€Ÿå‡†ç¡®ç»“æœã€‚</li>
<li>VoxelOptæ¡†æ¶ç»“åˆäº†å­¦ä¹ æ–¹æ³•å’Œè¿­ä»£æ–¹æ³•çš„ä¼˜ç‚¹ã€‚</li>
<li>VoxelOpté‡‡ç”¨å±€éƒ¨æˆæœ¬ä½“ç§¯çš„ä½ç§»ç†µæ¥æµ‹é‡ä½ç§»ä¿¡å·å¼ºåº¦ã€‚</li>
<li>VoxelOptå…·æœ‰ä¸‰ä¸ªå…³é”®ç‰¹ç‚¹ï¼šåƒç´ çº§çš„è‡ªé€‚åº”ä¿¡æ¯ä¼ é€’ã€å¤šå±‚æ¬¡çš„å›¾åƒé‡‘å­—å¡”ã€ä½¿ç”¨é¢„è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹è¿›è¡Œç‰¹å¾æå–ã€‚</li>
<li>VoxelOptåœ¨è…¹éƒ¨CTé…å‡†ä¸­è¡¨ç°å‡ºè‰²ï¼Œæé«˜äº†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>VoxelOptæ€§èƒ½ä¸æœ‰æ ‡ç­¾ç›‘ç£è®­ç»ƒçš„é¡¶å°–å­¦ä¹ æ–¹æ³•ç›¸åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a509089b9351b85951b6e6847e2f092.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85e6e690137da0f6236069e43bbea602.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bec3ff526c6e5a569a1bb831b3c8f669.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LKA-Large-Kernel-Adapter-for-Enhanced-Medical-Image-Classification"><a href="#LKA-Large-Kernel-Adapter-for-Enhanced-Medical-Image-Classification" class="headerlink" title="LKA: Large Kernel Adapter for Enhanced Medical Image Classification"></a>LKA: Large Kernel Adapter for Enhanced Medical Image Classification</h2><p><strong>Authors:Ziquan Zhu, Si-Yuan Lu, Tianjin Huang, Lu Liu, Zhe Liu</strong></p>
<p>Despite the notable success of current Parameter-Efficient Fine-Tuning (PEFT) methods across various domains, their effectiveness on medical datasets falls short of expectations. This limitation arises from two key factors: (1) medical images exhibit extensive anatomical variation and low contrast, necessitating a large receptive field to capture critical features, and (2) existing PEFT methods do not explicitly address the enhancement of receptive fields. To overcome these challenges, we propose the Large Kernel Adapter (LKA), designed to expand the receptive field while maintaining parameter efficiency. The proposed LKA consists of three key components: down-projection, channel-wise large kernel convolution, and up-projection. Through extensive experiments on various datasets and pre-trained models, we demonstrate that the incorporation of a larger kernel size is pivotal in enhancing the adaptation of pre-trained models for medical image analysis. Our proposed LKA outperforms 11 commonly used PEFT methods, surpassing the state-of-the-art by 3.5% in top-1 accuracy across five medical datasets. </p>
<blockquote>
<p>å°½ç®¡å½“å‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨ä¸åŒé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨åŒ»å­¦æ•°æ®é›†ä¸Šçš„æ•ˆæœå´ä¸å°½å¦‚äººæ„ã€‚è¿™ä¸€å±€é™æ€§æºäºä¸¤ä¸ªå…³é”®å› ç´ ï¼šï¼ˆ1ï¼‰åŒ»å­¦å›¾åƒè¡¨ç°å‡ºå¹¿æ³›çš„è§£å‰–å˜å¼‚å’Œä½å¯¹æ¯”åº¦ï¼Œéœ€è¦è¾ƒå¤§çš„æ„Ÿå—é‡æ¥æ•æ‰å…³é”®ç‰¹å¾ï¼›ï¼ˆ2ï¼‰ç°æœ‰çš„PEFTæ–¹æ³•æ²¡æœ‰æ˜ç¡®åœ°è§£å†³æ„Ÿå—é‡å¢å¼ºçš„é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤§å‹å†…æ ¸é€‚é…å™¨ï¼ˆLKAï¼‰ï¼Œæ—¨åœ¨åœ¨ä¿æŒå‚æ•°æ•ˆç‡çš„åŒæ—¶æ‰©å¤§æ„Ÿå—é‡ã€‚æ‰€æå‡ºçš„LKAç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šä¸‹æŠ•å½±ã€é€šé“å¤§å‹å†…æ ¸å·ç§¯å’Œä¸ŠæŠ•å½±ã€‚é€šè¿‡å¯¹å„ç§æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨æ›´å¤§çš„å†…æ ¸å¤§å°å¯¹äºå¢å¼ºé¢„è®­ç»ƒæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é€‚åº”æ€§è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºçš„å¤§å‹å†…æ ¸é€‚é…å™¨ï¼ˆLKAï¼‰åœ¨äº”ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šè¶…è¶Šäº†11ç§å¸¸ç”¨çš„PEFTæ–¹æ³•ï¼Œåœ¨top-1å‡†ç¡®ç‡ä¸Šè¶…è¿‡äº†ç°æœ‰æœ€ä½³æ°´å¹³3.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19118v2">PDF</a> Some aspects of the experimental setup were not clearly described in   the current version. We plan to revise and clarify these points before   resubmitting</p>
<p><strong>Summary</strong><br>     å½“å‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨å¤šé¢†åŸŸè¡¨ç°å‡ºæ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨åŒ»å­¦æ•°æ®é›†ä¸Šæ•ˆæœæ¬ ä½³ã€‚é’ˆå¯¹åŒ»å­¦å›¾åƒçš„ç‰¹ç‚¹å’Œç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œæå‡ºäº†å¤§å‹å†…æ ¸é€‚é…å™¨ï¼ˆLKAï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ‰©å±•æ„Ÿå—é‡å’Œæé«˜å‚æ•°æ•ˆç‡æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç»è¿‡å¤šä¸ªæ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹çš„å®éªŒéªŒè¯ï¼ŒLKAæ–¹æ³•è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸè¶…è¶Šäº†11ç§å¸¸ç”¨çš„PEFTæ–¹æ³•ï¼Œå¹¶åœ¨äº”ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„top-1å‡†ç¡®ç‡æé«˜äº†3.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰PEFTæ–¹æ³•åœ¨åŒ»å­¦æ•°æ®é›†ä¸Šçš„æ•ˆæœæœ‰é™ï¼Œä¸»è¦ç”±äºåŒ»å­¦å›¾åƒå­˜åœ¨å¹¿æ³›çš„è§£å‰–å˜å¼‚å’Œä½å¯¹æ¯”åº¦ï¼Œéœ€è¦æ›´å¤§çš„æ„Ÿå—é‡æ¥æ•æ‰å…³é”®ç‰¹å¾ã€‚</li>
<li>ç°æœ‰PEFTæ–¹æ³•æœªæ˜ç¡®è§£å†³æ„Ÿå—é‡å¢å¼ºé—®é¢˜ã€‚</li>
<li>é’ˆå¯¹åŒ»å­¦å›¾åƒçš„ç‰¹ç‚¹ï¼Œæå‡ºäº†Large Kernel Adapterï¼ˆLKAï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨æ‰©å¤§æ„Ÿå—é‡å¹¶ä¿æŒå‚æ•°æ•ˆç‡ã€‚</li>
<li>LKAæ–¹æ³•åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šä¸‹æŠ•å½±ã€é€šé“å¤§å‹å†…æ ¸å·ç§¯å’Œä¸ŠæŠ•å½±ã€‚</li>
<li>é€šè¿‡å¤šä¸ªæ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹çš„å®éªŒéªŒè¯ï¼ŒLKAæ–¹æ³•æ˜¾è‘—æé«˜äº†åŒ»å­¦å›¾åƒåˆ†æçš„é€‚åº”æ€§ã€‚</li>
<li>LKAæ–¹æ³•åœ¨äº”ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„top-1å‡†ç¡®ç‡è¶…è¿‡äº†ç°æœ‰æœ€å…ˆè¿›çš„PEFTæ–¹æ³•ï¼Œæé«˜äº†3.5%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-73619839500ded2ef30b7caf04d24e18.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbec7e26dc8fb1af5f39c9886ac4b020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60d3822294460fec056aef4a4ced486a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55936770fbc3ceffe9ede2093420c107.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LVPNet-A-Latent-variable-based-Prediction-driven-End-to-end-Framework-for-Lossless-Compression-of-Medical-Images"><a href="#LVPNet-A-Latent-variable-based-Prediction-driven-End-to-end-Framework-for-Lossless-Compression-of-Medical-Images" class="headerlink" title="LVPNet: A Latent-variable-based Prediction-driven End-to-end Framework   for Lossless Compression of Medical Images"></a>LVPNet: A Latent-variable-based Prediction-driven End-to-end Framework   for Lossless Compression of Medical Images</h2><p><strong>Authors:Chenyue Song, Chen Hui, Qing Lin, Wei Zhang, Siqiao Li, Haiqi Zhu, Zhixuan Li, Shengping Zhang, Shaohui Liu, Feng Jiang, Xiang Li</strong></p>
<p>Autoregressive Initial Bits is a framework that integrates sub-image autoregression and latent variable modeling, demonstrating its advantages in lossless medical image compression. However, in existing methods, the image segmentation process leads to an even distribution of latent variable information across each sub-image, which in turn causes posterior collapse and inefficient utilization of latent variables. To deal with these issues, we propose a prediction-based end-to-end lossless medical image compression method named LVPNet, leveraging global latent variables to predict pixel values and encoding predicted probabilities for lossless compression. Specifically, we introduce the Global Multi-scale Sensing Module (GMSM), which extracts compact and informative latent representations from the entire image, effectively capturing spatial dependencies within the latent space. Furthermore, to mitigate the information loss introduced during quantization, we propose the Quantization Compensation Module (QCM), which learns the distribution of quantization errors and refines the quantized features to compensate for quantization loss. Extensive experiments on challenging benchmarks demonstrate that our method achieves superior compression efficiency compared to state-of-the-art lossless image compression approaches, while maintaining competitive inference speed. The code is at <a target="_blank" rel="noopener" href="https://github.com/scy-Jackel/LVPNet">https://github.com/scy-Jackel/LVPNet</a>. </p>
<blockquote>
<p>Autoregressive Initial Bitsæ˜¯ä¸€ä¸ªèåˆäº†å­å›¾åƒè‡ªå›å½’å’Œæ½œåœ¨å˜é‡å»ºæ¨¡çš„æ¡†æ¶ï¼Œåœ¨æ— æŸåŒ»å­¦å›¾åƒå‹ç¼©ä¸­å±•ç¤ºäº†å…¶ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œåœ¨ç°æœ‰æ–¹æ³•ä¸­ï¼Œå›¾åƒåˆ†å‰²è¿‡ç¨‹å¯¼è‡´æ½œåœ¨å˜é‡ä¿¡æ¯å‡åŒ€åˆ†å¸ƒåœ¨æ¯ä¸ªå­å›¾åƒä¸­ï¼Œè¿™è¿›è€Œå¯¼è‡´åå´©æºƒå’Œæ½œåœ¨å˜é‡çš„ä½æ•ˆåˆ©ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé¢„æµ‹çš„ç«¯åˆ°ç«¯æ— æŸåŒ»å­¦å›¾åƒå‹ç¼©æ–¹æ³•ï¼Œåä¸ºLVPNetã€‚è¯¥æ–¹æ³•åˆ©ç”¨å…¨å±€æ½œåœ¨å˜é‡æ¥é¢„æµ‹åƒç´ å€¼ï¼Œå¹¶å¯¹é¢„æµ‹æ¦‚ç‡è¿›è¡Œç¼–ç ä»¥å®ç°æ— æŸå‹ç¼©ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨å±€å¤šå°ºåº¦æ„ŸçŸ¥æ¨¡å—ï¼ˆGMSMï¼‰ï¼Œè¯¥æ¨¡å—ä»æ•´ä¸ªå›¾åƒä¸­æå–ç´§å‡‘ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ½œåœ¨è¡¨ç¤ºï¼Œæœ‰æ•ˆåœ°æ•è·æ½œåœ¨ç©ºé—´ä¸­çš„ç©ºé—´ä¾èµ–æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡è½»é‡åŒ–è¿‡ç¨‹ä¸­å¼•å…¥çš„ä¿¡æ¯æŸå¤±ï¼Œæˆ‘ä»¬æå‡ºäº†é‡åŒ–è¡¥å¿æ¨¡å—ï¼ˆQCMï¼‰ï¼Œè¯¥æ¨¡å—å­¦ä¹ é‡åŒ–è¯¯å·®çš„åˆ†å¸ƒï¼Œå¹¶ç»†åŒ–é‡åŒ–ç‰¹å¾ä»¥è¡¥å¿é‡åŒ–æŸå¤±ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€æ–°çš„æ— æŸå›¾åƒå‹ç¼©æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„å‹ç¼©æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„æ¨ç†é€Ÿåº¦ã€‚ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/scy-Jackel/LVPNet%E3%80%82">https://github.com/scy-Jackel/LVPNetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17983v2">PDF</a> Accepted to MICCAI 2025</p>
<p><strong>Summary</strong><br>     æå‡ºçš„LVPNetåˆ©ç”¨å…¨å±€æ½œå˜é‡é¢„æµ‹åƒç´ å€¼å¹¶ç¼–ç é¢„æµ‹æ¦‚ç‡ä»¥å®ç°æ— æŸå‹ç¼©ï¼Œé€šè¿‡Global Multi-scale Sensing Moduleï¼ˆGMSMï¼‰å’ŒQuantization Compensation Moduleï¼ˆQCMï¼‰æé«˜å‹ç¼©æ•ˆç‡å’Œå‡å°‘é‡åŒ–æŸå¤±ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Autoregressive Initial Bitsæ¡†æ¶ç»“åˆäº†å­å›¾åƒè‡ªå›å½’å’Œæ½œå˜é‡å»ºæ¨¡ï¼Œç”¨äºæ— æŸåŒ»å­¦å›¾åƒå‹ç¼©ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸­çš„å›¾åƒåˆ†å‰²è¿‡ç¨‹å¯¼è‡´æ½œå˜é‡ä¿¡æ¯åœ¨å­å›¾åƒé—´çš„å‡åŒ€åˆ†å¸ƒï¼Œè¿™ä¼šå¼•èµ·åéªŒå´©æºƒå’Œæ½œå˜é‡åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>LVPNetåˆ©ç”¨å…¨å±€æ½œå˜é‡é¢„æµ‹åƒç´ å€¼ï¼Œå¹¶æå‡ºä¸€ç§åŸºäºé¢„æµ‹çš„æ— æŸåŒ»å­¦å›¾åƒå‹ç¼©æ–¹æ³•ã€‚</li>
<li>GMSMæ¨¡å—èƒ½å¤Ÿä»æ•´ä¸ªå›¾åƒä¸­æå–ç´§å‡‘ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ½œå˜é‡è¡¨ç¤ºï¼Œæœ‰æ•ˆåœ°æ•è·æ½œç©ºé—´ä¸­çš„ç©ºé—´ä¾èµ–æ€§ã€‚</li>
<li>QCMæ¨¡å—å­¦ä¹ é‡åŒ–è¯¯å·®çš„åˆ†å¸ƒï¼Œå¹¶ç»†åŒ–é‡åŒ–ç‰¹å¾ä»¥è¡¥å¿é‡åŒ–æŸå¤±ï¼Œå‡è½»é‡åŒ–è¿‡ç¨‹ä¸­å¼•å…¥çš„ä¿¡æ¯æŸå¤±ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLVPNetåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†è¾ƒé«˜çš„å‹ç¼©æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›çš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>LVPNetçš„ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d51507a6dbfa3df093053a3efc50555.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1be60f74d4cdcb120c4380ffb4ae4aa4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64dfc19febbf337ccb5b87909a2fa1ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-21404e2e1b50294969658f73fe7a7082.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CLAIM-Clinically-Guided-LGE-Augmentation-for-Realistic-and-Diverse-Myocardial-Scar-Synthesis-and-Segmentation"><a href="#CLAIM-Clinically-Guided-LGE-Augmentation-for-Realistic-and-Diverse-Myocardial-Scar-Synthesis-and-Segmentation" class="headerlink" title="CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse   Myocardial Scar Synthesis and Segmentation"></a>CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse   Myocardial Scar Synthesis and Segmentation</h2><p><strong>Authors:Farheen Ramzan, Yusuf Kiberu, Nikesh Jathanna, Shahnaz Jamil-Copley, Richard H. Clayton, Chen Chen</strong></p>
<p>Deep learning-based myocardial scar segmentation from late gadolinium enhancement (LGE) cardiac MRI has shown great potential for accurate and timely diagnosis and treatment planning for structural cardiac diseases. However, the limited availability and variability of LGE images with high-quality scar labels restrict the development of robust segmentation models. To address this, we introduce CLAIM: \textbf{C}linically-Guided \textbf{L}GE \textbf{A}ugmentation for Real\textbf{i}stic and Diverse \textbf{M}yocardial Scar Synthesis and Segmentation framework, a framework for anatomically grounded scar generation and segmentation. At its core is the SMILE module (Scar Mask generation guided by cLinical knowledgE), which conditions a diffusion-based generator on the clinically adopted AHA 17-segment model to synthesize images with anatomically consistent and spatially diverse scar patterns. In addition, CLAIM employs a joint training strategy in which the scar segmentation network is optimized alongside the generator, aiming to enhance both the realism of synthesized scars and the accuracy of the scar segmentation performance. Experimental results show that CLAIM produces anatomically coherent scar patterns and achieves higher Dice similarity with real scar distributions compared to baseline models. Our approach enables controllable and realistic myocardial scar synthesis and has demonstrated utility for downstream medical imaging task. Code is available at <a target="_blank" rel="noopener" href="https://github.com/farheenjabeen/CLAIM-Scar-Synthesis">https://github.com/farheenjabeen/CLAIM-Scar-Synthesis</a>. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„æ™šæœŸé’†å¢å¼ºï¼ˆLGEï¼‰å¿ƒè„MRIå¿ƒè‚Œç–¤ç—•åˆ†å‰²åœ¨ç»“æ„æ€§å¿ƒè„ç–¾ç—…çš„å‡†ç¡®åŠæ—¶è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œé«˜è´¨é‡ç–¤ç—•æ ‡ç­¾çš„LGEå›¾åƒæœ‰é™ä¸”å­˜åœ¨å¯å˜æ€§é—®é¢˜ï¼Œé™åˆ¶äº†ç¨³å¥åˆ†å‰²æ¨¡å‹çš„å‘å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CLAIMï¼šç”¨äºç°å®å’Œå¤šæ ·åŒ–å¿ƒè‚Œç–¤ç—•åˆæˆå’Œåˆ†å‰²çš„ä¸´åºŠæŒ‡å¯¼LGEå¢å¼ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯SMILEæ¨¡å—ï¼ˆç”±ä¸´åºŠçŸ¥è¯†å¼•å¯¼çš„ç–¤ç—•æ©è†œç”Ÿæˆï¼‰ï¼Œè¯¥æ¨¡å—ä»¥ä¸´åºŠä¸Šé‡‡ç”¨çš„AHA 1 7æ®µæ¨¡å‹ä¸ºæ¡ä»¶ï¼ŒåŸºäºæ‰©æ•£ç”Ÿæˆå™¨åˆæˆè§£å‰–ä¸Šä¸€è‡´ä¸”ç©ºé—´å¤šæ ·çš„ç–¤ç—•æ¨¡å¼å›¾åƒã€‚æ­¤å¤–ï¼ŒCLAIMé‡‡ç”¨è”åˆè®­ç»ƒç­–ç•¥ï¼Œä¼˜åŒ–ç–¤ç—•åˆ†å‰²ç½‘ç»œä¸ç”Ÿæˆå™¨ï¼Œæ—¨åœ¨æé«˜åˆæˆç–¤ç—•çš„çœŸå®æ€§å’Œç–¤ç—•åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLAIMäº§ç”Ÿè§£å‰–ä¸Šè¿è´¯çš„ç–¤ç—•æ¨¡å¼ï¼Œä¸çœŸå®ç–¤ç—•åˆ†å¸ƒç›¸æ¯”ï¼Œä¸åŸºçº¿æ¨¡å‹çš„Diceç›¸ä¼¼æ€§æ›´é«˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°å¯æ§ä¸”ç°å®çš„å¿ƒè‚Œç–¤ç—•åˆæˆï¼Œå¹¶å·²è¯æ˜å…¶åœ¨ä¸‹æ¸¸åŒ»å­¦æˆåƒä»»åŠ¡ä¸­çš„å®ç”¨æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/farheenjabeen/CLAIM-Scar-Synthesis%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/farheenjabeen/CLAIM-Scar-Synthesisæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15549v2">PDF</a> 14 Pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„å¿ƒè‚Œç˜¢ç—•åˆ†å‰²æŠ€æœ¯ä»æ™šæœŸé•“å¢å¼ºï¼ˆLGEï¼‰å¿ƒè„MRIä¸­å±•ç°å‡ºå¯¹ç»“æ„æ€§å¿ƒè„ç—…ç²¾å‡†åŠæ—¶è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’åˆ¶å®šçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œé«˜è´¨é‡ç˜¢ç—•æ ‡ç­¾çš„LGEå›¾åƒæœ‰é™ä¸”å­˜åœ¨å¯å˜æ€§é—®é¢˜ï¼Œåˆ¶çº¦äº†ç¨³å¥åˆ†å‰²æ¨¡å‹çš„å‘å±•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºCLAIMæ¡†æ¶ï¼Œç”¨äºä¸´åºŠæŒ‡å¯¼çš„LGEå¢å¼ºå’Œåˆæˆå¿ƒè‚Œç˜¢ç—•åˆ†å‰²ã€‚å…¶æ ¸å¿ƒSMILEæ¨¡å—æ ¹æ®AHA 17æ®µæ¨¡å‹é‡‡ç”¨æ‰©æ•£ç”Ÿæˆå™¨ç”Ÿæˆè§£å‰–ç»“æ„ä¸€è‡´çš„ç˜¢ç—•å›¾åƒã€‚æ­¤å¤–ï¼ŒCLAIMé‡‡ç”¨è”åˆè®­ç»ƒç­–ç•¥ä¼˜åŒ–ç”Ÿæˆå™¨å’Œç˜¢ç—•åˆ†å‰²ç½‘ç»œï¼Œæ—¨åœ¨æé«˜åˆæˆç˜¢ç—•çš„çœŸå®æ€§å’Œç˜¢ç—•åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒCLAIMç”Ÿæˆè§£å‰–ç»“æ„è¿è´¯çš„ç˜¢ç—•æ¨¡å¼ï¼Œå¹¶å®ç°äº†æ›´é«˜çš„Diceç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯å®ç°å¯æ§ä¸”çœŸå®çš„å¿ƒè‚Œç˜¢ç—•åˆæˆï¼Œä¸ºä¸‹æ¸¸åŒ»å­¦æˆåƒä»»åŠ¡æä¾›å®ç”¨åŠŸèƒ½ã€‚ç›¸å…³ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/farheenjabeen/CLAIM-Scar-Synthesis">https://github.com/farheenjabeen/CLAIM-Scar-Synthesis</a> è·å¾—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ·±å­¦ä¹ åœ¨å¿ƒè‚Œç˜¢ç—•åˆ†å‰²æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¿ƒè„MRIä¸­ã€‚</li>
<li>LGEå›¾åƒçš„è´¨é‡å’Œå¯ç”¨æ€§é™åˆ¶äº†åˆ†å‰²æ¨¡å‹çš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼šCLAIMï¼Œç»“åˆäº†ä¸´åºŠæŒ‡å¯¼çš„ç–¤ç—•åˆæˆå’Œåˆ†å‰²æŠ€æœ¯ã€‚</li>
<li>æ ¸å¿ƒæ¨¡å—SMILEåŸºäºAHAæ¨¡å‹ç”Ÿæˆè§£å‰–ç»“æ„ä¸€è‡´çš„ç–¤ç—•å›¾åƒã€‚</li>
<li>è”åˆè®­ç»ƒç­–ç•¥æé«˜äº†åˆæˆç–¤ç—•çš„çœŸå®æ€§å’Œåˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒCLAIMç”Ÿæˆçš„ç–¤ç—•æ¨¡å¼æ›´è¿è´¯ä¸”æ›´å‡†ç¡®ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0dd60edd147ee2073f47d2d7d04e9756.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c8ec43e91eb1328db9ed61151def060.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bc34478c3174c17e09d886bf60d8d81.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="C3S3-Complementary-Competition-and-Contrastive-Selection-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#C3S3-Complementary-Competition-and-Contrastive-Selection-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="C3S3: Complementary Competition and Contrastive Selection for   Semi-Supervised Medical Image Segmentation"></a>C3S3: Complementary Competition and Contrastive Selection for   Semi-Supervised Medical Image Segmentation</h2><p><strong>Authors:Jiaying He, Yitong Lin, Jiahe Chen, Honghui Xu, Jianwei Zheng</strong></p>
<p>For the immanent challenge of insufficiently annotated samples in the medical field, semi-supervised medical image segmentation (SSMIS) offers a promising solution. Despite achieving impressive results in delineating primary target areas, most current methodologies struggle to precisely capture the subtle details of boundaries. This deficiency often leads to significant diagnostic inaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised segmentation model that synergistically integrates complementary competition and contrastive selection. This design significantly sharpens boundary delineation and enhances overall precision. Specifically, we develop an Outcome-Driven Contrastive Learning module dedicated to refining boundary localization. Additionally, we incorporate a Dynamic Complementary Competition module that leverages two high-performing sub-networks to generate pseudo-labels, thereby further improving segmentation quality. The proposed C3S3 undergoes rigorous validation on two publicly accessible datasets, encompassing the practices of both MRI and CT scans. The results demonstrate that our method achieves superior performance compared to previous cutting-edge competitors. Especially, on the 95HD and ASD metrics, our approach achieves a notable improvement of at least 6%, highlighting the significant advancements. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Y-TARL/C3S3">https://github.com/Y-TARL/C3S3</a>. </p>
<blockquote>
<p>é’ˆå¯¹åŒ»å­¦é¢†åŸŸæ ‡æ³¨æ ·æœ¬ä¸è¶³è¿™ä¸€ç´§è¿«æŒ‘æˆ˜ï¼ŒåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰æä¾›äº†ä¸€ç§å‰æ™¯å¹¿é˜”çš„è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡åœ¨å‹¾ç”»ä¸»è¦ç›®æ ‡åŒºåŸŸæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†å¤§å¤šæ•°å½“å‰çš„æ–¹æ³•åœ¨ç²¾ç¡®æ•æ‰è¾¹ç•Œçš„ç»†å¾®ç»†èŠ‚æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚è¿™ç§ç¼ºé™·å¾€å¾€å¯¼è‡´è¯Šæ–­å‡ºç°é‡å¤§è¯¯å·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†C3S3ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹åŠç›‘ç£åˆ†å‰²æ¨¡å‹ï¼ŒååŒèåˆäº†äº’è¡¥ç«äº‰å’Œå¯¹æ¯”é€‰æ‹©ã€‚è¿™ç§è®¾è®¡æ˜¾è‘—æé«˜äº†è¾¹ç•Œçš„å‹¾ç”»ç²¾åº¦ï¼Œå¹¶æé«˜äº†æ•´ä½“ç²¾åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç»“æœé©±åŠ¨çš„å¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼Œä¸“é—¨ç”¨äºä¼˜åŒ–è¾¹ç•Œå®šä½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŠ å…¥äº†ä¸€ä¸ªåŠ¨æ€äº’è¡¥ç«äº‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨ä¸¤ä¸ªé«˜æ€§èƒ½å­ç½‘ç»œæ¥ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜åˆ†å‰²è´¨é‡ã€‚æ‰€æå‡ºçš„C3S3åœ¨ä¸¤ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸¥æ ¼éªŒè¯ï¼Œæ¶µç›–äº†MRIå’ŒCTæ‰«æçš„å®è·µã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä¹‹å‰çš„å°–ç«¯ç«äº‰å¯¹æ‰‹ç›¸æ¯”ï¼Œå®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚ç‰¹åˆ«åœ°ï¼Œåœ¨95HDå’ŒASDæŒ‡æ ‡ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†è‡³å°‘6%çš„æ˜¾è‘—æ”¹è¿›ï¼Œçªæ˜¾äº†é‡å¤§è¿›å±•ã€‚ä»£ç å¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/Y-TARL/C3S3%E3%80%82">https://github.com/Y-TARL/C3S3ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07368v2">PDF</a> Accepted to ICME 2025</p>
<p><strong>Summary</strong></p>
<p>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰å¯¹äºåŒ»å­¦é¢†åŸŸä¸­æ ·æœ¬æ ‡æ³¨ä¸è¶³çš„æŒ‘æˆ˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡å½“å‰æ–¹æ³•èƒ½å¤Ÿæç»˜å‡ºä¸»è¦ç›®æ ‡åŒºåŸŸï¼Œä½†å®ƒä»¬å¾€å¾€éš¾ä»¥ç²¾ç¡®æ•æ‰è¾¹ç•Œçš„ç»†å¾®ç»†èŠ‚ï¼Œå¯¼è‡´è¯Šæ–­ä¸å‡†ç¡®ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†C3S3ï¼Œä¸€ç§æ–°å‹åŠç›‘ç£åˆ†å‰²æ¨¡å‹ï¼Œé€šè¿‡äº’è¡¥ç«äº‰å’Œå¯¹æ¯”é€‰æ‹©æ¥å®ç°æ›´ç²¾ç¡®çš„è¾¹ç•Œæç»˜ã€‚è¯¥æ¨¡å‹åŒ…æ‹¬ä¸€ä¸ªç»“æœé©±åŠ¨çš„å¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼Œç”¨äºæ”¹è¿›è¾¹ç•Œå®šä½ï¼Œä»¥åŠä¸€ä¸ªåŠ¨æ€äº’è¡¥ç«äº‰æ¨¡å—ï¼Œåˆ©ç”¨ä¸¤ä¸ªé«˜æ€§èƒ½å­ç½‘ç»œç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œè¿›ä¸€æ­¥æé«˜åˆ†å‰²è´¨é‡ã€‚åœ¨MRIå’ŒCTæ‰«æçš„å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œçš„ä¸¥æ ¼éªŒè¯è¡¨æ˜ï¼ŒC3S3çš„æ€§èƒ½ä¼˜äºå…¶ä»–å‰æ²¿æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨95HDå’ŒASDæŒ‡æ ‡ä¸Šå–å¾—äº†è‡³å°‘6%çš„æ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰æ˜¯è§£å†³åŒ»å­¦é¢†åŸŸæ ·æœ¬æ ‡æ³¨ä¸è¶³é—®é¢˜çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨æ•æ‰å›¾åƒè¾¹ç•Œç»†å¾®ç»†èŠ‚æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´è¯Šæ–­ä¸å‡†ç¡®ã€‚</li>
<li>C3S3æ¨¡å‹é€šè¿‡æ•´åˆäº’è¡¥ç«äº‰å’Œå¯¹æ¯”é€‰æ‹©æ¥æå‡è¾¹ç•Œæç»˜çš„ç²¾ç¡®åº¦ã€‚</li>
<li>C3S3åŒ…æ‹¬ä¸€ä¸ªç»“æœé©±åŠ¨çš„å¯¹æ¯”å­¦ä¹ æ¨¡å—å’Œä¸€ä¸ªåŠ¨æ€äº’è¡¥ç«äº‰æ¨¡å—ï¼Œä»¥æé«˜åˆ†å‰²è´¨é‡ã€‚</li>
<li>C3S3åœ¨MRIå’ŒCTæ‰«æçš„å…¬å¼€æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–å‰æ²¿æ–¹æ³•ã€‚</li>
<li>C3S3åœ¨95HDå’ŒASDæŒ‡æ ‡ä¸Šå–å¾—äº†è‡³å°‘6%çš„æ˜¾è‘—æ”¹å–„ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e7791a87b1dd1e0035133c64ac482834.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fef6d69631480ef26f7b36de351b39c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b9c37a653b3601863b124337903c75f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86db6b50f76f6c9a229871490c79d7ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7d7ae36a9e0a0a7745fcce72c182b4a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Temporal-Differential-Fields-for-4D-Motion-Modeling-via-Image-to-Video-Synthesis"><a href="#Temporal-Differential-Fields-for-4D-Motion-Modeling-via-Image-to-Video-Synthesis" class="headerlink" title="Temporal Differential Fields for 4D Motion Modeling via Image-to-Video   Synthesis"></a>Temporal Differential Fields for 4D Motion Modeling via Image-to-Video   Synthesis</h2><p><strong>Authors:Xin You, Minghui Zhang, Hanxiao Zhang, Jie Yang, Nassir Navab</strong></p>
<p>Temporal modeling on regular respiration-induced motions is crucial to image-guided clinical applications. Existing methods cannot simulate temporal motions unless high-dose imaging scans including starting and ending frames exist simultaneously. However, in the preoperative data acquisition stage, the slight movement of patients may result in dynamic backgrounds between the first and last frames in a respiratory period. This additional deviation can hardly be removed by image registration, thus affecting the temporal modeling. To address that limitation, we pioneeringly simulate the regular motion process via the image-to-video (I2V) synthesis framework, which animates with the first frame to forecast future frames of a given length. Besides, to promote the temporal consistency of animated videos, we devise the Temporal Differential Diffusion Model to generate temporal differential fields, which measure the relative differential representations between adjacent frames. The prompt attention layer is devised for fine-grained differential fields, and the field augmented layer is adopted to better interact these fields with the I2V framework, promoting more accurate temporal variation of synthesized videos. Extensive results on ACDC cardiac and 4D Lung datasets reveal that our approach simulates 4D videos along the intrinsic motion trajectory, rivaling other competitive methods on perceptual similarity and temporal consistency. Codes will be available soon. </p>
<blockquote>
<p>å¯¹äºå›¾åƒå¼•å¯¼çš„ä¸´åºŠåº”ç”¨è€Œè¨€ï¼Œå¯¹å¸¸è§„å‘¼å¸å¼•èµ·çš„è¿åŠ¨çš„æ—¶åºå»ºæ¨¡è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æ¨¡æ‹Ÿæ—¶åºè¿åŠ¨ï¼Œé™¤éåŒæ—¶å­˜åœ¨åŒ…æ‹¬èµ·å§‹å¸§å’Œç»“æŸå¸§åœ¨å†…çš„é«˜å‰‚é‡æˆåƒæ‰«æã€‚ç„¶è€Œï¼Œåœ¨æœ¯å‰æ•°æ®é‡‡é›†é˜¶æ®µï¼Œæ‚£è€…çš„è½»å¾®ç§»åŠ¨å¯èƒ½å¯¼è‡´å‘¼å¸å‘¨æœŸå†…ç¬¬ä¸€å¸§å’Œæœ€åä¸€å¸§ä¹‹é—´çš„åŠ¨æ€èƒŒæ™¯ã€‚è¿™ç§é¢å¤–çš„åå·®å‡ ä¹æ— æ³•é€šè¿‡å›¾åƒé…å‡†å»é™¤ï¼Œä»è€Œå½±å“æ—¶åºå»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–åˆ›æ€§åœ°é€šè¿‡å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰åˆæˆæ¡†æ¶æ¨¡æ‹Ÿå¸¸è§„è¿åŠ¨è¿‡ç¨‹ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨ç¬¬ä¸€å¸§è¿›è¡ŒåŠ¨ç”»é¢„æµ‹ç»™å®šé•¿åº¦çš„æœªæ¥å¸§ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºåŠ¨ç”»è§†é¢‘çš„æ—¶åºä¸€è‡´æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†æ—¶åºå·®åˆ†æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆæ—¶åºå·®åˆ†åœºï¼Œè¯¥æ¨¡å‹æµ‹é‡ç›¸é‚»å¸§ä¹‹é—´çš„ç›¸å¯¹å·®åˆ†è¡¨ç¤ºã€‚è®¾è®¡å¿«é€Ÿæ³¨æ„å±‚æ˜¯ä¸ºäº†ç²¾ç»†çš„å·®åˆ†åœºï¼Œå¹¶é‡‡ç”¨åœºå¢å¼ºå±‚æ¥æ›´å¥½åœ°å°†è¿™äº›å­—æ®µä¸I2Væ¡†æ¶è¿›è¡Œäº¤äº’ï¼Œä»è€Œä¿ƒè¿›åˆæˆè§†é¢‘çš„æ—¶åºå˜åŒ–æ›´åŠ å‡†ç¡®ã€‚åœ¨ACDCå¿ƒè„å’Œ4Dè‚ºéƒ¨æ•°æ®é›†çš„å¤§é‡ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¨¡æ‹Ÿäº†æ²¿å›ºæœ‰è¿åŠ¨è½¨è¿¹çš„4Dè§†é¢‘ï¼Œåœ¨æ„ŸçŸ¥ç›¸ä¼¼æ€§å’Œæ—¶åºä¸€è‡´æ€§æ–¹é¢ä¸å…¶ä»–ç«äº‰æ–¹æ³•ç›¸åŒ¹æ•Œã€‚ä»£ç å¾ˆå¿«å°†å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17333v2">PDF</a> early accepted by MICCAI</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå›¾åƒè½¬è§†é¢‘ï¼ˆI2Vï¼‰åˆæˆæ¡†æ¶ï¼Œç”¨äºæ¨¡æ‹Ÿè§„å¾‹è¿åŠ¨è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨åˆå§‹å¸§é¢„æµ‹ç»™å®šé•¿åº¦çš„æœªæ¥å¸§ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•æ— æ³•æ¨¡æ‹Ÿæ—¶é—´è¿åŠ¨çš„é—®é¢˜ã€‚ä¸ºè§£å†³åŠ¨æ€èƒŒæ™¯å¯¹æ—¶é—´å»ºæ¨¡çš„å½±å“ï¼Œæå‡ºäº†æ—¶é—´å·®åˆ†æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆæ—¶é—´å·®åˆ†åœºï¼Œå¹¶é€šè¿‡å³æ—¶æ³¨æ„å±‚å¯¹ç²¾ç»†ç²’åº¦å·®åˆ†åœºè¿›è¡Œè¡¡é‡ã€‚åœºå¢å¼ºå±‚æœ‰åŠ©äºå¢å¼ºI2Væ¡†æ¶çš„æ—¶é—´å˜åŒ–æ¨¡æ‹Ÿå‡†ç¡®åº¦ã€‚å®éªŒç»“æœåœ¨ACDCå¿ƒè„å’Œ4Dè‚ºéƒ¨æ•°æ®é›†ä¸Šå±•ç°äº†æœ¬æ–‡æ–¹æ³•æ¨¡æ‹Ÿ4Dè§†é¢‘çš„å¼ºå¤§æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºå›¾åƒè½¬è§†é¢‘ï¼ˆI2Vï¼‰åˆæˆæ¡†æ¶ä»¥æ¨¡æ‹Ÿè§„å¾‹è¿åŠ¨è¿‡ç¨‹ã€‚</li>
<li>åˆ©ç”¨åˆå§‹å¸§é¢„æµ‹æœªæ¥å¸§ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•æ— æ³•æ¨¡æ‹Ÿæ—¶é—´è¿åŠ¨çš„é—®é¢˜ã€‚</li>
<li>é’ˆå¯¹åŠ¨æ€èƒŒæ™¯å¯¹æ—¶é—´å»ºæ¨¡çš„å½±å“ï¼Œæå‡ºäº†æ—¶é—´å·®åˆ†æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ—¶é—´å·®åˆ†åœºã€‚</li>
<li>é‡‡ç”¨å³æ—¶æ³¨æ„å±‚è¡¡é‡ç²¾ç»†ç²’åº¦å·®åˆ†åœºï¼Œé€šè¿‡åœºå¢å¼ºå±‚æé«˜æ—¶é—´å˜åŒ–æ¨¡æ‹Ÿå‡†ç¡®åº¦ã€‚</li>
<li>åœ¨ACDCå¿ƒè„å’Œ4Dè‚ºéƒ¨æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†æ„ŸçŸ¥ç›¸ä¼¼æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6af52a55648faa98312538e984e5bb7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bee09bc37e1b88a1130e6604c0bfd205.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-774cbf78098487d114a0dfbb92fe815d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73b1a2a7d9a5884b44a6ef11aab50faa.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Hallucination-Aware-Multimodal-Benchmark-for-Gastrointestinal-Image-Analysis-with-Large-Vision-Language-Models"><a href="#Hallucination-Aware-Multimodal-Benchmark-for-Gastrointestinal-Image-Analysis-with-Large-Vision-Language-Models" class="headerlink" title="Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image   Analysis with Large Vision-Language Models"></a>Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image   Analysis with Large Vision-Language Models</h2><p><strong>Authors:Bidur Khanal, Sandesh Pokhrel, Sanjay Bhandari, Ramesh Rana, Nikesh Shrestha, Ram Bahadur Gurung, Cristian Linte, Angus Watson, Yash Raj Shrestha, Binod Bhattarai</strong></p>
<p>Vision-Language Models (VLMs) are becoming increasingly popular in the medical domain, bridging the gap between medical images and clinical language. Existing VLMs demonstrate an impressive ability to comprehend medical images and text queries to generate detailed, descriptive diagnostic medical reports. However, hallucinationâ€“the tendency to generate descriptions that are inconsistent with the visual contentâ€“remains a significant issue in VLMs, with particularly severe implications in the medical field. To facilitate VLM research on gastrointestinal (GI) image analysis and study hallucination, we curate a multimodal image-text GI dataset: Gut-VLM. This dataset is created using a two-stage pipeline: first, descriptive medical reports of Kvasir-v2 images are generated using ChatGPT, which introduces some hallucinated or incorrect texts. In the second stage, medical experts systematically review these reports, and identify and correct potential inaccuracies to ensure high-quality, clinically reliable annotations. Unlike traditional datasets that contain only descriptive texts, our dataset also features tags identifying hallucinated sentences and their corresponding corrections. A common approach to reducing hallucination in VLM is to finetune the model on a small-scale, problem-specific dataset. However, we take a different strategy using our dataset. Instead of finetuning the VLM solely for generating textual reports, we finetune it to detect and correct hallucinations, an approach we call hallucination-aware finetuning. Our results show that this approach is better than simply finetuning for descriptive report generation. Additionally, we conduct an extensive evaluation of state-of-the-art VLMs across several metrics, establishing a benchmark. GitHub Repo: <a target="_blank" rel="noopener" href="https://github.com/bhattarailab/Hallucination-Aware-VLM">https://github.com/bhattarailab/Hallucination-Aware-VLM</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸè¶Šæ¥è¶Šå—åˆ°æ¬¢è¿ï¼Œå®ƒä»¬èƒ½å¤Ÿå¼¥åˆåŒ»å­¦å½±åƒä¸ä¸´åºŠè¯­è¨€ä¹‹é—´çš„é¸¿æ²Ÿã€‚ç°æœ‰çš„VLMæ¨¡å‹è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„ç†è§£å’Œå¤„ç†åŒ»å­¦å½±åƒå’Œæ–‡å­—æŸ¥è¯¢çš„èƒ½åŠ›ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆè¯¦ç»†çš„æè¿°æ€§è¯Šæ–­æŠ¥å‘Šã€‚ç„¶è€Œï¼Œå¹»è§†ï¼ˆå€¾å‘äºç”Ÿæˆä¸è§†è§‰å†…å®¹ä¸ä¸€è‡´çš„æè¿°ï¼‰ä»ç„¶æ˜¯VLMæ¨¡å‹ä¸­çš„ä¸€å¤§é—®é¢˜ï¼Œåœ¨åŒ»å­¦é¢†åŸŸå…·æœ‰å°¤ä¸ºä¸¥é‡çš„åæœã€‚ä¸ºäº†ä¿ƒè¿›èƒƒè‚ é“ï¼ˆGIï¼‰å›¾åƒåˆ†æçš„VLMç ”ç©¶å’Œç ”ç©¶å¹»è§†é—®é¢˜ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªå¤šæ¨¡æ€å›¾åƒæ–‡æœ¬èƒƒè‚ é“æ•°æ®é›†ï¼šGut-VLMã€‚è¯¥æ•°æ®é›†é‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“åˆ›å»ºï¼šé¦–å…ˆï¼Œä½¿ç”¨ChatGPTç”ŸæˆKvasir-v2å›¾åƒçš„æè¿°æ€§åŒ»ç–—æŠ¥å‘Šï¼Œè¿™å¯èƒ½ä¼šå¼•å…¥ä¸€äº›å¹»è§†æˆ–é”™è¯¯çš„æ–‡æœ¬ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒåŒ»å­¦ä¸“å®¶ä¼šç³»ç»Ÿåœ°å®¡æŸ¥è¿™äº›æŠ¥å‘Šï¼Œå¹¶è¯†åˆ«ä¸çº æ­£æ½œåœ¨çš„ä¸å‡†ç¡®ä¹‹å¤„ï¼Œä»¥ç¡®ä¿é«˜è´¨é‡ã€ä¸´åºŠå¯é çš„æ³¨é‡Šã€‚ä¸ä¼ ç»Ÿçš„ä»…åŒ…å«æè¿°æ€§æ–‡æœ¬çš„æ•°æ®é›†ä¸åŒï¼Œæˆ‘ä»¬çš„æ•°æ®é›†è¿˜åŒ…å«æ ‡è¯†å¹»è§†å¥å­åŠå…¶ç›¸åº”æ›´æ­£çš„æ ‡ç­¾ã€‚å‡å°‘VLMä¸­å¹»è§†çš„ä¸€ç§å¸¸è§æ–¹æ³•æ˜¯ä½¿ç”¨å°è§„æ¨¡ã€ç‰¹å®šé—®é¢˜çš„æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸åŒçš„ç­–ç•¥ï¼Œåˆ©ç”¨æˆ‘ä»¬çš„æ•°æ®é›†æ¥è®­ç»ƒæ¨¡å‹ä¸ä»…ä»…æ˜¯ä¸ºäº†ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Šè¿›è¡Œå¾®è°ƒï¼Œè€Œæ˜¯æ£€æµ‹å’Œçº æ­£å¹»è§†ç°è±¡ï¼Œæˆ‘ä»¬ç§°è¿™ç§ç­–ç•¥ä¸ºâ€œå¹»è§†æ„ŸçŸ¥å¾®è°ƒâ€ã€‚ç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•ä¼˜äºä»…ç”¨äºç”Ÿæˆæè¿°æ€§æŠ¥å‘Šçš„å¾®è°ƒæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„VLMsè¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œå»ºç«‹äº†åŸºå‡†æµ‹è¯•ã€‚GitHubä»“åº“é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/bhattarailab/Hallucination-Aware-VLM%E3%80%82">https://github.com/bhattarailab/Hallucination-Aware-VLMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07001v2">PDF</a> Accepted at MICCAI 2025</p>
<p><strong>æ‘˜è¦</strong><br>åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨åŒ»ç–—é¢†åŸŸæ­£æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œèƒ½å¤Ÿå¼¥è¡¥åŒ»å­¦å›¾åƒå’Œä¸´åºŠæ–‡æœ¬ä¹‹é—´çš„é¸¿æ²Ÿã€‚ç°æœ‰çš„VLMå¯ä»¥ç†è§£å’Œæè¿°åŒ»å­¦å›¾åƒï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„è¯Šæ–­æŠ¥å‘Šã€‚ç„¶è€Œï¼Œå¹»è§‰ï¼ˆç”Ÿæˆä¸è§†è§‰å†…å®¹ä¸ä¸€è‡´çš„æè¿°ï¼‰ä»æ˜¯VLMçš„ä¸€ä¸ªé‡å¤§é—®é¢˜ï¼Œåœ¨åŒ»ç–—é¢†åŸŸçš„å½±å“å°¤ä¸ºä¸¥é‡ã€‚ä¸ºäº†ä¿ƒè¿›èƒƒè‚ é“ï¼ˆGIï¼‰å›¾åƒåˆ†æçš„VLMç ”ç©¶å¹¶ç ”ç©¶å¹»è§‰ç°è±¡ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå¤šæ¨¡å¼å›¾åƒæ–‡æœ¬GIæ•°æ®é›†ï¼šè‚ é“VLMã€‚æ­¤æ•°æ®é›†é€šè¿‡ä¸¤ä¸ªé˜¶æ®µç”Ÿæˆï¼šé¦–å…ˆï¼Œä½¿ç”¨ChatGPTç”ŸæˆKvasir-v2å›¾åƒçš„è¯¦ç»†æè¿°æ€§æŠ¥å‘Šï¼Œè¿™å¯èƒ½ä¼šå¼•å…¥å¹»è§‰æˆ–é”™è¯¯æ–‡æœ¬ã€‚ç¬¬äºŒé˜¶æ®µï¼ŒåŒ»å­¦ä¸“å®¶ç³»ç»Ÿå®¡æŸ¥è¿™äº›æŠ¥å‘Šï¼Œå¹¶è¯†åˆ«ä¸çº æ­£æ½œåœ¨çš„ä¸å‡†ç¡®ä¹‹å¤„ï¼Œä»¥ç¡®ä¿é«˜è´¨é‡ã€ä¸´åºŠå¯é çš„æ³¨é‡Šã€‚ä¸ä¼ ç»Ÿçš„ä»…åŒ…å«æè¿°æ€§æ–‡æœ¬çš„æ•°æ®é›†ä¸åŒï¼Œæˆ‘ä»¬çš„æ•°æ®é›†è¿˜åŒ…å«æ ‡è¯†å¹»è§‰å¥å­åŠå…¶ç›¸åº”æ ¡æ­£çš„æ ‡ç­¾ã€‚å‡å°‘VLMä¸­å¹»è§‰çš„å¸¸è§æ–¹æ³•æ˜¯ä½¿ç”¨å°è§„æ¨¡ç‰¹å®šé—®é¢˜æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ä½†æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä¸åŒçš„ç­–ç•¥ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œè€Œä¸æ˜¯ä»…ç”¨äºç”Ÿæˆæ–‡æœ¬æŠ¥å‘Šã€‚æˆ‘ä»¬å¾®è°ƒæ¨¡å‹ä»¥æ£€æµ‹å’Œçº æ­£å¹»è§‰ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œå¹»è§‰æ„ŸçŸ¥å¾®è°ƒâ€ã€‚ç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•ä¼˜äºä»…ç”¨äºç”Ÿæˆæè¿°æ€§æŠ¥å‘Šçš„å¾®è°ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„VLMè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå»ºç«‹äº†åŸºå‡†æµ‹è¯•ã€‚GitHubä»“åº“é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/bhattarailab/Hallucination-Aware-VLM%E3%80%82">https://github.com/bhattarailab/Hallucination-Aware-VLMã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>VLMåœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨æ­£åœ¨é€æ¸æ™®åŠï¼Œèƒ½å¤Ÿç”Ÿæˆè¯¦ç»†çš„è¯Šæ–­æŠ¥å‘Šã€‚</li>
<li>å¹»è§‰ç°è±¡åœ¨VLMä¸­ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦é—®é¢˜ï¼Œå¯¹åŒ»ç–—é¢†åŸŸæœ‰ä¸¥é‡åæœã€‚</li>
<li>ä¸ºäº†è§£å†³å¹»è§‰é—®é¢˜å¹¶ä¿ƒè¿›èƒƒè‚ é“å›¾åƒåˆ†æçš„VLMç ”ç©¶ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€å›¾åƒæ–‡æœ¬GIæ•°æ®é›†ï¼šè‚ é“VLMã€‚</li>
<li>æ•°æ®é›†åŒ…å«ç”±ChatGPTç”Ÿæˆçš„æè¿°æ€§æŠ¥å‘Šï¼Œå¹¶é€šè¿‡åŒ»å­¦ä¸“å®¶è¿›è¡Œå®¡æŸ¥ä¸æ ¡æ­£ï¼Œç¡®ä¿é«˜è´¨é‡å’Œä¸´åºŠå¯é æ€§ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ•°æ®é›†ä¸åŒï¼Œè‚ é“VLMæ•°æ®é›†åŒ…å«æ ‡è¯†å¹»è§‰å¥å­åŠå…¶æ ¡æ­£çš„æ ‡ç­¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥â€”â€”å¹»è§‰æ„ŸçŸ¥å¾®è°ƒæ¥å‡å°‘å¹»è§‰é—®é¢˜ï¼Œè¯¥ç­–ç•¥ä¼˜äºä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07001">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e41f297a105be4d43471124ad210eda5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fca630c2a7a89c512ac80ada20a1086c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-274278afe6bb2a3e0b40cdad36258392.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="VesselSAM-Leveraging-SAM-for-Aortic-Vessel-Segmentation-with-AtrousLoRA"><a href="#VesselSAM-Leveraging-SAM-for-Aortic-Vessel-Segmentation-with-AtrousLoRA" class="headerlink" title="VesselSAM: Leveraging SAM for Aortic Vessel Segmentation with AtrousLoRA"></a>VesselSAM: Leveraging SAM for Aortic Vessel Segmentation with AtrousLoRA</h2><p><strong>Authors:Adnan Iltaf, Rayan Merghani Ahmed, Zhenxi Zhang, Bin Li, Shoujun Zhou</strong></p>
<p>Medical image segmentation is crucial for clinical diagnosis and treatment planning, especially when dealing with complex anatomical structures such as vessels. However, accurately segmenting vessels remains challenging due to their small size, intricate edge structures, and susceptibility to artifacts and imaging noise. In this work, we propose VesselSAM, an enhanced version of the Segment Anything Model (SAM), specifically tailored for aortic vessel segmentation. VesselSAM incorporates AtrousLoRA, a novel module integrating Atrous Attention and Low-Rank Adaptation (LoRA), to enhance segmentation performance. Atrous Attention enables the model to capture multi-scale contextual information, preserving both fine-grained local details and broader global context. Additionally, LoRA facilitates efficient fine-tuning of the frozen SAM image encoder, reducing the number of trainable parameters and thereby enhancing computational efficiency. We evaluate VesselSAM using two challenging datasets: the Aortic Vessel Tree (AVT) dataset and the Type-B Aortic Dissection (TBAD) dataset. VesselSAM achieves state-of-the-art performance, attaining DSC scores of 93.50%, 93.25%, 93.02%, and 93.26% across multi-center datasets. Our results demonstrate that VesselSAM delivers high segmentation accuracy while significantly reducing computational overhead compared to existing large-scale models. This development paves the way for enhanced AI-based aortic vessel segmentation in clinical environments. The code and models will be released at <a target="_blank" rel="noopener" href="https://github.com/Adnan-CAS/AtrousLora">https://github.com/Adnan-CAS/AtrousLora</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’çš„åˆ¶å®šè‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„è§£å‰–ç»“æ„ï¼ˆå¦‚è¡€ç®¡ï¼‰æ—¶ã€‚ç„¶è€Œï¼Œç”±äºè¡€ç®¡å°ºå¯¸å°ã€è¾¹ç¼˜ç»“æ„å¤æ‚ä»¥åŠå®¹æ˜“å—åˆ°ä¼ªå½±å’Œæˆåƒå™ªå£°çš„å½±å“ï¼Œå‡†ç¡®åˆ†å‰²è¡€ç®¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸“ä¸ºä¸»åŠ¨è„‰è¡€ç®¡åˆ†å‰²å®šåˆ¶çš„å¢å¼ºç‰ˆSegment Anything Modelï¼ˆSAMï¼‰ï¼Œç§°ä¸ºVesselSAMã€‚VesselSAMç»“åˆäº†AtrousLoRAè¿™ä¸€æ–°å‹æ¨¡å—ï¼Œè¯¥æ¨¡å—é›†æˆäº†Atrous Attentionå’ŒLow-Rank Adaptationï¼ˆLoRAï¼‰æŠ€æœ¯ï¼Œä»¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚Atrous Attentionå…è®¸æ¨¡å‹æ•è·å¤šå°ºåº¦ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™ç²¾ç»†çš„å±€éƒ¨ç»†èŠ‚å’Œæ›´å¹¿æ³›çš„å…¨å±€ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼ŒLoRAæœ‰åŠ©äºå¯¹å†»ç»“çš„SAMå›¾åƒç¼–ç å™¨è¿›è¡Œé«˜æ•ˆå¾®è°ƒï¼Œå‡å°‘å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†å¯¹VesselSAMè¿›è¡Œäº†è¯„ä¼°ï¼šä¸»åŠ¨è„‰è¡€ç®¡æ ‘ï¼ˆAVTï¼‰æ•°æ®é›†å’ŒBå‹ä¸»åŠ¨è„‰å¤¹å±‚ï¼ˆTBADï¼‰æ•°æ®é›†ã€‚VesselSAMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œåœ¨å¤šä¸­å¿ƒæ•°æ®é›†ä¸Šçš„DSCå¾—åˆ†åˆ†åˆ«ä¸º93.50ï¼…ã€93.25ï¼…ã€93.02ï¼…å’Œ93.26ï¼…ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„å¤§è§„æ¨¡æ¨¡å‹ç›¸æ¯”ï¼ŒVesselSAMåœ¨æä¾›é«˜åˆ†å‰²å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œè®¡ç®—å¼€é”€å¤§å¤§é™ä½ã€‚è¿™ä¸ºä¸´åºŠç¯å¢ƒä¸­å¢å¼ºçš„åŸºäºAIçš„ä¸»åŠ¨è„‰è¡€ç®¡åˆ†å‰²é“ºå¹³äº†é“è·¯ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Adnan-CAS/AtrousLora%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Adnan-CAS/AtrousLoraå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18185v4">PDF</a> Work in progress</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯å¤„ç†å¤æ‚çš„è§£å‰–ç»“æ„å¦‚è¡€ç®¡æ—¶ã€‚ç„¶è€Œï¼Œç”±äºè¡€ç®¡å°ºå¯¸å°ã€è¾¹ç¼˜ç»“æ„å¤æ‚ä»¥åŠæ˜“å—ä¼ªå½±å’Œæˆåƒå™ªå£°å½±å“ï¼Œå‡†ç¡®åˆ†å‰²è¡€ç®¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†VesselSAMï¼Œå®ƒæ˜¯ä¸“ä¸ºä¸»åŠ¨è„‰è¡€ç®¡åˆ†å‰²è®¾è®¡çš„Segment Anything Modelï¼ˆSAMï¼‰çš„å¢å¼ºç‰ˆã€‚VesselSAMç»“åˆäº†AtrousLoRAè¿™ä¸€æ–°å‹æ¨¡å—ï¼Œè¯¥æ¨¡å—é›†æˆäº†Atrous Attentionå’ŒLow-Rank Adaptationï¼ˆLoRAï¼‰ï¼Œä»¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚Atrous Attentionä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰å¤šå°ºåº¦ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™ç²¾ç»†çš„å±€éƒ¨ç»†èŠ‚å’Œæ›´å¹¿æ³›çš„å…¨çƒä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼ŒLoRAæœ‰åŠ©äºæœ‰æ•ˆå¾®è°ƒå†»ç»“çš„SAMå›¾åƒç¼–ç å™¨ï¼Œå‡å°‘è®­ç»ƒå‚æ•°æ•°é‡ï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ï¼šä¸»åŠ¨è„‰è¡€ç®¡æ ‘ï¼ˆAVTï¼‰æ•°æ®é›†å’ŒBå‹ä¸»åŠ¨è„‰å¤¹å±‚ï¼ˆTBADï¼‰æ•°æ®é›†å¯¹VesselSAMè¿›è¡Œäº†è¯„ä¼°ã€‚VesselSAMå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å¤šä¸­å¿ƒæ•°æ®é›†ä¸Šçš„DSCå¾—åˆ†åˆ†åˆ«ä¸º93.50ï¼…ã€93.25ï¼…ã€93.02ï¼…å’Œ93.26ï¼…ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰å¤§è§„æ¨¡æ¨¡å‹ç›¸æ¯”ï¼ŒVesselSAMå®ç°äº†é«˜åˆ†å‰²ç²¾åº¦ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†è®¡ç®—å¼€é”€ã€‚è¿™ä¸ºä¸´åºŠç¯å¢ƒä¸­å¢å¼ºçš„åŸºäºAIçš„ä¸»åŠ¨è„‰è¡€ç®¡åˆ†å‰²å¼€è¾Ÿäº†é“è·¯ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Adnan-CAS/AtrousLora">https://github.com/Adnan-CAS/AtrousLora</a>ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹å¤æ‚è§£å‰–ç»“æ„å¦‚è¡€ç®¡çš„åˆ†å‰²ã€‚</li>
<li>è¡€ç®¡åˆ†å‰²å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¡€ç®¡å°ºå¯¸å°ã€è¾¹ç¼˜å¤æ‚ä¸”æ˜“å—ä¼ªå½±å’Œå™ªå£°å½±å“ã€‚</li>
<li>VesselSAMæ¨¡å‹è¢«æå‡ºç”¨äºä¸»åŠ¨è„‰è¡€ç®¡åˆ†å‰²ï¼Œå®ƒåœ¨Segment Anything Modelï¼ˆSAMï¼‰çš„åŸºç¡€ä¸Šè¿›è¡Œäº†å¢å¼ºã€‚</li>
<li>VesselSAMç»“åˆäº†AtrousLoRAæ¨¡å—ï¼Œè¯¥æ¨¡å—åŒ…å«Atrous Attentionå’ŒLow-Rank Adaptationï¼ˆLoRAï¼‰ï¼Œæ—¨åœ¨æé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>Atrous Attentionèƒ½å¤Ÿæ•æ‰å¤šå°ºåº¦ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚</li>
<li>LoRAæœ‰åŠ©äºæœ‰æ•ˆå¾®è°ƒå›¾åƒç¼–ç å™¨ï¼Œå‡å°‘è®­ç»ƒå‚æ•°ï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cecbe9bcf5ec74f8bd77261b2a408054.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7f9b9202919789f64ff782408507847.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1189f1176021633bc7ca495514baea08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d36bba57e581e7a5345445352b7a4f78.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="VICCA-Visual-Interpretation-and-Comprehension-of-Chest-X-ray-Anomalies-in-Generated-Report-Without-Human-Feedback"><a href="#VICCA-Visual-Interpretation-and-Comprehension-of-Chest-X-ray-Anomalies-in-Generated-Report-Without-Human-Feedback" class="headerlink" title="VICCA: Visual Interpretation and Comprehension of Chest X-ray Anomalies   in Generated Report Without Human Feedback"></a>VICCA: Visual Interpretation and Comprehension of Chest X-ray Anomalies   in Generated Report Without Human Feedback</h2><p><strong>Authors:Sayeh Gholipour Picha, Dawood Al Chanti, Alice Caplier</strong></p>
<p>As artificial intelligence (AI) becomes increasingly central to healthcare, the demand for explainable and trustworthy models is paramount. Current report generation systems for chest X-rays (CXR) often lack mechanisms for validating outputs without expert oversight, raising concerns about reliability and interpretability. To address these challenges, we propose a novel multimodal framework designed to enhance the semantic alignment and localization accuracy of AI-generated medical reports. Our framework integrates two key modules: a Phrase Grounding Model, which identifies and localizes pathologies in CXR images based on textual prompts, and a Text-to-Image Diffusion Module, which generates synthetic CXR images from prompts while preserving anatomical fidelity. By comparing features between the original and generated images, we introduce a dual-scoring system: one score quantifies localization accuracy, while the other evaluates semantic consistency. This approach significantly outperforms existing methods, achieving state-of-the-art results in pathology localization and text-to-image alignment. The integration of phrase grounding with diffusion models, coupled with the dual-scoring evaluation system, provides a robust mechanism for validating report quality, paving the way for more trustworthy and transparent AI in medical imaging. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„é‡è¦æ€§ä¸æ–­æå‡ï¼Œå¯¹å¯è§£é‡Šæ€§å’Œå¯é æ€§çš„æ¨¡å‹çš„éœ€æ±‚è‡³å…³é‡è¦ã€‚ç›®å‰é’ˆå¯¹èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰çš„æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿå¸¸å¸¸ç¼ºä¹åœ¨æ²¡æœ‰ä¸“å®¶ç›‘ç£çš„æƒ…å†µä¸‹éªŒè¯è¾“å‡ºçš„æœºåˆ¶ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹å¯é æ€§å’Œå¯è§£é‡Šæ€§çš„æ‹…å¿§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜äººå·¥æ™ºèƒ½ç”Ÿæˆçš„åŒ»ç–—æŠ¥å‘Šçš„è¯­ä¹‰å¯¹é½å’Œå®šä½ç²¾åº¦ã€‚æˆ‘ä»¬çš„æ¡†æ¶é›†æˆäº†ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šçŸ­è¯­å®šä½æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ ¹æ®æ–‡æœ¬æç¤ºè¯†åˆ«å’Œå®šä½CXRå›¾åƒä¸­çš„ç—…ç†ç‰¹å¾ï¼›æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å—ï¼Œè¯¥æ¨¡å—ä»æç¤ºä¸­ç”ŸæˆåˆæˆCXRå›¾åƒï¼ŒåŒæ—¶ä¿ç•™è§£å‰–ç»“æ„çš„ä¿çœŸåº¦ã€‚é€šè¿‡æ¯”è¾ƒåŸå§‹å›¾åƒå’Œç”Ÿæˆå›¾åƒä¹‹é—´çš„ç‰¹å¾ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŒé‡è¯„åˆ†ç³»ç»Ÿï¼šä¸€ä¸ªåˆ†æ•°é‡åŒ–å®šä½ç²¾åº¦ï¼Œå¦ä¸€ä¸ªåˆ†æ•°è¯„ä¼°è¯­ä¹‰ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ç—…ç†å®šä½ã€æ–‡æœ¬åˆ°å›¾åƒå¯¹é½æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ã€‚çŸ­è¯­å®šä½ä¸æ‰©æ•£æ¨¡å‹çš„é›†æˆï¼ŒåŠ ä¸ŠåŒé‡è¯„åˆ†è¯„ä¼°ç³»ç»Ÿï¼Œä¸ºéªŒè¯æŠ¥å‘Šè´¨é‡æä¾›äº†ç¨³å¥çš„æœºåˆ¶ï¼Œä¸ºåŒ»å­¦å½±åƒé¢†åŸŸä¸­æ›´å¯é ã€æ›´é€æ˜çš„AIåº”ç”¨é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17726v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»‹ç»äº†éšç€äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„ä¸­å¿ƒåœ°ä½æ—¥ç›Šå‡¸æ˜¾ï¼Œå¯¹å¯è§£é‡Šæ€§å’Œå¯é æ€§çš„éœ€æ±‚æ„ˆå‘é‡è¦ã€‚é’ˆå¯¹å½“å‰èƒ¸éƒ¨Xå…‰å°„çº¿æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿç¼ºä¹éªŒè¯æœºåˆ¶çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜AIç”Ÿæˆçš„åŒ»ç–—æŠ¥å‘Šçš„è¯­ä¹‰å¯¹é½å’Œå®šä½ç²¾åº¦ã€‚è¯¥æ¡†æ¶é›†æˆäº†ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šçŸ­è¯­å®šä½æ¨¡å‹å’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å—ã€‚çŸ­è¯­å®šä½æ¨¡å‹æ ¹æ®æ–‡æœ¬æç¤ºè¯†åˆ«å’Œå®šä½CXRå›¾åƒä¸­çš„ç—…ç†ç‰¹å¾ï¼Œè€Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å—åˆ™ä»æç¤ºç”ŸæˆåˆæˆCXRå›¾åƒï¼Œä¿æŒè§£å‰–å­¦çš„çœŸå®æ€§ã€‚é€šè¿‡æ¯”è¾ƒåŸå§‹å’Œç”Ÿæˆå›¾åƒçš„ç‰¹å¾ï¼Œå¼•å…¥äº†ä¸€ç§åŒé‡è¯„åˆ†ç³»ç»Ÿï¼Œæ—¢é‡åŒ–å®šä½ç²¾åº¦åˆè¯„ä¼°è¯­ä¹‰ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ç—…ç†å®šä½å’Œæ–‡æœ¬åˆ°å›¾åƒå¯¹é½æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚çŸ­è¯­å®šä½ä¸æ‰©æ•£æ¨¡å‹çš„é›†æˆä»¥åŠåŒé‡è¯„åˆ†è¯„ä¼°ç³»ç»Ÿç›¸ç»“åˆï¼Œä¸ºéªŒè¯æŠ¥å‘Šè´¨é‡æä¾›äº†ç¨³å¥æœºåˆ¶ï¼Œä¸ºåŒ»ç–—å½±åƒä¸­çš„æ›´å¯ä¿¡å’Œé€æ˜çš„AIé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è¯¥æ–‡æœ¬çš„ä¸»è¦è§‚ç‚¹æ‘˜è¦ï¼š</p>
<ol>
<li>éšç€äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„é‡è¦æ€§å¢å¼ºï¼Œå¯¹å¯è§£é‡Šæ€§å’Œå¯é æ€§çš„éœ€æ±‚æ„ˆå‘é‡è¦ã€‚</li>
<li>å½“å‰èƒ¸éƒ¨Xå…‰å°„çº¿æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿç¼ºä¹éªŒè¯æœºåˆ¶ï¼Œå¯¼è‡´å¯é æ€§å’Œè§£é‡Šæ€§å—åˆ°è´¨ç–‘ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜AIç”Ÿæˆçš„åŒ»ç–—æŠ¥å‘Šçš„è¯­ä¹‰å¯¹é½å’Œå®šä½ç²¾åº¦ã€‚</li>
<li>è¯¥æ¡†æ¶é›†æˆäº†çŸ­è¯­å®šä½æ¨¡å‹å’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å—ã€‚</li>
<li>çŸ­è¯­å®šä½æ¨¡å‹æ ¹æ®æ–‡æœ¬æç¤ºå®šä½å’Œè¯†åˆ«CXRå›¾åƒä¸­çš„ç—…ç†ç‰¹å¾ã€‚</li>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å—ç”ŸæˆåˆæˆCXRå›¾åƒï¼Œå¹¶ä¿æŒè§£å‰–å­¦çœŸå®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-065b3ca3ca9136c1ed7f70ab937643ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ceb4e1750a2153c2f42efe68b6ef3f35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-342ae454f3a989555f3394911d455226.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40014d89b47ab1d002ccc659a60e296d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="CAD-GPT-Synthesising-CAD-Construction-Sequence-with-Spatial-Reasoning-Enhanced-Multimodal-LLMs"><a href="#CAD-GPT-Synthesising-CAD-Construction-Sequence-with-Spatial-Reasoning-Enhanced-Multimodal-LLMs" class="headerlink" title="CAD-GPT: Synthesising CAD Construction Sequence with Spatial   Reasoning-Enhanced Multimodal LLMs"></a>CAD-GPT: Synthesising CAD Construction Sequence with Spatial   Reasoning-Enhanced Multimodal LLMs</h2><p><strong>Authors:Siyu Wang, Cailian Chen, Xinyi Le, Qimin Xu, Lei Xu, Yanzhou Zhang, Jie Yang</strong></p>
<p>Computer-aided design (CAD) significantly enhances the efficiency, accuracy, and innovation of design processes by enabling precise 2D and 3D modeling, extensive analysis, and optimization. Existing methods for creating CAD models rely on latent vectors or point clouds, which are difficult to obtain, and storage costs are substantial. Recent advances in Multimodal Large Language Models (MLLMs) have inspired researchers to use natural language instructions and images for CAD model construction. However, these models still struggle with inferring accurate 3D spatial location and orientation, leading to inaccuracies in determining the spatial 3D starting points and extrusion directions for constructing geometries. This work introduces CAD-GPT, a CAD synthesis method with spatial reasoning-enhanced MLLM that takes either a single image or a textual description as input. To achieve precise spatial inference, our approach introduces a 3D Modeling Spatial Mechanism. This method maps 3D spatial positions and 3D sketch plane rotation angles into a 1D linguistic feature space using a specialized spatial unfolding mechanism, while discretizing 2D sketch coordinates into an appropriate planar space to enable precise determination of spatial starting position, sketch orientation, and 2D sketch coordinate translations. Extensive experiments demonstrate that CAD-GPT consistently outperforms existing state-of-the-art methods in CAD model synthesis, both quantitatively and qualitatively. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰é€šè¿‡å®ç°ç²¾ç¡®çš„2Då’Œ3Då»ºæ¨¡ã€å…¨é¢çš„åˆ†æå’Œä¼˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†è®¾è®¡è¿‡ç¨‹çš„æ•ˆç‡ã€å‡†ç¡®æ€§å’Œåˆ›æ–°æ€§ã€‚ç°æœ‰çš„åˆ›å»ºCADæ¨¡å‹çš„æ–¹æ³•ä¾èµ–äºæ½œåœ¨å‘é‡æˆ–ç‚¹äº‘ï¼Œè¿™äº›éš¾ä»¥è·å¾—ï¼Œä¸”å­˜å‚¨æˆæœ¬é«˜æ˜‚ã€‚æœ€è¿‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è¿›æ­¥æ¿€å‘äº†ç ”ç©¶äººå‘˜ä½¿ç”¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œå›¾åƒè¿›è¡ŒCADæ¨¡å‹æ„å»ºã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨æ¨æ–­å‡†ç¡®çš„3Dç©ºé—´ä½ç½®å’Œæ–¹å‘æ—¶ä»å­˜åœ¨é—®é¢˜ï¼Œå¯¼è‡´åœ¨ç¡®å®šæ„å»ºå‡ ä½•ä½“çš„ç©ºé—´3Dèµ·ç‚¹å’ŒæŒ¤å‹æ–¹å‘æ—¶å‡ºç°ä¸å‡†ç¡®ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†CAD-GPTï¼Œè¿™æ˜¯ä¸€ç§å¸¦æœ‰ç©ºé—´æ¨ç†å¢å¼ºçš„MLLMçš„CADåˆæˆæ–¹æ³•ï¼Œå®ƒæ¥å—å•å¼ å›¾åƒæˆ–æ–‡æœ¬æè¿°ä½œä¸ºè¾“å…¥ã€‚ä¸ºäº†å®ç°ç²¾ç¡®çš„ç©ºé—´æ¨æ–­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§3Då»ºæ¨¡ç©ºé—´æœºåˆ¶ã€‚è¯¥æ–¹æ³•ä½¿ç”¨ä¸“é—¨çš„ç©ºé—´å±•å¼€æœºåˆ¶å°†3Dç©ºé—´ä½ç½®å’Œ3Dè‰å›¾å¹³é¢æ—‹è½¬è§’åº¦æ˜ å°„åˆ°1Dè¯­è¨€ç‰¹å¾ç©ºé—´ä¸­ï¼ŒåŒæ—¶å°†2Dè‰å›¾åæ ‡ç¦»æ•£åŒ–åˆ°é€‚å½“çš„å¹³é¢ç©ºé—´ä¸­ï¼Œä»è€Œå®ç°ç©ºé—´èµ·å§‹ä½ç½®ã€è‰å›¾æ–¹å‘å’Œ2Dè‰å›¾åæ ‡ç¿»è¯‘çš„ç²¾ç¡®ç¡®å®šã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨å®šé‡è¿˜æ˜¯å®šæ€§æ–¹é¢ï¼ŒCAD-GPTåœ¨CADæ¨¡å‹åˆæˆæ–¹é¢éƒ½å§‹ç»ˆä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19663v2">PDF</a> Accepted at AAAI 2025 (Vol. 39, No. 8), pages 7880-7888. DOI:   10.1609&#x2F;aaai.v39i8.32849</p>
<p><strong>Summary</strong><br>     åŸºäºè®¡ç®—æœºçš„è®¾è®¡ï¼ˆCADï¼‰æŠ€æœ¯æé«˜äº†è®¾è®¡è¿‡ç¨‹çš„æ•ˆç‡ã€ç²¾åº¦å’Œåˆ›æ–°èƒ½åŠ›ï¼Œå¯å®ç°ç²¾ç¡®äºŒç»´å’Œä¸‰ç»´å»ºæ¨¡ã€å…¨é¢åˆ†æå’Œä¼˜åŒ–ã€‚ç°æœ‰åˆ›å»ºCADæ¨¡å‹çš„æ–¹æ³•ä¾èµ–äºæ½œåœ¨å‘é‡æˆ–ç‚¹äº‘ï¼Œè·å–å›°éš¾ä¸”å­˜å‚¨æˆæœ¬é«˜ã€‚æœ€æ–°ç ”ç©¶å¼•å…¥äº†å…·æœ‰ç©ºé—´æ¨ç†èƒ½åŠ›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œé‡‡ç”¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œå›¾åƒè¿›è¡ŒCADæ¨¡å‹æ„å»ºã€‚æœ¬ç ”ç©¶æå‡ºCAD-GPTæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»¥å•å¼ å›¾åƒæˆ–æ–‡æœ¬æè¿°ä½œä¸ºè¾“å…¥ï¼Œå®ç°äº†ç²¾ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶åœ¨CADåˆæˆä¸­å¼•å…¥äº†ä¸€ç§æ–°çš„ç©ºé—´æœºåˆ¶ã€‚å®éªŒè¯æ˜ï¼ŒCAD-GPTåœ¨CADæ¨¡å‹åˆæˆæ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<pre><code> 1. è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æé«˜äº†è®¾è®¡æ•ˆç‡ã€ç²¾åº¦å’Œåˆ›æ–°æ€§ï¼Œå€ŸåŠ©ç²¾ç¡®å»ºæ¨¡ã€åˆ†æå’Œä¼˜åŒ–å®ç°ã€‚
 2. å½“å‰CADæ¨¡å‹åˆ›å»ºæ–¹æ³•é¢ä¸´è·å–å’Œå­˜å‚¨æˆæœ¬é«˜çš„é—®é¢˜ã€‚
 3. å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸ºCADæ¨¡å‹æ„å»ºæä¾›äº†æ–°çš„æ€è·¯ï¼Œç»“åˆè‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œå›¾åƒè¿›è¡Œæ¨¡å‹æ„å»ºã€‚
 4. MLLMsåœ¨æ¨æ–­ç²¾ç¡®çš„3Dç©ºé—´ä½ç½®å’Œæ–¹å‘æ—¶ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œå½±å“ç©ºé—´3Dèµ·ç‚¹å’ŒæŒ¤å‹æ–¹å‘çš„ç²¾ç¡®åˆ¤æ–­ã€‚
 5. CAD-GPTæ–¹æ³•è¢«å¼•å…¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé‡‡ç”¨å•ä¸€å›¾åƒæˆ–æ–‡æœ¬æè¿°ä½œä¸ºè¾“å…¥ï¼Œå…·æœ‰ç²¾ç¡®çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚
 6. CAD-GPTé€šè¿‡å¼•å…¥3Då»ºæ¨¡ç©ºé—´æœºåˆ¶ï¼Œå°†3Dç©ºé—´ä½ç½®å’Œè‰å›¾å¹³é¢æ—‹è½¬è§’åº¦æ˜ å°„åˆ°ä¸€ç»´è¯­è¨€ç‰¹å¾ç©ºé—´ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19663">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3dd237828cd0091463f64a0321862037.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c88bad49f607f685340be8d2797c1d50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24f8b7ce53d1fc77b8ecbe2473e811b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abd49eff977e05357a6b5d8cafa676d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a8ed9460edbaf2249ed95329c66dfb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad5689cfd06c4120dd98435300697524.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff1f4e721b6d7230872b5f5f1d75a157.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-27/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-27/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-27/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b43e16c2dc6c208b4208616dd9d17404.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-27  Causal Inference for Latent Outcomes Learned with Factor Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-27/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-77d0cdd4f4d6244df74e879fff65c6d9.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-27  Shape2Animal Creative Animal Generation from Natural Silhouettes
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
