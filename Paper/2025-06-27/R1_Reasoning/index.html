<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-27  MMSearch-R1 Incentivizing LMMs to Search">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b5c727c232a9c7e176f040eee15e49e1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-27-æ›´æ–°"><a href="#2025-06-27-æ›´æ–°" class="headerlink" title="2025-06-27 æ›´æ–°"></a>2025-06-27 æ›´æ–°</h1><h2 id="MMSearch-R1-Incentivizing-LMMs-to-Search"><a href="#MMSearch-R1-Incentivizing-LMMs-to-Search" class="headerlink" title="MMSearch-R1: Incentivizing LMMs to Search"></a>MMSearch-R1: Incentivizing LMMs to Search</h2><p><strong>Authors:Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, Ziwei Liu</strong></p>
<p>Robust deployment of large multimodal models (LMMs) in real-world scenarios requires access to external knowledge sources, given the complexity and dynamic nature of real-world information. Existing approaches such as retrieval-augmented generation (RAG) and prompt engineered search agents rely on rigid pipelines, often leading to inefficient or excessive search behaviors. We present MMSearch-R1, the first end-to-end reinforcement learning framework that enables LMMs to perform on-demand, multi-turn search in real-world Internet environments. Our framework integrates both image and text search tools, allowing the model to reason about when and how to invoke them guided by an outcome-based reward with a search penalty. To support training, We collect a multimodal search VQA dataset through a semi-automated pipeline that covers diverse visual and textual knowledge needs and curate a search-balanced subset with both search-required and search-free samples, which proves essential for shaping efficient and on-demand search behavior. Extensive experiments on knowledge-intensive and info-seeking VQA tasks show that our model not only outperforms RAG-based baselines of the same model size, but also matches the performance of a larger RAG-based model while reducing search calls by over 30%. We further analyze key empirical findings to offer actionable insights for advancing research in multimodal search. </p>
<blockquote>
<p>åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­ç¨³å¥éƒ¨ç½²å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰éœ€è¦è®¿é—®å¤–éƒ¨çŸ¥è¯†æºï¼Œè€ƒè™‘åˆ°çœŸå®ä¸–ç•Œä¿¡æ¯çš„å¤æ‚æ€§å’ŒåŠ¨æ€æ€§ã€‚ç°æœ‰çš„æ–¹æ³•å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæç¤ºå·¥ç¨‹æœç´¢ä»£ç†ä¾èµ–äºåƒµåŒ–ç®¡é“ï¼Œé€šå¸¸å¯¼è‡´ä½æ•ˆæˆ–è¿‡åº¦æœç´¢è¡Œä¸ºã€‚æˆ‘ä»¬æå‡ºäº†MMSearch-R1ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿LMMsèƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•Œçš„äº’è”ç½‘ç¯å¢ƒä¸­è¿›è¡ŒæŒ‰éœ€å¤šè½®æœç´¢ã€‚æˆ‘ä»¬çš„æ¡†æ¶é›†æˆäº†å›¾åƒå’Œæ–‡æœ¬æœç´¢å·¥å…·ï¼Œå…è®¸æ¨¡å‹æ ¹æ®ä»¥ç»“æœä¸ºåŸºç¡€çš„å¥–åŠ±å’Œæœç´¢æƒ©ç½šæ¥æ¨ç†ä½•æ—¶ä»¥åŠå¦‚ä½•è°ƒç”¨å®ƒä»¬ã€‚ä¸ºäº†æ”¯æŒè®­ç»ƒï¼Œæˆ‘ä»¬é€šè¿‡åŠè‡ªåŠ¨åŒ–ç®¡é“æ”¶é›†äº†ä¸€ä¸ªå¤šæ¨¡æ€æœç´¢VQAæ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šæ ·åŒ–çš„è§†è§‰å’Œæ–‡æœ¬çŸ¥è¯†éœ€æ±‚ï¼Œå¹¶ç­›é€‰å‡ºä¸€ä¸ªæœç´¢å¹³è¡¡çš„å­é›†ï¼Œå…¶ä¸­åŒ…å«æœç´¢æ‰€éœ€å’Œæ— éœ€æœç´¢çš„æ ·æœ¬ï¼Œè¿™å¯¹äºå¡‘é€ é«˜æ•ˆå’ŒæŒ‰éœ€æœç´¢è¡Œä¸ºè‡³å…³é‡è¦ã€‚åœ¨çŸ¥è¯†å¯†é›†å‹å’Œä¿¡æ¯æœç´¢VQAä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ä»…ä¼˜äºåŸºäºRAGçš„ç›¸åŒæ¨¡å‹å¤§å°çš„åŸºçº¿ï¼Œè€Œä¸”ä¸æ›´å¤§çš„åŸºäºRAGçš„æ¨¡å‹ç›¸åŒ¹é…ï¼ŒåŒæ—¶å‡å°‘äº†è¶…è¿‡30%çš„æœç´¢è°ƒç”¨ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥åˆ†æäº†å…³é”®å®éªŒç»“æœï¼Œä¸ºæ¨è¿›å¤šæ¨¡æ€æœç´¢ç ”ç©¶æä¾›äº†å¯æ“ä½œçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20670v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/EvolvingLMMs-Lab/multimodal-search-r1">https://github.com/EvolvingLMMs-Lab/multimodal-search-r1</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç«¯å¯¹ç«¯æ¡†æ¶MMSearch-R1ï¼Œç”¨äºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨çœŸå®äº’è”ç½‘ç¯å¢ƒä¸‹çš„æŒ‰éœ€å¤šè½®æœç´¢ã€‚æ¡†æ¶é›†æˆäº†å›¾åƒå’Œæ–‡æœ¬æœç´¢å·¥å…·ï¼Œé€šè¿‡ç»“æœå¯¼å‘çš„å¥–åŠ±æœºåˆ¶å’Œæœç´¢æƒ©ç½šæ¥æŒ‡å¯¼æ¨¡å‹ä½•æ—¶ä»¥åŠå¦‚ä½•è°ƒç”¨è¿™äº›å·¥å…·ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ”¯æŒè®­ç»ƒï¼Œæœ¬æ–‡è¿˜é€šè¿‡åŠè‡ªåŠ¨åŒ–ç®¡é“æ”¶é›†äº†ä¸€ä¸ªå¤šæ¨¡æ€æœç´¢é—®ç­”æ•°æ®é›†ï¼Œå¹¶ç­›é€‰å‡ºä¸€ä¸ªæ—¢åŒ…å«éœ€è¦æœç´¢çš„æ ·æœ¬åˆåŒ…å«ä¸éœ€è¦æœç´¢çš„æ ·æœ¬çš„æœç´¢å¹³è¡¡å­é›†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ä¸ä»…åœ¨çŸ¥è¯†å¯†é›†å‹å’Œä¿¡æ¯æœå¯»é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜äºåŸºäºRAGçš„åŸºçº¿æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨å‡å°‘æœç´¢è°ƒç”¨æ¬¡æ•°æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨çœŸå®åœºæ™¯åº”ç”¨éœ€è®¿é—®å¤–éƒ¨çŸ¥è¯†æºï¼Œå¤„ç†å¤æ‚ä¸”åŠ¨æ€çš„ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚RAGå’Œæç¤ºå·¥ç¨‹æœç´¢ä»£ç†å­˜åœ¨æ•ˆç‡ä½ä¸‹æˆ–è¿‡åº¦æœç´¢çš„é—®é¢˜ã€‚</li>
<li>MMSearch-R1æ˜¯é¦–ä¸ªç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿LMMsèƒ½åœ¨çœŸå®äº’è”ç½‘ç¯å¢ƒä¸­è¿›è¡ŒæŒ‰éœ€å¤šè½®æœç´¢ã€‚</li>
<li>æ¡†æ¶é›†æˆäº†å›¾åƒå’Œæ–‡æœ¬æœç´¢å·¥å…·ï¼Œç”±ç»“æœå¯¼å‘çš„å¥–åŠ±å’Œæœç´¢æƒ©ç½šæœºåˆ¶å¼•å¯¼æ¨¡å‹å†³å®šä½•æ—¶åŠå¦‚ä½•è°ƒç”¨è¿™äº›å·¥å…·ã€‚</li>
<li>ä½¿ç”¨äº†åŠè‡ªåŠ¨åŒ–ç®¡é“æ”¶é›†å¤šæ¨¡æ€æœç´¢é—®ç­”æ•°æ®é›†ï¼Œå¹¶ç­›é€‰å‡ºæœç´¢å¹³è¡¡å­é›†ï¼Œå¯¹å¡‘é€ é«˜æ•ˆä¸”æŒ‰éœ€çš„æœç´¢è¡Œä¸ºè‡³å…³é‡è¦ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMMMsearch-R1åœ¨çŸ¥è¯†å¯†é›†å‹å’Œä¿¡æ¯æœå¯»é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œä¸ä»…æ€§èƒ½æ›´ä½³ï¼Œè¿˜èƒ½å‡å°‘è¶…è¿‡30%çš„æœç´¢è°ƒç”¨æ¬¡æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8fb5eaccebbb2cfdbe007d11a1d70a26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40749615b0bf0810a22333cb4625d06e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-290213ccc59b187b55ad11cbc1999407.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="The-Decrypto-Benchmark-for-Multi-Agent-Reasoning-and-Theory-of-Mind"><a href="#The-Decrypto-Benchmark-for-Multi-Agent-Reasoning-and-Theory-of-Mind" class="headerlink" title="The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind"></a>The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind</h2><p><strong>Authors:Andrei Lupu, Timon Willi, Jakob Foerster</strong></p>
<p>As Large Language Models (LLMs) gain agentic abilities, they will have to navigate complex multi-agent scenarios, interacting with human users and other agents in cooperative and competitive settings. This will require new reasoning skills, chief amongst them being theory of mind (ToM), or the ability to reason about the â€œmentalâ€ states of other agents. However, ToM and other multi-agent abilities in LLMs are poorly understood, since existing benchmarks suffer from narrow scope, data leakage, saturation, and lack of interactivity. We thus propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM drawing inspiration from cognitive science, computational pragmatics and multi-agent reinforcement learning. It is designed to be as easy as possible in all other dimensions, eliminating confounding factors commonly found in other benchmarks. To our knowledge, it is also the first platform for designing interactive ToM experiments.   We validate the benchmark design through comprehensive empirical evaluations of frontier LLMs, robustness studies, and human-AI cross-play experiments. We find that LLM game-playing abilities lag behind humans and simple word-embedding baselines. We then create variants of two classic cognitive science experiments within Decrypto to evaluate three key ToM abilities. Surprisingly, we find that state-of-the-art reasoning models are significantly worse at those tasks than their older counterparts. This demonstrates that Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and paves the path towards better artificial agents. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è·å¾—ä»£ç†èƒ½åŠ›ï¼Œå®ƒä»¬å°†éœ€è¦åº”å¯¹å¤æ‚çš„å¤šä»£ç†åœºæ™¯ï¼Œåœ¨åˆä½œå’Œç«äº‰ç¯å¢ƒä¸­ä¸äººç±»ç”¨æˆ·å’Œå…¶ä»–ä»£ç†è¿›è¡Œäº¤äº’ã€‚è¿™å°†éœ€è¦æ–°çš„æ¨ç†èƒ½åŠ›ï¼Œå…¶ä¸­æœ€ä¸»è¦çš„æ˜¯å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰èƒ½åŠ›ï¼Œå³æ¨ç†å…¶ä»–ä»£ç†çš„â€œç²¾ç¥â€çŠ¶æ€çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMä¸­çš„å¿ƒæ™ºç†è®ºå’Œå…¶ä»–å¤šä»£ç†èƒ½åŠ›å°šå¾…ç†è§£ï¼Œå› ä¸ºç°æœ‰çš„åŸºå‡†æµ‹è¯•å­˜åœ¨èŒƒå›´ç‹­çª„ã€æ•°æ®æ³„éœ²ã€é¥±å’Œå’Œç¼ºä¹äº¤äº’æ€§çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ¸¸æˆçš„åŸºå‡†æµ‹è¯•Decryptoï¼Œç”¨äºå¤šä»£ç†æ¨ç†å’Œå¿ƒæ™ºç†è®ºï¼Œä»è®¤çŸ¥ç§‘å­¦ã€è®¡ç®—è¯­ç”¨å­¦å’Œå¤šä»£ç†å¼ºåŒ–å­¦ä¹ ä¸­æ±²å–çµæ„Ÿã€‚å®ƒçš„è®¾è®¡åœ¨æ‰€æœ‰å…¶ä»–ç»´åº¦ä¸Šå°½å¯èƒ½ç®€å•ï¼Œæ¶ˆé™¤äº†åœ¨å…¶ä»–åŸºå‡†æµ‹è¯•ä¸­é€šå¸¸å­˜åœ¨çš„æ··æ·†å› ç´ ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒä¹Ÿæ˜¯è®¾è®¡äº¤äº’å¼å¿ƒæ™ºç†è®ºå®éªŒçš„ç¬¬ä¸€ä¸ªå¹³å°ã€‚æˆ‘ä»¬é€šè¿‡å…¨é¢è¯„ä¼°å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€ç¨³å¥æ€§ç ”ç©¶å’Œäººæœºäº¤å‰å®éªŒæ¥éªŒè¯åŸºå‡†æµ‹è¯•çš„è®¾è®¡ã€‚æˆ‘ä»¬å‘ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¸¸æˆèƒ½åŠ›è½åäºäººç±»å’Œç®€å•çš„è¯åµŒå…¥åŸºçº¿ã€‚ç„¶åæˆ‘ä»¬åœ¨Decryptoä¸­åˆ›å»ºäº†ä¸¤ç§ç»å…¸è®¤çŸ¥ç§‘å­¦å®éªŒçš„å˜ä½“ï¼Œä»¥è¯„ä¼°ä¸‰ç§å…³é”®çš„å¿ƒæ™ºç†è®ºèƒ½åŠ›ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜æ˜¾é€Šè‰²äºå®ƒä»¬çš„æ—§ç‰ˆæœ¬æ¨¡å‹ã€‚è¿™è¡¨æ˜Decryptoè§£å†³äº†å½“å‰æ¨ç†å’Œå¿ƒæ™ºç†è®ºè¯„ä¼°ä¸­çš„å…³é”®å·®è·ï¼Œå¹¶ä¸ºæ›´å¥½çš„äººå·¥æ™ºèƒ½ä»£ç†çš„å‘å±•é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20664v1">PDF</a> 41 pages, 19 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è·å¾—ä»£ç†èƒ½åŠ›åå°†éœ€è¦åº”å¯¹å¤æ‚çš„å¤šä»£ç†åœºæ™¯ï¼Œéœ€è¦åœ¨åˆä½œå’Œç«äº‰ç¯å¢ƒä¸­ä¸äººç±»ç”¨æˆ·å’Œå…¶ä»–ä»£ç†è¿›è¡Œäº¤äº’ã€‚è¿™è¦æ±‚æ–°çš„æ¨ç†èƒ½åŠ›ï¼Œå…¶ä¸­ä¸»è¦æ˜¯å¿ƒç†ç†è®ºï¼ˆToMï¼‰ï¼Œå³æ¨ç†å…¶ä»–ä»£ç†çš„â€œå¿ƒç†â€çŠ¶æ€çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒToMå’ŒLLMsä¸­çš„å…¶ä»–å¤šä»£ç†èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†ç†è§£ï¼Œå› ä¸ºç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨èŒƒå›´ç‹­çª„ã€æ•°æ®æ³„éœ²ã€é¥±å’Œå’Œç¼ºä¹äº¤äº’ç­‰é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ¸¸æˆçš„åŸºå‡†æµ‹è¯•Decryptoï¼Œç”¨äºå¤šä»£ç†æ¨ç†å’ŒToMï¼Œçµæ„Ÿæ¥è‡ªè®¤çŸ¥ç§‘å­¦ã€è®¡ç®—è¯­ç”¨å­¦å’Œå¤šä»£ç†å¼ºåŒ–å­¦ä¹ ã€‚å®ƒæ˜¯ä¸ºäº†å°½å¯èƒ½åœ¨å…¶ä»–ç»´åº¦ä¸Šç®€åŒ–è€Œè®¾è®¡çš„ï¼Œæ¶ˆé™¤äº†å…¶ä»–åŸºå‡†æµ‹è¯•ä¸­é€šå¸¸å­˜åœ¨çš„æ··æ·†å› ç´ ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒä¹Ÿæ˜¯è®¾è®¡äº¤äº’å¼ToMå®éªŒçš„ç¬¬ä¸€ä¸ªå¹³å°ã€‚æˆ‘ä»¬é€šè¿‡å…¨é¢çš„å®è¯è¯„ä¼°ã€ç¨³å¥æ€§ç ”ç©¶å’Œäººæœºäº¤å‰å®éªŒéªŒè¯äº†åŸºå‡†æµ‹è¯•è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘ç°LLMçš„æ¸¸æˆèƒ½åŠ›è½åäºäººç±»å’Œç®€å•çš„è¯åµŒå…¥åŸºçº¿ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨Decryptoå†…åˆ›å»ºäº†ä¸¤ä¸ªç»å…¸è®¤çŸ¥ç§‘å­¦å®éªŒçš„å˜ç§ï¼Œä»¥è¯„ä¼°ä¸‰ç§å…³é”®çš„ToMèƒ½åŠ›ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¸å¦‚æ—§æ¨¡å‹ã€‚è¿™è¡¨æ˜Decryptoè§£å†³äº†å½“å‰æ¨ç†å’ŒToMè¯„ä¼°ä¸­çš„å…³é”®å·®è·ï¼Œå¹¶ä¸ºæ›´å¥½çš„äººå·¥æ™ºèƒ½ä»£ç†é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è·å¾—ä»£ç†èƒ½åŠ›åé¢ä¸´å¤šä»£ç†åœºæ™¯çš„å¤æ‚æ€§ï¼Œéœ€å…·å¤‡æ–°çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯å¿ƒç†ç†è®ºï¼ˆToMï¼‰ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨ç¼ºé™·ï¼Œå¦‚èŒƒå›´ç‹­çª„ã€æ•°æ®æ³„éœ²å’Œç¼ºä¹äº¤äº’ï¼Œå¯¼è‡´å¯¹LLMsä¸­ToMå’Œå…¶ä»–å¤šä»£ç†èƒ½åŠ›çš„ç†è§£ä¸è¶³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ¸¸æˆçš„åŸºå‡†æµ‹è¯•Decryptoï¼Œç”¨äºè¯„ä¼°å¤šä»£ç†æ¨ç†å’ŒToMï¼Œç»“åˆè®¤çŸ¥ç§‘å­¦ã€è®¡ç®—è¯­ç”¨å­¦å’Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>Decryptoè®¾è®¡ç®€æ´ï¼Œæ¶ˆé™¤äº†å…¶ä»–åŸºå‡†æµ‹è¯•ä¸­çš„æ··æ·†å› ç´ ï¼Œæ˜¯é¦–ä¸ªäº¤äº’å¼ToMå®éªŒå¹³å°ã€‚</li>
<li>LLMåœ¨æ¸¸æˆèƒ½åŠ›æ–¹é¢è½åäºäººç±»å’Œè¯åµŒå…¥åŸºçº¿ã€‚</li>
<li>é€šè¿‡å˜ç§å®éªŒå‘ç°ï¼Œæœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸å¦‚æ—§æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2edcf303d1122d08144611c632f2f348.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f3c1ed44efb0cbfb794708947c3304d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4618ab1b41a9c4225ff843cb752c5c28.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Towards-Community-Driven-Agents-for-Machine-Learning-Engineering"><a href="#Towards-Community-Driven-Agents-for-Machine-Learning-Engineering" class="headerlink" title="Towards Community-Driven Agents for Machine Learning Engineering"></a>Towards Community-Driven Agents for Machine Learning Engineering</h2><p><strong>Authors:Sijie Li, Weiwei Sun, Shanda Li, Ameet Talwalkar, Yiming Yang</strong></p>
<p>Large language model-based machine learning (ML) agents have shown great promise in automating ML research. However, existing agents typically operate in isolation on a given research problem, without engaging with the broader research community, where human researchers often gain insights and contribute by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live evaluation framework designed to assess an agentâ€™s ability to communicate with and leverage collective knowledge from a simulated Kaggle research community. Building on this framework, we propose CoMind, a novel agent that excels at exchanging insights and developing novel solutions within a community context. CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2% human competitors on average across four ongoing Kaggle competitions. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/comind-ml/CoMind">https://github.com/comind-ml/CoMind</a>. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä»£ç†åœ¨è‡ªåŠ¨åŒ–MLç ”ç©¶æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä»£ç†é€šå¸¸åœ¨ä¸€ä¸ªç»™å®šçš„ç ”ç©¶é—®é¢˜ä¸Šå­¤ç«‹è¿è¡Œï¼Œè€Œæ²¡æœ‰ä¸æ›´å¹¿æ³›çš„ç ”ç©¶ç¤¾åŒºæ¥è§¦ï¼Œè€Œäººç±»ç ”ç©¶äººå‘˜å¾€å¾€é€šè¿‡åˆ†äº«çŸ¥è¯†è·å¾—æ´å¯ŸåŠ›å¹¶åšå‡ºè´¡çŒ®ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MLE-Liveï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°ä»£ç†ä¸æ¨¡æ‹ŸKaggleç ”ç©¶ç¤¾åŒºè¿›è¡Œäº¤æµå¹¶åˆ©ç”¨é›†ä½“çŸ¥è¯†çš„èƒ½åŠ›çš„å®æ—¶è¯„ä¼°æ¡†æ¶ã€‚åœ¨æ­¤æ¡†æ¶çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†CoMindï¼Œè¿™æ˜¯ä¸€ä¸ªæ“…é•¿åœ¨ç¤¾åŒºç¯å¢ƒä¸­äº¤æµè§è§£å¹¶å¼€å‘æ–°è§£å†³æ–¹æ¡ˆçš„æ–°å‹ä»£ç†ã€‚CoMindåœ¨MLE-Liveä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å››ä¸ªæ­£åœ¨è¿›è¡Œçš„Kaggleç«èµ›ä¸­å¹³å‡å‡»è´¥äº†79.2%çš„äººç±»ç«äº‰å¯¹æ‰‹ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/comind-ml/CoMind%E3%80%82">https://github.com/comind-ml/CoMindã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20640v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ç ”ç©¶æ–¹é¢ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä»£ç†è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ä¸ºè§£å†³ç°æœ‰ä»£ç†åœ¨å¤„ç†ç‰¹å®šé—®é¢˜æ—¶å­¤ç«‹è¿è¡Œã€æ— æ³•ä¸æ›´å¹¿æ³›çš„ç¤¾åŒºè¿›è¡Œäº¤æµäº’åŠ¨çš„é—®é¢˜ï¼Œç ”ç©¶è€…å¼•å…¥äº†MLE-Liveè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°ä»£ç†ä¸æ¨¡æ‹ŸKaggleç ”ç©¶ç¤¾åŒºè¿›è¡Œæ²Ÿé€šå’Œåˆ©ç”¨é›†ä½“çŸ¥è¯†çš„èƒ½åŠ›ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºäº†CoMindè¿™ä¸€æ–°å‹ä»£ç†ï¼Œæ“…é•¿åœ¨ç¤¾åŒºç¯å¢ƒä¸­äº¤æµè§è§£å¹¶å¼€å‘æ–°é¢–è§£å†³æ–¹æ¡ˆã€‚CoMindåœ¨MLE-Liveä¸Šè¡¨ç°å“è¶Šï¼Œå¹¶åœ¨å››ä¸ªæŒç»­è¿›è¡Œçš„Kaggleç«èµ›ä¸­å¹³å‡å‡»è´¥äº†79.2%çš„äººç±»ç«äº‰å¯¹æ‰‹ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šï¼š<a target="_blank" rel="noopener" href="https://github.com/comind-ml/CoMind%E3%80%82">https://github.com/comind-ml/CoMindã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æœºå™¨å­¦ä¹ ä»£ç†åœ¨è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ç ”ç©¶é¢†åŸŸå±•ç°æ½œåŠ›ã€‚</li>
<li>ç›®å‰å­˜åœ¨çš„ä»£ç†ä¸»è¦åœ¨å­¤ç«‹ç¯å¢ƒä¸­è¿è¡Œï¼Œç¼ºä¹ä¸å¹¿å¤§ç ”ç©¶ç¤¾åŒºçš„äº’åŠ¨ã€‚</li>
<li>MLE-Liveè¯„ä¼°æ¡†æ¶æ—¨åœ¨è¯„ä¼°ä»£ç†ä¸æ¨¡æ‹ŸKaggleç ”ç©¶ç¤¾åŒºäº¤æµçš„èƒ½åŠ›ã€‚</li>
<li>CoMindæ˜¯ä¸€ä¸ªåŸºäºMLE-Liveæ¡†æ¶æ„å»ºçš„æ–°å‹ä»£ç†ï¼Œæ“…é•¿åœ¨ç¤¾åŒºç¯å¢ƒä¸­äº¤æµå’Œå¼€å‘è§£å†³æ–¹æ¡ˆã€‚</li>
<li>CoMindåœ¨æ¨¡æ‹Ÿçš„Kaggleç«èµ›ç¯å¢ƒä¸­è¡¨ç°å“è¶Šï¼Œè¶…è¿‡äº†å¤§éƒ¨åˆ†äººç±»å‚èµ›è€…ã€‚</li>
<li>CoMindå·²åœ¨GitHubä¸Šå…¬å¼€å‘å¸ƒï¼Œä»¥ä¾›ç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20640">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aa54024434c0ed81ac5ec7aee270d998.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-903826177fc7ccba3c3d21d1685e3579.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b80b5fe056a8daa761053ec93500776e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DiffuCoder-Understanding-and-Improving-Masked-Diffusion-Models-for-Code-Generation"><a href="#DiffuCoder-Understanding-and-Improving-Masked-Diffusion-Models-for-Code-Generation" class="headerlink" title="DiffuCoder: Understanding and Improving Masked Diffusion Models for Code   Generation"></a>DiffuCoder: Understanding and Improving Masked Diffusion Models for Code   Generation</h2><p><strong>Authors:Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, Yizhe Zhang</strong></p>
<p>Diffusion large language models (dLLMs) are compelling alternatives to autoregressive (AR) models because their denoising models operate over the entire sequence. The global planning and iterative refinement features of dLLMs are particularly useful for code generation. However, current training and inference mechanisms for dLLMs in coding are still under-explored. To demystify the decoding behavior of dLLMs and unlock their potential for coding, we systematically investigate their denoising processes and reinforcement learning (RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code. Using this model as a testbed, we analyze its decoding behavior, revealing how it differs from that of AR models: (1) dLLMs can decide how causal their generation should be without relying on semi-AR decoding, and (2) increasing the sampling temperature diversifies not only token choices but also their generation order. This diversity creates a rich search space for RL rollouts. For RL training, to reduce the variance of token log-likelihood estimates and maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel sampling scheme that constructs complementary mask noise for completions used in training. In our experiments, coupled-GRPO significantly improves DiffuCoderâ€™s performance on code generation benchmarks (+4.4% on EvalPlus) and reduces reliance on AR causal during decoding. Our work provides deeper insight into the machinery of dLLM generation and offers an effective, diffusion-native RL training framework. <a target="_blank" rel="noopener" href="https://github.com/apple/ml-diffucoder">https://github.com/apple/ml-diffucoder</a>. </p>
<blockquote>
<p>æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰æ˜¯ä»¤äººä¿¡æœçš„è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå› ä¸ºå®ƒä»¬çš„é™å™ªæ¨¡å‹åœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿è¡Œã€‚dLLMsçš„å…¨å±€è§„åˆ’å’Œè¿­ä»£ä¼˜åŒ–åŠŸèƒ½å¯¹äºä»£ç ç”Ÿæˆç‰¹åˆ«æœ‰ç”¨ã€‚ç„¶è€Œï¼Œç›®å‰é’ˆå¯¹ç¼–ç ä¸­çš„dLLMçš„è®­ç»ƒå’Œæ¨ç†æœºåˆ¶ä»è¢«æ¢ç´¢ä¸è¶³ã€‚ä¸ºäº†æ­ç¤ºdLLMsçš„è§£ç è¡Œä¸ºå¹¶è§£é”å…¶åœ¨ç¼–ç æ–¹é¢çš„æ½œåŠ›ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†å®ƒä»¬çš„é™å™ªè¿‡ç¨‹å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨130Bä»£ç æ ‡è®°ä¸Šè®­ç»ƒäº†ä¸€ä¸ª7Bçš„dLLMï¼Œåä¸º<strong>DiffuCoder</strong>ã€‚ä»¥æ­¤æ¨¡å‹ä¸ºæµ‹è¯•å¹³å°ï¼Œæˆ‘ä»¬åˆ†æäº†å…¶è§£ç è¡Œä¸ºï¼Œæ­ç¤ºäº†å…¶ä¸ARæ¨¡å‹çš„ä¸åŒä¹‹å¤„ï¼šï¼ˆ1ï¼‰dLLMèƒ½å¤Ÿå†³å®šå…¶ç”Ÿæˆåº”è¯¥å¦‚ä½•å› æœå…³ç³»ï¼Œè€Œæ— éœ€ä¾èµ–åŠè‡ªå›å½’è§£ç ï¼›ï¼ˆ2ï¼‰å¢åŠ é‡‡æ ·æ¸©åº¦ä¸ä»…ä½¿æ ‡è®°é€‰æ‹©å¤šæ ·åŒ–ï¼Œè¿˜ä½¿å…¶ç”Ÿæˆé¡ºåºå¤šæ ·åŒ–ã€‚è¿™ç§å¤šæ ·æ€§ä¸ºRLå›æ»šåˆ›å»ºäº†ä¸€ä¸ªä¸°å¯Œçš„æœç´¢ç©ºé—´ã€‚å¯¹äºRLè®­ç»ƒï¼Œä¸ºäº†å‡å°‘æ ‡è®°å¯¹æ•°ä¼¼ç„¶ä¼°è®¡çš„æ–¹å·®å¹¶ä¿æŒè®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>coupled-GRPO</strong>ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„é‡‡æ ·æ–¹æ¡ˆï¼Œç”¨äºæ„å»ºç”¨äºè®­ç»ƒä¸­çš„å®Œæˆçš„äº’è¡¥æ©ç å™ªå£°ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œcoupled-GRPOæ˜¾è‘—æé«˜äº†DiffuCoderåœ¨ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼ˆåœ¨EvalPlusä¸Šæé«˜äº†4.4ï¼…ï¼‰ï¼Œå¹¶å‡å°‘äº†è§£ç è¿‡ç¨‹ä¸­ARå› æœçš„ä¾èµ–ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºdLLMç”Ÿæˆæœºåˆ¶æä¾›äº†æ›´æ·±å…¥çš„äº†è§£ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„ã€åŸºäºæ‰©æ•£çš„RLè®­ç»ƒæ¡†æ¶ã€‚è¯¦æƒ…è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/apple/ml-diffucoder">https://github.com/apple/ml-diffucoder</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20639v1">PDF</a> preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„ä¼˜åŠ¿åŠå…¶è§£ç è¡Œä¸ºçš„ç ”ç©¶ã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªè§„æ¨¡ä¸º7Bçš„dLLMæ¨¡å‹DiffuCoderï¼Œå¹¶å¯¹å…¶è¿›è¡Œç³»ç»Ÿåˆ†æï¼Œå‘ç°dLLMså…·æœ‰å…¨å±€è§„åˆ’å’Œè¿­ä»£ä¼˜åŒ–çš„ç‰¹ç‚¹ï¼Œå…¶è§£ç è¡Œä¸ºä¸è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹æœ‰æ‰€ä¸åŒã€‚ç ”ç©¶è¿˜æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨dLLMsè®­ç»ƒä¸­çš„åº”ç”¨ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„é‡‡æ ·æ–¹æ¡ˆcoupled-GRPOï¼Œä»¥æé«˜ä»£ç ç”Ÿæˆæ€§èƒ½å¹¶å‡å°‘è§£ç è¿‡ç¨‹ä¸­å¯¹ARçš„ä¾èµ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>dLLMsæ˜¯ä»£ç ç”Ÿæˆé¢†åŸŸå…·æœ‰å¸å¼•åŠ›çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå…·æœ‰å…¨å±€è§„åˆ’å’Œè¿­ä»£ä¼˜åŒ–ç‰¹ç‚¹ã€‚</li>
<li>dLLMsçš„è§£ç è¡Œä¸ºä¸è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ä¸åŒï¼Œå¯ä»¥å†³å®šç”Ÿæˆçš„å› æœæ€§ï¼Œå¹¶ä¸”å¢åŠ é‡‡æ ·æ¸©åº¦å¯ä»¥å¤šæ ·åŒ–ç”Ÿæˆé¡ºåºã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨dLLMsè®­ç»ƒä¸­èµ·ç€é‡è¦ä½œç”¨ï¼Œå¯ä»¥å‡å°‘token log-likelihoodä¼°è®¡çš„æ–¹å·®å¹¶ä¿æŒè®­ç»ƒæ•ˆç‡ã€‚</li>
<li>coupled-GRPOæ˜¯ä¸€ç§æ–°å‹çš„é‡‡æ ·æ–¹æ¡ˆï¼Œä¸ºdLLMsçš„è®­ç»ƒæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ‰©æ•£åŸç”ŸRLè®­ç»ƒæ¡†æ¶ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8a5b0b51e74200edd6980f3bfbdb579.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fab6f03b8677a7805ff29cfa9802755.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-894b6e6328ece7c23bc3d00fd9739b61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc7edce831c554d0a6d32308f871a1e8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PLoP-Precise-LoRA-Placement-for-Efficient-Finetuning-of-Large-Models"><a href="#PLoP-Precise-LoRA-Placement-for-Efficient-Finetuning-of-Large-Models" class="headerlink" title="PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models"></a>PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models</h2><p><strong>Authors:Soufiane Hayou, Nikhil Ghosh, Bin Yu</strong></p>
<p>Low-Rank Adaptation (LoRA) is a widely used finetuning method for large models. Its small memory footprint allows practitioners to adapt large models to specific tasks at a fraction of the cost of full finetuning. Different modifications have been proposed to enhance its efficiency by, for example, setting the learning rate, the rank, and the initialization. Another improvement axis is adapter placement strategy: when using LoRA, practitioners usually pick module types to adapt with LoRA, such as Query and Key modules. Few works have studied the problem of adapter placement, with nonconclusive results: original LoRA paper suggested placing adapters in attention modules, while other works suggested placing them in the MLP modules. Through an intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a lightweight method that allows automatic identification of module types where LoRA adapters should be placed, given a pretrained model and a finetuning task. We demonstrate that PLoP consistently outperforms, and in the worst case competes, with commonly used placement strategies through comprehensive experiments on supervised finetuning and reinforcement learning for reasoning. </p>
<blockquote>
<p>ä½ç§©é€‚é…ï¼ˆLoRAï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºå¤§å‹æ¨¡å‹çš„å¾®è°ƒæ–¹æ³•ã€‚å®ƒè¾ƒå°çš„å†…å­˜å ç”¨ç‡ä½¿å¾—å®è·µè€…èƒ½å¤Ÿä»¥å…¨é‡å¾®è°ƒçš„ä¸€å°éƒ¨åˆ†æˆæœ¬å°†å¤§å‹æ¨¡å‹é€‚é…åˆ°ç‰¹å®šä»»åŠ¡ä¸Šã€‚ä¸ºäº†æå‡æ•ˆç‡ï¼Œå·²ç»æå‡ºäº†å„ç§æ”¹è‰¯æ–¹æ³•ï¼Œä¾‹å¦‚è®¾ç½®å­¦ä¹ ç‡ã€ç­‰çº§å’Œåˆå§‹åŒ–ç­‰ã€‚å¦ä¸€ä¸ªæ”¹è¿›æ–¹å‘æ˜¯é€‚é…å™¨æ”¾ç½®ç­–ç•¥ï¼šåœ¨ä½¿ç”¨LoRAæ—¶ï¼Œå®è·µè€…é€šå¸¸ä¼šé€‰æ‹©ä½¿ç”¨LoRAé€‚é…çš„æ¨¡å—ç±»å‹ï¼Œä¾‹å¦‚æŸ¥è¯¢å’Œé”®æ¨¡å—ã€‚å…³äºé€‚é…å™¨æ”¾ç½®çš„é—®é¢˜ç ”ç©¶è¾ƒå°‘ï¼Œä¸”ç»“æœå°šæ— å®šè®ºï¼šåŸå§‹LoRAè®ºæ–‡å»ºè®®åœ¨æ³¨æ„åŠ›æ¨¡å—ä¸­æ”¾ç½®é€‚é…å™¨ï¼Œè€Œå…¶ä»–ç ”ç©¶åˆ™å»ºè®®å°†å®ƒä»¬æ”¾ç½®åœ¨MLPæ¨¡å—ä¸­ã€‚é€šè¿‡ç›´è§‚çš„ç†è®ºåˆ†æï¼Œæˆ‘ä»¬å¼•å…¥äº†PLoPï¼ˆç²¾ç¡®LoRAæ”¾ç½®ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ç»™å®šé¢„è®­ç»ƒæ¨¡å‹å’Œå¾®è°ƒä»»åŠ¡çš„æƒ…å†µä¸‹è‡ªåŠ¨è¯†åˆ«åº”æ”¾ç½®LoRAé€‚é…å™¨çš„æ¨¡å—ç±»å‹ã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡çš„å®éªŒæ¼”ç¤ºäº†PLoPåœ¨ç›‘ç£å¾®è°ƒä»¥åŠå¼ºåŒ–å­¦ä¹ æ¨ç†æ–¹é¢çš„ä¸€è‡´ä¼˜åŠ¿ï¼Œåœ¨æœ€åçš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¸å¸¸ç”¨æ”¾ç½®ç­–ç•¥ç›¸ç«äº‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20629v1">PDF</a> TD,LR: A lightweight module type selection method for LoRA   finetuning. PLoP gives precise placements for LoRA adapters for improved   performance</p>
<p><strong>Summary</strong></p>
<p>LoRAï¼ˆä½ç§©é€‚åº”ï¼‰æ˜¯ä¸€ç§ç”¨äºå¤§å‹æ¨¡å‹çš„å¾®è°ƒæ–¹æ³•ï¼Œå› å…¶è¾ƒå°çš„å†…å­˜å ç”¨è€Œå¤‡å—é’çã€‚ä¸ºæé«˜æ•ˆç‡ï¼Œäººä»¬å¯¹å…¶è¿›è¡Œäº†å¤šç§æ”¹è¿›ï¼Œå¦‚è®¾ç½®å­¦ä¹ ç‡ã€ç­‰çº§å’Œåˆå§‹åŒ–ç­‰ã€‚æœ€è¿‘çš„ç ”ç©¶å…³æ³¨äºé€‚é…å™¨æ”¾ç½®ç­–ç•¥ï¼Œå³é€‰æ‹©å“ªäº›æ¨¡å—ç±»å‹ä½¿ç”¨LoRAé€‚é…å™¨è¿›è¡Œé€‚åº”ã€‚æœ¬æ–‡é€šè¿‡ç›´è§‚çš„ç†è®ºåˆ†æï¼Œä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•PLoPï¼ˆç²¾ç¡®LoRAæ”¾ç½®ï¼‰ï¼Œå¯è‡ªåŠ¨è¯†åˆ«åº”æ”¾ç½®LoRAé€‚é…å™¨çš„æ¨¡å—ç±»å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒPLoPåœ¨ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºå¸¸è§„æ”¾ç½®ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoRAæ˜¯ä¸€ç§ç”¨äºå¤§å‹æ¨¡å‹çš„å¾®è°ƒæ–¹æ³•ï¼Œå…·æœ‰è¾ƒå°çš„å†…å­˜å ç”¨å’Œè¾ƒä½çš„æˆæœ¬ã€‚</li>
<li>LoRAçš„æ•ˆç‡å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼è¿›è¡Œæ”¹è¿›ï¼ŒåŒ…æ‹¬è®¾ç½®å­¦ä¹ ç‡ã€ç­‰çº§å’Œåˆå§‹åŒ–ç­‰ã€‚</li>
<li>é€‚é…å™¨æ”¾ç½®ç­–ç•¥æ˜¯LoRAçš„ä¸€ä¸ªé‡è¦æ”¹è¿›æ–¹å‘ï¼Œç›®å‰å¯¹æ­¤çš„ç ”ç©¶å°šå°‘ä¸”ç»“æœéç»“è®ºæ€§ã€‚</li>
<li>åŸLoRAè®ºæ–‡å»ºè®®å°†é€‚é…å™¨æ”¾ç½®åœ¨æ³¨æ„åŠ›æ¨¡å—ä¸­ï¼Œè€Œå…¶ä»–ç ”ç©¶åˆ™å»ºè®®æ”¾ç½®åœ¨MLPæ¨¡å—ä¸­ã€‚</li>
<li>PLoPæ–¹æ³•èƒ½å¤Ÿé€šè¿‡ç›´è§‚çš„ç†è®ºåˆ†æï¼Œè‡ªåŠ¨è¯†åˆ«åº”æ”¾ç½®LoRAé€‚é…å™¨çš„æ¨¡å—ç±»å‹ã€‚</li>
<li>PLoPåœ¨ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºå¸¸è§„æ”¾ç½®ç­–ç•¥ã€‚</li>
<li>PLoPçš„å¼•å…¥ä¸ºå¤§å‹æ¨¡å‹çš„å¾®è°ƒæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dfd3a05715d244cceefacbfff4901a79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-222fa9c9af3dc7627bb7b154ae5d8b62.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Video-Perception-Models-for-3D-Scene-Synthesis"><a href="#Video-Perception-Models-for-3D-Scene-Synthesis" class="headerlink" title="Video Perception Models for 3D Scene Synthesis"></a>Video Perception Models for 3D Scene Synthesis</h2><p><strong>Authors:Rui Huang, Guangyao Zhai, Zuria Bauer, Marc Pollefeys, Federico Tombari, Leonidas Guibas, Gao Huang, Francis Engelmann</strong></p>
<p>Traditionally, 3D scene synthesis requires expert knowledge and significant manual effort. Automating this process could greatly benefit fields such as architectural design, robotics simulation, virtual reality, and gaming. Recent approaches to 3D scene synthesis often rely on the commonsense reasoning of large language models (LLMs) or strong visual priors of modern image generation models. However, current LLMs demonstrate limited 3D spatial reasoning ability, which restricts their ability to generate realistic and coherent 3D scenes. Meanwhile, image generation-based methods often suffer from constraints in viewpoint selection and multi-view inconsistencies. In this work, we present Video Perception models for 3D Scene synthesis (VIPScene), a novel framework that exploits the encoded commonsense knowledge of the 3D physical world in video generation models to ensure coherent scene layouts and consistent object placements across views. VIPScene accepts both text and image prompts and seamlessly integrates video generation, feedforward 3D reconstruction, and open-vocabulary perception models to semantically and geometrically analyze each object in a scene. This enables flexible scene synthesis with high realism and structural consistency. For more precise analysis, we further introduce First-Person View Score (FPVScore) for coherence and plausibility evaluation, utilizing continuous first-person perspective to capitalize on the reasoning ability of multimodal large language models. Extensive experiments show that VIPScene significantly outperforms existing methods and generalizes well across diverse scenarios. The code will be released. </p>
<blockquote>
<p>ä¼ ç»Ÿä¸Šï¼Œ3Dåœºæ™¯åˆæˆéœ€è¦ä¸“ä¸šçŸ¥è¯†çš„å¤§é‡æŠ•å…¥å’Œæ˜¾è‘—çš„æ‰‹åŠ¨æ“ä½œã€‚è‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹å¯ä»¥ä¸ºå»ºç­‘è®¾è®¡ã€æœºå™¨äººä»¿çœŸã€è™šæ‹Ÿç°å®å’Œæ¸¸æˆç­‰é¢†åŸŸå¸¦æ¥å·¨å¤§çš„å¥½å¤„ã€‚æœ€è¿‘çš„3Dåœºæ™¯åˆæˆæ–¹æ³•å¸¸å¸¸ä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¸¸è¯†æ¨ç†æˆ–ç°ä»£å›¾åƒç”Ÿæˆæ¨¡å‹çš„å¼ºå¤§è§†è§‰å…ˆéªŒã€‚ç„¶è€Œï¼Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºæœ‰é™çš„3Dç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬ç”ŸæˆçœŸå®å’Œè¿è´¯çš„3Dåœºæ™¯çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼ŒåŸºäºå›¾åƒç”Ÿæˆçš„æ–¹æ³•ç»å¸¸å—åˆ°è§†ç‚¹é€‰æ‹©å’Œå¤šè§†å›¾ä¸ä¸€è‡´æ€§çš„çº¦æŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äº3Dåœºæ™¯åˆæˆçš„Video Perceptionæ¨¡å‹ï¼ˆVIPSceneï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­çš„3Dç‰©ç†ä¸–ç•Œå¸¸è¯†çŸ¥è¯†ç¼–ç çš„æ–°æ¡†æ¶ï¼Œä»¥ç¡®ä¿è·¨è§†å›¾çš„è¿è´¯åœºæ™¯å¸ƒå±€å’Œä¸€è‡´çš„å¯¹è±¡æ”¾ç½®ã€‚VIPSceneæ¥å—æ–‡æœ¬å’Œå›¾åƒæç¤ºï¼Œæ— ç¼é›†æˆè§†é¢‘ç”Ÿæˆã€å‰é¦ˆ3Dé‡å»ºå’Œå¼€æ”¾è¯æ±‡æ„ŸçŸ¥æ¨¡å‹ï¼Œä»¥è¯­ä¹‰å’Œå‡ ä½•æ–¹å¼åˆ†æåœºæ™¯ä¸­çš„æ¯ä¸ªå¯¹è±¡ã€‚è¿™å®ç°äº†å…·æœ‰é«˜åº¦çš„çœŸå®æ„Ÿå’Œç»“æ„ä¸€è‡´æ€§çš„çµæ´»åœºæ™¯åˆæˆã€‚ä¸ºäº†è¿›è¡Œæ›´ç²¾ç¡®çš„åˆ†æï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ç¬¬ä¸€äººç§°è§†è§’è¯„åˆ†ï¼ˆFPVScoreï¼‰æ¥è¿›è¡Œè¿è´¯æ€§å’Œå¯è¡Œæ€§è¯„ä¼°ï¼Œåˆ©ç”¨è¿ç»­çš„ç¬¬ä¸€äººç§°è§†è§’æ¥åˆ©ç”¨å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVIPSceneæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å„ç§åœºæ™¯ä¸­å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚ä»£ç å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20601v1">PDF</a> </p>
<p><strong>Summary</strong><br>    VIPSceneé€šè¿‡ç»“åˆè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­çš„ä¸‰ç»´ç‰©ç†ä¸–ç•Œå¸¸è¯†çŸ¥è¯†ï¼Œå®ç°äº†è¿è´¯çš„åœºæ™¯å¸ƒå±€å’Œè·¨è§†å›¾çš„ç‰©ä½“ä½ç½®ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•æ¥å—æ–‡æœ¬å’Œå›¾åƒæç¤ºï¼Œæ— ç¼é›†æˆè§†é¢‘ç”Ÿæˆã€å‰é¦ˆä¸‰ç»´é‡å»ºå’Œå¼€æ”¾è¯æ±‡æ„ŸçŸ¥æ¨¡å‹ï¼Œå®ç°åœºæ™¯çš„è¯­ä¹‰å’Œå‡ ä½•åˆ†æã€‚è¿™ä¸ºå…·æœ‰é«˜åº¦ç°å®æ„Ÿå’Œç»“æ„ä¸€è‡´æ€§çš„çµæ´»åœºæ™¯åˆæˆæä¾›äº†æ–°çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VIPSceneæ˜¯ä¸€ä¸ªç”¨äºä¸‰ç»´åœºæ™¯åˆæˆçš„æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿåˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­çš„ä¸‰ç»´ç‰©ç†ä¸–ç•Œå¸¸è¯†çŸ¥è¯†ã€‚</li>
<li>VIPSceneå¯ä»¥æ¥å—æ–‡æœ¬å’Œå›¾åƒæç¤ºï¼Œå¹¶é›†æˆäº†è§†é¢‘ç”Ÿæˆã€å‰é¦ˆä¸‰ç»´é‡å»ºå’Œå¼€æ”¾è¯æ±‡æ„ŸçŸ¥æ¨¡å‹ã€‚</li>
<li>VIPSceneèƒ½å¤Ÿç¡®ä¿åœºæ™¯è¿è´¯æ€§å’Œç‰©ä½“ä½ç½®çš„ä¸€è‡´æ€§ã€‚</li>
<li>VIPSceneå…·å¤‡çµæ´»çš„åœºæ™¯åˆæˆèƒ½åŠ›ï¼Œå…·æœ‰é«˜åº¦çš„ç°å®æ„Ÿå’Œç»“æ„ä¸€è‡´æ€§ã€‚</li>
<li>VIPSceneå¼•å…¥ç¬¬ä¸€äººç§°è§†è§’è¯„åˆ†ï¼ˆFPVScoreï¼‰æ¥è¯„ä¼°åœºæ™¯åˆæˆçš„è¿è´¯æ€§å’Œå¯ä¿¡åº¦ã€‚</li>
<li>VIPSceneåœ¨å®éªŒä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨å„ç§åœºæ™¯ä¸‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5fdd369ebe958ca3dee71570684739a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e376b160c3d4577103790f2e6ce08135.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-444ecfe291803ea1ade0e301005fc635.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5236392d5fadd6b1a9c1a6b0b3434d9c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Case-based-Reasoning-Augmented-Large-Language-Model-Framework-for-Decision-Making-in-Realistic-Safety-Critical-Driving-Scenarios"><a href="#Case-based-Reasoning-Augmented-Large-Language-Model-Framework-for-Decision-Making-in-Realistic-Safety-Critical-Driving-Scenarios" class="headerlink" title="Case-based Reasoning Augmented Large Language Model Framework for   Decision Making in Realistic Safety-Critical Driving Scenarios"></a>Case-based Reasoning Augmented Large Language Model Framework for   Decision Making in Realistic Safety-Critical Driving Scenarios</h2><p><strong>Authors:Wenbin Gan, Minh-Son Dao, Koji Zettsu</strong></p>
<p>Driving in safety-critical scenarios requires quick, context-aware decision-making grounded in both situational understanding and experiential reasoning. Large Language Models (LLMs), with their powerful general-purpose reasoning capabilities, offer a promising foundation for such decision-making. However, their direct application to autonomous driving remains limited due to challenges in domain adaptation, contextual grounding, and the lack of experiential knowledge needed to make reliable and interpretable decisions in dynamic, high-risk environments. To address this gap, this paper presents a Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for evasive maneuver decision-making in complex risk scenarios. Our approach integrates semantic scene understanding from dashcam video inputs with the retrieval of relevant past driving cases, enabling LLMs to generate maneuver recommendations that are both context-sensitive and human-aligned. Experiments across multiple open-source LLMs show that our framework improves decision accuracy, justification quality, and alignment with human expert behavior. Risk-aware prompting strategies further enhance performance across diverse risk types, while similarity-based case retrieval consistently outperforms random sampling in guiding in-context learning. Case studies further demonstrate the frameworkâ€™s robustness in challenging real-world conditions, underscoring its potential as an adaptive and trustworthy decision-support tool for intelligent driving systems. </p>
<blockquote>
<p>åœ¨å…³é”®çš„é©¾é©¶åœºæ™¯ä¸­ï¼Œé©¾é©¶å‘˜éœ€è¦åŸºäºæƒ…å¢ƒç†è§£å’Œç»éªŒæ¨ç†çš„å¿«é€Ÿã€çµæ´»çš„å†³ç­–èƒ½åŠ›ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥å…¶å¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä¸ºè¿™ç§å†³ç­–æä¾›äº†æœ‰å‰æ™¯çš„åŸºç¡€ã€‚ç„¶è€Œï¼Œå°†å…¶ç›´æ¥åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢†åŸŸé€‚åº”ã€ä¸Šä¸‹æ–‡å®šä½ä»¥åŠåœ¨åŠ¨æ€é«˜é£é™©ç¯å¢ƒä¸­åšå‡ºå¯é å’Œå¯è§£é‡Šå†³ç­–æ‰€éœ€ç»éªŒçŸ¥è¯†çš„ç¼ºä¹æ–¹é¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¡ˆä¾‹æ¨ç†çš„å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆCBR-LLMï¼‰æ¡†æ¶ï¼Œç”¨äºå¤æ‚é£é™©åœºæ™¯ä¸­çš„è§„é¿åŠ¨ä½œå†³ç­–ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä»è¡Œè½¦è®°å½•ä»ªè§†é¢‘è¾“å…¥ä¸­ç†è§£åœºæ™¯è¯­ä¹‰ä¸æ£€ç´¢ç›¸å…³å†å²é©¾é©¶æ¡ˆä¾‹ï¼Œä½¿LLMèƒ½å¤Ÿç”Ÿæˆæ—¢æ•æ„Ÿäºä¸Šä¸‹æ–‡åˆä¸äººç±»è¡Œä¸ºä¸€è‡´çš„æœºåŠ¨å»ºè®®ã€‚è·¨å¤šä¸ªå¼€æºLLMçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æé«˜äº†å†³ç­–å‡†ç¡®æ€§ã€è§£é‡Šè´¨é‡ä»¥åŠä¸äººç±»ä¸“å®¶è¡Œä¸ºçš„å¥‘åˆåº¦ã€‚é£é™©æ„ŸçŸ¥æç¤ºç­–ç•¥è¿›ä¸€æ­¥å¢å¼ºäº†å„ç§é£é™©ç±»å‹çš„æ€§èƒ½ï¼Œè€ŒåŸºäºç›¸ä¼¼æ€§çš„æ¡ˆä¾‹æ£€ç´¢åœ¨æŒ‡å¯¼ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹é¢å§‹ç»ˆä¼˜äºéšæœºæŠ½æ ·ã€‚æ¡ˆä¾‹ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†è¯¥æ¡†æ¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ï¼Œçªæ˜¾äº†å…¶ä½œä¸ºæ™ºèƒ½é©¾é©¶ç³»ç»Ÿçš„è‡ªé€‚åº”å’Œå¯ä¿¡èµ–çš„å†³ç­–æ”¯æŒå·¥å…·çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20531v1">PDF</a> 12 pages, 10 figures, under-review conference</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæƒ…å¢ƒç†è§£å’Œç»éªŒæ¨ç†çš„å¿«é€Ÿå†³ç­–å¯¹äºå®‰å…¨é©¾é©¶è‡³å…³é‡è¦ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºæ­¤ç±»å†³ç­–æä¾›äº†æœ‰åŠ›æ”¯æŒï¼Œä½†åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨ä»é¢ä¸´åŸŸé€‚åº”ã€ä¸Šä¸‹æ–‡å®šä½ä»¥åŠç¯å¢ƒç»éªŒçŸ¥è¯†çš„ç¼ºä¹ç­‰æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¡ˆä¾‹æ¨ç†è¾…åŠ©çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼ˆCBR-LLMï¼‰ï¼Œè¯¥æ¡†æ¶é€šè¿‡é©¾é©¶èˆ±è§†é¢‘è¾“å…¥å¯¹è¯­ä¹‰åœºæ™¯è¿›è¡Œç†è§£ï¼Œå¹¶æ£€ç´¢ç›¸å…³é©¾é©¶æ¡ˆä¾‹ï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ—¢ç¬¦åˆä¸Šä¸‹æ–‡åˆç¬¦åˆäººç±»è¡Œä¸ºçš„æœºåŠ¨å»ºè®®ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æé«˜äº†å†³ç­–å‡†ç¡®æ€§ã€è§£é‡Šè´¨é‡å’Œå¯¹äººç±»ä¸“å®¶è¡Œä¸ºçš„åŒ¹é…åº¦ã€‚é£é™©æ„ŸçŸ¥æç¤ºç­–ç•¥è¿›ä¸€æ­¥æé«˜äº†å„ç§é£é™©ç±»å‹çš„æ€§èƒ½ï¼ŒåŸºäºç›¸ä¼¼æ€§çš„æ¡ˆä¾‹æ£€ç´¢åœ¨æŒ‡å¯¼ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹é¢è¡¨ç°ä¼˜äºéšæœºæŠ½æ ·ã€‚æ¡ˆä¾‹ç ”ç©¶è¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤æ‚ç°å®æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ï¼Œå¯ä½œä¸ºæ™ºèƒ½é©¾é©¶ç³»ç»Ÿçš„è‡ªé€‚åº”å’Œå¯é å†³ç­–æ”¯æŒå·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„å†³ç­–åº”ç”¨å…·æœ‰æ½œåŠ›ï¼Œä½†éœ€è§£å†³åŸŸé€‚åº”ã€ä¸Šä¸‹æ–‡ç†è§£å’Œç¯å¢ƒç»éªŒçŸ¥è¯†ç¼ºä¹çš„é—®é¢˜ã€‚</li>
<li>CBR-LLMæ¡†æ¶ç»“åˆäº†è¯­ä¹‰åœºæ™¯ç†è§£ä¸ç›¸å…³é©¾é©¶æ¡ˆä¾‹æ£€ç´¢ï¼Œæé«˜è¯­è¨€æ¨¡å‹åœ¨å¤æ‚é£é™©åœºæ™¯ä¸­çš„å†³ç­–èƒ½åŠ›ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºCBR-LLMæ¡†æ¶èƒ½æé«˜å†³ç­–å‡†ç¡®æ€§ã€è§£é‡Šè´¨é‡ï¼Œå¹¶æ›´è´´è¿‘äººç±»ä¸“å®¶è¡Œä¸ºã€‚</li>
<li>é£é™©æ„ŸçŸ¥æç¤ºç­–ç•¥æœ‰åŠ©äºæå‡åœ¨å„ç§é£é™©åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</li>
<li>ç›¸ä¼¼æ€§åŸºç¡€ä¸Šçš„æ¡ˆä¾‹æ£€ç´¢åœ¨æŒ‡å¯¼è¯­è¨€æ¨¡å‹å­¦ä¹ æ–¹é¢æ¯”éšæœºæŠ½æ ·æ›´æœ‰æ•ˆã€‚</li>
<li>CBR-LLMæ¡†æ¶åœ¨æŒ‘æˆ˜ç°å®æ¡ä»¶ä¸‹å±•ç°å‡ºç¨³å¥æ€§ï¼Œèƒ½å¤Ÿä¸ºæ™ºèƒ½é©¾é©¶ç³»ç»Ÿæä¾›è‡ªé€‚åº”å’Œå¯é çš„å†³ç­–æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-99506f51adfb1599392df2d24eed2111.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8808f0e46c899b6c379d1c73736e07df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76b8c4c785e59e0621cf3d253c7c2a38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16039e115844d8790958561292ac8ae5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35f0ba200717aeef6838a1f239680f08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb27286930aa02cfabafdf41d3fe3046.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b42b5d524dfc51a829d87f365534559.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Asymmetric-REINFORCE-for-off-Policy-Reinforcement-Learning-Balancing-positive-and-negative-rewards"><a href="#Asymmetric-REINFORCE-for-off-Policy-Reinforcement-Learning-Balancing-positive-and-negative-rewards" class="headerlink" title="Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing   positive and negative rewards"></a>Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing   positive and negative rewards</h2><p><strong>Authors:Charles Arnal, GaÃ«tan Narozniak, Vivien Cabannes, Yunhao Tang, Julia Kempe, Remi Munos</strong></p>
<p>Reinforcement learning (RL) is increasingly used to align large language models (LLMs). Off-policy methods offer greater implementation simplicity and data efficiency than on-policy techniques, but often result in suboptimal performance. In this work, we study the intermediate range of algorithms between off-policy RL and supervised fine-tuning by analyzing a simple off-policy REINFORCE algorithm, where the advantage is defined as $A&#x3D;r-V$, with $r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$ emphasizes high-reward samples, while raising it penalizes low-reward ones more heavily. We first provide a theoretical analysis of this off-policy REINFORCE algorithm, showing that when the baseline $V$ lower-bounds the expected reward, the algorithm enjoys a policy improvement guarantee. Our analysis reveals that while on-policy updates can safely leverage both positive and negative signals, off-policy updates benefit from focusing more on positive rewards than on negative ones. We validate our findings experimentally in a controlled stochastic bandit setting and through fine-tuning state-of-the-art LLMs on reasoning tasks. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç›¸æ¯”on-policyæŠ€æœ¯ï¼Œoff-policyæ–¹æ³•åœ¨å®ç°ç®€å•æ€§å’Œæ•°æ®æ•ˆç‡æ–¹é¢æä¾›äº†æ›´å¤§çš„ä¼˜åŠ¿ï¼Œä½†é€šå¸¸ä¼šå¯¼è‡´æ¬¡ä¼˜æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æä¸€ä¸ªç®€å•çš„off-policy REINFORCEç®—æ³•æ¥ç ”ç©¶off-policy RLå’Œå—ç›‘ç£å¾®è°ƒä¹‹é—´çš„ç®—æ³•ä¸­ä»‹èŒƒå›´ï¼Œè¯¥ç®—æ³•ä¸­çš„ä¼˜åŠ¿å®šä¹‰ä¸ºA&#x3D;r-Vï¼Œå…¶ä¸­ræ˜¯å¥–åŠ±ï¼ŒVæ˜¯å¯è°ƒæ•´çš„åŸºçº¿ã€‚ç›´è§‚åœ°è¯´ï¼Œé™ä½Vä¼šçªå‡ºé«˜å¥–åŠ±æ ·æœ¬ï¼Œè€Œæé«˜å…¶åˆ™ä¼šæ›´ä¸¥å‰åœ°æƒ©ç½šä½å¥–åŠ±æ ·æœ¬ã€‚æˆ‘ä»¬é¦–å…ˆå¯¹è¿™ç§off-policy REINFORCEç®—æ³•è¿›è¡Œç†è®ºåˆ†æï¼Œè¡¨æ˜å½“åŸºçº¿Vé¢„æœŸå¥–åŠ±çš„ä¸‹ç•Œæ—¶ï¼Œè¯¥ç®—æ³•å…·æœ‰ç­–ç•¥æ”¹è¿›ä¿è¯ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå½“on-policyæ›´æ–°å¯ä»¥å®‰å…¨åœ°åˆ©ç”¨æ­£è´Ÿä¿¡å·æ—¶ï¼Œoff-policyæ›´æ–°æ›´å¤šåœ°å—ç›Šäºå…³æ³¨æ­£å‘å¥–åŠ±è€Œéè´Ÿå‘å¥–åŠ±ã€‚æˆ‘ä»¬åœ¨å—æ§çš„éšæœºå¼ºç›—ç¯å¢ƒå’Œé€šè¿‡å¾®è°ƒæœ€æ–°çš„LLMè¿›è¡Œæ¨ç†ä»»åŠ¡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20520v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¢«è¶Šæ¥è¶Šå¤šåœ°ç”¨äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æœ¬å·¥ä½œç ”ç©¶äº†ä»‹äºç¦»çº¿ç­–ç•¥å¼ºåŒ–å­¦ä¹ ä¸ç›‘ç£å¾®è°ƒä¹‹é—´çš„ç®—æ³•ï¼Œé€šè¿‡åˆ†æç®€å•çš„ç¦»çº¿ç­–ç•¥REINFORCEç®—æ³•çš„ä¼˜åŠ¿å®šä¹‰ï¼Œæ­ç¤ºå‡ºåŸºçº¿å€¼Vçš„è°ƒæ•´å¯¹äºå¼ºåŒ–å­¦ä¹ æ€§èƒ½çš„å½±å“ã€‚ç†è®ºä¸Šåˆ†æè¡¨æ˜ï¼Œå½“åŸºçº¿å€¼Vä¸‹ç•Œé¢„æœŸå¥–åŠ±æ—¶ï¼Œç®—æ³•å…·æœ‰ç­–ç•¥æ”¹è¿›ä¿è¯ã€‚åˆ†æè¿˜å‘ç°ï¼Œç›¸æ¯”äºè´Ÿå‘å¥–åŠ±ï¼Œç¦»çº¿ç­–ç•¥æ›´æ–°æ›´ä¾§é‡äºæ­£å‘å¥–åŠ±ã€‚å®éªŒéªŒè¯äº†åœ¨å—æ§çš„éšæœºæ€§æƒ…å¢ƒä»¥åŠå¾®è°ƒçŠ¶æ€æœ€ä¼˜çš„è¯­è¨€æ¨¡å‹ä¸Šï¼Œæˆ‘ä»¬çš„å‘ç°å‡æˆç«‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ç”¨äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹è¶Šæ¥è¶Šæ™®éã€‚</li>
<li>ç¦»çº¿ç­–ç•¥å¼ºåŒ–å­¦ä¹ ç›¸è¾ƒäºåœ¨çº¿ç­–ç•¥å…·æœ‰å®æ–½ç®€å•å’Œé«˜æ•ˆçš„æ•°æ®ä½¿ç”¨æ•ˆç‡ï¼Œä½†æ€§èƒ½å¯èƒ½ä¸å¦‚åœ¨çº¿ç­–ç•¥ã€‚</li>
<li>ç ”ç©¶é›†ä¸­åœ¨ç¦»çº¿ç­–ç•¥REINFORCEç®—æ³•çš„ä¼˜åŠ¿å®šä¹‰ä¸Šï¼Œå…¶ä¸­ä¼˜åŠ¿è¢«å®šä¹‰ä¸ºå¥–åŠ±rä¸å¯è°ƒåŸºçº¿å€¼Vçš„å·®ã€‚</li>
<li>åŸºçº¿å€¼Vçš„è°ƒæ•´å¯ä»¥å½±å“ç®—æ³•æ€§èƒ½ï¼Œé™ä½Vå€¼å¼ºè°ƒé«˜å¥–åŠ±æ ·æœ¬ï¼Œæé«˜Vå€¼åˆ™æ›´ä¸¥å‰åœ°æƒ©ç½šä½å¥–åŠ±æ ·æœ¬ã€‚</li>
<li>ç†è®ºä¸Šåˆ†æè¡¨æ˜ï¼Œå½“åŸºçº¿å€¼Vä½œä¸ºé¢„æœŸå¥–åŠ±çš„ä¸‹ç•Œæ—¶ï¼Œç®—æ³•å…·æœ‰ç­–ç•¥æ”¹è¿›ä¿éšœã€‚</li>
<li>åˆ†æå‘ç°ç¦»çº¿ç­–ç•¥æ›´æ–°æ›´ä¾§é‡äºæ­£å‘å¥–åŠ±ï¼Œè€Œä¸æ˜¯è´Ÿå‘å¥–åŠ±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20520">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-139861d0888a0e03cd190b3794c9577b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ReCode-Updating-Code-API-Knowledge-with-Reinforcement-Learning"><a href="#ReCode-Updating-Code-API-Knowledge-with-Reinforcement-Learning" class="headerlink" title="ReCode: Updating Code API Knowledge with Reinforcement Learning"></a>ReCode: Updating Code API Knowledge with Reinforcement Learning</h2><p><strong>Authors:Haoze Wu, Yunzhi Yao, Wenhao Yu, Huajun Chen, Ningyu Zhang</strong></p>
<p>Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMsâ€™ code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMsâ€™ general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zjunlp/ReCode">https://github.com/zjunlp/ReCode</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é€‚åº”å¤–éƒ¨åº“APIçš„é¢‘ç¹æ›´æ–°æ—¶å´ä¼šé™·å…¥å›°å¢ƒã€‚è¿™ä¸€å…³é”®å±€é™æ€§æºäºå¯¹è®­ç»ƒæ•°æ®ä¸­è¿‡æ—¶APIçŸ¥è¯†çš„ä¾èµ–ï¼Œå³ä½¿æœ‰è®¿é—®å½“å‰æ–‡æ¡£ï¼Œä¹Ÿé˜»ç¢äº†åŠ¨æ€ç¯å¢ƒä¸­çš„å¯é ä»£ç ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReCodeï¼ˆåŸºäºè§„åˆ™çš„ä»£ç æ›´æ–°å¼ºåŒ–å­¦ä¹ ï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒæ¨¡æ‹Ÿç¨‹åºå‘˜å¯¹APIå˜åŒ–çš„é€‚åº”ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤§çº¦2000ä¸ªæ•°æ®æ¡ç›®æ•°æ®é›†ï¼Œä»¥è®­ç»ƒLLMæ ¹æ®æ›´æ–°ä¿¡æ¯è¿›è¡Œç‰ˆæœ¬è¿ç§»ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ”¹è¿›çš„å­—ç¬¦ä¸²ç›¸ä¼¼æ€§åº¦é‡ä½œä¸ºä»£ç è¯„ä»·çš„å¥–åŠ±ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒReCodeåœ¨åŠ¨æ€APIåœºæ™¯ä¸­å¤§å¹…æå‡äº†LLMçš„ä»£ç ç”Ÿæˆæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§è¿‡çš„CodeUpdateArenaä»»åŠ¡ä¸­ã€‚å…³é”®çš„æ˜¯ï¼Œä¸ç›‘ç£å¾®è°ƒç›¸æ¯”ï¼ŒReCodeå¯¹LLMçš„ä¸€èˆ¬ä»£ç ç”Ÿæˆèƒ½åŠ›çš„å½±å“è¾ƒå°ã€‚æˆ‘ä»¬åœ¨å„ç§LLMå’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆGRPOå’ŒDAPOï¼‰ä¸Šåº”ç”¨äº†ReCodeï¼Œå‡å®ç°äº†æŒç»­æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç»è¿‡è®­ç»ƒåï¼ŒQwen2.5-Coder-7Bçš„è¡¨ç°ä¼˜äº32Bå‚æ•°ä»£ç æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä»¥åŠç›¸åŒç»“æ„ä¸‹çš„æ¨ç†æ¨¡å‹ã€‚ç›¸å…³ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/zjunlp/ReCode%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/zjunlp/ReCodeè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20495v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é€‚åº”å¤–éƒ¨åº“APIçš„é¢‘ç¹æ›´æ–°æ—¶é‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºReCodeçš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¨¡æ‹Ÿç¨‹åºå‘˜å¯¹APIå˜åŒ–çš„é€‚åº”ã€‚å®éªŒè¡¨æ˜ï¼ŒReCodeæ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€APIåœºæ™¯ä¸‹çš„ä»£ç ç”Ÿæˆæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§è¿‡çš„CodeUpdateArenaä»»åŠ¡ä¸Šã€‚æ­¤å¤–ï¼ŒReCodeå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å¸¸è§„ä»£ç ç”Ÿæˆèƒ½åŠ›å½±å“è¾ƒå°ã€‚ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨ReCodeåº”ç”¨åå‡å®ç°äº†ä¸€è‡´æ€§çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å…·æœ‰å‡ºè‰²èƒ½åŠ›ï¼Œä½†åœ¨åŠ¨æ€APIæ›´æ–°é€‚åº”æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ReCodeæ¡†æ¶é€šè¿‡ç»“åˆè§„åˆ™åŸºç¡€å’Œå¼ºåŒ–å­¦ä¹ æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ReCodeä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥è¿›è¡Œç‰ˆæœ¬è¿ç§»ã€‚</li>
<li>å¼•å…¥ä¿®æ”¹åçš„å­—ç¬¦ä¸²ç›¸ä¼¼æ€§åº¦é‡ä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å‡½æ•°ï¼Œç”¨äºä»£ç è¯„ä»·ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒReCodeæ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€APIåœºæ™¯ä¸‹çš„ä»£ç ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>ReCodeå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å¸¸è§„ä»£ç ç”Ÿæˆèƒ½åŠ›å½±å“è¾ƒå°ã€‚</li>
<li>ReCodeå¯åº”ç”¨äºä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå®ç°ä¸€è‡´æ€§çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dd9e66065bbf8428e33d2da7113c54df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5cceb681fede92d1147e826604bf89b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-07ace76844054edeab6cde0a4c32eb44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-392cd8b6e733cdb84e634470f036f74b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d21925700dbf40a39c62cdafac483c65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5c727c232a9c7e176f040eee15e49e1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="An-Agentic-System-for-Rare-Disease-Diagnosis-with-Traceable-Reasoning"><a href="#An-Agentic-System-for-Rare-Disease-Diagnosis-with-Traceable-Reasoning" class="headerlink" title="An Agentic System for Rare Disease Diagnosis with Traceable Reasoning"></a>An Agentic System for Rare Disease Diagnosis with Traceable Reasoning</h2><p><strong>Authors:Weike Zhao, Chaoyi Wu, Yanjie Fan, Xiaoman Zhang, Pengcheng Qiu, Yuze Sun, Xiao Zhou, Yanfeng Wang, Ya Zhang, Yongguo Yu, Kun Sun, Weidi Xie</strong></p>
<p>Rare diseases collectively affect over 300 million individuals worldwide, yet timely and accurate diagnosis remains a pervasive challenge. This is largely due to their clinical heterogeneity, low individual prevalence, and the limited familiarity most clinicians have with rare conditions. Here, we introduce DeepRare, the first rare disease diagnosis agentic system powered by a large language model (LLM), capable of processing heterogeneous clinical inputs. The system generates ranked diagnostic hypotheses for rare diseases, each accompanied by a transparent chain of reasoning that links intermediate analytic steps to verifiable medical evidence.   DeepRare comprises three key components: a central host with a long-term memory module; specialized agent servers responsible for domain-specific analytical tasks integrating over 40 specialized tools and web-scale, up-to-date medical knowledge sources, ensuring access to the most current clinical information. This modular and scalable design enables complex diagnostic reasoning while maintaining traceability and adaptability. We evaluate DeepRare on eight datasets. The system demonstrates exceptional diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013 diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15 methods, like traditional bioinformatics diagnostic tools, LLMs, and other agentic systems, achieving an average Recall@1 score of 57.18% and surpassing the second-best method (Reasoning LLM) by a substantial margin of 23.79 percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at Recall@1 compared to Exomiserâ€™s 53.20% in 109 cases. Manual verification of reasoning chains by clinical experts achieves 95.40% agreements. Furthermore, the DeepRare system has been implemented as a user-friendly web application <a target="_blank" rel="noopener" href="http://raredx.cn/doctor">http://raredx.cn/doctor</a>. </p>
<blockquote>
<p>ç½•è§ç–¾ç—…åœ¨å…¨çƒèŒƒå›´å†…å…±åŒå½±å“ç€è¶…è¿‡3äº¿äººï¼Œä½†åŠæ—¶å‡†ç¡®çš„è¯Šæ–­ä»ç„¶æ˜¯ä¸€ä¸ªæ™®éå­˜åœ¨çš„æŒ‘æˆ˜ã€‚è¿™ä¸»è¦æ˜¯ç”±äºå…¶ä¸´åºŠå¼‚è´¨æ€§ã€ä¸ªäººå‘ç—…ç‡ä½ä»¥åŠå¤§å¤šæ•°ä¸´åºŠåŒ»ç”Ÿå¯¹ç½•è§ç–¾ç—…çš„ä¸ç†Ÿæ‚‰ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ¨å‡ºDeepRareï¼Œè¿™æ˜¯ä¸€ä¸ªç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„é¦–ä¸ªç½•è§ç–¾ç—…è¯Šæ–­æ™ºèƒ½ç³»ç»Ÿï¼Œèƒ½å¤Ÿå¤„ç†å„ç§ä¸´åºŠè¾“å…¥ã€‚è¯¥ç³»ç»Ÿä¸ºç½•è§ç–¾ç—…ç”Ÿæˆæ’åè¯Šæ–­å‡è®¾ï¼Œæ¯ä¸ªå‡è®¾éƒ½ä¼´æœ‰é€æ˜çš„æ¨ç†é“¾ï¼Œå°†ä¸­é—´åˆ†ææ­¥éª¤ä¸å¯éªŒè¯çš„åŒ»å­¦è¯æ®è”ç³»èµ·æ¥ã€‚DeepRareåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šä¸€ä¸ªå¸¦æœ‰é•¿æœŸè®°å¿†æ¨¡å—çš„ä¸­å¿ƒä¸»æœºï¼›è´Ÿè´£ç‰¹å®šé¢†åŸŸåˆ†æä»»åŠ¡çš„ä¸“ç”¨ä»£ç†æœåŠ¡å™¨ï¼Œé›†æˆè¶…è¿‡40ç§ä¸“ä¸šå·¥å…·å’Œæœ€æ–°ç½‘ç»œè§„æ¨¡åŒ»å­¦çŸ¥è¯†æºï¼Œç¡®ä¿è®¿é—®æœ€æ–°ä¸´åºŠä¿¡æ¯ã€‚è¿™ç§æ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„è®¾è®¡åœ¨ä¿æŒå¯è¿½æº¯æ€§å’Œé€‚åº”æ€§çš„åŒæ—¶ï¼Œå®ç°äº†å¤æ‚çš„è¯Šæ–­æ¨ç†ã€‚æˆ‘ä»¬åœ¨å…«ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†DeepRareã€‚è¯¥ç³»ç»Ÿåœ¨2919ç§ç–¾ç—…ä¸­è¡¨ç°å‡ºå“è¶Šçš„è¯Šæ–­æ€§èƒ½ï¼Œå¯¹1013ç§ç–¾ç—…çš„å‡†ç¡®æ€§è¾¾åˆ°100%ã€‚åœ¨åŸºäºHPOçš„è¯„ä¼°ä¸­ï¼ŒDeepRareæ˜¾è‘—ä¼˜äºå…¶ä»–15ç§æ–¹æ³•ï¼Œå¦‚ä¼ ç»Ÿç”Ÿç‰©ä¿¡æ¯å­¦è¯Šæ–­å·¥å…·ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œå…¶ä»–çš„æ™ºèƒ½ç³»ç»Ÿï¼Œå¹³å‡Recall@1åˆ†æ•°è¾¾åˆ°57.18%ï¼Œå¹¶å¤§å¹…åº¦è¶…è¶Šæ’åç¬¬äºŒçš„æ–¹æ³•ï¼ˆæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼‰ã€‚å¯¹äºå¤šæ¨¡å¼è¾“å…¥åœºæ™¯ï¼ŒDeepRareåœ¨Recall@1è¾¾åˆ°70.60%ï¼Œç›¸æ¯”ä¹‹ä¸‹Exomiserä¸º53.20%ï¼Œåœ¨109ä¸ªæ¡ˆä¾‹ä¸­ã€‚ä¸´åºŠä¸“å®¶å¯¹æ¨ç†é“¾çš„æ‰‹åŠ¨éªŒè¯è¾¾åˆ°95.40%çš„å…±è¯†ã€‚æ­¤å¤–ï¼ŒDeepRareç³»ç»Ÿå·²ä½œä¸ºä¸€ä¸ªç”¨æˆ·å‹å¥½çš„ç½‘é¡µåº”ç”¨ç¨‹åºå®ç°ï¼Œç½‘å€ä¸ºï¼š<a target="_blank" rel="noopener" href="http://raredx.cn/doctor%E3%80%82">http://raredx.cn/doctorã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20430v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æ·±ç½•ç—…è¯Šæ–­ç³»ç»ŸDeepRareé—®ä¸–ï¼Œå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤„ç†å¤šæ ·åŒ–ä¸´åºŠè¾“å…¥ï¼Œä¸ºç½•è§ç–¾ç—…ç”Ÿæˆæ’åè¯Šæ–­å‡è®¾ã€‚è¯¥ç³»ç»Ÿä¼´æœ‰é€æ˜åŒ–æ¨ç†é“¾ï¼Œé“¾æ¥ä¸­é—´åˆ†ææ­¥éª¤ä¸å¯éªŒè¯åŒ»å­¦è¯æ®ã€‚DeepRareåŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šä¸­å¤®ä¸»æœºã€ä¸“ä¸šä»£ç†æœåŠ¡å™¨åŠåŒ»å­¦çŸ¥è¯†æºã€‚å…¶åœ¨å…«ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨2919ç§ç–¾ç—…ä¸­å‡†ç¡®ç‡é«˜è¾¾ç™¾åˆ†ä¹‹ç™¾ã€‚ç›¸æ¯”å…¶ä»–å·¥å…·ï¼ŒDeepRareæ˜¾è‘—æé«˜è¯Šæ–­æ€§èƒ½ï¼Œå¹³å‡Recall@1åˆ†æ•°ä¸º57.18%ã€‚å¤šæ¨¡æ€è¾“å…¥åœºæ™¯ä¸‹æ€§èƒ½æ›´ä½³ï¼Œä¸”å·²è·å¾—ä¸´åºŠä¸“å®¶éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>DeepRareæ˜¯é¦–ä¸ªå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç½•è§ç–¾ç—…è¯Šæ–­çš„ç³»ç»Ÿã€‚</li>
<li>å¯å¤„ç†å¤šæ ·åŒ–çš„ä¸´åºŠè¾“å…¥ï¼Œç”Ÿæˆæ’åè¯Šæ–­å‡è®¾ï¼Œå¹¶é™„æœ‰é€æ˜åŒ–æ¨ç†é“¾ã€‚</li>
<li>åŒ…æ‹¬ä¸­å¤®ä¸»æœºã€ä¸“ä¸šä»£ç†æœåŠ¡å™¨ç­‰ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼Œæ•´åˆè¶…è¿‡40ç§ä¸“ä¸šå·¥å…·å’Œæœ€æ–°åŒ»å­¦çŸ¥è¯†æºã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç½•è§ç–¾ç—…è¯Šæ–­å‡†ç¡®ç‡æé«˜ã€‚</li>
<li>å¯¹æ¯”å…¶ä»–æ–¹æ³•ï¼ŒDeepRareåœ¨è¯Šæ–­æ€§èƒ½ä¸Šæ˜¾è‘—æé«˜ï¼Œå¹³å‡Recall@1åˆ†æ•°é¢†å…ˆã€‚</li>
<li>å¤šæ¨¡æ€è¾“å…¥åœºæ™¯ä¸‹æ€§èƒ½ä¼˜è¶Šï¼Œä¸Exomiserç›¸æ¯”æœ‰è¾ƒé«˜Recall@1åˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20430">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-195b057b5a3938bd8f4059156eac4058.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b57c6ae52801ac6938956e93d7a5983.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50676bf2597ba2e2fc6783543b76bebd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Mobile-R1-Towards-Interactive-Reinforcement-Learning-for-VLM-Based-Mobile-Agent-via-Task-Level-Rewards"><a href="#Mobile-R1-Towards-Interactive-Reinforcement-Learning-for-VLM-Based-Mobile-Agent-via-Task-Level-Rewards" class="headerlink" title="Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based   Mobile Agent via Task-Level Rewards"></a>Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based   Mobile Agent via Task-Level Rewards</h2><p><strong>Authors:Jihao Gu, Qihang Ai, Yingyao Wang, Pi Bu, Jingxuan Xing, Zekun Zhu, Wei Jiang, Ziming Wang, Yingxiu Zhao, Ming-Liang Zhang, Jun Song, Yuning Jiang, Bo Zheng</strong></p>
<p>Vision-language model-based mobile agents have gained the ability to not only understand complex instructions and mobile screenshots, but also optimize their action outputs via thinking and reasoning, benefiting from reinforcement learning, such as Group Relative Policy Optimization (GRPO). However, existing research centers on offline reinforcement learning training or online optimization using action-level rewards, which limits the agentâ€™s dynamic interaction with the environment. This often results in agents settling into local optima, thereby weakening their ability for exploration and error action correction. To address these challenges, we introduce an approach called Mobile-R1, which employs interactive multi-turn reinforcement learning with task-level rewards for mobile agents. Our training framework consists of three stages: initial format finetuning, single-step online training via action-level reward, followed by online training via task-level reward based on multi-turn trajectories. This strategy is designed to enhance the exploration and error correction capabilities of Mobile-R1, leading to significant performance improvements. Moreover, we have collected a dataset covering 28 Chinese applications with 24,521 high-quality manual annotations and established a new benchmark with 500 trajectories. We will open source all resources, including the dataset, benchmark, model weight, and codes: <a target="_blank" rel="noopener" href="https://mobile-r1.github.io/Mobile-R1/">https://mobile-r1.github.io/Mobile-R1/</a>. </p>
<blockquote>
<p>åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç§»åŠ¨ä»£ç†ä¸ä»…è·å¾—äº†ç†è§£å¤æ‚æŒ‡ä»¤å’Œç§»åŠ¨æˆªå›¾çš„èƒ½åŠ›ï¼Œè¿˜é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆå¦‚ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼‰ä¼˜åŒ–äº†å…¶è¡ŒåŠ¨è¾“å‡ºã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒæˆ–åŸºäºè¡ŒåŠ¨å±‚é¢çš„å¥–åŠ±è¿›è¡Œåœ¨çº¿ä¼˜åŒ–ï¼Œè¿™é™åˆ¶äº†ä»£ç†ä¸ç¯å¢ƒçš„åŠ¨æ€äº¤äº’ã€‚è¿™é€šå¸¸å¯¼è‡´ä»£ç†é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œä»è€Œå‰Šå¼±äº†å®ƒä»¬çš„æ¢ç´¢èƒ½åŠ›å’Œé”™è¯¯è¡ŒåŠ¨çº æ­£èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºMobile-R1çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨äº¤äº’å¼å¤šå›åˆå¼ºåŒ–å­¦ä¹ ï¼Œä¸ºç§»åŠ¨ä»£ç†æä¾›ä»»åŠ¡çº§å¥–åŠ±ã€‚æˆ‘ä»¬çš„è®­ç»ƒæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šåˆå§‹æ ¼å¼å¾®è°ƒã€é€šè¿‡è¡ŒåŠ¨çº§å¥–åŠ±è¿›è¡Œå•æ­¥åœ¨çº¿è®­ç»ƒï¼Œç„¶åæ˜¯åŸºäºå¤šå›åˆè½¨è¿¹çš„ä»»åŠ¡çº§å¥–åŠ±åœ¨çº¿è®­ç»ƒã€‚è¯¥ç­–ç•¥æ—¨åœ¨å¢å¼ºMobile-R1çš„æ¢ç´¢å’Œé”™è¯¯çº æ­£èƒ½åŠ›ï¼Œä»è€Œå¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªåŒ…å«28ä¸ªä¸­æ–‡åº”ç”¨ç¨‹åºçš„æ•°æ®é›†ï¼ŒåŒ…å«24521ä¸ªé«˜è´¨é‡çš„æ‰‹åŠ¨æ³¨é‡Šï¼Œå¹¶å»ºç«‹äº†åŒ…å«500ä¸ªè½¨è¿¹çš„æ–°åŸºå‡†ã€‚æˆ‘ä»¬å°†å…¬å¼€æ‰€æœ‰èµ„æºï¼ŒåŒ…æ‹¬æ•°æ®é›†ã€åŸºå‡†ã€æ¨¡å‹æƒé‡å’Œä»£ç ï¼š<a target="_blank" rel="noopener" href="https://mobile-r1.github.io/Mobile-R1/">https://mobile-r1.github.io/Mobile-R1/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20332v1">PDF</a> 14 pages, 12 figures</p>
<p><strong>Summary</strong><br>ç§»åŠ¨æ™ºèƒ½ä»£ç†é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ç»“åˆå¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œå®ç°äº†å¯¹å¤æ‚æŒ‡ä»¤å’Œç§»åŠ¨æˆªå›¾çš„ç†è§£ï¼Œå¹¶ä¼˜åŒ–è¡ŒåŠ¨è¾“å‡ºã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶é›†ä¸­åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒæˆ–è¡ŒåŠ¨çº§åˆ«çš„åœ¨çº¿ä¼˜åŒ–ä¸Šï¼Œé™åˆ¶äº†æ™ºèƒ½ä»£ç†ä¸ç¯å¢ƒçš„åŠ¨æ€äº¤äº’èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºMobile-R1æ–¹æ³•ï¼Œé‡‡ç”¨å¤šä»»åŠ¡çº§åˆ«å¥–åŠ±çš„äº’åŠ¨å¤šå›åˆå¼ºåŒ–å­¦ä¹ æ¨¡å¼è¿›è¡Œè®­ç»ƒã€‚è®­ç»ƒæ¡†æ¶åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šåˆå§‹æ ¼å¼å¾®è°ƒã€é€šè¿‡è¡ŒåŠ¨çº§åˆ«å¥–åŠ±è¿›è¡Œå•æ­¥åœ¨çº¿è®­ç»ƒï¼Œä»¥åŠåŸºäºå¤šå›åˆè½¨è¿¹çš„ä»»åŠ¡çº§åˆ«å¥–åŠ±åœ¨çº¿è®­ç»ƒã€‚æ­¤æ–¹æ³•æ—¨åœ¨æé«˜Mobile-R1çš„æ¢ç´¢å’Œé”™è¯¯ä¿®æ­£èƒ½åŠ›ï¼Œå®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æ”¶é›†äº†æ¶µç›–28ä¸ªä¸­æ–‡åº”ç”¨ç¨‹åºçš„æ ‡æ³¨æ•°æ®é›†å¹¶å»ºç«‹æ–°åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç§»åŠ¨æ™ºèƒ½ä»£ç†èƒ½å¤Ÿé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ç†è§£å¤æ‚æŒ‡ä»¤å’Œç§»åŠ¨æˆªå›¾ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ è¢«ç”¨äºä¼˜åŒ–ç§»åŠ¨æ™ºèƒ½ä»£ç†çš„è¡ŒåŠ¨è¾“å‡ºã€‚</li>
<li>ç°æœ‰ç ”ç©¶é›†ä¸­åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒæˆ–è¡ŒåŠ¨çº§åˆ«å¥–åŠ±çš„åœ¨çº¿ä¼˜åŒ–ä¸Šï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Mobile-R1æ–¹æ³•é‡‡ç”¨å¤šä»»åŠ¡çº§åˆ«å¥–åŠ±çš„äº’åŠ¨å¤šå›åˆå¼ºåŒ–å­¦ä¹ æ¨¡å¼è®­ç»ƒç§»åŠ¨æ™ºèƒ½ä»£ç†ã€‚</li>
<li>Mobile-R1çš„è®­ç»ƒæ¡†æ¶åŒ…æ‹¬åˆå§‹æ ¼å¼å¾®è°ƒã€å•æ­¥åœ¨çº¿è®­ç»ƒå’ŒåŸºäºå¤šå›åˆè½¨è¿¹çš„åœ¨çº¿è®­ç»ƒä¸‰ä¸ªé˜¶æ®µã€‚</li>
<li>Mobile-R1æ—¨åœ¨æé«˜æ¢ç´¢å’Œé”™è¯¯ä¿®æ­£èƒ½åŠ›ï¼Œå®ç°æ€§èƒ½æå‡ã€‚</li>
<li>å»ºç«‹äº†ä¸€ä¸ªæ–°çš„åŸºå‡†å¹¶æ”¶é›†äº†ä¸€ä¸ªæ¶µç›–å¤šä¸ªä¸­æ–‡åº”ç”¨ç¨‹åºçš„æ ‡æ³¨æ•°æ®é›†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20332">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-352c66259c45d2509b27f1a88266eb6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4f11634f38e1adf173487f146e8d128.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac037de80d310a1b4d5b440557fe2948.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f48800b37ff69204ffefe5d608ce8de7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f462316ef48c2506828a89724016e12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d24431eea05581b90288ae4c006193c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Seeing-is-Believing-Mitigating-OCR-Hallucinations-in-Multimodal-Large-Language-Models"><a href="#Seeing-is-Believing-Mitigating-OCR-Hallucinations-in-Multimodal-Large-Language-Models" class="headerlink" title="Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large   Language Models"></a>Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large   Language Models</h2><p><strong>Authors:Zhentao He, Can Zhang, Ziheng Wu, Zhenghao Chen, Yufei Zhan, Yifan Li, Zhao Zhang, Xian Wang, Minghui Qiu</strong></p>
<p>Recent advancements in multimodal large language models have enhanced document understanding by integrating textual and visual information. However, existing models exhibit incompleteness within their paradigm in real-world scenarios, particularly under visual degradation. In such conditions, the current response paradigm often fails to adequately perceive visual degradation and ambiguity, leading to overreliance on linguistic priors or misaligned visual-textual reasoning. This difficulty in recognizing uncertainty frequently results in the generation of hallucinatory content, especially when a precise answer is not feasible. To better demonstrate and analyze this phenomenon and problem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR hallucination in degraded document understanding. This dataset includes test samples spanning identity cards and invoices, with simulated real-world degradations for OCR reliability. This setup allows for evaluating modelsâ€™ capacity, under degraded input, to distinguish reliable visual information and answer accordingly, thereby highlighting the challenge of avoiding hallucination on uncertain data. To achieve vision-faithful reasoning and thereby avoid the aforementioned issues, we further introduce a GRPO-based framework featuring a novel reward mechanism. By incorporating a self-awareness of visual uncertainty and an analysis method that initiates refusal to answer to increase task difficulty within our supervised fine-tuning and reinforcement learning framework, we successfully mitigated hallucinations in ambiguous regions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model achieves a 22% absolute improvement in hallucination-free accuracy over GPT-4o on KIE-HVQA and there is no significant performance drop in standard tasks, highlighting both effectiveness and robustness. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥é€šè¿‡æ•´åˆæ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯å¢å¼ºäº†æ–‡æ¡£ç†è§£ã€‚ç„¶è€Œï¼Œåœ¨çœŸå®åœºæ™¯ä¸­ï¼Œç°æœ‰æ¨¡å‹åœ¨å…¶èŒƒå¼å†…è¡¨ç°å‡ºä¸å®Œæ•´æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰é€€åŒ–çš„æƒ…å†µä¸‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå½“å‰å“åº”èŒƒå¼é€šå¸¸æ— æ³•å……åˆ†æ„ŸçŸ¥è§†è§‰é€€åŒ–å’Œæ¨¡ç³Šï¼Œå¯¼è‡´è¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒæˆ–è§†è§‰æ–‡æœ¬æ¨ç†å¤±è¯¯ã€‚éš¾ä»¥è¯†åˆ«ä¸ç¡®å®šæ€§ç»å¸¸å¯¼è‡´äº§ç”Ÿå¹»è§‰å†…å®¹ï¼Œå°¤å…¶æ˜¯åœ¨æ— æ³•ç»™å‡ºç²¾ç¡®ç­”æ¡ˆçš„æƒ…å†µä¸‹ã€‚ä¸ºäº†æ›´å¥½åœ°æ¼”ç¤ºå’Œåˆ†æè¿™ç§ç°è±¡å’Œé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†KIE-HVQAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°é€€åŒ–æ–‡æ¡£ç†è§£ä¸­OCRå¹»è§‰çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥æ•°æ®é›†åŒ…å«èº«ä»½è¯å’Œå‘ç¥¨ç­‰æµ‹è¯•æ ·æœ¬ï¼Œé’ˆå¯¹OCRå¯é æ€§æ¨¡æ‹Ÿäº†çœŸå®ä¸–ç•Œçš„é€€åŒ–æƒ…å†µã€‚æ­¤è®¾ç½®å¯ä»¥è¯„ä¼°æ¨¡å‹åœ¨é€€åŒ–è¾“å…¥ä¸‹çš„èƒ½åŠ›ï¼ŒåŒºåˆ†å¯é è§†è§‰ä¿¡æ¯å¹¶ç›¸åº”åœ°å›ç­”ï¼Œä»è€Œçªå‡ºé¿å…åœ¨ä¸ç¡®å®šæ•°æ®ä¸Šäº§ç”Ÿå¹»è§‰çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å®ç°å¿ äºè§†è§‰çš„æ¨ç†ï¼Œé¿å…ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†åŸºäºGRPOçš„æ¡†æ¶å’Œæ–°å‹å¥–åŠ±æœºåˆ¶ã€‚é€šè¿‡èå…¥å¯¹è§†è§‰ä¸ç¡®å®šæ€§çš„è‡ªæˆ‘æ„è¯†ä»¥åŠä¸€ç§åˆ†ææ–¹æ³•æ‹’ç»å›ç­”æ¥å¢åŠ ä»»åŠ¡éš¾åº¦ï¼Œè¿™æ¶µç›–åœ¨æˆ‘ä»¬çš„ç›‘ç£å¾®è°ƒä¸­ä»¥åŠå¼ºåŒ–å­¦ä¹ æ¡†æ¶å†…ï¼Œæˆ‘ä»¬æˆåŠŸå‡è½»äº†æ¨¡ç³ŠåŒºåŸŸçš„å¹»è§‰ç°è±¡ã€‚åœ¨Qwen2.5-VLä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„7Bå‚æ•°æ¨¡å‹åœ¨KIE-HVQAä¸Šç›¸å¯¹äºGPT-4oå®ç°äº†æ— å¹»è§‰å‡†ç¡®åº¦çš„ç»å¯¹æå‡22%ï¼Œå¹¶ä¸”åœ¨æ ‡å‡†ä»»åŠ¡ä¸­æ²¡æœ‰æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œè¿™çªæ˜¾äº†æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20168v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æ¡£ç†è§£æ–¹é¢çš„æœ€æ–°è¿›å±•ï¼Œè¿™äº›æ¨¡å‹é€šè¿‡æ•´åˆæ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯æé«˜äº†æ–‡æ¡£ç†è§£çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸‹é¢ä¸´ä¸å®Œå…¨é€‚åº”çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰é€€åŒ–çš„æƒ…å†µä¸‹ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†KIE-HVQAåŸºå‡†æµ‹è¯•é›†ï¼Œç”¨äºè¯„ä¼°OCRå¹»è§‰åœ¨é€€åŒ–æ–‡æ¡£ç†è§£ä¸­çš„è¡¨ç°ã€‚åŒæ—¶ï¼Œè¿˜ä»‹ç»äº†ä¸€ç§åŸºäºGRPOçš„æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥æ–°çš„å¥–åŠ±æœºåˆ¶å’Œè‡ªæˆ‘æ„ŸçŸ¥è§†è§‰ä¸ç¡®å®šæ€§çš„åˆ†ææ–¹æ³•æ¥å‡å°‘æ¨¡ç³ŠåŒºåŸŸçš„å¹»è§‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨KIE-HVQAä¸Šå–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡æ•´åˆæ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯æé«˜äº†æ–‡æ¡£ç†è§£ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨è§†è§‰é€€åŒ–æƒ…å†µä¸‹å­˜åœ¨ä¸è¶³ï¼Œå¦‚è¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒæˆ–è§†è§‰æ–‡æœ¬æ¨ç†å¤±è°ƒã€‚</li>
<li>KIE-HVQAåŸºå‡†æµ‹è¯•é›†ç”¨äºè¯„ä¼°OCRå¹»è§‰åœ¨é€€åŒ–æ–‡æ¡£ç†è§£ä¸­çš„è¡¨ç°ã€‚</li>
<li>æå‡ºçš„GRPOæ¡†æ¶é€šè¿‡å¼•å…¥æ–°çš„å¥–åŠ±æœºåˆ¶å’Œè‡ªæˆ‘æ„ŸçŸ¥è§†è§‰ä¸ç¡®å®šæ€§çš„åˆ†ææ–¹æ³•ï¼ŒæˆåŠŸå‡å°‘äº†æ¨¡ç³ŠåŒºåŸŸçš„å¹»è§‰ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨KIE-HVQAä¸Šè¾ƒGPT-4oæé«˜äº†22%çš„æ— å¹»è§‰å‡†ç¡®ç‡ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨æ ‡å‡†ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ²¡æœ‰æ˜æ˜¾ä¸‹é™ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20168">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c9cec61b212aeafef0925453f6dd78df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42783b689f19c305a9e88907bef0ea07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5940859b179e53617fc6052a0d1bd295.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ba1f95ec09f43d0cd611edc85d2e0c4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MIRAGE-A-Benchmark-for-Multimodal-Information-Seeking-and-Reasoning-in-Agricultural-Expert-Guided-Conversations"><a href="#MIRAGE-A-Benchmark-for-Multimodal-Information-Seeking-and-Reasoning-in-Agricultural-Expert-Guided-Conversations" class="headerlink" title="MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in   Agricultural Expert-Guided Conversations"></a>MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in   Agricultural Expert-Guided Conversations</h2><p><strong>Authors:Vardhan Dongre, Chi Gui, Shubham Garg, Hooshang Nayyeri, Gokhan Tur, Dilek Hakkani-TÃ¼r, Vikram S. Adve</strong></p>
<p>We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning and decision-making in consultative interaction settings. Designed for the agriculture domain, MIRAGE captures the full complexity of expert consultations by combining natural user queries, expert-authored responses, and image-based context, offering a high-fidelity benchmark for evaluating models on grounded reasoning, clarification strategies, and long-form generation in a real-world, knowledge-intensive domain. Grounded in over 35,000 real user-expert interactions and curated through a carefully designed multi-step pipeline, MIRAGE spans diverse crop health, pest diagnosis, and crop management scenarios. The benchmark includes more than 7,000 unique biological entities, covering plant species, pests, and diseases, making it one of the most taxonomically diverse benchmarks available for vision-language models, grounded in the real world. Unlike existing benchmarks that rely on well-specified user inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich scenarios with open-world settings, requiring models to infer latent knowledge gaps, handle rare entities, and either proactively guide the interaction or respond. Project Page: <a target="_blank" rel="noopener" href="https://mirage-benchmark.github.io/">https://mirage-benchmark.github.io</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºMIRAGEï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€ä¸“å®¶çº§æ¨ç†å’Œå†³ç­–åŸºå‡†ï¼Œé€‚ç”¨äºå’¨è¯¢äº¤äº’ç¯å¢ƒä¸­çš„ä¸“å®¶çº§æ¨ç†å’Œå†³ç­–ã€‚MIRAGEä¸“ä¸ºå†œä¸šé¢†åŸŸè®¾è®¡ï¼Œé€šè¿‡ç»“åˆè‡ªç„¶ç”¨æˆ·æŸ¥è¯¢ã€ä¸“å®¶æ’°å†™çš„å›åº”å’Œå›¾åƒä¸Šä¸‹æ–‡ï¼Œæ•æ‰ä¸“å®¶å’¨è¯¢çš„å…¨è¿‡ç¨‹å¤æ‚æ€§ï¼Œä¸ºè¯„ä¼°æ¨¡å‹åœ¨ç°å®ä¸–ç•Œçš„çŸ¥è¯†å¯†é›†å‹é¢†åŸŸä¸­çš„åŸºäºäº‹å®æ¨ç†ã€æ¾„æ¸…ç­–ç•¥å’Œé•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›æä¾›é«˜ä¿çœŸåŸºå‡†ã€‚MIRAGEä»¥è¶…è¿‡3ä¸‡5åƒæ¬¡çš„çœŸå®ç”¨æˆ·ä¸ä¸“å®¶äº’åŠ¨ä¸ºåŸºç¡€ï¼Œç»è¿‡ç²¾å¿ƒè®¾è®¡çš„å¤šæ­¥éª¤æµç¨‹è¿›è¡Œç­›é€‰ï¼Œæ¶µç›–äº†ä½œç‰©å¥åº·ã€ç—…è™«å®³è¯Šæ–­åŠä½œç‰©ç®¡ç†ç­‰å¤šç§åœºæ™¯ã€‚è¯¥åŸºå‡†åŒ…å«è¶…è¿‡7åƒä¸ªç‹¬ç‰¹çš„ç”Ÿç‰©å®ä½“ï¼Œæ¶µç›–æ¤ç‰©ç§ç±»ã€å®³è™«å’Œç–¾ç—…ï¼Œæˆä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ä¸­æœ€å…·ç°å®ä¸–ç•Œç‰¹å¾çš„ã€åˆ†ç±»å­¦ä¸Šæœ€å¤šå…ƒåŒ–çš„åŸºå‡†ä¹‹ä¸€ã€‚ä¸ä¾èµ–æ˜ç¡®ç”¨æˆ·è¾“å…¥å’Œå°é—­åˆ†ç±»ç³»ç»Ÿçš„ç°æœ‰åŸºå‡†ä¸åŒï¼ŒMIRAGEçš„ç‰¹ç‚¹æ˜¯åœºæ™¯ä¸°å¯Œä¸”ä¸Šä¸‹æ–‡ä¸æ˜ç¡®ï¼Œè¦æ±‚æ¨¡å‹èƒ½å¤Ÿæ¨æ–­æ½œåœ¨çš„çŸ¥è¯†ç©ºç™½ï¼Œå¤„ç†ç½•è§å®ä½“ï¼Œå¹¶èƒ½å¤Ÿä¸»åŠ¨å¼•å¯¼äº¤äº’æˆ–ä½œå‡ºå›åº”ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://mirage-benchmark.github.io/">https://mirage-benchmark.github.io</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20100v1">PDF</a> 66 pages, 32 figures, 23 tables</p>
<p><strong>Summary</strong></p>
<p>MIRAGEæ˜¯ä¸€ä¸ªä¸ºå†œä¸šé¢†åŸŸè®¾è®¡çš„æ–°åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡å¼ä¸“å®¶çº§æ¨ç†å’Œå’¨è¯¢äº¤äº’è®¾ç½®ä¸­çš„å†³ç­–èƒ½åŠ›ã€‚å®ƒç»“åˆäº†è‡ªç„¶ç”¨æˆ·æŸ¥è¯¢ã€ä¸“å®¶æ’°å†™çš„å›åº”å’Œå›¾åƒä¸Šä¸‹æ–‡ï¼Œæä¾›äº†ä¸€ä¸ªé«˜ä¿çœŸåº¦çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ç°å®ä¸–ç•Œçš„å¤æ‚åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥é¡¹ç›®è·¨è¶Šå¤šç§ä½œç‰©å¥åº·ã€ç—…è™«å®³è¯Šæ–­å’Œä½œç‰©ç®¡ç†åœºæ™¯ï¼Œå¹¶åŒ…æ‹¬è¶…è¿‡7000ä¸ªç‹¬ç‰¹çš„ç”Ÿç‰©å®ä½“ã€‚å…¶ç‰¹ç‚¹æ˜¯æƒ…æ™¯ä¸°å¯Œä¸”å¼€æ”¾ä¸–ç•Œè®¾ç½®ï¼Œè¦æ±‚æ¨¡å‹æ¨æ–­æ½œåœ¨çŸ¥è¯†ç©ºç™½ã€å¤„ç†ç½•è§å®ä½“ä»¥åŠä¸»åŠ¨å¼•å¯¼äº¤äº’æˆ–ä½œå‡ºååº”ã€‚æ›´å¤šè¯¦æƒ…å¯è§é¡¹ç›®é¡µé¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MIRAGEæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºå†œä¸šé¢†åŸŸçš„å¤šæ¨¡å¼ä¸“å®¶çº§æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•ç»“åˆäº†è‡ªç„¶ç”¨æˆ·æŸ¥è¯¢ã€ä¸“å®¶å“åº”å’Œå›¾åƒä¸Šä¸‹æ–‡ï¼Œæ¨¡æ‹ŸçœŸå®çš„ä¸“å®¶å’¨è¯¢åœºæ™¯ã€‚</li>
<li>MIRAGEæä¾›äº†é«˜ä¿çœŸåº¦çš„è¯„ä¼°ç¯å¢ƒï¼Œç”¨äºæµ‹è¯•æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä½œç‰©å¥åº·ã€ç—…è™«å®³è¯Šæ–­å’Œä½œç‰©ç®¡ç†ç­‰æ–¹é¢ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•åŒ…å«è¶…è¿‡7000ä¸ªç‹¬ç‰¹çš„ç”Ÿç‰©å®ä½“ï¼Œæ¶µç›–æ¤ç‰©ç§ç±»ã€å®³è™«å’Œç–¾ç—…ï¼Œæ˜¯ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æœ€ä¸ºå¤šæ ·åŒ–çš„åŸºå‡†æµ‹è¯•ä¹‹ä¸€ã€‚</li>
<li>MIRAGEçš„ç‰¹ç‚¹æ˜¯æƒ…æ™¯ä¸°å¯Œä¸”å¼€æ”¾ä¸–ç•Œè®¾ç½®ï¼Œè¦æ±‚æ¨¡å‹å…·å¤‡å¤„ç†å¤æ‚ç¯å¢ƒå’ŒæœªçŸ¥å®ä½“çš„èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹éœ€è¦èƒ½å¤Ÿæ¨æ–­æ½œåœ¨çŸ¥è¯†ç©ºç™½ï¼Œå¤„ç†ç½•è§å®ä½“ï¼Œå¹¶èƒ½å¤Ÿåœ¨äº¤äº’ä¸­ä¸»åŠ¨å¼•å¯¼å’Œä½œå‡ºååº”ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dcd4d53884b4dedb9ca023bf40175f13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e37f3333f8736aba23fdd24c7bfc9e54.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="A-Modular-Multitask-Reasoning-Framework-Integrating-Spatio-temporal-Models-and-LLMs"><a href="#A-Modular-Multitask-Reasoning-Framework-Integrating-Spatio-temporal-Models-and-LLMs" class="headerlink" title="A Modular Multitask Reasoning Framework Integrating Spatio-temporal   Models and LLMs"></a>A Modular Multitask Reasoning Framework Integrating Spatio-temporal   Models and LLMs</h2><p><strong>Authors:Kethmi Hirushini Hettige, Jiahao Ji, Cheng Long, Shili Xiang, Gao Cong, Jingyuan Wang</strong></p>
<p>Spatio-temporal data mining plays a pivotal role in informed decision making across diverse domains. However, existing models are often restricted to narrow tasks, lacking the capacity for multi-task inference and complex long-form reasoning that require generation of in-depth, explanatory outputs. These limitations restrict their applicability to real-world, multi-faceted decision scenarios. In this work, we introduce STReason, a novel framework that integrates the reasoning strengths of large language models (LLMs) with the analytical capabilities of spatio-temporal models for multi-task inference and execution. Without requiring task-specific finetuning, STReason leverages in-context learning to decompose complex natural language queries into modular, interpretable programs, which are then systematically executed to generate both solutions and detailed rationales. To facilitate rigorous evaluation, we construct a new benchmark dataset and propose a unified evaluation framework with metrics specifically designed for long-form spatio-temporal reasoning. Experimental results show that STReason significantly outperforms advanced LLM baselines across all metrics, particularly excelling in complex, reasoning-intensive spatio-temporal scenarios. Human evaluations further validate STReasonâ€™s credibility and practical utility, demonstrating its potential to reduce expert workload and broaden the applicability to real-world spatio-temporal tasks. We believe STReason provides a promising direction for developing more capable and generalizable spatio-temporal reasoning systems. </p>
<blockquote>
<p>æ—¶ç©ºæ•°æ®æŒ–æ˜åœ¨å¤šä¸ªé¢†åŸŸä¸­çš„å†³ç­–åˆ¶å®šä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸ä»…é™äºç‹­çª„çš„ä»»åŠ¡ï¼Œç¼ºä¹å¤šä»»åŠ¡æ¨ç†å’Œå¤æ‚é•¿å½¢å¼æ¨ç†çš„èƒ½åŠ›ï¼Œæ— æ³•ç”Ÿæˆæ·±å…¥çš„ã€è§£é‡Šæ€§çš„è¾“å‡ºã€‚è¿™äº›é™åˆ¶é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­å¤šé¢å†³ç­–åœºæ™¯çš„åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†STReasonï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸æ—¶ç©ºæ¨¡å‹çš„åˆ†æèƒ½åŠ›ç›¸ç»“åˆï¼Œç”¨äºå¤šä»»åŠ¡æ¨ç†å’Œæ‰§è¡Œã€‚STReasonä¸éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œå®ƒåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ å°†å¤æ‚çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢åˆ†è§£ä¸ºæ¨¡å—åŒ–ã€å¯è§£é‡Šçš„ç¨‹åºï¼Œç„¶åç³»ç»Ÿåœ°æ‰§è¡Œè¿™äº›ç¨‹åºä»¥ç”Ÿæˆè§£å†³æ–¹æ¡ˆå’Œè¯¦ç»†çš„ç†ç”±ã€‚ä¸ºäº†ä¿ƒè¿›ä¸¥æ ¼è¯„ä¼°ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œå…¶ä¸­è®¾è®¡äº†ä¸“é—¨é’ˆå¯¹é•¿å½¢å¼æ—¶ç©ºæ¨ç†çš„æŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTReasonåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½æ˜¾è‘—ä¼˜äºå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ã€æ¨ç†å¯†é›†å‹çš„æ—¶ç©ºåœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†STReasonçš„å¯ä¿¡åº¦å’Œå®ç”¨æ€§ï¼Œè¡¨æ˜å…¶å‡å°‘ä¸“å®¶å·¥ä½œé‡å¹¶æ‰©å¤§åœ¨ç°å®ä¸–ç•Œæ—¶ç©ºä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚æˆ‘ä»¬ç›¸ä¿¡STReasonä¸ºå¼€å‘æ›´å…·èƒ½åŠ›å’Œé€šç”¨æ€§çš„æ—¶ç©ºæ¨ç†ç³»ç»Ÿæä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20073v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ—¶ç©ºæ•°æ®æŒ–æ˜åœ¨å¤šä¸ªé¢†åŸŸä¸­çš„å†³ç­–åˆ¶å®šä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸å±€é™äºç‹­çª„çš„ä»»åŠ¡ï¼Œç¼ºä¹è¿›è¡Œå¤šä»»åŠ¡æ¨ç†å’Œå¤æ‚é•¿å½¢å¼æ¨ç†çš„èƒ½åŠ›ï¼Œè¿™è¦æ±‚ç”Ÿæˆæ·±å…¥çš„ã€è§£é‡Šæ€§çš„è¾“å‡ºã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†STReasonæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ—¶ç©ºæ¨¡å‹çš„åˆ†æèƒ½åŠ›ï¼Œç”¨äºå¤šä»»åŠ¡æ¨ç†å’Œæ‰§è¡Œã€‚STReasonåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå°†å¤æ‚çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢åˆ†è§£ä¸ºæ¨¡å—åŒ–ã€å¯è§£é‡Šçš„ç¨‹åºï¼Œç„¶åç³»ç»Ÿåœ°æ‰§è¡Œä»¥ç”Ÿæˆè§£å†³æ–¹æ¡ˆå’Œè¯¦ç»†çš„ç†ç”±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTReasonåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½æ˜¾è‘—ä¼˜äºå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ã€æ¨ç†å¯†é›†å‹çš„æ—¶ç©ºåœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶ç©ºæ•°æ®æŒ–æ˜åœ¨å†³ç­–åˆ¶å®šä¸­èµ·å…³é”®ä½œç”¨ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°æœ‰é™ã€‚</li>
<li>STReasonæ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ—¶ç©ºæ¨¡å‹çš„èƒ½åŠ›ï¼Œæ”¯æŒå¤šä»»åŠ¡æ¨ç†å’Œæ‰§è¡Œã€‚</li>
<li>STReasonåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ åˆ†è§£è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œå¹¶ç³»ç»Ÿåœ°æ‰§è¡Œä»¥ç”Ÿæˆè§£é‡Šæ€§è¾“å‡ºã€‚</li>
<li>STReasonåœ¨å¤æ‚ã€æ¨ç†å¯†é›†å‹çš„æ—¶ç©ºåœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ–°æ„å»ºçš„åŸºå‡†æ•°æ®é›†å’Œç»Ÿä¸€è¯„ä¼°æ¡†æ¶ä¸ºé•¿å½¢å¼æ—¶ç©ºæ¨ç†æä¾›äº†ä¸¥è°¨çš„è¯„ä»·æ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœå’Œäººç±»è¯„ä¼°éªŒè¯äº†STReasonçš„å¯é æ€§å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20073">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-52923f82e35618993d2f3640beda2c94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5879953c48701d0f0acaaf03055a4dc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-664a3347dfb74d287f8d75db4f064c1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f190b14b703b2146f3551f9e69bb4bdd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="QHackBench-Benchmarking-Large-Language-Models-for-Quantum-Code-Generation-Using-PennyLane-Hackathon-Challenges"><a href="#QHackBench-Benchmarking-Large-Language-Models-for-Quantum-Code-Generation-Using-PennyLane-Hackathon-Challenges" class="headerlink" title="QHackBench: Benchmarking Large Language Models for Quantum Code   Generation Using PennyLane Hackathon Challenges"></a>QHackBench: Benchmarking Large Language Models for Quantum Code   Generation Using PennyLane Hackathon Challenges</h2><p><strong>Authors:Abdul Basit, Minghao Shao, Haider Asif, Nouhaila Innan, Muhammad Kashif, Alberto Marchisio, Muhammad Shafique</strong></p>
<p>Recent advances in Large Language Models (LLMs) have demonstrated strong potential in code generation, yet their effectiveness in quantum computing remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum code generation using real-world challenges from the Quantum Hackathon (QHack). We introduce QHackBench, a novel benchmark dataset derived from QHack competitions, and evaluate model performance under vanilla prompting and Retrieval-Augmented Generation (RAG). Our structured evaluation framework assesses functional correctness, syntactic validity, and execution success across varying challenge difficulties. Results indicate that RAG-enhanced models, supplemented with an augmented PennyLane dataset, approximately generate similar results as the standard prompting, particularly in complex quantum algorithms. Additionally, we introduce a multi-agent evaluation pipeline that iteratively refines incorrect solutions, further enhancing execution success rates. To foster further research, we commit to publicly releasing QHackBench, along with our evaluation framework and experimental results, enabling continued advancements in AI-assisted quantum programming. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å±•ç°å‡ºäº†å¼ºå¤§çš„æ½œåŠ›ï¼Œç„¶è€Œå®ƒä»¬åœ¨é‡å­è®¡ç®—ä¸­çš„åº”ç”¨ä»ç„¶ç¼ºä¹è¶³å¤Ÿçš„æ¢ç´¢ã€‚æœ¬æ–‡ä½¿ç”¨æ¥è‡ªé‡å­é»‘å®¢é©¬æ‹‰æ¾ï¼ˆQHackï¼‰çš„ç°å®æŒ‘æˆ˜ï¼Œå¯¹åŸºäºPennyLaneçš„é‡å­ä»£ç ç”Ÿæˆçš„LLMæ€§èƒ½è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬ä»‹ç»äº†QHackBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä»QHackç«èµ›ä¸­è¡ç”Ÿå‡ºæ¥çš„æ–°å‹åŸºå‡†æ•°æ®é›†ï¼Œå¹¶è¯„ä¼°äº†æ ‡å‡†æç¤ºå’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸‹çš„æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æ„åŒ–è¯„ä¼°æ¡†æ¶è¯„ä¼°äº†ä¸åŒæŒ‘æˆ˜éš¾åº¦ä¸‹çš„åŠŸèƒ½æ­£ç¡®æ€§ã€è¯­æ³•æœ‰æ•ˆæ€§å’Œæ‰§è¡ŒæˆåŠŸç‡ã€‚ç»“æœè¡¨æ˜ï¼Œé€šè¿‡å¢å¼ºPennyLaneæ•°æ®é›†å¢å¼ºçš„RAGæ¨¡å‹åœ¨å¤æ‚é‡å­ç®—æ³•æ–¹é¢å¤§çº¦ç”Ÿæˆäº†ä¸æ ‡å‡†æç¤ºç›¸ä¼¼çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå¤šä»£ç†è¯„ä¼°ç®¡é“ï¼Œè¯¥ç®¡é“å¯ä»¥è¿­ä»£ä¼˜åŒ–é”™è¯¯è§£å†³æ–¹æ¡ˆï¼Œè¿›ä¸€æ­¥æé«˜æ‰§è¡ŒæˆåŠŸç‡ã€‚ä¸ºäº†ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒQHackBenchï¼Œä»¥åŠæˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶å’Œå®éªŒç»“æœï¼Œä»¥ä¿ƒè¿›äººå·¥æ™ºèƒ½è¾…åŠ©é‡å­ç¼–ç¨‹çš„æŒç»­å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20008v1">PDF</a> 8 pages, 6 figures, 3 tables, submitted to QAI 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å±•ç°å‡ºå¼ºå¤§æ½œåŠ›ï¼Œä½†åœ¨é‡å­è®¡ç®—é¢†åŸŸçš„åº”ç”¨å°šå¾…æ¢ç´¢ã€‚æœ¬æ–‡ä½¿ç”¨æ¥è‡ªé‡å­é»‘å®¢æ¾ï¼ˆQHackï¼‰çš„ç°å®æŒ‘æˆ˜ï¼Œå¯¹PennyLaneåŸºç¡€çš„é‡å­ä»£ç ç”Ÿæˆè¿›è¡ŒLLMåŸºå‡†æµ‹è¯•ã€‚å¼•å…¥QHackBenchåŸºå‡†æ•°æ®é›†ï¼Œè¯„ä¼°äº†æ ‡å‡†æç¤ºå’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸‹çš„æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡ç»“æ„åŒ–çš„è¯„ä¼°æ¡†æ¶å¯¹åŠŸèƒ½æ­£ç¡®æ€§ã€è¯­æ³•æœ‰æ•ˆæ€§åŠæ‰§è¡ŒæˆåŠŸç‡åœ¨ä¸åŒéš¾åº¦çš„æŒ‘æˆ˜ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æ‰©å……çš„PennyLaneæ•°æ®é›†æ”¯æŒä¸‹ï¼Œä½¿ç”¨RAGå¢å¼ºçš„æ¨¡å‹èƒ½å¤Ÿåœ¨å¤æ‚é‡å­ç®—æ³•ä¸­ç”Ÿæˆä¸æ ‡å‡†æç¤ºç›¸ä¼¼çš„ç»“æœã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§å¤šæ™ºèƒ½ä½“è¯„ä¼°æµç¨‹ï¼Œå¯è¿­ä»£ä¼˜åŒ–é”™è¯¯è§£å†³æ–¹æ¡ˆï¼Œè¿›ä¸€æ­¥æé«˜æ‰§è¡ŒæˆåŠŸç‡ã€‚ä¸ºæ¨è¿›è¿›ä¸€æ­¥ç ”ç©¶ï¼Œå…¬å¼€å‘å¸ƒQHackBenchåŸºå‡†æ•°æ®é›†ã€è¯„ä¼°æ¡†æ¶åŠå®éªŒç»“æœï¼Œæ¨åŠ¨AIè¾…åŠ©é‡å­ç¼–ç¨‹çš„æŒç»­å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é‡å­ä»£ç ç”Ÿæˆæ–¹é¢æ½œåŠ›å·¨å¤§ï¼Œä½†ä»å¾…æ¢ç´¢ã€‚</li>
<li>QHackBenchåŸºå‡†æ•°æ®é›†ç”¨äºPennyLaneåŸºç¡€çš„é‡å­ä»£ç ç”Ÿæˆçš„LLMåŸºå‡†æµ‹è¯•ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½é€šè¿‡æ ‡å‡†æç¤ºå’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>æ¨¡å‹åœ¨åŠŸèƒ½æ­£ç¡®æ€§ã€è¯­æ³•æœ‰æ•ˆæ€§åŠæ‰§è¡ŒæˆåŠŸç‡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå°¤å…¶åœ¨å¤æ‚é‡å­ç®—æ³•ä¸­ã€‚</li>
<li>RAGå¢å¼ºæ¨¡å‹åœ¨æ‰©å……æ•°æ®é›†æ”¯æŒä¸‹å¯ç”Ÿæˆä¸æ ‡å‡†æç¤ºç›¸ä¼¼ç»“æœã€‚</li>
<li>å¼•å…¥å¤šæ™ºèƒ½ä½“è¯„ä¼°æµç¨‹ä¼˜åŒ–é”™è¯¯è§£å†³æ–¹æ¡ˆï¼Œæé«˜æ‰§è¡ŒæˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20008">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9af4e20b705a584cbef76d26a28b5af8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9be187eb8f2f9108c153a59cb4088a61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b940c6e17f14f98a747b4921b8c8b69.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b61bd4dab0ca0d4c1ab74ec603a1a9a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d05a9af53738092e30ce68d89dad78f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac56b866b18ae1c46dfdeea1b3363175.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21176d9e9c7d923da0e87f62d5e3ef11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ab1d7858905909146c93dc9efbaac26.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Thought-Anchors-Which-LLM-Reasoning-Steps-Matter"><a href="#Thought-Anchors-Which-LLM-Reasoning-Steps-Matter" class="headerlink" title="Thought Anchors: Which LLM Reasoning Steps Matter?"></a>Thought Anchors: Which LLM Reasoning Steps Matter?</h2><p><strong>Authors:Paul C. Bogdan, Uzay Macar, Neel Nanda, Arthur Conmy</strong></p>
<p>Reasoning large language models have recently achieved state-of-the-art performance in many fields. However, their long-form chain-of-thought reasoning creates interpretability challenges as each generated token depends on all previous ones, making the computation harder to decompose. We argue that analyzing reasoning traces at the sentence level is a promising approach to understanding reasoning processes. We present three complementary attribution methods: (1) a black-box method measuring each sentenceâ€™s counterfactual importance by comparing final answers across 100 rollouts conditioned on the model generating that sentence or one with a different meaning; (2) a white-box method of aggregating attention patterns between pairs of sentences, which identified â€œbroadcastingâ€ sentences that receive disproportionate attention from all future sentences via â€œreceiverâ€ attention heads; (3) a causal attribution method measuring logical connections between sentences by suppressing attention toward one sentence and measuring the effect on each future sentenceâ€™s tokens. Each method provides evidence for the existence of thought anchors, reasoning steps that have outsized importance and that disproportionately influence the subsequent reasoning process. These thought anchors are typically planning or backtracking sentences. We provide an open-source tool (<a target="_blank" rel="noopener" href="http://www.thought-anchors.com/">www.thought-anchors.com</a>) for visualizing the outputs of our methods, and present a case study showing converging patterns across methods that map how a model performs multi-step reasoning. The consistency across methods demonstrates the potential of sentence-level analysis for a deeper understanding of reasoning models. </p>
<blockquote>
<p>æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹å·²åœ¨è®¸å¤šé¢†åŸŸå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„é•¿å½¢å¼æ€ç»´é“¾æ¨ç†å¸¦æ¥äº†å¯è§£é‡Šæ€§æŒ‘æˆ˜ï¼Œå› ä¸ºæ¯ä¸ªç”Ÿæˆçš„ä»¤ç‰Œéƒ½ä¾èµ–äºæ‰€æœ‰å…ˆå‰çš„ä»¤ç‰Œï¼Œä½¿å¾—è®¡ç®—æ›´éš¾ä»¥åˆ†è§£ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨å¥å­çº§åˆ«åˆ†ææ¨ç†ç—•è¿¹æ˜¯ç†è§£æ¨ç†è¿‡ç¨‹çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§äº’è¡¥çš„å½’å› æ–¹æ³•ï¼šï¼ˆ1ï¼‰ä¸€ç§é»‘ç›’æ–¹æ³•ï¼Œé€šè¿‡æ¯”è¾ƒæ¨¡å‹ç”Ÿæˆè¯¥å¥å­æˆ–å…·æœ‰ä¸åŒå«ä¹‰çš„å¥å­çš„100æ¬¡æ»šåŠ¨ç»“æœä¸­çš„æœ€ç»ˆç­”æ¡ˆï¼Œæ¥è¡¡é‡æ¯ä¸ªå¥å­çš„åäº‹å®é‡è¦æ€§ï¼›ï¼ˆ2ï¼‰ä¸€ç§ç™½ç›’æ–¹æ³•ï¼Œèšåˆå¥å­å¯¹ä¹‹é—´çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œè¯¥æ–¹æ³•ç¡®å®šäº†é€šè¿‡â€œæ¥æ”¶è€…â€æ³¨æ„åŠ›å¤´ä»æ‰€æœ‰æœªæ¥å¥å­æ¥æ”¶ä¸æˆæ¯”ä¾‹çš„æ³¨æ„åŠ›çš„â€œå¹¿æ’­â€å¥å­ï¼›ï¼ˆ3ï¼‰ä¸€ç§å› æœå½’å› æ–¹æ³•ï¼Œé€šè¿‡æŠ‘åˆ¶å¯¹ä¸€ä¸ªå¥å­çš„æ³¨æ„åŠ›å¹¶æµ‹é‡å¯¹æ¯ä¸€ä¸ªæœªæ¥å¥å­çš„å½±å“æ¥è¡¡é‡å¥å­ä¹‹é—´çš„é€»è¾‘è”ç³»ã€‚æ¯ç§æ–¹æ³•éƒ½è¯æ˜å­˜åœ¨æ€ç»´é”šç‚¹ï¼Œå³å…·æœ‰é‡å¤§å½±å“çš„æ¨ç†æ­¥éª¤ï¼Œè¿™äº›æ­¥éª¤ä¼šä¸æˆæ¯”ä¾‹åœ°å½±å“éšåçš„æ¨ç†è¿‡ç¨‹ã€‚è¿™äº›æ€ç»´é”šç‚¹é€šå¸¸æ˜¯è§„åˆ’æˆ–å›æº¯å¥å­ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¼€æºå·¥å…·ï¼ˆ<a target="_blank" rel="noopener" href="http://www.thought-anchors.com),ç”¨äºå¯è§†åŒ–æˆ‘ä»¬æ–¹æ³•çš„è¾“å‡º,å¹¶å±•ç¤ºäº†ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶,å±•ç¤ºå„æ–¹æ³•ä¹‹é—´æ”¶æ•›çš„æ¨¡å¼,è¿™äº›æ¨¡å¼å¯ä»¥æ˜ å°„æ¨¡å‹å¦‚ä½•è¿›è¡Œå¤šæ­¥æ¨ç†.å„æ–¹æ³•ä¹‹é—´çš„ä¸€è‡´æ€§è¡¨æ˜å¥å­çº§åˆ†æåœ¨æ·±å…¥ç†è§£æ¨ç†æ¨¡å‹æ–¹é¢å…·æœ‰æ½œåŠ›./">www.thought-anchors.comï¼‰ï¼Œç”¨äºå¯è§†åŒ–æˆ‘ä»¬æ–¹æ³•çš„è¾“å‡ºï¼Œå¹¶å±•ç¤ºäº†ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºå„æ–¹æ³•ä¹‹é—´æ”¶æ•›çš„æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼å¯ä»¥æ˜ å°„æ¨¡å‹å¦‚ä½•è¿›è¡Œå¤šæ­¥æ¨ç†ã€‚å„æ–¹æ³•ä¹‹é—´çš„ä¸€è‡´æ€§è¡¨æ˜å¥å­çº§åˆ†æåœ¨æ·±å…¥ç†è§£æ¨ç†æ¨¡å‹æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19143v2">PDF</a> Paul C. Bogdan and Uzay Macar contributed equally to this work, and   their listed order was determined by coinflip. Neel Nanda and Arthur Conmy   contributed equally to this work as senior authors, and their listed order   was determined by coinflip</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†å…¶é•¿å½¢å¼æ€ç»´é“¾æ¨ç†å¸¦æ¥çš„è§£é‡Šæ€§æŒ‘æˆ˜ä¸å®¹å¿½è§†ã€‚æ¯ä¸ªç”Ÿæˆçš„æ ‡è®°éƒ½ä¾èµ–äºä¹‹å‰çš„æ‰€æœ‰æ ‡è®°ï¼Œè¿™ä½¿å¾—è®¡ç®—éš¾ä»¥åˆ†è§£ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•æ¥ç†è§£æ¨ç†è¿‡ç¨‹ï¼Œå³åœ¨å¥å­å±‚é¢åˆ†ææ¨ç†ç—•è¿¹ã€‚æ–‡ç« æå‡ºäº†ä¸‰ç§äº’è¡¥å½’å› æ–¹æ³•ï¼Œæ¯ç§æ–¹æ³•å‡æ”¯æŒå¥å­åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„é‡è¦æ€§ï¼Œå³æ€ç»´é”šçš„å­˜åœ¨ã€‚æœ¬æ–‡è¿˜æä¾›äº†ä¸€ä¸ªå¼€æºå·¥å…·ï¼Œç”¨äºå¯è§†åŒ–è¿™äº›æ–¹æ³•çš„ç»“æœï¼Œå¹¶é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†è¿™äº›æ–¹æ³•å¦‚ä½•æ˜ å°„æ¨¡å‹çš„å¤šæ­¥æ¨ç†è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å­˜åœ¨è§£é‡Šæ€§æŒ‘æˆ˜ã€‚</li>
<li>å¥å­å±‚é¢çš„åˆ†ææ˜¯ä¸€ç§ç†è§£æ¨ç†è¿‡ç¨‹çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>ä¸‰ç§å½’å› æ–¹æ³•ç”¨äºåˆ†æå¥å­åœ¨æ¨ç†ä¸­çš„é‡è¦æ€§ï¼Œå³æ€ç»´é”šçš„å­˜åœ¨ã€‚è¿™äº›æ–¹æ³•åŒ…æ‹¬ï¼šæ¯”è¾ƒå¥å­ç”Ÿæˆçš„æœ€ç»ˆç­”æ¡ˆçš„é»‘ç®±æ–¹æ³•ï¼›èšåˆå¥å­é—´æ³¨æ„åŠ›æ¨¡å¼çš„ç™½ç®±æ–¹æ³•ï¼›ä»¥åŠè¡¡é‡å¥å­é—´é€»è¾‘è¿æ¥çš„å› æœå½’å› æ–¹æ³•ã€‚</li>
<li>å¼€æºå·¥å…·å¯ç”¨äºå¯è§†åŒ–å½’å› æ–¹æ³•çš„ç»“æœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c9bf6130754c6d512daed283a9d9e976.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bdea3335500968ce818f6fba1d902529.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a61ead1e975ced32a761c1c5d0a204c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9c154753648bca28752c395685217b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94b85e1c2bf3e627443ff6e9f2a827be.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Confucius3-Math-A-Lightweight-High-Performance-Reasoning-LLM-for-Chinese-K-12-Mathematics-Learning"><a href="#Confucius3-Math-A-Lightweight-High-Performance-Reasoning-LLM-for-Chinese-K-12-Mathematics-Learning" class="headerlink" title="Confucius3-Math: A Lightweight High-Performance Reasoning LLM for   Chinese K-12 Mathematics Learning"></a>Confucius3-Math: A Lightweight High-Performance Reasoning LLM for   Chinese K-12 Mathematics Learning</h2><p><strong>Authors:Lixin Wu, Na Cai, Qiao Cheng, Jiachen Wang, Yitao Duan</strong></p>
<p>We introduce Confucius3-Math, an open-source large language model with 14B parameters that (1) runs efficiently on a single consumer-grade GPU; (2) achieves SOTA performances on a range of mathematical reasoning tasks, outperforming many models with significantly larger sizes. In particular, as part of our mission to enhancing education and knowledge dissemination with AI, Confucius3-Math is specifically committed to mathematics learning for Chinese K-12 students and educators. Built via post-training with large-scale reinforcement learning (RL), Confucius3-Math aligns with national curriculum and excels at solving main-stream Chinese K-12 mathematical problems with low cost. In this report we share our development recipe, the challenges we encounter and the techniques we develop to overcome them. In particular, we introduce three technical innovations: Targeted Entropy Regularization, Recent Sample Recovery and Policy-Specific Hardness Weighting. These innovations encompass a new entropy regularization, a novel data scheduling policy, and an improved group-relative advantage estimator. Collectively, they significantly stabilize the RL training, improve data efficiency, and boost performance. Our work demonstrates the feasibility of building strong reasoning models in a particular domain at low cost. We open-source our model and code at <a target="_blank" rel="noopener" href="https://github.com/netease-youdao/Confucius3-Math">https://github.com/netease-youdao/Confucius3-Math</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†å­”å­3-æ•°å­¦ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰14äº¿å‚æ•°çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ƒï¼ˆ1ï¼‰åœ¨å•ä¸ªæ¶ˆè´¹çº§GPUä¸Šè¿è¡Œé«˜æ•ˆï¼›ï¼ˆ2ï¼‰åœ¨å„ç§æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¼˜äºè®¸å¤šè§„æ¨¡æ›´å¤§çš„æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯ï¼Œä½œä¸ºæˆ‘ä»¬åˆ©ç”¨äººå·¥æ™ºèƒ½å¢å¼ºæ•™è‚²å’ŒçŸ¥è¯†ä¼ æ’­çš„ä½¿å‘½çš„ä¸€éƒ¨åˆ†ï¼Œå­”å­3-æ•°å­¦è‡´åŠ›äºä¸ºä¸­å›½çš„K-12å­¦ç”Ÿå’Œæ•™è‚²å·¥ä½œè€…æä¾›æ•°å­¦å­¦ä¹ æ”¯æŒã€‚é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œåè®­ç»ƒï¼Œå­”å­3-æ•°å­¦ä¸å›½å®¶è¯¾ç¨‹ç›¸ä¸€è‡´ï¼Œæ“…é•¿è§£å†³ä¸»æµçš„ä¸­å›½K-12æ•°å­¦é—®é¢˜ï¼Œæˆæœ¬ä½å»‰ã€‚åœ¨è¿™ä»½æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬åˆ†äº«äº†æˆ‘ä»¬çš„å¼€å‘é…æ–¹ã€é‡åˆ°çš„æŒ‘æˆ˜ä»¥åŠä¸ºå…‹æœè¿™äº›æŒ‘æˆ˜è€Œå¼€å‘çš„æŠ€æœ¯ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸‰é¡¹æŠ€æœ¯åˆ›æ–°ï¼šç›®æ ‡ç†µæ­£åˆ™åŒ–ã€æœ€è¿‘æ ·æœ¬æ¢å¤å’Œæ”¿ç­–ç‰¹å®šç¡¬åº¦åŠ æƒã€‚è¿™äº›åˆ›æ–°åŒ…æ‹¬ä¸€ç§æ–°çš„ç†µæ­£åˆ™åŒ–ã€ä¸€ç§æ–°çš„æ•°æ®è°ƒåº¦ç­–ç•¥å’Œæ”¹è¿›çš„ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡å™¨ã€‚å®ƒä»¬å…±åŒæ˜¾è‘—ç¨³å®šäº†RLè®­ç»ƒï¼Œæé«˜äº†æ•°æ®æ•ˆç‡ï¼Œå¹¶æé«˜äº†æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜äº†åœ¨ç‰¹å®šé¢†åŸŸæ„å»ºå¼ºå¤§çš„æ¨ç†æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œå¹¶ä¸”æˆæœ¬ä½å»‰ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/netease-youdao/Confucius3-Math%E4%B8%8A%E5%85%AC%E5%BC%80%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%92%8C%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/netease-youdao/Confucius3-Mathä¸Šå…¬å¼€äº†æˆ‘ä»¬çš„æ¨¡å‹å’Œä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18330v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹ä¸­æ–‡åŸºç¡€æ•™è‚²é¢†åŸŸçš„æ•°å­¦é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹Confucius3-Mathã€‚æ­¤æ¨¡å‹å‚æ•°è¾¾åˆ°åå››äº¿è§„æ¨¡ï¼Œèƒ½åœ¨å•ä¸€æ¶ˆè´¹çº§GPUä¸Šé«˜æ•ˆè¿è¡Œï¼Œå¹¶åœ¨ä¸€ç³»åˆ—æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†å“è¶Šè¡¨ç°ã€‚é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä¸å›½å®¶æ”¿ç­–ä¸€è‡´å¹¶èƒ½ä½æˆæœ¬åœ°è§£å†³ä¸»æµä¸­å°å­¦æ•°å­¦é—®é¢˜ã€‚æˆ‘ä»¬åˆ†äº«äº†è¿™ä¸ªæ¨¡å‹çš„å‘å±•è¿‡ç¨‹åŠæ‰€é‡‡ç”¨çš„åˆ›æ–°æ–¹æ³•ã€‚å¼€æºæ­¤æ¨¡å‹å’Œä»£ç äºå…¬ä¼—è§†é‡ï¼Œå¸Œæœ›èƒ½åŠ©åŠ›æ•™è‚²æ™®åŠå’ŒçŸ¥è¯†ä¼ æ’­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Confucius3-Mathæ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸­æ–‡åŸºç¡€æ•™è‚²é¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æ•°å­¦å­¦ä¹ é—®é¢˜ã€‚</li>
<li>æ¨¡å‹å…·å¤‡ä¼˜å¼‚æ€§èƒ½å’Œé«˜æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªæ¶ˆè´¹çº§GPUä¸Šè¿è¡Œã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºï¼Œè¶…è¶Šè®¸å¤šæ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä¸å›½å®¶è¯¾ç¨‹å¤§çº²ç›¸ç¬¦ï¼Œæ“…é•¿è§£å†³ä¸»æµä¸­å°å­¦æ•°å­¦é—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18330">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce5edbf071f25efc26f68d2d83fa4481.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cbac252c05a66617f75b69f2e790afd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b2f5a6652e4ff38348aca6472f4a103.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="PhysUniBench-An-Undergraduate-Level-Physics-Reasoning-Benchmark-for-Multimodal-Models"><a href="#PhysUniBench-An-Undergraduate-Level-Physics-Reasoning-Benchmark-for-Multimodal-Models" class="headerlink" title="PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for   Multimodal Models"></a>PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for   Multimodal Models</h2><p><strong>Authors:Lintao Wang, Encheng Su, Jiaqi Liu, Pengze Li, Peng Xia, Jiabei Xiao, Wenlong Zhang, Xinnan Dai, Xi Chen, Yuan Meng, Mingyu Ding, Lei Bai, Wanli Ouyang, Shixiang Tang, Aoran Wang, Xinzhu Ma</strong></p>
<p>Physics problem-solving is a challenging domain for large AI models, requiring integration of conceptual understanding, mathematical reasoning, and interpretation of physical diagrams. Current evaluation methodologies show notable limitations in capturing the breadth and complexity of undergraduate-level physics, underscoring the need for more rigorous assessments. To this end, we present PhysUniBench, a large-scale multimodal benchmark designed to evaluate and improve the reasoning capabilities of multimodal large language models (MLLMs) specifically on undergraduate-level physics problems. PhysUniBench consists of 3,304 physics questions spanning 8 major sub-disciplines of physics, each accompanied by one visual diagrams. The benchmark includes both open-ended and multiple-choice questions, systematically curated and difficulty-rated through an iterative model-in-the-loop process. The benchmarkâ€™s construction involved a rigorous multi-stage process, including multiple roll-outs, expert-level evaluation, automated filtering of easily solved problems, and a nuanced difficulty grading system with five levels. Through extensive experiments, we observe that current state-of-the-art models encounter substantial challenges in physics reasoning. For example, GPT-4o mini achieves only about 34.2% accuracy in the proposed PhysUniBench. These results highlight that current MLLMs struggle with advanced physics reasoning, especially on multi-step problems and those requiring precise diagram interpretation. By providing a broad and rigorous assessment tool, PhysUniBench aims to drive progress in AI for Science, encouraging the development of models with stronger physical reasoning, problem-solving skills, and multimodal understanding. The benchmark and evaluation scripts are available at <a target="_blank" rel="noopener" href="https://prismax-team.github.io/PhysUniBenchmark/">https://prismax-team.github.io/PhysUniBenchmark/</a>. </p>
<blockquote>
<p>ç‰©ç†é—®é¢˜è§£å†³æ˜¯ä¸€ä¸ªå¯¹å¤§å‹AIæ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸï¼Œå®ƒè¦æ±‚æ•´åˆæ¦‚å¿µç†è§£ã€æ•°å­¦æ¨ç†å’Œç‰©ç†å›¾è¡¨è§£é‡Šã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•åœ¨è®¡ç®—æœ¬ç§‘ç”Ÿæ°´å¹³çš„ç‰©ç†çŸ¥è¯†çš„å¹¿åº¦å’Œå¤æ‚æ€§æ–¹é¢å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ï¼Œè¿™å¼ºè°ƒäº†éœ€è¦è¿›è¡Œæ›´ä¸¥æ ¼è¯„ä¼°çš„å¿…è¦æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PhysUniBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡å¼åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæé«˜å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æœ¬ç§‘ç”Ÿæ°´å¹³ç‰©ç†é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚PhysUniBenchåŒ…å«3304ä¸ªç‰©ç†é—®é¢˜ï¼Œæ¶µç›–ç‰©ç†å­¦çš„8ä¸ªä¸»è¦å­å­¦ç§‘ï¼Œæ¯ä¸ªé—®é¢˜éƒ½é…æœ‰ä¸€ä¸ªè§†è§‰å›¾è¡¨ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬å¼€æ”¾é—®é¢˜å’Œå¤šé¡¹é€‰æ‹©é¢˜ï¼Œé€šè¿‡å¾ªç¯æ¨¡å‹è¿‡ç¨‹è¿›è¡Œç³»ç»Ÿçš„æ•´ç†å’Œéš¾åº¦è¯„çº§ã€‚åŸºå‡†æµ‹è¯•çš„æ„å»ºæ¶‰åŠä¸€ä¸ªä¸¥æ ¼çš„å¤šé˜¶æ®µè¿‡ç¨‹ï¼ŒåŒ…æ‹¬å¤šæ¬¡æ¨å‡ºã€ä¸“å®¶çº§è¯„ä¼°ã€è‡ªåŠ¨è¿‡æ»¤å®¹æ˜“è§£å†³çš„é—®é¢˜ï¼Œä»¥åŠä¸€ä¸ªäº”çº§ç²¾ç»†çš„éš¾åº¦åˆ†çº§ç³»ç»Ÿã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ç‰©ç†æ¨ç†æ–¹é¢é‡åˆ°äº†å¾ˆå¤§çš„æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼ŒGPT-4o miniåœ¨æå‡ºçš„PhysUniBenchä¸Šä»…è¾¾åˆ°çº¦34.2%çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœå¼ºè°ƒï¼Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é«˜çº§ç‰©ç†æ¨ç†æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ­¥éª¤é—®é¢˜å’Œéœ€è¦ç²¾ç¡®å›¾è¡¨è§£é‡Šçš„é—®é¢˜æ–¹é¢ã€‚é€šè¿‡æä¾›å¹¿æ³›è€Œä¸¥æ ¼çš„è¯„ä¼°å·¥å…·ï¼ŒPhysUniBenchæ—¨åœ¨æ¨åŠ¨äººå·¥æ™ºèƒ½ç§‘å­¦çš„å‘å±•ï¼Œé¼“åŠ±å¼€å‘å…·æœ‰æ›´å¼ºç‰©ç†æ¨ç†èƒ½åŠ›ã€é—®é¢˜è§£å†³èƒ½åŠ›å’Œå¤šæ¨¡å¼ç†è§£èƒ½åŠ›çš„æ¨¡å‹ã€‚åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°è„šæœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://prismax-team.github.io/PhysUniBenchmark/">é“¾æ¥åœ°å€</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17667v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºPhysUniBenchçš„å¤§è§„æ¨¡å¤šæ¨¡å¼åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœ¬ç§‘ç‰©ç†å­¦é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«3304ä¸ªç‰©ç†å­¦é—®é¢˜ï¼Œæ¶µç›–ç‰©ç†å­¦çš„8ä¸ªä¸»è¦å­å­¦ç§‘ï¼Œæ¯ä¸ªé—®é¢˜éƒ½é…å¤‡äº†ä¸€ä¸ªè§†è§‰å›¾è¡¨ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒè§‚å¯Ÿï¼Œå‘ç°å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ç‰©ç†æ¨ç†æ–¹é¢é‡åˆ°è¾ƒå¤§æŒ‘æˆ˜ã€‚PhysUniBenchçš„æ¨å‡ºæ—¨åœ¨ä¸ºäººå·¥æ™ºèƒ½ç§‘å­¦é¢†åŸŸçš„å‘å±•æä¾›æ¨åŠ¨åŠ›ï¼Œé¼“åŠ±å¼€å‘å…·æœ‰æ›´å¼ºç‰©ç†æ¨ç†ã€é—®é¢˜è§£å†³èƒ½åŠ›å’Œå¤šæ¨¡å¼ç†è§£èƒ½åŠ›çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Physics problem-solving is challenging for large AI models, requiring integration of conceptual understanding, mathematical reasoning, and diagram interpretation.</li>
<li>å½“å‰è¯„ä¼°æ–¹æ³•åœ¨æ•æ‰æœ¬ç§‘ç‰©ç†å­¦å¹¿åº¦ä¸å¤æ‚æ€§æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>æ¨å‡ºPhysUniBenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«3304ä¸ªç‰©ç†å­¦é—®é¢˜ï¼Œæ¶‰åŠ8ä¸ªä¸»è¦ç‰©ç†å­å­¦ç§‘ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•åŒ…å«å¼€æ”¾é—®é¢˜å’Œé€‰æ‹©é¢˜ï¼Œé€šè¿‡æ¨¡å‹å¾ªç¯è¿‡ç¨‹ä¸­çš„è¿­ä»£è¿›è¡Œç³»ç»Ÿæ€§ç­›é€‰å’Œéš¾åº¦è¯„çº§ã€‚</li>
<li>åŸºå‡†æµ‹è¯•çš„æ„å»ºæ¶‰åŠå¤šé˜¶æ®µè¿‡ç¨‹ï¼ŒåŒ…æ‹¬å¤šæ¬¡æ»šåŠ¨å‘å¸ƒã€ä¸“å®¶è¯„ä¼°ã€è‡ªåŠ¨è¿‡æ»¤æ˜“è§£å†³é—®é¢˜å’Œç»†è‡´çš„éš¾åº¦åˆ†çº§ç³»ç»Ÿã€‚</li>
<li>å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ç‰©ç†æ¨ç†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚GPT-4o miniåœ¨PhysUniBenchä¸Šçš„å‡†ç¡®ç‡ä»…ä¸º34.2%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9efd008cbb26a0f4748943663122f64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb7340026ac7bbb2f2a6cb27ee24038a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-949a8efd8599be9582d46baa06cb3c44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d07ec680c46599e63294869925c8bbf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a98263515a5431758e9f7c9129d5f75.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Discrete-JEPA-Learning-Discrete-Token-Representations-without-Reconstruction"><a href="#Discrete-JEPA-Learning-Discrete-Token-Representations-without-Reconstruction" class="headerlink" title="Discrete JEPA: Learning Discrete Token Representations without   Reconstruction"></a>Discrete JEPA: Learning Discrete Token Representations without   Reconstruction</h2><p><strong>Authors:Junyeob Baek, Hosung Lee, Christopher Hoang, Mengye Ren, Sungjin Ahn</strong></p>
<p>The cornerstone of cognitive intelligence lies in extracting hidden patterns from observations and leveraging these principles to systematically predict future outcomes. However, current image tokenization methods demonstrate significant limitations in tasks requiring symbolic abstraction and logical reasoning capabilities essential for systematic inference. To address this challenge, we propose Discrete-JEPA, extending the latent predictive coding framework with semantic tokenization and novel complementary objectives to create robust tokenization for symbolic reasoning tasks. Discrete-JEPA dramatically outperforms baselines on visual symbolic prediction tasks, while striking visual evidence reveals the spontaneous emergence of deliberate systematic patterns within the learned semantic token space. Though an initial model, our approach promises a significant impact for advancing Symbolic world modeling and planning capabilities in artificial intelligence systems. </p>
<blockquote>
<p>è®¤çŸ¥æ™ºèƒ½çš„æ ¸å¿ƒåœ¨äºä»è§‚å¯Ÿä¸­æå–éšè—çš„æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨è¿™äº›åŸç†ç³»ç»Ÿåœ°é¢„æµ‹æœªæ¥ç»“æœã€‚ç„¶è€Œï¼Œå½“å‰çš„å›¾åƒä»¤ç‰ŒåŒ–æ–¹æ³•åœ¨éœ€è¦è±¡å¾æ€§æŠ½è±¡å’Œé€»è¾‘æ¨ç†èƒ½åŠ›çš„ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºé‡å¤§å±€é™æ€§ï¼Œè¿™å¯¹äºç³»ç»Ÿæ¨ç†è‡³å…³é‡è¦ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Discrete-JEPAï¼Œå®ƒåœ¨æ½œåœ¨é¢„æµ‹ç¼–ç æ¡†æ¶çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡è¯­ä¹‰ä»¤ç‰ŒåŒ–å’Œæ–°å‹äº’è¡¥ç›®æ ‡ï¼Œä¸ºç¬¦å·æ¨ç†ä»»åŠ¡åˆ›å»ºäº†ç¨³å¥çš„ä»¤ç‰ŒåŒ–ã€‚Discrete-JEPAåœ¨è§†è§‰ç¬¦å·é¢„æµ‹ä»»åŠ¡ä¸Šçš„è¡¨ç°è¿œè¶…åŸºçº¿ï¼Œè€Œæœ‰åŠ›çš„è§†è§‰è¯æ®è¡¨æ˜ï¼Œåœ¨æ‰€å­¦çš„è¯­ä¹‰ä»¤ç‰Œç©ºé—´å†…å‡ºç°äº†è‡ªå‘ç³»ç»Ÿçš„æ¨¡å¼ã€‚å°½ç®¡è¿™æ˜¯ä¸€ä¸ªåˆæ­¥æ¨¡å‹ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæ¨è¿›äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„ç¬¦å·ä¸–ç•Œå»ºæ¨¡å’Œè§„åˆ’èƒ½åŠ›å¸¦æ¥äº†é‡å¤§å½±å“çš„æ‰¿è¯ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14373v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè®¤çŸ¥æ™ºèƒ½çš„æ ¸å¿ƒåœ¨äºä»è§‚å¯Ÿä¸­æå–éšè—æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨è¿™äº›åŸåˆ™ç³»ç»Ÿåœ°é¢„æµ‹æœªæ¥ç»“æœã€‚ç„¶è€Œï¼Œå½“å‰å›¾åƒä»¤ç‰ŒåŒ–æ–¹æ³•åœ¨éœ€è¦ç¬¦å·æŠ½è±¡å’Œé€»è¾‘æ¨ç†èƒ½åŠ›çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—å±€é™æ€§ï¼Œè¿™å¯¹äºç³»ç»Ÿæ¨ç†è‡³å…³é‡è¦ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Discrete-JEPAï¼Œå®ƒé€šè¿‡è¯­ä¹‰ä»¤ç‰ŒåŒ–å’Œæ–°çš„è¡¥å……ç›®æ ‡æ‰©å±•æ½œåœ¨é¢„æµ‹ç¼–ç æ¡†æ¶ï¼Œä¸ºç¬¦å·æ¨ç†ä»»åŠ¡åˆ›å»ºç¨³å¥çš„ä»¤ç‰ŒåŒ–ã€‚Discrete-JEPAåœ¨è§†è§‰ç¬¦å·é¢„æµ‹ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºåŸºå‡†æµ‹è¯•ï¼Œè€Œæœ‰åŠ›çš„è§†è§‰è¯æ®è¡¨æ˜ï¼Œåœ¨å­¦ä¹ çš„è¯­ä¹‰ä»¤ç‰Œç©ºé—´å†…å‡ºç°äº†è‡ªå‘ç³»ç»Ÿçš„æ¨¡å¼ã€‚å°½ç®¡æ˜¯ä¸€ä¸ªåˆæ­¥æ¨¡å‹ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæ¨è¿›äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„ç¬¦å·ä¸–ç•Œå»ºæ¨¡å’Œè§„åˆ’èƒ½åŠ›äº§ç”Ÿäº†é‡å¤§å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è®¤çŸ¥æ™ºèƒ½çš„æ ¸å¿ƒæ˜¯æå–éšè—æ¨¡å¼å¹¶è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>å½“å‰å›¾åƒä»¤ç‰ŒåŒ–æ–¹æ³•åœ¨ç¬¦å·æŠ½è±¡å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Discrete-JEPAé€šè¿‡è¯­ä¹‰ä»¤ç‰ŒåŒ–å’Œè¡¥å……ç›®æ ‡æ‰©å±•æ½œåœ¨é¢„æµ‹ç¼–ç æ¡†æ¶ã€‚</li>
<li>Discrete-JEPAåœ¨è§†è§‰ç¬¦å·é¢„æµ‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å­¦ä¹ çš„è¯­ä¹‰ä»¤ç‰Œç©ºé—´å†…å‡ºç°äº†è‡ªå‘ç³»ç»Ÿçš„æ¨¡å¼ã€‚</li>
<li>Discrete-JEPAå¯¹æ¨è¿›äººå·¥æ™ºèƒ½çš„ç¬¦å·ä¸–ç•Œå»ºæ¨¡æœ‰å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e3080322e65552d83db735ac7e906545.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1fe1f872b2bd84d3719c7c990f8358d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae76f10bbb3b312c8866f26d6a9f2d7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c83a4ed70085832f20b077e52d4174f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Scientistsâ€™-First-Exam-Probing-Cognitive-Abilities-of-MLLM-via-Perception-Understanding-and-Reasoning"><a href="#Scientistsâ€™-First-Exam-Probing-Cognitive-Abilities-of-MLLM-via-Perception-Understanding-and-Reasoning" class="headerlink" title="Scientistsâ€™ First Exam: Probing Cognitive Abilities of MLLM via   Perception, Understanding, and Reasoning"></a>Scientistsâ€™ First Exam: Probing Cognitive Abilities of MLLM via   Perception, Understanding, and Reasoning</h2><p><strong>Authors:Yuhao Zhou, Yiheng Wang, Xuming He, Ruoyao Xiao, Zhiwei Li, Qiantai Feng, Zijie Guo, Yuejin Yang, Hao Wu, Wenxuan Huang, Jiaqi Wei, Dan Si, Xiuqi Yao, Jia Bu, Haiwen Huang, Tianfan Fu, Shixiang Tang, Ben Fei, Dongzhan Zhou, Fenghua Ling, Yan Lu, Siqi Sun, Chenhui Li, Guanjie Zheng, Jiancheng Lv, Wenlong Zhang, Lei Bai</strong></p>
<p>Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientistsâ€™ First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries. </p>
<blockquote>
<p>ç§‘å­¦å‘ç°è¶Šæ¥è¶Šä¾èµ–äºåŸºäºä¿¡æ¯å¯†é›†çš„ç§‘å­¦æ•°æ®å’Œç‰¹å®šé¢†åŸŸä¸“ä¸šçŸ¥è¯†è¿›è¡Œå¤æ‚çš„å¤šæ¨¡å¼æ¨ç†ã€‚å¾—ç›Šäºä¸“å®¶çº§çš„ç§‘å­¦åŸºå‡†æµ‹è¯•ï¼Œç§‘å­¦å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨çœŸå®çš„å·¥ä½œæµç¨‹ä¸­æœ‰æ½œåŠ›æ˜¾è‘—å¢å¼ºè¿™ä¸€å‘ç°è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç§‘å­¦åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨è¯„ä¼°MLLMsçš„çŸ¥è¯†ç†è§£èƒ½åŠ›ä¸Šï¼Œå¯¼è‡´å¯¹å…¶æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›çš„è¯„ä¼°ä¸è¶³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ç§‘å­¦å®¶ç¬¬ä¸€æ¬¡è€ƒè¯•ï¼ˆSFEï¼‰åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡ä¸‰ä¸ªç›¸äº’å…³è”çš„æ°´å¹³æ¥è¯„ä¼°MLLMsçš„ç§‘å­¦è®¤çŸ¥èƒ½åŠ›ï¼šç§‘å­¦ä¿¡å·æ„ŸçŸ¥ã€ç§‘å­¦å±æ€§ç†è§£ã€ç§‘å­¦æ¯”è¾ƒæ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼ŒSFEåŒ…å«830ä¸ªä¸“å®¶éªŒè¯è¿‡çš„é—®ç­”å¯¹ï¼Œæ¶‰åŠä¸‰ç§é—®é¢˜ç±»å‹ï¼Œæ¶µç›–äº”ä¸ªé«˜ä»·å€¼å­¦ç§‘çš„66ä¸ªå¤šæ¨¡å¼ä»»åŠ¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œç›®å‰æœ€å…ˆè¿›çš„GPT-o3å’ŒInternVL-3åœ¨SFEä¸Šçš„è¡¨ç°ä»…ä¸º34.08%å’Œ26.52%ï¼Œè¿™è¡¨æ˜MLLMsåœ¨ç§‘å­¦é¢†åŸŸä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æˆ‘ä»¬å¸Œæœ›ä»SFEä¸­è·å¾—çš„è®¤è¯†èƒ½å¤Ÿä¿ƒè¿›äººå·¥æ™ºèƒ½å¢å¼ºç§‘å­¦å‘ç°çš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10521v3">PDF</a> 82 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¿¡æ¯å¯†é›†å‹ç§‘å­¦æ•°æ®å’Œé¢†åŸŸç‰¹å®šä¸“ä¸šçŸ¥è¯†ï¼Œç§‘å­¦å‘ç°è¶Šæ¥è¶Šä¾èµ–äºå¤æ‚çš„è·¨æ¨¡æ€æ¨ç†ã€‚ç§‘å­¦è·¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å€ŸåŠ©ä¸“å®¶çº§ç§‘å­¦åŸºå‡†ï¼Œå…·æœ‰å¢å¼ºç°å®å·¥ä½œæµç¨‹ä¸­çš„å‘ç°è¿‡ç¨‹çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç§‘å­¦åŸºå‡†ä¸»è¦ä¾§é‡äºè¯„ä¼°MLLMsçš„çŸ¥è¯†ç†è§£èƒ½åŠ›ï¼Œå¯¼è‡´å¯¹å…¶æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›çš„è¯„ä¼°ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ç§‘å­¦å®¶ç¬¬ä¸€æ¬¡è€ƒè¯•ï¼ˆSFEï¼‰åŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡ä¸‰ä¸ªç›¸äº’è”ç³»çš„æ°´å¹³è¯„ä¼°MLLMsçš„ç§‘å­¦è®¤çŸ¥èƒ½åŠ›ï¼šç§‘å­¦ä¿¡å·æ„ŸçŸ¥ã€ç§‘å­¦å±æ€§ç†è§£ã€ç§‘å­¦æ¯”è¾ƒæ¨ç†ã€‚SFEåŒ…å«830ä¸ªä¸“å®¶éªŒè¯çš„é—®ç­”å¯¹ï¼Œæ¶µç›–ä¸‰ç§é—®é¢˜ç±»å‹å’Œ66ä¸ªè·¨æ¨¡æ€ä»»åŠ¡ï¼Œæ¶‰åŠäº”ä¸ªé«˜ä»·å€¼å­¦ç§‘ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„GPT-o3å’ŒInternVL-3åœ¨SFEä¸Šçš„è¡¨ç°ä»…ä¸º34.08%å’Œ26.52%ï¼Œè¿™çªæ˜¾å‡ºMLLMsåœ¨ç§‘å­¦é¢†åŸŸæœ‰å·¨å¤§çš„æ”¹è¿›ç©ºé—´ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡SFEè·å¾—çš„è§è§£èƒ½ä¿ƒè¿›AIå¢å¼ºç§‘å­¦å‘ç°çš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§‘å­¦å‘ç°ç°åœ¨ä¾èµ–äºå¤æ‚çš„è·¨æ¨¡æ€æ¨ç†å’Œä¿¡æ¯å¯†é›†å‹æ•°æ®ã€‚</li>
<li>ç§‘å­¦è·¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ‰æ½œåŠ›æ”¹å–„ç§‘å­¦å‘ç°è¿‡ç¨‹ã€‚</li>
<li>å½“å‰çš„ç§‘å­¦è¯„ä¼°åŸºå‡†ä¸»è¦å…³æ³¨MLLMsçš„çŸ¥è¯†ç†è§£èƒ½åŠ›ï¼Œå¿½è§†äº†å…¶æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç§‘å­¦å®¶ç¬¬ä¸€æ¬¡è€ƒè¯•ï¼ˆSFEï¼‰åŸºå‡†æ—¨åœ¨å…¨é¢è¯„ä¼°MLLMsçš„ç§‘å­¦è®¤çŸ¥èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç§‘å­¦ä¿¡å·æ„ŸçŸ¥ã€ç§‘å­¦å±æ€§ç†è§£å’Œç§‘å­¦æ¯”è¾ƒæ¨ç†ã€‚</li>
<li>SFEåŒ…å«ä¸“å®¶éªŒè¯çš„é—®ç­”å¯¹ï¼Œæ¶µç›–å¤šç§é—®é¢˜ç±»å‹å’Œè·¨æ¨¡æ€ä»»åŠ¡ï¼Œæ¶‰åŠå¤šä¸ªå­¦ç§‘ã€‚</li>
<li>æœ€å…ˆè¿›çš„MLLMsåœ¨SFEä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºå…¶åœ¨ç§‘å­¦é¢†åŸŸçš„å·¨å¤§æ”¹è¿›ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10521">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cb3d92158598397970c6aedce3c5df2b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a970e2bbb9edf925c2b94b260c5e7e8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-122fcd0cf5bc197d0dedc0a1a73382e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40e346a7ed5d56d9af01b6b00ac708e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e579c00f906d25d004c5ecb1e3ff9026.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-27/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-27/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-27/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6f3c1ed44efb0cbfb794708947c3304d.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-27  The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-26/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5b16fb436db5f89f8658599ea6ddbe9e.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-26  Bind-Your-Avatar Multi-Talking-Character Video Generation with Dynamic   3D-mask-based Embedding Router
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
