<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-06-27  MMSearch-R1 Incentivizing LMMs to Search">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b5c727c232a9c7e176f040eee15e49e1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    84 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-27-更新"><a href="#2025-06-27-更新" class="headerlink" title="2025-06-27 更新"></a>2025-06-27 更新</h1><h2 id="MMSearch-R1-Incentivizing-LMMs-to-Search"><a href="#MMSearch-R1-Incentivizing-LMMs-to-Search" class="headerlink" title="MMSearch-R1: Incentivizing LMMs to Search"></a>MMSearch-R1: Incentivizing LMMs to Search</h2><p><strong>Authors:Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, Ziwei Liu</strong></p>
<p>Robust deployment of large multimodal models (LMMs) in real-world scenarios requires access to external knowledge sources, given the complexity and dynamic nature of real-world information. Existing approaches such as retrieval-augmented generation (RAG) and prompt engineered search agents rely on rigid pipelines, often leading to inefficient or excessive search behaviors. We present MMSearch-R1, the first end-to-end reinforcement learning framework that enables LMMs to perform on-demand, multi-turn search in real-world Internet environments. Our framework integrates both image and text search tools, allowing the model to reason about when and how to invoke them guided by an outcome-based reward with a search penalty. To support training, We collect a multimodal search VQA dataset through a semi-automated pipeline that covers diverse visual and textual knowledge needs and curate a search-balanced subset with both search-required and search-free samples, which proves essential for shaping efficient and on-demand search behavior. Extensive experiments on knowledge-intensive and info-seeking VQA tasks show that our model not only outperforms RAG-based baselines of the same model size, but also matches the performance of a larger RAG-based model while reducing search calls by over 30%. We further analyze key empirical findings to offer actionable insights for advancing research in multimodal search. </p>
<blockquote>
<p>在真实世界场景中稳健部署大型多模态模型（LMMs）需要访问外部知识源，考虑到真实世界信息的复杂性和动态性。现有的方法如检索增强生成（RAG）和提示工程搜索代理依赖于僵化管道，通常导致低效或过度搜索行为。我们提出了MMSearch-R1，这是第一个端到端的强化学习框架，使LMMs能够在真实世界的互联网环境中进行按需多轮搜索。我们的框架集成了图像和文本搜索工具，允许模型根据以结果为基础的奖励和搜索惩罚来推理何时以及如何调用它们。为了支持训练，我们通过半自动化管道收集了一个多模态搜索VQA数据集，涵盖了多样化的视觉和文本知识需求，并筛选出一个搜索平衡的子集，其中包含搜索所需和无需搜索的样本，这对于塑造高效和按需搜索行为至关重要。在知识密集型和信息搜索VQA任务上的大量实验表明，我们的模型不仅优于基于RAG的相同模型大小的基线，而且与更大的基于RAG的模型相匹配，同时减少了超过30%的搜索调用。我们还进一步分析了关键实验结果，为推进多模态搜索研究提供了可操作的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20670v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/EvolvingLMMs-Lab/multimodal-search-r1">https://github.com/EvolvingLMMs-Lab/multimodal-search-r1</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于强化学习的端对端框架MMSearch-R1，用于大型多模态模型在真实互联网环境下的按需多轮搜索。框架集成了图像和文本搜索工具，通过结果导向的奖励机制和搜索惩罚来指导模型何时以及如何调用这些工具。此外，为了支持训练，本文还通过半自动化管道收集了一个多模态搜索问答数据集，并筛选出一个既包含需要搜索的样本又包含不需要搜索的样本的搜索平衡子集。实验表明，该模型不仅在知识密集型和信息搜寻问答任务上表现出优于基于RAG的基线模型的性能，而且在减少搜索调用次数方面表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型多模态模型在真实场景应用需访问外部知识源，处理复杂且动态的信息。</li>
<li>现有方法如RAG和提示工程搜索代理存在效率低下或过度搜索的问题。</li>
<li>MMSearch-R1是首个端到端的强化学习框架，使LMMs能在真实互联网环境中进行按需多轮搜索。</li>
<li>框架集成了图像和文本搜索工具，由结果导向的奖励和搜索惩罚机制引导模型决定何时及如何调用这些工具。</li>
<li>使用了半自动化管道收集多模态搜索问答数据集，并筛选出搜索平衡子集，对塑造高效且按需的搜索行为至关重要。</li>
<li>实验表明，MMMsearch-R1在知识密集型和信息搜寻问答任务上表现优异，相较于基线模型，不仅性能更佳，还能减少超过30%的搜索调用次数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20670">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8fb5eaccebbb2cfdbe007d11a1d70a26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40749615b0bf0810a22333cb4625d06e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-290213ccc59b187b55ad11cbc1999407.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="The-Decrypto-Benchmark-for-Multi-Agent-Reasoning-and-Theory-of-Mind"><a href="#The-Decrypto-Benchmark-for-Multi-Agent-Reasoning-and-Theory-of-Mind" class="headerlink" title="The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind"></a>The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind</h2><p><strong>Authors:Andrei Lupu, Timon Willi, Jakob Foerster</strong></p>
<p>As Large Language Models (LLMs) gain agentic abilities, they will have to navigate complex multi-agent scenarios, interacting with human users and other agents in cooperative and competitive settings. This will require new reasoning skills, chief amongst them being theory of mind (ToM), or the ability to reason about the “mental” states of other agents. However, ToM and other multi-agent abilities in LLMs are poorly understood, since existing benchmarks suffer from narrow scope, data leakage, saturation, and lack of interactivity. We thus propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM drawing inspiration from cognitive science, computational pragmatics and multi-agent reinforcement learning. It is designed to be as easy as possible in all other dimensions, eliminating confounding factors commonly found in other benchmarks. To our knowledge, it is also the first platform for designing interactive ToM experiments.   We validate the benchmark design through comprehensive empirical evaluations of frontier LLMs, robustness studies, and human-AI cross-play experiments. We find that LLM game-playing abilities lag behind humans and simple word-embedding baselines. We then create variants of two classic cognitive science experiments within Decrypto to evaluate three key ToM abilities. Surprisingly, we find that state-of-the-art reasoning models are significantly worse at those tasks than their older counterparts. This demonstrates that Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and paves the path towards better artificial agents. </p>
<blockquote>
<p>随着大型语言模型（LLM）获得代理能力，它们将需要应对复杂的多代理场景，在合作和竞争环境中与人类用户和其他代理进行交互。这将需要新的推理能力，其中最主要的是心智理论（ToM）能力，即推理其他代理的“精神”状态的能力。然而，LLM中的心智理论和其他多代理能力尚待理解，因为现有的基准测试存在范围狭窄、数据泄露、饱和和缺乏交互性的问题。因此，我们提出了基于游戏的基准测试Decrypto，用于多代理推理和心智理论，从认知科学、计算语用学和多代理强化学习中汲取灵感。它的设计在所有其他维度上尽可能简单，消除了在其他基准测试中通常存在的混淆因素。据我们所知，它也是设计交互式心智理论实验的第一个平台。我们通过全面评估前沿的大型语言模型、稳健性研究和人机交叉实验来验证基准测试的设计。我们发现大型语言模型的游戏能力落后于人类和简单的词嵌入基线。然后我们在Decrypto中创建了两种经典认知科学实验的变体，以评估三种关键的心智理论能力。令人惊讶的是，我们发现最先进的大型语言模型在这些任务上的表现明显逊色于它们的旧版本模型。这表明Decrypto解决了当前推理和心智理论评估中的关键差距，并为更好的人工智能代理的发展铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20664v1">PDF</a> 41 pages, 19 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在获得代理能力后将需要应对复杂的多代理场景，需要在合作和竞争环境中与人类用户和其他代理进行交互。这要求新的推理能力，其中主要是心理理论（ToM），即推理其他代理的“心理”状态的能力。然而，ToM和LLMs中的其他多代理能力尚未得到充分理解，因为现有基准测试存在范围狭窄、数据泄露、饱和和缺乏交互等问题。因此，我们提出了基于游戏的基准测试Decrypto，用于多代理推理和ToM，灵感来自认知科学、计算语用学和多代理强化学习。它是为了尽可能在其他维度上简化而设计的，消除了其他基准测试中通常存在的混淆因素。据我们所知，它也是设计交互式ToM实验的第一个平台。我们通过全面的实证评估、稳健性研究和人机交叉实验验证了基准测试设计的有效性。我们发现LLM的游戏能力落后于人类和简单的词嵌入基线。然后，我们在Decrypto内创建了两个经典认知科学实验的变种，以评估三种关键的ToM能力。令人惊讶的是，我们发现最先进的推理模型在这些任务上的表现显著不如旧模型。这表明Decrypto解决了当前推理和ToM评估中的关键差距，并为更好的人工智能代理铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在获得代理能力后面临多代理场景的复杂性，需具备新的推理能力，尤其是心理理论（ToM）。</li>
<li>现有基准测试存在缺陷，如范围狭窄、数据泄露和缺乏交互，导致对LLMs中ToM和其他多代理能力的理解不足。</li>
<li>提出了一种新的基于游戏的基准测试Decrypto，用于评估多代理推理和ToM，结合认知科学、计算语用学和强化学习。</li>
<li>Decrypto设计简洁，消除了其他基准测试中的混淆因素，是首个交互式ToM实验平台。</li>
<li>LLM在游戏能力方面落后于人类和词嵌入基线。</li>
<li>通过变种实验发现，最先进的推理模型在特定任务上的表现不如旧模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20664">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2edcf303d1122d08144611c632f2f348.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f3c1ed44efb0cbfb794708947c3304d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4618ab1b41a9c4225ff843cb752c5c28.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Towards-Community-Driven-Agents-for-Machine-Learning-Engineering"><a href="#Towards-Community-Driven-Agents-for-Machine-Learning-Engineering" class="headerlink" title="Towards Community-Driven Agents for Machine Learning Engineering"></a>Towards Community-Driven Agents for Machine Learning Engineering</h2><p><strong>Authors:Sijie Li, Weiwei Sun, Shanda Li, Ameet Talwalkar, Yiming Yang</strong></p>
<p>Large language model-based machine learning (ML) agents have shown great promise in automating ML research. However, existing agents typically operate in isolation on a given research problem, without engaging with the broader research community, where human researchers often gain insights and contribute by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live evaluation framework designed to assess an agent’s ability to communicate with and leverage collective knowledge from a simulated Kaggle research community. Building on this framework, we propose CoMind, a novel agent that excels at exchanging insights and developing novel solutions within a community context. CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2% human competitors on average across four ongoing Kaggle competitions. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/comind-ml/CoMind">https://github.com/comind-ml/CoMind</a>. </p>
<blockquote>
<p>基于大型语言模型的机器学习（ML）代理在自动化ML研究方面显示出巨大的潜力。然而，现有的代理通常在一个给定的研究问题上孤立运行，而没有与更广泛的研究社区接触，而人类研究人员往往通过分享知识获得洞察力并做出贡献。为了弥补这一差距，我们推出了MLE-Live，这是一个旨在评估代理与模拟Kaggle研究社区进行交流并利用集体知识的能力的实时评估框架。在此框架的基础上，我们提出了CoMind，这是一个擅长在社区环境中交流见解并开发新解决方案的新型代理。CoMind在MLE-Live上实现了最先进的性能，并在四个正在进行的Kaggle竞赛中平均击败了79.2%的人类竞争对手。我们的代码已发布在<a target="_blank" rel="noopener" href="https://github.com/comind-ml/CoMind%E3%80%82">https://github.com/comind-ml/CoMind。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20640v1">PDF</a> </p>
<p><strong>Summary</strong><br>自动化机器学习研究方面，大型语言模型驱动的机器学习（ML）代理表现出了巨大的潜力。为解决现有代理在处理特定问题时孤立运行、无法与更广泛的社区进行交流互动的问题，研究者引入了MLE-Live评估框架，旨在评估代理与模拟Kaggle研究社区进行沟通和利用集体知识的能力。基于该框架，我们提出了CoMind这一新型代理，擅长在社区环境中交流见解并开发新颖解决方案。CoMind在MLE-Live上表现卓越，并在四个持续进行的Kaggle竞赛中平均击败了79.2%的人类竞争对手。我们的代码已发布在GitHub上：<a target="_blank" rel="noopener" href="https://github.com/comind-ml/CoMind%E3%80%82">https://github.com/comind-ml/CoMind。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型驱动的机器学习代理在自动化机器学习研究领域展现潜力。</li>
<li>目前存在的代理主要在孤立环境中运行，缺乏与广大研究社区的互动。</li>
<li>MLE-Live评估框架旨在评估代理与模拟Kaggle研究社区交流的能力。</li>
<li>CoMind是一个基于MLE-Live框架构建的新型代理，擅长在社区环境中交流和开发解决方案。</li>
<li>CoMind在模拟的Kaggle竞赛环境中表现卓越，超过了大部分人类参赛者。</li>
<li>CoMind已在GitHub上公开发布，以供研究者和开发者使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20640">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-aa54024434c0ed81ac5ec7aee270d998.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-903826177fc7ccba3c3d21d1685e3579.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b80b5fe056a8daa761053ec93500776e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DiffuCoder-Understanding-and-Improving-Masked-Diffusion-Models-for-Code-Generation"><a href="#DiffuCoder-Understanding-and-Improving-Masked-Diffusion-Models-for-Code-Generation" class="headerlink" title="DiffuCoder: Understanding and Improving Masked Diffusion Models for Code   Generation"></a>DiffuCoder: Understanding and Improving Masked Diffusion Models for Code   Generation</h2><p><strong>Authors:Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, Yizhe Zhang</strong></p>
<p>Diffusion large language models (dLLMs) are compelling alternatives to autoregressive (AR) models because their denoising models operate over the entire sequence. The global planning and iterative refinement features of dLLMs are particularly useful for code generation. However, current training and inference mechanisms for dLLMs in coding are still under-explored. To demystify the decoding behavior of dLLMs and unlock their potential for coding, we systematically investigate their denoising processes and reinforcement learning (RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code. Using this model as a testbed, we analyze its decoding behavior, revealing how it differs from that of AR models: (1) dLLMs can decide how causal their generation should be without relying on semi-AR decoding, and (2) increasing the sampling temperature diversifies not only token choices but also their generation order. This diversity creates a rich search space for RL rollouts. For RL training, to reduce the variance of token log-likelihood estimates and maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel sampling scheme that constructs complementary mask noise for completions used in training. In our experiments, coupled-GRPO significantly improves DiffuCoder’s performance on code generation benchmarks (+4.4% on EvalPlus) and reduces reliance on AR causal during decoding. Our work provides deeper insight into the machinery of dLLM generation and offers an effective, diffusion-native RL training framework. <a target="_blank" rel="noopener" href="https://github.com/apple/ml-diffucoder">https://github.com/apple/ml-diffucoder</a>. </p>
<blockquote>
<p>扩散大型语言模型（dLLMs）是令人信服的自回归（AR）模型的替代方案，因为它们的降噪模型在整个序列上运行。dLLMs的全局规划和迭代优化功能对于代码生成特别有用。然而，目前针对编码中的dLLM的训练和推理机制仍被探索不足。为了揭示dLLMs的解码行为并解锁其在编码方面的潜力，我们系统地研究了它们的降噪过程和强化学习（RL）方法。我们在130B代码标记上训练了一个7B的dLLM，名为<strong>DiffuCoder</strong>。以此模型为测试平台，我们分析了其解码行为，揭示了其与AR模型的不同之处：（1）dLLM能够决定其生成应该如何因果关系，而无需依赖半自回归解码；（2）增加采样温度不仅使标记选择多样化，还使其生成顺序多样化。这种多样性为RL回滚创建了一个丰富的搜索空间。对于RL训练，为了减少标记对数似然估计的方差并保持训练效率，我们提出了<strong>coupled-GRPO</strong>，这是一种新的采样方案，用于构建用于训练中的完成的互补掩码噪声。在我们的实验中，coupled-GRPO显著提高了DiffuCoder在代码生成基准测试上的性能（在EvalPlus上提高了4.4％），并减少了解码过程中AR因果的依赖。我们的工作为dLLM生成机制提供了更深入的了解，并提供了一个有效的、基于扩散的RL训练框架。详情请访问：<a target="_blank" rel="noopener" href="https://github.com/apple/ml-diffucoder">https://github.com/apple/ml-diffucoder</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20639v1">PDF</a> preprint</p>
<p><strong>Summary</strong></p>
<p>本文介绍了扩散大型语言模型（dLLMs）在代码生成方面的优势及其解码行为的研究。通过训练一个规模为7B的dLLM模型DiffuCoder，并对其进行系统分析，发现dLLMs具有全局规划和迭代优化的特点，其解码行为与自回归（AR）模型有所不同。研究还探讨了强化学习（RL）在dLLMs训练中的应用，并提出了一个新的采样方案coupled-GRPO，以提高代码生成性能并减少解码过程中对AR的依赖。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>dLLMs是代码生成领域具有吸引力的替代方案，具有全局规划和迭代优化特点。</li>
<li>dLLMs的解码行为与自回归（AR）模型不同，可以决定生成的因果性，并且增加采样温度可以多样化生成顺序。</li>
<li>强化学习（RL）在dLLMs训练中起着重要作用，可以减少token log-likelihood估计的方差并保持训练效率。</li>
<li>coupled-GRPO是一种新型的采样方案，为dLLMs的训练提供了一种有效的扩散原生RL训练框架。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20639">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a8a5b0b51e74200edd6980f3bfbdb579.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fab6f03b8677a7805ff29cfa9802755.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-894b6e6328ece7c23bc3d00fd9739b61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc7edce831c554d0a6d32308f871a1e8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PLoP-Precise-LoRA-Placement-for-Efficient-Finetuning-of-Large-Models"><a href="#PLoP-Precise-LoRA-Placement-for-Efficient-Finetuning-of-Large-Models" class="headerlink" title="PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models"></a>PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models</h2><p><strong>Authors:Soufiane Hayou, Nikhil Ghosh, Bin Yu</strong></p>
<p>Low-Rank Adaptation (LoRA) is a widely used finetuning method for large models. Its small memory footprint allows practitioners to adapt large models to specific tasks at a fraction of the cost of full finetuning. Different modifications have been proposed to enhance its efficiency by, for example, setting the learning rate, the rank, and the initialization. Another improvement axis is adapter placement strategy: when using LoRA, practitioners usually pick module types to adapt with LoRA, such as Query and Key modules. Few works have studied the problem of adapter placement, with nonconclusive results: original LoRA paper suggested placing adapters in attention modules, while other works suggested placing them in the MLP modules. Through an intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a lightweight method that allows automatic identification of module types where LoRA adapters should be placed, given a pretrained model and a finetuning task. We demonstrate that PLoP consistently outperforms, and in the worst case competes, with commonly used placement strategies through comprehensive experiments on supervised finetuning and reinforcement learning for reasoning. </p>
<blockquote>
<p>低秩适配（LoRA）是一种广泛应用于大型模型的微调方法。它较小的内存占用率使得实践者能够以全量微调的一小部分成本将大型模型适配到特定任务上。为了提升效率，已经提出了各种改良方法，例如设置学习率、等级和初始化等。另一个改进方向是适配器放置策略：在使用LoRA时，实践者通常会选择使用LoRA适配的模块类型，例如查询和键模块。关于适配器放置的问题研究较少，且结果尚无定论：原始LoRA论文建议在注意力模块中放置适配器，而其他研究则建议将它们放置在MLP模块中。通过直观的理论分析，我们引入了PLoP（精确LoRA放置），这是一种轻量级的方法，能够在给定预训练模型和微调任务的情况下自动识别应放置LoRA适配器的模块类型。我们通过大量的实验演示了PLoP在监督微调以及强化学习推理方面的一致优势，在最坏的情况下也能与常用放置策略相竞争。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20629v1">PDF</a> TD,LR: A lightweight module type selection method for LoRA   finetuning. PLoP gives precise placements for LoRA adapters for improved   performance</p>
<p><strong>Summary</strong></p>
<p>LoRA（低秩适应）是一种用于大型模型的微调方法，因其较小的内存占用而备受青睐。为提高效率，人们对其进行了多种改进，如设置学习率、等级和初始化等。最近的研究关注于适配器放置策略，即选择哪些模块类型使用LoRA适配器进行适应。本文通过直观的理论分析，介绍了一种新方法PLoP（精确LoRA放置），可自动识别应放置LoRA适配器的模块类型。实验表明，PLoP在监督微调与强化学习等任务上的表现均优于常规放置策略。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoRA是一种用于大型模型的微调方法，具有较小的内存占用和较低的成本。</li>
<li>LoRA的效率可以通过多种方式进行改进，包括设置学习率、等级和初始化等。</li>
<li>适配器放置策略是LoRA的一个重要改进方向，目前对此的研究尚少且结果非结论性。</li>
<li>原LoRA论文建议将适配器放置在注意力模块中，而其他研究则建议放置在MLP模块中。</li>
<li>PLoP方法能够通过直观的理论分析，自动识别应放置LoRA适配器的模块类型。</li>
<li>PLoP在监督微调与强化学习等任务上的表现均优于常规放置策略。</li>
<li>PLoP的引入为大型模型的微调提供了新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20629">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dfd3a05715d244cceefacbfff4901a79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-222fa9c9af3dc7627bb7b154ae5d8b62.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Video-Perception-Models-for-3D-Scene-Synthesis"><a href="#Video-Perception-Models-for-3D-Scene-Synthesis" class="headerlink" title="Video Perception Models for 3D Scene Synthesis"></a>Video Perception Models for 3D Scene Synthesis</h2><p><strong>Authors:Rui Huang, Guangyao Zhai, Zuria Bauer, Marc Pollefeys, Federico Tombari, Leonidas Guibas, Gao Huang, Francis Engelmann</strong></p>
<p>Traditionally, 3D scene synthesis requires expert knowledge and significant manual effort. Automating this process could greatly benefit fields such as architectural design, robotics simulation, virtual reality, and gaming. Recent approaches to 3D scene synthesis often rely on the commonsense reasoning of large language models (LLMs) or strong visual priors of modern image generation models. However, current LLMs demonstrate limited 3D spatial reasoning ability, which restricts their ability to generate realistic and coherent 3D scenes. Meanwhile, image generation-based methods often suffer from constraints in viewpoint selection and multi-view inconsistencies. In this work, we present Video Perception models for 3D Scene synthesis (VIPScene), a novel framework that exploits the encoded commonsense knowledge of the 3D physical world in video generation models to ensure coherent scene layouts and consistent object placements across views. VIPScene accepts both text and image prompts and seamlessly integrates video generation, feedforward 3D reconstruction, and open-vocabulary perception models to semantically and geometrically analyze each object in a scene. This enables flexible scene synthesis with high realism and structural consistency. For more precise analysis, we further introduce First-Person View Score (FPVScore) for coherence and plausibility evaluation, utilizing continuous first-person perspective to capitalize on the reasoning ability of multimodal large language models. Extensive experiments show that VIPScene significantly outperforms existing methods and generalizes well across diverse scenarios. The code will be released. </p>
<blockquote>
<p>传统上，3D场景合成需要专业知识的大量投入和显著的手动操作。自动化这一过程可以为建筑设计、机器人仿真、虚拟现实和游戏等领域带来巨大的好处。最近的3D场景合成方法常常依赖于大型语言模型的常识推理或现代图像生成模型的强大视觉先验。然而，当前的大型语言模型表现出有限的3D空间推理能力，这限制了它们生成真实和连贯的3D场景的能力。同时，基于图像生成的方法经常受到视点选择和多视图不一致性的约束。在这项工作中，我们提出了用于3D场景合成的Video Perception模型（VIPScene），这是一个利用视频生成模型中的3D物理世界常识知识编码的新框架，以确保跨视图的连贯场景布局和一致的对象放置。VIPScene接受文本和图像提示，无缝集成视频生成、前馈3D重建和开放词汇感知模型，以语义和几何方式分析场景中的每个对象。这实现了具有高度的真实感和结构一致性的灵活场景合成。为了进行更精确的分析，我们还引入了第一人称视角评分（FPVScore）来进行连贯性和可行性评估，利用连续的第一人称视角来利用多模式大型语言模型的推理能力。大量实验表明，VIPScene显著优于现有方法，并在各种场景中具有良好的通用性。代码将很快发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20601v1">PDF</a> </p>
<p><strong>Summary</strong><br>    VIPScene通过结合视频生成模型中的三维物理世界常识知识，实现了连贯的场景布局和跨视图的物体位置一致性。该方法接受文本和图像提示，无缝集成视频生成、前馈三维重建和开放词汇感知模型，实现场景的语义和几何分析。这为具有高度现实感和结构一致性的灵活场景合成提供了新的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VIPScene是一个用于三维场景合成的新型框架，能够利用视频生成模型中的三维物理世界常识知识。</li>
<li>VIPScene可以接受文本和图像提示，并集成了视频生成、前馈三维重建和开放词汇感知模型。</li>
<li>VIPScene能够确保场景连贯性和物体位置的一致性。</li>
<li>VIPScene具备灵活的场景合成能力，具有高度的现实感和结构一致性。</li>
<li>VIPScene引入第一人称视角评分（FPVScore）来评估场景合成的连贯性和可信度。</li>
<li>VIPScene在实验中表现出优异的性能，显著优于现有方法，并且在各种场景下具有良好的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20601">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5fdd369ebe958ca3dee71570684739a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e376b160c3d4577103790f2e6ce08135.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-444ecfe291803ea1ade0e301005fc635.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5236392d5fadd6b1a9c1a6b0b3434d9c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Case-based-Reasoning-Augmented-Large-Language-Model-Framework-for-Decision-Making-in-Realistic-Safety-Critical-Driving-Scenarios"><a href="#Case-based-Reasoning-Augmented-Large-Language-Model-Framework-for-Decision-Making-in-Realistic-Safety-Critical-Driving-Scenarios" class="headerlink" title="Case-based Reasoning Augmented Large Language Model Framework for   Decision Making in Realistic Safety-Critical Driving Scenarios"></a>Case-based Reasoning Augmented Large Language Model Framework for   Decision Making in Realistic Safety-Critical Driving Scenarios</h2><p><strong>Authors:Wenbin Gan, Minh-Son Dao, Koji Zettsu</strong></p>
<p>Driving in safety-critical scenarios requires quick, context-aware decision-making grounded in both situational understanding and experiential reasoning. Large Language Models (LLMs), with their powerful general-purpose reasoning capabilities, offer a promising foundation for such decision-making. However, their direct application to autonomous driving remains limited due to challenges in domain adaptation, contextual grounding, and the lack of experiential knowledge needed to make reliable and interpretable decisions in dynamic, high-risk environments. To address this gap, this paper presents a Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for evasive maneuver decision-making in complex risk scenarios. Our approach integrates semantic scene understanding from dashcam video inputs with the retrieval of relevant past driving cases, enabling LLMs to generate maneuver recommendations that are both context-sensitive and human-aligned. Experiments across multiple open-source LLMs show that our framework improves decision accuracy, justification quality, and alignment with human expert behavior. Risk-aware prompting strategies further enhance performance across diverse risk types, while similarity-based case retrieval consistently outperforms random sampling in guiding in-context learning. Case studies further demonstrate the framework’s robustness in challenging real-world conditions, underscoring its potential as an adaptive and trustworthy decision-support tool for intelligent driving systems. </p>
<blockquote>
<p>在关键的驾驶场景中，驾驶员需要基于情境理解和经验推理的快速、灵活的决策能力。大型语言模型（LLM）以其强大的通用推理能力，为这种决策提供了有前景的基础。然而，将其直接应用于自动驾驶仍存在挑战，特别是在领域适应、上下文定位以及在动态高风险环境中做出可靠和可解释决策所需经验知识的缺乏方面。为了弥补这一差距，本文提出了一种基于案例推理的增强大型语言模型（CBR-LLM）框架，用于复杂风险场景中的规避动作决策。我们的方法结合了从行车记录仪视频输入中理解场景语义与检索相关历史驾驶案例，使LLM能够生成既敏感于上下文又与人类行为一致的机动建议。跨多个开源LLM的实验表明，我们的框架提高了决策准确性、解释质量以及与人类专家行为的契合度。风险感知提示策略进一步增强了各种风险类型的性能，而基于相似性的案例检索在指导上下文学习方面始终优于随机抽样。案例研究进一步证明了该框架在具有挑战性的现实条件下的稳健性，突显了其作为智能驾驶系统的自适应和可信赖的决策支持工具的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20531v1">PDF</a> 12 pages, 10 figures, under-review conference</p>
<p><strong>Summary</strong></p>
<p>基于情境理解和经验推理的快速决策对于安全驾驶至关重要。大型语言模型为此类决策提供了有力支持，但在自动驾驶领域的应用仍面临域适应、上下文定位以及环境经验知识的缺乏等挑战。针对这一问题，本文提出了一种基于案例推理辅助的大型语言模型框架（CBR-LLM），该框架通过驾驶舱视频输入对语义场景进行理解，并检索相关驾驶案例，使语言模型能够生成既符合上下文又符合人类行为的机动建议。实验表明，该框架提高了决策准确性、解释质量和对人类专家行为的匹配度。风险感知提示策略进一步提高了各种风险类型的性能，基于相似性的案例检索在指导上下文学习方面表现优于随机抽样。案例研究证明了该框架在复杂现实条件下的稳健性，可作为智能驾驶系统的自适应和可靠决策支持工具。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在自动驾驶中的决策应用具有潜力，但需解决域适应、上下文理解和环境经验知识缺乏的问题。</li>
<li>CBR-LLM框架结合了语义场景理解与相关驾驶案例检索，提高语言模型在复杂风险场景中的决策能力。</li>
<li>实验显示CBR-LLM框架能提高决策准确性、解释质量，并更贴近人类专家行为。</li>
<li>风险感知提示策略有助于提升在各种风险场景中的性能。</li>
<li>相似性基础上的案例检索在指导语言模型学习方面比随机抽样更有效。</li>
<li>CBR-LLM框架在挑战现实条件下展现出稳健性，能够为智能驾驶系统提供自适应和可靠的决策支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20531">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-99506f51adfb1599392df2d24eed2111.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8808f0e46c899b6c379d1c73736e07df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76b8c4c785e59e0621cf3d253c7c2a38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16039e115844d8790958561292ac8ae5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35f0ba200717aeef6838a1f239680f08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb27286930aa02cfabafdf41d3fe3046.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b42b5d524dfc51a829d87f365534559.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Asymmetric-REINFORCE-for-off-Policy-Reinforcement-Learning-Balancing-positive-and-negative-rewards"><a href="#Asymmetric-REINFORCE-for-off-Policy-Reinforcement-Learning-Balancing-positive-and-negative-rewards" class="headerlink" title="Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing   positive and negative rewards"></a>Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing   positive and negative rewards</h2><p><strong>Authors:Charles Arnal, Gaëtan Narozniak, Vivien Cabannes, Yunhao Tang, Julia Kempe, Remi Munos</strong></p>
<p>Reinforcement learning (RL) is increasingly used to align large language models (LLMs). Off-policy methods offer greater implementation simplicity and data efficiency than on-policy techniques, but often result in suboptimal performance. In this work, we study the intermediate range of algorithms between off-policy RL and supervised fine-tuning by analyzing a simple off-policy REINFORCE algorithm, where the advantage is defined as $A&#x3D;r-V$, with $r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$ emphasizes high-reward samples, while raising it penalizes low-reward ones more heavily. We first provide a theoretical analysis of this off-policy REINFORCE algorithm, showing that when the baseline $V$ lower-bounds the expected reward, the algorithm enjoys a policy improvement guarantee. Our analysis reveals that while on-policy updates can safely leverage both positive and negative signals, off-policy updates benefit from focusing more on positive rewards than on negative ones. We validate our findings experimentally in a controlled stochastic bandit setting and through fine-tuning state-of-the-art LLMs on reasoning tasks. </p>
<blockquote>
<p>强化学习（RL）越来越多地被用于对齐大型语言模型（LLM）。相比on-policy技术，off-policy方法在实现简单性和数据效率方面提供了更大的优势，但通常会导致次优性能。在这项工作中，我们通过分析一个简单的off-policy REINFORCE算法来研究off-policy RL和受监督微调之间的算法中介范围，该算法中的优势定义为A&#x3D;r-V，其中r是奖励，V是可调整的基线。直观地说，降低V会突出高奖励样本，而提高其则会更严厉地惩罚低奖励样本。我们首先对这种off-policy REINFORCE算法进行理论分析，表明当基线V预期奖励的下界时，该算法具有策略改进保证。我们的分析表明，当on-policy更新可以安全地利用正负信号时，off-policy更新更多地受益于关注正向奖励而非负向奖励。我们在受控的随机强盗环境和通过微调最新的LLM进行推理任务实验验证了我们的发现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20520v1">PDF</a> </p>
<p><strong>Summary</strong><br>强化学习（RL）被越来越多地用于对齐大型语言模型（LLM）。本工作研究了介于离线策略强化学习与监督微调之间的算法，通过分析简单的离线策略REINFORCE算法的优势定义，揭示出基线值V的调整对于强化学习性能的影响。理论上分析表明，当基线值V下界预期奖励时，算法具有策略改进保证。分析还发现，相比于负向奖励，离线策略更新更侧重于正向奖励。实验验证了在受控的随机性情境以及微调状态最优的语言模型上，我们的发现均成立。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习用于对齐大型语言模型越来越普遍。</li>
<li>离线策略强化学习相较于在线策略具有实施简单和高效的数据使用效率，但性能可能不如在线策略。</li>
<li>研究集中在离线策略REINFORCE算法的优势定义上，其中优势被定义为奖励r与可调基线值V的差。</li>
<li>基线值V的调整可以影响算法性能，降低V值强调高奖励样本，提高V值则更严厉地惩罚低奖励样本。</li>
<li>理论上分析表明，当基线值V作为预期奖励的下界时，算法具有策略改进保障。</li>
<li>分析发现离线策略更新更侧重于正向奖励，而不是负向奖励。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20520">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-139861d0888a0e03cd190b3794c9577b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ReCode-Updating-Code-API-Knowledge-with-Reinforcement-Learning"><a href="#ReCode-Updating-Code-API-Knowledge-with-Reinforcement-Learning" class="headerlink" title="ReCode: Updating Code API Knowledge with Reinforcement Learning"></a>ReCode: Updating Code API Knowledge with Reinforcement Learning</h2><p><strong>Authors:Haoze Wu, Yunzhi Yao, Wenhao Yu, Huajun Chen, Ningyu Zhang</strong></p>
<p>Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs’ code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs’ general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zjunlp/ReCode">https://github.com/zjunlp/ReCode</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在代码生成方面表现出色，但在适应外部库API的频繁更新时却会陷入困境。这一关键局限性源于对训练数据中过时API知识的依赖，即使有访问当前文档，也阻碍了动态环境中的可靠代码生成。为了解决这个问题，我们提出了ReCode（基于规则的代码更新强化学习）这一新型框架，它模拟程序员对API变化的适应。具体来说，我们构建了大约2000个数据条目数据集，以训练LLM根据更新信息进行版本迁移。然后，我们引入了一种改进的字符串相似性度量作为代码评价的奖励，用于强化学习。我们的实验表明，ReCode在动态API场景中大幅提升了LLM的代码生成性能，特别是在未见过的CodeUpdateArena任务中。关键的是，与监督微调相比，ReCode对LLM的一般代码生成能力的影响较小。我们在各种LLM和强化学习算法（GRPO和DAPO）上应用了ReCode，均实现了持续改进。值得注意的是，经过训练后，Qwen2.5-Coder-7B的表现优于32B参数代码指令调整模型以及相同结构下的推理模型。相关代码可访问<a target="_blank" rel="noopener" href="https://github.com/zjunlp/ReCode%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/zjunlp/ReCode获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20495v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong>：大型语言模型在代码生成方面表现出色，但在适应外部库API的频繁更新时遇到困难。为了解决这一问题，提出了一种名为ReCode的新框架，该框架通过强化学习模拟程序员对API变化的适应。实验表明，ReCode显著提高了大型语言模型在动态API场景下的代码生成性能，特别是在未见过的CodeUpdateArena任务上。此外，ReCode对大型语言模型的常规代码生成能力影响较小。不同的大型语言模型和强化学习算法在ReCode应用后均实现了一致性的改进。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型在代码生成方面具有出色能力，但在动态API更新适应方面存在局限性。</li>
<li>ReCode框架通过结合规则基础和强化学习来解决这一问题。</li>
<li>ReCode使用自定义数据集训练大型语言模型以进行版本迁移。</li>
<li>引入修改后的字符串相似性度量作为强化学习的奖励函数，用于代码评价。</li>
<li>实验表明，ReCode显著提高了大型语言模型在动态API场景下的代码生成性能。</li>
<li>ReCode对大型语言模型的常规代码生成能力影响较小。</li>
<li>ReCode可应用于不同的大型语言模型和强化学习算法，实现一致性的改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20495">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dd9e66065bbf8428e33d2da7113c54df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5cceb681fede92d1147e826604bf89b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-07ace76844054edeab6cde0a4c32eb44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-392cd8b6e733cdb84e634470f036f74b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d21925700dbf40a39c62cdafac483c65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5c727c232a9c7e176f040eee15e49e1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="An-Agentic-System-for-Rare-Disease-Diagnosis-with-Traceable-Reasoning"><a href="#An-Agentic-System-for-Rare-Disease-Diagnosis-with-Traceable-Reasoning" class="headerlink" title="An Agentic System for Rare Disease Diagnosis with Traceable Reasoning"></a>An Agentic System for Rare Disease Diagnosis with Traceable Reasoning</h2><p><strong>Authors:Weike Zhao, Chaoyi Wu, Yanjie Fan, Xiaoman Zhang, Pengcheng Qiu, Yuze Sun, Xiao Zhou, Yanfeng Wang, Ya Zhang, Yongguo Yu, Kun Sun, Weidi Xie</strong></p>
<p>Rare diseases collectively affect over 300 million individuals worldwide, yet timely and accurate diagnosis remains a pervasive challenge. This is largely due to their clinical heterogeneity, low individual prevalence, and the limited familiarity most clinicians have with rare conditions. Here, we introduce DeepRare, the first rare disease diagnosis agentic system powered by a large language model (LLM), capable of processing heterogeneous clinical inputs. The system generates ranked diagnostic hypotheses for rare diseases, each accompanied by a transparent chain of reasoning that links intermediate analytic steps to verifiable medical evidence.   DeepRare comprises three key components: a central host with a long-term memory module; specialized agent servers responsible for domain-specific analytical tasks integrating over 40 specialized tools and web-scale, up-to-date medical knowledge sources, ensuring access to the most current clinical information. This modular and scalable design enables complex diagnostic reasoning while maintaining traceability and adaptability. We evaluate DeepRare on eight datasets. The system demonstrates exceptional diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013 diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15 methods, like traditional bioinformatics diagnostic tools, LLMs, and other agentic systems, achieving an average Recall@1 score of 57.18% and surpassing the second-best method (Reasoning LLM) by a substantial margin of 23.79 percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at Recall@1 compared to Exomiser’s 53.20% in 109 cases. Manual verification of reasoning chains by clinical experts achieves 95.40% agreements. Furthermore, the DeepRare system has been implemented as a user-friendly web application <a target="_blank" rel="noopener" href="http://raredx.cn/doctor">http://raredx.cn/doctor</a>. </p>
<blockquote>
<p>罕见疾病在全球范围内共同影响着超过3亿人，但及时准确的诊断仍然是一个普遍存在的挑战。这主要是由于其临床异质性、个人发病率低以及大多数临床医生对罕见疾病的不熟悉。在这里，我们推出DeepRare，这是一个由大型语言模型（LLM）驱动的首个罕见疾病诊断智能系统，能够处理各种临床输入。该系统为罕见疾病生成排名诊断假设，每个假设都伴有透明的推理链，将中间分析步骤与可验证的医学证据联系起来。DeepRare包含三个关键组件：一个带有长期记忆模块的中心主机；负责特定领域分析任务的专用代理服务器，集成超过40种专业工具和最新网络规模医学知识源，确保访问最新临床信息。这种模块化且可扩展的设计在保持可追溯性和适应性的同时，实现了复杂的诊断推理。我们在八个数据集上评估了DeepRare。该系统在2919种疾病中表现出卓越的诊断性能，对1013种疾病的准确性达到100%。在基于HPO的评估中，DeepRare显著优于其他15种方法，如传统生物信息学诊断工具、大型语言模型和其他的智能系统，平均Recall@1分数达到57.18%，并大幅度超越排名第二的方法（推理大型语言模型）。对于多模式输入场景，DeepRare在Recall@1达到70.60%，相比之下Exomiser为53.20%，在109个案例中。临床专家对推理链的手动验证达到95.40%的共识。此外，DeepRare系统已作为一个用户友好的网页应用程序实现，网址为：<a target="_blank" rel="noopener" href="http://raredx.cn/doctor%E3%80%82">http://raredx.cn/doctor。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20430v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>深罕病诊断系统DeepRare问世，借助大型语言模型（LLM）处理多样化临床输入，为罕见疾病生成排名诊断假设。该系统伴有透明化推理链，链接中间分析步骤与可验证医学证据。DeepRare包括三个关键组件：中央主机、专业代理服务器及医学知识源。其在八个数据集上的表现优异，在2919种疾病中准确率高达百分之百。相比其他工具，DeepRare显著提高诊断性能，平均Recall@1分数为57.18%。多模态输入场景下性能更佳，且已获得临床专家验证。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>DeepRare是首个借助大型语言模型（LLM）进行罕见疾病诊断的系统。</li>
<li>可处理多样化的临床输入，生成排名诊断假设，并附有透明化推理链。</li>
<li>包括中央主机、专业代理服务器等三个关键组件，整合超过40种专业工具和最新医学知识源。</li>
<li>在多个数据集上表现优异，罕见疾病诊断准确率极高。</li>
<li>对比其他方法，DeepRare在诊断性能上显著提高，平均Recall@1分数领先。</li>
<li>多模态输入场景下性能优越，与Exomiser相比有较高Recall@1分数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20430">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-195b057b5a3938bd8f4059156eac4058.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b57c6ae52801ac6938956e93d7a5983.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50676bf2597ba2e2fc6783543b76bebd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Mobile-R1-Towards-Interactive-Reinforcement-Learning-for-VLM-Based-Mobile-Agent-via-Task-Level-Rewards"><a href="#Mobile-R1-Towards-Interactive-Reinforcement-Learning-for-VLM-Based-Mobile-Agent-via-Task-Level-Rewards" class="headerlink" title="Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based   Mobile Agent via Task-Level Rewards"></a>Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based   Mobile Agent via Task-Level Rewards</h2><p><strong>Authors:Jihao Gu, Qihang Ai, Yingyao Wang, Pi Bu, Jingxuan Xing, Zekun Zhu, Wei Jiang, Ziming Wang, Yingxiu Zhao, Ming-Liang Zhang, Jun Song, Yuning Jiang, Bo Zheng</strong></p>
<p>Vision-language model-based mobile agents have gained the ability to not only understand complex instructions and mobile screenshots, but also optimize their action outputs via thinking and reasoning, benefiting from reinforcement learning, such as Group Relative Policy Optimization (GRPO). However, existing research centers on offline reinforcement learning training or online optimization using action-level rewards, which limits the agent’s dynamic interaction with the environment. This often results in agents settling into local optima, thereby weakening their ability for exploration and error action correction. To address these challenges, we introduce an approach called Mobile-R1, which employs interactive multi-turn reinforcement learning with task-level rewards for mobile agents. Our training framework consists of three stages: initial format finetuning, single-step online training via action-level reward, followed by online training via task-level reward based on multi-turn trajectories. This strategy is designed to enhance the exploration and error correction capabilities of Mobile-R1, leading to significant performance improvements. Moreover, we have collected a dataset covering 28 Chinese applications with 24,521 high-quality manual annotations and established a new benchmark with 500 trajectories. We will open source all resources, including the dataset, benchmark, model weight, and codes: <a target="_blank" rel="noopener" href="https://mobile-r1.github.io/Mobile-R1/">https://mobile-r1.github.io/Mobile-R1/</a>. </p>
<blockquote>
<p>基于视觉语言模型的移动代理不仅获得了理解复杂指令和移动截图的能力，还通过强化学习（如群体相对策略优化（GRPO））优化了其行动输出。然而，现有研究主要关注离线强化学习训练或基于行动层面的奖励进行在线优化，这限制了代理与环境的动态交互。这通常导致代理陷入局部最优，从而削弱了它们的探索能力和错误行动纠正能力。为了解决这些挑战，我们引入了一种名为Mobile-R1的方法，该方法采用交互式多回合强化学习，为移动代理提供任务级奖励。我们的训练框架包括三个阶段：初始格式微调、通过行动级奖励进行单步在线训练，然后是基于多回合轨迹的任务级奖励在线训练。该策略旨在增强Mobile-R1的探索和错误纠正能力，从而带来显著的性能改进。此外，我们收集了一个包含28个中文应用程序的数据集，包含24521个高质量的手动注释，并建立了包含500个轨迹的新基准。我们将公开所有资源，包括数据集、基准、模型权重和代码：<a target="_blank" rel="noopener" href="https://mobile-r1.github.io/Mobile-R1/">https://mobile-r1.github.io/Mobile-R1/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20332v1">PDF</a> 14 pages, 12 figures</p>
<p><strong>Summary</strong><br>移动智能代理通过视觉语言模型结合强化学习技术，实现了对复杂指令和移动截图的理解，并优化行动输出。然而，现有研究集中在离线强化学习训练或行动级别的在线优化上，限制了智能代理与环境的动态交互能力。为解决这一问题，我们提出Mobile-R1方法，采用多任务级别奖励的互动多回合强化学习模式进行训练。训练框架包含三个阶段：初始格式微调、通过行动级别奖励进行单步在线训练，以及基于多回合轨迹的任务级别奖励在线训练。此方法旨在提高Mobile-R1的探索和错误修正能力，实现显著的性能提升。同时，我们收集了涵盖28个中文应用程序的标注数据集并建立新基准。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>移动智能代理能够通过视觉语言模型理解复杂指令和移动截图。</li>
<li>强化学习被用于优化移动智能代理的行动输出。</li>
<li>现有研究集中在离线强化学习训练或行动级别奖励的在线优化上，存在局限性。</li>
<li>Mobile-R1方法采用多任务级别奖励的互动多回合强化学习模式训练移动智能代理。</li>
<li>Mobile-R1的训练框架包括初始格式微调、单步在线训练和基于多回合轨迹的在线训练三个阶段。</li>
<li>Mobile-R1旨在提高探索和错误修正能力，实现性能提升。</li>
<li>建立了一个新的基准并收集了一个涵盖多个中文应用程序的标注数据集。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20332">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-352c66259c45d2509b27f1a88266eb6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4f11634f38e1adf173487f146e8d128.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac037de80d310a1b4d5b440557fe2948.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f48800b37ff69204ffefe5d608ce8de7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f462316ef48c2506828a89724016e12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d24431eea05581b90288ae4c006193c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Seeing-is-Believing-Mitigating-OCR-Hallucinations-in-Multimodal-Large-Language-Models"><a href="#Seeing-is-Believing-Mitigating-OCR-Hallucinations-in-Multimodal-Large-Language-Models" class="headerlink" title="Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large   Language Models"></a>Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large   Language Models</h2><p><strong>Authors:Zhentao He, Can Zhang, Ziheng Wu, Zhenghao Chen, Yufei Zhan, Yifan Li, Zhao Zhang, Xian Wang, Minghui Qiu</strong></p>
<p>Recent advancements in multimodal large language models have enhanced document understanding by integrating textual and visual information. However, existing models exhibit incompleteness within their paradigm in real-world scenarios, particularly under visual degradation. In such conditions, the current response paradigm often fails to adequately perceive visual degradation and ambiguity, leading to overreliance on linguistic priors or misaligned visual-textual reasoning. This difficulty in recognizing uncertainty frequently results in the generation of hallucinatory content, especially when a precise answer is not feasible. To better demonstrate and analyze this phenomenon and problem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR hallucination in degraded document understanding. This dataset includes test samples spanning identity cards and invoices, with simulated real-world degradations for OCR reliability. This setup allows for evaluating models’ capacity, under degraded input, to distinguish reliable visual information and answer accordingly, thereby highlighting the challenge of avoiding hallucination on uncertain data. To achieve vision-faithful reasoning and thereby avoid the aforementioned issues, we further introduce a GRPO-based framework featuring a novel reward mechanism. By incorporating a self-awareness of visual uncertainty and an analysis method that initiates refusal to answer to increase task difficulty within our supervised fine-tuning and reinforcement learning framework, we successfully mitigated hallucinations in ambiguous regions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model achieves a 22% absolute improvement in hallucination-free accuracy over GPT-4o on KIE-HVQA and there is no significant performance drop in standard tasks, highlighting both effectiveness and robustness. </p>
<blockquote>
<p>近年来，多模态大型语言模型的进步通过整合文本和视觉信息增强了文档理解。然而，在真实场景中，现有模型在其范式内表现出不完整性，特别是在视觉退化的情况下。在这种情况下，当前响应范式通常无法充分感知视觉退化和模糊，导致过度依赖语言先验或视觉文本推理失误。难以识别不确定性经常导致产生幻觉内容，尤其是在无法给出精确答案的情况下。为了更好地演示和分析这种现象和问题，我们提出了KIE-HVQA，这是第一个专门用于评估退化文档理解中OCR幻觉的基准测试。该数据集包含身份证和发票等测试样本，针对OCR可靠性模拟了真实世界的退化情况。此设置可以评估模型在退化输入下的能力，区分可靠视觉信息并相应地回答，从而突出避免在不确定数据上产生幻觉的挑战。为了实现忠于视觉的推理，避免上述问题，我们进一步引入了基于GRPO的框架和新型奖励机制。通过融入对视觉不确定性的自我意识以及一种分析方法拒绝回答来增加任务难度，这涵盖在我们的监督微调中以及强化学习框架内，我们成功减轻了模糊区域的幻觉现象。在Qwen2.5-VL上的实验表明，我们的7B参数模型在KIE-HVQA上相对于GPT-4o实现了无幻觉准确度的绝对提升22%，并且在标准任务中没有显著的性能下降，这突显了有效性和稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20168v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了多模态大型语言模型在文档理解方面的最新进展，这些模型通过整合文本和视觉信息提高了文档理解的能力。然而，现有模型在真实场景下面临不完全适应的问题，特别是在视觉退化的情况下。针对这一问题，本文提出了KIE-HVQA基准测试集，用于评估OCR幻觉在退化文档理解中的表现。同时，还介绍了一种基于GRPO的框架，通过引入新的奖励机制和自我感知视觉不确定性的分析方法来减少模糊区域的幻觉。实验结果表明，该框架在KIE-HVQA上取得了显著成效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型通过整合文本和视觉信息提高了文档理解。</li>
<li>现有模型在视觉退化情况下存在不足，如过度依赖语言先验或视觉文本推理失调。</li>
<li>KIE-HVQA基准测试集用于评估OCR幻觉在退化文档理解中的表现。</li>
<li>提出的GRPO框架通过引入新的奖励机制和自我感知视觉不确定性的分析方法，成功减少了模糊区域的幻觉。</li>
<li>实验结果表明，该框架在KIE-HVQA上较GPT-4o提高了22%的无幻觉准确率。</li>
<li>该框架在标准任务上的性能没有明显下降，证明了其有效性和稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20168">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c9cec61b212aeafef0925453f6dd78df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42783b689f19c305a9e88907bef0ea07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5940859b179e53617fc6052a0d1bd295.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ba1f95ec09f43d0cd611edc85d2e0c4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MIRAGE-A-Benchmark-for-Multimodal-Information-Seeking-and-Reasoning-in-Agricultural-Expert-Guided-Conversations"><a href="#MIRAGE-A-Benchmark-for-Multimodal-Information-Seeking-and-Reasoning-in-Agricultural-Expert-Guided-Conversations" class="headerlink" title="MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in   Agricultural Expert-Guided Conversations"></a>MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in   Agricultural Expert-Guided Conversations</h2><p><strong>Authors:Vardhan Dongre, Chi Gui, Shubham Garg, Hooshang Nayyeri, Gokhan Tur, Dilek Hakkani-Tür, Vikram S. Adve</strong></p>
<p>We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning and decision-making in consultative interaction settings. Designed for the agriculture domain, MIRAGE captures the full complexity of expert consultations by combining natural user queries, expert-authored responses, and image-based context, offering a high-fidelity benchmark for evaluating models on grounded reasoning, clarification strategies, and long-form generation in a real-world, knowledge-intensive domain. Grounded in over 35,000 real user-expert interactions and curated through a carefully designed multi-step pipeline, MIRAGE spans diverse crop health, pest diagnosis, and crop management scenarios. The benchmark includes more than 7,000 unique biological entities, covering plant species, pests, and diseases, making it one of the most taxonomically diverse benchmarks available for vision-language models, grounded in the real world. Unlike existing benchmarks that rely on well-specified user inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich scenarios with open-world settings, requiring models to infer latent knowledge gaps, handle rare entities, and either proactively guide the interaction or respond. Project Page: <a target="_blank" rel="noopener" href="https://mirage-benchmark.github.io/">https://mirage-benchmark.github.io</a> </p>
<blockquote>
<p>我们推出MIRAGE，这是一个新的多模态专家级推理和决策基准，适用于咨询交互环境中的专家级推理和决策。MIRAGE专为农业领域设计，通过结合自然用户查询、专家撰写的回应和图像上下文，捕捉专家咨询的全过程复杂性，为评估模型在现实世界的知识密集型领域中的基于事实推理、澄清策略和长文本生成能力提供高保真基准。MIRAGE以超过3万5千次的真实用户与专家互动为基础，经过精心设计的多步骤流程进行筛选，涵盖了作物健康、病虫害诊断及作物管理等多种场景。该基准包含超过7千个独特的生物实体，涵盖植物种类、害虫和疾病，成为视觉语言模型中最具现实世界特征的、分类学上最多元化的基准之一。与依赖明确用户输入和封闭分类系统的现有基准不同，MIRAGE的特点是场景丰富且上下文不明确，要求模型能够推断潜在的知识空白，处理罕见实体，并能够主动引导交互或作出回应。项目页面：<a target="_blank" rel="noopener" href="https://mirage-benchmark.github.io/">https://mirage-benchmark.github.io</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20100v1">PDF</a> 66 pages, 32 figures, 23 tables</p>
<p><strong>Summary</strong></p>
<p>MIRAGE是一个为农业领域设计的新基准测试，用于评估多模式专家级推理和咨询交互设置中的决策能力。它结合了自然用户查询、专家撰写的回应和图像上下文，提供了一个高保真度的基准测试，用于评估模型在现实世界的复杂场景中的推理能力。该项目跨越多种作物健康、病虫害诊断和作物管理场景，并包括超过7000个独特的生物实体。其特点是情景丰富且开放世界设置，要求模型推断潜在知识空白、处理罕见实体以及主动引导交互或作出反应。更多详情可见项目页面。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MIRAGE是一个新的基准测试，专注于农业领域的多模式专家级推理和决策能力。</li>
<li>该基准测试结合了自然用户查询、专家响应和图像上下文，模拟真实的专家咨询场景。</li>
<li>MIRAGE提供了高保真度的评估环境，用于测试模型在现实世界场景中的推理能力，包括作物健康、病虫害诊断和作物管理等方面。</li>
<li>该基准测试包含超过7000个独特的生物实体，涵盖植物种类、害虫和疾病，是现有视觉语言模型中最为多样化的基准测试之一。</li>
<li>MIRAGE的特点是情景丰富且开放世界设置，要求模型具备处理复杂环境和未知实体的能力。</li>
<li>模型需要能够推断潜在知识空白，处理罕见实体，并能够在交互中主动引导和作出反应。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20100">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dcd4d53884b4dedb9ca023bf40175f13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e37f3333f8736aba23fdd24c7bfc9e54.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="A-Modular-Multitask-Reasoning-Framework-Integrating-Spatio-temporal-Models-and-LLMs"><a href="#A-Modular-Multitask-Reasoning-Framework-Integrating-Spatio-temporal-Models-and-LLMs" class="headerlink" title="A Modular Multitask Reasoning Framework Integrating Spatio-temporal   Models and LLMs"></a>A Modular Multitask Reasoning Framework Integrating Spatio-temporal   Models and LLMs</h2><p><strong>Authors:Kethmi Hirushini Hettige, Jiahao Ji, Cheng Long, Shili Xiang, Gao Cong, Jingyuan Wang</strong></p>
<p>Spatio-temporal data mining plays a pivotal role in informed decision making across diverse domains. However, existing models are often restricted to narrow tasks, lacking the capacity for multi-task inference and complex long-form reasoning that require generation of in-depth, explanatory outputs. These limitations restrict their applicability to real-world, multi-faceted decision scenarios. In this work, we introduce STReason, a novel framework that integrates the reasoning strengths of large language models (LLMs) with the analytical capabilities of spatio-temporal models for multi-task inference and execution. Without requiring task-specific finetuning, STReason leverages in-context learning to decompose complex natural language queries into modular, interpretable programs, which are then systematically executed to generate both solutions and detailed rationales. To facilitate rigorous evaluation, we construct a new benchmark dataset and propose a unified evaluation framework with metrics specifically designed for long-form spatio-temporal reasoning. Experimental results show that STReason significantly outperforms advanced LLM baselines across all metrics, particularly excelling in complex, reasoning-intensive spatio-temporal scenarios. Human evaluations further validate STReason’s credibility and practical utility, demonstrating its potential to reduce expert workload and broaden the applicability to real-world spatio-temporal tasks. We believe STReason provides a promising direction for developing more capable and generalizable spatio-temporal reasoning systems. </p>
<blockquote>
<p>时空数据挖掘在多个领域中的决策制定中起着至关重要的作用。然而，现有模型通常仅限于狭窄的任务，缺乏多任务推理和复杂长形式推理的能力，无法生成深入的、解释性的输出。这些限制限制了它们在现实世界中多面决策场景的应用。在这项工作中，我们引入了STReason，这是一个新型框架，它将大型语言模型的推理能力与时空模型的分析能力相结合，用于多任务推理和执行。STReason不需要针对特定任务进行微调，它利用上下文学习将复杂的自然语言查询分解为模块化、可解释的程序，然后系统地执行这些程序以生成解决方案和详细的理由。为了促进严格评估，我们构建了一个新的基准数据集，并提出了一个统一的评估框架，其中设计了专门针对长形式时空推理的指标。实验结果表明，STReason在所有指标上都显著优于先进的大型语言模型基线，特别是在复杂、推理密集型的时空场景中表现尤为出色。人类评估进一步验证了STReason的可信度和实用性，表明其减少专家工作量并扩大在现实世界时空任务中的应用潜力。我们相信STReason为开发更具能力和通用性的时空推理系统提供了有前景的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20073v1">PDF</a> </p>
<p><strong>Summary</strong><br>时空数据挖掘在多个领域中的决策制定中发挥着至关重要的作用。然而，现有模型通常局限于狭窄的任务，缺乏进行多任务推理和复杂长形式推理的能力，这要求生成深入的、解释性的输出。为了克服这些限制，本研究引入了STReason框架，该框架结合了大型语言模型的推理能力和时空模型的分析能力，用于多任务推理和执行。STReason利用上下文学习，将复杂的自然语言查询分解为模块化、可解释的程序，然后系统地执行以生成解决方案和详细的理由。实验结果表明，STReason在所有指标上都显著优于先进的大型语言模型基准测试，特别是在复杂、推理密集型的时空场景中表现尤为出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时空数据挖掘在决策制定中起关键作用，但现有模型在复杂任务中表现有限。</li>
<li>STReason框架结合了大型语言模型和时空模型的能力，支持多任务推理和执行。</li>
<li>STReason利用上下文学习分解自然语言查询，并系统地执行以生成解释性输出。</li>
<li>STReason在复杂、推理密集型的时空场景中表现优异。</li>
<li>新构建的基准数据集和统一评估框架为长形式时空推理提供了严谨的评价方法。</li>
<li>实验结果和人类评估验证了STReason的可靠性和实用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20073">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-52923f82e35618993d2f3640beda2c94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5879953c48701d0f0acaaf03055a4dc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-664a3347dfb74d287f8d75db4f064c1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f190b14b703b2146f3551f9e69bb4bdd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="QHackBench-Benchmarking-Large-Language-Models-for-Quantum-Code-Generation-Using-PennyLane-Hackathon-Challenges"><a href="#QHackBench-Benchmarking-Large-Language-Models-for-Quantum-Code-Generation-Using-PennyLane-Hackathon-Challenges" class="headerlink" title="QHackBench: Benchmarking Large Language Models for Quantum Code   Generation Using PennyLane Hackathon Challenges"></a>QHackBench: Benchmarking Large Language Models for Quantum Code   Generation Using PennyLane Hackathon Challenges</h2><p><strong>Authors:Abdul Basit, Minghao Shao, Haider Asif, Nouhaila Innan, Muhammad Kashif, Alberto Marchisio, Muhammad Shafique</strong></p>
<p>Recent advances in Large Language Models (LLMs) have demonstrated strong potential in code generation, yet their effectiveness in quantum computing remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum code generation using real-world challenges from the Quantum Hackathon (QHack). We introduce QHackBench, a novel benchmark dataset derived from QHack competitions, and evaluate model performance under vanilla prompting and Retrieval-Augmented Generation (RAG). Our structured evaluation framework assesses functional correctness, syntactic validity, and execution success across varying challenge difficulties. Results indicate that RAG-enhanced models, supplemented with an augmented PennyLane dataset, approximately generate similar results as the standard prompting, particularly in complex quantum algorithms. Additionally, we introduce a multi-agent evaluation pipeline that iteratively refines incorrect solutions, further enhancing execution success rates. To foster further research, we commit to publicly releasing QHackBench, along with our evaluation framework and experimental results, enabling continued advancements in AI-assisted quantum programming. </p>
<blockquote>
<p>近年来，大型语言模型（LLM）在代码生成方面展现出了强大的潜力，然而它们在量子计算中的应用仍然缺乏足够的探索。本文使用来自量子黑客马拉松（QHack）的现实挑战，对基于PennyLane的量子代码生成的LLM性能进行了基准测试。我们介绍了QHackBench，这是一个从QHack竞赛中衍生出来的新型基准数据集，并评估了标准提示和检索增强生成（RAG）下的模型性能。我们的结构化评估框架评估了不同挑战难度下的功能正确性、语法有效性和执行成功率。结果表明，通过增强PennyLane数据集增强的RAG模型在复杂量子算法方面大约生成了与标准提示相似的结果。此外，我们还引入了一个多代理评估管道，该管道可以迭代优化错误解决方案，进一步提高执行成功率。为了促进进一步研究，我们将公开发布QHackBench，以及我们的评估框架和实验结果，以促进人工智能辅助量子编程的持续发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20008v1">PDF</a> 8 pages, 6 figures, 3 tables, submitted to QAI 2025</p>
<p><strong>Summary</strong></p>
<p>近期大型语言模型（LLM）在代码生成方面展现出强大潜力，但在量子计算领域的应用尚待探索。本文使用来自量子黑客松（QHack）的现实挑战，对PennyLane基础的量子代码生成进行LLM基准测试。引入QHackBench基准数据集，评估了标准提示和检索增强生成（RAG）下的模型性能。通过结构化的评估框架对功能正确性、语法有效性及执行成功率在不同难度的挑战上进行了评估。结果表明，在扩充的PennyLane数据集支持下，使用RAG增强的模型能够在复杂量子算法中生成与标准提示相似的结果。此外，还引入了一种多智能体评估流程，可迭代优化错误解决方案，进一步提高执行成功率。为推进进一步研究，公开发布QHackBench基准数据集、评估框架及实验结果，推动AI辅助量子编程的持续发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在量子代码生成方面潜力巨大，但仍待探索。</li>
<li>QHackBench基准数据集用于PennyLane基础的量子代码生成的LLM基准测试。</li>
<li>模型性能通过标准提示和检索增强生成（RAG）进行评估。</li>
<li>模型在功能正确性、语法有效性及执行成功率上表现良好，尤其在复杂量子算法中。</li>
<li>RAG增强模型在扩充数据集支持下可生成与标准提示相似结果。</li>
<li>引入多智能体评估流程优化错误解决方案，提高执行成功率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20008">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9af4e20b705a584cbef76d26a28b5af8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9be187eb8f2f9108c153a59cb4088a61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b940c6e17f14f98a747b4921b8c8b69.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b61bd4dab0ca0d4c1ab74ec603a1a9a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d05a9af53738092e30ce68d89dad78f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac56b866b18ae1c46dfdeea1b3363175.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21176d9e9c7d923da0e87f62d5e3ef11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ab1d7858905909146c93dc9efbaac26.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Thought-Anchors-Which-LLM-Reasoning-Steps-Matter"><a href="#Thought-Anchors-Which-LLM-Reasoning-Steps-Matter" class="headerlink" title="Thought Anchors: Which LLM Reasoning Steps Matter?"></a>Thought Anchors: Which LLM Reasoning Steps Matter?</h2><p><strong>Authors:Paul C. Bogdan, Uzay Macar, Neel Nanda, Arthur Conmy</strong></p>
<p>Reasoning large language models have recently achieved state-of-the-art performance in many fields. However, their long-form chain-of-thought reasoning creates interpretability challenges as each generated token depends on all previous ones, making the computation harder to decompose. We argue that analyzing reasoning traces at the sentence level is a promising approach to understanding reasoning processes. We present three complementary attribution methods: (1) a black-box method measuring each sentence’s counterfactual importance by comparing final answers across 100 rollouts conditioned on the model generating that sentence or one with a different meaning; (2) a white-box method of aggregating attention patterns between pairs of sentences, which identified “broadcasting” sentences that receive disproportionate attention from all future sentences via “receiver” attention heads; (3) a causal attribution method measuring logical connections between sentences by suppressing attention toward one sentence and measuring the effect on each future sentence’s tokens. Each method provides evidence for the existence of thought anchors, reasoning steps that have outsized importance and that disproportionately influence the subsequent reasoning process. These thought anchors are typically planning or backtracking sentences. We provide an open-source tool (<a target="_blank" rel="noopener" href="http://www.thought-anchors.com/">www.thought-anchors.com</a>) for visualizing the outputs of our methods, and present a case study showing converging patterns across methods that map how a model performs multi-step reasoning. The consistency across methods demonstrates the potential of sentence-level analysis for a deeper understanding of reasoning models. </p>
<blockquote>
<p>推理大型语言模型已在许多领域取得了最先进的性能。然而，它们的长形式思维链推理带来了可解释性挑战，因为每个生成的令牌都依赖于所有先前的令牌，使得计算更难以分解。我们认为，在句子级别分析推理痕迹是理解推理过程的一种有前途的方法。我们提出了三种互补的归因方法：（1）一种黑盒方法，通过比较模型生成该句子或具有不同含义的句子的100次滚动结果中的最终答案，来衡量每个句子的反事实重要性；（2）一种白盒方法，聚合句子对之间的注意力模式，该方法确定了通过“接收者”注意力头从所有未来句子接收不成比例的注意力的“广播”句子；（3）一种因果归因方法，通过抑制对一个句子的注意力并测量对每一个未来句子的影响来衡量句子之间的逻辑联系。每种方法都证明存在思维锚点，即具有重大影响的推理步骤，这些步骤会不成比例地影响随后的推理过程。这些思维锚点通常是规划或回溯句子。我们提供了一个开源工具（<a target="_blank" rel="noopener" href="http://www.thought-anchors.com),用于可视化我们方法的输出,并展示了一个案例研究,展示各方法之间收敛的模式,这些模式可以映射模型如何进行多步推理.各方法之间的一致性表明句子级分析在深入理解推理模型方面具有潜力./">www.thought-anchors.com），用于可视化我们方法的输出，并展示了一个案例研究，展示各方法之间收敛的模式，这些模式可以映射模型如何进行多步推理。各方法之间的一致性表明句子级分析在深入理解推理模型方面具有潜力。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19143v2">PDF</a> Paul C. Bogdan and Uzay Macar contributed equally to this work, and   their listed order was determined by coinflip. Neel Nanda and Arthur Conmy   contributed equally to this work as senior authors, and their listed order   was determined by coinflip</p>
<p><strong>Summary</strong></p>
<p>大型语言模型在多个领域取得了最先进的性能，但其长形式思维链推理带来的解释性挑战不容忽视。每个生成的标记都依赖于之前的所有标记，这使得计算难以分解。本文提出了一种有前景的方法来理解推理过程，即在句子层面分析推理痕迹。文章提出了三种互补归因方法，每种方法均支持句子在推理过程中的重要性，即思维锚的存在。本文还提供了一个开源工具，用于可视化这些方法的结果，并通过案例研究展示了这些方法如何映射模型的多步推理过程。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型在多个领域表现出卓越性能，但存在解释性挑战。</li>
<li>句子层面的分析是一种理解推理过程的有前途的方法。</li>
<li>三种归因方法用于分析句子在推理中的重要性，即思维锚的存在。这些方法包括：比较句子生成的最终答案的黑箱方法；聚合句子间注意力模式的白箱方法；以及衡量句子间逻辑连接的因果归因方法。</li>
<li>开源工具可用于可视化归因方法的结果。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19143">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c9bf6130754c6d512daed283a9d9e976.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bdea3335500968ce818f6fba1d902529.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a61ead1e975ced32a761c1c5d0a204c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9c154753648bca28752c395685217b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94b85e1c2bf3e627443ff6e9f2a827be.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Confucius3-Math-A-Lightweight-High-Performance-Reasoning-LLM-for-Chinese-K-12-Mathematics-Learning"><a href="#Confucius3-Math-A-Lightweight-High-Performance-Reasoning-LLM-for-Chinese-K-12-Mathematics-Learning" class="headerlink" title="Confucius3-Math: A Lightweight High-Performance Reasoning LLM for   Chinese K-12 Mathematics Learning"></a>Confucius3-Math: A Lightweight High-Performance Reasoning LLM for   Chinese K-12 Mathematics Learning</h2><p><strong>Authors:Lixin Wu, Na Cai, Qiao Cheng, Jiachen Wang, Yitao Duan</strong></p>
<p>We introduce Confucius3-Math, an open-source large language model with 14B parameters that (1) runs efficiently on a single consumer-grade GPU; (2) achieves SOTA performances on a range of mathematical reasoning tasks, outperforming many models with significantly larger sizes. In particular, as part of our mission to enhancing education and knowledge dissemination with AI, Confucius3-Math is specifically committed to mathematics learning for Chinese K-12 students and educators. Built via post-training with large-scale reinforcement learning (RL), Confucius3-Math aligns with national curriculum and excels at solving main-stream Chinese K-12 mathematical problems with low cost. In this report we share our development recipe, the challenges we encounter and the techniques we develop to overcome them. In particular, we introduce three technical innovations: Targeted Entropy Regularization, Recent Sample Recovery and Policy-Specific Hardness Weighting. These innovations encompass a new entropy regularization, a novel data scheduling policy, and an improved group-relative advantage estimator. Collectively, they significantly stabilize the RL training, improve data efficiency, and boost performance. Our work demonstrates the feasibility of building strong reasoning models in a particular domain at low cost. We open-source our model and code at <a target="_blank" rel="noopener" href="https://github.com/netease-youdao/Confucius3-Math">https://github.com/netease-youdao/Confucius3-Math</a>. </p>
<blockquote>
<p>我们介绍了孔子3-数学，这是一个拥有14亿参数的开源大型语言模型，它（1）在单个消费级GPU上运行高效；（2）在各种数学推理任务上达到了最先进的性能，优于许多规模更大的模型。特别是，作为我们利用人工智能增强教育和知识传播的使命的一部分，孔子3-数学致力于为中国的K-12学生和教育工作者提供数学学习支持。通过大规模强化学习（RL）进行后训练，孔子3-数学与国家课程相一致，擅长解决主流的中国K-12数学问题，成本低廉。在这份报告中，我们分享了我们的开发配方、遇到的挑战以及为克服这些挑战而开发的技术。特别是，我们介绍了三项技术创新：目标熵正则化、最近样本恢复和政策特定硬度加权。这些创新包括一种新的熵正则化、一种新的数据调度策略和改进的组相对优势估计器。它们共同显著稳定了RL训练，提高了数据效率，并提高了性能。我们的工作证明了在特定领域构建强大的推理模型的可行性，并且成本低廉。我们在<a target="_blank" rel="noopener" href="https://github.com/netease-youdao/Confucius3-Math%E4%B8%8A%E5%85%AC%E5%BC%80%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%92%8C%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/netease-youdao/Confucius3-Math上公开了我们的模型和代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18330v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>针对中文基础教育领域的数学问题，提出了一种新的大型语言模型Confucius3-Math。此模型参数达到十四亿规模，能在单一消费级GPU上高效运行，并在一系列数学推理任务上取得了卓越表现。通过大规模强化学习进行训练，与国家政策一致并能低成本地解决主流中小学数学问题。我们分享了这个模型的发展过程及所采用的创新方法。开源此模型和代码于公众视野，希望能助力教育普及和知识传播。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Confucius3-Math是一个针对中文基础教育领域的大型语言模型，旨在解决数学学习问题。</li>
<li>模型具备优异性能和高效率，能够在单个消费级GPU上运行。在多个数学推理任务上表现突出，超越许多更大规模的模型。</li>
<li>模型通过大规模强化学习进行训练，与国家课程大纲相符，擅长解决主流中小学数学问题。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18330">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ce5edbf071f25efc26f68d2d83fa4481.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cbac252c05a66617f75b69f2e790afd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b2f5a6652e4ff38348aca6472f4a103.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="PhysUniBench-An-Undergraduate-Level-Physics-Reasoning-Benchmark-for-Multimodal-Models"><a href="#PhysUniBench-An-Undergraduate-Level-Physics-Reasoning-Benchmark-for-Multimodal-Models" class="headerlink" title="PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for   Multimodal Models"></a>PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for   Multimodal Models</h2><p><strong>Authors:Lintao Wang, Encheng Su, Jiaqi Liu, Pengze Li, Peng Xia, Jiabei Xiao, Wenlong Zhang, Xinnan Dai, Xi Chen, Yuan Meng, Mingyu Ding, Lei Bai, Wanli Ouyang, Shixiang Tang, Aoran Wang, Xinzhu Ma</strong></p>
<p>Physics problem-solving is a challenging domain for large AI models, requiring integration of conceptual understanding, mathematical reasoning, and interpretation of physical diagrams. Current evaluation methodologies show notable limitations in capturing the breadth and complexity of undergraduate-level physics, underscoring the need for more rigorous assessments. To this end, we present PhysUniBench, a large-scale multimodal benchmark designed to evaluate and improve the reasoning capabilities of multimodal large language models (MLLMs) specifically on undergraduate-level physics problems. PhysUniBench consists of 3,304 physics questions spanning 8 major sub-disciplines of physics, each accompanied by one visual diagrams. The benchmark includes both open-ended and multiple-choice questions, systematically curated and difficulty-rated through an iterative model-in-the-loop process. The benchmark’s construction involved a rigorous multi-stage process, including multiple roll-outs, expert-level evaluation, automated filtering of easily solved problems, and a nuanced difficulty grading system with five levels. Through extensive experiments, we observe that current state-of-the-art models encounter substantial challenges in physics reasoning. For example, GPT-4o mini achieves only about 34.2% accuracy in the proposed PhysUniBench. These results highlight that current MLLMs struggle with advanced physics reasoning, especially on multi-step problems and those requiring precise diagram interpretation. By providing a broad and rigorous assessment tool, PhysUniBench aims to drive progress in AI for Science, encouraging the development of models with stronger physical reasoning, problem-solving skills, and multimodal understanding. The benchmark and evaluation scripts are available at <a target="_blank" rel="noopener" href="https://prismax-team.github.io/PhysUniBenchmark/">https://prismax-team.github.io/PhysUniBenchmark/</a>. </p>
<blockquote>
<p>物理问题解决是一个对大型AI模型具有挑战性的领域，它要求整合概念理解、数学推理和物理图表解释。现有的评估方法在计算本科生水平的物理知识的广度和复杂性方面存在明显的局限性，这强调了需要进行更严格评估的必要性。为此，我们推出了PhysUniBench，这是一个大规模的多模式基准测试，旨在评估和提高多模式大型语言模型（MLLMs）在本科生水平物理问题上的推理能力。PhysUniBench包含3304个物理问题，涵盖物理学的8个主要子学科，每个问题都配有一个视觉图表。该基准测试包括开放问题和多项选择题，通过循环模型过程进行系统的整理和难度评级。基准测试的构建涉及一个严格的多阶段过程，包括多次推出、专家级评估、自动过滤容易解决的问题，以及一个五级精细的难度分级系统。通过广泛的实验，我们发现最先进的模型在物理推理方面遇到了很大的挑战。例如，GPT-4o mini在提出的PhysUniBench上仅达到约34.2%的准确率。这些结果强调，当前的大型语言模型在高级物理推理方面遇到困难，尤其是在多步骤问题和需要精确图表解释的问题方面。通过提供广泛而严格的评估工具，PhysUniBench旨在推动人工智能科学的发展，鼓励开发具有更强物理推理能力、问题解决能力和多模式理解能力的模型。基准测试和评估脚本可在<a target="_blank" rel="noopener" href="https://prismax-team.github.io/PhysUniBenchmark/">链接地址</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17667v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个名为PhysUniBench的大规模多模式基准测试，旨在评估和改进大型语言模型在本科物理学问题上的推理能力。该基准测试包含3304个物理学问题，涵盖物理学的8个主要子学科，每个问题都配备了一个视觉图表。通过广泛的实验观察，发现当前最先进的模型在物理推理方面遇到较大挑战。PhysUniBench的推出旨在为人工智能科学领域的发展提供推动力，鼓励开发具有更强物理推理、问题解决能力和多模式理解能力的模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Physics problem-solving is challenging for large AI models, requiring integration of conceptual understanding, mathematical reasoning, and diagram interpretation.</li>
<li>当前评估方法在捕捉本科物理学广度与复杂性方面存在局限。</li>
<li>推出PhysUniBench基准测试，包含3304个物理学问题，涉及8个主要物理子学科。</li>
<li>该基准测试包含开放问题和选择题，通过模型循环过程中的迭代进行系统性筛选和难度评级。</li>
<li>基准测试的构建涉及多阶段过程，包括多次滚动发布、专家评估、自动过滤易解决问题和细致的难度分级系统。</li>
<li>广泛实验表明，当前最先进的模型在物理推理方面面临挑战，如GPT-4o mini在PhysUniBench上的准确率仅为34.2%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17667">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f9efd008cbb26a0f4748943663122f64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb7340026ac7bbb2f2a6cb27ee24038a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-949a8efd8599be9582d46baa06cb3c44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d07ec680c46599e63294869925c8bbf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a98263515a5431758e9f7c9129d5f75.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Discrete-JEPA-Learning-Discrete-Token-Representations-without-Reconstruction"><a href="#Discrete-JEPA-Learning-Discrete-Token-Representations-without-Reconstruction" class="headerlink" title="Discrete JEPA: Learning Discrete Token Representations without   Reconstruction"></a>Discrete JEPA: Learning Discrete Token Representations without   Reconstruction</h2><p><strong>Authors:Junyeob Baek, Hosung Lee, Christopher Hoang, Mengye Ren, Sungjin Ahn</strong></p>
<p>The cornerstone of cognitive intelligence lies in extracting hidden patterns from observations and leveraging these principles to systematically predict future outcomes. However, current image tokenization methods demonstrate significant limitations in tasks requiring symbolic abstraction and logical reasoning capabilities essential for systematic inference. To address this challenge, we propose Discrete-JEPA, extending the latent predictive coding framework with semantic tokenization and novel complementary objectives to create robust tokenization for symbolic reasoning tasks. Discrete-JEPA dramatically outperforms baselines on visual symbolic prediction tasks, while striking visual evidence reveals the spontaneous emergence of deliberate systematic patterns within the learned semantic token space. Though an initial model, our approach promises a significant impact for advancing Symbolic world modeling and planning capabilities in artificial intelligence systems. </p>
<blockquote>
<p>认知智能的核心在于从观察中提取隐藏的模式，并利用这些原理系统地预测未来结果。然而，当前的图像令牌化方法在需要象征性抽象和逻辑推理能力的任务中显示出重大局限性，这对于系统推理至关重要。为了应对这一挑战，我们提出了Discrete-JEPA，它在潜在预测编码框架的基础上，通过语义令牌化和新型互补目标，为符号推理任务创建了稳健的令牌化。Discrete-JEPA在视觉符号预测任务上的表现远超基线，而有力的视觉证据表明，在所学的语义令牌空间内出现了自发系统的模式。尽管这是一个初步模型，但我们的方法为推进人工智能系统中的符号世界建模和规划能力带来了重大影响的承诺。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14373v2">PDF</a> </p>
<p><strong>Summary</strong>：认知智能的核心在于从观察中提取隐藏模式，并利用这些原则系统地预测未来结果。然而，当前图像令牌化方法在需要符号抽象和逻辑推理能力的任务中表现出显著局限性，这对于系统推理至关重要。为解决这一挑战，我们提出了Discrete-JEPA，它通过语义令牌化和新的补充目标扩展潜在预测编码框架，为符号推理任务创建稳健的令牌化。Discrete-JEPA在视觉符号预测任务上显著优于基准测试，而有力的视觉证据表明，在学习的语义令牌空间内出现了自发系统的模式。尽管是一个初步模型，但我们的方法为推进人工智能系统中的符号世界建模和规划能力产生了重大影响。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>认知智能的核心是提取隐藏模式并进行预测。</li>
<li>当前图像令牌化方法在符号抽象和逻辑推理任务中存在局限性。</li>
<li>Discrete-JEPA通过语义令牌化和补充目标扩展潜在预测编码框架。</li>
<li>Discrete-JEPA在视觉符号预测任务上表现优异。</li>
<li>学习的语义令牌空间内出现了自发系统的模式。</li>
<li>Discrete-JEPA对推进人工智能的符号世界建模有影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14373">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e3080322e65552d83db735ac7e906545.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1fe1f872b2bd84d3719c7c990f8358d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae76f10bbb3b312c8866f26d6a9f2d7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c83a4ed70085832f20b077e52d4174f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Scientists’-First-Exam-Probing-Cognitive-Abilities-of-MLLM-via-Perception-Understanding-and-Reasoning"><a href="#Scientists’-First-Exam-Probing-Cognitive-Abilities-of-MLLM-via-Perception-Understanding-and-Reasoning" class="headerlink" title="Scientists’ First Exam: Probing Cognitive Abilities of MLLM via   Perception, Understanding, and Reasoning"></a>Scientists’ First Exam: Probing Cognitive Abilities of MLLM via   Perception, Understanding, and Reasoning</h2><p><strong>Authors:Yuhao Zhou, Yiheng Wang, Xuming He, Ruoyao Xiao, Zhiwei Li, Qiantai Feng, Zijie Guo, Yuejin Yang, Hao Wu, Wenxuan Huang, Jiaqi Wei, Dan Si, Xiuqi Yao, Jia Bu, Haiwen Huang, Tianfan Fu, Shixiang Tang, Ben Fei, Dongzhan Zhou, Fenghua Ling, Yan Lu, Siqi Sun, Chenhui Li, Guanjie Zheng, Jiancheng Lv, Wenlong Zhang, Lei Bai</strong></p>
<p>Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientists’ First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries. </p>
<blockquote>
<p>科学发现越来越依赖于基于信息密集的科学数据和特定领域专业知识进行复杂的多模式推理。得益于专家级的科学基准测试，科学多模式大型语言模型（MLLMs）在真实的工作流程中有潜力显著增强这一发现过程。然而，当前的科学基准测试主要集中在评估MLLMs的知识理解能力上，导致对其感知和推理能力的评估不足。为了弥补这一空白，我们提出了科学家第一次考试（SFE）基准测试，旨在通过三个相互关联的水平来评估MLLMs的科学认知能力：科学信号感知、科学属性理解、科学比较推理。具体来说，SFE包含830个专家验证过的问答对，涉及三种问题类型，涵盖五个高价值学科的66个多模式任务。大量实验表明，目前最先进的GPT-o3和InternVL-3在SFE上的表现仅为34.08%和26.52%，这表明MLLMs在科学领域仍有很大的改进空间。我们希望从SFE中获得的认识能够促进人工智能增强科学发现的进一步发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10521v3">PDF</a> 82 pages</p>
<p><strong>Summary</strong></p>
<p>基于信息密集型科学数据和领域特定专业知识，科学发现越来越依赖于复杂的跨模态推理。科学跨模态大型语言模型（MLLMs）借助专家级科学基准，具有增强现实工作流程中的发现过程的潜力。然而，当前的科学基准主要侧重于评估MLLMs的知识理解能力，导致对其感知和推理能力的评估不足。为解决这一空白，我们提出了科学家第一次考试（SFE）基准，旨在通过三个相互联系的水平评估MLLMs的科学认知能力：科学信号感知、科学属性理解、科学比较推理。SFE包含830个专家验证的问答对，涵盖三种问题类型和66个跨模态任务，涉及五个高价值学科。实验表明，当前最先进的GPT-o3和InternVL-3在SFE上的表现仅为34.08%和26.52%，这突显出MLLMs在科学领域有巨大的改进空间。我们希望通过SFE获得的见解能促进AI增强科学发现的进一步发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>科学发现现在依赖于复杂的跨模态推理和信息密集型数据。</li>
<li>科学跨模态大型语言模型（MLLMs）有潜力改善科学发现过程。</li>
<li>当前的科学评估基准主要关注MLLMs的知识理解能力，忽视了其感知和推理能力。</li>
<li>科学家第一次考试（SFE）基准旨在全面评估MLLMs的科学认知能力，包括科学信号感知、科学属性理解和科学比较推理。</li>
<li>SFE包含专家验证的问答对，涵盖多种问题类型和跨模态任务，涉及多个学科。</li>
<li>最先进的MLLMs在SFE上的表现不佳，显示其在科学领域的巨大改进空间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10521">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cb3d92158598397970c6aedce3c5df2b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a970e2bbb9edf925c2b94b260c5e7e8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-122fcd0cf5bc197d0dedc0a1a73382e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40e346a7ed5d56d9af01b6b00ac708e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e579c00f906d25d004c5ecb1e3ff9026.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-27/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-27/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-27/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6f3c1ed44efb0cbfb794708947c3304d.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-06-27  The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-26/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5b16fb436db5f89f8658599ea6ddbe9e.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-06-26  Bind-Your-Avatar Multi-Talking-Character Video Generation with Dynamic   3D-mask-based Embedding Router
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27685.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
