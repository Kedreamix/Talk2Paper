<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-09-18  Curriculum Multi-Task Self-Supervision Improves Lightweight   Architectures for Onboard Satellite Hyperspectral Image Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12600v1/page_5_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    84 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-18-更新"><a href="#2025-09-18-更新" class="headerlink" title="2025-09-18 更新"></a>2025-09-18 更新</h1><h2 id="Curriculum-Multi-Task-Self-Supervision-Improves-Lightweight-Architectures-for-Onboard-Satellite-Hyperspectral-Image-Segmentation"><a href="#Curriculum-Multi-Task-Self-Supervision-Improves-Lightweight-Architectures-for-Onboard-Satellite-Hyperspectral-Image-Segmentation" class="headerlink" title="Curriculum Multi-Task Self-Supervision Improves Lightweight   Architectures for Onboard Satellite Hyperspectral Image Segmentation"></a>Curriculum Multi-Task Self-Supervision Improves Lightweight   Architectures for Onboard Satellite Hyperspectral Image Segmentation</h2><p><strong>Authors:Hugo Carlesso, Josiane Mothe, Radu Tudor Ionescu</strong></p>
<p>Hyperspectral imaging (HSI) captures detailed spectral signatures across hundreds of contiguous bands per pixel, being indispensable for remote sensing applications such as land-cover classification, change detection, and environmental monitoring. Due to the high dimensionality of HSI data and the slow rate of data transfer in satellite-based systems, compact and efficient models are required to support onboard processing and minimize the transmission of redundant or low-value data, e.g. cloud-covered areas. To this end, we introduce a novel curriculum multi-task self-supervised learning (CMTSSL) framework designed for lightweight architectures for HSI analysis. CMTSSL integrates masked image modeling with decoupled spatial and spectral jigsaw puzzle solving, guided by a curriculum learning strategy that progressively increases data complexity during self-supervision. This enables the encoder to jointly capture fine-grained spectral continuity, spatial structure, and global semantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously addresses spatial and spectral reasoning within a unified and computationally efficient design, being particularly suitable for training lightweight models for onboard satellite deployment. We validate our approach on four public benchmark datasets, demonstrating consistent gains in downstream segmentation tasks, using architectures that are over 16,000x lighter than some state-of-the-art models. These results highlight the potential of CMTSSL in generalizable representation learning with lightweight architectures for real-world HSI applications. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/hugocarlesso/CMTSSL">https://github.com/hugocarlesso/CMTSSL</a>. </p>
<blockquote>
<p>高光谱成像（HSI）能够捕捉每个像素数百个连续波段内的详细光谱特征，对于土地覆盖分类、变化检测和环境监测等遥感应用是必不可少的。由于HSI数据的高维度和卫星系统数据传输速率的缓慢，需要紧凑而高效的模型来支持机上处理和减少冗余或低价值数据的传输，例如云覆盖区域。为此，我们引入了一种新型课程多任务自监督学习（CMTSSL）框架，该框架专为HSI分析的轻型架构而设计。CMTSSL将遮罩图像建模与解耦的空间和光谱拼图解决相结合，由课程学习策略引导，在自监督过程中逐步增加数据的复杂性。这使得编码器能够联合捕获精细的光谱连续性、空间结构和全局语义特征。与之前的双任务SSL方法不同，CMTSSL在一个统一且计算高效的设计中同时解决空间推理和光谱推理，特别适合训练用于卫星部署的轻型模型。我们在四个公共基准数据集上验证了我们的方法，在下游分割任务中表现出持续的增益，所使用的架构比一些最新模型轻了超过1万六千倍。这些结果突显了CMTSSL在用于真实世界HSI应用的通用表示学习和轻型架构中的潜力。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/hugocarlesso/CMTSSL%E5%B9%B3%E5%AE%9A%E3%80%82">https://github.com/hugocarlesso/CMTSSL公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13229v1">PDF</a> </p>
<p><strong>摘要</strong><br>    超光谱成像（HSI）捕获数百个连续波段每个像素的详细光谱特征，对于土地覆盖分类、变化检测和环境监测等遥感应用是不可或缺的。由于HSI数据的高维度和卫星系统数据传输速率较慢，需要紧凑高效的模型来支持机上处理和减少冗余或低价值数据的传输，例如云覆盖区域。为此，我们引入了一种新型课程多任务自监督学习（CMTSSL）框架，专为HSI分析的轻型架构而设计。CMTSSL将遮罩图像建模与解耦的空间和光谱拼图解决相结合，由课程学习策略引导，在自监督过程中逐步增加数据复杂性。这使编码器能够联合捕获精细光谱连续性、空间结构和全局语义特征。与先前的双任务SSL方法不同，CMTSSL在统一且计算高效的设计中同时解决空间和光谱推理，特别适用于训练用于机上卫星部署的轻型模型。我们在四个公共基准数据集上验证了我们的方法，在下游分割任务中表现出持续的增益，使用的架构比一些最新模型轻16,000倍以上。这些结果突出了CMTSSL在用于真实世界HSI应用的轻量级架构中的通用表示学习的潜力。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/hugocarlesso/CMTSSL%E5%85%AC%E5%BC%BA%E7%9A%84%E3%80%82">https://github.com/hugocarlesso/CMTSSL公开访问。</a></p>
<p><strong>要点</strong></p>
<ol>
<li>超光谱成像（HSI）能够捕获详细的光谱特征，对于遥感应用至关重要。</li>
<li>由于数据的高维度和卫星数据传输的缓慢，需要紧凑且高效的模型进行处理。</li>
<li>引入了一种新的课程多任务自监督学习（CMTSSL）框架，适用于HSI分析的轻型架构。</li>
<li>CMTSSL结合遮罩图像建模与空间及光谱拼图解决，通过课程学习策略实现自我监督。</li>
<li>CMTSSL同时处理空间和光谱推理，不同于之前的双任务SSL方法。</li>
<li>CMTSSL框架在四个公共数据集上的表现优异，训练的模型轻盈，适合在卫星上部署。</li>
<li>该研究突出了CMTSSL在轻量级架构中的通用表示学习潜力，代码已公开。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13229">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13229v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13229v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13229v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13229v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ROOM-A-Physics-Based-Continuum-Robot-Simulator-for-Photorealistic-Medical-Datasets-Generation"><a href="#ROOM-A-Physics-Based-Continuum-Robot-Simulator-for-Photorealistic-Medical-Datasets-Generation" class="headerlink" title="ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic   Medical Datasets Generation"></a>ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic   Medical Datasets Generation</h2><p><strong>Authors:Salvatore Esposito, Matías Mattamala, Daniel Rebain, Francis Xiatian Zhang, Kevin Dhaliwal, Mohsen Khadem, Subramanian Ramamoorthy</strong></p>
<p>Continuum robots are advancing bronchoscopy procedures by accessing complex lung airways and enabling targeted interventions. However, their development is limited by the lack of realistic training and test environments: Real data is difficult to collect due to ethical constraints and patient safety concerns, and developing autonomy algorithms requires realistic imaging and physical feedback. We present ROOM (Realistic Optical Observation in Medicine), a comprehensive simulation framework designed for generating photorealistic bronchoscopy training data. By leveraging patient CT scans, our pipeline renders multi-modal sensor data including RGB images with realistic noise and light specularities, metric depth maps, surface normals, optical flow and point clouds at medically relevant scales. We validate the data generated by ROOM in two canonical tasks for medical robotics – multi-view pose estimation and monocular depth estimation, demonstrating diverse challenges that state-of-the-art methods must overcome to transfer to these medical settings. Furthermore, we show that the data produced by ROOM can be used to fine-tune existing depth estimation models to overcome these challenges, also enabling other downstream applications such as navigation. We expect that ROOM will enable large-scale data generation across diverse patient anatomies and procedural scenarios that are challenging to capture in clinical settings. Code and data: <a target="_blank" rel="noopener" href="https://github.com/iamsalvatore/room">https://github.com/iamsalvatore/room</a>. </p>
<blockquote>
<p>连续型机器人通过进入复杂的肺气道并能够实现针对性干预，从而推动了支气管镜检查手术的进展。然而，它们的发展受限于缺乏真实的训练和测试环境：由于伦理约束和患者安全担忧，真实数据的收集很困难，开发自主算法需要逼真的成像和物理反馈。我们提出了ROOM（医学中的逼真光学观察），这是一个为生成逼真支气管镜检查训练数据而设计的综合仿真框架。通过利用患者CT扫描，我们的管道生成多模态传感器数据，包括带有逼真噪声和光泽的RGB图像、度量深度图、表面法线、光流和医学相关尺度上的点云。我们通过医学机器人的两个典型任务——多视角姿态估计和单目深度估计，验证了ROOM生成数据的真实性，展示了现有方法必须克服的各种挑战才能适应这些医学环境。此外，我们还表明，ROOM生成的数据可用于微调现有的深度估计模型以克服这些挑战，还能够支持其他下游应用，如导航。我们期望ROOM能够在各种患者解剖结构和手术场景中实现大规模的数据生成，这在临床环境中是难以捕获的。代码和数据：<a target="_blank" rel="noopener" href="https://github.com/iamsalvatore/room%E3%80%82">https://github.com/iamsalvatore/room。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13177v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于患者CT扫描数据的仿真框架ROOM为医学领域的支气管镜检查提供逼真的训练数据。该框架能生成多模态传感器数据，包括具有真实噪声和光影特点的RGB图像、度量深度图、表面法线、光学流动和点云等医学相关尺度的数据。此外，该数据可用于优化现有深度估计模型，克服特定医学环境的挑战，并推动下游应用如导航等的发展。ROOM有望生成大规模、多样化的患者解剖结构和手术场景数据，为临床环境中难以捕获的数据提供解决方案。有关代码和数据可在GitHub上获取（<a target="_blank" rel="noopener" href="https://github.com/iamsalvatore/room%EF%BC%89%E3%80%82">https://github.com/iamsalvatore/room）。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>续体机器人正在推动支气管镜检查手术的发展，但仍面临缺乏现实训练和测试环境的挑战。</li>
<li>由于伦理和患者安全限制，真实数据的收集具有挑战性。<br>3.ROOM仿真框架利用患者CT扫描生成逼真的支气管镜检查训练数据。</li>
<li>该框架能够生成多模态传感器数据，包括RGB图像等医学相关尺度的数据。<br>5.ROOM验证数据显示其可用于两个医疗机器人的典型任务：多视角姿态估计和单眼深度估计。<br>6.ROOM生成的医疗数据可用于优化现有深度估计模型并推动下游应用的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13177">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13177v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13177v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13177v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13177v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13177v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13177v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="More-performant-and-scalable-Rethinking-contrastive-vision-language-pre-training-of-radiology-in-the-LLM-era"><a href="#More-performant-and-scalable-Rethinking-contrastive-vision-language-pre-training-of-radiology-in-the-LLM-era" class="headerlink" title="More performant and scalable: Rethinking contrastive vision-language   pre-training of radiology in the LLM era"></a>More performant and scalable: Rethinking contrastive vision-language   pre-training of radiology in the LLM era</h2><p><strong>Authors:Yingtai Li, Haoran Lai, Xiaoqian Zhou, Shuai Ming, Wenxin Ma, Wei Wei, Shaohua Kevin Zhou</strong></p>
<p>The emergence of Large Language Models (LLMs) presents unprecedented opportunities to revolutionize medical contrastive vision-language pre-training. In this paper, we show how LLMs can facilitate large-scale supervised pre-training, thereby advancing vision-language alignment. We begin by demonstrate that modern LLMs can automatically extract diagnostic labels from radiology reports with remarkable precision (&gt;96% AUC in our experiments) without complex prompt engineering, enabling the creation of large-scale “silver-standard” datasets at a minimal cost (~$3 for 50k CT image-report pairs). Further, we find that vision encoder trained on this “silver-standard” dataset achieves performance comparable to those trained on labels extracted by specialized BERT-based models, thereby democratizing the access to large-scale supervised pre-training. Building on this foundation, we proceed to reveal that supervised pre-training fundamentally improves contrastive vision-language alignment. Our approach achieves state-of-the-art performance using only a 3D ResNet-18 with vanilla CLIP training, including 83.8% AUC for zero-shot diagnosis on CT-RATE, 77.3% AUC on RAD-ChestCT, and substantial improvements in cross-modal retrieval (MAP@50&#x3D;53.7% for image-image, Recall@100&#x3D;52.2% for report-image). These results demonstrate the potential of utilizing LLMs to facilitate {\bf more performant and scalable} medical AI systems. Our code is avaiable at <a target="_blank" rel="noopener" href="https://github.com/SadVoxel/More-performant-and-scalable">https://github.com/SadVoxel/More-performant-and-scalable</a>. </p>
<blockquote>
<p>大型语言模型（LLMs）的出现为医学对比视觉语言预训练带来了前所未有的机会。在本文中，我们展示了LLMs如何促进大规模监督预训练，从而推动视觉语言对齐的进步。我们首先证明，现代LLMs能够自动从放射学报告中提取诊断标签，精度极高（在我们的实验中AUC大于96%），无需复杂的提示工程，能够以最低的成本（每5万张CT图像报告对约为3美元）创建大规模的“银标准”数据集。此外，我们发现，在此“银标准”数据集上训练的视觉编码器，其性能与在由特殊BERT模型提取的标签上训练的视觉编码器相当，从而使大规模监督预训练的访问更加普及。在此基础上，我们进一步发现监督预训练从根本上改善了对比视觉语言的对齐。我们的方法仅使用标准的3D ResNet-18和CLIP训练即可实现最先进的性能，包括在CT-RATE上的零样本诊断AUC为83.8%，RAD-ChestCT上的AUC为77.3%，跨模态检索也有显著改善（图像-图像的MAP@50为53.7%，报告-图像的Recall@100为52.2%）。这些结果证明了利用LLMs促进更具性能和可扩展性的医疗人工智能系统的潜力。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/SadVoxel/More-performant-and-scalable%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SadVoxel/More-performant-and-scalable上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13175v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）的出现为医学对比视觉语言预训练带来了前所未有的机会。本研究展示了LLMs如何促进大规模监督预训练，进而推动视觉语言对齐的进步。通过自动从放射学报告中提取诊断标签，LLMs创建了大规模“银标准”数据集，实现了出色的精度（实验中的AUC值超过96%）。在此基础上，本研究发现基于该“银标准”数据集训练的视觉编码器性能与基于BERT模型的标签提取相当，使得大规模监督预训练更加普及。进一步，通过监督预训练，本研究实现了对比视觉语言对齐的显著改善，并在多个任务上达到了领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在医学对比视觉语言预训练中带来革命性机会。</li>
<li>LLMs能够自动从放射学报告中提取诊断标签，创建大规模“银标准”数据集，降低成本。</li>
<li>基于LLMs的“银标准”数据集训练的视觉编码器性能优越。</li>
<li>监督预训练有助于改善对比视觉语言对齐。</li>
<li>研究在多个任务上达到领先水平，包括零样本诊断、跨模态检索等。</li>
<li>利用LLMs可实现更高效、性能更优的医疗AI系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13175">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13175v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13175v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13175v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13175v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Enhancing-Dual-Network-Based-Semi-Supervised-Medical-Image-Segmentation-with-Uncertainty-Guided-Pseudo-Labeling"><a href="#Enhancing-Dual-Network-Based-Semi-Supervised-Medical-Image-Segmentation-with-Uncertainty-Guided-Pseudo-Labeling" class="headerlink" title="Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation   with Uncertainty-Guided Pseudo-Labeling"></a>Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation   with Uncertainty-Guided Pseudo-Labeling</h2><p><strong>Authors:Yunyao Lu, Yihang Wu, Ahmad Chaddad, Tareef Daqqaq, Reem Kateb</strong></p>
<p>Despite the remarkable performance of supervised medical image segmentation models, relying on a large amount of labeled data is impractical in real-world situations. Semi-supervised learning approaches aim to alleviate this challenge using unlabeled data through pseudo-label generation. Yet, existing semi-supervised segmentation methods still suffer from noisy pseudo-labels and insufficient supervision within the feature space. To solve these challenges, this paper proposes a novel semi-supervised 3D medical image segmentation framework based on a dual-network architecture. Specifically, we investigate a Cross Consistency Enhancement module using both cross pseudo and entropy-filtered supervision to reduce the noisy pseudo-labels, while we design a dynamic weighting strategy to adjust the contributions of pseudo-labels using an uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). In addition, we use a self-supervised contrastive learning mechanism to align uncertain voxel features with reliable class prototypes by effectively differentiating between trustworthy and uncertain predictions, thus reducing prediction uncertainty. Extensive experiments are conducted on three 3D segmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposed approach consistently exhibits superior performance across various settings (e.g., 89.95% Dice score on left Atrial with 10% labeled data) compared to the state-of-the-art methods. Furthermore, the usefulness of the proposed modules is further validated via ablation experiments. </p>
<blockquote>
<p>尽管监督式医学图像分割模型表现出卓越的性能，但在现实情况中，依赖大量标记数据并不实际。半监督学习方法旨在通过生成伪标签使用无标签数据来缓解这一挑战。然而，现有的半监督分割方法仍然受到伪标签噪声和特征空间监督不足的影响。为了解决这些挑战，本文提出了一种基于双网络架构的新型半监督3D医学图像分割框架。具体来说，我们研究了一个交叉一致性增强模块，使用交叉伪监督和熵过滤监督来减少噪声伪标签，同时设计了一种动态加权策略，利用不确定性感知机制（即Kullback-Leibler散度）来调整伪标签的贡献。此外，我们采用自我监督的对比学习机制，通过有效区分可信和不确定的预测，将不确定的体素特征与可靠的类别原型对齐，从而减少预测的不确定性。在三个3D分割数据集（左心房、NIH胰腺和BraTS-2019）上进行了广泛实验，所提出的方法在各种设置下始终表现出卓越的性能（例如在左心房数据集上，使用10%的标记数据达到89.95%的Dice得分），与最先进的方法相比具有优势。此外，通过消融实验进一步验证了所提出模块的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13084v1">PDF</a> Accpeted in Knowledge-Based Systems</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于双网络架构的新型半监督3D医学图像分割框架，旨在解决现有方法面临的噪声伪标签和特征空间监督不足的问题。通过交叉伪一致增强模块和熵滤波监督减少噪声伪标签，设计动态权重策略，利用不确定性感知机制（如Kullback-Leibler散度）调整伪标签的贡献。同时，采用自我监督的对比学习机制，通过有效区分可信和不确定的预测，将不确定的体素特征与可靠的类别原型对齐，从而减少预测的不确定性。在三个3D分割数据集上的实验表明，该方法在各种设置下均表现出卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文提出了一种新的半监督3D医学图像分割框架，基于双网络架构。</li>
<li>框架中包含了交叉伪一致增强模块和熵滤波监督，以减少噪声伪标签。</li>
<li>通过动态权重策略，利用不确定性感知机制（如Kullback-Leibler散度）调整伪标签的贡献。</li>
<li>引入自我监督的对比学习机制，将不确定的体素特征与可靠的类别原型对齐。</li>
<li>在三个不同的3D医学图像分割数据集上进行了广泛实验，验证了该框架的优越性能。</li>
<li>在有限制的标签数据下，该框架依然能够展现出出色的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13084">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13084v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13084v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13084v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13084v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Patient-Perspectives-on-Telemonitoring-during-Colorectal-Cancer-Surgery-Prehabilitation"><a href="#Patient-Perspectives-on-Telemonitoring-during-Colorectal-Cancer-Surgery-Prehabilitation" class="headerlink" title="Patient Perspectives on Telemonitoring during Colorectal Cancer Surgery   Prehabilitation"></a>Patient Perspectives on Telemonitoring during Colorectal Cancer Surgery   Prehabilitation</h2><p><strong>Authors:Irina Bianca Serban, Dimitra Dritsa, David ten Cate, Loes Janssen, Margot Heijmans, Sara Colombo, Aarnout Brombacher, Steven Houben</strong></p>
<p>Multimodal prehabilitation for colorectal cancer (CRC) surgery aims to optimize patient fitness and reduce postoperative complications. While telemonitoring’s clinical value in supporting decision-making is recognized, patient perspectives on its use in prehabilitation remain underexplored, particularly compared to its related clinical context, rehabilitation. To address this gap, we conducted interviews with five patients who completed a four-week CRC prehabilitation program incorporating continuous telemonitoring. Our findings reveal patients’ willingness to engage with telemonitoring, shaped by their motivations, perceived benefits, and concerns. We outline design considerations for patient-centered systems and offer a foundation for further research on telemonitoring in CRC prehabilitation. </p>
<blockquote>
<p>针对结直肠癌（CRC）手术的多模式预康复旨在优化患者体能并减少术后并发症。虽然远程监控在支持决策制定方面的临床价值已被认可，但患者对其在预康复中的使用持保留意见，特别是与相关的临床背景相比，康复方面尤其如此。为了弥补这一差距，我们对完成了为期四周CRC预康复计划且连续使用远程监控的五个患者进行了访谈。我们的调查结果表明，患者的参与意愿受他们的动机、感知到的优势和担忧所影响。我们概述了以患者为中心系统的设计注意事项，并为进一步了解CRC预康复中远程监控的研究提供了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13064v1">PDF</a> 20 pages, 3 figures, presented at the 19th EAI International   Conference on Pervasive Computing Technologies for Healthcare, to be   published in the Springer - LNICST series</p>
<p><strong>Summary</strong></p>
<p>针对结直肠癌（CRC）手术的多模式预康复旨在优化患者体能并减少术后并发症。尽管远程监控在支持决策制定中的临床价值得到认可，但关于其在预康复中的使用，尤其是与康复相关的临床背景相比，患者观点仍然鲜有探索。本研究通过访谈完成四周CRC预康复计划的五名患者，发现患者愿意参与远程监控，这受到他们的动机、感知到的利益和担忧的影响。本研究为患者为中心的远程监控系统提供了设计考量，并为CRC预康复中的远程监控研究奠定了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模式预康复旨在优化结直肠癌手术患者的体能并减少术后并发症。</li>
<li>远程监控在预康复中的患者视角尚未得到充分探索。</li>
<li>患者对远程监控的接受程度受动机、感知到的利益和担忧的影响。</li>
<li>访谈发现患者对参与远程监控持积极态度。</li>
<li>设计患者为中心的远程监控系统需要考虑患者的需求和体验。</li>
<li>远程监控在预康复中有潜力支持决策制定并改善患者康复。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13064">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13064v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.13064v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Magnetic-Reconnection-as-a-Potential-Driver-of-X-ray-Variability-in-Active-Galactic-Nuclei"><a href="#Magnetic-Reconnection-as-a-Potential-Driver-of-X-ray-Variability-in-Active-Galactic-Nuclei" class="headerlink" title="Magnetic Reconnection as a Potential Driver of X-ray Variability in   Active Galactic Nuclei"></a>Magnetic Reconnection as a Potential Driver of X-ray Variability in   Active Galactic Nuclei</h2><p><strong>Authors:Chen-Ran Hu, Yong-Feng Huang, Lang Cui, Hanle Zhang, Jiang-Tao Li, Li Ji, Jin-Jun Geng, Orkash Amat, Fan Xu, Chen Du, Wen-Long Zhang, Ze-Cheng Zou, Xiao-Fei Dong, Chen Deng, Pengfei Jiang, Jie Liao</strong></p>
<p>We present a systematic analysis on the X-ray variability in 13 bright quasars at z &gt; 4.5, combining recent Swift observations from 2021 to 2023 and archival multi-epoch observations. Upper limits of the luminosity measurements were included in the analysis by using the Kaplan-Meier estimator method. It is found that the high-z quasars exhibit X-ray variability on both short-term (hours-to-days) and intermediate-term (weeks-to-months) timescales, with short-term variability dominating the overall variation. A linear correlation exists between the global mean ($\mu_{\mathrm{L_{2-10,keV}}}$) and standard deviation ($\sigma_{\mathrm{L_{2-10,keV}}}$) of X-ray luminosities, which is independent of the X-ray photon index and optical-to-X-ray spectral slope. The localized stochastic magnetic reconnection mechanism is strongly favored, which can naturally lead to a scale-invariant power-law energy distribution and satisfactorily explain the correlation. The $\sigma$-$\mu$ correlation parallels with the well-documented rms-flux relation of low-z active galactic nuclei (AGNs), implying the magnetic reconnection mechanism could drive short-timescale X-ray variability in both high- and low-z AGNs. The highest-z quasar in our sample, J142952+544717 (z &#x3D; 6.18), shows a luminosity distribution extending to ${10}^{47}\ \rm{erg\ {s}^{-1}}$ with a not conspicuous median luminosity. On the other hand, J143023+420436 (z &#x3D; 4.7), which hosts the most relativistic jet among known high-z blazars, is dominated in the high-luminosity regime (${10}^{47}\ \rm{erg\ {s}^{-1}}$ ), making it an ideal target for multi-wavelength follow-up observations. J090630+693030 is found to have a rest-frame period of 182.46 days and J143023+420436 has a period of 16.89 days, both could be explained by the global evolution of plasmoid chains, in which magnetic islands formed during reconnection may merge successively. </p>
<blockquote>
<p>我们对13个红移z&gt;4.5的明亮类星体进行了X射线变化性的系统分析，结合了2021年至2023年的最新Swift观测数据和档案多时代观测数据。通过使用Kaplan-Meier估计器方法，我们将光度测量的上限值纳入分析。发现高红移类星体在短期（小时到天）和中期（周到月）的时间尺度上表现出X射线变化性，其中短期变化性在总体变化中占主导地位。全局平均（μL_{2-10 keV}）和标准差（σL_{2-10 keV}）之间存在线性相关性，这种相关性独立于X射线光子指数和光学到X射线光谱斜率。局部随机磁重联机制更有可能存在，它可以自然地导致尺度不变幂律能量分布，并能很好地解释这种相关性。σ-μ的相关性类似于低红移活动星系核（AGNs）的rms-flux关系，这表明磁重联机制可能是驱动高红移和低红移AGNs短期X射线变化性的原因。我们样本中的最高红移类星体J142952+544717（z&#x3D;6.18）表现出亮度分布延伸至erg s^-1量级，具有不显眼的平均亮度。另一方面，拥有已知高红移类星体中最为相对论性喷射流体的J143023+420436（z&#x3D;4.7），在高亮度区域（erg s^-1量级）占据主导地位，是开展多波长后续观测的理想目标。发现J090630+693030的静止框架周期为182.46天，而J143023+420436的周期为16.89天，这两者都可以用全球等离子体链演化来解释，在重联过程中形成的磁岛可能依次合并。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12978v1">PDF</a> ApJ in press, 17 pages, 5 figures; DOI: 10.3847&#x2F;1538-4357&#x2F;adfed2</p>
<p><strong>Summary</strong><br>     本文研究了z &gt; 4.5的13个明亮类星体的X射线可变性，结合了最近的Swift观测数据和历史多时段观测档案。发现高红移类星体在短期（小时到天）和中期（周到月）时间尺度上表现出X射线可变性，短期可变性占主导地位。X射线光度全局平均值与标准偏差之间存在线性相关性，与低红移活动星系核的rms-flux关系相似，暗示磁场重联机制可能驱动高、低红移AGNs的短期X射线可变性。最高红移类星体表现出较不明显的中位光度分布，而另一目标则显示出强烈的相对论性喷流特征，为多种波长下的后续观测提供了理想目标。最后还发现两个类星体的周期可能与磁场重新连接期间磁岛依次合并的全球演化有关。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高红移类星体在短期和中期时间尺度上展现出X射线可变性。</li>
<li>短期可变性在整体变化中占主导地位。</li>
<li>X射线光度全局平均值与标准偏差之间存在线性相关性，与低红移活动星系核的rms-flux关系类似。</li>
<li>磁场重联机制可能是驱动高、低红移AGNs短期X射线可变性的原因。</li>
<li>最高红移类星体的中位光度分布相对不显眼。</li>
<li>存在相对论性喷流特征的类星体是多种波长下的理想观测目标。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12978">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12978v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12978v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Structural-and-Electrocatalytic-Properties-of-La-Co-Ni-Oxide-Thin-Films"><a href="#Structural-and-Electrocatalytic-Properties-of-La-Co-Ni-Oxide-Thin-Films" class="headerlink" title="Structural and Electrocatalytic Properties of La-Co-Ni Oxide Thin Films"></a>Structural and Electrocatalytic Properties of La-Co-Ni Oxide Thin Films</h2><p><strong>Authors:Patrick Marx, Shivam Shukla, Alejandro Esteban Perez Mendoza, Florian Lourens, Corina Andronescu, Alfred Ludwig</strong></p>
<p>La-Co-Ni oxides were fabricated in the form of thin-film materials libraries by combinatorial reactive co-sputtering and analyzed for structural and functional properties over large compositional ranges: normalized to the metals of the film they span about 0 - 70 at.-% for Co, 18 - 81 at.-% for La and 11 - 25 at.-% for Ni. Composition-dependent phase analysis shows formation of three areas with different phase constitutions in dependance of Co-content: In the La-rich region with low Co content, a mixture of the phases La2O3, perovskite, and La(OH)3 is observed. In the Co-rich region, perovskite and spinel phases form. Between the three-phase region and the Co-rich two-phase region, a single-phase perovskite region emerges. Surface microstructure analysis shows formation of additional crystallites on the surface in the two-phase area, which become more numerous with increasing Ni-content. Energy-dispersive X-ray analysis indicates that these crystallites mainly contain Co and Ni, so they could be spinels growing on the surface. The analysis of the oxygen evolution reaction (OER) electrocatalytic activity over all compositions and phase constitutions reveals that the perovskite&#x2F;spinel two-phase region shows the highest catalytic activity, which increases with higher Ni-content. The highest OER current density was measured as 2.24 mA&#x2F;cm2 at 1.8 V vs. RHE for the composition La11Co20Ni9O60. </p>
<blockquote>
<p>La-Co-Ni氧化物通过组合反应共溅射技术制成薄膜材料库的形式，并在大范围成分范围内分析其结构和功能特性：归一化到薄膜的金属，Co的含量大约在0-70原子百分比之间，La的含量在18-81原子百分比之间，Ni的含量在11-25原子百分比之间。根据成分进行相分析显示，根据Co含量的不同，形成了三个具有不同相结构的区域：在La含量丰富、Co含量低的区域，观察到La2O3、钙钛矿和La(OH)3的混合相。在Co含量丰富的区域，形成钙钛矿和尖晶石相。在三相区和Co富集的两相区之间，出现了一个单相钙钛矿区。表面微观结构分析显示，在两相区域表面形成了额外的晶粒，并且随着Ni含量的增加，这些晶粒的数量也在增加。能量色散X射线分析表明，这些晶粒主要含有Co和Ni，因此它们可能是表面上的尖晶石。对所有成分和相结构的氧演化反应（OER）电催化活性进行分析，结果显示钙钛矿&#x2F;尖晶石两相区具有最高的催化活性，并且随着Ni含量的增加，活性也在增加。对于La11Co20Ni9O60的组成，在1.8V相对于RHE的电压下，测量的最高OER电流密度为2.24mA&#x2F;cm2。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12946v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本描述了通过组合反应共溅射法制备的La-Co-Ni氧化物薄膜材料库的结构和功能特性分析。研究表明，不同成分构成的区域具有不同的相结构，且表面微结构和催化活性受成分影响显著。其中，含有特定成分的La11Co20Ni9O60的样品在氧析出反应中表现出最佳催化活性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>La-Co-Ni氧化物以薄膜材料库的形式通过组合反应共溅射法制造。</li>
<li>薄膜的成分范围：Co为0-70 at.-%，La为18-81 at.-%，Ni为11-25 at.-%。</li>
<li>根据成分不同，可分为三个相构成区域：La丰富的区域、Co丰富的区域以及两者之间的单相钙钛矿区域。</li>
<li>两相区域表面会形成额外的晶粒，且随着Ni含量的增加，这些晶粒的数量也会增加。</li>
<li>能量散射X射线分析显示，这些晶粒主要包含Co和Ni，可能是表面生长的尖晶石。</li>
<li>氧析出反应的电催化活性分析表明，钙钛矿&#x2F;尖晶石两相区域具有最高的催化活性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12946">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12946v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12946v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12946v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AREPAS-Anomaly-Detection-in-Fine-Grained-Anatomy-with-Reconstruction-Based-Semantic-Patch-Scoring"><a href="#AREPAS-Anomaly-Detection-in-Fine-Grained-Anatomy-with-Reconstruction-Based-Semantic-Patch-Scoring" class="headerlink" title="AREPAS: Anomaly Detection in Fine-Grained Anatomy with   Reconstruction-Based Semantic Patch-Scoring"></a>AREPAS: Anomaly Detection in Fine-Grained Anatomy with   Reconstruction-Based Semantic Patch-Scoring</h2><p><strong>Authors:Branko Mitic, Philipp Seeböck, Helmut Prosch, Georg Langs</strong></p>
<p>Early detection of newly emerging diseases, lesion severity assessment, differentiation of medical conditions and automated screening are examples for the wide applicability and importance of anomaly detection (AD) and unsupervised segmentation in medicine. Normal fine-grained tissue variability such as present in pulmonary anatomy is a major challenge for existing generative AD methods. Here, we propose a novel generative AD approach addressing this issue. It consists of an image-to-image translation for anomaly-free reconstruction and a subsequent patch similarity scoring between observed and generated image-pairs for precise anomaly localization. We validate the new method on chest computed tomography (CT) scans for the detection and segmentation of infectious disease lesions. To assess generalizability, we evaluate the method on an ischemic stroke lesion segmentation task in T1-weighted brain MRI. Results show improved pixel-level anomaly segmentation in both chest CTs and brain MRIs, with relative DICE score improvements of +1.9% and +4.4%, respectively, compared to other state-of-the-art reconstruction-based methods. </p>
<blockquote>
<p>异常检测（AD）和无监督分割在医学中的广泛应用和重要性，体现在新兴疾病的早期检测、病变严重程度评估、医疗条件鉴别和自动筛查等方面。肺部解剖结构中存在的正常细粒度组织变化对现有生成式AD方法构成重大挑战。针对这一问题，我们提出了一种新颖的生成式AD方法。它包含异常重建的图像转换，以及用于精确异常定位的观测图像和生成图像之间的斑块相似性评分。我们在胸部计算机断层扫描（CT）上验证了新方法，用于检测并分割传染病病变。为了评估方法的通用性，我们在加权脑MRI的缺血性卒中病变分割任务上进行了评估。结果显示，与传统的重建方法相比，该新方法在胸部CT和脑部MRI上均实现了改进的像素级异常分割，相对Dice得分分别提高了+1.9%和+4.4%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12905v1">PDF</a> </p>
<p><strong>Summary</strong><br>     医学图像异常检测（AD）与无监督分割在疾病早期检测、病变严重程度评估、医学条件鉴别和自动筛选等方面具有广泛的应用和重要性。针对现有生成式AD方法在面对正常精细组织变化（如肺部结构）时的挑战，本文提出了一种新的生成式AD方法。该方法包括异常重建的图像到图像转换，以及观测与生成图像之间的精确异常定位。经过对胸部CT扫描的传染病病变检测和分割验证，以及在T1加权脑MRI的缺血性卒中病变分割任务上的泛化性评估，该方法提高了像素级别的异常分割效果，相较于其他最先进的重建方法，相对DICE得分提高了+1.9%（胸部CT）和+4.4%（脑MRI）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像异常检测（AD）与无监督分割在医学领域具有广泛的应用价值，如疾病早期检测、病变评估等。</li>
<li>针对现有生成式AD方法在应对正常精细组织变化时的挑战，提出了一种新的生成式AD方法。</li>
<li>新方法包括图像到图像的异常重建转换和精确异常定位。</li>
<li>方法在胸部CT扫描的传染病病变检测和分割上进行了验证。</li>
<li>方法在T1加权脑MRI的缺血性卒中病变分割任务上进行了泛化性评估。</li>
<li>与其他先进的重建方法相比，新方法的像素级异常分割效果有所提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12905">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12905v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12905v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12905v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Runge-Kutta-Approximation-and-Decoupled-Attention-for-Rectified-Flow-Inversion-and-Semantic-Editing"><a href="#Runge-Kutta-Approximation-and-Decoupled-Attention-for-Rectified-Flow-Inversion-and-Semantic-Editing" class="headerlink" title="Runge-Kutta Approximation and Decoupled Attention for Rectified Flow   Inversion and Semantic Editing"></a>Runge-Kutta Approximation and Decoupled Attention for Rectified Flow   Inversion and Semantic Editing</h2><p><strong>Authors:Weiming Chen, Zhihan Zhu, Yijia Wang, Zhihai He</strong></p>
<p>Rectified flow (RF) models have recently demonstrated superior generative performance compared to DDIM-based diffusion models. However, in real-world applications, they suffer from two major challenges: (1) low inversion accuracy that hinders the consistency with the source image, and (2) entangled multimodal attention in diffusion transformers, which hinders precise attention control. To address the first challenge, we propose an efficient high-order inversion method for rectified flow models based on the Runge-Kutta solver of differential equations. To tackle the second challenge, we introduce Decoupled Diffusion Transformer Attention (DDTA), a novel mechanism that disentangles text and image attention inside the multimodal diffusion transformers, enabling more precise semantic control. Extensive experiments on image reconstruction and text-guided editing tasks demonstrate that our method achieves state-of-the-art performance in terms of fidelity and editability. Code is available at <a target="_blank" rel="noopener" href="https://github.com/wmchen/RKSovler_DDTA">https://github.com/wmchen/RKSovler_DDTA</a>. </p>
<blockquote>
<p>纠正流（RF）模型最近展现出比基于DDIM的扩散模型更高的生成性能。然而，在实际应用中，它们面临两大挑战：（1）较低的倒转精度，阻碍了与源图像的一致性；（2）扩散变压器中的纠缠多模式注意力，这阻碍了精确的注意力控制。为了解决第一个挑战，我们提出了一种基于微分方程Runge-Kutta求解器的高效高阶倒转方法，用于纠正流模型。为解决第二个挑战，我们引入了“解耦扩散变压器注意力”（DDTA）机制，该机制在跨模式扩散变压器内部解开文本和图像注意力，从而实现更精确语义控制。在图像重建和文本指导编辑任务上的大量实验表明，我们的方法在保真度和可编辑性方面达到了最先进的性能。代码可访问<a target="_blank" rel="noopener" href="https://github.com/wmchen/RKSovler_DDTA%E3%80%82">https://github.com/wmchen/RKSovler_DDTA。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12888v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对Rectified Flow（RF）模型的两个主要挑战的解决方案。一是利用Runge-Kutta求解器提高RF模型的高阶反演方法，提高了反演准确性并增强了与源图像的一致性。二是提出了Decoupled Diffusion Transformer Attention（DDTA）机制，解决了扩散变压器中的纠缠多模态注意力问题，实现了更精确的控制语义。实验证明，该方法在图像重建和文本引导编辑任务上达到了最先进的性能，实现了保真度和可编辑性的提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Rectified Flow（RF）模型在生成性能上展现出优越性，但在实际应用中面临两个主要挑战：低反演准确性和纠缠的多模态注意力问题。</li>
<li>提出了一种基于Runge-Kutta求解器的高效高阶反演方法，提高了RF模型的反演准确性，增强了与源图像的一致性。</li>
<li>引入了Decoupled Diffusion Transformer Attention（DDTA）机制，解决了扩散变压器中的多模态注意力问题，实现了更精确的语义控制。</li>
<li>通过广泛的实验验证，该方法在图像重建和文本引导编辑任务上达到了最先进的性能。</li>
<li>所提出的方法在保真度和可编辑性方面有了显著的提升。</li>
<li>相关的代码已实现并公开可用，方便其他研究者进行进一步的研究和改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12888">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12888v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12888v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12888v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12888v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12888v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DyGLNet-Hybrid-Global-Local-Feature-Fusion-with-Dynamic-Upsampling-for-Medical-Image-Segmentation"><a href="#DyGLNet-Hybrid-Global-Local-Feature-Fusion-with-Dynamic-Upsampling-for-Medical-Image-Segmentation" class="headerlink" title="DyGLNet: Hybrid Global-Local Feature Fusion with Dynamic Upsampling for   Medical Image Segmentation"></a>DyGLNet: Hybrid Global-Local Feature Fusion with Dynamic Upsampling for   Medical Image Segmentation</h2><p><strong>Authors:Yican Zhao, Ce Wang, You Hao, Lei Li, Tianli Liao</strong></p>
<p>Medical image segmentation grapples with challenges including multi-scale lesion variability, ill-defined tissue boundaries, and computationally intensive processing demands. This paper proposes the DyGLNet, which achieves efficient and accurate segmentation by fusing global and local features with a dynamic upsampling mechanism. The model innovatively designs a hybrid feature extraction module (SHDCBlock), combining single-head self-attention and multi-scale dilated convolutions to model local details and global context collaboratively. We further introduce a dynamic adaptive upsampling module (DyFusionUp) to realize high-fidelity reconstruction of feature maps based on learnable offsets. Then, a lightweight design is adopted to reduce computational overhead. Experiments on seven public datasets demonstrate that DyGLNet outperforms existing methods, particularly excelling in boundary accuracy and small-object segmentation. Meanwhile, it exhibits lower computation complexity, enabling an efficient and reliable solution for clinical medical image analysis. The code will be made available soon. </p>
<blockquote>
<p>医学图像分割面临多种挑战，包括病变的多尺度变化、组织边界不清以及计算处理需求大。本文提出了DyGLNet，它通过融合全局和局部特征以及采用动态上采样机制，实现了高效且准确的分割。该模型创新地设计了一个混合特征提取模块（SHDCBlock），结合了单头自注意力和多尺度膨胀卷积，以协同建模局部细节和全局上下文。我们进一步引入了一个动态自适应上采样模块（DyFusionUp），基于可学习的偏移量实现特征图的高保真重建。同时，采用了一种轻量级设计来降低计算开销。在七个公共数据集上的实验表明，DyGLNet优于现有方法，尤其在边界精度和小目标分割方面表现突出。同时，它表现出较低的计算复杂度，为临床医学图像分析提供了高效可靠的解决方案。代码将很快公布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12763v1">PDF</a> 18pages, under review</p>
<p><strong>Summary</strong><br>医学图像分割面临多尺度病变差异、组织边界模糊和计算处理需求高等挑战。本文提出DyGLNet模型，通过融合全局和局部特征，并采用动态上采样机制实现高效准确的分割。模型创新设计混合特征提取模块SHDCBlock，结合单头自注意力机制和多尺度膨胀卷积，协同建模局部细节和全局上下文。引入动态自适应上采样模块DyFusionUp，基于可学习偏移量实现特征图的高保真重建。采用轻量化设计降低计算开销。在七个公开数据集上的实验表明，DyGLNet优于现有方法，尤其在边界精度和小目标分割上表现突出，且计算复杂度较低，为临床医疗图像分析提供高效可靠解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分割面临多尺度病变差异、组织边界模糊和计算处理需求高的挑战。</li>
<li>DyGLNet模型通过融合全局和局部特征实现高效准确的分割。</li>
<li>SHDCBlock模块结合自注意力机制和卷积，协同建模局部和全局信息。</li>
<li>DyFusionUp模块实现特征图的高保真重建。</li>
<li>DyGLNet采用轻量化设计以降低计算开销。</li>
<li>实验证明DyGLNet在边界精度和小目标分割上优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12763">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12763v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12763v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12763v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12763v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12763v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="RIS-FUSION-Rethinking-Text-Driven-Infrared-and-Visible-Image-Fusion-from-the-Perspective-of-Referring-Image-Segmentation"><a href="#RIS-FUSION-Rethinking-Text-Driven-Infrared-and-Visible-Image-Fusion-from-the-Perspective-of-Referring-Image-Segmentation" class="headerlink" title="RIS-FUSION: Rethinking Text-Driven Infrared and Visible Image Fusion   from the Perspective of Referring Image Segmentation"></a>RIS-FUSION: Rethinking Text-Driven Infrared and Visible Image Fusion   from the Perspective of Referring Image Segmentation</h2><p><strong>Authors:Siju Ma, Changsiyu Gong, Xiaofeng Fan, Yong Ma, Chengjie Jiang</strong></p>
<p>Text-driven infrared and visible image fusion has gained attention for enabling natural language to guide the fusion process. However, existing methods lack a goal-aligned task to supervise and evaluate how effectively the input text contributes to the fusion outcome. We observe that referring image segmentation (RIS) and text-driven fusion share a common objective: highlighting the object referred to by the text. Motivated by this, we propose RIS-FUSION, a cascaded framework that unifies fusion and RIS through joint optimization. At its core is the LangGatedFusion module, which injects textual features into the fusion backbone to enhance semantic alignment. To support multimodal referring image segmentation task, we introduce MM-RIS, a large-scale benchmark with 12.5k training and 3.5k testing triplets, each consisting of an infrared-visible image pair, a segmentation mask, and a referring expression. Extensive experiments show that RIS-FUSION achieves state-of-the-art performance, outperforming existing methods by over 11% in mIoU. Code and dataset will be released at <a target="_blank" rel="noopener" href="https://github.com/SijuMa2003/RIS-FUSION">https://github.com/SijuMa2003/RIS-FUSION</a>. </p>
<blockquote>
<p>文本驱动的红外和可见光图像融合因其能够通过自然语言引导融合过程而受到关注。然而，现有方法缺乏目标对齐的任务来监督和评估输入文本对融合结果的影响程度。我们发现，图像分割参照（RIS）和文本驱动的融合共享一个共同的目标：突出文本所引用的对象。基于此，我们提出了RIS-FUSION，这是一个通过联合优化将融合和RIS统一起来的级联框架。其核心是LangGatedFusion模块，该模块将文本特征注入融合主干，以增强语义对齐。为了支持多模态参照图像分割任务，我们引入了MM-RIS，这是一个大规模基准测试，包含12.5k个训练样本和3.5k个测试样本三元组，每个三元组都由一个红外-可见光图像对、一个分割掩膜和一个引用表达式组成。大量实验表明，RIS-FUSION达到了最先进的性能，在mIoU上较现有方法提高了超过11%。代码和数据集将在<a target="_blank" rel="noopener" href="https://github.com/SijuMa2003/RIS-FUSION%E4%B8%8A%E9%80%A0%E9%BD%BF%E5%BC%8F%E9%AB%98%E6%9D%A1%E5%AF%BC%E5%BC%BA%E7%9A%84%E5%BE%AE%E7%AC%A6s%E3%80%82">https://github.com/SijuMa2003/RIS-FUSION上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12710v1">PDF</a> 5 pages, 2 figures</p>
<p><strong>Summary</strong><br>     文本驱动的红外和可见光图像融合方法受到关注，但仍缺乏目标导向的任务来监督并评估输入文本对融合结果的影响。本文观察到融合和参照图像分割（RIS）的共同目标：突出文本所指的物体。因此，提出RIS-FUSION框架，通过联合优化融合和RIS。其核心是LangGatedFusion模块，该模块将文本特征注入融合主干以增强语义对齐。为支持多模态参照图像分割任务，引入MM-RIS大规模基准测试，包含12.5k训练样本和3.5k测试样本。实验表明，RIS-FUSION达到最佳性能，较现有方法提高了超过11%的mIoU。相关代码和数据集将在<a target="_blank" rel="noopener" href="https://github.com/SijuMa2003/RIS-FUSION%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/SijuMa2003/RIS-FUSION发布。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本驱动的红外和可见光图像融合受到重视，但仍需解决缺乏有效监督与评估的问题。</li>
<li>参照图像分割（RIS）与图像融合有共同目标：突出文本所指的物体。</li>
<li>提出RIS-FUSION框架，通过联合优化融合和RIS来提升性能。</li>
<li>LangGatedFusion模块是RIS-FUSION的核心，它能够将文本特征注入融合过程，增强语义对齐。</li>
<li>为支持多模态参照图像分割任务，引入MM-RIS大规模基准测试。</li>
<li>RIS-FUSION在实验中表现出卓越性能，相较于现有方法显著提高mIoU超过11%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12710">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12710v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12710v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12710v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12710v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12710v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Multimodal-Foundation-Model-to-Enhance-Generalizability-and-Data-Efficiency-for-Pan-cancer-Prognosis-Prediction"><a href="#A-Multimodal-Foundation-Model-to-Enhance-Generalizability-and-Data-Efficiency-for-Pan-cancer-Prognosis-Prediction" class="headerlink" title="A Multimodal Foundation Model to Enhance Generalizability and Data   Efficiency for Pan-cancer Prognosis Prediction"></a>A Multimodal Foundation Model to Enhance Generalizability and Data   Efficiency for Pan-cancer Prognosis Prediction</h2><p><strong>Authors:Huajun Zhou, Fengtao Zhou, Jiabo Ma, Yingxue Xu, Xi Wang, Xiuming Zhang, Li Liang, Zhenhui Li, Hao Chen</strong></p>
<p>Multimodal data provides heterogeneous information for a holistic understanding of the tumor microenvironment. However, existing AI models often struggle to harness the rich information within multimodal data and extract poorly generalizable representations. Here we present MICE (Multimodal data Integration via Collaborative Experts), a multimodal foundation model that effectively integrates pathology images, clinical reports, and genomics data for precise pan-cancer prognosis prediction. Instead of conventional multi-expert modules, MICE employs multiple functionally diverse experts to comprehensively capture both cross-cancer and cancer-specific insights. Leveraging data from 11,799 patients across 30 cancer types, we enhanced MICE’s generalizability by coupling contrastive and supervised learning. MICE outperformed both unimodal and state-of-the-art multi-expert-based multimodal models, demonstrating substantial improvements in C-index ranging from 3.8% to 11.2% on internal cohorts and 5.8% to 8.8% on independent cohorts, respectively. Moreover, it exhibited remarkable data efficiency across diverse clinical scenarios. With its enhanced generalizability and data efficiency, MICE establishes an effective and scalable foundation for pan-cancer prognosis prediction, holding strong potential to personalize tailored therapies and improve treatment outcomes. </p>
<blockquote>
<p>多模态数据为全面理解肿瘤微环境提供了异质信息。然而，现有的AI模型往往难以利用多模态数据中的丰富信息，并提取出泛化性较差的表示。在这里，我们提出了MICE（通过协作专家进行多模态数据集成），这是一种多模态基础模型，可以有效地整合病理图像、临床报告和基因组数据，进行精确的泛癌预后预测。与传统的多专家模块不同，MICE采用多个功能各异的专家来全面捕捉跨癌症和癌症特定的见解。我们利用来自30种癌症类型的11799名患者的数据，通过对比学习和有监督学习相结合，提高了MICE的泛化能力。MICE在内部队列和独立队列上的C指数分别提高了3.8%~11.2%和5.8%~8.8%，表现出优于单模态和基于多专家的最新多模态模型。此外，它在各种临床场景中表现出显著的数据效率。凭借其增强的泛化能力和数据效率，MICE为泛癌预后预测建立了有效且可扩展的基础，具有强大的潜力来个性化定制疗法并改善治疗效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12600v1">PDF</a> 27 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>多模态数据提供了关于肿瘤微环境的全面信息。然而，现有的AI模型难以利用多模态数据中的丰富信息并提取出可概括性差的表示。这里我们提出了MICE（通过协作专家进行多模态数据集成），这是一种有效的多模态基础模型，可以整合病理图像、临床报告和基因组数据，进行精确的泛癌预后预测。MICE通过多个功能各异的专家来全面捕捉跨癌症和癌症特定的见解，并利用来自30种癌症的11,799名患者的数据，通过对比学习和监督学习增强了其通用性。MICE在内部队列和独立队列上的C指数分别提高了3.8%至11.2%和5.8%至8.8%，显示出比单模态和目前的多专家多模态模型更优越的性能。此外，它在各种临床场景下具有出色数据效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态数据包含关于肿瘤微环境的丰富和多样化的信息，有助于更全面地理解癌症。</li>
<li>现有AI模型在多模态数据的利用上表现不足，难以提取可概括的代表性信息。</li>
<li>MICE是一种新型多模态基础模型，能够整合多种数据类型（如病理图像、临床报告和基因组数据）。</li>
<li>MICE采用多个功能各异的专家来捕捉跨癌症和特定癌症的见解。</li>
<li>MICE在泛癌预后预测方面表现出卓越性能，C指数显著提高。</li>
<li>MICE在内部和独立队列的实验中都表现出良好的性能，证明了其泛化能力和数据效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12600">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12600v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12600v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12600v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DinoAtten3D-Slice-Level-Attention-Aggregation-of-DinoV2-for-3D-Brain-MRI-Anomaly-Classification"><a href="#DinoAtten3D-Slice-Level-Attention-Aggregation-of-DinoV2-for-3D-Brain-MRI-Anomaly-Classification" class="headerlink" title="DinoAtten3D: Slice-Level Attention Aggregation of DinoV2 for 3D Brain   MRI Anomaly Classification"></a>DinoAtten3D: Slice-Level Attention Aggregation of DinoV2 for 3D Brain   MRI Anomaly Classification</h2><p><strong>Authors:Fazle Rafsani, Jay Shah, Catherine D. Chong, Todd J. Schwedt, Teresa Wu</strong></p>
<p>Anomaly detection and classification in medical imaging are critical for early diagnosis but remain challenging due to limited annotated data, class imbalance, and the high cost of expert labeling. Emerging vision foundation models such as DINOv2, pretrained on extensive, unlabeled datasets, offer generalized representations that can potentially alleviate these limitations. In this study, we propose an attention-based global aggregation framework tailored specifically for 3D medical image anomaly classification. Leveraging the self-supervised DINOv2 model as a pretrained feature extractor, our method processes individual 2D axial slices of brain MRIs, assigning adaptive slice-level importance weights through a soft attention mechanism. To further address data scarcity, we employ a composite loss function combining supervised contrastive learning with class-variance regularization, enhancing inter-class separability and intra-class consistency. We validate our framework on the ADNI dataset and an institutional multi-class headache cohort, demonstrating strong anomaly classification performance despite limited data availability and significant class imbalance. Our results highlight the efficacy of utilizing pretrained 2D foundation models combined with attention-based slice aggregation for robust volumetric anomaly detection in medical imaging. Our implementation is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Rafsani/DinoAtten3D.git">https://github.com/Rafsani/DinoAtten3D.git</a>. </p>
<blockquote>
<p>医学成像中的异常检测与分类对早期诊断至关重要，但由于标注数据有限、类别不平衡以及专家标注的高成本，仍然面临挑战。新兴的视觉基础模型，如DINOv2，在大量无标签数据集上进行预训练，提供通用的表示，可能有助于缓解这些限制。本研究中，我们提出了一种针对3D医学图像异常分类的基于注意力的全局聚合框架。我们利用自监督的DINOv2模型作为预训练特征提取器，处理大脑MRI的单个2D轴向切片，通过软注意力机制分配自适应的切片级重要性权重。为了进一步解决数据稀缺问题，我们采用了一种组合损失函数，将监督对比学习与类方差正则化相结合，增强类间可分性和类内一致性。我们在ADNI数据集和机构多类头痛队列上验证了我们的框架，尽管数据有限且存在显著的类别不平衡问题，但仍表现出强大的异常分类性能。我们的结果强调了利用预训练的2D基础模型结合基于注意力的切片聚合，在医学成像中进行稳健的体积异常检测的有效性。我们的实现可在<a target="_blank" rel="noopener" href="https://github.com/Rafsani/DinoAtten3D.git%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Rafsani/DinoAtten3D.git公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12512v1">PDF</a> ACCEPTED at the ICCV 2025 Workshop on Anomaly Detection with   Foundation Models</p>
<p><strong>Summary</strong><br>医学图像异常检测与分类对早期诊断至关重要，但受限于标注数据不足、类别不均衡以及专家标注的高成本。研究提出利用预训练的DINOv2模型作为特征提取器，结合注意力机制对医学图像进行全局聚合，实现自适应切片级别的权重分配。通过结合监督对比学习和类方差正则化的复合损失函数，提高类间可分性和类内一致性。在ADNI数据集和机构多类头痛队列中验证了框架的有效性，即使在数据有限和类别不平衡的情况下也表现出强大的异常分类性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像异常检测与分类面临数据标注不足、类别不均衡及高成本标注的挑战。</li>
<li>研究提出利用预训练的DINOv2模型作为特征提取器，该模型在大量无标签数据集上进行训练，能提供通用表示。</li>
<li>结合注意力机制对医学图像进行全局聚合，实现切片级别的自适应权重分配。</li>
<li>利用复合损失函数结合监督对比学习和类方差正则化，增强模型的性能。</li>
<li>方法在ADNI数据集和机构多类头痛队列中验证了有效性。</li>
<li>在数据有限和类别不平衡的情况下，该框架表现出强大的异常分类性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12512">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12512v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12512v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12512v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12512v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12512v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12512v1/page_5_2.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Developing-an-aeroponic-smart-experimental-greenhouse-for-controlling-irrigation-and-plant-disease-detection-using-deep-learning-and-IoT"><a href="#Developing-an-aeroponic-smart-experimental-greenhouse-for-controlling-irrigation-and-plant-disease-detection-using-deep-learning-and-IoT" class="headerlink" title="Developing an aeroponic smart experimental greenhouse for controlling   irrigation and plant disease detection using deep learning and IoT"></a>Developing an aeroponic smart experimental greenhouse for controlling   irrigation and plant disease detection using deep learning and IoT</h2><p><strong>Authors:Mohammadreza Narimani, Ali Hajiahmad, Ali Moghimi, Reza Alimardani, Shahin Rafiee, Amir Hossein Mirzabe</strong></p>
<p>Controlling environmental conditions and monitoring plant status in greenhouses is critical to promptly making appropriate management decisions aimed at promoting crop production. The primary objective of this research study was to develop and test a smart aeroponic greenhouse on an experimental scale where the status of Geranium plant and environmental conditions are continuously monitored through the integration of the internet of things (IoT) and artificial intelligence (AI). An IoT-based platform was developed to control the environmental conditions of plants more efficiently and provide insights to users to make informed management decisions. In addition, we developed an AI-based disease detection framework using VGG-19, InceptionResNetV2, and InceptionV3 algorithms to analyze the images captured periodically after an intentional inoculation. The performance of the AI framework was compared with an expert’s evaluation of disease status. Preliminary results showed that the IoT system implemented in the greenhouse environment is able to publish data such as temperature, humidity, water flow, and volume of charge tanks online continuously to users and adjust the controlled parameters to provide an optimal growth environment for the plants. Furthermore, the results of the AI framework demonstrate that the VGG-19 algorithm was able to identify drought stress and rust leaves from healthy leaves with the highest accuracy, 92% among the other algorithms. </p>
<blockquote>
<p>控制温室的环境条件并监测植物生长状况，对于及时做出恰当的管理决策以促进作物生产至关重要。本研究的主要目的是开发和测试一个实验规模的智能气培温室，通过物联网（IoT）和人工智能（AI）的集成，持续监测天竺葵植物的生长状况和环境条件。开发了一个基于IoT的平台，以更有效地控制植物的环境条件，为用户提供见解，以做出明智的管理决策。此外，我们利用VGG-19、InceptionResNetV2和InceptionV3算法开发了一个基于AI的疾病检测框架，对有意接种后定期拍摄的图片进行分析。人工智能框架的性能与专家对疾病状况的评价进行了比较。初步结果表明，在温室环境中实施的IoT系统能够连续向用户发布温度、湿度、水流和储罐电量等数据，并调整控制参数，为植物提供最佳生长环境。另外，AI框架的结果显示，VGG-19算法能够最准确地识别干旱胁迫和生锈叶片与健康叶片的区别，准确率为92%，高于其他算法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12274v1">PDF</a> Author-accepted version. Presented at ASABE Annual International   Meeting (AIM) 2021 (virtual), Paper 2101252. Please cite the published   meeting paper: doi:10.13031&#x2F;aim.202101252. Minor wording and formatting   updates in this preprint</p>
<p><strong>Summary</strong><br>     本研究利用物联网和人工智能技术，开发并测试了智能气培温室，实现对温室环境条件的控制和对植物状态的持续监测。初步结果表明，该系统能够在线发布温度、湿度、水流和储罐电量等数据，并能调整控制参数，为植物生长提供最佳环境。此外，基于AI的疾病检测框架利用VGG-19算法能够准确识别健康叶片与干旱胁迫和生锈叶片，准确率最高，达92%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究开发了智能气培温室，整合了物联网和人工智能技术，用于控制温室环境条件和监测植物状态。</li>
<li>物联网系统能够连续监测并发布温室环境数据，如温度、湿度、水流和电量等。</li>
<li>物联网系统可调整控制参数，为植物提供最佳生长环境。</li>
<li>运用了VGG-19、InceptionResNetV2和InceptionV3算法构建AI疾病检测框架。</li>
<li>VGG-19算法在识别健康叶片与干旱胁迫和生锈叶片方面表现出最高准确性。</li>
<li>AI疾病检测框架的初步结果与专家评估相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12274">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12274v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12274v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12274v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12274v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12274v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12274v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="RU-Net-for-Automatic-Characterization-of-TRISO-Fuel-Cross-Sections"><a href="#RU-Net-for-Automatic-Characterization-of-TRISO-Fuel-Cross-Sections" class="headerlink" title="RU-Net for Automatic Characterization of TRISO Fuel Cross Sections"></a>RU-Net for Automatic Characterization of TRISO Fuel Cross Sections</h2><p><strong>Authors:Lu Cai, Fei Xu, Min Xian, Yalei Tang, Shoukun Sun, John Stempien</strong></p>
<p>During irradiation, phenomena such as kernel swelling and buffer densification may impact the performance of tristructural isotropic (TRISO) particle fuel. Post-irradiation microscopy is often used to identify these irradiation-induced morphologic changes. However, each fuel compact generally contains thousands of TRISO particles. Manually performing the work to get statistical information on these phenomena is cumbersome and subjective. To reduce the subjectivity inherent in that process and to accelerate data analysis, we used convolutional neural networks (CNNs) to automatically segment cross-sectional images of microscopic TRISO layers. CNNs are a class of machine-learning algorithms specifically designed for processing structured grid data. They have gained popularity in recent years due to their remarkable performance in various computer vision tasks, including image classification, object detection, and image segmentation. In this research, we generated a large irradiated TRISO layer dataset with more than 2,000 microscopic images of cross-sectional TRISO particles and the corresponding annotated images. Based on these annotated images, we used different CNNs to automatically segment different TRISO layers. These CNNs include RU-Net (developed in this study), as well as three existing architectures: U-Net, Residual Network (ResNet), and Attention U-Net. The preliminary results show that the model based on RU-Net performs best in terms of Intersection over Union (IoU). Using CNN models, we can expedite the analysis of TRISO particle cross sections, significantly reducing the manual labor involved and improving the objectivity of the segmentation results. </p>
<blockquote>
<p>在辐射过程中，核膨胀和缓冲层密度变化等现象可能会影响三结构同向性（TRISO）颗粒燃料的表现。经常采用后辐射显微镜来识别这些辐射诱导的形态学变化。然而，每个燃料块通常包含成千上万的TRISO颗粒。手动获取这些现象统计数据的工作十分繁琐且主观。为了减少这一过程中的主观性并加速数据分析，我们使用卷积神经网络（CNN）自动分割显微镜TRISO层横截面图像。卷积神经网络是专门用于处理结构化网格数据的机器学习算法的一类。近年来，它们在各种计算机视觉任务中的出色表现使其广受欢迎，包括图像分类、目标检测和图像分割。在这项研究中，我们生成了一个大型受辐射TRISO层数据集，包含超过2000张TRISO颗粒横截面微观图像和相应的注释图像。基于这些注释图像，我们使用了不同的CNN自动分割不同的TRISO层。这些CNN包括在本研究中开发的RU-Net，以及三种现有架构：U-Net、残差网络（ResNet）和注意力U-Net。初步结果表明，基于RU-Net的模型在交并比（IoU）方面表现最佳。使用CNN模型，我们可以加速TRISO颗粒横截面分析，大大减少所涉及的手动劳动，并提高分割结果的客观性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12244v1">PDF</a> </p>
<p><strong>Summary</strong><br>     采用卷积神经网络（CNN）自动分割TRISO颗粒截面图像，可加快数据分析，减少主观性。研究使用大量辐射后的TRISO层数据集，包括超过2000张微观TRISO颗粒截面图像和相应标注图像，对比不同CNN模型的分割效果，初步结果显示RU-Net模型在交并比（IoU）方面表现最佳。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在辐射过程中，现象如核肿胀和缓冲区致密化可能影响TRISO粒子燃料的性能。</li>
<li>显微镜用于观察辐射引起的形态变化。</li>
<li>TRISO燃料块中通常包含数千个TRISO颗粒，手动获取这些现象的统计数据既繁琐又主观。</li>
<li>使用卷积神经网络（CNN）自动分割TRISO颗粒截面图像以减少主观性和加速数据分析。</li>
<li>研究生成大量辐射后的TRISO层数据集，包括超过2000张微观图像和相应标注图像。</li>
<li>对比了多种CNN模型（RU-Net、U-Net、Residual Network和Attention U-Net）在分割TRISO颗粒截面图像方面的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12244">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12244v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12244v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Artificial-Intelligence-in-Breast-Cancer-Care-Transforming-Preoperative-Planning-and-Patient-Education-with-3D-Reconstruction"><a href="#Artificial-Intelligence-in-Breast-Cancer-Care-Transforming-Preoperative-Planning-and-Patient-Education-with-3D-Reconstruction" class="headerlink" title="Artificial Intelligence in Breast Cancer Care: Transforming Preoperative   Planning and Patient Education with 3D Reconstruction"></a>Artificial Intelligence in Breast Cancer Care: Transforming Preoperative   Planning and Patient Education with 3D Reconstruction</h2><p><strong>Authors:Mustafa Khanbhai, Giulia Di Nardo, Jun Ma, Vivienne Freitas, Caterina Masino, Ali Dolatabadi, Zhaoxun “Lorenz” Liu, Wey Leong, Wagner H. Souza, Amin Madani</strong></p>
<p>Effective preoperative planning requires accurate algorithms for segmenting anatomical structures across diverse datasets, but traditional models struggle with generalization. This study presents a novel machine learning methodology to improve algorithm generalization for 3D anatomical reconstruction beyond breast cancer applications. We processed 120 retrospective breast MRIs (January 2018-June 2023) through three phases: anonymization and manual segmentation of T1-weighted and dynamic contrast-enhanced sequences; co-registration and segmentation of whole breast, fibroglandular tissue, and tumors; and 3D visualization using ITK-SNAP. A human-in-the-loop approach refined segmentations using U-Mamba, designed to generalize across imaging scenarios. Dice similarity coefficient assessed overlap between automated segmentation and ground truth. Clinical relevance was evaluated through clinician and patient interviews. U-Mamba showed strong performance with DSC values of 0.97 ($\pm$0.013) for whole organs, 0.96 ($\pm$0.024) for fibroglandular tissue, and 0.82 ($\pm$0.12) for tumors on T1-weighted images. The model generated accurate 3D reconstructions enabling visualization of complex anatomical features. Clinician interviews indicated improved planning, intraoperative navigation, and decision support. Integration of 3D visualization enhanced patient education, communication, and understanding. This human-in-the-loop machine learning approach successfully generalizes algorithms for 3D reconstruction and anatomical segmentation across patient datasets, offering enhanced visualization for clinicians, improved preoperative planning, and more effective patient education, facilitating shared decision-making and empowering informed patient choices across medical applications. </p>
<blockquote>
<p>有效的术前规划需要针对多样数据集进行解剖结构分割的精确算法，但传统模型在泛化方面存在困难。本研究提出了一种新的机器学习方法来改进用于乳腺癌应用以外3D解剖重建的算法泛化能力。我们对120例回顾性乳腺MRI（2018年1月至2023年6月）进行了三个阶段处理：匿名化处理以及T1加权和动态增强序列的手动分割；对整个乳房、纤维腺组织和肿瘤进行配准和分割；使用ITK-SNAP进行3D可视化。一种人机结合的方法使用U-Mamba对分割进行细化，旨在跨成像场景进行泛化。使用迪克斯相似系数评估自动分割与真实值之间的重叠情况。通过医生和患者访谈评估临床相关性。U-Mamba表现优异，对于整体器官，DSC值为0.97（±0.013）；对于纤维腺组织，DSC值为0.96（±0.024）；对于T1加权图像上的肿瘤，DSC值为0.82（±0.12）。该模型生成了准确的3D重建，能够实现复杂解剖特征的可视化。医生访谈表明，其改进了规划、术中导航和决策支持。3D可视化的整合增强了患者教育、沟通和理解。这种人机结合机器学习的方法成功地使算法在患者数据集上进行3D重建和解剖分割泛化，为临床医生提供增强的可视化效果，改进了术前规划，并更有效地进行了患者教育，促进了共享决策制定和赋能患者在医疗应用中的知情选择。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12242v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该研究提出了一种新的人工智能算法模型，称为U-Mamba，该模型能够改善算法在多样数据集上的通用性，并用于进行三维解剖重建。通过对乳房MRI图像进行匿名化处理、手动分割动态增强序列图像等步骤，U-Mamba模型实现了对人体乳房、纤维腺组织和肿瘤进行精细分割，并使用ITK-SNAP软件进行三维可视化展示。实验结果表明，该模型具有较高的准确度，并对肿瘤病变情况有很好的鉴别能力。通过访谈临床医生和患者反馈表明，U-Mamba模型的整合可显著改善医生规划、手术导航和决策支持，同时增强患者教育、沟通和理解。该模型为医学图像领域提供了一种有效的通用算法，可广泛应用于术前规划和医患沟通等医学应用场景中。此方法的实现使术前计划更为精确有效，为后续手术治疗带来了很大帮助。综上分析可得模型适用范围广且具有卓越的可视化效果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>研究提出了一种名为U-Mamba的新型机器学习模型，用于提高算法在多样数据集上的通用性。</li>
<li>该模型经过三个阶段处理乳房MRI图像数据，包括匿名化、手动分割和三维可视化。</li>
<li>U-Mamba模型能够精细分割乳房、纤维腺组织和肿瘤，具有高准确度，其表现得到了实验验证和临床医生的肯定。此外还具有高效沟通患者的能力。这对未来医疗服务起到了巨大的促进作用。这带来了准确率高并且极具前瞻性的优秀技术。这不仅仅是技术应用领域的革新还改变了我们的工作方式并产生了良好的经济效益与社会影响。提高了医患之间的沟通效果并能进一步提升治疗效果为临床医生提供更全面的信息以便做出更准确的诊断方案为患者带来更好的医疗体验和服务效果。。这一方法使医学界在利用先进科技手段改善医疗服务方面取得了重大突破并展现了广泛的应用前景。。此外这一技术还有助于提升公众对医疗技术的认知度和接受度并推动医疗行业的持续发展。通过提高医生的专业技能和患者满意度推动了医疗行业的进步和患者的福祉促进了社会和谐与稳定。这证明了技术进步与医疗行业结合的巨大潜力及未来发展的广阔前景以及技术应用过程中对患者体验的重视和提升体现了以人为本的服务理念体现了对生命尊重的道德原则这是此项研究的深刻意义所在与临床的完美融合有助于开启医疗服务行业新篇章丰富了人类的医疗资源和实践探索填补了领域的空白为今后更深入的研究提供了一次可靠的借鉴平台并不断推动着整个医疗体系朝着更为高效智能人性化的方向发展改进服务水平和效率解决现实中遇到的问题以适应社会对高质量医疗服务的需求并且得到了社会大众的广泛认可和赞誉从而进一步推动了该领域技术的不断发展和完善最终惠及更多的患者和社会大众为人类健康事业作出更大的贡献同时展示了科研与实际应用相结合的巨大潜力及广阔前景为未来的医学发展提供了宝贵的经验和启示。以下是具体要点：</li>
</ul>
<ol>
<li>U-Mamba模型提高了算法在多样数据集上的通用性。</li>
<li>模型能够精细分割乳房结构并准确识别肿瘤。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12242">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12242v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12242v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12242v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.12242v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="The-SRG-eROSITA-All-Sky-Survey-J-Band-Follow-Up-Observations-for-Selected-High-Redshift-Galaxy-Cluster-Candidates"><a href="#The-SRG-eROSITA-All-Sky-Survey-J-Band-Follow-Up-Observations-for-Selected-High-Redshift-Galaxy-Cluster-Candidates" class="headerlink" title="The SRG&#x2F;eROSITA All-Sky Survey $J$-Band Follow-Up Observations for   Selected High-Redshift Galaxy Cluster Candidates"></a>The SRG&#x2F;eROSITA All-Sky Survey $J$-Band Follow-Up Observations for   Selected High-Redshift Galaxy Cluster Candidates</h2><p><strong>Authors:N. Zimmermann, M. Kluge, S. Grandis, T. Schrabback, F. Balzer, E. Bulbul, J. Comparat, B. Csizi, V. Ghirardini, H. Jansen, F. Kleinebreil, A. Liu, A. Merloni, M. E. Ramos-Ceja, J. Sanders, X. Zhang, P. Aschenbrenner, F. Enescu, S. Keiler, M. Märk, M. Rinner, P. Schweitzer, E. Silvestre-Rosello, L. Stepman</strong></p>
<p>We select galaxy cluster candidates from the high-redshift (BEST_Z &gt; 0.9) end of the first SRG&#x2F;eROSITA All-Sky Survey (eRASS1) galaxy cluster catalogue, for which we obtain moderately deep J-band imaging data with the OMEGA2000 camera at the 3.5m telescope of the Calar Alto Observatory. We include J-band data of four additional targets obtained with the three-channel camera at the 2m Fraunhofer telescope at the Wendelstein Observatory. We complement the new J-band photometric catalogue with forced photometry in the i- and z-bands of the tenth data release of the Legacy Survey (LSDR10) to derive the radial colour distribution around the eRASS1 clusters. Without assuming a priori to find a cluster red sequence at a specific colour, we try to find a radially weighted colour over-density to confirm the presence of high-redshift optical counterparts for the X-ray emission. We compare our confirmation with optical properties derived in earlier works based on LSDR10 data to refine the existing high-redshift cluster confirmation of eROSITA-selected clusters. We attempt to calibrate the colour-redshift-relation including the new J-band data by comparing our obtained photometric redshift estimate with the spectroscopic redshift of a confirmed, optically selected, high-redshift galaxy cluster. We confirm 9 out of 18 of the selected galaxy cluster candidates with a radial over-density of similar coloured galaxies for which we provide a photometric redshift estimate. We can report an increase in the relative colour measurement precision from 8% to 4% when including J-band data. In conclusion, our findings indicate a not insignificant spurious contaminant fraction at the high-redshift end (BEST_Z &gt; 0.9) of the eROSITA&#x2F;eRASS1 galaxy cluster catalogue, as well as it underlines the necessity for wide and deep near infrared imaging data for confirmation and characterisation of high-$z$ galaxy clusters. </p>
<blockquote>
<p>我们从SRG&#x2F;eROSITA全天空调查（eRASS1）星系团目录的高红移（BEST_Z &gt; 0.9）端选择星系团候选对象。我们利用卡拉阿托天文台3.5米望远镜上的OMEGA2000相机获得了适中的深度J波段成像数据。我们还包括了用温德斯坦天文台2米劳芬望远镜的三通道相机获得的四个额外目标的J波段数据。我们补充了第十次数据发布的遗产调查（LSDR10）的i波段和z波段的强制光度测量，以得出eRASS1星团的径向颜色分布。我们没有假定先验地在特定颜色上找到红序星团，而是试图找到径向加权颜色的密度过高以确认高红移X射线发射的光学对应物。我们将我们的确认结果与早期基于LSDR10数据的作品中的光学属性进行比较，以完善eROSITA选定星团的高红移确认。我们试图通过比较获得的光度红移估计和经确认的光学选择的高红移星系团的谱红移来校准颜色-红移关系，包括新的J波段数据。我们确认的星系团候选对象中有9个显示出类似颜色的星系径向密度过高，我们为其提供了光度红移估计。我们可以报告相对颜色测量精度从8%提高到4%，包括J波段数据时。总之，我们的研究结果表明，在eROSITA&#x2F;eRASS1星系团目录的高红移端（BEST_Z &gt; 0.9）存在不可忽视的虚假污染物比例，这也强调了广泛而深入的近红外成像数据对于高z星系团确认和特征描述的必要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11305v2">PDF</a> 24 pages, 13 figures, 8 tables, submitted to A&amp;A, changed   affiliations, added acknowledgement</p>
<p><strong>Summary</strong></p>
<p>基于SRG&#x2F;eROSITA全天空首测（eRASS1）星系团目录的高红移（BEST_Z &gt; 0.9）端，选取星系团候选对象，并对其进行了J波段成像数据观测。通过对新J波段光度目录以及Legacy Survey的i-和z-波段强制光度测量的补充，推导了eRASS1星团的径向颜色分布。尝试通过寻找径向加权颜色过密度来确认X射线发射的高红移光学对应物。通过比较新获得的J波段数据校准颜色与红移关系的光度红移估计与光谱红移的确认，成功确认了其中9个星系团候选对象。研究强调了包括J波段数据在内的近红外成像数据对高红移星系团确认和特征描述的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基于eROSITA&#x2F;eRASS1星系团目录的高红移端选取星系团候选对象进行研究。</li>
<li>通过J波段成像数据观测以及Legacy Survey的光度测量补充，推导星系团径向颜色分布。</li>
<li>尝试寻找径向加权颜色过密度来确认X射线发射的高红移光学对应物。</li>
<li>通过比较光度红移估计与光谱红移，成功确认了9个星系团候选对象。</li>
<li>J波段数据的加入提高了颜色测量的相对精度。</li>
<li>研究发现eROSITA&#x2F;eRASS1星系团目录的高红移端存在非微不足道的虚假污染物。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11305">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.11305v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.11305v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.11305v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.11305v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.11305v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Implicit-Neural-Representations-of-Intramyocardial-Motion-and-Strain"><a href="#Implicit-Neural-Representations-of-Intramyocardial-Motion-and-Strain" class="headerlink" title="Implicit Neural Representations of Intramyocardial Motion and Strain"></a>Implicit Neural Representations of Intramyocardial Motion and Strain</h2><p><strong>Authors:Andrew Bell, Yan Kit Choi, Steffen E Petersen, Andrew King, Muhummad Sohaib Nazir, Alistair A Young</strong></p>
<p>Automatic quantification of intramyocardial motion and strain from tagging MRI remains an important but challenging task. We propose a method using implicit neural representations (INRs), conditioned on learned latent codes, to predict continuous left ventricular (LV) displacement – without requiring inference-time optimisation. Evaluated on 452 UK Biobank test cases, our method achieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined error in global circumferential (2.86%) and radial (6.42%) strain compared to three deep learning baselines. In addition, our method is $\sim$380$\times$ faster than the most accurate baseline. These results highlight the suitability of INR-based models for accurate and scalable analysis of myocardial strain in large CMR datasets. </p>
<blockquote>
<p>心肌标记MRI的自动量化运动与应变仍然是一个重要且具有挑战性的任务。我们提出了一种使用基于学习潜在代码的条件隐性神经表示（INR）的方法，用于预测左心室（LV）的连续位移，而无需进行推理时间优化。在452例英国生物银行测试病例上进行了评估，我们的方法达到了最佳的跟踪精度（2.14毫米RMSE），并且在全球圆周应变（2.86%）和径向应变（6.42%）方面误差最小，与其他三种深度学习基线相比表现优异。此外，我们的方法速度比最准确的基线快约380倍。这些结果突出了基于INR的模型在大型CMR数据集中准确且可扩展地分析心肌应变的适用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09004v3">PDF</a> STACOM 2025 @ MICCAI</p>
<p><strong>Summary</strong></p>
<p>利用隐神经表征（INR）和学得潜在编码的方法预测左心室（LV）位移，无需推理时间优化，实现了自动量化心肌运动与应变分析。在452例英国生物银行测试案例中，该方法跟踪精度最佳（均方根误差为2.14mm），圆周和径向应变组合误差最低（分别为2.86%和6.42%），并且运行速度是最准确基准测试的约380倍。这些结果凸显了INR模型在大型心脏磁共振数据集中进行准确且可伸缩的心肌应变分析的适用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>隐神经表征（INR）用于预测左心室位移。</li>
<li>方法无需推理时间优化，提高了分析效率。</li>
<li>在英国生物银行测试案例中，该方法跟踪精度最佳。</li>
<li>与其他深度学习基准相比，该方法的圆周和径向应变组合误差最低。</li>
<li>该方法运行速度显著快于现有最准确的基准测试。</li>
<li>INR模型的适用性在大型心脏磁共振数据集中得到了验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09004">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.09004v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.09004v3/page_3_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Clustering-methods-for-Categorical-Time-Series-and-Sequences-A-scoping-review"><a href="#Clustering-methods-for-Categorical-Time-Series-and-Sequences-A-scoping-review" class="headerlink" title="Clustering methods for Categorical Time Series and Sequences : A scoping   review"></a>Clustering methods for Categorical Time Series and Sequences : A scoping   review</h2><p><strong>Authors:Ottavio Khalifa, Viet-Thi Tran, Alan Balendran, François Petit</strong></p>
<p>Objective: To provide an overview of clustering methods for categorical time series (CTS), a data structure commonly found in epidemiology, sociology, biology, and marketing, and to support method selection in regards to data characteristics.   Methods: We searched PubMed, Web of Science, and Google Scholar, from inception up to November 2024 to identify articles that propose and evaluate clustering techniques for CTS. Methods were classified according to three major families – distance-based, feature-based, and model-based – and assessed on their ability to handle data challenges such as variable sequence length, multivariate data, continuous time, missing data, time-invariant covariates, and large data volumes.   Results: Out of 14607 studies, we included 124 articles describing 129 methods, spanning domains such as artificial intelligence, social sciences, and epidemiology. Distance-based methods, particularly those using Optimal Matching, were most prevalent, with 56 methods. We identified 28 model-based methods, which demonstrated superior flexibility for handling complex data structures such as multivariate data, continuous time and time-invariant covariates. We also recorded 45 feature-based approaches, which were on average more scalable but less flexible. A searchable Web application was developed to facilitate method selection based on dataset characteristics ( <a target="_blank" rel="noopener" href="https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/">https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/</a> )   Discussion: While distance-based methods dominate, model-based approaches offer the richest modeling potential but are less scalable. Feature-based methods favor performance over flexibility, with limited support for complex data structures.   Conclusion: This review highlights methodological diversity and gaps in CTS clustering. The proposed typology aims to guide researchers in selecting methods for their specific use cases. </p>
<blockquote>
<p>目标：旨在为分类时间序列（CTS）的聚类方法提供概述。CTS是常见于流行病学、社会学、生物学和市场营销中的数据结构，并旨在根据数据特征支持方法选择。</p>
</blockquote>
<p>方法：我们搜索了PubMed、Web of Science和Google Scholar，从建库开始至2024年11月，以识别提出并评估CTS聚类技术的文章。方法按三大类别进行分类——基于距离的方法、基于特征的方法和基于模型的方法，并评估它们处理数据挑战的能力，例如变量序列长度、多元数据、连续时间、缺失数据、时间恒定协变量和大量数据。</p>
<p>结果：在14607项研究中，我们包括了124篇文章，描述了129种方法，涉及人工智能、社会科学和流行病学等领域。基于距离的方法，特别是使用最优匹配的方法最为普遍，共有56种方法。我们确定了28种基于模型的方法，它们在处理多元数据、连续时间和时间恒定协变量等复杂数据结构方面表现出较高的灵活性。我们还记录了45种基于特征的方法，这些方法在平均水平上更具可扩展性但灵活性较差。开发了一个可搜索的Web应用程序，以便根据数据集特征促进方法选择（<a target="_blank" rel="noopener" href="https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/%EF%BC%89%E3%80%82">https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/）。</a></p>
<p>讨论：虽然基于距离的方法占主导地位，但基于模型的方法提供了最丰富的建模潜力，但扩展性较差。基于特征的方法更侧重于性能而非灵活性，对复杂数据结构的支持有限。</p>
<p>结论：此综述突出了CTS聚类中的方法多样性和差距。提出的分类旨在指导研究人员根据其特定用例选择方法。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07885v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文概述了针对类别时间序列（CTS）数据的聚类方法，并探讨了这些聚类方法在流行病学、社会学、生物学和市场营销等领域的应用。文章介绍了距离基础、特征基础和模型基础的三种主要聚类方法，并评估了它们在处理数据挑战方面的能力。研究发现距离基础方法最为普遍，但模型基础方法在复杂数据结构处理上更具优势。同时，文章还提供了一个Web应用程序以帮助研究人员根据数据集特点选择适合的聚类方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章概述了类别时间序列（CTS）数据的聚类方法，并探讨了其在不同领域的应用。</li>
<li>介绍了三种主要的聚类方法：距离基础、特征基础和模型基础。</li>
<li>距离基础方法最为普遍，但模型基础方法在复杂数据结构处理上表现更好。</li>
<li>模型基础方法具有丰富建模潜力但扩展性较差，特征基础方法注重性能而灵活性有限。</li>
<li>文章提供了一个Web应用程序以帮助选择聚类方法。</li>
<li>研究发现距离基础方法中，使用Optimal Matching的方法尤为普遍。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07885">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.07885v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.07885v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Does-the-high-energy-AMS-02-positron-flux-originate-from-the-dark-matter-density-spikes-around-nearby-black-holes"><a href="#Does-the-high-energy-AMS-02-positron-flux-originate-from-the-dark-matter-density-spikes-around-nearby-black-holes" class="headerlink" title="Does the high-energy AMS-02 positron flux originate from the dark matter   density spikes around nearby black holes?"></a>Does the high-energy AMS-02 positron flux originate from the dark matter   density spikes around nearby black holes?</h2><p><strong>Authors:Man Ho Chan, Chak Man Lee</strong></p>
<p>Recent measurements made by the Alpha Magnetic Spectrometer (AMS) have detected accurate positron flux for energy range 1-1000 GeV. The energy spectrum can be best described by two source terms: the low-energy background diffusion term and an unknown high-energy source term. In this article, we discuss the possibility of the emission of positrons originating from dark matter annihilation in two nearby black hole X-ray binaries A0620-00 and XTE J1118+480. We show that the dark matter density spikes around these two black holes can best produce the observed AMS-02 high-energy positron flux due to dark matter annihilation with rest mass $m_{\rm DM} \approx 8000$ GeV via the $W^+W^-$ annihilation channel. This initiates a new proposal to account for the unknown high-energy source term in the AMS-02 positron spectrum. </p>
<blockquote>
<p>阿尔法磁谱仪（AMS）的最新测量结果显示，在能量范围1-1000吉电子伏特内精确检测到正电子流。能量谱的最佳描述涉及两个源项：低能背景扩散项和一个未知的高能源项。在本文中，我们讨论了来自两个邻近黑洞X射线双星A0620-00和XTE J1118+480的暗物质发射正电子的可能性。我们展示，由于暗物质通过$W^+W^-$湮灭通道进行湮灭，在这两个黑洞周围形成的暗物质密度峰值可以最好地产生观察到的AMS-02高能正电子流，其剩余质量约为$m_{\rm DM} \approx 8000$吉电子伏特。这为解释AMS-02正电子谱中的未知高能源项提出了一个新的提议。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01860v2">PDF</a> Accepted in Phys. Rev. D</p>
<p><strong>Summary</strong><br>     阿尔法磁谱仪（AMS）最新测量结果显示，能量范围在1-1000 GeV之间的正电子流量非常准确。能量谱可由两个源项描述：低能背景扩散项和未知高能源项。本文讨论来自暗物质湮灭的正电子发射可能性，特别是在两个黑洞X射线双星A0620-00和XTE J1118+480附近。由于暗物质湮灭，围绕这两个黑洞的暗物质密度峰值能够最好地产生观察到的AMS-02高能正电子流量，其中暗物质剩余质量通过$W^+W^-$湮灭通道约为8000 GeV。这为解释AMS-02正电子谱中的未知高能源项提出了新提案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Alpha Magnetic Spectrometer (AMS) 准确测量了能量范围在 1-1000 GeV 的正电子流量。</li>
<li>正电子的能量谱主要由两个源项描述：低能背景扩散和未知的高能源项。</li>
<li>讨论了来自暗物质湮灭的正电子发射的可能性。</li>
<li>两个黑洞X射线双星A0620-00和XTE J1118+480附近的暗物质密度峰值能产生观察到的AMS-02高能正电子流量。</li>
<li>暗物质剩余质量通过$W^+W^-$湮灭通道约为 8000 GeV。</li>
<li>提出了一个新的理论模型来解释AMS-02正电子谱中的未知高能源项。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01860">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.01860v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.01860v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_医学图像/2509.01860v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-18/TTS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_TTS/2509.12875v1/page_0_0.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-09-18  MSR-Codec A Low-Bitrate Multi-Stream Residual Codec for High-Fidelity   Speech Generation with Information Disentanglement
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-18/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Diffusion Models/2507.02598v2/page_2_1.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-09-18  MIA-EPT Membership Inference Attack via Error Prediction for Tabular   Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
