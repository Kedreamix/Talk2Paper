<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-09-18  GLAD Global-Local Aware Dynamic Mixture-of-Experts for Multi-Talker ASR">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12508v1/page_5_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    36 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-18-更新"><a href="#2025-09-18-更新" class="headerlink" title="2025-09-18 更新"></a>2025-09-18 更新</h1><h2 id="GLAD-Global-Local-Aware-Dynamic-Mixture-of-Experts-for-Multi-Talker-ASR"><a href="#GLAD-Global-Local-Aware-Dynamic-Mixture-of-Experts-for-Multi-Talker-ASR" class="headerlink" title="GLAD: Global-Local Aware Dynamic Mixture-of-Experts for Multi-Talker ASR"></a>GLAD: Global-Local Aware Dynamic Mixture-of-Experts for Multi-Talker ASR</h2><p><strong>Authors:Yujie Guo, Jiaming Zhou, Yuhang Jia, Shiwan Zhao, Yong Qin</strong></p>
<p>End-to-end multi-talker automatic speech recognition (MTASR) faces significant challenges in accurately transcribing overlapping speech, especially under high-overlap conditions. To address these challenges, we proposed Global-Local Aware Dynamic (GLAD) Mixture-of-Experts, which dynamically fuse speaker-aware global information and fine-grained local features to guide expert selection. This mechanism enables speaker-specific routing by leveraging both global context and local acoustic cues. Experiments on LibriSpeechMix show that GLAD outperforms existing MTASR approaches, particularly in challenging multi-talker scenarios. To our best knowledge, this is the first work to apply Mixture-of-Experts (MoE) to end-to-end MTASR with a global-local fusion strategy. Our code and train dataset can be found at <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/GLAD">https://github.com/NKU-HLT/GLAD</a>. </p>
<blockquote>
<p>端到端多说话人自动语音识别（MTASR）在准确转录重叠语音方面面临重大挑战，尤其是在高重叠条件下。为了解决这些挑战，我们提出了Global-Local Aware Dynamic（GLAD）专家混合模型，该模型能够动态融合说话人感知全局信息和精细局部特征，以指导专家选择。这种机制能够利用全局上下文和局部声学线索，实现针对说话人的路由。在LibriSpeechMix上的实验表明，GLAD优于现有的MTASR方法，特别是在具有挑战性的多说话人场景中。据我们所知，这是首次将专家混合（MoE）应用于端到端MTASR的全局-局部融合策略。我们的代码和训练数据集可在<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/GLAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NKU-HLT/GLAD找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13093v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对多说话人自动语音识别（MTASR）的挑战，提出了Global-Local Aware Dynamic（GLAD）Mixture-of-Experts方案。该方案通过融合说话者感知全局信息和精细局部特征，实现专家选择的动态融合，利用全局上下文和局部声学线索进行说话者特定路由。在LibriSpeechMix上的实验表明，GLAD在挑战性多说话人场景下优于现有MTASR方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>端到端多说话人自动语音识别（MTASR）面临准确转录重叠语音的挑战。</li>
<li>提出Global-Local Aware Dynamic（GLAD）Mixture-of-Experts方案来解决这些挑战。</li>
<li>GLAD通过融合说话者感知全局信息和精细局部特征，实现专家选择的动态融合。</li>
<li>GLAD利用全局上下文和局部声学线索进行说话者特定路由。</li>
<li>在LibriSpeechMix上的实验表明，GLAD在挑战性多说话人场景下具有优越性。</li>
<li>GLAD是首次将Mixture-of-Experts（MoE）应用于端到端MTASR的方案，采用全球本地融合策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13093">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.13093v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.13093v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.13093v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.13093v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.13093v1/page_3_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Token-based-Attractors-and-Cross-attention-in-Spoof-Diarization"><a href="#Token-based-Attractors-and-Cross-attention-in-Spoof-Diarization" class="headerlink" title="Token-based Attractors and Cross-attention in Spoof Diarization"></a>Token-based Attractors and Cross-attention in Spoof Diarization</h2><p><strong>Authors:Kyo-Won Koo, Chan-yeong Lim, Jee-weon Jung, Hye-jin Shim, Ha-Jin Yu</strong></p>
<p>Spoof diarization identifies &#96;&#96;what spoofed when” in a given speech by temporally locating spoofed regions and determining their manipulation techniques. As a first step toward this task, prior work proposed a two-branch model for localization and spoof type clustering, which laid the foundation for spoof diarization. However, its simple structure limits the ability to capture complex spoofing patterns and lacks explicit reference points for distinguishing between bona fide and various spoofing types. To address these limitations, our approach introduces learnable tokens where each token represents acoustic features of bona fide and spoofed speech. These attractors interact with frame-level embeddings to extract discriminative representations, improving separation between genuine and generated speech. Vast experiments on PartialSpoof dataset consistently demonstrate that our approach outperforms existing methods in bona fide detection and spoofing method clustering. </p>
<blockquote>
<p>欺骗语音的日记识别通过定位欺骗区域并确定其操纵技术，识别给定语音中的“何时被欺骗”。在此任务的第一步中，早期的研究提出了一个用于定位和欺骗类型聚类的两分支模型，为欺骗语音的日记识别奠定了基础。然而，其简单的结构限制了捕捉复杂欺骗模式的能力，并且在区分真实和多种欺骗类型方面缺乏明确的参考点。为了克服这些局限性，我们的方法引入了可学习的令牌，每个令牌代表真实和欺骗语音的声学特征。这些吸引器与帧级嵌入进行交互，以提取判别表示，从而提高真实和生成语音之间的分离度。在PartialSpoof数据集上的大量实验表明，我们的方法在真实检测器和欺骗方法聚类方面的性能均优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13085v1">PDF</a> Accepted to IEEE ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Spoof日记化技术的最新发展。针对先前模型在识别伪造语音时的局限性，提出一种基于学习符号（令牌）的新方法。这种方法通过引入学习令牌来改善对真实和伪造语音的辨别能力，提高了在部分伪造数据集上的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Spoof日记化技术能够识别给定语音中的伪造部分并确定其操纵技术。</li>
<li>之前的模型存在对复杂伪造模式识别能力有限和缺乏明确区分真实和伪造语音的参照点的问题。</li>
<li>新方法引入学习令牌来代表真实和伪造语音的声学特征。</li>
<li>这些令牌与帧级嵌入相互作用，提取更具区分性的表示。</li>
<li>新方法在PartialSpoof数据集上的实验表现优于现有方法，特别是在真实语音检测和伪造方法聚类方面。</li>
<li>通过改进模型结构，提高了对伪造语音的识别能力和真实与伪造语音之间的区分度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13085">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.13085v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.13085v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.13085v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.13085v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.13085v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="The-CCF-AATC-2025-Speech-Restoration-Challenge"><a href="#The-CCF-AATC-2025-Speech-Restoration-Challenge" class="headerlink" title="The CCF AATC 2025: Speech Restoration Challenge"></a>The CCF AATC 2025: Speech Restoration Challenge</h2><p><strong>Authors:Junan Zhang, Mengyao Zhu, Xin Xu, Hui Bu, Zhenhua Ling, Zhizheng Wu</strong></p>
<p>Real-world speech communication is often hampered by a variety of distortions that degrade quality and intelligibility. While many speech enhancement algorithms target specific degradations like noise or reverberation, they often fall short in realistic scenarios where multiple distortions co-exist and interact. To spur research in this area, we introduce the Speech Restoration Challenge as part of the China Computer Federation (CCF) Advanced Audio Technology Competition (AATC) 2025. This challenge focuses on restoring speech signals affected by a composite of three degradation types: (1) complex acoustic degradations including non-stationary noise and reverberation; (2) signal-chain artifacts such as those from MP3 compression; and (3) secondary artifacts introduced by other pre-processing enhancement models. We describe the challenge’s background, the design of the task, the comprehensive dataset creation methodology, and the detailed evaluation protocol, which assesses both objective performance and model complexity. Homepage: <a target="_blank" rel="noopener" href="https://ccf-aatc.org.cn/">https://ccf-aatc.org.cn/</a>. </p>
<blockquote>
<p>现实世界中的语音通信常常受到各种失真的阻碍，导致语音质量和清晰度下降。虽然许多语音增强算法针对噪声或回声等特定失真问题，但在多种失真共存并相互作用的现实场景中，这些算法往往表现不足。为了刺激该领域的研究，我们推出语音恢复挑战赛，作为2025年中国计算机联合会（CCF）先进音频技术竞赛（AATC）的一部分。本次挑战的核心是恢复受到三种类型组合影响的语音信号：（1）复杂声学失真，包括非平稳噪声和回声；（2）信号链伪影，如MP3压缩产生的伪影；（3）其他预处理增强模型引入的次要伪影。我们描述了挑战的背景、任务设计、综合数据集创建方法和详细的评估协议，该协议既评估客观性能又评估模型复杂度。主页链接：<a target="_blank" rel="noopener" href="https://ccf-aatc.org.cn/%E3%80%82">https://ccf-aatc.org.cn/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12974v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>本文介绍了中国计算机联合会（CCF）高级音频技术竞赛（AATC）2025中的语音恢复挑战。该挑战旨在解决现实语音通信中多种失真问题并存的难题，包括复杂声学失真、信号链伪迹以及由其他预处理增强模型引入的二次伪迹。文章介绍了挑战背景、任务设计、综合数据集创建方法和详细的评估协议。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音恢复挑战旨在解决现实语音通信中的多种失真问题。</li>
<li>挑战包括三种类型的失真：复杂声学失真、信号链伪迹和二次伪迹。</li>
<li>挑战作为CCF高级音频技术竞赛（AATC）2025的一部分。</li>
<li>综合数据集创建方法用于评估各种语音失真情况。</li>
<li>评估协议既考虑客观性能也考虑模型复杂性。</li>
<li>竞赛网站地址为：<a target="_blank" rel="noopener" href="https://ccf-aatc.org.cn./">https://ccf-aatc.org.cn/。</a></li>
<li>此挑战旨在促进该领域的研究发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12974">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12974v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12974v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PAC-Pronunciation-Aware-Contextualized-Large-Language-Model-based-Automatic-Speech-Recognition"><a href="#PAC-Pronunciation-Aware-Contextualized-Large-Language-Model-based-Automatic-Speech-Recognition" class="headerlink" title="PAC: Pronunciation-Aware Contextualized Large Language Model-based   Automatic Speech Recognition"></a>PAC: Pronunciation-Aware Contextualized Large Language Model-based   Automatic Speech Recognition</h2><p><strong>Authors:Li Fu, Yu Xin, Sunlu Zeng, Lu Fan, Youzheng Wu, Xiaodong He</strong></p>
<p>This paper presents a Pronunciation-Aware Contextualized (PAC) framework to address two key challenges in Large Language Model (LLM)-based Automatic Speech Recognition (ASR) systems: effective pronunciation modeling and robust homophone discrimination. Both are essential for raw or long-tail word recognition. The proposed approach adopts a two-stage learning paradigm. First, we introduce a pronunciation-guided context learning method. It employs an interleaved grapheme-phoneme context modeling strategy that incorporates grapheme-only distractors, encouraging the model to leverage phonemic cues for accurate recognition. Then, we propose a pronunciation-discriminative reinforcement learning method with perturbed label sampling to further enhance the model&#39;s ability to distinguish contextualized homophones. Experimental results on the public English Librispeech and Mandarin AISHELL-1 datasets indicate that PAC: (1) reduces relative Word Error Rate (WER) by 30.2% and 53.8% compared to pre-trained LLM-based ASR models, and (2) achieves 31.8% and 60.5% relative reductions in biased WER for long-tail words compared to strong baselines, respectively. </p>
<blockquote>
<p>本文提出了一个发音感知上下文（PAC）框架，以解决基于大型语言模型（LLM）的自动语音识别（ASR）系统中的两个关键挑战：有效的发音建模和稳健的同音字辨别。这两个对于原始或长尾词的识别都至关重要。所提出的方法采用了两阶段学习范式。首先，我们引入了一种发音引导上下文学习方法。它采用了一种交错的字母-发音上下文建模策略，该策略结合了字母特有的干扰项，鼓励模型利用音位线索进行准确识别。然后，我们提出了一种基于受扰标签采样的发音判别强化学习方法，以进一步增强模型在区分上下文同音字方面的能力。在公共英语Librispeech和汉语AISHELL-1数据集上的实验结果表明，PAC：（1）与基于预训练LLM的ASR模型相比，相对单词错误率（WER）降低了30.2%和53.8%；（2）与强大的基准模型相比，长尾词的偏向WER分别实现了31.8%和60.5%的相对降低。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12647v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种发音感知语境化（PAC）框架，以解决大型语言模型（LLM）为基础的自动语音识别（ASR）系统中的两个关键挑战：有效的发音建模和稳健的同音字辨别。该框架采用两阶段学习模式，首先引入发音引导语境学习方法，采用交织的字母-语音语境建模策略，并结合字母型干扰项，鼓励模型利用语音线索进行准确识别。然后，提出一种带有扰动标签采样的发音判别强化学习方法，进一步提高模型对语境化同音词的区分能力。在公共英语Librispeech和汉语AISHELL-1数据集上的实验结果表明，PAC相较于预训练LLM-based ASR模型，相对单词错误率（WER）降低了30.2%和53.8%，长尾词的偏差WER相对强基线分别降低了31.8%和60.5%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PAC框架旨在解决LLM-based ASR系统中的有效发音建模和同音字辨别挑战。</li>
<li>采用两阶段学习模式，包括发音引导语境学习方法和发音判别强化学习方法。</li>
<li>发音引导语境学习方法采用字母-语音语境建模策略，结合字母型干扰项，鼓励模型利用语音线索。</li>
<li>发音判别强化学习方法通过扰动标签采样进一步提高模型对语境化同音词的区分能力。</li>
<li>在Librispeech和AISHELL-1数据集上的实验结果表明，PAC相较于基线模型显著降低WER。</li>
<li>PAC框架对于长尾词的识别能力有显著提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12647">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12647v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12647v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12647v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12647v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12647v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="High-Energy-Concentration-for-Federated-Learning-in-Frequency-Domain"><a href="#High-Energy-Concentration-for-Federated-Learning-in-Frequency-Domain" class="headerlink" title="High-Energy Concentration for Federated Learning in Frequency Domain"></a>High-Energy Concentration for Federated Learning in Frequency Domain</h2><p><strong>Authors:Haozhi Shi, Weiying Xie, Hangyu Ye, Daixun Li, Jitao Ma, Leyuan Fang</strong></p>
<p>Federated Learning (FL) presents significant potential for collaborative optimization without data sharing. Since synthetic data is sent to the server, leveraging the popular concept of dataset distillation, this FL framework protects real data privacy while alleviating data heterogeneity. However, such methods are still challenged by the redundant information and noise in entire spatial-domain designs, which inevitably increases the communication burden. In this paper, we propose a novel Frequency-Domain aware FL method with high-energy concentration (FedFD) to address this problem. Our FedFD is inspired by the discovery that the discrete cosine transform predominantly distributes energy to specific regions, referred to as high-energy concentration. The principle behind FedFD is that low-energy like high-frequency components usually contain redundant information and noise, thus filtering them helps reduce communication costs and optimize performance. Our FedFD is mathematically formulated to preserve the low-frequency components using a binary mask, facilitating an optimal solution through frequency-domain distribution alignment. In particular, real data-driven synthetic classification is imposed into the loss to enhance the quality of the low-frequency components. On five image and speech datasets, FedFD achieves superior performance than state-of-the-art methods while reducing communication costs. For example, on the CIFAR-10 dataset with Dirichlet coefficient $\alpha &#x3D; 0.01$, FedFD achieves a minimum reduction of 37.78% in the communication cost, while attaining a 10.88% performance gain. </p>
<blockquote>
<p>联邦学习（FL）在无需数据共享的情况下，为协同优化提供了巨大的潜力。由于采用数据集蒸馏的流行概念，合成数据被发送到服务器，这种FL框架保护了真实数据的隐私，同时缓解了数据异质性。然而，整个空间域设计中的冗余信息和噪声给这些方法带来了挑战，这不可避免地增加了通信负担。针对这一问题，本文提出了一种新型的高能量集中频域感知联邦学习（FedFD）方法。我们的FedFD受到离散余弦变换主要将能量分布到特定区域的发现的启发，这些特定区域被称为高能量集中区域。FedFD的原理是，低能量（如高频分量）通常包含冗余信息和噪声，因此过滤它们有助于降低通信成本并优化性能。我们的FedFD通过数学公式进行表述，使用二进制掩码保留低频分量，通过频域分布对齐实现最优解。特别是，在损失中加入现实数据驱动的合成分类，以提高低频分量的质量。在五个图像和语音数据集中，FedFD在降低通信成本的同时，实现了比最新技术更优越的性能。例如，在CIFAR-10数据集上，狄利克雷系数α&#x3D;0.01的情况下，FedFD的通信成本最低降低了37.78%，同时性能提高了10.88%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12630v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于频率域感知的联邦学习（FedFD）方法，旨在解决联邦学习中冗余信息和噪声带来的通信负担问题。FedFD通过保留低频成分并过滤掉冗余的高频成分，优化了通信成本并提升了性能。在图像和语音数据集上的实验表明，FedFD相较于其他最新方法取得了更好的性能表现。例如，在CIFAR-10数据集上，相较于传统的联邦学习方法，FedFD降低了37.78%的通信成本，同时性能提升了10.88%。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>联邦学习（FL）通过合成数据实现了数据隐私保护并解决了数据异质性挑战。</li>
<li>现有方法面临整个空间域设计冗余信息和噪声的问题，增加了通信负担。</li>
<li>FedFD方法通过保留低频成分并过滤高频成分来优化通信成本和性能。</li>
<li>FedFD利用离散余弦变换的高能量集中特性，通过频率域分布对齐实现优化。</li>
<li>真实数据驱动的合成分类被纳入损失函数，提升低频成分质量。</li>
<li>在多个图像和语音数据集上的实验表明，FedFD相比现有方法表现出卓越性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12630">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12630v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12630v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12630v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12630v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multi-Modal-Embedding-based-Target-Speaker-Enhancement"><a href="#Multi-Modal-Embedding-based-Target-Speaker-Enhancement" class="headerlink" title="Multi-Modal Embedding-based Target Speaker Enhancement"></a>Multi-Modal Embedding-based Target Speaker Enhancement</h2><p><strong>Authors:Zhan Jin</strong></p>
<p>Target Speaker Extraction (TSE) is a critical challenge in cocktail party scenarios. While leveraging multiple modalities, such as voice, lip, face, and expression embeddings, can enhance performance, real-world applications often suffer from intermittent modality dropout. This paper presents a comprehensive study on the interactions and robustness of various multimodal fusion strategies under varying degrees of modality dropout. We build upon a state-of-the-art audio-visual speech enhancement system and integrate four distinct speaker identity cues: lip embeddings for synchronized contextual information, a voice speaker embedding extracted via cross-attention for acoustic consistency, a static face embedding for speaker identity, and a novel dynamic expression embedding for frame-wise emotional features. We systematically evaluate different combinations of these modalities under two key training regimes: zero dropout and 80% modality dropout. Extensive experiments demonstrate that while a full multimodal ensemble achieves optimal performance under ideal (zero dropout) conditions, its effectiveness diminishes significantly when test-time dropout occurs without prior exposure during training. Crucially, we show that training with a high (80%) modality dropout rate dramatically enhances model robustness, enabling the system to maintain superior performance even under severe test-time missing modalities. Our findings highlight that voice embeddings exhibit consistent robustness, while the proposed expression embedding provides valuable complementary information. This work underscores the importance of training strategies that account for real-world imperfection, moving beyond pure performance maximization to achieve practical reliability in multimodal speech enhancement systems. </p>
<blockquote>
<p>目标说话人提取（TSE）是鸡尾酒会场景中的一个关键挑战。虽然利用多种模式（如语音、嘴唇、面部和表情嵌入）可以提高性能，但实际应用中经常遭受间歇模式中断的影响。本文对不同多模式融合策略在不同程度模式中断下的交互和稳健性进行了全面研究。我们建立在先进的视听语音增强系统之上，并集成了四种不同的说话人身份线索：用于同步上下文信息的嘴唇嵌入、通过交叉注意提取的语音说话人嵌入以用于声音一致性、用于说话人身份的静态面部嵌入以及用于帧级情感特征的新型动态表情嵌入。我们系统地评估了这两种关键训练方案下不同模式的组合：零中断和80%模式中断。大量实验表明，在理想条件下（零中断），全模式组合达到最佳性能，但在测试时发生中断而没有事先暴露于训练中的情况下，其有效性会大大降低。关键的是，我们表明，以高（80%）模式中断率进行训练可以大大提高模型的稳健性，使系统在测试时即使面临严重的模式缺失也能保持卓越的性能。我们的研究结果表明，语音嵌入表现出持续的稳健性，而所提出的表情嵌入提供了有价值的补充信息。这项工作强调了训练策略的重要性，这些策略需要考虑现实世界的缺陷，超越纯粹的性能最大化，以实现多模式语音增强系统的实际可靠性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12583v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该论文研究了目标说话者提取（TSE）在多模态融合策略中的交互作用及稳健性，面临现实世界中的间歇性模态掉线问题。文章利用音频视觉语音增强系统，集成四种说话人身份线索，并在零掉线率和80%模态掉线率两种关键训练环境下进行系统评估。实验表明，在理想条件下，全模态组合表现最佳，但在测试时出现掉线未经训练前的暴露，其效果会显著下降。训练时的高模态掉线率能显著提高模型的稳健性，即使在测试时缺失多个模态也能保持优越性能。此外，本研究强调在训练策略中考虑现实世界的缺陷至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>目标说话者提取（TSE）在多模态融合策略中面临间歇性模态掉线问题。</li>
<li>研究利用四种不同的说话人身份线索：唇嵌入、语音嵌入、静态面部嵌入和动态表情嵌入。</li>
<li>在零掉线率和80%模态掉线率两种训练环境下进行实验评估。</li>
<li>全模态组合在理想条件下表现最佳，但面临掉线问题时光靠增加多模态融合可能并不理想。</li>
<li>训练时的高模态掉线率能提高模型的稳健性，使其能在缺失多个模态的情况下保持优越性能。</li>
<li>语音嵌入展现出持续的稳健性，而动态表情嵌入提供了有价值的补充信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12583">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12583v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12583v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12583v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12583v1/page_2_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12583v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12583v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12583v1/page_3_2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FunAudio-ASR-Technical-Report"><a href="#FunAudio-ASR-Technical-Report" class="headerlink" title="FunAudio-ASR Technical Report"></a>FunAudio-ASR Technical Report</h2><p><strong>Authors:Keyu An, Yanni Chen, Chong Deng, Changfeng Gao, Zhifu Gao, Bo Gong, Xiangang Li, Yabin Li, Xiang Lv, Yunjie Ji, Yiheng Jiang, Bin Ma, Haoneng Luo, Chongjia Ni, Zexu Pan, Yiping Peng, Zhendong Peng, Peiyao Wang, Hao Wang, Wen Wang, Wupeng Wang, Biao Tian, Zhentao Tan, Nan Yang, Bin Yuan, Jieping Ye, Jixing Yu, Qinglin Zhang, Kun Zou, Han Zhao, Shengkui Zhao, Jingren Zhou</strong></p>
<p>In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present FunAudio-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, FunAudio-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, FunAudio-ASR achieves SOTA performance on real application datasets, demonstrating its effectiveness and robustness in practical settings. </p>
<blockquote>
<p>近年来，自动语音识别（ASR）见证了由三个互补范式驱动的革命性进步：数据规模扩展、模型规模扩展以及与大型语言模型（LLM）的深度集成。然而，LLM容易出错，这在现实世界的ASR应用中可能会显著影响用户体验。在本文中，我们介绍了FunAudio-ASR，这是一个基于LLM的大规模ASR系统，它协同结合了大规模数据、大型模型容量、LLM集成和强化学习，以在多样且复杂的语音识别场景中实现最先进的性能。此外，FunAudio-ASR针对实际部署进行了专门优化，增强了流式处理能力、噪声鲁棒性、代码切换、热词定制以及其他现实应用的要求。实验结果表明，虽然大多数基于LLM的ASR系统在开源基准测试上表现强劲，但它们在真实行业评估集上往往表现不佳。由于面向生产的优化，FunAudio-ASR在真实应用数据集上实现了最新技术性能，证明了其在实际应用环境中的有效性和稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12508v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期，自动语音识别（ASR）领域在数据规模扩大、模型规模扩展以及与大型语言模型（LLM）的深度整合三大互补范式推动下取得了突破性进展。然而，LLM易产生幻觉，显著降低了真实世界ASR应用中的用户体验。本研究提出FunAudio-ASR系统，结合大规模数据、大模型容量、LLM集成和强化学习，针对多种复杂语音识别场景实现前沿性能。此外，FunAudio-ASR针对实际部署进行优化，增强流式处理、抗噪声、代码切换、热词定制等能力，满足其他真实世界应用需求。实验结果显示，大多数LLM基础的ASR系统在开源基准测试中表现良好，但在真实行业评估集上表现不佳。得益于面向生产的优化，FunAudio-ASR在真实应用数据集上实现最佳性能，证明了其在真实环境中的有效性和稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动语音识别（ASR）在数据规模、模型规模及大型语言模型（LLM）整合方面取得显著进展。</li>
<li>LLM易产生幻觉，影响真实世界ASR应用中的用户体验。</li>
<li>FunAudio-ASR系统结合多种技术实现前沿性能，包括大规模数据、大模型容量、LLM集成和强化学习。</li>
<li>FunAudio-ASR针对实际部署进行优化，包括流式处理、抗噪声、代码切换和热词定制等能力。</li>
<li>大多数LLM基础的ASR系统在开源基准测试与真实行业评估集上的表现存在差异。</li>
<li>FunAudio-ASR在真实应用数据集上实现最佳性能，显示出其在真实环境中的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12508">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12508v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12508v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12508v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12508v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="More-Similar-than-Dissimilar-Modeling-Annotators-for-Cross-Corpus-Speech-Emotion-Recognition"><a href="#More-Similar-than-Dissimilar-Modeling-Annotators-for-Cross-Corpus-Speech-Emotion-Recognition" class="headerlink" title="More Similar than Dissimilar: Modeling Annotators for Cross-Corpus   Speech Emotion Recognition"></a>More Similar than Dissimilar: Modeling Annotators for Cross-Corpus   Speech Emotion Recognition</h2><p><strong>Authors:James Tavernor, Emily Mower Provost</strong></p>
<p>Speech emotion recognition systems often predict a consensus value generated from the ratings of multiple annotators. However, these models have limited ability to predict the annotation of any one person. Alternatively, models can learn to predict the annotations of all annotators. Adapting such models to new annotators is difficult as new annotators must individually provide sufficient labeled training data. We propose to leverage inter-annotator similarity by using a model pre-trained on a large annotator population to identify a similar, previously seen annotator. Given a new, previously unseen, annotator and limited enrollment data, we can make predictions for a similar annotator, enabling off-the-shelf annotation of unseen data in target datasets, providing a mechanism for extremely low-cost personalization. We demonstrate our approach significantly outperforms other off-the-shelf approaches, paving the way for lightweight emotion adaptation, practical for real-world deployment. </p>
<blockquote>
<p>语音情绪识别系统通常会预测由多个注释器评分生成的共识值。然而，这些模型预测任何一个人的注释的能力有限。或者，模型可以学习预测所有注释器的注释。适应新注释器的模型很困难，因为新注释器必须分别提供足够的标记训练数据。我们提出利用注释器之间的相似性，通过使用在大量注释器群体上预训练的模型来识别相似的、以前见过的注释器。对于新的、以前未见过的注释器和有限的注册数据，我们可以为相似的注释器进行预测，实现对目标数据集中未见数据的即时注释，提供一种极其低成本个性化的机制。我们证明我们的方法显著优于其他即时方法，为轻量级的情绪适应铺平了道路，适用于现实世界部署。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12295v1">PDF</a> \copyright 20XX IEEE. Personal use of this material is permitted.   Permission from IEEE must be obtained for all other uses, in any current or   future media, including reprinting&#x2F;republishing this material for advertising   or promotional purposes, creating new collective works, for resale or   redistribution to servers or lists, or reuse of any copyrighted component of   this work in other works</p>
<p><strong>Summary</strong>：</p>
<p>本文提出利用预训练在大规模标注者群体上的模型来识别相似标注者，从而实现对新标注者的标注预测。通过利用标注者间的相似性，可以在新标注者提供有限标注数据的情况下，对目标数据集中的数据进行预测。此方法显著优于其他现成方法，为轻量级情感适应提供了可能，适用于实际部署场景。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>语音情感识别系统通常基于多个标注者的评分生成共识值进行预测。</li>
<li>当前模型在预测单一标注者标注方面能力有限。</li>
<li>可以学习预测所有标注者的标注，但适应新标注者时面临困难。</li>
<li>提出利用预训练模型识别相似标注者，基于大规模标注者群体数据。</li>
<li>对于新标注者，在有限数据下可通过相似标注者预测进行标注。</li>
<li>此方法显著优于其他现成方法，为情感适应提供了低成本个人化的可能性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12295">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12295v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12295v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12295v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.12295v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TSPC-A-Two-Stage-Phoneme-Centric-Architecture-for-code-switching-Vietnamese-English-Speech-Recognition"><a href="#TSPC-A-Two-Stage-Phoneme-Centric-Architecture-for-code-switching-Vietnamese-English-Speech-Recognition" class="headerlink" title="TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching   Vietnamese-English Speech Recognition"></a>TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching   Vietnamese-English Speech Recognition</h2><p><strong>Authors:Minh N. H. Nguyen, Anh Nguyen Tran, Dung Truong Dinh, Nam Van Vo</strong></p>
<p>Code-switching (CS) presents a significant challenge for general Auto-Speech Recognition (ASR) systems. Existing methods often fail to capture the subtle phonological shifts inherent in CS scenarios. The challenge is particularly difficult for language pairs like Vietnamese and English, where both distinct phonological features and the ambiguity arising from similar sound recognition are present. In this paper, we propose a novel architecture for Vietnamese-English CS ASR, a Two-Stage Phoneme-Centric model (TSPC). The TSPC employs a phoneme-centric approach, built upon an extended Vietnamese phoneme set as an intermediate representation to facilitate mixed-lingual modeling. Experimental results demonstrate that TSPC consistently outperforms existing baselines, including PhoWhisper-base, in Vietnamese-English CS ASR, achieving a significantly lower word error rate of 20.8% with reduced training resources. Furthermore, the phonetic-based two-stage architecture enables phoneme adaptation and language conversion to enhance ASR performance in complex CS Vietnamese-English ASR scenarios. </p>
<blockquote>
<p>语言转换（CS）对于通用自动语音识别（ASR）系统来说是一个重大挑战。现有方法往往无法捕捉到CS场景中固有的细微语音变化。对于越南语和英语这样的语言对来说，挑战尤其艰巨，这两种语言具有截然不同的语音特征，以及由相似声音识别产生的歧义。在本文中，我们为越南语-英语CS ASR提出了一种新型架构，即两阶段音素中心模型（TSPC）。TSPC采用音素中心法，以扩展的越南语音素集作为中间表示，促进混合语言建模。实验结果表明，TSPC在越南语-英语CS ASR中始终优于现有基线，包括PhoWhisper-base，实现了显著降低的词错误率20.8%，同时减少了训练资源。此外，基于音素的两阶段架构能够实现音素适配和语言转换，以提高复杂CS越南语-英语ASR场景中的ASR性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05983v2">PDF</a> I need to withdraw the paper as there something wrong</p>
<p><strong>总结</strong></p>
<p>本文提出一种针对越南语-英语代码切换自动语音识别（ASR）系统的两阶段音素中心模型（TSPC）。该模型采用音素中心方法，基于扩展的越南语音素集作为中间表示，以促进混合语言建模。实验结果表明，TSPC在越南语-英语代码切换ASR中持续优于现有基线，包括PhoWhisper-base，实现了显著降低的20.8%的词错误率，并在减少训练资源的情况下仍具有良好的性能。此外，基于音素的两阶段架构能够实现音素适应和语言转换，从而提高复杂越南语-英语代码切换ASR场景的识别性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>代码切换（CS）给通用自动语音识别（ASR）系统带来了挑战，特别是在越南语和英语之间，存在独特的音位特征和由于相似声音识别而产生的歧义。</li>
<li>提出了一种新的越南语-英语CS ASR系统架构——两阶段音素中心模型（TSPC）。</li>
<li>TSPC模型基于扩展的越南语音素集作为中间表示，促进混合语言建模。</li>
<li>实验结果显示TSPC在越南语-英语CS ASR中表现优异，显著优于现有基线，包括PhoWhisper-base。</li>
<li>TSPC实现了显著降低的词错误率，仅为20.8%，并在减少训练资源的情况下仍具有良好的性能。</li>
<li>基于音素的两阶段架构可实现音素适应和语言转换，适用于复杂的CS场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05983">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.05983v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.05983v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.05983v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.05983v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2509.05983v2/page_3_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Quality-Assessment-of-Noisy-and-Enhanced-Speech-with-Limited-Data-UWB-NTIS-System-for-VoiceMOS-2024"><a href="#Quality-Assessment-of-Noisy-and-Enhanced-Speech-with-Limited-Data-UWB-NTIS-System-for-VoiceMOS-2024" class="headerlink" title="Quality Assessment of Noisy and Enhanced Speech with Limited Data:   UWB-NTIS System for VoiceMOS 2024"></a>Quality Assessment of Noisy and Enhanced Speech with Limited Data:   UWB-NTIS System for VoiceMOS 2024</h2><p><strong>Authors:Marie Kunešová, Aleš Pražák, Jan Lehečka</strong></p>
<p>We present a system for non-intrusive prediction of speech quality in noisy and enhanced speech, developed for Track 3 of the VoiceMOS 2024 Challenge. The task required estimating the ITU-T P.835 metrics SIG, BAK, and OVRL without reference signals and with only 100 subjectively labeled utterances for training. Our approach uses wav2vec 2.0 with a two-stage transfer learning strategy: initial fine-tuning on automatically labeled noisy data, followed by adaptation to the challenge data. The system achieved the best performance on BAK prediction (LCC&#x3D;0.867) and a very close second place in OVRL (LCC&#x3D;0.711) in the official evaluation. Post-challenge experiments show that adding artificially degraded data to the first fine-tuning stage substantially improves SIG prediction, raising correlation with ground truth scores from 0.207 to 0.516. These results demonstrate that transfer learning with targeted data generation is effective for predicting P.835 scores under severe data constraints. </p>
<blockquote>
<p>我们呈现了一个系统，用于在噪声和增强语音中进行非侵入式语音质量预测，该系统是为VoiceMOS 2024挑战赛的第三赛道开发的。该任务要求在没有任何参考信号的情况下，仅使用100个主观标记的发音来估计ITU-T P.835指标的SIG、BAK和OVRL。我们的方法使用wav2vec 2.0和两阶段迁移学习策略：首先在自动标记的噪声数据上进行初步微调，然后适应挑战赛数据。在官方评估中，该系统在BAK预测方面表现最佳（LCC&#x3D;0.867），在OVRL（LCC&#x3D;0.711）中位列第二。挑战后的实验表明，在第一阶段微调中添加人工退化数据可以大大提高SIG预测，将其与基准真实分数的相关性从0.207提高到0.516。这些结果表明，在严格的数据约束下，使用有针对性的数据生成进行迁移学习对于预测P.835分数是有效的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00506v3">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种用于非侵入性预测噪声和增强语音质量的系统，该系统是为VoiceMOS 2024挑战赛的Track 3任务开发的。系统需在无需参考信号的情况下，仅使用100个主观标记的语音片段进行训练，估计ITU-T P.835指标的SIG、BAK和OVRL。该研究采用wav2vec 2.0与两阶段迁移学习策略：首先在自动标记的噪声数据上进行初步微调，然后适应挑战赛数据。在官方评估中，该系统在BAK预测方面表现最佳（LCC&#x3D;0.867），并在OVRL预测中取得第二名（LCC&#x3D;0.711）。实验后表明，在第一阶段微调中添加人工退化数据可以大大提高SIG预测效果，与地面真实分数的相关性从0.207提高到0.516。这些结果证明了在严格数据约束下，使用有针对性的数据生成进行迁移学习是有效的。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>系统针对非侵入性预测噪声和增强语音质量进行设计，适用于VoiceMOS 2024挑战赛Track 3任务。</li>
<li>系统在仅使用100个主观标记的语音片段进行训练的情况下，估计ITU-T P.835指标的SIG、BAK和OVRL。</li>
<li>采用wav2vec 2.0与两阶段迁移学习策略，初步微调在自动标记的噪声数据上，然后适应挑战赛数据。</li>
<li>在官方评估中，系统表现最佳的是BAK预测（LCC&#x3D;0.867），OVRL预测取得第二名（LCC&#x3D;0.711）。</li>
<li>实验表明，添加人工退化数据到第一阶段微调中可以显著提高SIG预测效果。</li>
<li>该研究证明了迁移学习在严格数据约束下的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00506">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2506.00506v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2506.00506v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2506.00506v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Speech/2506.00506v3/page_3_1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-18/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-18/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-18/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_元宇宙_虚拟人/2509.13013v1/page_3_0.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-09-18  Dream3DAvatar Text-Controlled 3D Avatar Reconstruction from a Single   Image
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-18/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_无监督_半监督_对比学习/2509.12771v1/page_2_0.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-09-18  More performant and scalable Rethinking contrastive vision-language   pre-training of radiology in the LLM era
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29885.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
