<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-09-18  The Few-shot Dilemma Over-prompting Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12387v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    37 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-18-更新"><a href="#2025-09-18-更新" class="headerlink" title="2025-09-18 更新"></a>2025-09-18 更新</h1><h2 id="The-Few-shot-Dilemma-Over-prompting-Large-Language-Models"><a href="#The-Few-shot-Dilemma-Over-prompting-Large-Language-Models" class="headerlink" title="The Few-shot Dilemma: Over-prompting Large Language Models"></a>The Few-shot Dilemma: Over-prompting Large Language Models</h2><p><strong>Authors:Yongjian Tang, Doruk Tuncel, Christian Koerner, Thomas Runkler</strong></p>
<p>Over-prompting, a phenomenon where excessive examples in prompts lead to diminished performance in Large Language Models (LLMs), challenges the conventional wisdom about in-context few-shot learning. To investigate this few-shot dilemma, we outline a prompting framework that leverages three standard few-shot selection methods - random sampling, semantic embedding, and TF-IDF vectors - and evaluate these methods across multiple LLMs, including GPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral. Our experimental results reveal that incorporating excessive domain-specific examples into prompts can paradoxically degrade performance in certain LLMs, which contradicts the prior empirical conclusion that more relevant few-shot examples universally benefit LLMs. Given the trend of LLM-assisted software engineering and requirement analysis, we experiment with two real-world software requirement classification datasets. By gradually increasing the number of TF-IDF-selected and stratified few-shot examples, we identify their optimal quantity for each LLM. This combined approach achieves superior performance with fewer examples, avoiding the over-prompting problem, thus surpassing the state-of-the-art by 1% in classifying functional and non-functional requirements. </p>
<blockquote>
<p>过度提示现象是指，在提示中过多的示例会导致大型语言模型（LLM）性能下降，这一现象挑战了关于上下文中的少量样本学习的传统智慧。为了研究这一少量的难题，我们概述了一个提示框架，该框架利用三种标准的少量样本选择方法：随机抽样、语义嵌入和TF-IDF向量，并在多个LLM上评估这些方法，包括GPT-4o、GPT-3.5-turbo、DeepSeek-V3、Gemma-3、LLaMA-3.1、LLaMA-3.2和Mistral。我们的实验结果表明，在提示中融入过多的特定领域示例可能会反常地导致某些LLM的性能下降，这与之前的经验性结论相悖，即更相关的少量示例普遍有益于LLM。考虑到大型语言模型辅助的软件工程和需求分析的趋势，我们在两个真实世界的软件需求分类数据集上进行实验。通过逐渐增加TF-IDF选择和分层选择的少量示例的数量，我们确定了每个LLM的最佳数量。这种结合方法使用较少的例子就能实现卓越的性能，避免了过度提示问题，从而在分类功能和非功能需求方面超越了最新技术状态1%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13196v1">PDF</a> accepted for the main track of FLLM</p>
<p><strong>Summary</strong></p>
<p>该文探讨了大型语言模型（LLM）中过度提示现象，即过多的示例提示会导致性能下降。研究团队提出了一个基于三种标准少样本选择方法的提示框架，并在多个LLM上进行了实验验证，包括GPT-4o、GPT-3.5-turbo等。实验结果显示，在特定LLM中，过度融入领域特定示例可能会意外导致性能下降，这与先前认为更多相关少样本示例普遍有利于LLM的经验结论相悖。针对软件工程和需求分析的LLM辅助趋势，研究团队使用两个真实软件需求分类数据集进行实验，通过逐渐增加TF-IDF精选的少样本示例数量，确定每个LLM的最佳示例数量。这种结合方法实现了使用更少示例的卓越性能，避免了过度提示问题，在分类功能和非功能需求方面超过了现有技术1%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>过度的示例提示可能会导致大型语言模型的性能下降，这种现象被称为“过度提示”。</li>
<li>研究提出了一个基于三种少样本选择方法的提示框架来应对这一挑战。</li>
<li>实验结果显示，在某些LLM中，过多的领域特定示例可能降低性能。</li>
<li>在软件需求和功能分类方面，使用逐渐增加的少样本示例数量进行实验，确定了每个LLM的最佳示例数量。</li>
<li>结合多种方法可实现使用更少示例的卓越性能，避免了过度提示问题。</li>
<li>该研究在真实软件需求分类数据集上的表现超过了现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13196">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13196v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13196v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13196v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13196v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13196v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13196v1/page_5_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Is-Meta-Learning-Out-Rethinking-Unsupervised-Few-Shot-Classification-with-Limited-Entropy"><a href="#Is-Meta-Learning-Out-Rethinking-Unsupervised-Few-Shot-Classification-with-Limited-Entropy" class="headerlink" title="Is Meta-Learning Out? Rethinking Unsupervised Few-Shot Classification   with Limited Entropy"></a>Is Meta-Learning Out? Rethinking Unsupervised Few-Shot Classification   with Limited Entropy</h2><p><strong>Authors:Yunchuan Guan, Yu Liu, Ke Zhou, Zhiqi Shen, Jenq-Neng Hwang, Serge Belongie, Lei Li</strong></p>
<p>Meta-learning is a powerful paradigm for tackling few-shot tasks. However, recent studies indicate that models trained with the whole-class training strategy can achieve comparable performance to those trained with meta-learning in few-shot classification tasks. To demonstrate the value of meta-learning, we establish an entropy-limited supervised setting for fair comparisons. Through both theoretical analysis and experimental validation, we establish that meta-learning has a tighter generalization bound compared to whole-class training. We unravel that meta-learning is more efficient with limited entropy and is more robust to label noise and heterogeneous tasks, making it well-suited for unsupervised tasks. Based on these insights, We propose MINO, a meta-learning framework designed to enhance unsupervised performance. MINO utilizes the adaptive clustering algorithm DBSCAN with a dynamic head for unsupervised task construction and a stability-based meta-scaler for robustness against label noise. Extensive experiments confirm its effectiveness in multiple unsupervised few-shot and zero-shot tasks. </p>
<blockquote>
<p>元学习是解决小样本任务的一种强大范式。然而，最近的研究表明，采用整体类训练策略训练的模型在小样分类任务上的性能可以与元学习训练的模型相媲美。为了证明元学习的价值，我们建立了熵限制监督环境，以便进行公平比较。通过理论分析和实验验证，我们得出元学习具有比整体类训练更严格的一般化边界。我们发现元学习在受限的熵下更高效，并且对于标签噪声和异类任务更具鲁棒性，因此非常适合用于无监督任务。基于这些见解，我们提出了MINO，这是一个旨在提高无监督性能的元学习框架。MINO利用自适应聚类算法DBSCAN和动态头进行无监督任务构建，以及基于稳定性的元标度因子来提高对标签噪声的鲁棒性。大量实验证明，它在多个无监督和零样本小样本任务中均有效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13185v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>在解决小样本任务时，元学习是一种强大的范式。但近期研究表明，采用全类训练策略的模型在少样本分类任务中的表现与元学习相当。为了证明元学习的价值，研究者在熵受限的监督环境中进行公平比较。通过理论分析和实验验证，发现元学习的泛化界限更紧密，且在有限熵下更高效，对标签噪声和异质任务更具鲁棒性，因此非常适合无监督任务。基于此，提出了MINO元学习框架，旨在提高无监督性能。MINO利用自适应聚类算法DBSCAN和动态头进行无监督任务构建，并采用基于稳定性的元缩放器增强对标签噪声的鲁棒性。实验证明，该框架在多类无监督小样本和零样本任务中均表现有效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>元学习是处理小样本任务的有力工具。</li>
<li>全类训练策略在某些少样本分类任务中的表现与元学习相当。</li>
<li>在熵受限的监督环境中，元学习展现出更紧密的泛化界限。</li>
<li>元学习在有限熵条件下更高效，对标签噪声和异质任务更具鲁棒性，适合无监督任务。</li>
<li>提出了MINO元学习框架，结合了自适应聚类算法DBSCAN和动态头进行无监督任务构建。</li>
<li>MINO采用稳定性基础上的元缩放器，以增强对标签噪声的抵抗能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13185">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13185v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13185v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13185v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13185v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13185v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13185v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13185v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Empowering-LLMs-with-Parameterized-Skills-for-Adversarial-Long-Horizon-Planning"><a href="#Empowering-LLMs-with-Parameterized-Skills-for-Adversarial-Long-Horizon-Planning" class="headerlink" title="Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon   Planning"></a>Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon   Planning</h2><p><strong>Authors:Sijia Cui, Shuai Xu, Aiyao He, Yanna Wang, Bo Xu</strong></p>
<p>Recent advancements in Large Language Models(LLMs) have led to the development of LLM-based AI agents. A key challenge is the creation of agents that can effectively ground themselves in complex, adversarial long-horizon environments. Existing methods mainly focus on (1) using LLMs as policies to interact with the environment through generating low-level feasible actions, and (2) utilizing LLMs to generate high-level tasks or language guides to stimulate action generation. However, the former struggles to generate reliable actions, while the latter relies heavily on expert experience to translate high-level tasks into specific action sequences. To address these challenges, we introduce the Plan with Language, Act with Parameter (PLAP) planning framework that facilitates the grounding of LLM-based agents in long-horizon environments. The PLAP method comprises three key components: (1) a skill library containing environment-specific parameterized skills, (2) a skill planner powered by LLMs, and (3) a skill executor converting the parameterized skills into executable action sequences. We implement PLAP in MicroRTS, a long-horizon real-time strategy game that provides an unfamiliar and challenging environment for LLMs. The experimental results demonstrate the effectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting outperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully crafted few-shot examples, surpasses the top-tier scripted agent, CoacAI. Additionally, we design comprehensive evaluation metrics and test 6 closed-source and 2 open-source LLMs within the PLAP framework, ultimately releasing an LLM leaderboard ranking long-horizon skill planning ability. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/AI-Research-TeamX/PLAP">https://github.com/AI-Research-TeamX/PLAP</a>. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进步推动了基于LLM的AI代理的发展。一个关键挑战是创建能够在复杂、对抗性的长期环境中有效立足的代理。现有方法主要集中在：（1）将LLM用作策略，通过与环境生成低级可行动作进行交互；（2）利用LLM生成高级任务或语言指南来刺激动作生成。然而，前者在生成可靠动作方面存在困难，而后者则严重依赖于专家经验将高级任务翻译成特定的动作序列。为了解决这些挑战，我们引入了“用语言规划，用参数执行”（PLAP）规划框架，该框架有助于基于LLM的代理在长期环境中立足。PLAP方法包含三个关键组件：（1）包含环境特定参数化技能的技能库；（2）由LLM驱动的技能规划器；（3）将参数化技能转换为可执行动作序列的技能执行器。我们在MicroRTS中实现了PLAP，这是一款长期实时策略游戏，为LLM提供了一个不熟悉且具有挑战性的环境。实验结果表明PLAP的有效性。特别是，GPT-4o在零射击设置下驱动的PLAP优于80%的基线代理，而经过精心设计的Qwen2-72B少数示例驱动的PLAP则超越了顶级脚本代理CoacAI。此外，我们设计了全面的评估指标，并在PLAP框架内测试了6个闭源和2个开源LLM，最终发布了一个长期技能规划能力的LLM排行榜。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/AI-Research-TeamX/PLAP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AI-Research-TeamX/PLAP上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13127v1">PDF</a> Accepted to IJCNN 2025</p>
<p><strong>Summary</strong></p>
<p>基于最新大型语言模型（LLM）技术的AI代理正在逐步发展，然而如何在复杂且对立的长期环境中使这些代理落地仍然是一个挑战。为了解决现有方法的局限性，研究者提出了一种名为PLAP的规划框架，它包含三个关键组件，如技能库、技能规划器和技能执行器。该框架在MicroRTS游戏中取得了良好效果，实现了在复杂的长期环境下的AI代理规划能力。<strong>Key Takeaways</strong></p>
<ul>
<li>LLMs已经发展到应用于AI代理，但在复杂的长期环境中还存在落地难题。</li>
<li>当前的主要方法包括使用LLM作为策略生成低级行动和使用LLM生成高级任务或语言指南来刺激行动生成，但存在缺陷。</li>
<li>PLAP规划框架包括三个关键组件：环境特定参数化技能的技能库、由LLM驱动的技能规划器和将参数化技能转换为可执行行动序列的技能执行器。</li>
<li>PLAP在MicroRTS游戏中的实施证明了其有效性，不同LLM的表现通过新设计的评估指标进行了测试和排名。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13127">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13127v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13127v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13127v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13127v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13127v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.13127v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Automated-Generation-of-Research-Workflows-from-Academic-Papers-A-Full-text-Mining-Framework"><a href="#Automated-Generation-of-Research-Workflows-from-Academic-Papers-A-Full-text-Mining-Framework" class="headerlink" title="Automated Generation of Research Workflows from Academic Papers: A   Full-text Mining Framework"></a>Automated Generation of Research Workflows from Academic Papers: A   Full-text Mining Framework</h2><p><strong>Authors:Heng Zhang, Chengzhi Zhang</strong></p>
<p>The automated generation of research workflows is essential for improving the reproducibility of research and accelerating the paradigm of “AI for Science”. However, existing methods typically extract merely fragmented procedural components and thus fail to capture complete research workflows. To address this gap, we propose an end-to-end framework that generates comprehensive, structured research workflows by mining full-text academic papers. As a case study in the Natural Language Processing (NLP) domain, our paragraph-centric approach first employs Positive-Unlabeled (PU) Learning with SciBERT to identify workflow-descriptive paragraphs, achieving an F1-score of 0.9772. Subsequently, we utilize Flan-T5 with prompt learning to generate workflow phrases from these paragraphs, yielding ROUGE-1, ROUGE-2, and ROUGE-L scores of 0.4543, 0.2877, and 0.4427, respectively. These phrases are then systematically categorized into data preparation, data processing, and data analysis stages using ChatGPT with few-shot learning, achieving a classification precision of 0.958. By mapping categorized phrases to their document locations in the documents, we finally generate readable visual flowcharts of the entire research workflows. This approach facilitates the analysis of workflows derived from an NLP corpus and reveals key methodological shifts over the past two decades, including the increasing emphasis on data analysis and the transition from feature engineering to ablation studies. Our work offers a validated technical framework for automated workflow generation, along with a novel, process-oriented perspective for the empirical investigation of evolving scientific paradigms. Source code and data are available at: <a target="_blank" rel="noopener" href="https://github.com/ZH-heng/research_workflow">https://github.com/ZH-heng/research_workflow</a>. </p>
<blockquote>
<p>研究工作流程的自动化生成对于提高研究的可重复性和加速“人工智能科学”范式至关重要。然而，现有方法通常仅提取零碎的流程组件，因此无法捕获完整的研究工作流程。为了弥补这一差距，我们提出了一种端到端的框架，通过挖掘全文学术论文来生成全面、结构化的研究工作流程。作为自然语言处理（NLP）领域的案例研究，我们采用以段落为中心的方法，首先使用带有SciBERT的PU（Positive-Unlabeled）学习来识别描述工作流程的段落，F1分数达到0.9772。随后，我们使用带有提示学习的Flan-T5从这些段落中生成工作流程短语，得到ROUGE-1、ROUGE-2和ROUGE-L的分数分别为0.4543、0.2877和0.4427。这些短语随后使用ChatGPT进行少量学习，系统地分类为数据准备、数据处理和数据分析三个阶段，分类精度达到0.958。通过将分类后的短语映射到文档中的位置，我们最终生成了可读的研究工作流图表。这种方法便于分析从NLP语料库中派生出的工作流程，并揭示了过去二十年中关键方法论的变化，包括数据分析越来越受到重视以及从特征工程到消融研究的转变。我们的工作提供了一个经过验证的自动化工作流程生成技术框架，以及一个用于实证调查科学范式演变的新颖、面向流程的视角。源代码和数据可在<a target="_blank" rel="noopener" href="https://github.com/ZH-heng/research_workflow%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZH-heng/research_workflow找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12955v1">PDF</a> </p>
<p><strong>Summary</strong><br>     该研究提出了一个端到端的框架，通过挖掘全文学术论文来生成全面、结构化的研究工作流程。采用自然语言处理技术，如Positive-Unlabeled Learning与Flan-T5模型，结合SciBERT与ChatGPT进行流程描述段落识别、工作流短语生成及阶段分类。最终生成可视化的研究工作流程图，为自动化生成工作流程提供技术框架与面向过程的视角。</p>
<p><strong>Key Takeaways</strong></p>
<pre><code> * 自动化生成研究工作流程是提高研究可重复性与加速“人工智能科学”范式转变的关键。
 
 * 现有方法主要提取碎片化的流程组件，无法捕捉完整的研究工作流程。
 
 * 提出一种端到端的框架，通过挖掘全文学术论文生成全面、结构化的研究工作流程。
 
 * 采用SciBERT与Positive-Unlabeled Learning识别流程描述段落，Flan-T5模型与prompt学习从段落中生成工作流短语。
 
 * 使用ChatGPT进行阶段分类，将短语分类为数据准备、数据处理与数据分析三个阶段，并实现高分类精度。
 
 * 将分类的短语映射到文档位置，生成可视化的整个研究工作流程图。
 
 * 分析显示，过去二十年中方法论上的关键转变包括数据分析的加强以及从特征工程到消融研究的过渡。
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12955">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12955v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12955v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12955v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Few-to-Big-Prototype-Expansion-Network-via-Diffusion-Learner-for-Point-Cloud-Few-shot-Semantic-Segmentation"><a href="#Few-to-Big-Prototype-Expansion-Network-via-Diffusion-Learner-for-Point-Cloud-Few-shot-Semantic-Segmentation" class="headerlink" title="Few to Big: Prototype Expansion Network via Diffusion Learner for Point   Cloud Few-shot Semantic Segmentation"></a>Few to Big: Prototype Expansion Network via Diffusion Learner for Point   Cloud Few-shot Semantic Segmentation</h2><p><strong>Authors:Qianguang Zhao, Dongli Wang, Yan Zhou, Jianxun Li, Richard Irampa</strong></p>
<p>Few-shot 3D point cloud semantic segmentation aims to segment novel categories using a minimal number of annotated support samples. While existing prototype-based methods have shown promise, they are constrained by two critical challenges: (1) Intra-class Diversity, where a prototype’s limited representational capacity fails to cover a class’s full variations, and (2) Inter-set Inconsistency, where prototypes derived from the support set are misaligned with the query feature space. Motivated by the powerful generative capability of diffusion model, we re-purpose its pre-trained conditional encoder to provide a novel source of generalizable features for expanding the prototype’s representational range. Under this setup, we introduce the Prototype Expansion Network (PENet), a framework that constructs big-capacity prototypes from two complementary feature sources. PENet employs a dual-stream learner architecture: it retains a conventional fully supervised Intrinsic Learner (IL) to distill representative features, while introducing a novel Diffusion Learner (DL) to provide rich generalizable features. The resulting dual prototypes are then processed by a Prototype Assimilation Module (PAM), which adopts a novel push-pull cross-guidance attention block to iteratively align the prototypes with the query space. Furthermore, a Prototype Calibration Mechanism (PCM) regularizes the final big capacity prototype to prevent semantic drift. Extensive experiments on the S3DIS and ScanNet datasets demonstrate that PENet significantly outperforms state-of-the-art methods across various few-shot settings. </p>
<blockquote>
<p>少量标注样本支持下的三维点云语义分割旨在使用最少的标注样本对新型类别进行分割。虽然现有的基于原型的方法已经显示出潜力，但它们受到两个关键挑战的限制：（1）类内多样性，其中原型的有限表示能力无法覆盖类的全部变化；（2）集合间的不一致性，其中从支持集中得出的原型与查询特征空间不一致。受扩散模型强大生成能力的启发，我们重新使用其预训练的条件编码器，为扩展原型的表示范围提供一种新的可泛化特征来源。在这个设定下，我们引入了原型扩展网络（PENet），这是一个从两个互补特征源构建大容量原型的框架。PENet采用双流学习者架构：它保留传统的全监督内在学习者（IL）以提炼代表性特征，同时引入新型扩散学习者（DL）以提供丰富的可泛化特征。然后，得到的双原型经过原型融合模块（PAM）处理，该模块采用新型推拉交叉引导注意力块，以迭代方式将原型与查询空间对齐。此外，原型校准机制（PCM）对最终的大容量原型进行正则化，以防止语义漂移。在S3DIS和ScanNet数据集上的大量实验表明，PENet在各种小样本设置下显著优于现有最新方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12878v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于扩散模型的Few-Shot 3D点云语义分割研究。针对现有原型方法存在的类内多样性不足和跨集不一致性问题，提出一种新型原型扩展网络（PENet）。利用扩散模型的预训练条件编码器提供可泛化的特征，通过双流学习器架构构建大容量原型，并引入原型融合模块和校准机制，实现原型与查询空间的迭代对齐。在S3DIS和ScanNet数据集上的实验表明，PENet在多种少样本设置下显著优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot 3D点云语义分割旨在使用少量标注的支持样本对新型类别进行分割。</li>
<li>现有原型方法面临两个挑战：类内多样性不足和跨集不一致性。</li>
<li>扩散模型具有强大的生成能力，其预训练条件编码器可提供可泛化的特征。</li>
<li>PENet采用双流学习器架构，结合传统内在学习器和新型扩散学习者构建大容量原型。</li>
<li>PENet引入原型融合模块和校准机制，实现原型与查询空间的迭代对齐。</li>
<li>在S3DIS和ScanNet数据集上的实验表明PENet显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12878">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12878v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12878v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12878v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multi-Robot-Task-Planning-for-Multi-Object-Retrieval-Tasks-with-Distributed-On-Site-Knowledge-via-Large-Language-Models"><a href="#Multi-Robot-Task-Planning-for-Multi-Object-Retrieval-Tasks-with-Distributed-On-Site-Knowledge-via-Large-Language-Models" class="headerlink" title="Multi-Robot Task Planning for Multi-Object Retrieval Tasks with   Distributed On-Site Knowledge via Large Language Models"></a>Multi-Robot Task Planning for Multi-Object Retrieval Tasks with   Distributed On-Site Knowledge via Large Language Models</h2><p><strong>Authors:Kento Murata, Shoichi Hasegawa, Tomochika Ishikawa, Yoshinobu Hagiwara, Akira Taniguchi, Lotfi El Hafi, Tadahiro Taniguchi</strong></p>
<p>It is crucial to efficiently execute instructions such as “Find an apple and a banana” or “Get ready for a field trip,” which require searching for multiple objects or understanding context-dependent commands. This study addresses the challenging problem of determining which robot should be assigned to which part of a task when each robot possesses different situational on-site knowledge-specifically, spatial concepts learned from the area designated to it by the user. We propose a task planning framework that leverages large language models (LLMs) and spatial concepts to decompose natural language instructions into subtasks and allocate them to multiple robots. We designed a novel few-shot prompting strategy that enables LLMs to infer required objects from ambiguous commands and decompose them into appropriate subtasks. In our experiments, the proposed method achieved 47&#x2F;50 successful assignments, outperforming random (28&#x2F;50) and commonsense-based assignment (26&#x2F;50). Furthermore, we conducted qualitative evaluations using two actual mobile manipulators. The results demonstrated that our framework could handle instructions, including those involving ad hoc categories such as “Get ready for a field trip,” by successfully performing task decomposition, assignment, sequential planning, and execution. </p>
<blockquote>
<p>执行诸如“找一个苹果和一个香蕉”或“为实地考察做好准备”等指令至关重要，这些指令需要搜索多个物体或理解上下文相关的命令。本研究解决了这样一个挑战性问题：当每个机器人拥有不同的现场情境知识——特别是用户为其指定的区域所学的空间概念时，应确定将任务中的哪部分分配给哪个机器人。我们提出了一种任务规划框架，该框架利用大型语言模型（LLM）和空间概念将自然语言指令分解为子任务并分配给多个机器人。我们设计了一种新型少提示策略，使LLM能够从模糊的命令中推断出所需物体，并将其分解为适当的子任务。在我们的实验中，该方法实现了47&#x2F;50的成功分配任务，优于随机分配（28&#x2F;50）和基于常识的分配（26&#x2F;50）。此外，我们还使用两个实际的移动操纵器进行了定性评估。结果表明，我们的框架能够处理指令，包括涉及临时性类别的指令（如“为实地考察做好准备”），通过成功进行任务分解、分配、序列规划和执行。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12838v1">PDF</a> Submitted to AROB-ISBC 2026 (Journal Track option)</p>
<p><strong>Summary</strong></p>
<p>本文研究了在执行自然语言指令时，如何根据每个机器人所掌握的不同现场情境知识来分配任务。文章提出了一种任务规划框架，利用大型语言模型和空间概念将指令分解为子任务并分配给多个机器人。通过设计一种新颖的提示策略，使语言模型能够从模糊指令中推断所需对象，并将其分解为适当的子任务。实验结果显示，该方法在任务分配上的成功率达到了47&#x2F;50，优于随机分配和常识分配的方法。此外，在实地使用两种移动操作机器人进行的定性评估也验证了该方法的有效性。它能够处理包括临时分类在内的指令，并通过任务分解、分配、顺序规划和执行来成功完成任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自然语言指令在机器人任务执行中的重要性，特别是在涉及搜索多个对象或理解上下文相关指令时。</li>
<li>当每个机器人具备不同的现场情境知识时，如何为任务分配机器人成为了一个挑战。</li>
<li>提出了一种任务规划框架，结合大型语言模型和空间概念来处理自然语言指令。</li>
<li>设计了新颖的提示策略，使语言模型能够从模糊指令中推断所需对象并分解子任务。</li>
<li>实验结果显示，该框架在任务分配上的成功率较高。</li>
<li>定性评估证明该框架能够处理各种指令，包括涉及临时分类的指令。</li>
<li>通过任务分解、分配、顺序规划和执行，该框架能够成功完成任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12838">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12838v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12838v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12838v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12838v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12838v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12838v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Evaluating-Large-Language-Models-for-Functional-and-Maintainable-Code-in-Industrial-Settings-A-Case-Study-at-ASML"><a href="#Evaluating-Large-Language-Models-for-Functional-and-Maintainable-Code-in-Industrial-Settings-A-Case-Study-at-ASML" class="headerlink" title="Evaluating Large Language Models for Functional and Maintainable Code in   Industrial Settings: A Case Study at ASML"></a>Evaluating Large Language Models for Functional and Maintainable Code in   Industrial Settings: A Case Study at ASML</h2><p><strong>Authors:Yash Mundhra, Max Valk, Maliheh Izadi</strong></p>
<p>Large language models have shown impressive performance in various domains, including code generation across diverse open-source domains. However, their applicability in proprietary industrial settings, where domain-specific constraints and code interdependencies are prevalent, remains largely unexplored. We present a case study conducted in collaboration with the leveling department at ASML to investigate the performance of LLMs in generating functional, maintainable code within a closed, highly specialized software environment.   We developed an evaluation framework tailored to ASML’s proprietary codebase and introduced a new benchmark. Additionally, we proposed a new evaluation metric, build@k, to assess whether LLM-generated code successfully compiles and integrates within real industrial repositories. We investigate various prompting techniques, compare the performance of generic and code-specific LLMs, and examine the impact of model size on code generation capabilities, using both match-based and execution-based metrics. The findings reveal that prompting techniques and model size have a significant impact on output quality, with few-shot and chain-of-thought prompting yielding the highest build success rates. The difference in performance between the code-specific LLMs and generic LLMs was less pronounced and varied substantially across different model families. </p>
<blockquote>
<p>大型语言模型在包括跨不同开源领域的代码生成在内的各种领域中都表现出了令人印象深刻的性能。然而，它们在专有工业环境中的适用性，即存在特定领域约束和代码依赖关系的环境，仍被大大忽视。我们与ASML公司的平铺部门合作进行了一项案例研究，以调查大型语言模型在封闭的、高度专业化的软件环境中生成功能性、可维护性代码的性能。我们针对ASML的专有代码库开发了一个评估框架，并引入了一个新的基准测试。此外，我们提出了一种新的评估指标build@k，以评估大型语言模型生成的代码是否能够成功编译并集成到真实的工业存储库中。我们调查了各种提示技术，比较了通用和针对代码的大型语言模型的性能，并研究了模型大小对代码生成能力的影响，使用基于匹配和基于执行的指标进行评估。研究结果表明，提示技术和模型大小对输出质量有重大影响，其中少样本和链式思维提示技术获得了最高的构建成功率。针对代码的大型语言模型和通用大型语言模型之间的性能差异并不明显，并且在不同的模型家族之间存在很大差异。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12395v1">PDF</a> Accepted in the 40th IEEE&#x2F;ACM International Conference on Automated   Software Engineering, ASE 2025 (Industry track)</p>
<p><strong>Summary</strong></p>
<p>大型语言模型在多个领域表现出强大的性能，包括跨不同开源领域的代码生成。然而，它们在专有工业环境中的适用性，尤其是存在领域特定约束和代码依赖性的情况，仍被大量探索。本研究与ASML的分级部门合作，调查了大型语言模型在封闭、高度专业化的软件环境中生成功能性强、可维护的代码的性能。研究开发了针对ASML专有代码库的评价框架，并引入了新的基准测试。此外，还提出了一种新的评估指标build@k，以评估大型语言模型生成的代码是否能在真实的工业存储库中成功编译和集成。本研究调查了各种提示技术，比较了通用和特定代码大型语言模型的性能，并探讨了模型大小对代码生成能力的影响，使用基于匹配和基于执行的指标进行评估。研究发现，提示技术和模型大小对输出质量有重大影响，其中少样本和思维链提示获得了最高的构建成功率。特定代码大型语言模型和通用大型语言模型之间的性能差异不太明显，并且在不同的模型家族中差异很大。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在专有工业环境中的代码生成性能仍待探索。</li>
<li>研究与ASML合作，评估了大型语言模型在特定软件环境中的代码生成能力。</li>
<li>开发了针对ASML专有代码库的评价框架和新的基准测试。</li>
<li>引入了新的评估指标build@k，以评估代码在实际工业存储库中的集成能力。</li>
<li>调查了各种提示技术，发现少样本和思维链提示能提高代码生成的构建成功率。</li>
<li>模型大小对代码生成能力有影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12395">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12395v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12395v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12395v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12395v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12395v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Causal-Symbolic-Meta-Learning-CSML-Inducing-Causal-World-Models-for-Few-Shot-Generalization"><a href="#Causal-Symbolic-Meta-Learning-CSML-Inducing-Causal-World-Models-for-Few-Shot-Generalization" class="headerlink" title="Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for   Few-Shot Generalization"></a>Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for   Few-Shot Generalization</h2><p><strong>Authors:Mohamed Zayaan S</strong></p>
<p>Modern deep learning models excel at pattern recognition but remain fundamentally limited by their reliance on spurious correlations, leading to poor generalization and a demand for massive datasets. We argue that a key ingredient for human-like intelligence-robust, sample-efficient learning-stems from an understanding of causal mechanisms. In this work, we introduce Causal-Symbolic Meta-Learning (CSML), a novel framework that learns to infer the latent causal structure of a task distribution. CSML comprises three key modules: a perception module that maps raw inputs to disentangled symbolic representations; a differentiable causal induction module that discovers the underlying causal graph governing these symbols and a graph-based reasoning module that leverages this graph to make predictions. By meta-learning a shared causal world model across a distribution of tasks, CSML can rapidly adapt to novel tasks, including those requiring reasoning about interventions and counterfactuals, from only a handful of examples. We introduce CausalWorld, a new physics-based benchmark designed to test these capabilities. Our experiments show that CSML dramatically outperforms state-of-the-art meta-learning and neuro-symbolic baselines, particularly on tasks demanding true causal inference. </p>
<blockquote>
<p>现代深度学习模型在模式识别方面表现出色，但从根本上仍然受限于对偶然性关联的依赖，导致泛化能力较差和对大量数据集的需求。我们认为，人类智能的关键要素——稳健、高效的样本学习，源于对因果机制的理解。在这项工作中，我们引入了因果符号元学习（CSML），这是一种新型框架，能够推断任务分布的潜在因果结构。CSML包含三个关键模块：感知模块，将原始输入映射到解耦的符号表示；可微分的因果归纳模块，发现控制这些符号的潜在因果图；以及基于图的推理模块，利用该图进行预测。通过元学习任务分布中的共享因果世界模型，CSML可以快速适应新任务，包括那些只需要少量样本就能进行干预和假设检验的任务。我们引入了CausalWorld，这是一个新的基于物理的基准测试，旨在测试这些能力。我们的实验表明，CSML在需要真正因果推理的任务上显著优于最新的元学习和神经符号基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12387v1">PDF</a> 10 pages, 4 figures</p>
<p><strong>Summary</strong><br>深度学习方法在模式识别上表现出色，但仍受限于表面相关性的依赖，导致泛化能力不佳并依赖大规模数据集。本文提出Causal-Symbolic Meta-Learning（CSML）框架，旨在学习推断任务分布的潜在因果结构。CSML包括三个关键模块：感知模块将原始输入映射到符号表示；可分化因果归纳模块发现控制这些符号的潜在因果图；基于图的推理模块利用此图进行预测。通过元学习任务分布中的共享因果世界模型，CSML可以快速适应新任务，包括需要干预和假设的任务。本文还介绍了用于测试这些能力的全新物理基准测试CausalWorld。实验表明，CSML在需要真正因果推断的任务上显著优于最新的元学习和神经符号基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现代深度学习方法受限于依赖表面相关性，导致泛化能力不佳。</li>
<li>CSML框架旨在学习推断任务分布的潜在因果结构，由三个关键模块组成。</li>
<li>CSML能够迅速适应新任务，包括需要理解和应对干预与假设的任务。</li>
<li>引入了一个新的物理基准测试CausalWorld，用于测试模型的因果推理能力。</li>
<li>实验表明，CSML在需要真正因果推断的任务上表现优异。</li>
<li>CSML通过元学习共享因果世界模型来优化模型性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12387">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12387v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12387v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12387v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2509.12387v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Random-Rule-Forest-RRF-Interpretable-Ensembles-of-LLM-Generated-Questions-for-Predicting-Startup-Success"><a href="#Random-Rule-Forest-RRF-Interpretable-Ensembles-of-LLM-Generated-Questions-for-Predicting-Startup-Success" class="headerlink" title="Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated   Questions for Predicting Startup Success"></a>Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated   Questions for Predicting Startup Success</h2><p><strong>Authors:Ben Griffin, Diego Vidaurre, Ugur Koyluoglu, Joseph Ternasky, Fuat Alican, Yigit Ihlamur</strong></p>
<p>Predicting rare outcomes such as startup success is central to venture capital, demanding models that are both accurate and interpretable. We introduce Random Rule Forest (RRF), a lightweight ensemble method that uses a large language model (LLM) to generate simple YES&#x2F;NO questions in natural language. Each question functions as a weak learner, and their responses are combined using a threshold-based voting rule to form a strong, interpretable predictor.   Applied to a dataset of 9,892 founders, RRF achieves a 6.9x improvement over a random baseline on held-out data; adding expert-crafted questions lifts this to 8x and highlights the value of human-LLM collaboration. Compared with zero- and few-shot baselines across three LLM architectures, RRF attains an F0.5 of 0.121, versus 0.086 for the best baseline (+0.035 absolute, +41% relative). By combining the creativity of LLMs with the rigor of ensemble learning, RRF delivers interpretable, high-precision predictions suitable for decision-making in high-stakes domains. </p>
<blockquote>
<p>预测创业成功等稀有结果是对风险投资至关重要的，这要求模型既要准确又要可解释。我们引入了随机规则森林（RRF），这是一种轻量级的集成方法，它使用大型语言模型（LLM）生成简单的自然语言中的是非问题。每个问题都作为一个弱学习者发挥作用，它们的回答通过基于阈值的投票规则进行组合，形成一个强大且可解释的预测器。在应用于包含9892位创始人的数据集时，RRF在保留数据上实现了相对随机基准线的6.9倍改进；添加专家设计的问题将此提升到8倍，突显了人类与LLM合作的价值。与三种LLM架构的零样本和少样本基线相比，RRF的F0.5分数为0.121，而最佳基线的分数为0.086（绝对提高0.035，相对提高41%）。通过将LLM的创造力和集成学习的严谨性相结合，RRF能够做出可解释、高精确度的预测，适合用于高风险领域的决策制定。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24622v2">PDF</a> 13 pages including appendix, 4 figures</p>
<p><strong>Summary</strong></p>
<p>随机规则森林（RRF）是一种轻量级的集成方法，它利用大型语言模型（LLM）生成简单的自然语言中的是非问题。每个问题都作为一个弱学习者，其回答通过基于阈值的投票规则组合成一个强大、可解释的预测器。应用于9892名创始人的数据集，RRF在保留数据上实现了对随机基准线的6.9倍改进；添加专家制作的问题将这个比率提高到8倍，突显了人类与LLM协作的价值。与三种LLM架构的零和少镜头基线相比，RRF的F0.5值为0.121，而最佳基线的F0.5值为0.086（绝对提升0.035，相对提升41%）。通过将LLM的创造性和集成学习的严谨性相结合，RRF提供了可解释性高、精确度高的预测，适合在高风险领域进行决策。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RRF是一种集成方法，结合LLM生成的是非问题作为弱学习者。</li>
<li>RRF通过基于阈值的投票规则组合弱学习者，形成强大且可解释的预测器。</li>
<li>在创始人数据集上的实验显示，RRF相对于随机基准有显著改善。</li>
<li>添加专家制作的问题提高了预测性能，突显了人类与LLM协作的重要性。</li>
<li>RRF相比其他零和少镜头基线有显著优势，F0.5值更高。</li>
<li>RRF结合了LLM的创造性和集成学习的严谨性，提供高精确度预测。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24622">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2505.24622v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2505.24622v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Few-Shot/2505.24622v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-18/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-18/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-18/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_I2I Translation/2509.12278v1/page_5_0.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-09-18  AREPAS Anomaly Detection in Fine-Grained Anatomy with   Reconstruction-Based Semantic Patch-Scoring
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-18/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Agent/2508.16044v2/page_0_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-09-18  Scaling Agents via Continual Pre-training
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30055.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
