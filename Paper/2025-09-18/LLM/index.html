<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-18  Scaling Agents via Continual Pre-training">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13310v1/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    66 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-18-æ›´æ–°"><a href="#2025-09-18-æ›´æ–°" class="headerlink" title="2025-09-18 æ›´æ–°"></a>2025-09-18 æ›´æ–°</h1><h2 id="Scaling-Agents-via-Continual-Pre-training"><a href="#Scaling-Agents-via-Continual-Pre-training" class="headerlink" title="Scaling Agents via Continual Pre-training"></a>Scaling Agents via Continual Pre-training</h2><p><strong>Authors:Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, Chenxiong Qian, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou</strong></p>
<p>Large language models (LLMs) have evolved into agentic systems capable of autonomous tool use and multi-step reasoning for complex problem-solving. However, post-training approaches building upon general-purpose foundation models consistently underperform in agentic tasks, particularly in open-source implementations. We identify the root cause: the absence of robust agentic foundation models forces models during post-training to simultaneously learn diverse agentic behaviors while aligning them to expert demonstrations, thereby creating fundamental optimization tensions. To this end, we are the first to propose incorporating Agentic Continual Pre-training (Agentic CPT) into the deep research agents training pipeline to build powerful agentic foundational models. Based on this approach, we develop a deep research agent model named AgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve state-of-the-art performance while retains strong tool-use ability, notably 39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å‘å±•æˆä¸ºäº†å…·æœ‰è‡ªä¸»å·¥å…·ä½¿ç”¨å’Œå¤æ‚é—®é¢˜è§£å†³èƒ½åŠ›çš„å¤šæ­¥éª¤æ¨ç†çš„ä»£ç†ç³»ç»Ÿã€‚ç„¶è€Œï¼ŒåŸºäºé€šç”¨åŸºç¡€æ¨¡å‹çš„åç»­è®­ç»ƒæ–¹æ³•åœ¨ä»£ç†ä»»åŠ¡ä¸­çš„è¡¨ç°æŒç»­ä¸ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æºå®ç°ä¸­ã€‚æˆ‘ä»¬ç¡®å®šäº†æ ¹æœ¬åŸå› ï¼šç¼ºä¹ç¨³å¥çš„ä»£ç†åŸºç¡€æ¨¡å‹è¿«ä½¿æ¨¡å‹åœ¨åç»­è®­ç»ƒè¿‡ç¨‹ä¸­åŒæ—¶å­¦ä¹ å¤šæ ·åŒ–çš„ä»£ç†è¡Œä¸ºå¹¶ä½¿å…¶ä¸ä¸“å®¶æ¼”ç¤ºä¿æŒä¸€è‡´ï¼Œä»è€Œäº§ç”Ÿäº†åŸºæœ¬çš„ä¼˜åŒ–å¼ åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºåœ¨æ·±åº¦ç ”ç©¶ä»£ç†è®­ç»ƒç®¡é“ä¸­èå…¥ä»£ç†æŒç»­é¢„è®­ç»ƒï¼ˆAgentic CPTï¼‰ï¼Œä»¥æ„å»ºå¼ºå¤§çš„ä»£ç†åŸºç¡€æ¨¡å‹ã€‚åŸºäºè¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåä¸ºAgentFounderçš„æ·±åº¦ç ”ç©¶ä»£ç†æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨10ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¯¹AgentFounder-30Bè¿›è¡Œäº†è¯„ä¼°ï¼Œå–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨BrowseComp-enä¸Šè¾¾åˆ°39.9%ï¼ŒBrowseComp-zhä¸Šè¾¾åˆ°43.3%ï¼ŒHLEä¸ŠPass@1è¾¾åˆ°31.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13310v1">PDF</a> <a target="_blank" rel="noopener" href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/">https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²è¿›åŒ–ä¸ºèƒ½å¤Ÿè¿›è¡Œè‡ªä¸»å·¥å…·ä½¿ç”¨å’Œå¤šæ­¥éª¤æ¨ç†ä»¥è§£å†³å¤æ‚é—®é¢˜çš„ä»£ç†ç³»ç»Ÿã€‚ç„¶è€Œï¼ŒåŸºäºé€šç”¨åŸºç¡€æ¨¡å‹çš„åæœŸè®­ç»ƒæ–¹æ³•åœ¨ä»£ç†ä»»åŠ¡ä¸­è¡¨ç°æŒç»­ä¸ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æºå®ç°ä¸­ã€‚é—®é¢˜çš„æ ¹æºåœ¨äºç¼ºä¹ç¨³å¥çš„ä»£ç†åŸºç¡€æ¨¡å‹ï¼Œå¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒåå¿…é¡»åŒæ—¶å­¦ä¹ å¤šç§ä»£ç†è¡Œä¸ºå¹¶ä¸ä¸“å®¶æ¼”ç¤ºå¯¹é½ï¼Œä»è€Œäº§ç”ŸåŸºæœ¬çš„ä¼˜åŒ–ç´§å¼ ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºåœ¨æ·±åº¦ç ”ç©¶ä»£ç†è®­ç»ƒç®¡é“ä¸­èå…¥ä»£ç†æŒç»­é¢„è®­ç»ƒï¼ˆAgentic CPTï¼‰ï¼Œä»¥æ„å»ºå¼ºå¤§çš„ä»£ç†åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬åŸºäºæ­¤æ–¹æ³•å¼€å‘äº†ä¸€ä¸ªåä¸ºAgentFounderçš„æ·±åº¦ç ”ç©¶ä»£ç†æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨10ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¯¹AgentFounder-30Bè¿›è¡Œäº†è¯„ä¼°ï¼Œå–å¾—äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨BrowseComp-enä¸Šè¾¾åˆ°39.9%ã€BrowseComp-zhä¸Šè¾¾åˆ°43.3%ï¼Œä»¥åŠHLEä¸ŠPass@1è¾¾åˆ°31.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²å‘å±•ä¸ºå…·æœ‰è‡ªä¸»å·¥å…·ä½¿ç”¨å’Œå¤æ‚é—®é¢˜å¤šæ­¥éª¤æ¨ç†èƒ½åŠ›çš„ä»£ç†ç³»ç»Ÿã€‚</li>
<li>åŸºäºé€šç”¨åŸºç¡€æ¨¡å‹çš„åæœŸè®­ç»ƒåœ¨ä»£ç†ä»»åŠ¡ä¸­è¡¨ç°æ¬ ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æºå®ç°ä¸­ã€‚</li>
<li>ç¼ºä¹ç¨³å¥çš„ä»£ç†åŸºç¡€æ¨¡å‹å¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒåé¢ä¸´ä¼˜åŒ–æŒ‘æˆ˜ã€‚</li>
<li>é¦–æ¬¡æå‡ºèå…¥Agentic CPTåˆ°æ·±åº¦ç ”ç©¶ä»£ç†è®­ç»ƒç®¡é“ä¸­ï¼Œæ—¨åœ¨æ„å»ºå¼ºå¤§çš„ä»£ç†åŸºç¡€æ¨¡å‹ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªåä¸ºAgentFounderçš„æ·±åº¦ç ”ç©¶ä»£ç†æ¨¡å‹ã€‚</li>
<li>AgentFounder-30Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼Œå…·æœ‰å¼ºå¤§çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13310">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13310v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13310v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13310v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13310v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-assisted-Meta-optimizer-for-Automated-Design-of-Constrained-Evolutionary-Algorithm"><a href="#Large-Language-Model-assisted-Meta-optimizer-for-Automated-Design-of-Constrained-Evolutionary-Algorithm" class="headerlink" title="Large Language Model-assisted Meta-optimizer for Automated Design of   Constrained Evolutionary Algorithm"></a>Large Language Model-assisted Meta-optimizer for Automated Design of   Constrained Evolutionary Algorithm</h2><p><strong>Authors:Xu Yang, Rui Wang, Kaiwen Li, Wenhua Li, Weixiong Huang</strong></p>
<p>Meta-black-box optimization has been significantly advanced through the use of large language models (LLMs), yet in fancy on constrained evolutionary optimization. In this work, AwesomeDE is proposed that leverages LLMs as the strategy of meta-optimizer to generate update rules for constrained evolutionary algorithm without human intervention. On the meanwhile, $RTO^2H$ framework is introduced for standardize prompt design of LLMs. The meta-optimizer is trained on a diverse set of constrained optimization problems. Key components, including prompt design and iterative refinement, are systematically analyzed to determine their impact on design quality. Experimental results demonstrate that the proposed approach outperforms existing methods in terms of computational efficiency and solution accuracy. Furthermore, AwesomeDE is shown to generalize well across distinct problem domains, suggesting its potential for broad applicability. This research contributes to the field by providing a scalable and data-driven methodology for automated constrained algorithm design, while also highlighting limitations and directions for future work. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…ƒé»‘ç®±ä¼˜åŒ–åœ¨å—é™è¿›åŒ–ä¼˜åŒ–æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæå‡ºäº†AwesomeDEï¼Œå®ƒåˆ©ç”¨LLMä½œä¸ºå…ƒä¼˜åŒ–å™¨çš„ç­–ç•¥ï¼Œä¸ºçº¦æŸè¿›åŒ–ç®—æ³•ç”Ÿæˆæ›´æ–°è§„åˆ™ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚åŒæ—¶ï¼Œä»‹ç»äº†$RTO^2H$æ¡†æ¶ï¼Œç”¨äºæ ‡å‡†åŒ–LLMçš„æç¤ºè®¾è®¡ã€‚å…ƒä¼˜åŒ–å™¨åœ¨å¤šç§çº¦æŸä¼˜åŒ–é—®é¢˜ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚ç³»ç»Ÿåœ°åˆ†æäº†å…³é”®ç»„ä»¶ï¼ŒåŒ…æ‹¬æç¤ºè®¾è®¡å’Œè¿­ä»£ä¼˜åŒ–ï¼Œä»¥ç¡®å®šå®ƒä»¬å¯¹è®¾è®¡è´¨é‡çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œæ±‚è§£ç²¾åº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒAwesomeDEåœ¨å¤šä¸ªä¸åŒçš„é—®é¢˜åŸŸä¸­éƒ½è¡¨ç°è‰¯å¥½ï¼Œè¿™è¡¨æ˜å…¶å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚æœ¬ç ”ç©¶ä¸ºè‡ªåŠ¨åŒ–çº¦æŸç®—æ³•è®¾è®¡æä¾›äº†ä¸€ç§å¯æ‰©å±•å’Œæ•°æ®é©±åŠ¨çš„æ–¹æ³•è®ºï¼ŒåŒæ—¶æŒ‡å‡ºäº†æœªæ¥çš„å±€é™æ€§å’Œç ”ç©¶æ–¹å‘ï¼Œä¸ºè¿™ä¸€é¢†åŸŸåšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13251v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…ƒé»‘ç®±ä¼˜åŒ–å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œé€šè¿‡æå‡ºAwesomeDEæ–¹æ³•ï¼Œåˆ©ç”¨LLMä½œä¸ºå…ƒä¼˜åŒ–å™¨ç”Ÿæˆè¿›åŒ–ç®—æ³•çš„æ›´æ–°è§„åˆ™ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚åŒæ—¶ï¼Œå¼•å…¥$RTO^2H$æ¡†æ¶æ ‡å‡†åŒ–LLMçš„æç¤ºè®¾è®¡ã€‚è¯¥å…ƒä¼˜åŒ–å™¨åœ¨å¤šç§çº¦æŸä¼˜åŒ–é—®é¢˜ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå¹¶å¯¹æç¤ºè®¾è®¡å’Œè¿­ä»£ä¼˜åŒ–ç­‰å…³é”®ç»„ä»¶è¿›è¡Œäº†ç³»ç»Ÿåˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œè§£ç²¾åº¦ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨ä¸åŒé—®é¢˜åŸŸä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AwesomeDEåˆ©ç”¨LLMä½œä¸ºå…ƒä¼˜åŒ–å™¨ï¼Œç”Ÿæˆè¿›åŒ–ç®—æ³•çš„æ›´æ–°è§„åˆ™ï¼Œå®ç°äº†æ— éœ€äººå·¥å¹²é¢„çš„è‡ªåŠ¨åŒ–ä¼˜åŒ–ã€‚</li>
<li>$RTO^2H$æ¡†æ¶ç”¨äºæ ‡å‡†åŒ–LLMçš„æç¤ºè®¾è®¡ï¼Œæé«˜äº†ç®—æ³•è®¾è®¡çš„æ ‡å‡†åŒ–å’Œæ•ˆç‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§çº¦æŸä¼˜åŒ–é—®é¢˜ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¢å¼ºäº†å…ƒä¼˜åŒ–å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æç¤ºè®¾è®¡å’Œè¿­ä»£ä¼˜åŒ–ç­‰å…³é”®ç»„ä»¶å¯¹è®¾è®¡è´¨é‡æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒAwesomeDEåœ¨è®¡ç®—æ•ˆç‡å’Œè§£ç²¾åº¦ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨åŒ–çº¦æŸç®—æ³•è®¾è®¡æä¾›äº†å¯ä¼¸ç¼©å’Œæ•°æ®é©±åŠ¨çš„æ–¹æ³•è®ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13251">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13251v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13251v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13251v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13251v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Donâ€™t-Forget-the-Nonlinearity-Unlocking-Activation-Functions-in-Efficient-Fine-Tuning"><a href="#Donâ€™t-Forget-the-Nonlinearity-Unlocking-Activation-Functions-in-Efficient-Fine-Tuning" class="headerlink" title="Donâ€™t Forget the Nonlinearity: Unlocking Activation Functions in   Efficient Fine-Tuning"></a>Donâ€™t Forget the Nonlinearity: Unlocking Activation Functions in   Efficient Fine-Tuning</h2><p><strong>Authors:Bo Yin, Xingyi Yang, Xinchao Wang</strong></p>
<p>Existing parameter-efficient fine-tuning (PEFT) methods primarily adapt weight matrices while keeping activation functions fixed. We introduce \textbf{NoRA}, the first PEFT framework that directly adapts nonlinear activation functions in pretrained transformer-based models. NoRA replaces fixed activations with learnable rational functions and applies structured low-rank updates to numerator and denominator coefficients, with a group-wise design that localizes adaptation and improves stability at minimal cost. On vision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceeds full fine-tuning while updating only 0.4% of parameters (0.02M), achieving accuracy gains of +0.17% and +0.27%. When combined with LoRA (\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgets by adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++ consistently improves generation quality, yielding average MMLU gains of +0.3%â€“0.8%, including +1.6% on STEM (Alpaca) and +1.3% on OpenOrca. We further show that NoRA constrains adaptation to a low-dimensional functional subspace, implicitly regularizing update magnitude and direction. These results establish activation-space tuning as a complementary and highly parameter-efficient alternative to weight-based PEFT, positioning activation functions as first-class objects for model adaptation. </p>
<blockquote>
<p>ç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ä¸»è¦é€‚åº”æƒé‡çŸ©é˜µï¼ŒåŒæ—¶ä¿æŒæ¿€æ´»å‡½æ•°å›ºå®šã€‚æˆ‘ä»¬å¼•å…¥äº†<strong>NoRA</strong>ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç›´æ¥é€‚åº”é¢„è®­ç»ƒtransformeræ¨¡å‹éçº¿æ€§æ¿€æ´»å‡½æ•°çš„PEFTæ¡†æ¶ã€‚NoRAç”¨å¯å­¦ä¹ çš„æœ‰ç†å‡½æ•°æ›¿æ¢å›ºå®šæ¿€æ´»å‡½æ•°ï¼Œå¯¹åˆ†å­å’Œåˆ†æ¯ç³»æ•°åº”ç”¨ç»“æ„åŒ–ä½ç§©æ›´æ–°ï¼Œé‡‡ç”¨åˆ†ç»„è®¾è®¡å®ç°å±€éƒ¨é€‚åº”ï¼Œå¹¶åœ¨å‡ ä¹ä¸å¢åŠ æˆæœ¬çš„æƒ…å†µä¸‹æé«˜ç¨³å®šæ€§ã€‚åœ¨CIFAR-10å’ŒCIFAR-100ä¸Šè®­ç»ƒçš„è§†è§‰transformerä¸­ï¼ŒNoRAä»…æ›´æ–°0.4%ï¼ˆå³0.02Mï¼‰çš„å‚æ•°å³å¯è¾¾åˆ°æˆ–è¶…è¿‡å®Œå…¨å¾®è°ƒçš„æ•ˆæœï¼Œå¹¶åˆ†åˆ«æé«˜äº†+0.17%å’Œ+0.27%çš„å‡†ç¡®ç‡ã€‚å½“ä¸LoRAç»“åˆæ—¶ï¼ˆ**NoRA++**ï¼‰ï¼Œåœ¨åŒ¹é…çš„è®­ç»ƒé¢„ç®—ä¸‹ï¼Œå®ƒé€šè¿‡æ·»åŠ æ›´å°‘çš„å¯è®­ç»ƒå‚æ•°ï¼Œè¶…è¶Šäº†LoRAå’ŒDoRAã€‚åœ¨LLaMA3-8BæŒ‡ä»¤è°ƒæ•´ä¸­ï¼ŒNoRA++æŒç»­æé«˜ç”Ÿæˆè´¨é‡ï¼Œå¹³å‡MMLUå¢ç›Šä¸º+0.3%~+0.8%ï¼Œå…¶ä¸­STEMï¼ˆAlpacaï¼‰ä¸Šæé«˜äº†+1.6%ï¼ŒOpenOrcaä¸Šæé«˜äº†+1.3%ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒNoRAå°†é€‚åº”é™åˆ¶åœ¨ä½ç»´å‡½æ•°å­ç©ºé—´å†…ï¼Œéšå«åœ°æ­£åˆ™åŒ–æ›´æ–°å¹…åº¦å’Œæ–¹å‘ã€‚è¿™äº›ç»“æœç¡®ç«‹äº†æ¿€æ´»ç©ºé—´è°ƒæ•´ä½œä¸ºåŸºäºæƒé‡çš„PEFTçš„äº’è¡¥å’Œé«˜åº¦å‚æ•°é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå°†æ¿€æ´»å‡½æ•°å®šä½ä¸ºæ¨¡å‹é€‚åº”çš„ä¸€æµå¯¹è±¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13240v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•â€”â€”NoRAï¼Œå®ƒç›´æ¥é€‚åº”é¢„è®­ç»ƒtransformeræ¨¡å‹ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚NoRAç”¨å¯å­¦ä¹ çš„æœ‰ç†å‡½æ•°æ›¿æ¢å›ºå®šæ¿€æ´»ï¼Œå¹¶å¯¹åˆ†å­å’Œåˆ†æ¯ç³»æ•°åº”ç”¨ç»“æ„åŒ–ä½ç§©æ›´æ–°ï¼Œä»¥å±€éƒ¨åŒ–é€‚åº”å¹¶æé«˜ç¨³å®šæ€§ï¼ŒåŒæ—¶æˆæœ¬æä½ã€‚åœ¨CIFAR-10å’ŒCIFAR-100çš„è§†è§‰è½¬æ¢å™¨è®­ç»ƒä¸Šï¼ŒNoRAåœ¨ä»…æ›´æ–°0.4%çš„å‚æ•°ï¼ˆ0.02Mï¼‰çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†ä¸å…¨å¾®è°ƒç›¸åŒ¹é…æˆ–æ›´å¥½çš„æ•ˆæœï¼Œå‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†+0.17%å’Œ+0.27%ã€‚ä¸LoRAç»“åˆï¼ˆNoRA++ï¼‰åï¼Œåœ¨åŒ¹é…çš„è®­ç»ƒé¢„ç®—ä¸‹ï¼Œé€šè¿‡æ·»åŠ æ›´å°‘çš„å¯è®­ç»ƒå‚æ•°ï¼Œå®ƒä¼˜äºLoRAå’ŒDoRAã€‚åœ¨LLaMA3-8BæŒ‡ä»¤è°ƒæ•´ä¸­ï¼ŒNoRA++æŒç»­æé«˜ç”Ÿæˆè´¨é‡ï¼Œå¹³å‡MMLUå¢ç›Šä¸º+0.3%~+0.8%ï¼Œå…¶ä¸­STEMï¼ˆAlpacaï¼‰å’ŒOpenOrcaåˆ†åˆ«æé«˜äº†+1.6%å’Œ+1.3%ã€‚æ­¤å¤–ï¼ŒNoRAå°†é€‚åº”é™åˆ¶åœ¨ä½ç»´å‡½æ•°å­ç©ºé—´ä¸­ï¼Œéšå¼åœ°æ­£åˆ™åŒ–æ›´æ–°å¹…åº¦å’Œæ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NoRAæ˜¯ä¸€ç§æ–°çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œä¸“æ³¨äºé€‚åº”é¢„è®­ç»ƒtransformeræ¨¡å‹ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚</li>
<li>NoRAé€šè¿‡ç”¨å¯å­¦ä¹ çš„æœ‰ç†å‡½æ•°æ›¿æ¢å›ºå®šæ¿€æ´»ï¼Œä»¥åŠåº”ç”¨ç»“æ„åŒ–ä½ç§©æ›´æ–°æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨CIFAR-10å’ŒCIFAR-100çš„è§†è§‰è½¬æ¢å™¨è®­ç»ƒä¸­ï¼ŒNoRAå®ç°äº†è¾ƒé«˜çš„å‡†ç¡®ç‡æå‡ï¼ŒåŒæ—¶å‚æ•°æ›´æ–°æå°‘ã€‚</li>
<li>NoRA++æ˜¯NoRAä¸LoRAçš„ç»“åˆï¼Œå®ƒåœ¨åŒ¹é…çš„è®­ç»ƒé¢„ç®—ä¸‹è¡¨ç°æ›´ä½³ã€‚</li>
<li>åœ¨LLaMA3-8BæŒ‡ä»¤è°ƒæ•´ä¸­ï¼ŒNoRA++æ˜¾è‘—æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚</li>
<li>NoRAå°†æ¨¡å‹é€‚åº”é™åˆ¶åœ¨ä½ç»´å‡½æ•°å­ç©ºé—´ä¸­ï¼Œéšå¼åœ°æ§åˆ¶æ›´æ–°å¹…åº¦å’Œæ–¹å‘ã€‚</li>
<li>æ¿€æ´»å‡½æ•°åœ¨æ¨¡å‹é€‚åº”ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œæ˜¯å‚æ•°é«˜æ•ˆå¾®è°ƒçš„ä¸€ç§é‡è¦æ‰‹æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13240v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13240v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13240v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="The-Few-shot-Dilemma-Over-prompting-Large-Language-Models"><a href="#The-Few-shot-Dilemma-Over-prompting-Large-Language-Models" class="headerlink" title="The Few-shot Dilemma: Over-prompting Large Language Models"></a>The Few-shot Dilemma: Over-prompting Large Language Models</h2><p><strong>Authors:Yongjian Tang, Doruk Tuncel, Christian Koerner, Thomas Runkler</strong></p>
<p>Over-prompting, a phenomenon where excessive examples in prompts lead to diminished performance in Large Language Models (LLMs), challenges the conventional wisdom about in-context few-shot learning. To investigate this few-shot dilemma, we outline a prompting framework that leverages three standard few-shot selection methods - random sampling, semantic embedding, and TF-IDF vectors - and evaluate these methods across multiple LLMs, including GPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral. Our experimental results reveal that incorporating excessive domain-specific examples into prompts can paradoxically degrade performance in certain LLMs, which contradicts the prior empirical conclusion that more relevant few-shot examples universally benefit LLMs. Given the trend of LLM-assisted software engineering and requirement analysis, we experiment with two real-world software requirement classification datasets. By gradually increasing the number of TF-IDF-selected and stratified few-shot examples, we identify their optimal quantity for each LLM. This combined approach achieves superior performance with fewer examples, avoiding the over-prompting problem, thus surpassing the state-of-the-art by 1% in classifying functional and non-functional requirements. </p>
<blockquote>
<p>è¿‡åº¦æç¤ºç°è±¡ï¼Œå³åœ¨æç¤ºä¸­æä¾›è¿‡å¤šä¾‹å­å¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½ä¸‹é™ï¼Œè¿™ä¸€ç°è±¡å¯¹ä¼ ç»Ÿçš„ä¸Šä¸‹æ–‡å†…å°‘é‡æ ·æœ¬å­¦ä¹ æ™ºæ…§æå‡ºäº†æŒ‘æˆ˜ã€‚ä¸ºäº†ç ”ç©¶è¿™ä¸€å°‘é‡æ ·æœ¬å›°å¢ƒï¼Œæˆ‘ä»¬æ¦‚è¿°äº†ä¸€ä¸ªæç¤ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸‰ç§æ ‡å‡†çš„å°‘é‡æ ·æœ¬é€‰æ‹©æ–¹æ³•â€”â€”éšæœºæŠ½æ ·ã€è¯­ä¹‰åµŒå…¥å’ŒTF-IDFå‘é‡ï¼Œå¹¶å¯¹å¤šä¸ªLLMè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬GPT-4oã€GPT-3.5-turboã€DeepSeek-V3ã€Gemma-3ã€LLaMA-3.1ã€LLaMA-3.2å’ŒMistralã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æç¤ºä¸­èå…¥è¿‡å¤šçš„ç‰¹å®šé¢†åŸŸä¾‹å­å¯èƒ½ä¼šåœ¨æŸäº›LLMä¸­é€‚å¾—å…¶åï¼Œè¿™ä¸ä¹‹å‰çš„ç»éªŒç»“è®ºç›¸æ‚–ï¼Œå³æ›´å¤šçš„ç›¸å…³å°‘é‡æ ·æœ¬æ™®éå¯¹LLMæœ‰ç›Šã€‚è€ƒè™‘åˆ°LLMè¾…åŠ©çš„è½¯ä»¶å·¥ç¨‹å’Œéœ€æ±‚åˆ†æè¶‹åŠ¿ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œçš„è½¯ä»¶éœ€æ±‚åˆ†ç±»æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚é€šè¿‡é€æ¸å¢åŠ TF-IDFé€‰æ‹©å’Œåˆ†å±‚é€‰æ‹©çš„å°‘é‡æ ·æœ¬æ•°é‡ï¼Œæˆ‘ä»¬ç¡®å®šäº†æ¯ä¸ªLLMçš„æœ€ä½³æ•°é‡ã€‚è¿™ç§ç»“åˆæ–¹æ³•ä½¿ç”¨è¾ƒå°‘çš„ä¾‹å­å°±èƒ½å®ç°å“è¶Šæ€§èƒ½ï¼Œé¿å…äº†è¿‡åº¦æç¤ºé—®é¢˜ï¼Œä»è€Œåœ¨åˆ†ç±»åŠŸèƒ½å’ŒéåŠŸèƒ½éœ€æ±‚æ–¹é¢è¶…è¶Šäº†æœ€æ–°æŠ€æœ¯1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13196v1">PDF</a> accepted for the main track of FLLM</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è¿‡åº¦æç¤ºçš„ç°è±¡ï¼Œå³è¿‡å¤šçš„ç¤ºä¾‹å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªæç¤ºæ¡†æ¶ï¼Œåˆ©ç”¨éšæœºæŠ½æ ·ã€è¯­ä¹‰åµŒå…¥å’ŒTF-IDFå‘é‡ä¸‰ç§æ ‡å‡†æ–¹æ³•é€‰æ‹©ç¤ºä¾‹ï¼Œå¹¶åœ¨å¤šä¸ªLLMä¸Šè¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æŸäº›LLMä¸­ï¼ŒåŠ å…¥è¿‡å¤šçš„é¢†åŸŸç‰¹å®šç¤ºä¾‹ä¼šé€‚å¾—å…¶åï¼Œä¸å…ˆå‰çš„ç»“è®ºç›¸åï¼Œæ›´å¤šç›¸å…³çš„ç¤ºä¾‹å¹¶ä¸æ€»æ˜¯èƒ½æé«˜LLMçš„æ€§èƒ½ã€‚é’ˆå¯¹è½¯ä»¶å·¥ç¨‹å’Œéœ€æ±‚åˆ†æçš„LLMè¾…åŠ©è¶‹åŠ¿ï¼Œç ”ç©¶å›¢é˜Ÿä½¿ç”¨çœŸå®è½¯ä»¶éœ€æ±‚åˆ†ç±»æ•°æ®é›†è¿›è¡Œå®éªŒï¼Œé€šè¿‡é€æ­¥å¢åŠ TF-IDFé€‰æ‹©å’Œåˆ†å±‚é€‰æ‹©çš„ç¤ºä¾‹æ•°é‡ï¼Œæ‰¾åˆ°äº†æ¯ä¸ªLLMçš„æœ€ä½³ç¤ºä¾‹æ•°é‡ã€‚è¯¥æ–¹æ³•åœ¨åˆ†ç±»åŠŸèƒ½æ€§å’ŒéåŠŸèƒ½æ€§éœ€æ±‚æ–¹é¢å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œè§£å†³äº†è¿‡åº¦æç¤ºé—®é¢˜ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‡é‡æç¤ºç°è±¡ï¼šåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œè¿‡å¤šçš„ç¤ºä¾‹å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æ¢ç©¶æ–¹æ³•ï¼šç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªåŸºäºéšæœºæŠ½æ ·ã€è¯­ä¹‰åµŒå…¥å’ŒTF-IDFå‘é‡çš„æç¤ºæ¡†æ¶ã€‚</li>
<li>LLMè¯„ä¼°ï¼šå®éªŒåœ¨å¤šä¸ªLLMä¸Šè¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ã€‚</li>
<li>ä¸ä¼ ç»Ÿè§‚ç‚¹ç›¸æ‚–çš„å‘ç°ï¼šåŠ å…¥è¿‡å¤šçš„é¢†åŸŸç‰¹å®šç¤ºä¾‹å¯èƒ½é€‚å¾—å…¶åï¼Œå¹¶éè¶Šå¤šè¶Šå¥½ã€‚</li>
<li>å®é™…åº”ç”¨ç ”ç©¶ï¼šä½¿ç”¨çœŸå®è½¯ä»¶éœ€æ±‚åˆ†ç±»æ•°æ®é›†è¿›è¡Œå®éªŒï¼Œä»¥è§£å†³è½¯ä»¶å·¥ç¨‹å’Œéœ€æ±‚åˆ†æçš„LLMè¾…åŠ©é—®é¢˜ã€‚</li>
<li>æœ€ä½³å®è·µï¼šé€šè¿‡é€æ­¥å¢åŠ TF-IDFé€‰æ‹©å’Œåˆ†å±‚é€‰æ‹©çš„ç¤ºä¾‹æ•°é‡ï¼Œæ‰¾åˆ°äº†æ¯ä¸ªLLMçš„æœ€ä½³ç¤ºä¾‹æ•°é‡ï¼Œè§£å†³äº†è¿‡åº¦æç¤ºé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13196v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13196v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13196v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13196v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13196v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13196v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="More-performant-and-scalable-Rethinking-contrastive-vision-language-pre-training-of-radiology-in-the-LLM-era"><a href="#More-performant-and-scalable-Rethinking-contrastive-vision-language-pre-training-of-radiology-in-the-LLM-era" class="headerlink" title="More performant and scalable: Rethinking contrastive vision-language   pre-training of radiology in the LLM era"></a>More performant and scalable: Rethinking contrastive vision-language   pre-training of radiology in the LLM era</h2><p><strong>Authors:Yingtai Li, Haoran Lai, Xiaoqian Zhou, Shuai Ming, Wenxin Ma, Wei Wei, Shaohua Kevin Zhou</strong></p>
<p>The emergence of Large Language Models (LLMs) presents unprecedented opportunities to revolutionize medical contrastive vision-language pre-training. In this paper, we show how LLMs can facilitate large-scale supervised pre-training, thereby advancing vision-language alignment. We begin by demonstrate that modern LLMs can automatically extract diagnostic labels from radiology reports with remarkable precision (&gt;96% AUC in our experiments) without complex prompt engineering, enabling the creation of large-scale â€œsilver-standardâ€ datasets at a minimal cost (~$3 for 50k CT image-report pairs). Further, we find that vision encoder trained on this â€œsilver-standardâ€ dataset achieves performance comparable to those trained on labels extracted by specialized BERT-based models, thereby democratizing the access to large-scale supervised pre-training. Building on this foundation, we proceed to reveal that supervised pre-training fundamentally improves contrastive vision-language alignment. Our approach achieves state-of-the-art performance using only a 3D ResNet-18 with vanilla CLIP training, including 83.8% AUC for zero-shot diagnosis on CT-RATE, 77.3% AUC on RAD-ChestCT, and substantial improvements in cross-modal retrieval (MAP@50&#x3D;53.7% for image-image, Recall@100&#x3D;52.2% for report-image). These results demonstrate the potential of utilizing LLMs to facilitate {\bf more performant and scalable} medical AI systems. Our code is avaiable at <a target="_blank" rel="noopener" href="https://github.com/SadVoxel/More-performant-and-scalable">https://github.com/SadVoxel/More-performant-and-scalable</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ä¸ºåŒ»å­¦å¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒå¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„æœºä¼šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†LLMå¦‚ä½•ä¿ƒè¿›å¤§è§„æ¨¡ç›‘ç£é¢„è®­ç»ƒï¼Œä»è€Œä¿ƒè¿›è§†è§‰è¯­è¨€å¯¹é½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜ç°ä»£LLMèƒ½å¤Ÿè‡ªåŠ¨ä»æ”¾å°„å­¦æŠ¥å‘Šä¸­æå–è¯Šæ–­æ ‡ç­¾ï¼Œå…¶ç²¾ç¡®åº¦ä»¤äººå°è±¡æ·±åˆ»ï¼ˆåœ¨æˆ‘ä»¬çš„å®éªŒä¸­AUCå€¼å¤§äº96%ï¼‰ï¼Œæ— éœ€å¤æ‚çš„æç¤ºå·¥ç¨‹ï¼Œä»è€Œä»¥æœ€ä½çš„æˆæœ¬åˆ›å»ºå¤§è§„æ¨¡â€œé“¶æ ‡å‡†â€æ•°æ®é›†ï¼ˆæ¯å¯¹CTå›¾åƒæŠ¥å‘Šæˆæœ¬çº¦3ç¾å…ƒï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°åœ¨æ­¤â€œé“¶æ ‡å‡†â€æ•°æ®é›†ä¸Šè®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ä¸ç”¨åŸºäºBERTçš„æ¨¡å‹æå–çš„æ ‡ç­¾è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨çš„æ€§èƒ½ç›¸å½“ï¼Œä»è€Œå®ç°å¤§è§„æ¨¡ç›‘ç£é¢„è®­ç»ƒçš„æ™®åŠã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ­ç¤ºç›‘ç£é¢„è®­ç»ƒä»æ ¹æœ¬ä¸Šæ”¹å–„äº†å¯¹æ¯”è§†è§‰è¯­è¨€å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨æ ‡å‡†çš„CLIPè®­ç»ƒå’Œç®€å•çš„3D ResNet-18å°±è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬CT-RATEä¸Šçš„é›¶æ ·æœ¬è¯Šæ–­AUCä¸º83.8%ï¼ŒRAD-ChestCTä¸Šçš„AUCä¸º77.3%ï¼Œè·¨æ¨¡æ€æ£€ç´¢ä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ï¼ˆå›¾åƒ-å›¾åƒMAP@50ä¸º53.7%ï¼ŒæŠ¥å‘Š-å›¾åƒRecall@100ä¸º52.2%ï¼‰ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†åˆ©ç”¨LLMä¿ƒè¿›æ›´å…·æ€§èƒ½å’Œå¯æ‰©å±•æ€§çš„åŒ»å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SadVoxel/More-performant-and-scalable%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SadVoxel/More-performant-and-scalableä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13175v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong><br>     å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ä¸ºåŒ»å­¦å¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒå¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„æœºä¼šã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMèƒ½å¤Ÿä¿ƒè¿›å¤§è§„æ¨¡ç›‘ç£é¢„è®­ç»ƒï¼Œè¿›è€Œæ¨åŠ¨è§†è§‰è¯­è¨€å¯¹é½çš„å‘å±•ã€‚é€šè¿‡è‡ªåŠ¨ä»æ”¾å°„å­¦æŠ¥å‘Šä¸­æå–è¯Šæ–­æ ‡ç­¾ï¼ŒLLMèƒ½å¤Ÿåˆ›å»ºå¤§è§„æ¨¡â€œé“¶æ ‡å‡†â€æ•°æ®é›†ï¼Œé™ä½æˆæœ¬ã€‚æ­¤å¤–ï¼ŒåŸºäºè¿™äº›â€œé“¶æ ‡å‡†â€æ•°æ®é›†çš„è§†è§‰ç¼–ç å™¨æ€§èƒ½ä¸ç”¨ä¸“ä¸šBERTæ¨¡å‹æå–çš„æ ‡ç­¾è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ç›¸å½“ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å‘ç°ç›‘ç£é¢„è®­ç»ƒèƒ½ä»æ ¹æœ¬ä¸Šæ”¹å–„å¯¹æ¯”è§†è§‰è¯­è¨€å¯¹é½ã€‚åœ¨CT-RATEä¸Šå®ç°äº†é›¶æ ·æœ¬è¯Šæ–­çš„AUCå€¼ä¸º83.8%ï¼ŒRAD-ChestCTä¸Šçš„AUCå€¼ä¸º77.3%ï¼Œè·¨æ¨¡æ€æ£€ç´¢ä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ã€‚è¿™æ˜¾ç¤ºäº†åˆ©ç”¨LLMæ„å»ºæ›´é«˜æ•ˆã€å¯æ‰©å±•çš„åŒ»å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºåŒ»å­¦å¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒå¸¦æ¥äº†é©å‘½æ€§æœºä¼šã€‚</li>
<li>LLMèƒ½å¤Ÿè‡ªåŠ¨ä»æ”¾å°„å­¦æŠ¥å‘Šä¸­æå–è¯Šæ–­æ ‡ç­¾ï¼Œåˆ›å»ºå¤§è§„æ¨¡çš„â€œé“¶æ ‡å‡†â€æ•°æ®é›†ï¼Œå¹¶ä¸”è¿™ä¸€è¿‡ç¨‹çš„æˆæœ¬è¾ƒä½ã€‚</li>
<li>åŸºäºè¿™äº›â€œé“¶æ ‡å‡†â€æ•°æ®é›†çš„è§†è§‰ç¼–ç å™¨æ€§èƒ½ä¸ç”¨ä¸“ä¸šæ¨¡å‹è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ç›¸å½“ã€‚</li>
<li>ç›‘ç£é¢„è®­ç»ƒèƒ½å¤Ÿæ”¹å–„å¯¹æ¯”è§†è§‰è¯­è¨€å¯¹é½ã€‚</li>
<li>LLMåœ¨é›¶æ ·æœ¬è¯Šæ–­ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨CT-RATEå’ŒRAD-ChestCTä¸Šçš„AUCå€¼è¾ƒé«˜ã€‚</li>
<li>LLMè¿˜æ˜¾è‘—æ”¹å–„äº†è·¨æ¨¡æ€æ£€ç´¢çš„æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨LLMæ„å»ºçš„åŒ»å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿå…·æœ‰æ›´é«˜çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13175v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13175v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13175v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13175v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="WHU-STree-A-Multi-modal-Benchmark-Dataset-for-Street-Tree-Inventory"><a href="#WHU-STree-A-Multi-modal-Benchmark-Dataset-for-Street-Tree-Inventory" class="headerlink" title="WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory"></a>WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory</h2><p><strong>Authors:Ruifei Ding, Zhe Chen, Wen Fan, Chen Long, Huijuan Xiao, Yelu Zeng, Zhen Dong, Bisheng Yang</strong></p>
<p>Street trees are vital to urban livability, providing ecological and social benefits. Establishing a detailed, accurate, and dynamically updated street tree inventory has become essential for optimizing these multifunctional assets within space-constrained urban environments. Given that traditional field surveys are time-consuming and labor-intensive, automated surveys utilizing Mobile Mapping Systems (MMS) offer a more efficient solution. However, existing MMS-acquired tree datasets are limited by small-scale scene, limited annotation, or single modality, restricting their utility for comprehensive analysis. To address these limitations, we introduce WHU-STree, a cross-city, richly annotated, and multi-modal urban street tree dataset. Collected across two distinct cities, WHU-STree integrates synchronized point clouds and high-resolution images, encompassing 21,007 annotated tree instances across 50 species and 2 morphological parameters. Leveraging the unique characteristics, WHU-STree concurrently supports over 10 tasks related to street tree inventory. We benchmark representative baselines for two key tasksâ€“tree species classification and individual tree segmentation. Extensive experiments and in-depth analysis demonstrate the significant potential of multi-modal data fusion and underscore cross-domain applicability as a critical prerequisite for practical algorithm deployment. In particular, we identify key challenges and outline potential future works for fully exploiting WHU-STree, encompassing multi-modal fusion, multi-task collaboration, cross-domain generalization, spatial pattern learning, and Multi-modal Large Language Model for street tree asset management. The WHU-STree dataset is accessible at: <a target="_blank" rel="noopener" href="https://github.com/WHU-USI3DV/WHU-STree">https://github.com/WHU-USI3DV/WHU-STree</a>. </p>
<blockquote>
<p>è¡—é“æ ‘æœ¨å¯¹åŸå¸‚å®œå±…æ€§è‡³å…³é‡è¦ï¼Œèƒ½æä¾›ç”Ÿæ€å’Œç¤¾ä¼šæ•ˆç›Šã€‚åœ¨ç©ºé—´å—é™çš„åŸå¸‚ç¯å¢ƒä¸­ä¼˜åŒ–è¿™äº›å¤šåŠŸèƒ½èµ„äº§æ—¶ï¼Œå»ºç«‹è¯¦ç»†ã€å‡†ç¡®ã€åŠ¨æ€æ›´æ–°çš„è¡—é“æ ‘æœ¨æ¸…å•å·²å˜å¾—è‡³å…³é‡è¦ã€‚ç”±äºä¼ ç»Ÿå®åœ°è°ƒæŸ¥è€—æ—¶ä¸”åŠ³åŠ›å¯†é›†ï¼Œåˆ©ç”¨ç§»åŠ¨æµ‹é‡ç³»ç»Ÿ(MMS)çš„è‡ªåŠ¨åŒ–è°ƒæŸ¥æä¾›äº†æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰MMSè·å–çš„æ ‘æœ¨æ•°æ®é›†å—é™äºå°è§„æ¨¡åœºæ™¯ã€æœ‰é™æ³¨é‡Šæˆ–å•ä¸€æ¨¡å¼ï¼Œé™åˆ¶äº†å…¶è¿›è¡Œç»¼åˆåˆ†æçš„å®ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†WHU-STreeæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨åŸå¸‚çš„ã€æ³¨é‡Šä¸°å¯Œçš„ã€å¤šæ¨¡æ€çš„åŸå¸‚è¡—é“æ ‘æœ¨æ•°æ®é›†ã€‚WHU-STreeæ˜¯åœ¨ä¸¤ä¸ªä¸åŒåŸå¸‚æ”¶é›†çš„ï¼Œèåˆäº†åŒæ­¥ç‚¹äº‘å’Œé«˜åˆ†è¾¨ç‡å›¾åƒï¼ŒåŒ…å«50ä¸ªç‰©ç§çš„21,007ä¸ªæ³¨é‡Šæ ‘æœ¨å®ä¾‹å’Œ2ä¸ªå½¢æ€å‚æ•°ã€‚åˆ©ç”¨ç‹¬ç‰¹çš„ç‰¹ç‚¹ï¼ŒWHU-STreeåŒæ—¶æ”¯æŒè¶…è¿‡10ä¸ªä¸è¡—é“æ ‘æœ¨æ¸…å•ç›¸å…³çš„ä»»åŠ¡ã€‚æˆ‘ä»¬ä¸ºä¸¤ä¸ªå…³é”®ä»»åŠ¡â€”â€”æ ‘ç§åˆ†ç±»å’Œå•æ ªæ ‘æœ¨åˆ†å‰²â€”â€”è®¾å®šäº†åŸºå‡†çº¿ã€‚å¤§é‡çš„å®éªŒå’Œæ·±å…¥åˆ†æè¯æ˜äº†å¤šæ¨¡æ€æ•°æ®èåˆçš„æ˜¾è‘—æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒäº†è·¨åŸŸé€‚ç”¨æ€§ä½œä¸ºå®é™…åº”ç”¨ç®—æ³•éƒ¨ç½²çš„å…³é”®å…ˆå†³æ¡ä»¶ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ç¡®å®šäº†å……åˆ†åˆ©ç”¨WHU-STreeçš„ä¸»è¦æŒ‘æˆ˜ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥çš„å¯èƒ½å·¥ä½œæ–¹å‘ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€èåˆã€å¤šä»»åŠ¡åä½œã€è·¨åŸŸæ³›åŒ–ã€ç©ºé—´æ¨¡å¼å­¦ä¹ ä»¥åŠç”¨äºè¡—é“æ ‘æœ¨èµ„äº§ç®¡ç†çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚WHU-STreeæ•°æ®é›†å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/WHU-USI3DV/WHU-STree">https://github.com/WHU-USI3DV/WHU-STree</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13172v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¡—é“æ ‘æœ¨å¯¹åŸå¸‚å®œå±…æ€§è‡³å…³é‡è¦ï¼Œæä¾›ç”Ÿæ€å’Œç¤¾ä¼šæ•ˆç›Šã€‚å»ºç«‹è¯¦ç»†ã€å‡†ç¡®ã€åŠ¨æ€æ›´æ–°çš„è¡—é“æ ‘æœ¨æ¸…å•ï¼Œå¯¹äºä¼˜åŒ–ç©ºé—´å—é™çš„åŸå¸‚ç¯å¢ƒä¸­çš„å¤šåŠŸèƒ½èµ„äº§è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿç°åœºè°ƒæŸ¥è€—æ—¶è€—åŠ›ï¼Œè€Œé‡‡ç”¨ç§»åŠ¨æµ‹ç»˜ç³»ç»Ÿï¼ˆMMSï¼‰çš„è‡ªåŠ¨åŒ–è°ƒæŸ¥æä¾›äº†æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰MMSè·å¾—çš„æ ‘æœ¨æ•°æ®é›†å­˜åœ¨åœºæ™¯è§„æ¨¡å°ã€æ ‡æ³¨æœ‰é™æˆ–å•ä¸€æ¨¡æ€ç­‰é™åˆ¶ï¼Œéš¾ä»¥è¿›è¡Œå…¨é¢åˆ†æã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†WHU-STreeæ•°æ®é›†ï¼Œå®ƒæ˜¯ä¸€ä¸ªè·¨åŸå¸‚çš„ä¸°å¯Œæ ‡æ³¨ã€å¤šæ¨¡æ€çš„è¡—é“æ ‘æœ¨æ•°æ®é›†ã€‚WHU-STreeæ•´åˆäº†ç‚¹äº‘å’Œé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œæ¶µç›–ä¸¤ä¸ªä¸åŒåŸå¸‚çš„21,007ä¸ªæ ‡æ³¨æ ‘æœ¨å®ä¾‹ï¼Œæ¶‰åŠ50ä¸ªæ ‘ç§å’Œ2ä¸ªå½¢æ€å‚æ•°ã€‚è¯¥æ•°æ®é›†æ”¯æŒè¶…è¿‡10ä¸ªä¸è¡—é“æ ‘æœ¨æ¸…å•ç›¸å…³çš„ä»»åŠ¡ã€‚æˆ‘ä»¬å¯¹ä¸¤ä¸ªå…³é”®ä»»åŠ¡â€”â€”æ ‘ç§åˆ†ç±»å’Œå•æ ªæ ‘æœ¨åˆ†å‰²â€”â€”è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒå’Œåˆ†æè¡¨æ˜å¤šæ¨¡æ€æ•°æ®èåˆçš„æ˜¾è‘—æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒè·¨åŸŸé€‚ç”¨æ€§å¯¹äºå®é™…ç®—æ³•éƒ¨ç½²çš„å…³é”®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡—é“æ ‘æœ¨å¯¹åŸå¸‚çš„ç”Ÿæ€å’Œç¤¾ä¼šæ•ˆç›Šè‡³å…³é‡è¦ï¼Œéœ€è¦å»ºç«‹è¯¦ç»†ã€å‡†ç¡®ã€åŠ¨æ€æ›´æ–°çš„è¡—é“æ ‘æœ¨æ¸…å•æ¥ä¼˜åŒ–åŸå¸‚ä¸­çš„å¤šåŠŸèƒ½èµ„äº§ã€‚</li>
<li>ä¼ ç»Ÿç°åœºè°ƒæŸ¥æ–¹æ³•å­˜åœ¨æ—¶é—´æ¶ˆè€—å’ŒåŠ³åŠ¨åŠ›éœ€æ±‚å¤§çš„é—®é¢˜ï¼Œè€Œç§»åŠ¨æµ‹ç»˜ç³»ç»Ÿï¼ˆMMSï¼‰çš„è‡ªåŠ¨åŒ–è°ƒæŸ¥æ–¹æ³•æä¾›äº†æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰çš„MMSæ ‘æœ¨æ•°æ®é›†å­˜åœ¨å±€é™æ€§ï¼Œå¦‚åœºæ™¯è§„æ¨¡å°ã€æ ‡æ³¨æœ‰é™æˆ–å•ä¸€æ¨¡æ€ç­‰ï¼Œé™åˆ¶äº†å…¶å…¨é¢åˆ†æçš„èƒ½åŠ›ã€‚</li>
<li>WHU-STreeæ•°æ®é›†æ˜¯ä¸€ä¸ªè·¨åŸå¸‚ã€ä¸°å¯Œæ ‡æ³¨ã€å¤šæ¨¡æ€çš„è¡—é“æ ‘æœ¨æ•°æ®é›†ï¼Œæ•´åˆäº†ç‚¹äº‘å’Œé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œæ¶µç›–å¤šä¸ªåŸå¸‚å’Œæ ‘ç§ã€‚</li>
<li>WHU-STreeæ•°æ®é›†æ”¯æŒè¶…è¿‡10ä¸ªä¸è¡—é“æ ‘æœ¨ç›¸å…³çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ ‘ç§åˆ†ç±»å’Œå•æ ªæ ‘æœ¨åˆ†å‰²ç­‰ã€‚</li>
<li>å®éªŒå’Œåˆ†æè¡¨æ˜å¤šæ¨¡æ€æ•°æ®èåˆçš„æ½œåŠ›ï¼Œå¼ºè°ƒè·¨åŸŸé€‚ç”¨æ€§å¯¹äºå®é™…ç®—æ³•éƒ¨ç½²çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13172">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13172v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13172v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13172v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LLM-Hallucination-Detection-A-Fast-Fourier-Transform-Method-Based-on-Hidden-Layer-Temporal-Signals"><a href="#LLM-Hallucination-Detection-A-Fast-Fourier-Transform-Method-Based-on-Hidden-Layer-Temporal-Signals" class="headerlink" title="LLM Hallucination Detection: A Fast Fourier Transform Method Based on   Hidden Layer Temporal Signals"></a>LLM Hallucination Detection: A Fast Fourier Transform Method Based on   Hidden Layer Temporal Signals</h2><p><strong>Authors:Jinxin Li, Gang Tu, ShengYu Cheng, Junjie Hu, Jinting Wang, Rui Chen, Zhilong Zhou, Dongbo Shan</strong></p>
<p>Hallucination remains a critical barrier for deploying large language models (LLMs) in reliability-sensitive applications. Existing detection methods largely fall into two categories: factuality checking, which is fundamentally constrained by external knowledge coverage, and static hidden-state analysis, that fails to capture deviations in reasoning dynamics. As a result, their effectiveness and robustness remain limited. We propose HSAD (Hidden Signal Analysis-based Detection), a novel hallucination detection framework that models the temporal dynamics of hidden representations during autoregressive generation. HSAD constructs hidden-layer signals by sampling activations across layers, applies Fast Fourier Transform (FFT) to obtain frequency-domain representations, and extracts the strongest non-DC frequency component as spectral features. Furthermore, by leveraging the autoregressive nature of LLMs, HSAD identifies optimal observation points for effective and reliable detection. Across multiple benchmarks, including TruthfulQA, HSAD achieves over 10 percentage points improvement compared to prior state-of-the-art methods. By integrating reasoning-process modeling with frequency-domain analysis, HSAD establishes a new paradigm for robust hallucination detection in LLMs. </p>
<blockquote>
<p>å¹»è§‰ä»æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¯é æ€§æ•æ„Ÿåº”ç”¨éƒ¨ç½²ä¸­çš„å…³é”®éšœç¢ã€‚ç°æœ‰çš„æ£€æµ‹æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šå—æ§äºå¤–éƒ¨çŸ¥è¯†è¦†ç›–èŒƒå›´çš„åˆç†æ€§æ£€éªŒå’Œæ— æ³•æ•æ‰æ¨ç†åŠ¨æ€åå·®çš„é™æ€éšè—çŠ¶æ€åˆ†æã€‚å› æ­¤ï¼Œå®ƒä»¬çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ä»ç„¶æœ‰é™ã€‚æˆ‘ä»¬æå‡ºHSADï¼ˆåŸºäºéšè—ä¿¡å·åˆ†æçš„æ£€æµ‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¹»è§‰æ£€æµ‹æ¡†æ¶ï¼Œå¯¹è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­çš„éšè—è¡¨ç¤ºçš„æ—¶ç©ºåŠ¨æ€è¿›è¡Œå»ºæ¨¡ã€‚HSADé€šè¿‡è·¨å±‚é‡‡æ ·æ¿€æ´»æ„å»ºéšè—å±‚ä¿¡å·ï¼Œåº”ç”¨å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆFFTï¼‰è·å¾—é¢‘åŸŸè¡¨ç¤ºï¼Œå¹¶æå–æœ€å¼ºçš„éç›´æµé¢‘ç‡åˆ†é‡ä½œä¸ºè°±ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒHSADåˆ©ç”¨LLMçš„è‡ªå›å½’ç‰¹æ€§ï¼Œç¡®å®šæœ‰æ•ˆçš„å¯é æ£€æµ‹çš„æœ€ä½³è§‚æµ‹ç‚¹ã€‚åœ¨åŒ…æ‹¬TruthfulQAç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒHSADç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„æ£€æµ‹æ–¹æ³•ï¼Œå®ç°äº†è¶…è¿‡10ä¸ªç™¾åˆ†ç‚¹çš„æ”¹è¿›ã€‚é€šè¿‡å°†æ¨ç†è¿‡ç¨‹å»ºæ¨¡ä¸é¢‘åŸŸåˆ†æç›¸ç»“åˆï¼ŒHSADä¸ºLLMä¸­çš„ç¨³å¥å¹»è§‰æ£€æµ‹å»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13154v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¯é æ€§æ•æ„Ÿåº”ç”¨ä¸­çš„å…³é”®éšœç¢æ˜¯å¹»è§‰ç”Ÿæˆã€‚ç°æœ‰æ£€æµ‹æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šå—åˆ¶äºå¤–éƒ¨çŸ¥è¯†è¦†ç›–çš„äº‹å®æ€§æ£€æŸ¥å’Œæ— æ³•æ•æ‰æ¨ç†åŠ¨æ€å˜åŒ–çš„é™æ€éšè—çŠ¶æ€åˆ†æã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„å¹»è§‰æ£€æµ‹æ¡†æ¶HSADï¼ˆåŸºäºéšè—ä¿¡å·åˆ†æçš„æ£€æµ‹ï¼‰ï¼Œè¯¥æ¡†æ¶å¯¹è‡ªåŠ¨å›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­çš„éšè—è¡¨ç¤ºçš„æ—¶ç©ºåŠ¨æ€è¿›è¡Œå»ºæ¨¡ã€‚HSADé€šè¿‡é‡‡æ ·å„å±‚æ¿€æ´»å€¼æ„å»ºéšè—å±‚ä¿¡å·ï¼Œåº”ç”¨å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆFFTï¼‰è·å¾—é¢‘åŸŸè¡¨ç¤ºï¼Œå¹¶æå–éç›´æµåˆ†é‡ä¸­æœ€å¼ºçš„é¢‘è°±ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒHSADåˆ©ç”¨LLMçš„è‡ªåŠ¨å›å½’æ€§è´¨ï¼Œç¡®å®šæœ‰æ•ˆçš„è§‚å¯Ÿç‚¹ï¼Œä»¥å®ç°å¯é æ£€æµ‹ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŒ…æ‹¬TruthfulQAï¼ŒHSADç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„æ£€æµ‹æ–¹æ³•ï¼Œæé«˜äº†è¶…è¿‡10ä¸ªç™¾åˆ†ç‚¹ã€‚é€šè¿‡å°†æ¨ç†è¿‡ç¨‹å»ºæ¨¡ä¸é¢‘åŸŸåˆ†æç›¸ç»“åˆï¼ŒHSADä¸ºLLMä¸­çš„ç¨³å¥å¹»è§‰æ£€æµ‹å»ºç«‹äº†æ–°èŒƒä¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¹»è§‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¯é æ€§æ•æ„Ÿåº”ç”¨ä¸­é¢ä¸´çš„å…³é”®é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ£€æµ‹æ–¹æ³•ä¸»è¦åˆ†ä¸¤ç±»ï¼šäº‹å®æ€§æ£€æŸ¥å’Œé™æ€éšè—çŠ¶æ€åˆ†æï¼Œä½†éƒ½å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æ–°å‹å¹»è§‰æ£€æµ‹æ¡†æ¶HSADé€šè¿‡å»ºæ¨¡éšè—è¡¨ç¤ºçš„æ—¶ç©ºåŠ¨æ€æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>HSADåˆ©ç”¨å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆFFTï¼‰å’Œéšè—å±‚ä¿¡å·çš„é‡‡æ ·æ¥æå–å…³é”®ç‰¹å¾ã€‚</li>
<li>HSADç»“åˆLLMçš„è‡ªåŠ¨å›å½’æ€§è´¨ï¼Œç¡®å®šæœ‰æ•ˆçš„è§‚å¯Ÿç‚¹ä»¥æé«˜æ£€æµ‹å¯é æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒHSADç›¸è¾ƒäºç°æœ‰æ–¹æ³•æ˜¾è‘—æé«˜æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13154">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13154v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13154v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13154v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13154v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13154v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13154v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13154v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Reasoning-with-Preference-Constraints-A-Benchmark-for-Language-Models-in-Many-to-One-Matching-Markets"><a href="#Reasoning-with-Preference-Constraints-A-Benchmark-for-Language-Models-in-Many-to-One-Matching-Markets" class="headerlink" title="Reasoning with Preference Constraints: A Benchmark for Language Models   in Many-to-One Matching Markets"></a>Reasoning with Preference Constraints: A Benchmark for Language Models   in Many-to-One Matching Markets</h2><p><strong>Authors:Marylou Fauchard, Florian Carichon, Margarida Carvalho, Golnoosh Farnadi</strong></p>
<p>Recent advances in reasoning with large language models (LLMs) have demonstrated strong performance on complex mathematical tasks, including combinatorial optimization. Techniques such as Chain-of-Thought and In-Context Learning have further enhanced this capability, making LLMs both powerful and accessible tools for a wide range of users, including non-experts. However, applying LLMs to matching problems, which require reasoning under preferential and structural constraints, remains underexplored. To address this gap, we introduce a novel benchmark of 369 instances of the College Admission Problem, a canonical example of a matching problem with preferences, to evaluate LLMs across key dimensions: feasibility, stability, and optimality. We employ this benchmark to assess the performance of several open-weight LLMs. Our results first reveal that while LLMs can satisfy certain constraints, they struggle to meet all evaluation criteria consistently. They also show that reasoning LLMs, like QwQ and GPT-oss, significantly outperform traditional models such as Llama, Qwen or Mistral, defined here as models used without any dedicated reasoning mechanisms. Moreover, we observed that LLMs reacted differently to the various prompting strategies tested, which include Chain-of-Thought, In-Context Learning and role-based prompting, with no prompt consistently offering the best performance. Finally, we report the performances from iterative prompting with auto-generated feedback and show that they are not monotonic; they can peak early and then significantly decline in later attempts. Overall, this work offers a new perspective on model reasoning performance and the effectiveness of prompting strategies in combinatorial optimization problems with preferential constraints. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ•°å­¦ä»»åŠ¡ä¸Šçš„è¿›å±•ï¼ŒåŒ…æ‹¬ç»„åˆä¼˜åŒ–ï¼Œéƒ½è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚æ€ç»´é“¾å’Œä¸Šä¸‹æ–‡å­¦ä¹ ç­‰æŠ€æœ¯è¿›ä¸€æ­¥å¢å¼ºäº†è¿™ä¸€èƒ½åŠ›ï¼Œä½¿LLMæˆä¸ºå¼ºå¤§ä¸”æ˜“äºä½¿ç”¨çš„å·¥å…·ï¼Œé€‚ç”¨äºå¹¿å¤§ç”¨æˆ·ï¼ŒåŒ…æ‹¬éä¸“ä¸šäººå£«ã€‚ç„¶è€Œï¼Œå°†LLMåº”ç”¨äºåŒ¹é…é—®é¢˜ï¼ˆéœ€è¦åœ¨åå¥½å’Œç»“æ„çº¦æŸä¸‹è¿›è¡Œæ¨ç†ï¼‰ä»ç„¶é²œæœ‰ç ”ç©¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ…å«369ä¸ªå®ä¾‹çš„å¤§å­¦å½•å–é—®é¢˜åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯å¸¦æœ‰åå¥½åŒ¹é…é—®é¢˜çš„ä¸€ä¸ªå…¸å‹æ¡ˆä¾‹ï¼Œä»¥è¯„ä¼°LLMçš„å…³é”®ç»´åº¦ï¼šå¯è¡Œæ€§ã€ç¨³å®šæ€§å’Œæœ€ä¼˜æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å‡ ä¸ªå…¬å¼€æƒé‡LLMçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœé¦–å…ˆè¡¨æ˜ï¼Œè™½ç„¶LLMå¯ä»¥æ»¡è¶³æŸäº›çº¦æŸï¼Œä½†å®ƒä»¬éš¾ä»¥å§‹ç»ˆå¦‚ä¸€åœ°æ»¡è¶³æ‰€æœ‰è¯„ä¼°æ ‡å‡†ã€‚ä»–ä»¬è¿˜æ˜¾ç¤ºï¼ŒåƒQwQå’ŒGPT-ossè¿™æ ·çš„æ¨ç†LLMæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„LLMæ¨¡å‹ï¼Œå¦‚Llamaã€Qwenæˆ–Mistralï¼ˆè¿™é‡Œå®šä¹‰ä¸ºæ²¡æœ‰ä¸“ç”¨æ¨ç†æœºåˆ¶çš„æ¨¡å‹ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°LLMå¯¹ä¸åŒæç¤ºç­–ç•¥çš„ååº”å„ä¸ç›¸åŒï¼ŒåŒ…æ‹¬æ€ç»´é“¾ã€ä¸Šä¸‹æ–‡å­¦ä¹ å’ŒåŸºäºè§’è‰²çš„æç¤ºï¼Œæ²¡æœ‰ä»»ä½•ä¸€ç§æç¤ºå§‹ç»ˆæä¾›æœ€ä½³æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†é€šè¿‡è‡ªåŠ¨ç”Ÿæˆåé¦ˆè¿›è¡Œè¿­ä»£æç¤ºçš„æ€§èƒ½ï¼Œå¹¶æ˜¾ç¤ºå®ƒä»¬å¹¶éå•è°ƒï¼›å®ƒä»¬å¯èƒ½åœ¨æ—©æœŸè¾¾åˆ°å³°å€¼ï¼Œç„¶ååœ¨åç»­å°è¯•ä¸­æ˜¾è‘—ä¸‹é™ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹å·¥ä½œæä¾›äº†å…³äºæ¨¡å‹æ¨ç†æ€§èƒ½å’Œåå¥½çº¦æŸç»„åˆä¼˜åŒ–é—®é¢˜ä¸­çš„æç¤ºç­–ç•¥æœ‰æ•ˆæ€§çš„æ–°è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13131v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚çš„æ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ç»„åˆä¼˜åŒ–ã€‚é€šè¿‡Chain-of-Thoughtå’ŒIn-Context Learningç­‰æŠ€æœ¯ï¼ŒLLMçš„æ¨ç†èƒ½åŠ›å¾—åˆ°è¿›ä¸€æ­¥æå‡ï¼Œä½¿å…¶æˆä¸ºéä¸“å®¶ç”¨æˆ·ä¹Ÿèƒ½è½»æ¾ä½¿ç”¨çš„å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼Œå°†LLMåº”ç”¨äºå¸¦æœ‰åå¥½å’Œç»“æ„çº¦æŸçš„åŒ¹é…é—®é¢˜ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶ä¸ºè§£å†³è¿™ä¸€ç©ºç™½è€Œæ¨å‡ºæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œå¯¹LLMè§£å†³å¤§å­¦å½•å–é—®é¢˜ç­‰åŒ¹é…é—®é¢˜çš„èƒ½åŠ›è¿›è¡Œè¯„ä¼°ï¼Œæ¶µç›–å¯è¡Œæ€§ã€ç¨³å®šæ€§å’Œæœ€ä¼˜æ€§ç­‰æ–¹é¢ã€‚ç ”ç©¶å‘ç°LLMè™½èƒ½æ»¡è¶³æŸäº›çº¦æŸï¼Œä½†éš¾ä»¥ä¸€è‡´æ»¡è¶³æ‰€æœ‰è¯„ä¼°æ ‡å‡†ã€‚æ­¤å¤–ï¼Œç›¸æ¯”ä¼ ç»Ÿæ¨¡å‹ï¼Œå¦‚QwQå’ŒGPT-ossç­‰å¸¦æœ‰æ¨ç†æœºåˆ¶çš„LLMè¡¨ç°æ›´ä¼˜ã€‚åŒæ—¶ï¼Œä¸åŒçš„æç¤ºç­–ç•¥å¯¹LLMçš„å½±å“å„å¼‚ï¼Œæœªå‘ç°å§‹ç»ˆæœ€ä½³çš„æç¤ºæ–¹å¼ã€‚æœ€åï¼Œé€šè¿‡è¿­ä»£æç¤ºå’Œè‡ªåŠ¨ç”Ÿæˆåé¦ˆï¼Œå‘ç°æ€§èƒ½å¹¶éå•è°ƒæå‡ï¼Œå¯èƒ½åœ¨æ—©æœŸè¾¾åˆ°å³°å€¼åæ˜¾è‘—ä¸‹é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤æ‚çš„æ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ç»„åˆä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>LLMæŠ€æœ¯å¦‚Chain-of-Thoughtå’ŒIn-Context Learningå¢å¼ºäº†å…¶æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LLMåœ¨è§£å†³å¸¦æœ‰åå¥½å’Œç»“æ„çº¦æŸçš„åŒ¹é…é—®é¢˜ä¸Šä»æœ‰å¾…æ¢ç´¢ã€‚</li>
<li>æ¨å‡ºæ–°çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°LLMè§£å†³åŒ¹é…é—®é¢˜çš„èƒ½åŠ›ï¼Œæ¶µç›–å¯è¡Œæ€§ã€ç¨³å®šæ€§å’Œæœ€ä¼˜æ€§ã€‚</li>
<li>LLMåœ¨æŸäº›çº¦æŸæ¡ä»¶ä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†éš¾ä»¥æ»¡è¶³æ‰€æœ‰è¯„ä¼°æ ‡å‡†ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ¨¡å‹ç›¸æ¯”ï¼Œå¸¦æœ‰æ¨ç†æœºåˆ¶çš„LLMï¼ˆå¦‚QwQå’ŒGPT-ossï¼‰è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>æç¤ºç­–ç•¥å¯¹LLMçš„å½±å“ä¸åŒï¼Œæœªæ‰¾åˆ°å§‹ç»ˆæœ€ä½³çš„æç¤ºæ–¹å¼ï¼›æ€§èƒ½æå‡å¹¶éå•è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13131v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13131v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13131v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Empowering-LLMs-with-Parameterized-Skills-for-Adversarial-Long-Horizon-Planning"><a href="#Empowering-LLMs-with-Parameterized-Skills-for-Adversarial-Long-Horizon-Planning" class="headerlink" title="Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon   Planning"></a>Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon   Planning</h2><p><strong>Authors:Sijia Cui, Shuai Xu, Aiyao He, Yanna Wang, Bo Xu</strong></p>
<p>Recent advancements in Large Language Models(LLMs) have led to the development of LLM-based AI agents. A key challenge is the creation of agents that can effectively ground themselves in complex, adversarial long-horizon environments. Existing methods mainly focus on (1) using LLMs as policies to interact with the environment through generating low-level feasible actions, and (2) utilizing LLMs to generate high-level tasks or language guides to stimulate action generation. However, the former struggles to generate reliable actions, while the latter relies heavily on expert experience to translate high-level tasks into specific action sequences. To address these challenges, we introduce the Plan with Language, Act with Parameter (PLAP) planning framework that facilitates the grounding of LLM-based agents in long-horizon environments. The PLAP method comprises three key components: (1) a skill library containing environment-specific parameterized skills, (2) a skill planner powered by LLMs, and (3) a skill executor converting the parameterized skills into executable action sequences. We implement PLAP in MicroRTS, a long-horizon real-time strategy game that provides an unfamiliar and challenging environment for LLMs. The experimental results demonstrate the effectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting outperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully crafted few-shot examples, surpasses the top-tier scripted agent, CoacAI. Additionally, we design comprehensive evaluation metrics and test 6 closed-source and 2 open-source LLMs within the PLAP framework, ultimately releasing an LLM leaderboard ranking long-horizon skill planning ability. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/AI-Research-TeamX/PLAP">https://github.com/AI-Research-TeamX/PLAP</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†åŸºäºLLMçš„AIä»£ç†çš„å‘å±•ã€‚ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯åˆ›å»ºèƒ½å¤Ÿåœ¨å¤æ‚ã€å¯¹æŠ—æ€§çš„é•¿æœŸç¯å¢ƒä¸­æœ‰æ•ˆå®šä½è‡ªå·±çš„ä»£ç†ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ï¼šï¼ˆ1ï¼‰ä½¿ç”¨LLMä½œä¸ºç­–ç•¥ï¼Œé€šè¿‡ä¸ç¯å¢ƒç”Ÿæˆä½çº§å¯è¡ŒåŠ¨ä½œè¿›è¡Œäº¤äº’ï¼›ï¼ˆ2ï¼‰åˆ©ç”¨LLMç”Ÿæˆé«˜çº§ä»»åŠ¡æˆ–è¯­è¨€æŒ‡å—æ¥æ¿€å‘åŠ¨ä½œç”Ÿæˆã€‚ç„¶è€Œï¼Œå‰è€…åœ¨ç”Ÿæˆå¯é åŠ¨ä½œæ–¹é¢é‡åˆ°å›°éš¾ï¼Œè€Œåè€…åˆ™ä¸¥é‡ä¾èµ–äºä¸“å®¶ç»éªŒå°†é«˜çº§ä»»åŠ¡ç¿»è¯‘æˆç‰¹å®šçš„åŠ¨ä½œåºåˆ—ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œç”¨è¯­è¨€è§„åˆ’ï¼Œç”¨å‚æ•°è¡ŒåŠ¨â€ï¼ˆPLAPï¼‰è§„åˆ’æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ‰åŠ©äºåŸºäºLLMçš„ä»£ç†åœ¨é•¿æœŸç¯å¢ƒä¸­å®šä½ã€‚PLAPæ–¹æ³•åŒ…å«ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰åŒ…å«ç¯å¢ƒç‰¹å®šå‚æ•°åŒ–æŠ€èƒ½çš„æŠ€èƒ½åº“ï¼›ï¼ˆ2ï¼‰ç”±LLMé©±åŠ¨çš„æŠ€èƒ½è§„åˆ’å™¨ï¼›ï¼ˆ3ï¼‰å°†å‚æ•°åŒ–æŠ€èƒ½è½¬åŒ–ä¸ºå¯æ‰§è¡ŒåŠ¨ä½œåºåˆ—çš„æŠ€èƒ½æ‰§è¡Œå™¨ã€‚æˆ‘ä»¬åœ¨MicroRTSä¸­å®ç°äº†PLAPï¼Œè¿™æ˜¯ä¸€æ¬¾é•¿æœŸå®æ—¶ç­–ç•¥æ¸¸æˆï¼Œä¸ºLLMæä¾›äº†ä¸€ä¸ªä¸ç†Ÿæ‚‰ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒã€‚å®éªŒç»“æœè¡¨æ˜PLAPçš„æœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«æ˜¯ï¼ŒGPT-4oé©±åŠ¨çš„PLAPåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è¶…è¶Šäº†80%çš„åŸºçº¿ä»£ç†ï¼Œè€Œç»è¿‡ç²¾å¿ƒè®¾è®¡çš„Qwen2-72Bé©±åŠ¨çš„PLAPåˆ™åœ¨å°‘æ•°æ ·æœ¬ç¤ºä¾‹ä¸­è¶…è¶Šäº†é¡¶çº§è„šæœ¬ä»£ç†CoacAIã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶åœ¨PLAPæ¡†æ¶å†…æµ‹è¯•äº†6ä¸ªé—­æºå’Œ2ä¸ªå¼€æºLLMï¼Œæœ€ç»ˆå‘å¸ƒäº†ä¸€ä¸ªé•¿æœŸè§„åˆ’èƒ½åŠ›æ’è¡Œæ¦œçš„LLMé¢†å¯¼è€…æ¦œå•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AI-Research-TeamX/PLAP%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/AI-Research-TeamX/PLAPè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13127v1">PDF</a> Accepted to IJCNN 2025</p>
<p><strong>Summary</strong></p>
<p>LLMé¢†åŸŸçš„æ–°è¿›å±•å¸¦æ¥äº†åŸºäºLLMçš„AIä»£ç†çš„å¼€å‘ã€‚åˆ›å»ºèƒ½åœ¨å¤æ‚ã€å¯¹æŠ—æ€§çš„é•¿æœŸç¯å¢ƒä¸­æœ‰æ•ˆè‡ªæˆ‘å®šä½çš„æ™ºèƒ½ä½“æ˜¯å…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ä½¿ç”¨LLMç”Ÿæˆä½çº§å¯è¡ŒåŠ¨ä½œï¼Œä»¥åŠåˆ©ç”¨LLMç”Ÿæˆé«˜çº§ä»»åŠ¡æˆ–è¯­è¨€æŒ‡å—æ¥åˆºæ¿€åŠ¨ä½œç”Ÿæˆã€‚ç„¶è€Œï¼Œå‰è€…éš¾ä»¥ç”Ÿæˆå¯é çš„åŠ¨ä½œï¼Œåè€…åˆ™ä¸¥é‡ä¾èµ–äºä¸“å®¶ç»éªŒå°†é«˜çº§ä»»åŠ¡ç¿»è¯‘æˆç‰¹å®šçš„åŠ¨ä½œåºåˆ—ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥Plan with Language, Act with Parameterï¼ˆPLAPï¼‰è§„åˆ’æ¡†æ¶ï¼Œä¿ƒè¿›åŸºäºLLMçš„æ™ºèƒ½ä½“åœ¨é•¿æœŸç¯å¢ƒä¸­çš„å®šä½ã€‚å®éªŒç»“æœè¡¨æ˜PLAPçš„æœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«æ˜¯GPT-4é©±åŠ¨çš„PLAPåœ¨é›¶å°„å‡»ç¯å¢ƒä¸­è¡¨ç°ä¼˜äº80%çš„åŸºçº¿æ™ºèƒ½ä½“ï¼Œè€Œç»è¿‡ç²¾å¿ƒè®¾è®¡çš„Qwen2-72Bå°‘æ•°æ ·æœ¬è¡¨ç°è¶…è¶Šé¡¶å°–è„šæœ¬ä»£ç†CoacAIã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMçš„æ–°è¿›å±•å¯¼è‡´åŸºäºLLMçš„AIä»£ç†çš„å‘å±•ã€‚</li>
<li>åˆ›å»ºèƒ½åœ¨é•¿æœŸç¯å¢ƒä¸­æœ‰æ•ˆè‡ªæˆ‘å®šä½çš„æ™ºèƒ½ä½“æ˜¯å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼šéš¾ä»¥ç”Ÿæˆå¯é åŠ¨ä½œå’Œè¿‡åº¦ä¾èµ–ä¸“å®¶ç»éªŒã€‚</li>
<li>å¼•å…¥PLAPè§„åˆ’æ¡†æ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æŠ€èƒ½åº“ã€æŠ€èƒ½è§„åˆ’å™¨å’ŒæŠ€èƒ½æ‰§è¡Œå™¨ä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>PLAPåœ¨MicroRTSé•¿æ—¶ç­–ç•¥æ¸¸æˆä¸­çš„å®æ–½è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>GPT-4é©±åŠ¨çš„PLAPåœ¨é›¶å°„å‡»ç¯å¢ƒä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡å¤§å¤šæ•°åŸºçº¿æ™ºèƒ½ä½“ã€‚</li>
<li>Qwen2-72Båœ¨ç²¾å¿ƒè®¾è®¡çš„å°‘æ•°æ ·æœ¬ä¸­è¡¨ç°æœ€ä½³ï¼Œè¶…è¶Šé¡¶å°–è„šæœ¬ä»£ç†CoacAIã€‚</li>
<li>æä¾›äº†å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶åœ¨PLAPæ¡†æ¶ä¸‹æµ‹è¯•äº†å¤šä¸ªLLMã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13127">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13127v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13127v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13127v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13127v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13127v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13127v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-comparison-of-pipelines-for-the-translation-of-a-low-resource-language-based-on-transformers"><a href="#A-comparison-of-pipelines-for-the-translation-of-a-low-resource-language-based-on-transformers" class="headerlink" title="A comparison of pipelines for the translation of a low resource language   based on transformers"></a>A comparison of pipelines for the translation of a low resource language   based on transformers</h2><p><strong>Authors:Chiara Bonfanti, Michele Colombino, Giulia Coucourde, Faeze Memari, Stefano Pinardi, Rosa Meo</strong></p>
<p>This work compares three pipelines for training transformer-based neural networks to produce machine translators for Bambara, a Mand`e language spoken in Africa by about 14,188,850 people. The first pipeline trains a simple transformer to translate sentences from French into Bambara. The second fine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures for French-to-Bambara translation. Models from the first two pipelines were trained with different hyperparameter combinations to improve BLEU and chrF scores, evaluated on both test sentences and official Bambara benchmarks. The third pipeline uses language distillation with a student-teacher dual neural network to integrate Bambara into a pre-trained LaBSE model, which provides language-agnostic embeddings. A BERT extension is then applied to LaBSE to generate translations. All pipelines were tested on Dokotoro (medical) and Bayelemagaba (mixed domains). Results show that the first pipeline, although simpler, achieves the best translation accuracy (10% BLEU, 21% chrF on Bayelemagaba), consistent with low-resource translation results. On the Yiri dataset, created for this work, it achieves 33.81% BLEU and 41% chrF. Instructor-based models perform better on single datasets than on aggregated collections, suggesting they capture dataset-specific patterns more effectively. </p>
<blockquote>
<p>æœ¬æ–‡æ¯”è¾ƒäº†ä¸‰æ¡ç”¨äºè®­ç»ƒåŸºäºè½¬æ¢å™¨çš„ç¥ç»ç½‘ç»œä»¥ç”Ÿæˆå·´å§†å·´æ‹‰è¯­ï¼ˆä¸€ç§éæ´²çº¦14,188,850äººä½¿ç”¨çš„æ›¼å¾·è¯­è¨€ï¼‰çš„æœºå™¨ç¿»è¯‘å™¨çš„ç®¡é“ã€‚ç¬¬ä¸€æ¡ç®¡é“è®­ç»ƒä¸€ä¸ªç®€å•çš„è½¬æ¢å™¨ï¼Œå°†æ³•è¯­ç¿»è¯‘æˆå·´å§†å·´æ‹‰è¯­ã€‚ç¬¬äºŒæ¡ç®¡é“å¯¹LLaMA3ï¼ˆ3B-8Bï¼‰æŒ‡å¯¼å‘˜æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé‡‡ç”¨ä»…è§£ç å™¨æ¶æ„è¿›è¡Œæ³•è¯­è‡³å·´å§†å·´æ‹‰è¯­çš„ç¿»è¯‘ã€‚å‰ä¸¤æ¡ç®¡é“ä¸­çš„æ¨¡å‹é€šè¿‡ä¸åŒçš„è¶…å‚æ•°ç»„åˆè¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜BLEUå’ŒchrFåˆ†æ•°ï¼Œå¹¶åœ¨æµ‹è¯•å¥å’Œå®˜æ–¹å·´å§†å·´æ‹‰è¯­åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œè¯„ä¼°ã€‚ç¬¬ä¸‰æ¡ç®¡é“é‡‡ç”¨å­¦ç”Ÿ-æ•™å¸ˆåŒç¥ç»ç½‘ç»œè¿›è¡Œè¯­è¨€è’¸é¦ï¼Œå°†å·´å§†å·´æ‹‰è¯­é›†æˆåˆ°é¢„è®­ç»ƒçš„LaBSEæ¨¡å‹ä¸­ï¼Œè¯¥æ¨¡å‹æä¾›è¯­è¨€æ— å…³çš„åµŒå…¥ã€‚ç„¶ååº”ç”¨BERTæ‰©å±•ç‰ˆåˆ°LaBSEä»¥ç”Ÿæˆç¿»è¯‘ã€‚æ‰€æœ‰ç®¡é“å‡åœ¨Dokotoroï¼ˆåŒ»å­¦ï¼‰å’ŒBayelemagabaï¼ˆæ··åˆé¢†åŸŸï¼‰ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶ç¬¬ä¸€æ¡ç®¡é“æ›´ç®€å•ï¼Œä½†åœ¨ç¿»è¯‘å‡†ç¡®æ€§æ–¹é¢è¡¨ç°æœ€ä½³ï¼ˆåœ¨Bayelemagabaä¸ŠBLEUå¾—åˆ†ä¸º10%ï¼ŒchrFå¾—åˆ†ä¸º21%ï¼‰ï¼Œè¿™ä¸ä½èµ„æºç¿»è¯‘ç»“æœä¸€è‡´ã€‚é’ˆå¯¹æœ¬æ–‡åˆ›å»ºçš„æ•°æ®é›†Yiriï¼Œå…¶BLEUå¾—åˆ†ä¸º33.81%ï¼ŒchrFå¾—åˆ†ä¸º41%ã€‚åŸºäºæŒ‡å¯¼å‘˜çš„æ¨¡å‹åœ¨å•ä¸€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºèšåˆé›†åˆï¼Œè¿™è¡¨æ˜å®ƒä»¬æ›´æœ‰æ•ˆåœ°æ•æ‰æ•°æ®é›†ç‰¹å®šçš„æ¨¡å¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12514v1">PDF</a> 9 pages, 4 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¯”è¾ƒäº†ä¸‰ç§è®­ç»ƒåŸºäºè½¬æ¢å™¨çš„ç¥ç»ç½‘ç»œä»¥ç”Ÿæˆå·´é©¬è¯­è¨€ï¼ˆä¸€ç§éæ´²çš„æ›¼å¾·è¯­ï¼Œçº¦1418ä¸‡äººä½¿ç”¨ï¼‰ç¿»è¯‘æœºçš„ç®¡é“ã€‚ç¬¬ä¸€ç§ç®¡é“ç®€å•è®­ç»ƒä¸€ä¸ªç¿»è¯‘å™¨ï¼Œå®ç°ä»æ³•è¯­åˆ°å·´é©¬çš„ç¿»è¯‘ã€‚ç¬¬äºŒç§ç®¡é“é‡‡ç”¨LLaMA3ï¼ˆä»‹äºä½Bå’Œèµ«å…¹è‡³ä½©å¡”èµ«å…¹çº§åŠŸç‡è¿è¡Œçš„é«˜é€Ÿæ§åˆ¶è®ºé‡å­ç¼–ç æ•°å­—å¤šåŠŸèƒ½æ•´åˆå·¥ç¨‹çš„é«˜ä¿çœŸæé™åŠ¨åŠ›ç³»ç»Ÿç§‘æŠ€å¤æ‚ç§‘å­¦çš„é€»è¾‘æ¨ç†åŠ›ä¸æ–­æ¶æ„ç½‘ç»œå‹é€šä¿¡ç®¡ç†çš„ç¡¬ä»¶ä»£å·å‘½åçš„èŠ¯ç‰‡ä¸Šå°è£…å«æœ‰å­é“¾æ¥ç»„æˆä½ç çš„æåº¦é«˜é˜¶è¯­ä¹‰è±¡å¾ç®—æ³•çš„åŠ å¯†å¯†é›†å‹å†³ç­–çŸ©é˜µå¹³å°ä»£å·æŒ‡é¡¹ç›®ç¬¬ä¸‰çº§ç½‘ç»œç³»ç»Ÿä½“ç³»å‹ç¼©åˆ†æå†èšç„¦æ•°å€¼æ¨¡å‹ï¼‰æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç”¨äºæ³•å·´ç¿»è¯‘ã€‚å‰ä¸¤ç§ç®¡é“çš„æ¨¡å‹é€šè¿‡ä¸åŒçš„è¶…å‚æ•°ç»„åˆè®­ç»ƒï¼Œä»¥æé«˜BLEUå’ŒchrFåˆ†æ•°ã€‚ç¬¬ä¸‰ç§ç®¡é“ä½¿ç”¨å­¦ç”Ÿæ•™å¸ˆåŒç¥ç»ç½‘ç»œè¿›è¡Œè¯­è¨€è’¸é¦ï¼Œå°†å·´é©¬è¯­è¨€èå…¥é¢„è®­ç»ƒçš„LaBSEæ¨¡å‹ï¼Œå¹¶åˆ©ç”¨BERTæ‰©å±•ç”Ÿæˆç¿»è¯‘ã€‚åœ¨Dokotoroï¼ˆåŒ»ç–—é¢†åŸŸï¼‰å’ŒBayelemagabaï¼ˆæ··åˆé¢†åŸŸï¼‰çš„æµ‹è¯•ä¸­ï¼Œç¬¬ä¸€ç§ç®¡é“åœ¨ç¿»è¯‘å‡†ç¡®æ€§æ–¹é¢è¡¨ç°æœ€ä½³ã€‚åœ¨ä¸ºæ­¤å·¥ä½œåˆ›å»ºçš„Yiriæ•°æ®é›†ä¸Šï¼Œå®ƒå®ç°äº†33.81ï¼…çš„BLEUå’Œ41ï¼…çš„chrFåˆ†æ•°ã€‚åŸºäºæŒ‡å¯¼è€…çš„æ¨¡å‹åœ¨å•ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºèšåˆé›†åˆï¼Œè¿™è¡¨æ˜å®ƒä»¬æ›´æœ‰æ•ˆåœ°æ•æ‰æ•°æ®é›†ç‰¹å®šçš„æ¨¡å¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶æ¯”è¾ƒäº†ä¸‰ç§è®­ç»ƒç¥ç»ç½‘ç»œä»¥ç”Ÿæˆå·´é©¬è¯­è¨€ç¿»è¯‘çš„æ–¹æ³•ã€‚</li>
<li>ç¬¬ä¸€ç§ç®¡é“ç®€å•æœ‰æ•ˆï¼Œåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šå®ç°æœ€ä½³ç¿»è¯‘å‡†ç¡®æ€§ã€‚</li>
<li>ç¬¬äºŒç§ç®¡é“é‡‡ç”¨LLaMA3æ¨¡å‹å¾®è°ƒï¼Œç”¨äºæ³•å·´ç¿»è¯‘ï¼Œè¡¨ç°äº¦ä½³ã€‚</li>
<li>å‰ä¸¤ç§ç®¡é“çš„æ¨¡å‹é€šè¿‡è°ƒæ•´è¶…å‚æ•°æé«˜æ€§èƒ½ã€‚</li>
<li>ç¬¬ä¸‰ç§ç®¡é“ç»“åˆè¯­è¨€è’¸é¦å’Œå­¦ç”Ÿæ•™å¸ˆåŒç¥ç»ç½‘ç»œï¼Œé›†æˆå·´é©¬è¯­è¨€åˆ°é¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>åŸºäºæŒ‡å¯¼è€…çš„æ¨¡å‹åœ¨å•ä¸€æ•°æ®é›†ä¸Šä¼˜äºèšåˆæ•°æ®é›†ï¼Œæ›´æœ‰æ•ˆæ•æ‰æ•°æ®é›†ç‰¹å®šæ¨¡å¼ã€‚</li>
<li>ç ”ç©¶ç»“æœæ­ç¤ºäº†ä¸åŒç®¡é“åœ¨å¤„ç†ç‰¹å®šè¯­è¨€ç¿»è¯‘ä»»åŠ¡æ—¶çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.12514v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.12514v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.12514v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.12514v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.12514v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.12514v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.12514v1/page_5_2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TAPS-Tool-Augmented-Personalisation-via-Structured-Tagging"><a href="#TAPS-Tool-Augmented-Personalisation-via-Structured-Tagging" class="headerlink" title="TAPS: Tool-Augmented Personalisation via Structured Tagging"></a>TAPS: Tool-Augmented Personalisation via Structured Tagging</h2><p><strong>Authors:Ekaterina Taktasheva, Jeff Dalton</strong></p>
<p>Recent advancements in tool-augmented large language models have enabled them to interact with external tools, enhancing their ability to perform complex user tasks. However, existing approaches overlook the role of personalisation in guiding tool use. This work investigates how user preferences can be effectively integrated into goal-oriented dialogue agents. Through extensive analysis, we identify key weaknesses in the ability of LLMs to personalise tool use. To this end, we introduce TAPS, a novel solution that enhances personalised tool use by leveraging a structured tagging tool and an uncertainty-based tool detector. TAPS significantly improves the ability of LLMs to incorporate user preferences, achieving the new state-of-the-art for open source models on the NLSI task. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå·¥å…·å¢å¼ºå‹å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥å·²ç»ä½¿å¾—å®ƒä»¬å¯ä»¥ä¸å¤–éƒ¨å·¥å…·è¿›è¡Œäº¤äº’ï¼Œæé«˜äº†æ‰§è¡Œå¤æ‚ç”¨æˆ·ä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¿½è§†äº†ä¸ªæ€§åŒ–åœ¨æŒ‡å¯¼å·¥å…·ä½¿ç”¨ä¸­çš„ä½œç”¨ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨å¦‚ä½•æœ‰æ•ˆåœ°å°†ç”¨æˆ·åå¥½æ•´åˆåˆ°ç›®æ ‡å¯¼å‘çš„å¯¹è¯ä»£ç†ä¸­ã€‚é€šè¿‡å¹¿æ³›çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸ªæ€§åŒ–å·¥å…·ä½¿ç”¨æ–¹é¢çš„å…³é”®å¼±ç‚¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†TAPSï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è§£å†³æ–¹æ¡ˆï¼Œå®ƒé€šè¿‡åˆ©ç”¨ç»“æ„åŒ–æ ‡ç­¾å·¥å…·å’ŒåŸºäºä¸ç¡®å®šæ€§çš„å·¥å…·æ£€æµ‹å™¨æ¥å¢å¼ºä¸ªæ€§åŒ–å·¥å…·çš„ä½¿ç”¨ã€‚TAPSæ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹èå…¥ç”¨æˆ·åå¥½çš„èƒ½åŠ›ï¼Œåœ¨NLSIä»»åŠ¡ä¸Šå®ç°äº†å¼€æºæ¨¡å‹çš„æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20409v3">PDF</a> Accepted to EMNLP 2026 Main</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå·¥å…·å¢å¼ºå‹å¤§å‹è¯­è¨€æ¨¡å‹å–å¾—è¿›å±•ï¼Œèƒ½ä¸å¤–éƒ¨å·¥å…·äº¤äº’ï¼Œæ‰§è¡Œå¤æ‚ç”¨æˆ·ä»»åŠ¡èƒ½åŠ›å¢å¼ºã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¿½è§†äº†ä¸ªæ€§åŒ–åœ¨æŒ‡å¯¼å·¥å…·ä½¿ç”¨ä¸­çš„ä½œç”¨ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†å¦‚ä½•æœ‰æ•ˆæ•´åˆç”¨æˆ·åå¥½åˆ°ç›®æ ‡å¯¼å‘å¯¹è¯ä»£ç†ä¸­ã€‚é€šè¿‡æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å‘ç°LLMåœ¨ä¸ªæ€§åŒ–å·¥å…·ä½¿ç”¨ä¸Šçš„å…³é”®å¼±ç‚¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥TAPSï¼Œä¸€ç§åˆ©ç”¨ç»“æ„åŒ–æ ‡ç­¾å·¥å…·å’ŒåŸºäºä¸ç¡®å®šæ€§çš„å·¥å…·æ£€æµ‹å™¨å¢å¼ºä¸ªæ€§åŒ–å·¥å…·ä½¿ç”¨çš„æ–°è§£å†³æ–¹æ¡ˆã€‚TAPSæ˜¾è‘—æé«˜äº†LLMèå…¥ç”¨æˆ·åå¥½çš„èƒ½åŠ›ï¼Œåœ¨NLSIä»»åŠ¡ä¸Šå®ç°äº†å¼€æºæ¨¡å‹çš„æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·¥å…·å¢å¼ºå‹å¤§å‹è¯­è¨€æ¨¡å‹èƒ½ä¸å¤–éƒ¨å·¥å…·äº¤äº’ï¼Œå¢å¼ºæ‰§è¡Œå¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½è§†äº†ä¸ªæ€§åŒ–åœ¨æŒ‡å¯¼å·¥å…·ä½¿ç”¨ä¸­çš„ä½œç”¨ã€‚</li>
<li>ç”¨æˆ·åå¥½æ•´åˆåˆ°ç›®æ ‡å¯¼å‘å¯¹è¯ä»£ç†ä¸­æ˜¯æœ¬ç ”ç©¶çš„é‡ç‚¹ã€‚</li>
<li>LLMåœ¨ä¸ªæ€§åŒ–å·¥å…·ä½¿ç”¨ä¸Šå­˜åœ¨å…³é”®å¼±ç‚¹ã€‚</li>
<li>TAPSåˆ©ç”¨ç»“æ„åŒ–æ ‡ç­¾å·¥å…·å’ŒåŸºäºä¸ç¡®å®šæ€§çš„å·¥å…·æ£€æµ‹å™¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>TAPSæ˜¾è‘—æé«˜äº†LLMèå…¥ç”¨æˆ·åå¥½çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20409">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.20409v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.20409v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.20409v3/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.20409v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.20409v3/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.20409v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.20409v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.20409v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Prompting-Obscures-Hallucination-Cues-in-Large-Language-Models-An-Empirical-Evaluation"><a href="#Chain-of-Thought-Prompting-Obscures-Hallucination-Cues-in-Large-Language-Models-An-Empirical-Evaluation" class="headerlink" title="Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models: An Empirical Evaluation"></a>Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models: An Empirical Evaluation</h2><p><strong>Authors:Jiahao Cheng, Tiancheng Su, Jia Yuan, Guoxiu He, Jiawei Liu, Xinqi Tao, Jingwen Xie, Huaxia Li</strong></p>
<p>Large Language Models (LLMs) often exhibit \textit{hallucinations}, generating factually incorrect or semantically irrelevant content in response to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by encouraging step-by-step reasoning, but its impact on hallucination detection remains underexplored. To bridge this gap, we conduct a systematic empirical evaluation. We begin with a pilot experiment, revealing that CoT reasoning significantly affects the LLMâ€™s internal states and token probability distributions. Building on this, we evaluate the impact of various CoT prompting methods on mainstream hallucination detection methods across both instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three key dimensions: changes in hallucination score distributions, variations in detection accuracy, and shifts in detection confidence. Our findings show that while CoT prompting helps reduce hallucination frequency, it also tends to obscure critical signals used for detection, impairing the effectiveness of various detection methods. Our study highlights an overlooked trade-off in the use of reasoning. Code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ECNU-Text-Computing/cot-hallu-detect">https://github.com/ECNU-Text-Computing/cot-hallu-detect</a> . </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¸å¸¸å‡ºç°â€œå¹»è§‰â€ï¼Œå³ç”Ÿæˆä¸æç¤ºä¸ç¬¦çš„äº‹å®é”™è¯¯æˆ–è¯­ä¹‰æ— å…³çš„å†…å®¹ã€‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå¯ä»¥é€šè¿‡é¼“åŠ±é€æ­¥æ¨ç†æ¥ç¼“è§£å¹»è§‰é—®é¢˜ï¼Œä½†å…¶å¯¹å¹»è§‰æ£€æµ‹çš„å½±å“ä»è¢«å¿½è§†ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯ç ”ç©¶ã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œäº†ä¸€é¡¹è¯•ç‚¹å®éªŒï¼Œç»“æœæ˜¾ç¤ºCoTæ¨ç†æ˜¾è‘—å½±å“LLMçš„å†…éƒ¨çŠ¶æ€å’Œæ ‡è®°æ¦‚ç‡åˆ†å¸ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¯„ä¼°äº†å„ç§CoTæç¤ºæ–¹æ³•å¯¹ä¸»æµå¹»è§‰æ£€æµ‹æ–¹æ³•çš„æ£€æµ‹å½±å“ï¼Œæ¶‰åŠæŒ‡ä»¤è°ƒä¼˜å’Œé¢å‘æ¨ç†çš„LLMã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è€ƒå¯Ÿäº†ä¸‰ä¸ªæ–¹é¢ï¼šå¹»è§‰åˆ†æ•°åˆ†å¸ƒçš„å˜åŒ–ã€æ£€æµ‹å‡†ç¡®åº¦çš„å˜åŒ–ä»¥åŠæ£€æµ‹ä¿¡å¿ƒçš„è½¬å˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶CoTæç¤ºæœ‰åŠ©äºå‡å°‘å¹»è§‰é¢‘ç‡ï¼Œä½†å®ƒä¹Ÿæœ‰æ©ç›–ç”¨äºæ£€æµ‹çš„å…³é”®ä¿¡å·çš„è¶‹åŠ¿ï¼Œä»è€ŒæŸå®³å„ç§æ£€æµ‹æ–¹æ³•çš„æ•ˆåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‡¸æ˜¾äº†åœ¨ä½¿ç”¨æ¨ç†è¿‡ç¨‹ä¸­å¿½ç•¥çš„ä¸€ä¸ªæƒè¡¡é—®é¢˜ã€‚ä»£ç å·²å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ECNU-Text-Computing/cot-hallu-detect%E3%80%82">https://github.com/ECNU-Text-Computing/cot-hallu-detectã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17088v3">PDF</a> Accepted at EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¼šå‡ºç°ç”Ÿæˆé”™è¯¯æˆ–ä¸ç›¸å…³å†…å®¹çš„å¹»è§‰ç°è±¡ã€‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºé€šè¿‡é¼“åŠ±é€æ­¥æ¨ç†å¯ä»¥ç¼“è§£å¹»è§‰é—®é¢˜ï¼Œä½†å…¶å¯¹å¹»è§‰æ£€æµ‹çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬æ–‡è¿›è¡Œäº†ä¸€é¡¹ç³»ç»Ÿçš„å®è¯ç ”ç©¶ï¼Œä»¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚é€šè¿‡åˆæ­¥å®éªŒå‘ç°ï¼ŒCoTæ¨ç†å¯¹LLMçš„å†…éƒ¨çŠ¶æ€å’Œä»¤ç‰Œæ¦‚ç‡åˆ†å¸ƒäº§ç”Ÿäº†æ˜¾è‘—å½±å“ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬æ–‡è¯„ä¼°äº†ä¸åŒçš„CoTæç¤ºæ–¹æ³•å¯¹ä¸»æµå¹»è§‰æ£€æµ‹æ–¹æ³•çš„å†²å‡»ï¼Œæ¶‰åŠæŒ‡ä»¤å¾®è°ƒä¸é¢å‘æ¨ç†çš„LLMã€‚ç ”ç©¶å‘ç°ï¼ŒCoTæç¤ºè™½ç„¶æœ‰åŠ©äºå‡å°‘å¹»è§‰é¢‘ç‡ï¼Œä½†ä¹Ÿå¯èƒ½æ©ç›–ç”¨äºæ£€æµ‹çš„å…³é”®ä¿¡å·ï¼Œä»è€Œå½±å“å„ç§æ£€æµ‹æ–¹æ³•çš„å‡†ç¡®æ€§ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†ä½¿ç”¨æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªè¢«å¿½è§†çš„æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨ç”Ÿæˆé”™è¯¯æˆ–ä¸ç›¸å…³å†…å®¹çš„å¹»è§‰ç°è±¡ã€‚</li>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºé€šè¿‡é¼“åŠ±é€æ­¥æ¨ç†æœ‰åŠ©äºç¼“è§£LLMçš„å¹»è§‰é—®é¢˜ã€‚</li>
<li>CoTæ¨ç†å¯¹LLMçš„å†…éƒ¨çŠ¶æ€å’Œä»¤ç‰Œæ¦‚ç‡åˆ†å¸ƒäº§ç”Ÿæ˜¾è‘—å½±å“ã€‚</li>
<li>CoTæç¤ºæ–¹æ³•å¯¹ä¸»æµå¹»è§‰æ£€æµ‹æ–¹æ³•çš„å†²å‡»è¯„ä¼°æ˜¾ç¤ºï¼Œå®ƒå¯èƒ½å½±å“æ£€æµ‹æ–¹æ³•çš„å‡†ç¡®æ€§ã€‚</li>
<li>CoTæç¤ºåœ¨å‡å°‘å¹»è§‰é¢‘ç‡çš„åŒæ—¶ï¼Œä¹Ÿå¯èƒ½æ©ç›–ç”¨äºæ£€æµ‹çš„å…³é”®ä¿¡å·ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†ä½¿ç”¨æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªè¢«å¿½è§†çš„æƒè¡¡ï¼Œéœ€è¦åœ¨å‡å°‘å¹»è§‰å’Œæé«˜æ£€æµ‹å‡†ç¡®æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.17088v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.17088v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.17088v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.17088v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="UniversalCEFR-Enabling-Open-Multilingual-Research-on-Language-Proficiency-Assessment"><a href="#UniversalCEFR-Enabling-Open-Multilingual-Research-on-Language-Proficiency-Assessment" class="headerlink" title="UniversalCEFR: Enabling Open Multilingual Research on Language   Proficiency Assessment"></a>UniversalCEFR: Enabling Open Multilingual Research on Language   Proficiency Assessment</h2><p><strong>Authors:Joseph Marvin Imperial, Abdullah Barayan, Regina Stodden, Rodrigo Wilkens, Ricardo Munoz Sanchez, Lingyun Gao, Melissa Torgbi, Dawn Knight, Gail Forey, Reka R. Jablonkai, Ekaterina Kochmar, Robert Reynolds, EugÃ©nio Ribeiro, Horacio Saggion, Elena Volodina, Sowmya Vajjala, Thomas FranÃ§ois, Fernando Alva-Manchego, Harish Tayyar Madabushi</strong></p>
<p>We introduce UniversalCEFR, a large-scale multilingual and multidimensional dataset of texts annotated with CEFR (Common European Framework of Reference) levels in 13 languages. To enable open research in automated readability and language proficiency assessment, UniversalCEFR comprises 505,807 CEFR-labeled texts curated from educational and learner-oriented resources, standardized into a unified data format to support consistent processing, analysis, and modelling across tasks and languages. To demonstrate its utility, we conduct benchmarking experiments using three modelling paradigms: a) linguistic feature-based classification, b) fine-tuning pre-trained LLMs, and c) descriptor-based prompting of instruction-tuned LLMs. Our results support using linguistic features and fine-tuning pretrained models in multilingual CEFR level assessment. Overall, UniversalCEFR aims to establish best practices in data distribution for language proficiency research by standardising dataset formats, and promoting their accessibility to the global research community. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†UniversalCEFRï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè¯­è¨€ã€å¤šç»´æ–‡æœ¬æ•°æ®é›†ï¼Œç”¨CEFRï¼ˆæ¬§æ´²å…±åŒè¯­è¨€å‚è€ƒæ ‡å‡†ï¼‰çš„13ç§è¯­è¨€çš„ç­‰çº§è¿›è¡Œæ ‡æ³¨ã€‚ä¸ºäº†ä¿ƒè¿›è‡ªåŠ¨åŒ–å¯è¯»æ€§è¯„ä¼°å’Œè¯­è¨€æ°´å¹³è¯„ä¼°çš„å¼€æ”¾ç ”ç©¶ï¼ŒUniversalCEFRåŒ…å«äº†ä»æ•™è‚²å’Œå­¦ä¹ å¯¼å‘èµ„æºä¸­ç²¾å¿ƒæŒ‘é€‰çš„50ä¸‡äº”åƒå…«ç™¾é›¶ä¸ƒç¯‡CEFRæ ‡æ³¨æ–‡æœ¬ï¼Œç»Ÿä¸€çš„æ•°æ®æ ¼å¼æ”¯æŒè·¨ä»»åŠ¡å’Œè¯­è¨€çš„ç»Ÿä¸€å¤„ç†ã€åˆ†æå’Œå»ºæ¨¡ã€‚ä¸ºäº†è¯æ˜å…¶å®ç”¨æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸‰ç§å»ºæ¨¡èŒƒå¼è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼šaï¼‰åŸºäºè¯­è¨€ç‰¹å¾çš„åˆ†ç±»ï¼Œbï¼‰å¾®è°ƒé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥åŠcï¼‰åŸºäºæè¿°çš„æŒ‡ä»¤è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹çš„æç¤ºã€‚æˆ‘ä»¬çš„ç»“æœæ”¯æŒåœ¨å¤šè¯­è¨€CEFRæ°´å¹³è¯„ä¼°ä¸­ä½¿ç”¨è¯­è¨€ç‰¹å¾å’Œå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚æ€»ä½“è€Œè¨€ï¼ŒUniversalCEFRæ—¨åœ¨é€šè¿‡æ ‡å‡†åŒ–æ•°æ®é›†æ ¼å¼å’Œä¿ƒè¿›å…¨çƒç ”ç©¶ç¤¾åŒºçš„è®¿é—®ï¼Œä¸ºè¯­è¨€æ°´å¹³ç ”ç©¶å»ºç«‹æœ€ä½³çš„æ•°æ®åˆ†å¸ƒå®è·µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01419v2">PDF</a> Accepted to EMNLP 2025 (Main Conference)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†UniversalCEFRâ€”â€”ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè¯­ç§ã€å¤šç»´åº¦çš„æ–‡æœ¬æ•°æ®é›†ï¼Œé‡‡ç”¨CEFRï¼ˆæ¬§æ´²è¯­è¨€å…±åŒå‚è€ƒæ¡†æ¶ï¼‰æ°´å¹³æ ‡æ³¨ï¼ŒåŒ…å«æ•™è‚²å’Œå­¦ä¹ å¯¼å‘èµ„æºçš„50ä¸‡å¤šä¸ªCEFRæ ‡æ³¨æ–‡æœ¬ã€‚æ•°æ®é›†ç»Ÿä¸€æ ¼å¼æ”¯æŒè·¨ä»»åŠ¡å’Œè¯­è¨€çš„å¤„ç†ã€åˆ†æå’Œå»ºæ¨¡ã€‚é€šè¿‡å®éªŒéªŒè¯ï¼Œè¯æ˜äº†è¯¥æ•°æ®é›†åœ¨åŸºäºè¯­è¨€ç‰¹å¾çš„åˆ†ç±»å’Œå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ä¸­çš„å®ç”¨æ€§ã€‚UniversalCEFRæ—¨åœ¨é€šè¿‡æ ‡å‡†åŒ–æ•°æ®é›†æ ¼å¼å’Œä¿ƒè¿›å…¨çƒç ”ç©¶ç¤¾åŒºçš„è®¿é—®ï¼Œä¸ºè¯­è¨€æ°´å¹³ç ”ç©¶å»ºç«‹æœ€ä½³å®è·µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniversalCEFRæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè¯­ç§å’Œå¤šç»´åº¦æ–‡æœ¬æ•°æ®é›†ï¼ŒåŒ…å«æ•™è‚²å’Œå­¦ä¹ å¯¼å‘èµ„æºçš„CEFRæ ‡æ³¨æ–‡æœ¬ã€‚</li>
<li>æ•°æ®é›†é‡‡ç”¨ç»Ÿä¸€çš„æ ¼å¼ï¼Œæ”¯æŒè·¨ä»»åŠ¡å’Œè¯­è¨€çš„å¤„ç†ã€åˆ†æå’Œå»ºæ¨¡ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡å®éªŒéªŒè¯åœ¨åŸºäºè¯­è¨€ç‰¹å¾çš„åˆ†ç±»å’Œå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ä¸­çš„å®ç”¨æ€§ã€‚</li>
<li>è¯¥æ•°æ®é›†æœ‰åŠ©äºå»ºç«‹è¯­è¨€æ°´å¹³ç ”ç©¶çš„æœ€ä½³å®è·µï¼Œé€šè¿‡æ ‡å‡†åŒ–æ•°æ®é›†æ ¼å¼æ¥ä¿ƒè¿›å…¨çƒç ”ç©¶ç¤¾åŒºçš„è®¿é—®ã€‚</li>
<li>UniversalCEFRåŒ…å«ä¸‰ç§å»ºæ¨¡èŒƒå¼ï¼šåŸºäºè¯­è¨€ç‰¹å¾çš„åˆ†ç±»ã€å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹å’ŒåŸºäºæè¿°ç¬¦çš„æŒ‡ä»¤å¾®è°ƒLLMsã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯­è¨€ç‰¹å¾å’Œå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¤šè¯­ç§CEFRæ°´å¹³è¯„ä¼°æ˜¯æœ‰æ•ˆçš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.01419v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.01419v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.01419v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Game-RL-Synthesizing-Verifiable-Game-Tasks-at-Scale-to-Boost-VLMs-General-Reasoning"><a href="#Game-RL-Synthesizing-Verifiable-Game-Tasks-at-Scale-to-Boost-VLMs-General-Reasoning" class="headerlink" title="Game-RL: Synthesizing Verifiable Game Tasks at Scale to Boost VLMs   General Reasoning"></a>Game-RL: Synthesizing Verifiable Game Tasks at Scale to Boost VLMs   General Reasoning</h2><p><strong>Authors:Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Changhao Jiang, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang</strong></p>
<p>Real-world vision language reasoning scenarios often include diverse and complex tasks. However, vision language reinforcement learning has primarily focused on a narrow set of tasks (e.g. geometry or chart reasoning), limiting the improvement of Vision Language Modelsâ€™ (VLMs) general reasoning. Therefore, we propose a novel Code2Logic approach, using Large Language Models (LLMs) to synthesize verifiable game reasoning tasks at scale via adapting game code. Using the Code2Logic, we developed the GameQA dataset to train and evaluate VLMs. GameQA is verifiable and scalable, offers controllable difficulty gradation and is diverse with 30 games and 158 tasks. Then we apply Game-RL, which is simple reinforcement learning on GameQA. Surprisingly, despite training solely on game tasks, VLMs demonstrated out of domain generalization, specifically Qwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language benchmarks. Our code, dataset and models are available at the GitHub repository. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œä¸­çš„è§†è§‰è¯­è¨€æ¨ç†åœºæ™¯é€šå¸¸åŒ…å«å„ç§å¤æ‚å¤šæ ·çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè§†è§‰è¯­è¨€å¼ºåŒ–å­¦ä¹ ä¸»è¦é›†ä¸­åœ¨ç‹­çª„çš„ä»»åŠ¡é›†ä¸Šï¼ˆä¾‹å¦‚å‡ ä½•æˆ–å›¾è¡¨æ¨ç†ï¼‰ï¼Œè¿™é™åˆ¶äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ä¸€èˆ¬æ¨ç†èƒ½åŠ›çš„æ”¹è¿›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„Code2Logicæ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é€‚åº”æ¸¸æˆä»£ç æ¥å¤§è§„æ¨¡åˆæˆå¯éªŒè¯çš„æ¸¸æˆæ¨ç†ä»»åŠ¡ã€‚ä½¿ç”¨Code2Logicï¼Œæˆ‘ä»¬å¼€å‘äº†GameQAæ•°æ®é›†æ¥è®­ç»ƒå’Œè¯„ä¼°VLMã€‚GameQAå…·æœ‰å¯éªŒè¯æ€§å’Œå¯æ‰©å±•æ€§ï¼Œæä¾›å¯æ§çš„éš¾åº¦æ¢¯åº¦ï¼Œå¹¶ä¸”å…·æœ‰30ç§æ¸¸æˆå’Œ158ä¸ªä»»åŠ¡ï¼Œç§ç±»ç¹å¤šã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹GameQAåº”ç”¨Game-RLï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„å¼ºåŒ–å­¦ä¹ ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå°½ç®¡ä»…åœ¨æ¸¸æˆä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒVLMè¡¨ç°å‡ºäº†è·¨åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯Qwen2.5-VL-7Båœ¨7ä¸ªä¸åŒçš„è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šæ€§èƒ½æé«˜äº†2.33%ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹å¯åœ¨GitHubä»“åº“ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13886v4">PDF</a> 63 pages, 23 figures, submitted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Code2Logicæ–¹æ³•ï¼Œé€šè¿‡é€‚åº”æ¸¸æˆä»£ç ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åˆæˆå¯éªŒè¯çš„æ¸¸æˆæ¨ç†ä»»åŠ¡ã€‚åŒæ—¶å¼€å‘äº†GameQAæ•°æ®é›†ä»¥è®­ç»ƒå’Œè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚è¯¥æ–¹æ³•æ³¨é‡å¤šæ ·æ€§å’Œå¯æ§éš¾åº¦åˆ†çº§ï¼Œå±•ç¤ºäº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸‹çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚æ¨¡å‹çš„ä»£ç ã€æ•°æ®é›†å¯åœ¨GitHubä»“åº“ä¸­è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Code2Logicæ–¹æ³•é€šè¿‡é€‚åº”æ¸¸æˆä»£ç æ¥åˆæˆå¤§è§„æ¨¡å¯éªŒè¯çš„æ¨ç†ä»»åŠ¡ã€‚</li>
<li>å¼€å‘çš„GameQAæ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚</li>
<li>GameQAæ•°æ®é›†åŒ…å«30ä¸ªæ¸¸æˆå’Œ158ä¸ªä»»åŠ¡ï¼Œå…·æœ‰å¤šæ ·æ€§å’Œå¯æ§éš¾åº¦åˆ†çº§çš„ç‰¹ç‚¹ã€‚</li>
<li>é‡‡ç”¨åŸºäºæ¸¸æˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆGame-RLï¼‰æ¥è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸‹å±•ç°å‡ºè·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å…·ä½“æ¨¡å‹Qwen2.5-VL-7Båœ¨7ä¸ªä¸åŒçš„è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æå‡2.33%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2505.13886v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2505.13886v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2505.13886v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2505.13886v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2505.13886v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2505.13886v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SuPreME-A-Supervised-Pre-training-Framework-for-Multimodal-ECG-Representation-Learning"><a href="#SuPreME-A-Supervised-Pre-training-Framework-for-Multimodal-ECG-Representation-Learning" class="headerlink" title="SuPreME: A Supervised Pre-training Framework for Multimodal ECG   Representation Learning"></a>SuPreME: A Supervised Pre-training Framework for Multimodal ECG   Representation Learning</h2><p><strong>Authors:Mingsheng Cai, Jiuming Jiang, Wenhao Huang, Che Liu, Rossella Arcucci</strong></p>
<p>Cardiovascular diseases are a leading cause of death and disability worldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring cardiac health, but obtaining large-scale annotated ECG datasets is labor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL) methods mitigate this by learning features without extensive labels but fail to capture fine-grained clinical semantics and require extensive task-specific fine-tuning. To address these challenges, we propose $\textbf{SuPreME}$, a $\textbf{Su}$pervised $\textbf{Pre}$-training framework for $\textbf{M}$ultimodal $\textbf{E}$CG representation learning. SuPreME is pre-trained using structured diagnostic labels derived from ECG report entities through a one-time offline extraction with Large Language Models (LLMs), which help denoise, standardize cardiac concepts, and improve clinical representation learning. By fusing ECG signals with textual cardiac queries instead of fixed labels, SuPreME enables zero-shot classification of unseen conditions without further fine-tuning. We evaluate SuPreME on six downstream datasets covering 106 cardiac conditions, achieving superior zero-shot AUC performance of $77.20%$, surpassing state-of-the-art eSSLs by $4.98%$. Results demonstrate SuPreMEâ€™s effectiveness in leveraging structured, clinically relevant knowledge for high-quality ECG representations. </p>
<blockquote>
<p>å¿ƒè¡€ç®¡ç–¾ç—…æ˜¯å…¨çƒèŒƒå›´å†…å¯¼è‡´æ­»äº¡å’Œæ®‹ç–¾çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚å¿ƒç”µå›¾ï¼ˆECGï¼‰å¯¹äºè¯Šæ–­å’Œæ²»ç–—å¿ƒè„ç–¾ç—…è‡³å…³é‡è¦ï¼Œä½†è·å–å¤§è§„æ¨¡æ ‡æ³¨çš„å¿ƒç”µå›¾æ•°æ®é›†æ˜¯ä¸€é¡¹åŠ³åŠ¨å¯†é›†ä¸”è€—æ—¶çš„å·¥ä½œã€‚æœ€è¿‘çš„å¿ƒç”µå›¾è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆeSSLï¼‰é€šè¿‡æ— éœ€å¤§é‡æ ‡ç­¾å³å¯å­¦ä¹ ç‰¹å¾æ¥å‡è½»è¿™ä¸€è´Ÿæ‹…ï¼Œä½†æ— æ³•æ•æ‰ç²¾ç»†çš„ä¸´åºŠè¯­ä¹‰ï¼Œå¹¶éœ€è¦å¤§é‡é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>SuPreME</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº<strong>å¤šæ¨¡æ€å¿ƒç”µå›¾è¡¨ç¤ºå­¦ä¹ çš„ç›‘ç£é¢„è®­ç»ƒæ¡†æ¶</strong>ã€‚SuPreMEä½¿ç”¨é€šè¿‡ä¸€æ¬¡æ€§ç¦»çº¿æå–å¿ƒç”µå›¾æŠ¥å‘Šå®ä½“è€Œè·å¾—çš„ç»“æ„åŒ–è¯Šæ–­æ ‡ç­¾è¿›è¡Œé¢„è®­ç»ƒï¼Œå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸®åŠ©å»å™ªã€æ ‡å‡†åŒ–å¿ƒè„æ¦‚å¿µï¼Œæ”¹è¿›ä¸´åºŠè¡¨ç¤ºå­¦ä¹ ã€‚é€šè¿‡å°†å¿ƒç”µå›¾ä¿¡å·ä¸æ–‡æœ¬å¿ƒè„æŸ¥è¯¢è€Œä¸æ˜¯å›ºå®šæ ‡ç­¾èåˆï¼ŒSuPreMEå®ç°äº†å¯¹æœªè§æ¡ä»¶çš„é›¶æ ·æœ¬åˆ†ç±»ï¼Œæ— éœ€è¿›ä¸€æ­¥å¾®è°ƒã€‚æˆ‘ä»¬åœ¨åŒ…å«106ç§å¿ƒè„ç–¾ç—…çš„å…­ä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šè¯„ä¼°SuPreMEï¼Œå…¶é›¶æ ·æœ¬AUCæ€§èƒ½è¾¾åˆ°77.20%ï¼Œæ¯”æœ€å…ˆè¿›çš„eSSLé«˜å‡º4.98%ã€‚ç»“æœè¡¨æ˜ï¼ŒSuPreMEåœ¨åˆ©ç”¨ç»“æ„åŒ–ã€ä¸ä¸´åºŠç›¸å…³çš„çŸ¥è¯†æ–¹é¢è¿›è¡Œé«˜è´¨é‡å¿ƒç”µå›¾è¡¨ç¤ºæ–¹é¢éå¸¸æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19668v3">PDF</a> Findings of The 2025 Conference on Empirical Methods in Natural   Language Processing (EMNLP 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¿ƒè¡€ç®¡ç–¾ç—…çš„ä¸¥é‡æ€§åŠå…¶å¯¹å…¨çƒå¥åº·çš„å½±å“ã€‚å¿ƒç”µå›¾ï¼ˆECGï¼‰åœ¨è¯Šæ–­å’Œç›‘æµ‹å¿ƒè„å¥åº·æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä½†è·å–å¤§è§„æ¨¡æ³¨é‡Šçš„å¿ƒç”µå›¾æ•°æ®é›†æ˜¯åŠ³åŠ¨å¯†é›†å‹çš„ï¼Œéœ€è¦å¤§é‡æ—¶é—´ã€‚æœ€æ–°çš„å¿ƒç”µå›¾è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆeSSLï¼‰å¯ä»¥åœ¨æ²¡æœ‰å¤§é‡æ ‡ç­¾çš„æƒ…å†µä¸‹å­¦ä¹ ç‰¹å¾ï¼Œä½†æ— æ³•æ•æ‰ç²¾ç»†çš„ä¸´åºŠè¯­ä¹‰ï¼Œå¹¶éœ€è¦å¤§é‡é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†SuPreMEï¼Œä¸€ä¸ªç”¨äºå¤šæ¨¡æ€å¿ƒç”µå›¾è¡¨ç¤ºå­¦ä¹ çš„ç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ã€‚SuPreMEä½¿ç”¨ä»å¿ƒç”µå›¾æŠ¥å‘Šå®ä½“ä¸­æå–çš„ç»“æ„åŒ–è¯Šæ–­æ ‡ç­¾è¿›è¡Œé¢„è®­ç»ƒï¼Œå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸€æ¬¡ç¦»çº¿æå–æ•°æ®ï¼Œå¸®åŠ©å»å™ªã€æ ‡å‡†åŒ–å¿ƒè„æ¦‚å¿µï¼Œæé«˜ä¸´åºŠè¡¨ç¤ºå­¦ä¹ ã€‚é€šè¿‡å°†å¿ƒç”µå›¾ä¿¡å·ä¸æ–‡æœ¬å¿ƒè„æŸ¥è¯¢èåˆè€Œéå›ºå®šæ ‡ç­¾ï¼ŒSuPreMEå®ç°äº†å¯¹æœªè§æ¡ä»¶çš„é›¶æ ·æœ¬åˆ†ç±»ï¼Œæ— éœ€è¿›ä¸€æ­¥å¾®è°ƒã€‚åœ¨æ¶µç›–106ç§å¿ƒè„ç–¾ç—…çš„å…­ä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šè¯„ä¼°æ˜¾ç¤ºï¼ŒSuPreMEçš„é›¶æ ·æœ¬AUCæ€§èƒ½è¾¾åˆ°77.2%ï¼Œæ¯”æœ€å…ˆè¿›çš„eSSLé«˜å‡º4.98%ï¼Œè¯æ˜äº†åˆ©ç”¨ç»“æ„åŒ–ã€ä¸´åºŠç›¸å…³çŸ¥è¯†è¿›è¡Œé«˜è´¨é‡å¿ƒç”µå›¾è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¿ƒè¡€ç®¡ç–¾ç—…æ˜¯å…¨çƒæ­»äº¡å’Œæ®‹ç–¾çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œè€Œå¿ƒç”µå›¾ï¼ˆECGï¼‰åœ¨è¯Šæ–­å¿ƒè„å¥åº·æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚</li>
<li>è·å–å¤§è§„æ¨¡æ³¨é‡Šçš„å¿ƒç”µå›¾æ•°æ®é›†æ˜¯ä¸€ä¸ªåŠ³åŠ¨å¯†é›†å’Œæ—¶é—´å¯†é›†çš„è¿‡ç¨‹ã€‚</li>
<li>æœ€æ–°å¿ƒç”µå›¾è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆeSSLï¼‰å¯ä»¥åœ¨æ²¡æœ‰å¤§é‡æ ‡ç­¾çš„æƒ…å†µä¸‹å­¦ä¹ ç‰¹å¾ï¼Œä½†ç¼ºä¹æ•æ‰ä¸´åºŠè¯­ä¹‰çš„èƒ½åŠ›ï¼Œå¹¶éœ€è¦å¤§é‡ä»»åŠ¡ç‰¹å®šå¾®è°ƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•SuPreMEï¼Œä¸€ä¸ªç”¨äºå¤šæ¨¡æ€å¿ƒç”µå›¾è¡¨ç¤ºå­¦ä¹ çš„ç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>SuPreMEä½¿ç”¨ä»å¿ƒç”µå›¾æŠ¥å‘Šå®ä½“ä¸­æå–çš„ç»“æ„åŒ–è¯Šæ–­æ ‡ç­¾è¿›è¡Œé¢„è®­ç»ƒï¼Œå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>SuPreMEé€šè¿‡å°†å¿ƒç”µå›¾ä¿¡å·ä¸æ–‡æœ¬å¿ƒè„æŸ¥è¯¢èåˆæ¥å®ç°å¯¹æœªè§æ¡ä»¶çš„é›¶æ ·æœ¬åˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2502.19668v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2502.19668v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2502.19668v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2502.19668v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="SEVEN-Pruning-Transformer-Model-by-Reserving-Sentinels"><a href="#SEVEN-Pruning-Transformer-Model-by-Reserving-Sentinels" class="headerlink" title="SEVEN: Pruning Transformer Model by Reserving Sentinels"></a>SEVEN: Pruning Transformer Model by Reserving Sentinels</h2><p><strong>Authors:Jinying Xiao, Ping Li, Jie Nie, Zhe Tang</strong></p>
<p>Large-scale Transformer models (TM) have demonstrated outstanding performance across various tasks. However, their considerable parameter size restricts their applicability, particularly on mobile devices. Due to the dynamic and intricate nature of gradients on TM compared to Convolutional Neural Networks, commonly used pruning methods tend to retain weights with larger gradient noise. This results in pruned models that are sensitive to sparsity and datasets, exhibiting suboptimal performance. Symbolic Descent (SD) is a general approach for training and fine-tuning TM. In this paper, we attempt to describe the noisy batch gradient sequences on TM through the cumulative process of SD. We utilize this design to dynamically assess the importance scores of weights.SEVEN is introduced by us, which particularly favors weights with consistently high sensitivity, i.e., weights with small gradient noise. These weights are tended to be preserved by SEVEN. Extensive experiments on various TM in natural language, question-answering, and image classification domains are conducted to validate the effectiveness of SEVEN. The results demonstrate significant improvements of SEVEN in multiple pruning scenarios and across different sparsity levels. Additionally, SEVEN exhibits robust performance under various fine-tuning strategies. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/xiaojinying/SEVEN">https://github.com/xiaojinying/SEVEN</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡Transformeræ¨¡å‹ï¼ˆTMï¼‰å·²åœ¨å„ç§ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…¶åºå¤§çš„å‚æ•°è§„æ¨¡é™åˆ¶äº†å…¶åº”ç”¨åœºæ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šã€‚ä¸å·ç§¯ç¥ç»ç½‘ç»œç›¸æ¯”ï¼ŒTransformeræ¨¡å‹çš„æ¢¯åº¦å…·æœ‰åŠ¨æ€å’Œå¤æ‚çš„ç‰¹ç‚¹ï¼Œå¸¸ç”¨çš„å‰ªææ–¹æ³•å¾€å¾€ä¿ç•™å…·æœ‰è¾ƒå¤§æ¢¯åº¦å™ªå£°çš„æƒé‡ã€‚è¿™å¯¼è‡´å‰ªææ¨¡å‹å¯¹ç¨€ç–æ€§å’Œæ•°æ®é›†æ•æ„Ÿï¼Œè¡¨ç°å‡ºæ¬¡ä¼˜æ€§èƒ½ã€‚ç¬¦å·ä¸‹é™ï¼ˆSDï¼‰æ˜¯ä¸€ç§ç”¨äºè®­ç»ƒå’Œå¾®è°ƒTMçš„é€šç”¨æ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯•å›¾é€šè¿‡SDçš„ç´¯ç§¯è¿‡ç¨‹æ¥æè¿°TMä¸Šçš„å™ªå£°æ‰¹é‡æ¢¯åº¦åºåˆ—ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ç§è®¾è®¡æ¥åŠ¨æ€è¯„ä¼°æƒé‡çš„é‡è¦æ€§åˆ†æ•°ã€‚æˆ‘ä»¬æ¨å‡ºäº†SEVENï¼Œå®ƒç‰¹åˆ«å€¾å‘äºä¿ç•™é‚£äº›æŒç»­é«˜æ•æ„Ÿæ€§çš„æƒé‡ï¼Œå³å…·æœ‰è¾ƒå°æ¢¯åº¦å™ªå£°çš„æƒé‡ã€‚å¹¿æ³›åœ°åœ¨è‡ªç„¶è¯­è¨€ã€é—®ç­”å’Œå›¾åƒåˆ†ç±»é¢†åŸŸçš„å„ç§TMä¸Šè¿›è¡Œçš„å®éªŒéªŒè¯äº†SEVENçš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒSEVENåœ¨å¤šç§å‰ªæåœºæ™¯å’Œä¸åŒç¨€ç–æ€§æ°´å¹³ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒSEVENåœ¨å„ç§å¾®è°ƒç­–ç•¥ä¸‹è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/xiaojinying/SEVEN%E3%80%82">https://github.com/xiaojinying/SEVENã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.12688v2">PDF</a> IJCNN 2024</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹Transformeræ¨¡å‹ï¼ˆTMï¼‰åœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å…¶åºå¤§çš„å‚æ•°è§„æ¨¡é™åˆ¶äº†å…¶åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSEVENçš„æ–¹æ³•ï¼Œé€šè¿‡Symbolic Descentï¼ˆSDï¼‰çš„ç´¯ç§¯è¿‡ç¨‹æè¿°TMä¸Šçš„å™ªå£°æ‰¹æ¬¡æ¢¯åº¦åºåˆ—ï¼Œå¹¶æ®æ­¤åŠ¨æ€è¯„ä¼°æƒé‡çš„é‡è¦æ€§å¾—åˆ†ã€‚SEVENå€¾å‘äºä¿ç•™å…·æœ‰æŒç»­é«˜æ•æ„Ÿæ€§çš„æƒé‡ï¼Œå³å…·æœ‰å°æ¢¯åº¦å™ªå£°çš„æƒé‡ã€‚å®éªŒè¯æ˜ï¼ŒSEVENåœ¨å¤šç§å‰ªæåœºæ™¯å’Œä¸åŒç¨€ç–åº¦çº§åˆ«ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”åœ¨å„ç§å¾®è°ƒç­–ç•¥ä¸‹å±•ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹Transformeræ¨¡å‹ï¼ˆTMï¼‰å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œä½†å‚æ•°è§„æ¨¡é™åˆ¶äº†å…¶åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„åº”ç”¨ã€‚</li>
<li>TMçš„æ¢¯åº¦å…·æœ‰åŠ¨æ€å’Œå¤æ‚çš„ç‰¹ç‚¹ï¼Œå¸¸è§„å‰ªææ–¹æ³•å®¹æ˜“ä¿ç•™å¸¦æœ‰è¾ƒå¤§æ¢¯åº¦å™ªå£°çš„æƒé‡ã€‚</li>
<li>Symbolic Descentï¼ˆSDï¼‰æä¾›äº†ä¸€ç§è®­ç»ƒå’Œä¼˜åŒ–TMçš„é€šç”¨æ–¹æ³•ã€‚</li>
<li>SEVENæ–¹æ³•é€šè¿‡SDçš„ç´¯ç§¯è¿‡ç¨‹æè¿°TMä¸Šçš„å™ªå£°æ‰¹æ¬¡æ¢¯åº¦åºåˆ—ã€‚</li>
<li>SEVENå€¾å‘äºä¿ç•™å…·æœ‰æŒç»­é«˜æ•æ„Ÿæ€§å’Œå°æ¢¯åº¦å™ªå£°çš„æƒé‡ã€‚</li>
<li>å®éªŒè¯æ˜SEVENåœ¨å¤šç§å‰ªæåœºæ™¯å’Œä¸åŒç¨€ç–åº¦çº§åˆ«ä¸Šæ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.12688">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2403.12688v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2403.12688v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2403.12688v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2403.12688v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2403.12688v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2403.12688v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-18/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-18/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-18/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Agent/2508.16044v2/page_0_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-18  Scaling Agents via Continual Pre-training
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-18/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12883v1/page_0_0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-18  WebResearcher Unleashing unbounded reasoning capability in Long-Horizon   Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29739.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
