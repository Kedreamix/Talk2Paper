<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-09-18  Scaling Agents via Continual Pre-training">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13310v1/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    66 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-18-更新"><a href="#2025-09-18-更新" class="headerlink" title="2025-09-18 更新"></a>2025-09-18 更新</h1><h2 id="Scaling-Agents-via-Continual-Pre-training"><a href="#Scaling-Agents-via-Continual-Pre-training" class="headerlink" title="Scaling Agents via Continual Pre-training"></a>Scaling Agents via Continual Pre-training</h2><p><strong>Authors:Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, Chenxiong Qian, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou</strong></p>
<p>Large language models (LLMs) have evolved into agentic systems capable of autonomous tool use and multi-step reasoning for complex problem-solving. However, post-training approaches building upon general-purpose foundation models consistently underperform in agentic tasks, particularly in open-source implementations. We identify the root cause: the absence of robust agentic foundation models forces models during post-training to simultaneously learn diverse agentic behaviors while aligning them to expert demonstrations, thereby creating fundamental optimization tensions. To this end, we are the first to propose incorporating Agentic Continual Pre-training (Agentic CPT) into the deep research agents training pipeline to build powerful agentic foundational models. Based on this approach, we develop a deep research agent model named AgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve state-of-the-art performance while retains strong tool-use ability, notably 39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE. </p>
<blockquote>
<p>大型语言模型（LLM）已经发展成为了具有自主工具使用和复杂问题解决能力的多步骤推理的代理系统。然而，基于通用基础模型的后续训练方法在代理任务中的表现持续不佳，特别是在开源实现中。我们确定了根本原因：缺乏稳健的代理基础模型迫使模型在后续训练过程中同时学习多样化的代理行为并使其与专家演示保持一致，从而产生了基本的优化张力。为此，我们首次提出在深度研究代理训练管道中融入代理持续预训练（Agentic CPT），以构建强大的代理基础模型。基于这种方法，我们开发了一个名为AgentFounder的深度研究代理模型。我们在10个基准测试上对AgentFounder-30B进行了评估，取得了卓越的性能，同时保持了强大的工具使用能力，特别是在BrowseComp-en上达到39.9%，BrowseComp-zh上达到43.3%，HLE上Pass@1达到31.5%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13310v1">PDF</a> <a target="_blank" rel="noopener" href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/">https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）已进化为能够进行自主工具使用和多步骤推理以解决复杂问题的代理系统。然而，基于通用基础模型的后期训练方法在代理任务中表现持续不佳，特别是在开源实现中。问题的根源在于缺乏稳健的代理基础模型，导致模型在训练后必须同时学习多种代理行为并与专家演示对齐，从而产生基本的优化紧张。为此，我们首次提出在深度研究代理训练管道中融入代理持续预训练（Agentic CPT），以构建强大的代理基础模型。我们基于此方法开发了一个名为AgentFounder的深度研究代理模型。我们在10个基准测试上对AgentFounder-30B进行了评估，取得了最新技术性能，同时保持了强大的工具使用能力，特别是在BrowseComp-en上达到39.9%、BrowseComp-zh上达到43.3%，以及HLE上Pass@1达到31.5%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs已发展为具有自主工具使用和复杂问题多步骤推理能力的代理系统。</li>
<li>基于通用基础模型的后期训练在代理任务中表现欠佳，特别是在开源实现中。</li>
<li>缺乏稳健的代理基础模型导致模型在训练后面临优化挑战。</li>
<li>首次提出融入Agentic CPT到深度研究代理训练管道中，旨在构建强大的代理基础模型。</li>
<li>开发了一个名为AgentFounder的深度研究代理模型。</li>
<li>AgentFounder-30B在多个基准测试中取得最新技术性能，具有强大的工具使用能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13310">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13310v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13310v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13310v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13310v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-assisted-Meta-optimizer-for-Automated-Design-of-Constrained-Evolutionary-Algorithm"><a href="#Large-Language-Model-assisted-Meta-optimizer-for-Automated-Design-of-Constrained-Evolutionary-Algorithm" class="headerlink" title="Large Language Model-assisted Meta-optimizer for Automated Design of   Constrained Evolutionary Algorithm"></a>Large Language Model-assisted Meta-optimizer for Automated Design of   Constrained Evolutionary Algorithm</h2><p><strong>Authors:Xu Yang, Rui Wang, Kaiwen Li, Wenhua Li, Weixiong Huang</strong></p>
<p>Meta-black-box optimization has been significantly advanced through the use of large language models (LLMs), yet in fancy on constrained evolutionary optimization. In this work, AwesomeDE is proposed that leverages LLMs as the strategy of meta-optimizer to generate update rules for constrained evolutionary algorithm without human intervention. On the meanwhile, $RTO^2H$ framework is introduced for standardize prompt design of LLMs. The meta-optimizer is trained on a diverse set of constrained optimization problems. Key components, including prompt design and iterative refinement, are systematically analyzed to determine their impact on design quality. Experimental results demonstrate that the proposed approach outperforms existing methods in terms of computational efficiency and solution accuracy. Furthermore, AwesomeDE is shown to generalize well across distinct problem domains, suggesting its potential for broad applicability. This research contributes to the field by providing a scalable and data-driven methodology for automated constrained algorithm design, while also highlighting limitations and directions for future work. </p>
<blockquote>
<p>基于大型语言模型（LLM）的元黑箱优化在受限进化优化方面取得了显著进展。在这项工作中，提出了AwesomeDE，它利用LLM作为元优化器的策略，为约束进化算法生成更新规则，无需人工干预。同时，介绍了$RTO^2H$框架，用于标准化LLM的提示设计。元优化器在多种约束优化问题上进行了训练。系统地分析了关键组件，包括提示设计和迭代优化，以确定它们对设计质量的影响。实验结果表明，所提出的方法在计算效率和求解精度方面优于现有方法。此外，AwesomeDE在多个不同的问题域中都表现良好，这表明其广泛的应用潜力。本研究为自动化约束算法设计提供了一种可扩展和数据驱动的方法论，同时指出了未来的局限性和研究方向，为这一领域做出了贡献。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13251v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的元黑箱优化取得了显著进展，通过提出AwesomeDE方法，利用LLM作为元优化器生成进化算法的更新规则，无需人工干预。同时，引入$RTO^2H$框架标准化LLM的提示设计。该元优化器在多种约束优化问题上进行了训练，并对提示设计和迭代优化等关键组件进行了系统分析。实验结果表明，该方法在计算效率和解精度上均优于现有方法，并且在不同问题域中表现出良好的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AwesomeDE利用LLM作为元优化器，生成进化算法的更新规则，实现了无需人工干预的自动化优化。</li>
<li>$RTO^2H$框架用于标准化LLM的提示设计，提高了算法设计的标准化和效率。</li>
<li>该方法在多种约束优化问题上进行训练，增强了元优化器的泛化能力。</li>
<li>提示设计和迭代优化等关键组件对设计质量有显著影响。</li>
<li>实验结果表明，AwesomeDE在计算效率和解精度上优于现有方法。</li>
<li>该研究为自动化约束算法设计提供了可伸缩和数据驱动的方法论。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13251">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13251v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13251v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13251v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13251v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Don’t-Forget-the-Nonlinearity-Unlocking-Activation-Functions-in-Efficient-Fine-Tuning"><a href="#Don’t-Forget-the-Nonlinearity-Unlocking-Activation-Functions-in-Efficient-Fine-Tuning" class="headerlink" title="Don’t Forget the Nonlinearity: Unlocking Activation Functions in   Efficient Fine-Tuning"></a>Don’t Forget the Nonlinearity: Unlocking Activation Functions in   Efficient Fine-Tuning</h2><p><strong>Authors:Bo Yin, Xingyi Yang, Xinchao Wang</strong></p>
<p>Existing parameter-efficient fine-tuning (PEFT) methods primarily adapt weight matrices while keeping activation functions fixed. We introduce \textbf{NoRA}, the first PEFT framework that directly adapts nonlinear activation functions in pretrained transformer-based models. NoRA replaces fixed activations with learnable rational functions and applies structured low-rank updates to numerator and denominator coefficients, with a group-wise design that localizes adaptation and improves stability at minimal cost. On vision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceeds full fine-tuning while updating only 0.4% of parameters (0.02M), achieving accuracy gains of +0.17% and +0.27%. When combined with LoRA (\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgets by adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++ consistently improves generation quality, yielding average MMLU gains of +0.3%–0.8%, including +1.6% on STEM (Alpaca) and +1.3% on OpenOrca. We further show that NoRA constrains adaptation to a low-dimensional functional subspace, implicitly regularizing update magnitude and direction. These results establish activation-space tuning as a complementary and highly parameter-efficient alternative to weight-based PEFT, positioning activation functions as first-class objects for model adaptation. </p>
<blockquote>
<p>现有参数高效微调（PEFT）方法主要适应权重矩阵，同时保持激活函数固定。我们引入了<strong>NoRA</strong>，这是第一个直接适应预训练transformer模型非线性激活函数的PEFT框架。NoRA用可学习的有理函数替换固定激活函数，对分子和分母系数应用结构化低秩更新，采用分组设计实现局部适应，并在几乎不增加成本的情况下提高稳定性。在CIFAR-10和CIFAR-100上训练的视觉transformer中，NoRA仅更新0.4%（即0.02M）的参数即可达到或超过完全微调的效果，并分别提高了+0.17%和+0.27%的准确率。当与LoRA结合时（**NoRA++**），在匹配的训练预算下，它通过添加更少的可训练参数，超越了LoRA和DoRA。在LLaMA3-8B指令调整中，NoRA++持续提高生成质量，平均MMLU增益为+0.3%~+0.8%，其中STEM（Alpaca）上提高了+1.6%，OpenOrca上提高了+1.3%。我们进一步表明，NoRA将适应限制在低维函数子空间内，隐含地正则化更新幅度和方向。这些结果确立了激活空间调整作为基于权重的PEFT的互补和高度参数高效的替代方案，将激活函数定位为模型适应的一流对象。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13240v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新的参数高效微调（PEFT）方法——NoRA，它直接适应预训练transformer模型中的非线性激活函数。NoRA用可学习的有理函数替换固定激活，并对分子和分母系数应用结构化低秩更新，以局部化适应并提高稳定性，同时成本极低。在CIFAR-10和CIFAR-100的视觉转换器训练上，NoRA在仅更新0.4%的参数（0.02M）的情况下，达到了与全微调相匹配或更好的效果，准确率分别提高了+0.17%和+0.27%。与LoRA结合（NoRA++）后，在匹配的训练预算下，通过添加更少的可训练参数，它优于LoRA和DoRA。在LLaMA3-8B指令调整中，NoRA++持续提高生成质量，平均MMLU增益为+0.3%~+0.8%，其中STEM（Alpaca）和OpenOrca分别提高了+1.6%和+1.3%。此外，NoRA将适应限制在低维函数子空间中，隐式地正则化更新幅度和方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NoRA是一种新的参数高效微调（PEFT）方法，专注于适应预训练transformer模型中的非线性激活函数。</li>
<li>NoRA通过用可学习的有理函数替换固定激活，以及应用结构化低秩更新来提高模型性能。</li>
<li>在CIFAR-10和CIFAR-100的视觉转换器训练中，NoRA实现了较高的准确率提升，同时参数更新极少。</li>
<li>NoRA++是NoRA与LoRA的结合，它在匹配的训练预算下表现更佳。</li>
<li>在LLaMA3-8B指令调整中，NoRA++显著提高了生成质量。</li>
<li>NoRA将模型适应限制在低维函数子空间中，隐式地控制更新幅度和方向。</li>
<li>激活函数在模型适应中扮演重要角色，是参数高效微调的一种重要手段。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13240">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13240v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13240v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13240v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="The-Few-shot-Dilemma-Over-prompting-Large-Language-Models"><a href="#The-Few-shot-Dilemma-Over-prompting-Large-Language-Models" class="headerlink" title="The Few-shot Dilemma: Over-prompting Large Language Models"></a>The Few-shot Dilemma: Over-prompting Large Language Models</h2><p><strong>Authors:Yongjian Tang, Doruk Tuncel, Christian Koerner, Thomas Runkler</strong></p>
<p>Over-prompting, a phenomenon where excessive examples in prompts lead to diminished performance in Large Language Models (LLMs), challenges the conventional wisdom about in-context few-shot learning. To investigate this few-shot dilemma, we outline a prompting framework that leverages three standard few-shot selection methods - random sampling, semantic embedding, and TF-IDF vectors - and evaluate these methods across multiple LLMs, including GPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral. Our experimental results reveal that incorporating excessive domain-specific examples into prompts can paradoxically degrade performance in certain LLMs, which contradicts the prior empirical conclusion that more relevant few-shot examples universally benefit LLMs. Given the trend of LLM-assisted software engineering and requirement analysis, we experiment with two real-world software requirement classification datasets. By gradually increasing the number of TF-IDF-selected and stratified few-shot examples, we identify their optimal quantity for each LLM. This combined approach achieves superior performance with fewer examples, avoiding the over-prompting problem, thus surpassing the state-of-the-art by 1% in classifying functional and non-functional requirements. </p>
<blockquote>
<p>过度提示现象，即在提示中提供过多例子导致大型语言模型（LLM）性能下降，这一现象对传统的上下文内少量样本学习智慧提出了挑战。为了研究这一少量样本困境，我们概述了一个提示框架，该框架利用三种标准的少量样本选择方法——随机抽样、语义嵌入和TF-IDF向量，并对多个LLM进行了评估，包括GPT-4o、GPT-3.5-turbo、DeepSeek-V3、Gemma-3、LLaMA-3.1、LLaMA-3.2和Mistral。我们的实验结果表明，在提示中融入过多的特定领域例子可能会在某些LLM中适得其反，这与之前的经验结论相悖，即更多的相关少量样本普遍对LLM有益。考虑到LLM辅助的软件工程和需求分析趋势，我们在两个真实世界的软件需求分类数据集上进行了实验。通过逐渐增加TF-IDF选择和分层选择的少量样本数量，我们确定了每个LLM的最佳数量。这种结合方法使用较少的例子就能实现卓越性能，避免了过度提示问题，从而在分类功能和非功能需求方面超越了最新技术1%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13196v1">PDF</a> accepted for the main track of FLLM</p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型（LLM）中过度提示的现象，即过多的示例导致性能下降。为应对这一挑战，研究团队提出了一个提示框架，利用随机抽样、语义嵌入和TF-IDF向量三种标准方法选择示例，并在多个LLM上进行评估。实验结果显示，在某些LLM中，加入过多的领域特定示例会适得其反，与先前的结论相反，更多相关的示例并不总是能提高LLM的性能。针对软件工程和需求分析的LLM辅助趋势，研究团队使用真实软件需求分类数据集进行实验，通过逐步增加TF-IDF选择和分层选择的示例数量，找到了每个LLM的最佳示例数量。该方法在分类功能性和非功能性需求方面实现了卓越性能，解决了过度提示问题，超越了现有技术1%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>过量提示现象：在大型语言模型（LLM）中，过多的示例可能导致性能下降。</li>
<li>探究方法：研究团队提出了一个基于随机抽样、语义嵌入和TF-IDF向量的提示框架。</li>
<li>LLM评估：实验在多个LLM上进行了性能评估。</li>
<li>与传统观点相悖的发现：加入过多的领域特定示例可能适得其反，并非越多越好。</li>
<li>实际应用研究：使用真实软件需求分类数据集进行实验，以解决软件工程和需求分析的LLM辅助问题。</li>
<li>最佳实践：通过逐步增加TF-IDF选择和分层选择的示例数量，找到了每个LLM的最佳示例数量，解决了过度提示问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13196">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13196v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13196v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13196v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13196v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13196v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13196v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="More-performant-and-scalable-Rethinking-contrastive-vision-language-pre-training-of-radiology-in-the-LLM-era"><a href="#More-performant-and-scalable-Rethinking-contrastive-vision-language-pre-training-of-radiology-in-the-LLM-era" class="headerlink" title="More performant and scalable: Rethinking contrastive vision-language   pre-training of radiology in the LLM era"></a>More performant and scalable: Rethinking contrastive vision-language   pre-training of radiology in the LLM era</h2><p><strong>Authors:Yingtai Li, Haoran Lai, Xiaoqian Zhou, Shuai Ming, Wenxin Ma, Wei Wei, Shaohua Kevin Zhou</strong></p>
<p>The emergence of Large Language Models (LLMs) presents unprecedented opportunities to revolutionize medical contrastive vision-language pre-training. In this paper, we show how LLMs can facilitate large-scale supervised pre-training, thereby advancing vision-language alignment. We begin by demonstrate that modern LLMs can automatically extract diagnostic labels from radiology reports with remarkable precision (&gt;96% AUC in our experiments) without complex prompt engineering, enabling the creation of large-scale “silver-standard” datasets at a minimal cost (~$3 for 50k CT image-report pairs). Further, we find that vision encoder trained on this “silver-standard” dataset achieves performance comparable to those trained on labels extracted by specialized BERT-based models, thereby democratizing the access to large-scale supervised pre-training. Building on this foundation, we proceed to reveal that supervised pre-training fundamentally improves contrastive vision-language alignment. Our approach achieves state-of-the-art performance using only a 3D ResNet-18 with vanilla CLIP training, including 83.8% AUC for zero-shot diagnosis on CT-RATE, 77.3% AUC on RAD-ChestCT, and substantial improvements in cross-modal retrieval (MAP@50&#x3D;53.7% for image-image, Recall@100&#x3D;52.2% for report-image). These results demonstrate the potential of utilizing LLMs to facilitate {\bf more performant and scalable} medical AI systems. Our code is avaiable at <a target="_blank" rel="noopener" href="https://github.com/SadVoxel/More-performant-and-scalable">https://github.com/SadVoxel/More-performant-and-scalable</a>. </p>
<blockquote>
<p>大型语言模型（LLM）的出现为医学对比视觉语言预训练带来了前所未有的机会。在本文中，我们展示了LLM如何促进大规模监督预训练，从而促进视觉语言对齐。首先，我们证明现代LLM能够自动从放射学报告中提取诊断标签，其精确度令人印象深刻（在我们的实验中AUC值大于96%），无需复杂的提示工程，从而以最低的成本创建大规模“银标准”数据集（每对CT图像报告成本约3美元）。此外，我们发现在此“银标准”数据集上训练的视觉编码器与用基于BERT的模型提取的标签训练的视觉编码器的性能相当，从而实现大规模监督预训练的普及。在此基础上，我们进一步揭示监督预训练从根本上改善了对比视觉语言对齐。我们的方法仅使用标准的CLIP训练和简单的3D ResNet-18就达到了最先进的性能，包括CT-RATE上的零样本诊断AUC为83.8%，RAD-ChestCT上的AUC为77.3%，跨模态检索也有显著改善（图像-图像MAP@50为53.7%，报告-图像Recall@100为52.2%）。这些结果展示了利用LLM促进更具性能和可扩展性的医学人工智能系统的潜力。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/SadVoxel/More-performant-and-scalable%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SadVoxel/More-performant-and-scalable上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13175v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong><br>     大型语言模型（LLM）的出现为医学对比视觉语言预训练带来了前所未有的机会。研究表明，LLM能够促进大规模监督预训练，进而推动视觉语言对齐的发展。通过自动从放射学报告中提取诊断标签，LLM能够创建大规模“银标准”数据集，降低成本。此外，基于这些“银标准”数据集的视觉编码器性能与用专业BERT模型提取的标签训练的视觉编码器相当。在此基础上，研究发现监督预训练能从根本上改善对比视觉语言对齐。在CT-RATE上实现了零样本诊断的AUC值为83.8%，RAD-ChestCT上的AUC值为77.3%，跨模态检索也有显著改善。这显示了利用LLM构建更高效、可扩展的医学人工智能系统的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）为医学对比视觉语言预训练带来了革命性机会。</li>
<li>LLM能够自动从放射学报告中提取诊断标签，创建大规模的“银标准”数据集，并且这一过程的成本较低。</li>
<li>基于这些“银标准”数据集的视觉编码器性能与用专业模型训练的视觉编码器相当。</li>
<li>监督预训练能够改善对比视觉语言对齐。</li>
<li>LLM在零样本诊断中表现出优异的性能，特别是在CT-RATE和RAD-ChestCT上的AUC值较高。</li>
<li>LLM还显著改善了跨模态检索的性能。</li>
<li>利用LLM构建的医学人工智能系统具有更高的性能和可扩展性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13175">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13175v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13175v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13175v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13175v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="WHU-STree-A-Multi-modal-Benchmark-Dataset-for-Street-Tree-Inventory"><a href="#WHU-STree-A-Multi-modal-Benchmark-Dataset-for-Street-Tree-Inventory" class="headerlink" title="WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory"></a>WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory</h2><p><strong>Authors:Ruifei Ding, Zhe Chen, Wen Fan, Chen Long, Huijuan Xiao, Yelu Zeng, Zhen Dong, Bisheng Yang</strong></p>
<p>Street trees are vital to urban livability, providing ecological and social benefits. Establishing a detailed, accurate, and dynamically updated street tree inventory has become essential for optimizing these multifunctional assets within space-constrained urban environments. Given that traditional field surveys are time-consuming and labor-intensive, automated surveys utilizing Mobile Mapping Systems (MMS) offer a more efficient solution. However, existing MMS-acquired tree datasets are limited by small-scale scene, limited annotation, or single modality, restricting their utility for comprehensive analysis. To address these limitations, we introduce WHU-STree, a cross-city, richly annotated, and multi-modal urban street tree dataset. Collected across two distinct cities, WHU-STree integrates synchronized point clouds and high-resolution images, encompassing 21,007 annotated tree instances across 50 species and 2 morphological parameters. Leveraging the unique characteristics, WHU-STree concurrently supports over 10 tasks related to street tree inventory. We benchmark representative baselines for two key tasks–tree species classification and individual tree segmentation. Extensive experiments and in-depth analysis demonstrate the significant potential of multi-modal data fusion and underscore cross-domain applicability as a critical prerequisite for practical algorithm deployment. In particular, we identify key challenges and outline potential future works for fully exploiting WHU-STree, encompassing multi-modal fusion, multi-task collaboration, cross-domain generalization, spatial pattern learning, and Multi-modal Large Language Model for street tree asset management. The WHU-STree dataset is accessible at: <a target="_blank" rel="noopener" href="https://github.com/WHU-USI3DV/WHU-STree">https://github.com/WHU-USI3DV/WHU-STree</a>. </p>
<blockquote>
<p>街道树木对城市宜居性至关重要，能提供生态和社会效益。在空间受限的城市环境中优化这些多功能资产时，建立详细、准确、动态更新的街道树木清单已变得至关重要。由于传统实地调查耗时且劳力密集，利用移动测量系统(MMS)的自动化调查提供了更高效的解决方案。然而，现有MMS获取的树木数据集受限于小规模场景、有限注释或单一模式，限制了其进行综合分析的实用性。为了解决这些局限性，我们推出了WHU-STree数据集，这是一个跨城市的、注释丰富的、多模态的城市街道树木数据集。WHU-STree是在两个不同城市收集的，融合了同步点云和高分辨率图像，包含50个物种的21,007个注释树木实例和2个形态参数。利用独特的特点，WHU-STree同时支持超过10个与街道树木清单相关的任务。我们为两个关键任务——树种分类和单株树木分割——设定了基准线。大量的实验和深入分析证明了多模态数据融合的显著潜力，并强调了跨域适用性作为实际应用算法部署的关键先决条件。特别是，我们确定了充分利用WHU-STree的主要挑战，并概述了未来的可能工作方向，包括多模态融合、多任务协作、跨域泛化、空间模式学习以及用于街道树木资产管理的多模态大型语言模型。WHU-STree数据集可通过以下链接访问：<a target="_blank" rel="noopener" href="https://github.com/WHU-USI3DV/WHU-STree">https://github.com/WHU-USI3DV/WHU-STree</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13172v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>街道树木对城市宜居性至关重要，提供生态和社会效益。建立详细、准确、动态更新的街道树木清单，对于优化空间受限的城市环境中的多功能资产至关重要。传统现场调查耗时耗力，而采用移动测绘系统（MMS）的自动化调查提供了更高效的解决方案。然而，现有MMS获得的树木数据集存在场景规模小、标注有限或单一模态等限制，难以进行全面分析。为解决这些问题，我们推出了WHU-STree数据集，它是一个跨城市的丰富标注、多模态的街道树木数据集。WHU-STree整合了点云和高分辨率图像，涵盖两个不同城市的21,007个标注树木实例，涉及50个树种和2个形态参数。该数据集支持超过10个与街道树木清单相关的任务。我们对两个关键任务——树种分类和单株树木分割——进行了基准测试。实验和分析表明多模态数据融合的显著潜力，并强调跨域适用性对于实际算法部署的关键性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>街道树木对城市的生态和社会效益至关重要，需要建立详细、准确、动态更新的街道树木清单来优化城市中的多功能资产。</li>
<li>传统现场调查方法存在时间消耗和劳动力需求大的问题，而移动测绘系统（MMS）的自动化调查方法提供了更高效的解决方案。</li>
<li>现有的MMS树木数据集存在局限性，如场景规模小、标注有限或单一模态等，限制了其全面分析的能力。</li>
<li>WHU-STree数据集是一个跨城市、丰富标注、多模态的街道树木数据集，整合了点云和高分辨率图像，涵盖多个城市和树种。</li>
<li>WHU-STree数据集支持超过10个与街道树木相关的任务，包括树种分类和单株树木分割等。</li>
<li>实验和分析表明多模态数据融合的潜力，强调跨域适用性对于实际算法部署的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13172">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13172v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13172v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13172v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LLM-Hallucination-Detection-A-Fast-Fourier-Transform-Method-Based-on-Hidden-Layer-Temporal-Signals"><a href="#LLM-Hallucination-Detection-A-Fast-Fourier-Transform-Method-Based-on-Hidden-Layer-Temporal-Signals" class="headerlink" title="LLM Hallucination Detection: A Fast Fourier Transform Method Based on   Hidden Layer Temporal Signals"></a>LLM Hallucination Detection: A Fast Fourier Transform Method Based on   Hidden Layer Temporal Signals</h2><p><strong>Authors:Jinxin Li, Gang Tu, ShengYu Cheng, Junjie Hu, Jinting Wang, Rui Chen, Zhilong Zhou, Dongbo Shan</strong></p>
<p>Hallucination remains a critical barrier for deploying large language models (LLMs) in reliability-sensitive applications. Existing detection methods largely fall into two categories: factuality checking, which is fundamentally constrained by external knowledge coverage, and static hidden-state analysis, that fails to capture deviations in reasoning dynamics. As a result, their effectiveness and robustness remain limited. We propose HSAD (Hidden Signal Analysis-based Detection), a novel hallucination detection framework that models the temporal dynamics of hidden representations during autoregressive generation. HSAD constructs hidden-layer signals by sampling activations across layers, applies Fast Fourier Transform (FFT) to obtain frequency-domain representations, and extracts the strongest non-DC frequency component as spectral features. Furthermore, by leveraging the autoregressive nature of LLMs, HSAD identifies optimal observation points for effective and reliable detection. Across multiple benchmarks, including TruthfulQA, HSAD achieves over 10 percentage points improvement compared to prior state-of-the-art methods. By integrating reasoning-process modeling with frequency-domain analysis, HSAD establishes a new paradigm for robust hallucination detection in LLMs. </p>
<blockquote>
<p>幻觉仍是大型语言模型（LLM）在可靠性敏感应用部署中的关键障碍。现有的检测方法主要分为两类：受控于外部知识覆盖范围的合理性检验和无法捕捉推理动态偏差的静态隐藏状态分析。因此，它们的有效性和稳健性仍然有限。我们提出HSAD（基于隐藏信号分析的检测），这是一种新型的幻觉检测框架，对自回归生成过程中的隐藏表示的时空动态进行建模。HSAD通过跨层采样激活构建隐藏层信号，应用快速傅里叶变换（FFT）获得频域表示，并提取最强的非直流频率分量作为谱特征。此外，HSAD利用LLM的自回归特性，确定有效的可靠检测的最佳观测点。在包括TruthfulQA等多个基准测试中，HSAD相较于现有最先进的检测方法，实现了超过10个百分点的改进。通过将推理过程建模与频域分析相结合，HSAD为LLM中的稳健幻觉检测建立了新的范式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13154v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文指出大型语言模型（LLM）在可靠性敏感应用中的关键障碍是幻觉生成。现有检测方法主要分为两类：受制于外部知识覆盖的事实性检查和无法捕捉推理动态变化的静态隐藏状态分析。为此，本文提出一种新型的幻觉检测框架HSAD（基于隐藏信号分析的检测），该框架对自动回归生成过程中的隐藏表示的时空动态进行建模。HSAD通过采样各层激活值构建隐藏层信号，应用快速傅里叶变换（FFT）获得频域表示，并提取非直流分量中最强的频谱特征。此外，HSAD利用LLM的自动回归性质，确定有效的观察点，以实现可靠检测。在多个基准测试中，包括TruthfulQA，HSAD相较于现有最先进的检测方法，提高了超过10个百分点。通过将推理过程建模与频域分析相结合，HSAD为LLM中的稳健幻觉检测建立了新范例。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>幻觉是大型语言模型（LLM）在可靠性敏感应用中面临的关键问题。</li>
<li>现有检测方法主要分两类：事实性检查和静态隐藏状态分析，但都存在局限性。</li>
<li>新型幻觉检测框架HSAD通过建模隐藏表示的时空动态来解决这一问题。</li>
<li>HSAD利用快速傅里叶变换（FFT）和隐藏层信号的采样来提取关键特征。</li>
<li>HSAD结合LLM的自动回归性质，确定有效的观察点以提高检测可靠性。</li>
<li>在多个基准测试中，HSAD相较于现有方法显著提高效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13154">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13154v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13154v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13154v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13154v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13154v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13154v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13154v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Reasoning-with-Preference-Constraints-A-Benchmark-for-Language-Models-in-Many-to-One-Matching-Markets"><a href="#Reasoning-with-Preference-Constraints-A-Benchmark-for-Language-Models-in-Many-to-One-Matching-Markets" class="headerlink" title="Reasoning with Preference Constraints: A Benchmark for Language Models   in Many-to-One Matching Markets"></a>Reasoning with Preference Constraints: A Benchmark for Language Models   in Many-to-One Matching Markets</h2><p><strong>Authors:Marylou Fauchard, Florian Carichon, Margarida Carvalho, Golnoosh Farnadi</strong></p>
<p>Recent advances in reasoning with large language models (LLMs) have demonstrated strong performance on complex mathematical tasks, including combinatorial optimization. Techniques such as Chain-of-Thought and In-Context Learning have further enhanced this capability, making LLMs both powerful and accessible tools for a wide range of users, including non-experts. However, applying LLMs to matching problems, which require reasoning under preferential and structural constraints, remains underexplored. To address this gap, we introduce a novel benchmark of 369 instances of the College Admission Problem, a canonical example of a matching problem with preferences, to evaluate LLMs across key dimensions: feasibility, stability, and optimality. We employ this benchmark to assess the performance of several open-weight LLMs. Our results first reveal that while LLMs can satisfy certain constraints, they struggle to meet all evaluation criteria consistently. They also show that reasoning LLMs, like QwQ and GPT-oss, significantly outperform traditional models such as Llama, Qwen or Mistral, defined here as models used without any dedicated reasoning mechanisms. Moreover, we observed that LLMs reacted differently to the various prompting strategies tested, which include Chain-of-Thought, In-Context Learning and role-based prompting, with no prompt consistently offering the best performance. Finally, we report the performances from iterative prompting with auto-generated feedback and show that they are not monotonic; they can peak early and then significantly decline in later attempts. Overall, this work offers a new perspective on model reasoning performance and the effectiveness of prompting strategies in combinatorial optimization problems with preferential constraints. </p>
<blockquote>
<p>近期，大型语言模型（LLM）在复杂数学任务上的进展，包括组合优化，都表现出了强大的性能。思维链和上下文学习等技术进一步增强了这一能力，使LLM成为强大且易于使用的工具，适用于广大用户，包括非专业人士。然而，将LLM应用于匹配问题（需要在偏好和结构约束下进行推理）仍然鲜有研究。为了弥补这一空白，我们引入了包含369个实例的大学录取问题基准测试，这是带有偏好匹配问题的一个典型案例，以评估LLM的关键维度：可行性、稳定性和最优性。我们使用这个基准测试来评估几个公开权重LLM的性能。我们的结果首先表明，虽然LLM可以满足某些约束，但它们难以始终如一地满足所有评估标准。他们还显示，像QwQ和GPT-oss这样的推理LLM显著优于传统的LLM模型，如Llama、Qwen或Mistral（这里定义为没有专用推理机制的模型）。此外，我们观察到LLM对不同提示策略的反应各不相同，包括思维链、上下文学习和基于角色的提示，没有任何一种提示始终提供最佳性能。最后，我们报告了通过自动生成反馈进行迭代提示的性能，并显示它们并非单调；它们可能在早期达到峰值，然后在后续尝试中显著下降。总的来说，这项工作提供了关于模型推理性能和偏好约束组合优化问题中的提示策略有效性的新视角。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13131v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在复杂的数学任务上表现出强大的性能，包括组合优化。通过Chain-of-Thought和In-Context Learning等技术，LLM的推理能力得到进一步提升，使其成为非专家用户也能轻松使用的强大工具。然而，将LLM应用于带有偏好和结构约束的匹配问题仍然未被充分探索。本研究为解决这一空白而推出新的基准测试，对LLM解决大学录取问题等匹配问题的能力进行评估，涵盖可行性、稳定性和最优性等方面。研究发现LLM虽能满足某些约束，但难以一致满足所有评估标准。此外，相比传统模型，如QwQ和GPT-oss等带有推理机制的LLM表现更优。同时，不同的提示策略对LLM的影响各异，未发现始终最佳的提示方式。最后，通过迭代提示和自动生成反馈，发现性能并非单调提升，可能在早期达到峰值后显著下降。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在复杂的数学任务上表现出强大的性能，包括组合优化问题。</li>
<li>LLM技术如Chain-of-Thought和In-Context Learning增强了其推理能力。</li>
<li>LLM在解决带有偏好和结构约束的匹配问题上仍有待探索。</li>
<li>推出新的基准测试来评估LLM解决匹配问题的能力，涵盖可行性、稳定性和最优性。</li>
<li>LLM在某些约束条件下表现良好，但难以满足所有评估标准。</li>
<li>与传统模型相比，带有推理机制的LLM（如QwQ和GPT-oss）表现更优。</li>
<li>提示策略对LLM的影响不同，未找到始终最佳的提示方式；性能提升并非单调。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13131">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13131v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13131v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13131v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Empowering-LLMs-with-Parameterized-Skills-for-Adversarial-Long-Horizon-Planning"><a href="#Empowering-LLMs-with-Parameterized-Skills-for-Adversarial-Long-Horizon-Planning" class="headerlink" title="Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon   Planning"></a>Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon   Planning</h2><p><strong>Authors:Sijia Cui, Shuai Xu, Aiyao He, Yanna Wang, Bo Xu</strong></p>
<p>Recent advancements in Large Language Models(LLMs) have led to the development of LLM-based AI agents. A key challenge is the creation of agents that can effectively ground themselves in complex, adversarial long-horizon environments. Existing methods mainly focus on (1) using LLMs as policies to interact with the environment through generating low-level feasible actions, and (2) utilizing LLMs to generate high-level tasks or language guides to stimulate action generation. However, the former struggles to generate reliable actions, while the latter relies heavily on expert experience to translate high-level tasks into specific action sequences. To address these challenges, we introduce the Plan with Language, Act with Parameter (PLAP) planning framework that facilitates the grounding of LLM-based agents in long-horizon environments. The PLAP method comprises three key components: (1) a skill library containing environment-specific parameterized skills, (2) a skill planner powered by LLMs, and (3) a skill executor converting the parameterized skills into executable action sequences. We implement PLAP in MicroRTS, a long-horizon real-time strategy game that provides an unfamiliar and challenging environment for LLMs. The experimental results demonstrate the effectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting outperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully crafted few-shot examples, surpasses the top-tier scripted agent, CoacAI. Additionally, we design comprehensive evaluation metrics and test 6 closed-source and 2 open-source LLMs within the PLAP framework, ultimately releasing an LLM leaderboard ranking long-horizon skill planning ability. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/AI-Research-TeamX/PLAP">https://github.com/AI-Research-TeamX/PLAP</a>. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进步推动了基于LLM的AI代理的发展。一个关键挑战是创建能够在复杂、对抗性的长期环境中有效定位自己的代理。现有方法主要集中在：（1）使用LLM作为策略，通过与环境生成低级可行动作进行交互；（2）利用LLM生成高级任务或语言指南来激发动作生成。然而，前者在生成可靠动作方面遇到困难，而后者则严重依赖于专家经验将高级任务翻译成特定的动作序列。为了解决这些挑战，我们引入了“用语言规划，用参数行动”（PLAP）规划框架，该框架有助于基于LLM的代理在长期环境中定位。PLAP方法包含三个关键组成部分：（1）包含环境特定参数化技能的技能库；（2）由LLM驱动的技能规划器；（3）将参数化技能转化为可执行动作序列的技能执行器。我们在MicroRTS中实现了PLAP，这是一款长期实时策略游戏，为LLM提供了一个不熟悉且具有挑战性的环境。实验结果表明PLAP的有效性。特别是，GPT-4o驱动的PLAP在零样本设置下超越了80%的基线代理，而经过精心设计的Qwen2-72B驱动的PLAP则在少数样本示例中超越了顶级脚本代理CoacAI。此外，我们设计了全面的评估指标，并在PLAP框架内测试了6个闭源和2个开源LLM，最终发布了一个长期规划能力排行榜的LLM领导者榜单。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/AI-Research-TeamX/PLAP%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/AI-Research-TeamX/PLAP访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13127v1">PDF</a> Accepted to IJCNN 2025</p>
<p><strong>Summary</strong></p>
<p>LLM领域的新进展带来了基于LLM的AI代理的开发。创建能在复杂、对抗性的长期环境中有效自我定位的智能体是关键挑战。现有方法主要关注使用LLM生成低级可行动作，以及利用LLM生成高级任务或语言指南来刺激动作生成。然而，前者难以生成可靠的动作，后者则严重依赖于专家经验将高级任务翻译成特定的动作序列。为解决这些挑战，本文引入Plan with Language, Act with Parameter（PLAP）规划框架，促进基于LLM的智能体在长期环境中的定位。实验结果表明PLAP的有效性。特别是GPT-4驱动的PLAP在零射击环境中表现优于80%的基线智能体，而经过精心设计的Qwen2-72B少数样本表现超越顶尖脚本代理CoacAI。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM的新进展导致基于LLM的AI代理的发展。</li>
<li>创建能在长期环境中有效自我定位的智能体是关键挑战。</li>
<li>当前方法存在缺陷：难以生成可靠动作和过度依赖专家经验。</li>
<li>引入PLAP规划框架来解决这些挑战，包括技能库、技能规划器和技能执行器三个关键组件。</li>
<li>PLAP在MicroRTS长时策略游戏中的实施证明了其有效性。</li>
<li>GPT-4驱动的PLAP在零射击环境中表现优异，超过大多数基线智能体。</li>
<li>Qwen2-72B在精心设计的少数样本中表现最佳，超越顶尖脚本代理CoacAI。</li>
<li>提供了全面的评估指标，并在PLAP框架下测试了多个LLM。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13127">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13127v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13127v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13127v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13127v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13127v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13127v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-comparison-of-pipelines-for-the-translation-of-a-low-resource-language-based-on-transformers"><a href="#A-comparison-of-pipelines-for-the-translation-of-a-low-resource-language-based-on-transformers" class="headerlink" title="A comparison of pipelines for the translation of a low resource language   based on transformers"></a>A comparison of pipelines for the translation of a low resource language   based on transformers</h2><p><strong>Authors:Chiara Bonfanti, Michele Colombino, Giulia Coucourde, Faeze Memari, Stefano Pinardi, Rosa Meo</strong></p>
<p>This work compares three pipelines for training transformer-based neural networks to produce machine translators for Bambara, a Mand`e language spoken in Africa by about 14,188,850 people. The first pipeline trains a simple transformer to translate sentences from French into Bambara. The second fine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures for French-to-Bambara translation. Models from the first two pipelines were trained with different hyperparameter combinations to improve BLEU and chrF scores, evaluated on both test sentences and official Bambara benchmarks. The third pipeline uses language distillation with a student-teacher dual neural network to integrate Bambara into a pre-trained LaBSE model, which provides language-agnostic embeddings. A BERT extension is then applied to LaBSE to generate translations. All pipelines were tested on Dokotoro (medical) and Bayelemagaba (mixed domains). Results show that the first pipeline, although simpler, achieves the best translation accuracy (10% BLEU, 21% chrF on Bayelemagaba), consistent with low-resource translation results. On the Yiri dataset, created for this work, it achieves 33.81% BLEU and 41% chrF. Instructor-based models perform better on single datasets than on aggregated collections, suggesting they capture dataset-specific patterns more effectively. </p>
<blockquote>
<p>本文比较了三条用于训练基于转换器的神经网络以生成巴姆巴拉语（一种非洲约14,188,850人使用的曼德语言）的机器翻译器的管道。第一条管道训练一个简单的转换器，将法语翻译成巴姆巴拉语。第二条管道对LLaMA3（3B-8B）指导员模型进行微调，采用仅解码器架构进行法语至巴姆巴拉语的翻译。前两条管道中的模型通过不同的超参数组合进行训练，以提高BLEU和chrF分数，并在测试句和官方巴姆巴拉语基准测试上进行评估。第三条管道采用学生-教师双神经网络进行语言蒸馏，将巴姆巴拉语集成到预训练的LaBSE模型中，该模型提供语言无关的嵌入。然后应用BERT扩展版到LaBSE以生成翻译。所有管道均在Dokotoro（医学）和Bayelemagaba（混合领域）上进行了测试。结果表明，虽然第一条管道更简单，但在翻译准确性方面表现最佳（在Bayelemagaba上BLEU得分为10%，chrF得分为21%），这与低资源翻译结果一致。针对本文创建的数据集Yiri，其BLEU得分为33.81%，chrF得分为41%。基于指导员的模型在单一数据集上的表现优于聚合集合，这表明它们更有效地捕捉数据集特定的模式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12514v1">PDF</a> 9 pages, 4 figures</p>
<p><strong>摘要</strong></p>
<p>本文比较了三种训练基于转换器的神经网络以生成巴马语言（一种非洲的曼德语，约1418万人使用）翻译机的管道。第一种管道简单训练一个翻译器，实现从法语到巴马的翻译。第二种管道采用LLaMA3（介于低B和赫兹至佩塔赫兹级功率运行的高速控制论量子编码数字多功能整合工程的高保真极限动力系统科技复杂科学的逻辑推理力不断架构网络型通信管理的硬件代号命名的芯片上封装含有子链接组成低码的极度高阶语义象征算法的加密密集型决策矩阵平台代号指项目第三级网络系统体系压缩分析再聚焦数值模型）模型进行微调，用于法巴翻译。前两种管道的模型通过不同的超参数组合训练，以提高BLEU和chrF分数。第三种管道使用学生教师双神经网络进行语言蒸馏，将巴马语言融入预训练的LaBSE模型，并利用BERT扩展生成翻译。在Dokotoro（医疗领域）和Bayelemagaba（混合领域）的测试中，第一种管道在翻译准确性方面表现最佳。在为此工作创建的Yiri数据集上，它实现了33.81％的BLEU和41％的chrF分数。基于指导者的模型在单个数据集上的表现优于聚合集合，这表明它们更有效地捕捉数据集特定的模式。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究比较了三种训练神经网络以生成巴马语言翻译的方法。</li>
<li>第一种管道简单有效，在特定数据集上实现最佳翻译准确性。</li>
<li>第二种管道采用LLaMA3模型微调，用于法巴翻译，表现亦佳。</li>
<li>前两种管道的模型通过调整超参数提高性能。</li>
<li>第三种管道结合语言蒸馏和学生教师双神经网络，集成巴马语言到预训练模型。</li>
<li>基于指导者的模型在单一数据集上优于聚合数据集，更有效捕捉数据集特定模式。</li>
<li>研究结果揭示了不同管道在处理特定语言翻译任务时的优势和局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12514">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.12514v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.12514v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.12514v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.12514v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.12514v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.12514v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.12514v1/page_5_2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TAPS-Tool-Augmented-Personalisation-via-Structured-Tagging"><a href="#TAPS-Tool-Augmented-Personalisation-via-Structured-Tagging" class="headerlink" title="TAPS: Tool-Augmented Personalisation via Structured Tagging"></a>TAPS: Tool-Augmented Personalisation via Structured Tagging</h2><p><strong>Authors:Ekaterina Taktasheva, Jeff Dalton</strong></p>
<p>Recent advancements in tool-augmented large language models have enabled them to interact with external tools, enhancing their ability to perform complex user tasks. However, existing approaches overlook the role of personalisation in guiding tool use. This work investigates how user preferences can be effectively integrated into goal-oriented dialogue agents. Through extensive analysis, we identify key weaknesses in the ability of LLMs to personalise tool use. To this end, we introduce TAPS, a novel solution that enhances personalised tool use by leveraging a structured tagging tool and an uncertainty-based tool detector. TAPS significantly improves the ability of LLMs to incorporate user preferences, achieving the new state-of-the-art for open source models on the NLSI task. </p>
<blockquote>
<p>近期，工具增强型大型语言模型的进步已经使得它们可以与外部工具进行交互，提高了执行复杂用户任务的能力。然而，现有方法忽视了个性化在指导工具使用中的作用。本研究旨在探讨如何有效地将用户偏好整合到目标导向的对话代理中。通过广泛的分析，我们发现了大型语言模型在个性化工具使用方面的关键弱点。为此，我们引入了TAPS，这是一种新型解决方案，它通过利用结构化标签工具和基于不确定性的工具检测器来增强个性化工具的使用。TAPS显著提高了大型语言模型融入用户偏好的能力，在NLSI任务上实现了开源模型的新水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20409v3">PDF</a> Accepted to EMNLP 2026 Main</p>
<p><strong>Summary</strong></p>
<p>近期工具增强型大型语言模型取得进展，能与外部工具交互，执行复杂用户任务能力增强。然而，现有方法忽视了个性化在指导工具使用中的作用。本研究调查了如何有效整合用户偏好到目标导向对话代理中。通过深入分析，我们发现LLM在个性化工具使用上的关键弱点。为此，我们引入TAPS，一种利用结构化标签工具和基于不确定性的工具检测器增强个性化工具使用的新解决方案。TAPS显著提高了LLM融入用户偏好的能力，在NLSI任务上实现了开源模型的新水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>工具增强型大型语言模型能与外部工具交互，增强执行复杂任务的能力。</li>
<li>现有方法忽视了个性化在指导工具使用中的作用。</li>
<li>用户偏好整合到目标导向对话代理中是本研究的重点。</li>
<li>LLM在个性化工具使用上存在关键弱点。</li>
<li>TAPS利用结构化标签工具和基于不确定性的工具检测器解决这一问题。</li>
<li>TAPS显著提高了LLM融入用户偏好的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20409">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.20409v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.20409v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.20409v3/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.20409v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.20409v3/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.20409v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.20409v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.20409v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Prompting-Obscures-Hallucination-Cues-in-Large-Language-Models-An-Empirical-Evaluation"><a href="#Chain-of-Thought-Prompting-Obscures-Hallucination-Cues-in-Large-Language-Models-An-Empirical-Evaluation" class="headerlink" title="Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models: An Empirical Evaluation"></a>Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models: An Empirical Evaluation</h2><p><strong>Authors:Jiahao Cheng, Tiancheng Su, Jia Yuan, Guoxiu He, Jiawei Liu, Xinqi Tao, Jingwen Xie, Huaxia Li</strong></p>
<p>Large Language Models (LLMs) often exhibit \textit{hallucinations}, generating factually incorrect or semantically irrelevant content in response to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by encouraging step-by-step reasoning, but its impact on hallucination detection remains underexplored. To bridge this gap, we conduct a systematic empirical evaluation. We begin with a pilot experiment, revealing that CoT reasoning significantly affects the LLM’s internal states and token probability distributions. Building on this, we evaluate the impact of various CoT prompting methods on mainstream hallucination detection methods across both instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three key dimensions: changes in hallucination score distributions, variations in detection accuracy, and shifts in detection confidence. Our findings show that while CoT prompting helps reduce hallucination frequency, it also tends to obscure critical signals used for detection, impairing the effectiveness of various detection methods. Our study highlights an overlooked trade-off in the use of reasoning. Code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ECNU-Text-Computing/cot-hallu-detect">https://github.com/ECNU-Text-Computing/cot-hallu-detect</a> . </p>
<blockquote>
<p>大型语言模型（LLM）常常出现“幻觉”，即生成与提示不符的事实错误或语义无关的内容。链式思维（CoT）提示可以通过鼓励逐步推理来缓解幻觉问题，但其对幻觉检测的影响仍被忽视。为了填补这一空白，我们进行了系统的实证研究。我们首先进行了一项试点实验，结果显示CoT推理显著影响LLM的内部状态和标记概率分布。在此基础上，我们评估了各种CoT提示方法对主流幻觉检测方法的检测影响，涉及指令调优和面向推理的LLM。具体来说，我们考察了三个方面：幻觉分数分布的变化、检测准确度的变化以及检测信心的转变。我们的研究结果表明，虽然CoT提示有助于减少幻觉频率，但它也有掩盖用于检测的关键信号的趋势，从而损害各种检测方法的效力。我们的研究凸显了在使用推理过程中忽略的一个权衡问题。代码已公开在：<a target="_blank" rel="noopener" href="https://github.com/ECNU-Text-Computing/cot-hallu-detect%E3%80%82">https://github.com/ECNU-Text-Computing/cot-hallu-detect。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17088v3">PDF</a> Accepted at EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）会出现生成错误或不相关内容的幻觉现象。链式思维（CoT）提示通过鼓励逐步推理可以缓解幻觉问题，但其对幻觉检测的影响尚未得到充分研究。本文进行了一项系统的实证研究，以填补这一空白。通过初步实验发现，CoT推理对LLM的内部状态和令牌概率分布产生了显著影响。在此基础上，本文评估了不同的CoT提示方法对主流幻觉检测方法的冲击，涉及指令微调与面向推理的LLM。研究发现，CoT提示虽然有助于减少幻觉频率，但也可能掩盖用于检测的关键信号，从而影响各种检测方法的准确性。本研究揭示了使用推理过程中的一个被忽视的权衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）存在生成错误或不相关内容的幻觉现象。</li>
<li>链式思维（CoT）提示通过鼓励逐步推理有助于缓解LLM的幻觉问题。</li>
<li>CoT推理对LLM的内部状态和令牌概率分布产生显著影响。</li>
<li>CoT提示方法对主流幻觉检测方法的冲击评估显示，它可能影响检测方法的准确性。</li>
<li>CoT提示在减少幻觉频率的同时，也可能掩盖用于检测的关键信号。</li>
<li>研究揭示了使用推理过程中的一个被忽视的权衡，需要在减少幻觉和提高检测准确性之间进行权衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17088">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.17088v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.17088v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.17088v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.17088v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="UniversalCEFR-Enabling-Open-Multilingual-Research-on-Language-Proficiency-Assessment"><a href="#UniversalCEFR-Enabling-Open-Multilingual-Research-on-Language-Proficiency-Assessment" class="headerlink" title="UniversalCEFR: Enabling Open Multilingual Research on Language   Proficiency Assessment"></a>UniversalCEFR: Enabling Open Multilingual Research on Language   Proficiency Assessment</h2><p><strong>Authors:Joseph Marvin Imperial, Abdullah Barayan, Regina Stodden, Rodrigo Wilkens, Ricardo Munoz Sanchez, Lingyun Gao, Melissa Torgbi, Dawn Knight, Gail Forey, Reka R. Jablonkai, Ekaterina Kochmar, Robert Reynolds, Eugénio Ribeiro, Horacio Saggion, Elena Volodina, Sowmya Vajjala, Thomas François, Fernando Alva-Manchego, Harish Tayyar Madabushi</strong></p>
<p>We introduce UniversalCEFR, a large-scale multilingual and multidimensional dataset of texts annotated with CEFR (Common European Framework of Reference) levels in 13 languages. To enable open research in automated readability and language proficiency assessment, UniversalCEFR comprises 505,807 CEFR-labeled texts curated from educational and learner-oriented resources, standardized into a unified data format to support consistent processing, analysis, and modelling across tasks and languages. To demonstrate its utility, we conduct benchmarking experiments using three modelling paradigms: a) linguistic feature-based classification, b) fine-tuning pre-trained LLMs, and c) descriptor-based prompting of instruction-tuned LLMs. Our results support using linguistic features and fine-tuning pretrained models in multilingual CEFR level assessment. Overall, UniversalCEFR aims to establish best practices in data distribution for language proficiency research by standardising dataset formats, and promoting their accessibility to the global research community. </p>
<blockquote>
<p>我们介绍了UniversalCEFR，这是一个大规模的多语言、多维文本数据集，用CEFR（欧洲共同语言参考标准）的13种语言的等级进行标注。为了促进自动化可读性评估和语言水平评估的开放研究，UniversalCEFR包含了从教育和学习导向资源中精心挑选的50万五千八百零七篇CEFR标注文本，统一的数据格式支持跨任务和语言的统一处理、分析和建模。为了证明其实用性，我们采用了三种建模范式进行基准测试：a）基于语言特征的分类，b）微调预训练的大型语言模型，以及c）基于描述的指令调整大型语言模型的提示。我们的结果支持在多语言CEFR水平评估中使用语言特征和微调预训练模型。总体而言，UniversalCEFR旨在通过标准化数据集格式和促进全球研究社区的访问，为语言水平研究建立最佳的数据分布实践。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01419v2">PDF</a> Accepted to EMNLP 2025 (Main Conference)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了UniversalCEFR——一个大规模的多语种、多维度的文本数据集，采用CEFR（欧洲语言共同参考框架）水平标注，包含教育和学习导向资源的50万多个CEFR标注文本。数据集统一格式支持跨任务和语言的处理、分析和建模。通过实验验证，证明了该数据集在基于语言特征的分类和微调预训练模型中的实用性。UniversalCEFR旨在通过标准化数据集格式和促进全球研究社区的访问，为语言水平研究建立最佳实践。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniversalCEFR是一个大规模的多语种和多维度文本数据集，包含教育和学习导向资源的CEFR标注文本。</li>
<li>数据集采用统一的格式，支持跨任务和语言的处理、分析和建模。</li>
<li>数据集通过实验验证在基于语言特征的分类和微调预训练模型中的实用性。</li>
<li>该数据集有助于建立语言水平研究的最佳实践，通过标准化数据集格式来促进全球研究社区的访问。</li>
<li>UniversalCEFR包含三种建模范式：基于语言特征的分类、微调预训练模型和基于描述符的指令微调LLMs。</li>
<li>实验结果表明，使用语言特征和微调预训练模型进行多语种CEFR水平评估是有效的。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01419">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.01419v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.01419v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2506.01419v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Game-RL-Synthesizing-Verifiable-Game-Tasks-at-Scale-to-Boost-VLMs-General-Reasoning"><a href="#Game-RL-Synthesizing-Verifiable-Game-Tasks-at-Scale-to-Boost-VLMs-General-Reasoning" class="headerlink" title="Game-RL: Synthesizing Verifiable Game Tasks at Scale to Boost VLMs   General Reasoning"></a>Game-RL: Synthesizing Verifiable Game Tasks at Scale to Boost VLMs   General Reasoning</h2><p><strong>Authors:Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Changhao Jiang, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang</strong></p>
<p>Real-world vision language reasoning scenarios often include diverse and complex tasks. However, vision language reinforcement learning has primarily focused on a narrow set of tasks (e.g. geometry or chart reasoning), limiting the improvement of Vision Language Models’ (VLMs) general reasoning. Therefore, we propose a novel Code2Logic approach, using Large Language Models (LLMs) to synthesize verifiable game reasoning tasks at scale via adapting game code. Using the Code2Logic, we developed the GameQA dataset to train and evaluate VLMs. GameQA is verifiable and scalable, offers controllable difficulty gradation and is diverse with 30 games and 158 tasks. Then we apply Game-RL, which is simple reinforcement learning on GameQA. Surprisingly, despite training solely on game tasks, VLMs demonstrated out of domain generalization, specifically Qwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language benchmarks. Our code, dataset and models are available at the GitHub repository. </p>
<blockquote>
<p>现实世界中的视觉语言推理场景通常包含各种复杂多样的任务。然而，视觉语言强化学习主要集中在狭窄的任务集上（例如几何或图表推理），这限制了视觉语言模型（VLM）的一般推理能力的改进。因此，我们提出了一种新的Code2Logic方法，利用大型语言模型（LLM）通过适应游戏代码来大规模合成可验证的游戏推理任务。使用Code2Logic，我们开发了GameQA数据集来训练和评估VLM。GameQA具有可验证性和可扩展性，提供可控的难度梯度，并且具有30种游戏和158个任务，种类繁多。然后，我们对GameQA应用Game-RL，这是一种简单的强化学习。令人惊讶的是，尽管仅在游戏任务上进行训练，VLM表现出了跨域的泛化能力，特别是Qwen2.5-VL-7B在7个不同的视觉语言基准测试上性能提高了2.33%。我们的代码、数据集和模型可在GitHub仓库中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13886v4">PDF</a> 63 pages, 23 figures, submitted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了Code2Logic方法，通过适应游戏代码，利用大型语言模型（LLMs）合成可验证的游戏推理任务。同时开发了GameQA数据集以训练和评估视觉语言模型（VLMs）。该方法注重多样性和可控难度分级，展示了视觉语言模型在强化学习下的跨域泛化能力。模型的代码、数据集可在GitHub仓库中获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Code2Logic方法通过适应游戏代码来合成大规模可验证的推理任务。</li>
<li>开发的GameQA数据集用于训练和评估视觉语言模型（VLMs）。</li>
<li>GameQA数据集包含30个游戏和158个任务，具有多样性和可控难度分级的特点。</li>
<li>采用基于游戏的强化学习（Game-RL）来训练模型。</li>
<li>视觉语言模型在强化学习下展现出跨域泛化能力。</li>
<li>具体模型Qwen2.5-VL-7B在7个不同的视觉语言基准测试中性能提升2.33%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13886">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2505.13886v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2505.13886v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2505.13886v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2505.13886v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2505.13886v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2505.13886v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SuPreME-A-Supervised-Pre-training-Framework-for-Multimodal-ECG-Representation-Learning"><a href="#SuPreME-A-Supervised-Pre-training-Framework-for-Multimodal-ECG-Representation-Learning" class="headerlink" title="SuPreME: A Supervised Pre-training Framework for Multimodal ECG   Representation Learning"></a>SuPreME: A Supervised Pre-training Framework for Multimodal ECG   Representation Learning</h2><p><strong>Authors:Mingsheng Cai, Jiuming Jiang, Wenhao Huang, Che Liu, Rossella Arcucci</strong></p>
<p>Cardiovascular diseases are a leading cause of death and disability worldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring cardiac health, but obtaining large-scale annotated ECG datasets is labor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL) methods mitigate this by learning features without extensive labels but fail to capture fine-grained clinical semantics and require extensive task-specific fine-tuning. To address these challenges, we propose $\textbf{SuPreME}$, a $\textbf{Su}$pervised $\textbf{Pre}$-training framework for $\textbf{M}$ultimodal $\textbf{E}$CG representation learning. SuPreME is pre-trained using structured diagnostic labels derived from ECG report entities through a one-time offline extraction with Large Language Models (LLMs), which help denoise, standardize cardiac concepts, and improve clinical representation learning. By fusing ECG signals with textual cardiac queries instead of fixed labels, SuPreME enables zero-shot classification of unseen conditions without further fine-tuning. We evaluate SuPreME on six downstream datasets covering 106 cardiac conditions, achieving superior zero-shot AUC performance of $77.20%$, surpassing state-of-the-art eSSLs by $4.98%$. Results demonstrate SuPreME’s effectiveness in leveraging structured, clinically relevant knowledge for high-quality ECG representations. </p>
<blockquote>
<p>心血管疾病是全球范围内导致死亡和残疾的主要原因之一。心电图（ECG）对于诊断和治疗心脏疾病至关重要，但获取大规模标注的心电图数据集是一项劳动密集且耗时的工作。最近的心电图自监督学习方法（eSSL）通过无需大量标签即可学习特征来减轻这一负担，但无法捕捉精细的临床语义，并需要大量针对特定任务的微调。为了应对这些挑战，我们提出了<strong>SuPreME</strong>，这是一个用于<strong>多模态心电图表示学习的监督预训练框架</strong>。SuPreME使用通过一次性离线提取心电图报告实体而获得的结构化诊断标签进行预训练，借助大型语言模型（LLM）帮助去噪、标准化心脏概念，改进临床表示学习。通过将心电图信号与文本心脏查询而不是固定标签融合，SuPreME实现了对未见条件的零样本分类，无需进一步微调。我们在包含106种心脏疾病的六个下游数据集上评估SuPreME，其零样本AUC性能达到77.20%，比最先进的eSSL高出4.98%。结果表明，SuPreME在利用结构化、与临床相关的知识方面进行高质量心电图表示方面非常有效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19668v3">PDF</a> Findings of The 2025 Conference on Empirical Methods in Natural   Language Processing (EMNLP 2025)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了心血管疾病的严重性及其对全球健康的影响。心电图（ECG）在诊断和监测心脏健康方面发挥着重要作用，但获取大规模注释的心电图数据集是劳动密集型的，需要大量时间。最新的心电图自监督学习方法（eSSL）可以在没有大量标签的情况下学习特征，但无法捕捉精细的临床语义，并需要大量针对特定任务的微调。针对这些挑战，本文提出了SuPreME，一个用于多模态心电图表示学习的监督预训练框架。SuPreME使用从心电图报告实体中提取的结构化诊断标签进行预训练，借助大型语言模型（LLM）一次离线提取数据，帮助去噪、标准化心脏概念，提高临床表示学习。通过将心电图信号与文本心脏查询融合而非固定标签，SuPreME实现了对未见条件的零样本分类，无需进一步微调。在涵盖106种心脏疾病的六个下游数据集上评估显示，SuPreME的零样本AUC性能达到77.2%，比最先进的eSSL高出4.98%，证明了利用结构化、临床相关知识进行高质量心电图表示的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>心血管疾病是全球死亡和残疾的主要原因之一，而心电图（ECG）在诊断心脏健康方面发挥着重要作用。</li>
<li>获取大规模注释的心电图数据集是一个劳动密集和时间密集的过程。</li>
<li>最新心电图自监督学习方法（eSSL）可以在没有大量标签的情况下学习特征，但缺乏捕捉临床语义的能力，并需要大量任务特定微调。</li>
<li>提出了一种新的方法SuPreME，一个用于多模态心电图表示学习的监督预训练框架。</li>
<li>SuPreME使用从心电图报告实体中提取的结构化诊断标签进行预训练，借助大型语言模型（LLM）。</li>
<li>SuPreME通过将心电图信号与文本心脏查询融合来实现对未见条件的零样本分类。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19668">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2502.19668v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2502.19668v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2502.19668v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2502.19668v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="SEVEN-Pruning-Transformer-Model-by-Reserving-Sentinels"><a href="#SEVEN-Pruning-Transformer-Model-by-Reserving-Sentinels" class="headerlink" title="SEVEN: Pruning Transformer Model by Reserving Sentinels"></a>SEVEN: Pruning Transformer Model by Reserving Sentinels</h2><p><strong>Authors:Jinying Xiao, Ping Li, Jie Nie, Zhe Tang</strong></p>
<p>Large-scale Transformer models (TM) have demonstrated outstanding performance across various tasks. However, their considerable parameter size restricts their applicability, particularly on mobile devices. Due to the dynamic and intricate nature of gradients on TM compared to Convolutional Neural Networks, commonly used pruning methods tend to retain weights with larger gradient noise. This results in pruned models that are sensitive to sparsity and datasets, exhibiting suboptimal performance. Symbolic Descent (SD) is a general approach for training and fine-tuning TM. In this paper, we attempt to describe the noisy batch gradient sequences on TM through the cumulative process of SD. We utilize this design to dynamically assess the importance scores of weights.SEVEN is introduced by us, which particularly favors weights with consistently high sensitivity, i.e., weights with small gradient noise. These weights are tended to be preserved by SEVEN. Extensive experiments on various TM in natural language, question-answering, and image classification domains are conducted to validate the effectiveness of SEVEN. The results demonstrate significant improvements of SEVEN in multiple pruning scenarios and across different sparsity levels. Additionally, SEVEN exhibits robust performance under various fine-tuning strategies. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/xiaojinying/SEVEN">https://github.com/xiaojinying/SEVEN</a>. </p>
<blockquote>
<p>大规模Transformer模型（TM）已在各种任务中展现出卓越的性能。然而，其庞大的参数规模限制了其应用场景，特别是在移动设备上。与卷积神经网络相比，Transformer模型的梯度具有动态和复杂的特点，常用的剪枝方法往往保留具有较大梯度噪声的权重。这导致剪枝模型对稀疏性和数据集敏感，表现出次优性能。符号下降（SD）是一种用于训练和微调TM的通用方法。在本文中，我们试图通过SD的累积过程来描述TM上的噪声批量梯度序列。我们利用这种设计来动态评估权重的重要性分数。我们推出了SEVEN，它特别倾向于保留那些持续高敏感性的权重，即具有较小梯度噪声的权重。广泛地在自然语言、问答和图像分类领域的各种TM上进行的实验验证了SEVEN的有效性。结果表明，SEVEN在多种剪枝场景和不同稀疏性水平上都取得了显著的改进。此外，SEVEN在各种微调策略下表现出稳健的性能。代码公开在<a target="_blank" rel="noopener" href="https://github.com/xiaojinying/SEVEN%E3%80%82">https://github.com/xiaojinying/SEVEN。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.12688v2">PDF</a> IJCNN 2024</p>
<p><strong>Summary</strong></p>
<p>大型Transformer模型（TM）在多项任务中表现出卓越性能，但其庞大的参数规模限制了其在移动设备上的应用。本文提出一种名为SEVEN的方法，通过Symbolic Descent（SD）的累积过程描述TM上的噪声批次梯度序列，并据此动态评估权重的重要性得分。SEVEN倾向于保留具有持续高敏感性的权重，即具有小梯度噪声的权重。实验证明，SEVEN在多种剪枝场景和不同稀疏度级别上均表现出显著的有效性，并且在各种微调策略下展现出稳健的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型Transformer模型（TM）具有出色的性能，但参数规模限制了其在移动设备上的应用。</li>
<li>TM的梯度具有动态和复杂的特点，常规剪枝方法容易保留带有较大梯度噪声的权重。</li>
<li>Symbolic Descent（SD）提供了一种训练和优化TM的通用方法。</li>
<li>SEVEN方法通过SD的累积过程描述TM上的噪声批次梯度序列。</li>
<li>SEVEN倾向于保留具有持续高敏感性和小梯度噪声的权重。</li>
<li>实验证明SEVEN在多种剪枝场景和不同稀疏度级别上显著提高模型性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.12688">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2403.12688v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2403.12688v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2403.12688v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2403.12688v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2403.12688v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2403.12688v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-18/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-18/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-18/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Agent/2508.16044v2/page_0_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-09-18  Scaling Agents via Continual Pre-training
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-18/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12883v1/page_0_0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-09-18  WebResearcher Unleashing unbounded reasoning capability in Long-Horizon   Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29739.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
