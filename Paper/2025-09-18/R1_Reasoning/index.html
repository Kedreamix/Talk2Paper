<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-18  WebResearcher Unleashing unbounded reasoning capability in Long-Horizon   Agents">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12883v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-18-æ›´æ–°"><a href="#2025-09-18-æ›´æ–°" class="headerlink" title="2025-09-18 æ›´æ–°"></a>2025-09-18 æ›´æ–°</h1><h2 id="WebResearcher-Unleashing-unbounded-reasoning-capability-in-Long-Horizon-Agents"><a href="#WebResearcher-Unleashing-unbounded-reasoning-capability-in-Long-Horizon-Agents" class="headerlink" title="WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon   Agents"></a>WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon   Agents</h2><p><strong>Authors:Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, Rui Min, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou</strong></p>
<p>Recent advances in deep-research systems have demonstrated the potential for AI agents to autonomously discover and synthesize knowledge from external sources. In this paper, we introduce WebResearcher, a novel framework for building such agents through two key components: (1) WebResearcher, an iterative deep-research paradigm that reformulates deep research as a Markov Decision Process, where agents periodically consolidate findings into evolving reports while maintaining focused workspaces, overcoming the context suffocation and noise contamination that plague existing mono-contextual approaches; and (2) WebFrontier, a scalable data synthesis engine that generates high-quality training data through tool-augmented complexity escalation, enabling systematic creation of research tasks that bridge the gap between passive knowledge recall and active knowledge construction. Notably, we find that the training data from our paradigm significantly enhances tool-use capabilities even for traditional mono-contextual methods. Furthermore, our paradigm naturally scales through parallel thinking, enabling concurrent multi-agent exploration for more comprehensive conclusions. Extensive experiments across 6 challenging benchmarks demonstrate that WebResearcher achieves state-of-the-art performance, even surpassing frontier proprietary systems. </p>
<blockquote>
<p>è¿‘æœŸæ·±åº¦ç ”ç©¶ç³»ç»Ÿçš„è¿›å±•å·²ç»æ˜¾ç¤ºå‡ºäººå·¥æ™ºèƒ½ä»£ç†èƒ½å¤Ÿè‡ªä¸»ä»å¤–éƒ¨æ¥æºå‘ç°å’ŒåˆæˆçŸ¥è¯†çš„æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†WebResearcherï¼Œè¿™æ˜¯ä¸€ä¸ªæ„å»ºæ­¤ç±»ä»£ç†çš„æ–°å‹æ¡†æ¶ï¼Œå®ƒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰WebResearcherï¼Œè¿™æ˜¯ä¸€ç§è¿­ä»£æ·±åº¦ç ”ç©¶èŒƒå¼ï¼Œå®ƒå°†æ·±åº¦ç ”ç©¶é‡æ–°å®šä¹‰ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œä»£ç†ä¼šå®šæœŸæ•´åˆå‘ç°æˆæœå¹¶å½¢æˆä¸æ–­æ¼”å˜çš„æŠ¥å‘Šï¼ŒåŒæ—¶ä¿æŒä¸“æ³¨çš„å·¥ä½œç©ºé—´ï¼Œå…‹æœç°æœ‰å•ä¸Šä¸‹æ–‡æ–¹æ³•æ‰€é¢ä¸´çš„ä¸Šä¸‹æ–‡çª’æ¯å’Œå™ªå£°æ±¡æŸ“é—®é¢˜ï¼›ï¼ˆ2ï¼‰WebFrontierï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®åˆæˆå¼•æ“ï¼Œé€šè¿‡å·¥å…·å¢å¼ºå¤æ‚æ€§å‡çº§ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œèƒ½å¤Ÿç³»ç»Ÿåœ°åˆ›å»ºç ”ç©¶ä»»åŠ¡ï¼Œä»è€Œç¼©å°è¢«åŠ¨çŸ¥è¯†å›å¿†å’Œä¸»åŠ¨çŸ¥è¯†æ„å»ºä¹‹é—´çš„å·®è·ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„èŒƒå¼äº§ç”Ÿçš„è®­ç»ƒæ•°æ®ç”šè‡³å¯ä»¥æé«˜ä¼ ç»Ÿå•ä¸Šä¸‹æ–‡æ–¹æ³•çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„èŒƒå¼é€šè¿‡å¹¶è¡Œæ€ç»´è‡ªç„¶æ‰©å±•ï¼Œèƒ½å¤Ÿå®ç°å¤šä»£ç†å¹¶è¡Œæ¢ç´¢ä»¥å¾—å‡ºæ›´å…¨é¢çš„ç»“è®ºã€‚åœ¨6ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒWebResearcherè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œç”šè‡³è¶…è¶Šäº†å‰æ²¿ä¸“æœ‰ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13309v1">PDF</a> <a target="_blank" rel="noopener" href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/">https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/</a></p>
<p><strong>Summary</strong><br>åœ¨æ·±åº¦ç ”ç©¶ç³»ç»Ÿçš„æœ€æ–°è¿›å±•ä¸­ï¼ŒAIä»£ç†å¯ä»¥è‡ªä¸»å‘ç°å’Œæ•´åˆå¤–éƒ¨çŸ¥è¯†çš„èƒ½åŠ›å¾—åˆ°è¯æ˜ã€‚æœ¬æ–‡ä»‹ç»äº†WebResearcheræ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šä¸€æ˜¯WebResearcherè¿­ä»£æ·±åº¦ç ”ç©¶èŒƒå¼ï¼Œå°†æ·±åº¦ç ”ç©¶é‡æ–°å®šä¹‰ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œä»£ç†åœ¨æ­¤è¿‡ç¨‹ä¸­å®šæœŸæ•´åˆå‘ç°æˆæœå¹¶ç”Ÿæˆä¸æ–­æ›´æ–°çš„æŠ¥å‘Šï¼ŒåŒæ—¶ä¿æŒä¸“æ³¨çš„å·¥ä½œç©ºé—´ä»¥å…‹æœç°æœ‰å•ä¸Šä¸‹æ–‡æ–¹æ³•çš„ä¸Šä¸‹æ–‡çª’æ¯å’Œå™ªå£°æ±¡æŸ“é—®é¢˜ï¼›äºŒæ˜¯WebFrontierå¯æ‰©å±•æ•°æ®åˆæˆå¼•æ“ï¼Œå®ƒé€šè¿‡å·¥å…·å¢å¼ºå¤æ‚æ€§å‡çº§ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œèƒ½å¤Ÿç³»ç»Ÿåœ°åˆ›å»ºç ”ç©¶ä»»åŠ¡ï¼Œç¼©å°è¢«åŠ¨çŸ¥è¯†å›å¿†å’Œä¸»åŠ¨çŸ¥è¯†æ„å»ºä¹‹é—´çš„å·®è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWebResearcherè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œç”šè‡³åœ¨è¶…è¶Šå‰æ²¿ä¸“æœ‰ç³»ç»Ÿæ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒèƒ½å¾ˆå¥½åœ°åº”å¯¹å¤§è§„æ¨¡çš„å¹¶å‘éœ€æ±‚åœºæ™¯å’Œå¤šä¸»ä½“å‚ä¸çš„å†³ç­–åˆ¶å®šéœ€æ±‚åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>WebResearcheræ¡†æ¶å¼•å…¥äº†ä¸€ç§è¿­ä»£æ·±åº¦ç ”ç©¶èŒƒå¼ï¼Œå°†æ·±åº¦ç ”ç©¶å®šä¹‰ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>WebResearcheræ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•çš„ä¸Šä¸‹æ–‡çª’æ¯å’Œå™ªå£°æ±¡æŸ“é—®é¢˜ã€‚</li>
<li>WebFrontieræ•°æ®åˆæˆå¼•æ“å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œç¼©å°è¢«åŠ¨çŸ¥è¯†å›å¿†å’Œä¸»åŠ¨çŸ¥è¯†æ„å»ºä¹‹é—´çš„å·®è·ã€‚</li>
<li>è®­ç»ƒæ•°æ®è´¨é‡æå‡å¢å¼ºäº†å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œç”šè‡³å¯¹äºä¼ ç»Ÿçš„å•ä¸Šä¸‹æ–‡æ–¹æ³•ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</li>
<li>WebResearcheræ¡†æ¶å¤©ç„¶åœ°æ”¯æŒå¹¶è¡Œæ€è€ƒï¼Œèƒ½å¤Ÿæ”¯æŒå¤šä»£ç†çš„å¹¶å‘æ¢ç´¢ï¼Œæä¾›æ›´ä¸ºå…¨é¢çš„ç»“è®ºã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºWebResearcherè¾¾åˆ°ç”šè‡³è¶…è¶Šäº†æœ€å‰æ²¿çš„ä¸“æœ‰ç³»ç»Ÿæ€§èƒ½æ°´å¹³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13309">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13309v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13309v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13309v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="WebSailor-V2-Bridging-the-Chasm-to-Proprietary-Agents-via-Synthetic-Data-and-Scalable-Reinforcement-Learning"><a href="#WebSailor-V2-Bridging-the-Chasm-to-Proprietary-Agents-via-Synthetic-Data-and-Scalable-Reinforcement-Learning" class="headerlink" title="WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic   Data and Scalable Reinforcement Learning"></a>WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic   Data and Scalable Reinforcement Learning</h2><p><strong>Authors:Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, Xinyu Wang, Zile Qiao, Zhen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou</strong></p>
<p>Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all open-source agents in complex information-seeking tasks, matching proprietary agentsâ€™ performance and closing the capability gap. </p>
<blockquote>
<p>çªç ´äººç±»è®¤çŸ¥é™åˆ¶æ˜¯å¤§æ¨¡å‹è®­ç»ƒçš„é‡è¦å‰æ²¿é¢†åŸŸã€‚åƒDeepResearchè¿™æ ·çš„ä¸“æœ‰æ™ºèƒ½ä½“ç³»ç»Ÿå·²åœ¨æä¸ºå¤æ‚çš„æ£€ç´¢åŸºå‡†æµ‹è¯•ï¼ˆå¦‚BrowseCompï¼‰ä¸Šå±•ç°å‡ºè¶…äººç±»çš„èƒ½åŠ›ï¼Œè¿™æ˜¯ä¹‹å‰æ— æ³•å®ç°çš„ã€‚æˆ‘ä»¬è®¤ä¸ºå®ƒä»¬çš„æˆåŠŸä¾èµ–äºå¼€æºæ¨¡å‹æ‰€ç¼ºå°‘çš„é«˜çº§æ¨ç†æ¨¡å¼ï¼šåœ¨æµè§ˆæµ©ç€šçš„ä¿¡æ¯æ™¯è§‚æ—¶ï¼Œç³»ç»Ÿåœ°å‡å°‘æç«¯ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚åŸºäºæ­¤æ´å¯Ÿï¼Œæˆ‘ä»¬æ¨å‡ºäº†WebSailorï¼Œè¿™æ˜¯ä¸€ç§å®Œæ•´çš„åè®­ç»ƒæ–¹æ³•è®ºï¼Œæ—¨åœ¨åŸ¹å…»è¿™ç§å…³é”®èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬é€šè¿‡ç»“æ„åŒ–é‡‡æ ·ã€ä¿¡æ¯æ¨¡ç³Šå¤„ç†ã€RFTå†·å¯åŠ¨ä»¥åŠé«˜æ•ˆçš„æ™ºèƒ½å¼ºåŒ–å­¦ä¹ è®­ç»ƒç®—æ³•DUPOï¼ˆé‡å¤é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œç”Ÿæˆå…·æœ‰æ–°å‹å’Œé«˜ä¸ç¡®å®šæ€§çš„ä»»åŠ¡ã€‚æœ‰äº†è¿™ä¸ªç»¼åˆæµç¨‹ï¼ŒWebSailoråœ¨å¤æ‚çš„ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­å¤§å¤§è¶…è¶Šäº†æ‰€æœ‰å¼€æºæ™ºèƒ½ä½“ï¼ŒåŒ¹é…äº†ä¸“æœ‰æ™ºèƒ½ä½“çš„æ€§èƒ½ï¼Œå¹¶ç¼©å°äº†èƒ½åŠ›å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13305v1">PDF</a> <a target="_blank" rel="noopener" href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/">https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/</a></p>
<p><strong>æ‘˜è¦</strong><br>è¶…è¶Šäººç±»è®¤çŸ¥é™åˆ¶åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­ä»£è¡¨ç€é‡è¦çš„å‰æ²¿é¢†åŸŸã€‚ä¸“æœ‰æ™ºèƒ½ç³»ç»Ÿå¦‚DeepResearchå·²ç»åœ¨è¯¸å¦‚BrowseCompç­‰æç«¯å¤æ‚çš„ä¿¡æ¯æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†è¶…äººç±»çš„èƒ½åŠ›ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥å‰æ— æ³•è¾¾åˆ°çš„æˆå°±ã€‚æˆ‘ä»¬è®¤ä¸ºå…¶æˆåŠŸçš„å…³é”®åœ¨äºå¼€æ”¾æºä»£ç æ¨¡å‹æ‰€ç¼ºå°‘çš„å¤æ‚æ¨ç†æ¨¡å¼ï¼šåœ¨æµè§ˆå¤§é‡ä¿¡æ¯æ—¶ç³»ç»Ÿåœ°é™ä½æç«¯ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚åŸºäºæ­¤è§è§£ï¼Œæˆ‘ä»¬æ¨å‡ºäº†WebSailorï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨è®¾è®¡ç”¨äºåŸ¹å…»è¿™ç§é‡è¦èƒ½åŠ›çš„å…¨æ–°è®­ç»ƒåæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç»“æ„åŒ–é‡‡æ ·å’Œä¿¡æ¯æ¨¡ç³ŠåŒ–ç”Ÿæˆæ–°å‹é«˜ä¸ç¡®å®šæ€§ä»»åŠ¡ï¼Œç»“åˆåŸºäºå¼ºåŒ–å­¦ä¹ çš„å†·å¯åŠ¨å’Œé«˜æ•ˆçš„æ™ºèƒ½å¼ºåŒ–è®­ç»ƒç®—æ³•DUPOï¼ˆå¤åˆ¶é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œå½¢æˆäº†ä¸€ä¸ªç»¼åˆçš„ç®¡é“ã€‚WebSailoråœ¨å¤æ‚çš„ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºæ‰€æœ‰å¼€æºæ™ºèƒ½ä½“ï¼ŒåŒ¹é…ä¸“æœ‰æ™ºèƒ½ä½“çš„æ€§èƒ½å¹¶ç¼©å°äº†èƒ½åŠ›å·®è·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä¸“æœ‰æ™ºèƒ½ç³»ç»Ÿå·²ç»è¶…è¶Šäº†äººç±»åœ¨æŸäº›å¤æ‚ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚</li>
<li>DeepResearchåœ¨BrowseCompç­‰åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°è¯æ˜äº†è¿™ä¸€ç‚¹ã€‚</li>
<li>æˆåŠŸçš„åŸå› åœ¨äºä¸“æœ‰æ™ºèƒ½ç³»ç»Ÿå…·å¤‡é™ä½æç«¯ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚</li>
<li>WebSailoræ˜¯ä¸€ç§æ–°å‹è®­ç»ƒåæ–¹æ³•ï¼Œæ—¨åœ¨åŸ¹å…»è¿™ä¸€æ ¸å¿ƒèƒ½åŠ›ã€‚</li>
<li>WebSailoré€šè¿‡ç»“æ„åŒ–é‡‡æ ·å’Œä¿¡æ¯æ¨¡ç³ŠåŒ–ç”Ÿæˆé«˜ä¸ç¡®å®šæ€§ä»»åŠ¡ã€‚</li>
<li>å®ƒç»“åˆäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„å†·å¯åŠ¨æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13305">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13305v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13305v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Layout-Aware-OCR-for-Black-Digital-Archives-with-Unsupervised-Evaluation"><a href="#Layout-Aware-OCR-for-Black-Digital-Archives-with-Unsupervised-Evaluation" class="headerlink" title="Layout-Aware OCR for Black Digital Archives with Unsupervised Evaluation"></a>Layout-Aware OCR for Black Digital Archives with Unsupervised Evaluation</h2><p><strong>Authors:Fitsum Sileshi Beyene, Christopher L. Dancy</strong></p>
<p>Despite their cultural and historical significance, Black digital archives continue to be a structurally underrepresented area in AI research and infrastructure. This is especially evident in efforts to digitize historical Black newspapers, where inconsistent typography, visual degradation, and limited annotated layout data hinder accurate transcription, despite the availability of various systems that claim to handle optical character recognition (OCR) well. In this short paper, we present a layout-aware OCR pipeline tailored for Black newspaper archives and introduce an unsupervised evaluation framework suited to low-resource archival contexts. Our approach integrates synthetic layout generation, model pretraining on augmented data, and a fusion of state-of-the-art You Only Look Once (YOLO) detectors. We used three annotation-free evaluation metrics, the Semantic Coherence Score (SCS), Region Entropy (RE), and Textual Redundancy Score (TRS), which quantify linguistic fluency, informational diversity, and redundancy across OCR regions. Our evaluation on a 400-page dataset from ten Black newspaper titles demonstrates that layout-aware OCR improves structural diversity and reduces redundancy compared to full-page baselines, with modest trade-offs in coherence. Our results highlight the importance of respecting cultural layout logic in AI-driven document understanding and lay the foundation for future community-driven and ethically grounded archival AI systems. </p>
<blockquote>
<p>å°½ç®¡é»‘äººæ•°ç æ¡£æ¡ˆåœ¨å…¶æ–‡åŒ–å’Œå†å²æ„ä¹‰ä¸Šéå¸¸é‡è¦ï¼Œä½†åœ¨äººå·¥æ™ºèƒ½ç ”ç©¶å’ŒåŸºç¡€è®¾æ–½ä¸­ï¼Œå®ƒä»¬ä»ç„¶æ˜¯ä¸€ä¸ªè¢«ç»“æ„æ€§ä½ä¼°çš„é¢†åŸŸã€‚è¿™åœ¨å°†é»‘å†å²æŠ¥çº¸æ•°å­—åŒ–çš„å·¥ä½œä¸­å°¤ä¸ºæ˜æ˜¾ï¼Œå°½ç®¡æœ‰å„ç§å£°ç§°å¤„ç†å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å¾ˆå¥½çš„ç³»ç»Ÿï¼Œä½†ç”±äºæ’ç‰ˆä¸ä¸€è‡´ã€è§†è§‰é€€åŒ–ä»¥åŠæœ‰é™çš„æ³¨é‡Šå¸ƒå±€æ•°æ®ï¼Œä»é˜»ç¢äº†å‡†ç¡®çš„è½¬å½•ã€‚åœ¨è¿™ç¯‡ç®€çŸ­çš„è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹é»‘æŠ¥çº¸æ¡£æ¡ˆæå‡ºäº†ä¸€ç§å¸ƒå±€æ„ŸçŸ¥çš„OCRç®¡é“ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªé€‚ç”¨äºä½èµ„æºæ¡£æ¡ˆç¯å¢ƒçš„æ— ç›‘ç£è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†åˆæˆå¸ƒå±€ç”Ÿæˆã€åœ¨å¢å¼ºæ•°æ®ä¸Šè¿›è¡Œæ¨¡å‹é¢„è®­ç»ƒä»¥åŠæœ€å…ˆè¿›çš„You Only Look Onceï¼ˆYOLOï¼‰æ£€æµ‹å™¨çš„èåˆã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰ç§æ— æ³¨é‡Šè¯„ä¼°æŒ‡æ ‡ï¼Œå³è¯­ä¹‰è¿è´¯æ€§å¾—åˆ†ï¼ˆSCSï¼‰ã€åŒºåŸŸç†µï¼ˆREï¼‰å’Œæ–‡æœ¬å†—ä½™å¾—åˆ†ï¼ˆTRSï¼‰ï¼Œå®ƒä»¬å¯ä»¥é‡åŒ–OCRåŒºåŸŸçš„è¯­è¨€æµç•…æ€§ã€ä¿¡æ¯å¤šæ ·æ€§å’Œå†—ä½™æ€§ã€‚æˆ‘ä»¬å¯¹æ¥è‡ªåä»½é»‘æŠ¥çº¸æ ‡é¢˜çš„400é¡µæ•°æ®é›†è¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸å…¨é¡µåŸºçº¿ç›¸æ¯”ï¼Œå¸ƒå±€æ„ŸçŸ¥OCRæé«˜äº†ç»“æ„å¤šæ ·æ€§å¹¶å‡å°‘äº†å†—ä½™ï¼Œè¿è´¯æ€§æ–¹é¢åªæœ‰é€‚åº¦çš„æƒè¡¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†å°Šé‡æ–‡åŒ–å¸ƒå±€é€»è¾‘åœ¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ–‡ä»¶ç†è§£ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç¤¾åŒºé©±åŠ¨å’ŒåŸºäºä¼¦ç†çš„æ¡£æ¡ˆäººå·¥æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13236v1">PDF</a> IEEE-ISTAS conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨é»‘äººç¾¤ä½“çš„æ•°å­—æ¡£æ¡ˆåœ¨äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­çš„é‡è¦æ€§ã€‚é’ˆå¯¹æ•°å­—åŒ–é»‘å†å²æŠ¥çº¸é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é¢å‘å¸ƒå±€çš„å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ç®¡é“ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªé€‚åˆä½èµ„æºæ¡£æ¡ˆç¯å¢ƒçš„æ— ç›‘ç£è¯„ä¼°æ¡†æ¶ã€‚è¯¥ç ”ç©¶é›†æˆäº†åˆæˆå¸ƒå±€ç”Ÿæˆã€åœ¨å¢å¼ºæ•°æ®ä¸Šçš„æ¨¡å‹é¢„è®­ç»ƒï¼Œä»¥åŠæœ€å…ˆè¿›çš„You Only Look Onceï¼ˆYOLOï¼‰æ£€æµ‹å™¨çš„èåˆã€‚é€šè¿‡åœ¨ä¸€ä¸ªåŒ…å«æ¥è‡ªåä¸ªé»‘æŠ¥çº¸æ ‡é¢˜çš„400é¡µæ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼Œè¯æ˜äº†é¢å‘å¸ƒå±€çš„OCRåœ¨æé«˜ç»“æ„å¤šæ ·æ€§å’Œå‡å°‘å†—ä½™æ–¹é¢ç›¸è¾ƒäºå…¨é¡µåŸºçº¿æœ‰æ‰€æ”¹å–„ï¼Œå°½ç®¡åœ¨è¯­è¨€æµç•…æ€§æ–¹é¢å­˜åœ¨é€‚åº¦çš„æƒè¡¡ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å°Šé‡æ–‡åŒ–å¸ƒå±€é€»è¾‘åœ¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ–‡ä»¶ç†è§£ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç¤¾åŒºé©±åŠ¨å’Œé“å¾·ç«‹è¶³çš„æ¡£æ¡ˆäººå·¥æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é»‘äººç¾¤ä½“çš„æ•°å­—æ¡£æ¡ˆåœ¨äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­çš„é‡è¦æ€§è¢«å¿½è§†ã€‚</li>
<li>é¢å‘å¸ƒå±€çš„å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ç®¡é“å¯¹äºæ•°å­—åŒ–é»‘å†å²æŠ¥çº¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>é›†æˆåˆæˆå¸ƒå±€ç”Ÿæˆã€æ¨¡å‹é¢„è®­ç»ƒå’ŒYOLOæ£€æµ‹å™¨çš„èåˆæ˜¯è§£å†³è¿™ä¸€æŒ‘æˆ˜çš„å…³é”®ã€‚</li>
<li>é¢å‘å¸ƒå±€çš„OCRèƒ½å¤Ÿæé«˜ç»“æ„å¤šæ ·æ€§å’Œå‡å°‘å†—ä½™ã€‚</li>
<li>é¢å‘å¸ƒå±€çš„OCRç›¸è¾ƒäºå…¨é¡µåŸºçº¿åœ¨è¯­è¨€æµç•…æ€§æ–¹é¢å­˜åœ¨é€‚åº¦çš„æƒè¡¡ã€‚</li>
<li>å°Šé‡æ–‡åŒ–å¸ƒå±€é€»è¾‘åœ¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ–‡ä»¶ç†è§£ä¸­éå¸¸é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13236v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13236v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13236v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13236v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13236v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Reasoning-with-Preference-Constraints-A-Benchmark-for-Language-Models-in-Many-to-One-Matching-Markets"><a href="#Reasoning-with-Preference-Constraints-A-Benchmark-for-Language-Models-in-Many-to-One-Matching-Markets" class="headerlink" title="Reasoning with Preference Constraints: A Benchmark for Language Models   in Many-to-One Matching Markets"></a>Reasoning with Preference Constraints: A Benchmark for Language Models   in Many-to-One Matching Markets</h2><p><strong>Authors:Marylou Fauchard, Florian Carichon, Margarida Carvalho, Golnoosh Farnadi</strong></p>
<p>Recent advances in reasoning with large language models (LLMs) have demonstrated strong performance on complex mathematical tasks, including combinatorial optimization. Techniques such as Chain-of-Thought and In-Context Learning have further enhanced this capability, making LLMs both powerful and accessible tools for a wide range of users, including non-experts. However, applying LLMs to matching problems, which require reasoning under preferential and structural constraints, remains underexplored. To address this gap, we introduce a novel benchmark of 369 instances of the College Admission Problem, a canonical example of a matching problem with preferences, to evaluate LLMs across key dimensions: feasibility, stability, and optimality. We employ this benchmark to assess the performance of several open-weight LLMs. Our results first reveal that while LLMs can satisfy certain constraints, they struggle to meet all evaluation criteria consistently. They also show that reasoning LLMs, like QwQ and GPT-oss, significantly outperform traditional models such as Llama, Qwen or Mistral, defined here as models used without any dedicated reasoning mechanisms. Moreover, we observed that LLMs reacted differently to the various prompting strategies tested, which include Chain-of-Thought, In-Context Learning and role-based prompting, with no prompt consistently offering the best performance. Finally, we report the performances from iterative prompting with auto-generated feedback and show that they are not monotonic; they can peak early and then significantly decline in later attempts. Overall, this work offers a new perspective on model reasoning performance and the effectiveness of prompting strategies in combinatorial optimization problems with preferential constraints. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢çš„è¿›å±•å·²åœ¨å¤æ‚æ•°å­¦ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ç»„åˆä¼˜åŒ–ã€‚è¯¸å¦‚â€œæ€è€ƒé“¾â€å’Œâ€œä¸Šä¸‹æ–‡å­¦ä¹ â€ç­‰æŠ€æœ¯è¿›ä¸€æ­¥å¢å¼ºäº†è¿™ä¸€èƒ½åŠ›ï¼Œä½¿LLMæˆä¸ºå¼ºå¤§ä¸”æ˜“äºä½¿ç”¨çš„å·¥å…·ï¼Œé€‚ç”¨äºå¹¿æ³›çš„ç”¨æˆ·ï¼ŒåŒ…æ‹¬éä¸“ä¸šäººå£«ã€‚ç„¶è€Œï¼Œå°†LLMåº”ç”¨äºåŒ¹é…é—®é¢˜ï¼Œè¿™ä¸ªé—®é¢˜éœ€è¦åœ¨åå¥½å’Œç»“æ„çº¦æŸä¸‹è¿›è¡Œæ¨ç†ï¼Œä»ç„¶ç¼ºä¹è¶³å¤Ÿçš„æ¢ç´¢ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ…å«369ä¸ªå®ä¾‹çš„â€œå¤§å­¦å½•å–é—®é¢˜â€æ–°åŸºå‡†æµ‹è¯•ï¼Œä½œä¸ºå…·æœ‰åå¥½çš„åŒ¹é…é—®é¢˜çš„å…¸å‹ç¤ºä¾‹ï¼Œä»¥è¯„ä¼°LLMçš„å…³é”®ç»´åº¦ï¼šå¯è¡Œæ€§ã€ç¨³å®šæ€§å’Œæœ€ä¼˜æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤åŸºå‡†æ¥è¯„ä¼°å‡ ä¸ªå¼€æºæƒé‡LLMçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœé¦–å…ˆè¡¨æ˜ï¼Œè™½ç„¶LLMå¯ä»¥æ»¡è¶³æŸäº›çº¦æŸï¼Œä½†å®ƒä»¬éš¾ä»¥å§‹ç»ˆå¦‚ä¸€åœ°æ»¡è¶³æ‰€æœ‰è¯„ä¼°æ ‡å‡†ã€‚ä»–ä»¬è¿˜æ˜¾ç¤ºï¼ŒåƒQwQå’ŒGPT-ossè¿™æ ·çš„æ¨ç†LLMæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æ¨¡å‹ï¼Œå¦‚Llamaã€Qwenæˆ–Mistralï¼Œè¿™é‡Œå®šä¹‰ä¸ºæ²¡æœ‰ä»»ä½•ä¸“é—¨æ¨ç†æœºåˆ¶çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°LLMå¯¹æµ‹è¯•çš„å„ç§æç¤ºç­–ç•¥çš„ååº”å„ä¸ç›¸åŒï¼ŒåŒ…æ‹¬â€œæ€è€ƒé“¾â€ã€â€œä¸Šä¸‹æ–‡å­¦ä¹ â€å’ŒåŸºäºè§’è‰²çš„æç¤ºï¼Œæ²¡æœ‰ä»»ä½•æç¤ºå§‹ç»ˆæä¾›æœ€ä½³æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†é€šè¿‡è‡ªåŠ¨ç”Ÿæˆåé¦ˆè¿›è¡Œè¿­ä»£æç¤ºçš„æ€§èƒ½ï¼Œå¹¶æ˜¾ç¤ºå®ƒä»¬å¹¶éå•è°ƒï¼›å®ƒä»¬å¯èƒ½åœ¨æ—©æœŸè¾¾åˆ°å³°å€¼ï¼Œç„¶ååœ¨åç»­å°è¯•ä¸­æ˜¾è‘—ä¸‹é™ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹å·¥ä½œæä¾›äº†å…³äºæ¨¡å‹æ¨ç†æ€§èƒ½å’Œåå¥½çº¦æŸä¸‹çš„ç»„åˆä¼˜åŒ–é—®é¢˜ä¸­çš„æç¤ºç­–ç•¥æœ‰æ•ˆæ€§çš„æ–°è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13131v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚çš„æ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ç»„åˆä¼˜åŒ–ã€‚é€šè¿‡Chain-of-Thoughtå’ŒIn-Context Learningç­‰æŠ€æœ¯ï¼ŒLLMçš„æ¨ç†èƒ½åŠ›å¾—åˆ°äº†è¿›ä¸€æ­¥æå‡ï¼Œä½¿å…¶æˆä¸ºéä¸“å®¶ç”¨æˆ·ä¹Ÿèƒ½ä½¿ç”¨çš„å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼Œå°†LLMåº”ç”¨äºå¸¦æœ‰åå¥½å’Œç»“æ„çº¦æŸçš„åŒ¹é…é—®é¢˜ä¸Šä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ…å«369ä¸ªå­¦é™¢å½•å–é—®é¢˜å®ä¾‹çš„æ–°åŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°LLMåœ¨å¯è¡Œæ€§ã€ç¨³å®šæ€§å’Œæœ€ä¼˜æ€§ç­‰æ–¹é¢çš„å…³é”®ç»´åº¦ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLMè™½èƒ½æ»¡è¶³æŸäº›çº¦æŸï¼Œä½†åœ¨ä¸€è‡´æ»¡è¶³è¯„ä»·å‡†åˆ™ä¸Šä»æœ‰å›°éš¾ã€‚åŒæ—¶ï¼Œå…·æœ‰æ¨ç†æœºåˆ¶çš„LLMï¼ˆå¦‚QwQå’ŒGPT-ossï¼‰æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼ˆå¦‚Llamaã€Qwenå’ŒMistralï¼‰ã€‚æ­¤å¤–ï¼Œä¸åŒçš„æç¤ºç­–ç•¥å¯¹LLMçš„ååº”æœ‰æ‰€ä¸åŒï¼Œæ²¡æœ‰ä¸€ç§æç¤ºç­–ç•¥å§‹ç»ˆè¡¨ç°æœ€ä½³ã€‚æœ€åï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†è‡ªåŠ¨åé¦ˆè¿­ä»£æç¤ºçš„æ€§èƒ½ç»“æœï¼Œè¿™äº›æ€§èƒ½å¹¶ä¸å•è°ƒï¼Œå¯èƒ½åœ¨æ—©æœŸè¾¾åˆ°å³°å€¼ç„¶ååœ¨åç»­å°è¯•ä¸­æ˜¾è‘—ä¸‹é™ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹å·¥ä½œä¸ºæ¨¡å‹æ¨ç†æ€§èƒ½å’Œåå¥½çº¦æŸç»„åˆä¼˜åŒ–é—®é¢˜ä¸­çš„æç¤ºç­–ç•¥æœ‰æ•ˆæ€§æä¾›äº†æ–°çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚çš„æ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¤„ç†ç»„åˆä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>LLMå¯ä»¥é€šè¿‡Chain-of-Thoughtå’ŒIn-Context Learningç­‰æŠ€æœ¯è¿›è¡Œå¢å¼ºï¼Œä½¿å…¶å˜å¾—æ›´åŠ æ˜“ç”¨å’Œå¼ºå¤§ã€‚</li>
<li>LLMåœ¨è§£å†³å¸¦æœ‰åå¥½å’Œç»“æ„çº¦æŸçš„åŒ¹é…é—®é¢˜ä¸Šä»æœ‰å¾…æ¢ç´¢ã€‚</li>
<li>æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«369ä¸ªå­¦é™¢å½•å–é—®é¢˜å®ä¾‹ï¼Œä»¥è¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LLMåœ¨æ»¡è¶³æ‰€æœ‰è¯„ä»·å‡†åˆ™ä¸Šè¡¨ç°æŒ£æ‰ï¼Œä½†å…·æœ‰æ¨ç†æœºåˆ¶çš„LLMï¼ˆå¦‚QwQå’ŒGPT-ossï¼‰æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡å‹ã€‚</li>
<li>ä¸åŒçš„æç¤ºç­–ç•¥å¯¹LLMçš„å½±å“ä¸åŒï¼Œæ²¡æœ‰ä¸€ç§ç­–ç•¥å§‹ç»ˆæœ€ä½³ã€‚</li>
<li>è‡ªåŠ¨åé¦ˆè¿­ä»£æç¤ºçš„æ€§èƒ½ä¸å•è°ƒï¼Œå¯èƒ½åœ¨æ—©æœŸè¾¾åˆ°å³°å€¼åä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13131v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13131v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13131v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Rethinking-the-Evaluation-of-Alignment-Methods-Insights-into-Diversity-Generalisation-and-Safety"><a href="#Rethinking-the-Evaluation-of-Alignment-Methods-Insights-into-Diversity-Generalisation-and-Safety" class="headerlink" title="Rethinking the Evaluation of Alignment Methods: Insights into Diversity,   Generalisation, and Safety"></a>Rethinking the Evaluation of Alignment Methods: Insights into Diversity,   Generalisation, and Safety</h2><p><strong>Authors:Denis Janiak, Julia Moska, Dawid Motyka, Karolina Seweryn, PaweÅ‚ Walkowiak, Bartosz Å»uk, Arkadiusz Janz</strong></p>
<p>Large language models (LLMs) require careful alignment to balance competing objectives - factuality, safety, conciseness, proactivity, and diversity. Existing studies focus on individual techniques or specific dimensions, lacking a holistic assessment of the inherent trade-offs. We propose a unified evaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO) across these five axes, using both in-distribution and out-of-distribution datasets. Leveraging a specialized LLM-as-Judge prompt, validated through human studies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead in safety, and PPO best balances conciseness with proactivity. Our findings provide insights into trade-offs of common alignment methods, guiding the development of more balanced and reliable LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦è°¨æ…å¯¹é½ä»¥å¹³è¡¡ç›¸äº’ç«äº‰çš„ç›®æ ‡ï¼ŒåŒ…æ‹¬çœŸå®æ€§ã€å®‰å…¨æ€§ã€ç®€æ´æ€§ã€ä¸»åŠ¨æ€§å’Œå¤šæ ·æ€§ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¸ªåˆ«æŠ€æœ¯æˆ–ç‰¹å®šç»´åº¦ä¸Šï¼Œç¼ºä¹å¯¹è¿™äº›ç›®æ ‡å†…åœ¨æƒè¡¡çš„å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡è¿™ä¸ªæ¡†æ¶æ¯”è¾ƒLLMå¯¹é½æ–¹æ³•ï¼ˆPPOã€DPOã€ORPOã€KTOï¼‰åœ¨è¿™äº”ä¸ªè½´ä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶ä½¿ç”¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–çš„æ•°æ®é›†ã€‚é€šè¿‡å€ŸåŠ©ä¸“é—¨çš„LLM-as-Judgeæç¤ºå¹¶é€šè¿‡äººç±»ç ”ç©¶è¿›è¡ŒéªŒè¯ï¼Œæˆ‘ä»¬å‘ç°DPOå’ŒKTOåœ¨äº‹å®å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒPPOå’ŒDPOåœ¨å®‰å…¨æ€§æ–¹é¢é¢†å…ˆï¼Œè€ŒPPOåœ¨ç®€æ´æ€§å’Œä¸»åŠ¨æ€§ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå¸¸è§çš„å¯¹é½æ–¹æ³•æä¾›äº†æƒè¡¡çš„è§è§£ï¼Œä¸ºå¼€å‘æ›´å¹³è¡¡ã€æ›´å¯é çš„å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12936v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦åœ¨å¤šä¸ªç›®æ ‡ä¹‹é—´å®ç°è°¨æ…çš„å¯¹é½ï¼ŒåŒ…æ‹¬äº‹å®æ€§ã€å®‰å…¨æ€§ã€ç®€æ´æ€§ã€ä¸»åŠ¨æ€§å’Œå¤šæ ·æ€§ã€‚ç°æœ‰ç ”ç©¶ä¾§é‡äºä¸ªåˆ«æŠ€æœ¯æˆ–ç‰¹å®šç»´åº¦ï¼Œç¼ºä¹å¯¹ä¸åŒå¯¹é½æ–¹æ³•å†…åœ¨æƒè¡¡çš„å…¨é¢è¯„ä¼°ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæ¯”è¾ƒLLMå¯¹é½æ–¹æ³•ï¼ˆPPOã€DPOã€ORPOã€KTOï¼‰åœ¨è¿™äº”ä¸ªæ–¹é¢çš„è¡¨ç°ï¼ŒåŒæ—¶ä½¿ç”¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–çš„æ•°æ®é›†ã€‚é€šè¿‡ä¸“é¡¹çš„LLM-as-Judgeæç¤ºå’Œäººç±»ç ”ç©¶éªŒè¯ï¼Œå‘ç°DPOå’ŒKTOåœ¨äº‹å®å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒPPOå’ŒDPOåœ¨å®‰å…¨æ–¹é¢é¢†å…ˆï¼Œä¸”åœ¨ç®€æ´æ€§ä¸ä¸»åŠ¨æ€§ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡ã€‚æœ¬ç ”ç©¶ç»“æœæä¾›äº†å¸¸è§å¯¹é½æ–¹æ³•çš„æƒè¡¡æ´å¯Ÿï¼Œä¸ºå¼€å‘æ›´å¹³è¡¡ã€æ›´å¯é çš„å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦å¯¹å¤šä¸ªç›®æ ‡è¿›è¡Œå¯¹é½ï¼ŒåŒ…æ‹¬äº‹å®æ€§ã€å®‰å…¨æ€§ç­‰ã€‚</li>
<li>ç°æœ‰ç ”ç©¶åœ¨è¯„ä¼°LLMå¯¹é½æ–¹æ³•æ—¶ç¼ºä¹å…¨é¢è¯„ä¼°å†…åœ¨æƒè¡¡çš„æ¡†æ¶ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæ¯”è¾ƒä¸åŒLLMå¯¹é½æ–¹æ³•çš„è¡¨ç°ã€‚</li>
<li>é€šè¿‡ä¸“é¡¹çš„LLM-as-Judgeæç¤ºå’Œäººç±»ç ”ç©¶éªŒè¯ï¼Œå‘ç°DPOå’ŒKTOåœ¨äº‹å®å‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>PPOå’ŒDPOåœ¨å®‰å…¨æ–¹é¢é¢†å…ˆå…¶ä»–å¯¹é½æ–¹æ³•ã€‚</li>
<li>PPOåœ¨ç®€æ´æ€§ä¸ä¸»åŠ¨æ€§ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12936">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12936v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12936v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12936v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12936v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12936v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DialNav-Multi-turn-Dialog-Navigation-with-a-Remote-Guide"><a href="#DialNav-Multi-turn-Dialog-Navigation-with-a-Remote-Guide" class="headerlink" title="DialNav: Multi-turn Dialog Navigation with a Remote Guide"></a>DialNav: Multi-turn Dialog Navigation with a Remote Guide</h2><p><strong>Authors:Leekyeung Han, Hyunji Min, Gyeom Hwangbo, Jonghyun Choi, Paul Hongsuck Seo</strong></p>
<p>We introduce DialNav, a novel collaborative embodied dialog task, where a navigation agent (Navigator) and a remote guide (Guide) engage in multi-turn dialog to reach a goal location. Unlike prior work, DialNav aims for holistic evaluation and requires the Guide to infer the Navigatorâ€™s location, making communication essential for task success. To support this task, we collect and release the Remote Assistance in Navigation (RAIN) dataset, human-human dialog paired with navigation trajectories in photorealistic environments. We design a comprehensive benchmark to evaluate both navigation and dialog, and conduct extensive experiments analyzing the impact of different Navigator and Guide models. We highlight key challenges and publicly release the dataset, code, and evaluation framework to foster future research in embodied dialog. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†DialNavï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„åˆä½œå¼ä½“éªŒå¯¹è¯ä»»åŠ¡ï¼Œå…¶ä¸­å¯¼èˆªä»£ç†ï¼ˆNavigatorï¼‰å’Œè¿œç¨‹æŒ‡å—ï¼ˆGuideï¼‰è¿›è¡Œå¤šè½®å¯¹è¯ä»¥è¾¾åˆ°ç›®æ ‡åœ°ç‚¹ã€‚ä¸åŒäºä»¥å‰çš„å·¥ä½œï¼ŒDialNavæ—¨åœ¨è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œå¹¶è¦æ±‚æŒ‡å—æ¨æ–­å¯¼èˆªå™¨çš„ä½ç½®ï¼Œè¿™ä½¿å¾—æ²Ÿé€šå¯¹äºä»»åŠ¡æˆåŠŸè‡³å…³é‡è¦ã€‚ä¸ºäº†æ”¯æŒè¿™é¡¹ä»»åŠ¡ï¼Œæˆ‘ä»¬æ”¶é›†å’Œå‘å¸ƒäº†è¿œç¨‹å¯¼èˆªè¾…åŠ©ï¼ˆRAINï¼‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«äººç±»ä¹‹é—´çš„å¯¹è¯ä»¥åŠä¸é€¼çœŸç¯å¢ƒä¸­çš„å¯¼èˆªè½¨è¿¹é…å¯¹çš„æ•°æ®ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å¯¼èˆªå’Œå¯¹è¯ï¼Œå¹¶è¿›è¡Œäº†å¹¿æ³›å®éªŒåˆ†æä¸åŒå¯¼èˆªå™¨å’ŒæŒ‡å—æ¨¡å‹çš„å½±å“ã€‚æˆ‘ä»¬å¼ºè°ƒäº†å…³é”®æŒ‘æˆ˜ï¼Œå¹¶å…¬å¼€å‘å¸ƒæ•°æ®é›†ã€ä»£ç å’Œè¯„ä¼°æ¡†æ¶ï¼Œä»¥ä¿ƒè¿›æœªæ¥åœ¨ä½“éªŒå¼å¯¹è¯æ–¹é¢çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12894v1">PDF</a> 18 pages, 8 figures, ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DialNavè¿™ä¸€æ–°å‹åä½œå¼ä½“ç°å¯¹è¯ä»»åŠ¡ï¼Œä»¥åŠä¸ä¹‹é…å¥—çš„æ•°æ®é›†RAINã€‚åœ¨è¯¥ä»»åŠ¡ä¸­ï¼Œå¯¼èˆªä»£ç†ï¼ˆNavigatorï¼‰å’Œè¿œç¨‹æŒ‡å—ï¼ˆGuideï¼‰é€šè¿‡å¤šè½®å¯¹è¯è¾¾æˆç›®æ ‡åœ°ç‚¹ã€‚ä¸ä»¥å¾€å·¥ä½œä¸åŒï¼ŒDialNavè¿½æ±‚æ•´ä½“è¯„ä¼°ï¼Œå¹¶è¦æ±‚æŒ‡å—æ¨æµ‹å¯¼èˆªå™¨çš„ä½ç½®ï¼Œä½¿å¾—æ²Ÿé€šå¯¹ä»»åŠ¡æˆåŠŸè‡³å…³é‡è¦ã€‚æœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å¯¼èˆªå’Œå¯¹è¯èƒ½åŠ›ï¼Œå¹¶è¿›è¡Œäº†å¤§é‡å®éªŒåˆ†æä¸åŒå¯¼èˆªå™¨å’ŒæŒ‡å—æ¨¡å‹çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DialNavæ˜¯ä¸€ç§æ–°å‹åä½œå¼ä½“ç°å¯¹è¯ä»»åŠ¡ï¼Œæ¶‰åŠå¯¼èˆªä»£ç†å’Œè¿œç¨‹æŒ‡å—çš„å¤šè½®å¯¹è¯ä»¥è¾¾åˆ°ç›®æ ‡åœ°ç‚¹ã€‚</li>
<li>DialNavè¿½æ±‚æ•´ä½“è¯„ä¼°ï¼Œè¦æ±‚æŒ‡å—èƒ½å¤Ÿæ¨æ–­å¯¼èˆªå™¨çš„ä½ç½®ï¼Œæ²Ÿé€šå¯¹ä»»åŠ¡æˆåŠŸè‡³å…³é‡è¦ã€‚</li>
<li>ä¸ºæ”¯æŒDialNavä»»åŠ¡ï¼Œå‘å¸ƒäº†Remote Assistance in Navigation (RAIN)æ•°æ®é›†ï¼ŒåŒ…å«äººç±»å¯¹è¯ä¸å¯¼èˆªè½¨è¿¹ã€‚</li>
<li>è®¾è®¡äº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ä»¥è¯„ä¼°å¯¼èˆªå’Œå¯¹è¯èƒ½åŠ›ã€‚</li>
<li>è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œåˆ†æäº†ä¸åŒå¯¼èˆªå™¨å’ŒæŒ‡å—æ¨¡å‹çš„å½±å“ã€‚</li>
<li>å¼ºè°ƒäº†ä»»åŠ¡çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12894">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12894v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12894v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12894v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12894v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12894v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Lego-Edit-A-General-Image-Editing-Framework-with-Model-Level-Bricks-and-MLLM-Builder"><a href="#Lego-Edit-A-General-Image-Editing-Framework-with-Model-Level-Bricks-and-MLLM-Builder" class="headerlink" title="Lego-Edit: A General Image Editing Framework with Model-Level Bricks and   MLLM Builder"></a>Lego-Edit: A General Image Editing Framework with Model-Level Bricks and   MLLM Builder</h2><p><strong>Authors:Qifei Jia, Yu Liu, Yajie Chai, Xintong Yao, Qiming Lu, Yasen Zhang, Runyu Shi, Ying Huang, Guoquan Zhang</strong></p>
<p>Instruction-based image editing has garnered significant attention due to its direct interaction with users. However, real-world user instructions are immensely diverse, and existing methods often fail to generalize effectively to instructions outside their training domain, limiting their practical application. To address this, we propose Lego-Edit, which leverages the generalization capability of Multi-modal Large Language Model (MLLM) to organize a suite of model-level editing tools to tackle this challenge. Lego-Edit incorporates two key designs: (1) a model-level toolkit comprising diverse models efficiently trained on limited data and several image manipulation functions, enabling fine-grained composition of editing actions by the MLLM; and (2) a three-stage progressive reinforcement learning approach that uses feedback on unannotated, open-domain instructions to train the MLLM, equipping it with generalized reasoning capabilities for handling real-world instructions. Experiments demonstrate that Lego-Edit achieves state-of-the-art performance on GEdit-Bench and ImgBench. It exhibits robust reasoning capabilities for open-domain instructions and can utilize newly introduced editing tools without additional fine-tuning.   Code is available: <a target="_blank" rel="noopener" href="https://github.com/xiaomi-research/lego-edit">https://github.com/xiaomi-research/lego-edit</a>. </p>
<blockquote>
<p>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘å› å…¶ä¸ç”¨æˆ·ä¹‹é—´çš„ç›´æ¥äº¤äº’è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œä¸­çš„ç”¨æˆ·æŒ‡ä»¤æå…¶å¤šæ ·ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥æœ‰æ•ˆåœ°æ¨å¹¿åˆ°è®­ç»ƒåŸŸä¹‹å¤–çš„æŒ‡ä»¤ï¼Œä»è€Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Lego-Editï¼Œå®ƒåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ³›åŒ–èƒ½åŠ›ï¼Œç»„ç»‡äº†ä¸€ç³»åˆ—æ¨¡å‹çº§ç¼–è¾‘å·¥å…·æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚Lego-EditåŒ…å«ä¸¤ä¸ªå…³é”®è®¾è®¡ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªæ¨¡å‹çº§å·¥å…·åŒ…ï¼Œå…¶ä¸­åŒ…å«å¤šç§åœ¨æœ‰é™æ•°æ®ä¸Šæœ‰æ•ˆè®­ç»ƒè¿‡çš„æ¨¡å‹ä»¥åŠå¤šä¸ªå›¾åƒæ“ä½œåŠŸèƒ½ï¼Œä½¿MLLMèƒ½å¤Ÿè¿›è¡Œç²¾ç»†çš„ç¼–è¾‘åŠ¨ä½œç»„åˆï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªä¸‰é˜¶æ®µçš„æ¸è¿›å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå®ƒä½¿ç”¨å¯¹æœªæ ‡æ³¨çš„å¼€æ”¾åŸŸæŒ‡ä»¤çš„åé¦ˆæ¥è®­ç»ƒMLLMï¼Œä½¿å…¶å…·å¤‡å¤„ç†ç°å®ä¸–ç•Œä¸­æŒ‡ä»¤çš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒLego-Editåœ¨GEdit-Benchå’ŒImgBenchä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒå±•ç°å‡ºå¯¹å¼€æ”¾åŸŸæŒ‡ä»¤çš„ç¨³å¥æ¨ç†èƒ½åŠ›ï¼Œå¹¶èƒ½ä½¿ç”¨æ–°å¼•å…¥çš„ç¼–è¾‘å·¥å…·è€Œæ— éœ€é¢å¤–çš„å¾®è°ƒã€‚ä»£ç å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/xiaomi-research/lego-edit%E3%80%82">https://github.com/xiaomi-research/lego-editã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12883v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘å› å…¶ç›´æ¥çš„ç”¨æˆ·äº¤äº’è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼ŒçœŸå®ä¸–ç•Œçš„ç”¨æˆ·æŒ‡ä»¤æå…¶å¤šæ ·ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥æœ‰æ•ˆæ¨å¹¿åˆ°è®­ç»ƒåŸŸä¹‹å¤–çš„æŒ‡ä»¤ï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Lego-Editï¼Œå®ƒåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ³›åŒ–èƒ½åŠ›ï¼Œè®¾è®¡äº†ä¸€ç³»åˆ—æ¨¡å‹çº§ç¼–è¾‘å·¥å…·æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚Lego-EditåŒ…å«ä¸¤ä¸ªå…³é”®è®¾è®¡ï¼šä¸€æ˜¯åŒ…å«å¤šç§æ¨¡å‹çš„é«˜æ•ˆå·¥å…·åŒ…ï¼Œè¿™äº›æ¨¡å‹åœ¨æœ‰é™æ•°æ®ä¸Šè®­ç»ƒæœ‰å›¾åƒæ“ä½œåŠŸèƒ½ï¼Œä½¿å¾—MLLMèƒ½å¤Ÿè¿›è¡Œç²¾ç»†çš„ç¼–è¾‘åŠ¨ä½œç»„åˆï¼›äºŒæ˜¯é‡‡ç”¨ä¸‰é˜¶æ®µæ¸è¿›å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œåˆ©ç”¨æœªæ ‡æ³¨çš„å¼€æ”¾é¢†åŸŸæŒ‡ä»¤åé¦ˆæ¥è®­ç»ƒMLLMï¼Œä½¿å…¶å…·å¤‡å¤„ç†çœŸå®ä¸–ç•ŒæŒ‡ä»¤çš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒLego-Editåœ¨GEdit-Benchå’ŒImgbenchä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¯¹å¼€æ”¾é¢†åŸŸæŒ‡ä»¤å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶èƒ½ä½¿ç”¨æ–°å¼•å…¥çš„ç¼–è¾‘å·¥å…·è€Œæ— éœ€é¢å¤–å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘é¢ä¸´ç”¨æˆ·æŒ‡ä»¤å¤šæ ·æ€§å¯¼è‡´çš„æ³›åŒ–éš¾é¢˜ã€‚</li>
<li>Lego-Editåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>Lego-EditåŒ…å«æ¨¡å‹çº§å·¥å…·åŒ…ï¼Œèƒ½åœ¨æœ‰é™æ•°æ®ä¸Šè®­ç»ƒå¤šç§å›¾åƒæ“ä½œåŠŸèƒ½ã€‚</li>
<li>Lego-Edité‡‡ç”¨ä¸‰é˜¶æ®µæ¸è¿›å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨æœªæ ‡æ³¨çš„å¼€æ”¾é¢†åŸŸæŒ‡ä»¤åé¦ˆè®­ç»ƒæ¨¡å‹ã€‚</li>
<li>Lego-Editå…·å¤‡å¤„ç†çœŸå®ä¸–ç•ŒæŒ‡ä»¤çš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºLego-Editåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12883v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12883v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12883v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12883v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LTA-thinker-Latent-Thought-Augmented-Training-Framework-for-Large-Language-Models-on-Complex-Reasoning"><a href="#LTA-thinker-Latent-Thought-Augmented-Training-Framework-for-Large-Language-Models-on-Complex-Reasoning" class="headerlink" title="LTA-thinker: Latent Thought-Augmented Training Framework for Large   Language Models on Complex Reasoning"></a>LTA-thinker: Latent Thought-Augmented Training Framework for Large   Language Models on Complex Reasoning</h2><p><strong>Authors:Jiaqi Wang, Binquan Ji, Haibo Luo, Yiyang Qi, Ruiting Li, Huiyan Wang, Yuantao Han, Cangyi Yang, jiaxu Zhang, Feiliang Ren</strong></p>
<p>Complex Reasoning in Large Language Models can be dynamically optimized using Test-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut, SoftCoT and its variant are effective in continuous latent space inference, the core bottleneck still lies in the efficient generation and utilization of high-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger variance in the generated Latent Thought distribution more closely approximates the golden truth distribution, we propose a Latent Thought-Augmented Training Frameworkâ€“LTA-Thinker, which improves distributional variance and enhances reasoning performance from two perspectives. First, LTA-Thinker constructs a Latent Thought generation architecture based on a learnable prior. This architecture aims to increase the variance distribution of generated Latent Thought Vectors in order to simplify the overall structure and raise the performance ceiling. Second, LTA-Thinker introduces a distribution-based directional optimization paradigm that jointly constrains both distribution locality and distribution scale. This mechanism improves information efficiency and computational cost through a multi-objective co-training strategy, which combines standard Supervised Fine-Tuning (SFT) loss with two novel losses: Semantic Alignment Loss, which utilizes KL divergence to ensure that the Latent Thought is highly relevant to the semantics of the question; Reasoning Focus Loss, which utilizes a contrastive learning mechanism to guide the model to focus on the most critical reasoning steps. Experiments show that LTA-thinker achieves state-of-the-art (SOTA) performance among various baselines and demonstrates a higher performance ceiling and better scaling effects. </p>
<blockquote>
<p>åœ¨å¤§è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†ä¸­ï¼Œå¯ä»¥é€šè¿‡æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTest-Time Scalingï¼ŒTTSï¼‰è¿›è¡ŒåŠ¨æ€ä¼˜åŒ–ï¼Œä»¥å‡è½»è¿‡åº¦æ€è€ƒã€‚æ¤°å­ï¼ˆCoconutï¼‰ã€è½¯CoTåŠå…¶å˜ç§ç­‰æ–¹æ³•åœ¨è¿ç»­æ½œåœ¨ç©ºé—´æ¨ç†ä¸­æœ‰æ•ˆï¼Œä½†æ ¸å¿ƒç“¶é¢ˆä»åœ¨äºé«˜è´¨é‡æ½œåœ¨æ€ç»´çš„ç”Ÿæˆå’Œåˆ©ç”¨ã€‚SoftCoT++ç†è®ºæŒ‡å‡ºï¼Œç”Ÿæˆçš„æ½œåœ¨æ€ç»´åˆ†å¸ƒå…·æœ‰è¾ƒå¤§çš„æ–¹å·®ï¼Œæ›´æ¥è¿‘çœŸå®åˆ†å¸ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ€ç»´çš„å¢å¼ºè®­ç»ƒæ¡†æ¶â€”â€”LTA-Thinkerï¼Œä»ä¸¤ä¸ªæ–¹é¢æé«˜åˆ†å¸ƒæ–¹å·®å’Œæ¨ç†æ€§èƒ½ã€‚é¦–å…ˆï¼ŒLTA-Thinkeræ„å»ºäº†ä¸€ä¸ªåŸºäºå¯å­¦ä¹ å…ˆéªŒçš„æ½œåœ¨æ€ç»´ç”Ÿæˆæ¶æ„ã€‚è¯¥æ¶æ„æ—¨åœ¨å¢åŠ ç”Ÿæˆæ½œåœ¨æ€ç»´å‘é‡çš„æ–¹å·®åˆ†å¸ƒï¼Œä»¥ç®€åŒ–æ•´ä½“ç»“æ„å¹¶æé«˜æ€§èƒ½ä¸Šé™ã€‚å…¶æ¬¡ï¼ŒLTA-Thinkerå¼•å…¥äº†ä¸€ç§åŸºäºåˆ†å¸ƒçš„æ–¹å‘ä¼˜åŒ–èŒƒå¼ï¼ŒåŒæ—¶çº¦æŸåˆ†å¸ƒå±€éƒ¨æ€§å’Œåˆ†å¸ƒè§„æ¨¡ã€‚è¯¥æœºåˆ¶é€šè¿‡å¤šç›®æ ‡ååŒè®­ç»ƒç­–ç•¥æé«˜äº†ä¿¡æ¯æ•ˆç‡å’Œè®¡ç®—æˆæœ¬ï¼Œç»“åˆæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æŸå¤±å’Œä¸¤ä¸ªæ–°æŸå¤±ï¼šè¯­ä¹‰å¯¹é½æŸå¤±ï¼Œåˆ©ç”¨KLæ•£åº¦ç¡®ä¿æ½œåœ¨æ€ç»´ä¸é—®é¢˜è¯­ä¹‰é«˜åº¦ç›¸å…³ï¼›æ¨ç†ç„¦ç‚¹æŸå¤±ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æœºåˆ¶å¼•å¯¼æ¨¡å‹å…³æ³¨æœ€å…³é”®çš„æ¨ç†æ­¥éª¤ã€‚å®éªŒè¡¨æ˜ï¼ŒLTA-thinkeråœ¨å„ç§åŸºçº¿æ–¹æ³•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ä¸Šé™å’Œæ›´å¥½çš„ç¼©æ”¾æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12875v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†å¯é€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰è¿›è¡ŒåŠ¨æ€ä¼˜åŒ–ï¼Œä»¥å‡è½»è¿‡åº¦æ€è€ƒã€‚Coconutã€SoftCoTåŠå…¶å˜ç§ç­‰æ–¹æ³•åœ¨è¿ç»­æ½œåœ¨ç©ºé—´æ¨ç†ä¸­æœ‰æ•ˆï¼Œä½†æ ¸å¿ƒç“¶é¢ˆä»åœ¨äºé«˜æ•ˆç”Ÿæˆå’Œåˆ©ç”¨é«˜è´¨é‡çš„æ½œåœ¨æ€ç»´ã€‚åŸºäºSoftCoT++ç†è®ºï¼Œæå‡ºä¸€ç§æ½œåœ¨æ€ç»´å¢å¼ºè®­ç»ƒæ¡†æ¶â€”â€”LTA-Thinkerï¼Œä»ä¸¤ä¸ªæ–¹é¢æé«˜åˆ†å¸ƒæ–¹å·®å’Œæ¨ç†æ€§èƒ½ã€‚é¦–å…ˆï¼ŒLTA-Thinkeræ„å»ºäº†ä¸€ä¸ªåŸºäºå¯å­¦ä¹ å…ˆéªŒçš„æ½œåœ¨æ€ç»´ç”Ÿæˆæ¶æ„ï¼Œæ—¨åœ¨å¢åŠ ç”Ÿæˆæ½œåœ¨æ€ç»´å‘é‡çš„æ–¹å·®åˆ†å¸ƒï¼Œä»¥ç®€åŒ–æ•´ä½“ç»“æ„å¹¶æé«˜æ€§èƒ½ä¸Šé™ã€‚å…¶æ¬¡ï¼ŒLTA-Thinkerå¼•å…¥äº†ä¸€ç§åŸºäºåˆ†å¸ƒçš„æ–¹å‘ä¼˜åŒ–èŒƒå¼ï¼Œè”åˆçº¦æŸåˆ†å¸ƒå±€éƒ¨æ€§å’Œåˆ†å¸ƒè§„æ¨¡ï¼Œé€šè¿‡å¤šç›®æ ‡è”åˆè®­ç»ƒç­–ç•¥ï¼Œç»“åˆæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æŸå¤±ä¸ä¸¤ç§æ–°é¢–æŸå¤±ï¼šè¯­ä¹‰å¯¹é½æŸå¤±ï¼Œåˆ©ç”¨KLæ•£åº¦ç¡®ä¿æ½œåœ¨æ€ç»´ä¸é—®é¢˜è¯­ä¹‰é«˜åº¦ç›¸å…³ï¼›æ¨ç†ç„¦ç‚¹æŸå¤±ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æœºåˆ¶å¼•å¯¼æ¨¡å‹å…³æ³¨æœ€å…³é”®æ¨ç†æ­¥éª¤ã€‚å®éªŒè¡¨æ˜ï¼ŒLTA-Thinkeråœ¨å„ç§åŸºçº¿æ–¹æ³•ä¸­å®ç°æœ€ä½³æ€§èƒ½ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ä¸Šé™å’Œæ›´å¥½çš„ç¼©æ”¾æ•ˆæœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†å¯é€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ä¼˜åŒ–ï¼Œä»¥ç¼“è§£è¿‡åº¦æ€è€ƒé—®é¢˜ã€‚</li>
<li>Coconutã€SoftCoTç­‰æ–¹æ³•åœ¨è¿ç»­æ½œåœ¨ç©ºé—´æ¨ç†ä¸­è¡¨ç°æœ‰æ•ˆï¼Œä½†å­˜åœ¨ç”Ÿæˆé«˜è´¨é‡æ½œåœ¨æ€ç»´çš„ç“¶é¢ˆã€‚</li>
<li>LTA-ThinkeråŸºäºSoftCoT++ç†è®ºï¼Œæå‡ºä¸€ä¸ªæ½œåœ¨æ€ç»´å¢å¼ºè®­ç»ƒæ¡†æ¶ã€‚</li>
<li>LTA-Thinkeræ„å»ºäº†ä¸€ä¸ªæ—¨åœ¨å¢åŠ ç”Ÿæˆæ½œåœ¨æ€ç»´å‘é‡æ–¹å·®åˆ†å¸ƒçš„æ¶æ„ã€‚</li>
<li>LTA-Thinkerå¼•å…¥äº†ä¸€ç§è”åˆçº¦æŸåˆ†å¸ƒå±€éƒ¨æ€§å’Œåˆ†å¸ƒè§„æ¨¡çš„ä¼˜åŒ–èŒƒå¼ã€‚</li>
<li>LTA-Thinkeré€šè¿‡å¤šç›®æ ‡è”åˆè®­ç»ƒç­–ç•¥ç»“åˆæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æŸå¤±ä¸ä¸¤ç§æ–°é¢–æŸå¤±ï¼šè¯­ä¹‰å¯¹é½æŸå¤±å’Œæ¨ç†ç„¦ç‚¹æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12875">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12875v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12875v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12875v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Tool-R1-Sample-Efficient-Reinforcement-Learning-for-Agentic-Tool-Use"><a href="#Tool-R1-Sample-Efficient-Reinforcement-Learning-for-Agentic-Tool-Use" class="headerlink" title="Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use"></a>Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use</h2><p><strong>Authors:Yabo Zhang, Yihan Zeng, Qingyun Li, Zhen Hu, Kavin Han, Wangmeng Zuo</strong></p>
<p>Large language models (LLMs) have demonstrated strong capabilities in language understanding and reasoning, yet they remain limited when tackling real-world tasks that require up-to-date knowledge, precise operations, or specialized tool use. To address this, we propose Tool-R1, a reinforcement learning framework that enables LLMs to perform general, compositional, and multi-step tool use by generating executable Python code. Tool-R1 supports integration of user-defined tools and standard libraries, with variable sharing across steps to construct coherent workflows. An outcome-based reward function, combining LLM-based answer judgment and code execution success, guides policy optimization. To improve training efficiency, we maintain a dynamic sample queue to cache and reuse high-quality trajectories, reducing the overhead of costly online sampling. Experiments on the GAIA benchmark show that Tool-R1 substantially improves both accuracy and robustness, achieving about 10% gain over strong baselines, with larger improvements on complex multi-step tasks. These results highlight the potential of Tool-R1 for enabling reliable and efficient tool-augmented reasoning in real-world applications. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/YBYBZhang/Tool-R1">https://github.com/YBYBZhang/Tool-R1</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†éœ€è¦æœ€æ–°çŸ¥è¯†ã€ç²¾ç¡®æ“ä½œæˆ–ä¸“ä¸šå·¥å…·ä½¿ç”¨çš„ç°å®ä¸–ç•Œä»»åŠ¡æ—¶ä»å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Tool-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡ç”Ÿæˆå¯æ‰§è¡ŒPythonä»£ç ï¼Œä½¿LLMæ‰§è¡Œé€šç”¨ã€ç»„åˆå’Œå¤šæ­¥éª¤çš„å·¥å…·ä½¿ç”¨ã€‚Tool-R1æ”¯æŒç”¨æˆ·å®šä¹‰çš„å·¥å…·å’Œæ ‡å‡†åº“çš„é›†æˆï¼Œé€šè¿‡è·¨æ­¥éª¤çš„å˜é‡å…±äº«æ¥æ„å»ºè¿è´¯çš„å·¥ä½œæµç¨‹ã€‚ä¸€ä¸ªä»¥ç»“æœä¸ºåŸºç¡€çš„å¥–åŠ±å‡½æ•°ï¼Œç»“åˆLLMçš„ç­”æ¡ˆåˆ¤æ–­å’Œä»£ç æ‰§è¡ŒæˆåŠŸä¸å¦ï¼Œå¼•å¯¼ç­–ç•¥ä¼˜åŒ–ã€‚ä¸ºäº†æé«˜è®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬ç»´æŠ¤äº†ä¸€ä¸ªåŠ¨æ€æ ·æœ¬é˜Ÿåˆ—æ¥ç¼“å­˜å’Œé‡å¤ä½¿ç”¨é«˜è´¨é‡çš„è½¨è¿¹ï¼Œå‡å°‘æ˜‚è´µçš„åœ¨çº¿é‡‡æ ·çš„å¼€é”€ã€‚åœ¨GAIAåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTool-R1åœ¨å‡†ç¡®ç‡å’Œç¨³å¥æ€§æ–¹é¢æœ‰äº†æ˜¾è‘—æé«˜ï¼Œæ¯”å¼ºåŸºçº¿é«˜å‡ºçº¦10%çš„å¢ç›Šï¼Œåœ¨å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡ä¸Šå–å¾—äº†æ›´å¤§çš„æ”¹è¿›ã€‚è¿™äº›ç»“æœçªå‡ºäº†Tool-R1åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­å®ç°å¯é å’Œé«˜æ•ˆå·¥å…·å¢å¼ºæ¨ç†çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/YBYBZhang/Tool-R1%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/YBYBZhang/Tool-R1ä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12867v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†éœ€è¦æœ€æ–°çŸ¥è¯†ã€ç²¾ç¡®æ“ä½œæˆ–ä¸“ä¸šå·¥å…·ä½¿ç”¨çš„ç°å®ä¸–ç•Œä»»åŠ¡æ—¶ä»å­˜åœ¨å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Tool-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆå¯æ‰§è¡ŒPythonä»£ç ï¼Œä½¿LLMsèƒ½å¤Ÿæ‰§è¡Œé€šç”¨ã€ç»„åˆå’Œå¤šæ­¥éª¤çš„å·¥å…·ä½¿ç”¨ã€‚Tool-R1æ”¯æŒç”¨æˆ·å®šä¹‰å·¥å…·å’Œæ ‡å‡†åº“çš„é›†æˆï¼Œè·¨æ­¥éª¤è¿›è¡Œå˜é‡å…±äº«ä»¥æ„å»ºè¿è´¯çš„å·¥ä½œæµã€‚é‡‡ç”¨åŸºäºç»“æœçš„å¥–åŠ±å‡½æ•°ï¼Œç»“åˆLLMçš„ç­”æ¡ˆåˆ¤æ–­å’Œä»£ç æ‰§è¡ŒæˆåŠŸä¸å¦ï¼Œå¼•å¯¼ç­–ç•¥ä¼˜åŒ–ã€‚ä¸ºæé«˜è®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬ç»´æŠ¤äº†ä¸€ä¸ªåŠ¨æ€æ ·æœ¬é˜Ÿåˆ—æ¥ç¼“å­˜å’Œé‡ç”¨é«˜è´¨é‡çš„è½¨è¿¹ï¼Œå‡å°‘æ˜‚è´µçš„åœ¨çº¿é‡‡æ ·å¼€é”€ã€‚åœ¨GAIAåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTool-R1åœ¨å‡†ç¡®ç‡å’Œé²æ£’æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æé«˜ï¼Œè¾ƒå¼ºåŸºçº¿çº¦æé«˜10%ï¼Œåœ¨å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡ä¸Šè·å¾—æ›´å¤§æå‡ã€‚è¿™äº›ç»“æœçªæ˜¾äº†Tool-R1åœ¨ç°å®ä¸–ç•Œå·¥å…·è¾…åŠ©æ¨ç†ä¸­çš„å¯é å’Œé«˜æ•ˆæ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç°å®ä¸–ç•Œä»»åŠ¡ä¸­ä»æœ‰å±€é™æ€§ï¼Œéœ€è¦å·¥å…·ä½¿ç”¨çš„èƒ½åŠ›ã€‚</li>
<li>Tool-R1æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿LLMsèƒ½å¤Ÿæ‰§è¡Œé€šç”¨ã€ç»„åˆå’Œå¤šæ­¥éª¤çš„å·¥å…·ä½¿ç”¨ï¼Œé€šè¿‡ç”Ÿæˆå¯æ‰§è¡ŒPythonä»£ç ã€‚</li>
<li>Tool-R1æ”¯æŒç”¨æˆ·å®šä¹‰å·¥å…·å’Œæ ‡å‡†åº“çš„é›†æˆï¼Œå¹¶å…è®¸è·¨æ­¥éª¤å˜é‡å…±äº«ã€‚</li>
<li>åŸºäºç»“æœçš„å¥–åŠ±å‡½æ•°ç»“åˆLLMçš„ç­”æ¡ˆåˆ¤æ–­å’Œä»£ç æ‰§è¡ŒæˆåŠŸä¸å¦ï¼Œå¼•å¯¼ç­–ç•¥ä¼˜åŒ–ã€‚</li>
<li>ä¸ºæé«˜è®­ç»ƒæ•ˆç‡ï¼Œé‡‡ç”¨äº†åŠ¨æ€æ ·æœ¬é˜Ÿåˆ—æ¥ç¼“å­˜å’Œé‡ç”¨é«˜è´¨é‡çš„è½¨è¿¹ã€‚</li>
<li>åœ¨GAIAåŸºå‡†æµ‹è¯•ä¸Šï¼ŒTool-R1è¾ƒåŸºçº¿æœ‰æ˜¾è‘—æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å¤šæ­¥éª¤ä»»åŠ¡ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12867">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12867v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12867v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12867v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GRATE-a-Graph-transformer-based-deep-Reinforcement-learning-Approach-for-Time-efficient-autonomous-robot-Exploration"><a href="#GRATE-a-Graph-transformer-based-deep-Reinforcement-learning-Approach-for-Time-efficient-autonomous-robot-Exploration" class="headerlink" title="GRATE: a Graph transformer-based deep Reinforcement learning Approach   for Time-efficient autonomous robot Exploration"></a>GRATE: a Graph transformer-based deep Reinforcement learning Approach   for Time-efficient autonomous robot Exploration</h2><p><strong>Authors:Haozhan Ni, Jingsong Liang, Chenyu He, Yuhong Cao, Guillaume Sartoretti</strong></p>
<p>Autonomous robot exploration (ARE) is the process of a robot autonomously navigating and mapping an unknown environment. Recent Reinforcement Learning (RL)-based approaches typically formulate ARE as a sequential decision-making problem defined on a collision-free informative graph. However, these methods often demonstrate limited reasoning ability over graph-structured data. Moreover, due to the insufficient consideration of robot motion, the resulting RL policies are generally optimized to minimize travel distance, while neglecting time efficiency. To overcome these limitations, we propose GRATE, a Deep Reinforcement Learning (DRL)-based approach that leverages a Graph Transformer to effectively capture both local structure patterns and global contextual dependencies of the informative graph, thereby enhancing the modelâ€™s reasoning capability across the entire environment. In addition, we deploy a Kalman filter to smooth the waypoint outputs, ensuring that the resulting path is kinodynamically feasible for the robot to follow. Experimental results demonstrate that our method exhibits better exploration efficiency (up to 21.5% in distance and 21.3% in time to complete exploration) than state-of-the-art conventional and learning-based baselines in various simulation benchmarks. We also validate our planner in real-world scenarios. </p>
<blockquote>
<p>è‡ªä¸»æœºå™¨äººæ¢ç´¢ï¼ˆAREï¼‰æ˜¯æœºå™¨äººè‡ªä¸»å¯¼èˆªå’Œæ˜ å°„æœªçŸ¥ç¯å¢ƒçš„è¿‡ç¨‹ã€‚æœ€è¿‘çš„åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•é€šå¸¸å°†AREåˆ¶å®šä¸ºåœ¨å…ç¢°æ’ä¿¡æ¯å›¾ä¸Šå®šä¹‰çš„é¡ºåºå†³ç­–é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸åœ¨å›¾ç»“æ„æ•°æ®çš„æ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°æœ‰é™ã€‚æ­¤å¤–ï¼Œç”±äºæœºå™¨äººè¿åŠ¨è€ƒè™‘ä¸è¶³ï¼Œæ‰€å¾—çš„RLç­–ç•¥ä¸€èˆ¬ä¼˜åŒ–ä¸ºæœ€å°åŒ–æ—…è¡Œè·ç¦»ï¼Œè€Œå¿½ç•¥äº†æ—¶é—´æ•ˆç‡ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†GRATEï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å›¾è½¬æ¢å™¨æœ‰æ•ˆåœ°æ•è·ä¿¡æ¯å›¾çš„å±€éƒ¨ç»“æ„æ¨¡å¼å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œä»è€Œæé«˜äº†æ•´ä¸ªç¯å¢ƒçš„æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å¡å°”æ›¼æ»¤æ³¢å™¨å¯¹èˆªè¿¹ç‚¹è¾“å‡ºè¿›è¡Œå¹³æ»‘å¤„ç†ï¼Œç¡®ä¿ç”Ÿæˆçš„è·¯å¾„å¯¹äºæœºå™¨äººæ¥è¯´æ˜¯åŠ¨åŠ›å­¦å¯è¡Œçš„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§ä»¿çœŸåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æ¢ç´¢æ•ˆç‡ï¼ˆå®Œæˆæ¢ç´¢çš„è·ç¦»å’Œæ—¶é—´åˆ†åˆ«æé«˜äº†21.5%å’Œ21.3%ï¼‰ï¼Œå¹¶ä¸”æˆ‘ä»¬è¿˜éªŒè¯äº†æˆ‘ä»¬çš„è§„åˆ’å™¨åœ¨ç°å®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12863v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªä¸»æœºå™¨äººæ¢ç´¢ï¼ˆAREï¼‰æ˜¯æœºå™¨äººè‡ªä¸»å¯¼èˆªå’Œæ˜ å°„æœªçŸ¥ç¯å¢ƒçš„è¿‡ç¨‹ã€‚é’ˆå¯¹ç°æœ‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•åœ¨å¤„ç†å›¾å½¢ç»“æ„åŒ–æ•°æ®æ—¶çš„æ¨ç†èƒ½åŠ›æœ‰é™ä»¥åŠå¿½è§†æœºå™¨äººè¿åŠ¨è€ƒè™‘çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„æ–¹æ³•GRATEï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å›¾è½¬æ¢å™¨æœ‰æ•ˆåœ°æ•è·ä¿¡æ¯å›¾å½¢çš„å±€éƒ¨ç»“æ„æ¨¡å¼å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨æ•´ä¸ªç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬é‡‡ç”¨å¡å°”æ›¼æ»¤æ³¢å™¨å¯¹èˆªç‚¹è¾“å‡ºè¿›è¡Œå¹³æ»‘å¤„ç†ï¼Œç¡®ä¿æœºå™¨äººèƒ½å¤Ÿè·ŸéšåŠ¨åŠ›å­¦å¯è¡Œçš„è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§ä»¿çœŸåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¢ç´¢æ•ˆç‡ä¼˜äºç°æœ‰ä¼ ç»Ÿå’ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œè·ç¦»å’Œæ—¶é—´åˆ†åˆ«æé«˜äº†æœ€å¤šè¾¾21.5%å’Œ21.3%ã€‚æˆ‘ä»¬è¿˜åœ¨çœŸå®åœºæ™¯ä¸­å¯¹è§„åˆ’å™¨è¿›è¡Œäº†éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªä¸»æœºå™¨äººæ¢ç´¢ï¼ˆAREï¼‰æ¶‰åŠæœºå™¨äººè‡ªä¸»å¯¼èˆªå’Œæ˜ å°„æœªçŸ¥ç¯å¢ƒã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å›¾å½¢ç»“æ„åŒ–æ•°æ®æ—¶å­˜åœ¨æ¨ç†èƒ½åŠ›æœ‰é™çš„ç¼ºé™·ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„æ–¹æ³•GRATEï¼Œåˆ©ç”¨å›¾è½¬æ¢å™¨æ•è·ä¿¡æ¯å›¾å½¢çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚</li>
<li>GRATEæ–¹æ³•å¢å¼ºäº†æ¨¡å‹åœ¨æ•´ä¸ªç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨å¡å°”æ›¼æ»¤æ³¢å™¨å¹³æ»‘å¤„ç†èˆªç‚¹è¾“å‡ºï¼Œç¡®ä¿æœºå™¨äººè¿åŠ¨çš„åŠ¨åŠ›å­¦å¯è¡Œæ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒGRATEæ–¹æ³•åœ¨ä»¿çœŸæµ‹è¯•ä¸­ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæé«˜äº†æ¢ç´¢æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12863">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12863v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12863v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12863v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12863v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="EvoEmpirBench-Dynamic-Spatial-Reasoning-with-Agent-ExpVer"><a href="#EvoEmpirBench-Dynamic-Spatial-Reasoning-with-Agent-ExpVer" class="headerlink" title="EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer"></a>EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer</h2><p><strong>Authors:Pukun Zhao, Longxiang Wang, Miaowei Wang, Chen Chen, Fanqing Zhou, Haojian Huang</strong></p>
<p>Most existing spatial reasoning benchmarks focus on static or globally observable environments, failing to capture the challenges of long-horizon reasoning and memory utilization under partial observability and dynamic changes. We introduce two dynamic spatial benchmarks, locally observable maze navigation and match-2 elimination that systematically evaluate modelsâ€™ abilities in spatial understanding and adaptive planning when local perception, environment feedback, and global objectives are tightly coupled. Each action triggers structural changes in the environment, requiring continuous update of cognition and strategy. We further propose a subjective experience-based memory mechanism for cross-task experience transfer and validation. Experiments show that our benchmarks reveal key limitations of mainstream models in dynamic spatial reasoning and long-term memory, providing a comprehensive platform for future methodological advances. Our code and data are available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/EvoEmpirBench-143C/">https://anonymous.4open.science/r/EvoEmpirBench-143C/</a>. </p>
<blockquote>
<p>ç°æœ‰çš„å¤§éƒ¨åˆ†ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨é™æ€æˆ–å…¨å±€å¯è§‚å¯Ÿçš„ç¯å¢ƒï¼Œæœªèƒ½æ•æ‰åˆ°éƒ¨åˆ†å¯è§‚å¯Ÿæ€§å’ŒåŠ¨æ€å˜åŒ–ä¸‹é•¿å‘¨æœŸæ¨ç†å’Œå†…å­˜ä½¿ç”¨çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªåŠ¨æ€ç©ºé—´åŸºå‡†æµ‹è¯•ï¼Œå³å±€éƒ¨å¯è§‚å¯Ÿçš„è¿·å®«å¯¼èˆªå’ŒåŒ¹é…-2æ¶ˆé™¤ï¼Œä»¥ç³»ç»Ÿåœ°è¯„ä¼°æ¨¡å‹åœ¨å±€éƒ¨æ„ŸçŸ¥ã€ç¯å¢ƒåé¦ˆå’Œå…¨å±€ç›®æ ‡ç´§å¯†ç»“åˆæ—¶ï¼Œåœ¨ç©ºé—´ç†è§£å’Œè‡ªé€‚åº”è§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚æ¯ä¸ªåŠ¨ä½œéƒ½ä¼šè§¦å‘ç¯å¢ƒçš„å˜åŒ–ï¼Œéœ€è¦ä¸æ–­æ›´æ–°è®¤çŸ¥ç­–ç•¥ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºä¸»è§‚ç»éªŒçš„è®°å¿†æœºåˆ¶ï¼Œç”¨äºè·¨ä»»åŠ¡ç»éªŒè¿ç§»å’ŒéªŒè¯ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ­ç¤ºäº†ä¸»æµæ¨¡å‹åœ¨åŠ¨æ€ç©ºé—´æ¨ç†å’Œé•¿æœŸè®°å¿†æ–¹é¢çš„å…³é”®å±€é™æ€§ï¼Œä¸ºæœªæ¥çš„æ–¹æ³•è®ºè¿›æ­¥æä¾›äº†ä¸€ä¸ªå…¨é¢çš„å¹³å°ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨[<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/EvoEmpirBench-143C/]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/EvoEmpirBench-143C/]ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12718v1">PDF</a> Ongoing Work, 29 pages, 3 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ç°æœ‰çš„ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨é™æ€æˆ–å…¨å±€å¯è§‚å¯Ÿçš„ç¯å¢ƒä¸Šï¼Œå¿½è§†äº†å±€éƒ¨å¯è§‚å¯Ÿæ€§ã€åŠ¨æ€å˜åŒ–ä¸‹çš„é•¿å‘¨æœŸæ¨ç†å’Œè®°å¿†åˆ©ç”¨çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« å¼•å…¥äº†ä¸¤ä¸ªåŠ¨æ€ç©ºé—´åŸºå‡†æµ‹è¯•ï¼šå±€éƒ¨å¯è§‚å¯Ÿçš„è¿·å®«å¯¼èˆªå’ŒåŒ¹é…-2æ¶ˆé™¤ï¼Œä»¥ç³»ç»Ÿåœ°è¯„ä¼°æ¨¡å‹åœ¨å±€éƒ¨æ„ŸçŸ¥ã€ç¯å¢ƒåé¦ˆå’Œå…¨å±€ç›®æ ‡ç´§å¯†è€¦åˆæƒ…å†µä¸‹çš„ç©ºé—´ç†è§£å’Œè‡ªé€‚åº”è§„åˆ’èƒ½åŠ›ã€‚æ¯ä¸ªåŠ¨ä½œéƒ½ä¼šå¼•èµ·ç¯å¢ƒçš„ç»“æ„æ€§å˜åŒ–ï¼Œéœ€è¦ä¸æ–­æ›´æ–°è®¤çŸ¥å’Œè°ƒæ•´ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æå‡ºäº†ä¸€ç§åŸºäºä¸»è§‚ç»éªŒçš„è®°å¿†æœºåˆ¶ï¼Œç”¨äºè·¨ä»»åŠ¡ç»éªŒè¿ç§»å’ŒéªŒè¯ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™äº›åŸºå‡†æµ‹è¯•æ­ç¤ºäº†ä¸»æµæ¨¡å‹åœ¨åŠ¨æ€ç©ºé—´æ¨ç†å’Œé•¿æœŸè®°å¿†æ–¹é¢çš„å…³é”®å±€é™æ€§ï¼Œä¸ºæœªæ¥çš„æ–¹æ³•è¿›æ­¥æä¾›äº†ä¸€ä¸ªç»¼åˆå¹³å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨é™æ€æˆ–å…¨å±€å¯è§‚å¯Ÿçš„ç¯å¢ƒä¸Šï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¼•å…¥ä¸¤ä¸ªåŠ¨æ€ç©ºé—´åŸºå‡†æµ‹è¯•ï¼šå±€éƒ¨å¯è§‚å¯Ÿçš„è¿·å®«å¯¼èˆªå’ŒåŒ¹é…-2æ¶ˆé™¤ã€‚</li>
<li>è¿™äº›åŸºå‡†æµ‹è¯•è¦æ±‚æ¨¡å‹åœ¨å±€éƒ¨æ„ŸçŸ¥ã€ç¯å¢ƒåé¦ˆå’Œå…¨å±€ç›®æ ‡çš„äº¤äº’ä¸­å±•ç¤ºç©ºé—´ç†è§£å’Œè‡ªé€‚åº”è§„åˆ’èƒ½åŠ›ã€‚</li>
<li>ç¯å¢ƒç»“æ„ä¼šå› åŠ¨ä½œè€Œå‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦æ¨¡å‹æŒç»­æ›´æ–°è®¤çŸ¥å’Œè°ƒæ•´ç­–ç•¥ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºä¸»è§‚ç»éªŒçš„è®°å¿†æœºåˆ¶ï¼Œç”¨äºè·¨ä»»åŠ¡ç»éªŒè¿ç§»å’ŒéªŒè¯ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸»æµæ¨¡å‹åœ¨åŠ¨æ€ç©ºé—´æ¨ç†å’Œé•¿æœŸè®°å¿†æ–¹é¢å­˜åœ¨å…³é”®å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12718v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12718v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12718v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12718v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12718v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ActiveVLN-Towards-Active-Exploration-via-Multi-Turn-RL-in-Vision-and-Language-Navigation"><a href="#ActiveVLN-Towards-Active-Exploration-via-Multi-Turn-RL-in-Vision-and-Language-Navigation" class="headerlink" title="ActiveVLN: Towards Active Exploration via Multi-Turn RL in   Vision-and-Language Navigation"></a>ActiveVLN: Towards Active Exploration via Multi-Turn RL in   Vision-and-Language Navigation</h2><p><strong>Authors:Zekai Zhang, Weiye Zhu, Hewei Pan, Xiangchen Wang, Rongtao Xu, Xing Sun, Feng Zheng</strong></p>
<p>The Vision-and-Language Navigation (VLN) task requires an agent to follow natural language instructions and navigate through complex environments. Existing MLLM-based VLN methods primarily rely on imitation learning (IL) and often use DAgger for post-training to mitigate covariate shift. While effective, these approaches incur substantial data collection and training costs. Reinforcement learning (RL) offers a promising alternative. However, prior VLN RL methods lack dynamic interaction with the environment and depend on expert trajectories for reward shaping, rather than engaging in open-ended active exploration. This restricts the agentâ€™s ability to discover diverse and plausible navigation routes. To address these limitations, we propose ActiveVLN, a VLN framework that explicitly enables active exploration through multi-turn RL. In the first stage, a small fraction of expert trajectories is used for IL to bootstrap the agent. In the second stage, the agent iteratively predicts and executes actions, automatically collects diverse trajectories, and optimizes multiple rollouts via the GRPO objective. To further improve RL efficiency, we introduce a dynamic early-stopping strategy to prune long-tail or likely failed trajectories, along with additional engineering optimizations. Experiments show that ActiveVLN achieves the largest performance gains over IL baselines compared to both DAgger-based and prior RL-based post-training methods, while reaching competitive performance with state-of-the-art approaches despite using a smaller model. Code and data will be released soon. </p>
<blockquote>
<p>è§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ä»»åŠ¡è¦æ±‚æ™ºèƒ½ä½“éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤å¹¶é€šè¿‡å¤æ‚ç¯å¢ƒè¿›è¡Œå¯¼èˆªã€‚ç°æœ‰çš„åŸºäºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„VLNæ–¹æ³•ä¸»è¦ä¾èµ–äºæ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰ï¼Œå¹¶ä¸”ç»å¸¸ä½¿ç”¨DAggerè¿›è¡Œåè®­ç»ƒä»¥ç¼“è§£åå˜é‡åç§»é—®é¢˜ã€‚è™½ç„¶è¿™äº›æ–¹æ³•æœ‰æ•ˆï¼Œä½†å®ƒä»¬ä¼šäº§ç”Ÿå¤§é‡çš„æ•°æ®æ”¶é›†å’ŒåŸ¹è®­æˆæœ¬ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå…ˆå‰çš„VLN RLæ–¹æ³•ç¼ºä¹ä¸ç¯å¢ƒçš„åŠ¨æ€äº¤äº’ï¼Œå¹¶ä¸”ä¾èµ–äºä¸“å®¶è½¨è¿¹è¿›è¡Œå¥–åŠ±å¡‘é€ ï¼Œè€Œä¸æ˜¯è¿›è¡Œå¼€æ”¾å¼çš„ä¸»åŠ¨æ¢ç´¢ã€‚è¿™é™åˆ¶äº†æ™ºèƒ½ä½“å‘ç°å¤šæ ·åŒ–å’Œåˆç†çš„å¯¼èˆªè·¯çº¿çš„èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ActiveVLNï¼Œè¿™æ˜¯ä¸€ä¸ªVLNæ¡†æ¶ï¼Œé€šè¿‡å¤šå›åˆRLæ˜ç¡®å®ç°ä¸»åŠ¨æ¢ç´¢ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä¸€å°éƒ¨åˆ†ä¸“å®¶è½¨è¿¹ç”¨äºILæ¥å¼•å¯¼æ™ºèƒ½ä½“ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæ™ºèƒ½ä½“é¢„æµ‹å¹¶æ‰§è¡ŒåŠ¨ä½œï¼Œè‡ªåŠ¨æ”¶é›†å„ç§è½¨è¿¹ï¼Œå¹¶é€šè¿‡GRPOç›®æ ‡ä¼˜åŒ–å¤šæ¬¡è¿è¡Œã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜RLçš„æ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€æ—©æœŸåœæ­¢ç­–ç•¥æ¥å‰”é™¤é•¿å°¾æˆ–å¯èƒ½å¤±è´¥çš„è½¨è¿¹ï¼Œä»¥åŠå…¶ä»–å·¥ç¨‹ä¼˜åŒ–æªæ–½ã€‚å®éªŒè¡¨æ˜ï¼ŒActiveVLNåœ¨ILåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å¤§çš„æ€§èƒ½æå‡ï¼Œè¶…è¿‡äº†åŸºäºDAggerå’Œå…ˆå‰çš„RLåå¤„ç†æ–¹æ³•ï¼Œå°½ç®¡ä½¿ç”¨äº†è¾ƒå°çš„æ¨¡å‹ï¼Œä½†å…¶æ€§èƒ½ä¸æœ€å…ˆè¿›çš„æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚ä»£ç å’Œæ•°æ®å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12618v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ä»»åŠ¡è¦æ±‚æ™ºèƒ½ä½“æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œå¯¼èˆªã€‚ç°æœ‰åŸºäºå¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ–¹æ³•ä¸»è¦ä¾èµ–æ¨¡ä»¿å­¦ä¹ ï¼Œå¹¶é‡‡ç”¨DAggerè¿›è¡Œåè®­ç»ƒä»¥ç¼“è§£åå˜é‡åç§»é—®é¢˜ã€‚å°½ç®¡æœ‰æ•ˆï¼Œè¿™äº›æ–¹æ³•æ¶‰åŠå¤§é‡çš„æ•°æ®æ”¶é›†å’ŒåŸ¹è®­æˆæœ¬ã€‚å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å…ˆå‰çš„VLNå¼ºåŒ–å­¦ä¹ æ–¹æ³•ç¼ºä¹ä¸ç¯å¢ƒçš„åŠ¨æ€äº¤äº’ï¼Œå¹¶ä¾èµ–ä¸“å®¶è½¨è¿¹è¿›è¡Œå¥–åŠ±å¡‘å½¢ï¼Œè€Œéè¿›è¡Œå¼€æ”¾å¼ä¸»åŠ¨æ¢ç´¢ã€‚è¿™é™åˆ¶äº†æ™ºèƒ½ä½“å‘ç°å¤šæ ·æ€§å’Œå¯è¡Œçš„å¯¼èˆªè·¯çº¿çš„èƒ½åŠ›ã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ActiveVLNæ¡†æ¶ï¼Œå®ƒæ˜ç¡®æ”¯æŒé€šè¿‡å¤šè½®å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¸»åŠ¨æ¢ç´¢ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€å°éƒ¨åˆ†ä¸“å®¶è½¨è¿¹è¿›è¡Œæ¨¡ä»¿å­¦ä¹ æ¥å¼•å¯¼æ™ºèƒ½ä½“ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæ™ºèƒ½ä½“é¢„æµ‹å¹¶æ‰§è¡ŒåŠ¨ä½œï¼Œè‡ªåŠ¨æ”¶é›†å„ç§è½¨è¿¹ï¼Œå¹¶é€šè¿‡GRPOç›®æ ‡ä¼˜åŒ–å¤šæ¬¡è¿è¡Œã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€æ—©æœŸåœæ­¢ç­–ç•¥æ¥åˆ é™¤é•¿å°¾æˆ–å¯èƒ½å¤±è´¥çš„è½¨è¿¹ï¼Œä»¥åŠå…¶ä»–å·¥ç¨‹ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒActiveVLNç›¸è¾ƒäºåŸºäºDAggerçš„æ–¹æ³•å’Œå…ˆå‰çš„å¼ºåŒ–å­¦ä¹ åå¤„ç†æ–¹æ³•å–å¾—äº†æœ€å¤§çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨è¾ƒå°çš„æ¨¡å‹ä¸‹è¾¾åˆ°äº†ä¸æœ€æ–°æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚ä»£ç å’Œæ•°æ®å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>VLNä»»åŠ¡è¦æ±‚æ™ºèƒ½ä½“æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨å¤æ‚ç¯å¢ƒä¸­å¯¼èˆªã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ¨¡ä»¿å­¦ä¹ å’ŒDAggeråè®­ç»ƒã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä¸ºVLNä»»åŠ¡æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>å…ˆå‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç¼ºä¹ä¸ç¯å¢ƒçš„åŠ¨æ€äº¤äº’å’Œå¼€æ”¾å¼ä¸»åŠ¨æ¢ç´¢ã€‚</li>
<li>ActiveVLNæ¡†æ¶æ”¯æŒé€šè¿‡å¤šè½®å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¸»åŠ¨æ¢ç´¢ï¼Œç»“åˆæ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>ActiveVLNé€šè¿‡GRPOç›®æ ‡å’ŒåŠ¨æ€æ—©æœŸåœæ­¢ç­–ç•¥ä¼˜åŒ–è½¨è¿¹æ”¶é›†å’Œä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12618">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12618v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12618v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12618v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12618v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12618v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving"><a href="#EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving" class="headerlink" title="EconProver: Towards More Economical Test-Time Scaling for Automated   Theorem Proving"></a>EconProver: Towards More Economical Test-Time Scaling for Automated   Theorem Proving</h2><p><strong>Authors:Mukai Li, Linfeng Song, Zhenwen Liang, Jiahao Xu, Shansan Gong, Qi Liu, Haitao Mi, Dong Yu</strong></p>
<p>Large Language Models (LLMs) have recently advanced the field of Automated Theorem Proving (ATP), attaining substantial performance gains through widely adopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT) reasoning and increased sampling passes. However, they both introduce significant computational overhead for inference. Moreover, existing cost analyses typically regulate only the number of sampling passes, while neglecting the substantial disparities in sampling costs introduced by different scaling strategies. In this paper, we systematically compare the efficiency of different test-time scaling strategies for ATP models and demonstrate the inefficiency of the current state-of-the-art (SOTA) open-source approaches. We then investigate approaches to significantly reduce token usage and sample passes while maintaining the original performance. Specifically, we propose two complementary methods that can be integrated into a unified EconRL pipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching mechanism designed to mitigate unnecessary token consumption, and (2) Diverse parallel-scaled reinforcement learning (RL) with trainable prefixes to enhance pass rates under constrained sampling passes. Experiments on miniF2F and ProofNet demonstrate that our EconProver achieves comparable performance to baseline methods with only 12% of the computational cost. This work provides actionable insights for deploying lightweight ATP models without sacrificing performance. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘æ¨åŠ¨äº†è‡ªåŠ¨åŒ–å®šç†è¯æ˜ï¼ˆATPï¼‰é¢†åŸŸçš„å‘å±•ï¼Œé€šè¿‡å¹¿æ³›é‡‡ç”¨çš„æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åå°„å¼æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å’Œå¢åŠ é‡‡æ ·é€šé“ã€‚ç„¶è€Œï¼Œå®ƒä»¬éƒ½ä¸ºæ¨ç†å¼•å…¥äº†é‡å¤§çš„è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„æˆæœ¬åˆ†æé€šå¸¸åªæ§åˆ¶é‡‡æ ·é€šé“çš„æ•°é‡ï¼Œè€Œå¿½è§†ä¸åŒç¼©æ”¾ç­–ç•¥å¸¦æ¥çš„é‡‡æ ·æˆæœ¬å·®å¼‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¯”è¾ƒäº†ATPæ¨¡å‹çš„æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥çš„æ•ˆç‡ï¼Œå¹¶å±•ç¤ºäº†å½“å‰æœ€æ–°å¼€æºæ–¹æ³•çš„ä¸é«˜æ•ˆæ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬ç ”ç©¶äº†å‡å°‘ç¬¦å·ä½¿ç”¨å’Œæ ·æœ¬é€šé“çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒåŸå§‹æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§å¯ä»¥é›†æˆåˆ°ç»Ÿä¸€çš„EconRLç®¡é“ä»¥è·å–é¢å¤–æ”¶ç›Šçš„æ–¹æ³•ï¼šä¸€æ˜¯åŠ¨æ€æ€ç»´é“¾ï¼ˆCoTï¼‰åˆ‡æ¢æœºåˆ¶ï¼Œæ—¨åœ¨å‡å°‘ä¸å¿…è¦çš„ç¬¦å·æ¶ˆè€—ï¼›äºŒæ˜¯å…·æœ‰å¯è®­ç»ƒå‰ç¼€çš„å¤šæ ·åŒ–å¹¶è¡Œç¼©æ”¾å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»¥æé«˜å—é™é‡‡æ ·é€šé“ä¸‹çš„é€šè¿‡ç‡ã€‚åœ¨miniF2Få’ŒProofNetä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„EconProveråœ¨ä»…è®¡ç®—æˆæœ¬çš„12%çš„æƒ…å†µä¸‹å®ç°äº†ä¸åŸºå‡†æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œä¸ºéƒ¨ç½²ä¸ç‰ºç‰²æ€§èƒ½çš„è½»é‡çº§ATPæ¨¡å‹æä¾›äº†åˆ‡å®å¯è¡Œçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12603v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨å®šç†è¯æ˜é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä½†æµ‹è¯•æ—¶çš„ç¼©æ”¾ç­–ç•¥å¦‚åå°„å¼æ€ç»´é“¾å’Œå¢åŠ é‡‡æ ·æ¬¡æ•°ç­‰å¼•å…¥äº†å¤§é‡è®¡ç®—å¼€é”€ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ¯”è¾ƒäº†ä¸åŒæµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥çš„æ•ˆç‡ï¼Œå¹¶æ¢è®¨äº†é™ä½æ ‡è®°ä½¿ç”¨é‡å’Œé‡‡æ ·æ¬¡æ•°çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒåŸå§‹æ€§èƒ½ã€‚é€šè¿‡åŠ¨æ€æ€ç»´é“¾åˆ‡æ¢å’Œå¤šæ ·åŒ–å¹¶è¡Œå¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„EconProverç®¡é“ï¼Œå®ç°äº†è½»é‡çº§è‡ªåŠ¨å®šç†è¯æ˜æ¨¡å‹çš„é«˜æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒEconProverä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè®¡ç®—æˆæœ¬é™ä½äº†ä»…12%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨å®šç†è¯æ˜é¢†åŸŸé€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥å–å¾—äº†æ€§èƒ½æå‡ã€‚</li>
<li>ç°æœ‰æˆæœ¬åˆ†æé€šå¸¸åªå…³æ³¨é‡‡æ ·æ¬¡æ•°ï¼Œå¿½ç•¥äº†ä¸åŒç¼©æ”¾ç­–ç•¥å¸¦æ¥çš„é‡‡æ ·æˆæœ¬å·®å¼‚ã€‚</li>
<li>æœ¬æ–‡ç³»ç»Ÿåœ°æ¯”è¾ƒäº†ä¸åŒæµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥çš„æ•ˆç‡ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰å…ˆè¿›æ–¹æ³•çš„è®¡ç®—æ•ˆç‡é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„EconProverç®¡é“æ¥é™ä½æ ‡è®°ä½¿ç”¨é‡å’Œé‡‡æ ·æ¬¡æ•°ï¼Œç»´æŒåŸå§‹æ€§èƒ½ã€‚</li>
<li>é€šè¿‡åŠ¨æ€æ€ç»´é“¾åˆ‡æ¢æœºåˆ¶ç¼“è§£ä¸å¿…è¦çš„æ ‡è®°æ¶ˆè€—ã€‚</li>
<li>é€šè¿‡å¤šæ ·åŒ–å¹¶è¡Œå¼ºåŒ–å­¦ä¹ å¢å¼ºåœ¨æœ‰é™çš„é‡‡æ ·æ¬¡æ•°ä¸‹çš„é€šè¿‡ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12603">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12603v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12603v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12603v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Evaluating-Robustness-of-Vision-Language-Models-Under-Noisy-Conditions"><a href="#Evaluating-Robustness-of-Vision-Language-Models-Under-Noisy-Conditions" class="headerlink" title="Evaluating Robustness of Vision-Language Models Under Noisy Conditions"></a>Evaluating Robustness of Vision-Language Models Under Noisy Conditions</h2><p><strong>Authors: Purushoth,  Alireza</strong></p>
<p>Vision-Language Models (VLMs) have attained exceptional success across multimodal tasks such as image captioning and visual question answering. However, their robustness under noisy conditions remains unfamiliar. In this study, we present a comprehensive evaluation framework to evaluate the performance of several state-of-the-art VLMs under controlled perturbations, including lighting variation, motion blur, and compression artifacts. We used both lexical-based metrics (BLEU, METEOR, ROUGE, CIDEr) and neural-based similarity measures using sentence embeddings to quantify semantic alignment. Our experiments span diverse datasets, revealing key insights: (1) descriptiveness of ground-truth captions significantly influences model performance; (2) larger models like LLaVA excel in semantic understanding but do not universally outperform smaller models; and (3) certain noise types, such as JPEG compression and motion blur, dramatically degrade performance across models. Our findings highlight the nuanced trade-offs between model size, dataset characteristics, and noise resilience, offering a standardized benchmark for future robust multimodal learning. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ï¼ˆå¦‚å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ï¼‰ä¸­å–å¾—äº†å¼‚å¸¸æˆåŠŸã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å™ªå£°æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ä»ç„¶æœªçŸ¥ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°æœ€å‰æ²¿çš„VLMsåœ¨å—æ§æ‰°åŠ¨ä¸‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å…‰ç…§å˜åŒ–ã€è¿åŠ¨æ¨¡ç³Šå’Œå‹ç¼©ä¼ªå½±ã€‚æˆ‘ä»¬ä½¿ç”¨äº†åŸºäºè¯æ±‡çš„åº¦é‡æ ‡å‡†ï¼ˆBLEUã€METEORã€ROUGEã€CIDErï¼‰å’ŒåŸºäºç¥ç»çš„å¥å­åµŒå…¥ç›¸ä¼¼æ€§åº¦é‡æ¥é‡åŒ–è¯­ä¹‰å¯¹é½ã€‚æˆ‘ä»¬çš„å®éªŒæ¶µç›–äº†å„ç§æ•°æ®é›†ï¼Œæ­ç¤ºäº†å…³é”®è§è§£ï¼šï¼ˆ1ï¼‰çœŸå®åœºæ™¯æè¿°çš„æè¿°æ€§æ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ï¼›ï¼ˆ2ï¼‰å¤§å‹æ¨¡å‹ï¼ˆå¦‚LLaVAï¼‰åœ¨è¯­ä¹‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¹¶ä¸æ™®éä¼˜äºå°å‹æ¨¡å‹ï¼›ï¼ˆ3ï¼‰æŸäº›å™ªå£°ç±»å‹ï¼Œå¦‚JPEGå‹ç¼©å’Œè¿åŠ¨æ¨¡ç³Šï¼Œä¼šæ˜¾è‘—å½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†æ¨¡å‹å¤§å°ã€æ•°æ®é›†ç‰¹å¾å’Œå™ªå£°ç¨³å¥æ€§ä¹‹é—´çš„å¾®å¦™æƒè¡¡ï¼Œä¸ºæœªæ¥ç¨³å¥çš„å¤šæ¨¡æ€å­¦ä¹ æä¾›äº†æ ‡å‡†åŒ–çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12492v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡å¯¹è·¨æ¨¡æ€ä»»åŠ¡ä¸­çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œæ¶‰åŠå…‰ç…§å˜åŒ–ã€è¿åŠ¨æ¨¡ç³Šå’Œå‹ç¼©ä¼ªå½±ç­‰å—æ§æ‰°åŠ¨ã€‚ç ”ç©¶é‡‡ç”¨è¯æ±‡åŸºç¡€å’ŒåŸºäºç¥ç»çš„å¥å­åµŒå…¥ç›¸ä¼¼æ€§åº¦é‡æ¥é‡åŒ–è¯­ä¹‰å¯¹é½ã€‚å®éªŒæ¶µç›–ä¸åŒçš„æ•°æ®é›†ï¼Œæ­ç¤ºäº†æ¨¡å‹æ€§èƒ½çš„å…³é”®å½±å“å› ç´ ï¼šçœŸå®æ ‡ç­¾æè¿°çš„å½±å“ã€å¤§å‹æ¨¡å‹å¦‚LLaVAçš„è¯­ä¹‰ç†è§£ä¼˜åŠ¿ï¼Œä»¥åŠç‰¹å®šå™ªå£°ç±»å‹ï¼ˆå¦‚JPEGå‹ç¼©å’Œè¿åŠ¨æ¨¡ç³Šï¼‰å¯¹æ¨¡å‹æ€§èƒ½çš„æ˜¾è‘—å½±å“ã€‚è¯¥ç ”ç©¶ä¸ºæœªæ¥çš„ç¨³å¥è·¨æ¨¡æ€å­¦ä¹ æä¾›äº†æ ‡å‡†åŒ–çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çœŸå®æ ‡ç­¾æè¿°çš„è¯¦ç»†ç¨‹åº¦å¯¹æ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>å¤§å‹æ¨¡å‹å¦‚LLaVAåœ¨è¯­ä¹‰ç†è§£æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œä½†å¹¶ä¸æ€»æ˜¯ä¼˜äºå°å‹æ¨¡å‹ã€‚</li>
<li>ç‰¹å®šç±»å‹çš„å™ªå£°ï¼ˆå¦‚JPEGå‹ç¼©å’Œè¿åŠ¨æ¨¡ç³Šï¼‰ä¼šæ˜¾è‘—å½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹å¤§å°ã€æ•°æ®é›†ç‰¹æ€§å’Œå™ªå£°éŸ§æ€§ä¹‹é—´å­˜åœ¨å¾®å¦™çš„æƒè¡¡ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šç§æ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨è¯æ±‡åŸºç¡€å’ŒåŸºäºç¥ç»çš„å¥å­åµŒå…¥ç›¸ä¼¼æ€§åº¦é‡æ¥é‡åŒ–è¯­ä¹‰å¯¹é½çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12492">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12492v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12492v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12492v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12492v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12492v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Evaluating-Large-Language-Models-for-Functional-and-Maintainable-Code-in-Industrial-Settings-A-Case-Study-at-ASML"><a href="#Evaluating-Large-Language-Models-for-Functional-and-Maintainable-Code-in-Industrial-Settings-A-Case-Study-at-ASML" class="headerlink" title="Evaluating Large Language Models for Functional and Maintainable Code in   Industrial Settings: A Case Study at ASML"></a>Evaluating Large Language Models for Functional and Maintainable Code in   Industrial Settings: A Case Study at ASML</h2><p><strong>Authors:Yash Mundhra, Max Valk, Maliheh Izadi</strong></p>
<p>Large language models have shown impressive performance in various domains, including code generation across diverse open-source domains. However, their applicability in proprietary industrial settings, where domain-specific constraints and code interdependencies are prevalent, remains largely unexplored. We present a case study conducted in collaboration with the leveling department at ASML to investigate the performance of LLMs in generating functional, maintainable code within a closed, highly specialized software environment.   We developed an evaluation framework tailored to ASMLâ€™s proprietary codebase and introduced a new benchmark. Additionally, we proposed a new evaluation metric, build@k, to assess whether LLM-generated code successfully compiles and integrates within real industrial repositories. We investigate various prompting techniques, compare the performance of generic and code-specific LLMs, and examine the impact of model size on code generation capabilities, using both match-based and execution-based metrics. The findings reveal that prompting techniques and model size have a significant impact on output quality, with few-shot and chain-of-thought prompting yielding the highest build success rates. The difference in performance between the code-specific LLMs and generic LLMs was less pronounced and varied substantially across different model families. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ…æ‹¬è·¨å¤šç§å¼€æºåŸŸçš„ä»£ç ç”Ÿæˆåœ¨å†…çš„å„ä¸ªé¢†åŸŸè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ä¸“æœ‰å·¥ä¸šç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ï¼Œç‰¹åˆ«æ˜¯å­˜åœ¨ç‰¹å®šé¢†åŸŸçº¦æŸå’Œä»£ç ç›¸äº’ä¾èµ–æ€§çš„ç¯å¢ƒï¼Œä»å¾ˆå°‘è¢«æ¢ç´¢ã€‚æˆ‘ä»¬ä¸ASMLçš„æ ‡å‡†åŒ–éƒ¨é—¨åˆä½œè¿›è¡Œäº†ä¸€é¡¹æ¡ˆä¾‹ç ”ç©¶ï¼Œä»¥è°ƒæŸ¥å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°é—­ã€é«˜åº¦ä¸“ä¸šåŒ–çš„è½¯ä»¶ç¯å¢ƒä¸­ç”ŸæˆåŠŸèƒ½æ€§å¼ºã€å¯ç»´æŠ¤çš„ä»£ç çš„æ€§èƒ½ã€‚æˆ‘ä»¬é’ˆå¯¹ASMLçš„ä¸“æœ‰ä»£ç åº“å¼€å‘äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡build@kï¼Œä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä»£ç æ˜¯å¦èƒ½å¤Ÿåœ¨çœŸå®çš„å·¥ä¸šå­˜å‚¨åº“ä¸­æˆåŠŸç¼–è¯‘å’Œé›†æˆã€‚æˆ‘ä»¬ç ”ç©¶äº†å„ç§æç¤ºæŠ€æœ¯ï¼Œæ¯”è¾ƒäº†é€šç”¨å’Œç‰¹å®šä»£ç çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶è€ƒå¯Ÿäº†æ¨¡å‹å¤§å°å¯¹ä»£ç ç”Ÿæˆèƒ½åŠ›çš„å½±å“ï¼Œé‡‡ç”¨åŸºäºåŒ¹é…å’ŒåŸºäºæ‰§è¡Œä¸¤ç§æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæç¤ºæŠ€æœ¯å’Œæ¨¡å‹å¤§å°å¯¹è¾“å‡ºè´¨é‡æœ‰é‡å¤§å½±å“ï¼Œå…¶ä¸­å°‘æ ·æœ¬å’Œæ€ç»´é“¾æç¤ºäº§ç”Ÿçš„æ„å»ºæˆåŠŸç‡æœ€é«˜ã€‚ç‰¹å®šä»£ç çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œé€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ä¸å¤ªæ˜æ˜¾ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„æ¨¡å‹å®¶æ—ä¹‹é—´å­˜åœ¨å¾ˆå¤§å·®å¼‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12395v1">PDF</a> Accepted in the 40th IEEE&#x2F;ACM International Conference on Automated   Software Engineering, ASE 2025 (Industry track)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼€æºé¢†åŸŸçš„ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨ä¸“æœ‰å·¥ä¸šç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ä»å¾…æ¢ç´¢ã€‚æœ¬ç ”ç©¶ä¸ASMLçš„ç­‰çº§éƒ¨é—¨åˆä½œï¼Œç ”ç©¶LLMsåœ¨å°é—­ã€é«˜åº¦ä¸“ä¸šåŒ–çš„è½¯ä»¶ç¯å¢ƒä¸­ç”ŸæˆåŠŸèƒ½æ€§å¼ºã€å¯ç»´æŠ¤çš„ä»£ç çš„æ€§èƒ½ã€‚ç ”ç©¶é€šè¿‡å®šåˆ¶çš„è¯„ä»·æ¡†æ¶å’Œæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæå‡ºæ–°çš„è¯„ä¼°æŒ‡æ ‡â€œbuild@kâ€ï¼Œä»¥è¯„ä¼°LLMç”Ÿæˆçš„ä»£ç æ˜¯å¦èƒ½åœ¨çœŸå®çš„å·¥ä¸šå­˜å‚¨åº“ä¸­æˆåŠŸç¼–è¯‘å’Œé›†æˆã€‚ç ”ç©¶å‘ç°ï¼Œæç¤ºæŠ€æœ¯å’Œæ¨¡å‹å¤§å°å¯¹è¾“å‡ºè´¨é‡æœ‰é‡å¤§å½±å“ï¼Œä»¥å°‘é‡æ ·æœ¬å’Œè¿è´¯æ€ç»´æç¤ºç”Ÿæˆä»£ç æˆåŠŸç‡æœ€é«˜ã€‚ä»£ç ç‰¹å®šLLMsä¸é€šç”¨LLMsä¹‹é—´çš„æ€§èƒ½å·®å¼‚å¹¶ä¸æ˜¾è‘—ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„æ¨¡å‹å®¶æ—ä¸­å·®å¼‚å¾ˆå¤§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸“æœ‰å·¥ä¸šç¯å¢ƒä¸­çš„ä»£ç ç”Ÿæˆé€‚ç”¨æ€§å¾…æ¢ç´¢ã€‚</li>
<li>ä¸ASMLç­‰çº§éƒ¨é—¨çš„åˆä½œç ”ç©¶è¯„ä¼°äº†LLMsåœ¨ç‰¹å®šè½¯ä»¶ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚</li>
<li>å®šåˆ¶çš„è¯„ä»·æ¡†æ¶å’Œæ–°çš„åŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°LLMç”Ÿæˆçš„ä»£ç ã€‚</li>
<li>æå‡ºçš„â€œbuild@kâ€è¯„ä¼°æŒ‡æ ‡èƒ½åˆ¤æ–­ä»£ç æ˜¯å¦èƒ½åœ¨å·¥ä¸šå­˜å‚¨åº“ä¸­æˆåŠŸç¼–è¯‘å’Œé›†æˆã€‚</li>
<li>æç¤ºæŠ€æœ¯å’Œæ¨¡å‹å¤§å°å¯¹LLMè¾“å‡ºä»£ç çš„è´¨é‡æœ‰é‡å¤§å½±å“ã€‚</li>
<li>å°‘é‡æ ·æœ¬å’Œè¿è´¯æ€ç»´æç¤ºç”Ÿæˆä»£ç æˆåŠŸç‡æœ€é«˜ã€‚</li>
<li>ä»£ç ç‰¹å®šLLMsä¸é€šç”¨LLMsä¹‹é—´çš„æ€§èƒ½å·®å¼‚åœ¨ä¸åŒæ¨¡å‹å®¶æ—ä¸­è¡¨ç°ä¸åŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12395v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12395v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12395v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12395v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12395v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LLM-as-a-Judge-Rapid-Evaluation-of-Legal-Document-Recommendation-for-Retrieval-Augmented-Generation"><a href="#LLM-as-a-Judge-Rapid-Evaluation-of-Legal-Document-Recommendation-for-Retrieval-Augmented-Generation" class="headerlink" title="LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for   Retrieval-Augmented Generation"></a>LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for   Retrieval-Augmented Generation</h2><p><strong>Authors:Anu Pradhan, Alexandra Ortan, Apurv Verma, Madhavan Seshadri</strong></p>
<p>The evaluation bottleneck in recommendation systems has become particularly acute with the rise of Generative AI, where traditional metrics fall short of capturing nuanced quality dimensions that matter in specialized domains like legal research. Can we trust Large Language Models to serve as reliable judges of their own kind? This paper investigates LLM-as-a-Judge as a principled approach to evaluating Retrieval-Augmented Generation systems in legal contexts, where the stakes of recommendation quality are exceptionally high.   We tackle two fundamental questions that determine practical viability: which inter-rater reliability metrics best capture the alignment between LLM and human assessments, and how do we conduct statistically sound comparisons between competing systems? Through systematic experimentation, we discover that traditional agreement metrics like Krippendorffâ€™s alpha can be misleading in the skewed distributions typical of AI system evaluations. Instead, Gwetâ€™s AC2 and rank correlation coefficients emerge as more robust indicators for judge selection, while the Wilcoxon Signed-Rank Test with Benjamini-Hochberg corrections provides the statistical rigor needed for reliable system comparisons.   Our findings suggest a path toward scalable, cost-effective evaluation that maintains the precision demanded by legal applications, transforming what was once a human-intensive bottleneck into an automated, yet statistically principled, evaluation framework. </p>
<blockquote>
<p>éšç€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å…´èµ·ï¼Œæ¨èç³»ç»Ÿä¸­çš„è¯„ä¼°ç“¶é¢ˆé—®é¢˜å˜å¾—å°¤ä¸ºçªå‡ºã€‚åœ¨ä¼ ç»ŸæŒ‡æ ‡æ— æ³•æ•æ‰æ³•å¾‹ç ”ç©¶ç­‰ç‰¹å®šé¢†åŸŸä¸­çš„å¾®å¦™è´¨é‡ç»´åº¦çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬èƒ½ä¿¡ä»»å¤§å‹è¯­è¨€æ¨¡å‹å……å½“åŒç±»ä¹‹é—´çš„å¯é è¯„åˆ¤è€…å—ï¼Ÿæœ¬æ–‡è°ƒæŸ¥äº†LLMä½œä¸ºæ³•å®˜çš„æ–¹æ³•ï¼Œä½œä¸ºè¯„ä¼°æ³•å¾‹èƒŒæ™¯ä¸‹çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿçš„åŸåˆ™æ€§æ–¹æ³•ï¼Œå…¶ä¸­æ¨èè´¨é‡çš„èµŒæ³¨éå¸¸é«˜ã€‚æˆ‘ä»¬è§£å†³äº†ä¸¤ä¸ªå†³å®šå®é™…å¯è¡Œæ€§çš„åŸºæœ¬é—®é¢˜ï¼šå“ªäº›è¯„é‰´è€…ä¹‹é—´çš„ä¸€è‡´æ€§æŒ‡æ ‡æœ€èƒ½æ•æ‰LLMå’Œäººç±»è¯„ä¼°ä¹‹é—´çš„å¯¹é½æƒ…å†µï¼Œä»¥åŠæˆ‘ä»¬å¦‚ä½•åœ¨ç«äº‰ç³»ç»Ÿä¹‹é—´è¿›è¡Œç»Ÿè®¡ä¸Šå¥å…¨çš„æ¯”è¾ƒï¼Ÿé€šè¿‡ç³»ç»Ÿå®éªŒï¼Œæˆ‘ä»¬å‘ç°ä¼ ç»Ÿçš„åè®®æŒ‡æ ‡å¦‚Krippendorffçš„alphaåœ¨å…¸å‹çš„AIç³»ç»Ÿè¯„ä¼°çš„åæ€åˆ†å¸ƒä¸­å¯èƒ½ä¼šè¯¯å¯¼äººã€‚ç›¸åï¼ŒGwetçš„AC2å’Œç§©ç›¸å…³ç³»æ•°ä½œä¸ºæ³•å®˜é€‰æ‹©çš„æ›´ç¨³å¥æŒ‡æ ‡è€Œå‡ºç°ï¼Œè€Œä½¿ç”¨Benjamini-Hochbergä¿®æ­£çš„Wilcoxonç¬¦å·ç§©æ£€éªŒä¸ºå¯é çš„ç³»ç»Ÿæ¯”è¾ƒæä¾›äº†æ‰€éœ€çš„ç»Ÿè®¡ä¸¥è°¨æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†ä¸€æ¡é“è·¯ï¼Œæœç€å¯æ‰©å±•ä¸”ç»æµå®æƒ çš„è¯„ä¼°æ–¹å¼å‘å±•ï¼ŒåŒæ—¶ä¿æŒæ³•å¾‹åº”ç”¨æ‰€è¦æ±‚çš„ç²¾åº¦ï¼Œå°†æ›¾ç»çš„äººåŠ›å¯†é›†ç“¶é¢ˆè½¬å˜ä¸ºè‡ªåŠ¨åŒ–ä½†ç»Ÿè®¡ä¸ŠåŸåˆ™æ€§çš„è¯„ä¼°æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12382v1">PDF</a> Accepted in EARL 25: The 2nd Workshop on Evaluating and Applying   Recommender Systems with Large Language Models at RecSys 2025</p>
<p><strong>Summary</strong><br>åœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æ¨åŠ¨ä¸‹ï¼Œæ¨èç³»ç»Ÿçš„è¯„ä¼°ç“¶é¢ˆæ„ˆå‘çªå‡ºã€‚ä¼ ç»Ÿè¯„ä»·æŒ‡æ ‡éš¾ä»¥æ•æ‰ä¸“ä¸šé¢†åŸŸä¸­å¾®å¦™çš„å“è´¨ç»´åº¦ã€‚æœ¬æ–‡æ¢è®¨äº†LLMä½œä¸ºæ³•å®˜çš„æ–¹æ³•ï¼Œä»¥è¯„ä¼°å¢å¼ºæ£€ç´¢ç”Ÿæˆç³»ç»Ÿåœ¨æ³•å¾‹èƒŒæ™¯ä¸‹çš„å®ç”¨æ€§ã€‚é€šè¿‡ç³»ç»Ÿå®éªŒï¼Œå‘ç°ä¼ ç»Ÿçš„ä¸€è‡´æ€§æŒ‡æ ‡å¦‚Krippendorffçš„alphaåœ¨æŸäº›æƒ…å†µä¸‹å…·æœ‰è¯¯å¯¼æ€§ã€‚ç›¸åï¼ŒGwetçš„AC2å’Œç§©ç›¸å…³ç³»æ•°æ˜¯æ›´ç¨³å¥çš„è¯„ä¼°æŒ‡æ ‡ã€‚åŒæ—¶ï¼Œä½¿ç”¨Wilcoxonç¬¦å·ç§©æ£€éªŒå’ŒBenjamini-Hochbergæ ¡æ­£ä¸ºå¯é çš„ç³»çµ±æ¯”è¾ƒæä¾›äº†ç»Ÿè®¡ä¸¥è°¨æ€§ã€‚ç ”ç©¶ä¸ºè§„æ¨¡åŒ–ã€ç»æµæ•ˆç›Šçš„è¯„ä»·æä¾›äº†è·¯å¾„ï¼Œæ»¡è¶³æ³•å¾‹åº”ç”¨çš„ç²¾ç¡®æ€§è¦æ±‚ï¼Œå°†åŸæœ¬çš„äººåŠ›ç“¶é¢ˆè½¬åŒ–ä¸ºè‡ªåŠ¨åŒ–ã€ç»Ÿè®¡åŸåˆ™çš„è¯„ä»·æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨èç³»ç»Ÿçš„è¯„ä¼°é¢ä¸´ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æ³•å¾‹ç ”ç©¶ç­‰ç‰¹å®šé¢†åŸŸï¼Œä¼ ç»Ÿè¯„ä»·æŒ‡æ ‡æ— æ³•å……åˆ†æ•æ‰å¾®å¦™çš„å“è´¨ç»´åº¦ã€‚</li>
<li>LLMä½œä¸ºæ³•å®˜çš„æ–¹æ³•è¢«æå‡ºï¼Œæ—¨åœ¨è¯„ä¼°å¢å¼ºæ£€ç´¢ç”Ÿæˆç³»ç»Ÿåœ¨æ³•å¾‹èƒŒæ™¯ä¸‹çš„å®ç”¨æ€§ã€‚</li>
<li>ç³»ç»Ÿå®éªŒå‘ç°Krippendorffçš„alphaç­‰ä¼ ç»Ÿä¸€è‡´æ€§æŒ‡æ ‡åœ¨æŸäº›æƒ…å†µä¸‹å…·æœ‰è¯¯å¯¼æ€§ã€‚</li>
<li>Gwetçš„AC2å’Œç§©ç›¸å…³ç³»æ•°è¢«è®¤å®šä¸ºæ›´ç¨³å¥çš„è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>Wilcoxonç¬¦å·ç§©æ£€éªŒç»“åˆBenjamini-Hochbergæ ¡æ­£ä¸ºç³»ç»Ÿæ¯”è¾ƒæä¾›äº†ç»Ÿè®¡ä¸¥è°¨æ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœæœ‰åŠ©äºå»ºç«‹è§„æ¨¡åŒ–ã€é«˜æ•ˆçš„è¯„ä»·æ–¹æ³•ï¼Œæ»¡è¶³æ³•å¾‹åº”ç”¨çš„é«˜ç²¾åº¦è¦æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12382">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12382v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12382v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12382v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12382v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12382v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MORABLES-A-Benchmark-for-Assessing-Abstract-Moral-Reasoning-in-LLMs-with-Fables"><a href="#MORABLES-A-Benchmark-for-Assessing-Abstract-Moral-Reasoning-in-LLMs-with-Fables" class="headerlink" title="MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs   with Fables"></a>MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs   with Fables</h2><p><strong>Authors:Matteo Marcuzzo, Alessandro Zangari, Andrea Albarelli, Jose Camacho-Collados, Mohammad Taher Pilehvar</strong></p>
<p>As LLMs excel on standard reading comprehension benchmarks, attention is shifting toward evaluating their capacity for complex abstract reasoning and inference. Literature-based benchmarks, with their rich narrative and moral depth, provide a compelling framework for evaluating such deeper comprehension skills. Here, we present MORABLES, a human-verified benchmark built from fables and short stories drawn from historical literature. The main task is structured as multiple-choice questions targeting moral inference, with carefully crafted distractors that challenge models to go beyond shallow, extractive question answering. To further stress-test model robustness, we introduce adversarial variants designed to surface LLM vulnerabilities and shortcuts due to issues such as data contamination. Our findings show that, while larger models outperform smaller ones, they remain susceptible to adversarial manipulation and often rely on superficial patterns rather than true moral reasoning. This brittleness results in significant self-contradiction, with the best models refuting their own answers in roughly 20% of cases depending on the framing of the moral choice. Interestingly, reasoning-enhanced models fail to bridge this gap, suggesting that scale - not reasoning ability - is the primary driver of performance. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ ‡å‡†é˜…è¯»ç†è§£èƒ½åŠ›åŸºå‡†æµ‹è¯•ä¸­çš„å‡ºè‰²è¡¨ç°ï¼Œäººä»¬å¼€å§‹æ›´å…³æ³¨å®ƒä»¬åœ¨å¤æ‚æŠ½è±¡æ¨ç†å’Œæ¨æ–­æ–¹é¢çš„èƒ½åŠ›è¯„ä¼°ã€‚åŸºäºæ–‡çŒ®çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥å…¶ä¸°å¯Œçš„å™äº‹å’Œé“å¾·æ·±åº¦ï¼Œä¸ºè¯„ä¼°è¿™æ ·çš„æ·±å±‚æ¬¡ç†è§£æŠ€èƒ½æä¾›äº†ä¸€ä¸ªæœ‰å¸å¼•åŠ›çš„æ¡†æ¶ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†MORABLESï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡äººå·¥éªŒè¯çš„åŸºå‡†æµ‹è¯•ï¼Œå®ƒå»ºç«‹åœ¨ä»å†å²æ–‡çŒ®ä¸­æå–çš„å¯“è¨€å’ŒçŸ­ç¯‡æ•…äº‹ä¹‹ä¸Šã€‚ä¸»è¦ä»»åŠ¡æ˜¯ä»¥é’ˆå¯¹é“å¾·æ¨æ–­çš„å¤šé¡¹é€‰æ‹©é¢˜çš„å½¢å¼è¿›è¡Œï¼Œç²¾å¿ƒè®¾è®¡äº†ä¸€äº›å¹²æ‰°é€‰é¡¹ï¼Œä»¥æŒ‘æˆ˜æ¨¡å‹è¶…è¶Šæµ…å±‚æ¬¡çš„ã€æå–å¼çš„é—®é¢˜å›ç­”èƒ½åŠ›ã€‚ä¸ºäº†è¿›ä¸€æ­¥æµ‹è¯•æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯¹æŠ—æ€§å˜ä½“ï¼Œæ—¨åœ¨æš´éœ²ç”±äºæ•°æ®æ±¡æŸ“ç­‰é—®é¢˜å¯¼è‡´çš„LLMçš„è„†å¼±æ€§å’Œæ·å¾„ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè™½ç„¶å¤§å‹æ¨¡å‹çš„æ€§èƒ½ä¼˜äºå°å‹æ¨¡å‹ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ“ä½œçš„å¹²æ‰°ï¼Œå¹¶ä¸”å¾€å¾€ä¾èµ–äºè¡¨é¢æ¨¡å¼ï¼Œè€ŒéçœŸæ­£çš„é“å¾·æ¨ç†ã€‚è¿™ç§è„†å¼±æ€§å¯¼è‡´äº†æ˜¾è‘—çš„è‡ªæˆ‘çŸ›ç›¾ï¼Œåœ¨é“å¾·é€‰æ‹©çš„ä¸åŒè¡¨è¿°ä¸‹ï¼Œæœ€ä½³æ¨¡å‹çº¦æœ‰20%çš„æƒ…å†µä¸‹ä¼šåé©³è‡ªå·±çš„ç­”æ¡ˆã€‚æœ‰è¶£çš„æ˜¯ï¼Œå¢å¼ºæ¨ç†èƒ½åŠ›çš„æ¨¡å‹æœªèƒ½ç¼©å°è¿™ä¸€å·®è·ï¼Œè¿™è¡¨æ˜æ€§èƒ½çš„ä¸»è¦é©±åŠ¨å› ç´ æ˜¯è§„æ¨¡è€Œéæ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12371v1">PDF</a> Accepted to EMNLP 2025 Main Conference</p>
<p><strong>Summary</strong><br>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é˜…è¯»ç†è§£æ ‡å‡†ä¸Šçš„å‡ºè‰²è¡¨ç°ï¼Œäººä»¬å¼€å§‹å…³æ³¨å…¶åœ¨å¤æ‚æŠ½è±¡æ¨ç†å’Œæ¨æ–­æ–¹é¢çš„èƒ½åŠ›è¯„ä¼°ã€‚ä¸ºäº†è¯„ä¼°æ›´æ·±å±‚æ¬¡çš„ç†è§£æŠ€èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†MORABLESåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•é€šè¿‡äººç±»éªŒè¯ï¼ŒåŸºäºå†å²æ•…äº‹å’ŒçŸ­æ•…äº‹çš„é“å¾·å†…æ¶µä¸°å¯Œæ„å»ºè€Œæˆã€‚ä¸»è¦ä»»åŠ¡æ˜¯ä»¥é’ˆå¯¹é“å¾·æ¨ç†çš„å¤šé¡¹é€‰æ‹©é¢˜å½¢å¼è¿›è¡Œï¼Œç²¾å¿ƒè®¾è®¡å¹²æ‰°é¡¹ä»¥æŒ‘æˆ˜æ¨¡å‹è¶…è¶Šæµ…å±‚æ¬¡çš„æå–å¼é—®ç­”èƒ½åŠ›ã€‚ä¸ºäº†æµ‹è¯•æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯¹æŠ—æ€§å˜ä½“ï¼Œæ—¨åœ¨æ­ç¤ºç”±äºæ•°æ®æ±¡æŸ“ç­‰é—®é¢˜å¯¼è‡´çš„LLMæ¼æ´å’Œæ·å¾„ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶å¤§å‹æ¨¡å‹çš„æ€§èƒ½ä¼˜äºå°å‹æ¨¡å‹ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ“çºµçš„å½±å“ï¼Œå¹¶ç»å¸¸ä¾èµ–äºè¡¨é¢æ¨¡å¼è€ŒéçœŸæ­£çš„é“å¾·æ¨ç†ã€‚è¿™ç§è„†å¼±æ€§å¯¼è‡´æ˜¾è‘—çš„è‡ªæˆ‘çŸ›ç›¾ï¼Œæœ€ä½³æ¨¡å‹åœ¨çº¦20%çš„æƒ…å†µä¸‹ä¼šå› é“å¾·é€‰æ‹©çš„ä¸åŒè€Œå¦å®šè‡ªå·±çš„ç­”æ¡ˆã€‚æœ‰è¶£çš„æ˜¯ï¼Œå¢å¼ºæ¨ç†èƒ½åŠ›çš„æ¨¡å‹æ— æ³•ç¼©å°è¿™ä¸€å·®è·ï¼Œè¿™è¡¨æ˜æ€§èƒ½çš„ä¸»è¦é©±åŠ¨å› ç´ æ˜¯è§„æ¨¡è€Œéæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é˜…è¯»ç†è§£æ ‡å‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç°åœ¨äººä»¬æ›´å…³æ³¨å…¶åœ¨å¤æ‚æŠ½è±¡æ¨ç†å’Œæ¨æ–­æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>MORABLESæ˜¯ä¸€ä¸ªåŸºäºå†å²æ–‡çŒ®çš„å™äº‹å’Œé“å¾·æ·±åº¦æ„å»ºçš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ›´æ·±å±‚æ¬¡çš„é“å¾·æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MORABLESçš„ä¸»è¦ä»»åŠ¡æ˜¯å¤šé¡¹é€‰æ‹©é¢˜å½¢å¼çš„ç»“æ„åŒ–ä»»åŠ¡ï¼Œæ—¨åœ¨æŒ‘æˆ˜æ¨¡å‹è¶…è¶Šæµ…å±‚æ¬¡çš„æå–å¼é—®ç­”èƒ½åŠ›ã€‚</li>
<li>å¯¹æŠ—æ€§å˜ä½“è¢«ç”¨æ¥æµ‹è¯•æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œæ­ç¤ºLLMçš„æ¼æ´å’Œæ·å¾„ã€‚</li>
<li>å¤§å‹æ¨¡å‹è™½ç„¶æ€§èƒ½è¾ƒå¥½ï¼Œä½†ä»å­˜åœ¨è„†å¼±æ€§ï¼Œå®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ“çºµçš„å½±å“ã€‚</li>
<li>æ¨¡å‹åœ¨é“å¾·æ¨ç†æ–¹é¢å­˜åœ¨è‡ªæˆ‘çŸ›ç›¾ç°è±¡ï¼Œæœ€ä½³æ¨¡å‹åœ¨çº¦20%çš„æƒ…å†µä¸‹ä¼šå› é“å¾·é€‰æ‹©çš„ä¸åŒè€Œå¦å®šè‡ªå·±çš„ç­”æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12371v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12371v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12371v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12371v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12371v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="When-Safe-Unimodal-Inputs-Collide-Optimizing-Reasoning-Chains-for-Cross-Modal-Safety-in-Multimodal-Large-Language-Models"><a href="#When-Safe-Unimodal-Inputs-Collide-Optimizing-Reasoning-Chains-for-Cross-Modal-Safety-in-Multimodal-Large-Language-Models" class="headerlink" title="When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for   Cross-Modal Safety in Multimodal Large Language Models"></a>When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for   Cross-Modal Safety in Multimodal Large Language Models</h2><p><strong>Authors:Wei Cai, Shujuan Liu, Jian Zhao, Ziyan Shi, Yusheng Zhao, Yuchen Yuan, Tianle Zhang, Chi Zhang, Xuelong Li</strong></p>
<p>Multimodal Large Language Models (MLLMs) are susceptible to the implicit reasoning risk, wherein innocuous unimodal inputs synergistically assemble into risky multimodal data that produce harmful outputs. We attribute this vulnerability to the difficulty of MLLMs maintaining safety alignment through long-chain reasoning. To address this issue, we introduce Safe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring interpretable reasoning paths tailored for such a cross-modal challenge. A novel training framework, Safety-aware Reasoning Path Optimization (SRPO), is also designed based on the SSUI dataset to align the MLLMâ€™s internal reasoning process with human safety values. Experimental results show that our SRPO-trained models achieve state-of-the-art results on key safety benchmarks, including the proposed Reasoning Path Benchmark (RSBench), significantly outperforming both open-source and top-tier commercial MLLMs. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å®¹æ˜“å—åˆ°éšå¼æ¨ç†é£é™©çš„å½±å“ï¼Œå…¶ä¸­æ— å®³çš„å•æ¨¡æ€è¾“å…¥ä¼šååŒç»„åˆæˆé£é™©æ€§å¤šæ¨¡æ€æ•°æ®ï¼Œä»è€Œäº§ç”Ÿæœ‰å®³è¾“å‡ºã€‚æˆ‘ä»¬å°†è¿™ç§è„†å¼±æ€§å½’å› äºMLLMsåœ¨é€šè¿‡é•¿é“¾æ¨ç†ä¿æŒå®‰å…¨å¯¹é½æ–¹é¢çš„å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Safe-Semantics-but-Unsafe-Interpretationï¼ˆSSUIï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…·æœ‰å¯è§£é‡Šæ¨ç†è·¯å¾„çš„ç‰¹å®šæ•°æ®é›†ï¼Œæ—¨åœ¨åº”å¯¹æ­¤ç±»è·¨æ¨¡æ€æŒ‘æˆ˜ã€‚è¿˜è®¾è®¡äº†åŸºäºSSUIæ•°æ®é›†çš„æ–°å‹è®­ç»ƒæ¡†æ¶Safety-aware Reasoning Path Optimizationï¼ˆSRPOï¼‰ï¼Œä»¥ä½¿MLLMçš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ä¸äººç±»å®‰å…¨ä»·å€¼è§‚ä¿æŒä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡SRPOè®­ç»ƒçš„æ¨¡å‹åœ¨å…³é”®å®‰å…¨åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€æ–°æŠ€æœ¯æˆæœï¼ŒåŒ…æ‹¬æå‡ºçš„Reasoning Path Benchmarkï¼ˆRSBenchï¼‰ï¼Œæ˜¾è‘—ä¼˜äºå¼€æºå’Œé¡¶çº§å•†ä¸šMLLMsã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12060v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢ä¸´éšå¼æ¨ç†é£é™©ï¼Œå³æ— å®³çš„å•æ¨¡æ€è¾“å…¥ååŒå½¢æˆé£é™©æ€§å¤šæ¨¡æ€æ•°æ®ï¼Œäº§ç”Ÿæœ‰å®³è¾“å‡ºã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Safe-Semantics-but-Unsafe-Interpretationï¼ˆSSUIï¼‰æ•°æ®é›†å’ŒåŸºäºè¯¥æ•°æ®é›†çš„Safety-aware Reasoning Path Optimizationï¼ˆSRPOï¼‰è®­ç»ƒæ¡†æ¶ï¼Œä»¥å°†MLLMçš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ä¸äººç±»å®‰å…¨ä»·å€¼è§‚å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSRPOè®­ç»ƒçš„æ¨¡å‹åœ¨å…³é”®å®‰å…¨åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æˆæœã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢ä¸´éšå¼æ¨ç†é£é™©ã€‚</li>
<li>æ— å®³çš„å•æ¨¡æ€è¾“å…¥å¯ä»¥ç»„åˆæˆæœ‰å®³çš„å¤šæ¨¡æ€æ•°æ®ã€‚</li>
<li>MLLMsåœ¨ä¿æŒé•¿æœŸæ¨ç†å®‰å…¨æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>å¼•å…¥Safe-Semantics-but-Unsafe-Interpretationï¼ˆSSUIï¼‰æ•°æ®é›†ä»¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>è®¾è®¡äº†åŸºäºSSUIæ•°æ®é›†çš„Safety-aware Reasoning Path Optimizationï¼ˆSRPOï¼‰è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>SRPOè®­ç»ƒæ¨¡å‹åœ¨å…³é”®å®‰å…¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12060">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12060v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12060v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12060v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12060v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12060v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Agentic-Lybic-Multi-Agent-Execution-System-with-Tiered-Reasoning-and-Orchestration"><a href="#Agentic-Lybic-Multi-Agent-Execution-System-with-Tiered-Reasoning-and-Orchestration" class="headerlink" title="Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and   Orchestration"></a>Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and   Orchestration</h2><p><strong>Authors:Liangxuan Guo, Bin Zhu, Qingqian Tao, Kangning Liu, Xun Zhao, Xianzhe Qin, Jin Gao, Guangfu Hao</strong></p>
<p>Autonomous agents for desktop automation struggle with complex multi-step tasks due to poor coordination and inadequate quality control. We introduce Agentic Lybic, a novel multi-agent system where the entire architecture operates as a finite-state machine (FSM). This core innovation enables dynamic orchestration. Our system comprises four components: a Controller, a Manager, three Workers (Technician for code-based operations, Operator for GUI interactions, and Analyst for decision support), and an Evaluator. The critical mechanism is the FSM-based routing between these components, which provides flexibility and generalization by dynamically selecting the optimal execution strategy for each subtask. This principled orchestration, combined with robust quality gating, enables adaptive replanning and error recovery. Evaluated officially on the OSWorld benchmark, Agentic Lybic achieves a state-of-the-art 57.07% success rate in 50 steps, substantially outperforming existing methods. Results demonstrate that principled multi-agent orchestration with continuous quality control provides superior reliability for generalized desktop automation in complex computing environments. </p>
<blockquote>
<p>è‡ªä¸»ä»£ç†åœ¨æ¡Œé¢è‡ªåŠ¨åŒ–ä¸­å¤„ç†å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡æ—¶ï¼Œç”±äºåè°ƒæ€§å·®å’Œè´¨é‡æ§åˆ¶ä¸è¶³è€Œé‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬å¼•å…¥äº†Agentic Lybicï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šä»£ç†ç³»ç»Ÿï¼Œæ•´ä¸ªæ¶æ„ä½œä¸ºä¸€ä¸ªæœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰è¿è¡Œã€‚è¿™ä¸€æ ¸å¿ƒåˆ›æ–°å®ç°äº†åŠ¨æ€ååŒã€‚æˆ‘ä»¬çš„ç³»ç»Ÿç”±å››ä¸ªç»„ä»¶ç»„æˆï¼šæ§åˆ¶å™¨ã€ç®¡ç†å™¨ã€ä¸‰ä¸ªå·¥ä½œè€…ï¼ˆæŠ€æœ¯å·¥äººè´Ÿè´£åŸºäºä»£ç çš„æ“ä½œã€æ“ä½œå‘˜è´Ÿè´£GUIäº¤äº’ã€åˆ†æå¸ˆè´Ÿè´£å†³ç­–æ”¯æŒï¼‰ï¼Œä»¥åŠè¯„ä¼°å™¨ã€‚å…³é”®æœºåˆ¶æ˜¯è¿™äº›ç»„ä»¶ä¹‹é—´çš„åŸºäºFSMçš„è·¯ç”±ï¼Œå®ƒé€šè¿‡åŠ¨æ€é€‰æ‹©æ¯ä¸ªå­ä»»åŠ¡çš„æœ€ä½³æ‰§è¡Œç­–ç•¥æ¥æä¾›çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚è¿™ç§åŸåˆ™æ€§çš„ååŒï¼Œç»“åˆå¼ºå¤§çš„è´¨é‡é—¨æ§ï¼Œèƒ½å¤Ÿå®ç°è‡ªé€‚åº”çš„é‡æ–°è§„åˆ’å’Œé”™è¯¯æ¢å¤ã€‚åœ¨OSWorldåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œå®˜æ–¹è¯„ä¼°ï¼ŒAgentic Lybicåœ¨50æ­¥å†…è¾¾åˆ°äº†æœ€å…ˆè¿›çš„57.07%çš„æˆåŠŸç‡ï¼Œå¤§å¤§ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å¤æ‚çš„è®¡ç®—ç¯å¢ƒä¸­ï¼Œå…·æœ‰æŒç»­è´¨é‡æ§åˆ¶çš„åŸåˆ™æ€§å¤šä»£ç†ååŒä¸ºé€šç”¨çš„æ¡Œé¢è‡ªåŠ¨åŒ–æä¾›äº†æ›´é«˜çš„å¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11067v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡ç« ä»‹ç»äº†Agentic Lybicè¿™ä¸€æ–°å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨æœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰ä½œä¸ºæ ¸å¿ƒæ¶æ„ï¼Œå®ç°äº†åŠ¨æ€ååŒä½œä¸šã€‚ç³»ç»ŸåŒ…æ‹¬æ§åˆ¶å™¨ã€ç®¡ç†å™¨ã€ä¸‰ä¸ªå·¥ä½œè€…ï¼ˆæŠ€æœ¯å·¥äººè´Ÿè´£ä»£ç æ“ä½œã€æ“ä½œå·¥äººè´Ÿè´£ç•Œé¢äº¤äº’ã€åˆ†æå·¥äººè´Ÿè´£å†³ç­–æ”¯æŒï¼‰ï¼Œä»¥åŠè¯„ä¼°å™¨ã€‚å…³é”®æœºåˆ¶åœ¨äºåŸºäºFSMçš„å·¥ä½œæµç¨‹è°ƒåº¦ï¼Œèƒ½é’ˆå¯¹æ¯ä¸ªå­ä»»åŠ¡åŠ¨æ€é€‰æ‹©æœ€ä½³æ‰§è¡Œç­–ç•¥ï¼Œæä¾›çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚ç»“åˆå¼ºå¤§çš„è´¨é‡é—¨æ§æœºåˆ¶ï¼Œå¯å®ç°è‡ªé€‚åº”è§„åˆ’å’Œé”™è¯¯æ¢å¤ã€‚åœ¨OSWorldåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAgentic Lybicè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆçš„57.07%çš„50æ­¥æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œå…·æœ‰æŒç»­è´¨é‡æ§åˆ¶çš„å¤šæ™ºèƒ½ä½“ååŒä½œä¸šèƒ½ä¸ºå¤æ‚è®¡ç®—ç¯å¢ƒä¸‹çš„æ¡Œé¢è‡ªåŠ¨åŒ–æä¾›æ›´é«˜çš„å¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Agentic Lybicæ˜¯ä¸€ä¸ªæ–°å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé‡‡ç”¨æœ‰é™çŠ¶æ€æœºä½œä¸ºæ ¸å¿ƒæ¶æ„ã€‚</li>
<li>ç³»ç»ŸåŒ…æ‹¬æ§åˆ¶å™¨ã€ç®¡ç†å™¨ã€ä¸‰ä¸ªå·¥ä½œè€…ï¼ˆæŠ€æœ¯å·¥äººã€æ“ä½œå·¥äººã€åˆ†æå·¥äººï¼‰å’Œè¯„ä¼°å™¨ã€‚</li>
<li>åŸºäºFSMçš„å·¥ä½œæµç¨‹è°ƒåº¦æ˜¯ç³»ç»Ÿçš„å…³é”®æœºåˆ¶ï¼Œä¸ºä»»åŠ¡æä¾›çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚</li>
<li>ç³»ç»Ÿèƒ½å¤Ÿå®ç°åŠ¨æ€ååŒä½œä¸šï¼Œå¹¶å…·æœ‰å¼ºå¤§çš„è´¨é‡é—¨æ§æœºåˆ¶ï¼Œæ”¯æŒè‡ªé€‚åº”è§„åˆ’å’Œé”™è¯¯æ¢å¤ã€‚</li>
<li>Agentic Lybicåœ¨OSWorldåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸šç•Œé¢†å…ˆçš„æˆåŠŸç‡ã€‚</li>
<li>ç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æŒç»­è´¨é‡æ§åˆ¶çš„å¤šæ™ºèƒ½ä½“ååŒä½œä¸šåœ¨å¤æ‚è®¡ç®—ç¯å¢ƒä¸‹çš„æ¡Œé¢è‡ªåŠ¨åŒ–ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11067">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.11067v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.11067v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MachineLearningLM-Scaling-Many-shot-In-context-Learning-via-Continued-Pretraining"><a href="#MachineLearningLM-Scaling-Many-shot-In-context-Learning-via-Continued-Pretraining" class="headerlink" title="MachineLearningLM: Scaling Many-shot In-context Learning via Continued   Pretraining"></a>MachineLearningLM: Scaling Many-shot In-context Learning via Continued   Pretraining</h2><p><strong>Authors:Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke</strong></p>
<p>Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows.   Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference.   Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‹¥æœ‰å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œå¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ ‡å‡†æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä»»åŠ¡ä¸Šï¼Œå®ƒä»¬å¾ˆéš¾ä»å¤šä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­å­¦ä¹ ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¬æ— æ³•ä»…é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰åˆ©ç”¨å¤šä¸ªç¤ºä¾‹æ¼”ç¤ºï¼Œè€Œæ— éœ€è¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚æˆ‘ä»¬å¼•å…¥äº†MachineLearningLMï¼Œè¿™æ˜¯ä¸€ä¸ªä¾¿æºå¼æŒç»­é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒä½¿é€šç”¨LLMå…·å¤‡å¼ºå¤§çš„ä¸Šä¸‹æ–‡MLèƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™å…¶ä¸€èˆ¬çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥æ”¯æŒæ›´å¹¿æ³›çš„èŠå¤©å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬çš„é¢„è®­ç»ƒç¨‹åºä»æ•°ç™¾ä¸‡ä¸ªç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰ä¸­ç»¼åˆMLä»»åŠ¡ï¼Œæ¶µç›–ç¤ºä¾‹æ•°é‡é«˜è¾¾1024ä¸ªã€‚æˆ‘ä»¬ä»¥éšæœºæ£®æ—æ•™å¸ˆå¼€å§‹ï¼Œå°†åŸºäºæ ‘çš„å†³ç­–ç­–ç•¥è’¸é¦åˆ°LLMä¸­ï¼Œä»¥åŠ å¼ºæ•°å€¼å»ºæ¨¡ä¸­çš„ç¨³å¥æ€§ã€‚æ‰€æœ‰ä»»åŠ¡éƒ½é€šè¿‡é«˜æ•ˆçš„ä»¤ç‰Œæç¤ºè¿›è¡Œåºåˆ—åŒ–ï¼Œå¯ä»¥åœ¨æ¯ä¸ªä¸Šä¸‹æ–‡çª—å£ä¸­å®ç°3åˆ°6å€çš„æ›´å¤šç¤ºä¾‹ï¼Œå¹¶é€šè¿‡æ‰¹é‡æ¨ç†å®ç°é«˜è¾¾50å€çš„æ‘Šé”€ååé‡ã€‚å°½ç®¡é‡‡ç”¨äº†é€‚åº¦çš„è®¾ç½®ï¼ˆQwen-2.5-7B-Instructå¸¦æœ‰LoRAç­‰çº§8ï¼‰ï¼Œä½†MachineLearningLMåœ¨è´¢åŠ¡ã€ç‰©ç†ã€ç”Ÿç‰©å’ŒåŒ»ç–—ä¿å¥é¢†åŸŸçš„åˆ†å¸ƒå¤–çš„è¡¨æ ¼åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œç›¸å¯¹äºå¼ºå¤§çš„LLMåŸºçº¿ï¼ˆå¦‚GPT-5-miniï¼‰å¹³å‡æé«˜äº†çº¦15%çš„å‡†ç¡®ç‡ã€‚å®ƒè¡¨ç°å‡ºæƒŠäººçš„å¤šç¤ºä¾‹æ‰©å±•å®šå¾‹ï¼šéšç€ä¸Šä¸‹æ–‡æ¼”ç¤ºä»8ä¸ªå¢åŠ åˆ°1024ä¸ªï¼Œå‡†ç¡®ç‡å•è°ƒå¢åŠ ã€‚æ— éœ€ä»»ä½•ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒï¼Œå®ƒåœ¨æ•°ç™¾æ¬¡å°„å‡»ä¸­å°±èƒ½è¾¾åˆ°éšæœºæ£®æ—çº§åˆ«çš„ç²¾åº¦ã€‚åŒæ—¶ä¿ç•™äº†ä¸€èˆ¬èŠå¤©åŠŸèƒ½ï¼ŒåŒ…æ‹¬çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼šå®ƒåœ¨MMLUä¸Šè¾¾åˆ°äº†75.4%çš„å‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06806v5">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œå¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ ‡å‡†æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä»»åŠ¡ä¸Šå­¦ä¹ å¤šä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹æ—¶é‡åˆ°å›°éš¾ã€‚æœ¬æ–‡ä»‹ç»äº†MachineLearningLMï¼Œè¿™æ˜¯ä¸€ä¸ªä¾¿æºå¼æŒç»­é¢„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºé€šç”¨LLMæä¾›å¼ºå¤§çš„ä¸Šä¸‹æ–‡MLèƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™å…¶ä¸€èˆ¬çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ä»¥åº”å¯¹æ›´å¹¿æ³›çš„èŠå¤©å·¥ä½œæµç¨‹ã€‚é¢„è®­ç»ƒè¿‡ç¨‹é€šè¿‡åˆæˆæ¥è‡ªæ•°ç™¾ä¸‡ç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰çš„MLä»»åŠ¡è¿›è¡Œï¼Œæ¶µç›–ç¤ºä¾‹æ•°é‡é«˜è¾¾1024ä¸ªã€‚æœ¬æ–‡é‡‡ç”¨éšæœºæ£®æ—æ•™å¸ˆï¼Œå°†åŸºäºæ ‘çš„å†³ç­–ç­–ç•¥æç‚¼æˆLLMï¼Œä»¥å¢å¼ºæ•°å€¼å»ºæ¨¡ä¸­çš„ç¨³å¥æ€§ã€‚æ‰€æœ‰ä»»åŠ¡éƒ½é‡‡ç”¨é«˜æ•ˆçš„ä»¤ç‰Œæç¤ºè¿›è¡Œåºåˆ—åŒ–ï¼Œå¯ä»¥åœ¨æ¯ä¸ªä¸Šä¸‹æ–‡çª—å£ä¸­æä¾›3å€è‡³6å€çš„ç¤ºä¾‹ï¼Œå¹¶é€šè¿‡æ‰¹é‡æ¨ç†å®ç°é«˜è¾¾50å€çš„æ‘Šé”€ååé‡ã€‚å°½ç®¡é‡‡ç”¨äº†é€‚åº¦çš„è®¾ç½®ï¼ˆQwen-2.5-7B-Instruct with LoRA rank 8ï¼‰ï¼Œä½†MachineLearningLMåœ¨é‡‘èã€ç‰©ç†ã€ç”Ÿç‰©å’ŒåŒ»ç–—ä¿å¥é¢†åŸŸçš„ç¦»åˆ†å¸ƒè¡¨æ ¼åˆ†ç±»ä¸Šå¹³å‡ä¼˜äºå¼ºå¤§çš„LLMåŸºçº¿ï¼ˆå¦‚GPT-5-miniï¼‰çº¦15%ã€‚å®ƒå‘ˆç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„è®¸å¤šé•œå¤´è§„æ¨¡å®šå¾‹ï¼šéšç€ä¸Šä¸‹æ–‡æ¼”ç¤ºä»8ä¸ªå¢é•¿åˆ°1024ä¸ªï¼Œå‡†ç¡®æ€§å•è°ƒå¢åŠ ã€‚æ— éœ€ä»»ä½•ç‰¹å®šä»»åŠ¡è®­ç»ƒï¼Œå³å¯åœ¨æ•°ç™¾ä¸ªé•œå¤´ä¸Šå®ç°éšæœºæ£®æ—çº§åˆ«çš„ç²¾åº¦ã€‚åŒæ—¶ä¿ç•™äº†ä¸€èˆ¬èŠå¤©èƒ½åŠ›ï¼ŒåŒ…æ‹¬çŸ¥è¯†å’Œæ¨ç†ï¼Œåœ¨MMLUä¸Šè¾¾åˆ°75.4%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶æ‹¥æœ‰å¹¿æ³›çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ ‡å‡†æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸Šå­¦ä¹ å¤šä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>MachineLearningLMæ¡†æ¶é€šè¿‡æŒç»­é¢„è®­ç»ƒå¢å¼ºäº†LLMçš„ä¸Šä¸‹æ–‡æœºå™¨å­¦ä¹ èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™å…¶ä¸€èˆ¬çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>é¢„è®­ç»ƒè¿‡ç¨‹é€šè¿‡åˆæˆæ¥è‡ªç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰çš„MLä»»åŠ¡è¿›è¡Œï¼Œæ¶µç›–çš„ç¤ºä¾‹æ•°é‡æœ€å¤šå¯è¾¾1024ä¸ªã€‚</li>
<li>ä½¿ç”¨éšæœºæ£®æ—æ•™å¸ˆæ¥å¼ºåŒ–LLMåœ¨æ•°å€¼å»ºæ¨¡ä¸­çš„ç¨³å¥æ€§ã€‚</li>
<li>MachineLearningLMåœ¨å¤šä¸ªé¢†åŸŸï¼ˆåŒ…æ‹¬é‡‘èã€ç‰©ç†ã€ç”Ÿç‰©å’ŒåŒ»ç–—ä¿å¥ï¼‰çš„ç¦»åˆ†å¸ƒè¡¨æ ¼åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹³å‡ä¼˜äºç°æœ‰å¼ºå¤§LLMåŸºçº¿çº¦15%ã€‚</li>
<li>è¯¥æ¡†æ¶å±•ç°å‡ºæ˜¾è‘—çš„å¤šé•œå¤´è§„æ¨¡å®šå¾‹ï¼Œéšç€ä¸Šä¸‹æ–‡æ¼”ç¤ºæ•°é‡çš„å¢åŠ ï¼Œå‡†ç¡®æ€§ä¸æ–­æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06806">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.06806v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.06806v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.06806v5/page_4_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-18/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-18/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-18/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13310v1/page_2_0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-18  Scaling Agents via Continual Pre-training
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-18/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Talking Head Generation/2411.19331v3/page_4_0.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-18  Talking to DINO Bridging Self-Supervised Vision Backbones with Language   for Open-Vocabulary Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
