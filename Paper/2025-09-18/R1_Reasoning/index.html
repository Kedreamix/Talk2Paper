<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-09-18  WebResearcher Unleashing unbounded reasoning capability in Long-Horizon   Agents">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12883v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    82 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-18-更新"><a href="#2025-09-18-更新" class="headerlink" title="2025-09-18 更新"></a>2025-09-18 更新</h1><h2 id="WebResearcher-Unleashing-unbounded-reasoning-capability-in-Long-Horizon-Agents"><a href="#WebResearcher-Unleashing-unbounded-reasoning-capability-in-Long-Horizon-Agents" class="headerlink" title="WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon   Agents"></a>WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon   Agents</h2><p><strong>Authors:Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, Rui Min, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou</strong></p>
<p>Recent advances in deep-research systems have demonstrated the potential for AI agents to autonomously discover and synthesize knowledge from external sources. In this paper, we introduce WebResearcher, a novel framework for building such agents through two key components: (1) WebResearcher, an iterative deep-research paradigm that reformulates deep research as a Markov Decision Process, where agents periodically consolidate findings into evolving reports while maintaining focused workspaces, overcoming the context suffocation and noise contamination that plague existing mono-contextual approaches; and (2) WebFrontier, a scalable data synthesis engine that generates high-quality training data through tool-augmented complexity escalation, enabling systematic creation of research tasks that bridge the gap between passive knowledge recall and active knowledge construction. Notably, we find that the training data from our paradigm significantly enhances tool-use capabilities even for traditional mono-contextual methods. Furthermore, our paradigm naturally scales through parallel thinking, enabling concurrent multi-agent exploration for more comprehensive conclusions. Extensive experiments across 6 challenging benchmarks demonstrate that WebResearcher achieves state-of-the-art performance, even surpassing frontier proprietary systems. </p>
<blockquote>
<p>近期深度研究系统的进展已经显示出人工智能代理能够自主从外部来源发现和合成知识的潜力。在本文中，我们介绍了WebResearcher，这是一个构建此类代理的新型框架，它包含两个关键组成部分：（1）WebResearcher，这是一种迭代深度研究范式，它将深度研究重新定义为马尔可夫决策过程，在此过程中，代理会定期整合发现成果并形成不断演变的报告，同时保持专注的工作空间，克服现有单上下文方法所面临的上下文窒息和噪声污染问题；（2）WebFrontier，这是一个可扩展的数据合成引擎，通过工具增强复杂性升级生成高质量的训练数据，能够系统地创建研究任务，从而缩小被动知识回忆和主动知识构建之间的差距。值得注意的是，我们发现我们的范式产生的训练数据甚至可以提高传统单上下文方法的工具使用能力。此外，我们的范式通过并行思维自然扩展，能够实现多代理并行探索以得出更全面的结论。在6个具有挑战性的基准测试上的广泛实验表明，WebResearcher达到了最新技术水平，甚至超越了前沿专有系统。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13309v1">PDF</a> <a target="_blank" rel="noopener" href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/">https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/</a></p>
<p><strong>Summary</strong><br>在深度研究系统的最新进展中，AI代理可以自主发现和整合外部知识的能力得到证明。本文介绍了WebResearcher框架，该框架包含两个关键组成部分：一是WebResearcher迭代深度研究范式，将深度研究重新定义为马尔可夫决策过程，代理在此过程中定期整合发现成果并生成不断更新的报告，同时保持专注的工作空间以克服现有单上下文方法的上下文窒息和噪声污染问题；二是WebFrontier可扩展数据合成引擎，它通过工具增强复杂性升级生成高质量的训练数据，能够系统地创建研究任务，缩小被动知识回忆和主动知识构建之间的差距。实验结果表明，WebResearcher达到最先进的性能水平，甚至在超越前沿专有系统方面具有显著优势。更重要的是，它能很好地应对大规模的并发需求场景和多主体参与的决策制定需求场景。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>WebResearcher框架引入了一种迭代深度研究范式，将深度研究定义为马尔可夫决策过程。</li>
<li>WebResearcher框架解决了现有方法的上下文窒息和噪声污染问题。</li>
<li>WebFrontier数据合成引擎可以生成高质量的训练数据，缩小被动知识回忆和主动知识构建之间的差距。</li>
<li>训练数据质量提升增强了工具使用能力，甚至对于传统的单上下文方法也是如此。</li>
<li>WebResearcher框架天然地支持并行思考，能够支持多代理的并发探索，提供更为全面的结论。</li>
<li>实验结果显示WebResearcher达到甚至超越了最前沿的专有系统性能水平。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13309">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13309v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13309v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13309v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="WebSailor-V2-Bridging-the-Chasm-to-Proprietary-Agents-via-Synthetic-Data-and-Scalable-Reinforcement-Learning"><a href="#WebSailor-V2-Bridging-the-Chasm-to-Proprietary-Agents-via-Synthetic-Data-and-Scalable-Reinforcement-Learning" class="headerlink" title="WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic   Data and Scalable Reinforcement Learning"></a>WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic   Data and Scalable Reinforcement Learning</h2><p><strong>Authors:Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, Xinyu Wang, Zile Qiao, Zhen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou</strong></p>
<p>Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all open-source agents in complex information-seeking tasks, matching proprietary agents’ performance and closing the capability gap. </p>
<blockquote>
<p>突破人类认知限制是大模型训练的重要前沿领域。像DeepResearch这样的专有智能体系统已在极为复杂的检索基准测试（如BrowseComp）上展现出超人类的能力，这是之前无法实现的。我们认为它们的成功依赖于开源模型所缺少的高级推理模式：在浏览浩瀚的信息景观时，系统地减少极端不确定性的能力。基于此洞察，我们推出了WebSailor，这是一种完整的后训练方法论，旨在培养这种关键能力。我们的方法包括通过结构化采样、信息模糊处理、RFT冷启动以及高效的智能强化学习训练算法DUPO（重复采样策略优化），生成具有新型和高不确定性的任务。有了这个综合流程，WebSailor在复杂的信息检索任务中大大超越了所有开源智能体，匹配了专有智能体的性能，并缩小了能力差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13305v1">PDF</a> <a target="_blank" rel="noopener" href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/">https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/</a></p>
<p><strong>摘要</strong><br>超越人类认知限制在大规模语言模型训练中代表着重要的前沿领域。专有智能系统如DeepResearch已经在诸如BrowseComp等极端复杂的信息检索基准测试中展现了超人类的能力，这是一个以前无法达到的成就。我们认为其成功的关键在于开放源代码模型所缺少的复杂推理模式：在浏览大量信息时系统地降低极端不确定性的能力。基于此见解，我们推出了WebSailor，这是一种专门设计用于培养这种重要能力的全新训练后方法。我们的方法通过结构化采样和信息模糊化生成新型高不确定性任务，结合基于强化学习的冷启动和高效的智能强化训练算法DUPO（复制采样策略优化），形成了一个综合的管道。WebSailor在复杂的信息检索任务中显著优于所有开源智能体，匹配专有智能体的性能并缩小了能力差距。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>专有智能系统已经超越了人类在某些复杂信息检索任务中的能力。</li>
<li>DeepResearch在BrowseComp等基准测试中的表现证明了这一点。</li>
<li>成功的原因在于专有智能系统具备降低极端不确定性的能力。</li>
<li>WebSailor是一种新型训练后方法，旨在培养这一核心能力。</li>
<li>WebSailor通过结构化采样和信息模糊化生成高不确定性任务。</li>
<li>它结合了基于强化学习的冷启动方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13305">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13305v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13305v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Layout-Aware-OCR-for-Black-Digital-Archives-with-Unsupervised-Evaluation"><a href="#Layout-Aware-OCR-for-Black-Digital-Archives-with-Unsupervised-Evaluation" class="headerlink" title="Layout-Aware OCR for Black Digital Archives with Unsupervised Evaluation"></a>Layout-Aware OCR for Black Digital Archives with Unsupervised Evaluation</h2><p><strong>Authors:Fitsum Sileshi Beyene, Christopher L. Dancy</strong></p>
<p>Despite their cultural and historical significance, Black digital archives continue to be a structurally underrepresented area in AI research and infrastructure. This is especially evident in efforts to digitize historical Black newspapers, where inconsistent typography, visual degradation, and limited annotated layout data hinder accurate transcription, despite the availability of various systems that claim to handle optical character recognition (OCR) well. In this short paper, we present a layout-aware OCR pipeline tailored for Black newspaper archives and introduce an unsupervised evaluation framework suited to low-resource archival contexts. Our approach integrates synthetic layout generation, model pretraining on augmented data, and a fusion of state-of-the-art You Only Look Once (YOLO) detectors. We used three annotation-free evaluation metrics, the Semantic Coherence Score (SCS), Region Entropy (RE), and Textual Redundancy Score (TRS), which quantify linguistic fluency, informational diversity, and redundancy across OCR regions. Our evaluation on a 400-page dataset from ten Black newspaper titles demonstrates that layout-aware OCR improves structural diversity and reduces redundancy compared to full-page baselines, with modest trade-offs in coherence. Our results highlight the importance of respecting cultural layout logic in AI-driven document understanding and lay the foundation for future community-driven and ethically grounded archival AI systems. </p>
<blockquote>
<p>尽管黑人数码档案在其文化和历史意义上非常重要，但在人工智能研究和基础设施中，它们仍然是一个被结构性低估的领域。这在将黑历史报纸数字化的工作中尤为明显，尽管有各种声称处理光学字符识别（OCR）很好的系统，但由于排版不一致、视觉退化以及有限的注释布局数据，仍阻碍了准确的转录。在这篇简短的论文中，我们针对黑报纸档案提出了一种布局感知的OCR管道，并引入了一个适用于低资源档案环境的无监督评估框架。我们的方法结合了合成布局生成、在增强数据上进行模型预训练以及最先进的You Only Look Once（YOLO）检测器的融合。我们使用三种无注释评估指标，即语义连贯性得分（SCS）、区域熵（RE）和文本冗余得分（TRS），它们可以量化OCR区域的语言流畅性、信息多样性和冗余性。我们对来自十份黑报纸标题的400页数据集进行的评估表明，与全页基线相比，布局感知OCR提高了结构多样性并减少了冗余，连贯性方面只有适度的权衡。我们的研究结果强调了尊重文化布局逻辑在人工智能驱动的文件理解中的重要性，并为未来社区驱动和基于伦理的档案人工智能系统奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13236v1">PDF</a> IEEE-ISTAS conference</p>
<p><strong>Summary</strong></p>
<p>本文关注黑人群体的数字档案在人工智能研究中的重要性。针对数字化黑历史报纸面临的挑战，提出了一种面向布局的光学字符识别（OCR）管道，并引入了一个适合低资源档案环境的无监督评估框架。该研究集成了合成布局生成、在增强数据上的模型预训练，以及最先进的You Only Look Once（YOLO）检测器的融合。通过在一个包含来自十个黑报纸标题的400页数据集上的评估，证明了面向布局的OCR在提高结构多样性和减少冗余方面相较于全页基线有所改善，尽管在语言流畅性方面存在适度的权衡。该研究强调了尊重文化布局逻辑在人工智能驱动的文件理解中的重要性，并为未来社区驱动和道德立足的档案人工智能系统奠定了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>黑人群体的数字档案在人工智能研究中的重要性被忽视。</li>
<li>面向布局的光学字符识别（OCR）管道对于数字化黑历史报纸具有挑战性。</li>
<li>集成合成布局生成、模型预训练和YOLO检测器的融合是解决这一挑战的关键。</li>
<li>面向布局的OCR能够提高结构多样性和减少冗余。</li>
<li>面向布局的OCR相较于全页基线在语言流畅性方面存在适度的权衡。</li>
<li>尊重文化布局逻辑在人工智能驱动的文件理解中非常重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13236">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13236v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13236v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13236v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13236v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13236v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Reasoning-with-Preference-Constraints-A-Benchmark-for-Language-Models-in-Many-to-One-Matching-Markets"><a href="#Reasoning-with-Preference-Constraints-A-Benchmark-for-Language-Models-in-Many-to-One-Matching-Markets" class="headerlink" title="Reasoning with Preference Constraints: A Benchmark for Language Models   in Many-to-One Matching Markets"></a>Reasoning with Preference Constraints: A Benchmark for Language Models   in Many-to-One Matching Markets</h2><p><strong>Authors:Marylou Fauchard, Florian Carichon, Margarida Carvalho, Golnoosh Farnadi</strong></p>
<p>Recent advances in reasoning with large language models (LLMs) have demonstrated strong performance on complex mathematical tasks, including combinatorial optimization. Techniques such as Chain-of-Thought and In-Context Learning have further enhanced this capability, making LLMs both powerful and accessible tools for a wide range of users, including non-experts. However, applying LLMs to matching problems, which require reasoning under preferential and structural constraints, remains underexplored. To address this gap, we introduce a novel benchmark of 369 instances of the College Admission Problem, a canonical example of a matching problem with preferences, to evaluate LLMs across key dimensions: feasibility, stability, and optimality. We employ this benchmark to assess the performance of several open-weight LLMs. Our results first reveal that while LLMs can satisfy certain constraints, they struggle to meet all evaluation criteria consistently. They also show that reasoning LLMs, like QwQ and GPT-oss, significantly outperform traditional models such as Llama, Qwen or Mistral, defined here as models used without any dedicated reasoning mechanisms. Moreover, we observed that LLMs reacted differently to the various prompting strategies tested, which include Chain-of-Thought, In-Context Learning and role-based prompting, with no prompt consistently offering the best performance. Finally, we report the performances from iterative prompting with auto-generated feedback and show that they are not monotonic; they can peak early and then significantly decline in later attempts. Overall, this work offers a new perspective on model reasoning performance and the effectiveness of prompting strategies in combinatorial optimization problems with preferential constraints. </p>
<blockquote>
<p>近期，大型语言模型（LLM）在推理方面的进展已在复杂数学任务中展现出强大的性能，包括组合优化。诸如“思考链”和“上下文学习”等技术进一步增强了这一能力，使LLM成为强大且易于使用的工具，适用于广泛的用户，包括非专业人士。然而，将LLM应用于匹配问题，这个问题需要在偏好和结构约束下进行推理，仍然缺乏足够的探索。为了填补这一空白，我们引入了包含369个实例的“大学录取问题”新基准测试，作为具有偏好的匹配问题的典型示例，以评估LLM的关键维度：可行性、稳定性和最优性。我们使用此基准来评估几个开源权重LLM的性能。我们的结果首先表明，虽然LLM可以满足某些约束，但它们难以始终如一地满足所有评估标准。他们还显示，像QwQ和GPT-oss这样的推理LLM显著优于传统的模型，如Llama、Qwen或Mistral，这里定义为没有任何专门推理机制的模型。此外，我们观察到LLM对测试的各种提示策略的反应各不相同，包括“思考链”、“上下文学习”和基于角色的提示，没有任何提示始终提供最佳性能。最后，我们报告了通过自动生成反馈进行迭代提示的性能，并显示它们并非单调；它们可能在早期达到峰值，然后在后续尝试中显著下降。总体而言，这项工作提供了关于模型推理性能和偏好约束下的组合优化问题中的提示策略有效性的新视角。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13131v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在复杂的数学任务上表现出强大的性能，包括组合优化。通过Chain-of-Thought和In-Context Learning等技术，LLM的推理能力得到了进一步提升，使其成为非专家用户也能使用的强大工具。然而，将LLM应用于带有偏好和结构约束的匹配问题上仍被较少探索。为弥补这一空白，我们引入了包含369个学院录取问题实例的新基准测试，以评估LLM在可行性、稳定性和最优性等方面的关键维度。结果显示，LLM虽能满足某些约束，但在一致满足评价准则上仍有困难。同时，具有推理机制的LLM（如QwQ和GPT-oss）显著优于传统模型（如Llama、Qwen和Mistral）。此外，不同的提示策略对LLM的反应有所不同，没有一种提示策略始终表现最佳。最后，我们报告了自动反馈迭代提示的性能结果，这些性能并不单调，可能在早期达到峰值然后在后续尝试中显著下降。总体而言，这项工作为模型推理性能和偏好约束组合优化问题中的提示策略有效性提供了新的视角。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在复杂的数学任务上表现出强大的性能，包括处理组合优化问题。</li>
<li>LLM可以通过Chain-of-Thought和In-Context Learning等技术进行增强，使其变得更加易用和强大。</li>
<li>LLM在解决带有偏好和结构约束的匹配问题上仍有待探索。</li>
<li>我们引入了一个新的基准测试，包含369个学院录取问题实例，以评估LLM的推理能力。</li>
<li>LLM在满足所有评价准则上表现挣扎，但具有推理机制的LLM（如QwQ和GPT-oss）显著优于传统模型。</li>
<li>不同的提示策略对LLM的影响不同，没有一种策略始终最佳。</li>
<li>自动反馈迭代提示的性能不单调，可能在早期达到峰值后下降。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13131">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13131v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13131v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.13131v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Rethinking-the-Evaluation-of-Alignment-Methods-Insights-into-Diversity-Generalisation-and-Safety"><a href="#Rethinking-the-Evaluation-of-Alignment-Methods-Insights-into-Diversity-Generalisation-and-Safety" class="headerlink" title="Rethinking the Evaluation of Alignment Methods: Insights into Diversity,   Generalisation, and Safety"></a>Rethinking the Evaluation of Alignment Methods: Insights into Diversity,   Generalisation, and Safety</h2><p><strong>Authors:Denis Janiak, Julia Moska, Dawid Motyka, Karolina Seweryn, Paweł Walkowiak, Bartosz Żuk, Arkadiusz Janz</strong></p>
<p>Large language models (LLMs) require careful alignment to balance competing objectives - factuality, safety, conciseness, proactivity, and diversity. Existing studies focus on individual techniques or specific dimensions, lacking a holistic assessment of the inherent trade-offs. We propose a unified evaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO) across these five axes, using both in-distribution and out-of-distribution datasets. Leveraging a specialized LLM-as-Judge prompt, validated through human studies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead in safety, and PPO best balances conciseness with proactivity. Our findings provide insights into trade-offs of common alignment methods, guiding the development of more balanced and reliable LLMs. </p>
<blockquote>
<p>大型语言模型（LLM）需要谨慎对齐以平衡相互竞争的目标，包括真实性、安全性、简洁性、主动性和多样性。现有研究主要集中在个别技术或特定维度上，缺乏对这些目标内在权衡的全面评估。我们提出了一个统一的评估框架，通过这个框架比较LLM对齐方法（PPO、DPO、ORPO、KTO）在这五个轴上的表现，同时使用分布内和分布外的数据集。通过借助专门的LLM-as-Judge提示并通过人类研究进行验证，我们发现DPO和KTO在事实准确性方面表现出色，PPO和DPO在安全性方面领先，而PPO在简洁性和主动性之间达到了最佳平衡。我们的研究为常见的对齐方法提供了权衡的见解，为开发更平衡、更可靠的大型语言模型提供了指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12936v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）需要在多个目标之间实现谨慎的对齐，包括事实性、安全性、简洁性、主动性和多样性。现有研究侧重于个别技术或特定维度，缺乏对不同对齐方法内在权衡的全面评估。本研究提出了一个统一的评估框架，用于比较LLM对齐方法（PPO、DPO、ORPO、KTO）在这五个方面的表现，同时使用分布内和分布外的数据集。通过专项的LLM-as-Judge提示和人类研究验证，发现DPO和KTO在事实准确性方面表现出色，PPO和DPO在安全方面领先，且在简洁性与主动性之间取得最佳平衡。本研究结果提供了常见对齐方法的权衡洞察，为开发更平衡、更可靠的大型语言模型提供了指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）需要对多个目标进行对齐，包括事实性、安全性等。</li>
<li>现有研究在评估LLM对齐方法时缺乏全面评估内在权衡的框架。</li>
<li>本研究提出了一个统一的评估框架，用于比较不同LLM对齐方法的表现。</li>
<li>通过专项的LLM-as-Judge提示和人类研究验证，发现DPO和KTO在事实准确性方面表现优异。</li>
<li>PPO和DPO在安全方面领先其他对齐方法。</li>
<li>PPO在简洁性与主动性之间取得最佳平衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12936">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12936v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12936v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12936v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12936v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12936v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DialNav-Multi-turn-Dialog-Navigation-with-a-Remote-Guide"><a href="#DialNav-Multi-turn-Dialog-Navigation-with-a-Remote-Guide" class="headerlink" title="DialNav: Multi-turn Dialog Navigation with a Remote Guide"></a>DialNav: Multi-turn Dialog Navigation with a Remote Guide</h2><p><strong>Authors:Leekyeung Han, Hyunji Min, Gyeom Hwangbo, Jonghyun Choi, Paul Hongsuck Seo</strong></p>
<p>We introduce DialNav, a novel collaborative embodied dialog task, where a navigation agent (Navigator) and a remote guide (Guide) engage in multi-turn dialog to reach a goal location. Unlike prior work, DialNav aims for holistic evaluation and requires the Guide to infer the Navigator’s location, making communication essential for task success. To support this task, we collect and release the Remote Assistance in Navigation (RAIN) dataset, human-human dialog paired with navigation trajectories in photorealistic environments. We design a comprehensive benchmark to evaluate both navigation and dialog, and conduct extensive experiments analyzing the impact of different Navigator and Guide models. We highlight key challenges and publicly release the dataset, code, and evaluation framework to foster future research in embodied dialog. </p>
<blockquote>
<p>我们介绍了DialNav，这是一个新型的合作式体验对话任务，其中导航代理（Navigator）和远程指南（Guide）进行多轮对话以达到目标地点。不同于以前的工作，DialNav旨在进行全面评估，并要求指南推断导航器的位置，这使得沟通对于任务成功至关重要。为了支持这项任务，我们收集和发布了远程导航辅助（RAIN）数据集，该数据集包含人类之间的对话以及与逼真环境中的导航轨迹配对的数据。我们设计了一个全面的基准测试来评估导航和对话，并进行了广泛实验分析不同导航器和指南模型的影响。我们强调了关键挑战，并公开发布数据集、代码和评估框架，以促进未来在体验式对话方面的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12894v1">PDF</a> 18 pages, 8 figures, ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了DialNav这一新型协作式体现对话任务，以及与之配套的数据集RAIN。在该任务中，导航代理（Navigator）和远程指南（Guide）通过多轮对话达成目标地点。与以往工作不同，DialNav追求整体评估，并要求指南推测导航器的位置，使得沟通对任务成功至关重要。本文设计了一个全面的基准测试来评估导航和对话能力，并进行了大量实验分析不同导航器和指南模型的影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DialNav是一种新型协作式体现对话任务，涉及导航代理和远程指南的多轮对话以达到目标地点。</li>
<li>DialNav追求整体评估，要求指南能够推断导航器的位置，沟通对任务成功至关重要。</li>
<li>为支持DialNav任务，发布了Remote Assistance in Navigation (RAIN)数据集，包含人类对话与导航轨迹。</li>
<li>设计了全面的基准测试以评估导航和对话能力。</li>
<li>进行了大量实验，分析了不同导航器和指南模型的影响。</li>
<li>强调了任务的关键挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12894">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12894v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12894v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12894v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12894v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12894v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Lego-Edit-A-General-Image-Editing-Framework-with-Model-Level-Bricks-and-MLLM-Builder"><a href="#Lego-Edit-A-General-Image-Editing-Framework-with-Model-Level-Bricks-and-MLLM-Builder" class="headerlink" title="Lego-Edit: A General Image Editing Framework with Model-Level Bricks and   MLLM Builder"></a>Lego-Edit: A General Image Editing Framework with Model-Level Bricks and   MLLM Builder</h2><p><strong>Authors:Qifei Jia, Yu Liu, Yajie Chai, Xintong Yao, Qiming Lu, Yasen Zhang, Runyu Shi, Ying Huang, Guoquan Zhang</strong></p>
<p>Instruction-based image editing has garnered significant attention due to its direct interaction with users. However, real-world user instructions are immensely diverse, and existing methods often fail to generalize effectively to instructions outside their training domain, limiting their practical application. To address this, we propose Lego-Edit, which leverages the generalization capability of Multi-modal Large Language Model (MLLM) to organize a suite of model-level editing tools to tackle this challenge. Lego-Edit incorporates two key designs: (1) a model-level toolkit comprising diverse models efficiently trained on limited data and several image manipulation functions, enabling fine-grained composition of editing actions by the MLLM; and (2) a three-stage progressive reinforcement learning approach that uses feedback on unannotated, open-domain instructions to train the MLLM, equipping it with generalized reasoning capabilities for handling real-world instructions. Experiments demonstrate that Lego-Edit achieves state-of-the-art performance on GEdit-Bench and ImgBench. It exhibits robust reasoning capabilities for open-domain instructions and can utilize newly introduced editing tools without additional fine-tuning.   Code is available: <a target="_blank" rel="noopener" href="https://github.com/xiaomi-research/lego-edit">https://github.com/xiaomi-research/lego-edit</a>. </p>
<blockquote>
<p>基于指令的图像编辑因其与用户之间的直接交互而备受关注。然而，现实世界中的用户指令极其多样，现有方法往往难以有效地推广到训练域之外的指令，从而限制了其实际应用。为了解决这一问题，我们提出了Lego-Edit，它利用多模态大型语言模型（MLLM）的泛化能力，组织了一系列模型级编辑工具来应对这一挑战。Lego-Edit包含两个关键设计：（1）一个模型级工具包，其中包含多种在有限数据上有效训练过的模型以及多个图像操作功能，使MLLM能够进行精细的编辑动作组合；（2）一个三阶段的渐进强化学习方法，它使用对未标注的开放域指令的反馈来训练MLLM，使其具备处理现实世界中指令的通用推理能力。实验表明，Lego-Edit在GEdit-Bench和ImgBench上达到了最先进的性能。它展现出对开放域指令的稳健推理能力，并能使用新引入的编辑工具而无需额外的微调。代码可用：<a target="_blank" rel="noopener" href="https://github.com/xiaomi-research/lego-edit%E3%80%82">https://github.com/xiaomi-research/lego-edit。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12883v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于指令的图像编辑因其直接的用户交互而受到广泛关注。然而，真实世界的用户指令极其多样，现有方法往往难以有效推广到训练域之外的指令，限制了其实用性。为解决这一问题，我们提出了Lego-Edit，它利用多模态大型语言模型（MLLM）的泛化能力，设计了一系列模型级编辑工具来应对这一挑战。Lego-Edit包含两个关键设计：一是包含多种模型的高效工具包，这些模型在有限数据上训练有图像操作功能，使得MLLM能够进行精细的编辑动作组合；二是采用三阶段渐进强化学习的方法，利用未标注的开放领域指令反馈来训练MLLM，使其具备处理真实世界指令的通用推理能力。实验表明，Lego-Edit在GEdit-Bench和Imgbench上达到最新技术水平，对开放领域指令展现出强大的推理能力，并能使用新引入的编辑工具而无需额外微调。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基于指令的图像编辑面临用户指令多样性导致的泛化难题。</li>
<li>Lego-Edit利用多模态大型语言模型的泛化能力来解决这一问题。</li>
<li>Lego-Edit包含模型级工具包，能在有限数据上训练多种图像操作功能。</li>
<li>Lego-Edit采用三阶段渐进强化学习方法，利用未标注的开放领域指令反馈训练模型。</li>
<li>Lego-Edit具备处理真实世界指令的通用推理能力。</li>
<li>实验显示Lego-Edit在多个基准测试上达到最新技术水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12883">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12883v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12883v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12883v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12883v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LTA-thinker-Latent-Thought-Augmented-Training-Framework-for-Large-Language-Models-on-Complex-Reasoning"><a href="#LTA-thinker-Latent-Thought-Augmented-Training-Framework-for-Large-Language-Models-on-Complex-Reasoning" class="headerlink" title="LTA-thinker: Latent Thought-Augmented Training Framework for Large   Language Models on Complex Reasoning"></a>LTA-thinker: Latent Thought-Augmented Training Framework for Large   Language Models on Complex Reasoning</h2><p><strong>Authors:Jiaqi Wang, Binquan Ji, Haibo Luo, Yiyang Qi, Ruiting Li, Huiyan Wang, Yuantao Han, Cangyi Yang, jiaxu Zhang, Feiliang Ren</strong></p>
<p>Complex Reasoning in Large Language Models can be dynamically optimized using Test-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut, SoftCoT and its variant are effective in continuous latent space inference, the core bottleneck still lies in the efficient generation and utilization of high-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger variance in the generated Latent Thought distribution more closely approximates the golden truth distribution, we propose a Latent Thought-Augmented Training Framework–LTA-Thinker, which improves distributional variance and enhances reasoning performance from two perspectives. First, LTA-Thinker constructs a Latent Thought generation architecture based on a learnable prior. This architecture aims to increase the variance distribution of generated Latent Thought Vectors in order to simplify the overall structure and raise the performance ceiling. Second, LTA-Thinker introduces a distribution-based directional optimization paradigm that jointly constrains both distribution locality and distribution scale. This mechanism improves information efficiency and computational cost through a multi-objective co-training strategy, which combines standard Supervised Fine-Tuning (SFT) loss with two novel losses: Semantic Alignment Loss, which utilizes KL divergence to ensure that the Latent Thought is highly relevant to the semantics of the question; Reasoning Focus Loss, which utilizes a contrastive learning mechanism to guide the model to focus on the most critical reasoning steps. Experiments show that LTA-thinker achieves state-of-the-art (SOTA) performance among various baselines and demonstrates a higher performance ceiling and better scaling effects. </p>
<blockquote>
<p>在大语言模型的复杂推理中，可以通过测试时间缩放（Test-Time Scaling，TTS）进行动态优化，以减轻过度思考。椰子（Coconut）、软CoT及其变种等方法在连续潜在空间推理中有效，但核心瓶颈仍在于高质量潜在思维的生成和利用。SoftCoT++理论指出，生成的潜在思维分布具有较大的方差，更接近真实分布。因此，我们提出了一种基于潜在思维的增强训练框架——LTA-Thinker，从两个方面提高分布方差和推理性能。首先，LTA-Thinker构建了一个基于可学习先验的潜在思维生成架构。该架构旨在增加生成潜在思维向量的方差分布，以简化整体结构并提高性能上限。其次，LTA-Thinker引入了一种基于分布的方向优化范式，同时约束分布局部性和分布规模。该机制通过多目标协同训练策略提高了信息效率和计算成本，结合标准监督微调（SFT）损失和两个新损失：语义对齐损失，利用KL散度确保潜在思维与问题语义高度相关；推理焦点损失，利用对比学习机制引导模型关注最关键的推理步骤。实验表明，LTA-thinker在各种基线方法中实现了最先进的性能，并表现出更高的性能上限和更好的缩放效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12875v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>大型语言模型的复杂推理可通过测试时缩放（TTS）进行动态优化，以减轻过度思考。Coconut、SoftCoT及其变种等方法在连续潜在空间推理中有效，但核心瓶颈仍在于高效生成和利用高质量的潜在思维。基于SoftCoT++理论，提出一种潜在思维增强训练框架——LTA-Thinker，从两个方面提高分布方差和推理性能。首先，LTA-Thinker构建了一个基于可学习先验的潜在思维生成架构，旨在增加生成潜在思维向量的方差分布，以简化整体结构并提高性能上限。其次，LTA-Thinker引入了一种基于分布的方向优化范式，联合约束分布局部性和分布规模，通过多目标联合训练策略，结合标准监督微调（SFT）损失与两种新颖损失：语义对齐损失，利用KL散度确保潜在思维与问题语义高度相关；推理焦点损失，利用对比学习机制引导模型关注最关键推理步骤。实验表明，LTA-Thinker在各种基线方法中实现最佳性能，表现出更高的性能上限和更好的缩放效果。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型的复杂推理可通过测试时缩放（TTS）优化，以缓解过度思考问题。</li>
<li>Coconut、SoftCoT等方法在连续潜在空间推理中表现有效，但存在生成高质量潜在思维的瓶颈。</li>
<li>LTA-Thinker基于SoftCoT++理论，提出一个潜在思维增强训练框架。</li>
<li>LTA-Thinker构建了一个旨在增加生成潜在思维向量方差分布的架构。</li>
<li>LTA-Thinker引入了一种联合约束分布局部性和分布规模的优化范式。</li>
<li>LTA-Thinker通过多目标联合训练策略结合标准监督微调（SFT）损失与两种新颖损失：语义对齐损失和推理焦点损失。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12875">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12875v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12875v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12875v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Tool-R1-Sample-Efficient-Reinforcement-Learning-for-Agentic-Tool-Use"><a href="#Tool-R1-Sample-Efficient-Reinforcement-Learning-for-Agentic-Tool-Use" class="headerlink" title="Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use"></a>Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use</h2><p><strong>Authors:Yabo Zhang, Yihan Zeng, Qingyun Li, Zhen Hu, Kavin Han, Wangmeng Zuo</strong></p>
<p>Large language models (LLMs) have demonstrated strong capabilities in language understanding and reasoning, yet they remain limited when tackling real-world tasks that require up-to-date knowledge, precise operations, or specialized tool use. To address this, we propose Tool-R1, a reinforcement learning framework that enables LLMs to perform general, compositional, and multi-step tool use by generating executable Python code. Tool-R1 supports integration of user-defined tools and standard libraries, with variable sharing across steps to construct coherent workflows. An outcome-based reward function, combining LLM-based answer judgment and code execution success, guides policy optimization. To improve training efficiency, we maintain a dynamic sample queue to cache and reuse high-quality trajectories, reducing the overhead of costly online sampling. Experiments on the GAIA benchmark show that Tool-R1 substantially improves both accuracy and robustness, achieving about 10% gain over strong baselines, with larger improvements on complex multi-step tasks. These results highlight the potential of Tool-R1 for enabling reliable and efficient tool-augmented reasoning in real-world applications. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/YBYBZhang/Tool-R1">https://github.com/YBYBZhang/Tool-R1</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在理解和推理方面表现出了强大的能力，但在处理需要最新知识、精确操作或专业工具使用的现实世界任务时仍存在局限性。为了解决这一问题，我们提出了Tool-R1，这是一个强化学习框架，能够通过生成可执行Python代码，使LLM执行通用、组合和多步骤的工具使用。Tool-R1支持用户定义的工具和标准库的集成，通过跨步骤的变量共享来构建连贯的工作流程。一个以结果为基础的奖励函数，结合LLM的答案判断和代码执行成功与否，引导策略优化。为了提高训练效率，我们维护了一个动态样本队列来缓存和重复使用高质量的轨迹，减少昂贵的在线采样的开销。在GAIA基准测试上的实验表明，Tool-R1在准确率和稳健性方面有了显著提高，比强基线高出约10%的增益，在复杂的多步骤任务上取得了更大的改进。这些结果突出了Tool-R1在现实世界应用中实现可靠和高效工具增强推理的潜力。我们的代码将在<a target="_blank" rel="noopener" href="https://github.com/YBYBZhang/Tool-R1%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/YBYBZhang/Tool-R1上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12867v1">PDF</a> </p>
<p><strong>Summary</strong><br>大型语言模型（LLMs）在理解和推理方面表现出强大的能力，但在处理需要最新知识、精确操作或专业工具使用的现实世界任务时仍存在局限性。为解决这一问题，我们提出了Tool-R1，这是一个强化学习框架，通过生成可执行Python代码，使LLMs能够执行通用、组合和多步骤的工具使用。Tool-R1支持用户定义工具和标准库的集成，跨步骤进行变量共享以构建连贯的工作流。采用基于结果的奖励函数，结合LLM的答案判断和代码执行成功与否，引导策略优化。为提高训练效率，我们维护了一个动态样本队列来缓存和重用高质量的轨迹，减少昂贵的在线采样开销。在GAIA基准测试上的实验表明，Tool-R1在准确率和鲁棒性方面取得了显著提高，较强基线约提高10%，在复杂的多步骤任务上获得更大提升。这些结果突显了Tool-R1在现实世界工具辅助推理中的可靠和高效潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在现实世界任务中仍有局限性，需要工具使用的能力。</li>
<li>Tool-R1是一个强化学习框架，使LLMs能够执行通用、组合和多步骤的工具使用，通过生成可执行Python代码。</li>
<li>Tool-R1支持用户定义工具和标准库的集成，并允许跨步骤变量共享。</li>
<li>基于结果的奖励函数结合LLM的答案判断和代码执行成功与否，引导策略优化。</li>
<li>为提高训练效率，采用了动态样本队列来缓存和重用高质量的轨迹。</li>
<li>在GAIA基准测试上，Tool-R1较基线有显著提高，特别是在复杂多步骤任务上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12867">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12867v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12867v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12867v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GRATE-a-Graph-transformer-based-deep-Reinforcement-learning-Approach-for-Time-efficient-autonomous-robot-Exploration"><a href="#GRATE-a-Graph-transformer-based-deep-Reinforcement-learning-Approach-for-Time-efficient-autonomous-robot-Exploration" class="headerlink" title="GRATE: a Graph transformer-based deep Reinforcement learning Approach   for Time-efficient autonomous robot Exploration"></a>GRATE: a Graph transformer-based deep Reinforcement learning Approach   for Time-efficient autonomous robot Exploration</h2><p><strong>Authors:Haozhan Ni, Jingsong Liang, Chenyu He, Yuhong Cao, Guillaume Sartoretti</strong></p>
<p>Autonomous robot exploration (ARE) is the process of a robot autonomously navigating and mapping an unknown environment. Recent Reinforcement Learning (RL)-based approaches typically formulate ARE as a sequential decision-making problem defined on a collision-free informative graph. However, these methods often demonstrate limited reasoning ability over graph-structured data. Moreover, due to the insufficient consideration of robot motion, the resulting RL policies are generally optimized to minimize travel distance, while neglecting time efficiency. To overcome these limitations, we propose GRATE, a Deep Reinforcement Learning (DRL)-based approach that leverages a Graph Transformer to effectively capture both local structure patterns and global contextual dependencies of the informative graph, thereby enhancing the model’s reasoning capability across the entire environment. In addition, we deploy a Kalman filter to smooth the waypoint outputs, ensuring that the resulting path is kinodynamically feasible for the robot to follow. Experimental results demonstrate that our method exhibits better exploration efficiency (up to 21.5% in distance and 21.3% in time to complete exploration) than state-of-the-art conventional and learning-based baselines in various simulation benchmarks. We also validate our planner in real-world scenarios. </p>
<blockquote>
<p>自主机器人探索（ARE）是机器人自主导航和映射未知环境的过程。最近的基于强化学习（RL）的方法通常将ARE制定为在免碰撞信息图上定义的顺序决策问题。然而，这些方法通常在图结构数据的推理能力上表现有限。此外，由于机器人运动考虑不足，所得的RL策略一般优化为最小化旅行距离，而忽略了时间效率。为了克服这些局限性，我们提出了GRATE，这是一种基于深度强化学习（DRL）的方法，它利用图转换器有效地捕获信息图的局部结构模式和全局上下文依赖关系，从而提高了整个环境的模型推理能力。此外，我们采用卡尔曼滤波器对航迹点输出进行平滑处理，确保生成的路径对于机器人来说是动力学可行的。实验结果表明，我们的方法在多种仿真基准测试中表现出更高的探索效率（完成探索的距离和时间分别提高了21.5%和21.3%），并且我们还验证了我们的规划器在现实场景中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12863v1">PDF</a> </p>
<p><strong>Summary</strong><br>自主机器人探索（ARE）是机器人自主导航和映射未知环境的过程。针对现有强化学习（RL）方法在处理图形结构化数据时的推理能力有限以及忽视机器人运动考虑的问题，我们提出一种基于深度强化学习（DRL）的方法GRATE，该方法利用图转换器有效地捕获信息图形的局部结构模式和全局上下文依赖关系，增强了模型在整个环境中的推理能力。同时，我们采用卡尔曼滤波器对航点输出进行平滑处理，确保机器人能够跟随动力学可行的路径。实验结果表明，我们的方法在多种仿真基准测试中，探索效率优于现有传统和基于学习的方法，距离和时间分别提高了最多达21.5%和21.3%。我们还在真实场景中对规划器进行了验证。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自主机器人探索（ARE）涉及机器人自主导航和映射未知环境。</li>
<li>现有强化学习方法在处理图形结构化数据时存在推理能力有限的缺陷。</li>
<li>提出了一种基于深度强化学习（DRL）的方法GRATE，利用图转换器捕获信息图形的局部和全局特征。</li>
<li>GRATE方法增强了模型在整个环境中的推理能力。</li>
<li>采用卡尔曼滤波器平滑处理航点输出，确保机器人运动的动力学可行性。</li>
<li>实验结果表明，GRATE方法在仿真测试中优于其他方法，提高了探索效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12863">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12863v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12863v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12863v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12863v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="EvoEmpirBench-Dynamic-Spatial-Reasoning-with-Agent-ExpVer"><a href="#EvoEmpirBench-Dynamic-Spatial-Reasoning-with-Agent-ExpVer" class="headerlink" title="EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer"></a>EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer</h2><p><strong>Authors:Pukun Zhao, Longxiang Wang, Miaowei Wang, Chen Chen, Fanqing Zhou, Haojian Huang</strong></p>
<p>Most existing spatial reasoning benchmarks focus on static or globally observable environments, failing to capture the challenges of long-horizon reasoning and memory utilization under partial observability and dynamic changes. We introduce two dynamic spatial benchmarks, locally observable maze navigation and match-2 elimination that systematically evaluate models’ abilities in spatial understanding and adaptive planning when local perception, environment feedback, and global objectives are tightly coupled. Each action triggers structural changes in the environment, requiring continuous update of cognition and strategy. We further propose a subjective experience-based memory mechanism for cross-task experience transfer and validation. Experiments show that our benchmarks reveal key limitations of mainstream models in dynamic spatial reasoning and long-term memory, providing a comprehensive platform for future methodological advances. Our code and data are available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/EvoEmpirBench-143C/">https://anonymous.4open.science/r/EvoEmpirBench-143C/</a>. </p>
<blockquote>
<p>现有的大部分空间推理基准测试主要关注静态或全局可观察的环境，未能捕捉到部分可观察性和动态变化下长周期推理和内存使用的挑战。我们引入了两个动态空间基准测试，即局部可观察的迷宫导航和匹配-2消除，以系统地评估模型在局部感知、环境反馈和全局目标紧密结合时，在空间理解和自适应规划方面的能力。每个动作都会触发环境的变化，需要不断更新认知策略。我们还提出了一种基于主观经验的记忆机制，用于跨任务经验迁移和验证。实验表明，我们的基准测试揭示了主流模型在动态空间推理和长期记忆方面的关键局限性，为未来的方法论进步提供了一个全面的平台。我们的代码和数据可在[<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/EvoEmpirBench-143C/]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/EvoEmpirBench-143C/]上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12718v1">PDF</a> Ongoing Work, 29 pages, 3 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>该文介绍了现有的空间推理基准测试主要集中在静态或全局可观察的环境上，忽视了局部可观察性、动态变化下的长周期推理和记忆利用的挑战。为此，文章引入了两个动态空间基准测试：局部可观察的迷宫导航和匹配-2消除，以系统地评估模型在局部感知、环境反馈和全局目标紧密耦合情况下的空间理解和自适应规划能力。每个动作都会引起环境的结构性变化，需要不断更新认知和调整策略。此外，文章还提出了一种基于主观经验的记忆机制，用于跨任务经验迁移和验证。实验表明，这些基准测试揭示了主流模型在动态空间推理和长期记忆方面的关键局限性，为未来的方法进步提供了一个综合平台。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有空间推理基准测试主要集中在静态或全局可观察的环境上，存在局限性。</li>
<li>引入两个动态空间基准测试：局部可观察的迷宫导航和匹配-2消除。</li>
<li>这些基准测试要求模型在局部感知、环境反馈和全局目标的交互中展示空间理解和自适应规划能力。</li>
<li>环境结构会因动作而发生变化，需要模型持续更新认知和调整策略。</li>
<li>提出一种基于主观经验的记忆机制，用于跨任务经验迁移和验证。</li>
<li>实验结果显示，主流模型在动态空间推理和长期记忆方面存在关键局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12718">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12718v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12718v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12718v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12718v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12718v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ActiveVLN-Towards-Active-Exploration-via-Multi-Turn-RL-in-Vision-and-Language-Navigation"><a href="#ActiveVLN-Towards-Active-Exploration-via-Multi-Turn-RL-in-Vision-and-Language-Navigation" class="headerlink" title="ActiveVLN: Towards Active Exploration via Multi-Turn RL in   Vision-and-Language Navigation"></a>ActiveVLN: Towards Active Exploration via Multi-Turn RL in   Vision-and-Language Navigation</h2><p><strong>Authors:Zekai Zhang, Weiye Zhu, Hewei Pan, Xiangchen Wang, Rongtao Xu, Xing Sun, Feng Zheng</strong></p>
<p>The Vision-and-Language Navigation (VLN) task requires an agent to follow natural language instructions and navigate through complex environments. Existing MLLM-based VLN methods primarily rely on imitation learning (IL) and often use DAgger for post-training to mitigate covariate shift. While effective, these approaches incur substantial data collection and training costs. Reinforcement learning (RL) offers a promising alternative. However, prior VLN RL methods lack dynamic interaction with the environment and depend on expert trajectories for reward shaping, rather than engaging in open-ended active exploration. This restricts the agent’s ability to discover diverse and plausible navigation routes. To address these limitations, we propose ActiveVLN, a VLN framework that explicitly enables active exploration through multi-turn RL. In the first stage, a small fraction of expert trajectories is used for IL to bootstrap the agent. In the second stage, the agent iteratively predicts and executes actions, automatically collects diverse trajectories, and optimizes multiple rollouts via the GRPO objective. To further improve RL efficiency, we introduce a dynamic early-stopping strategy to prune long-tail or likely failed trajectories, along with additional engineering optimizations. Experiments show that ActiveVLN achieves the largest performance gains over IL baselines compared to both DAgger-based and prior RL-based post-training methods, while reaching competitive performance with state-of-the-art approaches despite using a smaller model. Code and data will be released soon. </p>
<blockquote>
<p>视觉与语言导航（VLN）任务要求智能体遵循自然语言指令并通过复杂环境进行导航。现有的基于多模态语言模型（MLLM）的VLN方法主要依赖于模仿学习（IL），并且经常使用DAgger进行后训练以缓解协变量偏移问题。虽然这些方法有效，但它们会产生大量的数据收集和培训成本。强化学习（RL）提供了一个有前景的替代方案。然而，先前的VLN RL方法缺乏与环境的动态交互，并且依赖于专家轨迹进行奖励塑造，而不是进行开放式的主动探索。这限制了智能体发现多样化和合理的导航路线的能力。为了克服这些局限性，我们提出了ActiveVLN，这是一个VLN框架，通过多回合RL明确实现主动探索。在第一阶段，一小部分专家轨迹用于IL来引导智能体。在第二阶段，智能体预测并执行动作，自动收集各种轨迹，并通过GRPO目标优化多次运行。为了进一步提高RL的效率，我们引入了一种动态早期停止策略来剔除长尾或可能失败的轨迹，以及其他工程优化措施。实验表明，ActiveVLN在IL基准测试上实现了最大的性能提升，超过了基于DAgger和先前的RL后处理方法，尽管使用了较小的模型，但其性能与最先进的方法具有竞争力。代码和数据将很快发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12618v1">PDF</a> </p>
<p><strong>Summary</strong>：视觉与语言导航（VLN）任务要求智能体根据自然语言指令在复杂环境中进行导航。现有基于大规模预训练语言模型的方法主要依赖模仿学习，并采用DAgger进行后训练以缓解协变量偏移问题。尽管有效，这些方法涉及大量的数据收集和培训成本。强化学习提供了一个有前途的替代方案，但先前的VLN强化学习方法缺乏与环境的动态交互，并依赖专家轨迹进行奖励塑形，而非进行开放式主动探索。这限制了智能体发现多样性和可行的导航路线的能力。针对这些局限性，我们提出了ActiveVLN框架，它明确支持通过多轮强化学习进行主动探索。在第一阶段，我们使用一小部分专家轨迹进行模仿学习来引导智能体。在第二阶段，智能体预测并执行动作，自动收集各种轨迹，并通过GRPO目标优化多次运行。为了进一步提高强化学习的效率，我们引入了一种动态早期停止策略来删除长尾或可能失败的轨迹，以及其他工程优化。实验表明，ActiveVLN相较于基于DAgger的方法和先前的强化学习后处理方法取得了最大的性能提升，并在较小的模型下达到了与最新方法相当的性能。代码和数据将很快发布。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>VLN任务要求智能体根据自然语言指令在复杂环境中导航。</li>
<li>现有方法主要依赖模仿学习和DAgger后训练。</li>
<li>强化学习为VLN任务提供了有前景的替代方案。</li>
<li>先前的强化学习方法缺乏与环境的动态交互和开放式主动探索。</li>
<li>ActiveVLN框架支持通过多轮强化学习进行主动探索，结合模仿学习和强化学习。</li>
<li>ActiveVLN通过GRPO目标和动态早期停止策略优化轨迹收集和优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12618">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12618v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12618v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12618v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12618v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12618v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving"><a href="#EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving" class="headerlink" title="EconProver: Towards More Economical Test-Time Scaling for Automated   Theorem Proving"></a>EconProver: Towards More Economical Test-Time Scaling for Automated   Theorem Proving</h2><p><strong>Authors:Mukai Li, Linfeng Song, Zhenwen Liang, Jiahao Xu, Shansan Gong, Qi Liu, Haitao Mi, Dong Yu</strong></p>
<p>Large Language Models (LLMs) have recently advanced the field of Automated Theorem Proving (ATP), attaining substantial performance gains through widely adopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT) reasoning and increased sampling passes. However, they both introduce significant computational overhead for inference. Moreover, existing cost analyses typically regulate only the number of sampling passes, while neglecting the substantial disparities in sampling costs introduced by different scaling strategies. In this paper, we systematically compare the efficiency of different test-time scaling strategies for ATP models and demonstrate the inefficiency of the current state-of-the-art (SOTA) open-source approaches. We then investigate approaches to significantly reduce token usage and sample passes while maintaining the original performance. Specifically, we propose two complementary methods that can be integrated into a unified EconRL pipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching mechanism designed to mitigate unnecessary token consumption, and (2) Diverse parallel-scaled reinforcement learning (RL) with trainable prefixes to enhance pass rates under constrained sampling passes. Experiments on miniF2F and ProofNet demonstrate that our EconProver achieves comparable performance to baseline methods with only 12% of the computational cost. This work provides actionable insights for deploying lightweight ATP models without sacrificing performance. </p>
<blockquote>
<p>大型语言模型（LLM）最近推动了自动化定理证明（ATP）领域的发展，通过广泛采用的测试时缩放策略，取得了显著的性能提升，特别是反射式思维链（CoT）推理和增加采样通道。然而，它们都为推理引入了重大的计算开销。此外，现有的成本分析通常只控制采样通道的数量，而忽视不同缩放策略带来的采样成本差异。在本文中，我们系统地比较了ATP模型的测试时缩放策略的效率，并展示了当前最新开源方法的不高效性。然后，我们研究了减少符号使用和样本通道的方法，同时保持原始性能。具体来说，我们提出了两种可以集成到统一的EconRL管道以获取额外收益的方法：一是动态思维链（CoT）切换机制，旨在减少不必要的符号消耗；二是具有可训练前缀的多样化并行缩放强化学习（RL），以提高受限采样通道下的通过率。在miniF2F和ProofNet上的实验表明，我们的EconProver在仅计算成本的12%的情况下实现了与基准方法相当的性能。这项工作为部署不牺牲性能的轻量级ATP模型提供了切实可行的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12603v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>大型语言模型在自动定理证明领域取得了显著的性能提升，但测试时的缩放策略如反射式思维链和增加采样次数等引入了大量计算开销。本文系统地比较了不同测试时缩放策略的效率，并探讨了降低标记使用量和采样次数的方法，同时保持原始性能。通过动态思维链切换和多样化并行强化学习等方法，我们提出了一个统一的EconProver管道，实现了轻量级自动定理证明模型的高性能。实验表明，EconProver与基准方法相比，计算成本降低了仅12%。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型在自动定理证明领域通过测试时缩放策略取得了性能提升。</li>
<li>现有成本分析通常只关注采样次数，忽略了不同缩放策略带来的采样成本差异。</li>
<li>本文系统地比较了不同测试时缩放策略的效率，并指出了当前先进方法的计算效率问题。</li>
<li>提出了一个统一的EconProver管道来降低标记使用量和采样次数，维持原始性能。</li>
<li>通过动态思维链切换机制缓解不必要的标记消耗。</li>
<li>通过多样化并行强化学习增强在有限的采样次数下的通过率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12603">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12603v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12603v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12603v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Evaluating-Robustness-of-Vision-Language-Models-Under-Noisy-Conditions"><a href="#Evaluating-Robustness-of-Vision-Language-Models-Under-Noisy-Conditions" class="headerlink" title="Evaluating Robustness of Vision-Language Models Under Noisy Conditions"></a>Evaluating Robustness of Vision-Language Models Under Noisy Conditions</h2><p><strong>Authors: Purushoth,  Alireza</strong></p>
<p>Vision-Language Models (VLMs) have attained exceptional success across multimodal tasks such as image captioning and visual question answering. However, their robustness under noisy conditions remains unfamiliar. In this study, we present a comprehensive evaluation framework to evaluate the performance of several state-of-the-art VLMs under controlled perturbations, including lighting variation, motion blur, and compression artifacts. We used both lexical-based metrics (BLEU, METEOR, ROUGE, CIDEr) and neural-based similarity measures using sentence embeddings to quantify semantic alignment. Our experiments span diverse datasets, revealing key insights: (1) descriptiveness of ground-truth captions significantly influences model performance; (2) larger models like LLaVA excel in semantic understanding but do not universally outperform smaller models; and (3) certain noise types, such as JPEG compression and motion blur, dramatically degrade performance across models. Our findings highlight the nuanced trade-offs between model size, dataset characteristics, and noise resilience, offering a standardized benchmark for future robust multimodal learning. </p>
<blockquote>
<p>视觉语言模型（VLMs）在多模态任务（如图像描述和视觉问答）中取得了异常成功。然而，它们在噪声条件下的稳健性仍然未知。本研究中，我们提出了一个全面的评估框架，以评估最前沿的VLMs在受控扰动下的性能，包括光照变化、运动模糊和压缩伪影。我们使用了基于词汇的度量标准（BLEU、METEOR、ROUGE、CIDEr）和基于神经的句子嵌入相似性度量来量化语义对齐。我们的实验涵盖了各种数据集，揭示了关键见解：（1）真实场景描述的描述性显著影响模型性能；（2）大型模型（如LLaVA）在语义理解方面表现出色，但并不普遍优于小型模型；（3）某些噪声类型，如JPEG压缩和运动模糊，会显著影响模型的性能。我们的研究突出了模型大小、数据集特征和噪声稳健性之间的微妙权衡，为未来稳健的多模态学习提供了标准化的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12492v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文对跨模态任务中的视觉语言模型（VLMs）进行了全面的评估，涉及光照变化、运动模糊和压缩伪影等受控扰动。研究采用词汇基础和基于神经的句子嵌入相似性度量来量化语义对齐。实验涵盖不同的数据集，揭示了模型性能的关键影响因素：真实标签描述的影响、大型模型如LLaVA的语义理解优势，以及特定噪声类型（如JPEG压缩和运动模糊）对模型性能的显著影响。该研究为未来的稳健跨模态学习提供了标准化的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>真实标签描述的详细程度对模型性能有显著影响。</li>
<li>大型模型如LLaVA在语义理解方面表现出优势，但并不总是优于小型模型。</li>
<li>特定类型的噪声（如JPEG压缩和运动模糊）会显著影响模型的性能。</li>
<li>模型大小、数据集特性和噪声韧性之间存在微妙的权衡。</li>
<li>该研究提供了一个全面的评估框架，用于评估视觉语言模型在多种条件下的性能。</li>
<li>采用词汇基础和基于神经的句子嵌入相似性度量来量化语义对齐的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12492">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12492v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12492v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12492v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12492v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12492v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Evaluating-Large-Language-Models-for-Functional-and-Maintainable-Code-in-Industrial-Settings-A-Case-Study-at-ASML"><a href="#Evaluating-Large-Language-Models-for-Functional-and-Maintainable-Code-in-Industrial-Settings-A-Case-Study-at-ASML" class="headerlink" title="Evaluating Large Language Models for Functional and Maintainable Code in   Industrial Settings: A Case Study at ASML"></a>Evaluating Large Language Models for Functional and Maintainable Code in   Industrial Settings: A Case Study at ASML</h2><p><strong>Authors:Yash Mundhra, Max Valk, Maliheh Izadi</strong></p>
<p>Large language models have shown impressive performance in various domains, including code generation across diverse open-source domains. However, their applicability in proprietary industrial settings, where domain-specific constraints and code interdependencies are prevalent, remains largely unexplored. We present a case study conducted in collaboration with the leveling department at ASML to investigate the performance of LLMs in generating functional, maintainable code within a closed, highly specialized software environment.   We developed an evaluation framework tailored to ASML’s proprietary codebase and introduced a new benchmark. Additionally, we proposed a new evaluation metric, build@k, to assess whether LLM-generated code successfully compiles and integrates within real industrial repositories. We investigate various prompting techniques, compare the performance of generic and code-specific LLMs, and examine the impact of model size on code generation capabilities, using both match-based and execution-based metrics. The findings reveal that prompting techniques and model size have a significant impact on output quality, with few-shot and chain-of-thought prompting yielding the highest build success rates. The difference in performance between the code-specific LLMs and generic LLMs was less pronounced and varied substantially across different model families. </p>
<blockquote>
<p>大型语言模型在包括跨多种开源域的代码生成在内的各个领域表现出令人印象深刻的性能。然而，它们在专有工业环境中的适用性，特别是存在特定领域约束和代码相互依赖性的环境，仍很少被探索。我们与ASML的标准化部门合作进行了一项案例研究，以调查大型语言模型在封闭、高度专业化的软件环境中生成功能性强、可维护的代码的性能。我们针对ASML的专有代码库开发了一个评估框架，并引入了一个新的基准测试。此外，我们提出了一种新的评估指标build@k，以评估大型语言模型生成的代码是否能够在真实的工业存储库中成功编译和集成。我们研究了各种提示技术，比较了通用和特定代码的大型语言模型的性能，并考察了模型大小对代码生成能力的影响，采用基于匹配和基于执行两种指标进行评估。研究结果表明，提示技术和模型大小对输出质量有重大影响，其中少样本和思维链提示产生的构建成功率最高。特定代码的大型语言模型和通用大型语言模型之间的性能差异不太明显，并且在不同的模型家族之间存在很大差异。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12395v1">PDF</a> Accepted in the 40th IEEE&#x2F;ACM International Conference on Automated   Software Engineering, ASE 2025 (Industry track)</p>
<p><strong>Summary</strong></p>
<p>大型语言模型在开源领域的代码生成方面表现出卓越性能，但在专有工业环境中的适用性仍待探索。本研究与ASML的等级部门合作，研究LLMs在封闭、高度专业化的软件环境中生成功能性强、可维护的代码的性能。研究通过定制的评价框架和新的基准测试，提出新的评估指标“build@k”，以评估LLM生成的代码是否能在真实的工业存储库中成功编译和集成。研究发现，提示技术和模型大小对输出质量有重大影响，以少量样本和连贯思维提示生成代码成功率最高。代码特定LLMs与通用LLMs之间的性能差异并不显著，并且在不同的模型家族中差异很大。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在专有工业环境中的代码生成适用性待探索。</li>
<li>与ASML等级部门的合作研究评估了LLMs在特定软件环境中的性能。</li>
<li>定制的评价框架和新的基准测试用于评估LLM生成的代码。</li>
<li>提出的“build@k”评估指标能判断代码是否能在工业存储库中成功编译和集成。</li>
<li>提示技术和模型大小对LLM输出代码的质量有重大影响。</li>
<li>少量样本和连贯思维提示生成代码成功率最高。</li>
<li>代码特定LLMs与通用LLMs之间的性能差异在不同模型家族中表现不同。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12395">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12395v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12395v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12395v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12395v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12395v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LLM-as-a-Judge-Rapid-Evaluation-of-Legal-Document-Recommendation-for-Retrieval-Augmented-Generation"><a href="#LLM-as-a-Judge-Rapid-Evaluation-of-Legal-Document-Recommendation-for-Retrieval-Augmented-Generation" class="headerlink" title="LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for   Retrieval-Augmented Generation"></a>LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for   Retrieval-Augmented Generation</h2><p><strong>Authors:Anu Pradhan, Alexandra Ortan, Apurv Verma, Madhavan Seshadri</strong></p>
<p>The evaluation bottleneck in recommendation systems has become particularly acute with the rise of Generative AI, where traditional metrics fall short of capturing nuanced quality dimensions that matter in specialized domains like legal research. Can we trust Large Language Models to serve as reliable judges of their own kind? This paper investigates LLM-as-a-Judge as a principled approach to evaluating Retrieval-Augmented Generation systems in legal contexts, where the stakes of recommendation quality are exceptionally high.   We tackle two fundamental questions that determine practical viability: which inter-rater reliability metrics best capture the alignment between LLM and human assessments, and how do we conduct statistically sound comparisons between competing systems? Through systematic experimentation, we discover that traditional agreement metrics like Krippendorff’s alpha can be misleading in the skewed distributions typical of AI system evaluations. Instead, Gwet’s AC2 and rank correlation coefficients emerge as more robust indicators for judge selection, while the Wilcoxon Signed-Rank Test with Benjamini-Hochberg corrections provides the statistical rigor needed for reliable system comparisons.   Our findings suggest a path toward scalable, cost-effective evaluation that maintains the precision demanded by legal applications, transforming what was once a human-intensive bottleneck into an automated, yet statistically principled, evaluation framework. </p>
<blockquote>
<p>随着生成式人工智能的兴起，推荐系统中的评估瓶颈问题变得尤为突出。在传统指标无法捕捉法律研究等特定领域中的微妙质量维度的情况下，我们能信任大型语言模型充当同类之间的可靠评判者吗？本文调查了LLM作为法官的方法，作为评估法律背景下的检索增强生成系统的原则性方法，其中推荐质量的赌注非常高。我们解决了两个决定实际可行性的基本问题：哪些评鉴者之间的一致性指标最能捕捉LLM和人类评估之间的对齐情况，以及我们如何在竞争系统之间进行统计上健全的比较？通过系统实验，我们发现传统的协议指标如Krippendorff的alpha在典型的AI系统评估的偏态分布中可能会误导人。相反，Gwet的AC2和秩相关系数作为法官选择的更稳健指标而出现，而使用Benjamini-Hochberg修正的Wilcoxon符号秩检验为可靠的系统比较提供了所需的统计严谨性。我们的研究结果表明了一条道路，朝着可扩展且经济实惠的评估方式发展，同时保持法律应用所要求的精度，将曾经的人力密集瓶颈转变为自动化但统计上原则性的评估框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12382v1">PDF</a> Accepted in EARL 25: The 2nd Workshop on Evaluating and Applying   Recommender Systems with Large Language Models at RecSys 2025</p>
<p><strong>Summary</strong><br>在生成式人工智能的推动下，推荐系统的评估瓶颈愈发突出。传统评价指标难以捕捉专业领域中微妙的品质维度。本文探讨了LLM作为法官的方法，以评估增强检索生成系统在法律背景下的实用性。通过系统实验，发现传统的一致性指标如Krippendorff的alpha在某些情况下具有误导性。相反，Gwet的AC2和秩相关系数是更稳健的评估指标。同时，使用Wilcoxon符号秩检验和Benjamini-Hochberg校正为可靠的系統比较提供了统计严谨性。研究为规模化、经济效益的评价提供了路径，满足法律应用的精确性要求，将原本的人力瓶颈转化为自动化、统计原则的评价框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>推荐系统的评估面临瓶颈，特别是在法律研究等特定领域，传统评价指标无法充分捕捉微妙的品质维度。</li>
<li>LLM作为法官的方法被提出，旨在评估增强检索生成系统在法律背景下的实用性。</li>
<li>系统实验发现Krippendorff的alpha等传统一致性指标在某些情况下具有误导性。</li>
<li>Gwet的AC2和秩相关系数被认定为更稳健的评估指标。</li>
<li>Wilcoxon符号秩检验结合Benjamini-Hochberg校正为系统比较提供了统计严谨性。</li>
<li>研究结果有助于建立规模化、高效的评价方法，满足法律应用的高精度要求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12382">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12382v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12382v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12382v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12382v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12382v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MORABLES-A-Benchmark-for-Assessing-Abstract-Moral-Reasoning-in-LLMs-with-Fables"><a href="#MORABLES-A-Benchmark-for-Assessing-Abstract-Moral-Reasoning-in-LLMs-with-Fables" class="headerlink" title="MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs   with Fables"></a>MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs   with Fables</h2><p><strong>Authors:Matteo Marcuzzo, Alessandro Zangari, Andrea Albarelli, Jose Camacho-Collados, Mohammad Taher Pilehvar</strong></p>
<p>As LLMs excel on standard reading comprehension benchmarks, attention is shifting toward evaluating their capacity for complex abstract reasoning and inference. Literature-based benchmarks, with their rich narrative and moral depth, provide a compelling framework for evaluating such deeper comprehension skills. Here, we present MORABLES, a human-verified benchmark built from fables and short stories drawn from historical literature. The main task is structured as multiple-choice questions targeting moral inference, with carefully crafted distractors that challenge models to go beyond shallow, extractive question answering. To further stress-test model robustness, we introduce adversarial variants designed to surface LLM vulnerabilities and shortcuts due to issues such as data contamination. Our findings show that, while larger models outperform smaller ones, they remain susceptible to adversarial manipulation and often rely on superficial patterns rather than true moral reasoning. This brittleness results in significant self-contradiction, with the best models refuting their own answers in roughly 20% of cases depending on the framing of the moral choice. Interestingly, reasoning-enhanced models fail to bridge this gap, suggesting that scale - not reasoning ability - is the primary driver of performance. </p>
<blockquote>
<p>随着大型语言模型（LLMs）在标准阅读理解能力基准测试中的出色表现，人们开始更关注它们在复杂抽象推理和推断方面的能力评估。基于文献的基准测试，以其丰富的叙事和道德深度，为评估这样的深层次理解技能提供了一个有吸引力的框架。在这里，我们介绍了MORABLES，这是一个经过人工验证的基准测试，它建立在从历史文献中提取的寓言和短篇故事之上。主要任务是以针对道德推断的多项选择题的形式进行，精心设计了一些干扰选项，以挑战模型超越浅层次的、提取式的问题回答能力。为了进一步测试模型的稳健性，我们引入了对抗性变体，旨在暴露由于数据污染等问题导致的LLM的脆弱性和捷径。我们的研究发现，虽然大型模型的性能优于小型模型，但它们仍然容易受到对抗性操作的干扰，并且往往依赖于表面模式，而非真正的道德推理。这种脆弱性导致了显著的自我矛盾，在道德选择的不同表述下，最佳模型约有20%的情况下会反驳自己的答案。有趣的是，增强推理能力的模型未能缩小这一差距，这表明性能的主要驱动因素是规模而非推理能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12371v1">PDF</a> Accepted to EMNLP 2025 Main Conference</p>
<p><strong>Summary</strong><br>基于大型语言模型（LLM）在阅读理解标准上的出色表现，人们开始关注其在复杂抽象推理和推断方面的能力评估。为了评估更深层次的理解技能，我们提出了MORABLES基准测试，该测试通过人类验证，基于历史故事和短故事的道德内涵丰富构建而成。主要任务是以针对道德推理的多项选择题形式进行，精心设计干扰项以挑战模型超越浅层次的提取式问答能力。为了测试模型的稳健性，我们引入了对抗性变体，旨在揭示由于数据污染等问题导致的LLM漏洞和捷径。研究发现，虽然大型模型的性能优于小型模型，但它们仍然容易受到对抗性操纵的影响，并经常依赖于表面模式而非真正的道德推理。这种脆弱性导致显著的自我矛盾，最佳模型在约20%的情况下会因道德选择的不同而否定自己的答案。有趣的是，增强推理能力的模型无法缩小这一差距，这表明性能的主要驱动因素是规模而非推理能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在阅读理解标准上表现优异，现在人们更关注其在复杂抽象推理和推断方面的能力。</li>
<li>MORABLES是一个基于历史文献的叙事和道德深度构建的基准测试，用于评估模型更深层次的道德推理能力。</li>
<li>MORABLES的主要任务是多项选择题形式的结构化任务，旨在挑战模型超越浅层次的提取式问答能力。</li>
<li>对抗性变体被用来测试模型的稳健性，揭示LLM的漏洞和捷径。</li>
<li>大型模型虽然性能较好，但仍存在脆弱性，容易受到对抗性操纵的影响。</li>
<li>模型在道德推理方面存在自我矛盾现象，最佳模型在约20%的情况下会因道德选择的不同而否定自己的答案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12371">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12371v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12371v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12371v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12371v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12371v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="When-Safe-Unimodal-Inputs-Collide-Optimizing-Reasoning-Chains-for-Cross-Modal-Safety-in-Multimodal-Large-Language-Models"><a href="#When-Safe-Unimodal-Inputs-Collide-Optimizing-Reasoning-Chains-for-Cross-Modal-Safety-in-Multimodal-Large-Language-Models" class="headerlink" title="When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for   Cross-Modal Safety in Multimodal Large Language Models"></a>When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for   Cross-Modal Safety in Multimodal Large Language Models</h2><p><strong>Authors:Wei Cai, Shujuan Liu, Jian Zhao, Ziyan Shi, Yusheng Zhao, Yuchen Yuan, Tianle Zhang, Chi Zhang, Xuelong Li</strong></p>
<p>Multimodal Large Language Models (MLLMs) are susceptible to the implicit reasoning risk, wherein innocuous unimodal inputs synergistically assemble into risky multimodal data that produce harmful outputs. We attribute this vulnerability to the difficulty of MLLMs maintaining safety alignment through long-chain reasoning. To address this issue, we introduce Safe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring interpretable reasoning paths tailored for such a cross-modal challenge. A novel training framework, Safety-aware Reasoning Path Optimization (SRPO), is also designed based on the SSUI dataset to align the MLLM’s internal reasoning process with human safety values. Experimental results show that our SRPO-trained models achieve state-of-the-art results on key safety benchmarks, including the proposed Reasoning Path Benchmark (RSBench), significantly outperforming both open-source and top-tier commercial MLLMs. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）容易受到隐式推理风险的影响，其中无害的单模态输入会协同组合成风险性多模态数据，从而产生有害输出。我们将这种脆弱性归因于MLLMs在通过长链推理保持安全对齐方面的困难。为了解决这一问题，我们引入了Safe-Semantics-but-Unsafe-Interpretation（SSUI），这是第一个具有可解释推理路径的特定数据集，旨在应对此类跨模态挑战。还设计了基于SSUI数据集的新型训练框架Safety-aware Reasoning Path Optimization（SRPO），以使MLLM的内部推理过程与人类安全价值观保持一致。实验结果表明，经过SRPO训练的模型在关键安全基准测试中实现了最新技术成果，包括提出的Reasoning Path Benchmark（RSBench），显著优于开源和顶级商业MLLMs。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12060v2">PDF</a> </p>
<p><strong>Summary</strong>：多模态大型语言模型（MLLMs）面临隐式推理风险，即无害的单模态输入协同形成风险性多模态数据，产生有害输出。针对这一挑战，我们提出了Safe-Semantics-but-Unsafe-Interpretation（SSUI）数据集和基于该数据集的Safety-aware Reasoning Path Optimization（SRPO）训练框架，以将MLLM的内部推理过程与人类安全价值观对齐。实验结果表明，SRPO训练的模型在关键安全基准测试中取得了最新成果。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>多模态大型语言模型（MLLMs）面临隐式推理风险。</li>
<li>无害的单模态输入可以组合成有害的多模态数据。</li>
<li>MLLMs在保持长期推理安全方面存在困难。</li>
<li>引入Safe-Semantics-but-Unsafe-Interpretation（SSUI）数据集以应对这一挑战。</li>
<li>设计了基于SSUI数据集的Safety-aware Reasoning Path Optimization（SRPO）训练框架。</li>
<li>SRPO训练模型在关键安全基准测试中表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12060">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12060v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12060v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12060v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12060v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.12060v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Agentic-Lybic-Multi-Agent-Execution-System-with-Tiered-Reasoning-and-Orchestration"><a href="#Agentic-Lybic-Multi-Agent-Execution-System-with-Tiered-Reasoning-and-Orchestration" class="headerlink" title="Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and   Orchestration"></a>Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and   Orchestration</h2><p><strong>Authors:Liangxuan Guo, Bin Zhu, Qingqian Tao, Kangning Liu, Xun Zhao, Xianzhe Qin, Jin Gao, Guangfu Hao</strong></p>
<p>Autonomous agents for desktop automation struggle with complex multi-step tasks due to poor coordination and inadequate quality control. We introduce Agentic Lybic, a novel multi-agent system where the entire architecture operates as a finite-state machine (FSM). This core innovation enables dynamic orchestration. Our system comprises four components: a Controller, a Manager, three Workers (Technician for code-based operations, Operator for GUI interactions, and Analyst for decision support), and an Evaluator. The critical mechanism is the FSM-based routing between these components, which provides flexibility and generalization by dynamically selecting the optimal execution strategy for each subtask. This principled orchestration, combined with robust quality gating, enables adaptive replanning and error recovery. Evaluated officially on the OSWorld benchmark, Agentic Lybic achieves a state-of-the-art 57.07% success rate in 50 steps, substantially outperforming existing methods. Results demonstrate that principled multi-agent orchestration with continuous quality control provides superior reliability for generalized desktop automation in complex computing environments. </p>
<blockquote>
<p>自主代理在桌面自动化中处理复杂的多步骤任务时，由于协调性差和质量控制不足而遇到困难。我们引入了Agentic Lybic，这是一种新型的多代理系统，整个架构作为一个有限状态机（FSM）运行。这一核心创新实现了动态协同。我们的系统由四个组件组成：控制器、管理器、三个工作者（技术工人负责基于代码的操作、操作员负责GUI交互、分析师负责决策支持），以及评估器。关键机制是这些组件之间的基于FSM的路由，它通过动态选择每个子任务的最佳执行策略来提供灵活性和通用性。这种原则性的协同，结合强大的质量门控，能够实现自适应的重新规划和错误恢复。在OSWorld基准测试上进行官方评估，Agentic Lybic在50步内达到了最先进的57.07%的成功率，大大优于现有方法。结果表明，在复杂的计算环境中，具有持续质量控制的原则性多代理协同为通用的桌面自动化提供了更高的可靠性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11067v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文章介绍了Agentic Lybic这一新型多智能体系统，该系统采用有限状态机（FSM）作为核心架构，实现了动态协同作业。系统包括控制器、管理器、三个工作者（技术工人负责代码操作、操作工人负责界面交互、分析工人负责决策支持），以及评估器。关键机制在于基于FSM的工作流程调度，能针对每个子任务动态选择最佳执行策略，提供灵活性和通用性。结合强大的质量门控机制，可实现自适应规划和错误恢复。在OSWorld基准测试中，Agentic Lybic达到了业界领先的57.07%的50步成功率，显著优于现有方法。结果表明，具有持续质量控制的多智能体协同作业能为复杂计算环境下的桌面自动化提供更高的可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Agentic Lybic是一个新型的多智能体系统，采用有限状态机作为核心架构。</li>
<li>系统包括控制器、管理器、三个工作者（技术工人、操作工人、分析工人）和评估器。</li>
<li>基于FSM的工作流程调度是系统的关键机制，为任务提供灵活性和通用性。</li>
<li>系统能够实现动态协同作业，并具有强大的质量门控机制，支持自适应规划和错误恢复。</li>
<li>Agentic Lybic在OSWorld基准测试中实现了业界领先的成功率。</li>
<li>相比现有方法，具有持续质量控制的多智能体协同作业在复杂计算环境下的桌面自动化中表现出更高的可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11067">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.11067v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.11067v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MachineLearningLM-Scaling-Many-shot-In-context-Learning-via-Continued-Pretraining"><a href="#MachineLearningLM-Scaling-Many-shot-In-context-Learning-via-Continued-Pretraining" class="headerlink" title="MachineLearningLM: Scaling Many-shot In-context Learning via Continued   Pretraining"></a>MachineLearningLM: Scaling Many-shot In-context Learning via Continued   Pretraining</h2><p><strong>Authors:Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke</strong></p>
<p>Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows.   Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference.   Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU. </p>
<blockquote>
<p>大型语言模型（LLM）拥有广泛的世界知识和强大的通用推理能力，但在标准机器学习（ML）任务上，它们很难从多个上下文示例中学习，也就是说，它们无法仅通过上下文学习（ICL）利用多个示例演示，而无需进行梯度下降。我们引入了MachineLearningLM，这是一个便携式持续预训练框架，它使通用LLM具备强大的上下文ML能力，同时保留其一般知识和推理能力，以支持更广泛的聊天工作流程。我们的预训练程序从数百万个结构因果模型（SCM）中综合ML任务，涵盖示例数量高达1024个。我们以随机森林教师开始，将基于树的决策策略蒸馏到LLM中，以加强数值建模中的稳健性。所有任务都通过高效的令牌提示进行序列化，可以在每个上下文窗口中实现3到6倍的更多示例，并通过批量推理实现高达50倍的摊销吞吐量。尽管采用了适度的设置（Qwen-2.5-7B-Instruct带有LoRA等级8），但MachineLearningLM在财务、物理、生物和医疗保健领域的分布外的表格分类任务上，相对于强大的LLM基线（如GPT-5-mini）平均提高了约15%的准确率。它表现出惊人的多示例扩展定律：随着上下文演示从8个增加到1024个，准确率单调增加。无需任何特定任务的训练，它在数百次射击中就能达到随机森林级别的精度。同时保留了一般聊天功能，包括知识和推理能力：它在MMLU上达到了75.4%的准确率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06806v5">PDF</a> </p>
<p><strong>摘要</strong><br>大型语言模型（LLM）具备广泛的世界知识和强大的通用推理能力，但在标准机器学习（ML）任务上学习多个上下文示例时遇到困难。本文介绍了MachineLearningLM，这是一个便携式持续预训练框架，旨在为通用LLM提供强大的上下文ML能力，同时保留其一般知识和推理能力以应对更广泛的聊天工作流程。预训练过程通过合成来自数百万结构因果模型（SCM）的ML任务进行，涵盖示例数量高达1024个。本文采用随机森林教师，将基于树的决策策略提炼成LLM，以增强数值建模中的稳健性。所有任务都采用高效的令牌提示进行序列化，可以在每个上下文窗口中提供3倍至6倍的示例，并通过批量推理实现高达50倍的摊销吞吐量。尽管采用了适度的设置（Qwen-2.5-7B-Instruct with LoRA rank 8），但MachineLearningLM在金融、物理、生物和医疗保健领域的离分布表格分类上平均优于强大的LLM基线（如GPT-5-mini）约15%。它呈现出令人印象深刻的许多镜头规模定律：随着上下文演示从8个增长到1024个，准确性单调增加。无需任何特定任务训练，即可在数百个镜头上实现随机森林级别的精度。同时保留了一般聊天能力，包括知识和推理，在MMLU上达到75.4%。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLM）虽然拥有广泛的知识和推理能力，但在标准机器学习任务上学习多个上下文示例时面临挑战。</li>
<li>MachineLearningLM框架通过持续预训练增强了LLM的上下文机器学习能力，同时保留其一般知识和推理能力。</li>
<li>预训练过程通过合成来自结构因果模型（SCM）的ML任务进行，涵盖的示例数量最多可达1024个。</li>
<li>使用随机森林教师来强化LLM在数值建模中的稳健性。</li>
<li>MachineLearningLM在多个领域（包括金融、物理、生物和医疗保健）的离分布表格分类任务上表现出卓越性能，平均优于现有强大LLM基线约15%。</li>
<li>该框架展现出显著的多镜头规模定律，随着上下文演示数量的增加，准确性不断提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06806">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.06806v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.06806v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_R1_Reasoning/2509.06806v5/page_4_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-18/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-18/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-18/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_LLM/2509.13310v1/page_2_0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-09-18  Scaling Agents via Continual Pre-training
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-18/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-18\./crop_Talking Head Generation/2411.19331v3/page_4_0.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-09-18  Talking to DINO Bridging Self-Supervised Vision Backbones with Language   for Open-Vocabulary Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
