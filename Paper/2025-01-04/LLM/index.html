<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-04  A Survey on Large Language Model Acceleration based on KV Cache   Management">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b5615297355166f8ca440a1e03b2419e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-04-æ›´æ–°"><a href="#2025-01-04-æ›´æ–°" class="headerlink" title="2025-01-04 æ›´æ–°"></a>2025-01-04 æ›´æ–°</h1><h2 id="A-Survey-on-Large-Language-Model-Acceleration-based-on-KV-Cache-Management"><a href="#A-Survey-on-Large-Language-Model-Acceleration-based-on-KV-Cache-Management" class="headerlink" title="A Survey on Large Language Model Acceleration based on KV Cache   Management"></a>A Survey on Large Language Model Acceleration based on KV Cache   Management</h2><p><strong>Authors:Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, Lei Chen</strong></p>
<p>Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \href{<a target="_blank" rel="noopener" href="https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management%7D%7Bhttps://github.com/TreeAI-Lab/Awesome-KV-Cache-Management%7D">https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å› å…¶å¯¹ä¸Šä¸‹æ–‡çš„ç†è§£å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œå·²åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ç­‰å¤šä¸ªé¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ã€‚ç„¶è€Œï¼ŒLLMçš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä½¿å…¶åœ¨æ‰©å±•åˆ°ç°å®ä¸–ç•Œã€é•¿ä¸Šä¸‹æ–‡å’Œå®æ—¶åº”ç”¨æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ç®¡ç†å·²æˆä¸ºåŠ é€ŸLLMæ¨ç†çš„å…³é”®ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡å‡å°‘å†—ä½™è®¡ç®—å’Œæé«˜å†…å­˜åˆ©ç”¨ç‡æ¥åŠ é€ŸLLMæ¨ç†ã€‚æœ¬æ–‡å…¨é¢æ¦‚è¿°äº†ç”¨äºLLMåŠ é€Ÿçš„KVç¼“å­˜ç®¡ç†ç­–ç•¥ï¼Œå°†å…¶åˆ†ç±»ä¸ºä»¤ç‰Œçº§ã€æ¨¡å‹çº§å’Œç³»ç»Ÿçº§ä¼˜åŒ–ã€‚ä»¤ç‰Œçº§ç­–ç•¥åŒ…æ‹¬KVç¼“å­˜é€‰æ‹©ã€é¢„ç®—åˆ†é…ã€åˆå¹¶ã€é‡åŒ–å’Œä½ç§©åˆ†è§£ï¼Œè€Œæ¨¡å‹çº§ä¼˜åŒ–åˆ™ä¾§é‡äºæ¶æ„åˆ›æ–°å’Œæ³¨æ„åŠ›æœºåˆ¶ä»¥æé«˜KVçš„é‡å¤ä½¿ç”¨ã€‚ç³»ç»Ÿçº§æ–¹æ³•åˆ™é’ˆå¯¹å†…å­˜ç®¡ç†ã€è°ƒåº¦å’Œç¡¬ä»¶æ„ŸçŸ¥è®¾è®¡ï¼Œä»¥æé«˜è·¨ä¸åŒè®¡ç®—ç¯å¢ƒçš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¦‚è¿°äº†ç”¨äºè¯„ä¼°è¿™äº›ç­–ç•¥çš„æ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ã€‚é€šè¿‡æä¾›è¯¦ç»†çš„åˆ†ç±»å’Œæ¯”è¾ƒåˆ†æï¼Œæœ¬æ–‡æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›æœ‰ç”¨çš„è§è§£ï¼Œä»¥æ”¯æŒå¼€å‘é«˜æ•ˆå’Œå¯æ‰©å±•çš„KVç¼“å­˜ç®¡ç†æŠ€å·§ï¼Œä¸ºLLMåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å®é™…éƒ¨ç½²åšå‡ºè´¡çŒ®ã€‚å…³äºKVç¼“å­˜ç®¡ç†çš„ç²¾é€‰è®ºæ–‡åˆ—è¡¨è¯·å‚è§ï¼š<a target="_blank" rel="noopener" href="https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management">https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19442v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šé¢†åŸŸå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­å¸¦æ¥é©å‘½æ€§å˜åŒ–ï¼Œä½†å…¶è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†é˜¶æ®µï¼Œå¯¹ç°å®ä¸–ç•Œã€é•¿ä¸Šä¸‹æ–‡å’Œå®æ—¶åº”ç”¨æ„æˆæŒ‘æˆ˜ã€‚é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ç®¡ç†ä½œä¸ºä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡å‡å°‘å†—ä½™è®¡ç®—å’Œæé«˜å†…å­˜åˆ©ç”¨ç‡æ¥åŠ é€ŸLLMæ¨ç†ã€‚æœ¬æ–‡ç»¼è¿°äº†KVç¼“å­˜ç®¡ç†ç­–ç•¥ä»¥åŠ é€ŸLLMçš„å„ä¸ªæ–¹é¢ï¼ŒåŒ…æ‹¬ä»¤ç‰Œçº§åˆ«ã€æ¨¡å‹çº§åˆ«å’Œç³»ç»Ÿçº§åˆ«çš„ä¼˜åŒ–ã€‚æ­¤è°ƒæŸ¥æ—¨åœ¨æä¾›æœ‰å…³KVç¼“å­˜ç®¡ç†çš„æ·±åˆ»è§è§£ï¼Œæ”¯æŒç ”ç©¶äººå‘˜å’Œå®è·µè€…å¼€å‘é«˜æ•ˆä¸”å¯æ‰©å±•çš„KVç¼“å­˜ç®¡ç†æŠ€æœ¯ï¼Œæ¨åŠ¨LLMåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å®é™…éƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå·²åœ¨å¤šä¸ªé¢†åŸŸå¸¦æ¥å˜é©ï¼Œä½†è®¡ç®—å’Œå†…å­˜éœ€æ±‚æŒ‘æˆ˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ‰©å±•ã€‚</li>
<li>KVç¼“å­˜ç®¡ç†æ˜¯ç”¨äºåŠ é€ŸLLMçš„å…³é”®ä¼˜åŒ–æŠ€æœ¯ï¼Œå¯å‡å°‘å†—ä½™è®¡ç®—å¹¶æ”¹å–„å†…å­˜ä½¿ç”¨ã€‚</li>
<li>KVç¼“å­˜ç®¡ç†ç­–ç•¥åˆ†ä¸ºä»¤ç‰Œçº§åˆ«ã€æ¨¡å‹çº§åˆ«å’Œç³»ç»Ÿçº§åˆ«çš„ä¼˜åŒ–ã€‚</li>
<li>ä»¤ç‰Œçº§åˆ«çš„ç­–ç•¥åŒ…æ‹¬KVç¼“å­˜é€‰æ‹©ã€é¢„ç®—åˆ†é…ã€åˆå¹¶ã€é‡åŒ–å’Œä½ç§©åˆ†è§£ã€‚</li>
<li>æ¨¡å‹çº§åˆ«çš„ä¼˜åŒ–ä¾§é‡äºæ¶æ„åˆ›æ–°å’Œæ³¨æ„åŠ›æœºåˆ¶ä»¥å¢å¼ºKVçš„é‡å¤ä½¿ç”¨ã€‚</li>
<li>ç³»ç»Ÿçº§åˆ«çš„ç­–ç•¥å¤„ç†å†…å­˜ç®¡ç†ã€è°ƒåº¦å’Œç¡¬ä»¶æ„ŸçŸ¥è®¾è®¡ï¼Œä»¥æé«˜å„ç§è®¡ç®—ç¯å¢ƒä¸­çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19442">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0620c23d2ac5e4c67806a67fd0f16bcd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7af90f8424a3df5a46727374ea26313a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61d1a46f4b97bac51862ee87e3f57f3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2b7aa6ec84db2bf2b32a504bb4f8953.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="RecLM-Recommendation-Instruction-Tuning"><a href="#RecLM-Recommendation-Instruction-Tuning" class="headerlink" title="RecLM: Recommendation Instruction Tuning"></a>RecLM: Recommendation Instruction Tuning</h2><p><strong>Authors:Yangqin Jiang, Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang</strong></p>
<p>Modern recommender systems aim to deeply understand usersâ€™ complex preferences through their past interactions. While deep collaborative filtering approaches using Graph Neural Networks (GNNs) excel at capturing user-item relationships, their effectiveness is limited when handling sparse data or zero-shot scenarios, primarily due to constraints in ID-based embedding functions. To address these challenges, we propose a model-agnostic recommendation instruction-tuning paradigm that seamlessly integrates large language models with collaborative filtering. Our proposed $\underline{Rec}$ommendation $\underline{L}$anguage $\underline{M}$odel (RecLM) enhances the capture of user preference diversity through a carefully designed reinforcement learning reward function that facilitates self-augmentation of language models. Comprehensive evaluations demonstrate significant advantages of our approach across various settings, and its plug-and-play compatibility with state-of-the-art recommender systems results in notable performance enhancements. The implementation of our RecLM framework is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/HKUDS/RecLM">https://github.com/HKUDS/RecLM</a>. </p>
<blockquote>
<p>ç°ä»£æ¨èç³»ç»Ÿæ—¨åœ¨é€šè¿‡ç”¨æˆ·è¿‡å»çš„äº¤äº’æ¥æ·±å…¥äº†è§£ç”¨æˆ·çš„å¤æ‚åå¥½ã€‚è™½ç„¶ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰çš„æ·±åº¦ååŒè¿‡æ»¤æ–¹æ³•åœ¨æ•æ‰ç”¨æˆ·-é¡¹ç›®å…³ç³»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†ç¨€ç–æ•°æ®æˆ–é›¶å°„å‡»åœºæ™¯æ—¶ï¼Œå…¶æœ‰æ•ˆæ€§ä¸»è¦å—åˆ°åŸºäºIDçš„åµŒå…¥å‡½æ•°çº¦æŸçš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡å‹æ— å…³çš„æ¨èæŒ‡ä»¤è°ƒæ•´èŒƒå¼ï¼Œè¯¥èŒƒå¼æ— ç¼é›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸ååŒè¿‡æ»¤ã€‚æˆ‘ä»¬æå‡ºçš„æ¨èè¯­è¨€æ¨¡å‹ï¼ˆRecLMï¼‰é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°å¢å¼ºäº†å¯¹ç”¨æˆ·åå¥½å¤šæ ·æ€§çš„æ•è·ï¼Œè¯¥å¥–åŠ±å‡½æ•°ä¿ƒè¿›äº†è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘å¢å¼ºã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§è®¾ç½®ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶ä¸”å…¶ä¸æœ€æ–°æ¨èç³»ç»Ÿçš„å³æ’å³ç”¨å…¼å®¹æ€§å¯¼è‡´æ€§èƒ½å¾—åˆ°æ˜¾ç€æå‡ã€‚æˆ‘ä»¬çš„RecLMæ¡†æ¶çš„å®ç°å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/HKUDS/RecLM%E3%80%82">https://github.com/HKUDS/RecLMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19302v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>èåˆæ·±åº¦ååŒè¿‡æ»¤ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨èæŒ‡ä»¤è°ƒä¼˜èŒƒå¼ã€‚é’ˆå¯¹ç°ä»£æ¨èç³»ç»Ÿé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚ç¨€ç–æ•°æ®æˆ–é›¶æ ·æœ¬åœºæ™¯ä¸‹çš„ç”¨æˆ·åå¥½ç†è§£éš¾é¢˜ï¼Œæå‡ºä¸€ç§æ¨¡å‹æ— å…³çš„æ¨èæŒ‡ä»¤è°ƒä¼˜æ–¹æ³•ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œä¿ƒè¿›è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘å¢å¼ºï¼Œæå‡ç”¨æˆ·åå¥½å¤šæ ·æ€§çš„æ•æ‰èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å…·æœ‰æ˜¾è‘—çš„ä¼˜è¶Šæ€§ï¼Œå¹¶å¯ä¸æœ€å…ˆè¿›çš„æ¨èç³»ç»Ÿæ— ç¼é›†æˆï¼Œå®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ›´å¤šç»†èŠ‚å¯é€šè¿‡è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/HKUDS/RecLM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HKUDS/RecLMè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£æ¨èç³»ç»Ÿè‡´åŠ›äºé€šè¿‡ç”¨æˆ·çš„è¿‡å»äº¤äº’æ¥æ·±å…¥ç†è§£å…¶å¤æ‚åå¥½ã€‚</li>
<li>æ·±åº¦ååŒè¿‡æ»¤æ–¹æ³•ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨æ•æ‰ç”¨æˆ·-ç‰©å“å…³ç³»æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>å½“å¤„ç†ç¨€ç–æ•°æ®æˆ–é›¶æ ·æœ¬åœºæ™¯æ—¶ï¼Œç°æœ‰æ–¹æ³•çš„æœ‰æ•ˆæ€§å—åˆ°é™åˆ¶ï¼Œä¸»è¦åŸå› æ˜¯åŸºäºIDçš„åµŒå…¥å‡½æ•°çš„çº¦æŸã€‚</li>
<li>æå‡ºä¸€ç§æ¨¡å‹æ— å…³çš„æ¨èæŒ‡ä»¤è°ƒä¼˜æ–¹æ³•ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸ååŒè¿‡æ»¤æ— ç¼é›†æˆã€‚</li>
<li>é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œæå‡ç”¨æˆ·åå¥½å¤šæ ·æ€§çš„æ•æ‰èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å„ç§è®¾ç½®ä¸‹å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸”ä¸æœ€å…ˆè¿›çš„æ¨èç³»ç»Ÿå…¼å®¹ï¼Œå®ç°æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-787be3a7ad75a1538d50420b589a137b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28ed79cf78bcfa943e4ad56ac97c5ce3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MEDEC-A-Benchmark-for-Medical-Error-Detection-and-Correction-in-Clinical-Notes"><a href="#MEDEC-A-Benchmark-for-Medical-Error-Detection-and-Correction-in-Clinical-Notes" class="headerlink" title="MEDEC: A Benchmark for Medical Error Detection and Correction in   Clinical Notes"></a>MEDEC: A Benchmark for Medical Error Detection and Correction in   Clinical Notes</h2><p><strong>Authors:Asma Ben Abacha, Wen-wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, Thomas Lin</strong></p>
<p>Several studies showed that Large Language Models (LLMs) can answer medical questions correctly, even outperforming the average human score in some medical exams. However, to our knowledge, no study has been conducted to assess the ability of language models to validate existing or generated medical text for correctness and consistency. In this paper, we introduce MEDEC (<a target="_blank" rel="noopener" href="https://github.com/abachaa/MEDEC">https://github.com/abachaa/MEDEC</a>), the first publicly available benchmark for medical error detection and correction in clinical notes, covering five types of errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal Organism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes from three US hospital systems that were not previously seen by any LLM. The dataset has been used for the MEDIQA-CORR shared task to evaluate seventeen participating systems [Ben Abacha et al., 2024]. In this paper, we describe the data creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4, Claude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and correcting medical errors requiring both medical knowledge and reasoning capabilities. We also conducted a comparative study where two medical doctors performed the same task on the MEDEC test set. The results showed that MEDEC is a sufficiently challenging benchmark to assess the ability of models to validate existing or generated notes and to correct medical errors. We also found that although recent LLMs have a good performance in error detection and correction, they are still outperformed by medical doctors in these tasks. We discuss the potential factors behind this gap, the insights from our experiments, the limitations of current evaluation metrics, and share potential pointers for future research. </p>
<blockquote>
<p>å¤šé¡¹ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿæ­£ç¡®å›ç­”åŒ»å­¦é—®é¢˜ï¼Œç”šè‡³åœ¨æŸäº›åŒ»å­¦è€ƒè¯•ä¸­çš„è¡¨ç°è¶…è¿‡äººç±»å¹³å‡æ°´å¹³ã€‚ç„¶è€Œï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå°šæœªæœ‰ç ”ç©¶è¯„ä¼°è¯­è¨€æ¨¡å‹éªŒè¯ç°æœ‰æˆ–ç”ŸæˆåŒ»å­¦æ–‡æœ¬çš„æ­£ç¡®æ€§å’Œä¸€è‡´æ€§çš„èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MEDECï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/abachaa/MEDEC%EF%BC%89%EF%BC%8C%E8%BF%99%E6%98%AF%E4%B8%B4%E5%BA%8A%E4%B8%8A%E9%A6%96%E4%B8%AA%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E7%9A%84%E5%8C%BB%E7%96%97%E9%94%99%E8%AF%AF%E6%A3%80%E6%B5%8B%E5%92%8C%E6%A0%A1%E6%AD%A3%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%EF%BC%8C%E6%B6%B5%E7%9B%96%E4%BA%86%E4%BA%94%E7%A7%8D%E7%B1%BB%E5%9E%8B%E7%9A%84%E9%94%99%E8%AF%AF%EF%BC%88%E8%AF%8A%E6%96%AD%E3%80%81%E7%AE%A1%E7%90%86%E3%80%81%E6%B2%BB%E7%96%97%E3%80%81%E8%8D%AF%E7%89%A9%E6%B2%BB%E7%96%97%E5%92%8C%E7%97%85%E5%8E%9F%E4%BD%93%EF%BC%89%E3%80%82MEDEC%E5%8C%85%E5%90%AB3848%E7%AF%87%E4%B8%B4%E5%BA%8A%E6%96%87%E6%9C%AC%EF%BC%8C%E5%85%B6%E4%B8%AD%E5%8C%85%E6%8B%AC%E6%9D%A5%E8%87%AA%E7%BE%8E%E5%9B%BD%E4%B8%89%E4%B8%AA%E5%8C%BB%E9%99%A2%E7%B3%BB%E7%BB%9F%E7%9A%84488%E7%AF%87%E4%BB%A5%E5%89%8D%E6%9C%AA%E8%A2%AB%E4%BB%BB%E4%BD%95LLM%E6%9F%A5%E7%9C%8B%E8%BF%87%E7%9A%84%E4%B8%B4%E5%BA%8A%E7%AC%94%E8%AE%B0%E3%80%82%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B7%B2%E8%A2%AB%E7%94%A8%E4%BA%8EMEDIQA-CORR%E5%85%B1%E4%BA%AB%E4%BB%BB%E5%8A%A1%E6%9D%A5%E8%AF%84%E4%BC%B0%E5%8D%81%E4%B8%83%E4%B8%AA%E5%8F%82%E4%B8%8E%E7%B3%BB%E7%BB%9F[Ben">https://github.com/abachaa/MEDECï¼‰ï¼Œè¿™æ˜¯ä¸´åºŠä¸Šé¦–ä¸ªå…¬å¼€å¯ç”¨çš„åŒ»ç–—é”™è¯¯æ£€æµ‹å’Œæ ¡æ­£çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†äº”ç§ç±»å‹çš„é”™è¯¯ï¼ˆè¯Šæ–­ã€ç®¡ç†ã€æ²»ç–—ã€è¯ç‰©æ²»ç–—å’Œç—…åŸä½“ï¼‰ã€‚MEDECåŒ…å«3848ç¯‡ä¸´åºŠæ–‡æœ¬ï¼Œå…¶ä¸­åŒ…æ‹¬æ¥è‡ªç¾å›½ä¸‰ä¸ªåŒ»é™¢ç³»ç»Ÿçš„488ç¯‡ä»¥å‰æœªè¢«ä»»ä½•LLMæŸ¥çœ‹è¿‡çš„ä¸´åºŠç¬”è®°ã€‚è¯¥æ•°æ®é›†å·²è¢«ç”¨äºMEDIQA-CORRå…±äº«ä»»åŠ¡æ¥è¯„ä¼°åä¸ƒä¸ªå‚ä¸ç³»ç»Ÿ[Ben</a> Abachaç­‰äººï¼ŒXXXX]ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†æ•°æ®åˆ›å»ºæ–¹æ³•ï¼Œå¹¶è¯„ä¼°äº†æœ€æ–°çš„LLMï¼ˆä¾‹å¦‚ï¼Œo1-previewã€GPT-4ã€Claude 3.5 Sonnetå’ŒGemini 2.0 Flashï¼‰åœ¨æ£€æµ‹å’Œæ ¡æ­£éœ€è¦åŒ»å­¦çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›çš„åŒ»ç–—é”™è¯¯ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸€é¡¹å¯¹æ¯”ç ”ç©¶ï¼Œå…¶ä¸­ä¸¤ååŒ»ç”Ÿæ‰§è¡Œäº†ç›¸åŒçš„ä»»åŠ¡åœ¨MEDECæµ‹è¯•é›†ä¸Šã€‚ç»“æœè¡¨æ˜ï¼ŒMEDECæ˜¯ä¸€ä¸ªè¶³å¤Ÿå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œå¯ä»¥è¯„ä¼°æ¨¡å‹éªŒè¯ç°æœ‰æˆ–ç”Ÿæˆçš„ç¬”è®°ä»¥åŠæ ¡æ­£åŒ»ç–—é”™è¯¯çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œå°½ç®¡æœ€æ–°çš„LLMåœ¨é”™è¯¯æ£€æµ‹å’Œæ ¡æ­£æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬ä»ç„¶åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¢«åŒ»ç”Ÿè¶…è¶Šã€‚æˆ‘ä»¬è®¨è®ºäº†é€ æˆè¿™ä¸€å·®è·çš„æ½œåœ¨å› ç´ ã€å®éªŒä¸­çš„è§è§£ã€å½“å‰è¯„ä¼°æŒ‡æ ‡çš„å±€é™æ€§ï¼Œå¹¶åˆ†äº«äº†æœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19260v2">PDF</a> This version has been updated with further clarification regarding   the model size estimates that were mined from public articles only and   provided to aid in contextualizing model performance. The authors cannot   vouch for the accuracy of those estimates</p>
<p><strong>Summary</strong><br>     å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½æ­£ç¡®å›ç­”åŒ»å­¦é—®é¢˜å¹¶åœ¨æŸäº›åŒ»å­¦è€ƒè¯•ä¸­è¡¨ç°è¶…è¶Šäººç±»å¹³å‡æ°´å¹³ã€‚ç„¶è€Œï¼Œå°šæ— ç ”ç©¶è¯„ä¼°è¯­è¨€æ¨¡å‹éªŒè¯ç°æœ‰æˆ–ç”ŸæˆåŒ»å­¦æ–‡æœ¬çš„æ­£ç¡®æ€§å’Œä¸€è‡´æ€§çš„èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†MEDECï¼ˆåŒ»ç–—é”™è¯¯æ£€æµ‹å’Œæ ¡æ­£åŸºå‡†æµ‹è¯•ï¼‰ï¼ŒåŒ…å«è¯Šæ–­ã€ç®¡ç†ã€æ²»ç–—ã€è¯ç‰©æ²»ç–—å’Œç—…åŸä½“ç­‰äº”ç§é”™è¯¯çš„ä¸´åºŠæ–‡æœ¬æ•°æ®é›†ã€‚è¯„ä¼°äº†å¤šä¸ªLLMåœ¨æ£€æµ‹å’Œæ ¡æ­£åŒ»å­¦é”™è¯¯æ–¹é¢çš„è¡¨ç°ï¼Œå¹¶ä¸ä¸¤ä½åŒ»ç”Ÿè¿›è¡Œå¯¹æ¯”ã€‚ç»“æœæ˜¾ç¤ºï¼ŒMEDECæ˜¯è¯„ä¼°æ¨¡å‹éªŒè¯å’Œæ ¡æ­£èƒ½åŠ›çš„æŒ‘æˆ˜åŸºå‡†æµ‹è¯•ï¼Œä½†LLMåœ¨è¿™äº›ä»»åŠ¡ä¸Šä»è¢«åŒ»ç”Ÿè¶…è¶Šã€‚æœ¬æ–‡æ¢è®¨äº†æ½œåœ¨å› ç´ ã€å®éªŒè§è§£ã€å½“å‰è¯„ä¼°æŒ‡æ ‡çš„å±€é™æ€§ä»¥åŠæœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMèƒ½å¤Ÿæ­£ç¡®å›ç­”åŒ»å­¦é—®é¢˜å¹¶åœ¨æŸäº›åŒ»å­¦è€ƒè¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
<li>ç›®å‰å°šæ— ç ”ç©¶è¯„ä¼°LLMåœ¨éªŒè¯åŒ»å­¦æ–‡æœ¬æ­£ç¡®æ€§å’Œä¸€è‡´æ€§æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>MEDECæ˜¯é¦–ä¸ªå…¬å¼€å¯ç”¨çš„åŒ»ç–—é”™è¯¯æ£€æµ‹å’Œæ ¡æ­£åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å¤šç§ç±»å‹çš„åŒ»ç–—é”™è¯¯ã€‚</li>
<li>LLMåœ¨æ£€æµ‹å’Œæ ¡æ­£åŒ»å­¦é”™è¯¯æ–¹é¢çš„æ€§èƒ½è‰¯å¥½ï¼Œä½†ä»è¢«åŒ»ç”Ÿè¶…è¶Šã€‚</li>
<li>åŒ»ç”Ÿå’ŒLLMä¹‹é—´çš„æ€§èƒ½å·®è·å¯èƒ½æºäºè¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ã€‚</li>
<li>å½“å‰è¯„ä¼°æŒ‡æ ‡å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦è¿›ä¸€æ­¥å®Œå–„å’Œæ”¹è¿›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cd2908436fb7f969d69f113d8024e948.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-546f656bf1124a0e26cad4bb9ca62195.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-132baaa17466147625f453fab11b132c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64fd1d90ae08ae5afed50400237e2414.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d65d3007465323cb3a8d43f9484ec7c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Speech-Retrieval-Augmented-Generation-without-Automatic-Speech-Recognition"><a href="#Speech-Retrieval-Augmented-Generation-without-Automatic-Speech-Recognition" class="headerlink" title="Speech Retrieval-Augmented Generation without Automatic Speech   Recognition"></a>Speech Retrieval-Augmented Generation without Automatic Speech   Recognition</h2><p><strong>Authors:Do June Min, Karel Mundnich, Andy Lapastora, Erfan Soltanmohammadi, Srikanth Ronanki, Kyu Han</strong></p>
<p>One common approach for question answering over speech data is to first transcribe speech using automatic speech recognition (ASR) and then employ text-based retrieval-augmented generation (RAG) on the transcriptions. While this cascaded pipeline has proven effective in many practical settings, ASR errors can propagate to the retrieval and generation steps. To overcome this limitation, we introduce SpeechRAG, a novel framework designed for open-question answering over spoken data. Our proposed approach fine-tunes a pre-trained speech encoder into a speech adapter fed into a frozen large language model (LLM)â€“based retrieval model. By aligning the embedding spaces of text and speech, our speech retriever directly retrieves audio passages from text-based queries, leveraging the retrieval capacity of the frozen text retriever. Our retrieval experiments on spoken question answering datasets show that direct speech retrieval does not degrade over the text-based baseline, and outperforms the cascaded systems using ASR. For generation, we use a speech language model (SLM) as a generator, conditioned on audio passages rather than transcripts. Without fine-tuning of the SLM, this approach outperforms cascaded text-based models when there is high WER in the transcripts. </p>
<blockquote>
<p>é’ˆå¯¹è¯­éŸ³æ•°æ®çš„é—®ç­”çš„ä¸€ç§å¸¸è§æ–¹æ³•æ˜¯é¦–å…ˆä½¿ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è¿›è¡Œè¯­éŸ³è½¬å½•ï¼Œç„¶ååœ¨è½¬å½•ä¸Šåº”ç”¨åŸºäºæ–‡æœ¬çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚è™½ç„¶è¿™ç§çº§è”ç®¡é“å·²åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œä½†ASRé”™è¯¯å¯èƒ½ä¼šä¼ æ’­åˆ°æ£€ç´¢å’Œç”Ÿæˆæ­¥éª¤ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpeechRAGï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå¼€æ”¾å¼é—®é¢˜å›ç­”è¯­éŸ³æ•°æ®è®¾è®¡çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•å°†é¢„è®­ç»ƒçš„è¯­éŸ³ç¼–ç å™¨å¾®è°ƒä¸ºè¯­éŸ³é€‚é…å™¨ï¼Œå¹¶è¾“å…¥åˆ°åŸºäºå†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢æ¨¡å‹ä¸­ã€‚é€šè¿‡å¯¹æ–‡æœ¬å’Œè¯­éŸ³çš„åµŒå…¥ç©ºé—´è¿›è¡Œå¯¹é½ï¼Œæˆ‘ä»¬çš„è¯­éŸ³æ£€ç´¢å™¨ç›´æ¥ä»åŸºäºæ–‡æœ¬çš„æŸ¥è¯¢ä¸­æ£€ç´¢è¯­éŸ³ç‰‡æ®µï¼Œåˆ©ç”¨å†»ç»“çš„æ–‡æœ¬æ£€ç´¢å™¨çš„æ£€ç´¢èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨è¯­éŸ³é—®ç­”æ•°æ®é›†ä¸Šçš„æ£€ç´¢å®éªŒè¡¨æ˜ï¼Œç›´æ¥è¯­éŸ³æ£€ç´¢å¹¶ä¸äºšäºåŸºäºæ–‡æœ¬çš„åŸºçº¿ï¼Œå¹¶ä¸”ä¼˜äºä½¿ç”¨ASRçš„çº§è”ç³»ç»Ÿã€‚å¯¹äºç”Ÿæˆï¼Œæˆ‘ä»¬ä½¿ç”¨è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä½œä¸ºç”Ÿæˆå™¨ï¼Œä»¥éŸ³é¢‘ç‰‡æ®µä¸ºæ¡ä»¶ï¼Œè€Œä¸æ˜¯ä»¥è½¬å½•ä¸ºæ¡ä»¶ã€‚åœ¨ä¸å¾®è°ƒSLMçš„æƒ…å†µä¸‹ï¼Œå½“è½¬å½•ä¸­çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰è¾ƒé«˜æ—¶ï¼Œè¿™ç§æ–¹æ³•ä¼˜äºçº§è”çš„åŸºäºæ–‡æœ¬æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16500v2">PDF</a> ICASSP 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ‘˜è¦ç®€è¦ä»‹ç»äº†é’ˆå¯¹è¯­éŸ³æ•°æ®çš„é—®ç­”ç³»ç»Ÿçš„å¸¸è§æ–¹æ³•ï¼Œå¹¶æŒ‡å‡ºäº†å…¶å­˜åœ¨çš„å±€é™æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶SpeechRAGï¼Œå®ƒé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„è¯­éŸ³ç¼–ç å™¨å¹¶å°†å…¶é¦ˆå…¥å†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢æ¨¡å‹æ¥å®ç°ç›´æ¥è¯­éŸ³æ£€ç´¢ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹æ–‡æœ¬å’Œè¯­éŸ³åµŒå…¥ç©ºé—´çš„å¯¹é½ï¼Œèƒ½å¤Ÿç›´æ¥ä»åŸºäºæ–‡æœ¬çš„æŸ¥è¯¢ä¸­æ£€ç´¢è¯­éŸ³æ®µè½ï¼Œä»è€Œåˆ©ç”¨å†»ç»“æ–‡æœ¬æ£€ç´¢å™¨çš„æ£€ç´¢èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œç›´æ¥è¯­éŸ³æ£€ç´¢ä¸é€Šè‰²äºåŸºäºæ–‡æœ¬çš„åŸºç¡€çº¿ï¼Œå¹¶ä¸”åœ¨å­˜åœ¨é«˜è¯­éŸ³è¯†åˆ«é”™è¯¯ç‡çš„æƒ…å†µä¸‹ï¼Œå…¶è¡¨ç°ä¼˜äºçº§è”ç³»ç»Ÿã€‚åœ¨ç”Ÿæˆæ–¹é¢ï¼Œä½¿ç”¨è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä½œä¸ºç”Ÿæˆå™¨ï¼Œä»¥éŸ³é¢‘æ®µè½ä¸ºæ¡ä»¶ï¼Œæ— éœ€å¯¹SLMè¿›è¡Œå¾®è°ƒï¼Œå³è¡¨ç°å‡ºä¼˜äºçº§è”æ–‡æœ¬æ¨¡å‹çš„æ•ˆæœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰é—®ç­”ç³»ç»Ÿé€šå¸¸é€šè¿‡çº§è”ç®¡é“å¤„ç†è¯­éŸ³æ•°æ®ï¼Œå³å…ˆä½¿ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è¿›è¡Œè¯­éŸ³è½¬å½•ï¼Œç„¶ååœ¨è½¬å½•ä¸Šåº”ç”¨åŸºäºæ–‡æœ¬çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚</li>
<li>ASRé”™è¯¯å¯èƒ½ä¼šä¼ æ’­åˆ°æ£€ç´¢å’Œç”Ÿæˆæ­¥éª¤ï¼Œå½±å“ç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>SpeechRAGæ¡†æ¶è¢«å¼•å…¥ä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ƒé€šè¿‡å¾®è°ƒè¯­éŸ³ç¼–ç å™¨å¹¶å°†å…¶è¾“å…¥åˆ°å†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹æ£€ç´¢æ¨¡å‹ä¸­ï¼Œå®ç°ç›´æ¥è¯­éŸ³æ£€ç´¢ã€‚</li>
<li>SpeechRAGé€šè¿‡å¯¹æ–‡æœ¬å’Œè¯­éŸ³åµŒå…¥ç©ºé—´çš„å¯¹é½ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿç›´æ¥ä»åŸºäºæ–‡æœ¬çš„æŸ¥è¯¢ä¸­æ£€ç´¢è¯­éŸ³æ®µè½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œç›´æ¥è¯­éŸ³æ£€ç´¢çš„æ€§èƒ½ä¸é€Šäºæ–‡æœ¬åŸºç¡€çº¿ï¼Œä¸”åœ¨è¯­éŸ³è¯†åˆ«é”™è¯¯ç‡è¾ƒé«˜çš„æƒ…å†µä¸‹ï¼Œå…¶è¡¨ç°ä¼˜äºçº§è”ç³»ç»Ÿã€‚</li>
<li>åœ¨ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä½œä¸ºç”Ÿæˆå™¨ï¼Œä»¥éŸ³é¢‘æ®µè½ä¸ºæ¡ä»¶ï¼Œæ— éœ€å¯¹SLMè¿›è¡Œå¾®è°ƒï¼Œå³å¯è·å¾—ä¼˜äºçº§è”æ–‡æœ¬æ¨¡å‹çš„æ•ˆæœã€‚</li>
<li>SpeechRAGæ¡†æ¶ä¸ºå¼€æ”¾é—®ç­”å›ç­”æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„æ–°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åŒ…å«é”™è¯¯æˆ–å™ªéŸ³çš„è¯­éŸ³æ•°æ®æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16500">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cee021b716efd502d2772cdd7e58990e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13a924273c299372d64abf8fbddee866.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee70a8bc71812a34552837322c47bc8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46a4c6ec08579c253d8ef68377e9820c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4eb1662aef874aa48fc2b5ed4c5a1da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e1cfcbcc67580c52fbe6fe9d1fa5714.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="World-knowledge-enhanced-Reasoning-Using-Instruction-guided-Interactor-in-Autonomous-Driving"><a href="#World-knowledge-enhanced-Reasoning-Using-Instruction-guided-Interactor-in-Autonomous-Driving" class="headerlink" title="World knowledge-enhanced Reasoning Using Instruction-guided Interactor   in Autonomous Driving"></a>World knowledge-enhanced Reasoning Using Instruction-guided Interactor   in Autonomous Driving</h2><p><strong>Authors:Mingliang Zhai, Cheng Li, Zengyuan Guo, Ningrui Yang, Xiameng Qin, Sanyuan Zhao, Junyu Han, Ji Tao, Yuwei Wu, Yunde Jia</strong></p>
<p>The Multi-modal Large Language Models (MLLMs) with extensive world knowledge have revitalized autonomous driving, particularly in reasoning tasks within perceivable regions. However, when faced with perception-limited areas (dynamic or static occlusion regions), MLLMs struggle to effectively integrate perception ability with world knowledge for reasoning. These perception-limited regions can conceal crucial safety information, especially for vulnerable road users. In this paper, we propose a framework, which aims to improve autonomous driving performance under perceptionlimited conditions by enhancing the integration of perception capabilities and world knowledge. Specifically, we propose a plug-and-play instruction-guided interaction module that bridges modality gaps and significantly reduces the input sequence length, allowing it to adapt effectively to multi-view video inputs. Furthermore, to better integrate world knowledge with driving-related tasks, we have collected and refined a large-scale multi-modal dataset that includes 2 million natural language QA pairs, 1.7 million grounding task data. To evaluate the modelâ€™s utilization of world knowledge, we introduce an object-level risk assessment dataset comprising 200K QA pairs, where the questions necessitate multi-step reasoning leveraging world knowledge for resolution. Extensive experiments validate the effectiveness of our proposed method. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ‹¥æœ‰ä¸°å¯Œçš„ä¸–ç•ŒçŸ¥è¯†ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶æ³¨å…¥äº†æ–°çš„æ´»åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯æ„ŸçŸ¥åŒºåŸŸçš„æ¨ç†ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œå½“é¢å¯¹æ„ŸçŸ¥å—é™åŒºåŸŸï¼ˆåŠ¨æ€æˆ–é™æ€é®æŒ¡åŒºåŸŸï¼‰æ—¶ï¼ŒMLLMsåœ¨å°†æ„ŸçŸ¥èƒ½åŠ›ä¸ä¸–ç•ŒçŸ¥è¯†è¿›è¡Œæœ‰æ•ˆæ•´åˆä»¥è¿›è¡Œæ¨ç†æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚è¿™äº›æ„ŸçŸ¥å—é™çš„åŒºåŸŸå¯èƒ½éšè—äº†å…³é”®çš„å®‰å…¨ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè„†å¼±çš„é“è·¯ä½¿ç”¨è€…ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¢å¼ºæ„ŸçŸ¥èƒ½åŠ›å’Œä¸–ç•ŒçŸ¥è¯†çš„æ•´åˆæ¥æé«˜æ„ŸçŸ¥å—é™æ¡ä»¶ä¸‹çš„è‡ªåŠ¨é©¾é©¶æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„æŒ‡ä»¤å¼•å¯¼äº¤äº’æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥å¼¥ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®è·å¹¶æ˜¾ç€ç¼©çŸ­è¾“å…¥åºåˆ—é•¿åº¦ï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆåœ°é€‚åº”å¤šè§†å›¾è§†é¢‘è¾“å…¥ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ›´å¥½åœ°å°†ä¸–ç•ŒçŸ¥è¯†ä¸é©¾é©¶ç›¸å…³ä»»åŠ¡è¿›è¡Œæ•´åˆï¼Œæˆ‘ä»¬å·²ç»æ”¶é›†å’Œç²¾ç‚¼äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬200ä¸‡ä¸ªè‡ªç„¶è¯­è¨€é—®ç­”å¯¹å’Œ170ä¸‡ä¸ªåŸºç¡€ä»»åŠ¡æ•°æ®ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹å¯¹ä¸–ç•ŒçŸ¥è¯†çš„åˆ©ç”¨æƒ…å†µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯¹è±¡çº§åˆ«çš„é£é™©è¯„ä¼°æ•°æ®é›†ï¼ŒåŒ…å«20ä¸‡ä¸ªé—®ç­”å¯¹ï¼Œè¿™äº›é—®é¢˜éœ€è¦åˆ©ç”¨ä¸–ç•ŒçŸ¥è¯†è¿›è¡Œå¤šæ­¥éª¤æ¨ç†æ¥è§£å†³ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06324v3">PDF</a> AAAI 2025. 14 pages. Supplementary Material</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯æ„ŸçŸ¥åŒºåŸŸçš„æ¨ç†ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œé¢å¯¹æ„ŸçŸ¥å—é™åŒºåŸŸï¼ˆåŠ¨æ€æˆ–é™æ€é®æŒ¡åŒºåŸŸï¼‰ï¼ŒMLLMsåœ¨æ•´åˆæ„ŸçŸ¥èƒ½åŠ›å’Œä¸–ç•ŒçŸ¥è¯†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¢å¼ºæ„ŸçŸ¥èƒ½åŠ›å’Œä¸–ç•ŒçŸ¥è¯†çš„æ•´åˆï¼Œæé«˜è‡ªåŠ¨é©¾é©¶åœ¨æ„ŸçŸ¥å—é™æ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªå³æ’å³ç”¨çš„æŒ‡ä»¤å¼•å¯¼äº¤äº’æ¨¡å—ï¼Œèƒ½å¤Ÿç¼©å°æ¨¡æ€å·®è·å¹¶æ˜¾è‘—å‡å°‘è¾“å…¥åºåˆ—é•¿åº¦ï¼Œé€‚åº”å¤šè§†è§’è§†é¢‘è¾“å…¥ã€‚åŒæ—¶ï¼Œä¸ºäº†æ•´åˆä¸–ç•ŒçŸ¥è¯†ä¸é©¾é©¶ä»»åŠ¡ï¼Œæ”¶é›†å’Œç²¾ç‚¼äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«200ä¸‡è‡ªç„¶è¯­è¨€é—®ç­”å¯¹å’Œ170ä¸‡å®šä½ä»»åŠ¡æ•°æ®ã€‚é€šè¿‡å¼•å…¥å¯¹è±¡çº§åˆ«çš„é£é™©è¯„ä¼°æ•°æ®é›†æ¥è¯„ä¼°æ¨¡å‹å¯¹ä¸–ç•ŒçŸ¥è¯†çš„åˆ©ç”¨æ•ˆæœï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨è‡ªåŠ¨é©¾é©¶çš„æ¨ç†ä»»åŠ¡ä¸­å…·æœ‰å¼ºå¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯æ„ŸçŸ¥åŒºåŸŸã€‚</li>
<li>æ„ŸçŸ¥å—é™åŒºåŸŸå¯¹MLLMsæ•´åˆæ„ŸçŸ¥èƒ½åŠ›å’Œä¸–ç•ŒçŸ¥è¯†æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªæŒ‡ä»¤å¼•å¯¼äº¤äº’æ¨¡å—ï¼Œå¯é€‚åº”å¤šè§†è§’è§†é¢‘è¾“å…¥å¹¶ç¼©å°æ¨¡æ€å·®è·ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œç”¨äºæ•´åˆä¸–ç•ŒçŸ¥è¯†å’Œé©¾é©¶ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥äº†å¯¹è±¡çº§åˆ«çš„é£é™©è¯„ä¼°æ•°æ®é›†ä»¥è¯„ä¼°æ¨¡å‹å¯¹ä¸–ç•ŒçŸ¥è¯†çš„åˆ©ç”¨ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å¢å¼ºæ„ŸçŸ¥èƒ½åŠ›å’Œä¸–ç•ŒçŸ¥è¯†çš„æ•´åˆï¼Œæé«˜äº†è‡ªåŠ¨é©¾é©¶åœ¨æ„ŸçŸ¥å—é™æ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06324">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e2cf5f867c3e5e69819b21afda22126.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d7fa80ff2a5845235f1ac5af60c3950.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c710f5e2beffdb6456ffd689b9abb363.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0e77472d46334a2eab2fe3b24c7e791.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa4b0134ada811e5253add22237cb010.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e3b346e59ee8982c58dc48a7b34773d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec0e621edd06d2b24fa112d8250c2b5d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ChatTS-Aligning-Time-Series-with-LLMs-via-Synthetic-Data-for-Enhanced-Understanding-and-Reasoning"><a href="#ChatTS-Aligning-Time-Series-with-LLMs-via-Synthetic-Data-for-Enhanced-Understanding-and-Reasoning" class="headerlink" title="ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced   Understanding and Reasoning"></a>ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced   Understanding and Reasoning</h2><p><strong>Authors:Zhe Xie, Zeyan Li, Xiao He, Longlong Xu, Xidao Wen, Tieying Zhang, Jianjun Chen, Rui Shi, Dan Pei</strong></p>
<p>Understanding time series is crucial for its application in real-world scenarios. Recently, large language models (LLMs) have been increasingly applied to time series tasks, leveraging their strong language capabilities to enhance various applications. However, research on multimodal LLMs (MLLMs) for time series understanding and reasoning remains limited, primarily due to the scarcity of high-quality datasets that align time series with textual information. This paper introduces ChatTS, a novel MLLM designed for time series analysis. ChatTS treats time series as a modality, similar to how vision MLLMs process images, enabling it to perform both understanding and reasoning with time series. To address the scarcity of training data, we propose an attribute-based method for generating synthetic time series with detailed attribute descriptions. We further introduce Time Series Evol-Instruct, a novel approach that generates diverse time series Q&amp;As, enhancing the modelâ€™s reasoning capabilities. To the best of our knowledge, ChatTS is the first TS-MLLM that takes multivariate time series as input for understanding and reasoning, which is fine-tuned exclusively on synthetic datasets. We evaluate its performance using benchmark datasets with real-world data, including six alignment tasks and four reasoning tasks. Our results show that ChatTS significantly outperforms existing vision-based MLLMs (e.g., GPT-4o) and text&#x2F;agent-based LLMs, achieving a 46.0% improvement in alignment tasks and a 25.8% improvement in reasoning tasks. </p>
<blockquote>
<p>ç†è§£æ—¶é—´åºåˆ—å¯¹äºå…¶åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«åº”ç”¨äºæ—¶é—´åºåˆ—ä»»åŠ¡ï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„è¯­è¨€èƒ½åŠ›æ¥å¢å¼ºå„ç§åº”ç”¨ç¨‹åºã€‚ç„¶è€Œï¼Œç”¨äºæ—¶é—´åºåˆ—ç†è§£å’Œæ¨ç†çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†èƒ½å¤Ÿå°†æ—¶é—´åºåˆ—ä¸æ–‡æœ¬ä¿¡æ¯å¯¹é½ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ—¶é—´åºåˆ—åˆ†æMLLMâ€”â€”ChatTSã€‚ChatTSå°†æ—¶é—´åºåˆ—è§†ä¸ºä¸€ç§æ¨¡æ€ï¼Œç±»ä¼¼äºè§†è§‰MLLMå¤„ç†å›¾åƒçš„æ–¹å¼ï¼Œèƒ½å¤Ÿæ‰§è¡Œæ—¶é—´åºåˆ—çš„ç†è§£å’Œæ¨ç†ã€‚ä¸ºäº†è§£å†³è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå±æ€§çš„æ–¹æ³•æ¥ç”Ÿæˆå…·æœ‰è¯¦ç»†å±æ€§æè¿°çš„åˆæˆæ—¶é—´åºåˆ—ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†æ—¶é—´åºåˆ—æ¼”åŒ–æŒ‡ä»¤ï¼ˆTime Series Evol-Instructï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆå¤šæ ·åŒ–æ—¶é—´åºåˆ—é—®ç­”çš„æ–°æ–¹æ³•ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒChatTSæ˜¯é¦–ä¸ªæ¥å—å¤šå…ƒæ—¶é—´åºåˆ—è¿›è¡Œç†è§£å’Œæ¨ç†çš„TS-MLLMï¼Œå®ƒä»…åœ¨åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«çœŸå®ä¸–ç•Œæ•°æ®çš„åŸºå‡†æ•°æ®é›†æ¥è¯„ä¼°å…¶æ€§èƒ½ï¼ŒåŒ…æ‹¬å…­ä¸ªå¯¹é½ä»»åŠ¡å’Œå››ä¸ªæ¨ç†ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒChatTSæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºè§†è§‰çš„MLLMï¼ˆä¾‹å¦‚GPT-4oï¼‰å’ŒåŸºäºæ–‡æœ¬&#x2F;ä»£ç†çš„LLMï¼Œåœ¨å¯¹é½ä»»åŠ¡ä¸Šæé«˜äº†46.0%ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šæé«˜äº†25.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03104v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç†è§£æ—¶é—´åºåˆ—å¯¹å®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«åº”ç”¨äºæ—¶é—´åºåˆ—ä»»åŠ¡ã€‚ç„¶è€Œï¼Œé’ˆå¯¹æ—¶é—´åºåˆ—ç†è§£å’Œæ¨ç†çš„å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹é«˜è´¨é‡çš„æ—¶é—´åºåˆ—ä¸æ–‡æœ¬ä¿¡æ¯å¯¹é½çš„æ•°æ®é›†ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ—¶é—´åºåˆ—åˆ†æMLLMâ€”â€”ChatTSã€‚ChatTSå°†æ—¶é—´åºåˆ—è§†ä¸ºä¸€ç§æ¨¡æ€ï¼Œç±»ä¼¼äºè§†è§‰MLLMå¤„ç†å›¾åƒçš„æ–¹å¼ï¼Œèƒ½å¤Ÿæ‰§è¡Œæ—¶é—´åºåˆ—çš„ç†è§£å’Œæ¨ç†ã€‚ä¸ºè§£å†³è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå±æ€§çš„æ–¹æ³•ç”Ÿæˆå¸¦æœ‰è¯¦ç»†å±æ€§æè¿°çš„æ—¶é—´åºåˆ—åˆæˆæ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†æ—¶é—´åºåˆ—æ¼”åŒ–æŒ‡ä»¤æ–¹æ³•ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„æ—¶é—´åºåˆ—é—®ç­”æ•°æ®ï¼Œå¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒChatTSæ˜¯é¦–ä¸ªæ¥å—å¤šå…ƒæ—¶é—´åºåˆ—è¾“å…¥è¿›è¡Œç†è§£å’Œæ¨ç†çš„TS-MLLMï¼Œå¹¶ä»…åœ¨åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«å…­ä¸ªå¯¹é½ä»»åŠ¡å’Œå››ä¸ªæ¨ç†ä»»åŠ¡çš„åŸºå‡†æ•°æ®é›†è¯„ä¼°å…¶æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒChatTSåœ¨å¯¹é½ä»»åŠ¡ä¸Šè¾ƒç°æœ‰çš„åŸºäºè§†è§‰çš„MLLMsï¼ˆå¦‚GPT-4oï¼‰å’ŒåŸºäºæ–‡æœ¬&#x2F;ä»£ç†çš„LLMsæœ‰æ˜¾è‘—æ”¹è¿›ï¼Œå…¶ä¸­å¯¹é½ä»»åŠ¡æé«˜äº†46.0%ï¼Œæ¨ç†ä»»åŠ¡æé«˜äº†25.8%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMsæœ€è¿‘è¶Šæ¥è¶Šå¤šåœ°è¢«åº”ç”¨äºæ—¶é—´åºåˆ—ä»»åŠ¡ï¼Œåˆ©ç”¨å®ƒä»¬å¼ºå¤§çš„è¯­è¨€èƒ½åŠ›å¢å¼ºå„ç§åº”ç”¨ã€‚</li>
<li>é’ˆå¯¹æ—¶é—´åºåˆ—ç†è§£å’Œæ¨ç†çš„å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œä¸»è¦åŸå› æ˜¯é«˜è´¨é‡æ•°æ®é›†çš„ç¼ºä¹ã€‚</li>
<li>ChatTSæ˜¯ä¸€ç§æ–°å‹çš„æ—¶é—´åºåˆ—åˆ†æMLLMï¼Œèƒ½å¤Ÿå°†æ—¶é—´åºåˆ—è§†ä¸ºä¸€ç§æ¨¡æ€è¿›è¡Œå¤„ç†ã€‚</li>
<li>ChatTSé€šè¿‡ç”Ÿæˆåˆæˆæ•°æ®å’Œé‡‡ç”¨æ–°é¢–çš„æ—¶é—´åºåˆ—æ¼”åŒ–æŒ‡ä»¤æ–¹æ³•è§£å†³è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>ChatTSæ˜¯é¦–ä¸ªæ¥å—å¤šå…ƒæ—¶é—´åºåˆ—è¿›è¡Œç†è§£å’Œæ¨ç†çš„TS-MLLMï¼Œä¸”åœ¨åˆæˆæ•°æ®é›†ä¸Šå¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-df6af4bbfbf5e9944d55c43e08c02f23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c4c53aa9919029236cbd151ba363fe0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1427fa44a952bcf70b0cfae2b430098.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b114e3f4612019e79019c85f9aa51b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-346498af034c5f4fae47a8dc47451b30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ba18ddf9cf00d2f0293c397c2df5ab4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e75502cafa33843b7d388d7e63a55adb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57200f944f3ef5436fbdf23b0195f1b1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MaLei-at-the-PLABA-Track-of-TAC-2024-RoBERTa-for-Task-1-â€“-LLaMA3-1-and-GPT-4o-for-Task-2"><a href="#MaLei-at-the-PLABA-Track-of-TAC-2024-RoBERTa-for-Task-1-â€“-LLaMA3-1-and-GPT-4o-for-Task-2" class="headerlink" title="MaLei at the PLABA Track of TAC-2024: RoBERTa for Task 1 â€“ LLaMA3.1 and   GPT-4o for Task 2"></a>MaLei at the PLABA Track of TAC-2024: RoBERTa for Task 1 â€“ LLaMA3.1 and   GPT-4o for Task 2</h2><p><strong>Authors:Zhidong Ling, Zihao Li, Pablo Romero, Lifeng Han, Goran Nenadic</strong></p>
<p>This report is the system description of the MaLei team (Manchester and Leiden) for shared task Plain Language Adaptation of Biomedical Abstracts (PLABA) 2024 (we had an earlier name BeeManc following last year). This report contains two sections corresponding to the two sub-tasks in PLABA 2024. In task one, we applied fine-tuned ReBERTa-Base models to identify and classify the difficult terms, jargon and acronyms in the biomedical abstracts and reported the F1 score. Due to time constraints, we didnâ€™t finish the replacement task. In task two, we leveraged Llamma3.1-70B-Instruct and GPT-4o with the one-shot prompts to complete the abstract adaptation and reported the scores in BLEU, SARI, BERTScore, LENS, and SALSA. From the official Evaluation from PLABA-2024 on Task 1A and 1B, our \textbf{much smaller fine-tuned RoBERTa-Base} model ranked 3rd and 2nd respectively on the two sub-task, and the \textbf{1st on averaged F1 scores across the two tasks} from 9 evaluated systems. Our LLaMA-3.1-70B-instructed model achieved the \textbf{highest Completeness} score for Task-2. We share our fine-tuned models and related resources at \url{<a target="_blank" rel="noopener" href="https://github.com/HECTA-UoM/PLABA2024%7D">https://github.com/HECTA-UoM/PLABA2024}</a> </p>
<blockquote>
<p>æœ¬æŠ¥å‘Šæ˜¯MaLeiå›¢é˜Ÿï¼ˆæ›¼å½»æ–¯ç‰¹ä¸è±é¡¿ï¼‰é’ˆå¯¹å…±äº«ä»»åŠ¡â€”â€”ç”Ÿç‰©åŒ»ç–—æ‘˜è¦çš„å¹³å®è¯­è¨€æ”¹ç¼–ï¼ˆPLABA 2024ï¼‰çš„ç³»ç»Ÿæè¿°ï¼ˆæˆ‘ä»¬å»å¹´ä½¿ç”¨çš„åå­—æ˜¯BeeMancï¼‰ã€‚æœ¬æŠ¥å‘ŠåŒ…å«ä¸¤éƒ¨åˆ†ï¼Œåˆ†åˆ«å¯¹åº”PLABA 2024çš„ä¸¤ä¸ªå­ä»»åŠ¡ã€‚åœ¨ä»»åŠ¡ä¸€ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¾®è°ƒè¿‡çš„ReBERTa-Baseæ¨¡å‹æ¥è¯†åˆ«å’Œåˆ†ç±»ç”Ÿç‰©åŒ»ç–—æ‘˜è¦ä¸­çš„éš¾è¯ã€è¡Œè¯å’Œç¼©å†™è¯ï¼Œå¹¶æŠ¥å‘Šäº†F1åˆ†æ•°ã€‚ç”±äºæ—¶é—´é™åˆ¶ï¼Œæˆ‘ä»¬æœªå®Œæˆæ›¿æ¢ä»»åŠ¡ã€‚åœ¨ä»»åŠ¡äºŒä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨Llamma3.1-70B-Instructå’ŒGPT-4oç»“åˆä¸€æ¬¡æ€§çš„æç¤ºæ¥å®Œæˆæ‘˜è¦çš„æ”¹ç¼–ï¼Œå¹¶æŠ¥å‘Šäº†BLEUã€SARIã€BERTScoreã€LENSå’ŒSALSAç­‰å„é¡¹åˆ†æ•°ã€‚æ ¹æ®PLABA-2024çš„å®˜æ–¹è¯„ä¼°ï¼Œåœ¨1Aå’Œ1Bå­ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬è¾ƒå°çš„å¾®è°ƒRoBERTa-Baseæ¨¡å‹åˆ†åˆ«æ’åç¬¬ä¸‰å’Œç¬¬äºŒï¼›åœ¨ä¸¤é¡¹ä»»åŠ¡çš„å¹³å‡F1åˆ†æ•°ä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨9ä¸ªè¯„ä¼°ç³»ç»Ÿä¸­æ’åç¬¬ä¸€ã€‚æˆ‘ä»¬çš„LLaMA-3.1-70B-Instructæ¨¡å‹åœ¨ä»»åŠ¡2ä¸­è·å¾—äº†æœ€é«˜çš„å®Œæ•´æ€§åˆ†æ•°ã€‚æˆ‘ä»¬ä¼šåœ¨<a target="_blank" rel="noopener" href="https://github.com/HECTA-UoM/PLABA2024">https://github.com/HECTA-UoM/PLABA2024</a>åˆ†äº«æˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹å’Œç›¸å…³èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07381v3">PDF</a> ongoing work - system report for PLABA2024 with TAC</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MaLeiå›¢é˜Ÿåœ¨Plain Language Adaptation of Biomedical Abstracts (PLABA) 2024å…±äº«ä»»åŠ¡ä¸­çš„ç³»ç»Ÿæè¿°ã€‚åœ¨ä»»åŠ¡ä¸€ä¸­ï¼Œå›¢é˜Ÿä½¿ç”¨fine-tuned ReBERTa-Baseæ¨¡å‹è¯†åˆ«å’Œåˆ†ç±»ç”Ÿç‰©åŒ»å­¦æ‘˜è¦ä¸­çš„éš¾è¯ã€æœ¯è¯­å’Œç¼©å†™ï¼Œå¹¶æŠ¥å‘Šäº†F1åˆ†æ•°ã€‚åœ¨ä»»åŠ¡äºŒä¸­ï¼Œä»–ä»¬åˆ©ç”¨Llamma3.1-70B-Instructå’ŒGPT-4oè¿›è¡Œäº†ä¸€æ¬¡æ€§æç¤ºæ¥å®Œæˆæ‘˜è¦çš„é€‚åº”ï¼Œå¹¶æŠ¥å‘Šäº†BLEUã€SARIã€BERTScoreã€LENSå’ŒSALSAçš„åˆ†æ•°ã€‚æ ¹æ®PLABA-2024çš„å®˜æ–¹è¯„ä¼°ï¼ŒMaLeiå›¢é˜Ÿåœ¨ä»»åŠ¡ä¸€ä¸­æ’åç¬¬ä¸‰å’Œç¬¬äºŒï¼Œå¹³å‡F1åˆ†æ•°åœ¨ä¸¤é¡¹ä»»åŠ¡ä¸­æ’åç¬¬ä¸€ï¼Œä»»åŠ¡äºŒä¸­LLaMA-3.1-70B-instructedæ¨¡å‹å®Œæ•´æ€§å¾—åˆ†æœ€é«˜ã€‚ç›¸å…³èµ„æºå’Œæ¨¡å‹å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/HECTA-UoM/PLABA2024%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HECTA-UoM/PLABA2024æ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MaLeiå›¢é˜Ÿå‚ä¸äº†PLABA 2024å…±äº«ä»»åŠ¡ï¼Œå¹¶è¿›è¡Œäº†ç³»ç»Ÿæè¿°ã€‚</li>
<li>ä»»åŠ¡ä¸€ä¸­ä½¿ç”¨fine-tuned ReBERTa-Baseæ¨¡å‹è¿›è¡Œéš¾è¯ã€æœ¯è¯­å’Œç¼©å†™çš„è¯†åˆ«ä¸åˆ†ç±»ï¼Œå–å¾—è¾ƒé«˜F1åˆ†æ•°ã€‚</li>
<li>ä»»åŠ¡äºŒä¸­åˆ©ç”¨Llamma3.1-70B-Instructå’ŒGPT-4oè¿›è¡Œæ‘˜è¦é€‚åº”ï¼Œæ¶‰åŠå¤šç§è¯„ä¼°æŒ‡æ ‡çš„åˆ†æ•°æŠ¥å‘Šã€‚</li>
<li>MaLeiå›¢é˜Ÿåœ¨ä»»åŠ¡ä¸€ä¸­æ’åç¬¬ä¸‰å’Œç¬¬äºŒï¼Œå¹³å‡F1åˆ†æ•°åœ¨ä¸¤é¡¹ä»»åŠ¡ä¸­æ’åç¬¬ä¸€ã€‚</li>
<li>LLaMA-3.1-70B-instructedæ¨¡å‹åœ¨ä»»åŠ¡äºŒä¸­çš„å®Œæ•´æ€§å¾—åˆ†æœ€é«˜ã€‚</li>
<li>MaLeiå›¢é˜Ÿåˆ†äº«äº†ä»–ä»¬çš„æ¨¡å‹å’Œèµ„æºï¼Œå¯åœ¨æŒ‡å®šé“¾æ¥å¤„è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07381">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c60a2a4d4285303ad25d0aa72754bc8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b958ed99ec3236960d5ced8472012a25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5615297355166f8ca440a1e03b2419e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cc75cb594615318b44c0dfb4eccfe1dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3eda450208da6eb9bf6d049163ebb89.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FALCON-Feedback-driven-Adaptive-Long-short-term-memory-reinforced-Coding-Optimization-system"><a href="#FALCON-Feedback-driven-Adaptive-Long-short-term-memory-reinforced-Coding-Optimization-system" class="headerlink" title="FALCON: Feedback-driven Adaptive Long&#x2F;short-term memory reinforced   Coding Optimization system"></a>FALCON: Feedback-driven Adaptive Long&#x2F;short-term memory reinforced   Coding Optimization system</h2><p><strong>Authors:Zeyuan Li, Yangfan He, Lewei He, Jianhui Wang, Tianyu Shi, Bin Lei, Yuchen Li, Qiuwu Chen</strong></p>
<p>Recently, large language models (LLMs) have achieved significant progress in automated code generation. Despite their strong instruction-following capabilities, these models frequently struggled to align with user intent in coding scenarios. In particular, they were hampered by datasets that lacked diversity and failed to address specialized tasks or edge cases. Furthermore, challenges in supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) led to failures in generating precise, human-intent-aligned code. To tackle these challenges and improve the code generation performance for automated programming systems, we propose Feedback-driven Adaptive Long&#x2F;short-term memory reinforced Coding Optimization (i.e., FALCON). FALCON is structured into two hierarchical levels. From the global level, long-term memory improves code quality by retaining and applying learned knowledge. At the local level, short-term memory allows for the incorporation of immediate feedback from compilers and AI systems. Additionally, we introduce meta-reinforcement learning with feedback rewards to solve the global-local bi-level optimization problem and enhance the modelâ€™s adaptability across diverse code generation tasks. Extensive experiments demonstrate that our technique achieves state-of-the-art performance, leading other reinforcement learning methods by more than 4.5 percentage points on the MBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The open-sourced code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/titurte/FALCON">https://github.com/titurte/FALCON</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å°½ç®¡è¿™äº›æ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ï¼Œä½†åœ¨ç¼–ç åœºæ™¯ä¸­ï¼Œå®ƒä»¬ç»å¸¸éš¾ä»¥ä¸ç”¨æˆ·æ„å›¾å¯¹é½ã€‚ç‰¹åˆ«æ˜¯ï¼Œç”±äºç¼ºä¹å¤šæ ·æ€§çš„æ•°æ®é›†ä»¥åŠæ— æ³•è§£å†³ä¸“ä¸šä»»åŠ¡æˆ–è¾¹ç¼˜æƒ…å†µï¼Œå®ƒä»¬å—åˆ°äº†é˜»ç¢ã€‚æ­¤å¤–ï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰çš„æŒ‘æˆ˜å¯¼è‡´ç”Ÿæˆç²¾ç¡®ã€ç¬¦åˆäººç±»æ„å›¾çš„ä»£ç å¤±è´¥ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜å¹¶æ”¹å–„è‡ªåŠ¨åŒ–ç¼–ç¨‹ç³»ç»Ÿçš„ä»£ç ç”Ÿæˆæ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåé¦ˆçš„è‡ªé€‚åº”é•¿çŸ­æ—¶è®°å¿†å¼ºåŒ–ç¼–ç ä¼˜åŒ–ï¼ˆå³FALCONï¼‰ã€‚FALCONåˆ†ä¸ºä¸¤ä¸ªå±‚æ¬¡ã€‚ä»å…¨å±€å±‚é¢æ¥çœ‹ï¼Œé•¿æœŸè®°å¿†é€šè¿‡ä¿ç•™å’Œåº”ç”¨å­¦åˆ°çš„çŸ¥è¯†æ¥æé«˜ä»£ç è´¨é‡ã€‚åœ¨å±€éƒ¨å±‚é¢ï¼ŒçŸ­æœŸè®°å¿†å…è®¸ç¼–è¯‘å™¨å’ŒAIç³»ç»Ÿçš„å³æ—¶åé¦ˆå¾—ä»¥èå…¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¸¦æœ‰åé¦ˆå¥–åŠ±çš„å…ƒå¼ºåŒ–å­¦ä¹ æ¥è§£å†³å…¨å±€-å±€éƒ¨ä¸¤çº§ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶å¢å¼ºæ¨¡å‹åœ¨ä¸åŒä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨MBPPåŸºå‡†æµ‹è¯•ä¸Šæ¯”å…¶ä»–å¼ºåŒ–å­¦ä¹ æ–¹æ³•é«˜å‡º4.5ä¸ªç™¾åˆ†ç‚¹ï¼Œåœ¨HumanevalåŸºå‡†æµ‹è¯•ä¸Šé«˜å‡º6.1ä¸ªç™¾åˆ†ç‚¹ã€‚å¼€æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/titurte/FALCON%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/titurte/FALCONå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21349v3">PDF</a> 20 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç”¨æˆ·æ„å›¾å¯¹é½æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æå‡ºåé¦ˆé©±åŠ¨çš„è‡ªé€‚åº”é•¿çŸ­æ—¶è®°å¿†å¢å¼ºç¼–ç ä¼˜åŒ–ï¼ˆFALCONï¼‰æ–¹æ³•ï¼Œé€šè¿‡å…¨çƒå±‚çº§é•¿æœŸè®°å¿†æ”¹å–„ä»£ç è´¨é‡å’Œæœ¬åœ°å±‚çº§çŸ­æœŸè®°å¿†ç»“åˆå³æ—¶åé¦ˆæé«˜æ¨¡å‹æ€§èƒ½ã€‚å¼•å…¥å…ƒå¼ºåŒ–å­¦ä¹ ä¸åé¦ˆå¥–åŠ±è§£å†³å…¨å±€-å±€éƒ¨ä¼˜åŒ–é—®é¢˜ï¼Œæå‡æ¨¡å‹åœ¨å¤šæ ·åŒ–ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚å®éªŒè¯æ˜ï¼ŒFALCONåœ¨MBPPå’ŒHumanevalåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆä¸­å–å¾—è¿›å±•ï¼Œä½†åœ¨ç”¨æˆ·æ„å›¾å¯¹é½æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ•°æ®é›†ç¼ºä¹å¤šæ ·æ€§å’Œæœªèƒ½è§£å†³ç‰¹å®šä»»åŠ¡æˆ–è¾¹ç¼˜æ¡ˆä¾‹æ˜¯æ¨¡å‹é¢ä¸´çš„ä¸»è¦é—®é¢˜ã€‚</li>
<li>æå‡ºåé¦ˆé©±åŠ¨çš„è‡ªé€‚åº”é•¿çŸ­æ—¶è®°å¿†å¢å¼ºç¼–ç ä¼˜åŒ–ï¼ˆFALCONï¼‰æ–¹æ³•ã€‚</li>
<li>å…¨çƒå±‚çº§é•¿æœŸè®°å¿†å’Œæœ¬åœ°å±‚çº§çŸ­æœŸè®°å¿†ç»“åˆå³æ—¶åé¦ˆæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥å…ƒå¼ºåŒ–å­¦ä¹ ä¸åé¦ˆå¥–åŠ±è§£å†³å…¨å±€-å±€éƒ¨ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>FALCONåœ¨MBPPå’ŒHumanevalåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œé¢†å…ˆå…¶ä»–å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>å¼€æ”¾æºä»£ç å·²å…¬å¼€å‘å¸ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21349">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db7a481450338388fb13e5d166fe5562.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a497010e8ea955f959ec3e3a5379f3d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ae12fd7e5e52232f48ed073e6a79554.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8acc0ed6b075510c291fc5508e2bb550.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-824e555445dceca09a6fdea56f4b2c03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-783b1ddeb96ed7d249a02fcdea4de15d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1369d596fd237fafb61d142edcf3a450.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86f6af322bcec4ff886018264932df7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c69917950088acef5f18f664e070fc0a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Satori-Towards-Proactive-AR-Assistant-with-Belief-Desire-Intention-User-Modeling"><a href="#Satori-Towards-Proactive-AR-Assistant-with-Belief-Desire-Intention-User-Modeling" class="headerlink" title="Satori: Towards Proactive AR Assistant with Belief-Desire-Intention User   Modeling"></a>Satori: Towards Proactive AR Assistant with Belief-Desire-Intention User   Modeling</h2><p><strong>Authors:Chenyi Li, Guande Wu, Gromit Yeuk-Yin Chan, Dishita G Turakhia, Sonia Castelo Quispe, Dong Li, Leslie Welch, Claudio Silva, Jing Qian</strong></p>
<p>Augmented Reality assistance are increasingly popular for supporting users with tasks like assembly and cooking. However, current practice typically provide reactive responses initialized from user requests, lacking consideration of rich contextual and user-specific information. To address this limitation, we propose a novel AR assistance system, Satori, that models both user states and environmental contexts to deliver proactive guidance. Our system combines the Belief-Desire-Intention (BDI) model with a state-of-the-art multi-modal large language model (LLM) to infer contextually appropriate guidance. The design is informed by two formative studies involving twelve experts. A sixteen within-subject study find that Satori achieves performance comparable to an designer-created Wizard-of-Oz (WoZ) system without relying on manual configurations or heuristics, thereby enhancing generalizability, reusability and opening up new possibilities for AR assistance. </p>
<blockquote>
<p>å¢å¼ºç°å®è¾…åŠ©æŠ€æœ¯è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œæ”¯æŒç”¨æˆ·å®Œæˆç»„è£…å’Œçƒ¹é¥ªç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“å‰å®è·µé€šå¸¸æä¾›åŸºäºç”¨æˆ·è¯·æ±‚çš„ååº”æ€§å“åº”ï¼Œç¼ºä¹ä¸°å¯Œçš„ä¸Šä¸‹æ–‡å’Œç”¨æˆ·ç‰¹å®šä¿¡æ¯çš„è€ƒè™‘ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ARè¾…åŠ©ç³»ç»ŸSatoriï¼Œè¯¥ç³»ç»Ÿå¯¹ç”¨æˆ·çŠ¶æ€å’Œç¯å¢ƒä¸Šä¸‹æ–‡è¿›è¡Œå»ºæ¨¡ï¼Œä»¥æä¾›ä¸»åŠ¨æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå°†ä¿¡å¿µ-æ¬²æœ›-æ„å›¾ï¼ˆBDIï¼‰æ¨¡å‹ä¸æœ€æ–°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸ç»“åˆï¼Œä»¥æ¨æ–­ç¬¦åˆä¸Šä¸‹æ–‡çš„æŒ‡å¯¼ã€‚è®¾è®¡çµæ„Ÿæ¥æºäºæ¶‰åŠåäºŒåä¸“å®¶çš„ä¸¤é¡¹å½¢æˆæ€§ç ”ç©¶ã€‚ä¸€é¡¹é’ˆå¯¹åŒä¸€ä¸»ä½“çš„å†…éƒ¨ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒSatoriåœ¨ä¸ä¾èµ–æ‰‹åŠ¨é…ç½®æˆ–å¯å‘å¼çš„æƒ…å†µä¸‹å®ç°äº†ä¸è®¾è®¡ç²¾è‰¯çš„Wizard-of-Ozï¼ˆWoZï¼‰ç³»ç»Ÿç›¸å½“çš„æ€§èƒ½ï¼Œä»è€Œå¢å¼ºäº†é€šç”¨æ€§å’Œå¯é‡ç”¨æ€§ï¼Œå¹¶ä¸ºARè¾…åŠ©å¼€è¾Ÿäº†æ–°å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16668v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºå¢å¼ºç°å®ï¼ˆARï¼‰çš„è¾…åŠ©ç³»ç»Ÿåœ¨ä»»åŠ¡æ”¯æŒï¼Œå¦‚ç»„è£…å’Œçƒ¹é¥ªæ–¹é¢çš„æ™®åŠæ€§é€æ¸å¢åŠ ã€‚ç„¶è€Œï¼Œå½“å‰å®è·µä¸»è¦æä¾›ä»ç”¨æˆ·è¯·æ±‚åˆå§‹åŒ–çš„ååº”å¼å“åº”ï¼Œç¼ºä¹ä¸°å¯Œçš„ä¸Šä¸‹æ–‡å’Œç”¨æˆ·ç‰¹å®šä¿¡æ¯çš„è€ƒè™‘ã€‚ä¸ºè§£å†³æ­¤å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ARè¾…åŠ©ç³»ç»ŸSatoriï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæ¨¡æ‹Ÿç”¨æˆ·çŠ¶æ€å’Œç¯å¢ƒä¸Šä¸‹æ–‡ä»¥æä¾›ä¸»åŠ¨æŒ‡å¯¼ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†ä¿¡å¿µ-æ¬²æœ›-æ„å›¾ï¼ˆBDIï¼‰æ¨¡å‹ä¸æœ€æ–°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä¸Šä¸‹æ–‡é€‚å½“æŒ‡å¯¼çš„æ¨æ–­ã€‚è®¾è®¡ç”±ä¸¤é¡¹åŒ…æ‹¬åäºŒåä¸“å®¶çš„å½¢æˆæ€§ç ”ç©¶æ”¯æ’‘ã€‚ä¸€é¡¹å†…éƒ¨å¯¹ç…§ç ”ç©¶å‘ç°ï¼ŒSatoriåœ¨æ— éœ€ä¾èµ–æ‰‹åŠ¨é…ç½®æˆ–å¯å‘å¼çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ä¸è®¾è®¡è‰¯å¥½çš„Wizard-of-Ozï¼ˆWoZï¼‰ç³»ç»Ÿç›¸å½“çš„æ€§èƒ½ï¼Œä»è€Œå¢å¼ºäº†é€šç”¨æ€§ã€å¯é‡ç”¨æ€§å¹¶ä¸ºARè¾…åŠ©å¼€å¯äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰ARè¾…åŠ©ç³»ç»Ÿä¸»è¦ä¸ºååº”å¼å“åº”ï¼Œéœ€æ”¹è¿›ä»¥è€ƒè™‘ä¸°å¯Œçš„ä¸Šä¸‹æ–‡å’Œç”¨æˆ·ç‰¹å®šä¿¡æ¯ã€‚</li>
<li>æ–°å‹çš„ARè¾…åŠ©ç³»ç»ŸSatorièƒ½æ¨¡æ‹Ÿç”¨æˆ·çŠ¶æ€å’Œç¯å¢ƒä¸Šä¸‹æ–‡ï¼Œæä¾›ä¸»åŠ¨æŒ‡å¯¼ã€‚</li>
<li>Satoriç³»ç»Ÿç»“åˆäº†BDIæ¨¡å‹ä¸å¤šæ¨¡æ€LLMè¿›è¡Œä¸Šä¸‹æ–‡æŒ‡å¯¼æ¨æ–­ã€‚</li>
<li>Satoriçš„è®¾è®¡åŸºäºåŒ…æ‹¬åäºŒåä¸“å®¶çš„ä¸¤é¡¹å½¢æˆæ€§ç ”ç©¶ã€‚</li>
<li>å†…éƒ¨å¯¹ç…§ç ”ç©¶è¡¨æ˜ï¼ŒSatoriçš„æ€§èƒ½ä¸è®¾è®¡è‰¯å¥½çš„WoZç³»ç»Ÿç›¸å½“ã€‚</li>
<li>Satoriå¢å¼ºäº†ARè¾…åŠ©çš„é€šç”¨æ€§å’Œå¯é‡ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5b68e4fcff5a61870454b7fe54bb310.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="TSDS-Data-Selection-for-Task-Specific-Model-Finetuning"><a href="#TSDS-Data-Selection-for-Task-Specific-Model-Finetuning" class="headerlink" title="TSDS: Data Selection for Task-Specific Model Finetuning"></a>TSDS: Data Selection for Task-Specific Model Finetuning</h2><p><strong>Authors:Zifan Liu, Amin Karbasi, Theodoros Rekatsinas</strong></p>
<p>Finetuning foundation models for specific tasks is an emerging paradigm in modern machine learning. The efficacy of task-specific finetuning largely depends on the selection of appropriate training data. We present TSDS (Task-Specific Data Selection), a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task. To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution. In addition, we add a regularizer to encourage the diversity of the selected data and incorporate kernel density estimation into the regularizer to reduce the negative effects of near-duplicates among the candidate data. We connect our optimization problem to nearest neighbor search and design efficient algorithms to compute the optimal solution based on approximate nearest neighbor search techniques. We evaluate our method on data selection for both continued pretraining and instruction tuning of language models. We show that instruction tuning using data selected by our method with a 1% selection ratio often outperforms using the full dataset and beats the baseline selection methods by 1.5 points in F1 score on average. </p>
<blockquote>
<p>åœ¨ç°ä»£æœºå™¨å­¦ä¹ ä¸­ï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡å¾®è°ƒåŸºç¡€æ¨¡å‹æ˜¯ä¸€ç§æ–°å…´èŒƒå¼ã€‚ä»»åŠ¡ç‰¹å®šå¾®è°ƒçš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºé€‚å½“è®­ç»ƒæ•°æ®çš„é€‰æ‹©ã€‚æˆ‘ä»¬æå‡ºäº†TSDSï¼ˆTask-Specific Data Selectionï¼Œä»»åŠ¡ç‰¹å®šæ•°æ®é€‰æ‹©ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸ºä»»åŠ¡ç‰¹å®šæ¨¡å‹å¾®è°ƒé€‰æ‹©æ•°æ®çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”±ç›®æ ‡ä»»åŠ¡çš„å°è€Œå…·æœ‰ä»£è¡¨æ€§çš„æ ·æœ¬é›†å¼•å¯¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä»»åŠ¡ç‰¹å®šå¾®è°ƒçš„æ•°æ®é€‰æ‹©åˆ¶å®šä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡åŸºäºæœ€ä¼˜ä¼ è¾“çš„åˆ†å¸ƒå¯¹é½æŸå¤±æ¥æ•è·æ‰€é€‰æ•°æ®ä¸ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ·»åŠ ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹æ¥é¼“åŠ±æ‰€é€‰æ•°æ®çš„å¤šæ ·æ€§ï¼Œå¹¶å°†å†…æ ¸å¯†åº¦ä¼°è®¡çº³å…¥æ­£åˆ™åŒ–é¡¹ä¸­ï¼Œä»¥å‡å°‘å€™é€‰æ•°æ®ä¸­è¿‘ä¼¼é‡å¤é¡¹çš„è´Ÿé¢å½±å“ã€‚æˆ‘ä»¬å°†ä¼˜åŒ–é—®é¢˜ä¸æœ€è¿‘é‚»æœç´¢è”ç³»èµ·æ¥ï¼Œå¹¶è®¾è®¡åŸºäºè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢æŠ€æœ¯çš„æœ‰æ•ˆç®—æ³•æ¥è®¡ç®—æœ€ä¼˜è§£ã€‚æˆ‘ä»¬åœ¨å¯¹è¯­è¨€æ¨¡å‹çš„æŒç»­é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´çš„æ•°æ®é€‰æ‹©ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•é€‰å®šçš„æ•°æ®è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œå³ä½¿ä½¿ç”¨1%çš„é€‰æ‹©æ¯”ç‡ï¼Œä¹Ÿå¾€å¾€ä¼˜äºä½¿ç”¨å…¨æ•°æ®é›†ï¼Œå¹¶ä¸”åœ¨F1åˆ†æ•°ä¸Šå¹³å‡æ¯”åŸºçº¿é€‰æ‹©æ–¹æ³•é«˜å‡º1.5ä¸ªç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11303v3">PDF</a> 31 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>ä»»åŠ¡ç‰¹å®šå¾®è°ƒæ˜¯ç°ä»£æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªæ–°å…´èŒƒå¼ï¼Œå…¶æ•ˆæœåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºé€‚å½“è®­ç»ƒæ•°æ®çš„é€‰æ‹©ã€‚æœ¬æ–‡æå‡ºTSDSï¼ˆä»»åŠ¡ç‰¹å®šæ•°æ®é€‰æ‹©ï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç›®æ ‡ä»»åŠ¡ä¸­å°è€Œå…·ä»£è¡¨æ€§çš„æ ·æœ¬é›†å¼•å¯¼ä»»åŠ¡ç‰¹å®šæ¨¡å‹å¾®è°ƒçš„æ•°æ®é€‰æ‹©ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå°†æ•°æ®é€‰æ‹©ä½œä¸ºåŸºäºæœ€ä¼˜ä¼ è¾“çš„åˆ†å¸ƒå¯¹é½æŸå¤±ä¼˜åŒ–é—®é¢˜ï¼Œä»¥æ•æ‰æ‰€é€‰æ•°æ®ä¸ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œè¿˜æ·»åŠ äº†ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ä»¥é¼“åŠ±æ‰€é€‰æ•°æ®çš„å¤šæ ·æ€§ï¼Œå¹¶ç»“åˆæ ¸å¯†åº¦ä¼°è®¡å‡å°‘å€™é€‰æ•°æ®ä¸­çš„è¿‘ä¼¼é‡å¤é¡¹çš„è´Ÿé¢å½±å“ã€‚æœ¬æ–‡é€šè¿‡å°†ä¼˜åŒ–é—®é¢˜ä¸æœ€è¿‘é‚»æœç´¢è”ç³»èµ·æ¥ï¼Œå¹¶åŸºäºè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢æŠ€æœ¯è®¾è®¡äº†æœ‰æ•ˆçš„ç®—æ³•æ¥è®¡ç®—æœ€ä¼˜è§£ã€‚é€šè¿‡å¯¹è¯­è¨€æ¨¡å‹çš„æŒç»­é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒä¼˜çš„æ•°æ®é€‰æ‹©è¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ‰€é€‰æ•°æ®çš„æŒ‡ä»¤è°ƒä¼˜æ–¹æ³•åœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¶Šäº†ä½¿ç”¨å…¨æ•°æ®é›†çš„æ•ˆæœï¼Œå¹¶ä¸”åœ¨F1åˆ†æ•°ä¸Šå¹³å‡æ¯”åŸºçº¿é€‰æ‹©æ–¹æ³•é«˜å‡º1.5åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»»åŠ¡ç‰¹å®šå¾®è°ƒæ˜¯ç°ä»£æœºå™¨å­¦ä¹ çš„æ–°è¶‹åŠ¿ï¼Œè®­ç»ƒæ•°æ®é€‰æ‹©è‡³å…³é‡è¦ã€‚</li>
<li>TSDSæ¡†æ¶é€šè¿‡å°æ ·æœ¬é›†å¼•å¯¼ä»»åŠ¡ç‰¹å®šæ¨¡å‹å¾®è°ƒçš„æ•°æ®é€‰æ‹©ã€‚</li>
<li>æ•°æ®é€‰æ‹©è¢«è¡¨è¿°ä¸ºåˆ†å¸ƒå¯¹é½æŸå¤±çš„ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>æ­£åˆ™åŒ–é¡¹é¼“åŠ±æ•°æ®å¤šæ ·æ€§ï¼Œç»“åˆæ ¸å¯†åº¦ä¼°è®¡å‡å°‘è¿‘ä¼¼é‡å¤çš„å½±å“ã€‚</li>
<li>ä¼˜åŒ–é—®é¢˜ä¸æœ€è¿‘é‚»æœç´¢ç›¸ç»“åˆï¼Œè®¾è®¡äº†é«˜æ•ˆç®—æ³•å¯»æ‰¾æœ€ä¼˜è§£ã€‚</li>
<li>åœ¨è¯­è¨€æ¨¡å‹çš„æŒç»­é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒä¼˜ä¸­è¯„ä¼°æ–¹æ³•ï¼Œæ•ˆæœæ˜æ˜¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11303">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-820c82b5a3911949856b751cab54ac6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-309ec78646f425613124ecc5e21c097b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1d4119d764aed3bf8f9c8f4396186dd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Reconstructive-Visual-Instruction-Tuning"><a href="#Reconstructive-Visual-Instruction-Tuning" class="headerlink" title="Reconstructive Visual Instruction Tuning"></a>Reconstructive Visual Instruction Tuning</h2><p><strong>Authors:Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang, Zheng Ge, Xiangyu Zhang, Zhaoxiang Zhang</strong></p>
<p>This paper introduces reconstructive visual instruction tuning (ROSS), a family of Large Multimodal Models (LMMs) that exploit vision-centric supervision signals. In contrast to conventional visual instruction tuning approaches that exclusively supervise text outputs, ROSS prompts LMMs to supervise visual outputs via reconstructing input images. By doing so, it capitalizes on the inherent richness and detail present within input images themselves, which are often lost in pure text supervision. However, producing meaningful feedback from natural images is challenging due to the heavy spatial redundancy of visual signals. To address this issue, ROSS employs a denoising objective to reconstruct latent representations of input images, avoiding directly regressing exact raw RGB values. This intrinsic activation design inherently encourages LMMs to maintain image detail, thereby enhancing their fine-grained comprehension capabilities and reducing hallucinations. Empirically, ROSS consistently brings significant improvements across different visual encoders and language models. In comparison with extrinsic assistance state-of-the-art alternatives that aggregate multiple visual experts, ROSS delivers competitive performance with a single SigLIP visual encoder, demonstrating the efficacy of our vision-centric supervision tailored for visual outputs. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†é‡å»ºè§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆROSSï¼‰ï¼Œè¿™æ˜¯ä¸€ç±»åˆ©ç”¨è§†è§‰ä¸ºä¸­å¿ƒç›‘ç£ä¿¡å·çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ã€‚ä¸ä¼ ç»Ÿä»…ç›‘ç£æ–‡æœ¬è¾“å‡ºçš„è§†è§‰æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ä¸åŒï¼ŒROSSé€šè¿‡é‡å»ºè¾“å…¥å›¾åƒæ¥ä¿ƒä½¿LMMsç›‘ç£è§†è§‰è¾“å‡ºã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå®ƒå……åˆ†åˆ©ç”¨äº†è¾“å…¥å›¾åƒæœ¬èº«æ‰€å›ºæœ‰çš„ä¸°å¯Œæ€§å’Œç»†èŠ‚ï¼Œè¿™äº›ç»†èŠ‚åœ¨çº¯æ–‡æœ¬ç›‘ç£ä¸­å¾€å¾€ä¼šè¢«ä¸¢å¤±ã€‚ç„¶è€Œï¼Œä»è‡ªç„¶å›¾åƒä¸­äº§ç”Ÿæœ‰æ„ä¹‰çš„åé¦ˆæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œå› ä¸ºè§†è§‰ä¿¡å·å­˜åœ¨å¤§é‡çš„ç©ºé—´å†—ä½™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒROSSé‡‡ç”¨å»å™ªç›®æ ‡æ¥é‡å»ºè¾“å…¥å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºï¼Œé¿å…ç›´æ¥å›å½’ç²¾ç¡®çš„åŸå§‹RGBå€¼ã€‚è¿™ç§å†…åœ¨æ¿€æ´»è®¾è®¡é¼“åŠ±LMMsä¿æŒå›¾åƒç»†èŠ‚ï¼Œä»è€Œæé«˜äº†å®ƒä»¬çš„ç²¾ç»†ç†è§£èƒ½åŠ›å’Œå‡å°‘äº†å¹»è§‰ã€‚ç»éªŒä¸Šï¼ŒROSSåœ¨ä¸åŒç±»å‹çš„è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹ä¸Šå‡å¸¦æ¥äº†æ˜¾è‘—æ”¹è¿›ã€‚ä¸èšé›†å¤šä¸ªè§†è§‰ä¸“å®¶çš„å¤–åœ¨è¾…åŠ©æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œä½¿ç”¨å•ä¸€SigLIPè§†è§‰ç¼–ç å™¨çš„ROSSè¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„é’ˆå¯¹è§†è§‰è¾“å‡ºçš„è§†è§‰ä¸ºä¸­å¿ƒç›‘ç£çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09575v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é‡å»ºè§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆROSSï¼‰æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨è§†è§‰ä¸­å¿ƒç›‘ç£ä¿¡å·çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å®¶æ—ã€‚ä¸ä¼ ç»Ÿçš„ä»…ç›‘ç£æ–‡æœ¬è¾“å‡ºçš„è§†è§‰æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ä¸åŒï¼ŒROSSé€šè¿‡é‡å»ºè¾“å…¥å›¾åƒæ¥ä¿ƒä½¿LMMsç›‘ç£è§†è§‰è¾“å‡ºã€‚å®ƒå……åˆ†åˆ©ç”¨äº†è¾“å…¥å›¾åƒæœ¬èº«çš„å†…åœ¨ä¸°å¯Œæ€§å’Œç»†èŠ‚ï¼Œé¿å…äº†çº¯æ–‡æœ¬ç›‘ç£ä¸­å¸¸å¸¸ä¸¢å¤±çš„ä¿¡æ¯ã€‚ä¸ºåº”å¯¹è‡ªç„¶å›¾åƒäº§ç”Ÿçš„æœ‰æ„ä¹‰åé¦ˆæ‰€é¢ä¸´çš„è§†è§‰ä¿¡å·ç©ºé—´å†—ä½™é—®é¢˜ï¼ŒROSSé‡‡ç”¨å»å™ªç›®æ ‡æ¥é‡å»ºè¾“å…¥å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºï¼Œé¿å…ç›´æ¥å›å½’ç²¾ç¡®çš„åŸå§‹RGBå€¼ã€‚è¿™ç§å†…åœ¨æ¿€æ´»è®¾è®¡é¼“åŠ±LMMsä¿æŒå›¾åƒç»†èŠ‚ï¼Œä»è€Œæé«˜å…¶ç²¾ç»†ç²’åº¦ç†è§£èƒ½åŠ›å¹¶å‡å°‘å¹»è§‰ã€‚å®è¯ç»“æœæ˜¾ç¤ºï¼ŒROSSåœ¨ä¸åŒè§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹ä¸Šå‡å¸¦æ¥æ˜¾è‘—æ”¹è¿›ï¼Œä¸èšåˆå¤šä¸ªè§†è§‰ä¸“å®¶çš„å¤–åœ¨è¾…åŠ©æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œä½¿ç”¨å•ä¸€SigLIPè§†è§‰ç¼–ç å™¨çš„ROSSè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œè¯æ˜äº†é’ˆå¯¹è§†è§‰è¾“å‡ºçš„è§†è§‰ä¸­å¿ƒç›‘ç£çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ROSSæ˜¯ä¸€ç§åˆ©ç”¨è§†è§‰ä¸­å¿ƒç›‘ç£ä¿¡å·çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ã€‚</li>
<li>ROSSé€šè¿‡é‡å»ºè¾“å…¥å›¾åƒæ¥ä¿ƒä½¿LMMsç›‘ç£è§†è§‰è¾“å‡ºï¼Œå……åˆ†åˆ©ç”¨è¾“å…¥å›¾åƒçš„å†…åœ¨ä¸°å¯Œæ€§å’Œç»†èŠ‚ã€‚</li>
<li>ROSSé‡‡ç”¨å»å™ªç›®æ ‡æ¥åº”å¯¹è‡ªç„¶å›¾åƒäº§ç”Ÿçš„æœ‰æ„ä¹‰åé¦ˆæ‰€é¢ä¸´çš„è§†è§‰ä¿¡å·ç©ºé—´å†—ä½™é—®é¢˜ã€‚</li>
<li>é€šè¿‡å†…åœ¨æ¿€æ´»è®¾è®¡ï¼ŒROSSé¼“åŠ±LMMsä¿æŒå›¾åƒç»†èŠ‚ï¼Œæé«˜ç²¾ç»†ç²’åº¦ç†è§£èƒ½åŠ›å¹¶å‡å°‘å¹»è§‰ã€‚</li>
<li>ROSSåœ¨å¤šç§è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
<li>ä¸èšåˆå¤šä¸ªè§†è§‰ä¸“å®¶çš„å¤–åœ¨è¾…åŠ©æŠ€æœ¯ç›¸æ¯”ï¼Œä½¿ç”¨å•ä¸€SigLIPè§†è§‰ç¼–ç å™¨çš„ROSSè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09575">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1711cebd4b8448dbae6ded0b25387c6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf3819bf25a8ccd994b7544111fe8d17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-348013c2b4a70c63287ea64e738c45bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b793fe813d544c16ddfabc613cb8778.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7d87c9b3aa4dc8b565e9c6ba62ebf79.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Text-Clustering-as-Classification-with-LLMs"><a href="#Text-Clustering-as-Classification-with-LLMs" class="headerlink" title="Text Clustering as Classification with LLMs"></a>Text Clustering as Classification with LLMs</h2><p><strong>Authors:Chen Huang, Guoxiu He</strong></p>
<p>Text clustering remains valuable in real-world applications where manual labeling is cost-prohibitive. It facilitates efficient organization and analysis of information by grouping similar texts based on their representations. However, implementing this approach necessitates fine-tuned embedders for downstream data and sophisticated similarity metrics. To address this issue, this study presents a novel framework for text clustering that effectively leverages the in-context learning capacity of Large Language Models (LLMs). Instead of fine-tuning embedders, we propose to transform the text clustering into a classification task via LLM. First, we prompt LLM to generate potential labels for a given dataset. Second, after integrating similar labels generated by the LLM, we prompt the LLM to assign the most appropriate label to each sample in the dataset. Our framework has been experimentally proven to achieve comparable or superior performance to state-of-the-art clustering methods that employ embeddings, without requiring complex fine-tuning or clustering algorithms. We make our code available to the public for utilization at <a target="_blank" rel="noopener" href="https://github.com/ECNU-Text-Computing/Text-Clustering-via-LLM">https://github.com/ECNU-Text-Computing/Text-Clustering-via-LLM</a>. </p>
<blockquote>
<p>æ–‡æœ¬èšç±»åœ¨ç°å®ä¸–ç•Œçš„è®¸å¤šåº”ç”¨ä¸­ä»ç„¶å…·æœ‰ä»·å€¼ï¼Œåœ¨è¿™äº›åº”ç”¨ä¸­ï¼Œæ‰‹åŠ¨æ ‡æ³¨çš„æˆæœ¬é«˜æ˜‚ã€‚å®ƒé€šè¿‡åŸºäºæ–‡æœ¬è¡¨ç¤ºå°†ç›¸ä¼¼çš„æ–‡æœ¬åˆ†ç»„ï¼Œä¿ƒè¿›äº†ä¿¡æ¯çš„æœ‰æ•ˆç»„ç»‡å’Œåˆ†æã€‚ç„¶è€Œï¼Œå®ç°è¿™ç§æ–¹æ³•éœ€è¦é’ˆå¯¹ä¸‹æ¸¸æ•°æ®çš„ç²¾ç»†åµŒå…¥å™¨ä»¥åŠå¤æ‚çš„ç›¸ä¼¼æ€§åº¦é‡æ ‡å‡†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬èšç±»æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚æˆ‘ä»¬ä¸éœ€è¦ç²¾ç»†è°ƒæ•´åµŒå…¥å™¨ï¼Œè€Œæ˜¯æè®®é€šè¿‡å°†æ–‡æœ¬èšç±»è½¬åŒ–ä¸ºåˆ†ç±»ä»»åŠ¡æ¥åˆ©ç”¨LLMçš„èƒ½åŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æç¤ºLLMä¸ºç»™å®šæ•°æ®é›†ç”Ÿæˆæ½œåœ¨æ ‡ç­¾ã€‚å…¶æ¬¡ï¼Œåœ¨æ•´åˆLLMç”Ÿæˆçš„ç›¸ä¼¼æ ‡ç­¾åï¼Œæˆ‘ä»¬å†æ¬¡æç¤ºLLMä¸ºæ•°æ®é›†ä¸­çš„æ¯ä¸ªæ ·æœ¬åˆ†é…æœ€é€‚å½“çš„æ ‡ç­¾ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸é‡‡ç”¨åµŒå…¥çš„å…ˆè¿›èšç±»æ–¹æ³•ç›¸æ¯”ï¼Œå¯ä»¥è¾¾åˆ°ç›¸å½“çš„æˆ–æ›´å¥½çš„æ€§èƒ½ï¼Œæ— éœ€å¤æ‚çš„å¾®è°ƒæˆ–èšç±»ç®—æ³•ã€‚æˆ‘ä»¬å·²å°†ä»£ç å…¬å¼€ä¾›å…¬ä¼—ä½¿ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ECNU-Text-Computing/Text-Clustering-via-LLM%E3%80%82">https://github.com/ECNU-Text-Computing/Text-Clustering-via-LLMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.00927v2">PDF</a> 12 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬èšç±»æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†æ–‡æœ¬èšç±»è½¬åŒ–ä¸ºåˆ†ç±»ä»»åŠ¡ï¼Œåˆ©ç”¨LLMçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ç”Ÿæˆæ½œåœ¨æ ‡ç­¾å¹¶åˆ†é…æ ‡ç­¾ç»™æ•°æ®é›†ä¸­çš„æ ·æœ¬ï¼Œæ— éœ€å¤æ‚çš„å¾®è°ƒæˆ–èšç±»ç®—æ³•ï¼Œå³å¯å®ç°ä¸åŸºäºåµŒå…¥çš„å…ˆè¿›èšç±»æ–¹æ³•ç›¸å½“çš„æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬èšç±»åœ¨æ‰‹åŠ¨æ ‡ç­¾æˆæœ¬é«˜æ˜‚çš„ç°å®ä¸–ç•Œåº”ç”¨ä¸­å…·æœ‰é‡è¦ä»·å€¼ï¼Œæœ‰åŠ©äºé«˜æ•ˆç»„ç»‡å’Œåˆ†æä¿¡æ¯ã€‚</li>
<li>å®æ–½æ–‡æœ¬èšç±»éœ€è¦é’ˆå¯¹ä¸‹æ¸¸æ•°æ®çš„ç²¾ç»†åµŒå…¥å™¨å’Œå¤æ‚ç›¸ä¼¼æ€§åº¦é‡ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬èšç±»æ–°æ¡†æ¶ï¼Œåˆ©ç”¨LLMçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å°†æ–‡æœ¬èšç±»è½¬åŒ–ä¸ºåˆ†ç±»ä»»åŠ¡æ¥å·¥ä½œï¼Œç”Ÿæˆæ½œåœ¨æ ‡ç­¾å¹¶åˆ†é…è¿™äº›æ ‡ç­¾ç»™æ•°æ®é›†ä¸­çš„æ ·æœ¬ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€å¤æ‚çš„å¾®è°ƒæˆ–èšç±»ç®—æ³•ï¼Œå³å¯å®ç°ä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯ç›¸å½“çš„æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>å…¬å¼€äº†ä»£ç ä»¥ä¾›å…¬ä¼—ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.00927">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-005f059db6fd95bcdeac5a431e415d25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48b7c310a7c1349c607609fa93bb68ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df95226dc9558ac7309771486e285943.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SoloAudio-Target-Sound-Extraction-with-Language-oriented-Audio-Diffusion-Transformer"><a href="#SoloAudio-Target-Sound-Extraction-with-Language-oriented-Audio-Diffusion-Transformer" class="headerlink" title="SoloAudio: Target Sound Extraction with Language-oriented Audio   Diffusion Transformer"></a>SoloAudio: Target Sound Extraction with Language-oriented Audio   Diffusion Transformer</h2><p><strong>Authors:Helin Wang, Jiarui Hai, Yen-Ju Lu, Karan Thakkar, Mounya Elhilali, Najim Dehak</strong></p>
<p>In this paper, we introduce SoloAudio, a novel diffusion-based generative model for target sound extraction (TSE). Our approach trains latent diffusion models on audio, replacing the previous U-Net backbone with a skip-connected Transformer that operates on latent features. SoloAudio supports both audio-oriented and language-oriented TSE by utilizing a CLAP model as the feature extractor for target sounds. Furthermore, SoloAudio leverages synthetic audio generated by state-of-the-art text-to-audio models for training, demonstrating strong generalization to out-of-domain data and unseen sound events. We evaluate this approach on the FSD Kaggle 2018 mixture dataset and real data from AudioSet, where SoloAudio achieves the state-of-the-art results on both in-domain and out-of-domain data, and exhibits impressive zero-shot and few-shot capabilities. Source code and demos are released. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†SoloAudioï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹ç›®æ ‡å£°éŸ³æå–ï¼ˆTSEï¼‰ç”Ÿæˆæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•è®­ç»ƒéŸ³é¢‘ä¸Šçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œç”¨æ“ä½œæ½œåœ¨ç‰¹å¾çš„è·³è¿‡è¿æ¥çš„Transformeræ›¿æ¢ä¹‹å‰çš„U-Netä¸»å¹²ã€‚SoloAudioé€šè¿‡åˆ©ç”¨CLAPæ¨¡å‹ä½œä¸ºç›®æ ‡å£°éŸ³çš„ç‰¹å¾æå–å™¨ï¼Œæ”¯æŒé¢å‘éŸ³é¢‘å’Œé¢å‘è¯­è¨€çš„TSEã€‚æ­¤å¤–ï¼ŒSoloAudioè¿˜åˆ©ç”¨æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°éŸ³é¢‘æ¨¡å‹ç”Ÿæˆçš„åˆæˆéŸ³é¢‘è¿›è¡Œè®­ç»ƒï¼Œè¡¨ç°å‡ºå¯¹åŸŸå¤–æ•°æ®å’Œæœªè§å£°éŸ³äº‹ä»¶çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨FSD Kaggle 2018æ··åˆæ•°æ®é›†å’ŒAudioSetçš„çœŸå®æ•°æ®ä¸Šè¯„ä¼°äº†è¿™ç§æ–¹æ³•ï¼ŒSoloAudioåœ¨åŸŸå†…å’ŒåŸŸå¤–æ•°æ®ä¸Šéƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼Œå¹¶å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬èƒ½åŠ›ã€‚å·²å‘å¸ƒæºä»£ç å’Œæ¼”ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08425v2">PDF</a> Submitted to ICASSP 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SoloAudioï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹ç›®æ ‡å£°éŸ³æå–ï¼ˆTSEï¼‰ç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ–¹æ³•å¯¹éŸ³é¢‘è¿›è¡Œæ½œä¼æ‰©æ•£æ¨¡å‹è®­ç»ƒï¼Œç”¨è·³è·ƒè¿æ¥çš„Transformeræ›¿æ¢ä¹‹å‰çš„U-Netä¸»å¹²ï¼Œè¯¥Transformeråœ¨æ½œä¼ç‰¹å¾ä¸Šè¿è¡Œã€‚SoloAudioé€šè¿‡åˆ©ç”¨é¢å‘éŸ³é¢‘å’Œé¢å‘è¯­è¨€çš„TSEï¼Œä»¥åŠåˆ©ç”¨CLAPæ¨¡å‹ä½œä¸ºç›®æ ‡å£°éŸ³çš„ç‰¹å¾æå–å™¨ï¼Œæ”¯æŒé¢å‘éŸ³é¢‘å’Œé¢å‘è¯­è¨€çš„TSEã€‚æ­¤å¤–ï¼ŒSoloAudioè¿˜åˆ©ç”¨æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°éŸ³é¢‘æ¨¡å‹ç”Ÿæˆçš„åˆæˆéŸ³é¢‘è¿›è¡Œè®­ç»ƒï¼Œè¡¨ç°å‡ºå¯¹åŸŸå¤–æ•°æ®å’Œæœªè§å£°éŸ³äº‹ä»¶çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨FSD Kaggle 2018æ··åˆæ•°æ®é›†å’ŒAudioSetçš„çœŸå®æ•°æ®ä¸Šè¯„ä¼°äº†è¿™ç§æ–¹æ³•ï¼ŒSoloAudioåœ¨åŸŸå†…å’ŒåŸŸå¤–æ•°æ®ä¸Šéƒ½å–å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœï¼Œå¹¶è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬èƒ½åŠ›ã€‚å…¬å¼€äº†æºä»£ç å’Œæ¼”ç¤ºã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ä»‹ç»äº†SoloAudioä½œä¸ºä¸€ç§æ–°å‹ç›®æ ‡å£°éŸ³æå–ï¼ˆTSEï¼‰ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨æ½œä¼æ‰©æ•£æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œæ›¿ä»£U-Netä¸»å¹²ï¼Œé‡‡ç”¨è·³è·ƒè¿æ¥çš„Transformerã€‚</li>
<li>æ”¯æŒéŸ³é¢‘å’Œè¯­è¨€ä¸¤ç§é¢å‘çš„TSEï¼Œåˆ©ç”¨CLAPæ¨¡å‹æå–ç›®æ ‡å£°éŸ³ç‰¹å¾ã€‚</li>
<li>åˆ©ç”¨å…ˆè¿›æ–‡æœ¬åˆ°éŸ³é¢‘æ¨¡å‹ç”Ÿæˆçš„åˆæˆéŸ³é¢‘è¿›è¡Œè®­ç»ƒã€‚</li>
<li>å¯¹åŸŸå†…å’ŒåŸŸå¤–æ•°æ®ä»¥åŠé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬èƒ½åŠ›è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>åœ¨FSD Kaggle 2018æ··åˆæ•°æ®é›†å’ŒAudioSetçœŸå®æ•°æ®ä¸Šå–å¾—æœ€æ–°æŠ€æœ¯æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-192ea7ae97aa3a854bfd90bae2e5b0b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33457aae34e6d8908fda1c71bbbc9f7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33de595edf63b0919720e7ad831d3ff6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18b370684c797bab7f699525f8226fea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3063b37c00b28c47ac44c6346e7e037e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2deeca685f3f92dcdc9b6f7c6792337.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58874e01a6285ef5933685313d3c1fff.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="The-Mamba-in-the-Llama-Distilling-and-Accelerating-Hybrid-Models"><a href="#The-Mamba-in-the-Llama-Distilling-and-Accelerating-Hybrid-Models" class="headerlink" title="The Mamba in the Llama: Distilling and Accelerating Hybrid Models"></a>The Mamba in the Llama: Distilling and Accelerating Hybrid Models</h2><p><strong>Authors:Junxiong Wang, Daniele Paliotta, Avner May, Alexander M. Rush, Tri Dao</strong></p>
<p>Linear RNN architectures, like Mamba, can be competitive with Transformer models in language modeling while having advantageous deployment characteristics. Given the focus on training large-scale Transformer models, we consider the challenge of converting these pretrained models for deployment. We demonstrate that it is feasible to distill large Transformers into linear RNNs by reusing the linear projection weights from attention layers with academic GPU resources. The resulting hybrid model, which incorporates a quarter of the attention layers, achieves performance comparable to the original Transformer in chat benchmarks and outperforms open-source hybrid Mamba models trained from scratch with trillions of tokens in both chat benchmarks and general benchmarks. Moreover, we introduce a hardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid models. Overall we show how, with limited computation resources, we can remove many of the original attention layers and generate from the resulting model more efficiently. Our top-performing model, distilled from Llama3-8B-Instruct, achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and 7.35 on MT-Bench, surpassing the best 8B scale instruction-tuned linear RNN model. We also find that the distilled model has natural length extrapolation, showing almost perfect accuracy in the needle-in-a-haystack test at 20x the distillation length. Code and pre-trained checkpoints are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/jxiw/MambaInLlama">https://github.com/jxiw/MambaInLlama</a> and <a target="_blank" rel="noopener" href="https://github.com/itsdaniele/speculative_mamba">https://github.com/itsdaniele/speculative_mamba</a>. </p>
<blockquote>
<p>çº¿æ€§RNNæ¶æ„ï¼ˆå¦‚Mambaï¼‰åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢å¯ä»¥ä¸Transformeræ¨¡å‹ç«äº‰ï¼ŒåŒæ—¶å…·æœ‰ä¼˜åŠ¿éƒ¨ç½²ç‰¹æ€§ã€‚é‰´äºå¤§å‹Transformeræ¨¡å‹çš„è®­ç»ƒé‡ç‚¹ï¼Œæˆ‘ä»¬é¢ä¸´ç€å°†è¿™äº›é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œéƒ¨ç½²çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¯æ˜äº†åˆ©ç”¨å­¦æœ¯GPUèµ„æºï¼Œé€šè¿‡é‡ç”¨æ³¨æ„åŠ›å±‚çš„çº¿æ€§æŠ•å½±æƒé‡ï¼Œå°†å¤§å‹Transformerè’¸é¦åˆ°çº¿æ€§RNNä¸­æ˜¯å¯è¡Œçš„ã€‚æ‰€å¾—æ··åˆæ¨¡å‹ä»…åŒ…å«å››åˆ†ä¹‹ä¸€çš„æ³¨æ„åŠ›å±‚ï¼Œåœ¨èŠå¤©åŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½ä¸åŸå§‹Transformerç›¸å½“ï¼Œå¹¶ä¸”åœ¨èŠå¤©åŸºå‡†æµ‹è¯•å’Œä¸€èˆ¬åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¼˜äºä»å¤´å¼€å§‹è®­ç»ƒã€æ¶µç›–æ•°åäº¿æ ‡è®°çš„å¼€æºæ··åˆMambaæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç¡¬ä»¶æ„ŸçŸ¥çš„æŠ•æœºè§£ç ç®—æ³•ï¼Œæé«˜äº†Mambaå’Œæ··åˆæ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹ï¼Œå»é™¤è®¸å¤šåŸå§‹æ³¨æ„åŠ›å±‚ï¼Œå¹¶ä»æ‰€å¾—æ¨¡å‹æ›´é«˜æ•ˆåœ°ç”Ÿæˆã€‚æˆ‘ä»¬çš„é«˜æ€§èƒ½æ¨¡å‹ä»Llama3-8B-Instructä¸­æç‚¼å‡ºæ¥ï¼Œåœ¨AlpacaEval 2ä¸Šç›¸å¯¹äºGPT-4è¾¾åˆ°äº†29.61çš„å—æ§é•¿åº¦èƒœç‡ï¼Œåœ¨MT-Benchä¸Šè¾¾åˆ°äº†7.35ï¼Œè¶…è¿‡äº†æœ€ä½³8Bè§„æ¨¡æŒ‡ä»¤è°ƒæ•´çº¿æ€§RNNæ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œæç‚¼åçš„æ¨¡å‹å…·æœ‰è‡ªç„¶çš„é•¿åº¦æ‰©å±•æ€§ï¼Œåœ¨è’¸é¦é•¿åº¦æé«˜20å€çš„æƒ…å†µä¸‹ï¼Œå‡ ä¹è¾¾åˆ°äº†å®Œç¾çš„å‡†ç¡®åº¦ï¼Œå¦‚åœ¨â€œæ²™é‡Œæ·˜é‡‘â€æµ‹è¯•ä¸­æ‰€å±•ç¤ºçš„ã€‚ä»£ç å’Œé¢„å…ˆè®­ç»ƒçš„æ£€æŸ¥ç‚¹å·²å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/jxiw/MambaInLlama">https://github.com/jxiw/MambaInLlama</a> å’Œ <a target="_blank" rel="noopener" href="https://github.com/itsdaniele/speculative_mamba">https://github.com/itsdaniele/speculative_mamba</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.15237v2">PDF</a> NeurIPS 2024. v2 updates: 1. Improved distillation approach and new   results for Llama 3.1&#x2F;3.2 distilled models. 2. Fixed math typos. 3. Added   needle in the haystack long-context experiments. 4. Mentioned Mamba-Zephyr as   subquadratic and added Mamba-Zephyr-8B lm_eval result</p>
<p><strong>æ‘˜è¦</strong></p>
<p>çº¿æ€§RNNæ¶æ„å¦‚Mambaåœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢å¯ä¸Transformeræ¨¡å‹ç«äº‰ï¼Œå¹¶å…·æœ‰ä¼˜åŠ¿éƒ¨ç½²ç‰¹æ€§ã€‚æˆ‘ä»¬å…³æ³¨äºå°†å¤§å‹Transformeræ¨¡å‹è¿›è¡Œéƒ¨ç½²çš„æŒ‘æˆ˜ï¼Œå¹¶å±•ç¤ºäº†ä½¿ç”¨å­¦æœ¯GPUèµ„æºå°†å¤§å‹Transformerè’¸é¦åˆ°çº¿æ€§RNNä¸­çš„å¯è¡Œæ€§ã€‚é€šè¿‡å¤ç”¨æ³¨æ„åŠ›å±‚çš„çº¿æ€§æŠ•å½±æƒé‡ï¼Œå¾—åˆ°çš„æ··åˆæ¨¡å‹å®ç°äº†ä¸åŸå§‹Transformerç›¸å½“çš„èŠå¤©åŸºå‡†æµ‹è¯•æ€§èƒ½ï¼Œå¹¶åœ¨ä¸€èˆ¬åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä»å¤´å¼€å§‹è®­ç»ƒçš„å¼€æºæ··åˆMambaæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç¡¬ä»¶æ„ŸçŸ¥çš„æŠ•æœºè§£ç ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥åŠ é€ŸMambaå’Œæ··åˆæ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹ç§»é™¤è®¸å¤šåŸå§‹æ³¨æ„åŠ›å±‚å¹¶ä»æ‰€å¾—æ¨¡å‹ä¸­æ›´æœ‰æ•ˆåœ°ç”Ÿæˆæ•°æ®ã€‚æˆ‘ä»¬çš„é«˜æ€§èƒ½æ¨¡å‹æ˜¯ä»Llama3-8B-Instructè’¸é¦è€Œæ¥ï¼Œåœ¨AlpacaEval 2å¯¹GPT-4æ¯”èµ›ä¸­è·å¾—29.61çš„å—æ§é•¿åº¦èƒœç‡ï¼Œå¹¶åœ¨MT-Benchä¸Šè·å¾—7.35çš„æˆç»©ï¼Œè¶…è¶Šäº†æœ€ä½³çš„8BæŒ‡ä»¤å¾®è°ƒçº¿æ€§RNNæ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å‘ç°è’¸é¦æ¨¡å‹å…·æœ‰è‡ªç„¶é•¿åº¦å¤–æ¨èƒ½åŠ›ï¼Œåœ¨è’¸é¦é•¿åº¦20å€çš„æƒ…å†µä¸‹ï¼Œå‡ ä¹è¾¾åˆ°äº†å®Œç¾çš„å‡†ç¡®åº¦ã€‚ä»£ç å’Œé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹æ£€æŸ¥ç‚¹å·²å¼€æºåœ¨[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çº¿æ€§RNNæ¶æ„å¦‚Mambaåœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢å¯ä¸Transformeræ¨¡å‹ç«äº‰ï¼Œå¹¶å…·æœ‰ä¼˜åŠ¿éƒ¨ç½²ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡å¤ç”¨æ³¨æ„åŠ›å±‚çš„çº¿æ€§æŠ•å½±æƒé‡ï¼Œå¯å°†å¤§å‹Transformerè’¸é¦æˆçº¿æ€§RNNã€‚</li>
<li>æ··åˆæ¨¡å‹å®ç°äº†ä¸åŸå§‹Transformerç›¸å½“çš„èŠå¤©åŸºå‡†æµ‹è¯•æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§ç¡¬ä»¶æ„ŸçŸ¥çš„æŠ•æœºè§£ç ç®—æ³•ä»¥åŠ é€Ÿæ¨¡å‹æ¨ç†é€Ÿåº¦ã€‚</li>
<li>åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹ç§»é™¤è®¸å¤šåŸå§‹æ³¨æ„åŠ›å±‚ï¼Œå¹¶ä»ç®€åŒ–æ¨¡å‹ä¸­æ›´æœ‰æ•ˆåœ°ç”Ÿæˆæ•°æ®ã€‚</li>
<li>é«˜æ€§èƒ½æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè¶…è¶Šæœ€ä½³çº¿æ€§RNNæ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.15237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40a4074076c93977b7c0943fd511a0aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bac1d2fec48a3b7d2e999520e44387d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab696f7f52dd3441321ee54257f1068e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dfa1f7063175ebd1c455a498d74b1ae.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Security-Attacks-on-LLM-based-Code-Completion-Tools"><a href="#Security-Attacks-on-LLM-based-Code-Completion-Tools" class="headerlink" title="Security Attacks on LLM-based Code Completion Tools"></a>Security Attacks on LLM-based Code Completion Tools</h2><p><strong>Authors:Wen Cheng, Ke Sun, Xinyu Zhang, Wei Wang</strong></p>
<p>The rapid development of large language models (LLMs) has significantly advanced code completion capabilities, giving rise to a new generation of LLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these tools possess unique workflows, integrating multiple information sources as input and prioritizing code suggestions over natural language interaction, which introduces distinct security challenges. Additionally, LCCTs often rely on proprietary code datasets for training, raising concerns about the potential exposure of sensitive data. This paper exploits these distinct characteristics of LCCTs to develop targeted attack methodologies on two critical security risks: jailbreaking and training data extraction attacks. Our experimental results expose significant vulnerabilities within LCCTs, including a 99.4% success rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate on Amazon Q. Furthermore, We successfully extracted sensitive user data from GitHub Copilot, including 54 real email addresses and 314 physical addresses associated with GitHub usernames. Our study also demonstrates that these code-based attack methods are effective against general-purpose LLMs, such as the GPT series, highlighting a broader security misalignment in the handling of code by modern LLMs. These findings underscore critical security challenges associated with LCCTs and suggest essential directions for strengthening their security frameworks. The example code and attack samples from our research are provided at <a target="_blank" rel="noopener" href="https://github.com/Sensente/Security-Attacks-on-LCCTs">https://github.com/Sensente/Security-Attacks-on-LCCTs</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æå¤§åœ°æå‡äº†ä»£ç è¡¥å…¨åŠŸèƒ½ï¼Œå¹¶å‚¬ç”Ÿäº†ä¸€ä»£åŸºäºLLMçš„ä»£ç è¡¥å…¨å·¥å…·ï¼ˆLCCTsï¼‰ã€‚ä¸åŒäºé€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¿™äº›å·¥å…·æ‹¥æœ‰ç‹¬ç‰¹çš„å·¥ä½œæµç¨‹ï¼Œèƒ½å¤Ÿæ•´åˆå¤šä¸ªä¿¡æ¯æºä½œä¸ºè¾“å…¥ï¼Œå¹¶ä¼˜å…ˆå¤„ç†ä»£ç å»ºè®®è€Œéè‡ªç„¶è¯­è¨€äº¤äº’ï¼Œè¿™å¸¦æ¥äº†ä¸åŒçš„å®‰å…¨æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒLCCTsé€šå¸¸ä¾èµ–äºä¸“æœ‰ä»£ç æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¼•å‘äº†å…³äºæ½œåœ¨æ•æ„Ÿæ•°æ®æ³„éœ²çš„æ‹…å¿§ã€‚æœ¬æ–‡åˆ©ç”¨LCCTçš„è¿™äº›ç‹¬ç‰¹ç‰¹ç‚¹ï¼Œé’ˆå¯¹ä¸¤ç§å…³é”®å®‰å…¨é£é™©å¼€å‘æœ‰é’ˆå¯¹æ€§çš„æ”»å‡»æ–¹æ³•ï¼šè¶Šç‹±æ”»å‡»å’Œè®­ç»ƒæ•°æ®æå–æ”»å‡»ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæš´éœ²äº†LCCTä¸­çš„é‡å¤§æ¼æ´ï¼ŒåŒ…æ‹¬å¯¹GitHub Copilotçš„è¶Šç‹±æ”»å‡»æˆåŠŸç‡ä¸º99.4%ï¼Œå¯¹äºšé©¬é€ŠQçš„æˆåŠŸç‡ä¸º46.3%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æˆåŠŸåœ°ä»GitHub Copilotä¸­æå–äº†æ•æ„Ÿç”¨æˆ·æ•°æ®ï¼ŒåŒ…æ‹¬54ä¸ªçœŸå®ç”µå­é‚®ä»¶åœ°å€å’Œä¸GitHubç”¨æˆ·åç›¸å…³çš„314ä¸ªç‰©ç†åœ°å€ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜è¡¨æ˜ï¼Œè¿™äº›åŸºäºä»£ç çš„æ”»å‡»æ–¹æ³•å¯¹é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPTç³»åˆ—ï¼‰åŒæ ·æœ‰æ•ˆï¼Œè¿™å‡¸æ˜¾äº†ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä»£ç æ–¹é¢çš„æ•´ä½“å®‰å…¨ä¸åŒ¹é…é—®é¢˜ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†LCCTé¢ä¸´çš„å…³é”®å®‰å…¨æŒ‘æˆ˜ï¼Œå¹¶ä¸ºåŠ å¼ºå…¶å®‰å…¨æ¡†æ¶æä¾›äº†é‡è¦æ–¹å‘ã€‚æˆ‘ä»¬ç ”ç©¶ä¸­çš„ç¤ºä¾‹ä»£ç å’Œæ”»å‡»æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Sensente/Security-Attacks-on-LCCTs">https://github.com/Sensente/Security-Attacks-on-LCCTs</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11006v4">PDF</a> Paper accepted at AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æå¤§åœ°æå‡äº†ä»£ç è¡¥å…¨åŠŸèƒ½ï¼Œå¹¶å‚¬ç”Ÿäº†ä¸€ç§æ–°å‹åŸºäºLLMçš„ä»£ç è¡¥å…¨å·¥å…·ï¼ˆLCCTsï¼‰ã€‚ç„¶è€Œï¼ŒLCCTsåœ¨é›†æˆå¤šä¿¡æ¯æºå’Œä¼˜å…ˆæä¾›ä»£ç å»ºè®®çš„è¿‡ç¨‹ä¸­é¢ä¸´ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒæ—¶å®ƒä»¬ä¾èµ–ä¸“æœ‰ä»£ç æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¿™å¯èƒ½å¼•å‘æ•°æ®æ³„éœ²é£é™©ã€‚æœ¬æ–‡æ·±å…¥æ¢è®¨äº†LCCTsçš„è¿™äº›ç‰¹æ€§ï¼Œå¹¶é’ˆå¯¹ä¸¤å¤§å…³é”®é£é™©è®¾è®¡æœ‰é’ˆå¯¹æ€§çš„æ”»å‡»æ–¹æ³•ï¼šè¶Šç‹±æ”»å‡»å’Œè®­ç»ƒæ•°æ®æå–æ”»å‡»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLCCTså­˜åœ¨é‡å¤§æ¼æ´ï¼Œå¦‚åœ¨GitHub Copilotä¸Šçš„è¶Šç‹±æ”»å‡»æˆåŠŸç‡é«˜è¾¾99.4%ï¼Œå¹¶åœ¨äºšé©¬é€ŠQä¸Šè¾¾åˆ°46.3%çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»GitHub CopilotæˆåŠŸæå–äº†ç”¨æˆ·çš„æ•æ„Ÿæ•°æ®ï¼ŒåŒ…æ‹¬54ä¸ªçœŸå®ç”µå­é‚®ä»¶åœ°å€å’Œ314ä¸ªä¸GitHubç”¨æˆ·åå…³è”çš„ç‰©ç†åœ°å€ã€‚æœ¬æ–‡çš„ç ”ç©¶æ–¹æ³•åŒæ ·é€‚ç”¨äºGPTç³»åˆ—ç­‰é€šç”¨LLMï¼Œè¡¨æ˜ç°ä»£LLMåœ¨å¤„ç†ä»£ç æ—¶çš„å®‰å…¨æ¼æ´æ›´ä¸ºæ™®éã€‚è¿™äº›å‘ç°çªæ˜¾äº†LCCTsçš„å®‰å…¨æŒ‘æˆ˜ï¼Œå¹¶ä¸ºåŠ å¼ºå…¶å®‰å…¨æ¡†æ¶æä¾›äº†å…³é”®æ–¹å‘ã€‚ç›¸å…³ç¤ºä¾‹ä»£ç å’Œæ”»å‡»æ ·æœ¬å¯åœ¨æˆ‘ä»¬çš„ç ”ç©¶ç½‘ç«™æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†ä»£ç è¡¥å…¨å·¥å…·ï¼ˆLCCTsï¼‰çš„å´›èµ·ï¼ŒLCCTsé›†æˆå¤šä¿¡æ¯æºå¹¶ä¼˜å…ˆæä¾›ä»£ç å»ºè®®ï¼Œå¸¦æ¥ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚</li>
<li>LCCTsä¾èµ–äºä¸“æœ‰ä»£ç æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå­˜åœ¨æ³„éœ²æ•æ„Ÿæ•°æ®çš„é£é™©ã€‚</li>
<li>é’ˆå¯¹LCCTsçš„è¶Šç‹±æ”»å‡»æˆåŠŸç‡é«˜ï¼Œå­˜åœ¨é‡å¤§å®‰å…¨æ¼æ´ã€‚</li>
<li>ä»GitHub CopilotæˆåŠŸæå–äº†ç”¨æˆ·çš„æ•æ„Ÿæ•°æ®ï¼ŒåŒ…æ‹¬ç”µå­é‚®ä»¶åœ°å€å’Œç‰©ç†åœ°å€ã€‚</li>
<li>æ”»å‡»æ–¹æ³•åŒæ ·é€‚ç”¨äºé€šç”¨LLMï¼Œå¦‚GPTç³»åˆ—ï¼Œè¡¨æ˜ç°ä»£LLMåœ¨å¤„ç†ä»£ç æ—¶çš„å®‰å…¨æ¼æ´å¹¿æ³›å­˜åœ¨ã€‚</li>
<li>ç ”ç©¶æä¾›äº†é’ˆå¯¹LCCTså®‰å…¨æŒ‘æˆ˜çš„å…³é”®æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6ac12dff45796ae2ab0d45a2453106c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7668e94e50398890bb977d4fa1da9eac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec877b1bb49f6654a10dd25e4d459bda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d4b1f7c8a80d41e58bf55896b1d5e62.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Unleashing-the-Power-of-Data-Tsunami-A-Comprehensive-Survey-on-Data-Assessment-and-Selection-for-Instruction-Tuning-of-Language-Models"><a href="#Unleashing-the-Power-of-Data-Tsunami-A-Comprehensive-Survey-on-Data-Assessment-and-Selection-for-Instruction-Tuning-of-Language-Models" class="headerlink" title="Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data   Assessment and Selection for Instruction Tuning of Language Models"></a>Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data   Assessment and Selection for Instruction Tuning of Language Models</h2><p><strong>Authors:Yulei Qin, Yuncheng Yang, Pengcheng Guo, Gang Li, Hang Shao, Yuchen Shi, Zihan Xu, Yun Gu, Ke Li, Xing Sun</strong></p>
<p>Instruction tuning plays a critical role in aligning large language models (LLMs) with human preference. Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical. To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning. However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism. To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured. For each category, representative methods are elaborated to describe the landscape of relevant research. In addition, comparison between the latest methods is conducted on their officially reported results to provide in-depth discussions on their limitations. Finally, we summarize the open challenges and propose the promosing avenues for future studies. All related contents are available at <a target="_blank" rel="noopener" href="https://github.com/yuleiqin/fantastic-data-engineering">https://github.com/yuleiqin/fantastic-data-engineering</a>. </p>
<blockquote>
<p>æŒ‡ä»¤è°ƒæ•´åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚å°½ç®¡å­˜åœ¨å¤§é‡çš„å¼€æ”¾æŒ‡ä»¤æ•°æ®é›†ï¼Œä½†ç›²ç›®åœ°è®­ç»ƒLLMä»¥é€‚åº”æ‰€æœ‰ç°æœ‰æŒ‡ä»¤å¯èƒ½å¹¶ä¸ç†æƒ³ä¸”ä¸åˆ‡å®é™…ã€‚ä¸ºäº†æ‰¾åˆ°æœ€æœ‰åˆ©çš„æ•°æ®ç‚¹ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œæ·±åº¦å­¦ä¹ é¢†åŸŸå·²ç»æå‡ºäº†æ•°æ®è¯„ä¼°ä¸é€‰æ‹©æ–¹æ³•ã€‚ç„¶è€Œï¼Œåœ¨æŒ‡ä»¤è°ƒæ•´çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œå…³äºå¯ä»¥ä½¿ç”¨ä»€ä¹ˆæ ·çš„æ•°æ®è¯„ä¼°æŒ‡æ ‡ä»¥åŠå¦‚ä½•å°†å®ƒä»¬æ•´åˆåˆ°é€‰æ‹©æœºåˆ¶ä¸­ï¼Œä»ç„¶å­˜åœ¨çŸ¥è¯†ç©ºç™½ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹ç°æœ‰æ–‡çŒ®è¿›è¡Œäº†å…¨é¢å›é¡¾ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹LLMæŒ‡ä»¤è°ƒæ•´çš„æ•°æ®è¯„ä¼°ä¸é€‰æ‹©ã€‚æˆ‘ä»¬å°†æ‰€æœ‰é€‚ç”¨çš„æ–¹æ³•ç³»ç»Ÿåœ°åˆ†ä¸ºåŸºäºè´¨é‡ã€åŸºäºå¤šæ ·æ€§å’ŒåŸºäºé‡è¦æ€§çš„ä¸‰ç±»ï¼Œå¹¶æ„å»ºäº†ç»Ÿä¸€ã€ç²¾ç»†çš„åˆ†ç±»ä½“ç³»ã€‚é’ˆå¯¹æ¯ä¸ªç±»åˆ«ï¼Œæˆ‘ä»¬è¯¦ç»†é˜è¿°äº†ä»£è¡¨æ€§æ–¹æ³•æ¥æè¿°ç›¸å…³ç ”ç©¶é¢†åŸŸçš„æ¦‚å†µã€‚æ­¤å¤–ï¼Œè¿˜å¯¹æœ€æ–°æ–¹æ³•è¿›è¡Œäº†å®˜æ–¹æŠ¥å‘Šç»“æœçš„æ¯”è¾ƒï¼Œå¯¹å…¶å±€é™æ€§è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚æœ€åï¼Œæˆ‘ä»¬æ€»ç»“äº†å¼€æ”¾æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æå‡ºäº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚æ‰€æœ‰ç›¸å…³å†…å®¹å‡å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/yuleiqin/fantastic-data-engineering%E3%80%82">https://github.com/yuleiqin/fantastic-data-engineeringã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.02085v5">PDF</a> Accepted to TMLR with Survey Certificate, review, survey, 37 pages, 5   figures, 4 tables</p>
<p><strong>Summary</strong><br>å¤§æ•°æ®æ—¶ä»£çš„è¯­è¨€æ¨¡å‹ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼ŒæŒ‡ä»¤è°ƒä¼˜æ‰®æ¼”å…³é”®è§’è‰²ã€‚ç›²ç›®é‡‡ç”¨æµ·é‡æŒ‡ä»¤æ•°æ®å¹¶éæœ€ä½³é€‰æ‹©ï¼Œéœ€å¯¹æ•°æ®è¯„ä¼°å’Œé€‰æ‹©æ–¹æ³•è¿›è¡Œæ·±å…¥ç ”ç©¶ã€‚å½“å‰ç ”ç©¶ä¸­è´¨é‡è¯„ä¼°ã€å¤šæ ·æ€§å’Œé‡è¦æ€§è¯„ä¼°æ˜¯ä¸‰å¤§ä¸»æµè¯„ä¼°æ–¹æ³•ã€‚é’ˆå¯¹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒä¼˜æ•°æ®è¯„ä¼°æ–¹æ³•å°šå¾…å®Œå–„ï¼Œæœ¬æ–‡æ¢³ç†äº†ç°æœ‰çš„æ•°æ®è¯„ä¼°æ–‡çŒ®ï¼Œè¯¦ç»†é˜è¿°äº†å„ç±»æ–¹æ³•çš„ä¼˜ç¼ºç‚¹åŠé€‚ç”¨åœºæ™¯ï¼Œå¹¶æ¯”è¾ƒäº†æœ€æ–°æ–¹æ³•çš„æ€§èƒ½å·®å¼‚ã€‚åŒæ—¶æŒ‡å‡ºå­˜åœ¨çš„æŒ‘æˆ˜å’Œæœªæ¥çš„ç ”ç©¶è¶‹åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æŒ‡ä»¤è°ƒä¼˜åœ¨LLMä¸äººç±»åå¥½å¯¹é½ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>å•çº¯ä¾èµ–å¤§é‡æŒ‡ä»¤æ•°æ®é›†å¹¶ä¸ç†æƒ³ï¼Œéœ€è¦æ•°æ®è¯„ä¼°ä¸é€‰æ‹©æ–¹æ³•ã€‚</li>
<li>æ•°æ®è¯„ä¼°ä¸»è¦åŒ…æ‹¬è´¨é‡è¯„ä¼°ã€å¤šæ ·æ€§å’Œé‡è¦æ€§è¯„ä¼°ä¸‰å¤§ç±»åˆ«ã€‚</li>
<li>æœ¬æ–‡ç»¼è¿°äº†ç°æœ‰æ–‡çŒ®ä¸­çš„æ–¹æ³•å¹¶åˆ†ç±»ä¸ºä¸‰å¤§ç±»åˆ«ã€‚</li>
<li>ä»£è¡¨æ€§æ–¹æ³•æè¿°äº†ç›¸å…³é¢†åŸŸçš„ç ”ç©¶ç°çŠ¶ã€‚</li>
<li>æ¯”è¾ƒäº†æœ€æ–°æ–¹æ³•çš„æ€§èƒ½å·®å¼‚å¹¶æ·±å…¥è®¨è®ºäº†å…¶å±€é™æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.02085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9d16d14c460dbf8de53812cafeb3c660.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf49cb322f5bfaf848dc956c13ea59ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4a7537d9835cfadc8cbb37ecbe3c9aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7fed18bc0668c1243a215fd28a32788.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Synergistic-Multi-Agent-Framework-with-Trajectory-Learning-for-Knowledge-Intensive-Tasks"><a href="#Synergistic-Multi-Agent-Framework-with-Trajectory-Learning-for-Knowledge-Intensive-Tasks" class="headerlink" title="Synergistic Multi-Agent Framework with Trajectory Learning for   Knowledge-Intensive Tasks"></a>Synergistic Multi-Agent Framework with Trajectory Learning for   Knowledge-Intensive Tasks</h2><p><strong>Authors:Shengbin Yue, Siyuan Wang, Wei Chen, Xuanjing Huang, Zhongyu Wei</strong></p>
<p>Recent advancements in Large Language Models (LLMs) have led to significant breakthroughs in various natural language processing tasks. However, generating factually consistent responses in knowledge-intensive scenarios remains a challenge due to issues such as hallucination, difficulty in acquiring long-tailed knowledge, and limited memory expansion. This paper introduces SMART, a novel multi-agent framework that leverages external knowledge to enhance the interpretability and factual consistency of LLM-generated responses. SMART comprises four specialized agents, each performing a specific sub-trajectory action to navigate complex knowledge-intensive tasks. We propose a multi-agent co-training paradigm, Long-Short Trajectory Learning, which ensures synergistic collaboration among agents while maintaining fine-grained execution by each agent. Extensive experiments on five knowledge-intensive tasks demonstrate SMARTâ€™s superior performance compared to widely adopted knowledge internalization and knowledge enhancement methods. Our framework can extend beyond knowledge-intensive tasks to more complex scenarios. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/yueshengbin/SMART">https://github.com/yueshengbin/SMART</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•å¯¼è‡´å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡å–å¾—äº†é‡å¤§çªç ´ã€‚ç„¶è€Œï¼Œåœ¨çŸ¥è¯†å¯†é›†å‹åœºæ™¯ä¸­ç”Ÿæˆäº‹å®ä¸€è‡´çš„å›åº”ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨è¯¸å¦‚å¹»è§‰ã€è·å–é•¿å°¾çŸ¥è¯†çš„å›°éš¾å’Œæœ‰é™çš„å†…å­˜æ‰©å±•ç­‰é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†SMARTï¼Œä¸€ä¸ªåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†æé«˜LLMç”Ÿæˆå“åº”çš„å¯è§£é‡Šæ€§å’Œäº‹å®ä¸€è‡´æ€§çš„æ–°å‹å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚SMARTåŒ…å«å››ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“æ‰§è¡Œç‰¹å®šçš„å­è½¨è¿¹åŠ¨ä½œï¼Œä»¥å®Œæˆå¤æ‚çš„çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“ååŒè®­ç»ƒèŒƒå¼â€”â€”é•¿çŸ­è½¨è¿¹å­¦ä¹ ï¼Œå®ƒç¡®ä¿æ™ºèƒ½ä½“ä¹‹é—´çš„ååŒåˆä½œï¼ŒåŒæ—¶ä¿æŒæ¯ä¸ªæ™ºèƒ½ä½“çš„ç²¾ç»†æ‰§è¡Œã€‚åœ¨äº”ä¸ªçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸å¹¿æ³›é‡‡ç”¨çš„çŸ¥è¯†å†…éƒ¨åŒ–å’ŒçŸ¥è¯†å¢å¼ºæ–¹æ³•ç›¸æ¯”ï¼ŒSMARTçš„æ€§èƒ½æ›´ä¼˜è¶Šã€‚æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥æ‰©å±•åˆ°çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä»¥å¤–çš„æ›´å¤æ‚çš„åœºæ™¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yueshengbin/SMART%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yueshengbin/SMARTä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.09893v3">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•ä¸ºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡å¸¦æ¥äº†é‡å¤§çªç ´ï¼Œä½†åœ¨çŸ¥è¯†å¯†é›†å‹åœºæ™¯ä¸­ç”Ÿæˆäº‹å®ä¸€è‡´çš„å›å¤ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶SMARTï¼Œåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†æé«˜LLMç”Ÿæˆå›å¤çš„å¯è§£é‡Šæ€§å’Œäº‹å®ä¸€è‡´æ€§ã€‚SMARTåŒ…å«å››ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼Œé€šè¿‡é•¿æœŸçŸ­æœŸè½¨è¿¹å­¦ä¹ ç­‰ååŒè®­ç»ƒæ¨¡å¼ï¼Œç¡®ä¿æ™ºèƒ½ä½“ä¹‹é—´çš„ååŒåˆä½œï¼ŒåŒæ—¶ä¿æŒç²¾ç»†çš„æ‰§è¡Œèƒ½åŠ›ã€‚åœ¨äº”ä¸ªçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSMARTä¼˜äºå¹¿æ³›é‡‡ç”¨çš„çŸ¥è¯†å†…åŒ–å’ŒçŸ¥è¯†å¢å¼ºæ–¹æ³•ã€‚æ­¤æ¡†æ¶å¯åº”ç”¨äºæ›´å¤æ‚åœºæ™¯ã€‚ä»£ç å…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/yueshengbin/SMART%E3%80%82">https://github.com/yueshengbin/SMARTã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>çŸ¥è¯†å¯†é›†å‹åœºæ™¯ä¸­ç”Ÿæˆäº‹å®ä¸€è‡´çš„å›å¤å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>SMARTæ˜¯ä¸€ä¸ªæ–°å‹çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†æé«˜LLMçš„å›å¤è´¨é‡ã€‚</li>
<li>SMARTåŒ…å«å››ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“æ‰§è¡Œç‰¹å®šå­è½¨è¿¹åŠ¨ä½œä»¥å®Œæˆå¤æ‚ä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“ååŒè®­ç»ƒæ¨¡å¼â€”â€”é•¿æœŸçŸ­æœŸè½¨è¿¹å­¦ä¹ ï¼Œç¡®ä¿æ™ºèƒ½ä½“é—´çš„ååŒåˆä½œå’Œç²¾ç»†æ‰§è¡Œã€‚</li>
<li>åœ¨äº”ä¸ªçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜SMARTä¼˜äºå…¶ä»–çŸ¥è¯†å†…åŒ–å’Œå¢å¼ºæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.09893">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-675dcf48643614baa0ecf8384eea0f77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3248ae8ef2590923fe9250d3e5dbe404.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-462fce9784f15da012df0e59e5d8af24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f6cb132c069755a284a2e55aeeb92c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ace0d31b72257c31c35058926a0c2c9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-24ea0a50f339793405f429b40be8494d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Sound-VECaps-Improving-Audio-Generation-with-Visual-Enhanced-Captions"><a href="#Sound-VECaps-Improving-Audio-Generation-with-Visual-Enhanced-Captions" class="headerlink" title="Sound-VECaps: Improving Audio Generation with Visual Enhanced Captions"></a>Sound-VECaps: Improving Audio Generation with Visual Enhanced Captions</h2><p><strong>Authors:Yi Yuan, Dongya Jia, Xiaobin Zhuang, Yuanzhe Chen, Zhengxi Liu, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xubo Liu, Xiyuan Kang, Mark D. Plumbley, Wenwu Wang</strong></p>
<p>Generative models have shown significant achievements in audio generation tasks. However, existing models struggle with complex and detailed prompts, leading to potential performance degradation. We hypothesize that this problem stems from the simplicity and scarcity of the training data. This work aims to create a large-scale audio dataset with rich captions for improving audio generation models. We first develop an automated pipeline to generate detailed captions by transforming predicted visual captions, audio captions, and tagging labels into comprehensive descriptions using a Large Language Model (LLM). The resulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption pairs with enriched details including audio event orders, occurred places and environment information. We then demonstrate that training the text-to-audio generation models with Sound-VECaps significantly improves the performance on complex prompts. Furthermore, we conduct ablation studies of the models on several downstream audio-language tasks, showing the potential of Sound-VECaps in advancing audio-text representation learning. Our dataset and models are available online from here <a target="_blank" rel="noopener" href="https://yyua8222.github.io/Sound-VECaps-demo/">https://yyua8222.github.io/Sound-VECaps-demo/</a>. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹åœ¨éŸ³é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆå°±ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨å¤æ‚ä¸”è¯¦ç»†çš„æç¤ºæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬å‡è®¾è¿™ä¸ªé—®é¢˜æºäºè®­ç»ƒæ•°æ®çš„ç®€å•æ€§å’Œç¨€ç¼ºæ€§ã€‚è¿™é¡¹å·¥ä½œæ—¨åœ¨åˆ›å»ºä¸€ä¸ªå¤§è§„æ¨¡éŸ³é¢‘æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä¸°å¯Œçš„å­—å¹•ï¼Œä»¥æ”¹è¿›éŸ³é¢‘ç”Ÿæˆæ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“ï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†é¢„æµ‹çš„è§†è§‰å­—å¹•ã€éŸ³é¢‘å­—å¹•å’Œæ ‡ç­¾è½¬æ¢ä¸ºç»¼åˆæè¿°ï¼Œæ¥ç”Ÿæˆè¯¦ç»†å­—å¹•ã€‚ç»“æœæ•°æ®é›†Sound-VECapsåŒ…å«166ä¸‡ä¸ªé«˜è´¨é‡éŸ³é¢‘å­—å¹•å¯¹ï¼Œå…¶ä¸­åŒ…å«ä¸°å¯Œçš„ç»†èŠ‚ï¼Œå¦‚éŸ³é¢‘äº‹ä»¶é¡ºåºã€å‘ç”Ÿåœ°ç‚¹å’Œç¯å¢ƒä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬è¯æ˜ä½¿ç”¨Sound-VECapsè®­ç»ƒæ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆæ¨¡å‹å¯ä»¥æ˜¾è‘—æé«˜å¤æ‚æç¤ºçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨å‡ ä¸ªä¸‹æ¸¸éŸ³é¢‘è¯­è¨€ä»»åŠ¡ä¸Šè¿›è¡Œäº†æ¨¡å‹çš„å‰”é™¤ç ”ç©¶ï¼Œæ˜¾ç¤ºäº†Sound-VECapsåœ¨æ¨è¿›éŸ³é¢‘æ–‡æœ¬è¡¨ç¤ºå­¦ä¹ æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œæ¨¡å‹å¯ä»<a target="_blank" rel="noopener" href="https://yyua8222.github.io/Sound-VECaps-demo/%E5%9C%A8%E7%BA%BF%E8%8E%B7%E5%8F%96%E3%80%82">https://yyua8222.github.io/Sound-VECaps-demo/åœ¨çº¿è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.04416v4">PDF</a> 5 pages with 1 appendix, accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”Ÿæˆæ¨¡å‹åœ¨éŸ³é¢‘ç”Ÿæˆä»»åŠ¡ä¸­çš„æˆå°±å’ŒæŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚å’Œè¯¦ç»†æç¤ºæ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºåˆ›å»ºä¸€ä¸ªå¤§è§„æ¨¡éŸ³é¢‘æ•°æ®é›†Sound-VECapsï¼Œæ—¨åœ¨æ”¹å–„éŸ³é¢‘ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†é¢„æµ‹çš„è§†è§‰å­—å¹•ã€éŸ³é¢‘å­—å¹•å’Œæ ‡ç­¾è½¬æ¢ä¸ºè¯¦ç»†çš„æè¿°ï¼Œç”Ÿæˆå¤§è§„æ¨¡éŸ³é¢‘æ•°æ®é›†ã€‚ä½¿ç”¨Sound-VECapsè®­ç»ƒæ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†å¤æ‚æç¤ºçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†å‡ é¡¹ä¸‹æ¸¸éŸ³é¢‘è¯­è¨€ä»»åŠ¡çš„æ¨¡å‹æ¶ˆèç ”ç©¶ï¼Œæ˜¾ç¤ºäº†Sound-VECapsåœ¨æ¨åŠ¨éŸ³é¢‘æ–‡æœ¬è¡¨ç¤ºå­¦ä¹ æ–¹é¢çš„æ½œåŠ›ã€‚æ•°æ®é›†å’Œæ¨¡å‹å¯ä»é“¾æ¥è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹åœ¨éŸ³é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆå°±ï¼Œä½†å¤„ç†å¤æ‚å’Œè¯¦ç»†æç¤ºæ—¶å­˜åœ¨æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>é—®é¢˜æ ¹æºè¢«è®¤ä¸ºæ˜¯è®­ç»ƒæ•°æ®çš„ç®€å•æ€§å’Œç¨€ç¼ºæ€§ã€‚</li>
<li>åˆ›å»ºä¸€ä¸ªåä¸ºSound-VECapsçš„å¤§è§„æ¨¡éŸ³é¢‘æ•°æ®é›†ï¼Œä»¥æ”¹å–„éŸ³é¢‘ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆè¯¦ç»†æè¿°æ¥åˆ›å»ºæ•°æ®é›†ï¼ŒåŒ…å«éŸ³é¢‘äº‹ä»¶é¡ºåºã€å‘ç”Ÿåœ°ç‚¹å’Œç¯å¢ƒä¿¡æ¯ç­‰ä¸°å¯Œç»†èŠ‚ã€‚</li>
<li>ä½¿ç”¨Sound-VECapsè®­ç»ƒçš„æ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤æ‚æç¤ºä¸Šçš„æ€§èƒ½æ˜¾è‘—æé«˜ã€‚</li>
<li>è¿›è¡Œäº†ä¸‹æ¸¸éŸ³é¢‘è¯­è¨€ä»»åŠ¡çš„æ¨¡å‹æ¶ˆèç ”ç©¶ï¼Œæ˜¾ç¤ºäº†Sound-VECapsåœ¨æ¨åŠ¨éŸ³é¢‘æ–‡æœ¬è¡¨ç¤ºå­¦ä¹ æ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.04416">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e1844463c908e9fe71b672fba258f9ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eaf113a68b0111d01c598f6c1841adb8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c1055e38b43126025cafea5b52dcc98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82276876074324307f90558992a2dea5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d5c6b07d279f6d0e4267ce72f7aeb44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c221cb8994214fc8173178afe0e9226.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6b3e86a8956f6e98bb66b5519b53f435.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87acd3dcc17a3fec1848a6ea48d65b43.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Beyond-Numeric-Awards-In-Context-Dueling-Bandits-with-LLM-Agents"><a href="#Beyond-Numeric-Awards-In-Context-Dueling-Bandits-with-LLM-Agents" class="headerlink" title="Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"></a>Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents</h2><p><strong>Authors:Fanzeng Xia, Hao Liu, Yisong Yue, Tongxin Li</strong></p>
<p>In-context reinforcement learning (ICRL) is a frontier paradigm for solving reinforcement learning problems in the foundation model era. While ICRL capabilities have been demonstrated in transformers through task-specific training, the potential of Large Language Models (LLMs) out-of-the-box remains largely unexplored. Recent findings highlight that LLMs often face challenges when dealing with numerical contexts, and limited attention has been paid to evaluating their performance through preference feedback generated by the environment. This paper is the first to investigate LLMs as in-context decision-makers under the problem of Dueling Bandits (DB), a stateless preference-based reinforcement learning setting that extends the classic Multi-Armed Bandit (MAB) model by querying for preference feedback. We compare GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-Preview against nine well-established DB algorithms. Our results reveal that our top-performing LLM, GPT-4 Turbo, has the zero-shot relative decision-making ability to achieve surprisingly low weak regret across all the DB environment instances by quickly including the best arm in duels. However, an optimality gap exists between LLMs and classic DB algorithms in terms of strong regret. LLMs struggle to converge and consistently exploit even when explicitly prompted to do so, and are sensitive to prompt variations. To bridge this gap, we propose an agentic flow framework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates off-the-shelf DB algorithms with LLM agents through fine-grained adaptive interplay. We show that LEAD has theoretical guarantees inherited from classic DB algorithms on both weak and strong regret. We validate its efficacy and robustness even with noisy and adversarial prompts. The design of our framework sheds light on how to enhance the trustworthiness of LLMs used for in-context decision-making. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å¢å¼ºå­¦ä¹ ï¼ˆICRLï¼‰æ˜¯åŸºç¡€æ¨¡å‹æ—¶ä»£è§£å†³å¢å¼ºå­¦ä¹ é—®é¢˜çš„ä¸€ç§å‰æ²¿èŒƒå¼ã€‚è™½ç„¶é€šè¿‡ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒï¼ŒICRLåœ¨Transformerä¸­çš„èƒ½åŠ›å·²ç»å¾—åˆ°äº†è¯æ˜ï¼Œä½†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å³æ’å³ç”¨æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ€è¿‘çš„å‘ç°è¡¨æ˜ï¼ŒLLMåœ¨å¤„ç†æ•°å€¼ä¸Šä¸‹æ–‡æ—¶ç»å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œè€Œä¸”å¾ˆå°‘æœ‰äººå…³æ³¨é€šè¿‡ç¯å¢ƒäº§ç”Ÿçš„åå¥½åé¦ˆæ¥è¯„ä¼°å®ƒä»¬çš„æ€§èƒ½ã€‚æœ¬æ–‡é¦–æ¬¡è°ƒæŸ¥äº†LLMä½œä¸ºä¸Šä¸‹æ–‡å†³ç­–è€…åœ¨å¤šè‡‚èµŒåšæœºï¼ˆDBï¼‰é—®é¢˜ä¸‹çš„è¡¨ç°ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ— çŠ¶æ€åå¥½å¢å¼ºå­¦ä¹ ç¯å¢ƒçš„æ¨¡å‹ï¼Œé€šè¿‡æŸ¥è¯¢åå¥½åé¦ˆæ¥æ‰©å±•ç»å…¸çš„å¤šè‡‚èµŒåšæœºï¼ˆMABï¼‰æ¨¡å‹ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†GPT-3.5 Turboã€GPT-4ã€GPT-4 Turboã€Llama 3.1å’Œo1é¢„è§ˆç‰ˆä¸ä¹ä¸ªæˆç†Ÿçš„DBç®—æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¡¨ç°æœ€ä½³çš„LLMâ€”â€”GPT-4 Turboå…·æœ‰é›¶å°„å†³ç­–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ‰€æœ‰DBç¯å¢ƒå®ä¾‹ä¸­å–å¾—ä»¤äººæƒŠè®¶çš„ä½å¼±é—æ†¾ï¼Œé€šè¿‡å¿«é€Ÿå°†æœ€ä½³æ‰‹è‡‚çº³å…¥å†³æ–—ä¸­ã€‚ç„¶è€Œï¼Œåœ¨å¼ºé—æ†¾æ–¹é¢ï¼ŒLLMä¸ç»å…¸DBç®—æ³•ä¹‹é—´å­˜åœ¨å·®è·ã€‚å³ä½¿æ˜ç¡®æç¤ºè¿›è¡Œæ”¶æ•›å’ŒæŒç»­å‰¥å‰Šæ—¶ï¼ŒLLMä¹Ÿå¾ˆéš¾åšåˆ°è¿™ä¸€ç‚¹ï¼Œå¹¶ä¸”å¯¹æç¤ºçš„å˜åŠ¨éå¸¸æ•æ„Ÿã€‚ä¸ºäº†ç¼©å°è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºä»£ç†æµæ¡†æ¶çš„LLMå¢å¼ºç®—æ³•å†³æ–—ï¼ˆLEADï¼‰ï¼Œå®ƒé€šè¿‡ç²¾ç»†çš„é€‚åº”æ€§äº’åŠ¨å°†ç°æˆçš„DBç®—æ³•ä¸LLMä»£ç†é›†æˆåœ¨ä¸€èµ·ã€‚æˆ‘ä»¬å±•ç¤ºäº†LEADåœ¨å¼±é—æ†¾å’Œå¼ºé—æ†¾æ–¹é¢éƒ½ç»§æ‰¿äº†ç»å…¸DBç®—æ³•çš„ç†è®ºä¿è¯ã€‚æˆ‘ä»¬åœ¨å­˜åœ¨å™ªå£°å’Œå¯¹æŠ—æç¤ºçš„æƒ…å†µä¸‹éªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶è®¾è®¡æ­ç¤ºäº†å¦‚ä½•å¢å¼ºç”¨äºä¸Šä¸‹æ–‡å†³ç­–LLMçš„å¯ä¿¡åº¦çš„å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.01887v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡é¦–æ¬¡æ¢ç´¢äº†åœ¨æ— çŠ¶æ€åå¥½å¼ºåŒ–å­¦ä¹ ç¯å¢ƒâ€”â€”Dueling Banditsä¸‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºä¸Šä¸‹æ–‡å†³ç­–è€…çš„æ½œåŠ›ã€‚ç ”ç©¶å¯¹æ¯”äº†GPT-3.5 Turboã€GPT-4ã€GPT-4 Turboã€Llama 3.1å’Œo1-Previewç­‰å¤šç§LLMä¸ä¹ä¸ªæˆç†Ÿçš„DBç®—æ³•çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼ŒGPT-4 Turboåœ¨ç›¸å¯¹å†³ç­–èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨å„ç§DBç¯å¢ƒå®ä¾‹ä¸­å®ç°ä½å¼±é—æ†¾ï¼Œä½†åœ¨å¼ºé—æ†¾æ–¹é¢å­˜åœ¨æœ€ä¼˜æ€§å·®è·ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ•´åˆç»å…¸DBç®—æ³•ä¸LLMä»£ç†çš„ä»£ç†æµæ¡†æ¶ï¼šLEADã€‚LEADå…·æœ‰ç†è®ºä¿è¯ï¼Œå¹¶åœ¨å¼±é—æ†¾å’Œå¼ºé—æ†¾æ–¹é¢å‡è¡¨ç°ç¨³å¥ï¼Œå³ä½¿åœ¨å™ªå£°å’Œå¯¹æŠ—æ€§æç¤ºä¸‹ä¹ŸéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚ç ”ç©¶ä¸ºæå‡LLMåœ¨ä¸Šä¸‹æ–‡å†³ç­–ä¸­çš„å¯ä¿¡åº¦æä¾›äº†å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³æ— çŠ¶æ€åå¥½å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸‹çš„Dueling Banditsé—®é¢˜ä¸Šå±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>GPT-4 Turboåœ¨ç›¸å¯¹å†³ç­–èƒ½åŠ›ä¸Šè¡¨ç°æœ€ä½³ï¼Œä½†å­˜åœ¨ä¸ç»å…¸DBç®—æ³•çš„æœ€ä¼˜æ€§å·®è·ã€‚</li>
<li>LLMåœ¨é¢å¯¹æ•°å€¼ä¸Šä¸‹æ–‡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¹æç¤ºçš„æ•æ„Ÿæ€§é«˜ï¼Œéš¾ä»¥æ”¶æ•›å¹¶æŒç»­åˆ©ç”¨ã€‚</li>
<li>æå‡ºLEADæ¡†æ¶ï¼Œæ•´åˆç»å…¸DBç®—æ³•ä¸LLMä»£ç†ï¼Œå…·æœ‰ç†è®ºä¿è¯å¹¶åœ¨å¼±é—æ†¾å’Œå¼ºé—æ†¾æ–¹é¢è¡¨ç°ç¨³å¥ã€‚</li>
<li>LEADæ¡†æ¶è®¾è®¡æå‡äº†LLMåœ¨ä¸Šä¸‹æ–‡å†³ç­–ä¸­çš„å¯ä¿¡åº¦ã€‚</li>
<li>LLMåœ¨é¢å¯¹å™ªå£°å’Œå¯¹æŠ—æ€§æç¤ºæ—¶ä»èƒ½ä¿æŒæœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.01887">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-233d6aec8860619450ea6d694d79e296.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43d40a4309885c3f0c42369a7664c8ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59accf88e122791decfc49bc19136500.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78a76314ff5019715def1ec80414ba11.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-04/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-04/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-04/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-57a03421c6a2a166dfe90aa78f6cc017.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-04  GAI Generative Agents for Innovation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b1c0c3f99332e35912e94921a2277486.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-03  When SAM2 Meets Video Shadow and Mirror Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17862.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
