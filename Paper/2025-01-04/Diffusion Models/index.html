<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-04  Prometheus 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D   Scene Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-d672e238c1a03deb47fedc1bfe354b39.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    47 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-04-æ›´æ–°"><a href="#2025-01-04-æ›´æ–°" class="headerlink" title="2025-01-04 æ›´æ–°"></a>2025-01-04 æ›´æ–°</h1><h2 id="Prometheus-3D-Aware-Latent-Diffusion-Models-for-Feed-Forward-Text-to-3D-Scene-Generation"><a href="#Prometheus-3D-Aware-Latent-Diffusion-Models-for-Feed-Forward-Text-to-3D-Scene-Generation" class="headerlink" title="Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D   Scene Generation"></a>Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D   Scene Generation</h2><p><strong>Authors:Yuanbo Yang, Jiahao Shao, Xinyang Li, Yujun Shen, Andreas Geiger, Yiyi Liao</strong></p>
<p>In this work, we introduce Prometheus, a 3D-aware latent diffusion model for text-to-3D generation at both object and scene levels in seconds. We formulate 3D scene generation as multi-view, feed-forward, pixel-aligned 3D Gaussian generation within the latent diffusion paradigm. To ensure generalizability, we build our model upon pre-trained text-to-image generation model with only minimal adjustments, and further train it using a large number of images from both single-view and multi-view datasets. Furthermore, we introduce an RGB-D latent space into 3D Gaussian generation to disentangle appearance and geometry information, enabling efficient feed-forward generation of 3D Gaussians with better fidelity and geometry. Extensive experimental results demonstrate the effectiveness of our method in both feed-forward 3D Gaussian reconstruction and text-to-3D generation. Project page: <a target="_blank" rel="noopener" href="https://freemty.github.io/project-prometheus/">https://freemty.github.io/project-prometheus/</a> </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Prometheusï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ–‡æœ¬åˆ°ä¸‰ç»´åœºæ™¯ç”Ÿæˆçš„3Dæ„ŸçŸ¥æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥åœ¨å¯¹è±¡å’Œåœºæ™¯çº§åˆ«å¿«é€Ÿå®ç°ã€‚æˆ‘ä»¬å°†ä¸‰ç»´åœºæ™¯çš„ç”Ÿæˆä½œä¸ºæ½œåœ¨æ‰©æ•£æ¡†æ¶ä¸­çš„å¤šè§†å›¾å‰é¦ˆåƒç´ å¯¹é½çš„3Dé«˜æ–¯ç”Ÿæˆé—®é¢˜ã€‚ä¸ºäº†ç¡®ä¿æ¨¡å‹çš„é€šç”¨æ€§ï¼Œæˆ‘ä»¬åœ¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„åŸºç¡€ä¸Šæ„å»ºæ¨¡å‹ï¼Œåªéœ€è¿›è¡Œå¾®è°ƒå³å¯è¿›ä¸€æ­¥è®­ç»ƒï¼Œå¹¶ä½¿ç”¨å¤§é‡çš„å•è§†å›¾å’Œå¤šè§†å›¾æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ä¸‰ç»´é«˜æ–¯ç”Ÿæˆä¸­å¼•å…¥äº†RGB-Dæ½œåœ¨ç©ºé—´ï¼Œä»¥åˆ†ç¦»å¤–è§‚å’Œå‡ ä½•ä¿¡æ¯ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„å‰é¦ˆä¸‰ç»´é«˜æ–¯ç”Ÿæˆï¼Œæé«˜ä¿çœŸåº¦å’Œå‡ ä½•ç²¾åº¦ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ç»´é«˜æ–¯é‡å»ºå’Œæ–‡æœ¬åˆ°ä¸‰ç»´åœºæ™¯ç”Ÿæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ›´å¤šå†…å®¹è¯·å‚è€ƒæˆ‘ä»¬çš„é¡¹ç›®ä¸»é¡µï¼š<a target="_blank" rel="noopener" href="https://freemty.github.io/project-prometheus/">https://freemty.github.io/project-prometheus/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21117v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬å·¥ä½œå¼•å…¥Prometheusï¼Œè¿™æ˜¯ä¸€æ¬¾åŸºäºæ–‡æœ¬è‡³åœºæ™¯çº§åˆ«çš„ä¸‰ç»´æ‰©æ•£æ¨¡å‹ï¼Œå¯å®ç°ç‰©ä½“ä¸åœºæ™¯çš„å³æ—¶ä¸‰ç»´ç”Ÿæˆã€‚æˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬è‡³å›¾åƒç”Ÿæˆæ¨¡å‹ä½œä¸ºåŸºç¡€ï¼Œé€šè¿‡æœ€å°è°ƒæ•´æ„å»ºæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨å¤§é‡å•è§†è§’å’Œå¤šè§†è§’å›¾åƒæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿æ¨¡å‹å…·æœ‰é€šç”¨æ€§ã€‚å¼•å…¥RGB-Dæ½œåœ¨ç©ºé—´åˆ°ä¸‰ç»´é«˜æ–¯ç”Ÿæˆä¸­ï¼Œè§£è€¦å¤–è§‚å’Œå‡ ä½•ä¿¡æ¯ï¼Œå®ç°é«˜æ•ˆçš„ä¸‰ç»´é«˜æ–¯ç”Ÿæˆã€‚å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•åœ¨ä¸‰ç»´é«˜æ–¯é‡å»ºå’Œæ–‡æœ¬è‡³ä¸‰ç»´ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Prometheusæ˜¯ä¸€ä¸ªæ–‡æœ¬è‡³åœºæ™¯çº§åˆ«çš„ä¸‰ç»´æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿå®ç°ç‰©ä½“å’Œåœºæ™¯çš„å³æ—¶ä¸‰ç»´ç”Ÿæˆã€‚</li>
<li>æ¨¡å‹åŸºäºé¢„è®­ç»ƒçš„æ–‡æœ¬è‡³å›¾åƒç”Ÿæˆæ¨¡å‹æ„å»ºï¼Œåªéœ€è¿›è¡Œæœ€å°è°ƒæ•´ã€‚</li>
<li>ä½¿ç”¨å¤§é‡å•è§†è§’å’Œå¤šè§†è§’å›¾åƒæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿æ¨¡å‹çš„é€šç”¨æ€§ã€‚</li>
<li>æ¨¡å‹å¼•å…¥äº†RGB-Dæ½œåœ¨ç©ºé—´ä»¥è§£è€¦å¤–è§‚å’Œå‡ ä½•ä¿¡æ¯ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†é«˜æ•ˆçš„ä¸‰ç»´é«˜æ–¯ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21117">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f2f81c87ffdc3b279db6e2a62213bf4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2edfb253d40daa0651ed94c7f536f78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f990bf61922d22659ee8687d9e07244.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f31bda12f30ec763942a5455b9eee2bc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="EraseAnything-Enabling-Concept-Erasure-in-Rectified-Flow-Transformers"><a href="#EraseAnything-Enabling-Concept-Erasure-in-Rectified-Flow-Transformers" class="headerlink" title="EraseAnything: Enabling Concept Erasure in Rectified Flow Transformers"></a>EraseAnything: Enabling Concept Erasure in Rectified Flow Transformers</h2><p><strong>Authors:Daiheng Gao, Shilin Lu, Shaw Walters, Wenbo Zhou, Jiaming Chu, Jie Zhang, Bang Zhang, Mengxi Jia, Jian Zhao, Zhaoxin Fan, Weiming Zhang</strong></p>
<p>Removing unwanted concepts from large-scale text-to-image (T2I) diffusion models while maintaining their overall generative quality remains an open challenge. This difficulty is especially pronounced in emerging paradigms, such as Stable Diffusion (SD) v3 and Flux, which incorporate flow matching and transformer-based architectures. These advancements limit the transferability of existing concept-erasure techniques that were originally designed for the previous T2I paradigm (e.g., SD v1.4). In this work, we introduce EraseAnything, the first method specifically developed to address concept erasure within the latest flow-based T2I framework. We formulate concept erasure as a bi-level optimization problem, employing LoRA-based parameter tuning and an attention map regularizer to selectively suppress undesirable activations. Furthermore, we propose a self-contrastive learning strategy to ensure that removing unwanted concepts does not inadvertently harm performance on unrelated ones. Experimental results demonstrate that EraseAnything successfully fills the research gap left by earlier methods in this new T2I paradigm, achieving state-of-the-art performance across a wide range of concept erasure tasks. </p>
<blockquote>
<p>ä»å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„æ‰©æ•£æ¨¡å‹ä¸­ç§»é™¤ä¸éœ€è¦çš„æ¦‚å¿µå¹¶ä¿æŒå…¶æ•´ä½“çš„ç”Ÿæˆè´¨é‡ä»ç„¶æ˜¯ä¸€ä¸ªå¾…è§£å†³çš„éš¾é¢˜ã€‚è¿™ç§éš¾åº¦åœ¨ç¨³å®šæ‰©æ•£ï¼ˆSDï¼‰v3å’ŒFluxç­‰æ–°å…´èŒƒå¼ä¸­å°¤ä¸ºçªå‡ºï¼Œè¿™äº›æ–°å…´èŒƒå¼èå…¥äº†æµåŒ¹é…å’ŒåŸºäºå˜å‹å™¨çš„æ¶æ„ã€‚è¿™äº›è¿›æ­¥é™åˆ¶äº†åŸæœ¬ä¸ºæ—©æœŸT2IèŒƒå¼ï¼ˆä¾‹å¦‚SD v1.4ï¼‰è®¾è®¡çš„æ¦‚å¿µåˆ é™¤æŠ€æœ¯çš„å¯è½¬ç§»æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†EraseAnythingï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨å¼€å‘ä»¥è§£å†³æœ€æ–°æµå¼T2Iæ¡†æ¶ä¸­æ¦‚å¿µåˆ é™¤é—®é¢˜çš„æ–¹æ³•ã€‚æˆ‘ä»¬å°†æ¦‚å¿µåˆ é™¤è¡¨è¿°ä¸ºä¸¤çº§ä¼˜åŒ–é—®é¢˜ï¼Œé‡‡ç”¨åŸºäºLoRAçš„å‚æ•°è°ƒæ•´å’Œæ³¨æ„åŠ›å›¾æ­£åˆ™åŒ–æ¥æœ‰é€‰æ‹©æ€§åœ°æŠ‘åˆ¶ä¸éœ€è¦çš„æ¿€æ´»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªæˆ‘å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œä»¥ç¡®ä¿ç§»é™¤ä¸éœ€è¦çš„æ¦‚å¿µä¸ä¼šæ— æ„ä¸­æŸå®³å¯¹æ— å…³æ¦‚å¿µçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEraseAnythingæˆåŠŸå¡«è¡¥äº†æ—©æœŸæ–¹æ³•åœ¨è¿™ä¸€æ–°T2IèŒƒå¼ä¸­çš„ç ”ç©¶ç©ºç™½ï¼Œåœ¨å¹¿æ³›çš„æ¦‚å¿µåˆ é™¤ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20413v2">PDF</a> 24 pages, 18 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æœ€æ–°åŸºäºæµçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¡†æ¶ä¸­çš„æ¦‚å¿µæ“¦é™¤é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°æ–¹æ³•EraseAnythingã€‚è¯¥æ–¹æ³•å°†æ¦‚å¿µæ“¦é™¤å…¬å¼åŒ–ä¸ºä¸€ä¸ªä¸¤çº§ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶é‡‡ç”¨LoRAå‚æ•°è°ƒæ•´å’Œæ³¨æ„åŠ›å›¾æ­£åˆ™åŒ–æ¥é€‰æ‹©æ€§æŠ‘åˆ¶ä¸éœ€è¦çš„æ¿€æ´»ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§è‡ªæˆ‘å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œä»¥ç¡®ä¿åœ¨å»é™¤ä¸éœ€è¦çš„æ¦‚å¿µæ—¶ä¸ä¼šæ„å¤–åœ°æŸå®³å…¶ä»–ç›¸å…³æ¦‚å¿µçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEraseAnythingåœ¨æ–°çš„T2IèŒƒå¼ä¸­å¡«è¡¥äº†æ—©æœŸæ–¹æ³•çš„ç©ºç™½ï¼Œå¹¶åœ¨å„ç§æ¦‚å¿µæ“¦é™¤ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EraseAnythingæ˜¯ä¸“é—¨ä¸ºæœ€æ–°åŸºäºæµçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¡†æ¶ä¸­çš„æ¦‚å¿µæ“¦é™¤é—®é¢˜å¼€å‘çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å°†æ¦‚å¿µæ“¦é™¤å…¬å¼åŒ–ä¸ºä¸€ä¸ªä¸¤çº§ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>LoRAå‚æ•°è°ƒæ•´å’Œæ³¨æ„åŠ›å›¾æ­£åˆ™åŒ–è¢«ç”¨æ¥é€‰æ‹©æ€§æŠ‘åˆ¶ä¸éœ€è¦çš„æ¿€æ´»ã€‚</li>
<li>EraseAnythingé‡‡ç”¨è‡ªæˆ‘å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œç¡®ä¿åœ¨å»é™¤ä¸éœ€è¦çš„æ¦‚å¿µæ—¶ä¸ä¼šæŸå®³å…¶ä»–æ¦‚å¿µçš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰çš„æ¦‚å¿µæ“¦é™¤æŠ€æœ¯åœ¨æ–°çš„T2IèŒƒå¼ï¼ˆå¦‚Stable Diffusion v3å’ŒFluxï¼‰ä¸­çš„è½¬ç§»æ€§å—åˆ°é™åˆ¶ã€‚</li>
<li>EraseAnythingå¡«è¡¥äº†æ—©æœŸæ–¹æ³•åœ¨T2IèŒƒå¼ä¸­çš„ç©ºç™½ï¼Œå®ç°äº†å¹¿æ³›çš„æ¦‚å¿µæ“¦é™¤ä»»åŠ¡çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-faa34e7088a2cc4845386aef1ac866bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf4c66f9cd09149fb6696b693caa5517.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9acf677d168d89c91a7cfb5c1be74884.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="UIBDiffusion-Universal-Imperceptible-Backdoor-Attack-for-Diffusion-Models"><a href="#UIBDiffusion-Universal-Imperceptible-Backdoor-Attack-for-Diffusion-Models" class="headerlink" title="UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion   Models"></a>UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion   Models</h2><p><strong>Authors:Yuning Han, Bingyin Zhao, Rui Chu, Feng Luo, Biplab Sikdar, Yingjie Lao</strong></p>
<p>Recent studies show that diffusion models (DMs) are vulnerable to backdoor attacks. Existing backdoor attacks impose unconcealed triggers (e.g., a gray box and eyeglasses) that contain evident patterns, rendering remarkable attack effects yet easy detection upon human inspection and defensive algorithms. While it is possible to improve stealthiness by reducing the strength of the backdoor, doing so can significantly compromise its generality and effectiveness. In this paper, we propose UIBDiffusion, the universal imperceptible backdoor attack for diffusion models, which allows us to achieve superior attack and generation performance while evading state-of-the-art defenses. We propose a novel trigger generation approach based on universal adversarial perturbations (UAPs) and reveal that such perturbations, which are initially devised for fooling pre-trained discriminative models, can be adapted as potent imperceptible backdoor triggers for DMs. We evaluate UIBDiffusion on multiple types of DMs with different kinds of samplers across various datasets and targets. Experimental results demonstrate that UIBDiffusion brings three advantages: 1) Universality, the imperceptible trigger is universal (i.e., image and model agnostic) where a single trigger is effective to any images and all diffusion models with different samplers; 2) Utility, it achieves comparable generation quality (e.g., FID) and even better attack success rate (i.e., ASR) at low poison rates compared to the prior works; and 3) Undetectability, UIBDiffusion is plausible to human perception and can bypass Elijah and TERD, the SOTA defenses against backdoors for DMs. We will release our backdoor triggers and code. </p>
<blockquote>
<p>è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å®¹æ˜“å—åˆ°åé—¨æ”»å‡»ã€‚ç°æœ‰çš„åé—¨æ”»å‡»é‡‡ç”¨æœªéšè”½çš„è§¦å‘å™¨ï¼ˆä¾‹å¦‚ï¼Œç°ç›’å’Œçœ¼é•œï¼‰ï¼Œè¿™äº›è§¦å‘å™¨åŒ…å«æ˜æ˜¾çš„æ¨¡å¼ï¼Œè™½ç„¶æ”»å‡»æ•ˆæœæ˜¾è‘—ï¼Œä½†å¾ˆå®¹æ˜“é€šè¿‡äººå·¥æ£€æŸ¥å’Œé˜²å¾¡ç®—æ³•è¿›è¡Œæ£€æµ‹ã€‚è™½ç„¶é€šè¿‡å‡å¼±åé—¨å¼ºåº¦å¯ä»¥æé«˜éšè”½æ€§ï¼Œä½†è¿™æ ·åšå¯èƒ½ä¼šæ˜¾è‘—æŸå®³å…¶é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„é€šç”¨éšè”½åé—¨æ”»å‡»æ–¹æ³•UIBDiffusionï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨èº²é¿æœ€æ–°é˜²å¾¡æªæ–½çš„åŒæ—¶å®ç°å“è¶Šçš„æ”»å‡»å’Œç”Ÿæˆæ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé€šç”¨å¯¹æŠ—æ€§æ‰°åŠ¨ï¼ˆUAPsï¼‰çš„æ–°å‹è§¦å‘å™¨ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶æ­ç¤ºè¿™ç§æœ€åˆè¢«è®¾è®¡ç”¨äºæ¬ºéª—é¢„è®­ç»ƒåˆ¤åˆ«æ¨¡å‹çš„æ‰°åŠ¨ï¼Œå¯ä»¥é€‚åº”æˆä¸ºé’ˆå¯¹DMsçš„å¼ºå¤§éšè”½åé—¨è§¦å‘å™¨ã€‚æˆ‘ä»¬åœ¨å¤šç§ç±»å‹çš„DMsã€å¤šç§é‡‡æ ·å™¨ä»¥åŠä¸åŒæ•°æ®é›†å’Œç›®æ ‡ä¸Šè¯„ä¼°äº†UIBDiffusionã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUIBDiffusionå…·æœ‰ä¸‰ä¸ªä¼˜åŠ¿ï¼š1ï¼‰é€šç”¨æ€§ï¼Œéšè”½è§¦å‘å™¨æ˜¯é€šç”¨çš„ï¼ˆå³å›¾åƒå’Œæ¨¡å‹æ— å…³ï¼‰ï¼Œå•ä¸ªè§¦å‘å™¨å¯¹æ‰€æœ‰å›¾åƒå’Œä¸åŒé‡‡æ ·å™¨çš„æ‰€æœ‰æ‰©æ•£æ¨¡å‹éƒ½æœ‰æ•ˆï¼›2ï¼‰å®ç”¨æ€§ï¼Œå®ƒåœ¨ä½æ¯’ç‡ä¸‹çš„ç”Ÿæˆè´¨é‡ï¼ˆä¾‹å¦‚FIDï¼‰ä¸å…ˆå‰ä½œå“ç›¸å½“ï¼Œç”šè‡³æ”»å‡»æˆåŠŸç‡ï¼ˆå³ASRï¼‰æ›´é«˜ï¼›3ï¼‰ä¸å¯æ£€æµ‹æ€§ï¼ŒUIBDiffusionå¯¹äººç±»æ„ŸçŸ¥æ˜¯åˆç†çš„ï¼Œå¹¶ä¸”èƒ½å¤Ÿç»•è¿‡é’ˆå¯¹DMåé—¨çš„æœ€æ–°é˜²å¾¡æ‰‹æ®µElijahå’ŒTERDã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„åé—¨è§¦å‘å™¨å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11441v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å­˜åœ¨åé—¨æ”»å‡»çš„é£é™©ã€‚ç°æœ‰çš„åé—¨æ”»å‡»æ–¹æ³•ä¼šä½¿ç”¨æ˜æ˜¾çš„è§¦å‘ç‰©ï¼ˆå¦‚ç°ç›’å’Œçœ¼é•œï¼‰ï¼Œè¿™äº›è§¦å‘ç‰©å«æœ‰æ˜æ˜¾çš„æ¨¡å¼ï¼Œæ˜“äºäººç±»æ£€æŸ¥å’Œé˜²å¾¡ç®—æ³•çš„æ£€æµ‹ã€‚åœ¨æé«˜éšè”½æ€§çš„åŒæ—¶é™ä½åé—¨å¼ºåº¦å¯èƒ½ä¼šå¯¼è‡´å…¶é€šç”¨æ€§å’Œæ•ˆæœå¤§æ‰“æŠ˜æ‰£ã€‚æœ¬æ–‡æå‡ºäº†é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„é€šç”¨éšè”½åé—¨æ”»å‡»æ–¹æ³•UIBDiffusionï¼Œå®ƒå¯ä»¥åœ¨èº²é¿ç°æœ‰æœ€å…ˆè¿›çš„é˜²å¾¡æ‰‹æ®µçš„åŒæ—¶å®ç°å‡ºè‰²çš„æ”»å‡»å’Œç”Ÿæˆæ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé€šç”¨å¯¹æŠ—æ€§æ‰°åŠ¨ï¼ˆUAPsï¼‰çš„æ–°å‹è§¦å‘ç‰©ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶å‘ç°è¿™ç§æ‰°åŠ¨ï¼Œæœ€åˆè¢«è®¾è®¡ç”¨æ¥æ¬ºéª—é¢„è®­ç»ƒçš„åˆ¤åˆ«æ¨¡å‹ï¼Œå¯ä»¥é€‚åº”æˆä¸ºé’ˆå¯¹DMsçš„å¼ºå¤§éšè”½åé—¨è§¦å‘ç‰©ã€‚æˆ‘ä»¬åœ¨å¤šç§ç±»å‹çš„æ‰©æ•£æ¨¡å‹ã€ä¸åŒé‡‡æ ·å™¨ä»¥åŠå¤šä¸ªæ•°æ®é›†å’Œç›®æ ‡ä¸Šè¯„ä¼°äº†UIBDiffusionã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUIBDiffusionå…·æœ‰ä¸‰ä¸ªä¼˜åŠ¿ï¼š1ï¼‰é€šç”¨æ€§ï¼Œéšè”½è§¦å‘ç‰©æ˜¯é€šç”¨çš„ï¼ˆå³å›¾åƒå’Œæ¨¡å‹æ— å…³ï¼‰ï¼Œå•ä¸ªè§¦å‘ç‰©å¯¹æ‰€æœ‰å›¾åƒå’Œæ‰€æœ‰æ‰©æ•£æ¨¡å‹åŠä¸åŒé‡‡æ ·å™¨å‡æœ‰æ•ˆï¼›2ï¼‰å®ç”¨æ€§ï¼Œå®ƒåœ¨ä½æ¯’ç‡ä¸‹çš„ç”Ÿæˆè´¨é‡ï¼ˆä¾‹å¦‚FIDï¼‰ä¸å…ˆå‰çš„å·¥ä½œç›¸å½“ï¼Œç”šè‡³æ”»å‡»æˆåŠŸç‡ï¼ˆå³ASRï¼‰æ›´é«˜ï¼›3ï¼‰éšè”½æ€§ï¼ŒUIBDiffusionå¯¹äººç±»æ„ŸçŸ¥æ˜¯åˆç†çš„ï¼Œå¯ä»¥ç»•è¿‡Elijahå’ŒTERDç­‰é’ˆå¯¹DMsåé—¨çš„æœ€å…ˆè¿›é˜²å¾¡æ‰‹æ®µã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„åé—¨è§¦å‘å™¨å’Œä»£ç ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰é¢ä¸´åé—¨æ”»å‡»é£é™©ï¼Œç°æœ‰æ–¹æ³•ä½¿ç”¨æ˜æ˜¾è§¦å‘ç‰©ï¼Œæ˜“æ£€æµ‹ã€‚</li>
<li>æå‡ºUIBDiffusionæ–¹æ³•ï¼Œå®ç°é’ˆå¯¹DMsçš„éšè”½åé—¨æ”»å‡»ã€‚</li>
<li>åŸºäºé€šç”¨å¯¹æŠ—æ€§æ‰°åŠ¨ï¼ˆUAPsï¼‰çš„è§¦å‘ç‰©ç”Ÿæˆï¼Œé€‚ç”¨äºå¤šç§æ‰©æ•£æ¨¡å‹å’Œé‡‡æ ·å™¨ã€‚</li>
<li>UIBDiffusionå…·æœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼šé€šç”¨æ€§ã€å®ç”¨æ€§å’Œéšè”½æ€§ã€‚</li>
<li>UIBDiffusionå¯å®ç°é«˜æ°´å¹³çš„æ”»å‡»æ€§èƒ½åŒæ—¶èº²é¿ç°æœ‰é˜²å¾¡æ‰‹æ®µã€‚</li>
<li>å°†å‘å¸ƒåé—¨è§¦å‘å™¨å’Œä»£ç ä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
<li>ä¸ºæ‰©æ•£æ¨¡å‹çš„å®‰å…¨æ€§é—®é¢˜æä¾›äº†æ–°çš„æ€è€ƒå’Œè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4db5d64700130fb76713e92798f23d80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc20b4d237d96ae1eca91895b3c5143a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-571b5258d7644936cd3fe44058fae48c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-786873e01a3549568ba95050f3b63655.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36cb40eff67b4d2e2e4a42d711c69ce7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef672e6684a2d18226e2d08a15a126f5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Dynamic-Negative-Guidance-of-Diffusion-Models"><a href="#Dynamic-Negative-Guidance-of-Diffusion-Models" class="headerlink" title="Dynamic Negative Guidance of Diffusion Models"></a>Dynamic Negative Guidance of Diffusion Models</h2><p><strong>Authors:Felix Koulischer, Johannes Deleu, Gabriel Raya, Thomas Demeester, Luca Ambrogioni</strong></p>
<p>Negative Prompting (NP) is widely utilized in diffusion models, particularly in text-to-image applications, to prevent the generation of undesired features. In this paper, we show that conventional NP is limited by the assumption of a constant guidance scale, which may lead to highly suboptimal results, or even complete failure, due to the non-stationarity and state-dependence of the reverse process. Based on this analysis, we derive a principled technique called Dynamic Negative Guidance, which relies on a near-optimal time and state dependent modulation of the guidance without requiring additional training. Unlike NP, negative guidance requires estimating the posterior class probability during the denoising process, which is achieved with limited additional computational overhead by tracking the discrete Markov Chain during the generative process. We evaluate the performance of DNG class-removal on MNIST and CIFAR10, where we show that DNG leads to higher safety, preservation of class balance and image quality when compared with baseline methods. Furthermore, we show that it is possible to use DNG with Stable Diffusion to obtain more accurate and less invasive guidance than NP. </p>
<blockquote>
<p>è´Ÿå‘æç¤ºï¼ˆNPï¼‰åœ¨æ‰©æ•£æ¨¡å‹ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„åº”ç”¨ä¸­ï¼Œç”¨äºé˜²æ­¢ç”Ÿæˆä¸éœ€è¦çš„ç‰¹å¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¼ ç»Ÿçš„NPå—åˆ°æ’å®šæŒ‡å¯¼å°ºåº¦çš„å‡è®¾çš„é™åˆ¶ï¼Œè¿™å¯èƒ½å¯¼è‡´ç»“æœé«˜åº¦ä¸ç†æƒ³ï¼Œç”šè‡³å®Œå…¨å¤±è´¥ï¼ŒåŸå› æ˜¯åå‘è¿‡ç¨‹å…·æœ‰éå¹³ç¨³æ€§å’ŒçŠ¶æ€ä¾èµ–æ€§ã€‚åŸºäºè¿™ä¸€åˆ†æï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†ä¸€ç§åŸºäºåŸç†çš„æŠ€æœ¯ï¼Œç§°ä¸ºåŠ¨æ€è´Ÿå‘æŒ‡å¯¼ï¼ˆDNGï¼‰ï¼Œå®ƒä¾èµ–äºè¿‘æœ€ä¼˜çš„æ—¶é—´å’ŒçŠ¶æ€ä¾èµ–çš„æŒ‡å¯¼è°ƒåˆ¶ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚ä¸NPä¸åŒï¼Œè´Ÿå‘æŒ‡å¯¼éœ€è¦åœ¨å»å™ªè¿‡ç¨‹ä¸­ä¼°è®¡åéªŒç±»åˆ«æ¦‚ç‡ï¼Œè¿™å¯ä»¥é€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è·Ÿè¸ªç¦»æ•£é©¬å°”å¯å¤«é“¾æ¥å®ç°ï¼Œå¹¶ä¸”åªéœ€è¦æœ‰é™çš„é¢å¤–è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬åœ¨MNISTå’ŒCIFAR10ä¸Šè¯„ä¼°äº†DNGç±»æ¶ˆé™¤çš„æ€§èƒ½ï¼Œç»“æœæ˜¾ç¤ºï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼ŒDNGæé«˜äº†å®‰å…¨æ€§ï¼Œä¿æŒäº†ç±»åˆ«å¹³è¡¡å’Œå›¾åƒè´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†å°†DNGä¸Stable Diffusionç»“åˆä½¿ç”¨ï¼Œå¯ä»¥è·å¾—æ¯”NPæ›´å‡†ç¡®ã€ä¾µå…¥æ€§è¾ƒå°çš„æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14398v2">PDF</a> Paper currently under review. Submitted to ICLR 2025. Our   implementation is available at   <a target="_blank" rel="noopener" href="https://github.com/FelixKoulischer/Dynamic-Negative-Guidance.git">https://github.com/FelixKoulischer/Dynamic-Negative-Guidance.git</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹ä¸­çš„è´Ÿæç¤ºï¼ˆNPï¼‰æŠ€æœ¯çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬è½¬å›¾åƒåº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„æŠ€æœ¯â€”â€”åŠ¨æ€è´ŸæŒ‡å¯¼ï¼ˆDNGï¼‰ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨ä¸éœ€è¦é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå¯¹æŒ‡å¯¼å’Œè°ƒåˆ¶è¿›è¡Œè¿‘ä¼˜çš„æ—¶é—´å’ŒçŠ¶æ€ä¾èµ–æ€§çš„è°ƒæ•´ã€‚åœ¨MNISTå’ŒCIFAR10ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼ŒDNGåœ¨å®‰å…¨æ€§ã€ä¿æŒç±»å¹³è¡¡å’Œå›¾åƒè´¨é‡æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå°†DNGä¸Stable Diffusionç»“åˆä½¿ç”¨ï¼Œå¯ä»¥è·å¾—æ¯”NPæ›´å‡†ç¡®ã€ä¾µå…¥æ€§è¾ƒå°çš„æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¸­å¹¿æ³›ä½¿ç”¨çš„è´Ÿæç¤ºï¼ˆNPï¼‰æŠ€æœ¯å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ–‡æœ¬è½¬å›¾åƒåº”ç”¨ä¸­ã€‚</li>
<li>åŠ¨æ€è´ŸæŒ‡å¯¼ï¼ˆDNGï¼‰æ˜¯ä¸€ç§æ–°çš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿè¿‘ä¼˜åœ°è°ƒæ•´æŒ‡å¯¼å’Œè°ƒåˆ¶çš„æ—¶é—´å’ŒçŠ¶æ€ä¾èµ–æ€§ã€‚</li>
<li>DNGä¸éœ€è¦é¢å¤–çš„è®­ç»ƒã€‚</li>
<li>åœ¨MNISTå’ŒCIFAR10ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒDNGåœ¨å®‰å…¨æ€§ã€ä¿æŒç±»å¹³è¡¡å’Œå›¾åƒè´¨é‡æ–¹é¢ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</li>
<li>DNGæŠ€æœ¯å¯ä»¥æœ‰æ•ˆåœ°é˜²æ­¢ç”Ÿæˆä¸éœ€è¦çš„ç‰¹å¾ã€‚</li>
<li>DNGå¯ç”¨äºæé«˜Stable Diffusionçš„å‡†ç¡®æ€§å¹¶å‡å°‘ä¾µå…¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14398">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a2de5c4de0fb4266bd215fec6034c3d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a9af734a4bd7bdc8d2600bb3c0b4f99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-081718a59f4832b132a57dc1e7c39aed.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AugGS-Self-augmented-Gaussians-with-Structural-Masks-for-Sparse-view-3D-Reconstruction"><a href="#AugGS-Self-augmented-Gaussians-with-Structural-Masks-for-Sparse-view-3D-Reconstruction" class="headerlink" title="AugGS: Self-augmented Gaussians with Structural Masks for Sparse-view 3D   Reconstruction"></a>AugGS: Self-augmented Gaussians with Structural Masks for Sparse-view 3D   Reconstruction</h2><p><strong>Authors:Biâ€™an Du, Lingbei Meng, Wei Hu</strong></p>
<p>Sparse-view 3D reconstruction is a major challenge in computer vision, aiming to create complete three-dimensional models from limited viewing angles. Key obstacles include: 1) a small number of input images with inconsistent information; 2) dependence on input image quality; and 3) large model parameter sizes. To tackle these issues, we propose a self-augmented two-stage Gaussian splatting framework enhanced with structural masks for sparse-view 3D reconstruction. Initially, our method generates a basic 3D Gaussian representation from sparse inputs and renders multi-view images. We then fine-tune a pre-trained 2D diffusion model to enhance these images, using them as augmented data to further optimize the 3D Gaussians. Additionally, a structural masking strategy during training enhances the modelâ€™s robustness to sparse inputs and noise. Experiments on benchmarks like MipNeRF360, OmniObject3D, and OpenIllumination demonstrate that our approach achieves state-of-the-art performance in perceptual quality and multi-view consistency with sparse inputs. </p>
<blockquote>
<p>ç¨€ç–è§†è§’ä¸‹çš„ä¸‰ç»´é‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€å¤§æŒ‘æˆ˜ï¼Œæ—¨åœ¨ä»æœ‰é™çš„è§†è§’åˆ›å»ºå®Œæ•´çš„ä¸‰ç»´æ¨¡å‹ã€‚ä¸»è¦éšœç¢åŒ…æ‹¬ï¼š1ï¼‰è¾“å…¥å›¾åƒæ•°é‡å°‘ä¸”ä¿¡æ¯ä¸ä¸€è‡´ï¼›2ï¼‰ä¾èµ–äºè¾“å…¥å›¾åƒçš„è´¨é‡ï¼›ä»¥åŠ3ï¼‰æ¨¡å‹å‚æ•°è§„æ¨¡å¤§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç»“æ„æ©è†œçš„è‡ªå¢å¼ºä¸¤é˜¶æ®µé«˜æ–¯å–·å°„æ¡†æ¶ï¼Œç”¨äºç¨€ç–è§†è§’ä¸‹çš„ä¸‰ç»´é‡å»ºã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»ç¨€ç–è¾“å…¥ç”ŸæˆåŸºæœ¬çš„ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºï¼Œå¹¶å‘ˆç°å¤šè§†è§’å›¾åƒã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹å¯¹è¿™äº›å›¾åƒè¿›è¡Œå¾®è°ƒä»¥å¢å¼ºå…¶è´¨é‡ï¼Œå°†å…¶ä½œä¸ºå¢å¼ºæ•°æ®è¿›ä¸€æ­¥ä¼˜åŒ–ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨ç»“æ„æ©è†œç­–ç•¥å¢å¼ºäº†æ¨¡å‹å¯¹ç¨€ç–è¾“å…¥å’Œå™ªå£°çš„é²æ£’æ€§ã€‚åœ¨MipNeRF360ã€OmniObject3Då’ŒOpenIlluminationç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡å’Œå¤šè§†è§’ä¸€è‡´æ€§æ–¹é¢è¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½ï¼Œä¸”åœ¨ç¨€ç–è¾“å…¥æƒ…å†µä¸‹è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.04831v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†é’ˆå¯¹ç¨€ç–è§†è§’3Dé‡å»ºçš„ä¸»è¦æŒ‘æˆ˜åŠå…¶è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æå‡ºä¸€ç§åŸºäºè‡ªå¢å¼ºå’Œä¸¤é˜¶æ®µé«˜æ–¯æ¶‚æŠ¹æ¡†æ¶çš„æ–¹æ³•ï¼Œç»“åˆç»“æ„æ©ç æŠ€æœ¯ï¼Œå®ç°äº†ä»æœ‰é™è§†è§’ä¿¡æ¯æ„å»ºå®Œæ•´ä¸‰ç»´æ¨¡å‹çš„ç›®æ ‡ã€‚é¦–å…ˆç”ŸæˆåŸºæœ¬çš„ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒå¢å¼ºï¼Œä½œä¸ºæ‰©å……æ•°æ®è¿›ä¸€æ­¥ä¼˜åŒ–ä¸‰ç»´é«˜æ–¯æ¨¡å‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡å’Œå¤šè§†è§’ä¸€è‡´æ€§æ–¹é¢è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¨€ç–è§†è§’3Dé‡å»ºé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬è¾“å…¥å›¾åƒæ•°é‡å°‘ä¸”ä¿¡æ¯ä¸ä¸€è‡´ã€ä¾èµ–è¾“å…¥å›¾åƒè´¨é‡å’Œæ¨¡å‹å‚æ•°è§„æ¨¡è¿‡å¤§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªå¢å¼ºä¸¤é˜¶æ®µé«˜æ–¯æ¶‚æŠ¹æ¡†æ¶ï¼Œç”¨äºè§£å†³ç¨€ç–è§†è§’3Dé‡å»ºé—®é¢˜ã€‚</li>
<li>é€šè¿‡ç”ŸæˆåŸºæœ¬çš„ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºå¹¶ä»ç¨€ç–è¾“å…¥æ¸²æŸ“å¤šè§†è§’å›¾åƒï¼Œä¸º3Dé‡å»ºæä¾›åŸºç¡€ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹å¢å¼ºå›¾åƒï¼Œä½œä¸ºæ‰©å……æ•°æ®è¿›ä¸€æ­¥ä¼˜åŒ–ä¸‰ç»´é«˜æ–¯æ¨¡å‹ã€‚</li>
<li>ç»“åˆç»“æ„æ©ç æŠ€æœ¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¢å¼ºæ¨¡å‹å¯¹ç¨€ç–è¾“å…¥å’Œå™ªå£°çš„é²æ£’æ€§ã€‚</li>
<li>åœ¨MipNeRF360ã€OmniObject3Då’ŒOpenIlluminationç­‰åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡å’Œå¤šè§†è§’ä¸€è‡´æ€§æ–¹é¢è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.04831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8ca103b0d2b965c34db218e2ec27fbca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52a4a78f54cfd0e14644f544afee6778.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-264f5c7c995c8c886b0f1cb2044b1802.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8855da46d8089177baaf2ead6ab9ba36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06316088fb123dc015a86b70f4a19ddd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SlerpFace-Face-Template-Protection-via-Spherical-Linear-Interpolation"><a href="#SlerpFace-Face-Template-Protection-via-Spherical-Linear-Interpolation" class="headerlink" title="SlerpFace: Face Template Protection via Spherical Linear Interpolation"></a>SlerpFace: Face Template Protection via Spherical Linear Interpolation</h2><p><strong>Authors:Zhizhou Zhong, Yuxi Mi, Yuge Huang, Jianqing Xu, Guodong Mu, Shouhong Ding, Jingyun Zhang, Rizen Guo, Yunsheng Wu, Shuigeng Zhou</strong></p>
<p>Contemporary face recognition systems use feature templates extracted from face images to identify persons. To enhance privacy, face template protection techniques are widely employed to conceal sensitive identity and appearance information stored in the template. This paper identifies an emerging privacy attack form utilizing diffusion models that could nullify prior protection. The attack can synthesize high-quality, identity-preserving face images from templates, revealing personsâ€™ appearance. Based on studies of the diffusion modelâ€™s generative capability, this paper proposes a defense by rotating templates to a noise-like distribution. This is achieved efficiently by spherically and linearly interpolating templates on their located hypersphere. This paper further proposes to group-wisely divide and drop out templatesâ€™ feature dimensions, to enhance the irreversibility of rotated templates. The proposed techniques are concretized as a novel face template protection technique, SlerpFace. Extensive experiments show that SlerpFace provides satisfactory recognition accuracy and comprehensive protection against inversion and other attack forms, superior to prior arts. </p>
<blockquote>
<p>å½“ä»£äººè„¸è¯†åˆ«ç³»ç»Ÿä½¿ç”¨ä»äººè„¸å›¾åƒä¸­æå–çš„ç‰¹å¾æ¨¡æ¿æ¥è¯†åˆ«ä¸ªäººã€‚ä¸ºäº†å¢å¼ºéšç§ä¿æŠ¤ï¼Œå¹¿æ³›é‡‡ç”¨äººè„¸æ¨¡æ¿ä¿æŠ¤æŠ€æœ¯æ¥éšè—å­˜å‚¨åœ¨æ¨¡æ¿ä¸­çš„æ•æ„Ÿèº«ä»½å’Œå¤–è§‚ä¿¡æ¯ã€‚æœ¬æ–‡å‘ç°äº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ–°å…´éšç§æ”»å‡»å½¢å¼ï¼Œå¯èƒ½ä¼šä½¿å…ˆå‰çš„ä¿æŠ¤å¤±æ•ˆã€‚è¿™ç§æ”»å‡»å¯ä»¥ä»æ¨¡æ¿ä¸­åˆæˆé«˜è´¨é‡ã€ä¿ç•™èº«ä»½çš„äººè„¸å›¾åƒï¼Œä»è€Œæ­ç¤ºä¸ªäººçš„å¤–è§‚ã€‚åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ç ”ç©¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡æ—‹è½¬æ¨¡æ¿åˆ°ç±»ä¼¼å™ªå£°çš„åˆ†å¸ƒæ¥è¿›è¡Œé˜²å¾¡çš„æ–¹æ³•ã€‚è¿™æ˜¯é€šè¿‡åœ¨è¶…çƒä½“ä¸Šçƒé¢å’Œçº¿æ€§æ’å€¼æ¨¡æ¿æ¥å®ç°çš„ã€‚æœ¬æ–‡è¿˜æå‡ºå°†æ¨¡æ¿çš„ç‰¹å¾ç»´åº¦åˆ†ç»„å¹¶åˆ é™¤ï¼Œä»¥å¢å¼ºæ—‹è½¬æ¨¡æ¿çš„ä¸å¯é€†æ€§ã€‚æ‰€æå‡ºçš„æŠ€æœ¯è¢«å…·ä½“åŒ–ä¸ºä¸€ç§æ–°å‹çš„äººè„¸æ¨¡æ¿ä¿æŠ¤æŠ€æœ¯â€”â€”SlerpFaceã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSlerpFaceæä¾›ä»¤äººæ»¡æ„çš„è¯†åˆ«ç²¾åº¦å’Œå…¨é¢çš„ä¿æŠ¤ï¼Œå¯¹æŠ—è§£å¯†å’Œå…¶ä»–æ”»å‡»å½¢å¼ï¼Œä¼˜äºå…ˆå‰æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.03043v2">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å½“ä»£äººè„¸è¯†åˆ«ç³»ç»Ÿä½¿ç”¨ç‰¹å¾æ¨¡æ¿è¿›è¡Œèº«ä»½è¯†åˆ«çš„æƒ…å†µï¼Œå¹¶æŒ‡å‡ºä¸ºäº†ä¿æŠ¤éšç§ï¼Œé‡‡å–äº†ä¿æŠ¤é¢éƒ¨æ¨¡æ¿çš„æŠ€æœ¯æ¥æ©ç›–å­˜å‚¨åœ¨æ¨¡æ¿ä¸­çš„æ•æ„Ÿèº«ä»½å’Œå¤–è§‚ä¿¡æ¯ã€‚ç„¶è€Œï¼Œæœ¬æ–‡å‘ç°äº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ–°å‹éšç§æ”»å‡»æ–¹å¼ï¼Œè¯¥æ”»å‡»å¯ä»¥ä»æ¨¡æ¿ä¸­åˆæˆé«˜è´¨é‡ã€ä¿ç•™èº«ä»½çš„äººè„¸å›¾åƒï¼Œä»è€Œæ­ç¤ºä¸ªäººçš„å¤–è§‚ä¿¡æ¯ã€‚é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡æ—‹è½¬æ¨¡æ¿åˆ°å™ªå£°çŠ¶åˆ†å¸ƒçš„é˜²å¾¡ç­–ç•¥ï¼Œå¹¶åœ¨è¶…çƒä½“ä¸Šå®ç°çƒé¢çº¿æ€§æ’å€¼ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†åˆ†ç»„æ™ºèƒ½çš„æ¨¡æ¿ç‰¹å¾ç»´åº¦åˆ å‡ç­–ç•¥ï¼Œä»¥æé«˜æ—‹è½¬æ¨¡æ¿çš„ä¸å¯é€†æ€§ã€‚è¿™äº›æŠ€æœ¯è¢«å…·ä½“åŒ–ä¸ºä¸€ç§æ–°å‹é¢éƒ¨æ¨¡æ¿ä¿æŠ¤æŠ€æœ¯â€”â€”SlerpFaceã€‚å®éªŒè¡¨æ˜ï¼ŒSlerpFaceåœ¨æä¾›è¯†åˆ«å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå¯¹é€†è½¬å˜ç§åŠå…¶ä»–æ”»å‡»å½¢å¼æä¾›äº†å‡ºè‰²çš„ä¿æŠ¤ï¼Œè¶…è¶Šäº†å…ˆå‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“ä»£äººè„¸è¯†åˆ«ç³»ç»Ÿä½¿ç”¨ç‰¹å¾æ¨¡æ¿è¿›è¡Œèº«ä»½è¯†åˆ«ã€‚</li>
<li>é¢éƒ¨æ¨¡æ¿ä¿æŠ¤æŠ€æœ¯ç”¨äºæ©ç›–å­˜å‚¨åœ¨æ¨¡æ¿ä¸­çš„æ•æ„Ÿèº«ä»½å’Œå¤–è§‚ä¿¡æ¯ã€‚</li>
<li>å‡ºç°äº†ä¸€ç§æ–°å‹éšç§æ”»å‡»æ–¹å¼ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä»æ¨¡æ¿ä¸­åˆæˆé«˜è´¨é‡äººè„¸å›¾åƒã€‚</li>
<li>æå‡ºäº†é€šè¿‡æ—‹è½¬æ¨¡æ¿åˆ°å™ªå£°çŠ¶åˆ†å¸ƒçš„é˜²å¾¡ç­–ç•¥ã€‚</li>
<li>å®ç°äº†çƒé¢çº¿æ€§æ’å€¼æŠ€æœ¯ä»¥æé«˜æ—‹è½¬æ¨¡æ¿çš„å®‰å…¨æ€§ã€‚</li>
<li>æå‡ºäº†åˆ†ç»„æ™ºèƒ½çš„æ¨¡æ¿ç‰¹å¾ç»´åº¦åˆ å‡ç­–ç•¥ä»¥å¢å¼ºæ¨¡æ¿çš„ä¸å¯é€†æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.03043">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d239e0e3428c4ef4516cc87ca73df8aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-380fdcd5189faad7506fb29f1b56e496.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f3095299d7149b8a6dbca3e09ef32b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85bf76ce40bdf957726531313f802b1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9645fcd23772df691db65fbd950ca54b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Adapting-to-Unknown-Low-Dimensional-Structures-in-Score-Based-Diffusion-Models"><a href="#Adapting-to-Unknown-Low-Dimensional-Structures-in-Score-Based-Diffusion-Models" class="headerlink" title="Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion   Models"></a>Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion   Models</h2><p><strong>Authors:Gen Li, Yuling Yan</strong></p>
<p>This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions. Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper. For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable. We further identify a unique design of coefficients that yields a converges rate at the order of $O(k^{2}&#x2F;\sqrt{T})$ (up to log factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of steps. This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design. All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹ï¼Œå½“åº•å±‚ç›®æ ‡åˆ†å¸ƒé›†ä¸­åœ¨å®ƒä»¬æ­£å¼å­˜åœ¨çš„é«˜ç»´ç©ºé—´ä¸­çš„ä½ç»´æµå½¢ä¸Šæˆ–é™„è¿‘æ—¶ï¼Œè¿™æ˜¯è‡ªç„¶å›¾åƒåˆ†å¸ƒçš„ä¸€ä¸ªå¸¸è§ç‰¹å¾ã€‚å°½ç®¡ä¹‹å‰å·²ç»æœ‰äººåŠªåŠ›ç†è§£æ‰©æ•£æ¨¡å‹çš„æ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼Œä½†åœ¨å­˜åœ¨ä½ç»´ç»“æ„çš„æƒ…å†µä¸‹ï¼Œç°æœ‰çš„ç†è®ºæ”¯æŒä»ç„¶è¿œè¿œä¸å¤Ÿï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­åŠ å¼ºäº†è¿™ä¸€ç‚¹ã€‚å¯¹äºæµè¡Œçš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ï¼Œæˆ‘ä»¬å‘ç°æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­äº§ç”Ÿçš„é”™è¯¯å¯¹ç¯å¢ƒç»´åº¦$d$çš„ä¾èµ–é€šå¸¸æ˜¯ä¸å¯é¿å…çš„ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯†åˆ«å‡ºäº†ä¸€ç§ç‹¬ç‰¹çš„ç³»æ•°è®¾è®¡ï¼Œå…¶æ”¶æ•›é€Ÿç‡ä¸º$O(k^{2}&#x2F;\sqrt{T})$ï¼ˆåŒ…å«å¯¹æ•°å› å­ï¼‰ï¼Œå…¶ä¸­$k$æ˜¯ç›®æ ‡åˆ†å¸ƒçš„å†…åœ¨ç»´åº¦ï¼Œ$T$æ˜¯æ­¥éª¤æ•°ã€‚è¿™é¦–æ¬¡ä»ç†è®ºä¸Šè¯æ˜äº†DDPMé‡‡æ ·å™¨å¯ä»¥é€‚åº”ç›®æ ‡åˆ†å¸ƒä¸­æœªçŸ¥çš„ä½ç»´ç»“æ„ï¼Œçªå‡ºäº†ç³»æ•°è®¾è®¡çš„å…³é”®é‡è¦æ€§ã€‚æ‰€æœ‰è¿™äº›éƒ½æ˜¯é€šè¿‡ä¸€å¥—æ–°å‹çš„åˆ†æå·¥å…·å®ç°çš„ï¼Œè¿™äº›å·¥å…·ä»¥æ›´ç¡®å®šçš„æ–¹å¼æè¿°äº†ç®—æ³•çš„åŠ¨åŠ›å­¦ç‰¹å¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14861v2">PDF</a> accepted to NeurIPS 2024</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†åŸºäºåˆ†æ•°æ‰©æ•£æ¨¡å‹åœ¨ç›®æ ‡åˆ†å¸ƒé›†ä¸­äºæˆ–æ¥è¿‘ä½ç»´æµå½¢æ—¶çš„è¡¨ç°ï¼Œè¿™å¸¸è§äºè‡ªç„¶å›¾åƒåˆ†å¸ƒã€‚å¯¹äºæµè¡Œçš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ï¼Œæœ¬æ–‡åˆ†æäº†åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­è¯¯å·®å¯¹å‘¨å›´ç»´åº¦çš„ä¾èµ–æ˜¯ä¸å¯é¿å…çš„ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ç‹¬ç‰¹çš„ç³»æ•°ï¼Œä½¿å¾—æ”¶æ•›é€Ÿåº¦è¾¾åˆ°O(k^2&#x2F;âˆšT)ï¼Œå…¶ä¸­kä¸ºç›®æ ‡åˆ†å¸ƒçš„å†…åœ¨ç»´åº¦ï¼ŒTä¸ºæ­¥éª¤æ•°ã€‚è¿™è¯æ˜äº†DDPMé‡‡æ ·å™¨èƒ½å¤Ÿé€‚åº”ç›®æ ‡åˆ†å¸ƒä¸­çš„æœªçŸ¥ä½ç»´ç»“æ„ï¼Œå¼ºè°ƒç³»æ•°è®¾è®¡çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ†æ•°æ‰©æ•£æ¨¡å‹åœ¨ä½ç»´æµå½¢ä¸Šçš„è¡¨ç°æ˜¯ç ”ç©¶é‡ç‚¹ï¼Œè‡ªç„¶å›¾åƒåˆ†å¸ƒå¸¸å…·æœ‰æ­¤ç‰¹æ€§ã€‚</li>
<li>å¯¹äºDDPMæ¨¡å‹ï¼Œåœ¨å»å™ªè¿‡ç¨‹ä¸­è¯¯å·®å¯¹å‘¨å›´ç»´åº¦çš„ä¾èµ–æ˜¯æ™®éå­˜åœ¨çš„ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§ç‹¬ç‰¹çš„ç³»æ•°ï¼Œæé«˜äº†æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦ï¼Œè¾¾åˆ°O(k^2&#x2F;âˆšT)ã€‚</li>
<li>è¯¥è®¾è®¡ä½¿å¾—DDPMèƒ½å¤Ÿé€‚åº”ç›®æ ‡åˆ†å¸ƒä¸­çš„æœªçŸ¥ä½ç»´ç»“æ„ã€‚</li>
<li>ç³»æ•°è®¾è®¡åœ¨æ‰©æ•£æ¨¡å‹ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>æœ¬æ–‡æä¾›äº†æ–°å‹åˆ†æå·¥å…·ï¼Œä»¥æ›´ç¡®å®šçš„æ–¹å¼æè¿°äº†ç®—æ³•åŠ¨æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14861">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-214222a4f5a5ee4841932e768473e4dd.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SOEDiff-Efficient-Distillation-for-Small-Object-Editing"><a href="#SOEDiff-Efficient-Distillation-for-Small-Object-Editing" class="headerlink" title="SOEDiff: Efficient Distillation for Small Object Editing"></a>SOEDiff: Efficient Distillation for Small Object Editing</h2><p><strong>Authors:Yiming Wu, Qihe Pan, Zhen Zhao, Zicheng Wang, Sifan Long, Ronghua Liang</strong></p>
<p>In this paper, we delve into a new task known as small object editing (SOE), which focuses on text-based image inpainting within a constrained, small-sized area. Despite the remarkable success have been achieved by current image inpainting approaches, their application to the SOE task generally results in failure cases such as Object Missing, Text-Image Mismatch, and Distortion. These failures stem from the limited use of small-sized objects in training datasets and the downsampling operations employed by U-Net models, which hinders accurate generation. To overcome these challenges, we introduce a novel training-based approach, SOEDiff, aimed at enhancing the capability of baseline models like StableDiffusion in editing small-sized objects while minimizing training costs. Specifically, our method involves two key components: SO-LoRA, which efficiently fine-tunes low-rank matrices, and Cross-Scale Score Distillation loss, which leverages high-resolution predictions from the pre-trained teacher diffusion model. Our method presents significant improvements on the test dataset collected from MSCOCO and OpenImage, validating the effectiveness of our proposed method in small object editing. In particular, when comparing SOEDiff with SD-I model on the OpenImage-f dataset, we observe a 0.99 improvement in CLIP-Score and a reduction of 2.87 in FID. </p>
<blockquote>
<p>æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†åä¸ºå°å¯¹è±¡ç¼–è¾‘ï¼ˆSOEï¼‰çš„æ–°ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡ä¸“æ³¨äºåœ¨å—é™çš„å°åŒºåŸŸå†…è¿›è¡ŒåŸºäºæ–‡æœ¬çš„å›¾åƒå¡«å……ã€‚å°½ç®¡å½“å‰çš„å›¾åƒå¡«å……æ–¹æ³•åœ¨æŠ€æœ¯ä¸Šå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å®ƒä»¬åº”ç”¨äºSOEä»»åŠ¡é€šå¸¸ä¼šå¯¼è‡´å¯¹è±¡ç¼ºå¤±ã€æ–‡æœ¬ä¸å›¾åƒä¸åŒ¹é…ä»¥åŠå¤±çœŸç­‰å¤±è´¥æ¡ˆä¾‹ã€‚è¿™äº›å¤±è´¥æºäºè®­ç»ƒæ•°æ®é›†ä¸­å°å¯¹è±¡çš„æœ‰é™ä½¿ç”¨ä»¥åŠU-Netæ¨¡å‹çš„ä¸‹é‡‡æ ·æ“ä½œï¼Œè¿™é˜»ç¢äº†å‡†ç¡®çš„ç”Ÿæˆã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºè®­ç»ƒçš„æ–¹æ³•SOEDiffï¼Œæ—¨åœ¨å¢å¼ºåŸºçº¿æ¨¡å‹ï¼ˆå¦‚StableDiffusionï¼‰ç¼–è¾‘å°å¯¹è±¡çš„èƒ½åŠ›ï¼ŒåŒæ—¶é™ä½è®­ç»ƒæˆæœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šSO-LoRAï¼Œå®ƒå¯ä»¥æœ‰æ•ˆåœ°å¾®è°ƒä½é˜¶çŸ©é˜µï¼›ä»¥åŠè·¨å°ºåº¦å¾—åˆ†è’¸é¦æŸå¤±ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒæ•™å¸ˆæ‰©æ•£æ¨¡å‹çš„é«˜åˆ†è¾¨ç‡é¢„æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨MSCOCOå’ŒOpenImageæ”¶é›†çš„æµ‹è¯•æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒéªŒè¯äº†æˆ‘ä»¬åœ¨å°å¯¹è±¡ç¼–è¾‘æ–¹æ³•ä¸­çš„æœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«æ˜¯å½“å°†SOEDiffä¸SD-Iæ¨¡å‹åœ¨OpenImage-fæ•°æ®é›†ä¸Šè¿›è¡Œæ¯”è¾ƒæ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°CLIPå¾—åˆ†æé«˜äº†0.99ï¼ŒFIDé™ä½äº†2.87ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.09114v3">PDF</a> preprint</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹æ–°ä»»åŠ¡â€”â€”å°ç›®æ ‡ç¼–è¾‘ï¼ˆSOEï¼‰ï¼Œä¸“æ³¨äºåœ¨é™å®šçš„å°åŒºåŸŸå†…è¿›è¡Œæ–‡æœ¬åŸºç¡€çš„å›¾åƒè¡¥å…¨ã€‚å½“å‰å›¾åƒè¡¥å…¨æ–¹æ³•åœ¨å°ç›®æ ‡ç¼–è¾‘ä»»åŠ¡ä¸­çš„åº”ç”¨å¸¸å‡ºç°å¤±è´¥æƒ…å†µï¼Œå¦‚ç›®æ ‡ç¼ºå¤±ã€æ–‡æœ¬ä¸å›¾åƒä¸åŒ¹é…å’Œå¤±çœŸã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè®­ç»ƒçš„æ–¹æ³•SOEDiffï¼Œæ—¨åœ¨æé«˜åŸºçº¿æ¨¡å‹å¦‚StableDiffusionç¼–è¾‘å°ç›®æ ‡çš„èƒ½åŠ›ï¼ŒåŒæ—¶é™ä½è®­ç»ƒæˆæœ¬ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šSO-LoRAï¼Œç”¨äºæœ‰æ•ˆå¾®è°ƒä½ç§©çŸ©é˜µï¼›Cross-Scale Score DistillationæŸå¤±ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ•™å¸ˆæ‰©æ•£æ¨¡å‹çš„é«˜åˆ†è¾¨ç‡é¢„æµ‹ã€‚åœ¨MSCOCOå’ŒOpenImageæ”¶é›†çš„æµ‹è¯•æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨å°ç›®æ ‡ç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ–‡ä¸­ä»‹ç»äº†ä¸€ç§æ–°çš„ä»»åŠ¡â€”â€”å°ç›®æ ‡ç¼–è¾‘ï¼ˆSOEï¼‰ï¼Œä¸“æ³¨äºå°åŒºåŸŸçš„æ–‡æœ¬å›¾åƒè¡¥å…¨ã€‚</li>
<li>å½“å‰å›¾åƒè¡¥å…¨æ–¹æ³•åœ¨å°ç›®æ ‡ç¼–è¾‘ä»»åŠ¡ä¸­æ˜“å‡ºç°ç›®æ ‡ç¼ºå¤±ã€æ–‡æœ¬ä¸å›¾åƒä¸åŒ¹é…å’Œå¤±çœŸç­‰é—®é¢˜ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†åŸºäºè®­ç»ƒçš„æ–¹æ³•SOEDiffï¼Œæ—¨åœ¨æé«˜æ¨¡å‹å¯¹å°ç›®æ ‡ç¼–è¾‘çš„èƒ½åŠ›å¹¶é™ä½è®­ç»ƒæˆæœ¬ã€‚</li>
<li>SOEDiffåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šSO-LoRAå’ŒCross-Scale Score DistillationæŸå¤±ã€‚</li>
<li>SO-LoRAç”¨äºæœ‰æ•ˆå¾®è°ƒä½ç§©çŸ©é˜µã€‚</li>
<li>Cross-Scale Score DistillationæŸå¤±åˆ©ç”¨é¢„è®­ç»ƒæ•™å¸ˆæ‰©æ•£æ¨¡å‹çš„é«˜åˆ†è¾¨ç‡é¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.09114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e16434807f40c1c6eb2b16a11112fba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffd5d654a9354c25db955c49bf57585e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3a188a53dcd29eaf70af17174effe08.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GeoDiffuser-Geometry-Based-Image-Editing-with-Diffusion-Models"><a href="#GeoDiffuser-Geometry-Based-Image-Editing-with-Diffusion-Models" class="headerlink" title="GeoDiffuser: Geometry-Based Image Editing with Diffusion Models"></a>GeoDiffuser: Geometry-Based Image Editing with Diffusion Models</h2><p><strong>Authors:Rahul Sajnani, Jeroen Vanbaar, Jie Min, Kapil Katyal, Srinath Sridhar</strong></p>
<p>The success of image generative models has enabled us to build methods that can edit images based on text or other user input. However, these methods are bespoke, imprecise, require additional information, or are limited to only 2D image edits. We present GeoDiffuser, a zero-shot optimization-based method that unifies common 2D and 3D image-based object editing capabilities into a single method. Our key insight is to view image editing operations as geometric transformations. We show that these transformations can be directly incorporated into the attention layers in diffusion models to implicitly perform editing operations. Our training-free optimization method uses an objective function that seeks to preserve object style but generate plausible images, for instance with accurate lighting and shadows. It also inpaints disoccluded parts of the image where the object was originally located. Given a natural image and user input, we segment the foreground object using SAM and estimate a corresponding transform which is used by our optimization approach for editing. GeoDiffuser can perform common 2D and 3D edits like object translation, 3D rotation, and removal. We present quantitative results, including a perceptual study, that shows how our approach is better than existing methods. Visit <a target="_blank" rel="noopener" href="https://ivl.cs.brown.edu/research/geodiffuser.html">https://ivl.cs.brown.edu/research/geodiffuser.html</a> for more information. </p>
<blockquote>
<p>å›¾åƒç”Ÿæˆæ¨¡å‹çš„æˆåŠŸä½¿æˆ‘ä»¬èƒ½å¤Ÿå»ºç«‹èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æˆ–å…¶ä»–ç”¨æˆ·è¾“å…¥ç¼–è¾‘å›¾åƒçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éƒ½æ˜¯å®šåˆ¶çš„ï¼Œä¸ç²¾ç¡®ï¼Œéœ€è¦é¢å¤–ä¿¡æ¯ï¼Œæˆ–è€…ä»…é™äº2Då›¾åƒç¼–è¾‘ã€‚æˆ‘ä»¬æå‡ºäº†GeoDiffuserï¼Œè¿™æ˜¯ä¸€ç§åŸºäºé›¶æ ·æœ¬ä¼˜åŒ–çš„æ–¹æ³•ï¼Œå®ƒå°†å¸¸è§çš„2Då’Œ3Då›¾åƒåŸºäºå¯¹è±¡çš„ç¼–è¾‘åŠŸèƒ½é›†æˆåˆ°ä¸€ç§æ–¹æ³•ä¸­ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯å°†å›¾åƒç¼–è¾‘æ“ä½œè§†ä¸ºå‡ ä½•å˜æ¢ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™äº›å˜æ¢å¯ä»¥ç›´æ¥é›†æˆåˆ°æ‰©æ•£æ¨¡å‹çš„æ³¨æ„åŠ›å±‚ä¸­ï¼Œä»¥éšå¼æ‰§è¡Œç¼–è¾‘æ“ä½œã€‚æˆ‘ä»¬çš„æ— è®­ç»ƒä¼˜åŒ–æ–¹æ³•ä½¿ç”¨ç›®æ ‡å‡½æ•°ï¼Œè¯¥å‡½æ•°æ—¨åœ¨ä¿æŒå¯¹è±¡é£æ ¼ï¼Œä½†ç”Ÿæˆé€¼çœŸçš„å›¾åƒï¼Œä¾‹å¦‚å…·æœ‰å‡†ç¡®çš„ç…§æ˜å’Œé˜´å½±ã€‚å®ƒè¿˜ä¼šå¯¹åŸå§‹å¯¹è±¡æ‰€åœ¨ä½ç½®è¢«é®æŒ¡çš„å›¾åƒéƒ¨åˆ†è¿›è¡Œå¡«å……ã€‚ç»™å®šè‡ªç„¶å›¾åƒå’Œç”¨æˆ·è¾“å…¥ï¼Œæˆ‘ä»¬ä½¿ç”¨SAMå¯¹å‰æ™¯å¯¹è±¡è¿›è¡Œåˆ†å‰²ï¼Œå¹¶ä¼°ç®—ç›¸åº”çš„å˜æ¢ï¼Œç„¶åæˆ‘ä»¬çš„ä¼˜åŒ–æ–¹æ³•ä½¿ç”¨è¯¥å˜æ¢è¿›è¡Œç¼–è¾‘ã€‚GeoDiffuserå¯ä»¥æ‰§è¡Œå¸¸è§çš„2Då’Œ3Dç¼–è¾‘ï¼Œå¦‚å¯¹è±¡å¹³ç§»ã€3Dæ—‹è½¬å’Œç§»é™¤ã€‚æˆ‘ä»¬æä¾›äº†å®šé‡ç»“æœï¼ŒåŒ…æ‹¬ä¸€é¡¹æ„ŸçŸ¥ç ”ç©¶ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•æ¯”ç°æœ‰æ–¹æ³•æ›´å¥½ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—®<a target="_blank" rel="noopener" href="https://ivl.cs.brown.edu/research/geodiffuser.html%E3%80%82">https://ivl.cs.brown.edu/research/geodiffuser.htmlã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.14403v2">PDF</a> Accepted to WACV 2025, Tucson, Arizona, USA. For project page, see   <a target="_blank" rel="noopener" href="https://ivl.cs.brown.edu/research/geodiffuser.html">https://ivl.cs.brown.edu/research/geodiffuser.html</a></p>
<p><strong>Summary</strong><br>     åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¼–è¾‘æ–¹æ³•GeoDiffuserèåˆäº†å¸¸è§çš„äºŒç»´å’Œä¸‰ç»´å›¾åƒç¼–è¾‘åŠŸèƒ½ï¼Œå°†å›¾åƒç¼–è¾‘æ“ä½œè§†ä¸ºå‡ ä½•å˜æ¢å¹¶ç›´æ¥èå…¥æ‰©æ•£æ¨¡å‹çš„æ³¨æ„åŠ›å±‚ã€‚è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œé€šè¿‡ä¼˜åŒ–ç›®æ ‡å‡½æ•°ä¿ç•™ç‰©ä½“é£æ ¼åŒæ—¶ç”Ÿæˆå¯ä¿¡å›¾åƒï¼Œå¯å®Œæˆç‰©ä½“ç§»åŠ¨ã€ä¸‰ç»´æ—‹è½¬å’Œç§»é™¤ç­‰å¸¸è§äºŒç»´å’Œä¸‰ç»´ç¼–è¾‘ä»»åŠ¡ã€‚è¯¦æƒ…è®¿é—®<a target="_blank" rel="noopener" href="https://ivl.cs.brown.edu/research/geodiffuser.html">é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GeoDiffuseræ˜¯ä¸€ä¸ªé›¶æ ·æœ¬ä¼˜åŒ–æ–¹æ³•ï¼Œå®ç°äº†ç»Ÿä¸€çš„äºŒç»´å’Œä¸‰ç»´å›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚</li>
<li>å®ƒå°†å›¾åƒç¼–è¾‘æ“ä½œè§†ä¸ºå‡ ä½•å˜æ¢å¹¶èå…¥æ‰©æ•£æ¨¡å‹çš„æ³¨æ„åŠ›å±‚ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œä½¿ç”¨ç›®æ ‡å‡½æ•°ä¼˜åŒ–æ¥ä¿ç•™ç‰©ä½“é£æ ¼å¹¶ç”Ÿæˆå¯ä¿¡å›¾åƒã€‚</li>
<li>GeoDiffuserå¯ä»¥æ‰§è¡Œå¸¸è§çš„äºŒç»´å’Œä¸‰ç»´ç¼–è¾‘ä»»åŠ¡ï¼Œå¦‚ç‰©ä½“ç§»åŠ¨ã€ä¸‰ç»´æ—‹è½¬å’Œç§»é™¤ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡åˆ†æ®µå‰æ™¯ç‰©ä½“å’Œç”¨æˆ·è¾“å…¥æ¥è¿›è¡Œç¼–è¾‘æ“ä½œã€‚</li>
<li>å®šé‡ç»“æœå’Œæ„ŸçŸ¥ç ”ç©¶è¡¨æ˜ï¼ŒGeoDiffuserä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.14403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28065d195a75265377bded13001b3c98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de8cbc99cab5d8512d9987be09aa44d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42f14c04f2d4e13c229c3f9f4c9b51e9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AGFSync-Leveraging-AI-Generated-Feedback-for-Preference-Optimization-in-Text-to-Image-Generation"><a href="#AGFSync-Leveraging-AI-Generated-Feedback-for-Preference-Optimization-in-Text-to-Image-Generation" class="headerlink" title="AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in   Text-to-Image Generation"></a>AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in   Text-to-Image Generation</h2><p><strong>Authors:Jingkun An, Yinghao Zhu, Zongjian Li, Enshen Zhou, Haoran Feng, Xijie Huang, Bohua Chen, Yemin Shi, Chengwei Pan</strong></p>
<p>Text-to-Image (T2I) diffusion models have achieved remarkable success in image generation. Despite their progress, challenges remain in both prompt-following ability, image quality and lack of high-quality datasets, which are essential for refining these models. As acquiring labeled data is costly, we introduce AGFSync, a framework that enhances T2I diffusion models through Direct Preference Optimization (DPO) in a fully AI-driven approach. AGFSync utilizes Vision-Language Models (VLM) to assess image quality across style, coherence, and aesthetics, generating feedback data within an AI-driven loop. By applying AGFSync to leading T2I models such as SD v1.4, v1.5, and SDXL-base, our extensive experiments on the TIFA dataset demonstrate notable improvements in VQA scores, aesthetic evaluations, and performance on the HPSv2 benchmark, consistently outperforming the base models. AGFSyncâ€™s method of refining T2I diffusion models paves the way for scalable alignment techniques. Our code and dataset are publicly available at <a target="_blank" rel="noopener" href="https://anjingkun.github.io/AGFSync">https://anjingkun.github.io/AGFSync</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚å°½ç®¡æœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€å›¾åƒè´¨é‡å’Œç¼ºä¹é«˜è´¨é‡æ•°æ®é›†ç­‰æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œè¿™äº›å¯¹äºå®Œå–„è¿™äº›æ¨¡å‹è‡³å…³é‡è¦ã€‚ç”±äºè·å–æ ‡è®°æ•°æ®æˆæœ¬é«˜æ˜‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†AGFSyncæ¡†æ¶ï¼Œé€šè¿‡å…¨AIé©±åŠ¨çš„æ–¹æ³•ç›´æ¥ä¼˜åŒ–åå¥½ï¼ˆDPOï¼‰å¢å¼ºT2Iæ‰©æ•£æ¨¡å‹ã€‚AGFSyncåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä»é£æ ¼ã€è¿è´¯æ€§å’Œç¾å­¦ä¸‰ä¸ªæ–¹é¢è¯„ä¼°å›¾åƒè´¨é‡ï¼Œåœ¨AIé©±åŠ¨çš„å¾ªç¯ä¸­ç”Ÿæˆåé¦ˆæ•°æ®ã€‚é€šè¿‡å°†AGFSyncåº”ç”¨äºé¢†å…ˆçš„T2Iæ¨¡å‹ï¼Œå¦‚SD v1.4ã€v1.5å’ŒSDXL-baseï¼Œæˆ‘ä»¬åœ¨TIFAæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVQAåˆ†æ•°ã€ç¾å­¦è¯„ä»·å’ŒHPSv2åŸºå‡†æµ‹è¯•æ€§èƒ½å‡æœ‰æ˜¾è‘—æé«˜ï¼Œå§‹ç»ˆä¼˜äºåŸºç¡€æ¨¡å‹ã€‚AGFSyncå¯¹T2Iæ‰©æ•£æ¨¡å‹çš„ç²¾ç‚¼æ–¹æ³•ä¸ºå¯æ‰©å±•çš„å¯¹é½æŠ€æœ¯é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://anjingkun.github.io/AGFSync%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://anjingkun.github.io/AGFSyncå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.13352v6">PDF</a> Accepted by AAAI-2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†T2Iæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„æ˜¾è‘—æˆæœï¼Œä½†ä¹ŸæŒ‡å‡ºäº†å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œå¦‚æç¤ºéµå¾ªèƒ½åŠ›ã€å›¾åƒè´¨é‡å’Œç¼ºä¹é«˜è´¨é‡æ•°æ®é›†ç­‰ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºAGFSyncçš„æ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä»¥å…¨AIé©±åŠ¨çš„æ–¹å¼å¢å¼ºT2Iæ‰©æ•£æ¨¡å‹ã€‚AGFSyncåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¯„ä¼°å›¾åƒè´¨é‡ï¼ŒåŒ…æ‹¬é£æ ¼ã€è¿è´¯æ€§å’Œç¾å­¦æ–¹é¢ï¼Œå¹¶åœ¨AIé©±åŠ¨çš„å¾ªç¯ä¸­ç”Ÿæˆåé¦ˆæ•°æ®ã€‚åœ¨TIFAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAGFSyncåœ¨VQAåˆ†æ•°ã€ç¾å­¦è¯„ä¼°ä»¥åŠHPSv2åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡æœ‰æ˜¾è‘—æé«˜ï¼Œä¸”å§‹ç»ˆä¼˜äºåŸºç¡€æ¨¡å‹ã€‚AGFSyncçš„æ–¹æ³•ä¸ºT2Iæ‰©æ•£æ¨¡å‹çš„ç²¾ç‚¼æä¾›äº†å¯æ‰©å±•çš„å¯¹é½æŠ€æœ¯é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Iæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—æˆåŠŸï¼Œä½†ä»é¢ä¸´æç¤ºéµå¾ªèƒ½åŠ›ã€å›¾åƒè´¨é‡å’Œæ•°æ®é›†è´¨é‡æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>AGFSyncæ¡†æ¶é€šè¿‡å…¨AIé©±åŠ¨çš„æ–¹å¼å¢å¼ºT2Iæ‰©æ•£æ¨¡å‹ï¼Œå¼•å…¥ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€‚</li>
<li>AGFSyncåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¯„ä¼°å›¾åƒè´¨é‡ï¼ŒåŒ…æ‹¬é£æ ¼ã€è¿è´¯æ€§å’Œç¾å­¦ã€‚</li>
<li>AGFSyncåœ¨TIFAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ï¼Œæé«˜äº†VQAåˆ†æ•°ã€ç¾å­¦è¯„ä¼°ä»¥åŠHPSv2åŸºå‡†æµ‹è¯•æˆç»©ã€‚</li>
<li>AGFSyncæ˜¾è‘—ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œä¸ºT2Iæ‰©æ•£æ¨¡å‹çš„ç²¾ç‚¼æä¾›äº†å¯æ‰©å±•çš„å¯¹é½æŠ€æœ¯é€”å¾„ã€‚</li>
<li>AGFSyncæ–¹æ³•å’Œæ•°æ®é›†å·²å…¬å¼€å¯ç”¨ï¼Œç½‘å€ä¸º<a target="_blank" rel="noopener" href="https://anjingkun.github.io/AGFSync%E3%80%82">https://anjingkun.github.io/AGFSyncã€‚</a></li>
<li>å¼•å…¥AGFSyncæ¡†æ¶ä¸ºè§£å†³æ‰©æ•£æ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.13352">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d10213fe8dd466bb674d502d8f6f3d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-817b8963937d1ff0a7497444285f4e0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3f402bcde3ec66cdf01e0e0558d12a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cce6dd7a4c345a26b15ffbae5e0d82dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a81f907708b94fdb89a5867f8fd7f84e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2981af3a0133ae50f57b9b6e92ce04ab.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Neural-Network-Diffusion"><a href="#Neural-Network-Diffusion" class="headerlink" title="Neural Network Diffusion"></a>Neural Network Diffusion</h2><p><strong>Authors:Kai Wang, Dongwen Tang, Boya Zeng, Yida Yin, Zhaopan Xu, Yukun Zhou, Zelin Zang, Trevor Darrell, Zhuang Liu, Yang You</strong></p>
<p>Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a diffusion model. The autoencoder extracts latent representations of a subset of the trained neural network parameters. Next, a diffusion model is trained to synthesize these latent representations from random noise. This model then generates new representations, which are passed through the autoencoderâ€™s decoder to produce new subsets of high-performing network parameters. Across various architectures and datasets, our approach consistently generates models with comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models are not memorizing the trained ones. Our results encourage more exploration into the versatile use of diffusion models. Our code is available \href{<a target="_blank" rel="noopener" href="https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion%7D%7Bhere%7D">https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion}{here}</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯æ˜æ‰©æ•£æ¨¡å‹ä¹Ÿèƒ½<strong>ç”Ÿæˆé«˜æ€§èƒ½çš„ç¥ç»ç½‘ç»œå‚æ•°</strong>ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¾ˆç®€å•ï¼Œåˆ©ç”¨è‡ªç¼–ç å™¨å’Œæ‰©æ•£æ¨¡å‹ã€‚è‡ªç¼–ç å™¨æå–è®­ç»ƒç¥ç»ç½‘ç»œå‚æ•°å­é›†ä¸­çš„æ½œåœ¨è¡¨ç¤ºã€‚æ¥ä¸‹æ¥ï¼Œè®­ç»ƒæ‰©æ•£æ¨¡å‹ä»éšæœºå™ªå£°ä¸­åˆæˆè¿™äº›æ½œåœ¨è¡¨ç¤ºã€‚è¯¥æ¨¡å‹ç„¶åç”Ÿæˆæ–°çš„è¡¨ç¤ºï¼Œé€šè¿‡è‡ªç¼–ç å™¨çš„è§£ç å™¨äº§ç”Ÿæ–°çš„é«˜æ€§èƒ½ç½‘ç»œå‚æ•°å­é›†ã€‚åœ¨å„ç§æ¶æ„å’Œæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆèƒ½ç”Ÿæˆä¸è®­ç»ƒç½‘ç»œæ€§èƒ½ç›¸å½“æˆ–æ›´å¥½çš„æ¨¡å‹ï¼Œä¸”åªéœ€æå°‘çš„é¢å¤–æˆæœ¬ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä»å®éªŒä¸­å‘ç°ï¼Œç”Ÿæˆçš„æ¨¡å‹å¹¶æ²¡æœ‰è®°å¿†è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬çš„ç»“æœé¼“åŠ±æ›´å¤šæ¢ç´¢æ‰©æ•£æ¨¡å‹çš„é€šç”¨ç”¨é€”ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion]%EF%BC%88%E6%AD%A4%E5%A4%84%EF%BC%89%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion]ï¼ˆæ­¤å¤„ï¼‰æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.13144v3">PDF</a> We introduce a novel approach for parameter generation, named neural   network parameter diffusion (\textbf{p-diff}), which employs a standard   latent diffusion model to synthesize a new set of parameters</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å±•ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜æ€§èƒ½ç¥ç»ç½‘ç»œå‚æ•°æ–¹é¢çš„æ½œåŠ›ã€‚é€šè¿‡ç»“åˆè‡ªç¼–ç å™¨ä¸æ‰©æ•£æ¨¡å‹ï¼Œè¯¥ç ”ç©¶é¦–å…ˆæå–è®­ç»ƒç¥ç»ç½‘ç»œå‚æ•°çš„æ½œåœ¨è¡¨ç¤ºï¼Œç„¶åç”¨æ‰©æ•£æ¨¡å‹ä»è¿™äº›æ½œåœ¨è¡¨ç¤ºä¸­åˆæˆæ–°çš„ç½‘ç»œå‚æ•°ã€‚è¿™ç§æ–¹æ³•ç”Ÿæˆçš„æ–°æ¨¡å‹æ€§èƒ½ä¸è®­ç»ƒç½‘ç»œç›¸å½“æˆ–æ›´ä¼˜ï¼Œä¸”é¢å¤–æˆæœ¬è¾ƒä½ã€‚ç ”ç©¶è¿˜è¡¨æ˜ç”Ÿæˆçš„æ¨¡å‹ä¸ä¼šè®°å¿†è®­ç»ƒæ¨¡å‹ã€‚è¯¥ç ”ç©¶çš„ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¯ç”¨äºç”Ÿæˆé«˜æ€§èƒ½ç¥ç»ç½‘ç»œå‚æ•°ã€‚</li>
<li>ç»“åˆè‡ªç¼–ç å™¨å’Œæ‰©æ•£æ¨¡å‹ï¼Œæå–å¹¶åˆæˆç¥ç»ç½‘ç»œå‚æ•°çš„æ½œåœ¨è¡¨ç¤ºã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ä¸è®­ç»ƒç½‘ç»œç›¸å½“æˆ–æ›´ä¼˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„é¢å¤–æˆæœ¬è¾ƒä½ã€‚</li>
<li>ç”Ÿæˆçš„æ¨¡å‹ä¸ä¼šè®°å¿†è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ‰©æ•£æ¨¡å‹çš„å¤šå…ƒåº”ç”¨æä¾›äº†å¯ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.13144">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5149eb4d7189d7749e4f959a15e478bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0157caf6fbec45bc2432363bb6ab60b1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-04\./crop_Diffusion Models/2402.13144v3/page_3_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08f47bc877e085936e0eb5241826ca71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-366d56bd243d64a0a2a484ed1d5a61c5.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Text2Data-Low-Resource-Data-Generation-with-Textual-Control"><a href="#Text2Data-Low-Resource-Data-Generation-with-Textual-Control" class="headerlink" title="Text2Data: Low-Resource Data Generation with Textual Control"></a>Text2Data: Low-Resource Data Generation with Textual Control</h2><p><strong>Authors:Shiyu Wang, Yihao Feng, Tian Lan, Ning Yu, Yu Bai, Ran Xu, Huan Wang, Caiming Xiong, Silvio Savarese</strong></p>
<p>Natural language serves as a common and straightforward signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, such as molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model. Subsequently, it undergoes controllable finetuning via a novel constraint optimization-based learning objective that ensures controllability and effectively counteracts catastrophic forgetting. Comprehensive experiments demonstrate that Text2Data is able to achieve enhanced performance regarding controllability across various modalities, including molecules, motions and time series, when compared to existing baselines. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€æ˜¯äººç±»ä¸æœºå™¨æ— ç¼äº¤äº’çš„å¸¸è§ä¸”ç›´è§‚ä¿¡å·ã€‚è®¤è¯†åˆ°è¿™ä¸€æ¥å£çš„é‡è¦æ€§ï¼Œæœºå™¨å­¦ä¹ ç¤¾åŒºæ­£åœ¨æŠ•å…¥å¤§é‡ç²¾åŠ›ç”Ÿæˆä¸æ–‡æœ¬æŒ‡ä»¤è¯­ä¹‰è¿è´¯çš„æ•°æ®ã€‚è™½ç„¶åœ¨å›¾åƒç¼–è¾‘ã€éŸ³é¢‘åˆæˆã€è§†é¢‘åˆ›å»ºç­‰æ–‡æœ¬åˆ°æ•°æ®ç”Ÿæˆæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†æ ‡æ³¨æ˜‚è´µæˆ–æ•°æ®ç»“æ„å¤æ‚çš„ä½èµ„æºé¢†åŸŸï¼Œå¦‚åˆ†å­ã€è¿åŠ¨åŠ¨åŠ›å­¦å’Œæ—¶åºæ•°æ®ç­‰ï¼Œé€šå¸¸ç¼ºä¹æ–‡æœ¬æ ‡ç­¾ã€‚è¿™ç§ç¼ºä¹é˜»ç¢äº†ç›‘ç£å­¦ä¹ ï¼Œä»è€Œé™åˆ¶äº†é«˜çº§ç”Ÿæˆæ¨¡å‹åœ¨æ–‡æœ¬åˆ°æ•°æ®ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹ä½èµ„æºåœºæ™¯ä¸­çš„è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Text2Dataè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ— æ ‡ç­¾æ•°æ®é€šè¿‡æ— ç›‘ç£æ‰©æ•£æ¨¡å‹ç†è§£åº•å±‚æ•°æ®åˆ†å¸ƒã€‚éšåï¼Œå®ƒé€šè¿‡åŸºäºæ–°å‹çº¦æŸä¼˜åŒ–çš„å­¦ä¹ ç›®æ ‡è¿›è¡Œå¯æ§å¾®è°ƒï¼Œç¡®ä¿å¯æ§æ€§å¹¶æœ‰æ•ˆå¯¹æŠ—ç¾éš¾æ€§é—å¿˜ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰åŸºçº¿ç›¸æ¯”ï¼ŒText2Dataåœ¨å„ç§æ¨¡å¼ï¼ˆåŒ…æ‹¬åˆ†å­ã€è¿åŠ¨å’Œæ—¶åºæ•°æ®ï¼‰ä¸‹åœ¨å¯æ§æ€§æ–¹é¢å–å¾—äº†æé«˜çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.10941v2">PDF</a> Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æŒ‡å‡ºè‡ªç„¶è¯­è¨€æ˜¯äººç±»ä¸æœºå™¨äº¤äº’çš„é€šç”¨å’Œç›´è§‚ä¿¡å·ã€‚æœºå™¨å­¦ä¹ é¢†åŸŸæ­£åœ¨è‡´åŠ›äºç”Ÿæˆä¸æ–‡æœ¬æŒ‡ä»¤è¯­ä¹‰ä¸Šä¸€è‡´çš„æ•°æ®ã€‚å°½ç®¡æ–‡æœ¬åœ¨å›¾åƒç¼–è¾‘ã€éŸ³é¢‘åˆæˆã€è§†é¢‘åˆ›å»ºç­‰æ–¹é¢çš„æ–‡æœ¬åˆ°æ•°æ®ç”Ÿæˆæœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨ä½èµ„æºé¢†åŸŸï¼Œå¦‚åˆ†å­ã€è¿åŠ¨åŠ¨åŠ›å­¦å’Œæ—¶åºç­‰ï¼Œç”±äºç¼ºä¹æ–‡æœ¬æ ‡ç­¾ï¼Œé˜»ç¢äº†ç›‘ç£å­¦ä¹ ï¼Œé™åˆ¶äº†é«˜çº§ç”Ÿæˆæ¨¡å‹åœ¨æ–‡æœ¬åˆ°æ•°æ®ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæå‡ºText2Dataæ–¹æ³•ï¼Œåˆ©ç”¨æ— æ ‡ç­¾æ•°æ®é€šè¿‡æ— ç›‘ç£æ‰©æ•£æ¨¡å‹ç†è§£åº•å±‚æ•°æ®åˆ†å¸ƒï¼Œå¹¶é€šè¿‡æ–°å‹çº¦æŸä¼˜åŒ–å­¦ä¹ ç›®æ ‡å®ç°å¯æ§å¾®è°ƒã€‚å®éªŒè¯æ˜ï¼ŒText2Dataåœ¨åˆ†å­ã€è¿åŠ¨å’Œæ—¶åºç­‰è·¨æ¨¡æ€é¢†åŸŸå®ç°è‰¯å¥½å¯æ§æ€§è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç„¶è¯­è¨€æ˜¯äººç±»ä¸æœºå™¨äº¤äº’çš„é‡è¦æ¥å£ï¼Œæœºå™¨å­¦ä¹ é¢†åŸŸæ­£åŠªåŠ›ç”Ÿæˆä¸æ–‡æœ¬æŒ‡ä»¤è¯­ä¹‰ä¸€è‡´çš„æ•°æ®ã€‚</li>
<li>æ–‡æœ¬åˆ°æ•°æ®ç”Ÿæˆå·²åœ¨å¤šä¸ªé¢†åŸŸå–å¾—è¿›å±•ï¼Œä½†ä½èµ„æºé¢†åŸŸç”±äºç¼ºä¹æ–‡æœ¬æ ‡ç­¾è€Œé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>Text2Dataæ–¹æ³•åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®é€šè¿‡æ— ç›‘ç£æ‰©æ•£æ¨¡å‹ç†è§£åº•å±‚æ•°æ®åˆ†å¸ƒã€‚</li>
<li>Text2Dataé€šè¿‡å¯æ§å¾®è°ƒå®ç°æ¨¡å‹çš„å¯æ§æ€§ï¼Œæœ‰æ•ˆå¯¹æŠ—ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>Text2Dataåœ¨åˆ†å­ã€è¿åŠ¨å’Œæ—¶åºç­‰è·¨æ¨¡æ€é¢†åŸŸçš„è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
<li>çº¦æŸä¼˜åŒ–å­¦ä¹ ç›®æ ‡æ˜¯å®ç°Text2Dataå¯æ§æ€§çš„å…³é”®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.10941">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c00ae61195219a14226b44fc22324526.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77c977e175e66613ee9159b6f1d29258.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47abe9c786413ca6905da56eb2bb992a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Realistic-Noise-Synthesis-with-Diffusion-Models"><a href="#Realistic-Noise-Synthesis-with-Diffusion-Models" class="headerlink" title="Realistic Noise Synthesis with Diffusion Models"></a>Realistic Noise Synthesis with Diffusion Models</h2><p><strong>Authors:Qi Wu, Mingyan Han, Ting Jiang, Chengzhi Jiang, Jinting Luo, Man Jiang, Haoqiang Fan, Shuaicheng Liu</strong></p>
<p>Deep denoising models require extensive real-world training data, which is challenging to acquire. Current noise synthesis techniques struggle to accurately model complex noise distributions. We propose a novel Realistic Noise Synthesis Diffusor (RNSD) method using diffusion models to address these challenges. By encoding camera settings into a time-aware camera-conditioned affine modulation (TCCAM), RNSD generates more realistic noise distributions under various camera conditions. Additionally, RNSD integrates a multi-scale content-aware module (MCAM), enabling the generation of structured noise with spatial correlations across multiple frequencies. We also introduce Deep Image Prior Sampling (DIPS), a learnable sampling sequence based on depth image prior, which significantly accelerates the sampling process while maintaining the high quality of synthesized noise. Extensive experiments demonstrate that our RNSD method significantly outperforms existing techniques in synthesizing realistic noise under multiple metrics and improving image denoising performance. </p>
<blockquote>
<p>æ·±åº¦å»å™ªæ¨¡å‹éœ€è¦å¤§é‡çš„çœŸå®ä¸–ç•Œè®­ç»ƒæ•°æ®ï¼Œè¿™å¾ˆéš¾è·å–ã€‚å½“å‰çš„å™ªå£°åˆæˆæŠ€æœ¯åœ¨å‡†ç¡®æ¨¡æ‹Ÿå¤æ‚çš„å™ªå£°åˆ†å¸ƒæ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æ–°å‹Realistic Noise Synthesis Diffusorï¼ˆRNSDï¼‰æ–¹æ³•ï¼Œä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚RNSDé€šè¿‡å°†ç›¸æœºè®¾ç½®ç¼–ç ä¸ºæ—¶é—´æ„ŸçŸ¥ç›¸æœºæ¡ä»¶ä»¿å°„è°ƒåˆ¶ï¼ˆTCCAMï¼‰ï¼Œåœ¨å„ç§ç›¸æœºæ¡ä»¶ä¸‹ç”Ÿæˆæ›´çœŸå®çš„å™ªå£°åˆ†å¸ƒã€‚æ­¤å¤–ï¼ŒRNSDè¿˜é›†æˆäº†ä¸€ä¸ªå¤šå°ºåº¦å†…å®¹æ„ŸçŸ¥æ¨¡å—ï¼ˆMCAMï¼‰ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰å¤šä¸ªé¢‘ç‡ä¹‹é—´ç©ºé—´ç›¸å…³æ€§çš„ç»“æ„åŒ–å™ªå£°ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†åŸºäºæ·±åº¦å›¾åƒå…ˆéªŒçš„Deep Image Prior Samplingï¼ˆDIPSï¼‰å¯å­¦ä¹ é‡‡æ ·åºåˆ—ï¼Œå®ƒåœ¨ä¿æŒåˆæˆå™ªå£°é«˜è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—åŠ é€Ÿäº†é‡‡æ ·è¿‡ç¨‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„RNSDæ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸‹åˆæˆç°å®å™ªå£°çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶æé«˜äº†å›¾åƒå»å™ªæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.14022v4">PDF</a> Accepted by AAAI25</p>
<p><strong>Summary</strong></p>
<p>åˆ©ç”¨æ‰©æ•£æ¨¡å‹æå‡ºä¸€ç§æ–°çš„çœŸå®å™ªå£°åˆæˆæ‰©æ•£å™¨ï¼ˆRNSDï¼‰æ–¹æ³•ï¼Œè§£å†³æ·±åº¦é™å™ªæ¨¡å‹é¢ä¸´çš„å®é™…æŒ‘æˆ˜ã€‚RNSDç»“åˆæ—¶é—´æ„ŸçŸ¥ç›¸æœºæ¡ä»¶ä»¿å°„è°ƒåˆ¶ï¼ˆTCCAMï¼‰å’Œå¤šå°ºåº¦å†…å®¹æ„ŸçŸ¥æ¨¡å—ï¼ˆMCAMï¼‰ï¼Œç”Ÿæˆæ›´ç¬¦åˆå„ç§ç›¸æœºæ¡ä»¶ä¸‹çš„çœŸå®å™ªå£°åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œå¼•å…¥æ·±åº¦å›¾åƒå…ˆéªŒé‡‡æ ·ï¼ˆDIPSï¼‰å¯åŠ é€Ÿé‡‡æ ·è¿‡ç¨‹å¹¶ä¿æŒåˆæˆå™ªå£°çš„é«˜è´¨é‡ã€‚å®éªŒè¯æ˜ï¼ŒRNSDæ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæé«˜äº†å›¾åƒå»å™ªæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºçš„Realistic Noise Synthesis Diffusor (RNSD)æ–¹æ³•ä½¿ç”¨æ‰©æ•£æ¨¡å‹è§£å†³æ·±åº¦é™å™ªæ¨¡å‹çš„æŒ‘æˆ˜ã€‚</li>
<li>RNSDç»“åˆæ—¶é—´æ„ŸçŸ¥ç›¸æœºæ¡ä»¶ä»¿å°„è°ƒåˆ¶ï¼ˆTCCAMï¼‰ï¼Œç”Ÿæˆæ›´ç¬¦åˆç›¸æœºæ¡ä»¶ä¸‹çš„çœŸå®å™ªå£°åˆ†å¸ƒã€‚</li>
<li>å¤šå°ºåº¦å†…å®¹æ„ŸçŸ¥æ¨¡å—ï¼ˆMCAMï¼‰ä½¿RNSDèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ç©ºé—´ç›¸å…³æ€§çš„ç»“æ„å™ªå£°ã€‚</li>
<li>å¼•å…¥Deep Image Prior Sampling (DIPS)ä»¥æé«˜é‡‡æ ·é€Ÿåº¦å¹¶ä¿æŒå™ªå£°åˆæˆçš„é«˜è´¨é‡ã€‚</li>
<li>RNSDåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰å™ªå£°åˆæˆæŠ€æœ¯ã€‚</li>
<li>RNSDæ–¹æ³•èƒ½æé«˜å›¾åƒå»å™ªæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2305.14022">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d672e238c1a03deb47fedc1bfe354b39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-939f178456b6300dceb43cae03d6014a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf8b6257d0574f9d5837427a735c7190.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa4eb02481a97445bc887965555f611f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e22d6eb13d43345550c198c9657464e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad608a38a4894bb647a8fd6cedafdf9a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-04/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-04/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-010366cd48b4598369b9130678ecc0b8.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-04  SegKAN High-Resolution Medical Image Segmentation with Long-Distance   Dependencies
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-04/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ef101938f9b717618e7f6fcec2d2ad11.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-04  Double-Flow GAN model for the reconstruction of perceived faces from   brain activities
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">12939.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
