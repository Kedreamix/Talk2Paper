<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-01-04  Prometheus 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D   Scene Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-d672e238c1a03deb47fedc1bfe354b39.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    47 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-04-更新"><a href="#2025-01-04-更新" class="headerlink" title="2025-01-04 更新"></a>2025-01-04 更新</h1><h2 id="Prometheus-3D-Aware-Latent-Diffusion-Models-for-Feed-Forward-Text-to-3D-Scene-Generation"><a href="#Prometheus-3D-Aware-Latent-Diffusion-Models-for-Feed-Forward-Text-to-3D-Scene-Generation" class="headerlink" title="Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D   Scene Generation"></a>Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D   Scene Generation</h2><p><strong>Authors:Yuanbo Yang, Jiahao Shao, Xinyang Li, Yujun Shen, Andreas Geiger, Yiyi Liao</strong></p>
<p>In this work, we introduce Prometheus, a 3D-aware latent diffusion model for text-to-3D generation at both object and scene levels in seconds. We formulate 3D scene generation as multi-view, feed-forward, pixel-aligned 3D Gaussian generation within the latent diffusion paradigm. To ensure generalizability, we build our model upon pre-trained text-to-image generation model with only minimal adjustments, and further train it using a large number of images from both single-view and multi-view datasets. Furthermore, we introduce an RGB-D latent space into 3D Gaussian generation to disentangle appearance and geometry information, enabling efficient feed-forward generation of 3D Gaussians with better fidelity and geometry. Extensive experimental results demonstrate the effectiveness of our method in both feed-forward 3D Gaussian reconstruction and text-to-3D generation. Project page: <a target="_blank" rel="noopener" href="https://freemty.github.io/project-prometheus/">https://freemty.github.io/project-prometheus/</a> </p>
<blockquote>
<p>在这项工作中，我们引入了Prometheus，这是一个用于文本到三维场景生成的3D感知潜在扩散模型，可以在对象和场景级别快速实现。我们将三维场景的生成作为潜在扩散框架中的多视图前馈像素对齐的3D高斯生成问题。为了确保模型的通用性，我们在预训练的文本到图像生成模型的基础上构建模型，只需进行微调即可进一步训练，并使用大量的单视图和多视图数据集进行训练。此外，我们在三维高斯生成中引入了RGB-D潜在空间，以分离外观和几何信息，从而实现高效的前馈三维高斯生成，提高保真度和几何精度。大量的实验结果证明了我们的方法在三维高斯重建和文本到三维场景生成方面的有效性。更多内容请参考我们的项目主页：<a target="_blank" rel="noopener" href="https://freemty.github.io/project-prometheus/">https://freemty.github.io/project-prometheus/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21117v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本工作引入Prometheus，这是一款基于文本至场景级别的三维扩散模型，可实现物体与场景的即时三维生成。我们利用预训练文本至图像生成模型作为基础，通过最小调整构建模型，并使用大量单视角和多视角图像数据集进行训练，确保模型具有通用性。引入RGB-D潜在空间到三维高斯生成中，解耦外观和几何信息，实现高效的三维高斯生成。实验结果证明该方法在三维高斯重建和文本至三维生成中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Prometheus是一个文本至场景级别的三维扩散模型。</li>
<li>该模型能够实现物体和场景的即时三维生成。</li>
<li>模型基于预训练的文本至图像生成模型构建，只需进行最小调整。</li>
<li>使用大量单视角和多视角图像数据集进行训练，确保模型的通用性。</li>
<li>模型引入了RGB-D潜在空间以解耦外观和几何信息。</li>
<li>该方法实现了高效的三维高斯生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21117">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f2f81c87ffdc3b279db6e2a62213bf4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2edfb253d40daa0651ed94c7f536f78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f990bf61922d22659ee8687d9e07244.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f31bda12f30ec763942a5455b9eee2bc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="EraseAnything-Enabling-Concept-Erasure-in-Rectified-Flow-Transformers"><a href="#EraseAnything-Enabling-Concept-Erasure-in-Rectified-Flow-Transformers" class="headerlink" title="EraseAnything: Enabling Concept Erasure in Rectified Flow Transformers"></a>EraseAnything: Enabling Concept Erasure in Rectified Flow Transformers</h2><p><strong>Authors:Daiheng Gao, Shilin Lu, Shaw Walters, Wenbo Zhou, Jiaming Chu, Jie Zhang, Bang Zhang, Mengxi Jia, Jian Zhao, Zhaoxin Fan, Weiming Zhang</strong></p>
<p>Removing unwanted concepts from large-scale text-to-image (T2I) diffusion models while maintaining their overall generative quality remains an open challenge. This difficulty is especially pronounced in emerging paradigms, such as Stable Diffusion (SD) v3 and Flux, which incorporate flow matching and transformer-based architectures. These advancements limit the transferability of existing concept-erasure techniques that were originally designed for the previous T2I paradigm (e.g., SD v1.4). In this work, we introduce EraseAnything, the first method specifically developed to address concept erasure within the latest flow-based T2I framework. We formulate concept erasure as a bi-level optimization problem, employing LoRA-based parameter tuning and an attention map regularizer to selectively suppress undesirable activations. Furthermore, we propose a self-contrastive learning strategy to ensure that removing unwanted concepts does not inadvertently harm performance on unrelated ones. Experimental results demonstrate that EraseAnything successfully fills the research gap left by earlier methods in this new T2I paradigm, achieving state-of-the-art performance across a wide range of concept erasure tasks. </p>
<blockquote>
<p>从大规模文本到图像（T2I）的扩散模型中移除不需要的概念并保持其整体的生成质量仍然是一个待解决的难题。这种难度在稳定扩散（SD）v3和Flux等新兴范式中尤为突出，这些新兴范式融入了流匹配和基于变压器的架构。这些进步限制了原本为早期T2I范式（例如SD v1.4）设计的概念删除技术的可转移性。在这项工作中，我们介绍了EraseAnything，这是首个专门开发以解决最新流式T2I框架中概念删除问题的方法。我们将概念删除表述为两级优化问题，采用基于LoRA的参数调整和注意力图正则化来有选择性地抑制不需要的激活。此外，我们提出了一种自我对比学习策略，以确保移除不需要的概念不会无意中损害对无关概念的性能。实验结果表明，EraseAnything成功填补了早期方法在这一新T2I范式中的研究空白，在广泛的概念删除任务中实现了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20413v2">PDF</a> 24 pages, 18 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对最新基于流的文本到图像（T2I）框架中的概念擦除问题，提出了一种新方法EraseAnything。该方法将概念擦除公式化为一个两级优化问题，并采用LoRA参数调整和注意力图正则化来选择性抑制不需要的激活。此外，还提出了一种自我对比学习策略，以确保在去除不需要的概念时不会意外地损害其他相关概念的性能。实验结果表明，EraseAnything在新的T2I范式中填补了早期方法的空白，并在各种概念擦除任务上实现了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EraseAnything是专门为最新基于流的文本到图像（T2I）框架中的概念擦除问题开发的方法。</li>
<li>该方法将概念擦除公式化为一个两级优化问题。</li>
<li>LoRA参数调整和注意力图正则化被用来选择性抑制不需要的激活。</li>
<li>EraseAnything采用自我对比学习策略，确保在去除不需要的概念时不会损害其他概念的性能。</li>
<li>现有的概念擦除技术在新的T2I范式（如Stable Diffusion v3和Flux）中的转移性受到限制。</li>
<li>EraseAnything填补了早期方法在T2I范式中的空白，实现了广泛的概念擦除任务的最先进性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20413">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-faa34e7088a2cc4845386aef1ac866bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf4c66f9cd09149fb6696b693caa5517.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9acf677d168d89c91a7cfb5c1be74884.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="UIBDiffusion-Universal-Imperceptible-Backdoor-Attack-for-Diffusion-Models"><a href="#UIBDiffusion-Universal-Imperceptible-Backdoor-Attack-for-Diffusion-Models" class="headerlink" title="UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion   Models"></a>UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion   Models</h2><p><strong>Authors:Yuning Han, Bingyin Zhao, Rui Chu, Feng Luo, Biplab Sikdar, Yingjie Lao</strong></p>
<p>Recent studies show that diffusion models (DMs) are vulnerable to backdoor attacks. Existing backdoor attacks impose unconcealed triggers (e.g., a gray box and eyeglasses) that contain evident patterns, rendering remarkable attack effects yet easy detection upon human inspection and defensive algorithms. While it is possible to improve stealthiness by reducing the strength of the backdoor, doing so can significantly compromise its generality and effectiveness. In this paper, we propose UIBDiffusion, the universal imperceptible backdoor attack for diffusion models, which allows us to achieve superior attack and generation performance while evading state-of-the-art defenses. We propose a novel trigger generation approach based on universal adversarial perturbations (UAPs) and reveal that such perturbations, which are initially devised for fooling pre-trained discriminative models, can be adapted as potent imperceptible backdoor triggers for DMs. We evaluate UIBDiffusion on multiple types of DMs with different kinds of samplers across various datasets and targets. Experimental results demonstrate that UIBDiffusion brings three advantages: 1) Universality, the imperceptible trigger is universal (i.e., image and model agnostic) where a single trigger is effective to any images and all diffusion models with different samplers; 2) Utility, it achieves comparable generation quality (e.g., FID) and even better attack success rate (i.e., ASR) at low poison rates compared to the prior works; and 3) Undetectability, UIBDiffusion is plausible to human perception and can bypass Elijah and TERD, the SOTA defenses against backdoors for DMs. We will release our backdoor triggers and code. </p>
<blockquote>
<p>近期研究表明，扩散模型（DMs）容易受到后门攻击。现有的后门攻击采用未隐蔽的触发器（例如，灰盒和眼镜），这些触发器包含明显的模式，虽然攻击效果显著，但很容易通过人工检查和防御算法进行检测。虽然通过减弱后门强度可以提高隐蔽性，但这样做可能会显著损害其通用性和有效性。在本文中，我们提出了针对扩散模型的通用隐蔽后门攻击方法UIBDiffusion，使我们能够在躲避最新防御措施的同时实现卓越的攻击和生成性能。我们提出了一种基于通用对抗性扰动（UAPs）的新型触发器生成方法，并揭示这种最初被设计用于欺骗预训练判别模型的扰动，可以适应成为针对DMs的强大隐蔽后门触发器。我们在多种类型的DMs、多种采样器以及不同数据集和目标上评估了UIBDiffusion。实验结果表明，UIBDiffusion具有三个优势：1）通用性，隐蔽触发器是通用的（即图像和模型无关），单个触发器对所有图像和不同采样器的所有扩散模型都有效；2）实用性，它在低毒率下的生成质量（例如FID）与先前作品相当，甚至攻击成功率（即ASR）更高；3）不可检测性，UIBDiffusion对人类感知是合理的，并且能够绕过针对DM后门的最新防御手段Elijah和TERD。我们将发布我们的后门触发器和代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11441v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>扩散模型（DMs）存在后门攻击的风险。现有的后门攻击方法会使用明显的触发物（如灰盒和眼镜），这些触发物含有明显的模式，易于人类检查和防御算法的检测。在提高隐蔽性的同时降低后门强度可能会导致其通用性和效果大打折扣。本文提出了针对扩散模型的通用隐蔽后门攻击方法UIBDiffusion，它可以在躲避现有最先进的防御手段的同时实现出色的攻击和生成性能。我们提出了一种基于通用对抗性扰动（UAPs）的新型触发物生成方法，并发现这种扰动，最初被设计用来欺骗预训练的判别模型，可以适应成为针对DMs的强大隐蔽后门触发物。我们在多种类型的扩散模型、不同采样器以及多个数据集和目标上评估了UIBDiffusion。实验结果表明，UIBDiffusion具有三个优势：1）通用性，隐蔽触发物是通用的（即图像和模型无关），单个触发物对所有图像和所有扩散模型及不同采样器均有效；2）实用性，它在低毒率下的生成质量（例如FID）与先前的工作相当，甚至攻击成功率（即ASR）更高；3）隐蔽性，UIBDiffusion对人类感知是合理的，可以绕过Elijah和TERD等针对DMs后门的最先进防御手段。我们将发布我们的后门触发器和代码。</p>
<p><strong>要点提炼</strong></p>
<ol>
<li>扩散模型（DMs）面临后门攻击风险，现有方法使用明显触发物，易检测。</li>
<li>提出UIBDiffusion方法，实现针对DMs的隐蔽后门攻击。</li>
<li>基于通用对抗性扰动（UAPs）的触发物生成，适用于多种扩散模型和采样器。</li>
<li>UIBDiffusion具有三大优势：通用性、实用性和隐蔽性。</li>
<li>UIBDiffusion可实现高水平的攻击性能同时躲避现有防御手段。</li>
<li>将发布后门触发器和代码以供研究使用。</li>
<li>为扩散模型的安全性问题提供了新的思考和解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11441">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4db5d64700130fb76713e92798f23d80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc20b4d237d96ae1eca91895b3c5143a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-571b5258d7644936cd3fe44058fae48c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-786873e01a3549568ba95050f3b63655.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36cb40eff67b4d2e2e4a42d711c69ce7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef672e6684a2d18226e2d08a15a126f5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Dynamic-Negative-Guidance-of-Diffusion-Models"><a href="#Dynamic-Negative-Guidance-of-Diffusion-Models" class="headerlink" title="Dynamic Negative Guidance of Diffusion Models"></a>Dynamic Negative Guidance of Diffusion Models</h2><p><strong>Authors:Felix Koulischer, Johannes Deleu, Gabriel Raya, Thomas Demeester, Luca Ambrogioni</strong></p>
<p>Negative Prompting (NP) is widely utilized in diffusion models, particularly in text-to-image applications, to prevent the generation of undesired features. In this paper, we show that conventional NP is limited by the assumption of a constant guidance scale, which may lead to highly suboptimal results, or even complete failure, due to the non-stationarity and state-dependence of the reverse process. Based on this analysis, we derive a principled technique called Dynamic Negative Guidance, which relies on a near-optimal time and state dependent modulation of the guidance without requiring additional training. Unlike NP, negative guidance requires estimating the posterior class probability during the denoising process, which is achieved with limited additional computational overhead by tracking the discrete Markov Chain during the generative process. We evaluate the performance of DNG class-removal on MNIST and CIFAR10, where we show that DNG leads to higher safety, preservation of class balance and image quality when compared with baseline methods. Furthermore, we show that it is possible to use DNG with Stable Diffusion to obtain more accurate and less invasive guidance than NP. </p>
<blockquote>
<p>负向提示（NP）在扩散模型中得到了广泛应用，特别是在文本到图像的应用中，用于防止生成不需要的特征。在本文中，我们展示了传统的NP受到恒定指导尺度的假设的限制，这可能导致结果高度不理想，甚至完全失败，原因是反向过程具有非平稳性和状态依赖性。基于这一分析，我们推导出了一种基于原理的技术，称为动态负向指导（DNG），它依赖于近最优的时间和状态依赖的指导调制，而无需额外的训练。与NP不同，负向指导需要在去噪过程中估计后验类别概率，这可以通过在生成过程中跟踪离散马尔可夫链来实现，并且只需要有限的额外计算开销。我们在MNIST和CIFAR10上评估了DNG类消除的性能，结果显示，与基准方法相比，DNG提高了安全性，保持了类别平衡和图像质量。此外，我们还展示了将DNG与Stable Diffusion结合使用，可以获得比NP更准确、侵入性较小的指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14398v2">PDF</a> Paper currently under review. Submitted to ICLR 2025. Our   implementation is available at   <a target="_blank" rel="noopener" href="https://github.com/FelixKoulischer/Dynamic-Negative-Guidance.git">https://github.com/FelixKoulischer/Dynamic-Negative-Guidance.git</a></p>
<p><strong>Summary</strong></p>
<p>本文探讨了扩散模型中的负提示（NP）技术的局限性，特别是在文本转图像应用中的使用。文章提出了一种新的技术——动态负指导（DNG），该技术能够在不需要额外训练的情况下，对指导和调制进行近优的时间和状态依赖性的调整。在MNIST和CIFAR10上的评估表明，与基准方法相比，DNG在安全性、保持类平衡和图像质量方面表现出更高的性能。此外，将DNG与Stable Diffusion结合使用，可以获得比NP更准确、侵入性较小的指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型中广泛使用的负提示（NP）技术存在局限性，尤其是在文本转图像应用中。</li>
<li>动态负指导（DNG）是一种新的技术，能够近优地调整指导和调制的时间和状态依赖性。</li>
<li>DNG不需要额外的训练。</li>
<li>在MNIST和CIFAR10上的评估显示，DNG在安全性、保持类平衡和图像质量方面优于基准方法。</li>
<li>DNG技术可以有效地防止生成不需要的特征。</li>
<li>DNG可用于提高Stable Diffusion的准确性并减少侵入性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14398">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a2de5c4de0fb4266bd215fec6034c3d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a9af734a4bd7bdc8d2600bb3c0b4f99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-081718a59f4832b132a57dc1e7c39aed.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AugGS-Self-augmented-Gaussians-with-Structural-Masks-for-Sparse-view-3D-Reconstruction"><a href="#AugGS-Self-augmented-Gaussians-with-Structural-Masks-for-Sparse-view-3D-Reconstruction" class="headerlink" title="AugGS: Self-augmented Gaussians with Structural Masks for Sparse-view 3D   Reconstruction"></a>AugGS: Self-augmented Gaussians with Structural Masks for Sparse-view 3D   Reconstruction</h2><p><strong>Authors:Bi’an Du, Lingbei Meng, Wei Hu</strong></p>
<p>Sparse-view 3D reconstruction is a major challenge in computer vision, aiming to create complete three-dimensional models from limited viewing angles. Key obstacles include: 1) a small number of input images with inconsistent information; 2) dependence on input image quality; and 3) large model parameter sizes. To tackle these issues, we propose a self-augmented two-stage Gaussian splatting framework enhanced with structural masks for sparse-view 3D reconstruction. Initially, our method generates a basic 3D Gaussian representation from sparse inputs and renders multi-view images. We then fine-tune a pre-trained 2D diffusion model to enhance these images, using them as augmented data to further optimize the 3D Gaussians. Additionally, a structural masking strategy during training enhances the model’s robustness to sparse inputs and noise. Experiments on benchmarks like MipNeRF360, OmniObject3D, and OpenIllumination demonstrate that our approach achieves state-of-the-art performance in perceptual quality and multi-view consistency with sparse inputs. </p>
<blockquote>
<p>稀疏视角下的三维重建是计算机视觉领域的一大挑战，旨在从有限的视角创建完整的三维模型。主要障碍包括：1）输入图像数量少且信息不一致；2）依赖于输入图像的质量；以及3）模型参数规模大。为了解决这些问题，我们提出了一种基于结构掩膜的自增强两阶段高斯喷射框架，用于稀疏视角下的三维重建。首先，我们的方法从稀疏输入生成基本的三维高斯表示，并呈现多视角图像。然后，我们使用预训练的二维扩散模型对这些图像进行微调以增强其质量，将其作为增强数据进一步优化三维高斯表示。此外，在训练过程中采用结构掩膜策略增强了模型对稀疏输入和噪声的鲁棒性。在MipNeRF360、OmniObject3D和OpenIllumination等基准测试上的实验表明，我们的方法在感知质量和多视角一致性方面达到了先进性能，且在稀疏输入情况下表现尤为出色。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.04831v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文介绍了针对稀疏视角3D重建的主要挑战及其解决方案。通过提出一种基于自增强和两阶段高斯涂抹框架的方法，结合结构掩码技术，实现了从有限视角信息构建完整三维模型的目标。首先生成基本的三维高斯表示，并利用预训练的二维扩散模型进行图像增强，作为扩充数据进一步优化三维高斯模型。实验证明，该方法在感知质量和多视角一致性方面达到了领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>稀疏视角3D重建面临的主要挑战包括输入图像数量少且信息不一致、依赖输入图像质量和模型参数规模过大。</li>
<li>提出了一种自增强两阶段高斯涂抹框架，用于解决稀疏视角3D重建问题。</li>
<li>通过生成基本的三维高斯表示并从稀疏输入渲染多视角图像，为3D重建提供基础。</li>
<li>利用预训练的二维扩散模型增强图像，作为扩充数据进一步优化三维高斯模型。</li>
<li>结合结构掩码技术在训练过程中增强模型对稀疏输入和噪声的鲁棒性。</li>
<li>在MipNeRF360、OmniObject3D和OpenIllumination等基准测试上进行了实验验证，表明该方法在感知质量和多视角一致性方面达到领先水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.04831">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8ca103b0d2b965c34db218e2ec27fbca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52a4a78f54cfd0e14644f544afee6778.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-264f5c7c995c8c886b0f1cb2044b1802.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8855da46d8089177baaf2ead6ab9ba36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06316088fb123dc015a86b70f4a19ddd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SlerpFace-Face-Template-Protection-via-Spherical-Linear-Interpolation"><a href="#SlerpFace-Face-Template-Protection-via-Spherical-Linear-Interpolation" class="headerlink" title="SlerpFace: Face Template Protection via Spherical Linear Interpolation"></a>SlerpFace: Face Template Protection via Spherical Linear Interpolation</h2><p><strong>Authors:Zhizhou Zhong, Yuxi Mi, Yuge Huang, Jianqing Xu, Guodong Mu, Shouhong Ding, Jingyun Zhang, Rizen Guo, Yunsheng Wu, Shuigeng Zhou</strong></p>
<p>Contemporary face recognition systems use feature templates extracted from face images to identify persons. To enhance privacy, face template protection techniques are widely employed to conceal sensitive identity and appearance information stored in the template. This paper identifies an emerging privacy attack form utilizing diffusion models that could nullify prior protection. The attack can synthesize high-quality, identity-preserving face images from templates, revealing persons’ appearance. Based on studies of the diffusion model’s generative capability, this paper proposes a defense by rotating templates to a noise-like distribution. This is achieved efficiently by spherically and linearly interpolating templates on their located hypersphere. This paper further proposes to group-wisely divide and drop out templates’ feature dimensions, to enhance the irreversibility of rotated templates. The proposed techniques are concretized as a novel face template protection technique, SlerpFace. Extensive experiments show that SlerpFace provides satisfactory recognition accuracy and comprehensive protection against inversion and other attack forms, superior to prior arts. </p>
<blockquote>
<p>当代人脸识别系统使用从人脸图像中提取的特征模板来识别个人。为了增强隐私保护，广泛采用人脸模板保护技术来隐藏存储在模板中的敏感身份和外观信息。本文发现了一种利用扩散模型的新兴隐私攻击形式，可能会使先前的保护失效。这种攻击可以从模板中合成高质量、保留身份的人脸图像，从而揭示个人的外观。基于扩散模型的生成能力研究，本文提出了一种通过旋转模板到类似噪声的分布来进行防御的方法。这是通过在超球体上球面和线性插值模板来实现的。本文还提出将模板的特征维度分组并删除，以增强旋转模板的不可逆性。所提出的技术被具体化为一种新型的人脸模板保护技术——SlerpFace。大量实验表明，SlerpFace提供令人满意的识别精度和全面的保护，对抗解密和其他攻击形式，优于先前技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.03043v2">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了当代人脸识别系统使用特征模板进行身份识别的情况，并指出为了保护隐私，采取了保护面部模板的技术来掩盖存储在模板中的敏感身份和外观信息。然而，本文发现了一种利用扩散模型的新型隐私攻击方式，该攻击可以从模板中合成高质量、保留身份的人脸图像，从而揭示个人的外观信息。针对扩散模型的生成能力，本文提出了一种通过旋转模板到噪声状分布的防御策略，并在超球体上实现球面线性插值。此外，本文还提出了分组智能的模板特征维度删减策略，以提高旋转模板的不可逆性。这些技术被具体化为一种新型面部模板保护技术——SlerpFace。实验表明，SlerpFace在提供识别准确性的同时，对逆转变种及其他攻击形式提供了出色的保护，超越了先前技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当代人脸识别系统使用特征模板进行身份识别。</li>
<li>面部模板保护技术用于掩盖存储在模板中的敏感身份和外观信息。</li>
<li>出现了一种新型隐私攻击方式，利用扩散模型从模板中合成高质量人脸图像。</li>
<li>提出了通过旋转模板到噪声状分布的防御策略。</li>
<li>实现了球面线性插值技术以提高旋转模板的安全性。</li>
<li>提出了分组智能的模板特征维度删减策略以增强模板的不可逆性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.03043">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d239e0e3428c4ef4516cc87ca73df8aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-380fdcd5189faad7506fb29f1b56e496.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f3095299d7149b8a6dbca3e09ef32b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85bf76ce40bdf957726531313f802b1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9645fcd23772df691db65fbd950ca54b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Adapting-to-Unknown-Low-Dimensional-Structures-in-Score-Based-Diffusion-Models"><a href="#Adapting-to-Unknown-Low-Dimensional-Structures-in-Score-Based-Diffusion-Models" class="headerlink" title="Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion   Models"></a>Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion   Models</h2><p><strong>Authors:Gen Li, Yuling Yan</strong></p>
<p>This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions. Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper. For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable. We further identify a unique design of coefficients that yields a converges rate at the order of $O(k^{2}&#x2F;\sqrt{T})$ (up to log factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of steps. This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design. All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner. </p>
<blockquote>
<p>本文研究了基于分数的扩散模型，当底层目标分布集中在它们正式存在的高维空间中的低维流形上或附近时，这是自然图像分布的一个常见特征。尽管之前已经有人努力理解扩散模型的数据生成过程，但在存在低维结构的情况下，现有的理论支持仍然远远不够，我们在本文中加强了这一点。对于流行的去噪扩散概率模型（DDPM），我们发现每个去噪步骤中产生的错误对环境维度$d$的依赖通常是不可避免的。我们进一步识别出了一种独特的系数设计，其收敛速率为$O(k^{2}&#x2F;\sqrt{T})$（包含对数因子），其中$k$是目标分布的内在维度，$T$是步骤数。这首次从理论上证明了DDPM采样器可以适应目标分布中未知的低维结构，突出了系数设计的关键重要性。所有这些都是通过一套新型的分析工具实现的，这些工具以更确定的方式描述了算法的动力学特征。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14861v2">PDF</a> accepted to NeurIPS 2024</p>
<p><strong>Summary</strong><br>     本文研究了基于分数扩散模型在目标分布集中于或接近低维流形时的表现，这常见于自然图像分布。对于流行的去噪扩散概率模型（DDPM），本文分析了在每个去噪步骤中误差对周围维度的依赖是不可避免的，并设计了一种独特的系数，使得收敛速度达到O(k^2&#x2F;√T)，其中k为目标分布的内在维度，T为步骤数。这证明了DDPM采样器能够适应目标分布中的未知低维结构，强调系数设计的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>分数扩散模型在低维流形上的表现是研究重点，自然图像分布常具有此特性。</li>
<li>对于DDPM模型，在去噪过程中误差对周围维度的依赖是普遍存在的。</li>
<li>设计了一种独特的系数，提高了模型的收敛速度，达到O(k^2&#x2F;√T)。</li>
<li>该设计使得DDPM能够适应目标分布中的未知低维结构。</li>
<li>系数设计在扩散模型中起到关键作用。</li>
<li>本文提供了新型分析工具，以更确定的方式描述了算法动态。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14861">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-214222a4f5a5ee4841932e768473e4dd.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SOEDiff-Efficient-Distillation-for-Small-Object-Editing"><a href="#SOEDiff-Efficient-Distillation-for-Small-Object-Editing" class="headerlink" title="SOEDiff: Efficient Distillation for Small Object Editing"></a>SOEDiff: Efficient Distillation for Small Object Editing</h2><p><strong>Authors:Yiming Wu, Qihe Pan, Zhen Zhao, Zicheng Wang, Sifan Long, Ronghua Liang</strong></p>
<p>In this paper, we delve into a new task known as small object editing (SOE), which focuses on text-based image inpainting within a constrained, small-sized area. Despite the remarkable success have been achieved by current image inpainting approaches, their application to the SOE task generally results in failure cases such as Object Missing, Text-Image Mismatch, and Distortion. These failures stem from the limited use of small-sized objects in training datasets and the downsampling operations employed by U-Net models, which hinders accurate generation. To overcome these challenges, we introduce a novel training-based approach, SOEDiff, aimed at enhancing the capability of baseline models like StableDiffusion in editing small-sized objects while minimizing training costs. Specifically, our method involves two key components: SO-LoRA, which efficiently fine-tunes low-rank matrices, and Cross-Scale Score Distillation loss, which leverages high-resolution predictions from the pre-trained teacher diffusion model. Our method presents significant improvements on the test dataset collected from MSCOCO and OpenImage, validating the effectiveness of our proposed method in small object editing. In particular, when comparing SOEDiff with SD-I model on the OpenImage-f dataset, we observe a 0.99 improvement in CLIP-Score and a reduction of 2.87 in FID. </p>
<blockquote>
<p>本文中，我们深入探讨了名为小对象编辑（SOE）的新任务，该任务专注于在受限的小区域内进行基于文本的图像填充。尽管当前的图像填充方法在技术上取得了显著的进步，但它们应用于SOE任务通常会导致对象缺失、文本与图像不匹配以及失真等失败案例。这些失败源于训练数据集中小对象的有限使用以及U-Net模型的下采样操作，这阻碍了准确的生成。为了克服这些挑战，我们提出了一种新型的基于训练的方法SOEDiff，旨在增强基线模型（如StableDiffusion）编辑小对象的能力，同时降低训练成本。具体来说，我们的方法包括两个关键组件：SO-LoRA，它可以有效地微调低阶矩阵；以及跨尺度得分蒸馏损失，它利用预训练教师扩散模型的高分辨率预测。我们的方法在MSCOCO和OpenImage收集的测试数据集上取得了显著的改进，验证了我们在小对象编辑方法中的有效性。特别是当将SOEDiff与SD-I模型在OpenImage-f数据集上进行比较时，我们观察到CLIP得分提高了0.99，FID降低了2.87。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.09114v3">PDF</a> preprint</p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了一项新任务——小目标编辑（SOE），专注于在限定的小区域内进行文本基础的图像补全。当前图像补全方法在小目标编辑任务中的应用常出现失败情况，如目标缺失、文本与图像不匹配和失真。为解决这些问题，提出了一种基于训练的方法SOEDiff，旨在提高基线模型如StableDiffusion编辑小目标的能力，同时降低训练成本。该方法包括两个关键组件：SO-LoRA，用于有效微调低秩矩阵；Cross-Scale Score Distillation损失，利用预训练教师扩散模型的高分辨率预测。在MSCOCO和OpenImage收集的测试数据集上，该方法在小目标编辑任务上取得了显著改进。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>文中介绍了一种新的任务——小目标编辑（SOE），专注于小区域的文本图像补全。</li>
<li>当前图像补全方法在小目标编辑任务中易出现目标缺失、文本与图像不匹配和失真等问题。</li>
<li>为解决上述问题，提出了基于训练的方法SOEDiff，旨在提高模型对小目标编辑的能力并降低训练成本。</li>
<li>SOEDiff包含两个关键组件：SO-LoRA和Cross-Scale Score Distillation损失。</li>
<li>SO-LoRA用于有效微调低秩矩阵。</li>
<li>Cross-Scale Score Distillation损失利用预训练教师扩散模型的高分辨率预测。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.09114">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1e16434807f40c1c6eb2b16a11112fba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffd5d654a9354c25db955c49bf57585e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3a188a53dcd29eaf70af17174effe08.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GeoDiffuser-Geometry-Based-Image-Editing-with-Diffusion-Models"><a href="#GeoDiffuser-Geometry-Based-Image-Editing-with-Diffusion-Models" class="headerlink" title="GeoDiffuser: Geometry-Based Image Editing with Diffusion Models"></a>GeoDiffuser: Geometry-Based Image Editing with Diffusion Models</h2><p><strong>Authors:Rahul Sajnani, Jeroen Vanbaar, Jie Min, Kapil Katyal, Srinath Sridhar</strong></p>
<p>The success of image generative models has enabled us to build methods that can edit images based on text or other user input. However, these methods are bespoke, imprecise, require additional information, or are limited to only 2D image edits. We present GeoDiffuser, a zero-shot optimization-based method that unifies common 2D and 3D image-based object editing capabilities into a single method. Our key insight is to view image editing operations as geometric transformations. We show that these transformations can be directly incorporated into the attention layers in diffusion models to implicitly perform editing operations. Our training-free optimization method uses an objective function that seeks to preserve object style but generate plausible images, for instance with accurate lighting and shadows. It also inpaints disoccluded parts of the image where the object was originally located. Given a natural image and user input, we segment the foreground object using SAM and estimate a corresponding transform which is used by our optimization approach for editing. GeoDiffuser can perform common 2D and 3D edits like object translation, 3D rotation, and removal. We present quantitative results, including a perceptual study, that shows how our approach is better than existing methods. Visit <a target="_blank" rel="noopener" href="https://ivl.cs.brown.edu/research/geodiffuser.html">https://ivl.cs.brown.edu/research/geodiffuser.html</a> for more information. </p>
<blockquote>
<p>图像生成模型的成功使我们能够建立能够根据文本或其他用户输入编辑图像的方法。然而，这些方法都是定制的，不精确，需要额外信息，或者仅限于2D图像编辑。我们提出了GeoDiffuser，这是一种基于零样本优化的方法，它将常见的2D和3D图像基于对象的编辑功能集成到一种方法中。我们的关键见解是将图像编辑操作视为几何变换。我们表明，这些变换可以直接集成到扩散模型的注意力层中，以隐式执行编辑操作。我们的无训练优化方法使用目标函数，该函数旨在保持对象风格，但生成逼真的图像，例如具有准确的照明和阴影。它还会对原始对象所在位置被遮挡的图像部分进行填充。给定自然图像和用户输入，我们使用SAM对前景对象进行分割，并估算相应的变换，然后我们的优化方法使用该变换进行编辑。GeoDiffuser可以执行常见的2D和3D编辑，如对象平移、3D旋转和移除。我们提供了定量结果，包括一项感知研究，该研究展示了我们的方法比现有方法更好。有关更多信息，请访问<a target="_blank" rel="noopener" href="https://ivl.cs.brown.edu/research/geodiffuser.html%E3%80%82">https://ivl.cs.brown.edu/research/geodiffuser.html。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.14403v2">PDF</a> Accepted to WACV 2025, Tucson, Arizona, USA. For project page, see   <a target="_blank" rel="noopener" href="https://ivl.cs.brown.edu/research/geodiffuser.html">https://ivl.cs.brown.edu/research/geodiffuser.html</a></p>
<p><strong>Summary</strong><br>     基于扩散模型的图像编辑方法GeoDiffuser融合了常见的二维和三维图像编辑功能，将图像编辑操作视为几何变换并直接融入扩散模型的注意力层。该方法无需训练，通过优化目标函数保留物体风格同时生成可信图像，可完成物体移动、三维旋转和移除等常见二维和三维编辑任务。详情访问<a target="_blank" rel="noopener" href="https://ivl.cs.brown.edu/research/geodiffuser.html">链接</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GeoDiffuser是一个零样本优化方法，实现了统一的二维和三维图像编辑能力。</li>
<li>它将图像编辑操作视为几何变换并融入扩散模型的注意力层。</li>
<li>该方法无需训练，使用目标函数优化来保留物体风格并生成可信图像。</li>
<li>GeoDiffuser可以执行常见的二维和三维编辑任务，如物体移动、三维旋转和移除。</li>
<li>该方法通过分段前景物体和用户输入来进行编辑操作。</li>
<li>定量结果和感知研究表明，GeoDiffuser优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.14403">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-28065d195a75265377bded13001b3c98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de8cbc99cab5d8512d9987be09aa44d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42f14c04f2d4e13c229c3f9f4c9b51e9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AGFSync-Leveraging-AI-Generated-Feedback-for-Preference-Optimization-in-Text-to-Image-Generation"><a href="#AGFSync-Leveraging-AI-Generated-Feedback-for-Preference-Optimization-in-Text-to-Image-Generation" class="headerlink" title="AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in   Text-to-Image Generation"></a>AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in   Text-to-Image Generation</h2><p><strong>Authors:Jingkun An, Yinghao Zhu, Zongjian Li, Enshen Zhou, Haoran Feng, Xijie Huang, Bohua Chen, Yemin Shi, Chengwei Pan</strong></p>
<p>Text-to-Image (T2I) diffusion models have achieved remarkable success in image generation. Despite their progress, challenges remain in both prompt-following ability, image quality and lack of high-quality datasets, which are essential for refining these models. As acquiring labeled data is costly, we introduce AGFSync, a framework that enhances T2I diffusion models through Direct Preference Optimization (DPO) in a fully AI-driven approach. AGFSync utilizes Vision-Language Models (VLM) to assess image quality across style, coherence, and aesthetics, generating feedback data within an AI-driven loop. By applying AGFSync to leading T2I models such as SD v1.4, v1.5, and SDXL-base, our extensive experiments on the TIFA dataset demonstrate notable improvements in VQA scores, aesthetic evaluations, and performance on the HPSv2 benchmark, consistently outperforming the base models. AGFSync’s method of refining T2I diffusion models paves the way for scalable alignment techniques. Our code and dataset are publicly available at <a target="_blank" rel="noopener" href="https://anjingkun.github.io/AGFSync">https://anjingkun.github.io/AGFSync</a>. </p>
<blockquote>
<p>文本到图像（T2I）扩散模型在图像生成方面取得了显著的成功。尽管有所进展，但在指令遵循能力、图像质量和缺乏高质量数据集等方面仍存在挑战，这些对于完善这些模型至关重要。由于获取标记数据成本高昂，我们引入了AGFSync框架，通过全AI驱动的方法直接优化偏好（DPO）增强T2I扩散模型。AGFSync利用视觉语言模型（VLM）从风格、连贯性和美学三个方面评估图像质量，在AI驱动的循环中生成反馈数据。通过将AGFSync应用于领先的T2I模型，如SD v1.4、v1.5和SDXL-base，我们在TIFA数据集上的大量实验表明，VQA分数、美学评价和HPSv2基准测试性能均有显著提高，始终优于基础模型。AGFSync对T2I扩散模型的精炼方法为可扩展的对齐技术铺平了道路。我们的代码和数据集可在<a target="_blank" rel="noopener" href="https://anjingkun.github.io/AGFSync%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://anjingkun.github.io/AGFSync公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.13352v6">PDF</a> Accepted by AAAI-2025</p>
<p><strong>Summary</strong></p>
<p>文本介绍了T2I扩散模型在图像生成方面的显著成果，但也指出了存在的挑战，如提示遵循能力、图像质量和缺乏高质量数据集等。为解决这些问题，提出了一种名为AGFSync的框架，通过直接偏好优化（DPO）以全AI驱动的方式增强T2I扩散模型。AGFSync利用视觉语言模型（VLM）评估图像质量，包括风格、连贯性和美学方面，并在AI驱动的循环中生成反馈数据。在TIFA数据集上的实验表明，AGFSync在VQA分数、美学评估以及HPSv2基准测试上的表现均有显著提高，且始终优于基础模型。AGFSync的方法为T2I扩散模型的精炼提供了可扩展的对齐技术途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2I扩散模型在图像生成方面取得显著成功，但仍面临提示遵循能力、图像质量和数据集质量方面的挑战。</li>
<li>AGFSync框架通过全AI驱动的方式增强T2I扩散模型，引入直接偏好优化（DPO）。</li>
<li>AGFSync利用视觉语言模型（VLM）评估图像质量，包括风格、连贯性和美学。</li>
<li>AGFSync在TIFA数据集上的实验表现优异，提高了VQA分数、美学评估以及HPSv2基准测试成绩。</li>
<li>AGFSync显著优于基础模型，为T2I扩散模型的精炼提供了可扩展的对齐技术途径。</li>
<li>AGFSync方法和数据集已公开可用，网址为<a target="_blank" rel="noopener" href="https://anjingkun.github.io/AGFSync%E3%80%82">https://anjingkun.github.io/AGFSync。</a></li>
<li>引入AGFSync框架为解决扩散模型面临的挑战提供了新的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.13352">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d10213fe8dd466bb674d502d8f6f3d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-817b8963937d1ff0a7497444285f4e0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3f402bcde3ec66cdf01e0e0558d12a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cce6dd7a4c345a26b15ffbae5e0d82dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a81f907708b94fdb89a5867f8fd7f84e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2981af3a0133ae50f57b9b6e92ce04ab.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Neural-Network-Diffusion"><a href="#Neural-Network-Diffusion" class="headerlink" title="Neural Network Diffusion"></a>Neural Network Diffusion</h2><p><strong>Authors:Kai Wang, Dongwen Tang, Boya Zeng, Yida Yin, Zhaopan Xu, Yukun Zhou, Zelin Zang, Trevor Darrell, Zhuang Liu, Yang You</strong></p>
<p>Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a diffusion model. The autoencoder extracts latent representations of a subset of the trained neural network parameters. Next, a diffusion model is trained to synthesize these latent representations from random noise. This model then generates new representations, which are passed through the autoencoder’s decoder to produce new subsets of high-performing network parameters. Across various architectures and datasets, our approach consistently generates models with comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models are not memorizing the trained ones. Our results encourage more exploration into the versatile use of diffusion models. Our code is available \href{<a target="_blank" rel="noopener" href="https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion%7D%7Bhere%7D">https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion}{here}</a>. </p>
<blockquote>
<p>扩散模型在图像和视频生成方面取得了显著的成功。在这项工作中，我们证明扩散模型也能<strong>生成高性能的神经网络参数</strong>。我们的方法很简单，利用自编码器和扩散模型。自编码器提取训练神经网络参数子集中的潜在表示。接下来，训练扩散模型从随机噪声中合成这些潜在表示。该模型然后生成新的表示，通过自编码器的解码器产生新的高性能网络参数子集。在各种架构和数据集上，我们的方法始终能生成与训练网络性能相当或更好的模型，且只需极少的额外成本。值得注意的是，我们从实验中发现，生成的模型并没有记忆训练模型。我们的结果鼓励更多探索扩散模型的通用用途。我们的代码可在[<a target="_blank" rel="noopener" href="https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion]%EF%BC%88%E6%AD%A4%E5%A4%84%EF%BC%89%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion]（此处）找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.13144v3">PDF</a> We introduce a novel approach for parameter generation, named neural   network parameter diffusion (\textbf{p-diff}), which employs a standard   latent diffusion model to synthesize a new set of parameters</p>
<p><strong>Summary</strong></p>
<p>本文展示了扩散模型在生成高性能神经网络参数方面的潜力。通过结合自编码器与扩散模型，该研究首先提取训练神经网络参数的潜在表示，然后用扩散模型从这些潜在表示中合成新的网络参数。这种方法生成的新模型性能与训练网络相当或更优，且额外成本较低。研究还表明生成的模型不会记忆训练模型。该研究的代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型可用于生成高性能神经网络参数。</li>
<li>结合自编码器和扩散模型，提取并合成神经网络参数的潜在表示。</li>
<li>生成模型的性能与训练网络相当或更优。</li>
<li>扩散模型的额外成本较低。</li>
<li>生成的模型不会记忆训练模型。</li>
<li>该研究为扩散模型的多元应用提供了启示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.13144">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5149eb4d7189d7749e4f959a15e478bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0157caf6fbec45bc2432363bb6ab60b1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-04\./crop_Diffusion Models/2402.13144v3/page_3_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08f47bc877e085936e0eb5241826ca71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-366d56bd243d64a0a2a484ed1d5a61c5.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Text2Data-Low-Resource-Data-Generation-with-Textual-Control"><a href="#Text2Data-Low-Resource-Data-Generation-with-Textual-Control" class="headerlink" title="Text2Data: Low-Resource Data Generation with Textual Control"></a>Text2Data: Low-Resource Data Generation with Textual Control</h2><p><strong>Authors:Shiyu Wang, Yihao Feng, Tian Lan, Ning Yu, Yu Bai, Ran Xu, Huan Wang, Caiming Xiong, Silvio Savarese</strong></p>
<p>Natural language serves as a common and straightforward signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, such as molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model. Subsequently, it undergoes controllable finetuning via a novel constraint optimization-based learning objective that ensures controllability and effectively counteracts catastrophic forgetting. Comprehensive experiments demonstrate that Text2Data is able to achieve enhanced performance regarding controllability across various modalities, including molecules, motions and time series, when compared to existing baselines. </p>
<blockquote>
<p>自然语言是人类与机器无缝交互的常见且直观信号。认识到这一接口的重要性，机器学习社区正在投入大量精力生成与文本指令语义连贯的数据。虽然在图像编辑、音频合成、视频创建等文本到数据生成方面取得了进展，但标注昂贵或数据结构复杂的低资源领域，如分子、运动动力学和时序数据等，通常缺乏文本标签。这种缺乏阻碍了监督学习，从而限制了高级生成模型在文本到数据任务中的应用。针对低资源场景中的这些挑战，我们提出了Text2Data这一新方法，它利用无标签数据通过无监督扩散模型理解底层数据分布。随后，它通过基于新型约束优化的学习目标进行可控微调，确保可控性并有效对抗灾难性遗忘。综合实验表明，与现有基线相比，Text2Data在各种模式（包括分子、运动和时序数据）下在可控性方面取得了提高的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.10941v2">PDF</a> Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)</p>
<p><strong>Summary</strong></p>
<p>文本指出自然语言是人类与机器交互的通用和直观信号。机器学习领域正在致力于生成与文本指令语义上一致的数据。尽管文本在图像编辑、音频合成、视频创建等方面的文本到数据生成有所进展，但在低资源领域，如分子、运动动力学和时序等，由于缺乏文本标签，阻碍了监督学习，限制了高级生成模型在文本到数据任务中的应用。为此，提出Text2Data方法，利用无标签数据通过无监督扩散模型理解底层数据分布，并通过新型约束优化学习目标实现可控微调。实验证明，Text2Data在分子、运动和时序等跨模态领域实现良好可控性表现优于现有基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自然语言是人类与机器交互的重要接口，机器学习领域正努力生成与文本指令语义一致的数据。</li>
<li>文本到数据生成已在多个领域取得进展，但低资源领域由于缺乏文本标签而面临挑战。</li>
<li>Text2Data方法利用无标签数据通过无监督扩散模型理解底层数据分布。</li>
<li>Text2Data通过可控微调实现模型的可控性，有效对抗灾难性遗忘。</li>
<li>Text2Data在分子、运动和时序等跨模态领域的表现优于现有基线。</li>
<li>约束优化学习目标是实现Text2Data可控性的关键。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.10941">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c00ae61195219a14226b44fc22324526.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77c977e175e66613ee9159b6f1d29258.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47abe9c786413ca6905da56eb2bb992a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Realistic-Noise-Synthesis-with-Diffusion-Models"><a href="#Realistic-Noise-Synthesis-with-Diffusion-Models" class="headerlink" title="Realistic Noise Synthesis with Diffusion Models"></a>Realistic Noise Synthesis with Diffusion Models</h2><p><strong>Authors:Qi Wu, Mingyan Han, Ting Jiang, Chengzhi Jiang, Jinting Luo, Man Jiang, Haoqiang Fan, Shuaicheng Liu</strong></p>
<p>Deep denoising models require extensive real-world training data, which is challenging to acquire. Current noise synthesis techniques struggle to accurately model complex noise distributions. We propose a novel Realistic Noise Synthesis Diffusor (RNSD) method using diffusion models to address these challenges. By encoding camera settings into a time-aware camera-conditioned affine modulation (TCCAM), RNSD generates more realistic noise distributions under various camera conditions. Additionally, RNSD integrates a multi-scale content-aware module (MCAM), enabling the generation of structured noise with spatial correlations across multiple frequencies. We also introduce Deep Image Prior Sampling (DIPS), a learnable sampling sequence based on depth image prior, which significantly accelerates the sampling process while maintaining the high quality of synthesized noise. Extensive experiments demonstrate that our RNSD method significantly outperforms existing techniques in synthesizing realistic noise under multiple metrics and improving image denoising performance. </p>
<blockquote>
<p>深度去噪模型需要大量的真实世界训练数据，这很难获取。当前的噪声合成技术在准确模拟复杂的噪声分布方面存在困难。我们提出了一种使用扩散模型的新型Realistic Noise Synthesis Diffusor（RNSD）方法，以解决这些挑战。RNSD通过将相机设置编码为时间感知相机条件仿射调制（TCCAM），在各种相机条件下生成更真实的噪声分布。此外，RNSD还集成了一个多尺度内容感知模块（MCAM），能够生成具有多个频率之间空间相关性的结构化噪声。我们还引入了基于深度图像先验的Deep Image Prior Sampling（DIPS）可学习采样序列，它在保持合成噪声高质量的同时，显著加速了采样过程。大量实验表明，我们的RNSD方法在多个指标下合成现实噪声的性能显著优于现有技术，并提高了图像去噪性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.14022v4">PDF</a> Accepted by AAAI25</p>
<p><strong>Summary</strong></p>
<p>利用扩散模型提出一种新的真实噪声合成扩散器（RNSD）方法，解决深度降噪模型面临的实际挑战。RNSD结合时间感知相机条件仿射调制（TCCAM）和多尺度内容感知模块（MCAM），生成更符合各种相机条件下的真实噪声分布。此外，引入深度图像先验采样（DIPS）可加速采样过程并保持合成噪声的高质量。实验证明，RNSD方法在多个指标上显著优于现有技术，提高了图像去噪性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出的Realistic Noise Synthesis Diffusor (RNSD)方法使用扩散模型解决深度降噪模型的挑战。</li>
<li>RNSD结合时间感知相机条件仿射调制（TCCAM），生成更符合相机条件下的真实噪声分布。</li>
<li>多尺度内容感知模块（MCAM）使RNSD能够生成具有空间相关性的结构噪声。</li>
<li>引入Deep Image Prior Sampling (DIPS)以提高采样速度并保持噪声合成的高质量。</li>
<li>RNSD在多个指标上显著优于现有噪声合成技术。</li>
<li>RNSD方法能提高图像去噪性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2305.14022">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d672e238c1a03deb47fedc1bfe354b39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-939f178456b6300dceb43cae03d6014a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf8b6257d0574f9d5837427a735c7190.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa4eb02481a97445bc887965555f611f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e22d6eb13d43345550c198c9657464e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad608a38a4894bb647a8fd6cedafdf9a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-04/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-04/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-010366cd48b4598369b9130678ecc0b8.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-01-04  SegKAN High-Resolution Medical Image Segmentation with Long-Distance   Dependencies
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-04/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ef101938f9b717618e7f6fcec2d2ad11.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-01-04  Double-Flow GAN model for the reconstruction of perceived faces from   brain activities
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">12939.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
