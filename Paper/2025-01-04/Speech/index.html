<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-04  Towards Expressive Video Dubbing with Multiscale Multimodal Context   Interaction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-19518325334857b2b8c3a912111f6187.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    43 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-04-æ›´æ–°"><a href="#2025-01-04-æ›´æ–°" class="headerlink" title="2025-01-04 æ›´æ–°"></a>2025-01-04 æ›´æ–°</h1><h2 id="Towards-Expressive-Video-Dubbing-with-Multiscale-Multimodal-Context-Interaction"><a href="#Towards-Expressive-Video-Dubbing-with-Multiscale-Multimodal-Context-Interaction" class="headerlink" title="Towards Expressive Video Dubbing with Multiscale Multimodal Context   Interaction"></a>Towards Expressive Video Dubbing with Multiscale Multimodal Context   Interaction</h2><p><strong>Authors:Yuan Zhao, Rui Liu, Gaoxiang Cong</strong></p>
<p>Automatic Video Dubbing (AVD) generates speech aligned with lip motion and facial emotion from scripts. Recent research focuses on modeling multimodal context to enhance prosody expressiveness but overlooks two key issues: 1) Multiscale prosody expression attributes in the context influence the current sentenceâ€™s prosody. 2) Prosody cues in context interact with the current sentence, impacting the final prosody expressiveness. To tackle these challenges, we propose M2CI-Dubber, a Multiscale Multimodal Context Interaction scheme for AVD. This scheme includes two shared M2CI encoders to model the multiscale multimodal context and facilitate its deep interaction with the current sentence. By extracting global and local features for each modality in the context, utilizing attention-based mechanisms for aggregation and interaction, and employing an interaction-based graph attention network for fusion, the proposed approach enhances the prosody expressiveness of synthesized speech for the current sentence. Experiments on the Chem dataset show our model outperforms baselines in dubbing expressiveness. The code and demos are available at \textcolor[rgb]{0.93,0.0,0.47}{<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/M2CI-Dubber%7D">https://github.com/AI-S2-Lab/M2CI-Dubber}</a>. </p>
<blockquote>
<p>è‡ªåŠ¨è§†é¢‘é…éŸ³ï¼ˆAVDï¼‰æ ¹æ®è„šæœ¬ç”Ÿæˆä¸å”‡éƒ¨åŠ¨ä½œå’Œé¢éƒ¨æƒ…ç»ªç›¸åŒ¹é…çš„è¯­éŸ³ã€‚æœ€è¿‘çš„ç ”ç©¶é›†ä¸­åœ¨å»ºç«‹å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä»¥æé«˜éŸµå¾‹è¡¨è¾¾æ€§ï¼Œä½†å¿½ç•¥äº†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼š1ï¼‰ä¸Šä¸‹æ–‡ä¸­çš„å¤šå°ºåº¦éŸµå¾‹è¡¨è¾¾å±æ€§ä¼šå½±å“å½“å‰å¥å­çš„éŸµå¾‹ã€‚2ï¼‰ä¸Šä¸‹æ–‡ä¸­çš„éŸµå¾‹çº¿ç´¢ä¸å½“å‰å¥å­ç›¸äº’ä½œç”¨ï¼Œå½±å“æœ€ç»ˆçš„éŸµå¾‹è¡¨è¾¾æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†M2CI-Dubberï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºAVDçš„å¤šå°ºåº¦å¤šæ¨¡æ€ä¸Šä¸‹æ–‡äº¤äº’æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆåŒ…æ‹¬ä¸¤ä¸ªå…±äº«çš„M2CIç¼–ç å™¨ï¼Œç”¨äºå»ºç«‹å¤šå°ºåº¦å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼Œå¹¶ä¿ƒè¿›å…¶ä¸å½“å‰å¥å­çš„æ·±åº¦äº¤äº’ã€‚é€šè¿‡æå–ä¸Šä¸‹æ–‡ä¸­æ¯ç§æ¨¡æ€çš„å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ï¼Œåˆ©ç”¨åŸºäºæ³¨æ„åŠ›çš„æœºåˆ¶è¿›è¡Œèšåˆå’Œäº¤äº’ï¼Œå¹¶é‡‡ç”¨åŸºäºäº¤äº’çš„å›¾æ³¨æ„åŠ›ç½‘ç»œè¿›è¡Œèåˆï¼Œæ‰€æå‡ºçš„æ–¹æ³•æé«˜äº†å½“å‰å¥å­åˆæˆè¯­éŸ³çš„éŸµå¾‹è¡¨è¾¾æ€§ã€‚åœ¨Chemæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é…éŸ³è¡¨è¾¾æ€§æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚ä»£ç å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/M2CI-Dubber%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AI-S2-Lab/M2CI-Dubberä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18748v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong><br>è§†é¢‘è‡ªåŠ¨é…éŸ³ï¼ˆAVDï¼‰æŠ€æœ¯é€šè¿‡è„šæœ¬ç”Ÿæˆä¸å”‡åŠ¨å’Œé¢éƒ¨è¡¨æƒ…ç›¸åŒ¹é…çš„è¯­éŸ³ã€‚æœ€è¿‘çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å»ºç«‹å¤šæ¨¡æ€è¯­å¢ƒæ¨¡å‹ä»¥æé«˜è¯­è°ƒçš„è¡¨è¾¾èƒ½åŠ›ï¼Œä½†å¿½è§†äº†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼š1ï¼‰å¤šå°ºåº¦è¯­å¢ƒè¡¨è¾¾å±æ€§å¯¹å½“å‰å¥å­è¯­è°ƒçš„å½±å“ï¼›2ï¼‰è¯­å¢ƒä¸­çš„è¯­è°ƒçº¿ç´¢ä¸å½“å‰å¥å­çš„äº’åŠ¨ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†M2CI-Dubberæ–¹æ¡ˆï¼Œè¿™æ˜¯ä¸€ç§å¤šå°ºåº¦å¤šæ¨¡æ€è¯­å¢ƒäº¤äº’çš„AVDæ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆåŒ…æ‹¬ä¸¤ä¸ªå…±äº«M2CIç¼–ç å™¨ï¼Œä»¥å»ºç«‹å¤šå°ºåº¦å¤šæ¨¡æ€è¯­å¢ƒæ¨¡å‹å¹¶ä¿ƒè¿›å…¶ä¸å½“å‰å¥å­çš„æ·±åº¦äº¤äº’ã€‚é€šè¿‡æå–ä¸Šä¸‹æ–‡ä¸­æ¯ä¸ªæ¨¡æ€çš„å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ï¼Œåˆ©ç”¨åŸºäºæ³¨æ„åŠ›çš„æœºåˆ¶è¿›è¡Œèšåˆå’Œäº¤äº’ï¼Œå¹¶ä½¿ç”¨åŸºäºäº¤äº’çš„å›¾æ³¨æ„åŠ›ç½‘ç»œè¿›è¡Œèåˆï¼Œæé«˜äº†åˆæˆè¯­éŸ³çš„è¯­è°ƒè¡¨è¾¾èƒ½åŠ›ã€‚åœ¨Chemæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é…éŸ³è¡¨ç°åŠ›æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AVDæŠ€æœ¯èƒ½ç”Ÿæˆä¸è§†é¢‘å†…å®¹åŒ¹é…çš„è¯­éŸ³ã€‚</li>
<li>æœ€è¿‘ç ”ç©¶é›†ä¸­åœ¨å»ºç«‹å¤šæ¨¡æ€è¯­å¢ƒæ¨¡å‹ä»¥æå‡è¯­è°ƒè¡¨è¾¾åŠ›ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¿½è§†äº†å¤šå°ºåº¦è¯­å¢ƒè¡¨è¾¾å±æ€§åŠè¯­å¢ƒä¸­çš„è¯­è°ƒçº¿ç´¢ä¸å½“å‰å¥å­çš„äº’åŠ¨ã€‚</li>
<li>æå‡ºM2CI-Dubberæ–¹æ¡ˆï¼ŒåŒ…å«å¤šå°ºåº¦å¤šæ¨¡æ€è¯­å¢ƒäº¤äº’æ¨¡å‹ã€‚</li>
<li>M2CIç¼–ç å™¨ç”¨äºå»ºæ¨¡å¤šå°ºåº¦å¤šæ¨¡æ€è¯­å¢ƒå¹¶ä¸å½“å‰å¥å­è¿›è¡Œæ·±åº¦äº¤äº’ã€‚</li>
<li>é€šè¿‡æå–å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ã€åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶åŠå›¾æ³¨æ„åŠ›ç½‘ç»œèåˆä¿¡æ¯æ¥æé«˜è¯­è°ƒè¡¨è¾¾åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d7bab0f5b66e55414d74aca81a96f97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-382e2029f9c9fd61f193a6c26890f878.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33b3352c6416b49f87cc3f8d26f2a07a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5bb1134bca674c35291c96ec5637b6a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Zero-resource-Speech-Translation-and-Recognition-with-LLMs"><a href="#Zero-resource-Speech-Translation-and-Recognition-with-LLMs" class="headerlink" title="Zero-resource Speech Translation and Recognition with LLMs"></a>Zero-resource Speech Translation and Recognition with LLMs</h2><p><strong>Authors:Karel Mundnich, Xing Niu, Prashant Mathur, Srikanth Ronanki, Brady Houston, Veera Raghavendra Elluru, Nilaksh Das, Zejiang Hou, Goeric Huybrechts, Anshu Bhatia, Daniel Garcia-Romero, Kyu J. Han, Katrin Kirchhoff</strong></p>
<p>Despite recent advancements in speech processing, zero-resource speech translation (ST) and automatic speech recognition (ASR) remain challenging problems. In this work, we propose to leverage a multilingual Large Language Model (LLM) to perform ST and ASR in languages for which the model has never seen paired audio-text data. We achieve this by using a pre-trained multilingual speech encoder, a multilingual LLM, and a lightweight adaptation module that maps the audio representations to the token embedding space of the LLM. We perform several experiments both in ST and ASR to understand how to best train the model and what data has the most impact on performance in previously unseen languages. In ST, our best model is capable to achieve BLEU scores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we achieve WERs of up to 28.2%. We finally show that the performance of our system is bounded by the ability of the LLM to output text in the desired language. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘è¯­éŸ³å¤„ç†é¢†åŸŸæœ‰æ‰€è¿›å±•ï¼Œä½†é›¶èµ„æºè¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ‰§è¡ŒSTå’ŒASRï¼Œé’ˆå¯¹æ¨¡å‹ä»æœªæ¥è§¦è¿‡çš„é…å¯¹è¯­éŸ³æ–‡æœ¬æ•°æ®çš„è¯­è¨€ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„å¤šè¯­è¨€è¯­éŸ³ç¼–ç å™¨ã€å¤šè¯­è¨€LLMå’Œä¸€ä¸ªè½»é‡çº§é€‚é…æ¨¡å—ï¼ˆè¯¥æ¨¡å—å°†è¯­éŸ³è¡¨ç¤ºæ˜ å°„åˆ°LLMçš„ä»¤ç‰ŒåµŒå…¥ç©ºé—´ï¼‰æ¥å®ç°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬åœ¨STå’ŒASRä¸­è¿›è¡Œäº†å¤šæ¬¡å®éªŒï¼Œä»¥äº†è§£å¦‚ä½•æœ€å¥½åœ°è®­ç»ƒæ¨¡å‹ï¼Œä»¥åŠå“ªäº›æ•°æ®å¯¹ä»¥å‰æœªè§è¿‡çš„è¯­è¨€çš„æ€§èƒ½å½±å“æœ€å¤§ã€‚åœ¨STä¸­ï¼Œæˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹èƒ½å¤Ÿåœ¨CoVoST2ä¸­è¾¾åˆ°è¶…è¿‡23çš„BLEUåˆ†æ•°ï¼Œç”¨äºä¸¤ç§ä»¥å‰æœªè§è¿‡çš„è¯­è¨€ï¼›è€Œåœ¨ASRä¸­ï¼Œæˆ‘ä»¬è¾¾åˆ°é«˜è¾¾28.2%çš„WERã€‚æœ€åï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬ç³»ç»Ÿçš„æ€§èƒ½å—é™äºLLMè¾“å‡ºæ‰€éœ€è¯­è¨€æ–‡æœ¬çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18566v2">PDF</a> ICASSP 2025, 5 pages, 2 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†é›¶èµ„æºè¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„éš¾é¢˜ï¼Œå¹¶æå‡ºåˆ©ç”¨å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»æœªæ¥è§¦è¿‡é…å¯¹éŸ³é¢‘æ–‡æœ¬æ•°æ®çš„è¯­è¨€ä¸­è¿›è¡ŒSTå’ŒASRã€‚é€šè¿‡é¢„è®­ç»ƒçš„å¤šè¯­è¨€è¯­éŸ³ç¼–ç å™¨ã€å¤šè¯­è¨€LLMå’Œè½»é‡çº§é€‚é…æ¨¡å—ï¼Œå°†è¯­éŸ³è¡¨ç¤ºæ˜ å°„åˆ°LLMçš„ä»¤ç‰ŒåµŒå…¥ç©ºé—´ã€‚åœ¨STå’ŒASRæ–¹é¢è¿›è¡Œäº†å¤šæ¬¡å®éªŒï¼Œä»¥äº†è§£å¦‚ä½•æœ€ä½³åœ°è®­ç»ƒæ¨¡å‹å’Œå“ªäº›æ•°æ®å¯¹æœªè§è¯­è¨€çš„æ€§èƒ½å½±å“æœ€å¤§ã€‚åœ¨STæ–¹é¢ï¼Œæœ€ä½³æ¨¡å‹åœ¨CoVoST2ä¸­çš„BLEUå¾—åˆ†è¶…è¿‡23ï¼Œè€Œåœ¨ASRæ–¹é¢ï¼Œæˆ‘ä»¬è¾¾åˆ°äº†é«˜è¾¾28.2%çš„WERã€‚æœ€ç»ˆç»“æœè¡¨æ˜ï¼Œç³»ç»Ÿçš„æ€§èƒ½å—é™äºLLMè¾“å‡ºæ‰€éœ€è¯­è¨€æ–‡æœ¬çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é›¶èµ„æºè¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„éš¾é¢˜ã€‚</li>
<li>ç ”ç©¶æå‡ºåˆ©ç”¨å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœªè§è¿‡çš„è¯­è¨€ä¸­è¿›è¡ŒSTå’ŒASRã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒçš„å¤šè¯­è¨€è¯­éŸ³ç¼–ç å™¨ã€å¤šè¯­è¨€LLMå’Œè½»é‡çº§é€‚é…æ¨¡å—å®ç°è¯­éŸ³åˆ°æ–‡æœ¬çš„è½¬æ¢ã€‚</li>
<li>åœ¨STå’ŒASRæ–¹é¢è¿›è¡Œäº†å¤šæ¬¡å®éªŒï¼Œä»¥ä¼˜åŒ–æ¨¡å‹è®­ç»ƒå¹¶äº†è§£æ•°æ®å¯¹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>åœ¨STæ–¹é¢ï¼Œæœ€ä½³æ¨¡å‹çš„BLEUå¾—åˆ†åœ¨CoVoST2æµ‹è¯•ä¸­è¶…è¿‡23ã€‚</li>
<li>åœ¨ASRæ–¹é¢ï¼Œè¾¾åˆ°äº†é«˜è¾¾28.2%çš„å­—é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b5ccb1f990f23aa1a37e5e5df7d35388.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fc23170abbc81a398366c2a19fa5513.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad0e1040b8bc8cb5a6123cd4919ddfcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3579566010d5739780e1821c0214b65b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Mamba-SEUNet-Mamba-UNet-for-Monaural-Speech-Enhancement"><a href="#Mamba-SEUNet-Mamba-UNet-for-Monaural-Speech-Enhancement" class="headerlink" title="Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement"></a>Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement</h2><p><strong>Authors:Junyu Wang, Zizhen Lin, Tianrui Wang, Meng Ge, Longbiao Wang, Jianwu Dang</strong></p>
<p>In recent speech enhancement (SE) research, transformer and its variants have emerged as the predominant methodologies. However, the quadratic complexity of the self-attention mechanism imposes certain limitations on practical deployment. Mamba, as a novel state-space model (SSM), has gained widespread application in natural language processing and computer vision due to its strong capabilities in modeling long sequences and relatively low computational complexity. In this work, we introduce Mamba-SEUNet, an innovative architecture that integrates Mamba with U-Net for SE tasks. By leveraging bidirectional Mamba to model forward and backward dependencies of speech signals at different resolutions, and incorporating skip connections to capture multi-scale information, our approach achieves state-of-the-art (SOTA) performance. Experimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet attains a PESQ score of 3.59, while maintaining low computational complexity. When combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet further improves the PESQ score to 3.73. </p>
<blockquote>
<p>åœ¨æœ€è¿‘çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰ç ”ç©¶ä¸­ï¼ŒTransformeråŠå…¶å˜ä½“å·²ç»æˆä¸ºä¸»è¦çš„æ–¹æ³•è®ºã€‚ç„¶è€Œï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚æ€§å¯¹å®é™…éƒ¨ç½²æ–½åŠ äº†ä¸€å®šçš„é™åˆ¶ã€‚Mambaä½œä¸ºä¸€ç§æ–°å‹çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ï¼Œç”±äºå…¶å»ºæ¨¡é•¿åºåˆ—çš„å¼ºå¤§èƒ½åŠ›å’Œç›¸å¯¹è¾ƒä½çš„è®¡ç®—å¤æ‚æ€§ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Mamba-SEUNetï¼Œè¿™æ˜¯ä¸€ç§å°†Mambaä¸U-Netç›¸ç»“åˆç”¨äºSEä»»åŠ¡çš„åˆ›æ–°æ¶æ„ã€‚é€šè¿‡åˆ©ç”¨åŒå‘Mambaæ¥å»ºæ¨¡è¯­éŸ³ä¿¡å·åœ¨ä¸åŒåˆ†è¾¨ç‡ä¸Šçš„å‰åä¾èµ–æ€§ï¼Œå¹¶åŠ å…¥è·³è¿‡è¿æ¥æ¥æ•è·å¤šå°ºåº¦ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°ï¼ˆSOTAï¼‰çš„æ€§èƒ½ã€‚åœ¨VCTK+DEMANDæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMamba-SEUNetçš„PESQå¾—åˆ†ä¸º3.59ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—å¤æ‚æ€§ã€‚å½“ä¸æ„ŸçŸ¥å¯¹æ¯”åº¦æ‹‰ä¼¸æŠ€æœ¯ç›¸ç»“åˆæ—¶ï¼ŒMamba-SEUNetè¿›ä¸€æ­¥å°†PESQå¾—åˆ†æé«˜åˆ°3.73ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16626v2">PDF</a> Accepted at ICASSP 2025, 5 pages, 1 figures, 5 tables</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸçš„ä¸€ç§æ–°å‹èåˆæ¶æ„Mamba-SEUNetã€‚è¯¥æ¶æ„ç»“åˆäº†Mambaå’ŒU-Netçš„ä¼˜åŠ¿ï¼Œé€šè¿‡åˆ©ç”¨åŒå‘Mambaå»ºæ¨¡è¯­éŸ³ä¿¡å·çš„å‰å‘å’Œåå‘ä¾èµ–å…³ç³»ï¼Œå¹¶ç»“åˆè·³è·ƒè¿æ¥æ•è·å¤šå°ºåº¦ä¿¡æ¯ï¼Œå®ç°äº†å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨VCTK+DEMANDæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMamba-SEUNetåœ¨ä¿æŒè¾ƒä½è®¡ç®—å¤æ‚åº¦çš„æƒ…å†µä¸‹å–å¾—äº†PESQå¾—åˆ†ä¸º3.59çš„æˆç»©ã€‚å½“ä¸æ„ŸçŸ¥å¯¹æ¯”åº¦æ‹‰ä¼¸æŠ€æœ¯ç›¸ç»“åˆæ—¶ï¼ŒMamba-SEUNetçš„PESQå¾—åˆ†è¿›ä¸€æ­¥æé«˜è‡³3.73ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è¿‘æœŸè¯­éŸ³å¢å¼ºç ”ç©¶ä¸­ï¼ŒtransformeråŠå…¶å˜ä½“æ˜¯ä¸»è¦çš„æ–¹æ³•è®ºã€‚ä½†è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚æ€§å¯¹å®é™…åº”ç”¨éƒ¨ç½²é€ æˆäº†ä¸€å®šé™åˆ¶ã€‚</li>
<li>Mambaä½œä¸ºä¸€ç§æ–°å‹çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼Œå…¶æ“…é•¿äºå»ºæ¨¡é•¿åºåˆ—ä¸”è®¡ç®—å¤æ‚åº¦ç›¸å¯¹è¾ƒä½ã€‚</li>
<li>Mamba-SEUNetæ˜¯ä¸€ä¸ªåˆ›æ–°æ¶æ„ï¼Œç»“åˆäº†Mambaå’ŒU-Netçš„ä¼˜åŠ¿ç”¨äºè¯­éŸ³å¢å¼ºä»»åŠ¡ã€‚</li>
<li>Mamba-SEUNetåˆ©ç”¨åŒå‘Mambaå»ºæ¨¡è¯­éŸ³ä¿¡å·çš„ä¸åŒåˆ†è¾¨ç‡çš„å‰å‘å’Œåå‘ä¾èµ–å…³ç³»ï¼Œå¹¶ç»“åˆè·³è·ƒè¿æ¥æ•è·å¤šå°ºåº¦ä¿¡æ¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMamba-SEUNetåœ¨VCTK+DEMANDæ•°æ®é›†ä¸Šå–å¾—äº†è¾ƒé«˜çš„PESQå¾—åˆ†ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜ç§€çš„æ€§èƒ½ã€‚</li>
<li>Mamba-SEUNetåœ¨ä¿æŒè¾ƒä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶å–å¾—äº†å…ˆè¿›çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-363265f0e9dce52de669098fa229aab9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff07745f91837f88041e25ae53de21f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4be3fd2343b6225fc8e3f924d60f5dfa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e89b3a40405367f8408ae2a34c82635.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41f4e90603b828568e3af194b361b54e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Speech-Retrieval-Augmented-Generation-without-Automatic-Speech-Recognition"><a href="#Speech-Retrieval-Augmented-Generation-without-Automatic-Speech-Recognition" class="headerlink" title="Speech Retrieval-Augmented Generation without Automatic Speech   Recognition"></a>Speech Retrieval-Augmented Generation without Automatic Speech   Recognition</h2><p><strong>Authors:Do June Min, Karel Mundnich, Andy Lapastora, Erfan Soltanmohammadi, Srikanth Ronanki, Kyu Han</strong></p>
<p>One common approach for question answering over speech data is to first transcribe speech using automatic speech recognition (ASR) and then employ text-based retrieval-augmented generation (RAG) on the transcriptions. While this cascaded pipeline has proven effective in many practical settings, ASR errors can propagate to the retrieval and generation steps. To overcome this limitation, we introduce SpeechRAG, a novel framework designed for open-question answering over spoken data. Our proposed approach fine-tunes a pre-trained speech encoder into a speech adapter fed into a frozen large language model (LLM)â€“based retrieval model. By aligning the embedding spaces of text and speech, our speech retriever directly retrieves audio passages from text-based queries, leveraging the retrieval capacity of the frozen text retriever. Our retrieval experiments on spoken question answering datasets show that direct speech retrieval does not degrade over the text-based baseline, and outperforms the cascaded systems using ASR. For generation, we use a speech language model (SLM) as a generator, conditioned on audio passages rather than transcripts. Without fine-tuning of the SLM, this approach outperforms cascaded text-based models when there is high WER in the transcripts. </p>
<blockquote>
<p>é’ˆå¯¹è¯­éŸ³æ•°æ®çš„é—®ç­”çš„ä¸€ç§å¸¸è§æ–¹æ³•é¦–å…ˆæ˜¯ä½¿ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è¿›è¡Œè¯­éŸ³è½¬å½•ï¼Œç„¶ååœ¨è½¬å½•ä¸Šåº”ç”¨åŸºäºæ–‡æœ¬çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚è™½ç„¶è¿™ç§çº§è”ç®¡é“å·²åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†ASRé”™è¯¯å¯èƒ½ä¼šä¼ æ’­åˆ°æ£€ç´¢å’Œç”Ÿæˆæ­¥éª¤ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpeechRAGï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå¼€æ”¾å¼é—®é¢˜å›ç­”å£è¯­æ•°æ®è®¾è®¡çš„æ–°æ¡†æ¶ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•å¯¹é¢„è®­ç»ƒçš„è¯­éŸ³ç¼–ç å™¨è¿›è¡Œå¾®è°ƒï¼Œå°†å…¶è°ƒæ•´ä¸ºé€‚åº”å†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³é€‚é…å™¨æ£€ç´¢æ¨¡å‹ã€‚é€šè¿‡å¯¹æ–‡æœ¬å’Œè¯­éŸ³çš„åµŒå…¥ç©ºé—´è¿›è¡Œå¯¹é½ï¼Œæˆ‘ä»¬çš„è¯­éŸ³æ£€ç´¢å™¨ç›´æ¥ä»åŸºäºæ–‡æœ¬çš„æŸ¥è¯¢ä¸­æ£€ç´¢éŸ³é¢‘ç‰‡æ®µï¼Œåˆ©ç”¨å†»ç»“çš„æ–‡æœ¬æ£€ç´¢å™¨çš„æ£€ç´¢èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨å£è¯­é—®ç­”æ•°æ®é›†ä¸Šçš„æ£€ç´¢å®éªŒè¡¨æ˜ï¼Œç›´æ¥è¯­éŸ³æ£€ç´¢å¹¶ä¸äºšäºåŸºäºæ–‡æœ¬çš„åŸºçº¿ï¼Œå¹¶ä¸”è¡¨ç°ä¼˜äºä½¿ç”¨ASRçš„çº§è”ç³»ç»Ÿã€‚å¯¹äºç”Ÿæˆï¼Œæˆ‘ä»¬ä½¿ç”¨è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä½œä¸ºç”Ÿæˆå™¨ï¼Œä»¥éŸ³é¢‘ç‰‡æ®µä¸ºæ¡ä»¶ï¼Œè€Œä¸æ˜¯æ–‡æœ¬ã€‚åœ¨ä¸å¾®è°ƒSLMçš„æƒ…å†µä¸‹ï¼Œå½“è½¬å½•ä¸­çš„å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰è¾ƒé«˜æ—¶ï¼Œè¿™ç§æ–¹æ³•çš„è¡¨ç°ä¼˜äºçº§è”çš„åŸºäºæ–‡æœ¬æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16500v2">PDF</a> ICASSP 2025</p>
<p><strong>Summary</strong>ï¼šä¸ºæé«˜è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨é—®ç­”ç³»ç»Ÿä¸­çš„æ€§èƒ½ï¼Œé¿å…ASRé”™è¯¯å¯¹æ£€ç´¢å’Œç”Ÿæˆæ­¥éª¤çš„å½±å“ï¼Œæå‡ºSpeechRAGæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„è¯­éŸ³ç¼–ç å™¨ï¼Œå°†è¯­éŸ³é€‚é…å™¨è¾“å…¥åˆ°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢æ¨¡å‹ä¸­ï¼Œå®ç°æ–‡æœ¬å’Œè¯­éŸ³åµŒå…¥ç©ºé—´çš„å¯¹é½ã€‚æ­¤è¯­éŸ³æ£€ç´¢å™¨å¯ç›´æ¥ä»åŸºäºæ–‡æœ¬çš„æŸ¥è¯¢ä¸­æ£€ç´¢éŸ³é¢‘ç‰‡æ®µï¼Œåˆ©ç”¨å†»ç»“æ–‡æœ¬æ£€ç´¢å™¨çš„æ£€ç´¢èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œç›´æ¥è¯­éŸ³æ£€ç´¢åœ¨å£è¯­é—®ç­”æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸äºšäºåŸºäºæ–‡æœ¬çš„åŸºçº¿æ–¹æ³•ï¼Œä¸”åœ¨ASRä½¿ç”¨çº§é”™è¯¯ç‡è¾ƒé«˜æ—¶è¡¨ç°ä¼˜äºçº§è”ç³»ç»Ÿã€‚å¯¹äºç”Ÿæˆéƒ¨åˆ†ï¼Œä½¿ç”¨åŸºäºéŸ³é¢‘ç‰‡æ®µè€Œéè½¬å½•æœ¬çš„è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä½œä¸ºç”Ÿæˆå™¨ï¼Œæ— éœ€å¯¹SLMè¿›è¡Œå¾®è°ƒå³å¯è¶…è¶Šçº§è”çš„æ–‡æœ¬æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SpeechRAGæ¡†æ¶æ—¨åœ¨æé«˜åœ¨å£è¯­é—®ç­”ç³»ç»Ÿä¸­çš„æ€§èƒ½ï¼Œè§£å†³ASRé”™è¯¯ä¼ æ’­çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„è¯­éŸ³ç¼–ç å™¨å¹¶è¾“å…¥åˆ°åŸºäºLLMçš„æ£€ç´¢æ¨¡å‹ä¸­ï¼Œå®ç°SpeechRAGæ¡†æ¶ã€‚</li>
<li>SpeechRAGå®ç°äº†æ–‡æœ¬å’Œè¯­éŸ³åµŒå…¥ç©ºé—´çš„ç›´æ¥å¯¹é½ï¼Œå…è®¸ç›´æ¥æ£€ç´¢éŸ³é¢‘ç‰‡æ®µã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œç›´æ¥è¯­éŸ³æ£€ç´¢åœ¨å£è¯­é—®ç­”æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸åŸºäºæ–‡æœ¬çš„åŸºçº¿æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½ã€‚</li>
<li>åœ¨ASRå­˜åœ¨é«˜é”™è¯¯ç‡çš„æƒ…å†µä¸‹ï¼ŒSpeechRAGåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†çº§è”ç³»ç»Ÿã€‚</li>
<li>ä½¿ç”¨åŸºäºéŸ³é¢‘ç‰‡æ®µçš„SLMä½œä¸ºç”Ÿæˆå™¨ï¼Œæ— éœ€å¾®è°ƒå³å¯æé«˜ç”Ÿæˆæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16500">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cee021b716efd502d2772cdd7e58990e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13a924273c299372d64abf8fbddee866.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee70a8bc71812a34552837322c47bc8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46a4c6ec08579c253d8ef68377e9820c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4eb1662aef874aa48fc2b5ed4c5a1da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e1cfcbcc67580c52fbe6fe9d1fa5714.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="WMCodec-End-to-End-Neural-Speech-Codec-with-Deep-Watermarking-for-Authenticity-Verification"><a href="#WMCodec-End-to-End-Neural-Speech-Codec-with-Deep-Watermarking-for-Authenticity-Verification" class="headerlink" title="WMCodec: End-to-End Neural Speech Codec with Deep Watermarking for   Authenticity Verification"></a>WMCodec: End-to-End Neural Speech Codec with Deep Watermarking for   Authenticity Verification</h2><p><strong>Authors:Junzuo Zhou, Jiangyan Yi, Yong Ren, Jianhua Tao, Tao Wang, Chu Yuan Zhang</strong></p>
<p>Recent advances in speech spoofing necessitate stronger verification mechanisms in neural speech codecs to ensure authenticity. Current methods embed numerical watermarks before compression and extract them from reconstructed speech for verification, but face limitations such as separate training processes for the watermark and codec, and insufficient cross-modal information integration, leading to reduced watermark imperceptibility, extraction accuracy, and capacity. To address these issues, we propose WMCodec, the first neural speech codec to jointly train compression-reconstruction and watermark embedding-extraction in an end-to-end manner, optimizing both imperceptibility and extractability of the watermark. Furthermore, We design an iterative Attention Imprint Unit (AIU) for deeper feature integration of watermark and speech, reducing the impact of quantization noise on the watermark. Experimental results show WMCodec outperforms AudioSeal with Encodec in most quality metrics for watermark imperceptibility and consistently exceeds both AudioSeal with Encodec and reinforced TraceableSpeech in extraction accuracy of watermark. At bandwidth of 6 kbps with a watermark capacity of 16 bps, WMCodec maintains over 99% extraction accuracy under common attacks, demonstrating strong robustness. </p>
<blockquote>
<p>è¿‘å¹´æ¥è¯­éŸ³æ¬ºéª—æŠ€æœ¯çš„è¿›å±•è¦æ±‚åœ¨ç¥ç»è¯­éŸ³ç¼–è§£ç å™¨ä¸­é‡‡ç”¨æ›´å¼ºå¤§çš„éªŒè¯æœºåˆ¶ï¼Œä»¥ç¡®ä¿è¯­éŸ³çš„çœŸå®æ€§ã€‚å½“å‰çš„æ–¹æ³•åœ¨å‹ç¼©ä¹‹å‰åµŒå…¥æ•°å­—æ°´å°ï¼Œç„¶åä»é‡å»ºçš„è¯­éŸ³ä¸­æå–æ°´å°è¿›è¡ŒéªŒè¯ï¼Œä½†é¢ä¸´æ°´å°å’Œç¼–è§£ç å™¨éœ€è¦å•ç‹¬è®­ç»ƒã€è·¨æ¨¡æ€ä¿¡æ¯èåˆä¸è¶³ç­‰å±€é™æ€§ï¼Œå¯¼è‡´æ°´å°çš„ä¸å¯æ„ŸçŸ¥æ€§ã€æå–å‡†ç¡®æ€§å’Œå®¹é‡é™ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†WMCodecï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè”åˆè®­ç»ƒå‹ç¼©-é‡å»ºå’ŒåµŒå…¥æ°´å°çš„ç«¯åˆ°ç«¯ç¥ç»è¯­éŸ³ç¼–è§£ç å™¨ï¼Œä»¥ä¼˜åŒ–æ°´å°çš„ä¸å¯æ„ŸçŸ¥æ€§å’Œå¯æå–æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè¿­ä»£å¼çš„æ³¨æ„åŠ›å°è®°å•å…ƒï¼ˆAIUï¼‰ï¼Œç”¨äºæ›´æ·±å±‚åœ°æ•´åˆæ°´å°å’Œè¯­éŸ³ç‰¹å¾ï¼Œå‡å°‘é‡åŒ–å™ªå£°å¯¹æ°´å°çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWMCodecåœ¨å¤§å¤šæ•°è´¨é‡æŒ‡æ ‡ä¸Šä¼˜äºå¸¦æœ‰Encodecçš„AudioSealçš„æ°´å°ä¸å¯æ„ŸçŸ¥æ€§ï¼Œå¹¶ä¸”åœ¨æå–æ°´å°çš„å‡†ç¡®æ€§æ–¹é¢å§‹ç»ˆè¶…è¿‡å¸¦æœ‰Encodecçš„AudioSealå’Œå¢å¼ºçš„TraceableSpeechã€‚åœ¨å¸¦å®½ä¸º6kbpsã€æ°´å°å®¹é‡ä¸º16bpsçš„æƒ…å†µä¸‹ï¼ŒWMCodecåœ¨å¸¸è§æ”»å‡»ä¸‹ä¿æŒè¶…è¿‡99%çš„æå–å‡†ç¡®ç‡ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.12121v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸè¯­éŸ³ä¼ªè£…æŠ€æœ¯çš„è¿›æ­¥å¯¹ç¥ç»ç½‘ç»œè¯­éŸ³ç¼–è§£ç å™¨çš„èº«ä»½éªŒè¯æœºåˆ¶æå‡ºäº†æ›´é«˜è¦æ±‚ã€‚ä¸ºæ­¤æå‡ºäº†ä¸€ç§åä¸ºWMCodecçš„ç¥ç»ç½‘ç»œè¯­éŸ³ç¼–è§£ç å™¨ï¼Œé¦–æ¬¡å°†å‹ç¼©é‡å»ºå’Œæ°´å°åµŒå…¥æå–ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè”åˆè®­ç»ƒï¼Œä¼˜åŒ–æ°´å°çš„ä¸æ„ŸçŸ¥æ€§å’Œæå–èƒ½åŠ›ã€‚è®¾è®¡äº†ä¸€ç§è¿­ä»£æ³¨æ„åŠ›å°è®°å•å…ƒï¼ˆAIUï¼‰ï¼Œç”¨äºæ›´æ·±å±‚åœ°æ•´åˆæ°´å°å’Œè¯­éŸ³ç‰¹å¾ï¼Œå‡å°‘é‡åŒ–å™ªå£°å¯¹æ°´å°çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWMCodecåœ¨æ°´å°çš„ä¸æ„ŸçŸ¥æ€§å’Œæå–å‡†ç¡®æ€§æ–¹é¢ä¼˜äºAudioSealå’ŒEncodecï¼Œå¹¶ä¸”åœ¨å¸¦å®½ä¸º6kbpsã€æ°´å°å®¹é‡ä¸º16bpsçš„æ¡ä»¶ä¸‹ï¼Œé¢å¯¹å¸¸è§æ”»å‡»ä»èƒ½ä¿æŒè¶…è¿‡99%çš„æå–å‡†ç¡®æ€§ï¼Œå±•ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³ä¼ªè£…æŠ€æœ¯çš„è¿›æ­¥è¦æ±‚ç¥ç»ç½‘ç»œè¯­éŸ³ç¼–è§£ç å™¨åŠ å¼ºèº«ä»½éªŒè¯æœºåˆ¶ã€‚</li>
<li>WMCodecæ˜¯é¦–ä¸ªå°†å‹ç¼©é‡å»ºå’Œæ°´å°åµŒå…¥æå–è”åˆè®­ç»ƒçš„ç¥ç»ç½‘ç»œè¯­éŸ³ç¼–è§£ç å™¨ã€‚</li>
<li>WMCodecä¼˜åŒ–äº†æ°´å°çš„ä¸æ„ŸçŸ¥æ€§å’Œæå–èƒ½åŠ›ã€‚</li>
<li>è¿­ä»£æ³¨æ„åŠ›å°è®°å•å…ƒï¼ˆAIUï¼‰ç”¨äºæ›´æ·±å±‚åœ°æ•´åˆæ°´å°å’Œè¯­éŸ³ç‰¹å¾ã€‚</li>
<li>AIUè®¾è®¡å‡å°‘äº†é‡åŒ–å™ªå£°å¯¹æ°´å°çš„å½±å“ã€‚</li>
<li>å®éªŒè¡¨æ˜WMCodecåœ¨æ°´å°ä¸æ„ŸçŸ¥æ€§å’Œæå–å‡†ç¡®æ€§æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.12121">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7f1911caacedf240e8ffd8012c838127.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55a6078de9ef1cc0e23733252039848e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21df97d751f03bd0c202c887d2ddff2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72313c0bc690078da71fe93b0cb97ebb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="M2R-Whisper-Multi-stage-and-Multi-scale-Retrieval-Augmentation-for-Enhancing-Whisper"><a href="#M2R-Whisper-Multi-stage-and-Multi-scale-Retrieval-Augmentation-for-Enhancing-Whisper" class="headerlink" title="M2R-Whisper: Multi-stage and Multi-scale Retrieval Augmentation for   Enhancing Whisper"></a>M2R-Whisper: Multi-stage and Multi-scale Retrieval Augmentation for   Enhancing Whisper</h2><p><strong>Authors:Jiaming Zhou, Shiwan Zhao, Jiabei He, Hui Wang, Wenjia Zeng, Yong Chen, Haoqin Sun, Aobo Kong, Yong Qin</strong></p>
<p>State-of-the-art models like OpenAIâ€™s Whisper exhibit strong performance in multilingual automatic speech recognition (ASR), but they still face challenges in accurately recognizing diverse subdialects. In this paper, we propose M2R-whisper, a novel multi-stage and multi-scale retrieval augmentation approach designed to enhance ASR performance in low-resource settings. Building on the principles of in-context learning (ICL) and retrieval-augmented techniques, our method employs sentence-level ICL in the pre-processing stage to harness contextual information, while integrating token-level k-Nearest Neighbors (kNN) retrieval as a post-processing step to further refine the final output distribution. By synergistically combining sentence-level and token-level retrieval strategies, M2R-whisper effectively mitigates various types of recognition errors. Experiments conducted on Mandarin and subdialect datasets, including AISHELL-1 and KeSpeech, demonstrate substantial improvements in ASR accuracy, all achieved without any parameter updates. </p>
<blockquote>
<p>å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚OpenAIçš„Whisperï¼Œåœ¨å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨è¯†åˆ«å¤šæ ·çš„æ¬¡æ–¹è¨€æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†M2R-whisperï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šé˜¶æ®µå¤šå°ºåº¦æ£€ç´¢å¢å¼ºæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ä½èµ„æºç¯å¢ƒä¸‹çš„ASRæ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å’Œæ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œåœ¨é¢„å¤„ç†é˜¶æ®µé‡‡ç”¨å¥å­çº§ICLæ¥åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒæ—¶é›†æˆåŸºäºæ ‡è®°çº§çš„kæœ€è¿‘é‚»ï¼ˆkNNï¼‰æ£€ç´¢ä½œä¸ºåå¤„ç†æ­¥éª¤ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æœ€ç»ˆè¾“å‡ºåˆ†å¸ƒã€‚é€šè¿‡ååŒç»“åˆå¥å­çº§å’Œæ ‡è®°çº§æ£€ç´¢ç­–ç•¥ï¼ŒM2R-whisperæœ‰æ•ˆåœ°å‡è½»äº†å„ç§ç±»å‹çš„è¯†åˆ«é”™è¯¯ã€‚åœ¨åŒ…æ‹¬AISHELL-1å’ŒKeSpeechåœ¨å†…çš„æ™®é€šè¯å’Œæ¬¡æ–¹è¨€æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒASRå‡†ç¡®ç‡æœ‰äº†æ˜¾è‘—æé«˜ï¼Œæ‰€æœ‰è¿™äº›æ”¹è¿›éƒ½æ²¡æœ‰æ¶‰åŠä»»ä½•å‚æ•°æ›´æ–°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11889v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>M2R-whisperæ˜¯ä¸€ç§é’ˆå¯¹ä½èµ„æºç¯å¢ƒä¸‹è¯­éŸ³è¯†åˆ«æ€§èƒ½æå‡çš„å¤šé˜¶æ®µå¤šå°ºåº¦æ£€ç´¢å¢å¼ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å’Œæ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œé€šè¿‡é¢„å¤„ç†é˜¶æ®µçš„å¥å­çº§ICLåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥åŠåå¤„ç†é˜¶æ®µçš„åŸºäºkè¿‘é‚»ï¼ˆkNNï¼‰çš„tokençº§æ£€ç´¢æ¥è¿›ä¸€æ­¥ä¼˜åŒ–æœ€ç»ˆè¾“å‡ºåˆ†å¸ƒã€‚è¯¥æ–¹æ³•æœ‰æ•ˆç¼“è§£äº†å„ç§è¯†åˆ«é”™è¯¯ï¼Œåœ¨æ™®é€šè¯å’Œæ–¹è¨€æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºè¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>M2R-whisperæ˜¯ä¸€ç§é’ˆå¯¹å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æ–°é¢–æ–¹æ³•ï¼Œæ—¨åœ¨æå‡ä½èµ„æºç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚</li>
<li>æ–¹æ³•ç»“åˆäº†ä¸Šä¸‹æ–‡å­¦ä¹ å’Œæ£€ç´¢å¢å¼ºæŠ€æœ¯ã€‚</li>
<li>é€šè¿‡é¢„å¤„ç†é˜¶æ®µçš„å¥å­çº§ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæœ‰æ•ˆåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨åå¤„ç†é˜¶æ®µçš„åŸºäºkè¿‘é‚»ï¼ˆkNNï¼‰çš„tokençº§æ£€ç´¢ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æœ€ç»ˆè¾“å‡ºåˆ†å¸ƒã€‚</li>
<li>M2R-whisperèƒ½æœ‰æ•ˆç¼“è§£å„ç§è¯†åˆ«é”™è¯¯ã€‚</li>
<li>åœ¨æ™®é€šè¯å’Œæ–¹è¨€æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒASRå‡†ç¡®ç‡æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11889">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fa6910ddb4db83529caacd970e15bff0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-213a5fb72883626f9098e0cfd082cb00.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-62fe17f43a8c8545694fa2482d74ec51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0ad7ce499f8ab419cca5e3545c90775.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38be93e10552c028e4dd8c6eaa9b2e10.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Stimulus-Modality-Matters-Impact-of-Perceptual-Evaluations-from-Different-Modalities-on-Speech-Emotion-Recognition-System-Performance"><a href="#Stimulus-Modality-Matters-Impact-of-Perceptual-Evaluations-from-Different-Modalities-on-Speech-Emotion-Recognition-System-Performance" class="headerlink" title="Stimulus Modality Matters: Impact of Perceptual Evaluations from   Different Modalities on Speech Emotion Recognition System Performance"></a>Stimulus Modality Matters: Impact of Perceptual Evaluations from   Different Modalities on Speech Emotion Recognition System Performance</h2><p><strong>Authors:Huang-Cheng Chou, Haibin Wu, Chi-Chun Lee</strong></p>
<p>Speech Emotion Recognition (SER) systems rely on speech input and emotional labels annotated by humans. However, various emotion databases collect perceptional evaluations in different ways. For instance, the IEMOCAP dataset uses video clips with sounds for annotators to provide their emotional perceptions. However, the most significant English emotion dataset, the MSP-PODCAST, only provides speech for raters to choose the emotional ratings. Nevertheless, using speech as input is the standard approach to training SER systems. Therefore, the open question is the emotional labels elicited by which scenarios are the most effective for training SER systems. We comprehensively compare the effectiveness of SER systems trained with labels elicited by different modality stimuli and evaluate the SER systems on various testing conditions. Also, we introduce an all-inclusive label that combines all labels elicited by various modalities. We show that using labels elicited by voice-only stimuli for training yields better performance on the test set, whereas labels elicited by voice-only stimuli. </p>
<blockquote>
<p>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ç³»ç»Ÿä¾èµ–äºè¯­éŸ³è¾“å…¥å’Œäººå·¥æ ‡æ³¨çš„æƒ…æ„Ÿæ ‡ç­¾ã€‚ç„¶è€Œï¼Œä¸åŒçš„æƒ…æ„Ÿæ•°æ®åº“é‡‡ç”¨å„ç§æ–¹å¼æ”¶é›†æ„ŸçŸ¥è¯„ä¼°ã€‚ä¾‹å¦‚ï¼ŒIEMOCAPæ•°æ®é›†ä½¿ç”¨å¸¦æœ‰å£°éŸ³çš„è§†é¢‘å‰ªè¾‘ä¾›æ³¨é‡Šè€…æä¾›ä»–ä»¬çš„æƒ…æ„Ÿæ„ŸçŸ¥ã€‚ç„¶è€Œï¼Œæœ€å¤§çš„è‹±è¯­æƒ…æ„Ÿæ•°æ®é›†MSP-PODCASTåªæä¾›è¯­éŸ³ä¾›è¯„ä¼°è€…é€‰æ‹©æƒ…æ„Ÿè¯„åˆ†ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä½¿ç”¨è¯­éŸ³ä½œä¸ºè¾“å…¥æ˜¯è®­ç»ƒSERç³»ç»Ÿçš„æ ‡å‡†æ–¹æ³•ã€‚å› æ­¤ï¼Œå…¬å¼€çš„é—®é¢˜æ˜¯å“ªç§åœºæ™¯å¼•å‘çš„æƒ…æ„Ÿæ ‡ç­¾å¯¹äºè®­ç»ƒSERç³»ç»Ÿæœ€æœ‰æ•ˆã€‚æˆ‘ä»¬å…¨é¢æ¯”è¾ƒäº†ä½¿ç”¨ä¸åŒæ¨¡å¼åˆºæ¿€å¼•å‘çš„æ ‡ç­¾è®­ç»ƒçš„SERç³»ç»Ÿçš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨å„ç§æµ‹è¯•æ¡ä»¶ä¸‹å¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªåŒ…å®¹æ€§æ ‡ç­¾ï¼Œè¯¥æ ‡ç­¾ç»“åˆäº†ç”±å„ç§æ¨¡å¼å¼•å‘çš„æ‰€æœ‰æ ‡ç­¾ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨ä»…ç”±å£°éŸ³åˆºæ¿€å¼•å‘çš„æ ‡ç­¾è¿›è¡Œè®­ç»ƒåœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº†æ›´å¥½çš„æ€§èƒ½ï¼Œè€Œç”±å¤šç§æ¨¡æ€ç»“åˆäº§ç”Ÿçš„æ ‡ç­¾æ€§èƒ½å¹¶æœªè¾¾åˆ°æœ€ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10762v2">PDF</a> 5 pages, 2 figures, 4 tables, acceptance for ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ç³»ç»Ÿçš„è®­ç»ƒæ•°æ®æ ‡æ³¨æ–¹å¼åŠå…¶æœ‰æ•ˆæ€§ã€‚æ–‡ç« æŒ‡å‡ºä¸åŒæƒ…æ„Ÿæ•°æ®åº“é€šè¿‡ä¸åŒçš„æ–¹å¼æ”¶é›†æ„ŸçŸ¥è¯„ä»·ï¼Œå¦‚IEMOCAPæ•°æ®é›†ä½¿ç”¨è§†é¢‘å‰ªè¾‘å’Œå£°éŸ³ä¾›æ³¨é‡Šè€…æä¾›æƒ…æ„Ÿæ„ŸçŸ¥ï¼Œè€Œæœ€å¤§çš„è‹±è¯­æƒ…æ„Ÿæ•°æ®åº“MSP-PODCASTä»…æä¾›è¯­éŸ³ä¾›è¯„ä¼°è€…é€‰æ‹©æƒ…æ„Ÿè¯„åˆ†ã€‚æ–‡ç« å…¨é¢æ¯”è¾ƒäº†ä½¿ç”¨ä¸åŒæ¨¡æ€åˆºæ¿€äº§ç”Ÿçš„æ ‡ç­¾è®­ç»ƒçš„SERç³»ç»Ÿçš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨å„ç§æµ‹è¯•æ¡ä»¶ä¸‹å¯¹ç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ä»…ç”±è¯­éŸ³åˆºæ¿€äº§ç”Ÿçš„æ ‡ç­¾è¿›è¡Œè®­ç»ƒåœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°æ›´å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ç³»ç»Ÿä¾èµ–äºäººç±»æ ‡æ³¨çš„è¯­éŸ³è¾“å…¥å’Œæƒ…æ„Ÿæ ‡ç­¾ã€‚</li>
<li>ä¸åŒæƒ…æ„Ÿæ•°æ®åº“é‡‡ç”¨ä¸åŒæ–¹å¼æ”¶é›†æ„ŸçŸ¥è¯„ä»·ï¼Œå¦‚IEMOCAPå’ŒMSP-PODCASTæ•°æ®é›†ã€‚</li>
<li>ä½¿ç”¨ä¸åŒæ¨¡æ€åˆºæ¿€äº§ç”Ÿçš„æ ‡ç­¾è®­ç»ƒçš„SERç³»ç»Ÿçš„æœ‰æ•ˆæ€§å­˜åœ¨ç–‘é—®ã€‚</li>
<li>å…¨é¢æ¯”è¾ƒäº†ä½¿ç”¨ä¸åŒæ–¹å¼äº§ç”Ÿçš„æ ‡ç­¾è®­ç»ƒçš„SERç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ ‡ç­¾çš„å…¨é¢æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾æ˜¯ç”±å„ç§æ¨¡æ€å¼•å‘çš„ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨ä»…ç”±è¯­éŸ³åˆºæ¿€äº§ç”Ÿçš„æ ‡ç­¾è¿›è¡Œè®­ç»ƒåœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10762">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3b6e3d8b916931b364367d2b367b0935.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-583ec50dfdc5971e2aa93d4729bdebb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5666314e7d42ee3f344f7fe0e8cc385d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cd81282c9a9216723c67bcce787a3ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a300624bf4fea92ea46e5c74e874ffb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ddd6f569f52bf2038439c19c64d9eb9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f1ee6ac6c85a9d524ef4c64f367534e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SSR-Speech-Towards-Stable-Safe-and-Robust-Zero-shot-Text-based-Speech-Editing-and-Synthesis"><a href="#SSR-Speech-Towards-Stable-Safe-and-Robust-Zero-shot-Text-based-Speech-Editing-and-Synthesis" class="headerlink" title="SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech   Editing and Synthesis"></a>SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech   Editing and Synthesis</h2><p><strong>Authors:Helin Wang, Meng Yu, Jiarui Hai, Chen Chen, Yuchen Hu, Rilin Chen, Najim Dehak, Dong Yu</strong></p>
<p>In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SSR-Speechï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸ºç¨³å®šã€å®‰å…¨å’Œç¨³å¥çš„é›¶èµ·ç‚¹æ–‡æœ¬åŸºç¡€è¯­éŸ³ç¼–è¾‘å’Œæ–‡æœ¬åˆ°è¯­éŸ³åˆæˆè€Œè®¾è®¡çš„ç¥ç»ç¼–ç è‡ªå›å½’æ¨¡å‹ã€‚SSR-SpeechåŸºäºTransformerè§£ç å™¨æ„å»ºï¼Œå¹¶é‡‡ç”¨äº†æ— åˆ†ç±»å™¨å¼•å¯¼æŠ€æœ¯ä»¥å¢å¼ºç”Ÿæˆè¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ°´å°Encodecï¼Œå¯ä»¥å°†å¸§çº§æ°´å°åµŒå…¥è¯­éŸ³çš„ç¼–è¾‘åŒºåŸŸï¼Œä»¥ä¾¿æ£€æµ‹å“ªäº›éƒ¨åˆ†è¢«ç¼–è¾‘è¿‡ã€‚æ­¤å¤–ï¼Œæ³¢å½¢é‡å»ºåˆ©ç”¨äº†åŸå§‹æœªç¼–è¾‘çš„è¯­éŸ³ç‰‡æ®µï¼Œä¸Encodecæ¨¡å‹ç›¸æ¯”ï¼Œæä¾›äº†æ›´å‡ºè‰²çš„æ¢å¤æ•ˆæœã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç°å®ç¼–è¾‘è¯­éŸ³ä»»åŠ¡ï¼ˆRealEditï¼‰å’ŒLibriTTSæ–‡æœ¬åˆ°è¯­éŸ³ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œè¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSSR-Speechåœ¨å¤šè·¨åº¦è¯­éŸ³ç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”å¯¹èƒŒæ™¯å£°éŸ³è¡¨ç°å‡ºæƒŠäººçš„ç¨³å¥æ€§ã€‚æºä»£ç å’Œæ¼”ç¤ºå·²ç»å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07556v2">PDF</a> ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SSR-Speechï¼Œä¸€ç§ç”¨äºç¨³å®šã€å®‰å…¨å’Œç¨³å¥çš„é›¶æ ·æœ¬æ–‡æœ¬è¯­éŸ³ç¼–è¾‘å’Œæ–‡æœ¬åˆ°è¯­éŸ³åˆæˆçš„ç¥ç»ç¼–ç è‡ªå›å½’æ¨¡å‹ã€‚SSR-SpeechåŸºäºTransformerè§£ç å™¨æ„å»ºï¼Œé‡‡ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼æé«˜ç”Ÿæˆè¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚æå‡ºäº†æ°´å°Encodecï¼Œå¯å°†å¸§çº§æ°´å°åµŒå…¥è¯­éŸ³çš„ç¼–è¾‘åŒºåŸŸï¼Œä»¥æ£€æµ‹å“ªäº›éƒ¨åˆ†è¢«ç¼–è¾‘è¿‡ã€‚æ­¤å¤–ï¼Œæ³¢å½¢é‡å»ºåˆ©ç”¨åŸå§‹æœªç¼–è¾‘çš„è¯­éŸ³ç‰‡æ®µï¼Œæä¾›äº†æ¯”Encodecæ¨¡å‹æ›´å‡ºè‰²çš„æ¢å¤æ•ˆæœã€‚SSR-Speechåœ¨RealEditè¯­éŸ³ç¼–è¾‘ä»»åŠ¡å’ŒLibriTTSæ–‡æœ¬åˆ°è¯­éŸ³ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½æ°´å¹³ï¼Œè¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSSR-Speechåœ¨å¤šè·¨åº¦è¯­éŸ³ç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”å¯¹èƒŒæ™¯å£°éŸ³å…·æœ‰æƒŠäººçš„ç¨³å¥æ€§ã€‚å·²å‘å¸ƒæºä»£ç å’Œæ¼”ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SSR-Speechæ˜¯ä¸€ç§ç¥ç»ç¼–ç è‡ªå›å½’æ¨¡å‹ï¼Œç”¨äºç¨³å®šã€å®‰å…¨å’Œç¨³å¥çš„é›¶æ ·æœ¬æ–‡æœ¬è¯­éŸ³ç¼–è¾‘å’Œæ–‡æœ¬åˆ°è¯­éŸ³åˆæˆã€‚</li>
<li>SSR-Speeché‡‡ç”¨Transformerè§£ç å™¨å’Œæ— åˆ†ç±»å™¨å¼•å¯¼å¢å¼ºç”Ÿæˆç¨³å®šæ€§ã€‚</li>
<li>SSR-Speechæå‡ºäº†æ°´å°EncodecæŠ€æœ¯ï¼Œèƒ½å¤Ÿæ£€æµ‹è¯­éŸ³ä¸­å“ªäº›éƒ¨åˆ†è¢«ç¼–è¾‘è¿‡ã€‚</li>
<li>æ³¢å½¢é‡å»ºåˆ©ç”¨åŸå§‹æœªç¼–è¾‘çš„è¯­éŸ³ç‰‡æ®µï¼Œå®ç°ä¼˜è´¨æ¢å¤ã€‚</li>
<li>SSR-Speechåœ¨RealEditè¯­éŸ³ç¼–è¾‘ä»»åŠ¡å’ŒLibriTTSæ–‡æœ¬åˆ°è¯­éŸ³ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šå…ˆå‰æ–¹æ³•ã€‚</li>
<li>SSR-Speechæ“…é•¿å¤šè·¨åº¦è¯­éŸ³ç¼–è¾‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.07556">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3191d01fe79dfd4a84375b2886105e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c26a34128e094bf7345ad8214e7bb33c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-62a38d303f5dcbc3e98ba6e6438ffab2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d8e273cfd1c9d9903109b5c111bf90f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7af88c7005e8487d83ca840eb2b5c5e3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Chain-of-Translation-Prompting-CoTR-A-Novel-Prompting-Technique-for-Low-Resource-Languages"><a href="#Chain-of-Translation-Prompting-CoTR-A-Novel-Prompting-Technique-for-Low-Resource-Languages" class="headerlink" title="Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for   Low Resource Languages"></a>Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for   Low Resource Languages</h2><p><strong>Authors:Tejas Deshpande, Nidhi Kowtal, Raviraj Joshi</strong></p>
<p>This paper introduces Chain of Translation Prompting (CoTR), a novel strategy designed to enhance the performance of language models in low-resource languages. CoTR restructures prompts to first translate the input context from a low-resource language into a higher-resource language, such as English. The specified task like generation, classification, or any other NLP function is then performed on the translated text, with the option to translate the output back to the original language if needed. All these steps are specified in a single prompt. We demonstrate the effectiveness of this method through a case study on the low-resource Indic language Marathi. The CoTR strategy is applied to various tasks, including sentiment analysis, hate speech classification, subject classification and text generation, and its efficacy is showcased by comparing it with regular prompting methods. Our results underscore the potential of translation-based prompting strategies to significantly improve multilingual LLM performance in low-resource languages, offering valuable insights for future research and applications. We specifically see the highest accuracy improvements with the hate speech detection task. The technique also has the potential to enhance the quality of synthetic data generation for underrepresented languages using LLMs. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ç¿»è¯‘é“¾æç¤ºï¼ˆCoTRï¼‰è¿™ä¸€æ–°å‹ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜ä½èµ„æºè¯­è¨€çš„è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚CoTRé‡æ–°æ„å»ºæç¤ºï¼Œé¦–å…ˆå°†è¾“å…¥ä¸Šä¸‹æ–‡ä»ä½èµ„æºè¯­è¨€ç¿»è¯‘åˆ°èµ„æºæ›´ä¸°å¯Œçš„è¯­è¨€ï¼Œå¦‚è‹±è¯­ã€‚ç„¶ååœ¨ç¿»è¯‘åçš„æ–‡æœ¬ä¸Šæ‰§è¡ŒæŒ‡å®šä»»åŠ¡ï¼Œå¦‚ç”Ÿæˆã€åˆ†ç±»æˆ–å…¶ä»–ä»»ä½•NLPåŠŸèƒ½ï¼Œå¦‚æœéœ€è¦å°†è¾“å‡ºç¿»è¯‘å›åŸå§‹è¯­è¨€ã€‚æ‰€æœ‰è¿™äº›æ­¥éª¤éƒ½åœ¨ä¸€ä¸ªæç¤ºä¸­æŒ‡å®šã€‚æˆ‘ä»¬é€šè¿‡é’ˆå¯¹ä½èµ„æºå°åº¦è¯­è¨€é©¬æ‹‰åœ°è¯­è¿›è¡Œæ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚CoTRç­–ç•¥åº”ç”¨äºå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€ä»‡æ¨è¨€è®ºåˆ†ç±»ã€ä¸»é¢˜åˆ†ç±»å’Œæ–‡æœ¬ç”Ÿæˆï¼Œé€šè¿‡ä¸å¸¸è§„æç¤ºæ–¹æ³•çš„æ¯”è¾ƒï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒåŸºäºç¿»è¯‘çš„æç¤ºç­–ç•¥åœ¨æé«˜ä½èµ„æºè¯­è¨€çš„å¤šè¯­ç§å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥ç ”ç©¶å’Œåº”ç”¨æä¾›äº†å®è´µçš„è§è§£ã€‚æˆ‘ä»¬ç‰¹åˆ«çœ‹åˆ°ä»‡æ¨è¨€è®ºæ£€æµ‹ä»»åŠ¡è·å¾—äº†æœ€é«˜çš„ç²¾åº¦æå‡ã€‚è¯¥æŠ€æœ¯è¿˜å…·æœ‰æé«˜ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€åˆæˆæ•°æ®ç”Ÿæˆè´¨é‡çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.04512v2">PDF</a> Accepted at PACLIC 38 (2024)</p>
<p><strong>Summary</strong><br>æ­¤è®ºæ–‡æå‡ºä¸€ç§åä¸ºâ€œç¿»è¯‘é“¾æç¤ºâ€ï¼ˆCoTRï¼‰çš„æ–°ç­–ç•¥ï¼Œæ—¨åœ¨æå‡ä½èµ„æºè¯­è¨€çš„è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚CoTRé€šè¿‡é‡æ–°æ„å»ºæç¤ºï¼Œå…ˆå°†è¾“å…¥å†…å®¹ä»ä½èµ„æºè¯­è¨€ç¿»è¯‘åˆ°èµ„æºä¸°å¯Œã€å¦‚è‹±è¯­è¿™æ ·çš„è¯­è¨€ï¼Œç„¶ååœ¨ç¿»è¯‘åçš„æ–‡æœ¬ä¸Šæ‰§è¡ŒæŒ‡å®šä»»åŠ¡ï¼Œå¦‚ç”Ÿæˆã€åˆ†ç±»æˆ–å…¶ä»–NLPåŠŸèƒ½ã€‚å¦‚æœéœ€è¦ï¼Œè¿˜å¯ä»¥å°†è¾“å‡ºç¿»è¯‘å›åŸå§‹è¯­è¨€ã€‚æ‰€æœ‰è¿™äº›æ­¥éª¤éƒ½åœ¨ä¸€ä¸ªæç¤ºä¸­æŒ‡å®šã€‚æˆ‘ä»¬é€šè¿‡åœ¨é©¬æ‹‰åœ°è¯­è¿™ä¸€ä½èµ„æºå°åº¦è¯­è¨€ä¸Šè¿›è¡Œçš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„å®ç”¨æ€§ã€‚æ­¤ç­–ç•¥é€‚ç”¨äºå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€ä»‡æ¨è¨€è®ºåˆ†ç±»ã€ä¸»é¢˜åˆ†ç±»å’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ˜¾ç¤ºç¿»è¯‘æç¤ºç­–ç•¥å…·æœ‰æ˜¾è‘—æ”¹å–„ä½èµ„æºè¯­è¨€çš„å¤šè¯­ç§å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚ç‰¹åˆ«åœ°ï¼Œä»‡æ¨è¨€è®ºæ£€æµ‹ä»»åŠ¡çš„æ•ˆæœæå‡æœ€ä¸ºæ˜¾è‘—ã€‚æ­¤æŠ€æœ¯è¿˜æœ‰åŠ©äºæé«˜ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ä»£è¡¨æ€§ä¸è¶³çš„è¯­ç§åˆæˆæ•°æ®ç”Ÿæˆçš„å“è´¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoTRæ˜¯ä¸€ç§æ–°å‹ç­–ç•¥ï¼Œæ—¨åœ¨å¢å¼ºä½èµ„æºè¯­è¨€çš„è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>CoTRé€šè¿‡ç¿»è¯‘è¾“å…¥å†…å®¹ä»ä½èµ„æºè¯­è¨€åˆ°èµ„æºä¸°å¯Œè¯­è¨€æ¥æå‡è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ­¤ç­–ç•¥é€‚ç”¨äºå„ç§NLPä»»åŠ¡ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€ä»‡æ¨è¨€è®ºåˆ†ç±»ã€ä¸»é¢˜åˆ†ç±»å’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚</li>
<li>CoTRåœ¨é©¬æ‹‰åœ°è¯­è¿™ä¸€ä½èµ„æºå°åº¦è¯­è¨€ä¸Šçš„æ¡ˆä¾‹ç ”ç©¶è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸å¸¸è§„æç¤ºæ–¹æ³•ç›¸æ¯”ï¼ŒCoTRç­–ç•¥æ˜¾è‘—æé«˜ä»‡æ¨è¨€è®ºæ£€æµ‹ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>CoTRå…·æœ‰æ”¹å–„ä»£è¡¨æ€§ä¸è¶³çš„è¯­ç§åˆæˆæ•°æ®ç”Ÿæˆçš„å“è´¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.04512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1d3451b9a5fa7c088ad12a25c5cea621.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fef1d8e1bd8204f072124f1255b87217.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f4d0a538d5e2b27b0d82eb96865eba4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2316c49207c21f97b357b569372a9389.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d72ca94203ef4cfcfab460a86990b86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fcf4794ac0fc91b7efd17f9343f2849.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Real-time-Speech-Enhancement-on-Raw-Signals-with-Deep-State-space-Modeling"><a href="#Real-time-Speech-Enhancement-on-Raw-Signals-with-Deep-State-space-Modeling" class="headerlink" title="Real-time Speech Enhancement on Raw Signals with Deep State-space   Modeling"></a>Real-time Speech Enhancement on Raw Signals with Deep State-space   Modeling</h2><p><strong>Authors:Yan Ru Pei, Ritik Shrivastava, FNU Sidharth</strong></p>
<p>We present aTENNuate, a simple deep state-space autoencoder configured for efficient online raw speech enhancement in an end-to-end fashion. The networkâ€™s performance is primarily evaluated on raw speech denoising, with additional assessments on tasks such as super-resolution and de-quantization. We benchmark aTENNuate on the VoiceBank + DEMAND and the Microsoft DNS1 synthetic test sets. The network outperforms previous real-time denoising models in terms of PESQ score, parameter count, MACs, and latency. Even as a raw waveform processing model, the model maintains high fidelity to the clean signal with minimal audible artifacts. In addition, the model remains performant even when the noisy input is compressed down to 4000Hz and 4 bits, suggesting general speech enhancement capabilities in low-resource environments. Code is available at github.com&#x2F;Brainchip-Inc&#x2F;aTENNuate </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†aTENNuateï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„æ·±åº¦çŠ¶æ€ç©ºé—´è‡ªç¼–ç å™¨ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼é…ç½®ï¼Œç”¨äºé«˜æ•ˆçš„åœ¨çº¿åŸå§‹è¯­éŸ³å¢å¼ºã€‚è¯¥ç½‘ç»œçš„ä¸»è¦è¯„ä¼°æ˜¯åŸºäºåŸå§‹è¯­éŸ³å»å™ªï¼Œè¿˜åŒ…æ‹¬è¶…åˆ†è¾¨ç‡å’Œå»é‡åŒ–ç­‰ä»»åŠ¡çš„è¯„ä¼°ã€‚æˆ‘ä»¬åœ¨VoiceBank + DEMANDå’ŒMicrosoft DNS1åˆæˆæµ‹è¯•é›†ä¸Šå¯¹aTENNuateè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚è¯¥ç½‘ç»œåœ¨PESQå¾—åˆ†ã€å‚æ•°è®¡æ•°ã€MACså’Œå»¶è¿Ÿæ–¹é¢ä¼˜äºä»¥å‰çš„å®æ—¶å»å™ªæ¨¡å‹ã€‚å³ä½¿ä½œä¸ºåŸå§‹æ³¢å½¢å¤„ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯¹æ¸…æ´ä¿¡å·çš„ä¿çœŸåº¦ä¹Ÿå¾ˆé«˜ï¼Œå‡ ä¹å¬ä¸åˆ°æ‚éŸ³ã€‚æ­¤å¤–ï¼Œå³ä½¿å˜ˆæ‚çš„è¾“å…¥å‹ç¼©åˆ°4000Hzå’Œ4ä½æ—¶ï¼Œè¯¥æ¨¡å‹ä»ç„¶æ€§èƒ½å“è¶Šï¼Œè¡¨æ˜å…¶åœ¨ä½èµ„æºç¯å¢ƒä¸­å…·æœ‰ä¸€èˆ¬çš„è¯­éŸ³å¢å¼ºèƒ½åŠ›ã€‚ä»£ç å¯åœ¨github.com&#x2F;Brainchip-Inc&#x2F;aTENNuateæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.03377v3">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>    æœ¬æ–‡ä»‹ç»äº†aTENNuateï¼Œä¸€ä¸ªç®€æ´çš„æ·±åº¦çŠ¶æ€ç©ºé—´è‡ªç¼–ç å™¨ï¼Œç”¨äºåœ¨çº¿åŸå§‹è¯­éŸ³å¢å¼ºï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œé…ç½®ã€‚è¯¥ç½‘ç»œåœ¨åŸå§‹è¯­éŸ³é™å™ªæ–¹é¢è¿›è¡Œäº†ä¸»è¦è¯„ä¼°ï¼Œå¹¶å¯¹è¶…åˆ†è¾¨ç‡å’Œå»é‡åŒ–ç­‰ä»»åŠ¡è¿›è¡Œäº†é¢å¤–è¯„ä¼°ã€‚åœ¨VoiceBank + DEMANDå’ŒMicrosoft DNS1åˆæˆæµ‹è¯•é›†ä¸Šï¼ŒaTENNuateçš„åŸºå‡†æµ‹è¯•è¡¨æ˜å…¶åœ¨PESQå¾—åˆ†ã€å‚æ•°è®¡æ•°ã€ä¹˜ç§¯ç´¯æ“ä½œå’Œå»¶è¿Ÿç­‰æ–¹é¢ä¼˜äºç°æœ‰çš„å®æ—¶é™å™ªæ¨¡å‹ã€‚å³ä½¿ä½œä¸ºåŸå§‹æ³¢å½¢å¤„ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯¹æ¸…æ´ä¿¡å·çš„ä¿çœŸåº¦ä¾ç„¶å¾ˆé«˜ï¼Œå‡ ä¹æ— å¬è§‰å¤±çœŸã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨å˜ˆæ‚è¾“å…¥è¢«å‹ç¼©è‡³4000Hzå’Œ4æ¯”ç‰¹çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹ä¾ç„¶è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå…¶åœ¨ä½èµ„æºç¯å¢ƒä¸­å…·æœ‰ä¸€èˆ¬çš„è¯­éŸ³å¢å¼ºèƒ½åŠ›ã€‚ä»£ç å¯åœ¨github.com&#x2F;Brainchip-Inc&#x2F;aTENNuateæ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>aTENNuateæ˜¯ä¸€ä¸ªæ·±åº¦çŠ¶æ€ç©ºé—´è‡ªç¼–ç å™¨ï¼Œç”¨äºåœ¨çº¿åŸå§‹è¯­éŸ³å¢å¼ºã€‚</li>
<li>è¯¥ç½‘ç»œåœ¨åŸå§‹è¯­éŸ³é™å™ªæ–¹é¢æœ‰å‡ºè‰²çš„è¡¨ç°ã€‚</li>
<li>aTENNuateåœ¨VoiceBank + DEMANDå’ŒMicrosoft DNS1åˆæˆæµ‹è¯•é›†ä¸Šçš„åŸºå‡†æµ‹è¯•ä¼˜äºå…¶ä»–å®æ—¶é™å™ªæ¨¡å‹ã€‚</li>
<li>aTENNuateåœ¨ä¿æŒé«˜ä¿çœŸæ¸…æ´ä¿¡å·çš„åŒæ—¶ï¼Œå‡ ä¹æ— å¬è§‰å¤±çœŸã€‚</li>
<li>aTENNuateèƒ½å¤Ÿåœ¨ä½èµ„æºç¯å¢ƒä¸­è¡¨ç°å‡ºè‰¯å¥½çš„è¯­éŸ³å¢å¼ºèƒ½åŠ›ã€‚</li>
<li>aTENNuateå…·å¤‡å¤„ç†å‹ç¼©è¯­éŸ³çš„èƒ½åŠ›ï¼Œå³ä½¿è¾“å…¥è¢«å‹ç¼©è‡³4000Hzå’Œ4æ¯”ç‰¹ï¼Œä¾ç„¶èƒ½å¤Ÿä¿æŒä¼˜è‰¯æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.03377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be178251c5e274bff76c6c8b49b7c4d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19518325334857b2b8c3a912111f6187.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6679b7f52ca2dad45354b44a66e3022.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc7f8be9729e298e0b9a9fdd190e7c6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aeb9e8759edb3cf3edffe609827ede88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e417c2f8518db1dbc219c62d9d4662da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47726c6c81a64d78c9e42732bcdc076e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Multimodal-Human-Autonomous-Agents-Interaction-Using-Pre-Trained-Language-and-Visual-Foundation-Models"><a href="#Multimodal-Human-Autonomous-Agents-Interaction-Using-Pre-Trained-Language-and-Visual-Foundation-Models" class="headerlink" title="Multimodal Human-Autonomous Agents Interaction Using Pre-Trained   Language and Visual Foundation Models"></a>Multimodal Human-Autonomous Agents Interaction Using Pre-Trained   Language and Visual Foundation Models</h2><p><strong>Authors:Linus Nwankwo, Elmar Rueckert</strong></p>
<p>In this paper, we extended the method proposed in [21] to enable humans to interact naturally with autonomous agents through vocal and textual conversations. Our extended method exploits the inherent capabilities of pre-trained large language models (LLMs), multimodal visual language models (VLMs), and speech recognition (SR) models to decode the high-level natural language conversations and semantic understanding of the robotâ€™s task environment, and abstract them to the robotâ€™s actionable commands or queries. We performed a quantitative evaluation of our frameworkâ€™s natural vocal conversation understanding with participants from different racial backgrounds and English language accents. The participants interacted with the robot using both spoken and textual instructional commands. Based on the logged interaction data, our framework achieved 87.55% vocal commands decoding accuracy, 86.27% commands execution success, and an average latency of 0.89 seconds from receiving the participantsâ€™ vocal chat commands to initiating the robotâ€™s actual physical action. The video demonstrations of this paper can be found at <a target="_blank" rel="noopener" href="https://linusnep.github.io/MTCC-IRoNL/">https://linusnep.github.io/MTCC-IRoNL/</a>. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å°†[21]ä¸­æå‡ºçš„æ–¹æ³•è¿›è¡Œæ‰©å±•ï¼Œä½¿äººç±»èƒ½å¤Ÿé€šè¿‡è¯­éŸ³å’Œæ–‡æœ¬å¯¹è¯ä¸è‡ªä¸»ä»£ç†è¿›è¡Œè‡ªç„¶äº¤äº’ã€‚æˆ‘ä»¬çš„æ‰©å±•æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œè¯­éŸ³è¯†åˆ«ï¼ˆSRï¼‰æ¨¡å‹çš„å›ºæœ‰åŠŸèƒ½ï¼Œè§£ç é«˜çº§è‡ªç„¶è¯­è¨€å¯¹è¯å’Œæœºå™¨äººä»»åŠ¡ç¯å¢ƒçš„è¯­ä¹‰ç†è§£ï¼Œå¹¶å°†å…¶æŠ½è±¡åŒ–ä¸ºæœºå™¨äººçš„å¯æ“ä½œå‘½ä»¤æˆ–æŸ¥è¯¢ã€‚æˆ‘ä»¬å¯¹æ¡†æ¶çš„è‡ªç„¶è¯­éŸ³å¯¹è¯ç†è§£è¿›è¡Œäº†å®šé‡è¯„ä¼°ï¼Œå‚ä¸è€…æ¥è‡ªä¸åŒçš„ç§æ—èƒŒæ™¯å’Œè‹±è¯­å£éŸ³ã€‚å‚ä¸è€…ä½¿ç”¨å£è¯­å’Œæ–‡æœ¬æŒ‡ä»¤å‘½ä»¤ä¸æœºå™¨äººè¿›è¡Œäº¤äº’ã€‚æ ¹æ®è®°å½•çš„äº¤äº’æ•°æ®ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å®ç°äº†87.55%çš„è¯­éŸ³å‘½ä»¤è§£ç å‡†ç¡®ç‡ã€86.27%çš„å‘½ä»¤æ‰§è¡ŒæˆåŠŸç‡ï¼Œä»æ¥æ”¶å‚ä¸è€…çš„è¯­éŸ³èŠå¤©å‘½ä»¤åˆ°å¯åŠ¨æœºå™¨äººçš„å®é™…ç‰©ç†åŠ¨ä½œçš„å¹³å‡å»¶è¿Ÿæ—¶é—´ä¸º0.89ç§’ã€‚æœ¬è®ºæ–‡çš„è§†é¢‘æ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://linusnep.github.io/MTCC-IRoNL/%E6%89%BE%E5%88%B0%E3%80%82">https://linusnep.github.io/MTCC-IRoNL/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.12273v2">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬è®ºæ–‡æ‰©å±•äº†æ–¹æ³•[21]ï¼Œä½¿äººç±»èƒ½å¤Ÿé€šè¿‡è¯­éŸ³å’Œæ–‡æœ¬ä¸è‡ªä¸»æ™ºèƒ½ä½“è¿›è¡Œè‡ªç„¶äº¤äº’ã€‚è¯¥ç ”ç©¶åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹å’Œè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„å›ºæœ‰åŠŸèƒ½ï¼Œè§£ç é«˜çº§è‡ªç„¶è¯­è¨€å¯¹è¯ï¼Œç†è§£æœºå™¨äººçš„ä»»åŠ¡ç¯å¢ƒè¯­ä¹‰ï¼Œå¹¶å°†å…¶æŠ½è±¡ä¸ºæœºå™¨äººçš„å¯æ‰§è¡Œå‘½ä»¤æˆ–æŸ¥è¯¢ã€‚ç ”ç©¶äººå‘˜å¯¹ä¸åŒç§æ—èƒŒæ™¯å’Œè‹±è¯­å£éŸ³çš„å‚ä¸è€…è¿›è¡Œäº†å®šé‡åˆ†æè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå‚ä¸è€…åœ¨ä»¥å£è¯­å’Œæ–‡æœ¬å½¢å¼ä¸æœºå™¨äººäº’åŠ¨æ—¶ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å®ç°äº†é«˜è¾¾87.55%çš„è¯­éŸ³å‘½ä»¤è§£ç å‡†ç¡®ç‡ã€86.27%çš„å‘½ä»¤æ‰§è¡ŒæˆåŠŸç‡ï¼Œä»¥åŠä»æ¥æ”¶å‚ä¸è€…è¯­éŸ³èŠå¤©å‘½ä»¤åˆ°å¯åŠ¨æœºå™¨äººå®é™…ç‰©ç†åŠ¨ä½œçš„å¹³å‡å»¶è¿Ÿä»…ä¸º0.89ç§’ã€‚è§†é¢‘æ¼”ç¤ºå†…å®¹å¯é€šè¿‡é“¾æ¥è§‚çœ‹ï¼š[<a target="_blank" rel="noopener" href="https://linusnep.github.io/MTCC-IRoNL/]%E8%BF%9B%E8%A1%8C%E6%9F%A5%E7%9C%8B%E3%80%82">https://linusnep.github.io/MTCC-IRoNL/]è¿›è¡ŒæŸ¥çœ‹ã€‚</a> </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶æ‰©å±•äº†æ–¹æ³•[21]ï¼Œå°†äººç±»è‡ªç„¶äº¤æµèƒ½åŠ›èµ‹äºˆäº†ä¸è‡ªä¸»æ™ºèƒ½ä½“çš„äº’åŠ¨ä¸­ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨äº†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹å’Œè¯­éŸ³è¯†åˆ«æ¨¡å‹ç­‰æŠ€æœ¯ã€‚</li>
<li>é€šè¿‡å®šé‡åˆ†æè¯„ä¼°ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤„ç†å£è¯­æŒ‡ä»¤æ—¶çš„é«˜æ•ˆæ€§ï¼Œè¾¾åˆ°äº†87.55%çš„è¯­éŸ³å‘½ä»¤è§£ç å‡†ç¡®ç‡ä»¥åŠå‘½ä»¤æ‰§è¡ŒæˆåŠŸç‡è¾¾åˆ°äº†86.27%ã€‚ </li>
<li>å¹³å‡å»¶è¿Ÿæ—¶é—´ä»…ä¸º0.89ç§’ï¼Œå“åº”è¿…é€Ÿã€‚ </li>
<li>è¯¥æ¡†æ¶æˆåŠŸå°†å¤æ‚çš„äººç±»äº¤æµä¿¡æ¯è½¬åŒ–ä¸ºæœºå™¨äººçš„å®é™…åŠ¨ä½œæŒ‡ä»¤ã€‚ </li>
<li>å®éªŒè¦†ç›–äº†ä¸åŒç§æ—èƒŒæ™¯å’Œè‹±è¯­å£éŸ³çš„å‚ä¸è€…ï¼Œå±•ç°äº†ç³»ç»Ÿçš„å¹¿æ³›é€‚åº”æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.12273">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f3a97419e55049fd1cbc10d7c7fb26d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7a91e48e51d6f8a2aaaf953f7aff5eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50f24188c445ad15e0ea6b67806fc889.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Face-StyleSpeech-Enhancing-Zero-shot-Speech-Synthesis-from-Face-Images-with-Improved-Face-to-Speech-Mapping"><a href="#Face-StyleSpeech-Enhancing-Zero-shot-Speech-Synthesis-from-Face-Images-with-Improved-Face-to-Speech-Mapping" class="headerlink" title="Face-StyleSpeech: Enhancing Zero-shot Speech Synthesis from Face Images   with Improved Face-to-Speech Mapping"></a>Face-StyleSpeech: Enhancing Zero-shot Speech Synthesis from Face Images   with Improved Face-to-Speech Mapping</h2><p><strong>Authors:Minki Kang, Wooseok Han, Eunho Yang</strong></p>
<p>Generating speech from a face image is crucial for developing virtual humans capable of interacting using their unique voices, without relying on pre-recorded human speech. In this paper, we propose Face-StyleSpeech, a zero-shot Text-To-Speech (TTS) synthesis model that generates natural speech conditioned on a face image rather than reference speech. We hypothesize that learning entire prosodic features from a face image poses a significant challenge. To address this, our TTS model incorporates both face and prosody encoders. The prosody encoder is specifically designed to model speech style characteristics that are not fully captured by the face image, allowing the face encoder to focus on extracting speaker-specific features such as timbre. Experimental results demonstrate that Face-StyleSpeech effectively generates more natural speech from a face image than baselines, even for unseen faces. Samples are available on our demo page. </p>
<blockquote>
<p>ä»é¢éƒ¨å›¾åƒç”Ÿæˆè¯­éŸ³å¯¹äºå¼€å‘èƒ½å¤Ÿä½¿ç”¨å…¶ç‹¬ç‰¹å£°éŸ³è¿›è¡Œäº¤äº’çš„è™šæ‹Ÿäººç±»è‡³å…³é‡è¦ï¼Œè€Œæ— éœ€ä¾èµ–é¢„å…ˆå½•åˆ¶çš„äººç±»è¯­éŸ³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Face-StyleSpeechï¼Œè¿™æ˜¯ä¸€ç§é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆæ¨¡å‹ï¼Œå®ƒæ ¹æ®é¢éƒ¨å›¾åƒè€Œä¸æ˜¯å‚è€ƒè¯­éŸ³ç”Ÿæˆè‡ªç„¶è¯­éŸ³ã€‚æˆ‘ä»¬å‡è®¾ä»é¢éƒ¨å›¾åƒå­¦ä¹ æ•´ä¸ªéŸµå¾‹ç‰¹å¾æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬çš„TTSæ¨¡å‹ç»“åˆäº†é¢éƒ¨å’ŒéŸµå¾‹ç¼–ç å™¨ã€‚éŸµå¾‹ç¼–ç å™¨ä¸“é—¨ç”¨äºå»ºæ¨¡è¯­éŸ³é£æ ¼ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾ä¸èƒ½å®Œå…¨ä»é¢éƒ¨å›¾åƒä¸­è·å–ï¼Œä»è€Œå…è®¸é¢éƒ¨ç¼–ç å™¨ä¸“æ³¨äºæå–å‘éŸ³äººç‰¹å®šç‰¹å¾ï¼Œå¦‚éŸ³è‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFace-StyleSpeechèƒ½å¤Ÿä»é¢éƒ¨å›¾åƒç”Ÿæˆæ¯”åŸºçº¿æ›´è‡ªç„¶çš„è¯­éŸ³ï¼Œå³ä½¿å¯¹äºæœªè§è¿‡çš„é¢å­”ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ ·æœ¬å¯åœ¨æˆ‘ä»¬çš„æ¼”ç¤ºé¡µé¢ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05844v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢éƒ¨å›¾åƒç”Ÿæˆè¯­éŸ³å¯¹äºå¼€å‘èƒ½å¤Ÿä½¿ç”¨å…¶ç‹¬ç‰¹å£°éŸ³è¿›è¡Œäº¤äº’çš„è™šæ‹Ÿäººç±»è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºFace-StyleSpeechï¼Œä¸€ç§é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ ¹æ®é¢éƒ¨å›¾åƒç”Ÿæˆè‡ªç„¶è¯­éŸ³ï¼Œè€Œéä¾èµ–é¢„å½•åˆ¶çš„è¯­éŸ³ã€‚ä¸ºåº”å¯¹ä»é¢éƒ¨å›¾åƒå­¦ä¹ æ•´ä½“éŸµå¾‹ç‰¹å¾çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„TTSæ¨¡å‹ç»“åˆäº†é¢éƒ¨å’ŒéŸµå¾‹ç¼–ç å™¨ã€‚éŸµå¾‹ç¼–ç å™¨ä¸“é—¨è®¾è®¡ç”¨äºæ¨¡æ‹Ÿé¢éƒ¨å›¾åƒæ— æ³•å®Œå…¨æ•æ‰çš„è¯­éŸ³é£æ ¼ç‰¹å¾ï¼Œä½¿å¾—é¢éƒ¨ç¼–ç å™¨èƒ½å¤Ÿä¸“æ³¨äºæå–è¯´è¯äººçš„ç‰¹å¾ï¼Œå¦‚éŸ³è´¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFace-StyleSpeechèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ ¹æ®é¢éƒ¨å›¾åƒç”Ÿæˆæ›´è‡ªç„¶çš„è¯­éŸ³ï¼Œè¶…è¶ŠåŸºçº¿æ¨¡å‹ï¼Œç”šè‡³å¯¹äºæœªè§è¿‡çš„é¢å­”ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Face-StyleSpeechæ˜¯ä¸€ç§é›¶æ ·æœ¬TTSåˆæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®é¢éƒ¨å›¾åƒç”Ÿæˆè‡ªç„¶è¯­éŸ³ã€‚</li>
<li>æ¨¡å‹ç»“åˆé¢éƒ¨å’ŒéŸµå¾‹ç¼–ç å™¨ï¼Œä»¥åº”å¯¹ä»é¢éƒ¨å›¾åƒå­¦ä¹ éŸµå¾‹ç‰¹å¾çš„æŒ‘æˆ˜ã€‚</li>
<li>éŸµå¾‹ç¼–ç å™¨ç”¨äºæ¨¡æ‹Ÿé¢éƒ¨å›¾åƒæ— æ³•å®Œå…¨æ•æ‰çš„è¯­éŸ³é£æ ¼ç‰¹å¾ã€‚</li>
<li>é¢éƒ¨ç¼–ç å™¨ä¸“æ³¨äºæå–è¯´è¯äººçš„ç‰¹å¾ï¼Œå¦‚éŸ³è´¨ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFace-StyleSpeechåœ¨ç”ŸæˆåŸºäºé¢éƒ¨å›¾åƒçš„è¯­éŸ³æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶ŠåŸºçº¿æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹ç”Ÿæˆçš„è¯­éŸ³å¯¹äºæœªè§è¿‡çš„é¢å­”åŒæ ·æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.05844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-83c43903c2a0bd6db5298f5d1fb98ba9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e7217e939336e06d64fe9d2e07ada4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-094ccae6af75cd56e466c323aea2fc39.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-04/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-04/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-04/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ef101938f9b717618e7f6fcec2d2ad11.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-04  Double-Flow GAN model for the reconstruction of perceived faces from   brain activities
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-04/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-56c26153e24a1b3b9dad338de57ec23e.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-04  Fine-grained Image-to-LiDAR Contrastive Distillation with Visual   Foundation Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17665k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
