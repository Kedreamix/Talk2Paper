<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-01-04  Towards Expressive Video Dubbing with Multiscale Multimodal Context   Interaction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-19518325334857b2b8c3a912111f6187.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    43 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-04-更新"><a href="#2025-01-04-更新" class="headerlink" title="2025-01-04 更新"></a>2025-01-04 更新</h1><h2 id="Towards-Expressive-Video-Dubbing-with-Multiscale-Multimodal-Context-Interaction"><a href="#Towards-Expressive-Video-Dubbing-with-Multiscale-Multimodal-Context-Interaction" class="headerlink" title="Towards Expressive Video Dubbing with Multiscale Multimodal Context   Interaction"></a>Towards Expressive Video Dubbing with Multiscale Multimodal Context   Interaction</h2><p><strong>Authors:Yuan Zhao, Rui Liu, Gaoxiang Cong</strong></p>
<p>Automatic Video Dubbing (AVD) generates speech aligned with lip motion and facial emotion from scripts. Recent research focuses on modeling multimodal context to enhance prosody expressiveness but overlooks two key issues: 1) Multiscale prosody expression attributes in the context influence the current sentence’s prosody. 2) Prosody cues in context interact with the current sentence, impacting the final prosody expressiveness. To tackle these challenges, we propose M2CI-Dubber, a Multiscale Multimodal Context Interaction scheme for AVD. This scheme includes two shared M2CI encoders to model the multiscale multimodal context and facilitate its deep interaction with the current sentence. By extracting global and local features for each modality in the context, utilizing attention-based mechanisms for aggregation and interaction, and employing an interaction-based graph attention network for fusion, the proposed approach enhances the prosody expressiveness of synthesized speech for the current sentence. Experiments on the Chem dataset show our model outperforms baselines in dubbing expressiveness. The code and demos are available at \textcolor[rgb]{0.93,0.0,0.47}{<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/M2CI-Dubber%7D">https://github.com/AI-S2-Lab/M2CI-Dubber}</a>. </p>
<blockquote>
<p>自动视频配音（AVD）根据脚本生成与唇部动作和面部情绪相匹配的语音。最近的研究集中在建立多模态上下文以提高韵律表达性，但忽略了两个关键问题：1）上下文中的多尺度韵律表达属性会影响当前句子的韵律。2）上下文中的韵律线索与当前句子相互作用，影响最终的韵律表达性。为了解决这些挑战，我们提出了M2CI-Dubber，这是一种用于AVD的多尺度多模态上下文交互方案。该方案包括两个共享的M2CI编码器，用于建立多尺度多模态上下文，并促进其与当前句子的深度交互。通过提取上下文中每种模态的全局和局部特征，利用基于注意力的机制进行聚合和交互，并采用基于交互的图注意力网络进行融合，所提出的方法提高了当前句子合成语音的韵律表达性。在Chem数据集上的实验表明，我们的模型在配音表达性方面优于基准模型。代码和演示可在<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/M2CI-Dubber%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AI-S2-Lab/M2CI-Dubber上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18748v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong><br>视频自动配音（AVD）技术通过脚本生成与唇动和面部表情相匹配的语音。最近的研究主要集中在建立多模态语境模型以提高语调的表达能力，但忽视了两个关键问题：1）多尺度语境表达属性对当前句子语调的影响；2）语境中的语调线索与当前句子的互动。为了应对这些挑战，我们提出了M2CI-Dubber方案，这是一种多尺度多模态语境交互的AVD方案。该方案包括两个共享M2CI编码器，以建立多尺度多模态语境模型并促进其与当前句子的深度交互。通过提取上下文中每个模态的全局和局部特征，利用基于注意力的机制进行聚合和交互，并使用基于交互的图注意力网络进行融合，提高了合成语音的语调表达能力。在Chem数据集上的实验表明，我们的模型在配音表现力方面优于基准模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AVD技术能生成与视频内容匹配的语音。</li>
<li>最近研究集中在建立多模态语境模型以提升语调表达力。</li>
<li>现有研究忽视了多尺度语境表达属性及语境中的语调线索与当前句子的互动。</li>
<li>提出M2CI-Dubber方案，包含多尺度多模态语境交互模型。</li>
<li>M2CI编码器用于建模多尺度多模态语境并与当前句子进行深度交互。</li>
<li>通过提取全局和局部特征、利用注意力机制及图注意力网络融合信息来提高语调表达力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18748">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d7bab0f5b66e55414d74aca81a96f97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-382e2029f9c9fd61f193a6c26890f878.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33b3352c6416b49f87cc3f8d26f2a07a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5bb1134bca674c35291c96ec5637b6a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Zero-resource-Speech-Translation-and-Recognition-with-LLMs"><a href="#Zero-resource-Speech-Translation-and-Recognition-with-LLMs" class="headerlink" title="Zero-resource Speech Translation and Recognition with LLMs"></a>Zero-resource Speech Translation and Recognition with LLMs</h2><p><strong>Authors:Karel Mundnich, Xing Niu, Prashant Mathur, Srikanth Ronanki, Brady Houston, Veera Raghavendra Elluru, Nilaksh Das, Zejiang Hou, Goeric Huybrechts, Anshu Bhatia, Daniel Garcia-Romero, Kyu J. Han, Katrin Kirchhoff</strong></p>
<p>Despite recent advancements in speech processing, zero-resource speech translation (ST) and automatic speech recognition (ASR) remain challenging problems. In this work, we propose to leverage a multilingual Large Language Model (LLM) to perform ST and ASR in languages for which the model has never seen paired audio-text data. We achieve this by using a pre-trained multilingual speech encoder, a multilingual LLM, and a lightweight adaptation module that maps the audio representations to the token embedding space of the LLM. We perform several experiments both in ST and ASR to understand how to best train the model and what data has the most impact on performance in previously unseen languages. In ST, our best model is capable to achieve BLEU scores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we achieve WERs of up to 28.2%. We finally show that the performance of our system is bounded by the ability of the LLM to output text in the desired language. </p>
<blockquote>
<p>尽管最近语音处理领域有所进展，但零资源语音翻译（ST）和自动语音识别（ASR）仍是具有挑战性的问题。在这项工作中，我们提出利用多语言大型语言模型（LLM）来执行ST和ASR，针对模型从未接触过的配对语音文本数据的语言。我们通过使用预训练的多语言语音编码器、多语言LLM和一个轻量级适配模块（该模块将语音表示映射到LLM的令牌嵌入空间）来实现这一点。我们在ST和ASR中进行了多次实验，以了解如何最好地训练模型，以及哪些数据对以前未见过的语言的性能影响最大。在ST中，我们最好的模型能够在CoVoST2中达到超过23的BLEU分数，用于两种以前未见过的语言；而在ASR中，我们达到高达28.2%的WER。最后，我们表明，我们系统的性能受限于LLM输出所需语言文本的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18566v2">PDF</a> ICASSP 2025, 5 pages, 2 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>本研究探讨了零资源语音翻译（ST）和自动语音识别（ASR）的难题，并提出利用多语言大型语言模型（LLM）在从未接触过配对音频文本数据的语言中进行ST和ASR。通过预训练的多语言语音编码器、多语言LLM和轻量级适配模块，将语音表示映射到LLM的令牌嵌入空间。在ST和ASR方面进行了多次实验，以了解如何最佳地训练模型和哪些数据对未见语言的性能影响最大。在ST方面，最佳模型在CoVoST2中的BLEU得分超过23，而在ASR方面，我们达到了高达28.2%的WER。最终结果表明，系统的性能受限于LLM输出所需语言文本的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>零资源语音翻译（ST）和自动语音识别（ASR）仍是具有挑战性的难题。</li>
<li>研究提出利用多语言大型语言模型（LLM）在未见过的语言中进行ST和ASR。</li>
<li>通过预训练的多语言语音编码器、多语言LLM和轻量级适配模块实现语音到文本的转换。</li>
<li>在ST和ASR方面进行了多次实验，以优化模型训练并了解数据对性能的影响。</li>
<li>在ST方面，最佳模型的BLEU得分在CoVoST2测试中超过23。</li>
<li>在ASR方面，达到了高达28.2%的字错误率（WER）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18566">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b5ccb1f990f23aa1a37e5e5df7d35388.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fc23170abbc81a398366c2a19fa5513.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad0e1040b8bc8cb5a6123cd4919ddfcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3579566010d5739780e1821c0214b65b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Mamba-SEUNet-Mamba-UNet-for-Monaural-Speech-Enhancement"><a href="#Mamba-SEUNet-Mamba-UNet-for-Monaural-Speech-Enhancement" class="headerlink" title="Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement"></a>Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement</h2><p><strong>Authors:Junyu Wang, Zizhen Lin, Tianrui Wang, Meng Ge, Longbiao Wang, Jianwu Dang</strong></p>
<p>In recent speech enhancement (SE) research, transformer and its variants have emerged as the predominant methodologies. However, the quadratic complexity of the self-attention mechanism imposes certain limitations on practical deployment. Mamba, as a novel state-space model (SSM), has gained widespread application in natural language processing and computer vision due to its strong capabilities in modeling long sequences and relatively low computational complexity. In this work, we introduce Mamba-SEUNet, an innovative architecture that integrates Mamba with U-Net for SE tasks. By leveraging bidirectional Mamba to model forward and backward dependencies of speech signals at different resolutions, and incorporating skip connections to capture multi-scale information, our approach achieves state-of-the-art (SOTA) performance. Experimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet attains a PESQ score of 3.59, while maintaining low computational complexity. When combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet further improves the PESQ score to 3.73. </p>
<blockquote>
<p>在最近的语音增强（SE）研究中，Transformer及其变体已经成为主要的方法论。然而，自注意力机制的二次复杂性对实际部署施加了一定的限制。Mamba作为一种新型的状态空间模型（SSM），由于其建模长序列的强大能力和相对较低的计算复杂性，在自然语言处理和计算机视觉中得到了广泛应用。在这项工作中，我们介绍了Mamba-SEUNet，这是一种将Mamba与U-Net相结合用于SE任务的创新架构。通过利用双向Mamba来建模语音信号在不同分辨率上的前后依赖性，并加入跳过连接来捕获多尺度信息，我们的方法达到了最新（SOTA）的性能。在VCTK+DEMAND数据集上的实验结果表明，Mamba-SEUNet的PESQ得分为3.59，同时保持较低的计算复杂性。当与感知对比度拉伸技术相结合时，Mamba-SEUNet进一步将PESQ得分提高到3.73。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16626v2">PDF</a> Accepted at ICASSP 2025, 5 pages, 1 figures, 5 tables</p>
<p><strong>总结</strong></p>
<p>本文介绍了在语音增强领域的一种新型融合架构Mamba-SEUNet。该架构结合了Mamba和U-Net的优势，通过利用双向Mamba建模语音信号的前向和后向依赖关系，并结合跳跃连接捕获多尺度信息，实现了先进的性能。在VCTK+DEMAND数据集上的实验结果表明，Mamba-SEUNet在保持较低计算复杂度的情况下取得了PESQ得分为3.59的成绩。当与感知对比度拉伸技术相结合时，Mamba-SEUNet的PESQ得分进一步提高至3.73。</p>
<p><strong>要点</strong></p>
<ol>
<li>近期语音增强研究中，transformer及其变体是主要的方法论。但自注意力机制的二次复杂性对实际应用部署造成了一定限制。</li>
<li>Mamba作为一种新型的状态空间模型（SSM），在自然语言处理和计算机视觉领域有广泛应用，其擅长于建模长序列且计算复杂度相对较低。</li>
<li>Mamba-SEUNet是一个创新架构，结合了Mamba和U-Net的优势用于语音增强任务。</li>
<li>Mamba-SEUNet利用双向Mamba建模语音信号的不同分辨率的前向和后向依赖关系，并结合跳跃连接捕获多尺度信息。</li>
<li>实验结果表明，Mamba-SEUNet在VCTK+DEMAND数据集上取得了较高的PESQ得分，显示出其优秀的性能。</li>
<li>Mamba-SEUNet在保持较低计算复杂度的同时取得了先进的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16626">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-363265f0e9dce52de669098fa229aab9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff07745f91837f88041e25ae53de21f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4be3fd2343b6225fc8e3f924d60f5dfa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e89b3a40405367f8408ae2a34c82635.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41f4e90603b828568e3af194b361b54e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Speech-Retrieval-Augmented-Generation-without-Automatic-Speech-Recognition"><a href="#Speech-Retrieval-Augmented-Generation-without-Automatic-Speech-Recognition" class="headerlink" title="Speech Retrieval-Augmented Generation without Automatic Speech   Recognition"></a>Speech Retrieval-Augmented Generation without Automatic Speech   Recognition</h2><p><strong>Authors:Do June Min, Karel Mundnich, Andy Lapastora, Erfan Soltanmohammadi, Srikanth Ronanki, Kyu Han</strong></p>
<p>One common approach for question answering over speech data is to first transcribe speech using automatic speech recognition (ASR) and then employ text-based retrieval-augmented generation (RAG) on the transcriptions. While this cascaded pipeline has proven effective in many practical settings, ASR errors can propagate to the retrieval and generation steps. To overcome this limitation, we introduce SpeechRAG, a novel framework designed for open-question answering over spoken data. Our proposed approach fine-tunes a pre-trained speech encoder into a speech adapter fed into a frozen large language model (LLM)–based retrieval model. By aligning the embedding spaces of text and speech, our speech retriever directly retrieves audio passages from text-based queries, leveraging the retrieval capacity of the frozen text retriever. Our retrieval experiments on spoken question answering datasets show that direct speech retrieval does not degrade over the text-based baseline, and outperforms the cascaded systems using ASR. For generation, we use a speech language model (SLM) as a generator, conditioned on audio passages rather than transcripts. Without fine-tuning of the SLM, this approach outperforms cascaded text-based models when there is high WER in the transcripts. </p>
<blockquote>
<p>针对语音数据的问答的一种常见方法首先是使用自动语音识别（ASR）进行语音转录，然后在转录上应用基于文本的检索增强生成（RAG）。虽然这种级联管道已在许多实际场景中证明是有效的，但ASR错误可能会传播到检索和生成步骤。为了克服这一局限性，我们引入了SpeechRAG，这是一个专为开放式问题回答口语数据设计的新框架。我们提出的方法对预训练的语音编码器进行微调，将其调整为适应冻结的大型语言模型（LLM）的语音适配器检索模型。通过对文本和语音的嵌入空间进行对齐，我们的语音检索器直接从基于文本的查询中检索音频片段，利用冻结的文本检索器的检索能力。我们在口语问答数据集上的检索实验表明，直接语音检索并不亚于基于文本的基线，并且表现优于使用ASR的级联系统。对于生成，我们使用语音语言模型（SLM）作为生成器，以音频片段为条件，而不是文本。在不微调SLM的情况下，当转录中的字词错误率（WER）较高时，这种方法的表现优于级联的基于文本模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16500v2">PDF</a> ICASSP 2025</p>
<p><strong>Summary</strong>：为提高语音识别（ASR）在问答系统中的性能，避免ASR错误对检索和生成步骤的影响，提出SpeechRAG框架。该框架通过微调预训练的语音编码器，将语音适配器输入到基于大型语言模型（LLM）的检索模型中，实现文本和语音嵌入空间的对齐。此语音检索器可直接从基于文本的查询中检索音频片段，利用冻结文本检索器的检索能力。实验表明，直接语音检索在口语问答数据集上的表现不亚于基于文本的基线方法，且在ASR使用级错误率较高时表现优于级联系统。对于生成部分，使用基于音频片段而非转录本的语音语言模型（SLM）作为生成器，无需对SLM进行微调即可超越级联的文本模型。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>SpeechRAG框架旨在提高在口语问答系统中的性能，解决ASR错误传播的问题。</li>
<li>通过微调预训练的语音编码器并输入到基于LLM的检索模型中，实现SpeechRAG框架。</li>
<li>SpeechRAG实现了文本和语音嵌入空间的直接对齐，允许直接检索音频片段。</li>
<li>实验表明，直接语音检索在口语问答数据集上的表现与基于文本的基线方法相当或更好。</li>
<li>在ASR存在高错误率的情况下，SpeechRAG在性能上超越了级联系统。</li>
<li>使用基于音频片段的SLM作为生成器，无需微调即可提高生成性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16500">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cee021b716efd502d2772cdd7e58990e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13a924273c299372d64abf8fbddee866.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee70a8bc71812a34552837322c47bc8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46a4c6ec08579c253d8ef68377e9820c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4eb1662aef874aa48fc2b5ed4c5a1da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e1cfcbcc67580c52fbe6fe9d1fa5714.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="WMCodec-End-to-End-Neural-Speech-Codec-with-Deep-Watermarking-for-Authenticity-Verification"><a href="#WMCodec-End-to-End-Neural-Speech-Codec-with-Deep-Watermarking-for-Authenticity-Verification" class="headerlink" title="WMCodec: End-to-End Neural Speech Codec with Deep Watermarking for   Authenticity Verification"></a>WMCodec: End-to-End Neural Speech Codec with Deep Watermarking for   Authenticity Verification</h2><p><strong>Authors:Junzuo Zhou, Jiangyan Yi, Yong Ren, Jianhua Tao, Tao Wang, Chu Yuan Zhang</strong></p>
<p>Recent advances in speech spoofing necessitate stronger verification mechanisms in neural speech codecs to ensure authenticity. Current methods embed numerical watermarks before compression and extract them from reconstructed speech for verification, but face limitations such as separate training processes for the watermark and codec, and insufficient cross-modal information integration, leading to reduced watermark imperceptibility, extraction accuracy, and capacity. To address these issues, we propose WMCodec, the first neural speech codec to jointly train compression-reconstruction and watermark embedding-extraction in an end-to-end manner, optimizing both imperceptibility and extractability of the watermark. Furthermore, We design an iterative Attention Imprint Unit (AIU) for deeper feature integration of watermark and speech, reducing the impact of quantization noise on the watermark. Experimental results show WMCodec outperforms AudioSeal with Encodec in most quality metrics for watermark imperceptibility and consistently exceeds both AudioSeal with Encodec and reinforced TraceableSpeech in extraction accuracy of watermark. At bandwidth of 6 kbps with a watermark capacity of 16 bps, WMCodec maintains over 99% extraction accuracy under common attacks, demonstrating strong robustness. </p>
<blockquote>
<p>近年来语音欺骗技术的进展要求在神经语音编解码器中采用更强大的验证机制，以确保语音的真实性。当前的方法在压缩之前嵌入数字水印，然后从重建的语音中提取水印进行验证，但面临水印和编解码器需要单独训练、跨模态信息融合不足等局限性，导致水印的不可感知性、提取准确性和容量降低。为了解决这些问题，我们提出了WMCodec，这是第一个联合训练压缩-重建和嵌入水印的端到端神经语音编解码器，以优化水印的不可感知性和可提取性。此外，我们设计了一个迭代式的注意力印记单元（AIU），用于更深层地整合水印和语音特征，减少量化噪声对水印的影响。实验结果表明，WMCodec在大多数质量指标上优于带有Encodec的AudioSeal的水印不可感知性，并且在提取水印的准确性方面始终超过带有Encodec的AudioSeal和增强的TraceableSpeech。在带宽为6kbps、水印容量为16bps的情况下，WMCodec在常见攻击下保持超过99%的提取准确率，显示出强大的稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.12121v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期语音伪装技术的进步对神经网络语音编解码器的身份验证机制提出了更高要求。为此提出了一种名为WMCodec的神经网络语音编解码器，首次将压缩重建和水印嵌入提取以端到端的方式进行联合训练，优化水印的不感知性和提取能力。设计了一种迭代注意力印记单元（AIU），用于更深层地整合水印和语音特征，减少量化噪声对水印的影响。实验结果表明，WMCodec在水印的不感知性和提取准确性方面优于AudioSeal和Encodec，并且在带宽为6kbps、水印容量为16bps的条件下，面对常见攻击仍能保持超过99%的提取准确性，展现出强大的稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音伪装技术的进步要求神经网络语音编解码器加强身份验证机制。</li>
<li>WMCodec是首个将压缩重建和水印嵌入提取联合训练的神经网络语音编解码器。</li>
<li>WMCodec优化了水印的不感知性和提取能力。</li>
<li>迭代注意力印记单元（AIU）用于更深层地整合水印和语音特征。</li>
<li>AIU设计减少了量化噪声对水印的影响。</li>
<li>实验表明WMCodec在水印不感知性和提取准确性方面优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.12121">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7f1911caacedf240e8ffd8012c838127.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55a6078de9ef1cc0e23733252039848e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21df97d751f03bd0c202c887d2ddff2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72313c0bc690078da71fe93b0cb97ebb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="M2R-Whisper-Multi-stage-and-Multi-scale-Retrieval-Augmentation-for-Enhancing-Whisper"><a href="#M2R-Whisper-Multi-stage-and-Multi-scale-Retrieval-Augmentation-for-Enhancing-Whisper" class="headerlink" title="M2R-Whisper: Multi-stage and Multi-scale Retrieval Augmentation for   Enhancing Whisper"></a>M2R-Whisper: Multi-stage and Multi-scale Retrieval Augmentation for   Enhancing Whisper</h2><p><strong>Authors:Jiaming Zhou, Shiwan Zhao, Jiabei He, Hui Wang, Wenjia Zeng, Yong Chen, Haoqin Sun, Aobo Kong, Yong Qin</strong></p>
<p>State-of-the-art models like OpenAI’s Whisper exhibit strong performance in multilingual automatic speech recognition (ASR), but they still face challenges in accurately recognizing diverse subdialects. In this paper, we propose M2R-whisper, a novel multi-stage and multi-scale retrieval augmentation approach designed to enhance ASR performance in low-resource settings. Building on the principles of in-context learning (ICL) and retrieval-augmented techniques, our method employs sentence-level ICL in the pre-processing stage to harness contextual information, while integrating token-level k-Nearest Neighbors (kNN) retrieval as a post-processing step to further refine the final output distribution. By synergistically combining sentence-level and token-level retrieval strategies, M2R-whisper effectively mitigates various types of recognition errors. Experiments conducted on Mandarin and subdialect datasets, including AISHELL-1 and KeSpeech, demonstrate substantial improvements in ASR accuracy, all achieved without any parameter updates. </p>
<blockquote>
<p>当前最先进的模型，如OpenAI的Whisper，在多语种自动语音识别（ASR）方面表现出强大的性能，但它们在识别多样的次方言方面仍面临挑战。在本文中，我们提出了M2R-whisper，这是一种新型的多阶段多尺度检索增强方法，旨在提高低资源环境下的ASR性能。我们的方法基于上下文学习（ICL）和检索增强技术，在预处理阶段采用句子级ICL来利用上下文信息，同时集成基于标记级的k最近邻（kNN）检索作为后处理步骤，以进一步优化最终输出分布。通过协同结合句子级和标记级检索策略，M2R-whisper有效地减轻了各种类型的识别错误。在包括AISHELL-1和KeSpeech在内的普通话和次方言数据集上进行的实验表明，ASR准确率有了显著提高，所有这些改进都没有涉及任何参数更新。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11889v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>M2R-whisper是一种针对低资源环境下语音识别性能提升的多阶段多尺度检索增强方法。该方法结合上下文学习（ICL）和检索增强技术，通过预处理阶段的句子级ICL利用上下文信息，以及后处理阶段的基于k近邻（kNN）的token级检索来进一步优化最终输出分布。该方法有效缓解了各种识别错误，在普通话和方言数据集上的实验显示语音识别准确率显著提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>M2R-whisper是一种针对多语种自动语音识别（ASR）的新颖方法，旨在提升低资源环境下的性能。</li>
<li>方法结合了上下文学习和检索增强技术。</li>
<li>通过预处理阶段的句子级上下文学习，有效利用上下文信息。</li>
<li>采用后处理阶段的基于k近邻（kNN）的token级检索，进一步优化最终输出分布。</li>
<li>M2R-whisper能有效缓解各种识别错误。</li>
<li>在普通话和方言数据集上的实验显示，ASR准确率显著提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11889">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fa6910ddb4db83529caacd970e15bff0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-213a5fb72883626f9098e0cfd082cb00.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-62fe17f43a8c8545694fa2482d74ec51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0ad7ce499f8ab419cca5e3545c90775.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38be93e10552c028e4dd8c6eaa9b2e10.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Stimulus-Modality-Matters-Impact-of-Perceptual-Evaluations-from-Different-Modalities-on-Speech-Emotion-Recognition-System-Performance"><a href="#Stimulus-Modality-Matters-Impact-of-Perceptual-Evaluations-from-Different-Modalities-on-Speech-Emotion-Recognition-System-Performance" class="headerlink" title="Stimulus Modality Matters: Impact of Perceptual Evaluations from   Different Modalities on Speech Emotion Recognition System Performance"></a>Stimulus Modality Matters: Impact of Perceptual Evaluations from   Different Modalities on Speech Emotion Recognition System Performance</h2><p><strong>Authors:Huang-Cheng Chou, Haibin Wu, Chi-Chun Lee</strong></p>
<p>Speech Emotion Recognition (SER) systems rely on speech input and emotional labels annotated by humans. However, various emotion databases collect perceptional evaluations in different ways. For instance, the IEMOCAP dataset uses video clips with sounds for annotators to provide their emotional perceptions. However, the most significant English emotion dataset, the MSP-PODCAST, only provides speech for raters to choose the emotional ratings. Nevertheless, using speech as input is the standard approach to training SER systems. Therefore, the open question is the emotional labels elicited by which scenarios are the most effective for training SER systems. We comprehensively compare the effectiveness of SER systems trained with labels elicited by different modality stimuli and evaluate the SER systems on various testing conditions. Also, we introduce an all-inclusive label that combines all labels elicited by various modalities. We show that using labels elicited by voice-only stimuli for training yields better performance on the test set, whereas labels elicited by voice-only stimuli. </p>
<blockquote>
<p>语音情感识别（SER）系统依赖于语音输入和人工标注的情感标签。然而，不同的情感数据库采用各种方式收集感知评估。例如，IEMOCAP数据集使用带有声音的视频剪辑供注释者提供他们的情感感知。然而，最大的英语情感数据集MSP-PODCAST只提供语音供评估者选择情感评分。尽管如此，使用语音作为输入是训练SER系统的标准方法。因此，公开的问题是哪种场景引发的情感标签对于训练SER系统最有效。我们全面比较了使用不同模式刺激引发的标签训练的SER系统的有效性，并在各种测试条件下对其进行了评估。此外，我们还引入了一个包容性标签，该标签结合了由各种模式引发的所有标签。我们的研究表明，使用仅由声音刺激引发的标签进行训练在测试集上取得了更好的性能，而由多种模态结合产生的标签性能并未达到最佳。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10762v2">PDF</a> 5 pages, 2 figures, 4 tables, acceptance for ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文主要探讨了语音情感识别（SER）系统的训练数据标注方式及其有效性。文章指出不同情感数据库通过不同的方式收集感知评价，如IEMOCAP数据集使用视频剪辑和声音供注释者提供情感感知，而最大的英语情感数据库MSP-PODCAST仅提供语音供评估者选择情感评分。文章全面比较了使用不同模态刺激产生的标签训练的SER系统的有效性，并在各种测试条件下对系统进行了评估。研究结果表明，使用仅由语音刺激产生的标签进行训练在测试集上表现更好。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音情感识别（SER）系统依赖于人类标注的语音输入和情感标签。</li>
<li>不同情感数据库采用不同方式收集感知评价，如IEMOCAP和MSP-PODCAST数据集。</li>
<li>使用不同模态刺激产生的标签训练的SER系统的有效性存在疑问。</li>
<li>全面比较了使用不同方式产生的标签训练的SER系统的性能。</li>
<li>引入了一个包含所有标签的全面标签，这些标签是由各种模态引发的。</li>
<li>研究表明，使用仅由语音刺激产生的标签进行训练在测试集上的表现更佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10762">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3b6e3d8b916931b364367d2b367b0935.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-583ec50dfdc5971e2aa93d4729bdebb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5666314e7d42ee3f344f7fe0e8cc385d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cd81282c9a9216723c67bcce787a3ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a300624bf4fea92ea46e5c74e874ffb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ddd6f569f52bf2038439c19c64d9eb9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f1ee6ac6c85a9d524ef4c64f367534e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SSR-Speech-Towards-Stable-Safe-and-Robust-Zero-shot-Text-based-Speech-Editing-and-Synthesis"><a href="#SSR-Speech-Towards-Stable-Safe-and-Robust-Zero-shot-Text-based-Speech-Editing-and-Synthesis" class="headerlink" title="SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech   Editing and Synthesis"></a>SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech   Editing and Synthesis</h2><p><strong>Authors:Helin Wang, Meng Yu, Jiarui Hai, Chen Chen, Yuchen Hu, Rilin Chen, Najim Dehak, Dong Yu</strong></p>
<p>In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released. </p>
<blockquote>
<p>在这篇论文中，我们介绍了SSR-Speech，这是一款为稳定、安全和稳健的零起点文本基础语音编辑和文本到语音合成而设计的神经编码自回归模型。SSR-Speech基于Transformer解码器构建，并采用了无分类器引导技术以增强生成过程的稳定性。我们提出了一种水印Encodec，可以将帧级水印嵌入语音的编辑区域，以便检测哪些部分被编辑过。此外，波形重建利用了原始未编辑的语音片段，与Encodec模型相比，提供了更出色的恢复效果。我们的方法在现实编辑语音任务（RealEdit）和LibriTTS文本到语音任务中取得了最先进的性能表现，超越了以前的方法。此外，SSR-Speech在多跨度语音编辑方面表现出色，并且对背景声音表现出惊人的稳健性。源代码和演示已经发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07556v2">PDF</a> ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了SSR-Speech，一种用于稳定、安全和稳健的零样本文本语音编辑和文本到语音合成的神经编码自回归模型。SSR-Speech基于Transformer解码器构建，采用无分类器引导提高生成过程的稳定性。提出了水印Encodec，可将帧级水印嵌入语音的编辑区域，以检测哪些部分被编辑过。此外，波形重建利用原始未编辑的语音片段，提供了比Encodec模型更出色的恢复效果。SSR-Speech在RealEdit语音编辑任务和LibriTTS文本到语音任务上达到了最新技术性能水平，超越了以前的方法。此外，SSR-Speech在多跨度语音编辑方面表现出色，并且对背景声音具有惊人的稳健性。已发布源代码和演示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SSR-Speech是一种神经编码自回归模型，用于稳定、安全和稳健的零样本文本语音编辑和文本到语音合成。</li>
<li>SSR-Speech采用Transformer解码器和无分类器引导增强生成稳定性。</li>
<li>SSR-Speech提出了水印Encodec技术，能够检测语音中哪些部分被编辑过。</li>
<li>波形重建利用原始未编辑的语音片段，实现优质恢复。</li>
<li>SSR-Speech在RealEdit语音编辑任务和LibriTTS文本到语音任务上表现优异，超越先前方法。</li>
<li>SSR-Speech擅长多跨度语音编辑。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.07556">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d3191d01fe79dfd4a84375b2886105e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c26a34128e094bf7345ad8214e7bb33c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-62a38d303f5dcbc3e98ba6e6438ffab2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d8e273cfd1c9d9903109b5c111bf90f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7af88c7005e8487d83ca840eb2b5c5e3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Chain-of-Translation-Prompting-CoTR-A-Novel-Prompting-Technique-for-Low-Resource-Languages"><a href="#Chain-of-Translation-Prompting-CoTR-A-Novel-Prompting-Technique-for-Low-Resource-Languages" class="headerlink" title="Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for   Low Resource Languages"></a>Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for   Low Resource Languages</h2><p><strong>Authors:Tejas Deshpande, Nidhi Kowtal, Raviraj Joshi</strong></p>
<p>This paper introduces Chain of Translation Prompting (CoTR), a novel strategy designed to enhance the performance of language models in low-resource languages. CoTR restructures prompts to first translate the input context from a low-resource language into a higher-resource language, such as English. The specified task like generation, classification, or any other NLP function is then performed on the translated text, with the option to translate the output back to the original language if needed. All these steps are specified in a single prompt. We demonstrate the effectiveness of this method through a case study on the low-resource Indic language Marathi. The CoTR strategy is applied to various tasks, including sentiment analysis, hate speech classification, subject classification and text generation, and its efficacy is showcased by comparing it with regular prompting methods. Our results underscore the potential of translation-based prompting strategies to significantly improve multilingual LLM performance in low-resource languages, offering valuable insights for future research and applications. We specifically see the highest accuracy improvements with the hate speech detection task. The technique also has the potential to enhance the quality of synthetic data generation for underrepresented languages using LLMs. </p>
<blockquote>
<p>本文介绍了翻译链提示（CoTR）这一新型策略，旨在提高低资源语言的语言模型性能。CoTR重新构建提示，首先将输入上下文从低资源语言翻译到资源更丰富的语言，如英语。然后在翻译后的文本上执行指定任务，如生成、分类或其他任何NLP功能，如果需要将输出翻译回原始语言。所有这些步骤都在一个提示中指定。我们通过针对低资源印度语言马拉地语进行案例研究，展示了该方法的有效性。CoTR策略应用于各种任务，包括情感分析、仇恨言论分类、主题分类和文本生成，通过与常规提示方法的比较，展示了其有效性。我们的结果强调基于翻译的提示策略在提高低资源语言的多语种大型语言模型性能方面的潜力，为未来研究和应用提供了宝贵的见解。我们特别看到仇恨言论检测任务获得了最高的精度提升。该技术还具有提高使用大型语言模型对代表性不足的语言合成数据生成质量的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.04512v2">PDF</a> Accepted at PACLIC 38 (2024)</p>
<p><strong>Summary</strong><br>此论文提出一种名为“翻译链提示”（CoTR）的新策略，旨在提升低资源语言的语言模型的性能。CoTR通过重新构建提示，先将输入内容从低资源语言翻译到资源丰富、如英语这样的语言，然后在翻译后的文本上执行指定任务，如生成、分类或其他NLP功能。如果需要，还可以将输出翻译回原始语言。所有这些步骤都在一个提示中指定。我们通过在马拉地语这一低资源印度语言上进行的案例研究，证明了该方法的实用性。此策略适用于各种任务，包括情感分析、仇恨言论分类、主题分类和文本生成等。我们的研究结果显示翻译提示策略具有显著改善低资源语言的多语种大型语言模型性能的潜力，为未来的研究和应用提供了有价值的见解。特别地，仇恨言论检测任务的效果提升最为显著。此技术还有助于提高使用大型语言模型对代表性不足的语种合成数据生成的品质。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoTR是一种新型策略，旨在增强低资源语言的语言模型性能。</li>
<li>CoTR通过翻译输入内容从低资源语言到资源丰富语言来提升语言模型的性能。</li>
<li>此策略适用于各种NLP任务，包括情感分析、仇恨言论分类、主题分类和文本生成等。</li>
<li>CoTR在马拉地语这一低资源印度语言上的案例研究证明了其有效性。</li>
<li>与常规提示方法相比，CoTR策略显著提高仇恨言论检测任务的准确性。</li>
<li>CoTR具有改善代表性不足的语种合成数据生成的品质潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.04512">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1d3451b9a5fa7c088ad12a25c5cea621.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fef1d8e1bd8204f072124f1255b87217.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f4d0a538d5e2b27b0d82eb96865eba4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2316c49207c21f97b357b569372a9389.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d72ca94203ef4cfcfab460a86990b86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fcf4794ac0fc91b7efd17f9343f2849.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Real-time-Speech-Enhancement-on-Raw-Signals-with-Deep-State-space-Modeling"><a href="#Real-time-Speech-Enhancement-on-Raw-Signals-with-Deep-State-space-Modeling" class="headerlink" title="Real-time Speech Enhancement on Raw Signals with Deep State-space   Modeling"></a>Real-time Speech Enhancement on Raw Signals with Deep State-space   Modeling</h2><p><strong>Authors:Yan Ru Pei, Ritik Shrivastava, FNU Sidharth</strong></p>
<p>We present aTENNuate, a simple deep state-space autoencoder configured for efficient online raw speech enhancement in an end-to-end fashion. The network’s performance is primarily evaluated on raw speech denoising, with additional assessments on tasks such as super-resolution and de-quantization. We benchmark aTENNuate on the VoiceBank + DEMAND and the Microsoft DNS1 synthetic test sets. The network outperforms previous real-time denoising models in terms of PESQ score, parameter count, MACs, and latency. Even as a raw waveform processing model, the model maintains high fidelity to the clean signal with minimal audible artifacts. In addition, the model remains performant even when the noisy input is compressed down to 4000Hz and 4 bits, suggesting general speech enhancement capabilities in low-resource environments. Code is available at github.com&#x2F;Brainchip-Inc&#x2F;aTENNuate </p>
<blockquote>
<p>我们提出了aTENNuate，这是一个简单的深度状态空间自编码器，以端到端的方式配置，用于高效的在线原始语音增强。该网络的主要评估是基于原始语音去噪，还包括超分辨率和去量化等任务的评估。我们在VoiceBank + DEMAND和Microsoft DNS1合成测试集上对aTENNuate进行了基准测试。该网络在PESQ得分、参数计数、MACs和延迟方面优于以前的实时去噪模型。即使作为原始波形处理模型，该模型对清洁信号的保真度也很高，几乎听不到杂音。此外，即使嘈杂的输入压缩到4000Hz和4位时，该模型仍然性能卓越，表明其在低资源环境中具有一般的语音增强能力。代码可在github.com&#x2F;Brainchip-Inc&#x2F;aTENNuate找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.03377v3">PDF</a> </p>
<p><strong>总结</strong><br>    本文介绍了aTENNuate，一个简洁的深度状态空间自编码器，用于在线原始语音增强，以端到端的方式进行配置。该网络在原始语音降噪方面进行了主要评估，并对超分辨率和去量化等任务进行了额外评估。在VoiceBank + DEMAND和Microsoft DNS1合成测试集上，aTENNuate的基准测试表明其在PESQ得分、参数计数、乘积累操作和延迟等方面优于现有的实时降噪模型。即使作为原始波形处理模型，该模型对清洁信号的保真度依然很高，几乎无听觉失真。此外，即使在嘈杂输入被压缩至4000Hz和4比特的情况下，该模型依然表现优异，显示其在低资源环境中具有一般的语音增强能力。代码可在github.com&#x2F;Brainchip-Inc&#x2F;aTENNuate找到。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>aTENNuate是一个深度状态空间自编码器，用于在线原始语音增强。</li>
<li>该网络在原始语音降噪方面有出色的表现。</li>
<li>aTENNuate在VoiceBank + DEMAND和Microsoft DNS1合成测试集上的基准测试优于其他实时降噪模型。</li>
<li>aTENNuate在保持高保真清洁信号的同时，几乎无听觉失真。</li>
<li>aTENNuate能够在低资源环境中表现出良好的语音增强能力。</li>
<li>aTENNuate具备处理压缩语音的能力，即使输入被压缩至4000Hz和4比特，依然能够保持优良性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.03377">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-be178251c5e274bff76c6c8b49b7c4d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19518325334857b2b8c3a912111f6187.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6679b7f52ca2dad45354b44a66e3022.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc7f8be9729e298e0b9a9fdd190e7c6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aeb9e8759edb3cf3edffe609827ede88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e417c2f8518db1dbc219c62d9d4662da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47726c6c81a64d78c9e42732bcdc076e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Multimodal-Human-Autonomous-Agents-Interaction-Using-Pre-Trained-Language-and-Visual-Foundation-Models"><a href="#Multimodal-Human-Autonomous-Agents-Interaction-Using-Pre-Trained-Language-and-Visual-Foundation-Models" class="headerlink" title="Multimodal Human-Autonomous Agents Interaction Using Pre-Trained   Language and Visual Foundation Models"></a>Multimodal Human-Autonomous Agents Interaction Using Pre-Trained   Language and Visual Foundation Models</h2><p><strong>Authors:Linus Nwankwo, Elmar Rueckert</strong></p>
<p>In this paper, we extended the method proposed in [21] to enable humans to interact naturally with autonomous agents through vocal and textual conversations. Our extended method exploits the inherent capabilities of pre-trained large language models (LLMs), multimodal visual language models (VLMs), and speech recognition (SR) models to decode the high-level natural language conversations and semantic understanding of the robot’s task environment, and abstract them to the robot’s actionable commands or queries. We performed a quantitative evaluation of our framework’s natural vocal conversation understanding with participants from different racial backgrounds and English language accents. The participants interacted with the robot using both spoken and textual instructional commands. Based on the logged interaction data, our framework achieved 87.55% vocal commands decoding accuracy, 86.27% commands execution success, and an average latency of 0.89 seconds from receiving the participants’ vocal chat commands to initiating the robot’s actual physical action. The video demonstrations of this paper can be found at <a target="_blank" rel="noopener" href="https://linusnep.github.io/MTCC-IRoNL/">https://linusnep.github.io/MTCC-IRoNL/</a>. </p>
<blockquote>
<p>在这篇论文中，我们将[21]中提出的方法进行扩展，使人类能够通过语音和文本对话与自主代理进行自然交互。我们的扩展方法利用预训练的大型语言模型（LLM）、多模态视觉语言模型（VLM）和语音识别（SR）模型的固有功能，解码高级自然语言对话和机器人任务环境的语义理解，并将其抽象化为机器人的可操作命令或查询。我们对框架的自然语音对话理解进行了定量评估，参与者来自不同的种族背景和英语口音。参与者使用口语和文本指令命令与机器人进行交互。根据记录的交互数据，我们的框架实现了87.55%的语音命令解码准确率、86.27%的命令执行成功率，从接收参与者的语音聊天命令到启动机器人的实际物理动作的平均延迟时间为0.89秒。本论文的视频演示可在<a target="_blank" rel="noopener" href="https://linusnep.github.io/MTCC-IRoNL/%E6%89%BE%E5%88%B0%E3%80%82">https://linusnep.github.io/MTCC-IRoNL/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.12273v2">PDF</a> </p>
<p><strong>Summary</strong><br>本论文扩展了方法[21]，使人类能够通过语音和文本与自主智能体进行自然交互。该研究利用预训练的大型语言模型、多模态视觉语言模型和语音识别模型的固有功能，解码高级自然语言对话，理解机器人的任务环境语义，并将其抽象为机器人的可执行命令或查询。研究人员对不同种族背景和英语口音的参与者进行了定量分析评估。结果显示，参与者在以口语和文本形式与机器人互动时，我们的框架实现了高达87.55%的语音命令解码准确率、86.27%的命令执行成功率，以及从接收参与者语音聊天命令到启动机器人实际物理动作的平均延迟仅为0.89秒。视频演示内容可通过链接观看：[<a target="_blank" rel="noopener" href="https://linusnep.github.io/MTCC-IRoNL/]%E8%BF%9B%E8%A1%8C%E6%9F%A5%E7%9C%8B%E3%80%82">https://linusnep.github.io/MTCC-IRoNL/]进行查看。</a> </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>研究扩展了方法[21]，将人类自然交流能力赋予了与自主智能体的互动中。</li>
<li>框架采用了预训练的大型语言模型、多模态视觉语言模型和语音识别模型等技术。</li>
<li>通过定量分析评估，证明了该框架在处理口语指令时的高效性，达到了87.55%的语音命令解码准确率以及命令执行成功率达到了86.27%。 </li>
<li>平均延迟时间仅为0.89秒，响应迅速。 </li>
<li>该框架成功将复杂的人类交流信息转化为机器人的实际动作指令。 </li>
<li>实验覆盖了不同种族背景和英语口音的参与者，展现了系统的广泛适应性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.12273">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f3a97419e55049fd1cbc10d7c7fb26d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7a91e48e51d6f8a2aaaf953f7aff5eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50f24188c445ad15e0ea6b67806fc889.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Face-StyleSpeech-Enhancing-Zero-shot-Speech-Synthesis-from-Face-Images-with-Improved-Face-to-Speech-Mapping"><a href="#Face-StyleSpeech-Enhancing-Zero-shot-Speech-Synthesis-from-Face-Images-with-Improved-Face-to-Speech-Mapping" class="headerlink" title="Face-StyleSpeech: Enhancing Zero-shot Speech Synthesis from Face Images   with Improved Face-to-Speech Mapping"></a>Face-StyleSpeech: Enhancing Zero-shot Speech Synthesis from Face Images   with Improved Face-to-Speech Mapping</h2><p><strong>Authors:Minki Kang, Wooseok Han, Eunho Yang</strong></p>
<p>Generating speech from a face image is crucial for developing virtual humans capable of interacting using their unique voices, without relying on pre-recorded human speech. In this paper, we propose Face-StyleSpeech, a zero-shot Text-To-Speech (TTS) synthesis model that generates natural speech conditioned on a face image rather than reference speech. We hypothesize that learning entire prosodic features from a face image poses a significant challenge. To address this, our TTS model incorporates both face and prosody encoders. The prosody encoder is specifically designed to model speech style characteristics that are not fully captured by the face image, allowing the face encoder to focus on extracting speaker-specific features such as timbre. Experimental results demonstrate that Face-StyleSpeech effectively generates more natural speech from a face image than baselines, even for unseen faces. Samples are available on our demo page. </p>
<blockquote>
<p>从面部图像生成语音对于开发能够使用其独特声音进行交互的虚拟人类至关重要，而无需依赖预先录制的人类语音。在本文中，我们提出了Face-StyleSpeech，这是一种零样本文本到语音（TTS）合成模型，它根据面部图像而不是参考语音生成自然语音。我们假设从面部图像学习整个韵律特征是一个巨大的挑战。为解决这一问题，我们的TTS模型结合了面部和韵律编码器。韵律编码器专门用于建模语音风格特征，这些特征不能完全从面部图像中获取，从而允许面部编码器专注于提取发音人特定特征，如音色。实验结果表明，Face-StyleSpeech能够从面部图像生成比基线更自然的语音，即使对于未见过的面孔也是如此。样本可在我们的演示页面上找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05844v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>基于面部图像生成语音对于开发能够使用其独特声音进行交互的虚拟人类至关重要。本文提出Face-StyleSpeech，一种零样本文本到语音（TTS）合成模型，该模型根据面部图像生成自然语音，而非依赖预录制的语音。为应对从面部图像学习整体韵律特征的挑战，我们的TTS模型结合了面部和韵律编码器。韵律编码器专门设计用于模拟面部图像无法完全捕捉的语音风格特征，使得面部编码器能够专注于提取说话人的特征，如音质。实验结果表明，Face-StyleSpeech能够更有效地根据面部图像生成更自然的语音，超越基线模型，甚至对于未见过的面孔也是如此。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Face-StyleSpeech是一种零样本TTS合成模型，能够根据面部图像生成自然语音。</li>
<li>模型结合面部和韵律编码器，以应对从面部图像学习韵律特征的挑战。</li>
<li>韵律编码器用于模拟面部图像无法完全捕捉的语音风格特征。</li>
<li>面部编码器专注于提取说话人的特征，如音质。</li>
<li>实验结果显示，Face-StyleSpeech在生成基于面部图像的语音方面表现优越，超越基线模型。</li>
<li>该模型生成的语音对于未见过的面孔同样有效。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.05844">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-83c43903c2a0bd6db5298f5d1fb98ba9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e7217e939336e06d64fe9d2e07ada4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-094ccae6af75cd56e466c323aea2fc39.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-04/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-04/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-04/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ef101938f9b717618e7f6fcec2d2ad11.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-01-04  Double-Flow GAN model for the reconstruction of perceived faces from   brain activities
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-04/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-56c26153e24a1b3b9dad338de57ec23e.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-01-04  Fine-grained Image-to-LiDAR Contrastive Distillation with Visual   Foundation Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17665k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
