<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-01-04  SegKAN High-Resolution Medical Image Segmentation with Long-Distance   Dependencies">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-010366cd48b4598369b9130678ecc0b8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    33 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-04-更新"><a href="#2025-01-04-更新" class="headerlink" title="2025-01-04 更新"></a>2025-01-04 更新</h1><h2 id="SegKAN-High-Resolution-Medical-Image-Segmentation-with-Long-Distance-Dependencies"><a href="#SegKAN-High-Resolution-Medical-Image-Segmentation-with-Long-Distance-Dependencies" class="headerlink" title="SegKAN: High-Resolution Medical Image Segmentation with Long-Distance   Dependencies"></a>SegKAN: High-Resolution Medical Image Segmentation with Long-Distance   Dependencies</h2><p><strong>Authors:Shengbo Tan, Rundong Xue, Shipeng Luo, Zeyu Zhang, Xinran Wang, Lei Zhang, Daji Ergu, Zhang Yi, Yang Zhao, Ying Cai</strong></p>
<p>Hepatic vessels in computed tomography scans often suffer from image fragmentation and noise interference, making it difficult to maintain vessel integrity and posing significant challenges for vessel segmentation. To address this issue, we propose an innovative model: SegKAN. First, we improve the conventional embedding module by adopting a novel convolutional network structure for image embedding, which smooths out image noise and prevents issues such as gradient explosion in subsequent stages. Next, we transform the spatial relationships between Patch blocks into temporal relationships to solve the problem of capturing positional relationships between Patch blocks in traditional Vision Transformer models. We conducted experiments on a Hepatic vessel dataset, and compared to the existing state-of-the-art model, the Dice score improved by 1.78%. These results demonstrate that the proposed new structure effectively enhances the segmentation performance of high-resolution extended objects. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/goblin327/SegKAN">https://github.com/goblin327/SegKAN</a> </p>
<blockquote>
<p>在计算机断层扫描中，肝脏血管经常受到图像碎片和噪声干扰的影响，这使得保持血管的完整性变得困难，并为血管分割带来了重大挑战。为了解决这一问题，我们提出了一种创新模型：SegKAN。首先，我们改进了传统的嵌入模块，采用了一种新型的卷积网络结构进行图像嵌入，该结构可以平滑图像噪声，防止后续阶段的梯度爆炸等问题。其次，我们将Patch块之间的空间关系转换为时间关系，解决了传统视觉Transformer模型中捕获Patch块之间位置关系的问题。我们在肝脏血管数据集上进行了实验，与现有最先进的模型相比，Dice得分提高了1.78%。这些结果表明，所提出的新结构有效地提高了高分辨率扩展对象的分割性能。代码将在<a target="_blank" rel="noopener" href="https://github.com/goblin327/SegKAN%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/goblin327/SegKAN上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19990v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的模型SegKAN，用于解决肝脏血管在CT扫描中的图像碎片化及噪声干扰问题，从而提高血管分割的准确性。SegKAN通过改进传统嵌入模块并采用新型卷积网络结构进行图像嵌入，平滑图像噪声，并在后续阶段防止梯度爆炸等问题。此外，SegKAN转换Patch块之间的空间关系为时间关系，解决了传统视觉转换器模型中捕获Patch块之间位置关系的问题。实验结果显示，与现有最先进的模型相比，SegKAN在肝脏血管数据集上的Dice得分提高了1.78%，表明该新结构有效提高了高分辨率扩展对象的分割性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SegKAN模型被提出以解决肝脏血管在CT扫描中的图像碎片化及噪声干扰问题。</li>
<li>SegKAN改进了传统嵌入模块，采用新型卷积网络结构进行图像嵌入，以平滑图像噪声并防止梯度爆炸。</li>
<li>SegKAN转换Patch块之间的空间关系为时间关系，以改善传统视觉转换器模型在捕获Patch块位置关系方面的问题。</li>
<li>实验结果显SegKAN在肝脏血管数据集上的性能优于现有最先进的模型，Dice得分提高了1.78%。</li>
<li>SegKAN能有效提高高分辨率扩展对象的分割性能。</li>
<li>SegKAN的代码将公开在<a target="_blank" rel="noopener" href="https://github.com/goblin327/SegKAN%E3%80%82">https://github.com/goblin327/SegKAN。</a></li>
<li>该模型的应用将有助于提高医学图像分析中血管分割的准确性和效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19990">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d6b1a2405d1829fc4b885098a2e6d2f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59bebfc6cd0ecc3a3e51b947d04010be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a761f6d887fc783a98a5498eb1679b12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22e687a0a0b7bbbbfbd1d2585c12b1f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbbe7e42d3a1a8be20aad442c6fb679a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4eb5aaa534fbddcd167b97673c8bf012.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8253e8b9bf8a65ee53ccb4ff55df2d6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9126be62539a85dc67029c28d3fb700.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UniMo-Universal-Motion-Correction-For-Medical-Images-without-Network-Retraining"><a href="#UniMo-Universal-Motion-Correction-For-Medical-Images-without-Network-Retraining" class="headerlink" title="UniMo: Universal Motion Correction For Medical Images without Network   Retraining"></a>UniMo: Universal Motion Correction For Medical Images without Network   Retraining</h2><p><strong>Authors:Jian Wang, Razieh Faghihpirayesh, Danny Joca, Polina Golland, Ali Gholipour</strong></p>
<p>In this paper, we introduce a Universal Motion Correction (UniMo) framework, leveraging deep neural networks to tackle the challenges of motion correction across diverse imaging modalities. Our approach employs advanced neural network architectures with equivariant filters, overcoming the limitations of current models that require iterative inference or retraining for new image modalities. UniMo enables one-time training on a single modality while maintaining high stability and adaptability for inference across multiple unseen image modalities. We developed a joint learning framework that integrates multimodal knowledge from both shape and images that faithfully improve motion correction accuracy despite image appearance variations. UniMo features a geometric deformation augmenter that enhances the robustness of global motion correction by addressing any local deformations whether they are caused by object deformations or geometric distortions, and also generates augmented data to improve the training process. Our experimental results, conducted on various datasets with four different image modalities, demonstrate that UniMo surpasses existing motion correction methods in terms of accuracy. By offering a comprehensive solution to motion correction, UniMo marks a significant advancement in medical imaging, especially in challenging applications with wide ranges of motion, such as fetal imaging. The code for this work is available online, <a target="_blank" rel="noopener" href="https://github.com/IntelligentImaging/UNIMO/">https://github.com/IntelligentImaging/UNIMO/</a>. </p>
<blockquote>
<p>本文介绍了一个通用运动校正（UniMo）框架，它利用深度神经网络来解决跨不同成像模式运动校正的挑战。我们的方法采用先进的神经网络架构和等价滤波器，克服了当前模型需要迭代推理或针对新图像模式进行再训练的局限性。UniMo实现了单一模态的一次性训练，同时保持了跨多个未见图像模式推理的高稳定性和适应性。我们开发了一个联合学习框架，该框架结合了形状和图像的跨模态知识，尽管图像外观发生变化，也能忠实地提高运动校正的准确性。UniMo具有几何变形增强器，它通过解决局部变形（无论是由于对象变形还是几何失真引起的）增强了全局运动校正的稳健性，并且还生成了增强数据以改进训练过程。我们的实验结果是在具有四种不同图像模式的各种数据集上进行的，证明了UniMo在准确性上超越了现有的运动校正方法。通过为运动校正提供全面解决方案，UniMo在医学成像领域取得了重大进展，特别是在具有大范围运动的挑战性应用中，如胎儿成像。该工作的代码可在网上找到：<a target="_blank" rel="noopener" href="https://github.com/IntelligentImaging/UNIMO/%E3%80%82">https://github.com/IntelligentImaging/UNIMO/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14204v2">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种利用深度神经网络解决多种成像模式运动校正问题的通用运动校正（UniMo）框架。该框架采用先进的神经网络架构和等价滤波器，克服了当前模型需要迭代推理或针对新图像模式进行再训练的局限性。UniMo框架能够在单一模态上进行一次性训练，同时保持对多种未见图像模式的高稳定性和适应性。通过整合形状和图像的跨模态知识，UniMo提高了运动校正的准确性，即使在图像外观变化的情况下也是如此。此外，UniMo还包含一个几何变形增强器，该增强器提高了全局运动校正的稳健性，解决了局部变形问题，并生成额外的数据以改进训练过程。在多种数据集和四种不同图像模式上的实验结果表明，UniMo在准确性上超越了现有的运动校正方法。它为运动校正提供了一个全面的解决方案，标志着医学影像技术的一大进步，特别是在大范围运动的挑战性应用中，如胎儿成像。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniMo框架利用深度神经网络解决了多种成像模式的运动校正问题。</li>
<li>该框架采用先进的神经网络架构和等价滤波器，具有高度的稳定性和适应性。</li>
<li>UniMo实现了在单一模态上的一次性训练，并能够在多种未见图像模式上进行推理。</li>
<li>通过整合形状和图像的跨模态知识，UniMo提高了运动校正的准确性。</li>
<li>UniMo包含一个几何变形增强器，可解决局部变形问题并增强全局运动校正的稳健性。</li>
<li>UniMo生成额外的数据以改进训练过程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.14204">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5c0fffce8bfe31d48cccf92e5023c751.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5031c7b28532f66893b9321655cf3e75.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Boosting-Memory-Efficiency-in-Transfer-Learning-for-High-Resolution-Medical-Image-Classification"><a href="#Boosting-Memory-Efficiency-in-Transfer-Learning-for-High-Resolution-Medical-Image-Classification" class="headerlink" title="Boosting Memory Efficiency in Transfer Learning for High-Resolution   Medical Image Classification"></a>Boosting Memory Efficiency in Transfer Learning for High-Resolution   Medical Image Classification</h2><p><strong>Authors:Yijin Huang, Pujin Cheng, Roger Tam, Xiaoying Tang</strong></p>
<p>The success of large-scale pre-trained models has established fine-tuning as a standard method for achieving significant improvements in downstream tasks. However, fine-tuning the entire parameter set of a pre-trained model is costly. Parameter-efficient transfer learning (PETL) has recently emerged as a cost-effective alternative for adapting pre-trained models to downstream tasks. Despite its advantages, the increasing model size and input resolution present challenges for PETL, as the training memory consumption is not reduced as effectively as the parameter usage. In this paper, we introduce Fine-grained Prompt Tuning plus (FPT+), a PETL method designed for high-resolution medical image classification, which significantly reduces the training memory consumption compared to other PETL methods. FPT+ performs transfer learning by training a lightweight side network and accessing pre-trained knowledge from a large pre-trained model (LPM) through fine-grained prompts and fusion modules. Specifically, we freeze the LPM of interest and construct a learnable lightweight side network. The frozen LPM processes high-resolution images to extract fine-grained features, while the side network employs corresponding down-sampled low-resolution images to minimize the memory usage. To enable the side network to leverage pre-trained knowledge, we propose fine-grained prompts and fusion modules, which collaborate to summarize information through the LPM’s intermediate activations. We evaluate FPT+ on eight medical image datasets of varying sizes, modalities, and complexities. Experimental results demonstrate that FPT+ outperforms other PETL methods, using only 1.03% of the learnable parameters and 3.18% of the memory required for fine-tuning an entire ViT-B model. Our code is available <a target="_blank" rel="noopener" href="https://github.com/YijinHuang/FPT">https://github.com/YijinHuang/FPT</a>. </p>
<blockquote>
<p>大规模预训练模型的成功使得微调成为在下游任务中实现显著改进的标准方法。然而，对预训练模型进行整体参数微调的成本很高。近期，参数高效迁移学习（PETL）作为一种经济实惠的替代方案应运而生，用于将预训练模型适应到下游任务。尽管有其优势，但随着模型大小和输入分辨率的增加，PETL面临挑战，因为训练内存消耗并没有像参数使用那样有效地减少。在本文中，我们介绍了精细提示微调加（FPT+），这是一种针对高分辨率医学图像分类设计的PETL方法，与其他PETL方法相比，它显著减少了训练内存消耗。FPT+通过训练一个轻量级侧网络并通过精细提示和融合模块访问大型预训练模型（LPM）的预训练知识，从而进行迁移学习。具体来说，我们冻结感兴趣的LPM并构建一个可学习的轻量级侧网络。冻结的LPM处理高分辨率图像以提取精细特征，而侧网络则使用相应的下采样低分辨率图像来最小化内存使用。为了允许侧网络利用预训练知识，我们提出了精细提示和融合模块，它们通过LPM的中间激活来协作汇总信息。我们在八个不同大小、模态和复杂度的医学图像数据集上评估了FPT+。实验结果表明，FPT+在其他PETL方法上表现出色，仅使用1.03%的可学习参数和3.18%的记忆，用于微调整个ViT-B模型。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/YijinHuang/FPT">https://github.com/YijinHuang/FPT</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.02426v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>大型预训练模型的巨大成功确立了微调作为实现下游任务显著改进的标准方法。然而，微调预训练模型的所有参数是成本高昂的。参数高效迁移学习（PETL）最近作为一种经济实惠的预训练模型适应下游任务的替代方法而兴起。尽管有其优点，但随着模型大小和输入分辨率的增加，PETL面临挑战，因为训练内存消耗并没有像参数使用那样有效地减少。本文介绍了用于高分辨率医学图像分类的精细提示微调加强版（FPT+），这是一种PETL方法，与其他PETL方法相比，它显著减少了训练内存消耗。FPT+通过训练轻量级侧网络并通过对齐预训练知识来实现迁移学习，对齐通过精细提示和融合模块进行。具体来说，我们冻结感兴趣的LPM并构建了一个可学习的轻量级侧网络。冻结的LPM处理高分辨率图像以提取精细特征，而侧网络则使用相应的降采样低分辨率图像来最小化内存使用。为了使侧网络能够利用预训练知识，我们提出了精细提示和融合模块，它们通过LPM的中间激活来协作总结信息。我们在八个医学图像数据集上评估了FPT+，这些数据集大小、模态和复杂性各不相同。实验结果表明，FPT+在只使用0.79%的参数和较小内存需求的情况下，即可超越其他PETL方法实现性能表现，对ViT-B模型的微调仅需要3.18%的内存。我们的代码可通过<a target="_blank" rel="noopener" href="https://github.com/YijinHuang/FPT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/YijinHuang/FPT获取。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>大规模预训练模型的成功确立了微调作为改进下游任务的标准方法，但微调整个参数集成本高昂。</li>
<li>参数高效迁移学习（PETL）作为适应预训练模型到下游任务的替代方法具有优势。</li>
<li>FPT+是一种针对高分辨医学图像分类的PETL方法，显著减少训练内存消耗。</li>
<li>FPT+通过训练轻量级侧网络并利用预训练知识来实现迁移学习，这些知识通过精细提示和融合模块获取。</li>
<li>冻结的LPM处理高分辨率图像以提取精细特征，而侧网络使用低分辨率图像优化内存使用。</li>
<li>精细提示和融合模块使侧网络能够利用预训练知识。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.02426">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ff28fb686523cad5ad087bb758ef4d4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7555b2402f0e0974cb6ae076b151ee0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05246585bf1e9f016fb075db7fef9458.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2de23a43ad24e8aef9af022ada7cf547.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Real-World-Federated-Learning-with-a-Knowledge-Distilled-Transformer-for-Cardiac-CT-Imaging"><a href="#Real-World-Federated-Learning-with-a-Knowledge-Distilled-Transformer-for-Cardiac-CT-Imaging" class="headerlink" title="Real World Federated Learning with a Knowledge Distilled Transformer for   Cardiac CT Imaging"></a>Real World Federated Learning with a Knowledge Distilled Transformer for   Cardiac CT Imaging</h2><p><strong>Authors:Malte Tölle, Philipp Garthe, Clemens Scherer, Jan Moritz Seliger, Andreas Leha, Nina Krüger, Stefan Simm, Simon Martin, Sebastian Eble, Halvar Kelm, Moritz Bednorz, Florian André, Peter Bannas, Gerhard Diller, Norbert Frey, Stefan Groß, Anja Hennemuth, Lars Kaderali, Alexander Meyer, Eike Nagel, Stefan Orwat, Moritz Seiffert, Tim Friede, Tim Seidler, Sandy Engelhardt</strong></p>
<p>Federated learning is a renowned technique for utilizing decentralized data while preserving privacy. However, real-world applications often face challenges like partially labeled datasets, where only a few locations have certain expert annotations, leaving large portions of unlabeled data unused. Leveraging these could enhance transformer architectures ability in regimes with small and diversely annotated sets. We conduct the largest federated cardiac CT analysis to date (n&#x3D;8,104) in a real-world setting across eight hospitals. Our two-step semi-supervised strategy distills knowledge from task-specific CNNs into a transformer. First, CNNs predict on unlabeled data per label type and then the transformer learns from these predictions with label-specific heads. This improves predictive accuracy and enables simultaneous learning of all partial labels across the federation, and outperforms UNet-based models in generalizability on downstream tasks. Code and model weights are made openly available for leveraging future cardiac CT analysis. </p>
<blockquote>
<p>联邦学习是利用分散数据同时保护隐私的知名技术。然而，现实世界的应用经常面临部分标注数据集的挑战，只有少数地点拥有某些专家标注，导致大量未标注数据无法使用。利用这些数据可以增强小且多样化标注集中的变换器架构的能力。我们在八个医院的真实世界环境中进行了迄今为止最大的联邦心脏CT分析（n&#x3D;8,104）。我们的两步半监督策略将任务特定CNN的知识蒸馏到变换器中。首先，CNN按标签类型对未标记数据进行预测，然后变换器通过特定标签头从这些预测中学习。这提高了预测精度，能够在联邦中同时学习所有部分标签，并且在下游任务上的泛化性能优于基于UNet的模型。代码和模型权重已公开提供，可用于未来的心脏CT分析。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.07557v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了联邦学习在利用分散数据的同时保护隐私的技术。针对现实世界中只有部分地点拥有特定专家标注、大量未标注数据未被使用的问题，文章通过利用这些未标注数据，增强了在小型和多样化标注集下的转换器架构能力。研究团队进行了迄今为止最大的联邦心脏CT分析（n&#x3D;8,104），跨越八家医院。他们采用的两步半监督策略将任务特定CNN的知识蒸馏到转换器中。首先，CNN按标签类型对未标记数据进行预测，然后转换器从这些预测中学习，并配备有标签特定头。这种方法提高了预测精度，能够同时学习联邦中的所有部分标签，并且在下游任务上的通用性优于基于UNet的模型。代码和模型权重已公开提供，可用于将来的心脏CT分析。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>联邦学习能够利用分散数据的同时保护隐私。</li>
<li>部分数据集只有部分标注，大量未标注数据未得到利用。</li>
<li>通过利用未标注数据，可增强在小型和多样化标注集下的转换器架构能力。</li>
<li>研究团队进行了大规模（n&#x3D;8,104）的联邦心脏CT分析，涉及八家医院的数据。</li>
<li>采用的两步半监督策略将CNN的知识蒸馏到转换器中。</li>
<li>该方法提高了预测精度，并能同时学习所有部分标签。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.07557">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-010366cd48b4598369b9130678ecc0b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec435473562b39724036a3310eca6ff5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Multi-modal-Evidential-Fusion-Network-for-Trustworthy-PET-CT-Tumor-Segmentation"><a href="#Multi-modal-Evidential-Fusion-Network-for-Trustworthy-PET-CT-Tumor-Segmentation" class="headerlink" title="Multi-modal Evidential Fusion Network for Trustworthy PET&#x2F;CT Tumor   Segmentation"></a>Multi-modal Evidential Fusion Network for Trustworthy PET&#x2F;CT Tumor   Segmentation</h2><p><strong>Authors:Yuxuan Qi, Li Lin, Jiajun Wang, Bin Zhang, Jingya Zhang</strong></p>
<p>Accurate tumor segmentation in PET&#x2F;CT images is crucial for computer-aided cancer diagnosis and treatment. The primary challenge lies in effectively integrating the complementary information from PET and CT images. In clinical settings, the quality of PET and CT images often varies significantly, leading to uncertainty in the modality information extracted by networks. To address this challenge, we propose a novel Multi-modal Evidential Fusion Network (MEFN), which consists of two core stages: Cross-Modal Feature Learning (CFL) and Multi-modal Trustworthy Fusion (MTF). The CFL stage aligns features across different modalities and learns more robust feature representations, thereby alleviating the negative effects of domain gap. The MTF stage utilizes mutual attention mechanisms and an uncertainty calibrator to fuse modality features based on modality uncertainty and then fuse the segmentation results under the guidance of Dempster-Shafer Theory. Besides, a new uncertainty perceptual loss is introduced to force the model focusing on uncertain features and hence improve its ability to extract trusted modality information. Extensive comparative experiments are conducted on two publicly available PET&#x2F;CT datasets to evaluate the performance of our proposed method whose results demonstrate that our MEFN significantly outperforms state-of-the-art methods with improvements of 3.10% and 3.23% in DSC scores on the AutoPET dataset and the Hecktor dataset, respectively. More importantly, our model can provide radiologists with credible uncertainty of the segmentation results for their decision in accepting or rejecting the automatic segmentation results, which is particularly important for clinical applications. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/QPaws/MEFN">https://github.com/QPaws/MEFN</a>. </p>
<blockquote>
<p>在PET&#x2F;CT图像中进行准确的肿瘤分割对于计算机辅助的癌症诊断和治疗至关重要。主要挑战在于有效地整合PET和CT图像中的互补信息。在临床环境中，PET和CT图像的质量经常有很大的差异，导致网络提取的模态信息存在不确定性。为了解决这一挑战，我们提出了一种新的多模态证据融合网络（MEFN），它包含两个阶段：跨模态特征学习（CFL）和多模态可信融合（MTF）。CFL阶段对不同模态的特征进行对齐，学习更稳健的特征表示，从而减轻领域差距的负面影响。MTF阶段利用相互注意机制和不确定性校准器，基于模态不确定性融合模态特征，然后在Dempster-Shafer理论的指导下融合分割结果。此外，引入了一种新的不确定性感知损失，以迫使模型关注不确定的特征，从而提高其提取可信模态信息的能力。我们在两个公开的PET&#x2F;CT数据集上进行了广泛的对比实验，以评估我们提出的方法的性能。结果表明，我们的MEFN在DSC得分上显著优于最先进的方法，在AutoPET数据集和Hecktor数据集上分别提高了3.10%和3.23%。更重要的是，我们的模型可以为放射科医生提供分割结果的可靠不确定性，以供他们决定是否接受或拒绝自动分割结果，这在临床应用中是特别重要的。我们的代码将在<a target="_blank" rel="noopener" href="https://github.com/QPaws/MEFN%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/QPaws/MEFN上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.18327v2">PDF</a> </p>
<p><strong>Summary</strong><br>     提出一种多模态证据融合网络（MEFN），用于PET&#x2F;CT图像的肿瘤分割。网络包括两个阶段：跨模态特征学习（CFL）和跨模态可信融合（MTF）。CFL阶段实现对不同模态的特征对齐，学习更稳健的特征表示，缓解领域差距的负面影响。MTF阶段利用互注意机制和不确定性校准器，基于模态不确定性融合模态特征，并在Dempster-Shafer理论的指导下融合分割结果。引入新的不确定性感知损失，使模型关注不确定特征，提高提取可靠模态信息的能力。在公开PET&#x2F;CT数据集上的实验表明，MEFN在DSC得分上较先进方法提高了3.10%和3.23%，并为放射科医生提供可信的分割结果不确定性，有助于他们决定是否接受自动分割结果，具有临床意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态图像分割的挑战在于整合PET和CT图像的互补信息。</li>
<li>提出的多模态证据融合网络（MEFN）包含跨模态特征学习（CFL）和跨模态可信融合（MTF）两个阶段。</li>
<li>CFL阶段实现不同模态的特征对齐，增强稳健性特征学习，减少领域差距影响。</li>
<li>MTF阶段利用互注意机制和不确定性校准器，结合模态不确定性进行特征融合和分割结果融合。</li>
<li>引入不确定性感知损失，使模型更注重不确定特征，提高提取可靠模态信息的准确度。</li>
<li>在公开数据集上的实验显示，MEFN显著优于其他方法，提高了分割精度。</li>
<li>MEFN能为放射科医生提供分割结果的可信不确定性，有助于临床决策。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.18327">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5f31307161655d457786782734c642d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-264d36bd2d7cb5fa873f8f91a8bd8b43.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="IVIM-Morph-Motion-compensated-quantitative-Intra-voxel-Incoherent-Motion-IVIM-analysis-for-functional-fetal-lung-maturity-assessment-from-diffusion-weighted-MRI-data"><a href="#IVIM-Morph-Motion-compensated-quantitative-Intra-voxel-Incoherent-Motion-IVIM-analysis-for-functional-fetal-lung-maturity-assessment-from-diffusion-weighted-MRI-data" class="headerlink" title="IVIM-Morph: Motion-compensated quantitative Intra-voxel Incoherent   Motion (IVIM) analysis for functional fetal lung maturity assessment from   diffusion-weighted MRI data"></a>IVIM-Morph: Motion-compensated quantitative Intra-voxel Incoherent   Motion (IVIM) analysis for functional fetal lung maturity assessment from   diffusion-weighted MRI data</h2><p><strong>Authors:Noga Kertes, Yael Zaffrani-Reznikov, Onur Afacan, Sila Kurugol, Simon K. Warfield, Moti Freiman</strong></p>
<p>Quantitative analysis of pseudo-diffusion in diffusion-weighted magnetic resonance imaging (DWI) data shows potential for assessing fetal lung maturation and generating valuable imaging biomarkers. Yet, the clinical utility of DWI data is hindered by unavoidable fetal motion during acquisition. We present IVIM-morph, a self-supervised deep neural network model for motion-corrected quantitative analysis of DWI data using the Intra-voxel Incoherent Motion (IVIM) model. IVIM-morph combines two sub-networks, a registration sub-network, and an IVIM model fitting sub-network, enabling simultaneous estimation of IVIM model parameters and motion. To promote physically plausible image registration, we introduce a biophysically informed loss function that effectively balances registration and model-fitting quality. We validated the efficacy of IVIM-morph by establishing a correlation between the predicted IVIM model parameters of the lung and gestational age (GA) using fetal DWI data of 39 subjects. IVIM-morph exhibited a notably improved correlation with gestational age (GA) when performing in-vivo quantitative analysis of fetal lung DWI data during the canalicular phase. IVIM-morph shows potential in developing valuable biomarkers for non-invasive assessment of fetal lung maturity with DWI data. Moreover, its adaptability opens the door to potential applications in other clinical contexts where motion compensation is essential for quantitative DWI analysis. The IVIM-morph code is readily available at: <a target="_blank" rel="noopener" href="https://github.com/TechnionComputationalMRILab/qDWI-Morph">https://github.com/TechnionComputationalMRILab/qDWI-Morph</a>. </p>
<blockquote>
<p>对扩散加权磁共振成像（DWI）数据中的伪扩散进行定量分析，在评估胎儿肺成熟度和产生有价值的成像生物标志物方面具有潜力。然而，DWI数据的临床应用受到采集过程中不可避免的胎儿运动的影响。我们提出了IVIM-morph，这是一种基于自监督深度神经网络模型的DWI数据运动校正定量分析系统，它采用体素内非相干运动（IVIM）模型。IVIM-morph结合了两个子网络，一个注册子网络和一个IVIM模型拟合子网络，可以同时估计IVIM模型参数和运动。为了促进物理上合理的图像配准，我们引入了一个基于生物物理信息的损失函数，该损失函数可以有效地平衡配准和模型拟合质量。我们通过建立肺预测的IVIM模型参数与胎龄（GA）之间的相关性，验证了IVIM-morph的有效性，使用的是来自39名胎儿的DWI数据。在胎儿肺DWI数据的峡部阶段的体内定量分析中，IVIM-morph与胎龄（GA）的相关性显著提高。IVIM-morph在利用DWI数据对胎儿肺成熟度进行无创评估方面具有发展有价值生物标志物的潜力。此外，其适应性为在其它需要定量DWI分析的运动补偿的临床环境中应用打开了大门。IVIM-morph的代码可轻松获取于：<a target="_blank" rel="noopener" href="https://github.com/TechnionComputationalMRILab/qDWI-Morph%E3%80%82">https://github.com/TechnionComputationalMRILab/qDWI-Morph。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.07126v3">PDF</a> Accepted for publication in the journal: “Medical Image Analysis”</p>
<p><strong>Summary</strong></p>
<p>该文介绍了利用定量扩散加权磁共振成像（DWI）数据评估胎儿肺成熟度的方法。研究中提出了一种名为IVIM-morph的自监督深度神经网络模型，该模型使用Intra-voxel Incoherent Motion（IVIM）模型进行运动校正的定量DWI数据分析。IVIM-morph结合了配准子网络和IVIM模型拟合子网络，可以同时估计IVIM模型参数和运动。通过引入生物物理信息损失函数，促进物理上合理的图像配准，有效平衡配准和模型拟合质量。使用胎儿DWI数据对IVIM-morph进行验证，结果表明预测的IVIM模型参数与胎龄有良好相关性，特别是在肺导管期。IVIM-morph具有开发基于DWI数据的胎儿肺成熟度非侵入评估生物标志物的潜力，并且其适应性为其他需要定量DWI分析的运动补偿临床应用提供了可能性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用定量扩散加权磁共振成像（DWI）数据评估胎儿肺成熟度具有潜力。</li>
<li>IVIM-morph是一个自监督深度神经网络模型，用于运动校正的定量DWI数据分析。</li>
<li>IVIM-morph结合了配准和IVIM模型拟合，可以同时估计模型参数和运动。</li>
<li>通过引入生物物理信息损失函数，促进了物理上合理的图像配准。</li>
<li>IVIM-morph在胎儿肺DWI数据的体内定量分析中表现出与胎龄的良好相关性。</li>
<li>IVIM-morph具有开发基于DWI数据的非侵入性胎儿肺成熟度评估生物标志物的潜力。</li>
<li>IVIM-morph的适应性为其在其他需要定量DWI分析的运动补偿临床应用中的使用提供了可能性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.07126">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e3a33ea1aa74743dcf17129660f66e2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e11db7c7642af770c0faaebf981e8ecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-785db2b85cbf7c8b98bae7263d744df0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-976f7d248400e11f9cc47280e22c6995.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Improve-Myocardial-Strain-Estimation-based-on-Deformable-Groupwise-Registration-with-a-Locally-Low-Rank-Dissimilarity-Metric"><a href="#Improve-Myocardial-Strain-Estimation-based-on-Deformable-Groupwise-Registration-with-a-Locally-Low-Rank-Dissimilarity-Metric" class="headerlink" title="Improve Myocardial Strain Estimation based on Deformable Groupwise   Registration with a Locally Low-Rank Dissimilarity Metric"></a>Improve Myocardial Strain Estimation based on Deformable Groupwise   Registration with a Locally Low-Rank Dissimilarity Metric</h2><p><strong>Authors:Haiyang Chen, Juan Gao, Zhuo Chen, Chenhao Gao, Sirui Huo, Meng Jiang, Jun Pu, Chenxi Hu</strong></p>
<p>Background: Current mainstream cardiovascular magnetic resonance-feature tracking (CMR-FT) methods, including optical flow and pairwise registration, often suffer from the drift effect caused by accumulative tracking errors. Here, we developed a CMR-FT method based on deformable groupwise registration with a locally low-rank (LLR) dissimilarity metric to improve myocardial tracking and strain estimation accuracy. Methods: The proposed method, Groupwise-LLR, performs feature tracking by iteratively updating the entire displacement field across all cardiac phases to minimize the sum of the patchwise signal ranks of the deformed movie. The method was compared with alternative CMR-FT methods including the Farneback optical flow, a sequentially pairwise registration method, and a global low rankness-based groupwise registration method via a simulated dataset (n &#x3D; 20), a public cine data set (n &#x3D; 100), and an in-house tagging-MRI patient dataset (n &#x3D; 16). The proposed method was also compared with two general groupwise registration methods, nD+t B-Splines and pTVreg, in simulations and in vivo tracking. Results: On the simulated dataset, Groupwise-LLR achieved the lowest point tracking errors and voxelwise&#x2F;global strain errors. On the public dataset, Groupwise-LLR achieved the lowest contour tracking errors, reduced the drift effect in late-diastole, and preserved similar inter-observer reproducibility as the alternative methods. On the patient dataset, Groupwise-LLR correlated better with tagging-MRI for radial strains than the other CMR-FT methods in multiple myocardial segments and levels. Conclusions: The proposed Groupwise-LLR reduces the drift effect and provides more accurate myocardial tracking and strain estimation than the alternative methods. The method may thus facilitate a more accurate estimation of myocardial strains for clinical assessments of cardiac function. </p>
<blockquote>
<p>背景：当前主流的心血管磁共振特征跟踪（CMR-FT）方法，包括光学流和配对注册，经常受到由累积跟踪误差引起的漂移效应的影响。在这里，我们开发了一种基于可变形组注册和局部低秩（LLR）差异度量的CMR-FT方法，以提高心肌跟踪和应变估计的准确性。方法：所提出的方法（Groupwise-LLR）通过迭代更新所有心脏阶段的整个位移场，以最小化变形电影中的斑块信号等级和，来执行特征跟踪。该方法通过模拟数据集（n&#x3D;20）、公共电影数据集（n&#x3D;100）和院内标签MRI患者数据集（n&#x3D;16）与替代的CMR-FT方法（包括Farneback光学流、顺序配对注册方法和基于全局低秩的组注册方法）进行比较。所提出的方法还在模拟和体内跟踪中与两种通用的组注册方法nD+t B-Splines和pTVreg进行了比较。结果：在模拟数据集上，Groupwise-LLR达到了最低的点跟踪误差和体素级&#x2F;全局应变误差。在公共数据集上，Groupwise-LLR取得了最低的轮廓跟踪误差，减少了舒张后期的漂移效应，并保持了与其他方法的相似观察者间可重复性。在患者数据集上，相较于其他CMR-FT方法，Groupwise-LLR在多心肌节段和水平上与标签MRI在径向应变上具有更好的相关性。结论：所提出的Groupwise-LLR减少了漂移效应，并提供了比其他方法更准确的心肌跟踪和应变估计。因此，该方法可能有助于更准确估计心肌应变，用于临床心脏功能评估。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07348v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于可变形组注册和局部低秩（LLR）差异度量的心血管磁共振特征跟踪（CMR-FT）方法，旨在提高心肌跟踪和应变估计的准确性。新方法通过迭代更新所有心脏阶段的整个位移场，以最小化变形电影的斑块信号等级和，从而进行特征跟踪。在模拟数据集、公开电影数据集和院内标签MRI患者数据集上的比较结果表明，该方法在点跟踪误差、轮廓跟踪误差、整体应变误差等方面表现优异，并减少了舒张后期的漂移效应。因此，所提出的方法可能更准确地估计心肌应变，有助于临床心脏功能评估。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前主流的心血管磁共振特征跟踪方法常受到由累积跟踪误差导致的漂移效应影响。</li>
<li>提出了基于可变形组注册和局部低秩差异度量的新方法（Groupwise-LLR）来改善心肌跟踪和应变估计。</li>
<li>Groupwise-LLR方法通过迭代更新整个位移场来最小化变形电影的斑块信号等级和，实现特征跟踪。</li>
<li>在模拟、公开和患者数据集上的比较显示，Groupwise-LLR在跟踪准确性和应变估计方面优于其他CMR-FT方法。</li>
<li>Groupwise-LLR在模拟数据集中实现了最低的点跟踪误差和全局应变误差。</li>
<li>在公共数据集中，Groupwise-LLR减少了舒张后期的漂移效应，并保持与其他方法的类似观察者间可重复性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.07348">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-18042e5f01389caf090a3f404562f7e3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-04/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b0270071ca38c5abcb34d690bb9ed4e0.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-01-04  SSR-Speech Towards Stable, Safe and Robust Zero-shot Text-based Speech   Editing and Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-04/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d672e238c1a03deb47fedc1bfe354b39.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-01-04  Prometheus 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D   Scene Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30055.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
