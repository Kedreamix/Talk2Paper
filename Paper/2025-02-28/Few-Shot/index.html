<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-02-28  FSPO Few-Shot Preference Optimization of Synthetic Preference Data in   LLMs Elicits Effective Personalization to Real Users">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2412.17242v2/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-28-更新"><a href="#2025-02-28-更新" class="headerlink" title="2025-02-28 更新"></a>2025-02-28 更新</h1><h2 id="FSPO-Few-Shot-Preference-Optimization-of-Synthetic-Preference-Data-in-LLMs-Elicits-Effective-Personalization-to-Real-Users"><a href="#FSPO-Few-Shot-Preference-Optimization-of-Synthetic-Preference-Data-in-LLMs-Elicits-Effective-Personalization-to-Real-Users" class="headerlink" title="FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in   LLMs Elicits Effective Personalization to Real Users"></a>FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in   LLMs Elicits Effective Personalization to Real Users</h2><p><strong>Authors:Anikait Singh, Sheryl Hsu, Kyle Hsu, Eric Mitchell, Stefano Ermon, Tatsunori Hashimoto, Archit Sharma, Chelsea Finn</strong></p>
<p>Effective personalization of LLMs is critical for a broad range of user-interfacing applications such as virtual assistants and content curation. Inspired by the strong in-context learning capabilities of LLMs, we propose Few-Shot Preference Optimization (FSPO), which reframes reward modeling as a meta-learning problem. Under this framework, an LLM learns to quickly adapt to a user via a few labeled preferences from that user, constructing a personalized reward function for them. Additionally, since real-world preference data is scarce and challenging to collect at scale, we propose careful design choices to construct synthetic preference datasets for personalization, generating over 1M synthetic personalized preferences using publicly available LLMs. In particular, to successfully transfer from synthetic data to real users, we find it crucial for the data to exhibit both high diversity and coherent, self-consistent structure. We evaluate FSPO on personalized open-ended generation for up to 1,500 synthetic users across across three domains: movie reviews, pedagogical adaptation based on educational background, and general question answering, along with a controlled human study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in generating responses that are personalized to synthetic users and a 72% winrate with real human users in open-ended question answering. </p>
<blockquote>
<p>有效的大模型个性化策略对于各种用户接口应用至关重要，例如虚拟助手和内容整理。我们受到大模型上下文学习能力的启发，提出了Few-Shot Preference Optimization（FSPO）方法，该方法将奖励建模重新定位为元学习问题。在该框架下，大模型通过学习快速适应用户通过少量标记偏好来自用户构建个性化的奖励功能。此外，由于现实世界中的偏好数据稀缺且难以大规模收集，我们提出了精心设计选择来构建个性化合成数据集，使用公开可用的大模型生成超过1M的合成个性化偏好。特别是，为了成功从合成数据转移到真实用户，我们发现数据的高多样性和连贯、一致的结构至关重要。我们在三个领域对FSPO进行了个性化开放式生成评估：电影评论、基于教育背景的教学适应和一般问答，以及受控的人类研究。总体而言，FSPO在针对合成用户的个性化生成方面平均获得了87%的Alpaca Eval胜率，在开放式问答中对真实人类用户的胜率为72%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19312v1">PDF</a> Website: <a target="_blank" rel="noopener" href="https://fewshot-preference-optimization.github.io/">https://fewshot-preference-optimization.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了基于LLM的个性化方法——Few-Shot Preference Optimization（FSPO）。通过重构奖励模型为元学习问题，LLM可以快速适应用户的偏好并构建个性化奖励函数。为了从合成数据转移到真实用户，合成数据必须展现高多样性和连贯性结构。评估结果显示，FSPO在个性化生成方面取得了显著成果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Preference Optimization（FSPO）是一种基于LLM的有效个性化方法，适用于虚拟助理和内容推荐等用户接口应用。</li>
<li>FSPO通过重构奖励模型为元学习问题，使LLM能够快速适应用户偏好并构建个性化奖励函数。</li>
<li>为解决真实世界偏好数据稀缺和难以大规模收集的问题，提出了构建合成偏好数据集的方法，成功生成超过1M的合成个性化偏好数据。</li>
<li>数据的高多样性和连贯性结构是从合成数据转移到真实用户的关键因素。</li>
<li>FSPO在个性化生成方面取得了显著成果，尤其是在电影评论、基于教育背景的教学适应和一般问答等领域。</li>
<li>在对合成用户的评估中，FSPO的平均Alpaca Eval胜率达到87%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19312">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2502.19312v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2502.19312v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2502.19312v1/page_3_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Brain-inspired-analogical-mixture-prototypes-for-few-shot-class-incremental-learning"><a href="#Brain-inspired-analogical-mixture-prototypes-for-few-shot-class-incremental-learning" class="headerlink" title="Brain-inspired analogical mixture prototypes for few-shot   class-incremental learning"></a>Brain-inspired analogical mixture prototypes for few-shot   class-incremental learning</h2><p><strong>Authors:Wanyi Li, Wei Wei, Yongkang Luo, Peng Wang</strong></p>
<p>Few-shot class-incremental learning (FSCIL) poses significant challenges for artificial neural networks due to the need to efficiently learn from limited data while retaining knowledge of previously learned tasks. Inspired by the brain’s mechanisms for categorization and analogical learning, we propose a novel approach called Brain-inspired Analogical Mixture Prototypes (BAMP). BAMP has three components: mixed prototypical feature learning, statistical analogy, and soft voting. Starting from a pre-trained Vision Transformer (ViT), mixed prototypical feature learning represents each class using a mixture of prototypes and fine-tunes these representations during the base session. The statistical analogy calibrates the mean and covariance matrix of prototypes for new classes according to similarity to the base classes, and computes classification score with Mahalanobis distance. Soft voting combines both merits of statistical analogy and an off-shelf FSCIL method. Our experiments on benchmark datasets demonstrate that BAMP outperforms state-of-the-art on both traditional big start FSCIL setting and challenging small start FSCIL setting. The study suggests that brain-inspired analogical mixture prototypes can alleviate catastrophic forgetting and over-fitting problems in FSCIL. </p>
<blockquote>
<p>少量类别增量学习（FSCIL）给人工神经网络带来了重大挑战，因为需要在有限数据中有效学习，同时保留先前学习任务的记忆。受大脑分类和类比学习机制的启发，我们提出了一种新的方法，称为Brain-inspired Analogical Mixture Prototypes（BAMP）。BAMP包含三个组成部分：混合原型特征学习、统计类比和软投票。从预训练的Vision Transformer（ViT）开始，混合原型特征学习通过混合原型表示每个类别，并在基础会话期间对这些表示进行微调。统计类比根据新类别与基础类别的相似性对原型的均值和协方差矩阵进行校准，并使用马氏距离计算分类分数。软投票结合了统计类比和现成的FSCIL方法两者的优点。我们在基准数据集上的实验表明，BAMP在传统的大起始FSCIL设置和具有挑战性的小起始FSCIL设置上都优于最新技术状态。研究表明，受大脑启发的类比混合原型可以缓解FSCIL中的灾难性遗忘和过度拟合问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18923v1">PDF</a> under review</p>
<p><strong>Summary</strong></p>
<p>基于预训练的Vision Transformer（ViT），提出一种新颖的类增量学习方法Brain-inspired Analogical Mixture Prototypes（BAMP）。它通过混合原型特征学习、统计类比和软投票三个组件，有效应对有限数据下的学习任务并保留先前学习的知识。实验结果在基准数据集上证明BAMP在传統的较大样本开始FSCIL场景和具有挑战性的小样本开始FSCIL场景中均优于当前顶尖水平。这暗示了BAMP可能有助于解决FSCIL中的灾难性遗忘和过度拟合问题。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>BAMP方法结合了预训练的Vision Transformer（ViT）。</li>
<li>混合原型特征学习用于表示每个类的混合原型并微调这些表示。</li>
<li>统计类比根据新类别与基础类别的相似性来调整原型的均值和协方差矩阵。</li>
<li>使用Mahalanobis距离计算分类分数。</li>
<li>软投票结合了统计类比和现有FSCIL方法的优点。</li>
<li>BAMP在基准数据集上的表现优于当前顶尖水平。</li>
<li>BAMP有助于解决FSCIL中的灾难性遗忘和过度拟合问题。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18923">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2502.18923v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2502.18923v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2502.18923v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Exploring-Graph-Tasks-with-Pure-LLMs-A-Comprehensive-Benchmark-and-Investigation"><a href="#Exploring-Graph-Tasks-with-Pure-LLMs-A-Comprehensive-Benchmark-and-Investigation" class="headerlink" title="Exploring Graph Tasks with Pure LLMs: A Comprehensive Benchmark and   Investigation"></a>Exploring Graph Tasks with Pure LLMs: A Comprehensive Benchmark and   Investigation</h2><p><strong>Authors:Yuxiang Wang, Xinnan Dai, Wenqi Fan, Yao Ma</strong></p>
<p>Graph-structured data has become increasingly prevalent across various domains, raising the demand for effective models to handle graph tasks like node classification and link prediction. Traditional graph learning models like Graph Neural Networks (GNNs) have made significant strides, but their capabilities in handling graph data remain limited in certain contexts. In recent years, large language models (LLMs) have emerged as promising candidates for graph tasks, yet most studies focus primarily on performance benchmarks and fail to address their broader potential, including their ability to handle limited data, their transferability across tasks, and their robustness. In this work, we provide a comprehensive exploration of LLMs applied to graph tasks. We evaluate the performance of pure LLMs, including those without parameter optimization and those fine-tuned with instructions, across various scenarios. Our analysis goes beyond accuracy, assessing LLM ability to perform in few-shot&#x2F;zero-shot settings, transfer across domains, understand graph structures, and demonstrate robustness in challenging scenarios. We conduct extensive experiments with 16 graph learning models alongside 6 LLMs (e.g., Llama3B, GPT-4o, Qwen-plus), comparing their performance on datasets like Cora, PubMed, ArXiv, and Products. Our findings show that LLMs, particularly those with instruction tuning, outperform traditional models in few-shot settings, exhibit strong domain transferability, and demonstrate excellent generalization and robustness. This work offers valuable insights into the capabilities of LLMs for graph learning, highlighting their advantages and potential for real-world applications, and paving the way for future research in this area. Codes and datasets are released in <a target="_blank" rel="noopener" href="https://github.com/myflashbarry/LLM-benchmarking">https://github.com/myflashbarry/LLM-benchmarking</a>. </p>
<blockquote>
<p>图形结构化数据在各个领域的普及程度越来越高，对处理图形任务（如节点分类和链接预测）的有效模型的需求也随之增加。传统的图形学习模型，如图神经网络（GNNs）已经取得了显著的进步，但在某些情况下处理图形数据的能力仍然有限。近年来，大型语言模型（LLMs）作为图形任务的候选者前景广阔，但大多数研究主要集中在性能基准测试上，未能解决其更广泛的潜力，包括处理有限数据的能力、跨任务的迁移能力以及稳健性。在这项工作中，我们对LLMs在图形任务中的应用进行了全面的探索。我们评估了纯LLMs的性能，包括那些无需参数优化和经过指令微调的情况。我们的分析超越了准确性，评估了LLM在少量样本&#x2F;零样本设置中的表现、跨域的迁移能力、对图形结构的理解以及在具有挑战的场景中展现的稳健性。我们使用大量的实验对与图学习模型的比较，对与16种图学习模型并行的六个大型语言模型进行了评估（如Llama3B、GPT-4o和Qwen-plus），对比其在诸如Cora、PubMed、ArXiv和Products等数据集上的性能表现。我们的研究结果表明，LLMs在少量样本设置中具有出色的表现，特别是在经过指令微调后，显示出强大的领域迁移能力，并展现出良好的泛化和稳健性。这项工作为LLMs在图形学习方面的能力提供了有价值的见解，突出了其在现实世界应用中的优势和潜力，并为该领域的未来研究铺平了道路。相关代码和数据集已在<a target="_blank" rel="noopener" href="https://github.com/myflashbarry/LLM-benchmarking%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/myflashbarry/LLM-benchmarking上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18771v1">PDF</a> </p>
<p><strong>Summary</strong><br>     大型语言模型（LLMs）在处理图任务时展现出潜力，特别是在少样本或无样本场景下表现优异。本研究对LLMs在图任务中的应用进行了全面探索，与16个图学习模型和6个LLMs进行实验对比，发现LLMs特别是经过指令调优的模型表现出强大的性能和优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在图任务中展现出潜力。</li>
<li>LLMs能在少样本或无样本场景下处理图数据。</li>
<li>LLMs具有强大的跨域转移能力。</li>
<li>相较于传统图学习模型，LLMs在图任务中表现出更好的性能。</li>
<li>指令调优的LLMs在图任务中展现更强大的性能。</li>
<li>LLMs在处理图任务时展现出良好的泛化和鲁棒性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18771">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2502.18771v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2502.18771v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2502.18771v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2502.18771v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2502.18771v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="On-the-Generalization-and-Adaptation-Ability-of-Machine-Generated-Text-Detectors-in-Academic-Writing"><a href="#On-the-Generalization-and-Adaptation-Ability-of-Machine-Generated-Text-Detectors-in-Academic-Writing" class="headerlink" title="On the Generalization and Adaptation Ability of Machine-Generated Text   Detectors in Academic Writing"></a>On the Generalization and Adaptation Ability of Machine-Generated Text   Detectors in Academic Writing</h2><p><strong>Authors:Yule Liu, Zhiyuan Zhong, Yifan Liao, Zhen Sun, Jingyi Zheng, Jiaheng Wei, Qingyuan Gong, Fenghua Tong, Yang Chen, Yang Zhang, Xinlei He</strong></p>
<p>The rising popularity of large language models (LLMs) has raised concerns about machine-generated text (MGT), particularly in academic settings, where issues like plagiarism and misinformation are prevalent. As a result, developing a highly generalizable and adaptable MGT detection system has become an urgent priority. Given that LLMs are most commonly misused in academic writing, this work investigates the generalization and adaptation capabilities of MGT detectors in three key aspects specific to academic writing: First, we construct MGT-Acedemic, a large-scale dataset comprising over 336M tokens and 749K samples. MGT-Acedemic focuses on academic writing, featuring human-written texts (HWTs) and MGTs across STEM, Humanities, and Social Sciences, paired with an extensible code framework for efficient benchmarking. Second, we benchmark the performance of various detectors for binary classification and attribution tasks in both in-domain and cross-domain settings. This benchmark reveals the often-overlooked challenges of attribution tasks. Third, we introduce a novel attribution task where models have to adapt to new classes over time without (or with very limited) access to prior training data in both few-shot and many-shot scenarios. We implement eight different adapting techniques to improve the performance and highlight the inherent complexity of the task. Our findings provide insights into the generalization and adaptation ability of MGT detectors across diverse scenarios and lay the foundation for building robust, adaptive detection systems. The code framework is available at <a target="_blank" rel="noopener" href="https://github.com/Y-L-LIU/MGTBench-2.0">https://github.com/Y-L-LIU/MGTBench-2.0</a>. </p>
<blockquote>
<p>随着大型语言模型（LLM）的日益普及，人们对机器生成文本（MGT）的担忧也随之增加，特别是在学术环境中，抄袭和误信息等问题的普遍存在。因此，开发一个高度通用化和适应性的MGT检测系统已成为一项紧迫的任务。鉴于LLM在学术写作中最常被滥用，本研究探讨了MGT检测器在学术写作方面的三个关键方面的通用性和适应性能力：首先，我们构建了MGT-Acedemic，这是一个大规模数据集，包含超过3.36亿个令牌和74.9万个样本。MGT-Acedemic专注于学术写作，涵盖了STEM、人文和社会科学领域的人类书写文本（HWTs）和MGTs，并配备了一个可扩展的代码框架，以便进行有效的基准测试。其次，我们对各种检测器在域内和跨域设置中的二元分类和归属任务性能进行了基准测试。这一基准测试揭示了归属任务经常被忽视的挑战。第三，我们引入了一个新的归属任务，即模型必须在没有（或非常有限的）先前训练数据的情况下，在少数和多数场景中逐渐适应新的类别。我们实施了八种不同的适应技术来提高性能，并突出了这项任务的固有复杂性。我们的研究为MGT检测器在不同场景下的通用性和适应能力提供了洞察，并为构建稳健、自适应的检测系统奠定了基础。代码框架可在<a target="_blank" rel="noopener" href="https://github.com/Y-L-LIU/MGTBench-2.0%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Y-L-LIU/MGTBench-2.0找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17242v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的普及引发了关于机器生成文本（MGT）的担忧，特别是在学术环境中。为了应对抄袭和误导信息等常见问题，开发具有高度通用性和适应性的MGT检测系统已成为当务之急。本文构建了一个专注于学术写作的大型数据集MGT-Acedemic，并评估了不同检测器在二元分类和归属任务中的性能，包括内部领域和跨领域设置。此外，本文引入了一种新的归属任务，要求模型在少量或几乎没有先前训练数据的情况下，随时间适应新类别。本文的研究结果揭示了MGT检测器在多种场景下的通用性和适应性，为构建稳健、自适应的检测系统提供了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的普及引发了机器生成文本（MGT）在学术写作中的滥用问题，需要开发高度通用和适应的MGT检测系统。</li>
<li>构建了专注于学术写作的大型数据集MGT-Acedemic，包含超过336M标记和749K样本，用于评估MGT检测器的性能。</li>
<li>评估了不同检测器在二元分类和归属任务中的性能，涉及内部领域和跨领域设置，揭示了归属任务的挑战。</li>
<li>引入了一种新的归属任务，要求模型在有限或没有先前训练数据的情况下随时间适应新类别，并实施了八种不同的适应技术来提高性能。</li>
<li>研究结果强调了MGT检测器在多种场景下的通用性和适应性挑战。</li>
<li>代码框架的可用性为研究人员提供了一个基础，以进一步开发和改进MGT检测系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17242">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2412.17242v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2412.17242v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2412.17242v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2412.17242v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2412.17242v2/page_4_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2412.17242v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2412.17242v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Multimodality-Helps-Few-shot-3D-Point-Cloud-Semantic-Segmentation"><a href="#Multimodality-Helps-Few-shot-3D-Point-Cloud-Semantic-Segmentation" class="headerlink" title="Multimodality Helps Few-shot 3D Point Cloud Semantic Segmentation"></a>Multimodality Helps Few-shot 3D Point Cloud Semantic Segmentation</h2><p><strong>Authors:Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Min Wu, Ming-Ming Cheng, Ender Konukoglu, Serge Belongie</strong></p>
<p>Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to segment novel categories with minimal annotated support samples. While existing FS-PCS methods have shown promise, they primarily focus on unimodal point cloud inputs, overlooking the potential benefits of leveraging multimodal information. In this paper, we address this gap by introducing a multimodal FS-PCS setup, utilizing textual labels and the potentially available 2D image modality. Under this easy-to-achieve setup, we present the MultiModal Few-Shot SegNet (MM-FSS), a model effectively harnessing complementary information from multiple modalities. MM-FSS employs a shared backbone with two heads to extract intermodal and unimodal visual features, and a pretrained text encoder to generate text embeddings. To fully exploit the multimodal information, we propose a Multimodal Correlation Fusion (MCF) module to generate multimodal correlations, and a Multimodal Semantic Fusion (MSF) module to refine the correlations using text-aware semantic guidance. Additionally, we propose a simple yet effective Test-time Adaptive Cross-modal Calibration (TACC) technique to mitigate training bias, further improving generalization. Experimental results on S3DIS and ScanNet datasets demonstrate significant performance improvements achieved by our method. The efficacy of our approach indicates the benefits of leveraging commonly-ignored free modalities for FS-PCS, providing valuable insights for future research. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot">https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot</a> </p>
<blockquote>
<p>少量射击3D点云分割（FS-PCS）旨在将模型推广到对新型类别的分割，且只需要极少量的注释支持样本。虽然现有的FS-PCS方法已经显示出潜力，但它们主要关注单模态点云输入，忽视了利用多模态信息可能带来的潜在好处。在本文中，我们通过引入多模态FS-PCS设置来解决这一差距，利用文本标签和可用的2D图像模式。在这个易于实现的设置中，我们提出了多模态少量射击SegNet（MM-FSS），该模型能够有效地利用来自多个模态的互补信息。MM-FSS采用共享主干和两个头来提取跨模态和单模态视觉特征，并使用预训练的文本编码器生成文本嵌入。为了充分利用多模态信息，我们提出了多模态关联融合（MCF）模块来生成多模态关联，以及多模态语义融合（MSF）模块，使用文本感知语义指导来完善关联。此外，我们提出了一种简单有效的测试时自适应跨模态校准（TACC）技术，以减轻训练偏见，进一步提高泛化能力。在S3DIS和ScanNet数据集上的实验结果证明了我们的方法取得了显著的性能改进。我们的方法的有效性表明了利用常被忽略的免费模态对于FS-PCS的好处，为未来的研究提供了有价值的见解。代码可在<a target="_blank" rel="noopener" href="https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot上获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.22489v4">PDF</a> Published at ICLR 2025 (Spotlight)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对少样本3D点云分割（FS-PCS）的多模态方法。现有FS-PCS方法主要关注单模态点云输入，忽略了多模态信息的潜力。本文引入多模态FS-PCS设置，利用文本标签和可用的2D图像模态。提出的多模态少样本SegNet（MM-FSS）模型能有效利用多模态的互补信息。通过共享骨架、两个头提取跨模态和单模态视觉特征，并使用预训练的文本编码器生成文本嵌入。为充分利用多模态信息，提出多模态关联融合（MCF）模块和多模态语义融合（MSF）模块。此外，还提出了一种简单有效的测试时自适应跨模态校准（TACC）技术，以减轻训练偏见，进一步提高泛化能力。在S3DIS和ScanNet数据集上的实验结果表明，该方法取得了显著的性能改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有FS-PCS方法主要关注单模态点云输入，忽略了多模态信息的潜力。</li>
<li>引入多模态FS-PCS设置，利用文本标签和2D图像模态。</li>
<li>提出的多模态少样本SegNet（MM-FSS）模型能有效利用多模态的互补信息。</li>
<li>MM-FSS使用共享骨架、两个头来提取跨模态和单模态视觉特征，并结合预训练的文本编码器。</li>
<li>提出了多模态关联融合（MCF）模块和多模态语义融合（MSF）模块以充分利用多模态信息。</li>
<li>采用测试时自适应跨模态校准（TACC）技术减轻训练偏见，提高模型泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.22489">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2410.22489v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2410.22489v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2410.22489v4/page_3_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Local-Prompt-Extensible-Local-Prompts-for-Few-Shot-Out-of-Distribution-Detection"><a href="#Local-Prompt-Extensible-Local-Prompts-for-Few-Shot-Out-of-Distribution-Detection" class="headerlink" title="Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution   Detection"></a>Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution   Detection</h2><p><strong>Authors:Fanhu Zeng, Zhen Cheng, Fei Zhu, Hongxin Wei, Xu-Yao Zhang</strong></p>
<p>Out-of-Distribution (OOD) detection, aiming to distinguish outliers from known categories, has gained prominence in practical scenarios. Recently, the advent of vision-language models (VLM) has heightened interest in enhancing OOD detection for VLM through few-shot tuning. However, existing methods mainly focus on optimizing global prompts, ignoring refined utilization of local information with regard to outliers. Motivated by this, we freeze global prompts and introduce Local-Prompt, a novel coarse-to-fine tuning paradigm to emphasize regional enhancement with local prompts. Our method comprises two integral components: global prompt guided negative augmentation and local prompt enhanced regional regularization. The former utilizes frozen, coarse global prompts as guiding cues to incorporate negative augmentation, thereby leveraging local outlier knowledge. The latter employs trainable local prompts and a regional regularization to capture local information effectively, aiding in outlier identification. We also propose regional-related metric to empower the enrichment of OOD detection. Moreover, since our approach explores enhancing local prompts only, it can be seamlessly integrated with trained global prompts during inference to boost the performance. Comprehensive experiments demonstrate the effectiveness and potential of our method. Notably, our method reduces average FPR95 by 5.17% against state-of-the-art method in 4-shot tuning on challenging ImageNet-1k dataset, even outperforming 16-shot results of previous methods. Code is released at <a target="_blank" rel="noopener" href="https://github.com/AuroraZengfh/Local-Prompt">https://github.com/AuroraZengfh/Local-Prompt</a>. </p>
<blockquote>
<p>异常检测（OOD检测）旨在区分已知类别中的异常值，在真实场景中已经得到了广泛应用。最近，视觉语言模型（VLM）的出现增强了通过少样本调整来增强VLM的异常检测的兴趣。然而，现有的方法主要集中在优化全局提示上，忽略了利用关于异常值的局部信息的精细化。受此启发，我们冻结全局提示并引入Local-Prompt，这是一种新的从粗到细的调整范式，旨在通过局部提示来强调区域增强。我们的方法包含两个基本组成部分：全局提示引导下的负增强和局部提示增强的区域正则化。前者利用冻结的粗略全局提示作为引导线索来引入负增强，从而利用局部异常值知识。后者采用可训练局部提示和区域正则化来有效地捕获局部信息，有助于异常值识别。我们还提出了与区域相关的度量指标来增强异常检测能力。此外，由于我们的方法只专注于增强局部提示，因此可以在推理过程中无缝集成已训练的全局提示以提高性能。综合实验证明了我们的方法的有效性和潜力。值得注意的是，在具有挑战性的ImageNet-1k数据集上进行4次样本调整时，我们的方法将平均FPR95降低了5.17%，超过了最新方法的性能，甚至超过了以前方法的16次样本调整结果。相关代码已发布在<a target="_blank" rel="noopener" href="https://github.com/AuroraZengfh/Local-Prompt%E3%80%82">https://github.com/AuroraZengfh/Local-Prompt。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.04796v3">PDF</a> Accepted by The Thirteenth International Conference on Learning   Representations (ICLR 2025). Code is available at   <a target="_blank" rel="noopener" href="https://github.com/AuroraZengfh/Local-Prompt">https://github.com/AuroraZengfh/Local-Prompt</a></p>
<p><strong>Summary</strong><br>在分布式外检测（OOD）中，区分已知类别与异常值至关重要。现有的视觉语言模型（VLM）主要用于全局提示优化，忽视了局部信息的重要性。本文提出一种新的方法Local-Prompt，旨在通过局部提示进行微调，以强调区域增强。它包括全局提示引导负增强和局部提示增强区域正则化两个组成部分。该方法可无缝集成到训练好的全局提示中以提高性能。实验证明，该方法在ImageNet-1k数据集上的表现优于当前其他方法。此外，本文还提出了一个用于强化OOD检测的局部相关度量指标。总的来说，该方法对增强异常检测能力有积极影响。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>OOD检测旨在区分已知类别与异常值，在实际场景中尤为重要。</li>
<li>现有的VLM模型主要关注全局提示优化，忽略了局部信息的重要性。</li>
<li>Local-Prompt是一种新的方法，旨在通过局部提示进行微调以增强区域信息。它包括全局提示引导负增强和局部提示增强区域正则化两个主要组成部分。</li>
<li>该方法可以无缝集成到训练好的全局提示中，以提高性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.04796">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2409.04796v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2409.04796v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2409.04796v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Meta-Prompting-for-AI-Systems"><a href="#Meta-Prompting-for-AI-Systems" class="headerlink" title="Meta Prompting for AI Systems"></a>Meta Prompting for AI Systems</h2><p><strong>Authors:Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao</strong></p>
<p>We introduce Meta Prompting (MP), a prompting paradigm designed to enhance the utilization of large language models (LLMs) and AI systems in complex problem-solving and data interaction. Grounded in type theory and category theory, Meta Prompting prioritizes structural and syntactical considerations over traditional content-centric methods. In this work, we formally define Meta Prompting, delineate its distinctions from few-shot prompting, and demonstrate its effectiveness across various AI applications. In particular, we show that Meta Prompting can decompose intricate reasoning tasks into simpler sub-problems, thereby improving token efficiency and enabling fairer comparisons with conventional few-shot techniques. Furthermore, we extend this framework to prompting tasks, allowing LLMs to recursively self-generate refined prompts in a metaprogramming-like manner. Empirical evaluations reveal that a Qwen-72B base language model equipped with Meta Prompting-without additional instruction tuning-achieves a PASS@1 accuracy of 46.3% on MATH problems, surpassing a supervised fine-tuned counterpart, 83.5% accuracy on GSM8K, and a 100% success rate on Game of 24 tasks using GPT-4. The code is available at <a target="_blank" rel="noopener" href="https://github.com/meta-prompting/meta-prompting">https://github.com/meta-prompting/meta-prompting</a>. </p>
<blockquote>
<p>我们介绍了Meta Prompting（MP），这是一种旨在提高大型语言模型（LLM）和人工智能系统在复杂问题解决和数据交互中的利用率的提示范式。基于类型理论和范畴理论，Meta Prompting 优先考虑结构和句法因素，而非传统的内容中心方法。在这项工作中，我们正式定义了Meta Prompting，详细阐述了它与少样本提示的区别，并在各种人工智能应用中证明了其有效性。特别是，我们展示了Meta Prompting能够将复杂的推理任务分解成更简单的子问题，从而提高标记效率，并能以更公平的方式与常规少样本技术进行比。此外，我们将此框架扩展到提示任务上，使LLM能够递归地自我生成精细化的提示，类似于元编程的方式。实证评估表明，配备Meta Prompting的Quwen-7 - 在不使用任何额外指令调整的情况下在MATH问题上达到了46.3%的PASS@1准确率，超过了经过监督微调的对标模型；在GSM8K上达到了83.5%的准确率；在Game of 24任务上使用GPT-4达到了100%的成功率。代码可在<a target="_blank" rel="noopener" href="https://github.com/meta-prompting/meta-prompting%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/meta-prompting/meta-prompting获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11482v7">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Meta Prompting（MP）这一新型提示范式，旨在提高大型语言模型（LLMs）和AI系统在复杂问题解决和数据交互中的利用效率。该研究基于类型理论和范畴理论，强调结构和句法因素，而非传统的内容中心方法。通过正式定义Meta Prompting，并详细阐述其与few-shot提示的区别，展示其在各种AI应用中的有效性。特别是，Meta Prompting能将复杂的推理任务分解为更简单的子问题，提高令牌效率，并与传统的few-shot技术进行更公平的比较。此外，该研究还将此框架扩展到提示任务，使LLMs能够以类似元编程的方式递归地自我生成精细提示。实证评估表明，配备Meta Prompting的Qwen-72B基础语言模型，无需额外的指令调整，在MATH问题上达到46.3%的PASS@1准确率，超越经过监督微调的对标模型；在GSM8K上达到83.5%的准确率；在Game of 24任务上使用GPT-4实现100%的成功率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Meta Prompting是一种新型的提示范式，旨在增强大型语言模型和AI系统在复杂问题解决和数据交互中的表现。</li>
<li>Meta Prompting基于类型理论和范畴理论，注重结构和句法因素。</li>
<li>Meta Prompting能将复杂的推理任务分解为更简单的子问题，提高令牌效率。</li>
<li>Meta Prompting在多种AI应用中表现出有效性，包括MATH问题、GSM8K和Game of 24任务。</li>
<li>配备Meta Prompting的Qwen-72B模型在MATH问题上实现较高的PASS@1准确率。</li>
<li>Meta Prompting框架可以扩展到提示任务，使LLMs能够自我生成精细提示。</li>
<li>实证评估证明了Meta Prompting的有效性和潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.11482">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2311.11482v7/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2311.11482v7/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Few-Shot/2311.11482v7/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-28/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-28/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-28/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_I2I Translation/2502.19247v1/page_0_0.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-02-28  ProxyTransformation Preshaping Point Cloud Manifold With Proxy   Attention For 3D Visual Grounding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-28/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Agent/2410.02551v2/page_5_2.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-02-28  Agentic Reward Modeling Integrating Human Preferences with Verifiable   Correctness Signals for Reliable Reward Systems
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29774.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
