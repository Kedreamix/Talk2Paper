<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-28  Norm Growth and Stability Challenges in Localized Sequential Knowledge   Editing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19312v1/page_3_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-28-æ›´æ–°"><a href="#2025-02-28-æ›´æ–°" class="headerlink" title="2025-02-28 æ›´æ–°"></a>2025-02-28 æ›´æ–°</h1><h2 id="Norm-Growth-and-Stability-Challenges-in-Localized-Sequential-Knowledge-Editing"><a href="#Norm-Growth-and-Stability-Challenges-in-Localized-Sequential-Knowledge-Editing" class="headerlink" title="Norm Growth and Stability Challenges in Localized Sequential Knowledge   Editing"></a>Norm Growth and Stability Challenges in Localized Sequential Knowledge   Editing</h2><p><strong>Authors:Akshat Gupta, Christine Fang, Atahan Ozdemir, Maochuan Lu, Ahmed Alaa, Thomas Hartvigsen, Gopala Anumanchipalli</strong></p>
<p>This study investigates the impact of localized updates to large language models (LLMs), specifically in the context of knowledge editing - a task aimed at incorporating or modifying specific facts without altering broader model capabilities. We first show that across different post-training interventions like continuous pre-training, full fine-tuning and LORA-based fine-tuning, the Frobenius norm of the updated matrices always increases. This increasing norm is especially detrimental for localized knowledge editing, where only a subset of matrices are updated in a model . We reveal a consistent phenomenon across various editing techniques, including fine-tuning, hypernetwork-based approaches, and locate-and-edit methods: the norm of the updated matrix invariably increases with successive updates. Such growth disrupts model balance, particularly when isolated matrices are updated while the rest of the model remains static, leading to potential instability and degradation of downstream performance. Upon deeper investigations of the intermediate activation vectors, we find that the norm of internal activations decreases and is accompanied by shifts in the subspaces occupied by these activations, which shows that these activation vectors now occupy completely different regions in the representation space compared to the unedited model. With our paper, we highlight the technical challenges with continuous and localized sequential knowledge editing and their implications for maintaining model stability and utility. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å±€éƒ¨æ›´æ–°å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ¥è¯†ç¼–è¾‘çš„æƒ…å¢ƒä¸­â€”â€”ä¸€é¡¹æ—¨åœ¨èå…¥æˆ–ä¿®æ”¹ç‰¹å®šäº‹å®çš„ä»»åŠ¡ï¼Œè€Œä¸ä¼šæ”¹å˜æ›´å¹¿æ³›çš„æ¨¡å‹èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆå±•ç¤ºäº†åœ¨ä¸åŒçš„åè®­ç»ƒå¹²é¢„æªæ–½ä¸­ï¼Œå¦‚æŒç»­é¢„è®­ç»ƒã€å…¨å¾®è°ƒä»¥åŠåŸºäºLORAçš„å¾®è°ƒï¼Œæ›´æ–°çŸ©é˜µçš„FrobeniusèŒƒæ•°æ€»ä¼šå¢åŠ ã€‚è¿™ç§å¢åŠ çš„èŒƒæ•°å¯¹äºå±€éƒ¨çŸ¥è¯†ç¼–è¾‘å°¤å…¶å…·æœ‰ç ´åæ€§ï¼Œå› ä¸ºåœ¨çŸ¥è¯†ç¼–è¾‘è¿‡ç¨‹ä¸­åªæœ‰æ¨¡å‹ä¸­çš„ä¸€éƒ¨åˆ†çŸ©é˜µè¢«æ›´æ–°ã€‚æˆ‘ä»¬æ­ç¤ºäº†å„ç§ç¼–è¾‘æŠ€æœ¯ä¸­çš„ä¸€è‡´ç°è±¡ï¼ŒåŒ…æ‹¬å¾®è°ƒã€åŸºäºè¶…ç½‘ç»œçš„æ–¹æ³•å’Œå®šä½ç¼–è¾‘æ–¹æ³•ï¼šéšç€è¿ç»­æ›´æ–°çš„è¿›è¡Œï¼Œæ›´æ–°çŸ©é˜µçš„èŒƒæ•°ä¸æ–­å¢åŠ ã€‚è¿™ç§å¢é•¿ç ´åäº†æ¨¡å‹çš„å¹³è¡¡ï¼Œç‰¹åˆ«æ˜¯å½“å­¤ç«‹çš„çŸ©é˜µè¢«æ›´æ–°è€Œå…¶ä½™æ¨¡å‹ä¿æŒé™æ€æ—¶ï¼Œå¯èƒ½å¯¼è‡´æ½œåœ¨çš„ä¸ç¨³å®šæ€§å’Œä¸‹æ¸¸æ€§èƒ½çš„ä¸‹é™ã€‚åœ¨å¯¹ä¸­é—´æ¿€æ´»å‘é‡è¿›è¡Œæ›´æ·±å…¥çš„ç ”ç©¶åï¼Œæˆ‘ä»¬å‘ç°å†…éƒ¨æ¿€æ´»çš„èŒƒæ•°å‡å°‘ï¼Œå¹¶ä¼´éšç€è¿™äº›æ¿€æ´»æ‰€å æ®çš„å­ç©ºé—´çš„è½¬ç§»ï¼Œè¿™è¡¨æ˜è¿™äº›æ¿€æ´»å‘é‡ç°åœ¨ä¸æœªç¼–è¾‘çš„æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨è¡¨ç¤ºç©ºé—´ä¸­å æ®äº†å®Œå…¨ä¸åŒçš„åŒºåŸŸã€‚é€šè¿‡æˆ‘ä»¬çš„è®ºæ–‡ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†è¿ç»­å’Œå±€éƒ¨é¡ºåºçŸ¥è¯†ç¼–è¾‘çš„æŠ€æœ¯æŒ‘æˆ˜åŠå…¶å¯¹ä¿æŒæ¨¡å‹ç¨³å®šæ€§å’Œæ•ˆç”¨çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19416v1">PDF</a> Accepted for Oral Presentation at KnowFM @ AAAI 2025. arXiv admin   note: text overlap with arXiv:2502.01636</p>
<p><strong>Summary</strong></p>
<p>è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å±€éƒ¨æ›´æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ¥è¯†ç¼–è¾‘ä»»åŠ¡ä¸­ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸åŒçš„è®­ç»ƒåå¹²é¢„æªæ–½ï¼Œå¦‚æŒç»­é¢„è®­ç»ƒã€å…¨é¢å¾®è°ƒä»¥åŠåŸºäºLORAçš„å¾®è°ƒï¼Œéƒ½ä¼šå¯¼è‡´æ›´æ–°çŸ©é˜µçš„FrobeniusèŒƒæ•°å¢åŠ ã€‚è¿™ç§ç°è±¡åœ¨å±€éƒ¨çŸ¥è¯†ç¼–è¾‘ä¸­å°¤ä¸ºæ˜æ˜¾ï¼Œå…¶ä¸­åªæœ‰æ¨¡å‹çš„ä¸€éƒ¨åˆ†çŸ©é˜µå¾—åˆ°æ›´æ–°ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œåœ¨å„ç§ç¼–è¾‘æŠ€æœ¯ä¸­ï¼Œå¦‚å¾®è°ƒã€è¶…ç½‘ç»œæ–¹æ³•å’Œå®šä½ç¼–è¾‘æ–¹æ³•ï¼Œéšç€è¿ç»­æ›´æ–°ï¼Œæ›´æ–°çŸ©é˜µçš„èŒƒæ•°ä¸å¯é¿å…åœ°å¢åŠ ã€‚è¿™ç ´åäº†æ¨¡å‹çš„å¹³è¡¡æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä»…æ›´æ–°æŸäº›çŸ©é˜µè€Œå…¶ä½™æ¨¡å‹ä¿æŒé™æ€æ—¶ï¼Œå¯èƒ½å¯¼è‡´ä¸‹æ¸¸æ€§èƒ½çš„ä¸ç¨³å®šç”šè‡³é€€åŒ–ã€‚å¯¹ä¸­é—´æ¿€æ´»å‘é‡çš„æ·±å…¥ç ”ç©¶è¿˜å‘ç°ï¼Œå†…éƒ¨æ¿€æ´»çš„èŒƒæ•°å‡å°‘ï¼Œå¹¶ä¸”è¿™äº›æ¿€æ´»æ‰€å æ®çš„å­ç©ºé—´å‘ç”Ÿå˜åŒ–ã€‚è¿™è¡¨æ˜è¿™äº›æ¿€æ´»å‘é‡åœ¨è¡¨ç¤ºç©ºé—´ä¸­å æ®äº†ä¸æœªç¼–è¾‘æ¨¡å‹å®Œå…¨ä¸åŒçš„åŒºåŸŸã€‚æœ¬æ–‡å¼ºè°ƒäº†è¿ç»­å’Œå±€éƒ¨é¡ºåºçŸ¥è¯†ç¼–è¾‘çš„æŠ€æœ¯æŒ‘æˆ˜åŠå…¶å¯¹ä¿æŒæ¨¡å‹ç¨³å®šæ€§å’Œå®ç”¨æ€§äº§ç”Ÿçš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å±€éƒ¨æ›´æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç ”ç©¶é‡ç‚¹åœ¨çŸ¥è¯†ç¼–è¾‘ä»»åŠ¡ä¸­ã€‚</li>
<li>ä¸åŒè®­ç»ƒåå¹²é¢„æªæ–½ä¼šå¯¼è‡´æ›´æ–°çŸ©é˜µçš„FrobeniusèŒƒæ•°å¢åŠ ã€‚</li>
<li>åœ¨å±€éƒ¨çŸ¥è¯†ç¼–è¾‘ä¸­ï¼Œåªæœ‰æ¨¡å‹çš„ä¸€éƒ¨åˆ†çŸ©é˜µå¾—åˆ°æ›´æ–°ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡å‹å¹³è¡¡æ€§çš„ç ´åã€‚</li>
<li>å„ç§ç¼–è¾‘æŠ€æœ¯ä¸­ï¼Œéšç€è¿ç»­æ›´æ–°ï¼Œæ›´æ–°çŸ©é˜µçš„èŒƒæ•°ä¸å¯é¿å…åœ°å¢åŠ ï¼Œå¯èƒ½å½±å“æ¨¡å‹çš„ç¨³å®šæ€§å’Œä¸‹æ¸¸æ€§èƒ½ã€‚</li>
<li>æ›´æ–°çŸ¥è¯†ç¼–è¾‘åçš„æ¨¡å‹ä¼šå¯¼è‡´ä¸­é—´æ¿€æ´»å‘é‡çš„èŒƒæ•°å‡å°‘å’Œå­ç©ºé—´å˜åŒ–ã€‚</li>
<li>æ¿€æ´»å‘é‡åœ¨è¡¨ç¤ºç©ºé—´ä¸­çš„ä½ç½®å‘ç”Ÿæ˜¾è‘—å˜åŒ–ï¼Œè¿™å¯èƒ½å¯¹æ¨¡å‹çš„æ€§èƒ½äº§ç”Ÿå½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19416">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19416v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19416v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19416v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19416v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19416v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19416v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19416v1/page_4_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DataMan-Data-Manager-for-Pre-training-Large-Language-Models"><a href="#DataMan-Data-Manager-for-Pre-training-Large-Language-Models" class="headerlink" title="DataMan: Data Manager for Pre-training Large Language Models"></a>DataMan: Data Manager for Pre-training Large Language Models</h2><p><strong>Authors:Ru Peng, Kexin Yang, Yawen Zeng, Junyang Lin, Dayiheng Liu, Junbo Zhao</strong></p>
<p>The performance emergence of large language models (LLMs) driven by data scaling laws makes the selection of pre-training data increasingly important. However, existing methods rely on limited heuristics and human intuition, lacking comprehensive and clear guidelines. To address this, we are inspired by &#96;&#96;reverse thinkingâ€™â€™ â€“ prompting LLMs to self-identify which criteria benefit its performance. As its pre-training capabilities are related to perplexity (PPL), we derive 14 quality criteria from the causes of text perplexity anomalies and introduce 15 common application domains to support domain mixing. In this paper, we train a Data Manager (DataMan) to learn quality ratings and domain recognition from pointwise rating, and use it to annotate a 447B token pre-training corpus with 14 quality ratings and domain type. Our experiments validate our approach, using DataMan to select 30B tokens to train a 1.3B-parameter language model, demonstrating significant improvements in in-context learning (ICL), perplexity, and instruction-following ability over the state-of-the-art baseline. The best-performing model, based on the Overall Score l&#x3D;5 surpasses a model trained with 50% more data using uniform sampling. We continue pre-training with high-rated, domain-specific data annotated by DataMan to enhance domain-specific ICL performance and thus verify DataManâ€™s domain mixing ability. Our findings emphasize the importance of quality ranking, the complementary nature of quality criteria, and their low correlation with perplexity, analyzing misalignment between PPL and ICL performance. We also thoroughly analyzed our pre-training dataset, examining its composition, the distribution of quality ratings, and the original document sources. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°æ®è§„æ¨¡å®šå¾‹çš„æ¨åŠ¨ä¸‹ï¼Œå…¶æ€§èƒ½ä¸æ–­æå‡ï¼Œè¿™ä½¿å¾—é¢„è®­ç»ƒæ•°æ®çš„é€‰æ‹©å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºæœ‰é™çš„å¯å‘å¼çŸ¥è¯†å’Œäººä¸ºç›´è§‰ï¼Œç¼ºä¹å…¨é¢ã€æ˜ç¡®çš„æŒ‡å¯¼ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä»â€œé€†å‘æ€ç»´â€ä¸­æ±²å–çµæ„Ÿï¼Œæ¿€åŠ±LLMè‡ªæˆ‘è¯†åˆ«å“ªäº›æ ‡å‡†å¯¹å…¶æ€§èƒ½æœ‰ç›Šã€‚ç”±äºå…¶é¢„è®­ç»ƒèƒ½åŠ›ä¸å›°æƒ‘åº¦ï¼ˆPPLï¼‰ç›¸å…³ï¼Œæˆ‘ä»¬ä»æ–‡æœ¬å›°æƒ‘åº¦å¼‚å¸¸çš„åŸå› ä¸­æ¨å¯¼å‡º14ä¸ªè´¨é‡æ ‡å‡†ï¼Œå¹¶å¼•å…¥15ä¸ªå¸¸è§åº”ç”¨é¢†åŸŸä»¥æ”¯æŒé¢†åŸŸæ··åˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæ•°æ®ç®¡ç†å™¨ï¼ˆDataManï¼‰æ¥å­¦ä¹ è´¨é‡è¯„åˆ†å’Œé¢†åŸŸè¯†åˆ«ï¼Œå¹¶ä½¿ç”¨å®ƒå¯¹ä¸€ä¸ªç”±ç‚¹çŠ¶è¯„åˆ†è¿›è¡Œæ³¨é‡Šçš„åŒ…å«é«˜è¾¾çº¦å››å…†å­—èŠ‚æ•°æ®çš„é¢„è®­ç»ƒè¯­æ–™åº“è¿›è¡Œæ³¨é‡Šã€‚æ­¤å¤–ï¼Œè¯­æ–™åº“åŒ…æ‹¬æ ‡æ³¨ä¸ºé«˜è´¨é‡çš„è¿‘åäº¿è¯æ ‡çš„éƒ¨åˆ†ç”¨äºè®­ç»ƒè¯­è¨€æ¨¡å‹å‚æ•°è¾¾åˆ°ç™¾äº¿çº§åˆ«çš„å¤§è§„æ¨¡æ¨¡å‹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šä¸ä¸šç•Œæœ€æ–°ç®—æ³•æ¯”è¾ƒï¼Œåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€å›°æƒ‘åº¦å’ŒæŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡æ•ˆæœã€‚åŸºäºæ€»ä½“å¾—åˆ†l&#x3D;5çš„æœ€ä½³æ€§èƒ½æ¨¡å‹è¶…è¶Šäº†ä½¿ç”¨å‡åŒ€é‡‡æ ·æ–¹æ³•è®­ç»ƒçš„æ•°æ®é‡å¤šå‡ºç™¾åˆ†ä¹‹äº”åçš„æ¨¡å‹ã€‚æˆ‘ä»¬ç»§ç»­åˆ©ç”¨DataManæ ‡æ³¨çš„é«˜è´¨é‡ã€ç‰¹å®šé¢†åŸŸçš„é¢„è®­ç»ƒæ•°æ®è¿›è¡Œè®­ç»ƒä»¥æé«˜ç‰¹å®šé¢†åŸŸçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›å¹¶éªŒè¯DataMançš„è·¨é¢†åŸŸèåˆèƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶é‡ç‚¹å¼ºè°ƒäº†è´¨é‡æ’åçš„é‡è¦æ€§ã€è´¨é‡æ ‡å‡†ä¹‹é—´çš„äº’è¡¥æ€§ä»¥åŠå®ƒä»¬ä¸å›°æƒ‘åº¦ä¹‹é—´è¾ƒä½çš„ç›¸å…³æ€§ï¼Œå¹¶åˆ†æäº†å›°æƒ‘åº¦ä¸ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ä¹‹é—´å¯èƒ½å­˜åœ¨çš„ä¸å¯¹é½ç°è±¡ã€‚æ­¤å¤–æˆ‘ä»¬è¿˜å…¨é¢åˆ†æäº†æˆ‘ä»¬çš„é¢„è®­ç»ƒæ•°æ®é›†åŠå…¶æ„æˆè´¨é‡è¯„ä¼°ç»“æœä»¥åŠåŸå§‹çš„æ–‡æ¡£æ¥æºç­‰ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19363v1">PDF</a> ICLR2025 paper</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½æå‡å¾—ç›Šäºæ•°æ®è§„æ¨¡å®šå¾‹çš„é©±åŠ¨ï¼Œä½¿å¾—é¢„è®­ç»ƒæ•°æ®çš„é€‰æ‹©å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºæœ‰é™çš„å¯å‘å¼å’Œäººç±»ç›´è§‰ï¼Œç¼ºä¹å…¨é¢æ¸…æ™°çš„æŒ‡å¯¼ã€‚æœ¬æ–‡å—â€œé€†å‘æ€ç»´â€çš„å¯å‘ï¼Œé€šè¿‡æç¤ºLLMè‡ªæˆ‘è¯†åˆ«å¯¹å…¶æ€§èƒ½æœ‰ç›Šçš„æ ‡å‡†æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚æœ¬æ–‡ä»æ–‡æœ¬å›°æƒ‘åº¦å¼‚å¸¸çš„åŸå› ä¸­æ¨å¯¼å‡º14ä¸ªè´¨é‡æ ‡å‡†ï¼Œå¹¶å¼•å…¥15ä¸ªå¸¸è§åº”ç”¨é¢†åŸŸæ”¯æŒé¢†åŸŸæ··åˆã€‚æœ¬æ–‡è®­ç»ƒäº†ä¸€ä¸ªæ•°æ®ç®¡ç†å™¨ï¼ˆDataManï¼‰æ¥å­¦ä¹ ç‚¹è¯„çº§çš„è´¨é‡è¯„ä¼°å’Œé¢†åŸŸè¯†åˆ«ï¼Œå¹¶ç”¨äºæ³¨é‡Šä¸€ä¸ªåŒ…å«447äº¿æ ‡è®°çš„é¢„è®­ç»ƒè¯­æ–™åº“ã€‚é€šè¿‡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä½¿ç”¨DataMané€‰æ‹©äº†è®­ç»ƒè§„æ¨¡ä¸ºä»…è®­ç»ƒå‚æ•°ä¸ºçš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ä¸ºè¯­è¨€æ¨¡å‹ï¼Œåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€å›°æƒ‘åº¦å’ŒæŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›æ–¹é¢å‡ä¼˜äºæœ€æ–°åŸºçº¿æ°´å¹³å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æœ€å¥½çš„æ¨¡å‹æ•´ä½“è¯„åˆ†Lè¶…è¿‡å‡åŒ€æŠ½æ ·å¾—åˆ°çš„åŒ…å«çš„è®­ç»ƒæ¨¡å‹çš„å¾—åˆ†è¡¨ç°å‡ºäº†ä¼˜ç§€çš„æ€§èƒ½ã€‚é€šè¿‡DataManæ ‡æ³¨çš„é«˜è´¨é‡ã€ç‰¹å®šé¢†åŸŸçš„é¢„è®­ç»ƒæ•°æ®ç»§ç»­é¢„è®­ç»ƒï¼Œæé«˜äº†ç‰¹å®šé¢†åŸŸçš„ä¸Šä¸‹æ–‡å­¦ä¹ æ•ˆæœï¼ŒéªŒè¯äº†DataMançš„é¢†åŸŸæ··åˆèƒ½åŠ›ã€‚æœ¬æ–‡å¼ºè°ƒäº†è´¨é‡è¯„ä¼°çš„é‡è¦æ€§ä»¥åŠè´¨é‡æ ‡å‡†çš„äº’è¡¥æ€§å’Œä½ç›¸å…³æ€§åˆ†æäº†å›°æƒ‘åº¦å’Œä¸Šä¸‹æ–‡å­¦ä¹ è¡¨ç°ä¹‹é—´çš„ä¸åŒ¹é…ç°è±¡åŒæ—¶ä¹Ÿæ·±å…¥åˆ†æäº†é¢„è®­ç»ƒæ•°æ®é›†çš„å†…å®¹æ„æˆã€è´¨é‡è¯„çº§åˆ†å¸ƒå’ŒåŸå§‹æ–‡æ¡£æ¥æºã€‚æ€»çš„æ¥è¯´ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®ç®¡ç†æ–¹æ³•ä»¥æé«˜LLMçš„æ€§èƒ½å¹¶æä¾›äº†ä¸°å¯Œçš„è§è§£å’Œå‘ç°ã€‚é€šè¿‡ä¼˜åŒ–æ•°æ®è´¨é‡å’Œé€‰æ‹©ç‰¹å®šé¢†åŸŸçš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹è¿™å¯èƒ½å¯¹æœªæ¥çš„LLMç ”ç©¶å’Œå¼€å‘äº§ç”Ÿé‡è¦å½±å“å¹¶å¸¦æ¥æ˜¾è‘—æ”¹è¿›ã€‚è¿™å°†æå¤§åœ°ä¿ƒè¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•å¹¶æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<p>ä»¥ä¸‹æ˜¯ä»è¯¥æ–‡ä¸­å¾—å‡ºçš„æœ€é‡è¦çš„ä¸ƒä¸ªå‘ç°æˆ–è¦ç‚¹ï¼Œç”¨ä¸­æ–‡è¿›è¡Œç²¾ç®€é˜è¿°ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19363v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19363v1/page_1_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Agentic-Reward-Modeling-Integrating-Human-Preferences-with-Verifiable-Correctness-Signals-for-Reliable-Reward-Systems"><a href="#Agentic-Reward-Modeling-Integrating-Human-Preferences-with-Verifiable-Correctness-Signals-for-Reliable-Reward-Systems" class="headerlink" title="Agentic Reward Modeling: Integrating Human Preferences with Verifiable   Correctness Signals for Reliable Reward Systems"></a>Agentic Reward Modeling: Integrating Human Preferences with Verifiable   Correctness Signals for Reliable Reward Systems</h2><p><strong>Authors:Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Bin Xu, Lei Hou, Juanzi Li</strong></p>
<p>Reward models (RMs) are crucial for the training and inference-time scaling up of large language models (LLMs). However, existing reward models primarily focus on human preferences, neglecting verifiable correctness signals which have shown strong potential in training LLMs. In this paper, we propose agentic reward modeling, a reward system that combines reward models with verifiable correctness signals from different aspects to provide reliable rewards. We empirically implement a reward agent, named RewardAgent, that combines human preference rewards with two verifiable signals: factuality and instruction following, to provide more reliable rewards. We conduct comprehensive experiments on existing reward model benchmarks and inference time best-of-n searches on real-world downstream tasks. RewardAgent significantly outperforms vanilla reward models, demonstrating its effectiveness. We further construct training preference pairs using RewardAgent and train an LLM with the DPO objective, achieving superior performance on various NLP benchmarks compared to conventional reward models. Our codes are publicly released to facilitate further research (<a target="_blank" rel="noopener" href="https://github.com/THU-KEG/Agentic-Reward-Modeling">https://github.com/THU-KEG/Agentic-Reward-Modeling</a>). </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒå’Œæ¨ç†æ—¶é—´æ‰©å±•è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¥–åŠ±æ¨¡å‹ä¸»è¦å…³æ³¨äººç±»åå¥½ï¼Œå¿½ç•¥äº†å¯éªŒè¯çš„æ­£ç¡®æ€§ä¿¡å·ï¼Œè¿™äº›ä¿¡å·åœ¨è®­ç»ƒLLMæ–¹é¢æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä»£ç†å¥–åŠ±å»ºæ¨¡ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆå¥–åŠ±æ¨¡å‹ä¸æ¥è‡ªä¸åŒæ–¹é¢çš„å¯éªŒè¯æ­£ç¡®æ€§ä¿¡å·çš„å¥–åŠ±ç³»ç»Ÿï¼Œä»¥æä¾›å¯é çš„å¥–åŠ±ã€‚æˆ‘ä»¬é€šè¿‡å®è¯å®ç°äº†ä¸€ä¸ªåä¸ºRewardAgentçš„å¥–åŠ±ä»£ç†ï¼Œå®ƒå°†äººç±»åå¥½å¥–åŠ±ä¸ä¸¤ä¸ªå¯éªŒè¯çš„ä¿¡å·ï¼ˆçœŸå®æ€§å’ŒæŒ‡ä»¤éµå¾ªæ€§ï¼‰ç›¸ç»“åˆï¼Œä»¥æä¾›æ›´å¯é çš„å¥–åŠ±ã€‚æˆ‘ä»¬åœ¨ç°æœ‰çš„å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•å’Œç°å®ä¸–ç•Œä¸‹æ¸¸ä»»åŠ¡çš„æ¨ç†æ—¶é—´æœ€ä½³næœç´¢ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒã€‚RewardAgentæ˜¾è‘—ä¼˜äºä¼ ç»Ÿå¥–åŠ±æ¨¡å‹ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨RewardAgentæ„å»ºè®­ç»ƒåå¥½å¯¹ï¼Œå¹¶ä½¿ç”¨DPOç›®æ ‡è®­ç»ƒLLMï¼Œåœ¨å„ç§NLPåŸºå‡†æµ‹è¯•ä¸­ç›¸æ¯”ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä»¥æ–¹ä¾¿è¿›ä¸€æ­¥çš„ç ”ç©¶ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/THU-KEG/Agentic-Reward-Modeling%EF%BC%89%E3%80%82">https://github.com/THU-KEG/Agentic-Reward-Modelingï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19328v1">PDF</a> 16 pages, 5 figures</p>
<p><strong>æ‘˜è¦</strong><br>å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒå’Œæ¨ç†æ—¶é—´æ‰©å±•è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰å¥–åŠ±æ¨¡å‹ä¸»è¦å…³æ³¨äººç±»åå¥½ï¼Œå¿½ç•¥äº†å¯éªŒè¯çš„æ­£ç¡®æ€§ä¿¡å·åœ¨è®­ç»ƒLLMæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚æœ¬æ–‡æå‡ºä»£ç†å¥–åŠ±å»ºæ¨¡ï¼Œä¸€ç§ç»“åˆå¥–åŠ±æ¨¡å‹ä¸ä»ä¸åŒæ–¹é¢è·å¾—çš„å¯éªŒè¯æ­£ç¡®æ€§ä¿¡å·çš„å¥–åŠ±ç³»ç»Ÿï¼Œä»¥æä¾›å¯é çš„å¥–åŠ±ã€‚æˆ‘ä»¬å®è¯åœ°å®ç°äº†ä¸€ä¸ªåä¸ºRewardAgentçš„å¥–åŠ±ä»£ç†ï¼Œå®ƒå°†äººç±»åå¥½å¥–åŠ±ä¸ä¸¤ä¸ªå¯éªŒè¯çš„ä¿¡å·ï¼ˆäº‹å®æ€§å’ŒæŒ‡ä»¤éµå¾ªæ€§ï¼‰ç›¸ç»“åˆï¼Œä»¥æä¾›æ›´å¯é çš„å¥–åŠ±ã€‚æˆ‘ä»¬åœ¨ç°æœ‰çš„å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•å’Œç°å®ä¸–ç•Œä¸‹æ¸¸ä»»åŠ¡çš„æ¨ç†æ—¶é—´æœ€ä½³næœç´¢ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒã€‚RewardAgentæ˜¾è‘—ä¼˜äºä¼ ç»Ÿå¥–åŠ±æ¨¡å‹ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨RewardAgentæ„å»ºè®­ç»ƒåå¥½å¯¹ï¼Œå¹¶ä½¿ç”¨DPOç›®æ ‡è®­ç»ƒLLMï¼Œä¸ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨å„ç§NLPåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä»¥ä¾¿è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/THU-KEG/Agentic-Reward-Modeling%EF%BC%89%E3%80%82">https://github.com/THU-KEG/Agentic-Reward-Modelingï¼‰ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æ—¶é—´æ‰©å±•è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰å¥–åŠ±æ¨¡å‹ä¸»è¦å…³æ³¨äººç±»åå¥½ï¼Œå¿½ç•¥äº†å¯éªŒè¯çš„æ­£ç¡®æ€§ä¿¡å·ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä»£ç†å¥–åŠ±å»ºæ¨¡ï¼Œç»“åˆäº†å¥–åŠ±æ¨¡å‹å’Œå¯éªŒè¯çš„æ­£ç¡®æ€§ä¿¡å·ï¼Œä»¥æä¾›å¯é çš„å¥–åŠ±ã€‚</li>
<li>RewardAgentå®è¯åœ°ç»“åˆäº†äººç±»åå¥½å¥–åŠ±ä¸äº‹å®æ€§å’ŒæŒ‡ä»¤éµå¾ªæ€§ä¸¤ä¸ªå¯éªŒè¯ä¿¡å·ã€‚</li>
<li>RewardAgentåœ¨å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•å’Œæ¨ç†æ—¶é—´æœç´¢ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨RewardAgentæ„å»ºçš„è®­ç»ƒåå¥½å¯¹å’ŒDPOç›®æ ‡è®­ç»ƒçš„LLMåœ¨NLPåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>ç›¸å…³ç ”ç©¶ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºè¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19328v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19328v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19328v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19328v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Shh-donâ€™t-say-that-Domain-Certification-in-LLMs"><a href="#Shh-donâ€™t-say-that-Domain-Certification-in-LLMs" class="headerlink" title="Shh, donâ€™t say that! Domain Certification in LLMs"></a>Shh, donâ€™t say that! Domain Certification in LLMs</h2><p><strong>Authors:Cornelius Emde, Alasdair Paren, Preetham Arvind, Maxime Kayser, Tom Rainforth, Thomas Lukasiewicz, Bernard Ghanem, Philip H. S. Torr, Adel Bibi</strong></p>
<p>Large language models (LLMs) are often deployed to perform constrained tasks, with narrow domains. For example, customer support bots can be built on top of LLMs, relying on their broad language understanding and capabilities to enhance performance. However, these LLMs are adversarially susceptible, potentially generating outputs outside the intended domain. To formalize, assess, and mitigate this risk, we introduce domain certification; a guarantee that accurately characterizes the out-of-domain behavior of language models. We then propose a simple yet effective approach, which we call VALID that provides adversarial bounds as a certificate. Finally, we evaluate our method across a diverse set of datasets, demonstrating that it yields meaningful certificates, which bound the probability of out-of-domain samples tightly with minimum penalty to refusal behavior. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸è¢«éƒ¨ç½²ç”¨äºæ‰§è¡Œå—é™åˆ¶çš„ä»»åŠ¡ï¼Œæ¶‰åŠç‹­çª„çš„åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åœ¨LLMçš„åŸºç¡€ä¸Šæ„å»ºå®¢æˆ·æ”¯æŒæœºå™¨äººï¼Œä¾é å®ƒä»¬å¹¿æ³›çš„è¯­è¨€ç†è§£å’Œèƒ½åŠ›æ¥æé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›LLMå®¹æ˜“å—å¯¹æŠ—å½±å“ï¼Œå¯èƒ½ä¼šç”Ÿæˆä¸åœ¨é¢„å®šèŒƒå›´å†…çš„è¾“å‡ºã€‚ä¸ºäº†æ­£å¼åŒ–ã€è¯„ä¼°å’Œå‡è½»è¿™ç§é£é™©ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸŸè®¤è¯çš„æ¦‚å¿µï¼Œä»¥ç¡®ä¿å‡†ç¡®æè¿°è¯­è¨€æ¨¡å‹çš„è·¨åŸŸè¡Œä¸ºã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºVALIDï¼Œè¯¥æ–¹æ³•æä¾›å¯¹æŠ—è¾¹ç•Œä½œä¸ºè¯ä¹¦ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜å®ƒäº§ç”Ÿäº†æœ‰æ„ä¹‰çš„è¯ä¹¦ï¼Œç´§å¯†åœ°é™åˆ¶äº†è·¨åŸŸæ ·æœ¬çš„æ¦‚ç‡ï¼Œå¹¶ä¸”å¯¹æ‹’ç»è¡Œä¸ºçš„å½±å“æœ€å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19320v1">PDF</a> 10 pages, includes appendix Published in International Conference on   Learning Representations (ICLR) 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­çš„éƒ¨ç½²å¾€å¾€é¢ä¸´æ¨¡å‹æ€§èƒ½çº¦æŸã€‚å°½ç®¡å¯ä»¥æ„å»ºåŸºäºLLMçš„å®¢æˆ·æ”¯æŒæœºå™¨äººï¼Œå¹¶åˆ©ç”¨å…¶å¹¿æ³›çš„è¯­è¨€ç†è§£å’Œå¢å¼ºæ€§èƒ½èƒ½åŠ›ï¼Œä½†è¿™äº›LLMså®¹æ˜“å—æ”»å‡»å¹¶å¯èƒ½äº§ç”Ÿè¶…å‡ºé¢„æœŸé¢†åŸŸçš„è¾“å‡ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥é¢†åŸŸè®¤è¯æ¥æ­£å¼è¯„ä¼°å¹¶ç¼“è§£è¿™ç§é£é™©ï¼Œç¡®ä¿è¯­è¨€æ¨¡å‹çš„è·¨é¢†åŸŸè¡Œä¸ºå¾—åˆ°å‡†ç¡®è¡¨å¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„éªŒè¯æ–¹æ³•VALIDï¼Œä¸ºæ¨¡å‹æä¾›å¯¹æŠ—æ€§è¾¹ç•Œè¯ä¹¦ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæœ‰æ„ä¹‰çš„è¯ä¹¦ï¼Œç´§å¯†çº¦æŸè·¨é¢†åŸŸæ ·æœ¬çš„æ¦‚ç‡ï¼Œå¯¹æ‹’ç»è¡Œä¸ºçš„å½±å“æœ€å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¸¸ç”¨äºæ‰§è¡Œç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ï¼Œä½†å­˜åœ¨æ€§èƒ½çº¦æŸå’Œæ˜“å—æ”»å‡»çš„é£é™©ã€‚</li>
<li>å®¢æˆ·æ”¯æŒæœºå™¨äººä¾èµ–äºLLMsè¿›è¡Œå¹¿æ³›çš„å¯¹è¯å’Œäº¤æµåŠŸèƒ½ã€‚</li>
<li>LLMså¯èƒ½äº§ç”Ÿè¶…å‡ºé¢„æœŸé¢†åŸŸçš„è¾“å‡ºï¼Œå¯¼è‡´æ½œåœ¨é£é™©ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼•å…¥äº†é¢†åŸŸè®¤è¯çš„æ¦‚å¿µæ¥æ­£å¼è¯„ä¼°å¹¶ä¿éšœè¯­è¨€æ¨¡å‹çš„è·¨é¢†åŸŸè¡Œä¸ºã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºVALIDçš„ç®€å•éªŒè¯æ–¹æ³•ï¼Œä¸ºè¯­è¨€æ¨¡å‹æä¾›å¯¹æŠ—æ€§è¾¹ç•Œè¯ä¹¦ã€‚</li>
<li>é€šè¿‡å¤šç§æ•°æ®é›†è¯„ä¼°æ˜¾ç¤ºï¼ŒVALIDæ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæœ‰æ„ä¹‰çš„è¯ä¹¦ï¼Œæœ‰æ•ˆçº¦æŸè·¨é¢†åŸŸæ ·æœ¬çš„æ¦‚ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19320v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19320v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19320v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FSPO-Few-Shot-Preference-Optimization-of-Synthetic-Preference-Data-in-LLMs-Elicits-Effective-Personalization-to-Real-Users"><a href="#FSPO-Few-Shot-Preference-Optimization-of-Synthetic-Preference-Data-in-LLMs-Elicits-Effective-Personalization-to-Real-Users" class="headerlink" title="FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in   LLMs Elicits Effective Personalization to Real Users"></a>FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in   LLMs Elicits Effective Personalization to Real Users</h2><p><strong>Authors:Anikait Singh, Sheryl Hsu, Kyle Hsu, Eric Mitchell, Stefano Ermon, Tatsunori Hashimoto, Archit Sharma, Chelsea Finn</strong></p>
<p>Effective personalization of LLMs is critical for a broad range of user-interfacing applications such as virtual assistants and content curation. Inspired by the strong in-context learning capabilities of LLMs, we propose Few-Shot Preference Optimization (FSPO), which reframes reward modeling as a meta-learning problem. Under this framework, an LLM learns to quickly adapt to a user via a few labeled preferences from that user, constructing a personalized reward function for them. Additionally, since real-world preference data is scarce and challenging to collect at scale, we propose careful design choices to construct synthetic preference datasets for personalization, generating over 1M synthetic personalized preferences using publicly available LLMs. In particular, to successfully transfer from synthetic data to real users, we find it crucial for the data to exhibit both high diversity and coherent, self-consistent structure. We evaluate FSPO on personalized open-ended generation for up to 1,500 synthetic users across across three domains: movie reviews, pedagogical adaptation based on educational background, and general question answering, along with a controlled human study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in generating responses that are personalized to synthetic users and a 72% winrate with real human users in open-ended question answering. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆä¸ªæ€§åŒ–å¯¹äºä¼—å¤šç”¨æˆ·æ¥å£åº”ç”¨ç¨‹åºï¼ˆå¦‚è™šæ‹ŸåŠ©ç†å’Œå†…å®¹ç­–åˆ’ï¼‰è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å—åˆ°LLMå¼ºå¤§ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›çš„å¯å‘ï¼Œæå‡ºäº†å°æ ·æœ¬åå¥½ä¼˜åŒ–ï¼ˆFSPOï¼‰æ–¹æ³•ï¼Œå®ƒå°†å¥–åŠ±å»ºæ¨¡é‡æ–°å®šä¹‰ä¸ºå…ƒå­¦ä¹ é—®é¢˜ã€‚åœ¨è¯¥æ¡†æ¶ä¸‹ï¼ŒLLMé€šè¿‡å¿«é€Ÿé€‚åº”å°‘é‡ç”¨æˆ·åå¥½æ ‡ç­¾æ¥å­¦ä¹ ä¸ºç”¨æˆ·æœåŠ¡ï¼Œå¹¶ä¸ºä»–ä»¬æ„å»ºä¸ªæ€§åŒ–çš„å¥–åŠ±åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œç”±äºç°å®ä¸–ç•Œä¸­çš„åå¥½æ•°æ®ç¨€ç¼ºä¸”éš¾ä»¥å¤§è§„æ¨¡æ”¶é›†ï¼Œæˆ‘ä»¬æå‡ºäº†ç²¾å¿ƒè®¾è®¡é€‰æ‹©æ¥æ„å»ºä¸ªæ€§åŒ–åˆæˆæ•°æ®é›†ï¼Œä½¿ç”¨å…¬å¼€å¯ç”¨çš„LLMç”Ÿæˆè¶…è¿‡100ä¸‡ä¸ªåˆæˆä¸ªæ€§åŒ–åå¥½ã€‚ç‰¹åˆ«æ˜¯ï¼Œä¸ºäº†æˆåŠŸåœ°ä»åˆæˆæ•°æ®è½¬ç§»åˆ°çœŸå®ç”¨æˆ·ï¼Œæˆ‘ä»¬å‘ç°æ•°æ®çš„é«˜å¤šæ ·æ€§å’Œè¿è´¯ã€è‡ªæˆ‘ä¸€è‡´çš„ç»“æ„è‡³å…³é‡è¦ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªé¢†åŸŸå¯¹FSPOè¿›è¡Œäº†è¯„ä¼°ï¼šç”µå½±è¯„è®ºã€åŸºäºæ•™è‚²èƒŒæ™¯çš„æ•™å­¦é€‚åº”å’Œä¸€èˆ¬é—®ç­”ï¼Œä»¥åŠä¸€é¡¹å—æ§çš„äººç±»ç ”ç©¶ã€‚æ€»ä½“è€Œè¨€ï¼ŒFSPOåœ¨é’ˆå¯¹åˆæˆç”¨æˆ·çš„ä¸ªæ€§åŒ–ç”Ÿæˆæ–¹é¢å¹³å‡è¾¾åˆ°äº†87%çš„Alpaca Evalèƒœç‡ï¼Œåœ¨å¼€æ”¾å¼é—®ç­”æ–¹é¢ä¸çœŸå®äººç±»ç”¨æˆ·çš„å¯¹æˆ˜ä¸­èƒœç‡è¾¾åˆ°äº†72%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19312v1">PDF</a> Website: <a target="_blank" rel="noopener" href="https://fewshot-preference-optimization.github.io/">https://fewshot-preference-optimization.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>LLMä¸ªæ€§åŒ–å¯¹äºå¹¿æ³›çš„ç”¨æˆ·ç•Œé¢åº”ç”¨è‡³å…³é‡è¦ï¼Œå¦‚è™šæ‹ŸåŠ©ç†å’Œå†…å®¹ç­–åˆ’ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºFew-Shot Preference Optimizationï¼ˆFSPOï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å°‘é‡ç”¨æˆ·åå¥½æ•°æ®å¿«é€Ÿé€‚åº”ä¸ªæ€§åŒ–å¥–åŠ±å‡½æ•°å»ºæ¨¡ã€‚ä¸ºè§£å†³çœŸå®åå¥½æ•°æ®ç¨€ç¼ºå’Œéš¾ä»¥å¤§è§„æ¨¡æ”¶é›†çš„é—®é¢˜ï¼Œæœ¬æ–‡è®¾è®¡äº†åˆæˆåå¥½æ•°æ®é›†ç”¨äºä¸ªæ€§åŒ–ç”Ÿæˆè¶…è¿‡1ç™¾ä¸‡çš„åˆæˆåå¥½æ•°æ®ã€‚å®éªŒè¯æ˜ï¼ŒFSPOåœ¨ä¸ªæ€§åŒ–å¼€æ”¾ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¯¹åˆæˆç”¨æˆ·å¹³å‡èƒœç‡è¾¾87%ï¼ŒçœŸå®ç”¨æˆ·å¼€æ”¾é—®ç­”ä»»åŠ¡ä¸­èƒœç‡ä¸º72%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMä¸ªæ€§åŒ–å¯¹äºç”¨æˆ·ç•Œé¢åº”ç”¨å¦‚è™šæ‹ŸåŠ©ç†å’Œå†…å®¹ç­–åˆ’è‡³å…³é‡è¦ã€‚</li>
<li>Few-Shot Preference Optimizationï¼ˆFSPOï¼‰é€šè¿‡å°‘é‡ç”¨æˆ·åå¥½æ•°æ®å¿«é€Ÿé€‚åº”ä¸ªæ€§åŒ–å¥–åŠ±å‡½æ•°å»ºæ¨¡ã€‚</li>
<li>æå‡ºåˆæˆåå¥½æ•°æ®é›†è®¾è®¡æ¥è§£å†³çœŸå®åå¥½æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>æˆåŠŸä»åˆæˆæ•°æ®è½¬ç§»åˆ°çœŸå®ç”¨æˆ·éœ€è¦æ•°æ®å±•ç°é«˜å¤šæ ·æ€§å’Œè¿è´¯æ€§ç»“æ„ã€‚</li>
<li>FSPOåœ¨ä¸ªæ€§åŒ–å¼€æ”¾ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬ç”µå½±è¯„è®ºã€æ•™è‚²èƒŒæ™¯å¯¼å‘çš„æ•™å­¦é€‚åº”å’Œä¸€èˆ¬é—®ç­”é¢†åŸŸã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19312v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19312v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19312v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Efficient-Federated-Search-for-Retrieval-Augmented-Generation"><a href="#Efficient-Federated-Search-for-Retrieval-Augmented-Generation" class="headerlink" title="Efficient Federated Search for Retrieval-Augmented Generation"></a>Efficient Federated Search for Retrieval-Augmented Generation</h2><p><strong>Authors:Rachid Guerraoui, Anne-Marie Kermarrec, Diana Petrescu, Rafael Pires, Mathis Randl, Martijn de Vos</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities across various domains but remain susceptible to hallucinations and inconsistencies, limiting their reliability. Retrieval-augmented generation (RAG) mitigates these issues by grounding model responses in external knowledge sources. Existing RAG workflows often leverage a single vector database, which is impractical in the common setting where information is distributed across multiple repositories. We introduce RAGRoute, a novel mechanism for federated RAG search. RAGRoute dynamically selects relevant data sources at query time using a lightweight neural network classifier. By not querying every data source, this approach significantly reduces query overhead, improves retrieval efficiency, and minimizes the retrieval of irrelevant information. We evaluate RAGRoute using the MIRAGE and MMLU benchmarks and demonstrate its effectiveness in retrieving relevant documents while reducing the number of queries. RAGRoute reduces the total number of queries up to 77.5% and communication volume up to 76.2%. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªé¢†åŸŸè¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†ä»æ˜“å—åˆ°é”™è§‰å’Œä¸ä¸€è‡´æ€§çš„å›°æ‰°ï¼Œè¿™é™åˆ¶äº†å…¶å¯é æ€§ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡ä»¥å¤–éƒ¨çŸ¥è¯†èµ„æºä¸ºåŸºç¡€æ¥å‡è½»è¿™äº›é—®é¢˜ã€‚ç°æœ‰çš„RAGå·¥ä½œæµç¨‹é€šå¸¸åˆ©ç”¨å•ä¸ªå‘é‡æ•°æ®åº“ï¼Œè¿™åœ¨ä¿¡æ¯åˆ†å¸ƒåœ¨å¤šä¸ªå­˜å‚¨åº“ä¸­çš„å¸¸è§ç¯å¢ƒä¸­å¹¶ä¸å®ç”¨ã€‚æˆ‘ä»¬å¼•å…¥äº†RAGRouteï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè”é‚¦RAGæœç´¢çš„æ–°æœºåˆ¶ã€‚RAGRouteä½¿ç”¨è½»é‡çº§ç¥ç»ç½‘ç»œåˆ†ç±»å™¨åœ¨æŸ¥è¯¢æ—¶åŠ¨æ€é€‰æ‹©ç›¸å…³æ•°æ®æºã€‚è¿™ç§æ–¹æ³•ä¸æŸ¥è¯¢æ¯ä¸ªæ•°æ®æºï¼Œå› æ­¤å¯ä»¥æ˜¾è‘—é™ä½æŸ¥è¯¢å¼€é”€ï¼Œæé«˜æ£€ç´¢æ•ˆç‡ï¼Œå¹¶å°½é‡å‡å°‘æ£€ç´¢æ— å…³ä¿¡æ¯ã€‚æˆ‘ä»¬ä½¿ç”¨MIRAGEå’ŒMMLUåŸºå‡†æµ‹è¯•è¯„ä¼°äº†RAGRouteï¼Œå¹¶è¯æ˜äº†å…¶åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£æ—¶çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶å‡å°‘äº†æŸ¥è¯¢æ¬¡æ•°ã€‚RAGRouteæœ€å¤šå¯å‡å°‘77.5%çš„æ€»æŸ¥è¯¢æ¬¡æ•°å’Œé«˜è¾¾76.2%çš„é€šä¿¡é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19280v1">PDF</a> To appear in the proceedings of EuroMLSysâ€™25</p>
<p><strong>Summary</strong></p>
<p>LLMå­˜åœ¨å¯é æ€§é—®é¢˜ï¼Œå¦‚æ˜“äº§ç”Ÿå¹»è§‰å’Œç»“æœä¸ä¸€è‡´ç­‰ã€‚ä¸ºæé«˜æ¨¡å‹å“åº”çš„å¯é æ€§ï¼Œæå‡ºäº†åŸºäºå¤–éƒ¨çŸ¥è¯†æºçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰RAGå·¥ä½œæµç¨‹ä¾èµ–äºå•ä¸€å‘é‡æ•°æ®åº“ï¼Œä¸é€‚ç”¨äºä¿¡æ¯åˆ†æ•£äºå¤šä¸ªå­˜å‚¨åº“çš„åœºæ™¯ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„è”é‚¦RAGæœç´¢æœºåˆ¶â€”â€”RAGRouteã€‚å®ƒé€šè¿‡è½»é‡çº§ç¥ç»ç½‘ç»œåˆ†ç±»å™¨åŠ¨æ€é€‰æ‹©ç›¸å…³æ•°æ®æºè¿›è¡ŒæŸ¥è¯¢ï¼Œå‡å°‘äº†æŸ¥è¯¢å¼€é”€ï¼Œæé«˜äº†æ£€ç´¢æ•ˆç‡å¹¶é™ä½äº†è·å–æ— å…³ä¿¡æ¯çš„é£é™©ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒRAGRouteåœ¨å‡å°‘æŸ¥è¯¢æ•°é‡å’Œé€šä¿¡ä½“ç§¯æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè™½åœ¨å¤šä¸ªé¢†åŸŸå±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨å¯é æ€§å’Œä¸€è‡´æ€§æ–¹é¢çš„é—®é¢˜ï¼Œå¦‚æ˜“äº§ç”Ÿå¹»è§‰ã€‚</li>
<li>RAGæ–¹æ³•é€šè¿‡ç»“åˆå¤–éƒ¨çŸ¥è¯†æºæé«˜äº†LLMçš„å“åº”å¯é æ€§ã€‚</li>
<li>ç°æœ‰RAGå·¥ä½œæµç¨‹ä¸»è¦ä¾èµ–å•ä¸€å‘é‡æ•°æ®åº“ï¼Œé™åˆ¶äº†å…¶åœ¨å¤šæ•°æ®æºåœºæ™¯çš„åº”ç”¨ã€‚</li>
<li>RAGRouteæ˜¯ä¸€ç§æ–°å‹çš„è”é‚¦RAGæœç´¢æœºåˆ¶ï¼Œé€šè¿‡åŠ¨æ€é€‰æ‹©ç›¸å…³æ•°æ®æºè¿›è¡ŒæŸ¥è¯¢ï¼Œæé«˜äº†æ•ˆç‡å¹¶é™ä½äº†è·å–æ— å…³ä¿¡æ¯çš„é£é™©ã€‚</li>
<li>RAGRouteåˆ©ç”¨è½»é‡çº§ç¥ç»ç½‘ç»œåˆ†ç±»å™¨æ¥é€‰æ‹©æ•°æ®æºï¼Œæ˜¾è‘—å‡å°‘äº†æŸ¥è¯¢å¼€é”€ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼ŒRAGRouteåœ¨å‡å°‘æŸ¥è¯¢æ•°é‡å’Œé€šä¿¡ä½“ç§¯æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>RAGRouteçš„æ–¹æ³•å¯¹äºæé«˜LLMåœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½å’Œå¯é æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19280">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19280v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19280v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19280v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19280v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19280v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19280v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LiGT-Layout-infused-Generative-Transformer-for-Visual-Question-Answering-on-Vietnamese-Receipts"><a href="#LiGT-Layout-infused-Generative-Transformer-for-Visual-Question-Answering-on-Vietnamese-Receipts" class="headerlink" title="LiGT: Layout-infused Generative Transformer for Visual Question   Answering on Vietnamese Receipts"></a>LiGT: Layout-infused Generative Transformer for Visual Question   Answering on Vietnamese Receipts</h2><p><strong>Authors:Thanh-Phong Le, Trung Le Chi Phan, Nghia Hieu Nguyen, Kiet Van Nguyen</strong></p>
<p>\textbf{Purpose:} Document Visual Question Answering (document VQA) challenges multimodal systems to holistically handle textual, layout, and visual modalities to provide appropriate answers. Document VQA has gained popularity in recent years due to the increasing amount of documents and the high demand for digitization. Nonetheless, most of document VQA datasets are developed in high-resource languages such as English.   \textbf{Methods:} In this paper, we present ReceiptVQA (\textbf{Receipt} \textbf{V}isual \textbf{Q}uestion \textbf{A}nswering), the initial large-scale document VQA dataset in Vietnamese dedicated to receipts, a document kind with high commercial potentials. The dataset encompasses \textbf{9,000+} receipt images and \textbf{60,000+} manually annotated question-answer pairs. In addition to our study, we introduce LiGT (\textbf{L}ayout-\textbf{i}nfused \textbf{G}enerative \textbf{T}ransformer), a layout-aware encoder-decoder architecture designed to leverage embedding layers of language models to operate layout embeddings, minimizing the use of additional neural modules.   \textbf{Results:} Experiments on ReceiptVQA show that our architecture yielded promising performance, achieving competitive results compared with outstanding baselines. Furthermore, throughout analyzing experimental results, we found evident patterns that employing encoder-only model architectures has considerable disadvantages in comparison to architectures that can generate answers. We also observed that it is necessary to combine multiple modalities to tackle our dataset, despite the critical role of semantic understanding from language models.   \textbf{Conclusion:} We hope that our work will encourage and facilitate future development in Vietnamese document VQA, contributing to a diverse multimodal research community in the Vietnamese language. </p>
<blockquote>
<p><strong>ç›®çš„</strong>ï¼šæ–‡æ¡£è§†è§‰é—®ç­”ï¼ˆDocument Visual Question Answeringï¼Œç®€ç§°Document VQAï¼‰æŒ‘æˆ˜äº†å¤šæ¨¡æ€ç³»ç»Ÿï¼Œä½¿å…¶èƒ½å¤Ÿå…¨é¢å¤„ç†æ–‡æœ¬ã€å¸ƒå±€å’Œè§†è§‰æ¨¡å¼ï¼Œä»¥æä¾›é€‚å½“çš„ç­”æ¡ˆã€‚ç”±äºæ–‡æ¡£æ•°é‡çš„ä¸æ–­å¢åŠ å’Œæ•°å­—åŒ–éœ€æ±‚çš„ä¸æ–­å¢é•¿ï¼Œæ–‡æ¡£VQAè¿‘å¹´æ¥å˜å¾—éå¸¸å—æ¬¢è¿ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–‡æ¡£VQAæ•°æ®é›†éƒ½æ˜¯ç”¨èµ„æºä¸°å¯Œçš„è¯­è¨€ï¼ˆå¦‚è‹±è¯­ï¼‰å¼€å‘çš„ã€‚</p>
</blockquote>
<p><strong>æ–¹æ³•</strong>ï¼šåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ReceiptVQAï¼ˆæ”¶æ®è§†è§‰é—®ç­”ï¼‰ï¼Œè¿™æ˜¯è¶Šå—è¯­ä¸­é’ˆå¯¹æ”¶æ®çš„åˆå§‹å¤§å‹æ–‡æ¡£VQAæ•°æ®é›†ã€‚æ”¶æ®æ˜¯ä¸€ç§å•†ä¸šæ½œåŠ›å¾ˆé«˜çš„æ–‡ä»¶ç±»å‹ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡<strong>9,000å¼ </strong>æ”¶æ®å›¾åƒå’Œè¶…è¿‡<strong>6ä¸‡å¯¹</strong>æ‰‹åŠ¨æ ‡æ³¨çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚é™¤äº†æˆ‘ä»¬çš„ç ”ç©¶ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†LiGTï¼ˆå¸ƒå±€æ³¨å…¥ç”Ÿæˆå™¨Transformerï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¸ƒå±€æ„ŸçŸ¥çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œæ—¨åœ¨åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„åµŒå…¥å±‚è¿›è¡Œæ“ä½œå¸ƒå±€åµŒå…¥ï¼Œå°½é‡å‡å°‘ä½¿ç”¨é¢å¤–çš„ç¥ç»ç½‘ç»œæ¨¡å—ã€‚</p>
<p><strong>ç»“æœ</strong>ï¼šåœ¨ReceiptVQAä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¶æ„æ€§èƒ½ä»¤äººé¼“èˆï¼Œä¸åŸºå‡†æµ‹è¯•ç›¸æ¯”å–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ†æå®éªŒç»“æœï¼Œæˆ‘ä»¬å‘ç°ä¸èƒ½å¤Ÿç”Ÿæˆç­”æ¡ˆçš„æ¶æ„ç›¸æ¯”ï¼Œä½¿ç”¨ä»…ç¼–ç å™¨æ¨¡å‹æ¶æ„å…·æœ‰æ˜æ˜¾çš„åŠ£åŠ¿ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼Œå°½ç®¡è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç†è§£èµ·ç€å…³é”®ä½œç”¨ï¼Œä½†ç»“åˆå¤šç§æ¨¡å¼æ¥è§£å†³æˆ‘ä»¬çš„æ•°æ®é›†æ˜¯å¿…è¦çš„ã€‚</p>
<p><strong>ç»“è®º</strong>ï¼šæˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½é¼“åŠ±å’Œä¿ƒè¿›è¶Šå—è¯­æ–‡æ¡£VQAçš„å‘å±•ï¼Œä¸ºè¶Šå—è¯­çš„å¤šæ¨¡æ€ç ”ç©¶ç¤¾åŒºåšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19202v1">PDF</a> Accepted at IJDAR</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¶Šå—è¯­æ”¶æ®æ–‡æ¡£çš„å¤§è§„æ¨¡è§†è§‰é—®ç­”æ•°æ®é›†ReceiptVQAçš„å¼€å‘ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡9000å¼ æ”¶æ®å›¾åƒå’Œè¶…è¿‡6ä¸‡ä¸ªæ‰‹åŠ¨æ ‡æ³¨çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†å¸ƒå±€æ„ŸçŸ¥ç”Ÿæˆå¼è½¬æ¢å™¨æ¶æ„LiGTï¼Œè¯¥æ¶æ„åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„åµŒå…¥å±‚è¿›è¡Œå¸ƒå±€åµŒå…¥ï¼Œæœ€å°åŒ–é¢å¤–ç¥ç»ç½‘ç»œæ¨¡å—çš„ä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¶æ„åœ¨ReceiptVQAä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æ­¤å¤–ï¼Œåˆ†æå®éªŒç»“æœè¿˜è¡¨æ˜ï¼Œä¸ä½¿ç”¨ç¼–ç å™¨çš„æ¨¡å‹æ¶æ„ç›¸æ¯”ï¼Œç”Ÿæˆå¼ç­”æ¡ˆçš„æ¶æ„å…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚æ–‡ç« è®¤ä¸ºæœ‰å¿…è¦ç»“åˆå¤šç§æ¨¡æ€æ¥è§£å†³æ–‡æ¡£è§†è§‰é—®ç­”é—®é¢˜ï¼ŒåŒæ—¶ä¹Ÿå¼ºè°ƒäº†è¯­è¨€æ¨¡å‹è¯­ä¹‰ç†è§£çš„é‡è¦æ€§ã€‚å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½ä¿ƒè¿›è¶Šå—è¯­æ–‡æ¡£è§†è§‰é—®ç­”çš„å‘å±•ï¼Œå¹¶ä¸ºå¤šæ ·åŒ–çš„å¤šåª’ä½“ç ”ç©¶ç¤¾åŒºåšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>æ–‡æ¡£è§†è§‰é—®ç­”ï¼ˆDocument VQAï¼‰æ˜¯å¤„ç†æ–‡æœ¬ã€å¸ƒå±€å’Œè§†è§‰æ¨¡æ€çš„å¤šæ¨¡æ€ç³»ç»Ÿçš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œç›®å‰ç”±äºæ•°å­—åŒ–éœ€æ±‚è€Œå¤‡å—å…³æ³¨ã€‚</li>
<li>ç›®å‰å¤§å¤šæ•°æ–‡æ¡£VQAæ•°æ®é›†éƒ½æ˜¯é’ˆå¯¹è‹±è¯­ç­‰èµ„æºä¸°å¯Œçš„è¯­è¨€å¼€å‘çš„ã€‚æœ¬æ–‡å¼•å…¥äº†é’ˆå¯¹è¶Šå—è¯­æ”¶æ®çš„ReceiptVQAæ•°æ®é›†ï¼Œå¡«è¡¥äº†è¶Šå—è¯­æ–‡æ¡£VQAæ•°æ®é›†çš„ç©ºç™½ã€‚</li>
<li>ä»‹ç»äº†LiGTæ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§å¸ƒå±€æ„ŸçŸ¥ç”Ÿæˆå¼è½¬æ¢å™¨ï¼Œèƒ½å¤Ÿåˆ©ç”¨è¯­è¨€æ¨¡å‹çš„åµŒå…¥å±‚è¿›è¡Œå¸ƒå±€åµŒå…¥ï¼Œå‡å°‘äº†é¢å¤–ç¥ç»ç½‘ç»œæ¨¡å—çš„ä½¿ç”¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLiGTæ¶æ„åœ¨ReceiptVQAæ•°æ®é›†ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</li>
<li>åˆ†æå®éªŒç»“æœè¿˜å‘ç°ï¼Œç”Ÿæˆç­”æ¡ˆçš„æ¨¡å‹æ¶æ„æ¯”ä»…ä½¿ç”¨ç¼–ç å™¨çš„æ¶æ„å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</li>
<li>åœ¨è§£å†³æ–‡æ¡£è§†è§‰é—®ç­”é—®é¢˜æ—¶ï¼Œéœ€è¦ç»“åˆå¤šç§æ¨¡æ€ï¼ŒåŒæ—¶é‡è§†è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç†è§£ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19202">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19202v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.19202v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="M2-omni-Advancing-Omni-MLLM-for-Comprehensive-Modality-Support-with-Competitive-Performance"><a href="#M2-omni-Advancing-Omni-MLLM-for-Comprehensive-Modality-Support-with-Competitive-Performance" class="headerlink" title="M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with   Competitive Performance"></a>M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with   Competitive Performance</h2><p><strong>Authors:Qingpei Guo, Kaiyou Song, Zipeng Feng, Ziping Ma, Qinglong Zhang, Sirui Gao, Xuzheng Yu, Yunxiao Sun,  Tai-WeiChang, Jingdong Chen, Ming Yang, Jun Zhou</strong></p>
<p>We present M2-omni, a cutting-edge, open-source omni-MLLM that achieves competitive performance to GPT-4o. M2-omni employs a unified multimodal sequence modeling framework, which empowers Large Language Models(LLMs) to acquire comprehensive cross-modal understanding and generation capabilities. Specifically, M2-omni can process arbitrary combinations of audio, video, image, and text modalities as input, generating multimodal sequences interleaving with audio, image, or text outputs, thereby enabling an advanced and interactive real-time experience. The training of such an omni-MLLM is challenged by significant disparities in data quantity and convergence rates across modalities. To address these challenges, we propose a step balance strategy during pre-training to handle the quantity disparities in modality-specific data. Additionally, a dynamically adaptive balance strategy is introduced during the instruction tuning stage to synchronize the modality-wise training progress, ensuring optimal convergence. Notably, we prioritize preserving strong performance on pure text tasks to maintain the robustness of M2-omniâ€™s language understanding capability throughout the training process. To our best knowledge, M2-omni is currently a very competitive open-source model to GPT-4o, characterized by its comprehensive modality and task support, as well as its exceptional performance. We expect M2-omni will advance the development of omni-MLLMs, thus facilitating future research in this domain. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†M2-omniï¼Œè¿™æ˜¯ä¸€æ¬¾å…ˆè¿›çš„å¼€æºé€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå…¶æ€§èƒ½ä¸GPT-4oç›¸å½“ã€‚M2-omnié‡‡ç”¨ç»Ÿä¸€çš„å¤šæ¨¡æ€åºåˆ—å»ºæ¨¡æ¡†æ¶ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å…¨é¢çš„è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒM2-omniå¯ä»¥å¤„ç†éŸ³é¢‘ã€è§†é¢‘ã€å›¾åƒå’Œæ–‡æœ¬ç­‰å¤šç§æ¨¡å¼çš„ä»»æ„ç»„åˆä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆäº¤æ›¿çš„éŸ³é¢‘ã€å›¾åƒæˆ–æ–‡æœ¬è¾“å‡ºï¼Œä»è€Œå®ç°å…ˆè¿›ä¸”äº¤äº’å¼çš„å®æ—¶ä½“éªŒã€‚è®­ç»ƒè¿™æ ·çš„é€šç”¨å¤šæ¨¡æ€LLMé¢ä¸´çš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ï¼Œä¸åŒæ¨¡æ€çš„æ•°æ®é‡å’Œæ”¶æ•›ç‡å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æå‡ºäº†ä¸€ç§æ­¥éª¤å¹³è¡¡ç­–ç•¥ï¼Œä»¥å¤„ç†ç‰¹å®šæ¨¡æ€æ•°æ®åœ¨æ•°é‡ä¸Šçš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œåœ¨æŒ‡ä»¤è°ƒæ•´é˜¶æ®µå¼•å…¥äº†ä¸€ç§åŠ¨æ€è‡ªé€‚åº”å¹³è¡¡ç­–ç•¥ï¼Œä»¥åŒæ­¥ä¸åŒæ¨¡æ€çš„è®­ç»ƒè¿›åº¦ï¼Œç¡®ä¿æœ€ä½³æ”¶æ•›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä¼˜å…ˆåœ¨çº¯æ–‡æœ¬ä»»åŠ¡ä¸Šä¿æŒå“è¶Šæ€§èƒ½ï¼Œä»¥åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒM2-omniçš„è¯­è¨€ç†è§£èƒ½åŠ›çš„ç¨³å¥æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒM2-omniç›®å‰æ˜¯ä¸€ä¸ªéå¸¸å…·æœ‰ç«äº‰åŠ›çš„å¼€æºæ¨¡å‹ï¼Œç±»ä¼¼äºGPT-4oï¼Œå…¶ç‰¹ç‚¹æ˜¯æ”¯æŒå…¨é¢çš„æ¨¡æ€å’Œä»»åŠ¡ï¼Œä»¥åŠå‡ºè‰²çš„æ€§èƒ½ã€‚æˆ‘ä»¬ç›¸ä¿¡M2-omniå°†æ¨åŠ¨é€šç”¨å¤šæ¨¡æ€LLMçš„å‘å±•ï¼Œä»è€Œæ¨åŠ¨è¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18778v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>M2-omniæ˜¯ä¸€æ¬¾å…ˆè¿›çš„å¼€æºé€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå¯å¤„ç†å¤šç§æ¨¡æ€è¾“å…¥å¹¶ç”Ÿæˆå¤šæ¨¡æ€åºåˆ—è¾“å‡ºã€‚å®ƒé‡‡ç”¨ç»Ÿä¸€çš„åºåˆ—å»ºæ¨¡æ¡†æ¶ï¼Œé€šè¿‡é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒä¼˜é˜¶æ®µçš„å¹³è¡¡ç­–ç•¥ï¼Œåº”å¯¹ä¸åŒæ¨¡æ€æ•°æ®é‡å’Œæ”¶æ•›ç‡çš„å·®å¼‚ã€‚M2-omniå¯¹çº¯æ–‡æœ¬ä»»åŠ¡ä¿æŒå‡ºè‰²æ€§èƒ½ï¼Œä»¥ç¡®ä¿è¯­è¨€ç†è§£èƒ½åŠ›çš„ç¨³å¥æ€§ã€‚ä¸GPT-4oç›¸æ¯”ï¼ŒM2-omniå…·æœ‰å…¨é¢çš„æ¨¡æ€å’Œä»»åŠ¡æ”¯æŒä»¥åŠå“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>M2-omniæ˜¯ä¸€æ¬¾å…ˆè¿›çš„å¼€æºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>å®ƒå…·å¤‡è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå¯å¤„ç†å¤šç§æ¨¡æ€è¾“å…¥å¹¶ç”Ÿæˆå¤šæ¨¡æ€åºåˆ—è¾“å‡ºã€‚</li>
<li>M2-omnié‡‡ç”¨ç»Ÿä¸€çš„åºåˆ—å»ºæ¨¡æ¡†æ¶ã€‚</li>
<li>é¢„è®­ç»ƒé˜¶æ®µé‡‡ç”¨æ­¥éª¤å¹³è¡¡ç­–ç•¥ï¼Œä»¥å¤„ç†ä¸åŒæ¨¡æ€æ•°æ®é‡çš„å·®å¼‚ã€‚</li>
<li>åœ¨æŒ‡ä»¤è°ƒä¼˜é˜¶æ®µå¼•å…¥åŠ¨æ€è‡ªé€‚åº”å¹³è¡¡ç­–ç•¥ï¼Œä»¥ç¡®ä¿å„æ¨¡æ€çš„è®­ç»ƒè¿›åº¦åŒæ­¥å¹¶ä¼˜åŒ–æ”¶æ•›ã€‚</li>
<li>M2-omniåœ¨çº¯æ–‡æœ¬ä»»åŠ¡ä¸Šä¿æŒå‡ºè‰²æ€§èƒ½ï¼Œä»¥ç¡®ä¿è¯­è¨€ç†è§£èƒ½åŠ›çš„ç¨³å¥æ€§ã€‚</li>
<li>M2-omniåœ¨ç»¼åˆæ¨¡æ€å’Œä»»åŠ¡æ”¯æŒä»¥åŠæ€§èƒ½è¡¨ç°æ–¹é¢ä¸GPT-4oç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18778">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18778v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18778v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18778v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18778v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18778v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Exploring-Graph-Tasks-with-Pure-LLMs-A-Comprehensive-Benchmark-and-Investigation"><a href="#Exploring-Graph-Tasks-with-Pure-LLMs-A-Comprehensive-Benchmark-and-Investigation" class="headerlink" title="Exploring Graph Tasks with Pure LLMs: A Comprehensive Benchmark and   Investigation"></a>Exploring Graph Tasks with Pure LLMs: A Comprehensive Benchmark and   Investigation</h2><p><strong>Authors:Yuxiang Wang, Xinnan Dai, Wenqi Fan, Yao Ma</strong></p>
<p>Graph-structured data has become increasingly prevalent across various domains, raising the demand for effective models to handle graph tasks like node classification and link prediction. Traditional graph learning models like Graph Neural Networks (GNNs) have made significant strides, but their capabilities in handling graph data remain limited in certain contexts. In recent years, large language models (LLMs) have emerged as promising candidates for graph tasks, yet most studies focus primarily on performance benchmarks and fail to address their broader potential, including their ability to handle limited data, their transferability across tasks, and their robustness. In this work, we provide a comprehensive exploration of LLMs applied to graph tasks. We evaluate the performance of pure LLMs, including those without parameter optimization and those fine-tuned with instructions, across various scenarios. Our analysis goes beyond accuracy, assessing LLM ability to perform in few-shot&#x2F;zero-shot settings, transfer across domains, understand graph structures, and demonstrate robustness in challenging scenarios. We conduct extensive experiments with 16 graph learning models alongside 6 LLMs (e.g., Llama3B, GPT-4o, Qwen-plus), comparing their performance on datasets like Cora, PubMed, ArXiv, and Products. Our findings show that LLMs, particularly those with instruction tuning, outperform traditional models in few-shot settings, exhibit strong domain transferability, and demonstrate excellent generalization and robustness. This work offers valuable insights into the capabilities of LLMs for graph learning, highlighting their advantages and potential for real-world applications, and paving the way for future research in this area. Codes and datasets are released in <a target="_blank" rel="noopener" href="https://github.com/myflashbarry/LLM-benchmarking">https://github.com/myflashbarry/LLM-benchmarking</a>. </p>
<blockquote>
<p>å›¾å½¢ç»“æ„åŒ–æ•°æ®åœ¨å„ä¸ªé¢†åŸŸçš„æ™®åŠç‡è¶Šæ¥è¶Šé«˜ï¼Œå¯¹å¤„ç†å›¾å½¢ä»»åŠ¡ï¼ˆå¦‚èŠ‚ç‚¹åˆ†ç±»å’Œé“¾æ¥é¢„æµ‹ï¼‰çš„æœ‰æ•ˆæ¨¡å‹çš„éœ€æ±‚ä¹Ÿåœ¨å¢åŠ ã€‚ä¼ ç»Ÿçš„å›¾å½¢å­¦ä¹ æ¨¡å‹ï¼Œå¦‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ï¼Œå·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨æŸäº›ä¸Šä¸‹æ–‡ä¸­å¤„ç†å›¾å½¢æ•°æ®çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°æˆä¸ºå›¾å½¢ä»»åŠ¡çš„æœ‰å‰é€”çš„å€™é€‰è€…ï¼Œä½†å¤§å¤šæ•°ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ€§èƒ½åŸºå‡†æµ‹è¯•ä¸Šï¼Œæœªèƒ½è§£å†³å…¶æ›´å¹¿æ³›çš„æ½œåŠ›ï¼ŒåŒ…æ‹¬å¤„ç†æœ‰é™æ•°æ®çš„èƒ½åŠ›ã€è·¨ä»»åŠ¡çš„è¿ç§»èƒ½åŠ›ä»¥åŠç¨³å¥æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹LLMåœ¨å›¾å½¢ä»»åŠ¡ä¸­çš„åº”ç”¨è¿›è¡Œäº†å…¨é¢çš„æ¢ç´¢ã€‚æˆ‘ä»¬è¯„ä¼°äº†çº¯LLMçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é‚£äº›æœªç»å‚æ•°ä¼˜åŒ–å’Œç»è¿‡æŒ‡ä»¤å¾®è°ƒçš„æƒ…å†µã€‚æˆ‘ä»¬çš„åˆ†æè¶…è¶Šäº†å‡†ç¡®æ€§ï¼Œè¯„ä¼°äº†LLMåœ¨å°‘é‡æ ·æœ¬&#x2F;é›¶æ ·æœ¬è®¾ç½®ä¸­çš„è¡¨ç°ã€è·¨åŸŸçš„è¿ç§»èƒ½åŠ›ã€å¯¹å›¾å½¢ç»“æ„çš„ç†è§£ï¼Œä»¥åŠåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§é‡çš„å®éªŒä¸16ä¸ªå›¾å½¢å­¦ä¹ æ¨¡å‹å’Œ6ä¸ªLLMï¼ˆå¦‚Llama3Bã€GPT-4oã€Qwen-plusï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œæ¯”è¾ƒå®ƒä»¬åœ¨Coraã€PubMedã€ArXivå’ŒProductsç­‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMï¼ˆç‰¹åˆ«æ˜¯ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„LLMï¼‰åœ¨å°‘é‡æ ·æœ¬è®¾ç½®ä¸­çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œå±•ç°å‡ºå¼ºå¤§çš„åŸŸè¿ç§»èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–å’Œç¨³å¥æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºLLMåœ¨å›¾å½¢å­¦ä¹ æ–¹é¢çš„èƒ½åŠ›æä¾›äº†å®è´µçš„è§è§£ï¼Œçªå‡ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿å’Œæ½œåŠ›ï¼Œä¸ºè¿™ä¸€é¢†åŸŸçš„æœªæ¥ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚ä»£ç å’Œæ•°æ®é›†å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/myflashbarry/LLM-benchmarking">https://github.com/myflashbarry/LLM-benchmarking</a>å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18771v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å›¾ç»“æ„æ•°æ®åœ¨å„ä¸ªé¢†åŸŸçš„æ™®åŠä½¿å¾—å¯¹å¤„ç†å›¾ä»»åŠ¡çš„æœ‰æ•ˆæ¨¡å‹çš„éœ€æ±‚æ—¥ç›Šå¢åŠ ï¼Œå¦‚èŠ‚ç‚¹åˆ†ç±»å’Œé“¾æ¥é¢„æµ‹ã€‚ä¼ ç»Ÿçš„å›¾å­¦ä¹ æ¨¡å‹ï¼Œå¦‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ï¼Œå·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹å¤„ç†å›¾æ•°æ®çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²æˆä¸ºå›¾ä»»åŠ¡çš„æœ‰å‰é€”çš„å€™é€‰è€…ï¼Œä½†å¤§å¤šæ•°ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ€§èƒ½åŸºå‡†æµ‹è¯•ä¸Šï¼Œæœªèƒ½è§£å†³å…¶æ›´å¹¿æ³›çš„æ½œåŠ›ï¼ŒåŒ…æ‹¬å¤„ç†æœ‰é™æ•°æ®çš„èƒ½åŠ›ã€è·¨ä»»åŠ¡çš„è¿ç§»èƒ½åŠ›ä»¥åŠç¨³å¥æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹LLMsåœ¨å›¾ä»»åŠ¡ä¸Šçš„åº”ç”¨è¿›è¡Œäº†å…¨é¢çš„æ¢ç´¢ã€‚æˆ‘ä»¬è¯„ä¼°äº†çº¯LLMsçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é‚£äº›æ²¡æœ‰ç»è¿‡å‚æ•°ä¼˜åŒ–å’Œç»è¿‡æŒ‡ä»¤å¾®è°ƒçš„æƒ…å†µã€‚æˆ‘ä»¬çš„åˆ†æè¶…è¶Šäº†å‡†ç¡®æ€§ï¼Œè¯„ä¼°äº†LLMåœ¨å°‘é‡æ ·æœ¬&#x2F;é›¶æ ·æœ¬è®¾ç½®ä¸­çš„è¡¨ç°ã€è·¨åŸŸçš„è¿ç§»èƒ½åŠ›ã€å¯¹å›¾ç»“æ„çš„ç†è§£ä»¥åŠæŒ‘æˆ˜æ€§åœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬åœ¨ä¸16ä¸ªå›¾å­¦ä¹ æ¨¡å‹çš„å¤§é‡å®éªŒä¸­ï¼Œä¸LLMsï¼ˆå¦‚Llama3Bã€GPT-4oã€Qwen-plusï¼‰è¿›è¡Œäº†æ¯”è¾ƒï¼Œä»–ä»¬åœ¨Coraã€PubMedã€ArXivå’ŒProductsç­‰æ•°æ®é›†ä¸Šçš„è¡¨ç°å¦‚ä½•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsï¼Œå°¤å…¶æ˜¯ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„LLMsï¼Œåœ¨å°‘é‡æ ·æœ¬è®¾ç½®ä¸­çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œå…·æœ‰è¾ƒå¼ºçš„åŸŸè¿ç§»èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºå‡ºè‰²çš„æ³›åŒ–å’Œç¨³å¥æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºLLMsåœ¨å›¾å­¦ä¹ æ–¹é¢çš„èƒ½åŠ›æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œçªå‡ºäº†å…¶ä¼˜ç‚¹å’Œæ½œåœ¨çš„å®é™…åº”ç”¨ï¼Œå¹¶ä¸ºè¿™ä¸€é¢†åŸŸçš„æœªæ¥ç ”å¥ å®šäº†åŸºç¡€ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å·²å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/myflashbarry/LLM-benchmarking%E4%B8%8A%E4%BE%9B%E6%9F%A5%E7%A9%B6%E5%92%8C%E5%85%AC%E4%BC%97%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/myflashbarry/LLM-benchmarkingä¸Šä¾›æŸ¥é˜…ç ”ç©¶è€…å’Œå…¬ä¼—ä½¿ç”¨ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMsåœ¨å›¾ä»»åŠ¡ä¸Šçš„è¡¨ç°æ—¥ç›Šçªå‡ºï¼Œç‰¹åˆ«æ˜¯åœ¨èŠ‚ç‚¹åˆ†ç±»å’Œé“¾æ¥é¢„æµ‹æ–¹é¢ã€‚</li>
<li>ä¸ä¼ ç»Ÿå›¾å­¦ä¹ æ¨¡å‹ç›¸æ¯”ï¼ŒLLMsåœ¨æœ‰é™æ•°æ®æƒ…å†µä¸‹å±•ç°å‡ºæ›´å¼ºçš„å¤„ç†èƒ½åŠ›ã€‚</li>
<li>LLMså…·æœ‰å¼ºå¤§çš„è·¨åŸŸè¿ç§»èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒæ•°æ®é›†ä¹‹é—´æœ‰æ•ˆè½¬æ¢çŸ¥è¯†ã€‚</li>
<li>LLMsèƒ½å¤Ÿç†è§£å›¾ç»“æ„ï¼Œè¿™æœ‰åŠ©äºå®ƒä»¬åœ¨å›¾ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>åœ¨æŒ‘æˆ˜æ€§åœºæ™¯ä¸­ï¼ŒLLMså±•ç°å‡ºä¼˜ç§€çš„æ³›åŒ–å’Œç¨³å¥æ€§ã€‚</li>
<li>ç»è¿‡æŒ‡ä»¤è°ƒæ•™çš„LLMsåœ¨å°‘é‡æ ·æœ¬è®¾ç½®ä¸­çš„è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18771">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18771v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18771v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18771v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18771v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18771v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Comprehensive-Analysis-of-Transparency-and-Accessibility-of-ChatGPT-DeepSeek-And-other-SoTA-Large-Language-Models"><a href="#Comprehensive-Analysis-of-Transparency-and-Accessibility-of-ChatGPT-DeepSeek-And-other-SoTA-Large-Language-Models" class="headerlink" title="Comprehensive Analysis of Transparency and Accessibility of ChatGPT,   DeepSeek, And other SoTA Large Language Models"></a>Comprehensive Analysis of Transparency and Accessibility of ChatGPT,   DeepSeek, And other SoTA Large Language Models</h2><p><strong>Authors:Ranjan Sapkota, Shaina Raza, Manoj Karkee</strong></p>
<p>Despite increasing discussions on open-source Artificial Intelligence (AI), existing research lacks a discussion on the transparency and accessibility of state-of-the-art (SoTA) Large Language Models (LLMs). The Open Source Initiative (OSI) has recently released its first formal definition of open-source software. This definition, when combined with standard dictionary definitions and the sparse published literature, provide an initial framework to support broader accessibility to AI models such as LLMs, but more work is essential to capture the unique dynamics of openness in AI. In addition, concerns about open-washing, where models claim openness but lack full transparency, has been raised, which limits the reproducibility, bias mitigation, and domain adaptation of these models. In this context, our study critically analyzes SoTA LLMs from the last five years, including ChatGPT, DeepSeek, LLaMA, and others, to assess their adherence to transparency standards and the implications of partial openness. Specifically, we examine transparency and accessibility from two perspectives: open-source vs. open-weight models. Our findings reveal that while some models are labeled as open-source, this does not necessarily mean they are fully open-sourced. Even in the best cases, open-source models often do not report model training data, and code as well as key metrics, such as weight accessibility, and carbon emissions. To the best of our knowledge, this is the first study that systematically examines the transparency and accessibility of over 100 different SoTA LLMs through the dual lens of open-source and open-weight models. The findings open avenues for further research and call for responsible and sustainable AI practices to ensure greater transparency, accountability, and ethical deployment of these models.(DeepSeek transparency, ChatGPT accessibility, open source, DeepSeek open source) </p>
<blockquote>
<p>å°½ç®¡å…³äºå¼€æºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„è®¨è®ºæ—¥ç›Šå¢å¤šï¼Œä½†ç°æœ‰ç ”ç©¶ç¼ºä¹å¯¹æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€æ˜åº¦å’Œå¯è®¿é—®æ€§çš„è®¨è®ºã€‚å¼€æ”¾æºä»£ç å€¡è®®ç»„ç»‡ï¼ˆOSIï¼‰æœ€è¿‘å‘å¸ƒäº†å…¶å…³äºå¼€æºè½¯ä»¶çš„é¦–ä¸ªæ­£å¼å®šä¹‰ã€‚è¯¥å®šä¹‰ä¸æ ‡å‡†è¯å…¸å®šä¹‰å’Œå·²å‘å¸ƒçš„ç¨€å°‘æ–‡çŒ®ç›¸ç»“åˆï¼Œä¸ºæ”¯æŒå¦‚LLMç­‰AIæ¨¡å‹çš„æ›´å¹¿æ³›å¯è®¿é—®æ€§æä¾›äº†åˆæ­¥æ¡†æ¶ï¼Œä½†è¦æ•æ‰AIä¸­å¼€æ”¾æ€§çš„ç‹¬ç‰¹åŠ¨æ€è¿˜éœ€è¦æ›´å¤šçš„å·¥ä½œã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†å¯¹â€œå¼€æºæ´—ç™½â€çš„æ‹…å¿§ï¼Œå³æ¨¡å‹å£°ç§°æ˜¯å¼€æ”¾çš„ï¼Œä½†å¹¶ä¸å®Œå…¨é€æ˜ï¼Œè¿™é™åˆ¶äº†è¿™äº›æ¨¡å‹çš„å¯å¤ç°æ€§ã€åè§ç¼“è§£å’Œé¢†åŸŸé€‚åº”æ€§ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ä»è¿‡å»äº”å¹´çš„æœ€å…ˆè¿›LLMå‡ºå‘ï¼ŒåŒ…æ‹¬ChatGPTã€DeepSeekã€LLaMAç­‰ï¼Œå¯¹å…¶æ˜¯å¦ç¬¦åˆé€æ˜åº¦æ ‡å‡†ä»¥åŠéƒ¨åˆ†å¼€æ”¾æ€§çš„å½±å“è¿›è¡Œäº†æ‰¹åˆ¤æ€§åˆ†æã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»ä¸¤ä¸ªè§’åº¦è€ƒå¯Ÿé€æ˜åº¦å’Œå¯è®¿é—®æ€§ï¼šå¼€æºä¸å¼€æ”¾å¼æƒé‡æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ä¸€äº›æ¨¡å‹è¢«æ ‡è®°ä¸ºå¼€æºï¼Œä½†è¿™å¹¶ä¸æ„å‘³ç€å®ƒä»¬å®Œå…¨æ˜¯å¼€æ”¾çš„ã€‚å³ä½¿åœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œå¼€æºæ¨¡å‹ä¹Ÿç»å¸¸ä¸æŠ¥å‘Šæ¨¡å‹è®­ç»ƒæ•°æ®ã€ä»£ç ä»¥åŠå…³é”®æŒ‡æ ‡ï¼Œå¦‚æƒé‡å¯è®¿é—®æ€§å’Œç¢³æ’æ”¾ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹é€šè¿‡å¼€æºå’Œå¼€æ”¾å¼æƒé‡æ¨¡å‹çš„åŒé‡è§†è§’ç³»ç»Ÿåœ°è€ƒå¯Ÿè¶…è¿‡100ç§æœ€å…ˆè¿›LLMçš„é€æ˜åº¦å’Œå¯è®¿é—®æ€§çš„ç ”ç©¶ã€‚è¯¥ç ”ç©¶ä¸ºè¿›ä¸€æ­¥çš„æ¢ç´¢æ‰“å¼€äº†é€”å¾„ï¼Œå¹¶å‘¼åé‡‡å–è´Ÿè´£ä»»å’Œå¯æŒç»­çš„AIå®è·µï¼Œä»¥ç¡®ä¿è¿™äº›æ¨¡å‹çš„æ›´å¤§é€æ˜åº¦ã€é—®è´£åˆ¶å’Œé“å¾·éƒ¨ç½²ã€‚ï¼ˆDeepSeekçš„é€æ˜åº¦ã€ChatGPTçš„å¯è®¿é—®æ€§ã€å¼€æºã€DeepSeekçš„å¼€æºï¼‰</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18505v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è®¨è®ºäº†å…³äºå¼€æºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„è®¨è®ºå¢åŠ ï¼Œä½†å¯¹æœ€å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†å¤§æ¨¡å‹ï¼ˆLLMï¼‰çš„é€æ˜åº¦å’Œå¯è®¿é—®æ€§çš„è®¨è®ºä»ç„¶ç¼ºä¹ã€‚Open Source Initiativeï¼ˆOSIï¼‰æœ€è¿‘å‘å¸ƒäº†å…¶é¦–ä¸ªå…³äºå¼€æºè½¯ä»¶çš„æ­£å¼å®šä¹‰ï¼Œä¸ºAIæ¨¡å‹çš„å¼€æ”¾æ€§å’Œå¯è®¿é—®æ€§æä¾›äº†åˆæ­¥æ¡†æ¶ã€‚ç„¶è€Œï¼Œæœ‰å…³â€œå¼€æ”¾æ´—ç™½â€ï¼ˆå£°ç§°å¼€æ”¾æ€§ä½†å®é™…ä¸Šç¼ºä¹é€æ˜åº¦ï¼‰çš„æ‹…å¿§é™åˆ¶äº†æ¨¡å‹çš„å¯é‡å¤æ€§ã€åè§ç¼“è§£å’Œé¢†åŸŸé€‚åº”æ€§ã€‚æœ¬ç ”ç©¶ä»é€æ˜åº¦å’Œå¯è®¿é—®æ€§çš„è§’åº¦è¯„ä¼°äº†è¿‘å¹´æ¥çš„æœ€å…ˆè¿›çš„LLMï¼Œå‘ç°å³ä½¿ä¸€äº›æ¨¡å‹è¢«æ ‡è®°ä¸ºå¼€æºï¼Œä¹Ÿå¹¶ä¸ä¸€å®šå®Œå…¨å¼€æ”¾æºä»£ç ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°è€ƒå¯Ÿäº†è¶…è¿‡ä¸€ç™¾ç§æœ€å…ˆè¿›çš„LLMçš„é€æ˜åº¦å’Œå¯è®¿é—®æ€§ï¼Œå¼€å¯äº†è¿›ä¸€æ­¥ç ”ç©¶çš„é€”å¾„ï¼Œå¹¶å‘¼åé‡‡å–è´Ÿè´£ä»»å’Œå¯æŒç»­çš„AIå®è·µï¼Œä»¥ç¡®ä¿è¿™äº›æ¨¡å‹çš„é€æ˜åº¦ã€é—®è´£åˆ¶å’Œé“å¾·éƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†å¤§æ¨¡å‹ï¼ˆLLMï¼‰çš„é€æ˜åº¦å’Œå¯è®¿é—®æ€§è®¨è®ºä»ç„¶ä¸è¶³ã€‚</li>
<li>Open Source Initiativeï¼ˆOSIï¼‰å‘å¸ƒäº†å…³äºå¼€æºè½¯ä»¶çš„æ­£å¼å®šä¹‰ï¼Œä¸ºAIæ¨¡å‹çš„å¼€æ”¾æ€§å’Œå¯è®¿é—®æ€§æä¾›äº†åˆæ­¥æ¡†æ¶ã€‚</li>
<li>å­˜åœ¨â€œå¼€æ”¾æ´—ç™½â€ç°è±¡ï¼Œå³æ¨¡å‹å£°ç§°å¼€æ”¾æ€§ä½†å®é™…ä¸Šç¼ºä¹é€æ˜åº¦ã€‚</li>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†è¿‘å¹´æ¥çš„LLMé€æ˜åº¦å’Œå¯è®¿é—®æ€§ï¼Œå‘ç°ä¸€äº›è¢«æ ‡è®°ä¸ºå¼€æºçš„æ¨¡å‹å¹¶ä¸å®Œå…¨å¼€æ”¾æºä»£ç ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯å¼€æºçš„æ¨¡å‹ï¼Œå…¶è®­ç»ƒæ•°æ®ã€ä»£ç å’Œä¸€äº›å…³é”®æŒ‡æ ‡ï¼ˆå¦‚æƒé‡å¯è®¿é—®æ€§å’Œç¢³æ’æ”¾ï¼‰å¾€å¾€ä¹Ÿæœªè¢«å…¬å¼€ã€‚</li>
<li>æœ¬ç ”ç©¶ç³»ç»Ÿåœ°è€ƒå¯Ÿäº†è¶…è¿‡ä¸€ç™¾ç§æœ€å…ˆè¿›çš„LLMçš„é€æ˜åº¦å’Œå¯è®¿é—®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18505">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18505v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18505v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18505v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18505v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18505v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18505v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.18505v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="InductionBench-LLMs-Fail-in-the-Simplest-Complexity-Class"><a href="#InductionBench-LLMs-Fail-in-the-Simplest-Complexity-Class" class="headerlink" title="InductionBench: LLMs Fail in the Simplest Complexity Class"></a>InductionBench: LLMs Fail in the Simplest Complexity Class</h2><p><strong>Authors:Wenyue Hua, Tyler Wong, Sun Fei, Liangming Pan, Adam Jardine, William Yang Wang</strong></p>
<p>Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMsâ€™ inductive reasoning capabilities. Coda and data are available <a target="_blank" rel="noopener" href="https://github.com/Wenyueh/inductive_reasoning_benchmark">https://github.com/Wenyueh/inductive_reasoning_benchmark</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œè®¸å¤šç°æœ‰åŸºå‡†æµ‹è¯•ï¼ˆå¦‚o1å’Œo3ï¼‰å·²å¾—åˆ°æ¨¡å‹çš„å…¨ç›˜æˆ–éƒ¨åˆ†è§£å†³ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è¿™äº›åŸºå‡†æµ‹è¯•ä¾§é‡äºæ¼”ç»æ¨ç†ï¼ŒåŒ…æ‹¬æ•°å­¦å’Œç¼–ç ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡ä¸­çš„è§„åˆ™ï¼ˆå¦‚æ•°å­¦å…¬ç†æˆ–ç¼–ç¨‹è¯­æ³•ï¼‰æ˜¯æ˜ç¡®å®šä¹‰çš„ï¼ŒLLMå¯ä»¥æ ¹æ®è¿™äº›è§„åˆ™è¿›è¡Œè®¡åˆ’å¹¶åº”ç”¨å®ƒä»¬ä»¥å¾—å‡ºè§£å†³æ–¹æ¡ˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä»è§‚å¯Ÿåˆ°çš„æ•°æ®ä¸­æ¨æ–­å‡ºæ½œåœ¨è§„åˆ™çš„å½’çº³æ¨ç†ä»è¾ƒå°‘æ¢ç´¢ã€‚è¿™æ ·çš„å½’çº³è¿‡ç¨‹å¤„äºç§‘å­¦å‘ç°çš„æ ¸å¿ƒï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿä½¿ç ”ç©¶äººå‘˜ä»å®è¯è§‚å¯Ÿä¸­æå–ä¸€èˆ¬åŸåˆ™ã€‚ä¸ºäº†è¯„ä¼°LLMæ˜¯å¦å…·å¤‡è¿™ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†InductionBenchè¿™ä¸€æ–°åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMçš„å½’çº³æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨åŠŸèƒ½å­å¸¸è§„å±‚æ¬¡ç»“æ„ä¸­æœ€ç®€å•çš„ç±»åˆ«ä¸­ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿå¾ˆéš¾æŒæ¡å…¶å¤æ‚æ€§ï¼Œè¿™çªæ˜¾äº†å½“å‰LLMå½’çº³æ¨ç†èƒ½åŠ›çš„æ˜¾è‘—ä¸è¶³ã€‚ä»£ç å’Œæ•°æ®å¯ç”¨<a target="_blank" rel="noopener" href="https://github.com/Wenyueh/inductive_reasoning_benchmark%E3%80%82">https://github.com/Wenyueh/inductive_reasoning_benchmarkã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15823v2">PDF</a> 24 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œç°æœ‰è®¸å¤šåŸºå‡†æµ‹è¯•å·²è¢«æ¨¡å‹å¦‚o1å’Œo3éƒ¨åˆ†æˆ–å®Œå…¨è§£å†³ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºå‡†æµ‹è¯•å¼ºè°ƒçš„æ˜¯è§„åˆ™æ˜ç¡®çš„æ¼”ç»æ¨ç†ä»»åŠ¡ï¼Œå¦‚æ•°å­¦å…¬ç†æˆ–ç¼–ç¨‹è¯­æ³•ï¼ŒLLMå¯ä»¥æ ¹æ®è¿™äº›è§„åˆ™è¿›è¡Œè®¡åˆ’å’Œåº”ç”¨æ¥å¾—åˆ°è§£å†³æ–¹æ¡ˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä»è§‚å¯Ÿæ•°æ®ä¸­æ¨æ–­å‡ºæ½œåœ¨è§„åˆ™çš„å½’çº³æ¨ç†åˆ™ç ”ç©¶è¾ƒå°‘ã€‚å½’çº³è¿‡ç¨‹å¤„äºç§‘å­¦å‘ç°çš„æ ¸å¿ƒï¼Œå› ä¸ºå®ƒä»¬ä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿä»å®è¯è§‚å¯Ÿä¸­æå–ä¸€èˆ¬åŸåˆ™ã€‚ä¸ºäº†è¯„ä¼°LLMæ˜¯å¦å…·å¤‡è¿™ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†InductionBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMçš„å½’çº³æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨å‡½æ•°å­æ­£åˆ™å±‚æ¬¡ç»“æ„ä¸­æœ€ç®€å•çš„å¤æ‚æ€§ç±»åˆ«ä¸­ä»ç„¶é¢ä¸´å›°éš¾ï¼Œè¿™çªæ˜¾äº†å½“å‰LLMåœ¨å½’çº³æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ˜¾è‘—ç¼ºé™·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œå·²èƒ½åœ¨è®¸å¤šåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å¤§å¤šå¼ºè°ƒæ¼”ç»æ¨ç†ä»»åŠ¡ï¼Œè¿™ç±»ä»»åŠ¡ä¸­è§„åˆ™æ˜ç¡®ï¼ŒLLMå¯åº”ç”¨è¿™äº›è§„åˆ™è¿›è¡Œé—®é¢˜è§£å†³ã€‚</li>
<li>å½’çº³æ¨ç†ï¼Œå³ä»è§‚å¯Ÿæ•°æ®ä¸­æ¨æ–­å‡ºæ½œåœ¨è§„åˆ™çš„è¿‡ç¨‹ï¼Œå¯¹äºç§‘å­¦å‘ç°è‡³å…³é‡è¦ï¼Œä½†åœ¨LLMä¸­ç ”ç©¶è¾ƒå°‘ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•InductionBenchï¼Œç”¨äºè¯„ä¼°LLMçš„å½’çº³æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æœ€å…ˆè¿›çš„LLMåœ¨ç®€å•çš„å½’çº³æ¨ç†ä»»åŠ¡ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>LLMåœ¨å½’çº³æ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.15823v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.15823v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.15823v2/page_1_1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Learning-to-Generate-Unit-Tests-for-Automated-Debugging"><a href="#Learning-to-Generate-Unit-Tests-for-Automated-Debugging" class="headerlink" title="Learning to Generate Unit Tests for Automated Debugging"></a>Learning to Generate Unit Tests for Automated Debugging</h2><p><strong>Authors:Archiki Prasad, Elias Stengel-Eskin, Justin Chih-Yao Chen, Zaid Khan, Mohit Bansal</strong></p>
<p>Unit tests (UTs) play an instrumental role in assessing code correctness as well as providing feedback to large language models (LLMs), motivating automated test generation. However, we uncover a trade-off between generating unit test inputs that reveal errors when given a faulty code and correctly predicting the unit test output without access to the gold solution. To address this trade-off, we propose UTGen, which teaches LLMs to generate unit test inputs that reveal errors along with their correct expected outputs based on task descriptions. Since model-generated tests can provide noisy signals (e.g., from incorrectly predicted outputs), we propose UTDebug that (i) scales UTGen via test-time compute to improve UT output prediction, and (ii) validates and backtracks edits based on multiple generated UTs to avoid overfitting, and helps LLMs debug effectively. We show that UTGen outperforms other LLM-based baselines by 7.59% based on a metric measuring the presence of both error-revealing UT inputs and correct UT outputs. When used with UTDebug, we find that feedback from UTGenâ€™s unit tests improves pass@1 accuracy of Qwen2.5 32B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3.17% and 12.35% (respectively) over other LLM-based UT generation baselines. Lastly, we demonstrate that UTGen is a better judge for code correctness, outperforming a state-of-the-art trained 8B reward model by 4.43% on HumanEval+ with best-of-10 sampling using Qwen2.5 7B. </p>
<blockquote>
<p>å•å…ƒæµ‹è¯•ï¼ˆUTsï¼‰åœ¨è¯„ä¼°ä»£ç æ­£ç¡®æ€§å¹¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›åé¦ˆæ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä»è€Œæ¿€åŠ±è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ç§æƒè¡¡ï¼šç”Ÿæˆæ­ç¤ºé”™è¯¯é”™è¯¯çš„å•å…ƒæµ‹è¯•è¾“å…¥å’Œåœ¨æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆçš„æƒ…å†µä¸‹æ­£ç¡®é¢„æµ‹å•å…ƒæµ‹è¯•è¾“å‡ºä¹‹é—´çš„æƒè¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æƒè¡¡é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UTGenï¼Œå®ƒæ•™ä¼šLLMæ ¹æ®ä»»åŠ¡æè¿°ç”Ÿæˆæ­ç¤ºé”™è¯¯çš„å•å…ƒæµ‹è¯•è¾“å…¥ä»¥åŠæ­£ç¡®çš„é¢„æœŸè¾“å‡ºã€‚ç”±äºæ¨¡å‹ç”Ÿæˆçš„æµ‹è¯•å¯èƒ½ä¼šäº§ç”Ÿå™ªå£°ä¿¡å·ï¼ˆä¾‹å¦‚ï¼Œæ¥è‡ªé”™è¯¯é¢„æµ‹çš„è¾“å‡ºï¼‰ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†UTDebugï¼Œå®ƒï¼ˆiï¼‰é€šè¿‡æµ‹è¯•æ—¶é—´çš„è®¡ç®—æ‰©å±•äº†UTGenï¼Œä»¥æé«˜UTè¾“å‡ºé¢„æµ‹èƒ½åŠ›ï¼Œï¼ˆiiï¼‰åŸºäºå¤šä¸ªç”Ÿæˆçš„UTè¿›è¡ŒéªŒè¯å’Œå›æº¯ç¼–è¾‘ï¼Œä»¥é¿å…è¿‡åº¦æ‹Ÿåˆï¼Œå¹¶å¸®åŠ©LLMæœ‰æ•ˆåœ°è¿›è¡Œè°ƒè¯•ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒUTGenåœ¨è¡¡é‡æ­ç¤ºé”™è¯¯çš„UTè¾“å…¥å’Œæ­£ç¡®çš„UTè¾“å‡ºçš„æŒ‡æ ‡ä¸Šï¼Œä¼˜äºå…¶ä»–LLMåŸºå‡†æµ‹è¯•7.59%ã€‚å½“ä¸UTDebugä¸€èµ·ä½¿ç”¨æ—¶ï¼Œæˆ‘ä»¬å‘ç°UTGençš„å•å…ƒæµ‹è¯•åé¦ˆæé«˜äº†Qwen2.5 32Båœ¨äººç±»è¯„ä¼°ä¿®å¤å’Œæˆ‘ä»¬çš„MBPP+æ›´éš¾è°ƒè¯•åˆ†å‰²ä¸Šçš„pass@1å‡†ç¡®ç‡ï¼Œåˆ†åˆ«è¶…è¿‡äº†å…¶ä»–çš„LLMåŸºå‡†æµ‹è¯•3.17%å’Œ12.35%ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜UTGenåœ¨ä»£ç æ­£ç¡®æ€§åˆ¤æ–­ä¸Šæ›´èƒœä¸€ç­¹ï¼Œåœ¨äººç±»è¯„ä¼°+ï¼ˆHumanEval+ï¼‰ä¸Šè¶…è¶Šäº†ä¸€ä¸ªæœ€å…ˆè¿›çš„è®­ç»ƒæœ‰ç´ çš„8Bå¥–åŠ±æ¨¡å‹4.43%ï¼Œå¹¶ä½¿ç”¨Qwen2.5 7Bçš„æœ€ä½³10æ¬¡é‡‡æ ·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01619v2">PDF</a> First two authors contributed equally. Dataset and Code:   <a target="_blank" rel="noopener" href="https://github.com/archiki/UTGenDebug">https://github.com/archiki/UTGenDebug</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å•å…ƒæµ‹è¯•ï¼ˆUTsï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„é‡è¦ä½œç”¨ï¼Œå¹¶æŒ‡å‡ºäº†ç”Ÿæˆèƒ½å¤Ÿæ­ç¤ºé”™è¯¯çš„å•å…ƒæµ‹è¯•ç”¨ä¾‹ä¸é¢„æµ‹æ­£ç¡®è¾“å‡ºä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†UTGenæ–¹æ³•ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡æè¿°ç”Ÿæˆæ­ç¤ºé”™è¯¯çš„å•å…ƒæµ‹è¯•ç”¨ä¾‹åŠå…¶æ­£ç¡®çš„é¢„æœŸè¾“å‡ºã€‚åŒæ—¶ï¼Œä¸ºäº†è§£å†³æ¨¡å‹ç”Ÿæˆæµ‹è¯•æ—¶å¯èƒ½å‡ºç°çš„å™ªå£°ä¿¡å·é—®é¢˜ï¼Œè¿›ä¸€æ­¥æå‡ºäº†UTDebugæ–¹æ³•ï¼Œé€šè¿‡æµ‹è¯•æ—¶é—´çš„è®¡ç®—æ¥æé«˜UTè¾“å‡ºçš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œå¹¶éªŒè¯å’Œå›æº¯å¤šä¸ªç”Ÿæˆçš„å•å…ƒæµ‹è¯•ï¼Œé¿å…è¿‡æ‹Ÿåˆï¼Œå¸®åŠ©LLMæ›´æœ‰æ•ˆåœ°è¿›è¡Œè°ƒè¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUTGenåœ¨å­˜åœ¨é”™è¯¯æ­ç¤ºçš„å•å…ƒæµ‹è¯•ç”¨ä¾‹å’Œæ­£ç¡®çš„å•å…ƒæµ‹è¯•ç”¨ä¾‹è¾“å‡ºçš„åº¦é‡æŒ‡æ ‡ä¸Šï¼Œä¼˜äºå…¶ä»–LLMåŸºçº¿æ–¹æ³•7.59%ã€‚ç»“åˆUTDebugä½¿ç”¨æ—¶ï¼ŒUTGençš„å•å…ƒæµ‹è¯•åé¦ˆæé«˜äº†Qwen2.5 32Båœ¨äººç±»è¯„ä¼°ä¿®å¤å’Œæ›´éš¾è°ƒè¯•çš„MBPP+åˆ†å‰²ä¸Šçš„å‡†ç¡®ç‡ã€‚æœ€åï¼Œå®éªŒæ˜¾ç¤ºUTGenåœ¨åˆ¤æ–­ä»£ç æ­£ç¡®æ€§æ–¹é¢è¡¨ç°æ›´ä¼˜ï¼Œä½¿ç”¨Qwen2.5 7Bçš„æœ€ä½³é‡‡æ ·æ–¹æ¡ˆæ—¶ï¼Œæ¯”å½“å‰æœ€å…ˆè¿›çš„8Bå¥–åŠ±æ¨¡å‹åœ¨HumanEval+ä¸Šçš„æ€§èƒ½é«˜å‡º4.43%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å•å…ƒæµ‹è¯•åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œç”¨äºè¯„ä¼°ä»£ç æ­£ç¡®æ€§å’Œæä¾›åé¦ˆã€‚</li>
<li>æå‡ºUTGenæ–¹æ³•ï¼Œæ ¹æ®ä»»åŠ¡æè¿°ç”Ÿæˆæ­ç¤ºé”™è¯¯çš„å•å…ƒæµ‹è¯•ç”¨ä¾‹åŠå…¶æ­£ç¡®é¢„æœŸè¾“å‡ºã€‚</li>
<li>UTDebugæ–¹æ³•ç”¨äºæé«˜UTè¾“å‡ºçš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œé€šè¿‡æµ‹è¯•æ—¶é—´çš„è®¡ç®—æ¥æ‰©å±•UTGenã€‚</li>
<li>UTDebugé€šè¿‡éªŒè¯å’Œå›æº¯å¤šä¸ªç”Ÿæˆçš„å•å…ƒæµ‹è¯•ï¼Œé¿å…æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œå¸®åŠ©æœ‰æ•ˆè°ƒè¯•ã€‚</li>
<li>UTGenåœ¨é”™è¯¯æ­ç¤ºçš„å•å…ƒæµ‹è¯•ç”¨ä¾‹å’Œæ­£ç¡®è¾“å‡ºçš„åº¦é‡ä¸Šä¼˜äºå…¶ä»–LLMåŸºçº¿æ–¹æ³•ã€‚</li>
<li>ç»“åˆUTDebugä½¿ç”¨ï¼ŒUTGençš„åé¦ˆæé«˜äº†LLMåœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.01619v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.01619v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2502.01619v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TAPO-Task-Referenced-Adaptation-for-Prompt-Optimization"><a href="#TAPO-Task-Referenced-Adaptation-for-Prompt-Optimization" class="headerlink" title="TAPO: Task-Referenced Adaptation for Prompt Optimization"></a>TAPO: Task-Referenced Adaptation for Prompt Optimization</h2><p><strong>Authors:Wenxin Luo, Weirui Wang, Xiaopeng Li, Weibo Zhou, Pengyue Jia, Xiangyu Zhao</strong></p>
<p>Prompt engineering can significantly improve the performance of large language models (LLMs), with automated prompt optimization (APO) gaining significant attention due to the time-consuming and laborious nature of manual prompt design. However, much of the existing work in APO overlooks task-specific characteristics, resulting in prompts that lack domain specificity and are not well-suited for task-specific optimization. In this paper, we introduce TAPO, a multitask-aware prompt optimization framework composed of three key modules. First, a task-aware metric selection module is proposed to enhance task-specific prompt generation capabilities. Second, we present a multi-metrics evaluation module to jointly evaluate prompts from multiple perspectives. Third, an evolution-based optimization framework is introduced for automatic prompt refinement, which improves adaptability across various tasks. Extensive experiments on six datasets demonstrate the effectiveness of our approach, and our code is publicly available. </p>
<blockquote>
<p>æç¤ºå·¥ç¨‹å¯ä»¥æ˜¾è‘—æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ï¼Œç”±äºæ‰‹åŠ¨æç¤ºè®¾è®¡çš„æ—¶é—´æ¶ˆè€—å’Œç¹çæ€§ï¼Œè‡ªåŠ¨æç¤ºä¼˜åŒ–ï¼ˆAPOï¼‰å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„APOå·¥ä½œå¤§å¤šå¿½è§†äº†ä»»åŠ¡ç‰¹æ€§ï¼Œå¯¼è‡´æç¤ºç¼ºä¹é¢†åŸŸç‰¹å¼‚æ€§ï¼Œä¸é€‚ç”¨äºç‰¹å®šä»»åŠ¡çš„ä¼˜åŒ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†TAPOï¼Œè¿™æ˜¯ä¸€ä¸ªç”±ä¸‰ä¸ªå…³é”®æ¨¡å—ç»„æˆçš„å¤šä»»åŠ¡æ„ŸçŸ¥æç¤ºä¼˜åŒ–æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»»åŠ¡æ„ŸçŸ¥æŒ‡æ ‡é€‰æ‹©æ¨¡å—ï¼Œä»¥æé«˜ä»»åŠ¡ç‰¹å®šæç¤ºç”Ÿæˆèƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šæŒ‡æ ‡è¯„ä¼°æ¨¡å—ï¼Œä»å¤šä¸ªè§’åº¦å¯¹æç¤ºè¿›è¡Œè”åˆè¯„ä¼°ã€‚ç¬¬ä¸‰ï¼Œä»‹ç»äº†ä¸€ç§åŸºäºè¿›åŒ–ä¼˜åŒ–çš„è‡ªåŠ¨æç¤ºæ”¹è¿›æ¡†æ¶ï¼Œæé«˜åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬çš„ä»£ç å·²ç»å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06689v3">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong><br>è‡ªåŠ¨æç¤ºä¼˜åŒ–ï¼ˆAPOï¼‰èƒ½æ˜¾è‘—æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ï¼Œä½†ç°æœ‰å·¥ä½œå¾€å¾€å¿½è§†ä»»åŠ¡ç‰¹æ€§ï¼Œå¯¼è‡´æç¤ºç¼ºä¹é¢†åŸŸç‰¹å¼‚æ€§ä¸”ä¸é€‚ç”¨äºç‰¹å®šä»»åŠ¡çš„ä¼˜åŒ–ã€‚æœ¬æ–‡æå‡ºTAPOï¼Œä¸€ä¸ªå¤šä»»åŠ¡æ„ŸçŸ¥çš„æç¤ºä¼˜åŒ–æ¡†æ¶ï¼ŒåŒ…æ‹¬ä»»åŠ¡æ„ŸçŸ¥åº¦é‡é€‰æ‹©æ¨¡å—ã€å¤šåº¦é‡è¯„ä¼°æ¨¡å—å’ŒåŸºäºè¿›åŒ–çš„ä¼˜åŒ–æ¡†æ¶ï¼Œä»¥æé«˜è·¨å„ç§ä»»åŠ¡çš„é€‚åº”æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨æç¤ºä¼˜åŒ–ï¼ˆAPOï¼‰èƒ½æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½ã€‚</li>
<li>ç°æœ‰APOæ–¹æ³•å¿½è§†ä»»åŠ¡ç‰¹æ€§ï¼Œå¯¼è‡´æç¤ºç¼ºä¹é¢†åŸŸç‰¹å¼‚æ€§ã€‚</li>
<li>TAPOæ¡†æ¶åŒ…å«ä»»åŠ¡æ„ŸçŸ¥åº¦é‡é€‰æ‹©æ¨¡å—ï¼Œæé«˜ä»»åŠ¡ç‰¹å®šæç¤ºç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>å¤šåº¦é‡è¯„ä¼°æ¨¡å—èƒ½ä»å¤šä¸ªè§’åº¦è”åˆè¯„ä¼°æç¤ºã€‚</li>
<li>åŸºäºè¿›åŒ–çš„ä¼˜åŒ–æ¡†æ¶èƒ½è‡ªåŠ¨æ”¹è¿›æç¤ºï¼Œæé«˜è·¨ä¸åŒä»»åŠ¡çš„é€‚åº”æ€§ã€‚</li>
<li>åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†TAPOæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06689">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2501.06689v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2501.06689v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2501.06689v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2501.06689v3/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2501.06689v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2501.06689v3/page_3_1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Privacy-Preserving-Retrieval-Augmented-Generation-with-Differential-Privacy"><a href="#Privacy-Preserving-Retrieval-Augmented-Generation-with-Differential-Privacy" class="headerlink" title="Privacy-Preserving Retrieval-Augmented Generation with Differential   Privacy"></a>Privacy-Preserving Retrieval-Augmented Generation with Differential   Privacy</h2><p><strong>Authors:Tatsuki Koga, Ruihan Wu, Kamalika Chaudhuri</strong></p>
<p>With the recent remarkable advancement of large language models (LLMs), there has been a growing interest in utilizing them in the domains with highly sensitive data that lies outside their training data. For this purpose, retrieval-augmented generation (RAG) is particularly effective â€“ it assists LLMs by directly providing relevant information from the external knowledge sources. However, without extra privacy safeguards, RAG outputs risk leaking sensitive information from the external data source. In this work, we explore RAG under differential privacy (DP), a formal guarantee of data privacy. The main challenge with differentially private RAG is how to generate long accurate answers within a moderate privacy budget. We address this by proposing an algorithm that smartly spends privacy budget only for the tokens that require the sensitive information and uses the non-private LLM for other tokens. Our extensive empirical evaluations reveal that our algorithm outperforms the non-RAG baseline under a reasonable privacy budget of $\epsilon\approx 10$ across different models and datasets. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€è¿‘æ˜¾è‘—è¿›æ­¥ï¼Œäººä»¬å¯¹å…¶åœ¨é«˜åº¦æ•æ„Ÿæ•°æ®é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šæ„Ÿå…´è¶£ï¼Œè¿™äº›æ•°æ®ä½äºå…¶è®­ç»ƒæ•°æ®ä¹‹å¤–ã€‚ä¸ºæ­¤ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç‰¹åˆ«æœ‰æ•ˆâ€”â€”å®ƒé€šè¿‡ç›´æ¥ä»å¤–éƒ¨çŸ¥è¯†æºæä¾›ç›¸å…³ä¿¡æ¯æ¥è¾…åŠ©LLMã€‚ç„¶è€Œï¼Œæ²¡æœ‰é¢å¤–çš„éšç§ä¿éšœï¼ŒRAGè¾“å‡ºå­˜åœ¨æ³„éœ²å¤–éƒ¨æ•°æ®æºä¸­æ•æ„Ÿä¿¡æ¯çš„é£é™©ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åœ¨å·®åˆ†éšç§ï¼ˆDPï¼‰çš„æ¡†æ¶ä¸‹æ¢ç´¢RAGï¼Œè¿™æ˜¯æ•°æ®éšç§çš„ä¸€ç§æ­£å¼ä¿éšœã€‚å·®åˆ†ç§æœ‰RAGçš„ä¸»è¦æŒ‘æˆ˜æ˜¯å¦‚ä½•åœ¨é€‚åº¦çš„éšç§é¢„ç®—å†…ç”Ÿæˆå‡†ç¡®ä¸”ç¯‡å¹…é•¿çš„ç­”æ¡ˆã€‚æˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§ç®—æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥ç®—æ³•æ™ºèƒ½åœ°ä»…å¯¹éœ€è¦æ•æ„Ÿä¿¡æ¯çš„ä»¤ç‰Œä½¿ç”¨éšç§é¢„ç®—ï¼Œå¹¶ä½¿ç”¨éç§æœ‰LLMå¤„ç†å…¶ä»–ä»¤ç‰Œã€‚æˆ‘ä»¬çš„å¹¿æ³›ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œåœ¨åˆç†çš„éšç§é¢„ç®—ä¸‹ï¼Œæˆ‘ä»¬çš„ç®—æ³•åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºéRAGåŸºçº¿ã€‚åœ¨$\epsilon \approx 10$æ—¶å°¤ä¸ºæ˜æ˜¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04697v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹äºå…·æœ‰é«˜åº¦æ•æ„Ÿæ•°æ®é¢†åŸŸçš„åº”ç”¨è¡¨ç°å‡ºäº†æµ“åšå…´è¶£ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯åœ¨ä¿æŠ¤éšç§çš„å‰æä¸‹ï¼Œé€šè¿‡å·®åˆ†éšç§ï¼ˆDPï¼‰æ¡†æ¶å¢å¼ºæ•°æ®éšç§ä¿æŠ¤ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨æ•æ„Ÿæ•°æ®ä¸Šå‘æŒ¥ä½œç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®—æ³•ï¼Œè¯¥ç®—æ³•ä»…åœ¨éœ€è¦æ•æ„Ÿä¿¡æ¯çš„æ ‡è®°ä¸Šæ¶ˆè€—éšç§é¢„ç®—ï¼Œå¹¶åœ¨å…¶ä»–æ ‡è®°ä¸Šä½¿ç”¨éç§æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ã€‚åœ¨åˆç†çš„éšç§é¢„ç®—ä¸‹ï¼Œè¯¥ç®—æ³•åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºéæ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜åº¦æ•æ„Ÿæ•°æ®é¢†åŸŸçš„åº”ç”¨æ­£åœ¨å¢é•¿ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯èƒ½æœ‰æ•ˆè¾…åŠ©å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†å¤–éƒ¨çŸ¥è¯†æºçš„ä¿¡æ¯ã€‚</li>
<li>å·®åˆ†éšç§ï¼ˆDPï¼‰æ¡†æ¶ç”¨äºå¢å¼ºæ•°æ®éšç§ä¿æŠ¤ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ™ºèƒ½æ¶ˆè€—éšç§é¢„ç®—çš„ç®—æ³•ï¼Œä»…åœ¨éœ€è¦æ•æ„Ÿä¿¡æ¯çš„æ ‡è®°ä¸Šæ¶ˆè€—é¢„ç®—ã€‚</li>
<li>è¯¥ç®—æ³•åœ¨åˆç†çš„éšç§é¢„ç®—ä¸‹è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>ç®—æ³•åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºéæ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04697">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2412.04697v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2412.04697v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2412.04697v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2412.04697v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2412.04697v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="The-Hyperfitting-Phenomenon-Sharpening-and-Stabilizing-LLMs-for-Open-Ended-Text-Generation"><a href="#The-Hyperfitting-Phenomenon-Sharpening-and-Stabilizing-LLMs-for-Open-Ended-Text-Generation" class="headerlink" title="The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for   Open-Ended Text Generation"></a>The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for   Open-Ended Text Generation</h2><p><strong>Authors:Fredrik Carlsson, Fangyu Liu, Daniel Ward, Murathan Kurfali, Joakim Nivre</strong></p>
<p>This paper introduces the counter-intuitive generalization results of overfitting pre-trained large language models (LLMs) on very small datasets. In the setting of open-ended text generation, it is well-documented that LLMs tend to generate repetitive and dull sequences, a phenomenon that is especially apparent when generating using greedy decoding. This issue persists even with state-of-the-art LLMs containing billions of parameters, trained via next-token prediction on large datasets. We find that by further fine-tuning these models to achieve a near-zero training loss on a small set of samples â€“ a process we refer to as hyperfitting â€“ the long-sequence generative capabilities are greatly enhanced. Greedy decoding with these Hyperfitted models even outperform Top-P sampling over long-sequences, both in terms of diversity and human preferences. This phenomenon extends to LLMs of various sizes, different domains, and even autoregressive image generation. We further find this phenomena to be distinctly different from that of Grokking and double descent. Surprisingly, our experiments indicate that hyperfitted models rarely fall into repeating sequences they were trained on, and even explicitly blocking these sequences results in high-quality output. All hyperfitted models produce extremely low-entropy predictions, often allocating nearly all probability to a single token. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éå¸¸å°çš„æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆçš„åç›´è§‰æ³›åŒ–ç»“æœã€‚åœ¨å¼€æ”¾æ–‡æœ¬ç”Ÿæˆçš„ç¯å¢ƒä¸­ï¼Œäººä»¬æ™®éè®¤ä¸ºLLMå€¾å‘äºç”Ÿæˆé‡å¤å’Œä¹å‘³çš„åºåˆ—ï¼Œè¿™ä¸€ç°è±¡åœ¨ä½¿ç”¨è´ªå¿ƒè§£ç è¿›è¡Œç”Ÿæˆæ—¶å°¤ä¸ºæ˜æ˜¾ã€‚å³ä½¿ä½¿ç”¨æœ€å…ˆè¿›çš„LLMï¼ŒåŒ…å«æ•°åäº¿å‚æ•°ï¼Œé€šè¿‡å¤§æ•°æ®é›†è¿›è¡Œä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹è®­ç»ƒï¼Œè¿™ä¸ªé—®é¢˜ä¾ç„¶å­˜åœ¨ã€‚æˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡è¿›ä¸€æ­¥å¾®è°ƒè¿™äº›æ¨¡å‹ï¼Œä½¿å…¶åœ¨å°æ ·æœ¬é›†ä¸Šè¾¾åˆ°æ¥è¿‘é›¶çš„è®­ç»ƒæŸå¤±â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºè¶…æ‹Ÿåˆï¼ˆHyperfittingï¼‰â€”â€”å¯ä»¥æå¤§åœ°æé«˜é•¿åºåˆ—çš„ç”Ÿæˆèƒ½åŠ›ã€‚ä½¿ç”¨è¿™äº›è¶…æ‹Ÿåˆæ¨¡å‹çš„è´ªå¿ƒè§£ç ç”šè‡³è¶…è¿‡äº†é•¿åºåˆ—çš„Top-Pé‡‡æ ·ï¼Œæ— è®ºæ˜¯åœ¨å¤šæ ·æ€§è¿˜æ˜¯äººç±»åå¥½æ–¹é¢ã€‚è¿™ä¸€ç°è±¡æ‰©å±•åˆ°äº†å„ç§è§„æ¨¡ã€ä¸åŒé¢†åŸŸçš„LLMï¼Œç”šè‡³æ‰©å±•åˆ°äº†è‡ªå›å½’å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬è¿˜å‘ç°è¿™ç§ç°è±¡ä¸Grokkingå’ŒåŒé‡ä¸‹é™æœ‰ç€æ˜æ˜¾çš„åŒºåˆ«ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¶…æ‹Ÿåˆæ¨¡å‹å¾ˆå°‘é™·å…¥è®­ç»ƒä¸­çš„é‡å¤åºåˆ—ï¼Œå³ä½¿æ˜ç¡®é˜»æ­¢è¿™äº›åºåˆ—ä¹Ÿä¼šå¯¼è‡´é«˜è´¨é‡è¾“å‡ºã€‚æ‰€æœ‰è¶…æ‹Ÿåˆæ¨¡å‹äº§ç”Ÿçš„é¢„æµ‹å…·æœ‰æä½çš„ç†µï¼Œé€šå¸¸å°†æ‰€æœ‰æ¦‚ç‡åˆ†é…ç»™å•ä¸ªæ ‡è®°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04318v2">PDF</a> Under review at ICLR</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æå°æ•°æ®é›†ä¸Šçš„åç›´è§‰æ³›åŒ–ç»“æœã€‚åœ¨å¼€æ”¾æ–‡æœ¬ç”Ÿæˆç¯å¢ƒä¸­ï¼ŒLLMå®¹æ˜“äº§ç”Ÿé‡å¤å’Œæ¯ç‡¥çš„åºåˆ—ï¼Œå°¤å…¶åœ¨ä½¿ç”¨è´ªå¿ƒè§£ç æ—¶æ›´ä¸ºæ˜æ˜¾ã€‚è¿™ä¸€ç°è±¡å³ä½¿åœ¨é‡‡ç”¨æœ€å…ˆè¿›çš„LLMï¼ˆä½¿ç”¨å¤§å‹æ•°æ®é›†è¿›è¡Œä¸‹ä¸€ä»¤ç‰Œé¢„æµ‹è®­ç»ƒï¼‰æ—¶ä¹Ÿä¼šæŒç»­å­˜åœ¨ã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡è¿›ä¸€æ­¥å¾®è°ƒè¿™äº›æ¨¡å‹ï¼Œä½¿å…¶åœ¨å°æ ·æœ¬é›†ä¸Šçš„è®­ç»ƒæŸå¤±æ¥è¿‘é›¶â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºè¶…æ‹Ÿåˆï¼ˆHyperfittingï¼‰â€”â€”å¯ä»¥å¤§å¤§æé«˜é•¿åºåˆ—çš„ç”Ÿæˆèƒ½åŠ›ã€‚ä½¿ç”¨è¶…æ‹Ÿåˆæ¨¡å‹è¿›è¡Œè´ªå¿ƒè§£ç ç”šè‡³åœ¨æŸäº›æ–¹é¢ä¼˜äºTop-Pé‡‡æ ·ï¼Œæ— è®ºæ˜¯å¤šæ ·æ€§è¿˜æ˜¯äººç±»åå¥½ã€‚è¿™ä¸€ç°è±¡æ‰©å±•åˆ°å„ç§è§„æ¨¡ã€ä¸åŒé¢†åŸŸçš„LLMï¼Œç”šè‡³æ‰©å±•åˆ°è‡ªå›å½’å›¾åƒç”Ÿæˆã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œè¶…æ‹Ÿåˆç°è±¡ä¸Grokkingå’ŒåŒé‡ä¸‹é™æœ‰ç€æ˜¾è‘—çš„ä¸åŒã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå®éªŒè¡¨æ˜ï¼Œè¶…æ‹Ÿåˆæ¨¡å‹å¾ˆå°‘é™·å…¥é‡å¤çš„è®­ç»ƒåºåˆ—ï¼Œå³ä½¿æ˜¾å¼é˜»æ­¢è¿™äº›åºåˆ—ä¹Ÿèƒ½äº§ç”Ÿé«˜è´¨é‡çš„è¾“å‡ºã€‚æ‰€æœ‰è¶…æ‹Ÿåˆæ¨¡å‹çš„é¢„æµ‹éƒ½å…·æœ‰æä½çš„ç†µï¼Œé€šå¸¸å°†æ‰€æœ‰æ¦‚ç‡åˆ†é…ç»™å•ä¸ªæ ‡è®°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¼€æ”¾æ–‡æœ¬ç”Ÿæˆç¯å¢ƒä¸­å®¹æ˜“ç”Ÿæˆé‡å¤å’Œæ¯ç‡¥çš„åºåˆ—ï¼Œå°¤å…¶åœ¨è´ªå¿ƒè§£ç æ—¶æ›´ä¸ºæ˜æ˜¾ã€‚</li>
<li>é€šè¿‡è¶…æ‹Ÿåˆï¼ˆHyperfittingï¼‰æŠ€æœ¯ï¼Œå³åœ¨æå°æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ä½¿è®­ç»ƒæŸå¤±æ¥è¿‘é›¶ï¼Œå¯ä»¥æé«˜LLMçš„é•¿åºåˆ—ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>è¶…æ‹Ÿåˆæ¨¡å‹çš„è´ªå¿ƒè§£ç åœ¨æŸäº›æ–¹é¢ä¼˜äºTop-Pé‡‡æ ·ï¼ŒåŒ…æ‹¬å¤šæ ·æ€§å’Œäººç±»åå¥½ã€‚</li>
<li>è¶…æ‹Ÿåˆç°è±¡é€‚ç”¨äºä¸åŒè§„æ¨¡ã€ä¸åŒé¢†åŸŸçš„LLMä»¥åŠè‡ªå›å½’å›¾åƒç”Ÿæˆã€‚</li>
<li>è¶…æ‹Ÿåˆä¸Grokkingå’ŒåŒé‡ä¸‹é™æœ‰æ˜¾è‘—ä¸åŒã€‚</li>
<li>è¶…æ‹Ÿåˆæ¨¡å‹ä¸ä¼šé™·å…¥é‡å¤çš„è®­ç»ƒåºåˆ—ï¼Œå³ä½¿é˜»æ­¢è¿™äº›åºåˆ—ä¹Ÿèƒ½äº§ç”Ÿé«˜è´¨é‡è¾“å‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2412.04318v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2412.04318v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2412.04318v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2412.04318v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2412.04318v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2412.04318v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2412.04318v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Drawing-Pandas-A-Benchmark-for-LLMs-in-Generating-Plotting-Code"><a href="#Drawing-Pandas-A-Benchmark-for-LLMs-in-Generating-Plotting-Code" class="headerlink" title="Drawing Pandas: A Benchmark for LLMs in Generating Plotting Code"></a>Drawing Pandas: A Benchmark for LLMs in Generating Plotting Code</h2><p><strong>Authors:Timur Galimzyanov, Sergey Titov, Yaroslav Golubev, Egor Bogomolov</strong></p>
<p>This paper introduces the human-curated PandasPlotBench dataset, designed to evaluate language modelsâ€™ effectiveness as assistants in visual data exploration. Our benchmark focuses on generating code for visualizing tabular data - such as a Pandas DataFrame - based on natural language instructions, complementing current evaluation tools and expanding their scope. The dataset includes 175 unique tasks. Our experiments assess several leading Large Language Models (LLMs) across three visualization libraries: Matplotlib, Seaborn, and Plotly. We show that the shortening of tasks has a minimal effect on plotting capabilities, allowing for the user interface that accommodates concise user input without sacrificing functionality or accuracy. Another of our findings reveals that while LLMs perform well with popular libraries like Matplotlib and Seaborn, challenges persist with Plotly, highlighting areas for improvement. We hope that the modular design of our benchmark will broaden the current studies on generating visualizations. Our dataset and benchmark code are available online: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/JetBrains-Research/PandasPlotBench">https://huggingface.co/datasets/JetBrains-Research/PandasPlotBench</a>; <a target="_blank" rel="noopener" href="https://github.com/JetBrains-Research/PandasPlotBench">https://github.com/JetBrains-Research/PandasPlotBench</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†äººä¸ºç­–åˆ’çš„PandasPlotBenchæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨è¯„ä¼°è¯­è¨€æ¨¡å‹ä½œä¸ºè§†è§‰æ•°æ®æ¢ç´¢åŠ©æ‰‹çš„æ•ˆç‡ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸“æ³¨äºæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸ºè¡¨æ ¼æ•°æ®ï¼ˆå¦‚Pandas DataFrameï¼‰ç”Ÿæˆå¯è§†åŒ–ä»£ç ï¼Œè¡¥å……äº†å½“å‰çš„è¯„ä»·å·¥å…·ï¼Œå¹¶æ‰©å¤§äº†å…¶èŒƒå›´ã€‚æ•°æ®é›†åŒ…å«175ä¸ªç‹¬ç‰¹ä»»åŠ¡ã€‚æˆ‘ä»¬çš„å®éªŒè¯„ä¼°äº†ä¸‰å¤§å¯è§†åŒ–åº“ä¸­çš„å‡ ä¸ªé¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ŒåŒ…æ‹¬Matplotlibã€Seabornå’ŒPlotlyã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä»»åŠ¡çš„ç¼©çŸ­å¯¹ç»˜å›¾èƒ½åŠ›çš„å½±å“å¾®ä¹å…¶å¾®ï¼Œè¿™ä¸ºç”¨æˆ·ç•Œé¢æä¾›äº†ä¾¿åˆ©ï¼Œè¯¥ç•Œé¢å¯å®¹çº³ç®€æ´çš„ç”¨æˆ·è¾“å…¥ï¼Œè€Œä¸ä¼šç‰ºç‰²åŠŸèƒ½æˆ–å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„å¦ä¸€é¡¹ç ”ç©¶å‘ç°ï¼Œè™½ç„¶LLMåœ¨æµè¡Œçš„åº“ï¼ˆå¦‚Matplotlibå’ŒSeabornï¼‰ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨Plotlyä¸Šä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œè¿™çªå‡ºäº†éœ€è¦æ”¹è¿›çš„é¢†åŸŸã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬åŸºå‡†æµ‹è¯•çš„æ¨¡å—åŒ–è®¾è®¡èƒ½å¤Ÿæ‰©å¤§å…³äºç”Ÿæˆå¯è§†åŒ–ç ”ç©¶çš„å½“å‰èŒƒå›´ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ä»£ç å¯ä»¥åœ¨ç½‘ä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/JetBrains-Research/PandasPlotBench%EF%BC%9Bhttps://github.com/JetBrains-Research/PandasPlotBench%E3%80%82">https://huggingface.co/datasets/JetBrains-Research/PandasPlotBenchï¼›https://github.com/JetBrains-Research/PandasPlotBenchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02764v2">PDF</a> 5 pages</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ä¸ªäººå·¥åˆ¶ä½œçš„PandasPlotBenchæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹ä½œä¸ºå¯è§†åŒ–æ•°æ®æ¢ç´¢åŠ©æ‰‹çš„æ•ˆèƒ½ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸“æ³¨äºæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆå¯è§†åŒ–è¡¨æ ¼æ•°æ®çš„ä»£ç ï¼Œæ˜¯å¯¹å½“å‰è¯„ä¼°å·¥å…·çš„ä¸€ç§è¡¥å……å’Œæ‰©å±•ã€‚å®éªŒè¯„ä¼°äº†å¤šä¸ªä¸»æµçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸‰ä¸ªå¯è§†åŒ–åº“ä¸Šçš„è¡¨ç°ï¼šMatplotlibã€Seabornå’ŒPlotlyã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»»åŠ¡ç®€åŒ–å¯¹ç»˜å›¾èƒ½åŠ›çš„å½±å“è¾ƒå°ï¼Œä¸”ç”¨æˆ·æ¥å£èƒ½å¤Ÿç®€æ´å¿«é€Ÿåœ°é€‚åº”æŒ‡ä»¤è¾“å…¥è€Œæ— éœ€ç‰ºç‰²åŠŸèƒ½æˆ–å‡†ç¡®æ€§ã€‚è™½ç„¶LLMåœ¨Matplotlibå’ŒSeabornç­‰æµè¡Œåº“ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨Plotlyä¸Šä»å­˜åœ¨æŒ‘æˆ˜ï¼Œè¿™ä¸ºæ”¹è¿›æä¾›äº†æ–¹å‘ã€‚PandasPlotBenchçš„æ¨¡å—åŒ–è®¾è®¡æœ‰æœ›æ‹“å®½å¯è§†åŒ–ç”Ÿæˆçš„ç ”ç©¶é¢†åŸŸã€‚æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ä»£ç å¯åœ¨ç½‘ä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/JetBrains-Research/PandasPlotBench">https://huggingface.co/datasets/JetBrains-Research/PandasPlotBench</a>ï¼›<a target="_blank" rel="noopener" href="https://github.com/JetBrains-Research/PandasPlotBench">https://github.com/JetBrains-Research/PandasPlotBench</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PandasPlotBenchæ•°æ®é›†æ—¨åœ¨è¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨å¯è§†åŒ–æ•°æ®æ¢ç´¢æ–¹é¢çš„æ•ˆèƒ½ï¼Œç‰¹åˆ«æ˜¯ç”ŸæˆåŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„è¡¨æ ¼æ•°æ®å¯è§†åŒ–ä»£ç ã€‚</li>
<li>è¯¥ç ”ç©¶é€šè¿‡ä¸‰ä¸ªå¯è§†åŒ–åº“ï¼ˆMatplotlibã€Seabornå’ŒPlotlyï¼‰è¯„ä¼°äº†LLMçš„è¡¨ç°ã€‚</li>
<li>ä»»åŠ¡ç®€åŒ–å¯¹ç»˜å›¾èƒ½åŠ›çš„å½±å“è¾ƒå°ï¼Œå…è®¸ç®€æ´çš„ç”¨æˆ·è¾“å…¥ä¸åŠŸèƒ½åŠå‡†ç¡®æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>LLMåœ¨æµè¡Œåº“ï¼ˆå¦‚Matplotlibå’ŒSeabornï¼‰ä¸Šçš„è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨Plotlyä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>PandasPlotBenchçš„æ¨¡å—åŒ–è®¾è®¡æœ‰åŠ©äºæ‹“å®½å¯è§†åŒ–ç”Ÿæˆçš„ç ”ç©¶æ–¹å‘ã€‚</li>
<li>è¯¥æ•°æ®é›†æä¾›äº†èµ„æºé“¾æ¥ä¾›ç ”ç©¶äººå‘˜è®¿é—®å’Œä½¿ç”¨ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/JetBrains-Research/PandasPlotBench">https://huggingface.co/datasets/JetBrains-Research/PandasPlotBench</a> å’Œ <a target="_blank" rel="noopener" href="https://github.com/JetBrains-Research/PandasPlotBench">https://github.com/JetBrains-Research/PandasPlotBench</a>ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02764">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2412.02764v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2412.02764v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2412.02764v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2412.02764v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Interpreting-Language-Reward-Models-via-Contrastive-Explanations"><a href="#Interpreting-Language-Reward-Models-via-Contrastive-Explanations" class="headerlink" title="Interpreting Language Reward Models via Contrastive Explanations"></a>Interpreting Language Reward Models via Contrastive Explanations</h2><p><strong>Authors:Junqi Jiang, Tom Bewley, Saumitra Mishra, Freddy Lecue, Manuela Veloso</strong></p>
<p>Reward models (RMs) are a crucial component in the alignment of large language modelsâ€™ (LLMs) outputs with human values. RMs approximate human preferences over possible LLM responses to the same prompt by predicting and comparing reward scores. However, as they are typically modified versions of LLMs with scalar output heads, RMs are large black boxes whose predictions are not explainable. More transparent RMs would enable improved trust in the alignment of LLMs. In this work, we propose to use contrastive explanations to explain any binary response comparison made by an RM. Specifically, we generate a diverse set of new comparisons similar to the original one to characterise the RMâ€™s local behaviour. The perturbed responses forming the new comparisons are generated to explicitly modify manually specified high-level evaluation attributes, on which analyses of RM behaviour are grounded. In quantitative experiments, we validate the effectiveness of our method for finding high-quality contrastive explanations. We then showcase the qualitative usefulness of our method for investigating global sensitivity of RMs to each evaluation attribute, and demonstrate how representative examples can be automatically extracted to explain and compare behaviours of different RMs. We see our method as a flexible framework for RM explanation, providing a basis for more interpretable and trustworthy LLM alignment. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾“å‡ºä¸äººç±»ä»·å€¼è§‚å¯¹é½çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚RMsé€šè¿‡é¢„æµ‹å’Œæ¯”è¾ƒå¥–åŠ±åˆ†æ•°æ¥è¿‘ä¼¼äººç±»å¯¹ç›¸åŒæç¤ºä¸‹å¯èƒ½çš„LLMå“åº”çš„åå¥½ã€‚ç„¶è€Œï¼Œç”±äºå®ƒä»¬é€šå¸¸æ˜¯å…·æœ‰æ ‡é‡è¾“å‡ºå¤´éƒ¨çš„LLMçš„ä¿®æ”¹ç‰ˆæœ¬ï¼ŒRMsæ˜¯å¤§å‹é»‘ç®±ï¼Œå…¶é¢„æµ‹ä¸å¯è§£é‡Šã€‚æ›´é€æ˜çš„RMå°†å¢å¼ºå¯¹LLMå¯¹é½çš„ä¿¡ä»»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å¯¹æ¯”è§£é‡Šæ¥è§£é‡ŠRMè¿›è¡Œçš„ä»»ä½•äºŒå…ƒå“åº”æ¯”è¾ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç”Ÿæˆä¸€ç»„ä¸åŸå§‹æ¯”è¾ƒç›¸ä¼¼çš„æ–°æ¯”è¾ƒæ¥è¡¨å¾RMçš„å±€éƒ¨è¡Œä¸ºã€‚ç”Ÿæˆæ–°çš„æ¯”è¾ƒæ—¶çš„æ‰°åŠ¨å“åº”ä¼šæ˜ç¡®ä¿®æ”¹æ‰‹åŠ¨æŒ‡å®šçš„é«˜çº§è¯„ä¼°å±æ€§ï¼ŒRMè¡Œä¸ºçš„åˆ†æå°†åŸºäºæ­¤ã€‚åœ¨å®šé‡å®éªŒä¸­ï¼Œæˆ‘ä»¬éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯»æ‰¾é«˜è´¨é‡å¯¹æ¯”è§£é‡Šæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨è°ƒæŸ¥RMå¯¹æ¯ä¸ªè¯„ä¼°å±æ€§çš„å…¨å±€æ•æ„Ÿæ€§æ–¹é¢çš„å®šæ€§å®ç”¨æ€§ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•è‡ªåŠ¨æå–ä»£è¡¨æ€§ç¤ºä¾‹æ¥è§£é‡Šå’Œæ¯”è¾ƒä¸åŒRMçš„è¡Œä¸ºã€‚æˆ‘ä»¬è®¤ä¸ºæˆ‘ä»¬çš„æ–¹æ³•æ˜¯ä¸€ä¸ªçµæ´»çš„RMè§£é‡Šæ¡†æ¶ï¼Œä¸ºæ›´å¯è§£é‡Šå’Œå¯ä¿¡èµ–çš„LLMå¯¹é½æä¾›äº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16502v2">PDF</a> Accepted at ICLR 2025 conference</p>
<p><strong>Summary</strong></p>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰å¯¹äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¾“å‡ºä¸äººç±»ä»·å€¼è§‚èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚é€šè¿‡é¢„æµ‹å’Œæ¯”è¾ƒå¥–åŠ±åˆ†æ•°ï¼ŒRMsè¿‘ä¼¼äººç±»å¯¹åŒä¸€æç¤ºä¸‹å¯èƒ½çš„LLMå“åº”çš„åå¥½ã€‚ç„¶è€Œï¼ŒRMsé€šå¸¸æ˜¯å…·æœ‰æ ‡é‡è¾“å‡ºå¤´éƒ¨çš„LLMä¿®æ”¹ç‰ˆæœ¬ï¼Œå…¶é¢„æµ‹ä¸å¯è§£é‡Šï¼Œæˆä¸ºå¤§å‹é»‘ç®±ã€‚ä¸ºæ”¹å–„å¯¹LLMå¯¹é½çš„ä¿¡ä»»åº¦ï¼Œæˆ‘ä»¬æè®®ä½¿ç”¨å¯¹æ¯”è§£é‡Šæ³•æ¥è§£é‡ŠRMsçš„äºŒå…ƒå“åº”æ¯”è¾ƒã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬ç”Ÿæˆä¸€ç³»åˆ—ä¸åŸæ¯”è¾ƒç›¸ä¼¼çš„æ–°æ¯”è¾ƒï¼Œä»¥è¡¨å¾RMçš„å±€éƒ¨è¡Œä¸ºã€‚ç”Ÿæˆæ‰°åŠ¨å“åº”ä»¥æ˜ç¡®ä¿®æ”¹æ‰‹åŠ¨æŒ‡å®šçš„é«˜çº§è¯„ä¼°å±æ€§ï¼Œè¿™äº›å±æ€§æ˜¯åˆ†æRMè¡Œä¸ºçš„åŸºç¡€ã€‚åœ¨å®šé‡å®éªŒä¸­ï¼Œæˆ‘ä»¬éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯»æ‰¾é«˜è´¨é‡å¯¹æ¯”è§£é‡Šæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç„¶åæˆ‘ä»¬é€šè¿‡å±•ç¤ºæˆ‘ä»¬çš„æ–¹æ³•åœ¨è°ƒæŸ¥RMå¯¹æ¯ä¸ªè¯„ä¼°å±æ€§çš„å…¨å±€æ•æ„Ÿæ€§æ–¹é¢çš„å®šæ€§ç”¨é€”ï¼Œå¹¶å±•ç¤ºå¦‚ä½•è‡ªåŠ¨æå–ä»£è¡¨æ€§ç¤ºä¾‹æ¥è§£é‡Šå’Œæ¯”è¾ƒä¸åŒRMçš„è¡Œä¸ºã€‚æˆ‘ä»¬è®¤ä¸ºæˆ‘ä»¬çš„æ–¹æ³•æ˜¯ä¸€ä¸ªçµæ´»çš„RMè§£é‡Šæ¡†æ¶ï¼Œä¸ºæ›´å¯è§£é‡Šå’Œæ›´å€¼å¾—ä¿¡èµ–çš„LLMå¯¹é½æä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»ä»·å€¼è§‚å¯¹é½æ–¹é¢èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>RMsé€šè¿‡é¢„æµ‹å’Œæ¯”è¾ƒå¥–åŠ±åˆ†æ•°æ¥è¿‘ä¼¼äººç±»å¯¹LLMå“åº”çš„åå¥½ã€‚</li>
<li>RMsä½œä¸ºå¤§å‹é»‘ç®±ï¼Œå…¶é¢„æµ‹ä¸å¯è§£é‡Šã€‚</li>
<li>æå‡ºä½¿ç”¨å¯¹æ¯”è§£é‡Šæ³•æ¥è§£é‡ŠRMsçš„äºŒå…ƒå“åº”æ¯”è¾ƒã€‚</li>
<li>ç”Ÿæˆä¸€ç³»åˆ—ä¸åŸæ¯”è¾ƒç›¸ä¼¼çš„æ–°æ¯”è¾ƒä»¥è¡¨å¾RMçš„å±€éƒ¨è¡Œä¸ºã€‚</li>
<li>åœ¨å®šé‡å®éªŒä¸­éªŒè¯äº†å¯¹æ¯”è§£é‡Šæ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16502">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2411.16502v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2411.16502v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2411.16502v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2411.16502v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Stronger-Models-are-NOT-Stronger-Teachers-for-Instruction-Tuning"><a href="#Stronger-Models-are-NOT-Stronger-Teachers-for-Instruction-Tuning" class="headerlink" title="Stronger Models are NOT Stronger Teachers for Instruction Tuning"></a>Stronger Models are NOT Stronger Teachers for Instruction Tuning</h2><p><strong>Authors:Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Radha Poovendran</strong></p>
<p>Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. We refer to this phenomenon as the Larger Modelsâ€™ Paradox. We observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. We thus develop a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Our experiments across five base models demonstrate that CAR outperforms almost all baselines. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒå·²è¢«å¹¿æ³›é‡‡ç”¨ï¼Œä»¥ç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ‰æ•ˆåœ°éµå¾ªç”¨æˆ·æŒ‡ä»¤ã€‚LLMçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¸¥é‡ä¾èµ–äºç”¨äºè°ƒæ ¡çš„æŒ‡ä»¤æ•°æ®é›†ã€‚æœ€è¿‘ï¼ŒåˆæˆæŒ‡ä»¤æ•°æ®é›†ä½œä¸ºç»æµå¯è¡Œçš„è§£å†³æ–¹æ¡ˆå‡ºç°ï¼Œä¸ºLLMæä¾›å¤šæ ·åŒ–å’Œé«˜è´¨é‡çš„æŒ‡ä»¤ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾æ›´å¤§æˆ–æ›´å¼ºçš„æ¨¡å‹æ˜¯æ›´å¼ºçš„æ•™å¸ˆï¼Œç”¨äºæŒ‡ä»¤å¾®è°ƒï¼Œå› æ­¤ç®€å•åœ°é‡‡ç”¨è¿™äº›æ¨¡å‹ä½œä¸ºå¯¹åˆæˆæŒ‡ä»¤çš„å›åº”ç”Ÿæˆå™¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è´¨ç–‘è¿™ä¸€æ™®éæ¥å—çš„å‡è®¾ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªåŸºç¡€æ¨¡å‹å’ŒäºŒåä¸ªå“åº”ç”Ÿæˆå™¨ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ›´å¤§ã€æ›´å¼ºçš„æ¨¡å‹ä¸ä¸€å®šæ˜¯æ›´å°æ¨¡å‹çš„æ›´å¼ºæ•™å¸ˆã€‚æˆ‘ä»¬å°†è¿™ç§ç°è±¡ç§°ä¸ºâ€œå¤§æ¨¡å‹æ‚–è®ºâ€ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œç°æœ‰æŒ‡æ ‡ä¸èƒ½ç²¾ç¡®é¢„æµ‹å“åº”ç”Ÿæˆå™¨çš„æœ‰æ•ˆæ€§ï¼Œå› ä¸ºå®ƒä»¬å¿½ç•¥äº†è¢«å¾®è°ƒæ•™å¸ˆä¸åŸºç¡€æ¨¡å‹ä¹‹é—´çš„å…¼å®¹æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åä¸ºå…¼å®¹æ€§è°ƒæ•´å¥–åŠ±ï¼ˆCARï¼‰çš„æ–°æŒ‡æ ‡ï¼Œä»¥è¡¡é‡å“åº”ç”Ÿæˆå™¨çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªåŸºç¡€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCARå‡ ä¹ä¼˜äºæ‰€æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07133v3">PDF</a> This is paper is accepted at NAACL 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æŒ‡ä»¤å¾®è°ƒå¹¿æ³›é‡‡çº³ä»¥ç¡®ä¿å…¶æœ‰æ•ˆéµå¾ªç”¨æˆ·æŒ‡ä»¤ã€‚LLMçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¸¥é‡ä¾èµ–äºç”¨äºå¾®è°ƒçš„æŒ‡ä»¤æ•°æ®é›†ã€‚è¿‘æœŸï¼ŒåˆæˆæŒ‡ä»¤æ•°æ®é›†ä½œä¸ºç»æµå¯è¡Œçš„è§£å†³æ–¹æ¡ˆå‡ºç°ï¼Œä¸ºLLMæä¾›å¤šæ ·ä¸”é«˜è´¨é‡æŒ‡ä»¤ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾æ›´å¤§æˆ–æ›´å¼ºçš„æ¨¡å‹æ˜¯æ›´å¼ºçš„æ•™å¸ˆç”¨äºæŒ‡ä»¤å¾®è°ƒï¼Œå¹¶ç®€å•åœ°é‡‡ç”¨è¿™äº›æ¨¡å‹ä½œä¸ºå¯¹åˆæˆæŒ‡ä»¤çš„å“åº”ç”Ÿæˆå™¨ã€‚æœ¬æ–‡æŒ‘æˆ˜è¿™ä¸€æ™®éå‡è®¾ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªåŸºç¡€æ¨¡å‹åŠäºŒåä¸ªå“åº”ç”Ÿæˆå™¨ä¸Šçš„å¹¿æ³›å®éªŒæ˜¾ç¤ºï¼Œæ›´å¤§å’Œæ›´å¼ºçš„æ¨¡å‹ä¸ä¸€å®šæ˜¯æ›´å°æ¨¡å‹æ›´å¼ºæ•™å¸ˆã€‚æˆ‘ä»¬ç§°æ­¤ç°è±¡ä¸ºâ€œå¤§æ¨¡å‹æ‚–è®ºâ€ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ç°æœ‰æŒ‡æ ‡æ— æ³•ç²¾ç¡®é¢„æµ‹å“åº”ç”Ÿæˆå™¨çš„æœ‰æ•ˆæ€§ï¼Œå› ä¸ºå®ƒä»¬å¿½ç•¥äº†ä¸æ•™å¸ˆåŠæ­£åœ¨å¾®è°ƒçš„åŸºç¡€æ¨¡å‹ä¹‹é—´çš„å…¼å®¹æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åä¸ºå…¼å®¹æ€§è°ƒæ•´å¥–åŠ±ï¼ˆCARï¼‰çš„æ–°æŒ‡æ ‡æ¥è¡¡é‡å“åº”ç”Ÿæˆå™¨çš„æœ‰æ•ˆæ€§ã€‚åœ¨äº”ä¸ªåŸºç¡€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCARå‡ ä¹è¶…è¶Šæ‰€æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æŒ‡ä»¤å¾®è°ƒä»¥å¢å¼ºéµå¾ªç”¨æˆ·æŒ‡ä»¤çš„èƒ½åŠ›ã€‚</li>
<li>æŒ‡ä»¤æ•°æ®é›†åœ¨LLMçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>åˆæˆæŒ‡ä»¤æ•°æ®é›†æ˜¯ç»æµå¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºLLMæä¾›å¤šæ ·ä¸”é«˜è´¨é‡çš„æŒ‡ä»¤ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å‡è®¾æ›´å¤§æ¨¡å‹æ˜¯æ›´å¼ºçš„æ•™å¸ˆï¼Œä½†å®éªŒè¡¨æ˜è¿™å¹¶éå¿…ç„¶ã€‚</li>
<li>æ›´å¤§å’Œæ›´å¼ºçš„æ¨¡å‹ä¸ä¸€å®šæ˜¯æ›´å°æ¨¡å‹æ›´å¼ºæ•™å¸ˆçš„ç°è±¡ç§°ä¸ºâ€œå¤§æ¨¡å‹æ‚–è®ºâ€ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æŒ‡æ ‡æ— æ³•ç²¾ç¡®é¢„æµ‹å“åº”ç”Ÿæˆå™¨çš„æœ‰æ•ˆæ€§ï¼Œå› ä¸ºå®ƒä»¬å¿½ç•¥äº†ä¸æ•™å¸ˆåŠåŸºç¡€æ¨¡å‹çš„å…¼å®¹æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07133">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2411.07133v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2411.07133v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2411.07133v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2411.07133v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2411.07133v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Lightning-IR-Straightforward-Fine-tuning-and-Inference-of-Transformer-based-Language-Models-for-Information-Retrieval"><a href="#Lightning-IR-Straightforward-Fine-tuning-and-Inference-of-Transformer-based-Language-Models-for-Information-Retrieval" class="headerlink" title="Lightning IR: Straightforward Fine-tuning and Inference of   Transformer-based Language Models for Information Retrieval"></a>Lightning IR: Straightforward Fine-tuning and Inference of   Transformer-based Language Models for Information Retrieval</h2><p><strong>Authors:Ferdinand Schlatt, Maik FrÃ¶be, Matthias Hagen</strong></p>
<p>A wide range of transformer-based language models have been proposed for information retrieval tasks. However, including transformer-based models in retrieval pipelines is often complex and requires substantial engineering effort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch Lightning-based framework for applying transformer-based language models in retrieval scenarios. Lightning IR provides a modular and extensible architecture that supports all stages of a retrieval pipeline: from fine-tuning and indexing to searching and re-ranking. Designed to be scalable and reproducible, Lightning IR is available as open-source: <a target="_blank" rel="noopener" href="https://github.com/webis-de/lightning-ir">https://github.com/webis-de/lightning-ir</a>. </p>
<blockquote>
<p>é’ˆå¯¹ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ï¼Œå·²ç»æå‡ºäº†å¤šç§åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹ã€‚ç„¶è€Œï¼Œåœ¨æ£€ç´¢ç®¡é“ä¸­åŒ…å«åŸºäºTransformerçš„æ¨¡å‹é€šå¸¸å¾ˆå¤æ‚ï¼Œéœ€è¦å¤§é‡çš„å·¥ç¨‹åŠªåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Lightning IRï¼Œè¿™æ˜¯ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„åŸºäºPyTorch Lightningçš„æ¡†æ¶ï¼Œå¯ç”¨äºåœ¨æ£€ç´¢åœºæ™¯ä¸­åº”ç”¨åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹ã€‚Lightning IRæä¾›äº†ä¸€ç§æ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„æ¶æ„ï¼Œæ”¯æŒæ£€ç´¢ç®¡é“çš„æ‰€æœ‰é˜¶æ®µï¼šä»å¾®è°ƒã€ç´¢å¼•åˆ°æœç´¢å’Œé‡æ–°æ’åºã€‚è®¾è®¡ç”¨äºå¯æ‰©å±•æ€§å’Œå¯é‡å¤æ€§ï¼ŒLightning IRå¯ä½œä¸ºå¼€æºä½¿ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/webis-de/lightning-ir%E3%80%82">https://github.com/webis-de/lightning-irã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04677v4">PDF</a> Accepted as a demo at WSDMâ€™25</p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡ä»‹ç»äº†Lightning IRæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºPyTorch Lightningçš„æ˜“äºä½¿ç”¨çš„æ¡†æ¶ï¼Œç”¨äºåœ¨æ£€ç´¢åœºæ™¯ä¸­åº”ç”¨åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹ã€‚å®ƒæä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„æ¶æ„ï¼Œæ”¯æŒæ£€ç´¢ç®¡é“çš„æ‰€æœ‰é˜¶æ®µï¼ŒåŒ…æ‹¬å¾®è°ƒã€ç´¢å¼•ã€æœç´¢å’Œé‡æ–°æ’åã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºPyTorch Lightningçš„æ¡†æ¶â€”â€”Lightning IRï¼Œç”¨äºåœ¨æ£€ç´¢ä»»åŠ¡ä¸­åº”ç”¨åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹ã€‚</li>
<li>è¯¥æ¡†æ¶è®¾è®¡æ¨¡å—åŒ–ï¼Œå¯æ”¯æŒä»å¾®è°ƒã€ç´¢å¼•åˆ°æœç´¢å’Œé‡æ–°æ’åçš„æ‰€æœ‰æ£€ç´¢é˜¶æ®µã€‚</li>
<li>Lightning IRå…·æœ‰å¯æ‰©å±•æ€§å’Œå¯é‡å¤æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶æ—¨åœ¨ç®€åŒ–åœ¨æ£€ç´¢ä»»åŠ¡ä¸­åº”ç”¨åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹çš„å¤æ‚æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶æ˜¯å¼€æºçš„ï¼Œå¯ä¾›å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚å¯é€šè¿‡é“¾æ¥<a target="_blank" rel="noopener" href="https://github.com/webis-de/lightning-ir%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/webis-de/lightning-irè·å–ã€‚</a></li>
<li>Lightning IRé€‚ç”¨äºå¹¿æ³›çš„åº”ç”¨åœºæ™¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ–‡æ¡£æ£€ç´¢ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2411.04677v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2411.04677v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2411.04677v4/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2411.04677v4/page_3_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Beyond-Linear-Approximations-A-Novel-Pruning-Approach-for-Attention-Matrix"><a href="#Beyond-Linear-Approximations-A-Novel-Pruning-Approach-for-Attention-Matrix" class="headerlink" title="Beyond Linear Approximations: A Novel Pruning Approach for Attention   Matrix"></a>Beyond Linear Approximations: A Novel Pruning Approach for Attention   Matrix</h2><p><strong>Authors:Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, Yufa Zhou</strong></p>
<p>Large Language Models (LLMs) have shown immense potential in enhancing various aspects of our daily lives, from conversational AI to search and AI assistants. However, their growing capabilities come at the cost of extremely large model sizes, making deployment on edge devices challenging due to memory and computational constraints. This paper introduces a novel approach to LLM weight pruning that directly optimizes for approximating the attention matrix, a core component of transformer architectures. Unlike existing methods that focus on linear approximations, our approach accounts for the non-linear nature of the Softmax attention mechanism. We provide theoretical guarantees for the convergence of our Gradient Descent-based optimization method to a near-optimal pruning mask solution. Our empirical results demonstrate the effectiveness of our non-linear pruning approach in maintaining model performance while significantly reducing computational costs, which is beyond the current state-of-the-art methods, i.e., SparseGPT and Wanda, by a large margin. This work establishes a new theoretical foundation for pruning algorithm design in LLMs, potentially paving the way for more efficient LLM inference on resource-constrained devices. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ä¸­å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œæ— è®ºæ˜¯ä»å¯¹è¯å¼äººå·¥æ™ºèƒ½åˆ°æœç´¢å’Œäººå·¥æ™ºèƒ½åŠ©æ‰‹ç­‰å„ä¸ªæ–¹é¢ã€‚ç„¶è€Œï¼Œå…¶èƒ½åŠ›çš„å¢é•¿éœ€è¦æå¤§çš„æ¨¡å‹è§„æ¨¡ä½œä¸ºä»£ä»·ï¼Œç”±äºå†…å­˜å’Œè®¡ç®—èµ„æºçš„é™åˆ¶ï¼Œä½¿å…¶åœ¨è¾¹ç¼˜è®¾å¤‡çš„éƒ¨ç½²å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹LLMæƒé‡ä¿®å‰ªçš„æ–°å‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥ä¼˜åŒ–è¿‘ä¼¼æ³¨æ„åŠ›çŸ©é˜µï¼Œè¿™æ˜¯è½¬æ¢å™¨æ¶æ„çš„æ ¸å¿ƒç»„ä»¶ã€‚ä¸å…¶ä»–å…³æ³¨çº¿æ€§è¿‘ä¼¼çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•è€ƒè™‘äº†Softmaxæ³¨æ„åŠ›æœºåˆ¶çš„éçº¿æ€§ç‰¹æ€§ã€‚æˆ‘ä»¬ä¸ºåŸºäºæ¢¯åº¦ä¸‹é™çš„ä¼˜åŒ–æ–¹æ³•çš„æ”¶æ•›æ€§æä¾›äº†ç†è®ºä¿è¯ï¼Œä»¥æ¥è¿‘æœ€ä¼˜çš„ä¿®å‰ªæ©è†œè§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„å®è¯ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„éçº¿æ€§ä¿®å‰ªæ–¹æ³•åœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶å¤§å¤§è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„SparseGPTå’ŒWandaæ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œä¸ºLLMä¸­çš„ä¿®å‰ªç®—æ³•è®¾è®¡å»ºç«‹äº†æ–°çš„ç†è®ºåŸºç¡€ï¼Œå¯èƒ½ä¸ºèµ„æºå—é™è®¾å¤‡ä¸Šçš„æ›´é«˜æ•ˆLLMæ¨ç†é“ºå¹³é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11261v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¥å¸¸ç”Ÿæ´»çš„å¤šä¸ªé¢†åŸŸï¼Œå¦‚å¯¹è¯å¼AIã€æœç´¢å’ŒAIåŠ©æ‰‹ç­‰æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…¶ä¸æ–­å¢é•¿çš„èƒ½åŠ›æ˜¯ä»¥æå¤§çš„æ¨¡å‹è§„æ¨¡ä¸ºä»£ä»·çš„ï¼Œè¿™ä½¿å¾—åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²é¢ä¸´å†…å­˜å’Œè®¡ç®—çº¦æŸçš„æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹LLMçš„æ–°æƒé‡ä¿®å‰ªæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥ä¼˜åŒ–è¿‘ä¼¼æ³¨æ„åŠ›çŸ©é˜µï¼Œè¿™æ˜¯å˜å‹å™¨æ¶æ„çš„æ ¸å¿ƒç»„ä»¶ã€‚ä¸å…¶ä»–å…³æ³¨çº¿æ€§è¿‘ä¼¼çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•è€ƒè™‘åˆ°Softmaxæ³¨æ„åŠ›æœºåˆ¶çš„éçº¿æ€§ç‰¹æ€§ã€‚æœ¬æ–‡ä¸ºåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•æä¾›äº†æ”¶æ•›åˆ°æ¥è¿‘æœ€ä¼˜ä¿®å‰ªæ©è†œè§£å†³æ–¹æ¡ˆçš„ç†è®ºä¿è¯ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„éçº¿æ€§ä¿®å‰ªæ–¹æ³•åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œå¤§å¤§é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶å¤§å¹…è¶…è¶Šäº†å½“å‰å…ˆè¿›çš„æ–¹æ³•ï¼Œå¦‚SparseGPTå’ŒWandaã€‚è¿™é¡¹ç ”ç©¶ä¸ºLLMä¿®å‰ªç®—æ³•è®¾è®¡å»ºç«‹äº†æ–°çš„ç†è®ºåŸºç¡€ï¼Œå¯èƒ½ä¸ºèµ„æºå—é™è®¾å¤‡ä¸Šçš„LLMæ¨ç†æä¾›æ›´æœ‰æ•ˆçš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤šä¸ªé¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†æ¨¡å‹è§„æ¨¡è¾ƒå¤§ï¼Œéƒ¨ç½²åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå…·æœ‰æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰LLMä¿®å‰ªæ–¹æ³•ä¸»è¦å…³æ³¨çº¿æ€§è¿‘ä¼¼ï¼Œè€Œæœ¬æ–‡æ–¹æ³•è€ƒè™‘åˆ°Softmaxæ³¨æ„åŠ›æœºåˆ¶çš„éçº¿æ€§ç‰¹æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„LLMæƒé‡ä¿®å‰ªæ–¹æ³•ï¼Œç›´æ¥ä¼˜åŒ–è¿‘ä¼¼æ³¨æ„åŠ›çŸ©é˜µã€‚</li>
<li>è¯¥æ–¹æ³•æä¾›äº†ç†è®ºä¿è¯ï¼Œè¯æ˜åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•å¯ä»¥æ”¶æ•›åˆ°æ¥è¿‘æœ€ä¼˜çš„ä¿®å‰ªæ©è†œè§£å†³æ–¹æ¡ˆã€‚</li>
<li>å®è¯ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</li>
<li>ä¸ç°æœ‰å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œå¦‚SparseGPTå’ŒWandaï¼Œæœ¬æ–‡æ–¹æ³•å…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11261">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2410.11261v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_LLM/2410.11261v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-28/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-28/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-28/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Agent/2410.02551v2/page_5_2.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-28  Agentic Reward Modeling Integrating Human Preferences with Verifiable   Correctness Signals for Reliable Reward Systems
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-27/Interactive/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Interactive/2409.05860v2/page_1_0.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-27  Nonlinear Gravitational Radiation Reaction Failed Tail, Memories &   Squares
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16470.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
