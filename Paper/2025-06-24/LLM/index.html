<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-24  Confidence Scoring for LLM-Generated SQL in Supply Chain Data Extraction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c3545f79463c4933987b03be47936731.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-24-æ›´æ–°"><a href="#2025-06-24-æ›´æ–°" class="headerlink" title="2025-06-24 æ›´æ–°"></a>2025-06-24 æ›´æ–°</h1><h2 id="Confidence-Scoring-for-LLM-Generated-SQL-in-Supply-Chain-Data-Extraction"><a href="#Confidence-Scoring-for-LLM-Generated-SQL-in-Supply-Chain-Data-Extraction" class="headerlink" title="Confidence Scoring for LLM-Generated SQL in Supply Chain Data Extraction"></a>Confidence Scoring for LLM-Generated SQL in Supply Chain Data Extraction</h2><p><strong>Authors:Jiekai Ma, Yikai Zhao</strong></p>
<p>Large Language Models (LLMs) have recently enabled natural language interfaces that translate user queries into executable SQL, offering a powerful solution for non-technical stakeholders to access structured data. However, one of the limitation that LLMs do not natively express uncertainty makes it difficult to assess the reliability of their generated queries. This paper presents a case study that evaluates multiple approaches to estimate confidence scores for LLM-generated SQL in supply chain data retrieval. We investigated three strategies: (1) translation-based consistency checks; (2) embedding-based semantic similarity between user questions and generated SQL; and (3) self-reported confidence scores directly produced by the LLM. Our findings reveal that LLMs are often overconfident in their own outputs, which limits the effectiveness of self-reported confidence. In contrast, embedding-based similarity methods demonstrate strong discriminative power in identifying inaccurate SQL. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘å¯ç”¨äº†è‡ªç„¶è¯­è¨€æ¥å£ï¼Œè¿™äº›æ¥å£èƒ½å¤Ÿå°†ç”¨æˆ·æŸ¥è¯¢ç¿»è¯‘æˆå¯æ‰§è¡Œçš„SQLï¼Œä¸ºéæŠ€æœ¯åˆ©ç›Šç›¸å…³è€…è®¿é—®ç»“æ„åŒ–æ•°æ®æä¾›äº†å¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒLLMçš„ä¸€ä¸ªå±€é™æ€§åœ¨äºå®ƒä»¬æ— æ³•å¤©ç”Ÿåœ°è¡¨è¾¾ä¸ç¡®å®šæ€§ï¼Œè¿™ä½¿å¾—è¯„ä¼°å®ƒä»¬ç”Ÿæˆçš„æŸ¥è¯¢çš„å¯é æ€§å˜å¾—å›°éš¾ã€‚æœ¬æ–‡è¿›è¡Œäº†ä¸€é¡¹æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯„ä¼°äº†å¤šç§æ–¹æ³•æ¥ä¼°ç®—LLMç”Ÿæˆçš„SQLçš„ä¿¡å¿ƒåˆ†æ•°ï¼Œåœ¨ä¾›åº”é“¾æ•°æ®æ£€ç´¢ä¸­ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†ä¸‰ç§ç­–ç•¥ï¼šï¼ˆ1ï¼‰åŸºäºç¿»è¯‘çš„ä¸€è‡´æ€§æ£€æŸ¥ï¼›ï¼ˆ2ï¼‰åŸºäºåµŒå…¥çš„ç”¨æˆ·é—®é¢˜ä¸ç”Ÿæˆçš„SQLä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼›ï¼ˆ3ï¼‰ç”±LLMç›´æ¥äº§ç”Ÿçš„è‡ªæˆ‘æŠ¥å‘Šçš„ä¿¡å¿ƒåˆ†æ•°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMé€šå¸¸å¯¹è‡ªå·±çš„è¾“å‡ºè¿‡äºè‡ªä¿¡ï¼Œè¿™é™åˆ¶äº†è‡ªæˆ‘æŠ¥å‘Šä¿¡å¿ƒçš„æœ‰æ•ˆæ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºåµŒå…¥çš„ç›¸ä¼¼æ€§æ–¹æ³•åœ¨è¯†åˆ«ä¸å‡†ç¡®çš„SQLæ–¹é¢è¡¨ç°å‡ºå¾ˆå¼ºçš„è¾¨åˆ«åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17203v1">PDF</a> accepted by KDD workshop AI for Supply Chain 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºç”¨æˆ·æŸ¥è¯¢æä¾›å¯æ‰§è¡Œçš„SQLç¿»è¯‘ï¼Œä¸ºéæŠ€æœ¯ç”¨æˆ·è®¿é—®ç»“æ„åŒ–æ•°æ®æä¾›äº†å¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒLLMä¸åŸç”Ÿè¡¨è¾¾ä¸ç¡®å®šæ€§ï¼Œéš¾ä»¥è¯„ä¼°å…¶ç”ŸæˆæŸ¥è¯¢çš„å¯é æ€§ã€‚æœ¬æ–‡è¯„ä¼°äº†å¤šç§æ–¹æ³•ä¸ºLLMç”Ÿæˆçš„SQLä¼°ç®—ç½®ä¿¡åº¦åˆ†æ•°ï¼ŒåŒ…æ‹¬åŸºäºç¿»è¯‘çš„ä¸€è‡´æ€§æ£€æŸ¥ã€åŸºäºåµŒå…¥çš„ç”¨æˆ·é—®é¢˜ä¸ç”ŸæˆSQLçš„è¯­ä¹‰ç›¸ä¼¼æ€§ä»¥åŠLLMç›´æ¥äº§ç”Ÿçš„è‡ªæˆ‘æŠ¥å‘Šç½®ä¿¡åº¦åˆ†æ•°ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMå¯¹å…¶è¾“å‡ºå¸¸å¸¸è¿‡äºè‡ªä¿¡ï¼Œè‡ªæˆ‘æŠ¥å‘Šçš„ç½®ä¿¡åº¦æœ‰æ•ˆæ€§æœ‰é™ï¼Œè€ŒåŸºäºåµŒå…¥çš„ç›¸ä¼¼æ€§æ–¹æ³•åœ¨è¯†åˆ«ä¸å‡†ç¡®SQLæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„è¾¨åˆ«åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¯å®ç°ç”¨æˆ·æŸ¥è¯¢çš„è‡ªç„¶è¯­è¨€åˆ°å¯æ‰§è¡ŒSQLçš„ç¿»è¯‘ï¼Œä¸ºéæŠ€æœ¯ç”¨æˆ·è®¿é—®ç»“æ„åŒ–æ•°æ®æä¾›ä¾¿æ·é€”å¾„ã€‚</li>
<li>LLMså­˜åœ¨ä¸ç¡®å®šæ€§è¡¨è¾¾çš„é™åˆ¶ï¼Œéš¾ä»¥è¯„ä¼°ç”ŸæˆæŸ¥è¯¢çš„å¯é æ€§ã€‚</li>
<li>è¯„ä¼°LLMç”Ÿæˆçš„SQLç½®ä¿¡åº¦çš„æ–¹æ³•åŒ…æ‹¬åŸºäºç¿»è¯‘çš„ä¸€è‡´æ€§æ£€æŸ¥ã€åŸºäºåµŒå…¥çš„è¯­ä¹‰ç›¸ä¼¼æ€§å’ŒLLMçš„è‡ªæˆ‘æŠ¥å‘Šç½®ä¿¡åº¦åˆ†æ•°ã€‚</li>
<li>LLMå¯¹å…¶è¾“å‡ºå¸¸å¸¸è¿‡äºè‡ªä¿¡ï¼Œè‡ªæˆ‘æŠ¥å‘Šçš„ç½®ä¿¡åº¦æœ‰æ•ˆæ€§å—é™ã€‚</li>
<li>åŸºäºåµŒå…¥çš„ç›¸ä¼¼æ€§æ–¹æ³•åœ¨è¯†åˆ«ä¸å‡†ç¡®çš„SQLæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„è¾¨åˆ«åŠ›ã€‚</li>
<li>LLMåœ¨ä¾›åº”é“¾æ•°æ®æ£€ç´¢ä¸­çš„åº”ç”¨é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¡®ä¿æŸ¥è¯¢çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e3f21617df665361ef6b0882d631352b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7460bfbe989d9ae106ac77db87da6780.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a3c236d6383b2dc51dc18a846d714af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac5efcfa96ca0a99bb104deef5cddc19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8a612c806771477d9a2e2f297866553.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05643a81335f94ae95f0616c6cda590a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ef224d57e9368778144b6030676b8c8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Detecting-LLM-Generated-Short-Answers-and-Effects-on-Learner-Performance"><a href="#Detecting-LLM-Generated-Short-Answers-and-Effects-on-Learner-Performance" class="headerlink" title="Detecting LLM-Generated Short Answers and Effects on Learner Performance"></a>Detecting LLM-Generated Short Answers and Effects on Learner Performance</h2><p><strong>Authors:Shambhavi Bhushan, Danielle R Thomas, Conrad Borchers, Isha Raghuvanshi, Ralph Abboud, Erin Gatz, Shivang Gupta, Kenneth Koedinger</strong></p>
<p>The increasing availability of large language models (LLMs) has raised concerns about their potential misuse in online learning. While tools for detecting LLM-generated text exist and are widely used by researchers and educators, their reliability varies. Few studies have compared the accuracy of detection methods, defined criteria to identify content generated by LLM, or evaluated the effect on learner performance from LLM misuse within learning. In this study, we define LLM-generated text within open responses as those produced by any LLM without paraphrasing or refinement, as evaluated by human coders. We then fine-tune GPT-4o to detect LLM-generated responses and assess the impact on learning from LLM misuse. We find that our fine-tuned LLM outperforms the existing AI detection tool GPTZero, achieving an accuracy of 80% and an F1 score of 0.78, compared to GPTZeroâ€™s accuracy of 70% and macro F1 score of 0.50, demonstrating superior performance in detecting LLM-generated responses. We also find that learners suspected of LLM misuse in the open response question were more than twice as likely to correctly answer the corresponding posttest MCQ, suggesting potential misuse across both question types and indicating a bypass of the learning process. We pave the way for future work by demonstrating a structured, code-based approach to improve LLM-generated response detection and propose using auxiliary statistical indicators such as unusually high assessment scores on related tasks, readability scores, and response duration. In support of open science, we contribute data and code to support the fine-tuning of similar models for similar use cases. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ—¥ç›Šæ™®åŠï¼Œäººä»¬è¶Šæ¥è¶Šæ‹…å¿ƒå…¶åœ¨åœ¨çº¿å­¦ä¹ ä¸­å¯èƒ½è¢«æ»¥ç”¨ã€‚å°½ç®¡å­˜åœ¨æ£€æµ‹LLMç”Ÿæˆæ–‡æœ¬çš„å·¥å…·ï¼Œç ”ç©¶è€…å’Œæ•™è‚²å·¥ä½œè€…ä¹Ÿå¹¿æ³›ä½¿ç”¨è¿™äº›å·¥å…·ï¼Œä½†å®ƒä»¬çš„å¯é æ€§å„ä¸ç›¸åŒã€‚å¾ˆå°‘æœ‰ç ”ç©¶æ¯”è¾ƒæ£€æµ‹æ–¹æ³•çš„å‡†ç¡®æ€§ï¼Œå®šä¹‰è¯†åˆ«LLMç”Ÿæˆå†…å®¹çš„æ ‡å‡†ï¼Œæˆ–è¯„ä¼°LLMåœ¨å­¦ä¹ ä¸­çš„æ»¥ç”¨å¯¹å­¦ä¹ è€…è¡¨ç°çš„å½±å“ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å°†å¼€æ”¾å›åº”ä¸­çš„LLMç”Ÿæˆæ–‡æœ¬å®šä¹‰ä¸ºç”±ä»»ä½•LLMäº§ç”Ÿä¸”æœªç»è½¬è¿°æˆ–æ¶¦è‰²çš„æ–‡æœ¬ï¼Œç”±äººç±»ç¼–ç å‘˜è¿›è¡Œè¯„ä¼°ã€‚ç„¶åï¼Œæˆ‘ä»¬å¾®è°ƒGPT-4oæ¥æ£€æµ‹LLMç”Ÿæˆçš„å›åº”ï¼Œå¹¶è¯„ä¼°LLMæ»¥ç”¨å¯¹å­¦ä¹ çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œæˆ‘ä»¬å¾®è°ƒåçš„LLMä¼˜äºç°æœ‰çš„AIæ£€æµ‹å·¥å…·GPTZeroï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†80%ï¼ŒF1åˆ†æ•°ä¸º0.78ï¼Œè€ŒGPTZeroçš„å‡†ç¡®ç‡ä¸º70%ï¼Œå®è§‚F1åˆ†æ•°ä¸º0.50ï¼Œæ˜¾ç¤ºå‡ºåœ¨æ£€æµ‹LLMç”Ÿæˆå›åº”æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œåœ¨å¼€æ”¾å›åº”é—®é¢˜ä¸­è¢«æ€€ç–‘æ»¥ç”¨LLMçš„å­¦ä¹ è€…æ­£ç¡®å›ç­”åç»­æµ‹è¯•é€‰æ‹©é¢˜çš„å¯èƒ½æ€§æ˜¯æœªæ»¥ç”¨è€…çš„ä¸¤å€ä»¥ä¸Šï¼Œè¿™è¡¨æ˜ä¸¤ç§ç±»å‹çš„é—®é¢˜ä¸­éƒ½å¯èƒ½å­˜åœ¨æ»¥ç”¨æƒ…å†µï¼Œå¹¶è¡¨æ˜å¯èƒ½ç»•è¿‡äº†å­¦ä¹ è¿‡ç¨‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§ç»“æ„åŒ–çš„ã€åŸºäºä»£ç çš„æ–¹æ³•æ¥æ”¹è¿›LLMç”Ÿæˆçš„å“åº”æ£€æµ‹ï¼Œå¹¶æå‡ºä½¿ç”¨è¾…åŠ©ç»Ÿè®¡æŒ‡æ ‡ï¼Œå¦‚ç›¸å…³ä»»åŠ¡çš„å¼‚å¸¸é«˜è¯„åˆ†ã€å¯è¯»æ€§è¯„åˆ†å’Œå“åº”æŒç»­æ—¶é—´ç­‰ã€‚æˆ‘ä»¬æ”¯æŒå¼€æ”¾ç§‘å­¦ï¼Œæä¾›æ•°æ®å’Œä»£ç ï¼Œä»¥æ”¯æŒç±»ä¼¼ç”¨ä¾‹çš„ç±»ä¼¼æ¨¡å‹çš„å¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17196v1">PDF</a> Accepted for publication at the 19th European Conference on   Technology Enhanced Learning (ECTEL 2025). This is the authorâ€™s accepted   manuscript</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™®åŠå¼•å‘äº†å¯¹å…¶åœ¨çº¿å­¦ä¹ ç¯å¢ƒä¸­æ½œåœ¨è¯¯ç”¨çš„å…³æ³¨ã€‚å°½ç®¡å­˜åœ¨æ£€æµ‹LLMç”Ÿæˆæ–‡æœ¬çš„å·¥å…·ï¼Œä½†å…¶å¯é æ€§å‚å·®ä¸é½ã€‚æœ¬ç ”ç©¶å®šä¹‰äº†LLMç”Ÿæˆæ–‡æœ¬çš„æ ‡å‡†ï¼Œå³ä»»ä½•æœªç»æ”¹ç¼–æˆ–å®Œå–„çš„LLMç”Ÿæˆå†…å®¹ã€‚é€šè¿‡å¾®è°ƒGPT-4oæ¥æ£€æµ‹LLMç”Ÿæˆçš„å“åº”ï¼Œå¹¶è¯„ä¼°LLMè¯¯ç”¨å¯¹å­¦ä¹ çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œå¾®è°ƒåçš„LLMæ£€æµ‹æ€§èƒ½ä¼˜äºç°æœ‰å·¥å…·GPTZeroï¼Œå‡†ç¡®ç‡é«˜è¾¾80%ï¼ŒF1åˆ†æ•°ä¸º0.78ï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„æ£€æµ‹å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œå‘ç°ç–‘ä¼¼ä½¿ç”¨LLMçš„å¼€æ”¾æ€§é—®é¢˜å›ç­”è€…åœ¨éšåçš„å¤šé¡¹é€‰æ‹©é¢˜æµ‹è¯•ä¸­æ­£ç¡®ç‡æ›´é«˜ï¼Œè¡¨æ˜å¯èƒ½å­˜åœ¨ç»•è¿‡å­¦ä¹ è¿‡ç¨‹çš„æƒ…å†µã€‚æœ¬ç ”ç©¶ä¸ºæœªæ¥æ”¹è¿›LLMç”Ÿæˆå“åº”æ£€æµ‹æä¾›äº†ç»“æ„åŒ–ä»£ç æ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä½¿ç”¨è¾…åŠ©ç»Ÿè®¡æŒ‡æ ‡çš„å»ºè®®ã€‚åŒæ—¶ï¼Œä¸ºäº†æ”¯æŒå¼€æ”¾ç§‘å­¦ï¼Œæœ¬ç ”ç©¶æä¾›æ•°æ®å’Œä»£ç æ”¯æŒç±»ä¼¼ç”¨ä¾‹çš„æ¨¡å‹å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™®åŠå¼•å‘äº†å¯¹å…¶åœ¨çº¿å­¦ä¹ ç¯å¢ƒè¯¯ç”¨çš„å…³æ³¨ã€‚</li>
<li>ç ”ç©¶å®šä¹‰äº†LLMç”Ÿæˆæ–‡æœ¬çš„æ ‡å‡†ä¸ºæœªç»æ”¹ç¼–æˆ–å®Œå–„çš„LLMå†…å®¹ã€‚</li>
<li>é€šè¿‡å¾®è°ƒGPT-4oæ£€æµ‹LLMç”Ÿæˆçš„å“åº”ï¼Œè¡¨ç°å‡ºè¾ƒé«˜çš„æ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>ç›¸æ¯”ç°æœ‰å·¥å…·GPTZeroï¼Œå¾®è°ƒåçš„LLMæ£€æµ‹æ€§èƒ½æ›´ä¼˜ã€‚</li>
<li>å­¦ä¹ è€…åœ¨ç–‘ä¼¼ä½¿ç”¨LLMçš„å¼€æ”¾æ€§é—®é¢˜å›ç­”ååœ¨å¤šé¡¹é€‰æ‹©é¢˜æµ‹è¯•ä¸­è¡¨ç°æ›´ä½³ï¼Œæš—ç¤ºå¯èƒ½ç»•è¿‡å­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºæœªæ¥æ”¹è¿›LLMå“åº”æ£€æµ‹æä¾›äº†ç»“æ„åŒ–ä»£ç æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c371411f8bad41b21063f4b4b7918292.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b2bf2a2e021f0386ca64eb9eef2230b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec9451ef3f6473b9ddb9e87f22d736c3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Prompting-Obscures-Hallucination-Cues-in-Large-Language-Models-An-Empirical-Evaluation"><a href="#Chain-of-Thought-Prompting-Obscures-Hallucination-Cues-in-Large-Language-Models-An-Empirical-Evaluation" class="headerlink" title="Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models: An Empirical Evaluation"></a>Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models: An Empirical Evaluation</h2><p><strong>Authors:Jiahao Cheng, Tiancheng Su, Jia Yuan, Guoxiu He, Jiawei Liu, Xinqi Tao, Jingwen Xie, Huaxia Li</strong></p>
<p>Large Language Models (LLMs) often exhibit \textit{hallucinations}, generating factually incorrect or semantically irrelevant content in response to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by encouraging step-by-step reasoning, but its impact on hallucination detection remains underexplored. To bridge this gap, we conduct a systematic empirical evaluation. We begin with a pilot experiment, revealing that CoT reasoning significantly affects the LLMâ€™s internal states and token probability distributions. Building on this, we evaluate the impact of various CoT prompting methods on mainstream hallucination detection methods across both instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three key dimensions: changes in hallucination score distributions, variations in detection accuracy, and shifts in detection confidence. Our findings show that while CoT prompting helps reduce hallucination frequency, it also tends to obscure critical signals used for detection, impairing the effectiveness of various detection methods. Our study highlights an overlooked trade-off in the use of reasoning. Code is publicly available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/cot-hallu-detect">https://anonymous.4open.science/r/cot-hallu-detect</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¸å¸¸å‡ºç°â€œå¹»è§‰â€ï¼Œå³ç”Ÿæˆä¸æç¤ºå†…å®¹äº‹å®ä¸Šä¸æ­£ç¡®æˆ–è¯­ä¹‰ä¸Šä¸ç›¸å…³çš„å†…å®¹ã€‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå¯ä»¥é€šè¿‡é¼“åŠ±é€æ­¥æ¨ç†æ¥ç¼“è§£å¹»è§‰é—®é¢˜ï¼Œä½†å…¶å¯¹å¹»è§‰æ£€æµ‹çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯ç ”ç©¶ã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œäº†ä¸€é¡¹è¯•ç‚¹å®éªŒï¼Œå‘ç°é“¾å¼æ€ç»´æ¨ç†å¯¹LLMçš„å†…éƒ¨çŠ¶æ€å’Œç¬¦å·æ¦‚ç‡åˆ†å¸ƒäº§ç”Ÿäº†æ˜¾è‘—å½±å“ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¤šç§é“¾å¼æ€ç»´æç¤ºæ–¹æ³•å¯¹ä¸»æµå¹»è§‰æ£€æµ‹æ–¹æ³•çš„å†²å‡»ï¼Œæ¶‰åŠæŒ‡ä»¤è°ƒä¼˜å’Œé¢å‘æ¨ç†çš„LLMã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è€ƒå¯Ÿäº†ä¸‰ä¸ªæ–¹é¢ï¼šå¹»è§‰åˆ†æ•°åˆ†å¸ƒçš„å˜åŒ–ã€æ£€æµ‹å‡†ç¡®åº¦çš„å˜åŒ–ä»¥åŠæ£€æµ‹ä¿¡å¿ƒçš„è½¬å˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè™½ç„¶é“¾å¼æ€ç»´æç¤ºæœ‰åŠ©äºå‡å°‘å¹»è§‰é¢‘ç‡ï¼Œä½†å®ƒä¹Ÿå€¾å‘äºæ©ç›–ç”¨äºæ£€æµ‹çš„å…³é”®ä¿¡å·ï¼Œä»è€ŒæŸå®³äº†å„ç§æ£€æµ‹æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶æŒ‡å‡ºäº†åœ¨è¿ç”¨æ¨ç†æ—¶ä¸€ä¸ªè¢«å¿½è§†çš„æƒè¡¡ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/cot-hallu-detect%E3%80%82">https://anonymous.4open.science/r/cot-hallu-detectã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17088v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¼šå‡ºç°å¹»è§‰ç°è±¡ï¼Œå³ç”Ÿæˆä¸æç¤ºå†…å®¹äº‹å®é”™è¯¯æˆ–è¯­ä¹‰ä¸ç›¸å…³çš„å†…å®¹ã€‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå¯ä»¥é€šè¿‡é¼“åŠ±é€æ­¥æ¨ç†æ¥å‡è½»å¹»è§‰ç°è±¡ï¼Œä½†å…¶å¯¹å¹»è§‰æ£€æµ‹çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿå®è¯ç ”ç©¶å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚åˆæ­¥å®éªŒè¡¨æ˜ï¼ŒCoTæ¨ç†å¯¹LLMçš„å†…éƒ¨çŠ¶æ€å’Œä»¤ç‰Œæ¦‚ç‡åˆ†å¸ƒäº§ç”Ÿäº†æ˜¾è‘—å½±å“ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¤šç§CoTæç¤ºæ–¹æ³•å¯¹ä¸»æµå¹»è§‰æ£€æµ‹æ–¹æ³•çš„å½±å“ï¼Œæ¶‰åŠæŒ‡ä»¤è°ƒä¼˜å’Œé¢å‘æ¨ç†çš„LLMã€‚ç ”ç©¶å‘ç°ï¼ŒCoTæç¤ºæœ‰åŠ©äºå‡å°‘å¹»è§‰é¢‘ç‡ï¼Œä½†ä¹Ÿä¼šæ©ç›–ç”¨äºæ£€æµ‹çš„å…³é”®ä¿¡å·ï¼Œä»è€Œå½±å“å„ç§æ£€æµ‹æ–¹æ³•çš„æ€§èƒ½ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†æ¨ç†ä½¿ç”¨ä¸­çš„æƒè¡¡é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¼šå‡ºç°å¹»è§‰ç°è±¡ï¼Œç”Ÿæˆä¸æç¤ºä¸ç¬¦çš„å†…å®¹ã€‚</li>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºèƒ½å¤Ÿé€šè¿‡é¼“åŠ±é€æ­¥æ¨ç†æ¥å‡è½»LLMçš„å¹»è§‰ç°è±¡ã€‚</li>
<li>CoTæ¨ç†å¯¹LLMçš„å†…éƒ¨çŠ¶æ€å’Œä»¤ç‰Œæ¦‚ç‡åˆ†å¸ƒæœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>CoTæç¤ºæ–¹æ³•å¯¹ä¸»æµå¹»è§‰æ£€æµ‹æ–¹æ³•çš„æ€§èƒ½æœ‰å½±å“ã€‚</li>
<li>CoTæç¤ºæœ‰åŠ©äºå‡å°‘å¹»è§‰é¢‘ç‡ï¼Œä½†å¯èƒ½æ©ç›–ç”¨äºæ£€æµ‹çš„å…³é”®ä¿¡å·ã€‚</li>
<li>åœ¨ä½¿ç”¨æ¨ç†æ—¶å­˜åœ¨æƒè¡¡é—®é¢˜ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘å¹»è§‰æ£€æµ‹å’Œæ¨ç†æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-48a09b4bec85782fca355316fec8dccc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-37e85487e3b3d22b59e1eb59ebf87266.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-796a4f0508590758ae8b352cb74ca8f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8470ed37decea58e3abb2b529d09897a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30fe41e44e05c928711f130a781cb57a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Tower-Bridging-Generality-and-Translation-Specialization-in-Multilingual-LLMs"><a href="#Tower-Bridging-Generality-and-Translation-Specialization-in-Multilingual-LLMs" class="headerlink" title="Tower+: Bridging Generality and Translation Specialization in   Multilingual LLMs"></a>Tower+: Bridging Generality and Translation Specialization in   Multilingual LLMs</h2><p><strong>Authors:Ricardo Rei, Nuno M. Guerreiro, JosÃ© Pombal, JoÃ£o Alves, Pedro Teixeirinha, Amin Farajian, AndrÃ© F. T. Martins</strong></p>
<p>Fine-tuning pretrained LLMs has been shown to be an effective strategy for reaching state-of-the-art performance on specific tasks like machine translation. However, this process of adaptation often implies sacrificing general-purpose capabilities, such as conversational reasoning and instruction-following, hampering the utility of the system in real-world applications that require a mixture of skills. In this paper, we introduce Tower+, a suite of models designed to deliver strong performance across both translation and multilingual general-purpose text capabilities. We achieve a Pareto frontier between translation specialization and multilingual general-purpose capabilities by introducing a novel training recipe that builds on Tower (Alves et al., 2024), comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. At each stage of training, we carefully generate and curate data to strengthen performance on translation as well as general-purpose tasks involving code generation, mathematics problem solving, and general instruction-following. We develop models at multiple scales: 2B, 9B, and 72B. Our smaller models often outperform larger general-purpose open-weight and proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we introduce for evaluating both translation and instruction-following. Our findings highlight that it is possible to rival frontier models in general capabilities, while optimizing for specific business domains, such as translation and localization. </p>
<blockquote>
<p>è°ƒæ•´é¢„è®­ç»ƒLLMå·²è¢«è¯æ˜æ˜¯è¾¾åˆ°æœºå™¨ç¿»è¯‘ç­‰ç‰¹å®šä»»åŠ¡çš„æœ€å…ˆè¿›æ€§èƒ½çš„æœ‰æ•ˆç­–ç•¥ã€‚ç„¶è€Œï¼Œè¿™ä¸€é€‚åº”è¿‡ç¨‹å¾€å¾€æ„å‘³ç€ç‰ºç‰²é€šç”¨èƒ½åŠ›ï¼Œå¦‚å¯¹è¯æ¨ç†å’ŒæŒ‡ä»¤éµå¾ªï¼Œä»è€Œå¦¨ç¢äº†ç³»ç»Ÿåœ¨éœ€è¦æ··åˆæŠ€èƒ½çš„å®é™…åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Tower+ï¼Œä¸€å¥—æ—¨åœ¨åŒæ—¶åœ¨ç¿»è¯‘å’Œè·¨è¯­è¨€é€šç”¨æ–‡æœ¬èƒ½åŠ›æ–¹é¢å®ç°å¼ºåŠ²æ€§èƒ½çš„æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„è®­ç»ƒé…æ–¹ï¼Œåœ¨Towerï¼ˆAlvesç­‰äººï¼Œ2024ï¼‰çš„åŸºç¡€ä¸Šï¼Œå®ç°äº†ç¿»è¯‘ä¸“ä¸šåŒ–ä¸è·¨è¯­è¨€é€šç”¨èƒ½åŠ›ä¹‹é—´çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚è¯¥è®­ç»ƒé…æ–¹åŒ…æ‹¬ç»§ç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒã€åå¥½ä¼˜åŒ–ä»¥åŠä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚åœ¨è®­ç»ƒçš„æ¯ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬éƒ½ä¼šç²¾å¿ƒç”Ÿæˆå’Œç­›é€‰æ•°æ®ï¼Œä»¥åŠ å¼ºç¿»è¯‘ä»¥åŠæ¶‰åŠä»£ç ç”Ÿæˆã€æ•°å­¦é—®é¢˜è§£å†³å’Œä¸€èˆ¬æŒ‡ä»¤éµå¾ªçš„é€šç”¨ä»»åŠ¡çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªè§„æ¨¡ä¸Šå¼€å‘æ¨¡å‹ï¼š2Bã€m 9Bå’Œ72Bã€‚æˆ‘ä»¬çš„å°å‹æ¨¡å‹é€šå¸¸è¡¨ç°ä¼˜äºå¤§å‹é€šç”¨å¼€æ”¾æƒé‡å’Œä¸“æœ‰LLMï¼ˆä¾‹å¦‚Llama 3.3 70Bã€GPT-4oï¼‰ã€‚æˆ‘ä»¬çš„å¤§å‹æ¨¡å‹åœ¨é«˜èµ„æºè¯­è¨€ç¿»è¯‘æ–¹é¢è¾¾åˆ°ä¸šå†…æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨æˆ‘ä»¬å¼•å…¥çš„å¤šè¯­ç§Arena Hardè¯„ä¼°å’ŒIF-MTåŸºå‡†æµ‹è¯•ä¸­å–å¾—é¡¶å°–ç»“æœã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨é’ˆå¯¹ç‰¹å®šä¸šåŠ¡é¢†åŸŸè¿›è¡Œä¼˜åŒ–æ—¶ï¼Œå®Œå…¨æœ‰å¯èƒ½èµ¶è¶…é€šç”¨èƒ½åŠ›çš„å°–ç«¯æ¨¡å‹ï¼Œå¦‚ç¿»è¯‘å’Œæœ¬åœ°åŒ–ç­‰é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17080v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Tower+æ¨¡å‹ç³»åˆ—ï¼Œæ—¨åœ¨å®ç°ç¿»è¯‘å’Œå¤šè¯­ç§é€šç”¨æ–‡æœ¬èƒ½åŠ›çš„å¹³è¡¡ã€‚é€šè¿‡å¼•å…¥æ–°çš„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒã€åå¥½ä¼˜åŒ–å’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼Œè¾¾åˆ°ç¿»è¯‘ä¸“ä¸šåŒ–å’Œå¤šè¯­ç§é€šç”¨èƒ½åŠ›ä¹‹é—´çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚å¼€å‘ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬2Bã€9Bå’Œ72Bï¼Œå¹¶åœ¨ç¿»è¯‘å’Œé€šç”¨ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚æœ€å¤§çš„æ¨¡å‹åœ¨é«˜èµ„æºè¯­è¨€ç¿»è¯‘æ–¹é¢è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨å¤šè¯­ç§Arena Hardè¯„ä¼°å’Œæ–°çš„IF-MTåŸºå‡†æµ‹è¯•ä¸­å–å¾—é¡¶å°–ç»“æœã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯ä»¥åœ¨ä¼˜åŒ–ç‰¹å®šä¸šåŠ¡é¢†åŸŸï¼ˆå¦‚ç¿»è¯‘å’Œæœ¬åœ°åŒ–ï¼‰çš„åŒæ—¶ï¼Œä¸å‰æ²¿æ¨¡å‹åœ¨é€šç”¨èƒ½åŠ›ä¸Šç›¸æŠ—è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Tower+æ¨¡å‹ç³»åˆ—æ—¨åœ¨å¹³è¡¡ç¿»è¯‘å’Œå¤šç§è¯­è¨€é€šç”¨æ–‡æœ¬èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç»“åˆå¤šç§è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒç­‰ï¼Œå®ç°å¸•ç´¯æ‰˜æœ€ä¼˜å‰æ²¿ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªè§„æ¨¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬å°å‹ã€ä¸­å‹å’Œå¤§å‹æ¨¡å‹ã€‚</li>
<li>åœ¨ç¿»è¯‘å’Œé€šç”¨ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬ä»£ç ç”Ÿæˆã€æ•°å­¦é—®é¢˜è§£å†³å’ŒæŒ‡ä»¤éµå¾ªç­‰ã€‚</li>
<li>æœ€å¤§æ¨¡å‹åœ¨é«˜èµ„æºè¯­è¨€ç¿»è¯‘ä¸Šè¡¨ç°æœ€ä½³ï¼Œå¹¶åœ¨å¤šè¯­ç§è¯„ä¼°ä¸­å–å¾—é¡¶å°–ç»“æœã€‚</li>
<li>å¼•å…¥æ–°çš„åŸºå‡†æµ‹è¯•IF-MTï¼Œç”¨äºè¯„ä¼°ç¿»è¯‘å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-938187e11a954f6183f36eab58d7d3a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0cf2b924c33bc7987b47f277cebe22f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a52d10368cb4bebc80ade7724feb278.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a41e9aab2c6b929e9f64f064645d83a0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Simultaneous-Translation-with-Offline-Speech-and-LLM-Models-in-CUNI-Submission-to-IWSLT-2025"><a href="#Simultaneous-Translation-with-Offline-Speech-and-LLM-Models-in-CUNI-Submission-to-IWSLT-2025" class="headerlink" title="Simultaneous Translation with Offline Speech and LLM Models in CUNI   Submission to IWSLT 2025"></a>Simultaneous Translation with Offline Speech and LLM Models in CUNI   Submission to IWSLT 2025</h2><p><strong>Authors:Dominik MachÃ¡Äek, Peter PolÃ¡k</strong></p>
<p>This paper describes Charles University submission to the Simultaneous Speech Translation Task of the IWSLT 2025. We cover all four language pairs with a direct or cascade approach. The backbone of our systems is the offline Whisper speech model, which we use for both translation and transcription in simultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We further improve the performance by prompting to inject in-domain terminology, and we accommodate context. Our cascaded systems further use EuroLLM for unbounded simultaneous translation. Compared to the Organizersâ€™ baseline, our systems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on English to German, Chinese and Japanese on the development sets. Additionally, we also propose a new enhanced measure of speech recognition latency. </p>
<blockquote>
<p>æœ¬æ–‡æè¿°äº†æŸ¥å°”æ–¯å¤§å­¦å¯¹IWSLT 2025åŒæ­¥è¯­éŸ³è¯†åˆ«ç¿»è¯‘ä»»åŠ¡çš„æäº¤å†…å®¹ã€‚æˆ‘ä»¬é‡‡ç”¨ç›´æ¥æˆ–çº§è”çš„æ–¹æ³•è¦†ç›–æ‰€æœ‰å››ç§è¯­è¨€å¯¹ã€‚æˆ‘ä»¬ç³»ç»Ÿçš„æ ¸å¿ƒæ˜¯ç¦»çº¿Whisperè¯­éŸ³æ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨åŒæ­¥æ¨¡å¼ä¸‹è¿›è¡Œç¿»è¯‘å’Œè½¬å½•æ—¶ä½¿ç”¨æœ€å…ˆè¿›çš„åŒæ­¥ç­–ç•¥AlignAttã€‚é€šè¿‡æç¤ºæ³¨å…¥é¢†åŸŸä¸“ä¸šæœ¯è¯­ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œå¹¶é€‚åº”äº†ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬çš„çº§è”ç³»ç»Ÿè¿˜è¿›ä¸€æ­¥ä½¿ç”¨EuroLLMè¿›è¡Œæ— ç•ŒåŒæ­¥ç¿»è¯‘ã€‚ä¸ç»„ç»‡è€…è®¾å®šçš„åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨å¼€å‘é›†ä¸Šæ·å…‹è¯­åˆ°è‹±è¯­çš„BLEUå¾—åˆ†æé«˜äº†2åˆ†ï¼Œè‹±è¯­åˆ°å¾·è¯­ã€ä¸­æ–‡å’Œæ—¥è¯­çš„BLEUå¾—åˆ†æé«˜äº†13åˆ°22åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æ”¹è¿›è¯­éŸ³è¯†åˆ«å»¶è¿Ÿçš„åº¦é‡æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17077v1">PDF</a> IWSLT 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡ä»‹ç»äº†Charleså¤§å­¦åœ¨IWSLT 2025åŒæ­¥è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é‡‡ç”¨å››ç§è¯­è¨€é…å¯¹æ–¹æ³•ï¼Œåˆ©ç”¨å‰æ²¿çš„åŒæ­¥ç­–ç•¥AlignAttå®ç°åŒæ—¶ç¿»è¯‘å’Œè½¬å½•åŠŸèƒ½ã€‚è®ºæ–‡è¿›ä¸€æ­¥æå‡äº†ç³»ç»Ÿæ€§èƒ½ï¼Œé€šè¿‡åœ¨ç³»ç»Ÿå†…æç¤ºä¸“ä¸šé¢†åŸŸæœ¯è¯­ä»¥é€‚åº”è¯­å¢ƒã€‚é‡‡ç”¨EuroLLMæ¨¡å‹è¿›è¡Œæ— ç•ŒåŒæ­¥ç¿»è¯‘ã€‚ç›¸è¾ƒäºä¸»åŠæ–¹åŸºå‡†çº¿ï¼Œè¯¥ç³»ç»Ÿåœ¨æ·å…‹è¯­è‡³è‹±è¯­ç¿»è¯‘æ–¹é¢æå‡äº†2ä¸ªBLEUç‚¹ï¼Œåœ¨è‹±è¯­è‡³å¾·è¯­ã€ä¸­æ–‡å’Œæ—¥è¯­æ–¹é¢åœ¨å¼€å‘é›†ä¸Šæå‡äº†13è‡³22ä¸ªBLEUç‚¹ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ”¹è¿›çš„è¯­éŸ³è¯†åˆ«å»¶è¿Ÿåº¦é‡æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Charleså¤§å­¦åœ¨IWSLT 2025çš„åŒæ­¥è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­æœ‰æ‰€è´¡çŒ®ã€‚</li>
<li>è¯¥ç ”ç©¶é‡‡ç”¨å››ç§è¯­è¨€é…å¯¹æ–¹æ³•ï¼Œåˆ©ç”¨å‰æ²¿çš„åŒæ­¥ç­–ç•¥AlignAttè¿›è¡Œç¿»è¯‘å’Œè½¬å½•ã€‚</li>
<li>ç³»ç»Ÿæ€§èƒ½é€šè¿‡æç¤ºä¸“ä¸šé¢†åŸŸæœ¯è¯­å’Œé€‚åº”è¯­å¢ƒå¾—åˆ°äº†è¿›ä¸€æ­¥æå‡ã€‚</li>
<li>é‡‡ç”¨EuroLLMæ¨¡å‹è¿›è¡Œæ— ç•ŒåŒæ­¥ç¿»è¯‘æ˜¯è¯¥ç ”ç©¶çš„äº®ç‚¹ä¹‹ä¸€ã€‚</li>
<li>ä¸ä¸»åŠæ–¹åŸºå‡†çº¿ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿåœ¨å¤šç§è¯­è¨€å¯¹çš„ç¿»è¯‘ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„è¯­éŸ³è¯†åˆ«å»¶è¿Ÿåº¦é‡æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17077">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-799366d1360e8d86ed3a73912a3a3ffc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfd32ed6ef97092d778005d88b869361.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c68dbede2536384458138ecc052758c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e964ed6a0684aaa0d13fa63be341cd69.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0505ee424bfaa03e228eadc2efc68857.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="From-Concepts-to-Components-Concept-Agnostic-Attention-Module-Discovery-in-Transformers"><a href="#From-Concepts-to-Components-Concept-Agnostic-Attention-Module-Discovery-in-Transformers" class="headerlink" title="From Concepts to Components: Concept-Agnostic Attention Module Discovery   in Transformers"></a>From Concepts to Components: Concept-Agnostic Attention Module Discovery   in Transformers</h2><p><strong>Authors:Jingtong Su, Julia Kempe, Karen Ullrich</strong></p>
<p>Transformers have achieved state-of-the-art performance across language and vision tasks. This success drives the imperative to interpret their internal mechanisms with the dual goals of enhancing performance and improving behavioral control. Attribution methods help advance interpretability by assigning model outputs associated with a target concept to specific model components. Current attribution research primarily studies multi-layer perceptron neurons and addresses relatively simple concepts such as factual associations (e.g., Paris is located in France). This focus tends to overlook the impact of the attention mechanism and lacks a unified approach for analyzing more complex concepts. To fill these gaps, we introduce Scalable Attention Module Discovery (SAMD), a concept-agnostic method for mapping arbitrary, complex concepts to specific attention heads of general transformer models. We accomplish this by representing each concept as a vector, calculating its cosine similarity with each attention head, and selecting the TopK-scoring heads to construct the concept-associated attention module. We then propose Scalar Attention Module Intervention (SAMI), a simple strategy to diminish or amplify the effects of a concept by adjusting the attention module using only a single scalar parameter. Empirically, we demonstrate SAMD on concepts of varying complexity, and visualize the locations of their corresponding modules. Our results demonstrate that module locations remain stable before and after LLM post-training, and confirm prior work on the mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on HarmBench (+72.7%) by diminishing â€œsafetyâ€ and improve performance on the GSM8K benchmark (+1.6%) by amplifying â€œreasoningâ€. Lastly, we highlight the domain-agnostic nature of our approach by suppressing the image classification accuracy of vision transformers on ImageNet. </p>
<blockquote>
<p>Transformeræ¨¡å‹åœ¨è¯­è¨€ä¸è§†è§‰ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚è¿™ä¸€æˆåŠŸä¿ƒä½¿æˆ‘ä»¬è¿«åˆ‡éœ€è¦å¯¹å†…éƒ¨æœºåˆ¶è¿›è¡Œè§£è¯»ï¼Œæ—¨åœ¨æé«˜æ€§èƒ½å’Œæ”¹å–„è¡Œä¸ºæ§åˆ¶ã€‚å½’å› æ–¹æ³•æœ‰åŠ©äºæ¨åŠ¨æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œé€šè¿‡å°†ä¸ç›®æ ‡æ¦‚å¿µç›¸å…³è”çš„æ¨¡å‹è¾“å‡ºåˆ†é…ç»™ç‰¹å®šçš„æ¨¡å‹ç»„ä»¶ã€‚ç›®å‰çš„å½’å› ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¤šå±‚æ„ŸçŸ¥å™¨ç¥ç»å…ƒä¸Šï¼Œå¹¶å¤„ç†ç›¸å¯¹ç®€å•çš„æ¦‚å¿µï¼Œå¦‚äº‹å®å…³è”ï¼ˆä¾‹å¦‚ï¼Œå·´é»ä½äºæ³•å›½ï¼‰ã€‚è¿™ç§å…³æ³¨å¾€å¾€å¿½è§†äº†æ³¨æ„åŠ›æœºåˆ¶çš„å½±å“ï¼Œç¼ºä¹åˆ†ææ›´å¤æ‚æ¦‚å¿µçš„ç»Ÿä¸€æ–¹æ³•ã€‚ä¸ºäº†å¡«è¡¥è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯æ‰©å±•çš„æ³¨æ„åŠ›æ¨¡å—å‘ç°ï¼ˆSAMDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ¦‚å¿µæ— å…³çš„æ–¹æ³•ï¼Œå¯å°†ä»»æ„å¤æ‚æ¦‚å¿µæ˜ å°„åˆ°é€šç”¨Transformeræ¨¡å‹ä¸­çš„ç‰¹å®šæ³¨æ„åŠ›å¤´ã€‚æˆ‘ä»¬é€šè¿‡å°†æ¯ä¸ªæ¦‚å¿µè¡¨ç¤ºä¸ºå‘é‡ã€è®¡ç®—å…¶ä¸æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ä½™å¼¦ç›¸ä¼¼åº¦å¹¶é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å¤´éƒ¨æ¥æ„å»ºä¸æ¦‚å¿µç›¸å…³çš„æ³¨æ„åŠ›æ¨¡å—æ¥å®ç°è¿™ä¸€ç‚¹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†ç®€å•çš„Scalar Attention Module Interventionï¼ˆSAMIï¼‰ç­–ç•¥ï¼Œé€šè¿‡ä»…ä½¿ç”¨ä¸€ä¸ªæ ‡é‡å‚æ•°æ¥è°ƒæ•´æ³¨æ„åŠ›æ¨¡å—æ¥å‡å¼±æˆ–å¢å¼ºæ¦‚å¿µçš„å½±å“ã€‚æˆ‘ä»¬é€šè¿‡å®è¯åœ¨å…·æœ‰ä¸åŒå¤æ‚æ€§çš„æ¦‚å¿µä¸Šå±•ç¤ºäº†SAMDï¼Œå¹¶å¯è§†åŒ–äº†å…¶å¯¹åº”æ¨¡å—çš„åˆ†å¸ƒä½ç½®ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å—ä½ç½®åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒå‰åçš„ä½ç½®ä¿æŒä¸å˜ï¼Œå¹¶è¯å®äº†å…³äºå¤§å‹è¯­è¨€æ¨¡å‹å¤šè¯­è¨€èƒ½åŠ›çš„æœºåˆ¶çš„å‰æœŸç ”ç©¶ã€‚é€šè¿‡SAMIï¼Œæˆ‘ä»¬åœ¨HarmBenchä¸Šé€šè¿‡å‡å¼±â€œå®‰å…¨â€æ¦‚å¿µå®ç°äº†çªç ´ï¼ˆ+72.7%ï¼‰ï¼Œåœ¨GSM8KåŸºå‡†æµ‹è¯•ä¸­é€šè¿‡å¢å¼ºâ€œæ¨ç†â€èƒ½åŠ›æé«˜äº†æ€§èƒ½ï¼ˆ+1.6%ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡æŠ‘åˆ¶è§†è§‰Transformeråœ¨ImageNetä¸Šçš„å›¾åƒåˆ†ç±»å‡†ç¡®ç‡æ¥å¼ºè°ƒæˆ‘ä»¬æ–¹æ³•çš„é¢†åŸŸæ— å…³æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17052v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>Transformeræ¨¡å‹åœ¨è¯­è¨€å’Œè§†è§‰ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¨åŠ¨äº†å¯¹å…¶å†…éƒ¨æœºåˆ¶çš„è§£é‡Šå…·æœ‰å¢å¼ºæ€§èƒ½å’Œæ”¹å–„è¡Œä¸ºæ§åˆ¶çš„ç›®æ ‡ã€‚å½’å› æ–¹æ³•æœ‰åŠ©äºé€šè¿‡å°†ä¸ç›®æ ‡æ¦‚å¿µç›¸å…³çš„æ¨¡å‹è¾“å‡ºåˆ†é…ç»™ç‰¹å®šçš„æ¨¡å‹ç»„ä»¶æ¥æé«˜è§£é‡Šæ€§ã€‚å½“å‰å½’å› ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¤šå±‚æ„ŸçŸ¥å™¨ç¥ç»å…ƒä¸Šï¼Œå¹¶å¤„ç†ç›¸å¯¹ç®€å•çš„æ¦‚å¿µï¼Œå¦‚äº‹å®å…³è”ï¼ˆä¾‹å¦‚ï¼Œå·´é»ä½äºæ³•å›½ï¼‰ã€‚è¿™ç§å…³æ³¨å¾€å¾€å¿½ç•¥äº†æ³¨æ„åŠ›æœºåˆ¶çš„å½±å“ï¼Œç¼ºä¹å¯¹æ›´å¤æ‚çš„åˆ†æç»Ÿä¸€çš„æ–¹æ³•ã€‚ä¸ºäº†å¡«è¡¥è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯æ‰©å±•çš„æ³¨æ„åŠ›æ¨¡å—å‘ç°ï¼ˆSAMDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„æ–¹æ³•ï¼Œå¯å°†ä»»æ„å¤æ‚æ¦‚å¿µæ˜ å°„åˆ°é€šç”¨Transformeræ¨¡å‹çš„ç‰¹å®šæ³¨æ„åŠ›å¤´ã€‚æˆ‘ä»¬é€šè¿‡å°†æ¯ä¸ªæ¦‚å¿µè¡¨ç¤ºä¸ºå‘é‡ã€è®¡ç®—å…¶ä¸æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ä½™å¼¦ç›¸ä¼¼åº¦å¹¶é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å¤´éƒ¨æ¥æ„å»ºæ¦‚å¿µç›¸å…³çš„æ³¨æ„åŠ›æ¨¡å—æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ç®€å•çš„Scalar Attention Module Interventionï¼ˆSAMIï¼‰ç­–ç•¥ï¼Œé€šè¿‡è°ƒæ•´æ³¨æ„åŠ›æ¨¡å—ä½¿ç”¨å•ä¸ªæ ‡é‡å‚æ•°æ¥å‡å°‘æˆ–æ”¾å¤§æ¦‚å¿µçš„å½±å“ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒæ¼”ç¤ºäº†SAMDåœ¨å…·æœ‰ä¸åŒå¤æ‚æ€§çš„æ¦‚å¿µä¸Šçš„ä½œç”¨ï¼Œå¹¶å¯è§†åŒ–å…¶å¯¹åº”æ¨¡å—çš„åœ°ç†ä½ç½®ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å—ä½ç½®åœ¨LLMè®­ç»ƒåä¿æŒç¨³å®šï¼Œå¹¶è¯å®äº†å…³äºLLMå¤šè¯­è¨€æœºæ¢°åŸç†çš„å…ˆå‰å·¥ä½œã€‚é€šè¿‡SAMIï¼Œæˆ‘ä»¬åœ¨HarmBenchï¼ˆ+72.7%ï¼‰ä¸Šé€šè¿‡å‡å°‘â€œå®‰å…¨â€å› ç´ å®ç°çªç ´ï¼Œå¹¶åœ¨GSM8KåŸºå‡†æµ‹è¯•ï¼ˆ+1.6%ï¼‰ä¸Šé€šè¿‡åŠ å¼ºâ€œæ¨ç†â€èƒ½åŠ›æé«˜æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡æŠ‘åˆ¶å›¾åƒåˆ†ç±»å™¨çš„å‡†ç¡®æ€§æ¥çªå‡ºæˆ‘ä»¬æ–¹æ³•çš„é¢†åŸŸæ— å…³æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Transformeræ¨¡å‹åœ¨è¯­è¨€å’Œè§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¼•å‘äº†å¯¹è§£é‡Šå…¶å†…éƒ¨æœºåˆ¶çš„éœ€æ±‚ï¼Œæ—¨åœ¨æé«˜æ€§èƒ½å’Œæ”¹å–„è¡Œä¸ºæ§åˆ¶ã€‚</li>
<li>å½“å‰å½’å› ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç®€å•æ¦‚å¿µä¸Šï¼Œå¿½ç•¥äº†æ³¨æ„åŠ›æœºåˆ¶çš„å½±å“å’Œæ›´å¤æ‚çš„åˆ†ææ–¹æ³•ã€‚</li>
<li>å¼•å…¥SAMDæ–¹æ³•ï¼šé€šè¿‡æ˜ å°„ä»»æ„å¤æ‚æ¦‚å¿µåˆ°ç‰¹å®šçš„æ³¨æ„åŠ›å¤´ï¼Œæé«˜äº†Transformeræ¨¡å‹çš„è§£é‡Šæ€§ã€‚</li>
<li>SAMDæ–¹æ³•èƒ½å¤Ÿå®ç°æ¦‚å¿µç›¸å…³çš„æ³¨æ„åŠ›æ¨¡å—å¯è§†åŒ–ï¼Œæ¨¡å—ä½ç½®åœ¨LLMè®­ç»ƒåä¿æŒç¨³å®šã€‚</li>
<li>é€šè¿‡SAMIç­–ç•¥ï¼Œå¯ä»¥è°ƒæ•´æ³¨æ„åŠ›æ¨¡å—æ¥å¢å¼ºæˆ–å‰Šå¼±ç‰¹å®šæ¦‚å¿µçš„å½±å“ï¼Œä»è€Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSAMDå’ŒSAMIç­–ç•¥åœ¨å¤šç§ä»»åŠ¡ä¸Šæœ‰æ•ˆï¼ŒåŒ…æ‹¬è¯­è¨€ç†è§£å’Œå›¾åƒåˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c788fdbf45f0483242ad62bb5152435a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f81f833962d05589bf63c11e7db5864.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b7fda66ea5682a5edde8e0d82001084.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MUCAR-Benchmarking-Multilingual-Cross-Modal-Ambiguity-Resolution-for-Multimodal-Large-Language-Models"><a href="#MUCAR-Benchmarking-Multilingual-Cross-Modal-Ambiguity-Resolution-for-Multimodal-Large-Language-Models" class="headerlink" title="MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for   Multimodal Large Language Models"></a>MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for   Multimodal Large Language Models</h2><p><strong>Authors:Xiaolong Wang, Zhaolu Kang, Wangyuxuan Zhai, Xinyue Lou, Yunghwei Lai, Ziyue Wang, Yawen Wang, Kaiyu Huang, Yile Wang, Peng Li, Yang Liu</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated significant advances across numerous vision-language tasks. Due to their strong image-text alignment capability, MLLMs can effectively understand image-text pairs with clear meanings. However, effectively resolving the inherent ambiguities in natural language and visual contexts remains challenging. Existing multimodal benchmarks typically overlook linguistic and visual ambiguities, relying mainly on unimodal context for disambiguation and thus failing to exploit the mutual clarification potential between modalities. To bridge this gap, we introduce MUCAR, a novel and challenging benchmark designed explicitly for evaluating multimodal ambiguity resolution across multilingual and cross-modal scenarios. MUCAR includes: (1) a multilingual dataset where ambiguous textual expressions are uniquely resolved by corresponding visual contexts, and (2) a dual-ambiguity dataset that systematically pairs ambiguous images with ambiguous textual contexts, with each combination carefully constructed to yield a single, clear interpretation through mutual disambiguation. Extensive evaluations involving 19 state-of-the-art multimodal modelsâ€“encompassing both open-source and proprietary architecturesâ€“reveal substantial gaps compared to human-level performance, highlighting the need for future research into more sophisticated cross-modal ambiguity comprehension methods, further pushing the boundaries of multimodal reasoning. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¼—å¤šçš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç”±äºå®ƒä»¬å…·æœ‰å¼ºå¤§çš„å›¾åƒæ–‡æœ¬å¯¹é½èƒ½åŠ›ï¼ŒMLLMså¯ä»¥æœ‰æ•ˆåœ°ç†è§£æ„ä¹‰æ˜ç¡®çš„å›¾åƒæ–‡æœ¬å¯¹ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆè§£å†³è‡ªç„¶è¯­è¨€å’Œè§†è§‰ä¸Šä¸‹æ–‡ä¸­çš„å›ºæœ‰æ­§ä¹‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•é€šå¸¸å¿½ç•¥äº†è¯­è¨€å’Œè§†è§‰çš„æ­§ä¹‰ï¼Œä¸»è¦ä¾èµ–äºå•æ¨¡æ€ä¸Šä¸‹æ–‡è¿›è¡Œæ­§ä¹‰è§£æï¼Œä»è€Œæ²¡æœ‰å……åˆ†åˆ©ç”¨æ¨¡æ€ä¹‹é—´çš„ç›¸äº’æ¾„æ¸…æ½œåŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†MUCARï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè¯„ä¼°å¤šè¯­è¨€å’Œå¤šæ¨¡æ€åœºæ™¯ä¸­çš„å¤šæ¨¡æ€æ­§ä¹‰è§£æè€Œè®¾è®¡çš„æ–°å‹ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ã€‚MUCARåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰ä¸€ç§å¤šè¯­è¨€æ•°æ®é›†ï¼Œå…¶ä¸­æ¨¡ç³Šçš„æ–‡æœ¬è¡¨è¾¾é€šè¿‡ç›¸åº”çš„è§†è§‰ä¸Šä¸‹æ–‡å”¯ä¸€è§£å†³ï¼›ï¼ˆ2ï¼‰ä¸€ç§åŒé‡æ­§ä¹‰æ•°æ®é›†ï¼Œç³»ç»Ÿåœ°é…å¯¹å…·æœ‰æ¨¡ç³Šæ€§çš„å›¾åƒå’Œæ–‡æœ¬ä¸Šä¸‹æ–‡ï¼Œæ¯ç§ç»„åˆéƒ½ç»è¿‡ç²¾å¿ƒæ„å»ºï¼Œä»¥é€šè¿‡ç›¸äº’è§£æå¾—å‡ºä¸€ä¸ªæ¸…æ™°ã€æ˜ç¡®çš„è§£é‡Šã€‚æ¶‰åŠ19ä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°â€”â€”åŒ…æ‹¬å¼€æºå’Œä¸“æœ‰æ¶æ„â€”â€”ä¸äººç±»æ°´å¹³æ€§èƒ½ç›¸æ¯”å­˜åœ¨å·¨å¤§å·®è·ï¼Œè¿™çªæ˜¾äº†æœªæ¥ç ”ç©¶æ›´å¤æ‚è·¨æ¨¡æ€æ­§ä¹‰ç†è§£æ–¹æ³•çš„å¿…è¦æ€§ï¼Œè¿›ä¸€æ­¥æ¨åŠ¨å¤šæ¨¡æ€æ¨ç†çš„è¾¹ç•Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17046v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MLLMsåœ¨è·¨è§†è§‰è¯­è¨€ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå…·å¤‡å¼ºå¤§çš„å›¾æ–‡å¯¹é½èƒ½åŠ›ï¼Œèƒ½æ¸…æ™°ç†è§£å›¾åƒå’Œæ–‡å­—çš„æ„ä¹‰ã€‚ç„¶è€Œï¼Œè§£å†³è‡ªç„¶è¯­è¨€å’Œè§†è§‰è¯­å¢ƒä¸­çš„å›ºæœ‰æ­§ä¹‰ä»æ˜¯æŒ‘æˆ˜ã€‚ç°æœ‰çš„å¤šæ¨¡å¼åŸºå‡†æµ‹è¯•å¿½ç•¥äº†è¯­è¨€å’Œè§†è§‰çš„æ­§ä¹‰æ€§ï¼Œä»…ä¾èµ–å•æ¨¡æ€ä¸Šä¸‹æ–‡è¿›è¡Œè§£æ­§ä¹‰ï¼Œæ— æ³•åˆ©ç”¨æ¨¡æ€ä¹‹é—´çš„äº’é‡Šæ½œåŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MUCARï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šè¯­è¨€åœºæ™¯ä¸‹å¤šæ¨¡å¼æ­§ä¹‰è§£å†³èƒ½åŠ›çš„æ–°é¢–åŸºå‡†æµ‹è¯•ã€‚MUCARåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰ä¸€ç§å¤šè¯­è¨€æ•°æ®é›†ï¼Œå…¶ä¸­æ¨¡ç³Šæ–‡æœ¬è¡¨è¾¾é€šè¿‡ç›¸åº”çš„è§†è§‰ä¸Šä¸‹æ–‡è¿›è¡Œå”¯ä¸€è§£æï¼›ï¼ˆ2ï¼‰ä¸€ç§åŒæ¨¡ç³Šæ•°æ®é›†ï¼Œç³»ç»Ÿåœ°é…å¯¹å…·æœ‰æ¨¡ç³Šæ€§çš„å›¾åƒå’Œæ–‡æœ¬ä¸Šä¸‹æ–‡ï¼Œæ¯ä¸ªç»„åˆéƒ½ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥é€šè¿‡ç›¸äº’è§£æäº§ç”Ÿå•ä¸€æ¸…æ™°çš„è§£é‡Šã€‚å¯¹å¤šä¸ªå…ˆè¿›çš„å¤šæ¨¡å¼æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œä¸äººç±»æ€§èƒ½ç›¸æ¯”ä»å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå‡¸æ˜¾äº†å¯¹æœªæ¥ç ”ç©¶æ›´å¤æ‚è·¨æ¨¡æ€æ­§ä¹‰ç†è§£æ–¹æ³•çš„éœ€è¦ï¼Œè¿›ä¸€æ­¥æ¨åŠ¨å¤šæ¨¡å¼æ¨ç†çš„è¾¹ç•Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MLLMsåœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œæ“…é•¿ç†è§£å›¾åƒå’Œæ–‡å­—çš„æ„ä¹‰ã€‚</li>
<li>è§£å†³è‡ªç„¶è¯­è¨€å’Œè§†è§‰è¯­å¢ƒä¸­çš„æ­§ä¹‰æ€§æ˜¯MLLMsé¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å¿½ç•¥äº†è¯­è¨€å’Œè§†è§‰çš„æ­§ä¹‰æ€§ï¼Œéœ€è¦æ–°çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å¤šæ¨¡å¼æ­§ä¹‰è§£å†³èƒ½åŠ›ã€‚</li>
<li>MUCARæ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šè¯­è¨€åœºæ™¯ä¸‹å¤šæ¨¡å¼æ­§ä¹‰è§£å†³èƒ½åŠ›çš„æ–°é¢–åŸºå‡†æµ‹è¯•ã€‚</li>
<li>MUCARåŒ…æ‹¬å¤šè¯­è¨€å’ŒåŒæ¨¡ç³Šæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚</li>
<li>å¯¹å¤šä¸ªå…ˆè¿›æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸äººç±»æ€§èƒ½ç›¸æ¯”ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17046">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-662f41d63b55b1210ec34e6019fe6e50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-694b2fb072f671dba320b6b3aabb1d59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0607ca5f99430ea0bab9842ae2d157ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f41828714d3425787b788b12943ecd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6db38988cf07a96fb498a5d97bebb2ac.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Latent-Concept-Disentanglement-in-Transformer-based-Language-Models"><a href="#Latent-Concept-Disentanglement-in-Transformer-based-Language-Models" class="headerlink" title="Latent Concept Disentanglement in Transformer-based Language Models"></a>Latent Concept Disentanglement in Transformer-based Language Models</h2><p><strong>Authors:Guan Zhe Hong, Bhavya Vasudeva, Vatsal Sharan, Cyrus Rashtchian, Prabhakar Raghavan, Rina Panigrahy</strong></p>
<p>When large language models (LLMs) use in-context learning (ICL) to solve a new task, they seem to grasp not only the goal of the task but also core, latent concepts in the demonstration examples. This begs the question of whether transformers represent latent structures as part of their computation or whether they take shortcuts to solve the problem. Prior mechanistic work on ICL does not address this question because it does not sufficiently examine the relationship between the learned representation and the latent concept, and the considered problem settings often involve only single-step reasoning. In this work, we examine how transformers disentangle and use latent concepts. We show that in 2-hop reasoning tasks with a latent, discrete concept, the model successfully identifies the latent concept and does step-by-step concept composition. In tasks parameterized by a continuous latent concept, we find low-dimensional subspaces in the representation space where the geometry mimics the underlying parameterization. Together, these results refine our understanding of ICL and the representation of transformers, and they provide evidence for highly localized structures in the model that disentangle latent concepts in ICL tasks. </p>
<blockquote>
<p>å½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¥è§£å†³æ–°ä»»åŠ¡æ—¶ï¼Œå®ƒä»¬ä¼¼ä¹ä¸ä»…æŒæ¡äº†ä»»åŠ¡çš„ç›®æ ‡ï¼Œè¿˜æŒæ¡äº†æ¼”ç¤ºç¤ºä¾‹ä¸­çš„æ ¸å¿ƒã€æ½œåœ¨æ¦‚å¿µã€‚è¿™å¼•å‘äº†ä¸€ä¸ªé—®é¢˜ï¼Œå³å˜å‹å™¨æ˜¯å°†å…¶ä»£è¡¨æ½œåœ¨ç»“æ„ä½œä¸ºè®¡ç®—çš„ä¸€éƒ¨åˆ†ï¼Œè¿˜æ˜¯é‡‡å–äº†è§£å†³é—®é¢˜çš„æ·å¾„ã€‚ä¹‹å‰å…³äºICLçš„æœºåˆ¶æ€§å·¥ä½œå¹¶æ²¡æœ‰è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå› ä¸ºå®ƒæ²¡æœ‰å……åˆ†æ£€æŸ¥å­¦åˆ°çš„è¡¨ç°å’Œæ½œåœ¨æ¦‚å¿µä¹‹é—´çš„å…³ç³»ï¼Œè€Œä¸”æ‰€è€ƒè™‘çš„é—®é¢˜è®¾ç½®é€šå¸¸åªæ¶‰åŠå•æ­¥æ¨ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å˜å‹å™¨å¦‚ä½•è§£å¼€å’Œåˆ©ç”¨æ½œåœ¨æ¦‚å¿µã€‚æˆ‘ä»¬å±•ç¤ºäº†åœ¨å…·æœ‰æ½œåœ¨ç¦»æ•£æ¦‚å¿µçš„2è·³æ¨ç†ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹æˆåŠŸåœ°è¯†åˆ«äº†æ½œåœ¨æ¦‚å¿µï¼Œå¹¶è¿›è¡Œäº†é€æ­¥çš„æ¦‚å¿µç»„åˆã€‚åœ¨ç”±è¿ç»­æ½œåœ¨æ¦‚å¿µå‚æ•°åŒ–çš„ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬åœ¨è¡¨å¾ç©ºé—´ä¸­æ‰¾åˆ°ä½ç»´å­ç©ºé—´ï¼Œå…¶å‡ ä½•å½¢çŠ¶æ¨¡ä»¿äº†æ½œåœ¨å‚æ•°åŒ–ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç»“æœæ”¹è¿›äº†æˆ‘ä»¬å¯¹ICLå’Œå˜å‹å™¨è¡¨ç¤ºçš„ç†è§£ï¼Œå¹¶ä¸ºæ¨¡å‹ä¸­é«˜åº¦å±€éƒ¨åŒ–çš„ç»“æ„æä¾›äº†è¯æ®ï¼Œè¿™äº›ç»“æ„åœ¨ICLä»»åŠ¡ä¸­è§£å¼€æ½œåœ¨æ¦‚å¿µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16975v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰è§£å†³æ–°ä»»åŠ¡æ—¶ï¼Œä¸ä»…èƒ½ç†è§£ä»»åŠ¡ç›®æ ‡ï¼Œè¿˜èƒ½æŠŠæ¡æ¼”ç¤ºä¾‹å­ä¸­çš„æ ¸å¿ƒæ½œåœ¨æ¦‚å¿µã€‚æœ¬æ–‡æ¢è®¨çš„æ˜¯ï¼Œå˜å‹å™¨æ¨¡å‹æ˜¯å¦å°†æ½œåœ¨ç»“æ„ä½œä¸ºè®¡ç®—çš„ä¸€éƒ¨åˆ†è¿›è¡Œè¡¨ç¤ºï¼Œè¿˜æ˜¯é‡‡å–æ·å¾„æ¥è§£å†³é—®é¢˜ã€‚è¿‡å»å¯¹ICLçš„æœºåˆ¶æ€§å·¥ä½œå¹¶æ²¡æœ‰å……åˆ†æ¢è®¨è¿™ä¸ªé—®é¢˜ï¼Œå› ä¸ºå®ƒæ²¡æœ‰æ·±å…¥ç ”ç©¶å­¦åˆ°çš„è¡¨ç¤ºå’Œæ½œåœ¨æ¦‚å¿µä¹‹é—´çš„å…³ç³»ï¼Œä¸”è€ƒè™‘çš„é—®é¢˜è®¾ç½®é€šå¸¸åªæ¶‰åŠå•æ­¥æ¨ç†ã€‚æœ¬æ–‡ç ”ç©¶å˜å‹å™¨å¦‚ä½•è§£å¼€å’Œåˆ©ç”¨æ½œåœ¨æ¦‚å¿µã€‚åœ¨å…·æœ‰æ½œåœ¨ç¦»æ•£æ¦‚å¿µçš„2è·³æ¨ç†ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹èƒ½å¤ŸæˆåŠŸè¯†åˆ«æ½œåœ¨æ¦‚å¿µå¹¶è¿›è¡Œé€æ­¥æ¦‚å¿µç»„åˆã€‚åœ¨ç”±è¿ç»­æ½œåœ¨æ¦‚å¿µå‚æ•°åŒ–çš„ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å‘ç°è¡¨ç¤ºç©ºé—´ä¸­çš„ä½ç»´å­ç©ºé—´ï¼Œå…¶å‡ ä½•ç»“æ„æ¨¡æ‹Ÿäº†æ½œåœ¨çš„å‚æ•°åŒ–ã€‚è¿™äº›ç»“æœæ·±åŒ–äº†æˆ‘ä»¬å¯¹ICLå’Œå˜å‹å™¨è¡¨ç¤ºçš„ç†è§£ï¼Œå¹¶ä¸ºæ¨¡å‹ä¸­å­˜åœ¨é«˜åº¦å±€éƒ¨åŒ–çš„ç»“æ„æä¾›äº†è¯æ®ï¼Œè¿™äº›ç»“æ„åœ¨ICLä»»åŠ¡ä¸­è§£å¼€æ½œåœ¨æ¦‚å¿µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸ä»…èƒ½ç†è§£ä»»åŠ¡ç›®æ ‡ï¼Œè¿˜èƒ½æŠŠæ¡æ¼”ç¤ºä¾‹å­ä¸­çš„æ ¸å¿ƒæ½œåœ¨æ¦‚å¿µã€‚</li>
<li>å˜å‹å™¨æ¨¡å‹åœ¨è§£å†³æ–°ä»»åŠ¡æ—¶å¯èƒ½è§£å¼€å¹¶åˆ©ç”¨æ½œåœ¨ç»“æ„ã€‚</li>
<li>ç°æœ‰å…³äºICLçš„æœºåˆ¶æ€§å·¥ä½œæœªèƒ½å……åˆ†æ¢è®¨å­¦åˆ°çš„è¡¨ç¤ºå’Œæ½œåœ¨æ¦‚å¿µä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>åœ¨2è·³æ¨ç†ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹èƒ½å¤Ÿè¯†åˆ«æ½œåœ¨ç¦»æ•£æ¦‚å¿µå¹¶è¿›è¡Œé€æ­¥ç»„åˆã€‚</li>
<li>åœ¨è¿ç»­æ½œåœ¨æ¦‚å¿µå‚æ•°åŒ–çš„ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„è¡¨ç¤ºç©ºé—´å­˜åœ¨ä½ç»´å­ç©ºé—´ï¼Œå…¶ç»“æ„ä¸æ½œåœ¨å‚æ•°ç›¸ç¬¦ã€‚</li>
<li>è¿™äº›å‘ç°æ·±åŒ–äº†æˆ‘ä»¬å¯¹ICLå’Œå˜å‹å™¨è¡¨ç¤ºçš„ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4275827855df24c57003a4c1a3ea409.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-787ec72d04b40f160ff22017daee6fbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-358165b10b1a1968a8a73c2886f55686.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Optimizing-MoE-Routers-Design-Implementation-and-Evaluation-in-Transformer-Models"><a href="#Optimizing-MoE-Routers-Design-Implementation-and-Evaluation-in-Transformer-Models" class="headerlink" title="Optimizing MoE Routers: Design, Implementation, and Evaluation in   Transformer Models"></a>Optimizing MoE Routers: Design, Implementation, and Evaluation in   Transformer Models</h2><p><strong>Authors:Daniel Fidel Harvey, George Weale, Berk Yilmaz</strong></p>
<p>Mixture of Experts (MoE) architectures increase large language model scalability, yet their performance depends on the router module that moves tokens to specialized experts. Bad routing can load imbalance and reduced accuracy. This project designed and implemented different router architectures within Transformer models to fix these limitations. We experimented with six distinct router variants Linear, Attention, Multi-Layer Perceptron (MLP), Hybrid, Hash, and our new MLP-Hadamard. We characterized these routers using BERT and the Qwen1.5-MoE model, looking at parameter efficiency, inference latency, routing entropy, and expert utilization patterns. Our evaluations showed distinct trade-offs: Linear routers offer speed, while MLP and Attention routers provide greater expressiveness. The MLP-Hadamard router shows a unique capability for structured, sparse routing. We successfully replaced and fine-tuned custom routers within the complex, quantized Qwen1.5-MoE model. This work provides a comparative analysis of MoE router designs and offers insights into optimizing their performance for efficient and effective large-scale model deployment. </p>
<blockquote>
<p>ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„æé«˜äº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å¯æ‰©å±•æ€§ï¼Œä½†å…¶æ€§èƒ½å–å†³äºå°†ä»¤ç‰Œç§»åŠ¨åˆ°ä¸“ä¸šä¸“å®¶çš„è·¯ç”±å™¨æ¨¡å—ã€‚ä¸è‰¯çš„è·¯ç”±ä¼šå¯¼è‡´è´Ÿè½½ä¸å‡è¡¡å’Œç²¾åº¦ä¸‹é™ã€‚æ­¤é¡¹ç›®åœ¨Transformeræ¨¡å‹ä¸­è®¾è®¡å’Œå®ç°äº†ä¸åŒçš„è·¯ç”±å™¨æ¶æ„ï¼Œä»¥è§£å†³è¿™äº›é™åˆ¶ã€‚æˆ‘ä»¬è¯•éªŒäº†å…­ç§ä¸åŒçš„è·¯ç”±å™¨å˜ä½“ï¼šLinearã€Attentionã€å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ã€Hybridã€Hashä»¥åŠæˆ‘ä»¬æ–°çš„MLP-Hadamardã€‚æˆ‘ä»¬ä½¿ç”¨BERTå’ŒQwen1.5-MoEæ¨¡å‹å¯¹è¿™äº›è·¯ç”±å™¨è¿›è¡Œäº†è¡¨å¾ï¼Œå…³æ³¨å‚æ•°æ•ˆç‡ã€æ¨ç†å»¶è¿Ÿã€è·¯ç”±ç†µå’Œä¸“å®¶åˆ©ç”¨æ¨¡å¼ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºäº†æ˜æ˜¾çš„æƒè¡¡ï¼šLinearè·¯ç”±å™¨æä¾›é€Ÿåº¦ï¼Œè€ŒMLPå’ŒAttentionè·¯ç”±å™¨æä¾›æ›´å¤§çš„è¡¨ç°åŠ›ã€‚MLP-Hadamardè·¯ç”±å™¨æ˜¾ç¤ºå‡ºç»“æ„åŒ–å’Œç¨€ç–è·¯ç”±çš„ç‹¬ç‰¹èƒ½åŠ›ã€‚æˆ‘ä»¬æˆåŠŸåœ°åœ¨å¤æ‚çš„é‡åŒ–Qwen1.5-MoEæ¨¡å‹ä¸­æ›¿æ¢å’Œå¾®è°ƒäº†è‡ªå®šä¹‰è·¯ç”±å™¨ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†å¯¹MoEè·¯ç”±å™¨è®¾è®¡çš„æ¯”è¾ƒåˆ†æï¼Œå¹¶æ·±å…¥äº†è§£äº†å¦‚ä½•ä¼˜åŒ–å…¶æ€§èƒ½ï¼Œä»¥å®ç°é«˜æ•ˆå’Œæœ‰æ•ˆçš„å¤§è§„æ¨¡æ¨¡å‹éƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16419v1">PDF</a> All authors contributed equally. 11 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†Mixture of Expertsï¼ˆMoEï¼‰æ¶æ„ä¸­çš„è·¯ç”±å™¨æ¨¡å—è®¾è®¡ï¼Œé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ‰©å±•æ€§è¿›è¡Œäº†ä¼˜åŒ–ã€‚æ–‡ç« ä»‹ç»äº†å…­ç§ä¸åŒçš„è·¯ç”±å™¨æ¶æ„ï¼ŒåŒ…æ‹¬Linearã€Attentionã€Multi-Layer Perceptronï¼ˆMLPï¼‰ã€Hybridã€Hashä»¥åŠæ–°çš„MLP-Hadamardè·¯ç”±å™¨ã€‚é€šè¿‡å®éªŒè¯„ä¼°ï¼Œæ–‡ç« åˆ†æäº†è¿™äº›è·¯ç”±å™¨çš„å‚æ•°æ•ˆç‡ã€æ¨ç†å»¶è¿Ÿã€è·¯ç”±ç†µå’Œä¸“å®¶åˆ©ç”¨æ¨¡å¼ï¼Œå¹¶æ¢è®¨äº†å„è‡ªçš„ç‰¹ç‚¹å’Œæƒè¡¡ã€‚ç ”ç©¶æˆåŠŸåœ°åœ¨å¤æ‚çš„é‡åŒ–Qwen1.5-MoEæ¨¡å‹ä¸­æ›¿æ¢å’Œå¾®è°ƒäº†è‡ªå®šä¹‰è·¯ç”±å™¨ï¼Œä¸ºMoEè·¯ç”±å™¨è®¾è®¡æä¾›äº†æ¯”è¾ƒåˆ†æï¼Œå¹¶ä¸ºå¤§å‹æ¨¡å‹çš„æ€§èƒ½ä¼˜åŒ–å’Œæœ‰æ•ˆéƒ¨ç½²æä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MoEæ¶æ„æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ‰©å±•æ€§ï¼Œä½†æ€§èƒ½å–å†³äºè·¯ç”±å™¨æ¨¡å—çš„è®¾è®¡ã€‚</li>
<li>è·¯ç”±å™¨è´Ÿè´£å°†ä»¤ç‰Œåˆ†é…ç»™ä¸“å®¶ï¼Œä¸è‰¯çš„è·¯ç”±å¯èƒ½å¯¼è‡´è´Ÿè½½ä¸å‡è¡¡å’Œå‡†ç¡®æ€§ä¸‹é™ã€‚</li>
<li>ç ”ç©¶è®¾è®¡äº†å…­ç§ä¸åŒçš„è·¯ç”±å™¨æ¶æ„ï¼ŒåŒ…æ‹¬Linearã€Attentionã€MLPã€Hybridã€Hashå’ŒMLP-Hadamardè·¯ç”±å™¨ã€‚</li>
<li>é€šè¿‡å®éªŒè¯„ä¼°ï¼Œæ–‡ç« åˆ†æäº†è·¯ç”±å™¨çš„å‚æ•°æ•ˆç‡ã€æ¨ç†å»¶è¿Ÿç­‰ç‰¹æ€§ã€‚</li>
<li>Linearè·¯ç”±å™¨æä¾›é€Ÿåº¦ä¼˜åŠ¿ï¼Œè€ŒMLPå’ŒAttentionè·¯ç”±å™¨å…·æœ‰æ›´é«˜çš„è¡¨è¾¾èƒ½åŠ›ã€‚</li>
<li>MLP-Hadamardè·¯ç”±å™¨å±•ç°å‡ºç»“æ„åŒ–ã€ç¨€ç–è·¯ç”±çš„ç‹¬ç‰¹èƒ½åŠ›ã€‚</li>
<li>æ–‡ç« æˆåŠŸåœ°åœ¨å¤æ‚çš„é‡åŒ–Qwen1.5-MoEæ¨¡å‹ä¸­æ›¿æ¢å’Œå¾®è°ƒäº†è‡ªå®šä¹‰è·¯ç”±å™¨ï¼Œä¸ºMoEè·¯ç”±å™¨è®¾è®¡çš„ä¼˜åŒ–æä¾›äº†é‡è¦å‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-407aa4a16c1d8f7f0db9940b64a83af8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de5f613de5e6bc033f58dfee858c1eda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-774ec15a2c8258a1c04f8cd98f75be43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17835ef7ca486344ecb3f665acca5e57.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CapsDT-Diffusion-Transformer-for-Capsule-Robot-Manipulation"><a href="#CapsDT-Diffusion-Transformer-for-Capsule-Robot-Manipulation" class="headerlink" title="CapsDT: Diffusion-Transformer for Capsule Robot Manipulation"></a>CapsDT: Diffusion-Transformer for Capsule Robot Manipulation</h2><p><strong>Authors:Xiting He, Mingwu Su, Xinqi Jiang, Long Bai, Jiewen Lai, Hongliang Ren</strong></p>
<p>Vision-Language-Action (VLA) models have emerged as a prominent research area, showcasing significant potential across a variety of applications. However, their performance in endoscopy robotics, particularly endoscopy capsule robots that perform actions within the digestive system, remains unexplored. The integration of VLA models into endoscopy robots allows more intuitive and efficient interactions between human operators and medical devices, improving both diagnostic accuracy and treatment outcomes. In this work, we design CapsDT, a Diffusion Transformer model for capsule robot manipulation in the stomach. By processing interleaved visual inputs, and textual instructions, CapsDT can infer corresponding robotic control signals to facilitate endoscopy tasks. In addition, we developed a capsule endoscopy robot system, a capsule robot controlled by a robotic arm-held magnet, addressing different levels of four endoscopy tasks and creating corresponding capsule robot datasets within the stomach simulator. Comprehensive evaluations on various robotic tasks indicate that CapsDT can serve as a robust vision-language generalist, achieving state-of-the-art performance in various levels of endoscopy tasks while achieving a 26.25% success rate in real-world simulation manipulation. </p>
<blockquote>
<p>è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹å·²ç»æˆä¸ºä¸€ä¸ªçªå‡ºçš„ç ”ç©¶é¢†åŸŸï¼Œåœ¨å„ç§åº”ç”¨ä¸­å±•ç¤ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å†…é•œæœºå™¨äººï¼Œå°¤å…¶æ˜¯åœ¨æ¶ˆåŒ–ç³»ç»Ÿå†…æ‰§è¡ŒåŠ¨ä½œçš„å¾®å‹å†…é•œæœºå™¨äººçš„æ€§èƒ½å°šæœªè¢«æ¢ç´¢ã€‚VLAæ¨¡å‹ä¸å†…é•œæœºå™¨äººçš„é›†æˆå…è®¸äººç±»æ“ä½œè€…ä¸åŒ»ç–—è®¾å¤‡ä¹‹é—´è¿›è¡Œæ›´ç›´è§‚å’Œé«˜æ•ˆçš„äº¤äº’ï¼Œæé«˜äº†è¯Šæ–­å‡†ç¡®æ€§å’Œæ²»ç–—æ•ˆæœã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†CapsDTï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºèƒƒå†…å¾®å‹æœºå™¨äººæ“ä½œçš„æ‰©æ•£å˜æ¢æ¨¡å‹ã€‚é€šè¿‡å¤„ç†äº¤æ›¿çš„è§†è§‰è¾“å…¥å’Œæ–‡æœ¬æŒ‡ä»¤ï¼ŒCapsDTå¯ä»¥æ¨æ–­å‡ºç›¸åº”çš„æœºå™¨äººæ§åˆ¶ä¿¡å·ï¼Œä»¥ä¿ƒè¿›å†…çª¥é•œä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¾®å‹å†…é•œæœºå™¨äººç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç”±æœºæ¢°è‡‚æŒæœ‰çš„ç£é“æ§åˆ¶çš„å¾®å‹æœºå™¨äººï¼Œè§£å†³äº†å››ä¸ªä¸åŒçº§åˆ«çš„å†…çª¥é•œä»»åŠ¡ï¼Œå¹¶åœ¨èƒƒæ¨¡æ‹Ÿå™¨å†…åˆ›å»ºäº†ç›¸åº”çš„å¾®å‹æœºå™¨äººæ•°æ®é›†ã€‚å¯¹å„ç§æœºå™¨äººä»»åŠ¡çš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒCapsDTå¯ä»¥ä½œä¸ºä¸€ä¸ªå¼ºå¤§çš„è§†è§‰è¯­è¨€ä¸“å®¶ï¼Œåœ¨ä¸åŒçº§åˆ«çš„å†…çª¥é•œä»»åŠ¡ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ç°å®ä¸–ç•Œçš„æ¨¡æ‹Ÿæ“ä½œä¸­å®ç°äº†26.25%çš„æˆåŠŸç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16263v1">PDF</a> IROS 2025</p>
<p><strong>æ‘˜è¦</strong><br>åœ¨æ¶ˆåŒ–é“å†…çª¥é•œèƒ¶å›Šæœºå™¨äººç­‰é¢†åŸŸï¼Œè·¨è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å…¶åœ¨å†…çª¥é•œæœºå™¨äººæ–¹é¢çš„åº”ç”¨å°šæœªè¢«æ¢ç´¢ã€‚æœ¬ç ”ç©¶è®¾è®¡äº†ä¸€ç§åä¸ºCapsDTçš„æ‰©æ•£å˜æ¢æ¨¡å‹ï¼Œç”¨äºèƒƒå†…èƒ¶å›Šæœºå™¨äººçš„æ“æ§ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†äº¤æ›¿çš„è§†è§‰è¾“å…¥å’Œæ–‡å­—æŒ‡ä»¤ï¼Œæ¨æ–­å‡ºç›¸åº”çš„æœºå™¨äººæ§åˆ¶ä¿¡å·ï¼Œä¿ƒè¿›å†…çª¥é•œä»»åŠ¡çš„å®Œæˆã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼€å‘äº†ä¸€å¥—èƒ¶å›Šå†…çª¥é•œæœºå™¨äººç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡æœºæ¢°è‡‚æ§åˆ¶çš„ç£é“æ§åˆ¶èƒ¶å›Šæœºå™¨äººï¼Œé’ˆå¯¹å››ç§ä¸åŒéš¾åº¦çš„å†…çª¥é•œä»»åŠ¡åˆ›å»ºäº†ç›¸åº”çš„æ•°æ®é›†ï¼Œå¹¶åœ¨èƒƒæ¨¡æ‹Ÿå™¨ä¸­è¿›è¡Œäº†æ¨¡æ‹Ÿæ“æ§ã€‚ç»è¿‡ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒCapsDTèƒ½å¤Ÿä½œä¸ºä¸€ç§å¼ºå¤§çš„è·¨è§†è§‰-è¯­è¨€é¢†åŸŸçš„ä¸“å®¶ç³»ç»Ÿï¼Œåœ¨å†…çª¥é•œä»»åŠ¡çš„ä¸åŒå±‚çº§å®ç°æœ€ä¼˜æ€§èƒ½ï¼Œå¹¶åœ¨æ¨¡æ‹Ÿæ“æ§ä¸­è¾¾åˆ°26.25%çš„æˆåŠŸç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VLAæ¨¡å‹åœ¨å†…çª¥é•œèƒ¶å›Šæœºå™¨äººé¢†åŸŸå…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†CapsDTæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†è§†è§‰å’Œæ–‡å­—æŒ‡ä»¤ä»¥æ§åˆ¶æœºå™¨äººã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€å¥—èƒ¶å›Šå†…çª¥é•œæœºå™¨äººç³»ç»ŸåŠå…¶ç›¸åº”çš„æ•°æ®é›†ã€‚</li>
<li>CapsDTæ¨¡å‹åœ¨å†…çª¥é•œä»»åŠ¡çš„ä¸åŒå±‚çº§å®ç°æœ€ä¼˜æ€§èƒ½ã€‚</li>
<li>CapsDTæ¨¡å‹åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„æ“æ§æˆåŠŸç‡ä¸º26.25%ã€‚</li>
<li>CapsDTæ¨¡å‹çš„å¼•å…¥æœ‰åŠ©äºå¢å¼ºå†…çª¥é•œä»»åŠ¡ä¸­äººæœºäº’åŠ¨çš„ç›´è§‚æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16263">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fc8f790fe5f530d8130a686cf3a44175.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95438ccbb92b9ee41e384aafe375883c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95c25216a8d6cd0fa50be22a95587cb5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-533903b8289e4c3e4112f62f1cdac2c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-496affe91f2f46f5881b94f7acfc01fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad106781062a02a00f76ed33b10fedc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c7a4f871944a22a438f7a98450ecd6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-adb68bdc2556cb7098c690a78d85eb50.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Polyline-Path-Masked-Attention-for-Vision-Transformer"><a href="#Polyline-Path-Masked-Attention-for-Vision-Transformer" class="headerlink" title="Polyline Path Masked Attention for Vision Transformer"></a>Polyline Path Masked Attention for Vision Transformer</h2><p><strong>Authors:Zhongchen Zhao, Chaodong Xiao, Hui Lin, Qi Xie, Lei Zhang, Deyu Meng</strong></p>
<p>Global dependency modeling and spatial position modeling are two core issues of the foundational architecture design in current deep learning frameworks. Recently, Vision Transformers (ViTs) have achieved remarkable success in computer vision, leveraging the powerful global dependency modeling capability of the self-attention mechanism. Furthermore, Mamba2 has demonstrated its significant potential in natural language processing tasks by explicitly modeling the spatial adjacency prior through the structured mask. In this paper, we propose Polyline Path Masked Attention (PPMA) that integrates the self-attention mechanism of ViTs with an enhanced structured mask of Mamba2, harnessing the complementary strengths of both architectures. Specifically, we first ameliorate the traditional structured mask of Mamba2 by introducing a 2D polyline path scanning strategy and derive its corresponding structured mask, polyline path mask, which better preserves the adjacency relationships among image tokens. Notably, we conduct a thorough theoretical analysis on the structural characteristics of the proposed polyline path mask and design an efficient algorithm for the computation of the polyline path mask. Next, we embed the polyline path mask into the self-attention mechanism of ViTs, enabling explicit modeling of spatial adjacency prior. Extensive experiments on standard benchmarks, including image classification, object detection, and segmentation, demonstrate that our model outperforms previous state-of-the-art approaches based on both state-space models and Transformers. For example, our proposed PPMA-T&#x2F;S&#x2F;B models achieve 48.7%&#x2F;51.1%&#x2F;52.3% mIoU on the ADE20K semantic segmentation task, surpassing RMT-T&#x2F;S&#x2F;B by 0.7%&#x2F;1.3%&#x2F;0.3%, respectively. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zhongchenzhao/PPMA">https://github.com/zhongchenzhao/PPMA</a>. </p>
<blockquote>
<p>å…¨å±€ä¾èµ–å»ºæ¨¡å’Œç©ºé—´ä½ç½®å»ºæ¨¡æ˜¯å½“å‰æ·±åº¦å­¦ä¹ æ¡†æ¶åŸºç¡€æ¶æ„è®¾è®¡ä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ã€‚æœ€è¿‘ï¼ŒVision Transformersï¼ˆViTsï¼‰åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œåˆ©ç”¨äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å¼ºå¤§å…¨å±€ä¾èµ–å»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒMamba2é€šè¿‡æ˜¾å¼å»ºæ¨¡ç©ºé—´é‚»æ¥å…ˆéªŒï¼Œé€šè¿‡ç»“æ„åŒ–æ©ç åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Polyline Path Masked Attentionï¼ˆPPMAï¼‰ï¼Œå®ƒç»“åˆäº†ViTsçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å’ŒMamba2çš„å¢å¼ºç»“æ„åŒ–æ©ç ï¼Œå……åˆ†åˆ©ç”¨äº†ä¸¤ç§æ¶æ„çš„äº’è¡¥ä¼˜åŠ¿ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡å¼•å…¥2DæŠ˜çº¿è·¯å¾„æ‰«æç­–ç•¥æ”¹è¿›äº†Mamba2çš„ä¼ ç»Ÿç»“æ„åŒ–æ©ç ï¼Œå¹¶å¾—å‡ºäº†ç›¸åº”çš„ç»“æ„åŒ–æ©ç ï¼Œå³æŠ˜çº¿è·¯å¾„æ©ç ï¼Œèƒ½æ›´å¥½åœ°ä¿ç•™å›¾åƒæ ‡è®°ä¹‹é—´çš„é‚»æ¥å…³ç³»ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å¯¹æ‰€æå‡ºçš„æŠ˜çº¿è·¯å¾„æ©ç çš„ç»“æ„ç‰¹æ€§è¿›è¡Œäº†æ·±å…¥çš„ç†è®ºåˆ†æï¼Œå¹¶è®¾è®¡äº†è®¡ç®—æŠ˜çº¿è·¯å¾„æ©ç çš„é«˜æ•ˆç®—æ³•ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æŠ˜çº¿è·¯å¾„æ©ç åµŒå…¥ViTsçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œå®ç°äº†ç©ºé—´é‚»æ¥å…ˆéªŒçš„æ˜¾å¼å»ºæ¨¡ã€‚åœ¨åŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²åœ¨å†…çš„æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºåŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹å’ŒTransformersçš„å…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æå‡ºçš„PPMA-T&#x2F;S&#x2F;Bæ¨¡å‹åœ¨ADE20Kè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„mIoUè¾¾åˆ°äº†48.7%&#x2F;51.1%&#x2F;52.3%ï¼Œåˆ†åˆ«è¶…è¶Šäº†RMT-T&#x2F;S&#x2F;Bæ¨¡å‹0.7%&#x2F;1.3%&#x2F;0.3%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhongchenzhao/PPMA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhongchenzhao/PPMAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15940v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†Polyline Path Masked Attentionï¼ˆPPMAï¼‰ï¼Œç»“åˆäº†Vision Transformersï¼ˆViTsï¼‰çš„è‡ªæˆ‘æ³¨æ„æœºåˆ¶ä¸Mamba2çš„ç»“æ„åŒ–æ©ç ï¼Œåˆ©ç”¨ä¸¤è€…çš„äº’è¡¥ä¼˜åŠ¿ã€‚é€šè¿‡å¼•å…¥2DæŠ˜çº¿è·¯å¾„æ‰«æç­–ç•¥å’Œç›¸åº”çš„ç»“æ„åŒ–æ©ç ï¼ŒPPMAæ›´å¥½åœ°ä¿ç•™äº†å›¾åƒæ ‡è®°ä¹‹é—´çš„é‚»æ¥å…³ç³»ã€‚åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²ç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒPPMAæ¨¡å‹çš„è¡¨ç°ä¼˜äºåŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹å’ŒTransformerçš„å…ˆå‰æœ€å‰æ²¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Polyline Path Masked Attentionç»“åˆäº†Vision Transformersçš„è‡ªæˆ‘æ³¨æ„æœºåˆ¶å’ŒMamba2çš„ç»“æ„åŒ–æ©ç ï¼Œæ—¨åœ¨åˆ©ç”¨ä¸¤ç§æ¶æ„çš„äº’è¡¥ä¼˜åŠ¿ã€‚</li>
<li>é€šè¿‡å¼•å…¥2DæŠ˜çº¿è·¯å¾„æ‰«æç­–ç•¥ï¼Œæ”¹è¿›äº†ä¼ ç»Ÿçš„ç»“æ„åŒ–æ©ç ï¼Œæ¨å‡ºäº†Polyline Path Maskï¼Œæ›´å¥½åœ°ä¿ç•™äº†å›¾åƒæ ‡è®°é—´çš„é‚»æ¥å…³ç³»ã€‚</li>
<li>PPMAæ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹å’ŒTransformerçš„å…ˆå‰æ–¹æ³•ã€‚</li>
<li>PPMAæ¨¡å‹åœ¨ADE20Kè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„è¡¨ç°å…·ä½“ï¼Œä¾‹å¦‚PPMA-T&#x2F;S&#x2F;Bæ¨¡å‹åˆ†åˆ«å®ç°äº†48.7%&#x2F;51.1%&#x2F;52.3%çš„mIoUï¼Œç›¸è¾ƒäºRMT-T&#x2F;S&#x2F;Bæœ‰æ˜¾è‘—çš„æå‡ã€‚</li>
<li>PPMAæ¨¡å‹çš„ä»£ç å·²å…¬å¼€ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…å‚è€ƒå’Œä½¿ç”¨ã€‚</li>
<li>PPMAæ¨¡å‹çš„æå‡ºåŸºäºæ·±åšçš„ç†è®ºåŸºç¡€å’Œé«˜æ•ˆç®—æ³•è®¾è®¡ï¼Œç¡®ä¿äº†å…¶æœ‰æ•ˆæ€§å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15940">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d48d93347ddde982a5c3619384669612.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05ec3b53a4a7ebc7c4e3ca2ba813bc23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25a246d40760c59c7265a59faaa7057d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4085aab682a851125227923d4396d560.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Privacy-Preserving-in-Connected-and-Autonomous-Vehicles-Through-Vision-to-Text-Transformation"><a href="#Privacy-Preserving-in-Connected-and-Autonomous-Vehicles-Through-Vision-to-Text-Transformation" class="headerlink" title="Privacy-Preserving in Connected and Autonomous Vehicles Through Vision   to Text Transformation"></a>Privacy-Preserving in Connected and Autonomous Vehicles Through Vision   to Text Transformation</h2><p><strong>Authors:Abdolazim Rezaei, Mehdi Sookhak, Ahmad Patooghy</strong></p>
<p>Connected and Autonomous Vehicles (CAVs) rely on a range of devices that often process privacy-sensitive data. Among these, roadside units play a critical role particularly through the use of AI-equipped (AIE) cameras for applications such as violation detection. However, the privacy risks associated with captured imagery remain a major concern, as such data can be misused for identity theft, profiling, or unauthorized commercial purposes. While traditional techniques such as face blurring and obfuscation have been applied to mitigate privacy risks, individual privacy remains at risk, as individuals can still be tracked using other features such as their clothing. This paper introduces a novel privacy-preserving framework that leverages feedback-based reinforcement learning (RL) and vision-language models (VLMs) to protect sensitive visual information captured by AIE cameras. The main idea is to convert images into semantically equivalent textual descriptions, ensuring that scene-relevant information is retained while visual privacy is preserved. A hierarchical RL strategy is employed to iteratively refine the generated text, enhancing both semantic accuracy and privacy. Evaluation results demonstrate significant improvements in both privacy protection and textual quality, with the Unique Word Count increasing by approximately 77% and Detail Density by around 50% compared to existing approaches. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶å’Œè”ç½‘è½¦è¾†ï¼ˆCAVsï¼‰ä¾èµ–äºä¸€ç³»åˆ—ç»å¸¸å¤„ç†æ•æ„Ÿéšç§æ•°æ®çš„è®¾å¤‡ã€‚å…¶ä¸­ï¼Œè·¯è¾¹å•å…ƒé€šè¿‡åº”ç”¨é…å¤‡äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„æ‘„åƒå¤´è¿›è¡Œè¿ç« æ£€æµ‹ç­‰åº”ç”¨å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œä¸æ•è·çš„å›¾åƒç›¸å…³çš„éšç§é—®é¢˜ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦çš„æ‹…å¿§ï¼Œå› ä¸ºè¿™ç§æ•°æ®å¯èƒ½ä¼šè¢«ç”¨äºèº«ä»½ç›—çªƒã€ä¸ªäººç‰¹å¾æè¿°æˆ–æœªç»æˆæƒçš„å•†ä¸šç›®çš„ç­‰ä¸å½“ç”¨é€”ã€‚è™½ç„¶ä¼ ç»Ÿçš„æŠ€æœ¯å¦‚é¢éƒ¨æ¨¡ç³Šå’Œä¼ªè£…å·²è¢«åº”ç”¨äºç¼“è§£éšç§é—®é¢˜ï¼Œä½†ä¸ªäººéšç§ä»ç„¶é¢ä¸´é£é™©ï¼Œå› ä¸ºä¸ªä½“ä»ç„¶å¯ä»¥é€šè¿‡å…¶ä»–ç‰¹å¾å¦‚è¡£ç€è¿›è¡Œè·Ÿè¸ªã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„éšç§ä¿æŠ¤æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åŸºäºåé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¥ä¿æŠ¤äººå·¥æ™ºèƒ½æ‘„åƒå¤´æ•è·çš„æ•æ„Ÿè§†è§‰ä¿¡æ¯ã€‚ä¸»è¦æ€æƒ³æ˜¯å°†å›¾åƒè½¬æ¢ä¸ºè¯­ä¹‰ç­‰æ•ˆçš„æ–‡æœ¬æè¿°ï¼Œç¡®ä¿ä¿ç•™åœºæ™¯ç›¸å…³ä¿¡æ¯çš„åŒæ—¶ä¿æŠ¤è§†è§‰éšç§ã€‚é‡‡ç”¨åˆ†å±‚RLç­–ç•¥æ¥è¿­ä»£åœ°ä¼˜åŒ–ç”Ÿæˆçš„æ–‡æœ¬ï¼Œæé«˜è¯­ä¹‰å‡†ç¡®æ€§å’Œéšç§ä¿æŠ¤èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œåœ¨éšç§ä¿æŠ¤å’Œæ–‡æœ¬è´¨é‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå”¯ä¸€å­—æ•°å¢åŠ äº†çº¦77%ï¼Œç»†èŠ‚å¯†åº¦å¢åŠ äº†çº¦50%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15854v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºä¸€ç§æ–°å‹çš„éšç§ä¿æŠ¤æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆåé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå°†äººå·¥æ™ºèƒ½è®¾å¤‡ï¼ˆAIEï¼‰ç›¸æœºæ•æ‰åˆ°çš„æ•æ„Ÿè§†è§‰ä¿¡æ¯è½¬æ¢ä¸ºè¯­ä¹‰ç­‰æ•ˆçš„æ–‡æœ¬æè¿°ï¼Œä»è€Œä¿æŠ¤ä¸ªäººéšç§ã€‚æ­¤æ–¹æ³•åœ¨ä¿æŠ¤éšç§çš„åŒæ—¶ä¿ç•™äº†åœºæ™¯ç›¸å…³çš„ä¿¡æ¯ã€‚é€šè¿‡åˆ†å±‚å¼ºåŒ–å­¦ä¹ ç­–ç•¥å¯¹ç”Ÿæˆçš„æ–‡æœ¬è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œæé«˜äº†è¯­ä¹‰å‡†ç¡®æ€§å’Œéšç§ä¿æŠ¤æ•ˆæœã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨éšç§ä¿æŠ¤å’Œæ–‡æœ¬è´¨é‡æ–¹é¢å‡æœ‰æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAVsä¸­ä½¿ç”¨çš„è·¯è¾¹å•å…ƒï¼ˆç‰¹åˆ«æ˜¯é…å¤‡AIçš„ç›¸æœºï¼‰åœ¨æ•è·éšç§æ•æ„Ÿæ•°æ®æ–¹é¢å­˜åœ¨é‡å¤§éšç§é—®é¢˜ã€‚</li>
<li>ä¼ ç»ŸæŠ€æœ¯ï¼ˆå¦‚é¢éƒ¨æ¨¡ç³Šå’Œæ¨¡ç³Šå¤„ç†ï¼‰åœ¨ä¿æŠ¤éšç§æ–¹é¢ä»æœ‰å±€é™æ€§ï¼Œä¸ªä½“ä»å¯èƒ½è¢«åŸºäºå…¶ä»–ç‰¹å¾è¿½è¸ªã€‚</li>
<li>æ–°æ¡†æ¶åˆ©ç”¨åé¦ˆå¼ºåŒ–å­¦ä¹ å’Œè§†è§‰è¯­è¨€æ¨¡å‹å°†å›¾åƒè½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼Œä»¥ä¿æŠ¤éšç§ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿä¿ç•™åœºæ™¯ç›¸å…³çš„ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŠ¤ä¸ªäººéšç§ã€‚</li>
<li>é€šè¿‡åˆ†å±‚å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–ç”Ÿæˆçš„æ–‡æœ¬ï¼Œæé«˜è¯­ä¹‰å‡†ç¡®æ€§å’Œéšç§ä¿æŠ¤æ•ˆæœã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ–°æ¡†æ¶åœ¨éšç§ä¿æŠ¤å’Œæ–‡æœ¬è´¨é‡æ–¹é¢æ˜¾è‘—æé«˜ï¼Œå…¶ä¸­å”¯ä¸€å•è¯è®¡æ•°å¢åŠ äº†çº¦77%ï¼Œç»†èŠ‚å¯†åº¦å¢åŠ äº†çº¦50%ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå¤„ç†CAVsä¸­éšç§æ•æ„Ÿæ•°æ®æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-59cc46fb1854d0d154438726886eab5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50076c15e1073e6d839d1ec0fbc40a66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a72c47ebca7f225ba96718e48107c11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe6f5d5739561574ab34b245ddc27c95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f76e0dc73d02a8b59c6f08d65bef1f2f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ScholarSearch-Benchmarking-Scholar-Searching-Ability-of-LLMs"><a href="#ScholarSearch-Benchmarking-Scholar-Searching-Ability-of-LLMs" class="headerlink" title="ScholarSearch: Benchmarking Scholar Searching Ability of LLMs"></a>ScholarSearch: Benchmarking Scholar Searching Ability of LLMs</h2><p><strong>Authors:Junting Zhou, Wang Li, Yiyan Liao, Nengyuan Zhang, Tingjia Miao, Zhihui Qi, Yuhan Wu, Tong Yang</strong></p>
<p>Large Language Models (LLMs)â€™ search capabilities have garnered significant attention. Existing benchmarks, such as OpenAIâ€™s BrowseComp, primarily focus on general search scenarios and fail to adequately address the specific demands of academic search. These demands include deeper literature tracing and organization, professional support for academic databases, the ability to navigate long-tail academic knowledge, and ensuring academic rigor. Here, we proposed ScholarSearch, the first dataset specifically designed to evaluate the complex information retrieval capabilities of Large Language Models (LLMs) in academic research. ScholarSearch possesses the following key characteristics: Academic Practicality, where question content closely mirrors real academic learning and research environments, avoiding deliberately misleading models; High Difficulty, with answers that are challenging for single models (e.g., Grok DeepSearch or Gemini Deep Research) to provide directly, often requiring at least three deep searches to derive; Concise Evaluation, where limiting conditions ensure answers are as unique as possible, accompanied by clear sources and brief solution explanations, greatly facilitating subsequent audit and verification, surpassing the current lack of analyzed search datasets both domestically and internationally; and Broad Coverage, as the dataset spans at least 15 different academic disciplines. Through ScholarSearch, we expect to more precisely measure and promote the performance improvement of LLMs in complex academic information retrieval tasks. The data is available at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/PKU-DS-LAB/ScholarSearch">https://huggingface.co/datasets/PKU-DS-LAB/ScholarSearch</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœç´¢èƒ½åŠ›å·²å¼•èµ·å¹¿æ³›å…³æ³¨ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ï¼Œå¦‚OpenAIçš„BrowseCompï¼Œä¸»è¦å…³æ³¨ä¸€èˆ¬æœç´¢åœºæ™¯ï¼Œæœªèƒ½å……åˆ†æ»¡è¶³å­¦æœ¯æœç´¢çš„ç‰¹å®šéœ€æ±‚ã€‚è¿™äº›éœ€æ±‚åŒ…æ‹¬æ›´æ·±å…¥çš„æ–‡çŒ®è¿½è¸ªå’Œç»„ç»‡ã€å¯¹å­¦æœ¯æ•°æ®åº“çš„ä¸“ä¸šæ”¯æŒã€æµè§ˆé•¿å°¾å­¦æœ¯çŸ¥è¯†çš„èƒ½åŠ›ä»¥åŠç¡®ä¿å­¦æœ¯ä¸¥è°¨æ€§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ScholarSearchï¼Œè¿™æ˜¯ä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å­¦æœ¯ç ”ç©¶ä¸­çš„å¤æ‚ä¿¡æ¯æ£€ç´¢èƒ½åŠ›çš„ç¬¬ä¸€ä¸ªæ•°æ®é›†ã€‚ScholarSearchå…·æœ‰ä»¥ä¸‹å…³é”®ç‰¹ç‚¹ï¼šå­¦æœ¯å®ç”¨æ€§ï¼Œé—®é¢˜å†…å®¹ç´§å¯†åæ˜ çœŸå®å­¦æœ¯å­¦ä¹ å’Œç ”ç©¶ç¯å¢ƒï¼Œé¿å…æ•…æ„è¯¯å¯¼æ¨¡å‹ï¼›é«˜éš¾åº¦ï¼Œç­”æ¡ˆå¯¹äºå•ä¸€æ¨¡å‹ï¼ˆå¦‚Grok DeepSearchæˆ–Gemini Deep Researchï¼‰æ¥è¯´å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç›´æ¥æä¾›ç­”æ¡ˆå¾€å¾€éœ€è¦è¿›è¡Œè‡³å°‘ä¸‰æ¬¡æ·±åº¦æœç´¢ï¼›ç®€æ´è¯„ä¼°ï¼Œé™åˆ¶æ¡ä»¶ç¡®ä¿ç­”æ¡ˆæ˜¯ç‹¬ä¸€æ— äºŒçš„ï¼ŒåŒæ—¶é…æœ‰æ¸…æ™°çš„æ¥æºå’Œç®€è¦çš„è§£å†³æ–¹æ¡ˆè§£é‡Šï¼Œæå¤§åœ°ä¿ƒè¿›äº†åç»­çš„å®¡è®¡å’ŒéªŒè¯ï¼Œå¼¥è¡¥äº†å›½å†…å¤–åˆ†ææœç´¢æ•°æ®é›†çš„ç¼ºä¹ï¼›å¹¿æ³›è¦†ç›–ï¼Œè¯¥æ•°æ®é›†æ¶µç›–è‡³å°‘15ä¸ªä¸åŒçš„å­¦ç§‘ã€‚é€šè¿‡ScholarSearchï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿæ›´ç²¾ç¡®åœ°è¡¡é‡å’Œä¿ƒè¿›LLMåœ¨å¤æ‚çš„å­¦æœ¯ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­çš„æ€§èƒ½æå‡ã€‚æ•°æ®å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/PKU-DS-LAB/ScholarSearch%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://huggingface.co/datasets/PKU-DS-LAB/ScholarSearchä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13784v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœç´¢èƒ½åŠ›å¤‡å—å…³æ³¨ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•å¦‚OpenAIçš„BrowseCompä¸»è¦å…³æ³¨é€šç”¨æœç´¢åœºæ™¯ï¼Œæœªèƒ½å……åˆ†æ»¡è¶³å­¦æœ¯æœç´¢çš„ç‰¹å®šéœ€æ±‚ã€‚å­¦æœ¯æœç´¢éœ€æ±‚åŒ…æ‹¬æ–‡çŒ®è¿½è¸ªä¸ç»„ç»‡ã€ä¸“ä¸šå­¦æœ¯æ•°æ®åº“æ”¯æŒã€é•¿å°¾å­¦æœ¯çŸ¥è¯†å¯¼èˆªä»¥åŠå­¦æœ¯ä¸¥è°¨æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ScholarSearchï¼Œé¦–ä¸ªä¸“ä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å­¦æœ¯ç ”ç©¶ä¸­çš„å¤æ‚ä¿¡æ¯æ£€ç´¢èƒ½åŠ›è€Œè®¾è®¡çš„æ•°æ®é›†ã€‚ScholarSearchå…·å¤‡å­¦æœ¯å®ç”¨æ€§ã€é«˜éš¾åº¦ã€ç®€æ´è¯„ä¼°åŠå¹¿æ³›è¦†ç›–ç­‰å¤šä¸ªå…³é”®ç‰¹æ€§ï¼Œæ¶µç›–è‡³å°‘15ä¸ªä¸åŒå­¦æœ¯é¢†åŸŸã€‚é€šè¿‡ScholarSearchï¼Œæˆ‘ä»¬æœŸæœ›æ›´ç²¾ç¡®åœ°è¡¡é‡å¹¶ä¿ƒè¿›LLMåœ¨å¤æ‚å­¦æœ¯ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœç´¢èƒ½åŠ›å·²å¼•èµ·å¹¿æ³›å…³æ³¨ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸è¶³ä»¥æ»¡è¶³å­¦æœ¯æœç´¢çš„ç‰¹å®šéœ€æ±‚ã€‚</li>
<li>ScholarSearchæ•°æ®é›†ä¸“ä¸ºè¯„ä¼°LLMåœ¨å­¦æœ¯ç ”ç©¶ä¸­çš„ä¿¡æ¯æ£€ç´¢èƒ½åŠ›è€Œè®¾è®¡ã€‚</li>
<li>ScholarSearchå…·å¤‡å­¦æœ¯å®ç”¨æ€§ã€é«˜éš¾åº¦ã€ç®€æ´è¯„ä¼°åŠå¹¿æ³›è¦†ç›–ç­‰å…³é”®ç‰¹æ€§ã€‚</li>
<li>è¯¥æ•°æ®é›†æ³¨é‡å­¦æœ¯ä¸¥è°¨æ€§ï¼Œæ¶µç›–çœŸå®å­¦æœ¯ç¯å¢ƒå’Œç ”ç©¶éœ€æ±‚ã€‚</li>
<li>ScholarSearchæ•°æ®é›†è‡³å°‘æ¶µç›–15ä¸ªä¸åŒå­¦æœ¯é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-64b57c0eea20d3b51c96cb33b9e9eac9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9be65f5c6b7fe5506a416aaa5d848a66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-196a3752772dcb192d414a0451009e06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99464c517a75932e75693c5a86493f52.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Calibrating-Pre-trained-Language-Classifiers-on-LLM-generated-Noisy-Labels-via-Iterative-Refinement"><a href="#Calibrating-Pre-trained-Language-Classifiers-on-LLM-generated-Noisy-Labels-via-Iterative-Refinement" class="headerlink" title="Calibrating Pre-trained Language Classifiers on LLM-generated Noisy   Labels via Iterative Refinement"></a>Calibrating Pre-trained Language Classifiers on LLM-generated Noisy   Labels via Iterative Refinement</h2><p><strong>Authors:Liqin Ye, Agam Shah, Chao Zhang, Sudheer Chava</strong></p>
<p>The traditional process of creating labeled datasets is labor-intensive and expensive. Recent breakthroughs in open-source large language models (LLMs) have opened up a new avenue in generating labeled datasets automatically for various natural language processing (NLP) tasks, providing an alternative to such an expensive annotation process. However, the reliability of such auto-generated labels remains a significant concern due to inherent inaccuracies. When learning from noisy labels, the modelâ€™s generalization is likely to be harmed as it is prone to overfit to those label noises. While previous studies in learning from noisy labels mainly focus on synthetic noise and real-world noise, LLM-generated label noise receives less attention. In this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to calibrate the classifierâ€™s prediction, thus enhancing its robustness towards LLM-generated noisy labels. SiDyP retrieves potential true label candidates by neighborhood label distribution in text embedding space and iteratively refines noisy candidates using a simplex diffusion model. Our framework can increase the performance of the BERT classifier fine-tuned on both zero-shot and few-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30% respectively. We demonstrate the effectiveness of SiDyP by conducting extensive benchmarking for different LLMs over a variety of NLP tasks. Our code is available on Github. </p>
<blockquote>
<p>ä¼ ç»Ÿåˆ›å»ºæ ‡è®°æ•°æ®é›†çš„è¿‡ç¨‹åŠ³åŠ¨å¯†é›†ä¸”è´¹ç”¨é«˜æ˜‚ã€‚æœ€è¿‘å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çªç ´ä¸ºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡è‡ªåŠ¨ç”Ÿæˆæ ‡è®°æ•°æ®é›†å¼€è¾Ÿäº†ä¸€æ¡æ–°é€”å¾„ï¼Œä¸ºè¿™ç§æ˜‚è´µçš„æ ‡æ³¨è¿‡ç¨‹æä¾›äº†æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç”±äºå†…åœ¨çš„ä¸å‡†ç¡®æ€§ï¼Œè¿™ç§è‡ªåŠ¨ç”Ÿæˆçš„æ ‡ç­¾çš„å¯é æ€§ä»ç„¶æ˜¯ä¸€ä¸ªä»¤äººå…³æ³¨çš„é—®é¢˜ã€‚ä»å™ªå£°æ ‡ç­¾ä¸­å­¦ä¹ æ—¶ï¼Œæ¨¡å‹å¾ˆå®¹æ˜“å—åˆ°è¿™äº›æ ‡ç­¾å™ªå£°çš„å½±å“è€Œè¿‡åº¦æ‹Ÿåˆï¼Œä»è€Œå½±å“å…¶æ³›åŒ–èƒ½åŠ›ã€‚è™½ç„¶ä¹‹å‰å…³äºä»å™ªå£°æ ‡ç­¾ä¸­å­¦ä¹ çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆæˆå™ªå£°å’Œç°å®ä¸–ç•Œçš„å™ªå£°ä¸Šï¼Œä½†LLMç”Ÿæˆçš„æ ‡ç­¾å™ªå£°å´è¢«å¿½è§†äº†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SiDyPï¼šåŸºäºåŠ¨æ€å…ˆéªŒçš„ç®€å•æ ‡ç­¾æ‰©æ•£æ–¹æ³•ï¼Œä»¥æ ¡å‡†åˆ†ç±»å™¨çš„é¢„æµ‹ï¼Œä»è€Œæé«˜å…¶å¯¹LLMç”Ÿæˆçš„å™ªå£°æ ‡ç­¾çš„é²æ£’æ€§ã€‚SiDyPé€šè¿‡æ–‡æœ¬åµŒå…¥ç©ºé—´ä¸­çš„é‚»è¿‘æ ‡ç­¾åˆ†å¸ƒæ£€ç´¢æ½œåœ¨çš„çœŸå®æ ‡ç­¾å€™é€‰è€…ï¼Œå¹¶ä½¿ç”¨ç®€å•æ‰©æ•£æ¨¡å‹è¿­ä»£ä¼˜åŒ–å™ªå£°å€™é€‰è€…ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥æé«˜åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬LLMç”Ÿæˆå™ªå£°æ ‡ç­¾æ•°æ®é›†ä¸Šå¾®è°ƒè¿‡çš„BERTåˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œå¹³å‡åˆ†åˆ«æé«˜äº†7.21%å’Œ7.30%ã€‚æˆ‘ä»¬å¯¹ä¸åŒçš„LLMè¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†å¤šç§NLPä»»åŠ¡ï¼Œè¯æ˜äº†SiDyPçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19675v2">PDF</a> Accepted at KDDâ€™25</p>
<p><strong>Summary</strong></p>
<p>LLMè‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾æ•°æ®é›†çš„æ–¹æ³•ä¸ºNLPä»»åŠ¡æä¾›äº†æ–°é€”å¾„ï¼Œä½†å­˜åœ¨å¯é æ€§é—®é¢˜ã€‚æœ¬æ–‡æå‡ºSiDyPæ–¹æ³•ï¼Œé€šè¿‡ç®€å•æ ‡ç­¾æ‰©æ•£å’ŒåŠ¨æ€å…ˆéªŒæ¥æ ¡å‡†åˆ†ç±»å™¨é¢„æµ‹ï¼Œæé«˜å…¶å¯¹LLMç”Ÿæˆå™ªå£°æ ‡ç­¾çš„é²æ£’æ€§ã€‚SiDyPé€šè¿‡æ–‡æœ¬åµŒå…¥ç©ºé—´ä¸­çš„é‚»åŸŸæ ‡ç­¾åˆ†å¸ƒæ£€ç´¢æ½œåœ¨çš„çœŸå®æ ‡ç­¾å€™é€‰è€…ï¼Œå¹¶ä½¿ç”¨ç®€å•æ‰©æ•£æ¨¡å‹è¿­ä»£ä¼˜åŒ–å™ªå£°å€™é€‰è€…ã€‚è¯¥æ–¹æ³•å¯æé«˜é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬LLMç”Ÿæˆå™ªå£°æ ‡ç­¾æ•°æ®é›†ä¸ŠBERTåˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œå¹³å‡æé«˜7.21%å’Œ7.30%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾æ•°æ®é›†ä¸ºNLPä»»åŠ¡å¸¦æ¥æ–°é€”å¾„ï¼Œä½†å¯é æ€§æˆå…³æ³¨é‡ç‚¹ã€‚</li>
<li>LLMç”Ÿæˆçš„æ ‡ç­¾å™ªå£°å¯¹æ¨¡å‹æ³›åŒ–èƒ½åŠ›å¯èƒ½é€ æˆæŸå®³ã€‚</li>
<li>SiDyPæ–¹æ³•é€šè¿‡ç®€å•æ ‡ç­¾æ‰©æ•£å’ŒåŠ¨æ€å…ˆéªŒæé«˜æ¨¡å‹å¯¹LLMç”Ÿæˆå™ªå£°æ ‡ç­¾çš„é²æ£’æ€§ã€‚</li>
<li>SiDyPåœ¨æ–‡æœ¬åµŒå…¥ç©ºé—´ä¸­é€šè¿‡é‚»åŸŸæ ‡ç­¾åˆ†å¸ƒæ£€ç´¢çœŸå®æ ‡ç­¾å€™é€‰è€…ã€‚</li>
<li>SiDyPä½¿ç”¨ç®€å•æ‰©æ•£æ¨¡å‹è¿­ä»£ä¼˜åŒ–å™ªå£°å€™é€‰æ ‡ç­¾ã€‚</li>
<li>SiDyPèƒ½æé«˜é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬LLMç”Ÿæˆå™ªå£°æ ‡ç­¾æ•°æ®é›†ä¸ŠBERTåˆ†ç±»å™¨çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19675">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d9c7f9ca24f5627fc678c855267cb4e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1596d9f57a99782033b4dc9164ce6df6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9504263dce41bee3476baaa8573d3899.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f50bcc49e408a4345384b8f18ae7044.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-083acf531e66e8209b12054d5896d9ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84c1e9258d3e21385efd2c7fb50aef2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce0e105c0af9af76ffad9398cf9d46bd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ReplaceMe-Network-Simplification-via-Depth-Pruning-and-Transformer-Block-Linearization"><a href="#ReplaceMe-Network-Simplification-via-Depth-Pruning-and-Transformer-Block-Linearization" class="headerlink" title="ReplaceMe: Network Simplification via Depth Pruning and Transformer   Block Linearization"></a>ReplaceMe: Network Simplification via Depth Pruning and Transformer   Block Linearization</h2><p><strong>Authors:Dmitriy Shopkhoev, Ammar Ali, Magauiya Zhussip, Valentin Malykh, Stamatios Lefkimmiatis, Nikos Komodakis, Sergey Zagoruyko</strong></p>
<p>We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation, which approximates the pruned blocks. The estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining&#x2F;fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original modelâ€™s performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at <a target="_blank" rel="noopener" href="https://github.com/mts-ai/ReplaceMe">https://github.com/mts-ai/ReplaceMe</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ReplaceMeï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„è®­ç»ƒå¤–æ·±åº¦å‰ªææ–¹æ³•ï¼Œå®ƒèƒ½æœ‰æ•ˆåœ°ç”¨çº¿æ€§è¿ç®—æ›¿ä»£transformerå—ï¼ŒåŒæ—¶åœ¨ä½å‹ç¼©æ¯”çš„æƒ…å†µä¸‹ä¿æŒé«˜æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„éœ€è¦é¢å¤–è®­ç»ƒæˆ–å¾®è°ƒçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åªéœ€è¦ä¸€ä¸ªå°å‹çš„æ ¡å‡†æ•°æ®é›†æ¥ä¼°è®¡çº¿æ€§å˜æ¢ï¼Œè¯¥å˜æ¢è¿‘ä¼¼äºå‰ªæå—ã€‚ä¼°è®¡çš„çº¿æ€§æ˜ å°„å¯ä»¥æ— ç¼åœ°ä¸å…¶ä»–transformerå—åˆå¹¶ï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„ç½‘ç»œå‚æ•°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒReplaceMeæŒç»­è¶…è¶Šå…¶ä»–æ— è®­ç»ƒæ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ¶‰åŠå¤§é‡é‡æ–°è®­ç»ƒ&#x2F;å¾®è°ƒå’Œç»“æ„ä¿®æ”¹çš„å°–ç«¯å‰ªææ–¹æ³•ä¸­è¡¨ç°å‡ºé«˜åº¦ç«äº‰åŠ›ã€‚åº”ç”¨äºå¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ï¼ŒReplaceMeå¯å®ç°é«˜è¾¾25%çš„å‰ªæç‡ï¼ŒåŒæ—¶åœ¨å¼€æ”¾åŸºå‡†æµ‹è¯•ä¸­ä¿ç•™åŸå§‹æ¨¡å‹çº¦90%çš„æ€§èƒ½â€”â€”æ— éœ€ä»»ä½•è®­ç»ƒæˆ–ä¿®å¤æ­¥éª¤ï¼Œä¸”è®¡ç®—å¼€é”€æœ€å°ï¼ˆè§å›¾1ï¼‰ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/mts-ai/ReplaceMe%E4%B8%8A%E6%8F%90%E4%BE%9B%E4%BA%86%E5%AE%9E%E7%8E%B0ReplaceMe%E4%BB%A5%E5%8F%8A%E5%87%A0%E7%A7%8D%E5%85%88%E8%BF%9B%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%89%AA%E6%9E%9D%E6%8A%80%E6%9C%AF%E7%9A%84%E5%BC%80%E6%BA%90%E5%BA%93%E3%80%82">https://github.com/mts-ai/ReplaceMeä¸Šæä¾›äº†å®ç°ReplaceMeä»¥åŠå‡ ç§å…ˆè¿›çš„æ·±åº¦å‰ªææŠ€æœ¯çš„å¼€æºåº“ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02819v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ReplaceMeæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ·±åº¦å‰ªææ–¹æ³•ï¼Œå¯æœ‰æ•ˆåœ°ç”¨çº¿æ€§è¿ç®—æ›¿æ¢å˜å‹å™¨å—ï¼Œå¹¶åœ¨ä½å‹ç¼©æ¯”ä¸‹ä¿æŒé«˜æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä»…éœ€è¦ä¸€ä¸ªå°çš„æ ¡å‡†æ•°æ®é›†æ¥ä¼°è®¡çº¿æ€§å˜æ¢ï¼Œä»¥è¿‘ä¼¼å‰ªæå—ï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„ç½‘ç»œå‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒReplaceMeåœ¨æ— éœ€è®­ç»ƒçš„æ–¹æ³•ä¸­å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æ¶‰åŠå¤§é‡é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒä»¥åŠæ¶æ„ä¿®æ”¹çš„æœ€å…ˆè¿›å‰ªææ–¹æ³•ä¸­ä¿æŒç«äº‰åŠ›ã€‚åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ï¼ŒReplaceMeå¯åœ¨ä¸æŸå¤±ä»»ä½•æ€§èƒ½çš„æƒ…å†µä¸‹å®ç°é«˜è¾¾25%çš„å‰ªæç‡ï¼Œå¹¶ä¸”å…·æœ‰æœ€å°çš„è®¡ç®—å¼€é”€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReplaceMeæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ·±åº¦å‰ªææ–¹æ³•ï¼Œèƒ½å¤Ÿæ›¿æ¢å˜å‹å™¨å—ä¸ºçº¿æ€§æ“ä½œã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨å°çš„æ ¡å‡†æ•°æ®é›†æ¥ä¼°è®¡çº¿æ€§å˜æ¢ï¼Œä»¥æ¨¡æ‹Ÿå‰ªæå—çš„æ•ˆæœã€‚</li>
<li>ReplaceMeä¸éœ€è¦ä»»ä½•é¢å¤–çš„ç½‘ç»œå‚æ•°ï¼Œå¯ä»¥æ— ç¼åœ°èå…¥å‰©ä½™çš„å˜å‹å™¨å—ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒReplaceMeåœ¨æ— éœ€è®­ç»ƒçš„æ–¹æ³•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œä¸å…¶ä»–å…ˆè¿›çš„å‰ªææ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>åº”ç”¨åˆ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ï¼ŒReplaceMeå¯å®ç°é«˜è¾¾25%çš„å‰ªæç‡ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹æ¨¡å‹æ€§èƒ½çš„çº¦90%ã€‚</li>
<li>ReplaceMeå…·æœ‰æœ€å°çš„è®¡ç®—å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fcc0d2c3745224d87c692993241ca95c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71ce383af7b6ac14409099db13be337d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaff315cec7a2d0ff3358e188fb4fa97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a715ef31b9d80e6527d241aa38d08d1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="TALE-A-Tool-Augmented-Framework-for-Reference-Free-Evaluation-of-Large-Language-Models"><a href="#TALE-A-Tool-Augmented-Framework-for-Reference-Free-Evaluation-of-Large-Language-Models" class="headerlink" title="TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large   Language Models"></a>TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large   Language Models</h2><p><strong>Authors:Sher Badshah, Ali Emami, Hassan Sajjad</strong></p>
<p>As Large Language Models (LLMs) become increasingly integrated into real-world, autonomous applications, relying on static, pre-annotated references for evaluation poses significant challenges in cost, scalability, and completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework to assess LLM outputs without predetermined ground-truth answers. Unlike conventional metrics that compare to fixed references or depend solely on LLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities that actively retrieves and synthesizes external evidence. It iteratively generates web queries, collects information, summarizes findings, and refines subsequent searches through reflection. By shifting away from static references, TALE aligns with free-form question-answering tasks common in real-world scenarios. Experimental results on multiple free-form QA benchmarks show that TALE not only outperforms standard reference-based metrics for measuring response accuracy but also achieves substantial to near-perfect agreement with human evaluations. TALE enhances the reliability of LLM evaluations in real-world, dynamic scenarios without relying on static references. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°å®ä¸–ç•Œè‡ªä¸»åº”ç”¨ä¸­çš„é›†æˆåº¦è¶Šæ¥è¶Šé«˜ï¼Œä¾èµ–é™æ€ã€é¢„å…ˆæ ‡æ³¨çš„å‚è€ƒè¿›è¡Œè¯„ä¼°åœ¨æˆæœ¬ã€å¯æ‰©å±•æ€§å’Œå®Œæ•´æ€§æ–¹é¢å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†å·¥å…·å¢å¼ºå‹LLMè¯„ä¼°ï¼ˆTALEï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè¯„ä¼°LLMè¾“å‡ºè€Œæ— éœ€é¢„å…ˆç¡®å®šçš„æ­£ç¡®ç­”æ¡ˆã€‚ä¸ä¼ ç»Ÿçš„ä»…ä¸å›ºå®šå‚è€ƒè¿›è¡Œæ¯”è¾ƒæˆ–ä»…ä¾èµ–LLMä½œä¸ºè¯„åˆ¤çŸ¥è¯†çš„æŒ‡æ ‡ä¸åŒï¼ŒTALEä½¿ç”¨ä¸€ä¸ªå…·æœ‰å·¥å…·è®¿é—®èƒ½åŠ›çš„æ™ºèƒ½ä½“ï¼Œè¯¥æ™ºèƒ½ä½“èƒ½ä¸»åŠ¨æ£€ç´¢å¹¶ç»¼åˆå¤–éƒ¨è¯æ®ã€‚å®ƒé€šè¿‡è¿­ä»£ç”Ÿæˆç½‘ç»œæŸ¥è¯¢ã€æ”¶é›†ä¿¡æ¯ã€æ€»ç»“å‘ç°å¹¶é€šè¿‡åæ€æ¥ä¼˜åŒ–åç»­æœç´¢ã€‚é€šè¿‡æ‘†è„±é™æ€å‚è€ƒï¼ŒTALEä¸çœŸå®åœºæ™¯ä¸­å¸¸è§çš„è‡ªç”±å½¢å¼é—®ç­”ä»»åŠ¡ç›¸ç¬¦ã€‚åœ¨å¤šä¸ªè‡ªç”±å½¢å¼é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTALEä¸ä»…ä¼˜äºåŸºäºæ ‡å‡†å‚è€ƒçš„åº¦é‡æ ‡å‡†æ¥è¡¡é‡å“åº”å‡†ç¡®æ€§ï¼Œè€Œä¸”åœ¨ä¸äººç±»è¯„ä¼°çš„å¯¹æ¯”ä¸­å–å¾—äº†ä»æ˜¾è‘—åˆ°è¿‘ä¹å®Œç¾çš„å…±è¯†ã€‚TALEæé«˜äº†åœ¨çœŸå®ä¸–ç•ŒåŠ¨æ€åœºæ™¯ä¸­LLMè¯„ä¼°çš„å¯é æ€§ï¼Œæ— éœ€ä¾èµ–é™æ€å‚è€ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07385v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°å®ä¸–ç•Œè‡ªä¸»åº”ç”¨ä¸­çš„é›†æˆåº¦ä¸æ–­æé«˜ï¼Œä¾èµ–é™æ€ã€é¢„å…ˆæ³¨é‡Šçš„å‚è€ƒè¿›è¡Œè¯„ä¼°åœ¨æˆæœ¬ã€å¯æ‰©å±•æ€§å’Œå®Œæ•´æ€§æ–¹é¢å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†å·¥å…·å¢å¼ºå‹LLMè¯„ä¼°ï¼ˆTALEï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯åœ¨æ— éœ€é¢„å…ˆè®¾å®šçš„æ ‡å‡†ç­”æ¡ˆçš„æƒ…å†µä¸‹è¯„ä¼°LLMè¾“å‡ºã€‚ä¸ä¼ ç»Ÿçš„ä¸å›ºå®šå‚è€ƒç‚¹è¿›è¡Œæ¯”è¾ƒæˆ–ä»…ä¾èµ–LLMä½œä¸ºæ³•å®˜çŸ¥è¯†çš„è¯„ä¼°æŒ‡æ ‡ä¸åŒï¼ŒTALEä½¿ç”¨ä¸€ä¸ªå…·å¤‡å·¥å…·è®¿é—®èƒ½åŠ›çš„ä»£ç†ï¼Œèƒ½å¤Ÿä¸»åŠ¨æ£€ç´¢å’Œç»¼åˆå¤–éƒ¨è¯æ®ã€‚å®ƒé€šè¿‡ç”Ÿæˆç½‘ç»œæŸ¥è¯¢ã€æ”¶é›†ä¿¡æ¯ã€æ€»ç»“å‘ç°å¹¶é€šè¿‡åæ€æ¥ä¼˜åŒ–åç»­æœç´¢ã€‚é€šè¿‡æ‘†è„±é™æ€å‚è€ƒï¼ŒTALEç¬¦åˆç°å®åœºæ™¯ä¸­å¸¸è§çš„è‡ªç”±å½¢å¼é—®ç­”ä»»åŠ¡ã€‚åœ¨å¤šä¸ªè‡ªç”±å½¢å¼é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTALEä¸ä»…åœ¨è¡¡é‡å›ç­”å‡†ç¡®æ€§æ–¹é¢ä¼˜äºåŸºäºæ ‡å‡†å‚è€ƒçš„åº¦é‡æŒ‡æ ‡ï¼Œè€Œä¸”åœ¨ä¸äººç±»è¯„ä¼°çš„å¯¹æ¯”ä¸­å–å¾—äº†æ˜¾è‘—è‡³è¿‘ä¹å®Œç¾çš„å…±è¯†ã€‚TALEæé«˜äº†åœ¨ç°å®ä¸–ç•ŒåŠ¨æ€åœºæ™¯ä¸­LLMè¯„ä¼°çš„å¯é æ€§ï¼Œæ— éœ€ä¾èµ–é™æ€å‚è€ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMåœ¨ç°å®ä¸–ç•Œçš„é›†æˆåº”ç”¨ä¸­ï¼Œä½¿ç”¨é™æ€å‚è€ƒè¿›è¡Œè¯„ä¼°å­˜åœ¨æˆæœ¬ã€å¯æ‰©å±•æ€§å’Œå®Œæ•´æ€§é—®é¢˜ã€‚</li>
<li>æå‡ºçš„TALEæ¡†æ¶èƒ½ä¸»åŠ¨æ£€ç´¢å¤–éƒ¨è¯æ®å¹¶è¯„ä¼°LLMè¾“å‡ºï¼Œæ— éœ€é¢„å…ˆè®¾å®šçš„æ ‡å‡†ç­”æ¡ˆã€‚</li>
<li>TALEä½¿ç”¨å…·å¤‡å·¥å…·è®¿é—®èƒ½åŠ›çš„ä»£ç†ï¼Œèƒ½å¤Ÿç”Ÿæˆç½‘ç»œæŸ¥è¯¢ã€æ”¶é›†ä¿¡æ¯å¹¶æ€»ç»“å‘ç°ã€‚</li>
<li>TALEé€šè¿‡è¿­ä»£æœç´¢å’Œåæ€ï¼Œä¼˜åŒ–äº†è¯„ä¼°è¿‡ç¨‹ã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„è¯„ä¼°æŒ‡æ ‡ç›¸æ¯”ï¼ŒTALEåœ¨è‡ªç”±å½¢å¼é—®ç­”ä»»åŠ¡ä¸Šçš„è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>TALEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨ç°æ˜¾ç¤ºï¼Œå…¶è¡¡é‡å›ç­”å‡†ç¡®æ€§çš„èƒ½åŠ›è¶…è¶Šæ ‡å‡†å‚è€ƒåº¦é‡æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d751a91df7c04fbde4c325d929fe06e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1775c9cb21327a1eb8cc8b40de07c58.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Large-Scale-Data-Selection-for-Instruction-Tuning"><a href="#Large-Scale-Data-Selection-for-Instruction-Tuning" class="headerlink" title="Large-Scale Data Selection for Instruction Tuning"></a>Large-Scale Data Selection for Instruction Tuning</h2><p><strong>Authors:Hamish Ivison, Muru Zhang, Faeze Brahman, Pang Wei Koh, Pradeep Dasigi</strong></p>
<p>Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typically tested by selecting small datasets (roughly 10k samples) from small pools (100-200k samples). However, popular deployed instruction-tuned models often train on hundreds of thousands to millions of samples, subsampled from even larger data pools. We present a systematic study of how well data selection methods scale to these settings, selecting up to 2.5M samples from pools of up to 5.8M samples and evaluating across 7 diverse tasks. We show that many recently proposed methods fall short of random selection in this setting (while using more compute), and even decline in performance when given access to larger pools of data to select over. However, we find that a variant of representation-based data selection (RDS+), which uses weighted mean pooling of pretrained LM hidden states, consistently outperforms more complex methods across all settings tested â€“ all whilst being more compute-efficient. Our findings highlight that the scaling properties of proposed automated selection methods should be more closely examined. We release our code, data, and models at <a target="_blank" rel="noopener" href="https://github.com/hamishivi/automated-instruction-selection">https://github.com/hamishivi/automated-instruction-selection</a>. </p>
<blockquote>
<p>ä»è¾ƒå¤§çš„æ•°æ®æ± ä¸­é€‰å–é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®æ˜¯å¾®è°ƒè¯­è¨€æ¨¡å‹æ—¶çš„å…³é”®æ­¥éª¤ã€‚ç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®é›†é€šå¸¸ä¼šäº§ç”Ÿä¼˜äºåœ¨æ›´å¤§ã€æ›´å˜ˆæ‚çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚è‡ªåŠ¨æ•°æ®é€‰æ‹©æ–¹æ³•ç”¨äºæŒ‡ä»¤å¾®è°ƒæ—¶ï¼Œé€šå¸¸ä»å°å‹æ•°æ®æ± ï¼ˆ10ä¸‡è‡³20ä¸‡æ ·æœ¬ï¼‰ä¸­é€‰æ‹©å°å‹æ•°æ®é›†ï¼ˆå¤§çº¦1ä¸‡æ ·æœ¬ï¼‰è¿›è¡Œæµ‹è¯•ã€‚ç„¶è€Œï¼Œæµè¡Œçš„éƒ¨ç½²æŒ‡ä»¤å¾®è°ƒæ¨¡å‹é€šå¸¸åœ¨ä»æ›´å¤§çš„æ•°æ®æ± ä¸­æŠ½å–æ•°ç™¾ä¸‡è‡³æ•°åƒä¸‡æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†æ•°æ®é€‰æ‹©æ–¹æ³•å¦‚ä½•é€‚åº”è¿™äº›è®¾ç½®ï¼Œä»å¤šè¾¾58ä¸‡ä¸ªæ ·æœ¬çš„æ•°æ®æ± ä¸­é€‰æ‹©äº†å¤šè¾¾25ä¸‡ä¸ªæ ·æœ¬ï¼Œå¹¶åœ¨7ä¸ªä¸åŒä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨æ­¤è®¾ç½®ä¸­ï¼Œè®¸å¤šæœ€è¿‘æå‡ºçš„æ–¹æ³•ä¸åŠéšæœºé€‰æ‹©ï¼ˆåŒæ—¶ä½¿ç”¨æ›´å¤šè®¡ç®—èµ„æºï¼‰ï¼Œå¹¶ä¸”åœ¨ç»™å®šçš„æ›´å¤§æ± ä¸­é€‰æ‹©æ•°æ®æ—¶æ€§èƒ½ç”šè‡³ä¸‹é™ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ä¸€ç§åŸºäºè¡¨ç¤ºçš„å˜ä½“æ•°æ®é€‰æ‹©ï¼ˆRDS+ï¼‰ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨é¢„è®­ç»ƒLMéšè—çŠ¶æ€çš„åŠ æƒå‡å€¼æ± åŒ–æŠ€æœ¯ï¼Œåœ¨æ‰€æœ‰æµ‹è¯•è®¾ç½®ä¸­å‡ä¼˜äºæ›´å¤æ‚çš„æ–¹æ³•ï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåº”æ›´ä»”ç»†åœ°æ£€æŸ¥æ‰€æå‡ºçš„è‡ªåŠ¨é€‰æ‹©æ–¹æ³•çš„å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/hamishivi/automated-instruction-selection">https://github.com/hamishivi/automated-instruction-selection</a>ä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01807v2">PDF</a> Updated, new baselines, removed some typos</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åœ¨æ›´å¤§è§„æ¨¡æ•°æ®é›†ä¸ŠæŒ‡ä»¤å¾®è°ƒè¯­è¨€æ¨¡å‹æ—¶çš„æ•°æ®é€‰æ‹©é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œè®¸å¤šè¿‘æœŸæå‡ºçš„æ•°æ®é€‰æ‹©æ–¹æ³•åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œä¸€ç§åŸºäºè¡¨ç¤ºçš„æ•°æ®é€‰æ‹©å˜ä½“ï¼ˆRDS+ï¼‰åœ¨æ‰€æœ‰æµ‹è¯•è®¾ç½®ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®é€‰æ‹©åœ¨æŒ‡ä»¤å¾®è°ƒè¯­è¨€æ¨¡å‹ä¸­è‡³å…³é‡è¦ï¼Œé«˜è´¨é‡æ•°æ®é›†å¾€å¾€èƒ½äº§ç”Ÿæ€§èƒ½æ›´ä½³çš„æ¨¡å‹ã€‚</li>
<li>è‡ªåŠ¨åŒ–æ•°æ®é€‰æ‹©æ–¹æ³•é€šå¸¸åœ¨è¾ƒå°çš„æ•°æ®é›†å’Œæ ·æœ¬æ± ä¸­è¿›è¡Œæµ‹è¯•ã€‚</li>
<li>åœ¨æ›´å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆæ•°ç™¾ä¸‡è‡³æ•°åƒä¸‡æ ·æœ¬ï¼‰ä¸Šï¼Œè®¸å¤šæå‡ºçš„æ•°æ®é€‰æ‹©æ–¹æ³•è¡¨ç°ä¸å¦‚éšæœºé€‰æ‹©ã€‚</li>
<li>åŸºäºè¡¨ç¤ºçš„æ•°æ®é€‰æ‹©å˜ä½“ï¼ˆRDS+ï¼‰åœ¨æ‰€æœ‰æµ‹è¯•è®¾ç½®ä¸­éƒ½è¡¨ç°ä¼˜å¼‚ï¼Œä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚</li>
<li>éšç€æ•°æ®æ± è§„æ¨¡çš„æ‰©å¤§ï¼Œæ•°æ®é€‰æ‹©æ–¹æ³•çš„æ€§èƒ½å¯èƒ½ä¼šä¸‹é™ã€‚</li>
<li>åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒæ—¶ï¼Œéœ€è¦æ›´ç´§å¯†åœ°æ£€æŸ¥æ•°æ®é€‰æ‹©æ–¹æ³•çš„å¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01807">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0cb8c08ef084b09208d7ef0e1fbc1a68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38a41db1ae4c051e92b210ec8d8e992a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3545f79463c4933987b03be47936731.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ALTA-Compiler-Based-Analysis-of-Transformers"><a href="#ALTA-Compiler-Based-Analysis-of-Transformers" class="headerlink" title="ALTA: Compiler-Based Analysis of Transformers"></a>ALTA: Compiler-Based Analysis of Transformers</h2><p><strong>Authors:Peter Shaw, James Cohan, Jacob Eisenstein, Kenton Lee, Jonathan Berant, Kristina Toutanova</strong></p>
<p>We propose a new programming language called ALTA and a compiler that can map ALTA programs to Transformer weights. ALTA is inspired by RASP, a language proposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler from RASP programs to Transformer weights. ALTA complements and extends this prior work, offering the ability to express loops and to compile programs to Universal Transformers, among other advantages. ALTA allows us to constructively show how Transformers can represent length-invariant algorithms for computing parity and addition, as well as a solution to the SCAN benchmark of compositional generalization tasks, without requiring intermediate scratchpad decoding steps. We also propose tools to analyze cases where the expressibility of an algorithm is established, but end-to-end training on a given training set fails to induce behavior consistent with the desired algorithm. To this end, we explore training from ALTA execution traces as a more fine-grained supervision signal. This enables additional experiments and theoretical analyses relating the learnability of various algorithms to data availability and modeling decisions, such as positional encodings. We make the ALTA framework â€“ language specification, symbolic interpreter, and weight compiler â€“ available to the community to enable further applications and insights. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç¼–ç¨‹è¯­è¨€ALTAï¼Œä»¥åŠèƒ½å¤Ÿå°†ALTAç¨‹åºæ˜ å°„åˆ°Transformeræƒé‡ï¼ˆTransformer weightsï¼‰çš„ç¼–è¯‘å™¨ã€‚ALTAè¯­è¨€å—åˆ°Weissç­‰äººï¼ˆ2021ï¼‰æå‡ºçš„RASPè¯­è¨€å’ŒLindnerç­‰äººï¼ˆ2023ï¼‰æå‡ºçš„å°†RASPç¨‹åºç¼–è¯‘æˆTransformeræƒé‡çš„Tracrç¼–è¯‘å™¨çš„å¯å‘ã€‚ALTAæ˜¯å¯¹è¿™äº›å…ˆå‰å·¥ä½œçš„è¡¥å……å’Œæ‰©å±•ï¼Œé™¤äº†å…·å¤‡è¡¨è¾¾å¾ªç¯çš„èƒ½åŠ›å¤–ï¼Œè¿˜èƒ½å°†ç¨‹åºç¼–è¯‘ä¸ºé€šç”¨Transformerï¼Œå…·æœ‰å…¶ä»–ä¼˜åŠ¿ã€‚é€šè¿‡ALTAï¼Œæˆ‘ä»¬èƒ½å¤Ÿç›´è§‚åœ°å±•ç¤ºTransformerå¦‚ä½•è¡¨ç¤ºç”¨äºè®¡ç®—å¥‡å¶æ€§å’ŒåŠ æ³•çš„ä¸å˜é•¿åº¦ç®—æ³•ï¼Œä»¥åŠè§£å†³æ— éœ€ä¸­é—´æš‚å­˜è§£ç æ­¥éª¤çš„ç»„åˆæ³›åŒ–ä»»åŠ¡çš„SCANåŸºå‡†æµ‹è¯•è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜æå‡ºäº†åˆ†æå·¥å…·ï¼Œç”¨äºåˆ†æç®—æ³•è¡¨è¾¾æ€§å·²ç¡®ç«‹ä½†ç‰¹å®šè®­ç»ƒé›†ç«¯åˆ°ç«¯è®­ç»ƒæœªèƒ½è¯±å¯¼å‡ºä¸æœŸæœ›ç®—æ³•ä¸€è‡´è¡Œä¸ºçš„æƒ…å†µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä»ALTAæ‰§è¡Œè½¨è¿¹è¿›è¡Œè®­ç»ƒä½œä¸ºæ›´ç²¾ç»†çš„ç›‘ç£ä¿¡å·ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿè¿›è¡Œæ›´å¤šå®éªŒå’Œç†è®ºåˆ†æï¼Œå°†å„ç§ç®—æ³•çš„å­¦ä¹ æ€§ä¸æ•°æ®å¯ç”¨æ€§å’Œå»ºæ¨¡å†³ç­–ï¼ˆå¦‚ä½ç½®ç¼–ç ï¼‰ç›¸å…³è”ã€‚æˆ‘ä»¬å°†ALTAæ¡†æ¶ï¼ˆåŒ…æ‹¬è¯­è¨€è§„èŒƒã€ç¬¦å·è§£é‡Šå™¨å’Œæƒé‡ç¼–è¯‘å™¨ï¼‰æä¾›ç»™ç¤¾åŒºï¼Œä»¥æ¨åŠ¨è¿›ä¸€æ­¥çš„åº”ç”¨å’Œè§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18077v2">PDF</a> TMLR 2025</p>
<p><strong>Summary</strong></p>
<p>ALTAæ˜¯ä¸€ç§æ–°å‹ç¼–ç¨‹è¯­è¨€ï¼Œå¯ç¼–è¯‘ä¸ºTransformeræƒé‡ï¼Œå—RASPå’ŒTracrå¯å‘ã€‚å®ƒå…·å¤‡è¡¨è¾¾å¾ªç¯çš„èƒ½åŠ›ï¼Œå¹¶èƒ½ç¼–è¯‘ä¸ºé€šç”¨Transformerï¼Œè¿˜å…·æœ‰å…¶ä»–ä¼˜åŠ¿ã€‚ALTAèƒ½å¤Ÿå±•ç¤ºTransformerå¦‚ä½•è¡¨ç¤ºé•¿åº¦ä¸å˜çš„ç®—æ³•ï¼Œè§£å†³SCANåŸºå‡†æµ‹è¯•ä¸­çš„ç»„åˆæ³›åŒ–ä»»åŠ¡ï¼Œä¸”æ— éœ€ä¸­é—´è§£ç æ­¥éª¤ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æä¾›äº†å·¥å…·æ¥åˆ†æç®—æ³•è¡¨è¾¾æ€§å»ºç«‹ä½†ç«¯åˆ°ç«¯è®­ç»ƒå¤±è´¥çš„æƒ…å†µï¼Œå¹¶æå‡ºäº†é€šè¿‡ALTAæ‰§è¡Œè½¨è¿¹è¿›è¡Œæ›´ç²¾ç»†çš„ç›‘ç£ä¿¡å·è®­ç»ƒçš„æ–¹æ³•ã€‚è¿™ä½¿æ›´å¤šçš„å®éªŒå’Œç†è®ºåˆ†ææˆä¸ºå¯èƒ½ï¼Œå…³è”äº†ä¸åŒç®—æ³•çš„å¯å­¦ä¹ æ€§ä¸æ•°æ®å¯ç”¨æ€§å’Œå»ºæ¨¡å†³ç­–ç­‰å› ç´ ã€‚ALTAæ¡†æ¶ç°å·²å‘å…¬ä¼—å¼€æ”¾ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„åº”ç”¨å’Œè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ALTAæ˜¯ä¸€ç§æ–°å‹ç¼–ç¨‹è¯­è¨€ï¼Œå¯ç¼–è¯‘ä¸ºTransformeræƒé‡ã€‚</li>
<li>ALTAå—RASPå’ŒTracrå¯å‘ï¼Œå…·å¤‡è¡¨è¾¾å¾ªç¯çš„èƒ½åŠ›ï¼Œå¹¶å¯ä»¥ç¼–è¯‘ä¸ºé€šç”¨Transformerã€‚</li>
<li>ALTAèƒ½å±•ç¤ºTransformerå¦‚ä½•è¡¨ç¤ºé•¿åº¦ä¸å˜çš„ç®—æ³•ï¼Œå¦‚è®¡ç®—å¥‡å¶æ€§å’ŒåŠ æ³•ã€‚</li>
<li>ALTAè§£å†³äº†SCANåŸºå‡†æµ‹è¯•ä¸­çš„ç»„åˆæ³›åŒ–ä»»åŠ¡ï¼Œæ— éœ€ä¸­é—´è§£ç æ­¥éª¤ã€‚</li>
<li>ALTAæä¾›äº†å·¥å…·æ¥åˆ†æç®—æ³•è¡¨è¾¾æ€§å»ºç«‹ä½†ç«¯åˆ°ç«¯è®­ç»ƒå¤±è´¥çš„æƒ…å†µã€‚</li>
<li>é€šè¿‡ALTAæ‰§è¡Œè½¨è¿¹è¿›è¡Œæ›´ç²¾ç»†çš„ç›‘ç£ä¿¡å·è®­ç»ƒçš„æ–¹æ³•è¢«æå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.18077">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf7cc29440ec7cc0275ae2589477d098.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f530f2eb677a2d52211102bc60dfe91c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b74a59903890c2223de200cd602e43b9.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Cost-effective-Instruction-Learning-for-Pathology-Vision-and-Language-Analysis"><a href="#Cost-effective-Instruction-Learning-for-Pathology-Vision-and-Language-Analysis" class="headerlink" title="Cost-effective Instruction Learning for Pathology Vision and Language   Analysis"></a>Cost-effective Instruction Learning for Pathology Vision and Language   Analysis</h2><p><strong>Authors:Kaitao Chen, Mianxin Liu, Fang Yan, Lei Ma, Xiaoming Shi, Lilong Wang, Xiaosong Wang, Lifeng Zhu, Zhe Wang, Mu Zhou, Shaoting Zhang</strong></p>
<p>The advent of vision-language models fosters the interactive conversations between AI-enabled models and humans. Yet applying these models into clinics must deal with daunting challenges around large-scale training data, financial, and computational resources. Here we propose a cost-effective instruction learning framework for conversational pathology named as CLOVER. CLOVER only trains a lightweight module and uses instruction tuning while freezing the parameters of the large language model. Instead of using costly GPT-4, we propose well-designed prompts on GPT-3.5 for building generation-based instructions, emphasizing the utility of pathological knowledge derived from the Internet source. To augment the use of instructions, we construct a high-quality set of template-based instructions in the context of digital pathology. From two benchmark datasets, our findings reveal the strength of hybrid-form instructions in the visual question-answer in pathology. Extensive results show the cost-effectiveness of CLOVER in answering both open-ended and closed-ended questions, where CLOVER outperforms strong baselines that possess 37 times more training parameters and use instruction data generated from GPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot learning in the external clinical dataset. These findings demonstrate that cost-effective modeling of CLOVER could accelerate the adoption of rapid conversational applications in the landscape of digital pathology. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹çš„å‡ºç°ä¿ƒè¿›äº†äººå·¥æ™ºèƒ½æ¨¡å‹ä¸äººç±»ä¹‹é—´çš„äº¤äº’å¼å¯¹è¯ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹åº”ç”¨äºä¸´åºŠå¿…é¡»åº”å¯¹å¤§è§„æ¨¡è®­ç»ƒæ•°æ®ã€èµ„é‡‘å’Œè®¡ç®—èµ„æºç­‰æ–¹é¢çš„å·¨å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é’ˆå¯¹åä¸ºCLOVERçš„å¯¹è¯å¼ç—…ç†å­¦æå‡ºäº†ç»æµé«˜æ•ˆçš„æŒ‡ä»¤å­¦ä¹ æ¡†æ¶ã€‚CLOVERåªè®­ç»ƒä¸€ä¸ªè½»é‡çº§æ¨¡å—ï¼Œå¹¶ä½¿ç”¨æŒ‡ä»¤å¾®è°ƒæ¥å†»ç»“å¤§å‹è¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚æˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨æ˜‚è´µçš„GPT-4ï¼Œè€Œæ˜¯æå‡ºäº†é’ˆå¯¹GPT-3.5çš„ç²¾å¿ƒè®¾è®¡æç¤ºæ¥æ„å»ºåŸºäºç”Ÿæˆçš„æŒ‡ä»¤ï¼Œå¼ºè°ƒä»äº’è”ç½‘èµ„æºä¸­è·å¾—çš„ç—…ç†å­¦çŸ¥è¯†çš„å®ç”¨æ€§ã€‚ä¸ºäº†å¢å¼ºæŒ‡ä»¤çš„ä½¿ç”¨ï¼Œæˆ‘ä»¬åœ¨æ•°å­—ç—…ç†çš„èƒŒæ™¯ä¸‹æ„å»ºäº†ä¸€å¥—é«˜è´¨é‡çš„åŸºäºæ¨¡æ¿çš„æŒ‡ä»¤é›†ã€‚ä»ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬å‘ç°æ··åˆå½¢å¼çš„æŒ‡ä»¤åœ¨ç—…ç†è§†è§‰é—®ç­”ä¸­çš„ä¼˜åŠ¿ã€‚å¤§é‡ç»“æœè¡¨æ˜ï¼ŒCLOVERåœ¨å›ç­”å¼€æ”¾æ€§å’Œå°é—­æ€§é—®é¢˜æ—¶çš„æˆæœ¬æ•ˆç›Šé«˜ï¼Œå…¶ä¸­CLOVERçš„è¡¨ç°ä¼˜äºæ‹¥æœ‰37å€ä»¥ä¸Šè®­ç»ƒå‚æ•°å’Œä½¿ç”¨GPT-4ç”ŸæˆæŒ‡ä»¤æ•°æ®çš„å¼ºå¤§åŸºçº¿ã€‚é€šè¿‡æŒ‡ä»¤å¾®è°ƒï¼ŒCLOVERæ˜¾ç¤ºå‡ºåœ¨å¤–éƒ¨ä¸´åºŠæ•°æ®é›†ä¸­çš„å°æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒCLOVERçš„ç»æµé«˜æ•ˆå»ºæ¨¡å¯ä»¥åŠ é€Ÿæ•°å­—ç—…ç†å­¦é¢†åŸŸå¿«é€Ÿå¯¹è¯å¼åº”ç”¨çš„é‡‡ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.17734v2">PDF</a> </p>
<p><strong>Summary</strong><br>     éšç€è§†è§‰è¯­è¨€æ¨¡å‹çš„å…´èµ·ï¼Œäººå·¥æ™ºèƒ½æ¨¡å‹ä¸äººç±»ä¹‹é—´çš„äº¤äº’å¯¹è¯æ—¥ç›Šé¢‘ç¹ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹åº”ç”¨äºä¸´åºŠæ—¶ï¼Œé¢ä¸´å¤§è§„æ¨¡è®­ç»ƒæ•°æ®ã€èµ„é‡‘å’Œè®¡ç®—èµ„æºçš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå¯¹è¯ç—…ç†å­¦çš„ä½æˆæœ¬æŒ‡ä»¤å­¦ä¹ æ¡†æ¶CLOVERã€‚CLOVERä»…è®­ç»ƒè½»é‡çº§æ¨¡å—ï¼Œå¹¶é‡‡ç”¨æŒ‡ä»¤å¾®è°ƒæ¥å†»ç»“å¤§å‹è¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚æˆ‘ä»¬å¹¶æœªä½¿ç”¨æˆæœ¬é«˜æ˜‚çš„GPT-4ï¼Œè€Œæ˜¯è®¾è®¡äº†é’ˆå¯¹GPT-3.5çš„æç¤ºæ¥æ„å»ºç”ŸæˆæŒ‡ä»¤ï¼Œå¼ºè°ƒäº†ä»äº’è”ç½‘èµ„æºä¸­è·å¾—çš„ç—…ç†å­¦çŸ¥è¯†çš„å®ç”¨æ€§ã€‚ä¸ºäº†å¢å¼ºæŒ‡ä»¤çš„ä½¿ç”¨æ•ˆæœï¼Œæˆ‘ä»¬åœ¨æ•°å­—ç—…ç†èƒŒæ™¯ä¸‹æ„å»ºäº†ä¸€å¥—é«˜è´¨é‡çš„æ¨¡æ¿æŒ‡ä»¤é›†ã€‚ä»ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†çš„ç ”ç©¶ç»“æœæ¥çœ‹ï¼Œæ··åˆå½¢å¼çš„æŒ‡ä»¤åœ¨ç—…ç†å­¦è§†è§‰é—®ç­”ä¸­çš„ä¼˜åŠ¿æ˜¾è‘—ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å›ç­”å¼€æ”¾å’Œå°é—­æ€§é—®é¢˜æ–¹é¢ï¼ŒCLOVERè¡¨ç°å‡ºæˆæœ¬æ•ˆç›Šé«˜ï¼Œä¼˜äºæ‹¥æœ‰æ›´å¤šè®­ç»ƒå‚æ•°å’ŒåŸºäºGPT-4ç”ŸæˆæŒ‡ä»¤çš„å¼ºå¤§åŸºçº¿ã€‚é€šè¿‡æŒ‡ä»¤å¾®è°ƒï¼ŒCLOVERå±•ç°å‡ºå¯¹å¤–éƒ¨ä¸´åºŠæ•°æ®é›†çš„å°‘æ ·æœ¬å­¦ä¹ çš„ç¨³å¥æ€§ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œä½æˆæœ¬å»ºæ¨¡çš„CLOVERå¯èƒ½åŠ é€Ÿæ•°å­—ç—…ç†å­¦é¢†åŸŸçš„å¿«é€Ÿå¯¹è¯åº”ç”¨ç¨‹åºçš„é‡‡ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹çš„å…´èµ·ä¿ƒè¿›äº†AIä¸äººç±»ä¹‹é—´çš„äº¤äº’å¯¹è¯ã€‚</li>
<li>å°†AIæ¨¡å‹åº”ç”¨äºä¸´åºŠæ—¶é¢ä¸´å¤§è§„æ¨¡è®­ç»ƒæ•°æ®ã€èµ„é‡‘å’Œè®¡ç®—èµ„æºçš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç”¨äºå¯¹è¯ç—…ç†å­¦çš„ä½æˆæœ¬æŒ‡ä»¤å­¦ä¹ æ¡†æ¶CLOVERã€‚</li>
<li>CLOVERé€šè¿‡ä»…è®­ç»ƒè½»é‡çº§æ¨¡å—å’Œé‡‡ç”¨æŒ‡ä»¤å¾®è°ƒæ¥é™ä½æˆæœ¬ã€‚</li>
<li>ä½¿ç”¨GPT-3.5çš„æç¤ºæ„å»ºç”ŸæˆæŒ‡ä»¤ï¼Œå¼ºè°ƒäº’è”ç½‘æ¥æºçš„ç—…ç†å­¦çŸ¥è¯†çš„å®ç”¨æ€§ã€‚</li>
<li>åœ¨æ•°å­—ç—…ç†èƒŒæ™¯ä¸‹æ„å»ºäº†ä¸€å¥—é«˜è´¨é‡çš„æ¨¡æ¿æŒ‡ä»¤é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.17734">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-305457b1ad5ab8ab2a78b56ba6bf0111.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb1257e8598da3ad107b8138434c8199.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dbb27237bd00903e6a9b6b822c8dfb5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-53092162b508feea490dc5989d9cd034.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Disentangling-and-Integrating-Relational-and-Sensory-Information-in-Transformer-Architectures"><a href="#Disentangling-and-Integrating-Relational-and-Sensory-Information-in-Transformer-Architectures" class="headerlink" title="Disentangling and Integrating Relational and Sensory Information in   Transformer Architectures"></a>Disentangling and Integrating Relational and Sensory Information in   Transformer Architectures</h2><p><strong>Authors:Awni Altabaa, John Lafferty</strong></p>
<p>Relational reasoning is a central component of generally intelligent systems, enabling robust and data-efficient inductive generalization. Recent empirical evidence shows that many existing neural architectures, including Transformers, struggle with tasks requiring relational reasoning. In this work, we distinguish between two types of information: sensory information about the properties of individual objects, and relational information about the relationships between objects. While neural attention provides a powerful mechanism for controlling the flow of sensory information between objects, the Transformer lacks an explicit computational mechanism for routing and processing relational information. To address this limitation, we propose an architectural extension of the Transformer framework that we call the Dual Attention Transformer (DAT), featuring two distinct attention mechanisms: sensory attention for directing the flow of sensory information, and a novel relational attention mechanism for directing the flow of relational information. We empirically evaluate DAT on a diverse set of tasks ranging from synthetic relational benchmarks to complex real-world tasks such as language modeling and visual processing. Our results demonstrate that integrating explicit relational computational mechanisms into the Transformer architecture leads to significant performance gains in terms of data efficiency and parameter efficiency. </p>
<blockquote>
<p>å…³ç³»æ¨ç†æ˜¯é€šç”¨æ™ºèƒ½ç³»ç»Ÿçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œèƒ½å¤Ÿå®ç°ç¨³å¥ä¸”æ•°æ®é«˜æ•ˆå½’çº³æ¦‚æ‹¬ã€‚æœ€æ–°çš„å®è¯è¯æ®è¡¨æ˜ï¼ŒåŒ…æ‹¬Transformeråœ¨å†…çš„è®¸å¤šç°æœ‰ç¥ç»ç½‘ç»œæ¶æ„åœ¨å¤„ç†éœ€è¦å…³ç³»æ¨ç†çš„ä»»åŠ¡æ—¶é¢ä¸´å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†ä¿¡æ¯åˆ†ä¸ºä¸¤ç±»ï¼šå…³äºå•ä¸ªå¯¹è±¡å±æ€§çš„æ„Ÿè§‰ä¿¡æ¯å’Œå…³äºå¯¹è±¡ä¹‹é—´å…³ç³»çš„å…³ç³»ä¿¡æ¯ã€‚è™½ç„¶ç¥ç»æ³¨æ„åŠ›ä¸ºæ§åˆ¶å¯¹è±¡ä¹‹é—´æ„Ÿè§‰ä¿¡æ¯çš„æµåŠ¨æä¾›äº†å¼ºå¤§çš„æœºåˆ¶ï¼Œä½†Transformerç¼ºä¹ç”¨äºè·¯ç”±å’Œå¤„ç†å…³ç³»ä¿¡æ¯çš„æ˜ç¡®è®¡ç®—æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Transformeræ¶æ„çš„æ‰©å±•ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºåŒæ³¨æ„åŠ›Transformerï¼ˆDATï¼‰ï¼Œå®ƒå…·æœ‰ä¸¤ç§ç‹¬ç‰¹çš„æ³¨æ„åŠ›æœºåˆ¶ï¼šæ„Ÿè§‰æ³¨æ„åŠ›ï¼Œç”¨äºå¼•å¯¼æ„Ÿè§‰ä¿¡æ¯çš„æµåŠ¨ï¼Œä»¥åŠæ–°å‹çš„å…³ç³»æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºå¼•å¯¼å…³ç³»ä¿¡æ¯çš„æµåŠ¨ã€‚æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—ä»»åŠ¡ä¸Šå¯¹DATè¿›è¡Œäº†å®è¯è¯„ä¼°ï¼Œè¿™äº›ä»»åŠ¡èŒƒå›´ä»åˆæˆå…³ç³»åŸºå‡†æµ‹è¯•åˆ°å¤æ‚çš„ç°å®ä¸–ç•Œä»»åŠ¡ï¼Œå¦‚è¯­è¨€å»ºæ¨¡å’Œè§†è§‰å¤„ç†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°†æ˜ç¡®çš„å…³ç³»è®¡ç®—æœºåˆ¶æ•´åˆåˆ°Transformeræ¶æ„ä¸­ï¼Œåœ¨æ•°æ®æ•ˆç‡å’Œå‚æ•°æ•ˆç‡æ–¹é¢å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.16727v3">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>å…³ç³»æ¨ç†æ˜¯æ™ºèƒ½ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå®ƒå¯å®ç°ç¨³å¥ä¸”é«˜æ•ˆçš„æ•°æ®å½’çº³æ¦‚æ‹¬ã€‚ç°æœ‰ç¥ç»æ¶æ„ï¼Œå¦‚Transformerç­‰ï¼Œåœ¨å¤„ç†éœ€è¦å…³ç³»æ¨ç†çš„ä»»åŠ¡æ—¶è¡¨ç°æ¬ ä½³ã€‚æœ¬æ–‡åŒºåˆ†äº†ä¸¤ç§ä¿¡æ¯ç±»å‹ï¼šå…³äºå•ä¸ªå¯¹è±¡å±æ€§çš„æ„Ÿè§‰ä¿¡æ¯ä¸å…³äºå¯¹è±¡é—´å…³ç³»çš„å…³è”ä¿¡æ¯ã€‚è™½ç„¶ç¥ç»æ³¨æ„åŠ›ä¸ºæ§åˆ¶å¯¹è±¡é—´æ„Ÿè§‰ä¿¡æ¯çš„æµåŠ¨æä¾›äº†å¼ºå¤§æœºåˆ¶ï¼Œä½†Transformerç¼ºä¹æ˜ç¡®è®¡ç®—æœºåˆ¶æ¥å¤„ç†å…³è”ä¿¡æ¯çš„è·¯ç”±å’Œå¤„ç†ã€‚ä¸ºè§£å†³æ­¤å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Transformeræ¶æ„çš„æ‰©å±•ç‰ˆæœ¬â€”â€”åŒæ³¨æ„åŠ›Transformerï¼ˆDATï¼‰ï¼Œå®ƒåŒ…å«ä¸¤ç§ä¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶ï¼šç”¨äºå¼•å¯¼æ„Ÿè§‰ä¿¡æ¯æµåŠ¨çš„æ„Ÿå®˜æ³¨æ„åŠ›ä¸ç”¨äºå¼•å¯¼å…³è”ä¿¡æ¯æµåŠ¨çš„æ–°å‹å…³è”æ³¨æ„åŠ›æœºåˆ¶ã€‚æˆ‘ä»¬å¯¹DATè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬ä»åˆæˆå…³ç³»åŸºå‡†æµ‹è¯•åˆ°å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡ï¼ˆå¦‚è¯­è¨€å»ºæ¨¡å’Œè§†è§‰å¤„ç†ï¼‰ç­‰å¤šæ ·åŒ–ä»»åŠ¡ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨Transformeræ¶æ„ä¸­æ•´åˆæ˜ç¡®çš„å…³è”è®¡ç®—æœºåˆ¶ï¼Œåœ¨æ•°æ®æ•ˆç‡å’Œå‚æ•°æ•ˆç‡æ–¹é¢å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…³ç³»æ¨ç†æ˜¯æ™ºèƒ½ç³»ç»Ÿçš„æ ¸å¿ƒï¼Œå¯¹ç¨³å¥ä¸”é«˜æ•ˆçš„æ•°æ®å½’çº³æ¦‚æ‹¬è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç¥ç»æ¶æ„å¦‚Transformeråœ¨å¤„ç†å…³ç³»æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°ä¸è¶³ã€‚</li>
<li>æœ¬æ–‡åŒºåˆ†äº†æ„Ÿè§‰ä¿¡æ¯ä¸å…³è”ä¿¡æ¯ä¸¤ç§ç±»å‹ã€‚</li>
<li>ç¥ç»æ³¨æ„åŠ›æœ‰åŠ©äºæ§åˆ¶æ„Ÿè§‰ä¿¡æ¯çš„æµåŠ¨ï¼Œä½†Transformerç¼ºä¹å¤„ç†å…³è”ä¿¡æ¯çš„æ˜ç¡®æœºåˆ¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»æ¶æ„â€”â€”åŒæ³¨æ„åŠ›Transformerï¼ˆDATï¼‰ï¼ŒåŒ…å«æ„Ÿå®˜æ³¨æ„åŠ›å’Œå…³è”æ³¨æ„åŠ›ä¸¤ç§æœºåˆ¶ã€‚</li>
<li>DATåœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå…¶åœ¨æ•°æ®æ•ˆç‡å’Œå‚æ•°æ•ˆç‡æ–¹é¢å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.16727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef619e22ecd5e3be2e3727333541fa7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6a2be5d61bf186841856a60f9be9e2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f0655504e552f212a9d07642aa1e561.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36aa137f1daec84e634ae1adb10f14ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d843dbad7644dd05add2fc59d4b5ff1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9f35788449f25c3ac391d6d19c4e11b0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-24  RAGentA Multi-Agent Retrieval-Augmented Generation for Attributed   Question Answering
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-81785035976daa414d4c9fa7bf86cc8c.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-24  Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models An Empirical Evaluation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29774.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
