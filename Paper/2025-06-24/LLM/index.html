<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-06-24  Confidence Scoring for LLM-Generated SQL in Supply Chain Data Extraction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c3545f79463c4933987b03be47936731.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    83 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-24-更新"><a href="#2025-06-24-更新" class="headerlink" title="2025-06-24 更新"></a>2025-06-24 更新</h1><h2 id="Confidence-Scoring-for-LLM-Generated-SQL-in-Supply-Chain-Data-Extraction"><a href="#Confidence-Scoring-for-LLM-Generated-SQL-in-Supply-Chain-Data-Extraction" class="headerlink" title="Confidence Scoring for LLM-Generated SQL in Supply Chain Data Extraction"></a>Confidence Scoring for LLM-Generated SQL in Supply Chain Data Extraction</h2><p><strong>Authors:Jiekai Ma, Yikai Zhao</strong></p>
<p>Large Language Models (LLMs) have recently enabled natural language interfaces that translate user queries into executable SQL, offering a powerful solution for non-technical stakeholders to access structured data. However, one of the limitation that LLMs do not natively express uncertainty makes it difficult to assess the reliability of their generated queries. This paper presents a case study that evaluates multiple approaches to estimate confidence scores for LLM-generated SQL in supply chain data retrieval. We investigated three strategies: (1) translation-based consistency checks; (2) embedding-based semantic similarity between user questions and generated SQL; and (3) self-reported confidence scores directly produced by the LLM. Our findings reveal that LLMs are often overconfident in their own outputs, which limits the effectiveness of self-reported confidence. In contrast, embedding-based similarity methods demonstrate strong discriminative power in identifying inaccurate SQL. </p>
<blockquote>
<p>大型语言模型（LLM）最近启用了自然语言接口，这些接口能够将用户查询翻译成可执行的SQL，为非技术利益相关者访问结构化数据提供了强大的解决方案。然而，LLM的一个局限性在于它们无法天生地表达不确定性，这使得评估它们生成的查询的可靠性变得困难。本文进行了一项案例研究，评估了多种方法来估算LLM生成的SQL的信心分数，在供应链数据检索中。我们调查了三种策略：（1）基于翻译的一致性检查；（2）基于嵌入的用户问题与生成的SQL之间的语义相似性；（3）由LLM直接产生的自我报告的信心分数。我们的研究结果表明，LLM通常对自己的输出过于自信，这限制了自我报告信心的有效性。相比之下，基于嵌入的相似性方法在识别不准确的SQL方面表现出很强的辨别力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17203v1">PDF</a> accepted by KDD workshop AI for Supply Chain 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）为用户查询提供可执行的SQL翻译，为非技术用户访问结构化数据提供了强大的解决方案。然而，LLM不原生表达不确定性，难以评估其生成查询的可靠性。本文评估了多种方法为LLM生成的SQL估算置信度分数，包括基于翻译的一致性检查、基于嵌入的用户问题与生成SQL的语义相似性以及LLM直接产生的自我报告置信度分数。研究发现，LLM对其输出常常过于自信，自我报告的置信度有效性有限，而基于嵌入的相似性方法在识别不准确SQL方面表现出强大的辨别力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs可实现用户查询的自然语言到可执行SQL的翻译，为非技术用户访问结构化数据提供便捷途径。</li>
<li>LLMs存在不确定性表达的限制，难以评估生成查询的可靠性。</li>
<li>评估LLM生成的SQL置信度的方法包括基于翻译的一致性检查、基于嵌入的语义相似性和LLM的自我报告置信度分数。</li>
<li>LLM对其输出常常过于自信，自我报告的置信度有效性受限。</li>
<li>基于嵌入的相似性方法在识别不准确的SQL方面表现出强大的辨别力。</li>
<li>LLM在供应链数据检索中的应用面临挑战，包括确保查询的准确性和可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17203">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e3f21617df665361ef6b0882d631352b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7460bfbe989d9ae106ac77db87da6780.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a3c236d6383b2dc51dc18a846d714af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac5efcfa96ca0a99bb104deef5cddc19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8a612c806771477d9a2e2f297866553.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05643a81335f94ae95f0616c6cda590a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ef224d57e9368778144b6030676b8c8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Detecting-LLM-Generated-Short-Answers-and-Effects-on-Learner-Performance"><a href="#Detecting-LLM-Generated-Short-Answers-and-Effects-on-Learner-Performance" class="headerlink" title="Detecting LLM-Generated Short Answers and Effects on Learner Performance"></a>Detecting LLM-Generated Short Answers and Effects on Learner Performance</h2><p><strong>Authors:Shambhavi Bhushan, Danielle R Thomas, Conrad Borchers, Isha Raghuvanshi, Ralph Abboud, Erin Gatz, Shivang Gupta, Kenneth Koedinger</strong></p>
<p>The increasing availability of large language models (LLMs) has raised concerns about their potential misuse in online learning. While tools for detecting LLM-generated text exist and are widely used by researchers and educators, their reliability varies. Few studies have compared the accuracy of detection methods, defined criteria to identify content generated by LLM, or evaluated the effect on learner performance from LLM misuse within learning. In this study, we define LLM-generated text within open responses as those produced by any LLM without paraphrasing or refinement, as evaluated by human coders. We then fine-tune GPT-4o to detect LLM-generated responses and assess the impact on learning from LLM misuse. We find that our fine-tuned LLM outperforms the existing AI detection tool GPTZero, achieving an accuracy of 80% and an F1 score of 0.78, compared to GPTZero’s accuracy of 70% and macro F1 score of 0.50, demonstrating superior performance in detecting LLM-generated responses. We also find that learners suspected of LLM misuse in the open response question were more than twice as likely to correctly answer the corresponding posttest MCQ, suggesting potential misuse across both question types and indicating a bypass of the learning process. We pave the way for future work by demonstrating a structured, code-based approach to improve LLM-generated response detection and propose using auxiliary statistical indicators such as unusually high assessment scores on related tasks, readability scores, and response duration. In support of open science, we contribute data and code to support the fine-tuning of similar models for similar use cases. </p>
<blockquote>
<p>随着大型语言模型（LLM）的日益普及，人们越来越担心其在在线学习中可能被滥用。尽管存在检测LLM生成文本的工具，研究者和教育工作者也广泛使用这些工具，但它们的可靠性各不相同。很少有研究比较检测方法的准确性，定义识别LLM生成内容的标准，或评估LLM在学习中的滥用对学习者表现的影响。在本研究中，我们将开放回应中的LLM生成文本定义为由任何LLM产生且未经转述或润色的文本，由人类编码员进行评估。然后，我们微调GPT-4o来检测LLM生成的回应，并评估LLM滥用对学习的影响。我们发现，我们微调后的LLM优于现有的AI检测工具GPTZero，准确率达到了80%，F1分数为0.78，而GPTZero的准确率为70%，宏观F1分数为0.50，显示出在检测LLM生成回应方面的卓越性能。我们还发现，在开放回应问题中被怀疑滥用LLM的学习者正确回答后续测试选择题的可能性是未滥用者的两倍以上，这表明两种类型的问题中都可能存在滥用情况，并表明可能绕过了学习过程。我们展示了一种结构化的、基于代码的方法来改进LLM生成的响应检测，并提出使用辅助统计指标，如相关任务的异常高评分、可读性评分和响应持续时间等。我们支持开放科学，提供数据和代码，以支持类似用例的类似模型的微调。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17196v1">PDF</a> Accepted for publication at the 19th European Conference on   Technology Enhanced Learning (ECTEL 2025). This is the author’s accepted   manuscript</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的普及引发了对其在线学习环境中潜在误用的关注。尽管存在检测LLM生成文本的工具，但其可靠性参差不齐。本研究定义了LLM生成文本的标准，即任何未经改编或完善的LLM生成内容。通过微调GPT-4o来检测LLM生成的响应，并评估LLM误用对学习的影响。研究发现，微调后的LLM检测性能优于现有工具GPTZero，准确率高达80%，F1分数为0.78，显示出更高的检测准确性。此外，发现疑似使用LLM的开放性问题回答者在随后的多项选择题测试中正确率更高，表明可能存在绕过学习过程的情况。本研究为未来改进LLM生成响应检测提供了结构化代码方法，并提出了使用辅助统计指标的建议。同时，为了支持开放科学，本研究提供数据和代码支持类似用例的模型微调。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的普及引发了对其在线学习环境误用的关注。</li>
<li>研究定义了LLM生成文本的标准为未经改编或完善的LLM内容。</li>
<li>通过微调GPT-4o检测LLM生成的响应，表现出较高的检测准确性。</li>
<li>相比现有工具GPTZero，微调后的LLM检测性能更优。</li>
<li>学习者在疑似使用LLM的开放性问题回答后在多项选择题测试中表现更佳，暗示可能绕过学习过程。</li>
<li>本研究为未来改进LLM响应检测提供了结构化代码方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17196">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c371411f8bad41b21063f4b4b7918292.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b2bf2a2e021f0386ca64eb9eef2230b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec9451ef3f6473b9ddb9e87f22d736c3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Prompting-Obscures-Hallucination-Cues-in-Large-Language-Models-An-Empirical-Evaluation"><a href="#Chain-of-Thought-Prompting-Obscures-Hallucination-Cues-in-Large-Language-Models-An-Empirical-Evaluation" class="headerlink" title="Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models: An Empirical Evaluation"></a>Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models: An Empirical Evaluation</h2><p><strong>Authors:Jiahao Cheng, Tiancheng Su, Jia Yuan, Guoxiu He, Jiawei Liu, Xinqi Tao, Jingwen Xie, Huaxia Li</strong></p>
<p>Large Language Models (LLMs) often exhibit \textit{hallucinations}, generating factually incorrect or semantically irrelevant content in response to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by encouraging step-by-step reasoning, but its impact on hallucination detection remains underexplored. To bridge this gap, we conduct a systematic empirical evaluation. We begin with a pilot experiment, revealing that CoT reasoning significantly affects the LLM’s internal states and token probability distributions. Building on this, we evaluate the impact of various CoT prompting methods on mainstream hallucination detection methods across both instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three key dimensions: changes in hallucination score distributions, variations in detection accuracy, and shifts in detection confidence. Our findings show that while CoT prompting helps reduce hallucination frequency, it also tends to obscure critical signals used for detection, impairing the effectiveness of various detection methods. Our study highlights an overlooked trade-off in the use of reasoning. Code is publicly available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/cot-hallu-detect">https://anonymous.4open.science/r/cot-hallu-detect</a>. </p>
<blockquote>
<p>大型语言模型（LLM）常常出现“幻觉”，即生成与提示内容事实上不正确或语义上不相关的内容。链式思维（CoT）提示可以通过鼓励逐步推理来缓解幻觉问题，但其对幻觉检测的影响尚未得到充分研究。为了弥补这一空白，我们进行了系统的实证研究。我们首先进行了一项试点实验，发现链式思维推理对LLM的内部状态和符号概率分布产生了显著影响。在此基础上，我们评估了多种链式思维提示方法对主流幻觉检测方法的冲击，涉及指令调优和面向推理的LLM。具体来说，我们考察了三个方面：幻觉分数分布的变化、检测准确度的变化以及检测信心的转变。我们的研究发现，虽然链式思维提示有助于减少幻觉频率，但它也倾向于掩盖用于检测的关键信号，从而损害了各种检测方法的有效性。我们的研究指出了在运用推理时一个被忽视的权衡。相关代码已公开在：<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/cot-hallu-detect%E3%80%82">https://anonymous.4open.science/r/cot-hallu-detect。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17088v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）会出现幻觉现象，即生成与提示内容事实错误或语义不相关的内容。链式思维（CoT）提示可以通过鼓励逐步推理来减轻幻觉现象，但其对幻觉检测的影响尚未得到充分探索。本研究通过系统实证研究填补了这一空白。初步实验表明，CoT推理对LLM的内部状态和令牌概率分布产生了显著影响。在此基础上，我们评估了多种CoT提示方法对主流幻觉检测方法的影响，涉及指令调优和面向推理的LLM。研究发现，CoT提示有助于减少幻觉频率，但也会掩盖用于检测的关键信号，从而影响各种检测方法的性能。本研究揭示了推理使用中的权衡问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）会出现幻觉现象，生成与提示不符的内容。</li>
<li>链式思维（CoT）提示能够通过鼓励逐步推理来减轻LLM的幻觉现象。</li>
<li>CoT推理对LLM的内部状态和令牌概率分布有显著影响。</li>
<li>CoT提示方法对主流幻觉检测方法的性能有影响。</li>
<li>CoT提示有助于减少幻觉频率，但可能掩盖用于检测的关键信号。</li>
<li>在使用推理时存在权衡问题，需要综合考虑幻觉检测和推理效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17088">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-48a09b4bec85782fca355316fec8dccc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-37e85487e3b3d22b59e1eb59ebf87266.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-796a4f0508590758ae8b352cb74ca8f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8470ed37decea58e3abb2b529d09897a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30fe41e44e05c928711f130a781cb57a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Tower-Bridging-Generality-and-Translation-Specialization-in-Multilingual-LLMs"><a href="#Tower-Bridging-Generality-and-Translation-Specialization-in-Multilingual-LLMs" class="headerlink" title="Tower+: Bridging Generality and Translation Specialization in   Multilingual LLMs"></a>Tower+: Bridging Generality and Translation Specialization in   Multilingual LLMs</h2><p><strong>Authors:Ricardo Rei, Nuno M. Guerreiro, José Pombal, João Alves, Pedro Teixeirinha, Amin Farajian, André F. T. Martins</strong></p>
<p>Fine-tuning pretrained LLMs has been shown to be an effective strategy for reaching state-of-the-art performance on specific tasks like machine translation. However, this process of adaptation often implies sacrificing general-purpose capabilities, such as conversational reasoning and instruction-following, hampering the utility of the system in real-world applications that require a mixture of skills. In this paper, we introduce Tower+, a suite of models designed to deliver strong performance across both translation and multilingual general-purpose text capabilities. We achieve a Pareto frontier between translation specialization and multilingual general-purpose capabilities by introducing a novel training recipe that builds on Tower (Alves et al., 2024), comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. At each stage of training, we carefully generate and curate data to strengthen performance on translation as well as general-purpose tasks involving code generation, mathematics problem solving, and general instruction-following. We develop models at multiple scales: 2B, 9B, and 72B. Our smaller models often outperform larger general-purpose open-weight and proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we introduce for evaluating both translation and instruction-following. Our findings highlight that it is possible to rival frontier models in general capabilities, while optimizing for specific business domains, such as translation and localization. </p>
<blockquote>
<p>调整预训练LLM已被证明是达到机器翻译等特定任务的最先进性能的有效策略。然而，这一适应过程往往意味着牺牲通用能力，如对话推理和指令遵循，从而妨碍了系统在需要混合技能的实际应用中的实用性。在本文中，我们介绍了Tower+，一套旨在同时在翻译和跨语言通用文本能力方面实现强劲性能的模型。我们通过引入一种新的训练配方，在Tower（Alves等人，2024）的基础上，实现了翻译专业化与跨语言通用能力之间的帕累托前沿。该训练配方包括继续预训练、监督微调、偏好优化以及使用可验证奖励的强化学习。在训练的每个阶段，我们都会精心生成和筛选数据，以加强翻译以及涉及代码生成、数学问题解决和一般指令遵循的通用任务的性能。我们在多个规模上开发模型：2B、m 9B和72B。我们的小型模型通常表现优于大型通用开放权重和专有LLM（例如Llama 3.3 70B、GPT-4o）。我们的大型模型在高资源语言翻译方面达到业内最佳性能，并在我们引入的多语种Arena Hard评估和IF-MT基准测试中取得顶尖结果。我们的研究结果表明，在针对特定业务领域进行优化时，完全有可能赶超通用能力的尖端模型，如翻译和本地化等领域。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17080v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Tower+模型系列，旨在实现翻译和多语种通用文本能力的平衡。通过引入新的训练策略，包括持续预训练、监督微调、偏好优化和可验证奖励的强化学习，达到翻译专业化和多语种通用能力之间的帕累托前沿。开发不同规模的模型，包括2B、9B和72B，并在翻译和通用任务上表现出色。最大的模型在高资源语言翻译方面达到最佳性能，并在多语种Arena Hard评估和新的IF-MT基准测试中取得顶尖结果。研究结果表明，可以在优化特定业务领域（如翻译和本地化）的同时，与前沿模型在通用能力上相抗衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Tower+模型系列旨在平衡翻译和多种语言通用文本能力。</li>
<li>通过结合多种训练策略，包括持续预训练、监督微调等，实现帕累托最优前沿。</li>
<li>模型在多个规模上表现优异，包括小型、中型和大型模型。</li>
<li>在翻译和通用任务上表现出色，包括代码生成、数学问题解决和指令遵循等。</li>
<li>最大模型在高资源语言翻译上表现最佳，并在多语种评估中取得顶尖结果。</li>
<li>引入新的基准测试IF-MT，用于评估翻译和指令遵循能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-938187e11a954f6183f36eab58d7d3a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0cf2b924c33bc7987b47f277cebe22f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a52d10368cb4bebc80ade7724feb278.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a41e9aab2c6b929e9f64f064645d83a0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Simultaneous-Translation-with-Offline-Speech-and-LLM-Models-in-CUNI-Submission-to-IWSLT-2025"><a href="#Simultaneous-Translation-with-Offline-Speech-and-LLM-Models-in-CUNI-Submission-to-IWSLT-2025" class="headerlink" title="Simultaneous Translation with Offline Speech and LLM Models in CUNI   Submission to IWSLT 2025"></a>Simultaneous Translation with Offline Speech and LLM Models in CUNI   Submission to IWSLT 2025</h2><p><strong>Authors:Dominik Macháček, Peter Polák</strong></p>
<p>This paper describes Charles University submission to the Simultaneous Speech Translation Task of the IWSLT 2025. We cover all four language pairs with a direct or cascade approach. The backbone of our systems is the offline Whisper speech model, which we use for both translation and transcription in simultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We further improve the performance by prompting to inject in-domain terminology, and we accommodate context. Our cascaded systems further use EuroLLM for unbounded simultaneous translation. Compared to the Organizers’ baseline, our systems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on English to German, Chinese and Japanese on the development sets. Additionally, we also propose a new enhanced measure of speech recognition latency. </p>
<blockquote>
<p>本文描述了查尔斯大学对IWSLT 2025同步语音识别翻译任务的提交内容。我们采用直接或级联的方法覆盖所有四种语言对。我们系统的核心是离线Whisper语音模型，我们在同步模式下进行翻译和转录时使用最先进的同步策略AlignAtt。通过提示注入领域专业术语，我们进一步提高了性能，并适应了上下文。我们的级联系统还进一步使用EuroLLM进行无界同步翻译。与组织者设定的基线相比，我们的系统在开发集上捷克语到英语的BLEU得分提高了2分，英语到德语、中文和日语的BLEU得分提高了13到22分。此外，我们还提出了一种新的改进语音识别延迟的度量方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17077v1">PDF</a> IWSLT 2025</p>
<p><strong>Summary</strong></p>
<p>本论文介绍了Charles大学在IWSLT 2025同步语音识别任务中的表现。采用四种语言配对方法，利用前沿的同步策略AlignAtt实现同时翻译和转录功能。论文进一步提升了系统性能，通过在系统内提示专业领域术语以适应语境。采用EuroLLM模型进行无界同步翻译。相较于主办方基准线，该系统在捷克语至英语翻译方面提升了2个BLEU点，在英语至德语、中文和日语方面在开发集上提升了13至22个BLEU点。此外，还提出了一种改进的语音识别延迟度量方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Charles大学在IWSLT 2025的同步语音识别任务中有所贡献。</li>
<li>该研究采用四种语言配对方法，利用前沿的同步策略AlignAtt进行翻译和转录。</li>
<li>系统性能通过提示专业领域术语和适应语境得到了进一步提升。</li>
<li>采用EuroLLM模型进行无界同步翻译是该研究的亮点之一。</li>
<li>与主办方基准线相比，该系统在多种语言对的翻译任务中取得了显著成果。</li>
<li>研究提出了一种改进的语音识别延迟度量方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17077">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-799366d1360e8d86ed3a73912a3a3ffc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfd32ed6ef97092d778005d88b869361.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c68dbede2536384458138ecc052758c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e964ed6a0684aaa0d13fa63be341cd69.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0505ee424bfaa03e228eadc2efc68857.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="From-Concepts-to-Components-Concept-Agnostic-Attention-Module-Discovery-in-Transformers"><a href="#From-Concepts-to-Components-Concept-Agnostic-Attention-Module-Discovery-in-Transformers" class="headerlink" title="From Concepts to Components: Concept-Agnostic Attention Module Discovery   in Transformers"></a>From Concepts to Components: Concept-Agnostic Attention Module Discovery   in Transformers</h2><p><strong>Authors:Jingtong Su, Julia Kempe, Karen Ullrich</strong></p>
<p>Transformers have achieved state-of-the-art performance across language and vision tasks. This success drives the imperative to interpret their internal mechanisms with the dual goals of enhancing performance and improving behavioral control. Attribution methods help advance interpretability by assigning model outputs associated with a target concept to specific model components. Current attribution research primarily studies multi-layer perceptron neurons and addresses relatively simple concepts such as factual associations (e.g., Paris is located in France). This focus tends to overlook the impact of the attention mechanism and lacks a unified approach for analyzing more complex concepts. To fill these gaps, we introduce Scalable Attention Module Discovery (SAMD), a concept-agnostic method for mapping arbitrary, complex concepts to specific attention heads of general transformer models. We accomplish this by representing each concept as a vector, calculating its cosine similarity with each attention head, and selecting the TopK-scoring heads to construct the concept-associated attention module. We then propose Scalar Attention Module Intervention (SAMI), a simple strategy to diminish or amplify the effects of a concept by adjusting the attention module using only a single scalar parameter. Empirically, we demonstrate SAMD on concepts of varying complexity, and visualize the locations of their corresponding modules. Our results demonstrate that module locations remain stable before and after LLM post-training, and confirm prior work on the mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on HarmBench (+72.7%) by diminishing “safety” and improve performance on the GSM8K benchmark (+1.6%) by amplifying “reasoning”. Lastly, we highlight the domain-agnostic nature of our approach by suppressing the image classification accuracy of vision transformers on ImageNet. </p>
<blockquote>
<p>Transformer模型在语言与视觉任务上取得了最先进的性能表现。这一成功促使我们迫切需要对内部机制进行解读，旨在提高性能和改善行为控制。归因方法有助于推动模型的可解释性，通过将与目标概念相关联的模型输出分配给特定的模型组件。目前的归因研究主要集中在多层感知器神经元上，并处理相对简单的概念，如事实关联（例如，巴黎位于法国）。这种关注往往忽视了注意力机制的影响，缺乏分析更复杂概念的统一方法。为了填补这些空白，我们引入了可扩展的注意力模块发现（SAMD），这是一种概念无关的方法，可将任意复杂概念映射到通用Transformer模型中的特定注意力头。我们通过将每个概念表示为向量、计算其与每个注意力头的余弦相似度并选择得分最高的头部来构建与概念相关的注意力模块来实现这一点。接下来，我们提出了简单的Scalar Attention Module Intervention（SAMI）策略，通过仅使用一个标量参数来调整注意力模块来减弱或增强概念的影响。我们通过实证在具有不同复杂性的概念上展示了SAMD，并可视化了其对应模块的分布位置。结果表明，模块位置在大型语言模型微调前后的位置保持不变，并证实了关于大型语言模型多语言能力的机制的前期研究。通过SAMI，我们在HarmBench上通过减弱“安全”概念实现了突破（+72.7%），在GSM8K基准测试中通过增强“推理”能力提高了性能（+1.6%）。最后，我们通过抑制视觉Transformer在ImageNet上的图像分类准确率来强调我们方法的领域无关性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17052v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>Transformer模型在语言和视觉任务上取得了最先进的性能，推动了对其内部机制的解释具有增强性能和改善行为控制的目标。归因方法有助于通过将与目标概念相关的模型输出分配给特定的模型组件来提高解释性。当前归因研究主要集中在多层感知器神经元上，并处理相对简单的概念，如事实关联（例如，巴黎位于法国）。这种关注往往忽略了注意力机制的影响，缺乏对更复杂的分析统一的方法。为了填补这些空白，我们引入了可扩展的注意力模块发现（SAMD），这是一种通用的方法，可将任意复杂概念映射到通用Transformer模型的特定注意力头。我们通过将每个概念表示为向量、计算其与每个注意力头的余弦相似度并选择得分最高的头部来构建概念相关的注意力模块来实现这一点。然后，我们提出了简单的Scalar Attention Module Intervention（SAMI）策略，通过调整注意力模块使用单个标量参数来减少或放大概念的影响。我们通过实验演示了SAMD在具有不同复杂性的概念上的作用，并可视化其对应模块的地理位置。结果表明，模块位置在LLM训练后保持稳定，并证实了关于LLM多语言机械原理的先前工作。通过SAMI，我们在HarmBench（+72.7%）上通过减少“安全”因素实现突破，并在GSM8K基准测试（+1.6%）上通过加强“推理”能力提高性能。最后，我们通过抑制图像分类器的准确性来突出我们方法的领域无关性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Transformer模型在语言和视觉任务上表现出卓越性能，引发了对解释其内部机制的需求，旨在提高性能和改善行为控制。</li>
<li>当前归因研究主要集中在简单概念上，忽略了注意力机制的影响和更复杂的分析方法。</li>
<li>引入SAMD方法：通过映射任意复杂概念到特定的注意力头，提高了Transformer模型的解释性。</li>
<li>SAMD方法能够实现概念相关的注意力模块可视化，模块位置在LLM训练后保持稳定。</li>
<li>通过SAMI策略，可以调整注意力模块来增强或削弱特定概念的影响，从而提高模型性能。</li>
<li>实验结果表明，SAMD和SAMI策略在多种任务上有效，包括语言理解和图像分类。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17052">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c788fdbf45f0483242ad62bb5152435a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f81f833962d05589bf63c11e7db5864.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b7fda66ea5682a5edde8e0d82001084.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MUCAR-Benchmarking-Multilingual-Cross-Modal-Ambiguity-Resolution-for-Multimodal-Large-Language-Models"><a href="#MUCAR-Benchmarking-Multilingual-Cross-Modal-Ambiguity-Resolution-for-Multimodal-Large-Language-Models" class="headerlink" title="MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for   Multimodal Large Language Models"></a>MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for   Multimodal Large Language Models</h2><p><strong>Authors:Xiaolong Wang, Zhaolu Kang, Wangyuxuan Zhai, Xinyue Lou, Yunghwei Lai, Ziyue Wang, Yawen Wang, Kaiyu Huang, Yile Wang, Peng Li, Yang Liu</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated significant advances across numerous vision-language tasks. Due to their strong image-text alignment capability, MLLMs can effectively understand image-text pairs with clear meanings. However, effectively resolving the inherent ambiguities in natural language and visual contexts remains challenging. Existing multimodal benchmarks typically overlook linguistic and visual ambiguities, relying mainly on unimodal context for disambiguation and thus failing to exploit the mutual clarification potential between modalities. To bridge this gap, we introduce MUCAR, a novel and challenging benchmark designed explicitly for evaluating multimodal ambiguity resolution across multilingual and cross-modal scenarios. MUCAR includes: (1) a multilingual dataset where ambiguous textual expressions are uniquely resolved by corresponding visual contexts, and (2) a dual-ambiguity dataset that systematically pairs ambiguous images with ambiguous textual contexts, with each combination carefully constructed to yield a single, clear interpretation through mutual disambiguation. Extensive evaluations involving 19 state-of-the-art multimodal models–encompassing both open-source and proprietary architectures–reveal substantial gaps compared to human-level performance, highlighting the need for future research into more sophisticated cross-modal ambiguity comprehension methods, further pushing the boundaries of multimodal reasoning. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）在众多的视觉语言任务中取得了显著的进步。由于它们具有强大的图像文本对齐能力，MLLMs可以有效地理解意义明确的图像文本对。然而，有效解决自然语言和视觉上下文中的固有歧义仍然是一个挑战。现有的多模态基准测试通常忽略了语言和视觉的歧义，主要依赖于单模态上下文进行歧义解析，从而没有充分利用模态之间的相互澄清潜力。为了弥补这一差距，我们引入了MUCAR，这是一个专门为评估多语言和多模态场景中的多模态歧义解析而设计的新型且具有挑战性的基准测试。MUCAR包括：（1）一种多语言数据集，其中模糊的文本表达通过相应的视觉上下文唯一解决；（2）一种双重歧义数据集，系统地配对具有模糊性的图像和文本上下文，每种组合都经过精心构建，以通过相互解析得出一个清晰、明确的解释。涉及19个最先进的多模态模型的广泛评估——包括开源和专有架构——与人类水平性能相比存在巨大差距，这突显了未来研究更复杂跨模态歧义理解方法的必要性，进一步推动多模态推理的边界。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17046v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MLLMs在跨视觉语言任务方面取得了显著进展，具备强大的图文对齐能力，能清晰理解图像和文字的意义。然而，解决自然语言和视觉语境中的固有歧义仍是挑战。现有的多模式基准测试忽略了语言和视觉的歧义性，仅依赖单模态上下文进行解歧义，无法利用模态之间的互释潜力。为解决这一差距，我们推出了MUCAR，这是一个专门用于评估多语言场景下多模式歧义解决能力的新颖基准测试。MUCAR包括：（1）一种多语言数据集，其中模糊文本表达通过相应的视觉上下文进行唯一解析；（2）一种双模糊数据集，系统地配对具有模糊性的图像和文本上下文，每个组合都经过精心设计，以通过相互解析产生单一清晰的解释。对多个先进的多模式模型的广泛评估表明，与人类性能相比仍存在显著差距，凸显了对未来研究更复杂跨模态歧义理解方法的需要，进一步推动多模式推理的边界。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MLLMs在视觉语言任务上取得显著进步，擅长理解图像和文字的意义。</li>
<li>解决自然语言和视觉语境中的歧义性是MLLMs面临的挑战。</li>
<li>现有基准测试忽略了语言和视觉的歧义性，需要新的基准测试来评估多模式歧义解决能力。</li>
<li>MUCAR是一个专门用于评估多语言场景下多模式歧义解决能力的新颖基准测试。</li>
<li>MUCAR包括多语言和双模糊数据集，用于评估模型在特定场景下的表现。</li>
<li>对多个先进模型的评估显示，与人类性能相比仍存在显著差距。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17046">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-662f41d63b55b1210ec34e6019fe6e50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-694b2fb072f671dba320b6b3aabb1d59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0607ca5f99430ea0bab9842ae2d157ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f41828714d3425787b788b12943ecd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6db38988cf07a96fb498a5d97bebb2ac.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Latent-Concept-Disentanglement-in-Transformer-based-Language-Models"><a href="#Latent-Concept-Disentanglement-in-Transformer-based-Language-Models" class="headerlink" title="Latent Concept Disentanglement in Transformer-based Language Models"></a>Latent Concept Disentanglement in Transformer-based Language Models</h2><p><strong>Authors:Guan Zhe Hong, Bhavya Vasudeva, Vatsal Sharan, Cyrus Rashtchian, Prabhakar Raghavan, Rina Panigrahy</strong></p>
<p>When large language models (LLMs) use in-context learning (ICL) to solve a new task, they seem to grasp not only the goal of the task but also core, latent concepts in the demonstration examples. This begs the question of whether transformers represent latent structures as part of their computation or whether they take shortcuts to solve the problem. Prior mechanistic work on ICL does not address this question because it does not sufficiently examine the relationship between the learned representation and the latent concept, and the considered problem settings often involve only single-step reasoning. In this work, we examine how transformers disentangle and use latent concepts. We show that in 2-hop reasoning tasks with a latent, discrete concept, the model successfully identifies the latent concept and does step-by-step concept composition. In tasks parameterized by a continuous latent concept, we find low-dimensional subspaces in the representation space where the geometry mimics the underlying parameterization. Together, these results refine our understanding of ICL and the representation of transformers, and they provide evidence for highly localized structures in the model that disentangle latent concepts in ICL tasks. </p>
<blockquote>
<p>当大型语言模型（LLM）使用上下文学习（ICL）来解决新任务时，它们似乎不仅掌握了任务的目标，还掌握了演示示例中的核心、潜在概念。这引发了一个问题，即变压器是将其代表潜在结构作为计算的一部分，还是采取了解决问题的捷径。之前关于ICL的机制性工作并没有解决这一问题，因为它没有充分检查学到的表现和潜在概念之间的关系，而且所考虑的问题设置通常只涉及单步推理。在这项工作中，我们研究了变压器如何解开和利用潜在概念。我们展示了在具有潜在离散概念的2跳推理任务中，模型成功地识别了潜在概念，并进行了逐步的概念组合。在由连续潜在概念参数化的任务中，我们在表征空间中找到低维子空间，其几何形状模仿了潜在参数化。总的来说，这些结果改进了我们对ICL和变压器表示的理解，并为模型中高度局部化的结构提供了证据，这些结构在ICL任务中解开潜在概念。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16975v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）利用上下文学习（ICL）解决新任务时，不仅能理解任务目标，还能把握演示例子中的核心潜在概念。本文探讨的是，变压器模型是否将潜在结构作为计算的一部分进行表示，还是采取捷径来解决问题。过去对ICL的机制性工作并没有充分探讨这个问题，因为它没有深入研究学到的表示和潜在概念之间的关系，且考虑的问题设置通常只涉及单步推理。本文研究变压器如何解开和利用潜在概念。在具有潜在离散概念的2跳推理任务中，模型能够成功识别潜在概念并进行逐步概念组合。在由连续潜在概念参数化的任务中，我们发现表示空间中的低维子空间，其几何结构模拟了潜在的参数化。这些结果深化了我们对ICL和变压器表示的理解，并为模型中存在高度局部化的结构提供了证据，这些结构在ICL任务中解开潜在概念。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）通过上下文学习（ICL）不仅能理解任务目标，还能把握演示例子中的核心潜在概念。</li>
<li>变压器模型在解决新任务时可能解开并利用潜在结构。</li>
<li>现有关于ICL的机制性工作未能充分探讨学到的表示和潜在概念之间的关系。</li>
<li>在2跳推理任务中，模型能够识别潜在离散概念并进行逐步组合。</li>
<li>在连续潜在概念参数化的任务中，模型的表示空间存在低维子空间，其结构与潜在参数相符。</li>
<li>这些发现深化了我们对ICL和变压器表示的理解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16975">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c4275827855df24c57003a4c1a3ea409.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-787ec72d04b40f160ff22017daee6fbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-358165b10b1a1968a8a73c2886f55686.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Optimizing-MoE-Routers-Design-Implementation-and-Evaluation-in-Transformer-Models"><a href="#Optimizing-MoE-Routers-Design-Implementation-and-Evaluation-in-Transformer-Models" class="headerlink" title="Optimizing MoE Routers: Design, Implementation, and Evaluation in   Transformer Models"></a>Optimizing MoE Routers: Design, Implementation, and Evaluation in   Transformer Models</h2><p><strong>Authors:Daniel Fidel Harvey, George Weale, Berk Yilmaz</strong></p>
<p>Mixture of Experts (MoE) architectures increase large language model scalability, yet their performance depends on the router module that moves tokens to specialized experts. Bad routing can load imbalance and reduced accuracy. This project designed and implemented different router architectures within Transformer models to fix these limitations. We experimented with six distinct router variants Linear, Attention, Multi-Layer Perceptron (MLP), Hybrid, Hash, and our new MLP-Hadamard. We characterized these routers using BERT and the Qwen1.5-MoE model, looking at parameter efficiency, inference latency, routing entropy, and expert utilization patterns. Our evaluations showed distinct trade-offs: Linear routers offer speed, while MLP and Attention routers provide greater expressiveness. The MLP-Hadamard router shows a unique capability for structured, sparse routing. We successfully replaced and fine-tuned custom routers within the complex, quantized Qwen1.5-MoE model. This work provides a comparative analysis of MoE router designs and offers insights into optimizing their performance for efficient and effective large-scale model deployment. </p>
<blockquote>
<p>专家混合（MoE）架构提高了大规模语言模型的可扩展性，但其性能取决于将令牌移动到专业专家的路由器模块。不良的路由会导致负载不均衡和精度下降。此项目在Transformer模型中设计和实现了不同的路由器架构，以解决这些限制。我们试验了六种不同的路由器变体：Linear、Attention、多层感知器（MLP）、Hybrid、Hash以及我们新的MLP-Hadamard。我们使用BERT和Qwen1.5-MoE模型对这些路由器进行了表征，关注参数效率、推理延迟、路由熵和专家利用模式。我们的评估显示了明显的权衡：Linear路由器提供速度，而MLP和Attention路由器提供更大的表现力。MLP-Hadamard路由器显示出结构化和稀疏路由的独特能力。我们成功地在复杂的量化Qwen1.5-MoE模型中替换和微调了自定义路由器。这项工作提供了对MoE路由器设计的比较分析，并深入了解了如何优化其性能，以实现高效和有效的大规模模型部署。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16419v1">PDF</a> All authors contributed equally. 11 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>本文研究了Mixture of Experts（MoE）架构中的路由器模块设计，针对大型语言模型的扩展性进行了优化。文章介绍了六种不同的路由器架构，包括Linear、Attention、Multi-Layer Perceptron（MLP）、Hybrid、Hash以及新的MLP-Hadamard路由器。通过实验评估，文章分析了这些路由器的参数效率、推理延迟、路由熵和专家利用模式，并探讨了各自的特点和权衡。研究成功地在复杂的量化Qwen1.5-MoE模型中替换和微调了自定义路由器，为MoE路由器设计提供了比较分析，并为大型模型的性能优化和有效部署提供了见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MoE架构提高了大型语言模型的扩展性，但性能取决于路由器模块的设计。</li>
<li>路由器负责将令牌分配给专家，不良的路由可能导致负载不均衡和准确性下降。</li>
<li>研究设计了六种不同的路由器架构，包括Linear、Attention、MLP、Hybrid、Hash和MLP-Hadamard路由器。</li>
<li>通过实验评估，文章分析了路由器的参数效率、推理延迟等特性。</li>
<li>Linear路由器提供速度优势，而MLP和Attention路由器具有更高的表达能力。</li>
<li>MLP-Hadamard路由器展现出结构化、稀疏路由的独特能力。</li>
<li>文章成功地在复杂的量化Qwen1.5-MoE模型中替换和微调了自定义路由器，为MoE路由器设计的优化提供了重要参考。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16419">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-407aa4a16c1d8f7f0db9940b64a83af8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de5f613de5e6bc033f58dfee858c1eda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-774ec15a2c8258a1c04f8cd98f75be43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17835ef7ca486344ecb3f665acca5e57.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CapsDT-Diffusion-Transformer-for-Capsule-Robot-Manipulation"><a href="#CapsDT-Diffusion-Transformer-for-Capsule-Robot-Manipulation" class="headerlink" title="CapsDT: Diffusion-Transformer for Capsule Robot Manipulation"></a>CapsDT: Diffusion-Transformer for Capsule Robot Manipulation</h2><p><strong>Authors:Xiting He, Mingwu Su, Xinqi Jiang, Long Bai, Jiewen Lai, Hongliang Ren</strong></p>
<p>Vision-Language-Action (VLA) models have emerged as a prominent research area, showcasing significant potential across a variety of applications. However, their performance in endoscopy robotics, particularly endoscopy capsule robots that perform actions within the digestive system, remains unexplored. The integration of VLA models into endoscopy robots allows more intuitive and efficient interactions between human operators and medical devices, improving both diagnostic accuracy and treatment outcomes. In this work, we design CapsDT, a Diffusion Transformer model for capsule robot manipulation in the stomach. By processing interleaved visual inputs, and textual instructions, CapsDT can infer corresponding robotic control signals to facilitate endoscopy tasks. In addition, we developed a capsule endoscopy robot system, a capsule robot controlled by a robotic arm-held magnet, addressing different levels of four endoscopy tasks and creating corresponding capsule robot datasets within the stomach simulator. Comprehensive evaluations on various robotic tasks indicate that CapsDT can serve as a robust vision-language generalist, achieving state-of-the-art performance in various levels of endoscopy tasks while achieving a 26.25% success rate in real-world simulation manipulation. </p>
<blockquote>
<p>视觉-语言-动作（VLA）模型已经成为一个突出的研究领域，在各种应用中展示了巨大的潜力。然而，它们在内镜机器人，尤其是在消化系统内执行动作的微型内镜机器人的性能尚未被探索。VLA模型与内镜机器人的集成允许人类操作者与医疗设备之间进行更直观和高效的交互，提高了诊断准确性和治疗效果。在这项工作中，我们设计了CapsDT，这是一种用于胃内微型机器人操作的扩散变换模型。通过处理交替的视觉输入和文本指令，CapsDT可以推断出相应的机器人控制信号，以促进内窥镜任务。此外，我们开发了一个微型内镜机器人系统，该系统由机械臂持有的磁铁控制的微型机器人，解决了四个不同级别的内窥镜任务，并在胃模拟器内创建了相应的微型机器人数据集。对各种机器人任务的全面评估表明，CapsDT可以作为一个强大的视觉语言专家，在不同级别的内窥镜任务中达到最先进的性能，同时在现实世界的模拟操作中实现了26.25%的成功率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16263v1">PDF</a> IROS 2025</p>
<p><strong>摘要</strong><br>在消化道内窥镜胶囊机器人等领域，跨视觉-语言-行动（VLA）模型展现出巨大潜力，但其在内窥镜机器人方面的应用尚未被探索。本研究设计了一种名为CapsDT的扩散变换模型，用于胃内胶囊机器人的操控。该模型能够处理交替的视觉输入和文字指令，推断出相应的机器人控制信号，促进内窥镜任务的完成。此外，研究团队还开发了一套胶囊内窥镜机器人系统，该系统通过机械臂控制的磁铁控制胶囊机器人，针对四种不同难度的内窥镜任务创建了相应的数据集，并在胃模拟器中进行了模拟操控。经过综合评估表明，CapsDT能够作为一种强大的跨视觉-语言领域的专家系统，在内窥镜任务的不同层级实现最优性能，并在模拟操控中达到26.25%的成功率。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>VLA模型在内窥镜胶囊机器人领域具有显著潜力。</li>
<li>研究团队提出了CapsDT模型，该模型能够处理视觉和文字指令以控制机器人。</li>
<li>研究团队开发了一套胶囊内窥镜机器人系统及其相应的数据集。</li>
<li>CapsDT模型在内窥镜任务的不同层级实现最优性能。</li>
<li>CapsDT模型在模拟环境中的操控成功率为26.25%。</li>
<li>CapsDT模型的引入有助于增强内窥镜任务中人机互动的直观性和效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16263">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-fc8f790fe5f530d8130a686cf3a44175.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95438ccbb92b9ee41e384aafe375883c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95c25216a8d6cd0fa50be22a95587cb5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-533903b8289e4c3e4112f62f1cdac2c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-496affe91f2f46f5881b94f7acfc01fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad106781062a02a00f76ed33b10fedc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c7a4f871944a22a438f7a98450ecd6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-adb68bdc2556cb7098c690a78d85eb50.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Polyline-Path-Masked-Attention-for-Vision-Transformer"><a href="#Polyline-Path-Masked-Attention-for-Vision-Transformer" class="headerlink" title="Polyline Path Masked Attention for Vision Transformer"></a>Polyline Path Masked Attention for Vision Transformer</h2><p><strong>Authors:Zhongchen Zhao, Chaodong Xiao, Hui Lin, Qi Xie, Lei Zhang, Deyu Meng</strong></p>
<p>Global dependency modeling and spatial position modeling are two core issues of the foundational architecture design in current deep learning frameworks. Recently, Vision Transformers (ViTs) have achieved remarkable success in computer vision, leveraging the powerful global dependency modeling capability of the self-attention mechanism. Furthermore, Mamba2 has demonstrated its significant potential in natural language processing tasks by explicitly modeling the spatial adjacency prior through the structured mask. In this paper, we propose Polyline Path Masked Attention (PPMA) that integrates the self-attention mechanism of ViTs with an enhanced structured mask of Mamba2, harnessing the complementary strengths of both architectures. Specifically, we first ameliorate the traditional structured mask of Mamba2 by introducing a 2D polyline path scanning strategy and derive its corresponding structured mask, polyline path mask, which better preserves the adjacency relationships among image tokens. Notably, we conduct a thorough theoretical analysis on the structural characteristics of the proposed polyline path mask and design an efficient algorithm for the computation of the polyline path mask. Next, we embed the polyline path mask into the self-attention mechanism of ViTs, enabling explicit modeling of spatial adjacency prior. Extensive experiments on standard benchmarks, including image classification, object detection, and segmentation, demonstrate that our model outperforms previous state-of-the-art approaches based on both state-space models and Transformers. For example, our proposed PPMA-T&#x2F;S&#x2F;B models achieve 48.7%&#x2F;51.1%&#x2F;52.3% mIoU on the ADE20K semantic segmentation task, surpassing RMT-T&#x2F;S&#x2F;B by 0.7%&#x2F;1.3%&#x2F;0.3%, respectively. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zhongchenzhao/PPMA">https://github.com/zhongchenzhao/PPMA</a>. </p>
<blockquote>
<p>全局依赖建模和空间位置建模是当前深度学习框架基础架构设计中的两个核心问题。最近，Vision Transformers（ViTs）在计算机视觉领域取得了显著的成功，利用了自注意力机制的强大全局依赖建模能力。此外，Mamba2通过显式建模空间邻接先验，通过结构化掩码在自然语言处理任务中表现出了巨大的潜力。在本文中，我们提出了Polyline Path Masked Attention（PPMA），它结合了ViTs的自注意力机制和Mamba2的增强结构化掩码，充分利用了两种架构的互补优势。具体来说，我们首先通过引入2D折线路径扫描策略改进了Mamba2的传统结构化掩码，并得出了相应的结构化掩码，即折线路径掩码，能更好地保留图像标记之间的邻接关系。值得注意的是，我们对所提出的折线路径掩码的结构特性进行了深入的理论分析，并设计了计算折线路径掩码的高效算法。接下来，我们将折线路径掩码嵌入ViTs的自注意力机制中，实现了空间邻接先验的显式建模。在包括图像分类、目标检测和分割在内的标准基准测试上的大量实验表明，我们的模型优于基于状态空间模型和Transformers的先前最先进的方法。例如，我们提出的PPMA-T&#x2F;S&#x2F;B模型在ADE20K语义分割任务上的mIoU达到了48.7%&#x2F;51.1%&#x2F;52.3%，分别超越了RMT-T&#x2F;S&#x2F;B模型0.7%&#x2F;1.3%&#x2F;0.3%。代码可在<a target="_blank" rel="noopener" href="https://github.com/zhongchenzhao/PPMA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhongchenzhao/PPMA找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15940v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文提出了Polyline Path Masked Attention（PPMA），结合了Vision Transformers（ViTs）的自我注意机制与Mamba2的结构化掩码，利用两者的互补优势。通过引入2D折线路径扫描策略和相应的结构化掩码，PPMA更好地保留了图像标记之间的邻接关系。在图像分类、目标检测和分割等标准基准测试上，PPMA模型的表现优于基于状态空间模型和Transformer的先前最前沿方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Polyline Path Masked Attention结合了Vision Transformers的自我注意机制和Mamba2的结构化掩码，旨在利用两种架构的互补优势。</li>
<li>通过引入2D折线路径扫描策略，改进了传统的结构化掩码，推出了Polyline Path Mask，更好地保留了图像标记间的邻接关系。</li>
<li>PPMA模型在图像分类、目标检测和语义分割等任务上表现出色，超越了基于状态空间模型和Transformer的先前方法。</li>
<li>PPMA模型在ADE20K语义分割任务上的表现具体，例如PPMA-T&#x2F;S&#x2F;B模型分别实现了48.7%&#x2F;51.1%&#x2F;52.3%的mIoU，相较于RMT-T&#x2F;S&#x2F;B有显著的提升。</li>
<li>PPMA模型的代码已公开，便于其他研究者参考和使用。</li>
<li>PPMA模型的提出基于深厚的理论基础和高效算法设计，确保了其有效性和性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15940">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d48d93347ddde982a5c3619384669612.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05ec3b53a4a7ebc7c4e3ca2ba813bc23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25a246d40760c59c7265a59faaa7057d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4085aab682a851125227923d4396d560.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Privacy-Preserving-in-Connected-and-Autonomous-Vehicles-Through-Vision-to-Text-Transformation"><a href="#Privacy-Preserving-in-Connected-and-Autonomous-Vehicles-Through-Vision-to-Text-Transformation" class="headerlink" title="Privacy-Preserving in Connected and Autonomous Vehicles Through Vision   to Text Transformation"></a>Privacy-Preserving in Connected and Autonomous Vehicles Through Vision   to Text Transformation</h2><p><strong>Authors:Abdolazim Rezaei, Mehdi Sookhak, Ahmad Patooghy</strong></p>
<p>Connected and Autonomous Vehicles (CAVs) rely on a range of devices that often process privacy-sensitive data. Among these, roadside units play a critical role particularly through the use of AI-equipped (AIE) cameras for applications such as violation detection. However, the privacy risks associated with captured imagery remain a major concern, as such data can be misused for identity theft, profiling, or unauthorized commercial purposes. While traditional techniques such as face blurring and obfuscation have been applied to mitigate privacy risks, individual privacy remains at risk, as individuals can still be tracked using other features such as their clothing. This paper introduces a novel privacy-preserving framework that leverages feedback-based reinforcement learning (RL) and vision-language models (VLMs) to protect sensitive visual information captured by AIE cameras. The main idea is to convert images into semantically equivalent textual descriptions, ensuring that scene-relevant information is retained while visual privacy is preserved. A hierarchical RL strategy is employed to iteratively refine the generated text, enhancing both semantic accuracy and privacy. Evaluation results demonstrate significant improvements in both privacy protection and textual quality, with the Unique Word Count increasing by approximately 77% and Detail Density by around 50% compared to existing approaches. </p>
<blockquote>
<p>自动驾驶和联网车辆（CAVs）依赖于一系列经常处理敏感隐私数据的设备。其中，路边单元通过应用配备人工智能（AI）的摄像头进行违章检测等应用发挥着至关重要的作用。然而，与捕获的图像相关的隐私问题仍然是一个主要的担忧，因为这种数据可能会被用于身份盗窃、个人特征描述或未经授权的商业目的等不当用途。虽然传统的技术如面部模糊和伪装已被应用于缓解隐私问题，但个人隐私仍然面临风险，因为个体仍然可以通过其他特征如衣着进行跟踪。本文介绍了一种新颖的隐私保护框架，该框架利用基于反馈的强化学习（RL）和视觉语言模型（VLMs）来保护人工智能摄像头捕获的敏感视觉信息。主要思想是将图像转换为语义等效的文本描述，确保保留场景相关信息的同时保护视觉隐私。采用分层RL策略来迭代地优化生成的文本，提高语义准确性和隐私保护能力。评估结果表明，在隐私保护和文本质量方面都有显著提高，与现有方法相比，唯一字数增加了约77%，细节密度增加了约50%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15854v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本论文提出一种新型的隐私保护框架，该框架结合反馈强化学习（RL）和视觉语言模型（VLMs），将人工智能设备（AIE）相机捕捉到的敏感视觉信息转换为语义等效的文本描述，从而保护个人隐私。此方法在保护隐私的同时保留了场景相关的信息。通过分层强化学习策略对生成的文本进行迭代优化，提高了语义准确性和隐私保护效果。评估结果表明，与现有方法相比，该框架在隐私保护和文本质量方面均有显著提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAVs中使用的路边单元（特别是配备AI的相机）在捕获隐私敏感数据方面存在重大隐私问题。</li>
<li>传统技术（如面部模糊和模糊处理）在保护隐私方面仍有局限性，个体仍可能被基于其他特征追踪。</li>
<li>新框架利用反馈强化学习和视觉语言模型将图像转换为文本描述，以保护隐私。</li>
<li>该框架能够保留场景相关的信息，同时保护个人隐私。</li>
<li>通过分层强化学习策略优化生成的文本，提高语义准确性和隐私保护效果。</li>
<li>评估结果显示，新框架在隐私保护和文本质量方面显著提高，其中唯一单词计数增加了约77%，细节密度增加了约50%。</li>
<li>该研究为处理CAVs中隐私敏感数据提供了新的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15854">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-59cc46fb1854d0d154438726886eab5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50076c15e1073e6d839d1ec0fbc40a66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a72c47ebca7f225ba96718e48107c11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe6f5d5739561574ab34b245ddc27c95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f76e0dc73d02a8b59c6f08d65bef1f2f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ScholarSearch-Benchmarking-Scholar-Searching-Ability-of-LLMs"><a href="#ScholarSearch-Benchmarking-Scholar-Searching-Ability-of-LLMs" class="headerlink" title="ScholarSearch: Benchmarking Scholar Searching Ability of LLMs"></a>ScholarSearch: Benchmarking Scholar Searching Ability of LLMs</h2><p><strong>Authors:Junting Zhou, Wang Li, Yiyan Liao, Nengyuan Zhang, Tingjia Miao, Zhihui Qi, Yuhan Wu, Tong Yang</strong></p>
<p>Large Language Models (LLMs)’ search capabilities have garnered significant attention. Existing benchmarks, such as OpenAI’s BrowseComp, primarily focus on general search scenarios and fail to adequately address the specific demands of academic search. These demands include deeper literature tracing and organization, professional support for academic databases, the ability to navigate long-tail academic knowledge, and ensuring academic rigor. Here, we proposed ScholarSearch, the first dataset specifically designed to evaluate the complex information retrieval capabilities of Large Language Models (LLMs) in academic research. ScholarSearch possesses the following key characteristics: Academic Practicality, where question content closely mirrors real academic learning and research environments, avoiding deliberately misleading models; High Difficulty, with answers that are challenging for single models (e.g., Grok DeepSearch or Gemini Deep Research) to provide directly, often requiring at least three deep searches to derive; Concise Evaluation, where limiting conditions ensure answers are as unique as possible, accompanied by clear sources and brief solution explanations, greatly facilitating subsequent audit and verification, surpassing the current lack of analyzed search datasets both domestically and internationally; and Broad Coverage, as the dataset spans at least 15 different academic disciplines. Through ScholarSearch, we expect to more precisely measure and promote the performance improvement of LLMs in complex academic information retrieval tasks. The data is available at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/PKU-DS-LAB/ScholarSearch">https://huggingface.co/datasets/PKU-DS-LAB/ScholarSearch</a> </p>
<blockquote>
<p>大型语言模型（LLM）的搜索能力已引起广泛关注。现有的基准测试，如OpenAI的BrowseComp，主要关注一般搜索场景，未能充分满足学术搜索的特定需求。这些需求包括更深入的文献追踪和组织、对学术数据库的专业支持、浏览长尾学术知识的能力以及确保学术严谨性。在这里，我们提出了ScholarSearch，这是专门设计用于评估大型语言模型（LLM）在学术研究中的复杂信息检索能力的第一个数据集。ScholarSearch具有以下关键特点：学术实用性，问题内容紧密反映真实学术学习和研究环境，避免故意误导模型；高难度，答案对于单一模型（如Grok DeepSearch或Gemini Deep Research）来说具有挑战性，直接提供答案往往需要进行至少三次深度搜索；简洁评估，限制条件确保答案是独一无二的，同时配有清晰的来源和简要的解决方案解释，极大地促进了后续的审计和验证，弥补了国内外分析搜索数据集的缺乏；广泛覆盖，该数据集涵盖至少15个不同的学科。通过ScholarSearch，我们希望能够更精确地衡量和促进LLM在复杂的学术信息检索任务中的性能提升。数据可在：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/PKU-DS-LAB/ScholarSearch%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://huggingface.co/datasets/PKU-DS-LAB/ScholarSearch上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13784v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的搜索能力备受关注。现有基准测试如OpenAI的BrowseComp主要关注通用搜索场景，未能充分满足学术搜索的特定需求。学术搜索需求包括文献追踪与组织、专业学术数据库支持、长尾学术知识导航以及学术严谨性。为此，我们提出了ScholarSearch，首个专为评估大型语言模型在学术研究中的复杂信息检索能力而设计的数据集。ScholarSearch具备学术实用性、高难度、简洁评估及广泛覆盖等多个关键特性，涵盖至少15个不同学术领域。通过ScholarSearch，我们期望更精确地衡量并促进LLM在复杂学术信息检索任务中的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的搜索能力已引起广泛关注。</li>
<li>现有基准测试不足以满足学术搜索的特定需求。</li>
<li>ScholarSearch数据集专为评估LLM在学术研究中的信息检索能力而设计。</li>
<li>ScholarSearch具备学术实用性、高难度、简洁评估及广泛覆盖等关键特性。</li>
<li>该数据集注重学术严谨性，涵盖真实学术环境和研究需求。</li>
<li>ScholarSearch数据集至少涵盖15个不同学术领域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13784">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-64b57c0eea20d3b51c96cb33b9e9eac9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9be65f5c6b7fe5506a416aaa5d848a66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-196a3752772dcb192d414a0451009e06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99464c517a75932e75693c5a86493f52.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Calibrating-Pre-trained-Language-Classifiers-on-LLM-generated-Noisy-Labels-via-Iterative-Refinement"><a href="#Calibrating-Pre-trained-Language-Classifiers-on-LLM-generated-Noisy-Labels-via-Iterative-Refinement" class="headerlink" title="Calibrating Pre-trained Language Classifiers on LLM-generated Noisy   Labels via Iterative Refinement"></a>Calibrating Pre-trained Language Classifiers on LLM-generated Noisy   Labels via Iterative Refinement</h2><p><strong>Authors:Liqin Ye, Agam Shah, Chao Zhang, Sudheer Chava</strong></p>
<p>The traditional process of creating labeled datasets is labor-intensive and expensive. Recent breakthroughs in open-source large language models (LLMs) have opened up a new avenue in generating labeled datasets automatically for various natural language processing (NLP) tasks, providing an alternative to such an expensive annotation process. However, the reliability of such auto-generated labels remains a significant concern due to inherent inaccuracies. When learning from noisy labels, the model’s generalization is likely to be harmed as it is prone to overfit to those label noises. While previous studies in learning from noisy labels mainly focus on synthetic noise and real-world noise, LLM-generated label noise receives less attention. In this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to calibrate the classifier’s prediction, thus enhancing its robustness towards LLM-generated noisy labels. SiDyP retrieves potential true label candidates by neighborhood label distribution in text embedding space and iteratively refines noisy candidates using a simplex diffusion model. Our framework can increase the performance of the BERT classifier fine-tuned on both zero-shot and few-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30% respectively. We demonstrate the effectiveness of SiDyP by conducting extensive benchmarking for different LLMs over a variety of NLP tasks. Our code is available on Github. </p>
<blockquote>
<p>传统创建标记数据集的过程劳动密集且费用高昂。最近开源大型语言模型（LLM）的突破为各种自然语言处理（NLP）任务自动生成标记数据集开辟了一条新途径，为这种昂贵的标注过程提供了替代方案。然而，由于内在的不准确性，这种自动生成的标签的可靠性仍然是一个令人关注的问题。从噪声标签中学习时，模型很容易受到这些标签噪声的影响而过度拟合，从而影响其泛化能力。虽然之前关于从噪声标签中学习的研究主要集中在合成噪声和现实世界的噪声上，但LLM生成的标签噪声却被忽视了。在本文中，我们提出了SiDyP：基于动态先验的简单标签扩散方法，以校准分类器的预测，从而提高其对LLM生成的噪声标签的鲁棒性。SiDyP通过文本嵌入空间中的邻近标签分布检索潜在的真实标签候选者，并使用简单扩散模型迭代优化噪声候选者。我们的框架可以提高在零样本和少样本LLM生成噪声标签数据集上微调过的BERT分类器的性能，平均分别提高了7.21%和7.30%。我们对不同的LLM进行了广泛的基准测试，涵盖了多种NLP任务，证明了SiDyP的有效性。我们的代码已在GitHub上发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19675v2">PDF</a> Accepted at KDD’25</p>
<p><strong>Summary</strong></p>
<p>LLM自动生成标签数据集的方法为NLP任务提供了新途径，但存在可靠性问题。本文提出SiDyP方法，通过简单标签扩散和动态先验来校准分类器预测，提高其对LLM生成噪声标签的鲁棒性。SiDyP通过文本嵌入空间中的邻域标签分布检索潜在的真实标签候选者，并使用简单扩散模型迭代优化噪声候选者。该方法可提高零样本和少样本LLM生成噪声标签数据集上BERT分类器的性能，平均提高7.21%和7.30%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM自动生成标签数据集为NLP任务带来新途径，但可靠性成关注重点。</li>
<li>LLM生成的标签噪声对模型泛化能力可能造成损害。</li>
<li>SiDyP方法通过简单标签扩散和动态先验提高模型对LLM生成噪声标签的鲁棒性。</li>
<li>SiDyP在文本嵌入空间中通过邻域标签分布检索真实标签候选者。</li>
<li>SiDyP使用简单扩散模型迭代优化噪声候选标签。</li>
<li>SiDyP能提高零样本和少样本LLM生成噪声标签数据集上BERT分类器的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19675">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d9c7f9ca24f5627fc678c855267cb4e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1596d9f57a99782033b4dc9164ce6df6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9504263dce41bee3476baaa8573d3899.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f50bcc49e408a4345384b8f18ae7044.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-083acf531e66e8209b12054d5896d9ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84c1e9258d3e21385efd2c7fb50aef2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce0e105c0af9af76ffad9398cf9d46bd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ReplaceMe-Network-Simplification-via-Depth-Pruning-and-Transformer-Block-Linearization"><a href="#ReplaceMe-Network-Simplification-via-Depth-Pruning-and-Transformer-Block-Linearization" class="headerlink" title="ReplaceMe: Network Simplification via Depth Pruning and Transformer   Block Linearization"></a>ReplaceMe: Network Simplification via Depth Pruning and Transformer   Block Linearization</h2><p><strong>Authors:Dmitriy Shopkhoev, Ammar Ali, Magauiya Zhussip, Valentin Malykh, Stamatios Lefkimmiatis, Nikos Komodakis, Sergey Zagoruyko</strong></p>
<p>We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation, which approximates the pruned blocks. The estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining&#x2F;fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model’s performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at <a target="_blank" rel="noopener" href="https://github.com/mts-ai/ReplaceMe">https://github.com/mts-ai/ReplaceMe</a>. </p>
<blockquote>
<p>我们介绍了ReplaceMe，这是一种通用的训练外深度剪枝方法，它能有效地用线性运算替代transformer块，同时在低压缩比的情况下保持高性能。与传统的需要额外训练或微调的方法不同，我们的方法只需要一个小型的校准数据集来估计线性变换，该变换近似于剪枝块。估计的线性映射可以无缝地与其他transformer块合并，无需任何额外的网络参数。我们的实验表明，ReplaceMe持续超越其他无训练方法，并且在涉及大量重新训练&#x2F;微调和结构修改的尖端剪枝方法中表现出高度竞争力。应用于多个大型语言模型（LLM）时，ReplaceMe可实现高达25%的剪枝率，同时在开放基准测试中保留原始模型约90%的性能——无需任何训练或修复步骤，且计算开销最小（见图1）。我们在<a target="_blank" rel="noopener" href="https://github.com/mts-ai/ReplaceMe%E4%B8%8A%E6%8F%90%E4%BE%9B%E4%BA%86%E5%AE%9E%E7%8E%B0ReplaceMe%E4%BB%A5%E5%8F%8A%E5%87%A0%E7%A7%8D%E5%85%88%E8%BF%9B%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%89%AA%E6%9E%9D%E6%8A%80%E6%9C%AF%E7%9A%84%E5%BC%80%E6%BA%90%E5%BA%93%E3%80%82">https://github.com/mts-ai/ReplaceMe上提供了实现ReplaceMe以及几种先进的深度剪枝技术的开源库。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02819v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ReplaceMe是一种无需训练的深度剪枝方法，可有效地用线性运算替换变压器块，并在低压缩比下保持高性能。该方法仅需要一个小的校准数据集来估计线性变换，以近似剪枝块，无需任何额外的网络参数。实验表明，ReplaceMe在无需训练的方法中具有出色的性能，并且在涉及大量重新训练或微调以及架构修改的最先进剪枝方法中保持竞争力。应用于大型语言模型（LLM）时，ReplaceMe可在不损失任何性能的情况下实现高达25%的剪枝率，并且具有最小的计算开销。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReplaceMe是一种无需训练的深度剪枝方法，能够替换变压器块为线性操作。</li>
<li>该方法通过使用小的校准数据集来估计线性变换，以模拟剪枝块的效果。</li>
<li>ReplaceMe不需要任何额外的网络参数，可以无缝地融入剩余的变压器块。</li>
<li>实验表明，ReplaceMe在无需训练的方法中表现优秀，与其他先进的剪枝方法相比具有竞争力。</li>
<li>应用到大语言模型（LLM）时，ReplaceMe可实现高达25%的剪枝率，同时保留原始模型性能的约90%。</li>
<li>ReplaceMe具有最小的计算开销。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02819">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fcc0d2c3745224d87c692993241ca95c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71ce383af7b6ac14409099db13be337d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaff315cec7a2d0ff3358e188fb4fa97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a715ef31b9d80e6527d241aa38d08d1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="TALE-A-Tool-Augmented-Framework-for-Reference-Free-Evaluation-of-Large-Language-Models"><a href="#TALE-A-Tool-Augmented-Framework-for-Reference-Free-Evaluation-of-Large-Language-Models" class="headerlink" title="TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large   Language Models"></a>TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large   Language Models</h2><p><strong>Authors:Sher Badshah, Ali Emami, Hassan Sajjad</strong></p>
<p>As Large Language Models (LLMs) become increasingly integrated into real-world, autonomous applications, relying on static, pre-annotated references for evaluation poses significant challenges in cost, scalability, and completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework to assess LLM outputs without predetermined ground-truth answers. Unlike conventional metrics that compare to fixed references or depend solely on LLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities that actively retrieves and synthesizes external evidence. It iteratively generates web queries, collects information, summarizes findings, and refines subsequent searches through reflection. By shifting away from static references, TALE aligns with free-form question-answering tasks common in real-world scenarios. Experimental results on multiple free-form QA benchmarks show that TALE not only outperforms standard reference-based metrics for measuring response accuracy but also achieves substantial to near-perfect agreement with human evaluations. TALE enhances the reliability of LLM evaluations in real-world, dynamic scenarios without relying on static references. </p>
<blockquote>
<p>随着大型语言模型（LLM）在现实世界自主应用中的集成度越来越高，依赖静态、预先标注的参考进行评估在成本、可扩展性和完整性方面带来了重大挑战。我们提出了工具增强型LLM评估（TALE）框架，该框架能够评估LLM输出而无需预先确定的正确答案。与传统的仅与固定参考进行比较或仅依赖LLM作为评判知识的指标不同，TALE使用一个具有工具访问能力的智能体，该智能体能主动检索并综合外部证据。它通过迭代生成网络查询、收集信息、总结发现并通过反思来优化后续搜索。通过摆脱静态参考，TALE与真实场景中常见的自由形式问答任务相符。在多个自由形式问答基准测试上的实验结果表明，TALE不仅优于基于标准参考的度量标准来衡量响应准确性，而且在与人类评估的对比中取得了从显著到近乎完美的共识。TALE提高了在真实世界动态场景中LLM评估的可靠性，无需依赖静态参考。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07385v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>随着大型语言模型（LLM）在现实世界自主应用中的集成度不断提高，依赖静态、预先注释的参考进行评估在成本、可扩展性和完整性方面带来了重大挑战。我们提出了工具增强型LLM评估（TALE）框架，该框架可在无需预先设定的标准答案的情况下评估LLM输出。与传统的与固定参考点进行比较或仅依赖LLM作为法官知识的评估指标不同，TALE使用一个具备工具访问能力的代理，能够主动检索和综合外部证据。它通过生成网络查询、收集信息、总结发现并通过反思来优化后续搜索。通过摆脱静态参考，TALE符合现实场景中常见的自由形式问答任务。在多个自由形式问答基准测试上的实验结果表明，TALE不仅在衡量回答准确性方面优于基于标准参考的度量指标，而且在与人类评估的对比中取得了显著至近乎完美的共识。TALE提高了在现实世界动态场景中LLM评估的可靠性，无需依赖静态参考。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM在现实世界的集成应用中，使用静态参考进行评估存在成本、可扩展性和完整性问题。</li>
<li>提出的TALE框架能主动检索外部证据并评估LLM输出，无需预先设定的标准答案。</li>
<li>TALE使用具备工具访问能力的代理，能够生成网络查询、收集信息并总结发现。</li>
<li>TALE通过迭代搜索和反思，优化了评估过程。</li>
<li>与传统的评估指标相比，TALE在自由形式问答任务上的表现更优秀。</li>
<li>TALE在多个基准测试上的实验表现显示，其衡量回答准确性的能力超越标准参考度量指标。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07385">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d751a91df7c04fbde4c325d929fe06e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1775c9cb21327a1eb8cc8b40de07c58.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Large-Scale-Data-Selection-for-Instruction-Tuning"><a href="#Large-Scale-Data-Selection-for-Instruction-Tuning" class="headerlink" title="Large-Scale Data Selection for Instruction Tuning"></a>Large-Scale Data Selection for Instruction Tuning</h2><p><strong>Authors:Hamish Ivison, Muru Zhang, Faeze Brahman, Pang Wei Koh, Pradeep Dasigi</strong></p>
<p>Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typically tested by selecting small datasets (roughly 10k samples) from small pools (100-200k samples). However, popular deployed instruction-tuned models often train on hundreds of thousands to millions of samples, subsampled from even larger data pools. We present a systematic study of how well data selection methods scale to these settings, selecting up to 2.5M samples from pools of up to 5.8M samples and evaluating across 7 diverse tasks. We show that many recently proposed methods fall short of random selection in this setting (while using more compute), and even decline in performance when given access to larger pools of data to select over. However, we find that a variant of representation-based data selection (RDS+), which uses weighted mean pooling of pretrained LM hidden states, consistently outperforms more complex methods across all settings tested – all whilst being more compute-efficient. Our findings highlight that the scaling properties of proposed automated selection methods should be more closely examined. We release our code, data, and models at <a target="_blank" rel="noopener" href="https://github.com/hamishivi/automated-instruction-selection">https://github.com/hamishivi/automated-instruction-selection</a>. </p>
<blockquote>
<p>从较大的数据池中选取高质量的训练数据是微调语言模型时的关键步骤。经过精心挑选的数据集通常会产生优于在更大、更嘈杂的数据集上训练的模型。自动数据选择方法用于指令微调时，通常从小型数据池（10万至20万样本）中选择小型数据集（大约1万样本）进行测试。然而，流行的部署指令微调模型通常在从更大的数据池中抽取数百万至数千万样本进行训练。我们系统地研究了数据选择方法如何适应这些设置，从多达58万个样本的数据池中选择了多达25万个样本，并在7个不同任务上进行了评估。我们发现，在此设置中，许多最近提出的方法不及随机选择（同时使用更多计算资源），并且在给定的更大池中选择数据时性能甚至下降。然而，我们发现一种基于表示的变体数据选择（RDS+），该方法使用预训练LM隐藏状态的加权均值池化技术，在所有测试设置中均优于更复杂的方法，同时计算效率更高。我们的研究结果表明，应更仔细地检查所提出的自动选择方法的可扩展性。我们在<a target="_blank" rel="noopener" href="https://github.com/hamishivi/automated-instruction-selection">https://github.com/hamishivi/automated-instruction-selection</a>上发布了我们的代码、数据和模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01807v2">PDF</a> Updated, new baselines, removed some typos</p>
<p><strong>Summary</strong></p>
<p>本文研究了在更大规模数据集上指令微调语言模型时的数据选择问题。研究发现，许多近期提出的数据选择方法在大规模数据集上表现不佳，而一种基于表示的数据选择变体（RDS+）在所有测试设置中均表现优异，且计算效率更高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据选择在指令微调语言模型中至关重要，高质量数据集往往能产生性能更佳的模型。</li>
<li>自动化数据选择方法通常在较小的数据集和样本池中进行测试。</li>
<li>在更大规模数据集（数百万至数千万样本）上，许多提出的数据选择方法表现不如随机选择。</li>
<li>基于表示的数据选择变体（RDS+）在所有测试设置中都表现优异，且计算效率更高。</li>
<li>随着数据池规模的扩大，数据选择方法的性能可能会下降。</li>
<li>在大规模数据集上进行指令微调时，需要更紧密地检查数据选择方法的可扩展性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01807">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0cb8c08ef084b09208d7ef0e1fbc1a68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38a41db1ae4c051e92b210ec8d8e992a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3545f79463c4933987b03be47936731.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ALTA-Compiler-Based-Analysis-of-Transformers"><a href="#ALTA-Compiler-Based-Analysis-of-Transformers" class="headerlink" title="ALTA: Compiler-Based Analysis of Transformers"></a>ALTA: Compiler-Based Analysis of Transformers</h2><p><strong>Authors:Peter Shaw, James Cohan, Jacob Eisenstein, Kenton Lee, Jonathan Berant, Kristina Toutanova</strong></p>
<p>We propose a new programming language called ALTA and a compiler that can map ALTA programs to Transformer weights. ALTA is inspired by RASP, a language proposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler from RASP programs to Transformer weights. ALTA complements and extends this prior work, offering the ability to express loops and to compile programs to Universal Transformers, among other advantages. ALTA allows us to constructively show how Transformers can represent length-invariant algorithms for computing parity and addition, as well as a solution to the SCAN benchmark of compositional generalization tasks, without requiring intermediate scratchpad decoding steps. We also propose tools to analyze cases where the expressibility of an algorithm is established, but end-to-end training on a given training set fails to induce behavior consistent with the desired algorithm. To this end, we explore training from ALTA execution traces as a more fine-grained supervision signal. This enables additional experiments and theoretical analyses relating the learnability of various algorithms to data availability and modeling decisions, such as positional encodings. We make the ALTA framework – language specification, symbolic interpreter, and weight compiler – available to the community to enable further applications and insights. </p>
<blockquote>
<p>我们提出了一种新的编程语言ALTA，以及能够将ALTA程序映射到Transformer权重（Transformer weights）的编译器。ALTA语言受到Weiss等人（2021）提出的RASP语言和Lindner等人（2023）提出的将RASP程序编译成Transformer权重的Tracr编译器的启发。ALTA是对这些先前工作的补充和扩展，除了具备表达循环的能力外，还能将程序编译为通用Transformer，具有其他优势。通过ALTA，我们能够直观地展示Transformer如何表示用于计算奇偶性和加法的不变长度算法，以及解决无需中间暂存解码步骤的组合泛化任务的SCAN基准测试解决方案。我们还提出了分析工具，用于分析算法表达性已确立但特定训练集端到端训练未能诱导出与期望算法一致行为的情况。为此，我们探索了从ALTA执行轨迹进行训练作为更精细的监督信号。这使得我们能够进行更多实验和理论分析，将各种算法的学习性与数据可用性和建模决策（如位置编码）相关联。我们将ALTA框架（包括语言规范、符号解释器和权重编译器）提供给社区，以推动进一步的应用和见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18077v2">PDF</a> TMLR 2025</p>
<p><strong>Summary</strong></p>
<p>ALTA是一种新型编程语言，可编译为Transformer权重，受RASP和Tracr启发。它具备表达循环的能力，并能编译为通用Transformer，还具有其他优势。ALTA能够展示Transformer如何表示长度不变的算法，解决SCAN基准测试中的组合泛化任务，且无需中间解码步骤。此外，它还提供了工具来分析算法表达性建立但端到端训练失败的情况，并提出了通过ALTA执行轨迹进行更精细的监督信号训练的方法。这使更多的实验和理论分析成为可能，关联了不同算法的可学习性与数据可用性和建模决策等因素。ALTA框架现已向公众开放，以促进进一步的应用和见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ALTA是一种新型编程语言，可编译为Transformer权重。</li>
<li>ALTA受RASP和Tracr启发，具备表达循环的能力，并可以编译为通用Transformer。</li>
<li>ALTA能展示Transformer如何表示长度不变的算法，如计算奇偶性和加法。</li>
<li>ALTA解决了SCAN基准测试中的组合泛化任务，无需中间解码步骤。</li>
<li>ALTA提供了工具来分析算法表达性建立但端到端训练失败的情况。</li>
<li>通过ALTA执行轨迹进行更精细的监督信号训练的方法被提出。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.18077">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bf7cc29440ec7cc0275ae2589477d098.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f530f2eb677a2d52211102bc60dfe91c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b74a59903890c2223de200cd602e43b9.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Cost-effective-Instruction-Learning-for-Pathology-Vision-and-Language-Analysis"><a href="#Cost-effective-Instruction-Learning-for-Pathology-Vision-and-Language-Analysis" class="headerlink" title="Cost-effective Instruction Learning for Pathology Vision and Language   Analysis"></a>Cost-effective Instruction Learning for Pathology Vision and Language   Analysis</h2><p><strong>Authors:Kaitao Chen, Mianxin Liu, Fang Yan, Lei Ma, Xiaoming Shi, Lilong Wang, Xiaosong Wang, Lifeng Zhu, Zhe Wang, Mu Zhou, Shaoting Zhang</strong></p>
<p>The advent of vision-language models fosters the interactive conversations between AI-enabled models and humans. Yet applying these models into clinics must deal with daunting challenges around large-scale training data, financial, and computational resources. Here we propose a cost-effective instruction learning framework for conversational pathology named as CLOVER. CLOVER only trains a lightweight module and uses instruction tuning while freezing the parameters of the large language model. Instead of using costly GPT-4, we propose well-designed prompts on GPT-3.5 for building generation-based instructions, emphasizing the utility of pathological knowledge derived from the Internet source. To augment the use of instructions, we construct a high-quality set of template-based instructions in the context of digital pathology. From two benchmark datasets, our findings reveal the strength of hybrid-form instructions in the visual question-answer in pathology. Extensive results show the cost-effectiveness of CLOVER in answering both open-ended and closed-ended questions, where CLOVER outperforms strong baselines that possess 37 times more training parameters and use instruction data generated from GPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot learning in the external clinical dataset. These findings demonstrate that cost-effective modeling of CLOVER could accelerate the adoption of rapid conversational applications in the landscape of digital pathology. </p>
<blockquote>
<p>视觉语言模型的出现促进了人工智能模型与人类之间的交互式对话。然而，将这些模型应用于临床必须应对大规模训练数据、资金和计算资源等方面的巨大挑战。在这里，我们针对名为CLOVER的对话式病理学提出了经济高效的指令学习框架。CLOVER只训练一个轻量级模块，并使用指令微调来冻结大型语言模型的参数。我们没有使用昂贵的GPT-4，而是提出了针对GPT-3.5的精心设计提示来构建基于生成的指令，强调从互联网资源中获得的病理学知识的实用性。为了增强指令的使用，我们在数字病理的背景下构建了一套高质量的基于模板的指令集。从两个基准数据集中，我们发现混合形式的指令在病理视觉问答中的优势。大量结果表明，CLOVER在回答开放性和封闭性问题时的成本效益高，其中CLOVER的表现优于拥有37倍以上训练参数和使用GPT-4生成指令数据的强大基线。通过指令微调，CLOVER显示出在外部临床数据集中的小样本学习能力。这些发现表明，CLOVER的经济高效建模可以加速数字病理学领域快速对话式应用的采用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.17734v2">PDF</a> </p>
<p><strong>Summary</strong><br>     随着视觉语言模型的兴起，人工智能模型与人类之间的交互对话日益频繁。然而，将这些模型应用于临床时，面临大规模训练数据、资金和计算资源的挑战。为此，我们提出了一种用于对话病理学的低成本指令学习框架CLOVER。CLOVER仅训练轻量级模块，并采用指令微调来冻结大型语言模型的参数。我们并未使用成本高昂的GPT-4，而是设计了针对GPT-3.5的提示来构建生成指令，强调了从互联网资源中获得的病理学知识的实用性。为了增强指令的使用效果，我们在数字病理背景下构建了一套高质量的模板指令集。从两个基准数据集的研究结果来看，混合形式的指令在病理学视觉问答中的优势显著。广泛的实验结果表明，在回答开放和封闭性问题方面，CLOVER表现出成本效益高，优于拥有更多训练参数和基于GPT-4生成指令的强大基线。通过指令微调，CLOVER展现出对外部临床数据集的少样本学习的稳健性。这些发现表明，低成本建模的CLOVER可能加速数字病理学领域的快速对话应用程序的采用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型的兴起促进了AI与人类之间的交互对话。</li>
<li>将AI模型应用于临床时面临大规模训练数据、资金和计算资源的挑战。</li>
<li>提出了一种用于对话病理学的低成本指令学习框架CLOVER。</li>
<li>CLOVER通过仅训练轻量级模块和采用指令微调来降低成本。</li>
<li>使用GPT-3.5的提示构建生成指令，强调互联网来源的病理学知识的实用性。</li>
<li>在数字病理背景下构建了一套高质量的模板指令集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.17734">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-305457b1ad5ab8ab2a78b56ba6bf0111.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb1257e8598da3ad107b8138434c8199.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dbb27237bd00903e6a9b6b822c8dfb5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-53092162b508feea490dc5989d9cd034.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Disentangling-and-Integrating-Relational-and-Sensory-Information-in-Transformer-Architectures"><a href="#Disentangling-and-Integrating-Relational-and-Sensory-Information-in-Transformer-Architectures" class="headerlink" title="Disentangling and Integrating Relational and Sensory Information in   Transformer Architectures"></a>Disentangling and Integrating Relational and Sensory Information in   Transformer Architectures</h2><p><strong>Authors:Awni Altabaa, John Lafferty</strong></p>
<p>Relational reasoning is a central component of generally intelligent systems, enabling robust and data-efficient inductive generalization. Recent empirical evidence shows that many existing neural architectures, including Transformers, struggle with tasks requiring relational reasoning. In this work, we distinguish between two types of information: sensory information about the properties of individual objects, and relational information about the relationships between objects. While neural attention provides a powerful mechanism for controlling the flow of sensory information between objects, the Transformer lacks an explicit computational mechanism for routing and processing relational information. To address this limitation, we propose an architectural extension of the Transformer framework that we call the Dual Attention Transformer (DAT), featuring two distinct attention mechanisms: sensory attention for directing the flow of sensory information, and a novel relational attention mechanism for directing the flow of relational information. We empirically evaluate DAT on a diverse set of tasks ranging from synthetic relational benchmarks to complex real-world tasks such as language modeling and visual processing. Our results demonstrate that integrating explicit relational computational mechanisms into the Transformer architecture leads to significant performance gains in terms of data efficiency and parameter efficiency. </p>
<blockquote>
<p>关系推理是通用智能系统的核心组成部分，能够实现稳健且数据高效归纳概括。最新的实证证据表明，包括Transformer在内的许多现有神经网络架构在处理需要关系推理的任务时面临困难。在这项工作中，我们将信息分为两类：关于单个对象属性的感觉信息和关于对象之间关系的关系信息。虽然神经注意力为控制对象之间感觉信息的流动提供了强大的机制，但Transformer缺乏用于路由和处理关系信息的明确计算机制。为了解决这一局限性，我们提出了Transformer架构的扩展，我们称之为双注意力Transformer（DAT），它具有两种独特的注意力机制：感觉注意力，用于引导感觉信息的流动，以及新型的关系注意力机制，用于引导关系信息的流动。我们在一系列任务上对DAT进行了实证评估，这些任务范围从合成关系基准测试到复杂的现实世界任务，如语言建模和视觉处理。我们的结果表明，将明确的关系计算机制整合到Transformer架构中，在数据效率和参数效率方面实现了显著的性能提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.16727v3">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>关系推理是智能系统的重要组成部分，它可实现稳健且高效的数据归纳概括。现有神经架构，如Transformer等，在处理需要关系推理的任务时表现欠佳。本文区分了两种信息类型：关于单个对象属性的感觉信息与关于对象间关系的关联信息。虽然神经注意力为控制对象间感觉信息的流动提供了强大机制，但Transformer缺乏明确计算机制来处理关联信息的路由和处理。为解决此局限性，我们提出了Transformer架构的扩展版本——双注意力Transformer（DAT），它包含两种不同的注意力机制：用于引导感觉信息流动的感官注意力与用于引导关联信息流动的新型关联注意力机制。我们对DAT进行了广泛的评估，包括从合成关系基准测试到复杂现实世界任务（如语言建模和视觉处理）等多样化任务。结果表明，在Transformer架构中整合明确的关联计算机制，在数据效率和参数效率方面实现了显著的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>关系推理是智能系统的核心，对稳健且高效的数据归纳概括至关重要。</li>
<li>现有神经架构如Transformer在处理关系推理任务时表现不足。</li>
<li>本文区分了感觉信息与关联信息两种类型。</li>
<li>神经注意力有助于控制感觉信息的流动，但Transformer缺乏处理关联信息的明确机制。</li>
<li>提出了一种新的神经架构——双注意力Transformer（DAT），包含感官注意力和关联注意力两种机制。</li>
<li>DAT在多样化任务上的评估结果表明，其在数据效率和参数效率方面实现了显著的性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.16727">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ef619e22ecd5e3be2e3727333541fa7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6a2be5d61bf186841856a60f9be9e2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f0655504e552f212a9d07642aa1e561.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36aa137f1daec84e634ae1adb10f14ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d843dbad7644dd05add2fc59d4b5ff1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9f35788449f25c3ac391d6d19c4e11b0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-06-24  RAGentA Multi-Agent Retrieval-Augmented Generation for Attributed   Question Answering
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-81785035976daa414d4c9fa7bf86cc8c.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-06-24  Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models An Empirical Evaluation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29774.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
