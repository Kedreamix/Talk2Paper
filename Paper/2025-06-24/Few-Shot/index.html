<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-24  Universal Music Representations? Evaluating Foundation Models on World   Music Corpora">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-eb1257e8598da3ad107b8138434c8199.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    51 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-24-æ›´æ–°"><a href="#2025-06-24-æ›´æ–°" class="headerlink" title="2025-06-24 æ›´æ–°"></a>2025-06-24 æ›´æ–°</h1><h2 id="Universal-Music-Representations-Evaluating-Foundation-Models-on-World-Music-Corpora"><a href="#Universal-Music-Representations-Evaluating-Foundation-Models-on-World-Music-Corpora" class="headerlink" title="Universal Music Representations? Evaluating Foundation Models on World   Music Corpora"></a>Universal Music Representations? Evaluating Foundation Models on World   Music Corpora</h2><p><strong>Authors:Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos</strong></p>
<p>Foundation models have revolutionized music information retrieval, but questions remain about their ability to generalize across diverse musical traditions. This paper presents a comprehensive evaluation of five state-of-the-art audio foundation models across six musical corpora spanning Western popular, Greek, Turkish, and Indian classical traditions. We employ three complementary methodologies to investigate these modelsâ€™ cross-cultural capabilities: probing to assess inherent representations, targeted supervised fine-tuning of 1-2 layers, and multi-label few-shot learning for low-resource scenarios. Our analysis shows varying cross-cultural generalization, with larger models typically outperforming on non-Western music, though results decline for culturally distant traditions. Notably, our approaches achieve state-of-the-art performance on five out of six evaluated datasets, demonstrating the effectiveness of foundation models for world music understanding. We also find that our targeted fine-tuning approach does not consistently outperform probing across all settings, suggesting foundation models already encode substantial musical knowledge. Our evaluation framework and benchmarking results contribute to understanding how far current models are from achieving universal music representations while establishing metrics for future progress. </p>
<blockquote>
<p>åŸºç¡€æ¨¡å‹å·²ç»å½»åº•æ”¹å˜äº†éŸ³ä¹ä¿¡æ¯æ£€ç´¢ï¼Œä½†å¯¹äºå®ƒä»¬åœ¨å¤šæ ·åŒ–éŸ³ä¹ä¼ ç»Ÿä¸­çš„æ³›åŒ–èƒ½åŠ›ä»å­˜åœ¨ç–‘é—®ã€‚æœ¬æ–‡å…¨é¢è¯„ä¼°äº†äº”ä¸ªæœ€å…ˆè¿›çš„éŸ³é¢‘åŸºç¡€æ¨¡å‹ï¼Œæ¶‰åŠå…­ç§éŸ³ä¹è¯­æ–™åº“ï¼ŒåŒ…æ‹¬è¥¿æ–¹æµè¡Œã€å¸Œè…Šã€åœŸè€³å…¶å’Œå°åº¦å¤å…¸ä¼ ç»Ÿã€‚æˆ‘ä»¬é‡‡ç”¨ä¸‰ç§äº’è¡¥æ–¹æ³•æ¥ç ”ç©¶è¿™äº›æ¨¡å‹çš„è·¨æ–‡åŒ–èƒ½åŠ›ï¼šæ¢æµ‹è¯„ä¼°å†…åœ¨è¡¨ç¤ºã€å¯¹1-2å±‚è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„ç›‘ç£å¾®è°ƒä»¥åŠç”¨äºä½èµ„æºåœºæ™¯çš„å¤šæ ‡ç­¾å°æ ·æœ¬å­¦ä¹ ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè·¨æ–‡åŒ–æ³›åŒ–èƒ½åŠ›å­˜åœ¨å·®å¼‚ï¼Œè¾ƒå¤§çš„æ¨¡å‹é€šå¸¸åœ¨éè¥¿æ–¹éŸ³ä¹ä¸Šè¡¨ç°æ›´å¥½ï¼Œä½†å¯¹äºæ–‡åŒ–è·ç¦»è¾ƒè¿œçš„ä¼ ç»Ÿï¼Œç»“æœä¼šä¸‹é™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…­ä¸ªè¯„ä¼°æ•°æ®é›†ä¸­æœ‰äº”ä¸ªè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œè¯æ˜äº†åŸºç¡€æ¨¡å‹åœ¨ä¸–ç•ŒéŸ³ä¹ç†è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œæˆ‘ä»¬çš„æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒæ–¹æ³•å¹¶ä¸å§‹ç»ˆåœ¨æ‰€æœ‰è®¾ç½®ä¸­ä¼˜äºæ¢æµ‹ï¼Œè¿™è¡¨æ˜åŸºç¡€æ¨¡å‹å·²ç»ç¼–ç äº†å¤§é‡çš„éŸ³ä¹çŸ¥è¯†ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•ç»“æœæœ‰åŠ©äºäº†è§£å½“å‰æ¨¡å‹è·ç¦»å®ç°é€šç”¨éŸ³ä¹è¡¨ç¤ºè¿˜æœ‰å¤šè¿œï¼ŒåŒæ—¶ä¸ºæœªæ¥è¿›å±•å»ºç«‹äº†è¡¡é‡æŒ‡æ ‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17055v1">PDF</a> Accepted at ISMIR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡å…¨é¢è¯„ä¼°äº†äº”æ¬¾å…ˆè¿›éŸ³é¢‘åŸºç¡€æ¨¡å‹åœ¨æ¶µç›–è¥¿æ–¹æµè¡Œã€å¸Œè…Šã€åœŸè€³å…¶å’Œå°åº¦å¤å…¸ä¼ ç»Ÿç­‰å…­ç§éŸ³ä¹è¯­æ–™åº“ä¸­çš„è·¨æ–‡åŒ–æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡æ¢é’ˆæµ‹è¯•å†…åœ¨è¡¨å¾ã€å¯¹ä¸€è‡³ä¸¤å±‚è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„ç›‘ç£å¾®è°ƒä»¥åŠç”¨äºä½èµ„æºåœºæ™¯çš„å¤šæ ‡ç­¾å°æ ·æœ¬å­¦ä¹ ç­‰æ–¹æ³•è¿›è¡Œç ”ç©¶ã€‚åˆ†æè¡¨æ˜ï¼ŒåŸºç¡€æ¨¡å‹åœ¨ä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›å­˜åœ¨å·®å¼‚ï¼Œå¤§å‹æ¨¡å‹åœ¨éè¥¿æ–¹éŸ³ä¹ä¸Šçš„è¡¨ç°é€šå¸¸è¾ƒå¥½ï¼Œä½†åœ¨æ–‡åŒ–è·ç¦»è¾ƒè¿œçš„ä¼ ç»Ÿä¸­è¡¨ç°ä¸‹é™ã€‚æœ¬ç ”ç©¶çš„æ–¹æ³•åœ¨å…­ä¸ªæ•°æ®é›†ä¸­æœ‰äº”ä¸ªè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œè¯æ˜äº†åŸºç¡€æ¨¡å‹å¯¹ä¸–ç•ŒéŸ³ä¹ç†è§£çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°é’ˆå¯¹æ€§çš„å¾®è°ƒæ–¹æ³•å¹¶ä¸æ€»æ˜¯ä¼˜äºæ¢é’ˆåœ¨æ‰€æœ‰åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œè¡¨æ˜åŸºç¡€æ¨¡å‹å·²ç»ç¼–ç äº†å¤§é‡çš„éŸ³ä¹çŸ¥è¯†ã€‚æœ¬è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•ç»“æœæœ‰åŠ©äºäº†è§£å½“å‰æ¨¡å‹è·ç¦»å®ç°é€šç”¨éŸ³ä¹è¡¨ç¤ºçš„ç¨‹åº¦ï¼Œå¹¶ä¸ºæœªæ¥çš„è¿›æ­¥å»ºç«‹äº†æŒ‡æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘åŸºç¡€æ¨¡å‹åœ¨è·¨æ–‡åŒ–éŸ³ä¹ä¿¡æ¯æ£€ç´¢ä¸­çš„æ³›åŒ–èƒ½åŠ›å­˜åœ¨å·®å¼‚ã€‚</li>
<li>å¤§å‹æ¨¡å‹åœ¨éè¥¿æ–¹éŸ³ä¹ä¸Šçš„è¡¨ç°è¾ƒå¥½ã€‚</li>
<li>åœ¨æ–‡åŒ–è·ç¦»è¾ƒè¿œçš„ä¼ ç»Ÿä¸­ï¼Œæ¨¡å‹è¡¨ç°å¯èƒ½ä¸‹é™ã€‚</li>
<li>è®ºæ–‡é‡‡ç”¨ä¸‰ç§æ–¹æ³•è¯„ä¼°æ¨¡å‹çš„è·¨æ–‡åŒ–èƒ½åŠ›ï¼šæ¢é’ˆæµ‹è¯•ã€é’ˆå¯¹æ€§ç›‘ç£å¾®è°ƒå’Œå°æ ·æœ¬å­¦ä¹ ã€‚</li>
<li>ç ”ç©¶æ–¹æ³•åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚</li>
<li>åŸºç¡€æ¨¡å‹å·²ç»ç¼–ç äº†å¤§é‡çš„éŸ³ä¹çŸ¥è¯†ï¼Œä½†ä»æœ‰æå‡ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef6b3ad7f0e46b926c06dd99107e3707.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4bc8c35aa40a515721127ed2f2eff5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f64a9f8dd2c8a8a3ec2556ee6cdf815f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55166605e64162bcc33a95edb4f4f24b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01e8305c9ecb63e3ba1683b0d0dfc759.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08c819e598b7abcc1ddb1935a60d39c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3c5000f235cf6a0c604f881f3cee922.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Prmpt2Adpt-Prompt-Based-Zero-Shot-Domain-Adaptation-for-Resource-Constrained-Environments"><a href="#Prmpt2Adpt-Prompt-Based-Zero-Shot-Domain-Adaptation-for-Resource-Constrained-Environments" class="headerlink" title="Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for   Resource-Constrained Environments"></a>Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for   Resource-Constrained Environments</h2><p><strong>Authors:Yasir Ali Farrukh, Syed Wali, Irfan Khan, Nathaniel D. Bastian</strong></p>
<p>Unsupervised Domain Adaptation (UDA) is a critical challenge in real-world vision systems, especially in resource-constrained environments like drones, where memory and computation are limited. Existing prompt-driven UDA methods typically rely on large vision-language models and require full access to source-domain data during adaptation, limiting their applicability. In this work, we propose Prmpt2Adpt, a lightweight and efficient zero-shot domain adaptation framework built around a teacher-student paradigm guided by prompt-based feature alignment. At the core of our method is a distilled and fine-tuned CLIP model, used as the frozen backbone of a Faster R-CNN teacher. A small set of low-level source features is aligned to the target domain semantics-specified only through a natural language prompt-via Prompt-driven Instance Normalization (PIN). These semantically steered features are used to briefly fine-tune the detection head of the teacher model. The adapted teacher then generates high-quality pseudo-labels, which guide the on-the-fly adaptation of a compact student model. Experiments on the MDS-A dataset demonstrate that Prmpt2Adpt achieves competitive detection performance compared to state-of-the-art methods, while delivering up to 7x faster adaptation and 5x faster inference speed using few source images-making it a practical and scalable solution for real-time adaptation in low-resource domains. </p>
<blockquote>
<p>æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰æ˜¯ç°å®ä¸–ç•Œè§†è§‰ç³»ç»Ÿä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒï¼ˆå¦‚æ— äººæœºï¼‰ä¸­ï¼Œå†…å­˜å’Œè®¡ç®—èµ„æºæœ‰é™ã€‚ç°æœ‰çš„åŸºäºæç¤ºçš„UDAæ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨é€‚åº”è¿‡ç¨‹ä¸­éœ€è¦å®Œå…¨è®¿é—®æºåŸŸæ•°æ®ï¼Œè¿™é™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Prompt2Adptï¼Œè¿™æ˜¯ä¸€ä¸ªå›´ç»•æ•™å¸ˆå­¦ç”ŸèŒƒå¼æ„å»ºçš„è½»é‡çº§é«˜æ•ˆé›¶æ ·æœ¬åŸŸè‡ªé€‚åº”æ¡†æ¶ï¼Œç”±åŸºäºæç¤ºçš„ç‰¹å¾å¯¹é½å¼•å¯¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªç»è¿‡è’¸é¦å’Œç²¾ç»†è°ƒæ•´çš„CLIPæ¨¡å‹ï¼Œç”¨ä½œFaster R-CNNæ•™å¸ˆçš„å†»ç»“ä¸»å¹²ã€‚é€šè¿‡æç¤ºé©±åŠ¨çš„å®ä¾‹å½’ä¸€åŒ–ï¼ˆPINï¼‰ï¼Œä¸€å°éƒ¨åˆ†ä½çº§çš„æºç‰¹å¾è¢«å¯¹é½åˆ°ç›®æ ‡åŸŸè¯­ä¹‰ï¼ˆä»…é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºæŒ‡å®šï¼‰ã€‚è¿™äº›è¯­ä¹‰å¼•å¯¼çš„ç‰¹å¾è¢«ç”¨æ¥å¾®è°ƒæ•™å¸ˆçš„æ£€æµ‹å¤´éƒ¨ã€‚é€‚åº”åçš„æ•™å¸ˆç„¶åç”Ÿæˆé«˜è´¨é‡çš„ä¼ªæ ‡ç­¾ï¼Œè¿™äº›ä¼ªæ ‡ç­¾æŒ‡å¯¼ç´§å‡‘çš„å­¦ç”Ÿæ¨¡å‹çš„å³æ—¶é€‚åº”ã€‚åœ¨MDS-Aæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPrompt2Adptä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨æ£€æµ‹æ€§èƒ½æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ï¼ŒåŒæ—¶ä½¿ç”¨å°‘é‡çš„æºå›¾åƒå®ç°äº†é«˜è¾¾7å€çš„å¿«é€Ÿé€‚åº”å’Œ5å€çš„æ¨ç†é€Ÿåº¦ï¼Œä½¿å…¶æˆä¸ºå®æ—¶é€‚åº”ä½èµ„æºåŸŸçš„å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16994v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ•™å¸ˆ-å­¦ç”ŸèŒƒå¼çš„è½»é‡çº§é«˜æ•ˆé›¶æ ·æœ¬åŸŸè‡ªé€‚åº”æ¡†æ¶ï¼Œåä¸ºPrmpt2Adptã€‚å®ƒåˆ©ç”¨CLIPæ¨¡å‹å’ŒPrompt-driven Instance Normalizationï¼ˆPINï¼‰è¿›è¡Œç‰¹å¾å¯¹é½ï¼Œå®ç°å¯¹æºåŸŸæ•°æ®çš„å°‘é‡ä½¿ç”¨å’Œå¯¹ç›®æ ‡åŸŸè¯­ä¹‰çš„è‡ªç„¶è¯­è¨€æç¤ºçš„å¼•å¯¼ã€‚è¯¥æ–¹æ³•å®ç°äº†å¿«é€Ÿè‡ªé€‚åº”å’Œæ¨ç†é€Ÿåº¦çš„æå‡ï¼Œä½¿å¾—åœ¨ä½èµ„æºé¢†åŸŸä¸­çš„å®æ—¶è‡ªé€‚åº”å˜å¾—æ›´åŠ å®ç”¨å’Œå¯æ‰©å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Prmpt2Adptæ˜¯ä¸€ç§é’ˆå¯¹æ— äººæœºç­‰èµ„æºå—é™ç¯å¢ƒçš„é›¶æ ·æœ¬åŸŸè‡ªé€‚åº”æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨æ•™å¸ˆ-å­¦ç”ŸèŒƒå¼ï¼Œåˆ©ç”¨CLIPæ¨¡å‹ä½œä¸ºå›ºå®šéª¨å¹²ç½‘ã€‚</li>
<li>é€šè¿‡Prompt-driven Instance Normalizationï¼ˆPINï¼‰å®ç°æºç‰¹å¾ä¸ç›®æ ‡åŸŸè¯­ä¹‰çš„å¯¹é½ã€‚</li>
<li>ä»…é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºè¿›è¡Œç‰¹å¾å¯¹é½ã€‚</li>
<li>Prmpt2Adptå®ç°äº†å¿«é€Ÿè‡ªé€‚åº”å’Œæ¨ç†é€Ÿåº¦çš„æå‡ã€‚</li>
<li>åœ¨MDS-Aæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPrmpt2Adptçš„æ£€æµ‹ç»“æœä¸æœ€æ–°æŠ€æœ¯ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16994">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00d1b215fc4d6a6fa2beeac18306ad76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abe433d56654973b69d20d902edb7142.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b30b1817879d38a68b6b02a474245150.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1274dfd5091279e65c1555970e72f639.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-add5936f0821d6203113d0ecb23bca05.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Single-shot-thermometry-of-simulated-Boseâ€“Einstein-condensates-using-artificial-intelligence"><a href="#Single-shot-thermometry-of-simulated-Boseâ€“Einstein-condensates-using-artificial-intelligence" class="headerlink" title="Single-shot thermometry of simulated Boseâ€“Einstein condensates using   artificial intelligence"></a>Single-shot thermometry of simulated Boseâ€“Einstein condensates using   artificial intelligence</h2><p><strong>Authors:Jack Griffiths, Steven A. Wrathmall, Simon A. Gardiner</strong></p>
<p>Precise determination of thermodynamic parameters in ultracold Bose gases remains challenging due to the destructive nature of conventional measurement techniques and inherent experimental uncertainties. We demonstrate an artificial intelligence approach for rapid, non-destructive estimation of the chemical potential and temperature from single-shot, in situ imaged density profiles of finite-temperature Bose gases. Our convolutional neural network is trained exclusively on quasi-2D &#96;pancakeâ€™ condensates in harmonic trap configurations. It achieves parameter extraction within fractions of a second. The model also demonstrates zero-shot generalisation across both trap geometry and thermalisation dynamics, successfully estimating thermodynamic parameters for toroidally trapped condensates with errors of only a few nanokelvin despite no prior exposure to such geometries during training, and maintaining predictive accuracy during dynamic thermalisation processes after a relatively brief evolution without explicit training on non-equilibrium states. These results suggest that supervised learning can overcome traditional limitations in ultracold atom thermometry, with extension to broader geometric configurations, temperature ranges, and additional parameters potentially enabling comprehensive real-time analysis of quantum gas experiments. Such capabilities could significantly streamline experimental workflows whilst improving measurement precision across a range of quantum fluid systems. </p>
<blockquote>
<p>åœ¨è¶…å†·ç»è‰²æ°”ä½“ä¸­ç²¾ç¡®ç¡®å®šçƒ­åŠ›å­¦å‚æ•°ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºä¼ ç»Ÿæµ‹é‡æŠ€æœ¯çš„ç ´åæ€§ä»¥åŠå®éªŒå›ºæœ‰çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§äººå·¥æ™ºèƒ½æ–¹æ³•ï¼Œå¯ä»¥å¿«é€Ÿã€éç ´åæ€§åœ°ä»å•æ¬¡ç°åœºæˆåƒçš„å¯†åº¦åˆ†å¸ƒå›¾ä¸­ä¼°è®¡æœ‰é™æ¸©åº¦ç»è‰²æ°”ä½“çš„åŒ–å­¦åŠ¿å’Œæ¸©åº¦ã€‚æˆ‘ä»¬çš„å·ç§¯ç¥ç»ç½‘ç»œä»…é’ˆå¯¹è°æ³¢é™·é˜±é…ç½®ä¸­çš„äºŒç»´ç±»â€ç…é¥¼â€å‡èšç‰©è¿›è¡Œè®­ç»ƒã€‚å®ƒèƒ½å¤Ÿåœ¨å‡ åˆ†ä¹‹ä¸€ç§’å†…å®ç°å‚æ•°æå–ã€‚è¯¥æ¨¡å‹è¿˜å±•ç¤ºäº†è·¨é™·é˜±å‡ ä½•å½¢çŠ¶å’Œçƒ­åŒ–åŠ¨åŠ›å­¦çš„é›¶èµ·ç‚¹æ³›åŒ–èƒ½åŠ›ï¼Œå°½ç®¡åœ¨è®­ç»ƒæœŸé—´æ²¡æœ‰æ¥è§¦è¿‡æ­¤ç±»å‡ ä½•å½¢çŠ¶ï¼Œä½†å¯¹ç¯å½¢é™·é˜±ä¸­çš„å‡èšç‰©çƒ­åŠ›å­¦å‚æ•°è¿›è¡Œä¼°è®¡æ—¶è¯¯å·®ä»…ä¸ºå‡ çº³å¼€å°”æ–‡ï¼Œå¹¶ä¸”åœ¨ç›¸å¯¹çŸ­æš‚çš„æ¼”åŒ–è¿‡ç¨‹ä¸­ï¼Œåœ¨éå¹³è¡¡æ€æ²¡æœ‰æ˜ç¡®çš„è®­ç»ƒæƒ…å†µä¸‹ä»èƒ½ä¿æŒé¢„æµ‹ç²¾åº¦ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæœ‰ç›‘ç£å­¦ä¹ å¯ä»¥å…‹æœè¶…å†·åŸå­æµ‹æ¸©ä¸­çš„ä¼ ç»Ÿå±€é™æ€§ï¼Œæ‰©å±•åˆ°æ›´å¹¿æ³›çš„å‡ ä½•ç»“æ„ã€æ¸©åº¦èŒƒå›´å’Œé™„åŠ å‚æ•°ï¼Œä»è€Œå¯èƒ½å®ç°å¯¹é‡å­æ°”ä½“å®éªŒçš„å…¨é¢å®æ—¶åˆ†æã€‚è¿™ç§èƒ½åŠ›å¯èƒ½ä¼šå¤§å¤§ç®€åŒ–å®éªŒå·¥ä½œæµç¨‹ï¼ŒåŒæ—¶æé«˜ä¸€ç³»åˆ—é‡å­æµä½“ç³»ç»Ÿçš„æµ‹é‡ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16925v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹è¶…å†·ç»è‰²æ°”ä½“ä¸­çš„çƒ­åŠ›å­¦å‚æ•°ç²¾ç¡®æµ‹å®šï¼Œå­˜åœ¨ä¼ ç»Ÿæµ‹é‡æŠ€æœ¯çš„ç ´åæ€§å’Œå®éªŒå›ºæœ‰ä¸ç¡®å®šæ€§çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†ä¸€ç§äººå·¥æ™ºèƒ½æ–¹æ³•ï¼Œç”¨äºå¿«é€Ÿã€éç ´åæ€§ä¼°è®¡åŒ–å­¦åŠ¿å’Œæ¸©åº¦ï¼Œä»…é€šè¿‡å•æ¬¡å³æ—¶æˆåƒçš„å¯†åº¦åˆ†å¸ƒæ•°æ®å³å¯å®Œæˆã€‚ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼Œä»…åœ¨å‡†äºŒç»´â€œç…é¥¼â€å‡èšç‰©å’Œè°æ³¢é™·é˜±é…ç½®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå³å¯åœ¨æ•°ç§’å†…æå–å‚æ•°ã€‚è¯¥æ¨¡å‹è¿˜å±•ç¤ºäº†åœ¨é™·é˜±å‡ ä½•å½¢çŠ¶å’Œçƒ­åŒ–åŠ¨åŠ›å­¦æ–¹é¢çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå°½ç®¡åœ¨è®­ç»ƒæœŸé—´æœªæ¥è§¦æ­¤ç±»å‡ ä½•å½¢çŠ¶ï¼Œä½†å¯¹äºç¯å½¢é™·é˜±ä¸­çš„ç»è‰²æ°”ä½“ï¼Œå…¶çƒ­åŠ›å­¦å‚æ•°çš„ä¼°è®¡è¯¯å·®ä»…ä¸ºå‡ çº³å¼€å°”æ–‡ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨åŠ¨æ€çƒ­åŒ–è¿‡ç¨‹ä¸­ä¿æŒé¢„æµ‹ç²¾åº¦ã€‚è¿™äº›ç»“æœå»ºè®®ï¼Œç›‘ç£å­¦ä¹ å¯ä»¥å…‹æœè¶…å†·åŸå­æµ‹æ¸©ä¸­çš„ä¼ ç»Ÿå±€é™æ€§ï¼Œæ‰©å±•åˆ°æ›´å¹¿æ³›çš„å‡ ä½•é…ç½®ã€æ¸©åº¦èŒƒå›´å’Œé¢å¤–å‚æ•°ï¼Œå¯èƒ½ä¼šå®ç°å¯¹é‡å­æ°”ä½“å®éªŒçš„å…¨é¢å®æ—¶åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å†·ç»è‰²æ°”ä½“ä¸­çƒ­åŠ›å­¦å‚æ•°çš„ç²¾ç¡®æµ‹å®šé¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºä¼ ç»Ÿæµ‹é‡æŠ€æœ¯å…·æœ‰ç ´åæ€§å’Œå®éªŒçš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>äººå·¥æ™ºèƒ½æ–¹æ³•å¯ç”¨äºå¿«é€Ÿã€éç ´åæ€§ä¼°è®¡åŒ–å­¦åŠ¿å’Œæ¸©åº¦ã€‚</li>
<li>ä½¿ç”¨çš„å·ç§¯ç¥ç»ç½‘ç»œèƒ½å¤Ÿåœ¨å‡†äºŒç»´â€œç…é¥¼â€å‡èšç‰©å’Œç‰¹å®šçš„é™·é˜±é…ç½®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶å¿«é€Ÿæå–å‚æ•°ã€‚</li>
<li>æ¨¡å‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åœ¨ä¸åŒçš„é™·é˜±å‡ ä½•å½¢çŠ¶å’Œçƒ­åŒ–åŠ¨åŠ›å­¦ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
<li>å¯¹äºç¯å½¢é™·é˜±ä¸­çš„ç»è‰²æ°”ä½“ï¼Œæ¨¡å‹å¯ä»¥é«˜ç²¾åº¦åœ°ä¼°è®¡çƒ­åŠ›å­¦å‚æ•°ï¼Œå³ä½¿åœ¨åŠ¨æ€çƒ­åŒ–è¿‡ç¨‹ä¸­ä¹Ÿèƒ½ä¿æŒå‡†ç¡®æ€§ã€‚</li>
<li>ç›‘ç£å­¦ä¹ å¯ä»¥å…‹æœè¶…å†·åŸå­æµ‹æ¸©ä¸­çš„ä¼ ç»Ÿå±€é™æ€§ï¼Œå¹¶å…·æœ‰åœ¨æ›´å¹¿æ³›çš„å‡ ä½•é…ç½®ã€æ¸©åº¦èŒƒå›´å’Œé¢å¤–å‚æ•°ä¸Šè¿›è¡Œæ‰©å±•çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16925">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94d6e7e171ca6a2456bf7f49fdf67b7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97d1128cbffec009ba75e94ce5310e55.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="With-Limited-Data-for-Multimodal-Alignment-Let-the-STRUCTURE-Guide-You"><a href="#With-Limited-Data-for-Multimodal-Alignment-Let-the-STRUCTURE-Guide-You" class="headerlink" title="With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You"></a>With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You</h2><p><strong>Authors:Fabian GrÃ¶ger, Shuo Wen, Huyen Le, Maria BrbiÄ‡</strong></p>
<p>Multimodal models have demonstrated powerful capabilities in complex tasks requiring multimodal alignment including zero-shot classification and cross-modal retrieval. However, existing models typically rely on millions of paired multimodal samples, which are prohibitively expensive or infeasible to obtain in many domains. In this work, we explore the feasibility of building multimodal models with limited amount of paired data by aligning pretrained unimodal foundation models. We show that high-quality alignment is possible with as few as tens of thousands of paired samples$\unicode{x2013}$less than $1%$ of the data typically used in the field. To achieve this, we introduce STRUCTURE, an effective regularization technique that preserves the neighborhood geometry of the latent space of unimodal encoders. Additionally, we show that aligning last layers is often suboptimal and demonstrate the benefits of aligning the layers with the highest representational similarity across modalities. These two components can be readily incorporated into existing alignment methods, yielding substantial gains across 24 zero-shot image classification and retrieval benchmarks, with average relative improvement of $51.6%$ in classification and $91.8%$ in retrieval tasks. Our results highlight the effectiveness and broad applicability of our framework for limited-sample multimodal learning and offer a promising path forward for resource-constrained domains. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ¨¡å‹åœ¨éœ€è¦å¤šæ¨¡æ€å¯¹é½çš„å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸ä¾èµ–äºæ•°ç™¾ä¸‡é…å¯¹çš„å¤šæ¨¡æ€æ ·æœ¬ï¼Œè¿™åœ¨è®¸å¤šé¢†åŸŸä¸­æ˜¯éš¾ä»¥è·å¾—æˆ–æˆæœ¬é«˜æ˜‚çš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†é€šè¿‡å¯¹é½é¢„è®­ç»ƒçš„å•æ¨¡æ€åŸºç¡€æ¨¡å‹æ¥æ„å»ºå¤šæ¨¡æ€æ¨¡å‹åœ¨æœ‰é™é…å¯¹æ•°æ®ä¸‹çš„å¯è¡Œæ€§ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä»…ä½¿ç”¨æ•°ä¸‡ä¸ªé…å¯¹æ ·æœ¬ï¼ˆä¸åˆ°è¯¥é¢†åŸŸé€šå¸¸ä½¿ç”¨çš„æ•°æ®çš„1%ï¼‰å°±èƒ½å®ç°é«˜è´¨é‡çš„å¯¹é½ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†STRUCTUREï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œèƒ½å¤Ÿä¿ç•™å•æ¨¡æ€ç¼–ç å™¨æ½œåœ¨ç©ºé—´çš„é‚»åŸŸå‡ ä½•ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜å¯¹é½æœ€åä¸€å±‚é€šå¸¸æ˜¯æ¬¡ä¼˜çš„ï¼Œå¹¶å±•ç¤ºäº†å¯¹é½å…·æœ‰æœ€é«˜è·¨æ¨¡æ€ä»£è¡¨æ€§ç›¸ä¼¼æ€§çš„å±‚çš„å¥½å¤„ã€‚è¿™ä¸¤ä¸ªç»„ä»¶å¯ä»¥è½»æ¾åœ°èå…¥ç°æœ‰çš„å¯¹é½æ–¹æ³•ï¼Œåœ¨24ä¸ªé›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œæ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œåˆ†ç±»ä»»åŠ¡çš„å¹³å‡ç›¸å¯¹æ”¹è¿›ä¸º51.6%ï¼Œæ£€ç´¢ä»»åŠ¡çš„å¹³å‡ç›¸å¯¹æ”¹è¿›ä¸º91.8%ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†æ¡†æ¶åœ¨æœ‰é™æ ·æœ¬å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ï¼Œå¹¶ä¸ºèµ„æºå—é™é¢†åŸŸæä¾›äº†å‰æ™¯å¹¿é˜”çš„å‰è¿›è·¯å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16895v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åœ¨æœ‰é™çš„é…å¯¹æ•°æ®ä¸‹æ„å»ºå¤šæ¨¡æ€æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œé€šè¿‡å¯¹é¢„è®­ç»ƒçš„å•æ¨¡æ€åŸºç¡€æ¨¡å‹è¿›è¡Œå¯¹é½å®ç°ã€‚å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„æ­£åˆ™åŒ–æŠ€æœ¯STRUCTUREï¼Œèƒ½å¤Ÿä¿ç•™å•æ¨¡æ€ç¼–ç å™¨æ½œåœ¨ç©ºé—´çš„é‚»è¿‘å‡ ä½•ç»“æ„ã€‚åŒæ—¶ï¼Œæ–‡ç« æŒ‡å‡ºå¯¹é½æœ€åä¸€å±‚é€šå¸¸æ˜¯æ¬¡ä¼˜çš„ï¼Œå¹¶å±•ç¤ºäº†å¯¹é½å…·æœ‰æœ€é«˜ä»£è¡¨æ€§ç›¸ä¼¼åº¦çš„å±‚çš„å¥½å¤„ã€‚è¿™ä¸¤ä¸ªç»„ä»¶å¯ä»¥è½»æ¾åœ°èå…¥ç°æœ‰çš„å¯¹é½æ–¹æ³•ï¼Œåœ¨24ä¸ªé›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œæ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„å¢ç›Šï¼Œåˆ†ç±»ä»»åŠ¡å¹³å‡ç›¸å¯¹æé«˜äº†51.6%ï¼Œæ£€ç´¢ä»»åŠ¡æé«˜äº†91.8%ã€‚ç»“æœå‡¸æ˜¾äº†æœ¬æ–‡æ¡†æ¶åœ¨æœ‰é™æ ·æœ¬å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ï¼Œä¸ºèµ„æºå—é™é¢†åŸŸæä¾›äº†æœ‰å‰æ™¯çš„å‘å±•è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ¨¡å‹åœ¨éœ€è¦å¤šæ¨¡æ€å¯¹é½çš„å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¦‚é›¶æ ·æœ¬åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ã€‚</li>
<li>ç°æœ‰æ¨¡å‹é€šå¸¸ä¾èµ–äºå¤§é‡çš„é…å¯¹å¤šæ¨¡æ€æ ·æœ¬ï¼Œè¿™åœ¨è®¸å¤šé¢†åŸŸä¸­æ˜¯æ˜‚è´µä¸”ä¸å¯è¡Œçš„ã€‚</li>
<li>æœ¬æ–‡æ¢ç´¢äº†åœ¨æœ‰é™çš„é…å¯¹æ•°æ®ä¸‹æ„å»ºå¤šæ¨¡æ€æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œé€šè¿‡å¯¹é¢„è®­ç»ƒçš„å•æ¨¡æ€åŸºç¡€æ¨¡å‹è¿›è¡Œå¯¹é½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åä¸ºSTRUCTUREçš„æœ‰æ•ˆæ­£åˆ™åŒ–æŠ€æœ¯ï¼Œèƒ½å¤Ÿä¿ç•™å•æ¨¡æ€ç¼–ç å™¨æ½œåœ¨ç©ºé—´çš„é‚»è¿‘å‡ ä½•ç»“æ„ã€‚</li>
<li>å¯¹é½æœ€åä¸€å±‚é€šå¸¸æ˜¯æ¬¡ä¼˜çš„ï¼Œè€Œå¯¹é½å…·æœ‰æœ€é«˜ä»£è¡¨æ€§ç›¸ä¼¼åº¦çš„å±‚èƒ½å¸¦æ¥æ›´å¤§å¥½å¤„ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨é›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16895">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae2990c0bcb5a4192dd55f289b512f21.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-44c08b3ac616c0dcf13448e1db574860.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acd307cb3629fc2ffd0ed7369507a593.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af4f9407c6f663205779e4eeb20ea5c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11075165cd2d197943ddb84dcce39646.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7aeab09d7ded1f5da050d56e98ee22b1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Generalized-Category-Discovery-With-Retrieval-Guided-Decision-Boundary-Enhancement"><a href="#Few-Shot-Generalized-Category-Discovery-With-Retrieval-Guided-Decision-Boundary-Enhancement" class="headerlink" title="Few-Shot Generalized Category Discovery With Retrieval-Guided Decision   Boundary Enhancement"></a>Few-Shot Generalized Category Discovery With Retrieval-Guided Decision   Boundary Enhancement</h2><p><strong>Authors:Yunhan Ren, Feng Luo, Siyu Huang</strong></p>
<p>While existing Generalized Category Discovery (GCD) models have achieved significant success, their performance with limited labeled samples and a small number of known categories remains largely unexplored. In this work, we introduce the task of Few-shot Generalized Category Discovery (FSGCD), aiming to achieve competitive performance in GCD tasks under conditions of known information scarcity. To tackle this challenge, we propose a decision boundary enhancement framework with affinity-based retrieval. Our framework is designed to learn the decision boundaries of known categories and transfer these boundaries to unknown categories. First, we use a decision boundary pre-training module to mitigate the overfitting of pre-trained information on known category boundaries and improve the learning of these decision boundaries using labeled samples. Second, we implement a two-stage retrieval-guided decision boundary optimization strategy. Specifically, this strategy further enhances the severely limited known boundaries by using affinity-retrieved pseudo-labeled samples. Then, these refined boundaries are applied to unknown clusters via guidance from affinity-based feature retrieval. Experimental results demonstrate that our proposed method outperforms existing methods on six public GCD benchmarks under the FSGCD setting. The codes are available at: <a target="_blank" rel="noopener" href="https://github.com/Ryh1218/FSGCD">https://github.com/Ryh1218/FSGCD</a> </p>
<blockquote>
<p>è™½ç„¶ç°æœ‰çš„å¹¿ä¹‰ç±»åˆ«å‘ç°ï¼ˆGCDï¼‰æ¨¡å‹å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å®ƒä»¬åœ¨æœ‰é™æ ‡ç­¾æ ·æœ¬å’Œå·²çŸ¥ç±»åˆ«æ•°é‡è¾ƒå°‘çš„æƒ…å†µä¸‹çš„æ€§èƒ½ä»ç„¶å°šæœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å°æ ·æœ¬å¹¿ä¹‰ç±»åˆ«å‘ç°ï¼ˆFSGCDï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨åœ¨å·²çŸ¥ä¿¡æ¯ç¨€ç¼ºçš„æ¡ä»¶ä¸‹å®ç°GCDä»»åŠ¡çš„ç«äº‰æ€§èƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºäº²å’Œæ€§æ£€ç´¢çš„å†³ç­–è¾¹ç•Œå¢å¼ºæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ—¨åœ¨å­¦ä¹ å·²çŸ¥ç±»åˆ«çš„å†³ç­–è¾¹ç•Œï¼Œå¹¶å°†è¿™äº›è¾¹ç•Œè½¬ç§»åˆ°æœªçŸ¥ç±»åˆ«ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨å†³ç­–è¾¹ç•Œé¢„è®­ç»ƒæ¨¡å—æ¥ç¼“è§£å¯¹å·²çŸ¥ç±»åˆ«è¾¹ç•Œçš„é¢„è®­ç»ƒä¿¡æ¯çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¹¶åˆ©ç”¨æ ‡ç­¾æ ·æœ¬æ”¹è¿›è¿™äº›å†³ç­–è¾¹ç•Œçš„å­¦ä¹ ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ£€ç´¢å¼•å¯¼å†³ç­–è¾¹ç•Œä¼˜åŒ–ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥ç­–ç•¥é€šè¿‡ä½¿ç”¨äº²å’Œæ€§æ£€ç´¢çš„ä¼ªæ ‡ç­¾æ ·æœ¬æ¥è¿›ä¸€æ­¥å¢å¼ºä¸¥é‡æœ‰é™çš„å·²çŸ¥è¾¹ç•Œã€‚ç„¶åï¼Œè¿™äº›ä¼˜åŒ–åçš„è¾¹ç•Œé€šè¿‡åŸºäºäº²å’Œæ€§çš„ç‰¹å¾æ£€ç´¢çš„æŒ‡å¯¼åº”ç”¨äºæœªçŸ¥é›†ç¾¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨FSGCDè®¾ç½®ä¸‹çš„å…­ä¸ªå…¬å…±GCDåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Ryh1218/FSGCD">https://github.com/Ryh1218/FSGCD</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16728v1">PDF</a> Accepted by ICMR 2025</p>
<p><strong>Summary</strong><br>å°‘é‡æ ·æœ¬ä¸‹çš„å¹¿ä¹‰ç±»åˆ«å‘ç°ï¼ˆFSGCDï¼‰ç ”ç©¶æå‡ºäº†æ–°çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚é€šè¿‡å†³ç­–è¾¹ç•Œå¢å¼ºæ¡†æ¶ä¸åŸºäºäº²å’ŒåŠ›çš„æ£€ç´¢ï¼Œå¯¹å·²çŸ¥ç±»åˆ«çš„å†³ç­–è¾¹ç•Œè¿›è¡Œå­¦ä¹ å¹¶åº”ç”¨äºæœªçŸ¥ç±»åˆ«ã€‚é¦–å…ˆé¢„è®­ç»ƒå†³ç­–è¾¹ç•Œæ¨¡å—å‡è½»å·²çŸ¥ç±»åˆ«è¾¹ç•Œä¸Šè¿‡æ‹Ÿåˆç°è±¡ï¼Œå¹¶ä½¿ç”¨æ ‡ç­¾æ ·æœ¬æ”¹å–„å†³ç­–è¾¹ç•Œå­¦ä¹ ã€‚æ¥ç€å®ç°ä¸¤é˜¶æ®µæ£€ç´¢å¼•å¯¼å†³ç­–è¾¹ç•Œä¼˜åŒ–ç­–ç•¥ï¼Œåˆ©ç”¨äº²å’ŒåŠ›æ£€ç´¢å¾—åˆ°çš„ä¼ªæ ‡ç­¾æ ·æœ¬è¿›ä¸€æ­¥å¢å¼ºå·²çŸ¥è¾¹ç•Œçš„å±€é™æ€§ï¼Œå¹¶åº”ç”¨äºæœªçŸ¥é›†ç¾¤çš„äº²å’Œç‰¹å¾æ£€ç´¢å¼•å¯¼ä¸­ã€‚åœ¨å…­ä¸ªå…¬å…±å¹¿ä¹‰ç±»åˆ«å‘ç°åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FSGCDä»»åŠ¡æ—¨åœ¨è§£å†³ç°æœ‰å¹¿ä¹‰ç±»åˆ«å‘ç°æ¨¡å‹åœ¨æœ‰é™æ ‡ç­¾æ ·æœ¬å’Œå·²çŸ¥ç±»åˆ«æ•°é‡è¾ƒå°‘çš„æƒ…å†µä¸‹çš„æ€§èƒ½é—®é¢˜ã€‚</li>
<li>æå‡ºå†³ç­–è¾¹ç•Œå¢å¼ºæ¡†æ¶æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œç»“åˆäº†å†³ç­–è¾¹ç•Œé¢„è®­ç»ƒæ¨¡å—ä¸åŸºäºäº²å’ŒåŠ›çš„æ£€ç´¢ã€‚</li>
<li>å†³ç­–è¾¹ç•Œé¢„è®­ç»ƒæ¨¡å—å¯ä»¥å‡è½»åœ¨å·²çŸ¥ç±»åˆ«è¾¹ç•Œä¸Šçš„è¿‡æ‹Ÿåˆç°è±¡ï¼Œå¹¶æ”¹å–„å†³ç­–è¾¹ç•Œçš„å­¦ä¹ ã€‚</li>
<li>å®ç°ä¸¤é˜¶æ®µæ£€ç´¢å¼•å¯¼å†³ç­–è¾¹ç•Œä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡äº²å’ŒåŠ›æ£€ç´¢å¾—åˆ°ä¼ªæ ‡ç­¾æ ·æœ¬ä»¥å¢å¼ºå·²çŸ¥è¾¹ç•Œçš„å±€é™æ€§ã€‚</li>
<li>å°†ä¼˜åŒ–åçš„å†³ç­–è¾¹ç•Œåº”ç”¨äºæœªçŸ¥ç±»åˆ«çš„èšç±»ä¸­ï¼Œé€šè¿‡äº²å’Œç‰¹å¾æ£€ç´¢è¿›è¡Œå¼•å¯¼ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å…±å¹¿ä¹‰ç±»åˆ«å‘ç°åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æ˜¾ç¤ºå‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16728">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7f864816f4694cac414377038602fde6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eace5338c631d3d091a90e2a3a61780c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb4096f3067fba1b4ac7a3ea51187a6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b976571b687331469062f5285dfb4f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-937feb9c5825d419091788161616e4fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-910e964f482dd3833be7b9a2c97f3bad.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Relic-Enhancing-Reward-Model-Generalization-for-Low-Resource-Indic-Languages-with-Few-Shot-Examples"><a href="#Relic-Enhancing-Reward-Model-Generalization-for-Low-Resource-Indic-Languages-with-Few-Shot-Examples" class="headerlink" title="Relic: Enhancing Reward Model Generalization for Low-Resource Indic   Languages with Few-Shot Examples"></a>Relic: Enhancing Reward Model Generalization for Low-Resource Indic   Languages with Few-Shot Examples</h2><p><strong>Authors:Soumya Suvra Ghosal, Vaibhav Singh, Akash Ghosh, Soumyabrata Pal, Subhadip Baidya, Sriparna Saha, Dinesh Manocha</strong></p>
<p>Reward models are essential for aligning large language models (LLMs) with human preferences. However, most open-source multilingual reward models are primarily trained on preference datasets in high-resource languages, resulting in unreliable reward signals for low-resource Indic languages. Collecting large-scale, high-quality preference data for these languages is prohibitively expensive, making preference-based training approaches impractical. To address this challenge, we propose RELIC, a novel in-context learning framework for reward modeling in low-resource Indic languages. RELIC trains a retriever with a pairwise ranking objective to select in-context examples from auxiliary high-resource languages that most effectively highlight the distinction between preferred and less-preferred responses. Extensive experiments on three preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art open-source reward models demonstrate that RELIC significantly improves reward model accuracy for low-resource Indic languages, consistently outperforming existing example selection methods. For example, on Bodo-a low-resource Indic language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13% improvement in accuracy over zero-shot prompting and state-of-the-art example selection method, respectively. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹å¯¹äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å¼€æºçš„å¤šè¯­è¨€å¥–åŠ±æ¨¡å‹ä¸»è¦åœ¨é«˜èµ„æºè¯­è¨€çš„åå¥½æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´å¯¹ä½èµ„æºå°åº¦è¯­ç³»çš„å¥–åŠ±ä¿¡å·ä¸å¯é ã€‚å¯¹äºè¿™äº›è¯­è¨€ï¼Œæ”¶é›†å¤§è§„æ¨¡é«˜è´¨é‡åå¥½æ•°æ®æˆæœ¬é«˜æ˜‚ï¼Œä½¿å¾—åŸºäºåå¥½çš„è®­ç»ƒæ–¹æ³•ä¸åˆ‡å®é™…ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†RELICï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä½èµ„æºå°åº¦è¯­ç³»å¥–åŠ±å»ºæ¨¡çš„æ–°å‹ä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ã€‚RELICè®­ç»ƒäº†ä¸€ä¸ªæ£€ç´¢å™¨ï¼Œä½¿ç”¨é…å¯¹æ’åç›®æ ‡æ¥é€‰æ‹©è¾…åŠ©é«˜èµ„æºè¯­è¨€ä¸­æœ€èƒ½çªå‡ºé¦–é€‰å’Œè¾ƒå°‘é¦–é€‰å›åº”ä¹‹é—´åŒºåˆ«çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ã€‚åœ¨PKU-SafeRLHFã€WebGPTå’ŒHH-RLHFä¸‰ä¸ªåå¥½æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œä½¿ç”¨æœ€å…ˆè¿›çš„å¼€æºå¥–åŠ±æ¨¡å‹è¯æ˜ï¼ŒRELICæ˜¾è‘—æé«˜äº†ä½èµ„æºå°åº¦è¯­ç³»çš„å¥–åŠ±æ¨¡å‹ç²¾åº¦ï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äºç°æœ‰çš„ç¤ºä¾‹é€‰æ‹©æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨èµ„æºè´«ä¹çš„å°åº¦è¯­åšå¤šè¯­ä¸­ï¼Œä½¿ç”¨LLaMA-3.2-3Bå¥–åŠ±æ¨¡å‹ï¼ŒRELICç›¸å¯¹äºé›¶æ ·æœ¬æç¤ºå’Œæœ€æ–°ç¤ºä¾‹é€‰æ‹©æ–¹æ³•ï¼Œå‡†ç¡®åº¦åˆ†åˆ«æé«˜äº†12.81%å’Œ10.13%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16502v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹ä½èµ„æºå°åœ°è¯­è¯­è¨€å¥–åŠ±æ¨¡å‹çš„æ–°æ¡†æ¶RELICã€‚RELICé€šè¿‡åˆ©ç”¨è¾…åŠ©é«˜èµ„æºè¯­è¨€ä¸­çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œè®­ç»ƒä¸€ä¸ªæ£€ç´¢å™¨ä»¥é€‰æ‹©æœ€èƒ½åŒºåˆ†åå¥½å“åº”å’Œéåå¥½å“åº”çš„ç¤ºä¾‹ã€‚å®éªŒè¯æ˜ï¼ŒRELICæ˜¾è‘—æé«˜äº†ä½èµ„æºå°åœ°è¯­è¯­è¨€çš„å¥–åŠ±æ¨¡å‹å‡†ç¡®æ€§ï¼Œå¹¶ä¼˜äºç°æœ‰çš„ç¤ºä¾‹é€‰æ‹©æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹å¯¹äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½è‡³å…³é‡è¦ã€‚</li>
<li>å¤§å¤šæ•°å¼€æºå¤šè¯­è¨€å¥–åŠ±æ¨¡å‹ä¸»è¦åœ¨é«˜èµ„æºè¯­è¨€åå¥½æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´å¯¹ä½èµ„æºå°åœ°è¯­è¯­è¨€çš„å¥–åŠ±ä¿¡å·ä¸å¯é ã€‚</li>
<li>æ”¶é›†å¤§è§„æ¨¡é«˜è´¨é‡åå¥½æ•°æ®å¯¹äºä½èµ„æºè¯­è¨€æ˜¯ä¸å®é™…çš„ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºRELICçš„æ–°å‹ä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåœ¨ä½èµ„æºå°åœ°è¯­è¯­è¨€ä¸­è¿›è¡Œå¥–åŠ±å»ºæ¨¡ã€‚</li>
<li>RELICé€šè¿‡ä»è¾…åŠ©é«˜èµ„æºè¯­è¨€ä¸­é€‰æ‹©ä¸Šä¸‹æ–‡ç¤ºä¾‹æ¥è®­ç»ƒæ£€ç´¢å™¨ï¼Œè¿™äº›ç¤ºä¾‹æœ€èƒ½åŒºåˆ†åå¥½å“åº”ä¸éåå¥½å“åº”ã€‚</li>
<li>å®éªŒè¯æ˜RELICåœ¨ä½èµ„æºå°åœ°è¯­è¯­è¨€çš„å¥–åŠ±æ¨¡å‹å‡†ç¡®æ€§ä¸Šæ˜¾è‘—æé«˜ï¼Œä¼˜äºç°æœ‰çš„ç¤ºä¾‹é€‰æ‹©æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16502">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bad72d71935f9a0e020b01c04f4ff53c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-763a4b24e198096a9ec34af2a8a79f5a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SynPo-Boosting-Training-Free-Few-Shot-Medical-Segmentation-via-High-Quality-Negative-Prompts"><a href="#SynPo-Boosting-Training-Free-Few-Shot-Medical-Segmentation-via-High-Quality-Negative-Prompts" class="headerlink" title="SynPo: Boosting Training-Free Few-Shot Medical Segmentation via   High-Quality Negative Prompts"></a>SynPo: Boosting Training-Free Few-Shot Medical Segmentation via   High-Quality Negative Prompts</h2><p><strong>Authors:Yufei Liu, Haoke Xiao, Jiaxing Chai, Yongcun Zhang, Rong Wang, Zijie Meng, Zhiming Luo</strong></p>
<p>The advent of Large Vision Models (LVMs) offers new opportunities for few-shot medical image segmentation. However, existing training-free methods based on LVMs fail to effectively utilize negative prompts, leading to poor performance on low-contrast medical images. To address this issue, we propose SynPo, a training-free few-shot method based on LVMs (e.g., SAM), with the core insight: improving the quality of negative prompts. To select point prompts in a more reliable confidence map, we design a novel Confidence Map Synergy Module by combining the strengths of DINOv2 and SAM. Based on the confidence map, we select the top-k pixels as the positive points set and choose the negative points set using a Gaussian distribution, followed by independent K-means clustering for both sets. Then, these selected points are leveraged as high-quality prompts for SAM to get the segmentation results. Extensive experiments demonstrate that SynPo achieves performance comparable to state-of-the-art training-based few-shot methods. </p>
<blockquote>
<p>å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰çš„å‡ºç°ä¸ºå°‘æ•°åŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼ŒåŸºäºLVMsçš„æ— è®­ç»ƒæ–¹æ³•æ— æ³•æœ‰æ•ˆåˆ©ç”¨è´Ÿæç¤ºï¼Œå¯¼è‡´åœ¨ä½å¯¹æ¯”åº¦åŒ»å­¦å›¾åƒä¸Šçš„æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SynPoï¼Œè¿™æ˜¯ä¸€ç§åŸºäºLVMsçš„æ— è®­ç»ƒå°‘æ•°æ–¹æ³•ï¼ˆä¾‹å¦‚SAMï¼‰ï¼Œå…¶æ ¸å¿ƒè§è§£æ˜¯æé«˜è´Ÿæç¤ºçš„è´¨é‡ã€‚ä¸ºäº†åœ¨æ›´å¯é çš„ç½®ä¿¡å›¾ä¸­é€‰æ‹©ç‚¹æç¤ºï¼Œæˆ‘ä»¬ç»“åˆäº†DINOv2å’ŒSAMçš„ä¼˜ç‚¹ï¼Œè®¾è®¡äº†ä¸€ç§æ–°å‹çš„ç½®ä¿¡å›¾ååŒæ¨¡å—ã€‚åŸºäºç½®ä¿¡å›¾ï¼Œæˆ‘ä»¬é€‰æ‹©å‰kä¸ªåƒç´ ä½œä¸ºæ­£ç‚¹é›†ï¼Œä½¿ç”¨é«˜æ–¯åˆ†å¸ƒé€‰æ‹©è´Ÿç‚¹é›†ï¼Œç„¶åå¯¹è¿™ä¸¤ä¸ªé›†åˆåˆ†åˆ«è¿›è¡Œç‹¬ç«‹çš„K-å‡å€¼èšç±»ã€‚ç„¶åï¼Œè¿™äº›é€‰å®šçš„ç‚¹è¢«ç”¨ä½œé«˜è´¨é‡æç¤ºï¼Œä¸ºSAMæä¾›åˆ†å‰²ç»“æœã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSynPoçš„æ€§èƒ½å¯ä¸æœ€å…ˆè¿›çš„åŸºäºè®­ç»ƒå°‘æ•°æ–¹æ³•ç›¸åª²ç¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15153v2">PDF</a> MICCAI 2025 Early Accept. Project Page:   <a target="_blank" rel="noopener" href="https://liu-yufei.github.io/synpo-project-page/">https://liu-yufei.github.io/synpo-project-page/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰ä¸ºå°‘æ ·æœ¬åŒ»ç–—å›¾åƒåˆ†å‰²æä¾›äº†æ–°çš„æœºä¼šï¼Œä½†ç°æœ‰çš„åŸºäºLVMsçš„æ— è®­ç»ƒæ–¹æ³•æ— æ³•æœ‰æ•ˆåˆ©ç”¨è´Ÿæç¤ºï¼Œå¯¼è‡´åœ¨ä½å¯¹æ¯”åº¦åŒ»ç–—å›¾åƒä¸Šçš„è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºLVMsçš„æ— è®­ç»ƒå°‘æ ·æœ¬æ–¹æ³•SynPoï¼Œå…¶æ ¸å¿ƒæ˜¯æ”¹è¿›è´Ÿæç¤ºçš„è´¨é‡ã€‚é€šè¿‡ç»“åˆDINOv2å’ŒSAMçš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„ç½®ä¿¡å›¾ååŒæ¨¡å—ï¼Œä»¥æ›´å¯é çš„æ–¹å¼é€‰æ‹©ç‚¹æç¤ºã€‚åŸºäºç½®ä¿¡å›¾ï¼Œæˆ‘ä»¬é€‰æ‹©å‰kä¸ªåƒç´ ä½œä¸ºæ­£ç‚¹é›†ï¼Œä½¿ç”¨é«˜æ–¯åˆ†å¸ƒé€‰æ‹©è´Ÿç‚¹é›†ï¼Œç„¶åå¯¹è¿™ä¸¤ç»„è¿›è¡Œç‹¬ç«‹çš„K-meansèšç±»ã€‚è¿™äº›é€‰å®šçš„ç‚¹è¢«ç”¨ä½œSAMçš„é«˜è´¨é‡æç¤ºæ¥è·å¾—åˆ†å‰²ç»“æœã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSynPoçš„æ€§èƒ½ä¸åŸºäºè®­ç»ƒçš„å°‘æ ·æœ¬æ–¹æ³•ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰åœ¨å°‘æ ·æœ¬åŒ»ç–—å›¾åƒåˆ†å‰²ä¸Šå…·æœ‰æ½œåŠ›ã€‚</li>
<li>ç°æœ‰åŸºäºLVMsçš„æ— è®­ç»ƒæ–¹æ³•åœ¨å¤„ç†ä½å¯¹æ¯”åº¦åŒ»ç–—å›¾åƒæ—¶è¡¨ç°ä¸ä½³ã€‚</li>
<li>SynPoæ–¹æ³•é€šè¿‡æ”¹è¿›è´Ÿæç¤ºçš„è´¨é‡æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>SynPoç»“åˆäº†DINOv2å’ŒSAMçš„ä¼˜ç‚¹ï¼Œè®¾è®¡äº†ä¸€ç§æ–°çš„ç½®ä¿¡å›¾ååŒæ¨¡å—ã€‚</li>
<li>åŸºäºç½®ä¿¡å›¾ï¼ŒSynPoé€‰æ‹©æ­£ã€è´Ÿç‚¹é›†ï¼Œå¹¶åˆ©ç”¨è¿™äº›ç‚¹ä½œä¸ºé«˜è´¨é‡æç¤ºè¿›è¡Œå›¾åƒåˆ†å‰²ã€‚</li>
<li>SynPoçš„æ€§èƒ½ä¸åŸºäºè®­ç»ƒçš„å°‘æ ·æœ¬æ–¹æ³•ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15153">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-96dbe471f0ef20ab45c582024ead24a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a64112893cb38c03306e6d6cd7e48c88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3b9f688b027c6b4228079a26d761301.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="IQE-CLIP-Instance-aware-Query-Embedding-for-Zero-Few-shot-Anomaly-Detection-in-Medical-Domain"><a href="#IQE-CLIP-Instance-aware-Query-Embedding-for-Zero-Few-shot-Anomaly-Detection-in-Medical-Domain" class="headerlink" title="IQE-CLIP: Instance-aware Query Embedding for Zero-&#x2F;Few-shot Anomaly   Detection in Medical Domain"></a>IQE-CLIP: Instance-aware Query Embedding for Zero-&#x2F;Few-shot Anomaly   Detection in Medical Domain</h2><p><strong>Authors:Hong Huang, Weixiang Sun, Zhijian Wu, Jingwen Niu, Donghuan Lu, Xian Wu, Yefeng Zheng</strong></p>
<p>Recently, the rapid advancements of vision-language models, such as CLIP, leads to significant progress in zero-&#x2F;few-shot anomaly detection (ZFSAD) tasks. However, most existing CLIP-based ZFSAD methods commonly assume prior knowledge of categories and rely on carefully crafted prompts tailored to specific scenarios. While such meticulously designed text prompts effectively capture semantic information in the textual space, they fall short of distinguishing normal and anomalous instances within the joint embedding space. Moreover, these ZFSAD methods are predominantly explored in industrial scenarios, with few efforts conducted to medical tasks. To this end, we propose an innovative framework for ZFSAD tasks in medical domain, denoted as IQE-CLIP. We reveal that query embeddings, which incorporate both textual and instance-aware visual information, are better indicators for abnormalities. Specifically, we first introduce class-based prompting tokens and learnable prompting tokens for better adaptation of CLIP to the medical domain. Then, we design an instance-aware query module (IQM) to extract region-level contextual information from both text prompts and visual features, enabling the generation of query embeddings that are more sensitive to anomalies. Extensive experiments conducted on six medical datasets demonstrate that IQE-CLIP achieves state-of-the-art performance on both zero-shot and few-shot tasks. We release our code and data at <a target="_blank" rel="noopener" href="https://github.com/hongh0/IQE-CLIP/">https://github.com/hongh0/IQE-CLIP/</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†é›¶&#x2F;å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZFSADï¼‰ä»»åŠ¡çš„æ˜¾è‘—è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„åŸºäºCLIPçš„ZFSADæ–¹æ³•é€šå¸¸å‡è®¾å¯¹ç±»åˆ«çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶ä¾èµ–äºé’ˆå¯¹ç‰¹å®šåœºæ™¯ç²¾å¿ƒè®¾è®¡çš„æç¤ºã€‚è™½ç„¶è¿™ç§ç²¾å¿ƒè®¾è®¡æ–‡æœ¬æç¤ºå¯ä»¥æœ‰æ•ˆåœ°æ•è·æ–‡æœ¬ç©ºé—´ä¸­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä½†å®ƒä»¬æ— æ³•åœ¨è”åˆåµŒå…¥ç©ºé—´ä¸­åŒºåˆ†æ­£å¸¸å’Œå¼‚å¸¸çš„å®ä¾‹ã€‚æ­¤å¤–ï¼Œè¿™äº›ZFSADæ–¹æ³•ä¸»è¦åœ¨å·¥ä¸šåœºæ™¯ä¸­å¾—åˆ°äº†æ¢ç´¢ï¼Œè€Œåœ¨åŒ»å­¦ä»»åŠ¡ä¸­çš„ç ”ç©¶å¾ˆå°‘ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é’ˆå¯¹åŒ»å­¦é¢†åŸŸçš„ZFSADä»»åŠ¡æå‡ºäº†ä¸€ä¸ªåˆ›æ–°æ¡†æ¶ï¼Œç§°ä¸ºIQE-CLIPã€‚æˆ‘ä»¬å‘ç°ï¼Œèåˆæ–‡æœ¬å’Œå®ä¾‹æ„ŸçŸ¥è§†è§‰ä¿¡æ¯çš„æŸ¥è¯¢åµŒå…¥æ˜¯æ›´å¥½çš„å¼‚å¸¸æŒ‡ç¤ºå™¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥åŸºäºç±»åˆ«çš„æç¤ºä»¤ç‰Œå’Œå¯å­¦ä¹ çš„æç¤ºä»¤ç‰Œï¼Œä»¥æ›´å¥½åœ°é€‚åº”CLIPåœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå®ä¾‹æ„ŸçŸ¥æŸ¥è¯¢æ¨¡å—ï¼ˆIQMï¼‰ï¼Œä»æ–‡æœ¬æç¤ºå’Œè§†è§‰ç‰¹å¾ä¸­æå–åŒºåŸŸçº§åˆ«çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œç”Ÿæˆå¯¹å¼‚å¸¸æ›´æ•æ„Ÿçš„æŸ¥è¯¢åµŒå…¥ã€‚åœ¨å…­ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒIQE-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ä»»åŠ¡ä¸Šéƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/hongh0/IQE-CLIP/">https://github.com/hongh0/IQE-CLIP/</a>ä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10730v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨é›¶&#x2F;å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZFSADï¼‰ä»»åŠ¡ä¸­ï¼ŒåŸºäºCLIPçš„è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–°è¿›å±•åŠå…¶åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¾èµ–ç‰¹å®šç±»åˆ«å…ˆéªŒçŸ¥è¯†å’Œæ–‡æœ¬æç¤ºçš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶IQE-CLIPã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆæ–‡æœ¬å’Œå®ä¾‹æ„ŸçŸ¥çš„è§†è§‰ä¿¡æ¯ï¼Œç”ŸæˆæŸ¥è¯¢åµŒå…¥ï¼Œä»¥æ›´æœ‰æ•ˆåœ°æ£€æµ‹å¼‚å¸¸ã€‚å®éªŒè¯æ˜ï¼ŒIQE-CLIPåœ¨å…­ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šå®ç°äº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ä»»åŠ¡çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ¨¡å‹åœ¨é›¶&#x2F;å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ï¼ˆZFSADï¼‰ä¸­æœ‰æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç°æœ‰CLIP-based ZFSADæ–¹æ³•ä¾èµ–ç±»åˆ«å…ˆéªŒçŸ¥è¯†å’Œé’ˆå¯¹ç‰¹å®šåœºæ™¯è®¾è®¡çš„æ–‡æœ¬æç¤ºã€‚</li>
<li>IQE-CLIPæ¡†æ¶ç»“åˆäº†æ–‡æœ¬å’Œå®ä¾‹æ„ŸçŸ¥çš„è§†è§‰ä¿¡æ¯ï¼Œç”ŸæˆæŸ¥è¯¢åµŒå…¥ä»¥æ£€æµ‹å¼‚å¸¸ã€‚</li>
<li>IQE-CLIPå¼•å…¥åŸºäºç±»åˆ«çš„æç¤ºä»¤ç‰Œå’Œå¯å­¦ä¹ çš„æç¤ºä»¤ç‰Œï¼Œä»¥æ›´å¥½åœ°é€‚åº”åŒ»ç–—é¢†åŸŸã€‚</li>
<li>è®¾è®¡äº†å®ä¾‹æ„ŸçŸ¥æŸ¥è¯¢æ¨¡å—ï¼ˆIQMï¼‰ä»¥æå–åŒºåŸŸçº§åˆ«çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæé«˜å¯¹å¼‚å¸¸çš„æ•æ„Ÿæ€§ã€‚</li>
<li>åœ¨å…­ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜IQE-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ä»»åŠ¡ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10730">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5961f2095532a8418992cd84409cddf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f54646641f5923a7ef176de168cb89c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c51ef4e50943405dae4645a0783f91ca.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Towards-Efficient-Few-shot-Graph-Neural-Architecture-Search-via-Partitioning-Gradient-Contribution"><a href="#Towards-Efficient-Few-shot-Graph-Neural-Architecture-Search-via-Partitioning-Gradient-Contribution" class="headerlink" title="Towards Efficient Few-shot Graph Neural Architecture Search via   Partitioning Gradient Contribution"></a>Towards Efficient Few-shot Graph Neural Architecture Search via   Partitioning Gradient Contribution</h2><p><strong>Authors:Wenhao Song, Xuan Wu, Bo Yang, You Zhou, Yubin Xiao, Yanchun Liang, Hongwei Ge, Heow Pueh Lee, Chunguo Wu</strong></p>
<p>To address the weight coupling problem, certain studies introduced few-shot Neural Architecture Search (NAS) methods, which partition the supernet into multiple sub-supernets. However, these methods often suffer from computational inefficiency and tend to provide suboptimal partitioning schemes. To address this problem more effectively, we analyze the weight coupling problem from a novel perspective, which primarily stems from distinct modules in succeeding layers imposing conflicting gradient directions on the preceding layer modules. Based on this perspective, we propose the Gradient Contribution (GC) method that efficiently computes the cosine similarity of gradient directions among modules by decomposing the Vector-Jacobian Product during supernet backpropagation. Subsequently, the modules with conflicting gradient directions are allocated to distinct sub-supernets while similar ones are grouped together. To assess the advantages of GC and address the limitations of existing Graph Neural Architecture Search methods, which are limited to searching a single type of Graph Neural Networks (Message Passing Neural Networks (MPNNs) or Graph Transformers (GTs)), we propose the Unified Graph Neural Architecture Search (UGAS) framework, which explores optimal combinations of MPNNs and GTs. The experimental results demonstrate that GC achieves state-of-the-art (SOTA) performance in supernet partitioning quality and time efficiency. In addition, the architectures searched by UGAS+GC outperform both the manually designed GNNs and those obtained by existing NAS methods. Finally, ablation studies further demonstrate the effectiveness of all proposed methods. </p>
<blockquote>
<p>ä¸ºäº†è§£å†³æƒé‡è€¦åˆé—®é¢˜ï¼Œä¸€äº›ç ”ç©¶å¼•å…¥äº†å°æ ·æœ¬ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰æ–¹æ³•ï¼Œå°†è¶…ç½‘åˆ’åˆ†ä¸ºå¤šä¸ªå­è¶…ç½‘ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å­˜åœ¨è®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå¹¶ä¸”å¾€å¾€æä¾›æ¬¡ä¼˜çš„åˆ’åˆ†æ–¹æ¡ˆã€‚ä¸ºäº†æ›´æœ‰æ•ˆåœ°è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªæ–°çš„è§’åº¦åˆ†æäº†æƒé‡è€¦åˆé—®é¢˜ï¼Œå…¶ä¸»è¦æºäºåç»­å±‚ä¸­çš„ä¸åŒæ¨¡å—å¯¹å‰ä¸€å±‚æ¨¡å—æ–½åŠ å†²çªçš„æ¢¯åº¦æ–¹å‘ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ¢¯åº¦è´¡çŒ®ï¼ˆGCï¼‰æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ†è§£è¶…ç½‘åå‘ä¼ æ’­ä¸­çš„å‘é‡-é›…å¯æ¯”ä¹˜ç§¯ï¼Œæœ‰æ•ˆåœ°è®¡ç®—äº†æ¨¡å—ä¹‹é—´æ¢¯åº¦æ–¹å‘çš„ä½™å¼¦ç›¸ä¼¼æ€§ã€‚éšåï¼Œå…·æœ‰å†²çªæ¢¯åº¦æ–¹å‘çš„æ¨¡å—è¢«åˆ†é…åˆ°ä¸åŒçš„å­è¶…ç½‘ä¸­ï¼Œè€Œç›¸ä¼¼çš„æ¨¡å—åˆ™ç»„åˆåœ¨ä¸€èµ·ã€‚ä¸ºäº†è¯„ä¼°GCçš„ä¼˜åŠ¿ï¼Œå¹¶è§£å†³ç°æœ‰å›¾ç¥ç»ç½‘ç»œæ¶æ„æœç´¢æ–¹æ³•çš„å±€é™æ€§ï¼ˆè¿™äº›æ–¹æ³•ä»…é™äºæœç´¢å•ä¸€ç±»å‹çš„å›¾ç¥ç»ç½‘ç»œï¼Œå¦‚æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œï¼ˆMPNNsï¼‰æˆ–å›¾è½¬æ¢å™¨ï¼ˆGTsï¼‰ï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€å›¾ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆUGASï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ¢ç´¢äº†MPNNså’ŒGTsçš„æœ€ä½³ç»„åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGCåœ¨è¶…ç½‘åˆ’åˆ†è´¨é‡å’Œæ—¶é—´æ•ˆç‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚æ­¤å¤–ï¼Œé€šè¿‡UGAS+GCæœç´¢çš„æ¶æ„ä¼˜äºæ‰‹åŠ¨è®¾è®¡çš„GNNså’Œç°æœ‰NASæ–¹æ³•å¾—åˆ°çš„æ¶æ„ã€‚æœ€åï¼Œæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†æ‰€æœ‰æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01231v2">PDF</a> Accepted by SIGKDD 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰çš„æƒé‡è§†è§’é—®é¢˜ï¼Œæå‡ºäº†æ¢¯åº¦è´¡çŒ®ï¼ˆGCï¼‰æ–¹æ³•å’Œç»Ÿä¸€å›¾ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆUGASï¼‰æ¡†æ¶ã€‚GCæ–¹æ³•é€šè¿‡è®¡ç®—æ¨¡å—é—´æ¢¯åº¦æ–¹å‘çš„ä½™å¼¦ç›¸ä¼¼æ€§æ¥è§£å†³æƒé‡è€¦åˆé—®é¢˜ï¼Œå°†å…·æœ‰å†²çªæ¢¯åº¦æ–¹å‘çš„æ¨¡å—åˆ†é…ç»™ä¸åŒçš„å­è¶…ç½‘ï¼Œè€Œç›¸ä¼¼çš„æ¨¡å—åˆ™ç»„åˆåœ¨ä¸€èµ·ã€‚UGASæ¡†æ¶æ—¨åœ¨æ¢ç´¢æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œï¼ˆMPNNsï¼‰å’Œå›¾å˜æ¢å™¨ï¼ˆGTsï¼‰çš„æœ€ä½³ç»„åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGCåœ¨è¶…ç½‘åˆ’åˆ†è´¨é‡å’Œæ—¶é—´æ•ˆç‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ŒUGAS+GCæœç´¢çš„æ¶æ„è¶…è¶Šäº†æ‰‹åŠ¨è®¾è®¡çš„GNNså’Œç°æœ‰NASæ–¹æ³•çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°çš„è§†è§’æ¥è§£å†³æƒé‡è€¦åˆé—®é¢˜ï¼Œä¸»è¦æºäºè¿ç»­å±‚ä¸­çš„ä¸åŒæ¨¡å—å¯¹å‰é¢å±‚æ¨¡å—æ–½åŠ å†²çªçš„æ¢¯åº¦æ–¹å‘ã€‚</li>
<li>æå‡ºäº†æ¢¯åº¦è´¡çŒ®ï¼ˆGCï¼‰æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—æ¨¡å—é—´æ¢¯åº¦æ–¹å‘çš„ä½™å¼¦ç›¸ä¼¼æ€§æ¥è§£å†³æƒé‡è€¦åˆé—®é¢˜ã€‚</li>
<li>GCæ–¹æ³•å®ç°äº†é«˜æ•ˆè®¡ç®—æ¨¡å—é—´çš„æ¢¯åº¦æ–¹å‘ç›¸ä¼¼æ€§ï¼Œå®ç°äº†å†²çªæ¢¯åº¦çš„åˆ†é…ä¸ç›¸ä¼¼æ¨¡å—çš„èšåˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å›¾ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆUGASï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æ¢ç´¢æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œï¼ˆMPNNsï¼‰å’Œå›¾å˜æ¢å™¨ï¼ˆGTsï¼‰çš„æœ€ä½³ç»„åˆã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒGCåœ¨è¶…ç½‘åˆ’åˆ†è´¨é‡å’Œæ—¶é—´æ•ˆç‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</li>
<li>UGAS+GCæœç´¢çš„æ¶æ„æ€§èƒ½è¶…è¶Šäº†æ‰‹åŠ¨è®¾è®¡çš„GNNså’Œç°æœ‰NASæ–¹æ³•çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01231">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fb5919d991ec5c7d84067f82f26da2e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c67e342f880ff0083b07ddfd6e84028.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8f305696bdb1cac5e28af9a31468418.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29f4cf0d1d4b80d2bd60eb9e474245c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15b1e2157809a64604b6e63f04637d44.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-79438395ee004ae9016437c45cf9b811.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Calibrating-Pre-trained-Language-Classifiers-on-LLM-generated-Noisy-Labels-via-Iterative-Refinement"><a href="#Calibrating-Pre-trained-Language-Classifiers-on-LLM-generated-Noisy-Labels-via-Iterative-Refinement" class="headerlink" title="Calibrating Pre-trained Language Classifiers on LLM-generated Noisy   Labels via Iterative Refinement"></a>Calibrating Pre-trained Language Classifiers on LLM-generated Noisy   Labels via Iterative Refinement</h2><p><strong>Authors:Liqin Ye, Agam Shah, Chao Zhang, Sudheer Chava</strong></p>
<p>The traditional process of creating labeled datasets is labor-intensive and expensive. Recent breakthroughs in open-source large language models (LLMs) have opened up a new avenue in generating labeled datasets automatically for various natural language processing (NLP) tasks, providing an alternative to such an expensive annotation process. However, the reliability of such auto-generated labels remains a significant concern due to inherent inaccuracies. When learning from noisy labels, the modelâ€™s generalization is likely to be harmed as it is prone to overfit to those label noises. While previous studies in learning from noisy labels mainly focus on synthetic noise and real-world noise, LLM-generated label noise receives less attention. In this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to calibrate the classifierâ€™s prediction, thus enhancing its robustness towards LLM-generated noisy labels. SiDyP retrieves potential true label candidates by neighborhood label distribution in text embedding space and iteratively refines noisy candidates using a simplex diffusion model. Our framework can increase the performance of the BERT classifier fine-tuned on both zero-shot and few-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30% respectively. We demonstrate the effectiveness of SiDyP by conducting extensive benchmarking for different LLMs over a variety of NLP tasks. Our code is available on Github. </p>
<blockquote>
<p>ä¼ ç»Ÿåˆ›å»ºæ ‡æ³¨æ•°æ®é›†çš„è¿‡ç¨‹åŠ³åŠ¨å¼ºåº¦é«˜ä¸”è´¹ç”¨æ˜‚è´µã€‚æœ€è¿‘å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çªç ´ä¸ºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡è‡ªåŠ¨ç”Ÿæˆæ ‡æ³¨æ•°æ®é›†å¼€è¾Ÿäº†ä¸€æ¡æ–°é€”å¾„ï¼Œä¸ºè¿™ç§æ˜‚è´µçš„æ ‡æ³¨è¿‡ç¨‹æä¾›äº†æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç”±äºå†…åœ¨çš„ä¸å‡†ç¡®æ€§ï¼Œè¿™ç§è‡ªåŠ¨ç”Ÿæˆçš„æ ‡ç­¾çš„å¯é æ€§ä»ç„¶æ˜¯ä¸€ä¸ªå€¼å¾—å…³æ³¨çš„é—®é¢˜ã€‚å½“æ¨¡å‹ä»å¸¦æœ‰å™ªå£°çš„æ ‡ç­¾ä¸­å­¦ä¹ æ—¶ï¼Œå…¶æ³›åŒ–èƒ½åŠ›å¯èƒ½ä¼šå—åˆ°æŸå®³ï¼Œå› ä¸ºå®ƒå®¹æ˜“è¿‡åº¦é€‚åº”è¿™äº›æ ‡ç­¾å™ªå£°ã€‚è™½ç„¶ä¹‹å‰å…³äºä»å™ªå£°æ ‡ç­¾ä¸­å­¦ä¹ çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆæˆå™ªå£°å’ŒçœŸå®ä¸–ç•Œå™ªå£°ä¸Šï¼Œä½†LLMç”Ÿæˆçš„æ ‡ç­¾å™ªå£°å—åˆ°çš„å…³æ³¨è¾ƒå°‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SiDyPï¼šåŸºäºåŠ¨æ€å…ˆéªŒçš„ç®€å•æ ‡ç­¾æ‰©æ•£ï¼Œä»¥æ ¡æ­£åˆ†ç±»å™¨çš„é¢„æµ‹ï¼Œä»è€Œæé«˜å…¶å¯¹LLMç”Ÿæˆçš„æœ‰å™ªå£°æ ‡ç­¾çš„é²æ£’æ€§ã€‚SiDyPé€šè¿‡æ–‡æœ¬åµŒå…¥ç©ºé—´ä¸­çš„é‚»åŸŸæ ‡ç­¾åˆ†å¸ƒæ£€ç´¢æ½œåœ¨çš„çœŸå®æ ‡ç­¾å€™é€‰è€…ï¼Œå¹¶ä½¿ç”¨ç®€å•æ‰©æ•£æ¨¡å‹è¿­ä»£ä¼˜åŒ–å™ªå£°å€™é€‰è€…ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥æé«˜åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬LLMç”Ÿæˆçš„æœ‰å™ªå£°æ ‡ç­¾æ•°æ®é›†ä¸Šå¾®è°ƒBERTåˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œå¹³å‡æé«˜7.21%å’Œ7.30%ã€‚æˆ‘ä»¬é€šè¿‡é’ˆå¯¹ä¸åŒLLMå’Œå¤šç§NLPä»»åŠ¡è¿›è¡Œå¹¿æ³›çš„æ ‡å‡†åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†SiDyPçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨GitHubä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19675v2">PDF</a> Accepted at KDDâ€™25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆæ ‡æ³¨æ•°æ®é›†çš„æ–¹æ³•ï¼Œä½œä¸ºå¯¹ä¼ ç»Ÿæ ‡æ³¨æ•°æ®é›†çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œè‡ªåŠ¨ç”Ÿæˆçš„æ ‡ç­¾å­˜åœ¨å¯é æ€§é—®é¢˜ï¼Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¯èƒ½ä¼šå—åˆ°æŸå®³ã€‚é’ˆå¯¹LLMç”Ÿæˆçš„æ ‡ç­¾å™ªå£°ï¼Œæœ¬æ–‡æå‡ºäº†SiDyPæ–¹æ³•ï¼Œé€šè¿‡ç®€å•æ ‡ç­¾æ‰©æ•£å’ŒåŠ¨æ€å…ˆéªŒæ¥æ ¡å‡†åˆ†ç±»å™¨é¢„æµ‹ï¼Œæé«˜å…¶é²æ£’æ€§ã€‚SiDyPé€šè¿‡æ–‡æœ¬åµŒå…¥ç©ºé—´ä¸­çš„é‚»åŸŸæ ‡ç­¾åˆ†å¸ƒæ£€ç´¢æ½œåœ¨çš„çœŸå®æ ‡ç­¾å€™é€‰è€…ï¼Œå¹¶ä½¿ç”¨ç®€å•æ‰©æ•£æ¨¡å‹è¿­ä»£ä¼˜åŒ–å™ªå£°å€™é€‰è€…ã€‚è¯¥æ¡†æ¶æé«˜äº†åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬LLMç”Ÿæˆå™ªå£°æ ‡ç­¾æ•°æ®é›†ä¸Šå¾®è°ƒBERTåˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œå¹³å‡æé«˜äº†7.21%å’Œ7.30%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯è‡ªåŠ¨ç”Ÿæˆæ ‡æ³¨æ•°æ®é›†ï¼Œä¸ºNLPä»»åŠ¡æä¾›ä¾¿æ·æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>è‡ªåŠ¨ç”Ÿæˆçš„æ ‡ç­¾å­˜åœ¨å¯é æ€§é—®é¢˜ï¼Œå¯èƒ½å½±å“æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>LLMç”Ÿæˆçš„æ ‡ç­¾å™ªå£°è¾ƒå°‘å—åˆ°å…³æ³¨ï¼Œä½†å¯èƒ½å¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†SiDyPæ–¹æ³•ï¼Œé€šè¿‡ç®€å•æ ‡ç­¾æ‰©æ•£å’ŒåŠ¨æ€å…ˆéªŒæé«˜åˆ†ç±»å™¨å¯¹LLMç”Ÿæˆå™ªå£°æ ‡ç­¾çš„é²æ£’æ€§ã€‚</li>
<li>SiDyPé€šè¿‡æ£€ç´¢æ–‡æœ¬åµŒå…¥ç©ºé—´ä¸­çš„é‚»åŸŸæ ‡ç­¾æ¥æé«˜æ€§èƒ½ï¼Œå¹¶è¿­ä»£ä¼˜åŒ–å™ªå£°å€™é€‰è€…ã€‚</li>
<li>SiDyPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬LLMç”Ÿæˆå™ªå£°æ ‡ç­¾æ•°æ®é›†ä¸Šå¾®è°ƒBERTåˆ†ç±»å™¨çš„æ€§èƒ½æœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19675">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9c7f9ca24f5627fc678c855267cb4e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1596d9f57a99782033b4dc9164ce6df6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9504263dce41bee3476baaa8573d3899.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f50bcc49e408a4345384b8f18ae7044.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-083acf531e66e8209b12054d5896d9ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84c1e9258d3e21385efd2c7fb50aef2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce0e105c0af9af76ffad9398cf9d46bd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Cost-effective-Instruction-Learning-for-Pathology-Vision-and-Language-Analysis"><a href="#Cost-effective-Instruction-Learning-for-Pathology-Vision-and-Language-Analysis" class="headerlink" title="Cost-effective Instruction Learning for Pathology Vision and Language   Analysis"></a>Cost-effective Instruction Learning for Pathology Vision and Language   Analysis</h2><p><strong>Authors:Kaitao Chen, Mianxin Liu, Fang Yan, Lei Ma, Xiaoming Shi, Lilong Wang, Xiaosong Wang, Lifeng Zhu, Zhe Wang, Mu Zhou, Shaoting Zhang</strong></p>
<p>The advent of vision-language models fosters the interactive conversations between AI-enabled models and humans. Yet applying these models into clinics must deal with daunting challenges around large-scale training data, financial, and computational resources. Here we propose a cost-effective instruction learning framework for conversational pathology named as CLOVER. CLOVER only trains a lightweight module and uses instruction tuning while freezing the parameters of the large language model. Instead of using costly GPT-4, we propose well-designed prompts on GPT-3.5 for building generation-based instructions, emphasizing the utility of pathological knowledge derived from the Internet source. To augment the use of instructions, we construct a high-quality set of template-based instructions in the context of digital pathology. From two benchmark datasets, our findings reveal the strength of hybrid-form instructions in the visual question-answer in pathology. Extensive results show the cost-effectiveness of CLOVER in answering both open-ended and closed-ended questions, where CLOVER outperforms strong baselines that possess 37 times more training parameters and use instruction data generated from GPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot learning in the external clinical dataset. These findings demonstrate that cost-effective modeling of CLOVER could accelerate the adoption of rapid conversational applications in the landscape of digital pathology. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹çš„å…´èµ·ä¿ƒè¿›äº†äººå·¥æ™ºèƒ½æ¨¡å‹ä¸äººç±»ä¹‹é—´çš„äº¤äº’å¼å¯¹è¯ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹åº”ç”¨äºè¯Šæ‰€å¿…é¡»åº”å¯¹å¤§è§„æ¨¡è®­ç»ƒæ•°æ®ã€è´¢åŠ¡å’Œè®¡ç®—èµ„æºæ–¹é¢çš„è‰°å·¨æŒ‘æˆ˜ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºå¯¹è¯ç—…ç†å­¦çš„ç»æµé«˜æ•ˆæŒ‡ä»¤å­¦ä¹ æ¡†æ¶ï¼Œåä¸ºCLOVERã€‚CLOVERåªè®­ç»ƒä¸€ä¸ªè½»é‡çº§æ¨¡å—ï¼Œå¹¶ä½¿ç”¨æŒ‡ä»¤å¾®è°ƒï¼ŒåŒæ—¶å†»ç»“å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚æˆ‘ä»¬å¹¶æ²¡æœ‰ä½¿ç”¨æˆæœ¬é«˜æ˜‚çš„GPT-4ï¼Œè€Œæ˜¯æå‡ºäº†åœ¨GPT-3.5ä¸Šè®¾è®¡è‰¯å¥½çš„æç¤ºæ¥æ„å»ºåŸºäºç”Ÿæˆçš„æŒ‡ä»¤ï¼Œå¼ºè°ƒä»äº’è”ç½‘èµ„æºä¸­æ´¾ç”Ÿå‡ºçš„ç—…ç†å­¦çŸ¥è¯†çš„å®ç”¨æ€§ã€‚ä¸ºäº†å¢å¼ºæŒ‡ä»¤çš„ä½¿ç”¨ï¼Œæˆ‘ä»¬åœ¨æ•°å­—ç—…ç†çš„è¯­å¢ƒä¸­æ„å»ºäº†ä¸€ç»„é«˜è´¨é‡çš„åŸºäºæ¨¡æ¿çš„æŒ‡ä»¤é›†ã€‚ä»ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨ç—…ç†è§†è§‰é—®ç­”ä¸­æ··åˆå½¢å¼çš„æŒ‡ä»¤çš„å¼ºå¤§æ€§ã€‚å¤§é‡ç»“æœè¡¨æ˜ï¼Œåœ¨å›ç­”å¼€æ”¾æ€§å’Œå°é—­æ€§é—®é¢˜æ–¹é¢ï¼ŒCLOVERå…·æœ‰æˆæœ¬æ•ˆç›Šï¼Œä¸”ä¼˜äºæ‹¥æœ‰37å€ä»¥ä¸Šè®­ç»ƒå‚æ•°çš„å¼ºå¤§åŸºçº¿ï¼Œå¹¶ä½¿ç”¨GPT-4ç”Ÿæˆçš„æŒ‡ä»¤æ•°æ®ã€‚é€šè¿‡æŒ‡ä»¤å¾®è°ƒï¼ŒCLOVERåœ¨å¤–éƒ¨ä¸´åºŠæ•°æ®é›†ä¸­å±•ç°äº†å°‘é‡å­¦ä¹ çš„ç¨³å¥æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒCLOVERçš„ç»æµé«˜æ•ˆå»ºæ¨¡å¯ä»¥åŠ é€Ÿæ•°å­—ç—…ç†é¢†åŸŸå¿«é€Ÿå¯¹è¯åº”ç”¨çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.17734v2">PDF</a> </p>
<p><strong>Summary</strong><br>     éšç€è§†è§‰è¯­è¨€æ¨¡å‹çš„å…´èµ·ï¼Œæ¨åŠ¨äº†äººå·¥æ™ºèƒ½æ¨¡å‹ä¸äººç±»ä¹‹é—´çš„äº¤äº’å¯¹è¯ã€‚æœ¬æ–‡å°†å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨äºä¸´åºŠé¢†åŸŸï¼Œæå‡ºäº†ä¸€ç§å…·æœ‰æˆæœ¬æ•ˆç›Šçš„æŒ‡ä»¤å­¦ä¹ æ¡†æ¶CLOVERã€‚CLOVERä»…è®­ç»ƒè½»é‡çº§æ¨¡å—ï¼Œå¹¶ä½¿ç”¨æŒ‡ä»¤å¾®è°ƒæ¥å†»ç»“å¤§å‹è¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚é€šè¿‡å·§å¦™è®¾è®¡GPT-3.5çš„æç¤ºç¬¦ï¼Œä¸ä½¿ç”¨æ˜‚è´µçš„GPT-4æ„å»ºç”ŸæˆæŒ‡ä»¤ï¼Œå¼ºè°ƒä»äº’è”ç½‘æ¥æºè·å–ç—…ç†çŸ¥è¯†çš„å®ç”¨æ€§ã€‚ä¸ºäº†å¢å¼ºæŒ‡ä»¤çš„ä½¿ç”¨ï¼Œæˆ‘ä»¬åœ¨æ•°å­—ç—…ç†å­¦èƒŒæ™¯ä¸‹æ„å»ºäº†ä¸€å¥—é«˜è´¨é‡æ¨¡æ¿æŒ‡ä»¤ã€‚ä»ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†çš„å®éªŒç»“æœæ¥çœ‹ï¼ŒCLOVERåœ¨ç—…ç†å­¦è§†è§‰é—®ç­”ä¸­å…·æœ‰å¼ºå¤§çš„æ··åˆå½¢å¼æŒ‡ä»¤èƒ½åŠ›ã€‚ç›¸è¾ƒäºå…·æœ‰å¼ºå¤§åŸºçº¿ä¸”è®­ç»ƒå‚æ•°å¤šå‡º37å€ä¸”ä½¿ç”¨GPT-4ç”Ÿæˆçš„æŒ‡ä»¤æ•°æ®ï¼ŒCLOVERå±•ç°å‡ºé«˜æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚é€šè¿‡æŒ‡ä»¤å¾®è°ƒï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œåº”å¯¹å¤–éƒ¨ä¸´åºŠæ•°æ®é›†çš„é²æ£’æ€§ã€‚è¡¨æ˜CLOVERçš„ç»æµå»ºæ¨¡å¯èƒ½åŠ é€Ÿæ•°å­—ç—…ç†å­¦é¢†åŸŸå¿«é€Ÿå¯¹è¯åº”ç”¨çš„å‘å±•ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLOVERæ˜¯ä¸€ä¸ªå…·æœ‰æˆæœ¬æ•ˆç›Šçš„æŒ‡ä»¤å­¦ä¹ æ¡†æ¶ï¼Œé€‚ç”¨äºå¯¹è¯å¼ç—…ç†å­¦åº”ç”¨ã€‚</li>
<li>CLOVERä»…è®­ç»ƒè½»é‡çº§æ¨¡å—ï¼Œå¹¶é‡‡ç”¨æŒ‡ä»¤å¾®è°ƒæ¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„å‚æ•°ä½¿ç”¨ã€‚</li>
<li>ä½¿ç”¨GPT-3.5æ„å»ºç”ŸæˆæŒ‡ä»¤ï¼Œé¿å…ä½¿ç”¨æ˜‚è´µçš„GPT-4ï¼ŒåŒæ—¶å¼ºè°ƒä»äº’è”ç½‘è·å–ç—…ç†çŸ¥è¯†çš„ä»·å€¼ã€‚</li>
<li>åœ¨æ•°å­—ç—…ç†å­¦èƒŒæ™¯ä¸‹ï¼Œæ„å»ºäº†é«˜è´¨é‡æ¨¡æ¿æŒ‡ä»¤é›†ï¼Œä»¥å¢å¼ºæŒ‡ä»¤çš„ä½¿ç”¨æ•ˆæœã€‚</li>
<li>ä»åŸºå‡†æ•°æ®é›†ä¸­å‘ç°ï¼ŒCLOVERåœ¨ç—…ç†å­¦è§†è§‰é—®ç­”ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ··åˆå½¢å¼æŒ‡ä»¤èƒ½åŠ›ã€‚</li>
<li>CLOVERç›¸è¾ƒäºå…¶ä»–è®­ç»ƒå‚æ•°æ›´å¤šçš„æ¨¡å‹å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Šã€‚</li>
<li>é€šè¿‡æŒ‡ä»¤å¾®è°ƒï¼ŒCLOVERå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œåº”å¯¹å¤–éƒ¨ä¸´åºŠæ•°æ®é›†çš„é²æ£’æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.17734">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-305457b1ad5ab8ab2a78b56ba6bf0111.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb1257e8598da3ad107b8138434c8199.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5dbb27237bd00903e6a9b6b822c8dfb5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-53092162b508feea490dc5989d9cd034.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="AQA-Bench-An-Interactive-Benchmark-for-Evaluating-LLMsâ€™-Sequential-Reasoning-Ability"><a href="#AQA-Bench-An-Interactive-Benchmark-for-Evaluating-LLMsâ€™-Sequential-Reasoning-Ability" class="headerlink" title="AQA-Bench: An Interactive Benchmark for Evaluating LLMsâ€™ Sequential   Reasoning Ability"></a>AQA-Bench: An Interactive Benchmark for Evaluating LLMsâ€™ Sequential   Reasoning Ability</h2><p><strong>Authors:Siwei Yang, Bingchen Zhao, Cihang Xie</strong></p>
<p>This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol - for example, in DFS, the availability of each nodeâ€™s connected edge is contingent upon the modelâ€™s traversal to that node, thereby necessitating the LLMâ€™s ability to effectively remember visited nodes and strategize subsequent moves considering the possible environmental feedback in the future steps. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 14 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show much stronger sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing in-context examples may inadvertently hurt few-shot performance in an interactive environment due to over-fitting to examples. (3) Instead of using optimal steps from another test case as the in-context example, a very limited number of predecessor steps in the current test case following the optimal policy can substantially boost small modelsâ€™ performance. (4) The performance gap between weak models and strong models is greatly due to the incapability of weak models to start well. (5) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend. We hope our study can catalyze future work on advancing the understanding and enhancement of LLMsâ€™ capabilities in sequential reasoning. The code is available at <a target="_blank" rel="noopener" href="https://github.com/UCSC-VLAA/AQA-Bench">https://github.com/UCSC-VLAA/AQA-Bench</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†AQA-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç®—æ³•ä¸Šä¸‹æ–‡ä¸­çš„é¡ºåºæ¨ç†èƒ½åŠ›ï¼Œä¾‹å¦‚æ·±åº¦ä¼˜å…ˆæœç´¢ï¼ˆDFSï¼‰ã€‚æˆ‘ä»¬çš„è¯„ä¼°åŸºå‡†çš„å…³é”®ç‰¹ç‚¹åœ¨äºå…¶äº¤äº’è¯„ä¼°åè®®â€”â€”ä¾‹å¦‚ï¼Œåœ¨DFSä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹çš„è¿é€šè¾¹çš„å¯ç”¨æ€§å–å†³äºæ¨¡å‹éå†åˆ°è¯¥èŠ‚ç‚¹çš„æƒ…å†µï¼Œä»è€Œéœ€è¦LLMæœ‰æ•ˆè®°å¿†å·²è®¿é—®èŠ‚ç‚¹å¹¶ç­–åˆ’åç»­è¡ŒåŠ¨ï¼Œè€ƒè™‘æœªæ¥æ­¥éª¤ä¸­å¯èƒ½çš„ç¯å¢ƒåé¦ˆã€‚æˆ‘ä»¬å…¨é¢æ„å»ºäº†AQA-Benchï¼ŒåŒ…å«ä¸‰ç§ä¸åŒçš„ç®—æ³•ï¼Œå³äºŒåˆ†æœç´¢ã€æ·±åº¦ä¼˜å…ˆæœç´¢å’Œå¹¿åº¦ä¼˜å…ˆæœç´¢ï¼Œå¹¶è¯„ä¼°äº†14ç§ä¸åŒLLMçš„é¡ºåºæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥å‘ç°äº†å‡ ä¸ªæœ‰è¶£çš„å‘ç°ï¼šï¼ˆ1ï¼‰åƒGPT-4å’ŒåŒå­åº§è¿™æ ·çš„å°é—­æºä»£ç æ¨¡å‹é€šå¸¸è¡¨ç°å‡ºæ›´å¼ºçš„é¡ºåºæ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºå¼€æºLLMã€‚ï¼ˆ2ï¼‰ç›²ç›®æä¾›ä¸Šä¸‹æ–‡ç¤ºä¾‹å¯èƒ½ä¼šæ— æ„ä¸­ä¼¤å®³äº¤äº’å¼ç¯å¢ƒä¸­çš„å°æ ·æœ¬æ€§èƒ½ï¼Œå› ä¸ºè¿‡åº¦æ‹Ÿåˆç¤ºä¾‹ã€‚ï¼ˆ3ï¼‰ä¸ä»å¦ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹æä¾›æœ€ä½³æ­¥éª¤ä½œä¸ºä¸Šä¸‹æ–‡ç¤ºä¾‹ç›¸æ¯”ï¼Œåœ¨å½“å‰æµ‹è¯•ç”¨ä¾‹ä¸­éµå¾ªæœ€ä½³ç­–ç•¥çš„å‡ ä¸ªå…ˆè¡Œæ­¥éª¤å¯ä»¥å¤§å¹…æå‡å°å‹æ¨¡å‹çš„æ€§èƒ½ã€‚ï¼ˆ4ï¼‰å¼±æ¨¡å‹ä¸å¼ºæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºå¼±æ¨¡å‹æ— æ³•è‰¯å¥½å¼€å±€ã€‚ï¼ˆ5ï¼‰æ€§èƒ½ä¸æ¨¡å‹å¤§å°ä¹‹é—´çš„è§„æ¨¡ç›¸å…³æ€§å¹¶ä¸æ€»æ˜¯æ˜¾è‘—ï¼Œæœ‰æ—¶ç”šè‡³å‘ˆç°é€†è¶‹åŠ¿ã€‚æˆ‘ä»¬å¸Œæœ›æœ¬ç ”ç©¶èƒ½å‚¬åŒ–æœªæ¥å·¥ä½œï¼Œæ¨åŠ¨å¯¹LLMåœ¨é¡ºåºæ¨ç†èƒ½åŠ›æ–¹é¢çš„ç†è§£å’Œæå‡ã€‚ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/UCSC-VLAA/AQA-Bench%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/UCSC-VLAA/AQA-Benchæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.09404v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†AQA-Benchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç®—æ³•ä¸Šä¸‹æ–‡ï¼ˆå¦‚æ·±åº¦ä¼˜å…ˆæœç´¢ï¼‰ä¸­çš„é¡ºåºæ¨ç†èƒ½åŠ›çš„æ–°é¢–åŸºå‡†æµ‹è¯•ã€‚å…¶å…³é”®ç‰¹æ€§åœ¨äºå…¶äº¤äº’å¼è¯„ä¼°åè®®ï¼Œä¾‹å¦‚åœ¨æ·±åº¦ä¼˜å…ˆæœç´¢ä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹çš„è¿é€šè¾¹çš„å¯ç”¨æ€§å–å†³äºæ¨¡å‹å¯¹è¯¥èŠ‚ç‚¹çš„éå†ï¼Œä»è€Œéœ€è¦å¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡åœ¨è€ƒè™‘åˆ°æœªæ¥æ­¥éª¤çš„å¯èƒ½ç¯å¢ƒåé¦ˆçš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆè®°ä½å·²è®¿é—®èŠ‚ç‚¹å¹¶ç­–åˆ’åç»­åŠ¨ä½œçš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä½¿ç”¨ä¸‰ç§ä¸åŒç®—æ³•æ„å»ºäº†AQA-Benchï¼ŒåŒ…æ‹¬äºŒåˆ†æœç´¢ã€æ·±åº¦ä¼˜å…ˆæœç´¢å’Œå¹¿åº¦ä¼˜å…ˆæœç´¢ï¼Œå¹¶è¯„ä¼°äº†14ç§å¤§å‹è¯­è¨€æ¨¡å‹çš„é¡ºåºæ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°äº†å¤šä¸ªæœ‰è¶£çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AQA-Benchæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç®—æ³•ä¸Šä¸‹æ–‡ä¸­çš„é¡ºåºæ¨ç†èƒ½åŠ›çš„æ–°é¢–åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å…¶äº¤äº’å¼è¯„ä¼°åè®®è¦æ±‚å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è€ƒè™‘åˆ°æœªæ¥ç¯å¢ƒåé¦ˆçš„æƒ…å†µä¸‹æœ‰æ•ˆè®°ä½å·²è®¿é—®èŠ‚ç‚¹å¹¶è§„åˆ’åç»­åŠ¨ä½œã€‚</li>
<li>åœ¨æ·±åº¦ä¼˜å…ˆæœç´¢ç­‰åœºæ™¯ä¸­ï¼Œæ¨¡å‹éœ€è¦æ ¹æ®è‡ªèº«éå†çš„èŠ‚ç‚¹æ¥å†³å®šè¿é€šè¾¹çš„å¯ç”¨æ€§ã€‚</li>
<li>å°é—­æºä»£ç çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4å’ŒGeminiï¼‰å±•ç°å‡ºæ›´å¼ºçš„é¡ºåºæ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>åœ¨äº¤äº’å¼ç¯å¢ƒä¸­ï¼Œè¿‡åº¦ä¾èµ–ä¸Šä¸‹æ–‡ç¤ºä¾‹å¯èƒ½ä¼šæŸå®³å°å‹æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨å½“å‰æµ‹è¯•ç”¨ä¾‹ä¸­çš„æœ€ä¼˜æ­¥éª¤çš„å°‘é‡å…ˆè¡Œæ­¥éª¤èƒ½æ˜¾è‘—æå‡å°å‹æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹é—´çš„æ€§èƒ½å·®è·å¾ˆå¤§ç¨‹åº¦ä¸Šæºäºå¼±æ¨¡å‹åœ¨èµ·å§‹é˜¶æ®µçš„è¡¨ç°ä¸ä½³ï¼Œè€Œæ¨¡å‹æ€§èƒ½ä¸æ¨¡å‹è§„æ¨¡ä¹‹é—´çš„ç›¸å…³æ€§å¹¶ä¸æ€»æ˜¯æ˜¾è‘—ï¼Œæœ‰æ—¶ç”šè‡³å‘ˆç°é€†è¶‹åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.09404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-05eb4867fc896a5a6cc66a90989550f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e20cf3bc6623e86371c07977353074f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fe9d8182d9cd8d6dbed415e256fdc1f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2f5922d7ace749899b09a181dc11cbfd.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-24  Seeing is Fixing Cross-Modal Reasoning with Multimodal LLMs for Visual   Software Issue Fixing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9f35788449f25c3ac391d6d19c4e11b0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-24  RAGentA Multi-Agent Retrieval-Augmented Generation for Attributed   Question Answering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32251.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
