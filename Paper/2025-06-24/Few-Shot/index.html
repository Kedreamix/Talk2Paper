<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-06-24  Universal Music Representations? Evaluating Foundation Models on World   Music Corpora">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-eb1257e8598da3ad107b8138434c8199.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    51 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-24-更新"><a href="#2025-06-24-更新" class="headerlink" title="2025-06-24 更新"></a>2025-06-24 更新</h1><h2 id="Universal-Music-Representations-Evaluating-Foundation-Models-on-World-Music-Corpora"><a href="#Universal-Music-Representations-Evaluating-Foundation-Models-on-World-Music-Corpora" class="headerlink" title="Universal Music Representations? Evaluating Foundation Models on World   Music Corpora"></a>Universal Music Representations? Evaluating Foundation Models on World   Music Corpora</h2><p><strong>Authors:Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos</strong></p>
<p>Foundation models have revolutionized music information retrieval, but questions remain about their ability to generalize across diverse musical traditions. This paper presents a comprehensive evaluation of five state-of-the-art audio foundation models across six musical corpora spanning Western popular, Greek, Turkish, and Indian classical traditions. We employ three complementary methodologies to investigate these models’ cross-cultural capabilities: probing to assess inherent representations, targeted supervised fine-tuning of 1-2 layers, and multi-label few-shot learning for low-resource scenarios. Our analysis shows varying cross-cultural generalization, with larger models typically outperforming on non-Western music, though results decline for culturally distant traditions. Notably, our approaches achieve state-of-the-art performance on five out of six evaluated datasets, demonstrating the effectiveness of foundation models for world music understanding. We also find that our targeted fine-tuning approach does not consistently outperform probing across all settings, suggesting foundation models already encode substantial musical knowledge. Our evaluation framework and benchmarking results contribute to understanding how far current models are from achieving universal music representations while establishing metrics for future progress. </p>
<blockquote>
<p>基础模型已经彻底改变了音乐信息检索，但对于它们在多样化音乐传统中的泛化能力仍存在疑问。本文全面评估了五个最先进的音频基础模型，涉及六种音乐语料库，包括西方流行、希腊、土耳其和印度古典传统。我们采用三种互补方法来研究这些模型的跨文化能力：探测评估内在表示、对1-2层进行有针对性的监督微调以及用于低资源场景的多标签小样本学习。我们的分析表明，跨文化泛化能力存在差异，较大的模型通常在非西方音乐上表现更好，但对于文化距离较远的传统，结果会下降。值得注意的是，我们的方法在六个评估数据集中有五个达到了最新水平，证明了基础模型在世界音乐理解方面的有效性。我们还发现，我们的有针对性的微调方法并不始终在所有设置中优于探测，这表明基础模型已经编码了大量的音乐知识。我们的评估框架和基准测试结果有助于了解当前模型距离实现通用音乐表示还有多远，同时为未来进展建立了衡量指标。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17055v1">PDF</a> Accepted at ISMIR 2025</p>
<p><strong>Summary</strong></p>
<p>本论文全面评估了五款先进音频基础模型在涵盖西方流行、希腊、土耳其和印度古典传统等六种音乐语料库中的跨文化泛化能力。通过探针测试内在表征、对一至两层进行有针对性的监督微调以及用于低资源场景的多标签小样本学习等方法进行研究。分析表明，基础模型在不同文化背景下的泛化能力存在差异，大型模型在非西方音乐上的表现通常较好，但在文化距离较远的传统中表现下降。本研究的方法在六个数据集中有五个达到了最先进的性能水平，证明了基础模型对世界音乐理解的有效性。此外，研究还发现针对性的微调方法并不总是优于探针在所有场景下的表现，表明基础模型已经编码了大量的音乐知识。本评估框架和基准测试结果有助于了解当前模型距离实现通用音乐表示的程度，并为未来的进步建立了指标。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频基础模型在跨文化音乐信息检索中的泛化能力存在差异。</li>
<li>大型模型在非西方音乐上的表现较好。</li>
<li>在文化距离较远的传统中，模型表现可能下降。</li>
<li>论文采用三种方法评估模型的跨文化能力：探针测试、针对性监督微调和小样本学习。</li>
<li>研究方法在五个数据集上达到了最先进的性能水平。</li>
<li>基础模型已经编码了大量的音乐知识，但仍有提升空间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17055">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ef6b3ad7f0e46b926c06dd99107e3707.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4bc8c35aa40a515721127ed2f2eff5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f64a9f8dd2c8a8a3ec2556ee6cdf815f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55166605e64162bcc33a95edb4f4f24b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01e8305c9ecb63e3ba1683b0d0dfc759.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08c819e598b7abcc1ddb1935a60d39c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3c5000f235cf6a0c604f881f3cee922.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Prmpt2Adpt-Prompt-Based-Zero-Shot-Domain-Adaptation-for-Resource-Constrained-Environments"><a href="#Prmpt2Adpt-Prompt-Based-Zero-Shot-Domain-Adaptation-for-Resource-Constrained-Environments" class="headerlink" title="Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for   Resource-Constrained Environments"></a>Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for   Resource-Constrained Environments</h2><p><strong>Authors:Yasir Ali Farrukh, Syed Wali, Irfan Khan, Nathaniel D. Bastian</strong></p>
<p>Unsupervised Domain Adaptation (UDA) is a critical challenge in real-world vision systems, especially in resource-constrained environments like drones, where memory and computation are limited. Existing prompt-driven UDA methods typically rely on large vision-language models and require full access to source-domain data during adaptation, limiting their applicability. In this work, we propose Prmpt2Adpt, a lightweight and efficient zero-shot domain adaptation framework built around a teacher-student paradigm guided by prompt-based feature alignment. At the core of our method is a distilled and fine-tuned CLIP model, used as the frozen backbone of a Faster R-CNN teacher. A small set of low-level source features is aligned to the target domain semantics-specified only through a natural language prompt-via Prompt-driven Instance Normalization (PIN). These semantically steered features are used to briefly fine-tune the detection head of the teacher model. The adapted teacher then generates high-quality pseudo-labels, which guide the on-the-fly adaptation of a compact student model. Experiments on the MDS-A dataset demonstrate that Prmpt2Adpt achieves competitive detection performance compared to state-of-the-art methods, while delivering up to 7x faster adaptation and 5x faster inference speed using few source images-making it a practical and scalable solution for real-time adaptation in low-resource domains. </p>
<blockquote>
<p>无监督域自适应（UDA）是现实世界视觉系统中的一个关键挑战，特别是在资源受限的环境（如无人机）中，内存和计算资源有限。现有的基于提示的UDA方法通常依赖于大型视觉语言模型，并且在适应过程中需要完全访问源域数据，这限制了其适用性。在这项工作中，我们提出了Prompt2Adpt，这是一个围绕教师学生范式构建的轻量级高效零样本域自适应框架，由基于提示的特征对齐引导。我们的方法的核心是一个经过蒸馏和精细调整的CLIP模型，用作Faster R-CNN教师的冻结主干。通过提示驱动的实例归一化（PIN），一小部分低级的源特征被对齐到目标域语义（仅通过自然语言提示指定）。这些语义引导的特征被用来微调教师的检测头部。适应后的教师然后生成高质量的伪标签，这些伪标签指导紧凑的学生模型的即时适应。在MDS-A数据集上的实验表明，Prompt2Adpt与最先进的方法相比，在检测性能方面表现出竞争力，同时使用少量的源图像实现了高达7倍的快速适应和5倍的推理速度，使其成为实时适应低资源域的实用且可扩展的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16994v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于教师-学生范式的轻量级高效零样本域自适应框架，名为Prmpt2Adpt。它利用CLIP模型和Prompt-driven Instance Normalization（PIN）进行特征对齐，实现对源域数据的少量使用和对目标域语义的自然语言提示的引导。该方法实现了快速自适应和推理速度的提升，使得在低资源领域中的实时自适应变得更加实用和可扩展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Prmpt2Adpt是一种针对无人机等资源受限环境的零样本域自适应方法。</li>
<li>该方法采用教师-学生范式，利用CLIP模型作为固定骨干网。</li>
<li>通过Prompt-driven Instance Normalization（PIN）实现源特征与目标域语义的对齐。</li>
<li>仅通过自然语言提示进行特征对齐。</li>
<li>Prmpt2Adpt实现了快速自适应和推理速度的提升。</li>
<li>在MDS-A数据集上的实验表明，Prmpt2Adpt的检测结果与最新技术相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16994">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-00d1b215fc4d6a6fa2beeac18306ad76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abe433d56654973b69d20d902edb7142.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b30b1817879d38a68b6b02a474245150.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1274dfd5091279e65c1555970e72f639.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-add5936f0821d6203113d0ecb23bca05.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Single-shot-thermometry-of-simulated-Bose–Einstein-condensates-using-artificial-intelligence"><a href="#Single-shot-thermometry-of-simulated-Bose–Einstein-condensates-using-artificial-intelligence" class="headerlink" title="Single-shot thermometry of simulated Bose–Einstein condensates using   artificial intelligence"></a>Single-shot thermometry of simulated Bose–Einstein condensates using   artificial intelligence</h2><p><strong>Authors:Jack Griffiths, Steven A. Wrathmall, Simon A. Gardiner</strong></p>
<p>Precise determination of thermodynamic parameters in ultracold Bose gases remains challenging due to the destructive nature of conventional measurement techniques and inherent experimental uncertainties. We demonstrate an artificial intelligence approach for rapid, non-destructive estimation of the chemical potential and temperature from single-shot, in situ imaged density profiles of finite-temperature Bose gases. Our convolutional neural network is trained exclusively on quasi-2D &#96;pancake’ condensates in harmonic trap configurations. It achieves parameter extraction within fractions of a second. The model also demonstrates zero-shot generalisation across both trap geometry and thermalisation dynamics, successfully estimating thermodynamic parameters for toroidally trapped condensates with errors of only a few nanokelvin despite no prior exposure to such geometries during training, and maintaining predictive accuracy during dynamic thermalisation processes after a relatively brief evolution without explicit training on non-equilibrium states. These results suggest that supervised learning can overcome traditional limitations in ultracold atom thermometry, with extension to broader geometric configurations, temperature ranges, and additional parameters potentially enabling comprehensive real-time analysis of quantum gas experiments. Such capabilities could significantly streamline experimental workflows whilst improving measurement precision across a range of quantum fluid systems. </p>
<blockquote>
<p>在超冷玻色气体中精确确定热力学参数仍然是一个挑战，这主要是由于传统测量技术的破坏性以及实验固有的不确定性。我们展示了一种人工智能方法，可以快速、非破坏性地从单次现场成像的密度分布图中估计有限温度玻色气体的化学势和温度。我们的卷积神经网络仅针对谐波陷阱配置中的二维类”煎饼”凝聚物进行训练。它能够在几分之一秒内实现参数提取。该模型还展示了跨陷阱几何形状和热化动力学的零起点泛化能力，尽管在训练期间没有接触过此类几何形状，但对环形陷阱中的凝聚物热力学参数进行估计时误差仅为几纳开尔文，并且在相对短暂的演化过程中，在非平衡态没有明确的训练情况下仍能保持预测精度。这些结果表明，有监督学习可以克服超冷原子测温中的传统局限性，扩展到更广泛的几何结构、温度范围和附加参数，从而可能实现对量子气体实验的全面实时分析。这种能力可能会大大简化实验工作流程，同时提高一系列量子流体系统的测量精度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16925v1">PDF</a> </p>
<p><strong>Summary</strong><br>     针对超冷玻色气体中的热力学参数精确测定，存在传统测量技术的破坏性和实验固有不确定性的问题。本研究展示了一种人工智能方法，用于快速、非破坏性估计化学势和温度，仅通过单次即时成像的密度分布数据即可完成。使用卷积神经网络，仅在准二维“煎饼”凝聚物和谐波陷阱配置上进行训练，即可在数秒内提取参数。该模型还展示了在陷阱几何形状和热化动力学方面的零样本泛化能力，尽管在训练期间未接触此类几何形状，但对于环形陷阱中的玻色气体，其热力学参数的估计误差仅为几纳开尔文。此外，它在动态热化过程中保持预测精度。这些结果建议，监督学习可以克服超冷原子测温中的传统局限性，扩展到更广泛的几何配置、温度范围和额外参数，可能会实现对量子气体实验的全面实时分析。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>超冷玻色气体中热力学参数的精确测定面临挑战，因为传统测量技术具有破坏性和实验的不确定性。</li>
<li>人工智能方法可用于快速、非破坏性估计化学势和温度。</li>
<li>使用的卷积神经网络能够在准二维“煎饼”凝聚物和特定的陷阱配置上进行训练，并快速提取参数。</li>
<li>模型具有良好的泛化能力，可以在不同的陷阱几何形状和热化动力学中表现良好。</li>
<li>对于环形陷阱中的玻色气体，模型可以高精度地估计热力学参数，即使在动态热化过程中也能保持准确性。</li>
<li>监督学习可以克服超冷原子测温中的传统局限性，并具有在更广泛的几何配置、温度范围和额外参数上进行扩展的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16925">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-94d6e7e171ca6a2456bf7f49fdf67b7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97d1128cbffec009ba75e94ce5310e55.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="With-Limited-Data-for-Multimodal-Alignment-Let-the-STRUCTURE-Guide-You"><a href="#With-Limited-Data-for-Multimodal-Alignment-Let-the-STRUCTURE-Guide-You" class="headerlink" title="With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You"></a>With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You</h2><p><strong>Authors:Fabian Gröger, Shuo Wen, Huyen Le, Maria Brbić</strong></p>
<p>Multimodal models have demonstrated powerful capabilities in complex tasks requiring multimodal alignment including zero-shot classification and cross-modal retrieval. However, existing models typically rely on millions of paired multimodal samples, which are prohibitively expensive or infeasible to obtain in many domains. In this work, we explore the feasibility of building multimodal models with limited amount of paired data by aligning pretrained unimodal foundation models. We show that high-quality alignment is possible with as few as tens of thousands of paired samples$\unicode{x2013}$less than $1%$ of the data typically used in the field. To achieve this, we introduce STRUCTURE, an effective regularization technique that preserves the neighborhood geometry of the latent space of unimodal encoders. Additionally, we show that aligning last layers is often suboptimal and demonstrate the benefits of aligning the layers with the highest representational similarity across modalities. These two components can be readily incorporated into existing alignment methods, yielding substantial gains across 24 zero-shot image classification and retrieval benchmarks, with average relative improvement of $51.6%$ in classification and $91.8%$ in retrieval tasks. Our results highlight the effectiveness and broad applicability of our framework for limited-sample multimodal learning and offer a promising path forward for resource-constrained domains. </p>
<blockquote>
<p>多模态模型在需要多模态对齐的复杂任务中表现出了强大的能力，包括零样本分类和跨模态检索。然而，现有模型通常依赖于数百万配对的多模态样本，这在许多领域中是难以获得或成本高昂的。在这项工作中，我们探索了通过对齐预训练的单模态基础模型来构建多模态模型在有限配对数据下的可行性。我们展示了仅使用数万个配对样本（不到该领域通常使用的数据的1%）就能实现高质量的对齐。为了实现这一点，我们引入了STRUCTURE，这是一种有效的正则化技术，能够保留单模态编码器潜在空间的邻域几何结构。此外，我们表明对齐最后一层通常是次优的，并展示了对齐具有最高跨模态代表性相似性的层的好处。这两个组件可以轻松地融入现有的对齐方法，在24个零样本图像分类和检索基准测试中取得了显著的提升，分类任务的平均相对改进为51.6%，检索任务的平均相对改进为91.8%。我们的研究突出了框架在有限样本多模态学习中的有效性和广泛适用性，并为资源受限领域提供了前景广阔的前进路径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16895v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探索了在有限的配对数据下构建多模态模型的可行性，通过对预训练的单模态基础模型进行对齐实现。引入了一种有效的正则化技术STRUCTURE，能够保留单模态编码器潜在空间的邻近几何结构。同时，文章指出对齐最后一层通常是次优的，并展示了对齐具有最高代表性相似度的层的好处。这两个组件可以轻松地融入现有的对齐方法，在24个零样本图像分类和检索基准测试中实现了显著的增益，分类任务平均相对提高了51.6%，检索任务提高了91.8%。结果凸显了本文框架在有限样本多模态学习中的有效性和广泛适用性，为资源受限领域提供了有前景的发展路径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态模型在需要多模态对齐的复杂任务中表现出强大的能力，如零样本分类和跨模态检索。</li>
<li>现有模型通常依赖于大量的配对多模态样本，这在许多领域中是昂贵且不可行的。</li>
<li>本文探索了在有限的配对数据下构建多模态模型的可行性，通过对预训练的单模态基础模型进行对齐。</li>
<li>引入了一种名为STRUCTURE的有效正则化技术，能够保留单模态编码器潜在空间的邻近几何结构。</li>
<li>对齐最后一层通常是次优的，而对齐具有最高代表性相似度的层能带来更大好处。</li>
<li>该框架在零样本图像分类和检索任务中实现了显著的性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16895">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ae2990c0bcb5a4192dd55f289b512f21.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-44c08b3ac616c0dcf13448e1db574860.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acd307cb3629fc2ffd0ed7369507a593.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af4f9407c6f663205779e4eeb20ea5c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11075165cd2d197943ddb84dcce39646.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7aeab09d7ded1f5da050d56e98ee22b1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Generalized-Category-Discovery-With-Retrieval-Guided-Decision-Boundary-Enhancement"><a href="#Few-Shot-Generalized-Category-Discovery-With-Retrieval-Guided-Decision-Boundary-Enhancement" class="headerlink" title="Few-Shot Generalized Category Discovery With Retrieval-Guided Decision   Boundary Enhancement"></a>Few-Shot Generalized Category Discovery With Retrieval-Guided Decision   Boundary Enhancement</h2><p><strong>Authors:Yunhan Ren, Feng Luo, Siyu Huang</strong></p>
<p>While existing Generalized Category Discovery (GCD) models have achieved significant success, their performance with limited labeled samples and a small number of known categories remains largely unexplored. In this work, we introduce the task of Few-shot Generalized Category Discovery (FSGCD), aiming to achieve competitive performance in GCD tasks under conditions of known information scarcity. To tackle this challenge, we propose a decision boundary enhancement framework with affinity-based retrieval. Our framework is designed to learn the decision boundaries of known categories and transfer these boundaries to unknown categories. First, we use a decision boundary pre-training module to mitigate the overfitting of pre-trained information on known category boundaries and improve the learning of these decision boundaries using labeled samples. Second, we implement a two-stage retrieval-guided decision boundary optimization strategy. Specifically, this strategy further enhances the severely limited known boundaries by using affinity-retrieved pseudo-labeled samples. Then, these refined boundaries are applied to unknown clusters via guidance from affinity-based feature retrieval. Experimental results demonstrate that our proposed method outperforms existing methods on six public GCD benchmarks under the FSGCD setting. The codes are available at: <a target="_blank" rel="noopener" href="https://github.com/Ryh1218/FSGCD">https://github.com/Ryh1218/FSGCD</a> </p>
<blockquote>
<p>虽然现有的广义类别发现（GCD）模型已经取得了显著的成功，但它们在有限标签样本和已知类别数量较少的情况下的性能仍然尚未被充分探索。在这项工作中，我们引入了小样本广义类别发现（FSGCD）任务，旨在在已知信息稀缺的条件下实现GCD任务的竞争性能。为了应对这一挑战，我们提出了一个基于亲和性检索的决策边界增强框架。我们的框架旨在学习已知类别的决策边界，并将这些边界转移到未知类别。首先，我们使用决策边界预训练模块来缓解对已知类别边界的预训练信息的过拟合问题，并利用标签样本改进这些决策边界的学习。其次，我们实现了一个两阶段的检索引导决策边界优化策略。具体来说，该策略通过使用亲和性检索的伪标签样本来进一步增强严重有限的已知边界。然后，这些优化后的边界通过基于亲和性的特征检索的指导应用于未知集群。实验结果表明，在FSGCD设置下的六个公共GCD基准测试中，我们提出的方法优于现有方法。代码可用在：<a target="_blank" rel="noopener" href="https://github.com/Ryh1218/FSGCD">https://github.com/Ryh1218/FSGCD</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16728v1">PDF</a> Accepted by ICMR 2025</p>
<p><strong>Summary</strong><br>少量样本下的广义类别发现（FSGCD）研究提出了新的挑战性问题。通过决策边界增强框架与基于亲和力的检索，对已知类别的决策边界进行学习并应用于未知类别。首先预训练决策边界模块减轻已知类别边界上过拟合现象，并使用标签样本改善决策边界学习。接着实现两阶段检索引导决策边界优化策略，利用亲和力检索得到的伪标签样本进一步增强已知边界的局限性，并应用于未知集群的亲和特征检索引导中。在六个公共广义类别发现基准测试中，该方法表现优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FSGCD任务旨在解决现有广义类别发现模型在有限标签样本和已知类别数量较少的情况下的性能问题。</li>
<li>提出决策边界增强框架来解决这一挑战，结合了决策边界预训练模块与基于亲和力的检索。</li>
<li>决策边界预训练模块可以减轻在已知类别边界上的过拟合现象，并改善决策边界的学习。</li>
<li>实现两阶段检索引导决策边界优化策略，通过亲和力检索得到伪标签样本以增强已知边界的局限性。</li>
<li>将优化后的决策边界应用于未知类别的聚类中，通过亲和特征检索进行引导。</li>
<li>在多个公共广义类别发现基准测试中，该方法显示出优于现有方法的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16728">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7f864816f4694cac414377038602fde6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eace5338c631d3d091a90e2a3a61780c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb4096f3067fba1b4ac7a3ea51187a6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b976571b687331469062f5285dfb4f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-937feb9c5825d419091788161616e4fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-910e964f482dd3833be7b9a2c97f3bad.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Relic-Enhancing-Reward-Model-Generalization-for-Low-Resource-Indic-Languages-with-Few-Shot-Examples"><a href="#Relic-Enhancing-Reward-Model-Generalization-for-Low-Resource-Indic-Languages-with-Few-Shot-Examples" class="headerlink" title="Relic: Enhancing Reward Model Generalization for Low-Resource Indic   Languages with Few-Shot Examples"></a>Relic: Enhancing Reward Model Generalization for Low-Resource Indic   Languages with Few-Shot Examples</h2><p><strong>Authors:Soumya Suvra Ghosal, Vaibhav Singh, Akash Ghosh, Soumyabrata Pal, Subhadip Baidya, Sriparna Saha, Dinesh Manocha</strong></p>
<p>Reward models are essential for aligning large language models (LLMs) with human preferences. However, most open-source multilingual reward models are primarily trained on preference datasets in high-resource languages, resulting in unreliable reward signals for low-resource Indic languages. Collecting large-scale, high-quality preference data for these languages is prohibitively expensive, making preference-based training approaches impractical. To address this challenge, we propose RELIC, a novel in-context learning framework for reward modeling in low-resource Indic languages. RELIC trains a retriever with a pairwise ranking objective to select in-context examples from auxiliary high-resource languages that most effectively highlight the distinction between preferred and less-preferred responses. Extensive experiments on three preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art open-source reward models demonstrate that RELIC significantly improves reward model accuracy for low-resource Indic languages, consistently outperforming existing example selection methods. For example, on Bodo-a low-resource Indic language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13% improvement in accuracy over zero-shot prompting and state-of-the-art example selection method, respectively. </p>
<blockquote>
<p>奖励模型对于对齐大型语言模型（LLM）与人类偏好至关重要。然而，大多数开源的多语言奖励模型主要在高资源语言的偏好数据集上进行训练，导致对低资源印度语系的奖励信号不可靠。对于这些语言，收集大规模高质量偏好数据成本高昂，使得基于偏好的训练方法不切实际。为了应对这一挑战，我们提出了RELIC，这是一个用于低资源印度语系奖励建模的新型上下文学习框架。RELIC训练了一个检索器，使用配对排名目标来选择辅助高资源语言中最能突出首选和较少首选回应之间区别的上下文示例。在PKU-SafeRLHF、WebGPT和HH-RLHF三个偏好数据集上进行的广泛实验，使用最先进的开源奖励模型证明，RELIC显著提高了低资源印度语系的奖励模型精度，并且始终优于现有的示例选择方法。例如，在资源贫乏的印度语博多语中，使用LLaMA-3.2-3B奖励模型，RELIC相对于零样本提示和最新示例选择方法，准确度分别提高了12.81%和10.13%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16502v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种针对低资源印地语语言奖励模型的新框架RELIC。RELIC通过利用辅助高资源语言中的上下文示例，训练一个检索器以选择最能区分偏好响应和非偏好响应的示例。实验证明，RELIC显著提高了低资源印地语语言的奖励模型准确性，并优于现有的示例选择方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>奖励模型对于对齐大型语言模型与人类偏好至关重要。</li>
<li>大多数开源多语言奖励模型主要在高资源语言偏好数据集上进行训练，导致对低资源印地语语言的奖励信号不可靠。</li>
<li>收集大规模高质量偏好数据对于低资源语言是不实际的。</li>
<li>提出了一种名为RELIC的新型上下文学习框架，用于在低资源印地语语言中进行奖励建模。</li>
<li>RELIC通过从辅助高资源语言中选择上下文示例来训练检索器，这些示例最能区分偏好响应与非偏好响应。</li>
<li>实验证明RELIC在低资源印地语语言的奖励模型准确性上显著提高，优于现有的示例选择方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16502">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bad72d71935f9a0e020b01c04f4ff53c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-763a4b24e198096a9ec34af2a8a79f5a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SynPo-Boosting-Training-Free-Few-Shot-Medical-Segmentation-via-High-Quality-Negative-Prompts"><a href="#SynPo-Boosting-Training-Free-Few-Shot-Medical-Segmentation-via-High-Quality-Negative-Prompts" class="headerlink" title="SynPo: Boosting Training-Free Few-Shot Medical Segmentation via   High-Quality Negative Prompts"></a>SynPo: Boosting Training-Free Few-Shot Medical Segmentation via   High-Quality Negative Prompts</h2><p><strong>Authors:Yufei Liu, Haoke Xiao, Jiaxing Chai, Yongcun Zhang, Rong Wang, Zijie Meng, Zhiming Luo</strong></p>
<p>The advent of Large Vision Models (LVMs) offers new opportunities for few-shot medical image segmentation. However, existing training-free methods based on LVMs fail to effectively utilize negative prompts, leading to poor performance on low-contrast medical images. To address this issue, we propose SynPo, a training-free few-shot method based on LVMs (e.g., SAM), with the core insight: improving the quality of negative prompts. To select point prompts in a more reliable confidence map, we design a novel Confidence Map Synergy Module by combining the strengths of DINOv2 and SAM. Based on the confidence map, we select the top-k pixels as the positive points set and choose the negative points set using a Gaussian distribution, followed by independent K-means clustering for both sets. Then, these selected points are leveraged as high-quality prompts for SAM to get the segmentation results. Extensive experiments demonstrate that SynPo achieves performance comparable to state-of-the-art training-based few-shot methods. </p>
<blockquote>
<p>大型视觉模型（LVMs）的出现为少数医学图像分割提供了新的机会。然而，基于LVMs的无训练方法无法有效利用负提示，导致在低对比度医学图像上的性能不佳。为了解决这一问题，我们提出了SynPo，这是一种基于LVMs的无训练少数方法（例如SAM），其核心见解是提高负提示的质量。为了在更可靠的置信图中选择点提示，我们结合了DINOv2和SAM的优点，设计了一种新型的置信图协同模块。基于置信图，我们选择前k个像素作为正点集，使用高斯分布选择负点集，然后对这两个集合分别进行独立的K-均值聚类。然后，这些选定的点被用作高质量提示，为SAM提供分割结果。大量实验表明，SynPo的性能可与最先进的基于训练少数方法相媲美。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15153v2">PDF</a> MICCAI 2025 Early Accept. Project Page:   <a target="_blank" rel="noopener" href="https://liu-yufei.github.io/synpo-project-page/">https://liu-yufei.github.io/synpo-project-page/</a></p>
<p><strong>Summary</strong></p>
<p>大型视觉模型（LVMs）为少样本医疗图像分割提供了新的机会，但现有的基于LVMs的无训练方法无法有效利用负提示，导致在低对比度医疗图像上的表现不佳。为此，我们提出了基于LVMs的无训练少样本方法SynPo，其核心是改进负提示的质量。通过结合DINOv2和SAM的优点，我们设计了一种新的置信图协同模块，以更可靠的方式选择点提示。基于置信图，我们选择前k个像素作为正点集，使用高斯分布选择负点集，然后对这两组进行独立的K-means聚类。这些选定的点被用作SAM的高质量提示来获得分割结果。大量实验表明，SynPo的性能与基于训练的少样本方法相当。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型视觉模型（LVMs）在少样本医疗图像分割上具有潜力。</li>
<li>现有基于LVMs的无训练方法在处理低对比度医疗图像时表现不佳。</li>
<li>SynPo方法通过改进负提示的质量来解决这一问题。</li>
<li>SynPo结合了DINOv2和SAM的优点，设计了一种新的置信图协同模块。</li>
<li>基于置信图，SynPo选择正、负点集，并利用这些点作为高质量提示进行图像分割。</li>
<li>SynPo的性能与基于训练的少样本方法相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15153">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-96dbe471f0ef20ab45c582024ead24a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a64112893cb38c03306e6d6cd7e48c88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3b9f688b027c6b4228079a26d761301.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="IQE-CLIP-Instance-aware-Query-Embedding-for-Zero-Few-shot-Anomaly-Detection-in-Medical-Domain"><a href="#IQE-CLIP-Instance-aware-Query-Embedding-for-Zero-Few-shot-Anomaly-Detection-in-Medical-Domain" class="headerlink" title="IQE-CLIP: Instance-aware Query Embedding for Zero-&#x2F;Few-shot Anomaly   Detection in Medical Domain"></a>IQE-CLIP: Instance-aware Query Embedding for Zero-&#x2F;Few-shot Anomaly   Detection in Medical Domain</h2><p><strong>Authors:Hong Huang, Weixiang Sun, Zhijian Wu, Jingwen Niu, Donghuan Lu, Xian Wu, Yefeng Zheng</strong></p>
<p>Recently, the rapid advancements of vision-language models, such as CLIP, leads to significant progress in zero-&#x2F;few-shot anomaly detection (ZFSAD) tasks. However, most existing CLIP-based ZFSAD methods commonly assume prior knowledge of categories and rely on carefully crafted prompts tailored to specific scenarios. While such meticulously designed text prompts effectively capture semantic information in the textual space, they fall short of distinguishing normal and anomalous instances within the joint embedding space. Moreover, these ZFSAD methods are predominantly explored in industrial scenarios, with few efforts conducted to medical tasks. To this end, we propose an innovative framework for ZFSAD tasks in medical domain, denoted as IQE-CLIP. We reveal that query embeddings, which incorporate both textual and instance-aware visual information, are better indicators for abnormalities. Specifically, we first introduce class-based prompting tokens and learnable prompting tokens for better adaptation of CLIP to the medical domain. Then, we design an instance-aware query module (IQM) to extract region-level contextual information from both text prompts and visual features, enabling the generation of query embeddings that are more sensitive to anomalies. Extensive experiments conducted on six medical datasets demonstrate that IQE-CLIP achieves state-of-the-art performance on both zero-shot and few-shot tasks. We release our code and data at <a target="_blank" rel="noopener" href="https://github.com/hongh0/IQE-CLIP/">https://github.com/hongh0/IQE-CLIP/</a>. </p>
<blockquote>
<p>最近，视觉语言模型（如CLIP）的快速发展推动了零&#x2F;少样本异常检测（ZFSAD）任务的显著进步。然而，大多数现有的基于CLIP的ZFSAD方法通常假设对类别的先验知识，并依赖于针对特定场景精心设计的提示。虽然这种精心设计文本提示可以有效地捕获文本空间中的语义信息，但它们无法在联合嵌入空间中区分正常和异常的实例。此外，这些ZFSAD方法主要在工业场景中得到了探索，而在医学任务中的研究很少。为此，我们针对医学领域的ZFSAD任务提出了一个创新框架，称为IQE-CLIP。我们发现，融合文本和实例感知视觉信息的查询嵌入是更好的异常指示器。具体来说，我们首先引入基于类别的提示令牌和可学习的提示令牌，以更好地适应CLIP在医学领域的应用。然后，我们设计了一个实例感知查询模块（IQM），从文本提示和视觉特征中提取区域级别的上下文信息，从而生成对异常更敏感的查询嵌入。在六个医学数据集上进行的广泛实验表明，IQE-CLIP在零样本和少样本任务上都达到了最先进的性能。我们在<a target="_blank" rel="noopener" href="https://github.com/hongh0/IQE-CLIP/">https://github.com/hongh0/IQE-CLIP/</a>上发布了我们的代码和数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10730v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了在零&#x2F;少样本异常检测（ZFSAD）任务中，基于CLIP的视觉语言模型的新进展及其在医疗领域的应用。针对现有方法依赖特定类别先验知识和文本提示的局限性，提出了一种新的框架IQE-CLIP。该框架通过结合文本和实例感知的视觉信息，生成查询嵌入，以更有效地检测异常。实验证明，IQE-CLIP在六个医疗数据集上实现了零样本和少样本任务的最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIP模型在零&#x2F;少样本异常检测任务（ZFSAD）中有显著进展。</li>
<li>现有CLIP-based ZFSAD方法依赖类别先验知识和针对特定场景设计的文本提示。</li>
<li>IQE-CLIP框架结合了文本和实例感知的视觉信息，生成查询嵌入以检测异常。</li>
<li>IQE-CLIP引入基于类别的提示令牌和可学习的提示令牌，以更好地适应医疗领域。</li>
<li>设计了实例感知查询模块（IQM）以提取区域级别的上下文信息，从而提高对异常的敏感性。</li>
<li>在六个医疗数据集上进行的广泛实验证明IQE-CLIP在零样本和少样本任务上实现了最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10730">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5961f2095532a8418992cd84409cddf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f54646641f5923a7ef176de168cb89c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c51ef4e50943405dae4645a0783f91ca.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Towards-Efficient-Few-shot-Graph-Neural-Architecture-Search-via-Partitioning-Gradient-Contribution"><a href="#Towards-Efficient-Few-shot-Graph-Neural-Architecture-Search-via-Partitioning-Gradient-Contribution" class="headerlink" title="Towards Efficient Few-shot Graph Neural Architecture Search via   Partitioning Gradient Contribution"></a>Towards Efficient Few-shot Graph Neural Architecture Search via   Partitioning Gradient Contribution</h2><p><strong>Authors:Wenhao Song, Xuan Wu, Bo Yang, You Zhou, Yubin Xiao, Yanchun Liang, Hongwei Ge, Heow Pueh Lee, Chunguo Wu</strong></p>
<p>To address the weight coupling problem, certain studies introduced few-shot Neural Architecture Search (NAS) methods, which partition the supernet into multiple sub-supernets. However, these methods often suffer from computational inefficiency and tend to provide suboptimal partitioning schemes. To address this problem more effectively, we analyze the weight coupling problem from a novel perspective, which primarily stems from distinct modules in succeeding layers imposing conflicting gradient directions on the preceding layer modules. Based on this perspective, we propose the Gradient Contribution (GC) method that efficiently computes the cosine similarity of gradient directions among modules by decomposing the Vector-Jacobian Product during supernet backpropagation. Subsequently, the modules with conflicting gradient directions are allocated to distinct sub-supernets while similar ones are grouped together. To assess the advantages of GC and address the limitations of existing Graph Neural Architecture Search methods, which are limited to searching a single type of Graph Neural Networks (Message Passing Neural Networks (MPNNs) or Graph Transformers (GTs)), we propose the Unified Graph Neural Architecture Search (UGAS) framework, which explores optimal combinations of MPNNs and GTs. The experimental results demonstrate that GC achieves state-of-the-art (SOTA) performance in supernet partitioning quality and time efficiency. In addition, the architectures searched by UGAS+GC outperform both the manually designed GNNs and those obtained by existing NAS methods. Finally, ablation studies further demonstrate the effectiveness of all proposed methods. </p>
<blockquote>
<p>为了解决权重耦合问题，一些研究引入了小样本神经网络架构搜索（NAS）方法，将超网划分为多个子超网。然而，这些方法通常存在计算效率低下的问题，并且往往提供次优的划分方案。为了更有效地解决这个问题，我们从一个新的角度分析了权重耦合问题，其主要源于后续层中的不同模块对前一层模块施加冲突的梯度方向。基于此，我们提出了梯度贡献（GC）方法，它通过分解超网反向传播中的向量-雅可比乘积，有效地计算了模块之间梯度方向的余弦相似性。随后，具有冲突梯度方向的模块被分配到不同的子超网中，而相似的模块则组合在一起。为了评估GC的优势，并解决现有图神经网络架构搜索方法的局限性（这些方法仅限于搜索单一类型的图神经网络，如消息传递神经网络（MPNNs）或图转换器（GTs）），我们提出了统一图神经网络架构搜索（UGAS）框架，该框架探索了MPNNs和GTs的最佳组合。实验结果表明，GC在超网划分质量和时间效率方面达到了最新水平。此外，通过UGAS+GC搜索的架构优于手动设计的GNNs和现有NAS方法得到的架构。最后，消融研究进一步证明了所有提出方法的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01231v2">PDF</a> Accepted by SIGKDD 2025</p>
<p><strong>Summary</strong></p>
<p>本文研究了基于神经网络架构搜索（NAS）的权重视角问题，提出了梯度贡献（GC）方法和统一图神经网络架构搜索（UGAS）框架。GC方法通过计算模块间梯度方向的余弦相似性来解决权重耦合问题，将具有冲突梯度方向的模块分配给不同的子超网，而相似的模块则组合在一起。UGAS框架旨在探索消息传递神经网络（MPNNs）和图变换器（GTs）的最佳组合。实验结果表明，GC在超网划分质量和时间效率方面达到了最新水平，UGAS+GC搜索的架构超越了手动设计的GNNs和现有NAS方法的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究引入了一种新的视角来解决权重耦合问题，主要源于连续层中的不同模块对前面层模块施加冲突的梯度方向。</li>
<li>提出了梯度贡献（GC）方法，通过计算模块间梯度方向的余弦相似性来解决权重耦合问题。</li>
<li>GC方法实现了高效计算模块间的梯度方向相似性，实现了冲突梯度的分配与相似模块的聚合。</li>
<li>提出了一种统一的图神经网络架构搜索（UGAS）框架，旨在探索消息传递神经网络（MPNNs）和图变换器（GTs）的最佳组合。</li>
<li>实验结果表明，GC在超网划分质量和时间效率方面达到了最新水平。</li>
<li>UGAS+GC搜索的架构性能超越了手动设计的GNNs和现有NAS方法的结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01231">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fb5919d991ec5c7d84067f82f26da2e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c67e342f880ff0083b07ddfd6e84028.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8f305696bdb1cac5e28af9a31468418.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29f4cf0d1d4b80d2bd60eb9e474245c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15b1e2157809a64604b6e63f04637d44.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-79438395ee004ae9016437c45cf9b811.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Calibrating-Pre-trained-Language-Classifiers-on-LLM-generated-Noisy-Labels-via-Iterative-Refinement"><a href="#Calibrating-Pre-trained-Language-Classifiers-on-LLM-generated-Noisy-Labels-via-Iterative-Refinement" class="headerlink" title="Calibrating Pre-trained Language Classifiers on LLM-generated Noisy   Labels via Iterative Refinement"></a>Calibrating Pre-trained Language Classifiers on LLM-generated Noisy   Labels via Iterative Refinement</h2><p><strong>Authors:Liqin Ye, Agam Shah, Chao Zhang, Sudheer Chava</strong></p>
<p>The traditional process of creating labeled datasets is labor-intensive and expensive. Recent breakthroughs in open-source large language models (LLMs) have opened up a new avenue in generating labeled datasets automatically for various natural language processing (NLP) tasks, providing an alternative to such an expensive annotation process. However, the reliability of such auto-generated labels remains a significant concern due to inherent inaccuracies. When learning from noisy labels, the model’s generalization is likely to be harmed as it is prone to overfit to those label noises. While previous studies in learning from noisy labels mainly focus on synthetic noise and real-world noise, LLM-generated label noise receives less attention. In this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to calibrate the classifier’s prediction, thus enhancing its robustness towards LLM-generated noisy labels. SiDyP retrieves potential true label candidates by neighborhood label distribution in text embedding space and iteratively refines noisy candidates using a simplex diffusion model. Our framework can increase the performance of the BERT classifier fine-tuned on both zero-shot and few-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30% respectively. We demonstrate the effectiveness of SiDyP by conducting extensive benchmarking for different LLMs over a variety of NLP tasks. Our code is available on Github. </p>
<blockquote>
<p>传统创建标注数据集的过程劳动强度高且费用昂贵。最近开源大型语言模型（LLM）的突破为各种自然语言处理（NLP）任务自动生成标注数据集开辟了一条新途径，为这种昂贵的标注过程提供了替代方案。然而，由于内在的不准确性，这种自动生成的标签的可靠性仍然是一个值得关注的问题。当模型从带有噪声的标签中学习时，其泛化能力可能会受到损害，因为它容易过度适应这些标签噪声。虽然之前关于从噪声标签中学习的研究主要集中在合成噪声和真实世界噪声上，但LLM生成的标签噪声受到的关注较少。在本文中，我们提出了SiDyP：基于动态先验的简单标签扩散，以校正分类器的预测，从而提高其对LLM生成的有噪声标签的鲁棒性。SiDyP通过文本嵌入空间中的邻域标签分布检索潜在的真实标签候选者，并使用简单扩散模型迭代优化噪声候选者。我们的框架可以提高在零样本和少样本LLM生成的有噪声标签数据集上微调BERT分类器的性能，平均提高7.21%和7.30%。我们通过针对不同LLM和多种NLP任务进行广泛的标准基准测试，证明了SiDyP的有效性。我们的代码已在GitHub上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19675v2">PDF</a> Accepted at KDD’25</p>
<p><strong>Summary</strong></p>
<p>本文介绍了利用大型语言模型自动生成标注数据集的方法，作为对传统标注数据集的替代方案。然而，自动生成的标签存在可靠性问题，模型的泛化能力可能会受到损害。针对LLM生成的标签噪声，本文提出了SiDyP方法，通过简单标签扩散和动态先验来校准分类器预测，提高其鲁棒性。SiDyP通过文本嵌入空间中的邻域标签分布检索潜在的真实标签候选者，并使用简单扩散模型迭代优化噪声候选者。该框架提高了在零样本和少样本LLM生成噪声标签数据集上微调BERT分类器的性能，平均提高了7.21%和7.30%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）可自动生成标注数据集，为NLP任务提供便捷替代方案。</li>
<li>自动生成的标签存在可靠性问题，可能影响模型泛化能力。</li>
<li>LLM生成的标签噪声较少受到关注，但可能对模型性能产生重大影响。</li>
<li>本文提出了SiDyP方法，通过简单标签扩散和动态先验提高分类器对LLM生成噪声标签的鲁棒性。</li>
<li>SiDyP通过检索文本嵌入空间中的邻域标签来提高性能，并迭代优化噪声候选者。</li>
<li>SiDyP在零样本和少样本LLM生成噪声标签数据集上微调BERT分类器的性能有所提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19675">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d9c7f9ca24f5627fc678c855267cb4e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1596d9f57a99782033b4dc9164ce6df6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9504263dce41bee3476baaa8573d3899.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f50bcc49e408a4345384b8f18ae7044.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-083acf531e66e8209b12054d5896d9ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84c1e9258d3e21385efd2c7fb50aef2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce0e105c0af9af76ffad9398cf9d46bd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Cost-effective-Instruction-Learning-for-Pathology-Vision-and-Language-Analysis"><a href="#Cost-effective-Instruction-Learning-for-Pathology-Vision-and-Language-Analysis" class="headerlink" title="Cost-effective Instruction Learning for Pathology Vision and Language   Analysis"></a>Cost-effective Instruction Learning for Pathology Vision and Language   Analysis</h2><p><strong>Authors:Kaitao Chen, Mianxin Liu, Fang Yan, Lei Ma, Xiaoming Shi, Lilong Wang, Xiaosong Wang, Lifeng Zhu, Zhe Wang, Mu Zhou, Shaoting Zhang</strong></p>
<p>The advent of vision-language models fosters the interactive conversations between AI-enabled models and humans. Yet applying these models into clinics must deal with daunting challenges around large-scale training data, financial, and computational resources. Here we propose a cost-effective instruction learning framework for conversational pathology named as CLOVER. CLOVER only trains a lightweight module and uses instruction tuning while freezing the parameters of the large language model. Instead of using costly GPT-4, we propose well-designed prompts on GPT-3.5 for building generation-based instructions, emphasizing the utility of pathological knowledge derived from the Internet source. To augment the use of instructions, we construct a high-quality set of template-based instructions in the context of digital pathology. From two benchmark datasets, our findings reveal the strength of hybrid-form instructions in the visual question-answer in pathology. Extensive results show the cost-effectiveness of CLOVER in answering both open-ended and closed-ended questions, where CLOVER outperforms strong baselines that possess 37 times more training parameters and use instruction data generated from GPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot learning in the external clinical dataset. These findings demonstrate that cost-effective modeling of CLOVER could accelerate the adoption of rapid conversational applications in the landscape of digital pathology. </p>
<blockquote>
<p>视觉语言模型的兴起促进了人工智能模型与人类之间的交互式对话。然而，将这些模型应用于诊所必须应对大规模训练数据、财务和计算资源方面的艰巨挑战。在这里，我们提出了一个用于对话病理学的经济高效指令学习框架，名为CLOVER。CLOVER只训练一个轻量级模块，并使用指令微调，同时冻结大规模语言模型的参数。我们并没有使用成本高昂的GPT-4，而是提出了在GPT-3.5上设计良好的提示来构建基于生成的指令，强调从互联网资源中派生出的病理学知识的实用性。为了增强指令的使用，我们在数字病理的语境中构建了一组高质量的基于模板的指令集。从两个基准数据集中，我们的研究结果表明，在病理视觉问答中混合形式的指令的强大性。大量结果表明，在回答开放性和封闭性问题方面，CLOVER具有成本效益，且优于拥有37倍以上训练参数的强大基线，并使用GPT-4生成的指令数据。通过指令微调，CLOVER在外部临床数据集中展现了少量学习的稳健性。这些结果表明，CLOVER的经济高效建模可以加速数字病理领域快速对话应用的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.17734v2">PDF</a> </p>
<p><strong>Summary</strong><br>     随着视觉语言模型的兴起，推动了人工智能模型与人类之间的交互对话。本文将大型语言模型应用于临床领域，提出了一种具有成本效益的指令学习框架CLOVER。CLOVER仅训练轻量级模块，并使用指令微调来冻结大型语言模型的参数。通过巧妙设计GPT-3.5的提示符，不使用昂贵的GPT-4构建生成指令，强调从互联网来源获取病理知识的实用性。为了增强指令的使用，我们在数字病理学背景下构建了一套高质量模板指令。从两个基准数据集的实验结果来看，CLOVER在病理学视觉问答中具有强大的混合形式指令能力。相较于具有强大基线且训练参数多出37倍且使用GPT-4生成的指令数据，CLOVER展现出高效性和优越性。通过指令微调，展现出强大的泛化能力和应对外部临床数据集的鲁棒性。表明CLOVER的经济建模可能加速数字病理学领域快速对话应用的发展。 </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLOVER是一个具有成本效益的指令学习框架，适用于对话式病理学应用。</li>
<li>CLOVER仅训练轻量级模块，并采用指令微调来优化大型语言模型的参数使用。</li>
<li>使用GPT-3.5构建生成指令，避免使用昂贵的GPT-4，同时强调从互联网获取病理知识的价值。</li>
<li>在数字病理学背景下，构建了高质量模板指令集，以增强指令的使用效果。</li>
<li>从基准数据集中发现，CLOVER在病理学视觉问答中表现出强大的混合形式指令能力。</li>
<li>CLOVER相较于其他训练参数更多的模型展现出优越的性能和成本效益。</li>
<li>通过指令微调，CLOVER展现出强大的泛化能力和应对外部临床数据集的鲁棒性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.17734">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-305457b1ad5ab8ab2a78b56ba6bf0111.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb1257e8598da3ad107b8138434c8199.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5dbb27237bd00903e6a9b6b822c8dfb5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-53092162b508feea490dc5989d9cd034.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="AQA-Bench-An-Interactive-Benchmark-for-Evaluating-LLMs’-Sequential-Reasoning-Ability"><a href="#AQA-Bench-An-Interactive-Benchmark-for-Evaluating-LLMs’-Sequential-Reasoning-Ability" class="headerlink" title="AQA-Bench: An Interactive Benchmark for Evaluating LLMs’ Sequential   Reasoning Ability"></a>AQA-Bench: An Interactive Benchmark for Evaluating LLMs’ Sequential   Reasoning Ability</h2><p><strong>Authors:Siwei Yang, Bingchen Zhao, Cihang Xie</strong></p>
<p>This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol - for example, in DFS, the availability of each node’s connected edge is contingent upon the model’s traversal to that node, thereby necessitating the LLM’s ability to effectively remember visited nodes and strategize subsequent moves considering the possible environmental feedback in the future steps. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 14 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show much stronger sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing in-context examples may inadvertently hurt few-shot performance in an interactive environment due to over-fitting to examples. (3) Instead of using optimal steps from another test case as the in-context example, a very limited number of predecessor steps in the current test case following the optimal policy can substantially boost small models’ performance. (4) The performance gap between weak models and strong models is greatly due to the incapability of weak models to start well. (5) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend. We hope our study can catalyze future work on advancing the understanding and enhancement of LLMs’ capabilities in sequential reasoning. The code is available at <a target="_blank" rel="noopener" href="https://github.com/UCSC-VLAA/AQA-Bench">https://github.com/UCSC-VLAA/AQA-Bench</a>. </p>
<blockquote>
<p>本文介绍了AQA-Bench，这是一个新的基准测试，旨在评估大型语言模型（LLM）在算法上下文中的顺序推理能力，例如深度优先搜索（DFS）。我们的评估基准的关键特点在于其交互评估协议——例如，在DFS中，每个节点的连通边的可用性取决于模型遍历到该节点的情况，从而需要LLM有效记忆已访问节点并策划后续行动，考虑未来步骤中可能的环境反馈。我们全面构建了AQA-Bench，包含三种不同的算法，即二分搜索、深度优先搜索和广度优先搜索，并评估了14种不同LLM的顺序推理能力。我们的调查发现了几个有趣的发现：（1）像GPT-4和双子座这样的封闭源代码模型通常表现出更强的顺序推理能力，显著优于开源LLM。（2）盲目提供上下文示例可能会无意中伤害交互式环境中的小样本性能，因为过度拟合示例。（3）与从另一个测试用例提供最佳步骤作为上下文示例相比，在当前测试用例中遵循最佳策略的几个先行步骤可以大幅提升小型模型的性能。（4）弱模型与强模型之间的性能差距很大程度上是由于弱模型无法良好开局。（5）性能与模型大小之间的规模相关性并不总是显著，有时甚至呈现逆趋势。我们希望本研究能催化未来工作，推动对LLM在顺序推理能力方面的理解和提升。代码可在[<a target="_blank" rel="noopener" href="https://github.com/UCSC-VLAA/AQA-Bench%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/UCSC-VLAA/AQA-Bench找到。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.09404v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文介绍了AQA-Bench，一个用于评估大型语言模型在算法上下文（如深度优先搜索）中的顺序推理能力的新颖基准测试。其关键特性在于其交互式评估协议，例如在深度优先搜索中，每个节点的连通边的可用性取决于模型对该节点的遍历，从而需要大型语言模型具备在考虑到未来步骤的可能环境反馈的情况下，有效记住已访问节点并策划后续动作的能力。该研究使用三种不同算法构建了AQA-Bench，包括二分搜索、深度优先搜索和广度优先搜索，并评估了14种大型语言模型的顺序推理能力。研究发现了多个有趣的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AQA-Bench是一个评估大型语言模型在算法上下文中的顺序推理能力的新颖基准测试。</li>
<li>其交互式评估协议要求大型语言模型在考虑到未来环境反馈的情况下有效记住已访问节点并规划后续动作。</li>
<li>在深度优先搜索等场景中，模型需要根据自身遍历的节点来决定连通边的可用性。</li>
<li>封闭源代码的大型语言模型（如GPT-4和Gemini）展现出更强的顺序推理能力，显著优于开源的大型语言模型。</li>
<li>在交互式环境中，过度依赖上下文示例可能会损害小型模型的性能。</li>
<li>使用当前测试用例中的最优步骤的少量先行步骤能显著提升小型模型的性能。</li>
<li>模型间的性能差距很大程度上源于弱模型在起始阶段的表现不佳，而模型性能与模型规模之间的相关性并不总是显著，有时甚至呈现逆趋势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.09404">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-05eb4867fc896a5a6cc66a90989550f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e20cf3bc6623e86371c07977353074f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fe9d8182d9cd8d6dbed415e256fdc1f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2f5922d7ace749899b09a181dc11cbfd.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-06-24  Seeing is Fixing Cross-Modal Reasoning with Multimodal LLMs for Visual   Software Issue Fixing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9f35788449f25c3ac391d6d19c4e11b0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-06-24  RAGentA Multi-Agent Retrieval-Augmented Generation for Attributed   Question Answering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
