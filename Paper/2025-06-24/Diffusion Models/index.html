<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-24  Assembler Scalable 3D Part Assembly via Anchor Point Diffusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9c753a371aee931ac0e062ce7117b103.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    45 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-24-æ›´æ–°"><a href="#2025-06-24-æ›´æ–°" class="headerlink" title="2025-06-24 æ›´æ–°"></a>2025-06-24 æ›´æ–°</h1><h2 id="Assembler-Scalable-3D-Part-Assembly-via-Anchor-Point-Diffusion"><a href="#Assembler-Scalable-3D-Part-Assembly-via-Anchor-Point-Diffusion" class="headerlink" title="Assembler: Scalable 3D Part Assembly via Anchor Point Diffusion"></a>Assembler: Scalable 3D Part Assembly via Anchor Point Diffusion</h2><p><strong>Authors:Wang Zhao, Yan-Pei Cao, Jiale Xu, Yuejiang Dong, Ying Shan</strong></p>
<p>We present Assembler, a scalable and generalizable framework for 3D part assembly that reconstructs complete objects from input part meshes and a reference image. Unlike prior approaches that mostly rely on deterministic part pose prediction and category-specific training, Assembler is designed to handle diverse, in-the-wild objects with varying part counts, geometries, and structures. It addresses the core challenges of scaling to general 3D part assembly through innovations in task formulation, representation, and data. First, Assembler casts part assembly as a generative problem and employs diffusion models to sample plausible configurations, effectively capturing ambiguities arising from symmetry, repeated parts, and multiple valid assemblies. Second, we introduce a novel shape-centric representation based on sparse anchor point clouds, enabling scalable generation in Euclidean space rather than SE(3) pose prediction. Third, we construct a large-scale dataset of over 320K diverse part-object assemblies using a synthesis and filtering pipeline built on existing 3D shape repositories. Assembler achieves state-of-the-art performance on PartNet and is the first to demonstrate high-quality assembly for complex, real-world objects. Based on Assembler, we further introduce an interesting part-aware 3D modeling system that generates high-resolution, editable objects from images, demonstrating potential for interactive and compositional design. Project page: <a target="_blank" rel="noopener" href="https://assembler3d.github.io/">https://assembler3d.github.io</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Assemblerï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº3Dé›¶ä»¶è£…é…çš„å¯æ‰©å±•å’Œé€šç”¨æ€§æ¡†æ¶ï¼Œå®ƒå¯ä»¥ä»è¾“å…¥é›¶ä»¶ç½‘æ ¼å’Œå‚è€ƒå›¾åƒé‡å»ºå®Œæ•´çš„ç‰©ä½“ã€‚ä¸åŒäºå¤§å¤šæ•°ä¾èµ–äºç¡®å®šæ€§é›¶ä»¶å§¿æ€é¢„æµ‹å’Œç‰¹å®šç±»åˆ«è®­ç»ƒçš„æ–¹æ³•ï¼ŒAssemblerè¢«è®¾è®¡æ¥å¤„ç†å„ç§çœŸå®ä¸–ç•Œçš„ç‰©ä½“ï¼Œå…·æœ‰ä¸åŒçš„é›¶ä»¶æ•°é‡ã€å‡ ä½•å½¢çŠ¶å’Œç»“æ„ã€‚å®ƒé€šè¿‡ä»»åŠ¡åˆ¶å®šã€è¡¨å¾å’Œæ•°æ®æ–¹é¢çš„åˆ›æ–°æ¥è§£å†³æ‰©å±•åˆ°é€šç”¨3Dé›¶ä»¶è£…é…çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚é¦–å…ˆï¼ŒAssemblerå°†é›¶ä»¶è£…é…ä½œä¸ºç”Ÿæˆæ€§é—®é¢˜ï¼Œå¹¶é‡‡ç”¨æ‰©æ•£æ¨¡å‹å¯¹å¯èƒ½çš„é…ç½®è¿›è¡Œé‡‡æ ·ï¼Œä»è€Œæœ‰æ•ˆåœ°è§£å†³ç”±å¯¹ç§°æ€§ã€é‡å¤é›¶ä»¶å’Œå¤šä¸ªæœ‰æ•ˆè£…é…æ‰€å¼•èµ·çš„æ­§ä¹‰ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºç¨€ç–é”šç‚¹äº‘çš„æ–°å‹å½¢çŠ¶ä¸­å¿ƒè¡¨å¾ï¼Œèƒ½å¤Ÿåœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­è¿›è¡Œå¯æ‰©å±•ç”Ÿæˆï¼Œè€Œä¸æ˜¯è¿›è¡ŒSE(3)å§¿æ€é¢„æµ‹ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬ä½¿ç”¨å»ºç«‹åœ¨ç°æœ‰3Då½¢çŠ¶å­˜å‚¨åº“ä¸Šçš„åˆæˆå’Œè¿‡æ»¤ç®¡é“ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡32ä¸‡ä¸ªå¤šæ ·åŒ–é›¶ä»¶ç‰©ä½“è£…é…çš„å¤§å‹æ•°æ®é›†ã€‚Assembleråœ¨PartNetä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œå¹¶ä¸”æ˜¯ç¬¬ä¸€ä¸ªå±•ç¤ºé’ˆå¯¹å¤æ‚ç°å®ä¸–ç•Œç‰©ä½“é«˜è´¨é‡è£…é…çš„ç³»ç»Ÿã€‚åŸºäºAssemblerï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æ¨å‡ºäº†ä¸€æ¬¾æœ‰è¶£çš„é›¶ä»¶æ„ŸçŸ¥3Då»ºæ¨¡ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå¯ä»¥ä»å›¾åƒç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„å¯ç¼–è¾‘ç‰©ä½“ï¼Œå±•ç¤ºäº†å…¶åœ¨äº¤äº’å¼å’Œç»„åˆè®¾è®¡æ–¹é¢çš„æ½œåŠ›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://assembler3d.github.io/">https://assembler3d.github.io</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17074v1">PDF</a> Technical Report. Project page: <a target="_blank" rel="noopener" href="https://assembler3d.github.io/">https://assembler3d.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åä¸ºAssemblerçš„æ¡†æ¶ï¼Œå®ƒå¯ä»¥ä»è¾“å…¥çš„éƒ¨åˆ†ç½‘æ ¼å’Œå‚è€ƒå›¾åƒé‡å»ºå®Œæ•´çš„ç‰©ä½“ï¼Œå®ç°3Déƒ¨ä»¶ç»„è£…ã€‚ä¸åŒäºå¤§å¤šæ•°ä¾èµ–äºç¡®å®šæ€§éƒ¨ä»¶å§¿æ€é¢„æµ‹å’Œç‰¹å®šç±»åˆ«è®­ç»ƒçš„æ–¹æ³•ï¼ŒAssemblerè®¾è®¡ç”¨äºå¤„ç†å„ç§é‡ç”Ÿå¯¹è±¡ï¼Œå…·æœ‰ä¸åŒçš„éƒ¨ä»¶æ•°é‡ã€å‡ ä½•å½¢çŠ¶å’Œç»“æ„ã€‚å®ƒé€šè¿‡ä»»åŠ¡åˆ¶å®šã€è¡¨ç¤ºå’Œæ•°æ®åˆ›æ–°è§£å†³æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå°†éƒ¨ä»¶ç»„è£…è½¬åŒ–ä¸ºç”Ÿæˆé—®é¢˜ï¼Œå¹¶é‡‡ç”¨æ‰©æ•£æ¨¡å‹å¯¹å¯èƒ½çš„é…ç½®è¿›è¡Œé‡‡æ ·ï¼Œæœ‰æ•ˆæ•æ‰ç”±å¯¹ç§°æ€§ã€é‡å¤éƒ¨ä»¶å’Œå¤šä¸ªæœ‰æ•ˆç»„è£…å¼•èµ·çš„æ­§ä¹‰ã€‚æ­¤å¤–ï¼Œå¼•å…¥åŸºäºç¨€ç–é”šç‚¹çš„å½¢çŠ¶ä¸­å¿ƒè¡¨ç¤ºæ³•ï¼Œå®ç°æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­çš„å¯æ‰©å±•ç”Ÿæˆï¼Œè€Œä¸æ˜¯SE(3)å§¿æ€é¢„æµ‹ã€‚ä½¿ç”¨åŸºäºç°æœ‰3Då½¢çŠ¶ä»“åº“çš„åˆæˆå’Œè¿‡æ»¤ç®¡é“æ„å»ºçš„å¤§å‹éƒ¨ä»¶å¯¹è±¡ç»„è£…æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡32ä¸‡ä¸ªå¤šæ ·åŒ–æ ·æœ¬ã€‚Assembleråœ¨PartNetä¸Šè¾¾åˆ°æœ€æ–°æ€§èƒ½ï¼Œå¹¶é¦–æ¬¡ä¸ºå¤æ‚ç°å®ä¸–ç•Œå¯¹è±¡å®ç°é«˜è´¨é‡ç»„è£…ã€‚åŸºäºAssemblerï¼Œè¿›ä¸€æ­¥å¼•å…¥æœ‰è¶£çš„éƒ¨ä»¶æ„ŸçŸ¥3Då»ºæ¨¡ç³»ç»Ÿï¼Œä»å›¾åƒç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€å¯ç¼–è¾‘çš„å¯¹è±¡ï¼Œå±•ç¤ºäº¤äº’å¼å’Œç»„åˆè®¾è®¡çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Assembleræ˜¯ä¸€ä¸ªç”¨äº3Déƒ¨ä»¶ç»„è£…çš„å¯ä¼¸ç¼©å’Œé€šç”¨æ¡†æ¶ï¼Œèƒ½å¤Ÿä»è¾“å…¥éƒ¨åˆ†ç½‘æ ¼å’Œå‚è€ƒå›¾åƒé‡å»ºå®Œæ•´ç‰©ä½“ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒAssemblerèƒ½å¤Ÿå¤„ç†å…·æœ‰ä¸åŒéƒ¨ä»¶æ•°é‡ã€å‡ ä½•å½¢çŠ¶å’Œç»“æ„çš„å„ç§å¯¹è±¡ã€‚</li>
<li>Assembleré€šè¿‡å°†éƒ¨ä»¶ç»„è£…è½¬åŒ–ä¸ºç”Ÿæˆé—®é¢˜ï¼Œå¹¶é‡‡ç”¨æ‰©æ•£æ¨¡å‹é‡‡æ ·æ¥è§£å†³æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>Assemblerå¼•å…¥åŸºäºç¨€ç–é”šç‚¹çš„å½¢çŠ¶ä¸­å¿ƒè¡¨ç¤ºæ³•ï¼Œå®ç°æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­çš„å¯æ‰©å±•ç”Ÿæˆã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ç”¨åˆæˆå’Œè¿‡æ»¤ç®¡é“æ„å»ºçš„å¤§å‹æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡32ä¸‡ä¸ªå¤šæ ·åŒ–çš„éƒ¨ä»¶å¯¹è±¡ç»„è£…æ ·æœ¬ã€‚</li>
<li>Assembleråœ¨PartNetä¸Šè¡¨ç°å‡ºæœ€æ–°æ€§èƒ½ï¼Œå¹¶ä¸ºå¤æ‚ç°å®ä¸–ç•Œå¯¹è±¡æä¾›é«˜è´¨é‡ç»„è£…èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-99c24bd49853240114b72c578ab2211e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd3b1b258fcababa80654acb8e3e129e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-545e00d751c3580e7fa78dec6376e769.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d98d168c3e46d64916c50e5e45e68299.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Reward-Agnostic-Prompt-Optimization-for-Text-to-Image-Diffusion-Models"><a href="#Reward-Agnostic-Prompt-Optimization-for-Text-to-Image-Diffusion-Models" class="headerlink" title="Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models"></a>Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models</h2><p><strong>Authors:Semin Kim, Yeonwoo Cha, Jaehoon Yoo, Seunghoon Hong</strong></p>
<p>We investigate a general approach for improving user prompts in text-to-image (T2I) diffusion models by finding prompts that maximize a reward function specified at test-time. Although diverse reward models are used for evaluating image generation, existing automated prompt engineering methods typically target specific reward configurations. Consequently, these specialized designs exhibit suboptimal performance when applied to new prompt engineering scenarios involving different reward models. To address this limitation, we introduce RATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time optimization method applicable across various reward scenarios without modification. RATTPO iteratively searches for optimized prompts by querying large language models (LLMs) \textit{without} requiring reward-specific task descriptions. Instead, it uses the optimization trajectory and a novel reward-aware feedback signal (termed a â€œhintâ€) as context. Empirical results demonstrate the versatility of RATTPO, effectively enhancing user prompts across diverse reward setups that assess various generation aspects, such as aesthetics, general human preference, or spatial relationships between objects. RATTPO surpasses other test-time search baselines in search efficiency, using up to 3.5 times less inference budget, and, given sufficient inference budget, achieves performance comparable to learning-based baselines that require reward-specific fine-tuning. The code is available at <a target="_blank" rel="noopener" href="https://github.com/seminkim/RATTPO">https://github.com/seminkim/RATTPO</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§é€šè¿‡å¯»æ‰¾åœ¨æµ‹è¯•æ—¶æœ€å¤§åŒ–æŒ‡å®šå¥–åŠ±å‡½æ•°çš„æç¤ºæ¥æ”¹å–„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸­çš„ç”¨æˆ·æç¤ºçš„é€šç”¨æ–¹æ³•ã€‚å°½ç®¡ç”¨äºè¯„ä¼°å›¾åƒç”Ÿæˆçš„ä¸åŒå¥–åŠ±æ¨¡å‹è¢«å¹¿æ³›åº”ç”¨ï¼Œä½†ç°æœ‰çš„è‡ªåŠ¨åŒ–æç¤ºå·¥ç¨‹æ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šçš„å¥–åŠ±é…ç½®ã€‚å› æ­¤ï¼Œå½“åº”ç”¨äºæ¶‰åŠä¸åŒå¥–åŠ±æ¨¡å‹çš„æ–°æç¤ºå·¥ç¨‹åœºæ™¯æ—¶ï¼Œè¿™äº›ä¸“ä¸šè®¾è®¡è¡¨ç°å‡ºæ¬¡ä¼˜æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸å¥–åŠ±æ— å…³çš„æµ‹è¯•æ—¶é—´æç¤ºä¼˜åŒ–ï¼ˆRATTPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§çµæ´»çš„åœ¨æµ‹è¯•æ—¶é—´è¿›è¡Œä¼˜åŒ–æ–¹æ³•ï¼Œå¯åœ¨å„ç§å¥–åŠ±åœºæ™¯ä¸­é€‚ç”¨è€Œæ— éœ€ä¿®æ”¹ã€‚RATTPOé€šè¿‡æŸ¥è¯¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¿­ä»£æœç´¢ä¼˜åŒ–çš„æç¤ºï¼Œè€Œæ— éœ€ç‰¹å®šçš„å¥–åŠ±ä»»åŠ¡æè¿°ã€‚ç›¸åï¼Œå®ƒä½¿ç”¨ä¼˜åŒ–è½¨è¿¹å’Œä¸€ç§æ–°é¢–çš„å¥–åŠ±æ„ŸçŸ¥åé¦ˆä¿¡å·ï¼ˆç§°ä¸ºâ€œçº¿ç´¢â€ï¼‰ä½œä¸ºä¸Šä¸‹æ–‡ã€‚ç»éªŒç»“æœè¡¨æ˜RATTPOçš„é€šç”¨æ€§ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°å¢å¼ºç”¨æˆ·æç¤ºï¼Œæ¶µç›–è¯„ä¼°å„ç§ç”Ÿæˆæ–¹é¢çš„ä¸åŒå¥–åŠ±è®¾ç½®ï¼Œå¦‚ç¾å­¦ã€ä¸€èˆ¬äººç±»åå¥½æˆ–å¯¹è±¡ä¹‹é—´çš„ç©ºé—´å…³ç³»ã€‚RATTPOåœ¨æœç´¢æ•ˆç‡ä¸Šè¶…è¶Šäº†å…¶ä»–æµ‹è¯•æ—¶é—´æœç´¢åŸºå‡†æµ‹è¯•ï¼Œä½¿ç”¨é«˜è¾¾3.5å€çš„æ¨ç†é¢„ç®—æ›´å°‘ï¼Œå¹¶ä¸”åœ¨ç»™å®šè¶³å¤Ÿçš„æ¨ç†é¢„ç®—çš„æƒ…å†µä¸‹ï¼Œå®ƒçš„æ€§èƒ½ä¸éœ€è¦é’ˆå¯¹å¥–åŠ±è¿›è¡Œç‰¹å®šå¾®è°ƒçš„å­¦ä¹ åŸºå‡†ç›¸å½“ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/seminkim/RATTPO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/seminkim/RATTPOæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16853v1">PDF</a> 28 pages, Under review</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¯»æ‰¾æœ€å¤§åŒ–æµ‹è¯•æ—¶æŒ‡å®šçš„å¥–åŠ±å‡½æ•°çš„æç¤ºæ¥æ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„ç”¨æˆ·æç¤ºã€‚ä¸ºè§£å†³ç°æœ‰è‡ªåŠ¨åŒ–æç¤ºå·¥ç¨‹æ–¹æ³•å¯¹æ–°å¥–åŠ±æ¨¡å‹çš„ä¸é€‚åº”é—®é¢˜ï¼Œæå‡ºä¸€ç§é€šç”¨æ–¹æ³•RATTPOï¼ˆå¥–åŠ±æ— å…³çš„æµ‹è¯•æ—¶é—´æç¤ºä¼˜åŒ–ï¼‰ã€‚RATTPOå¯ä»¥åœ¨ä¸åŒå¥–åŠ±åœºæ™¯ä¸‹çµæ´»åº”ç”¨ï¼Œæ— éœ€è¿›è¡Œä¿®æ”¹ã€‚å®ƒå¯ä»¥é€šè¿‡æŸ¥è¯¢å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¼˜åŒ–çš„æç¤ºæœç´¢ï¼Œä½¿ç”¨ä¼˜åŒ–è½¨è¿¹å’Œä¸€ç§æ–°é¢–çš„å¥–åŠ±æ„ŸçŸ¥åé¦ˆä¿¡å·ï¼ˆç§°ä¸ºâ€œçº¿ç´¢â€ï¼‰ä½œä¸ºä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRATTPOåœ¨å„ç§å¥–åŠ±è®¾ç½®ä¸‹å¯æœ‰æ•ˆæé«˜ç”¨æˆ·æç¤ºæ•ˆæœï¼Œå¹¶è¶…è¶Šäº†å…¶ä»–æµ‹è¯•æ—¶é—´æœç´¢åŸºçº¿ï¼Œæé«˜äº†æœç´¢æ•ˆç‡ï¼Œå‡å°‘äº†é«˜è¾¾3.5å€çš„æ¨ç†é¢„ç®—ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ç”¨æˆ·æç¤ºçš„é€šç”¨æ–¹æ³•RATTPOã€‚</li>
<li>RATTPOè§£å†³äº†ç°æœ‰è‡ªåŠ¨åŒ–æç¤ºå·¥ç¨‹æ–¹æ³•å¯¹æ–°å¥–åŠ±æ¨¡å‹çš„ä¸é€‚åº”é—®é¢˜ã€‚</li>
<li>RATTPOå¯åœ¨ä¸åŒå¥–åŠ±åœºæ™¯ä¸‹çµæ´»åº”ç”¨ï¼Œæ— éœ€ä¿®æ”¹ã€‚</li>
<li>RATTPOé€šè¿‡æŸ¥è¯¢å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¼˜åŒ–çš„æç¤ºæœç´¢ã€‚</li>
<li>RATTPOä½¿ç”¨ä¼˜åŒ–è½¨è¿¹å’Œå¥–åŠ±æ„ŸçŸ¥åé¦ˆä¿¡å·ï¼ˆç§°ä¸ºâ€œçº¿ç´¢â€ï¼‰ä½œä¸ºä¸Šä¸‹æ–‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒRATTPOåœ¨å„ç§å¥–åŠ±è®¾ç½®ä¸‹å¯æœ‰æ•ˆæé«˜ç”¨æˆ·æç¤ºæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16853">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c753a371aee931ac0e062ce7117b103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c89706d6721814abad5a176fb6855f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1dcfd298f406be7b197ca56467cd0ea9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Noise-Informed-Diffusion-Generated-Image-Detection-with-Anomaly-Attention"><a href="#Noise-Informed-Diffusion-Generated-Image-Detection-with-Anomaly-Attention" class="headerlink" title="Noise-Informed Diffusion-Generated Image Detection with Anomaly   Attention"></a>Noise-Informed Diffusion-Generated Image Detection with Anomaly   Attention</h2><p><strong>Authors:Weinan Guan, Wei Wang, Bo Peng, Ziwen He, Jing Dong, Haonan Cheng</strong></p>
<p>With the rapid development of image generation technologies, especially the advancement of Diffusion Models, the quality of synthesized images has significantly improved, raising concerns among researchers about information security. To mitigate the malicious abuse of diffusion models, diffusion-generated image detection has proven to be an effective countermeasure.However, a key challenge for forgery detection is generalising to diffusion models not seen during training. In this paper, we address this problem by focusing on image noise. We observe that images from different diffusion models share similar noise patterns, distinct from genuine images. Building upon this insight, we introduce a novel Noise-Aware Self-Attention (NASA) module that focuses on noise regions to capture anomalous patterns. To implement a SOTA detection model, we incorporate NASA into Swin Transformer, forming an novel detection architecture NASA-Swin. Additionally, we employ a cross-modality fusion embedding to combine RGB and noise images, along with a channel mask strategy to enhance feature learning from both modalities. Extensive experiments demonstrate the effectiveness of our approach in enhancing detection capabilities for diffusion-generated images. When encountering unseen generation methods, our approach achieves the state-of-the-art performance.Our code is available at <a target="_blank" rel="noopener" href="https://github.com/WeinanGuan/NASA-Swin">https://github.com/WeinanGuan/NASA-Swin</a>. </p>
<blockquote>
<p>éšç€å›¾åƒç”ŸæˆæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰çš„è¿›æ­¥ï¼Œåˆæˆå›¾åƒçš„è´¨é‡å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œè¿™å¼•èµ·äº†ç ”ç©¶è€…å¯¹ä¿¡æ¯å®‰å…¨çš„å…³æ³¨ã€‚ä¸ºäº†å‡å°‘æ‰©æ•£æ¨¡å‹çš„æ¶æ„æ»¥ç”¨ï¼Œæ‰©æ•£ç”Ÿæˆå›¾åƒæ£€æµ‹å·²è¢«è¯æ˜æ˜¯ä¸€ç§æœ‰æ•ˆçš„å¯¹ç­–ã€‚ç„¶è€Œï¼Œæ£€æµ‹ä¼ªé€ å›¾åƒçš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯å¦‚ä½•å¯¹è®­ç»ƒæ—¶æœªè§åˆ°çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œé€šç”¨åŒ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å…³æ³¨å›¾åƒå™ªå£°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ¥è‡ªä¸åŒæ‰©æ•£æ¨¡å‹çš„å›¾åƒå…·æœ‰ç›¸ä¼¼çš„å™ªå£°æ¨¡å¼ï¼Œè¿™ä¸çœŸå®å›¾åƒæˆªç„¶ä¸åŒã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹å™ªå£°æ„ŸçŸ¥è‡ªæ³¨æ„åŠ›ï¼ˆNASAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—ä¸“æ³¨äºå™ªå£°åŒºåŸŸä»¥æ•è·å¼‚å¸¸æ¨¡å¼ã€‚ä¸ºäº†å®ç°å…ˆè¿›çš„æ£€æµ‹æ¨¡å‹ï¼Œæˆ‘ä»¬å°†NASAèå…¥Swin Transformerï¼Œå½¢æˆäº†æ–°å‹çš„æ£€æµ‹æ¶æ„NASA-Swinã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨è·¨æ¨¡æ€èåˆåµŒå…¥æ–¹æ³•ï¼Œç»“åˆRGBå›¾åƒå’Œå™ªå£°å›¾åƒï¼Œä»¥åŠé€šé“æ©ç ç­–ç•¥ï¼Œä»¥å¢å¼ºä¸¤ç§æ¨¡æ€çš„ç‰¹å¾å­¦ä¹ ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¢å¼ºæ‰©æ•£ç”Ÿæˆå›¾åƒçš„æ£€æµ‹èƒ½åŠ›æ–¹é¢éå¸¸æœ‰æ•ˆã€‚å½“é‡åˆ°æœªè§è¿‡çš„ç”Ÿæˆæ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/WeinanGuan/NASA-Swin%E8%8E%B7%E5%8F%96%E3%80%82]">https://github.com/WeinanGuan/NASA-Swinè·å–ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16743v1">PDF</a> Accepted by TIFS 2025. Our code is availabel at   <a target="_blank" rel="noopener" href="https://github.com/WeinanGuan/NASA-Swin">https://github.com/WeinanGuan/NASA-Swin</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒæ£€æµ‹å·²æˆä¸ºåº”å¯¹æ¶æ„æ»¥ç”¨æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆå¯¹ç­–ã€‚æœ¬æ–‡å…³æ³¨å›¾åƒå™ªå£°ï¼Œå‘ç°ä¸åŒæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒå…·æœ‰ç›¸ä¼¼çš„å™ªå£°æ¨¡å¼ï¼Œä¸çœŸå®å›¾åƒä¸åŒã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°å‹çš„å™ªå£°æ„ŸçŸ¥è‡ªæ³¨æ„åŠ›ï¼ˆNASAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—ä¸“æ³¨äºå™ªå£°åŒºåŸŸä»¥æ•è·å¼‚å¸¸æ¨¡å¼ã€‚é€šè¿‡ç»“åˆNASAå’ŒSwin Transformerï¼Œæˆ‘ä»¬æ„å»ºäº†å…ˆè¿›çš„æ£€æµ‹æ¶æ„NASA-Swinã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æé«˜æ£€æµ‹æ‰©æ•£ç”Ÿæˆå›¾åƒçš„èƒ½åŠ›æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œå¹¶åœ¨é‡åˆ°æœªè§è¿‡çš„ç”Ÿæˆæ–¹æ³•æ—¶è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒè´¨é‡æé«˜å¼•å‘äº†ä¿¡æ¯å®‰å…¨çš„å…³æ³¨ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒå…·æœ‰ç‰¹å®šçš„å™ªå£°æ¨¡å¼ï¼Œä¸çœŸå®å›¾åƒä¸åŒã€‚</li>
<li>å™ªå£°æ„ŸçŸ¥è‡ªæ³¨æ„åŠ›ï¼ˆNASAï¼‰æ¨¡å—è¢«å¼•å…¥ï¼Œä¸“æ³¨äºå™ªå£°åŒºåŸŸä»¥æ£€æµ‹å¼‚å¸¸æ¨¡å¼ã€‚</li>
<li>ç»“åˆNASAå’ŒSwin Transformeræ„å»ºäº†å…ˆè¿›çš„æ£€æµ‹æ¶æ„NASA-Swinã€‚</li>
<li>é‡‡ç”¨è·¨æ¨¡æ€èåˆåµŒå…¥æŠ€æœ¯ï¼Œç»“åˆRGBå’Œå™ªå£°å›¾åƒã€‚</li>
<li>é€šé“æ©ç ç­–ç•¥è¢«ç”¨æ¥å¢å¼ºä¸¤ç§æ¨¡æ€çš„ç‰¹å¾å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16743">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ebb30fef72f4bcc71c5e2a5bb0753f7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0dc20c9fe5809990e5b3098983fc5367.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-223a178fbaee21460d03afc05faa333d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7b791017fed050e2c95b86f02cc4f76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d02c8dfd15feb21ca128ea0ae8aecd9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9003d4323a5383a8639e2f35f70791e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d34ae6a6a116e0fe717f25d4e9a1be3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Prior-Guided-Joint-Diffusion-Model-in-Projection-Domain-for-PET-Tracer-Conversion"><a href="#A-Prior-Guided-Joint-Diffusion-Model-in-Projection-Domain-for-PET-Tracer-Conversion" class="headerlink" title="A Prior-Guided Joint Diffusion Model in Projection Domain for PET Tracer   Conversion"></a>A Prior-Guided Joint Diffusion Model in Projection Domain for PET Tracer   Conversion</h2><p><strong>Authors:Fang Chen, Weifeng Zhang, Xingyu Ai, BingXuan Li, An Li, Qiegen Liu</strong></p>
<p>Positron emission tomography (PET) is widely used to assess metabolic activity, but its application is limited by the availability of radiotracers. 18F-labeled fluorodeoxyglucose (18F-FDG) is the most commonly used tracer but shows limited effectiveness for certain tumors. In contrast, 6-18F-fluoro-3,4-dihydroxy-L-phenylalanine (18F-DOPA) offers higher specificity for neuroendocrine tumors and neurological disorders. However, its complex synthesis and limitations in transportation and clinical use hinder widespread adoption. During PET imaging, the sinogram represents a form of raw data acquired by the scanner. Therefore, modeling in projection domain enables more direct utilization of the original information, potentially reducing the accumulation of errors introduced during the image reconstruction process. Inspired by these factors, this study proposes a prior-guided joint diffusion model (PJDM) for transforming 18F-FDG PET images into 18F-DOPA PET images in projection domain. Specifically, a coarse estimation model and a prior refinement model are trained independently. During inference, an initial synthetic 18F-DOPA PET sinogram is generated using a higher-order hybrid sampler. This sinogram is then degraded and serves as an additional condition to guide the iterative refinement process using learned prior. Experimental results demonstrated that PJDM effectively improved both sinogram quality and synthetic outcomes. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/yqx7150/PJDM">https://github.com/yqx7150/PJDM</a>. </p>
<blockquote>
<p>æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å¹¿æ³›åº”ç”¨äºè¯„ä¼°ä»£è°¢æ´»æ€§ï¼Œä½†å…¶åº”ç”¨å—åˆ°æ”¾å°„æ€§ç¤ºè¸ªå‰‚å¯ç”¨æ€§çš„é™åˆ¶ã€‚Â¹â¸Fæ ‡è®°çš„æ°Ÿè„±æ°§è‘¡è„ç³–ï¼ˆÂ¹â¸F-FDGï¼‰æ˜¯æœ€å¸¸ç”¨çš„ç¤ºè¸ªå‰‚ï¼Œä½†å¯¹æŸäº›è‚¿ç˜¤çš„æœ‰æ•ˆæ€§æœ‰é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ6-Â¹â¸F-æ°Ÿ-3,4-äºŒç¾ŸåŸº-L-è‹¯ä¸™æ°¨é…¸ï¼ˆÂ¹â¸F-DOPAï¼‰å¯¹ç¥ç»å†…åˆ†æ³Œè‚¿ç˜¤å’Œç¥ç»éšœç¢å…·æœ‰æ›´é«˜çš„ç‰¹å¼‚æ€§ã€‚ç„¶è€Œï¼Œå…¶å¤æ‚çš„åˆæˆä»¥åŠè¿è¾“å’Œä¸´åºŠä½¿ç”¨çš„é™åˆ¶é˜»ç¢äº†å…¶å¹¿æ³›é‡‡ç”¨ã€‚åœ¨PETæˆåƒä¸­ï¼Œè¾›æ ¼å›¾ï¼ˆsinogramï¼‰ä»£è¡¨ç”±æ‰«æä»ªè·å¾—çš„ä¸€ç§åŸå§‹æ•°æ®å½¢å¼ã€‚å› æ­¤ï¼ŒæŠ•å½±åŸŸå»ºæ¨¡èƒ½å¤Ÿæ›´ç›´æ¥åœ°åˆ©ç”¨åŸå§‹ä¿¡æ¯ï¼Œå¯èƒ½å‡å°‘å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­å¼•å…¥çš„è¯¯å·®ç§¯ç´¯ã€‚å—è¿™äº›å› ç´ å¯å‘ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å…ˆéªŒå¼•å¯¼è”åˆæ‰©æ•£æ¨¡å‹ï¼ˆPJDMï¼‰ï¼Œç”¨äºåœ¨æŠ•å½±åŸŸä¸­å°†Â¹â¸F-FDG PETå›¾åƒè½¬æ¢ä¸ºÂ¹â¸F-DOPA PETå›¾åƒã€‚å…·ä½“è€Œè¨€ï¼Œè®­ç»ƒäº†ä¸€ä¸ªç²—ç•¥ä¼°è®¡æ¨¡å‹å’Œå…ˆéªŒä¼˜åŒ–æ¨¡å‹ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨é«˜é˜¶æ··åˆé‡‡æ ·å™¨ç”Ÿæˆåˆå§‹åˆæˆÂ¹â¸F-DOPA PETè¾›æ ¼å›¾ï¼Œç„¶åå¯¹å…¶è¿›è¡Œé€€åŒ–å¤„ç†ï¼Œä½œä¸ºè¿­ä»£ä¼˜åŒ–è¿‡ç¨‹çš„é™„åŠ æ¡ä»¶ï¼Œä»¥å¼•å¯¼å­¦ä¹ å…ˆéªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPJDMæœ‰æ•ˆæé«˜äº†è¾›æ ¼å›¾çš„è´¨é‡å’Œåˆæˆæ•ˆæœã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/yqx7150/PJDM">https://github.com/yqx7150/PJDM</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16733v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå…ˆéªŒå¼•å¯¼çš„è”åˆæ‰©æ•£æ¨¡å‹ï¼ˆPJDMï¼‰ï¼Œç”¨äºåœ¨æŠ•å½±åŸŸå°†18F-FDG PETå›¾åƒè½¬æ¢ä¸º18F-DOPA PETå›¾åƒã€‚è¯¥æ¨¡å‹é€šè¿‡è®­ç»ƒç²—ç•¥ä¼°ç®—æ¨¡å‹å’Œå…ˆéªŒä¼˜åŒ–æ¨¡å‹ï¼Œç”Ÿæˆåˆå§‹çš„18F-DOPA PETæ­£å¼¦å›¾ï¼Œå¹¶å¯¹å…¶è¿›è¡Œé€€åŒ–å¤„ç†ï¼Œä½œä¸ºå¼•å¯¼è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹çš„æ¡ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPJDMèƒ½æœ‰æ•ˆæé«˜æ­£å¼¦å›¾è´¨é‡å’Œåˆæˆæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PETåœ¨ä»£è°¢æ´»æ€§è¯„ä¼°ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†å—é™äºæ”¾å°„ç¤ºè¸ªå‰‚çš„å¯è·å¾—æ€§ã€‚</li>
<li>18F-FDGæ˜¯æœ€å¸¸ç”¨çš„ç¤ºè¸ªå‰‚ï¼Œä½†å¯¹æŸäº›è‚¿ç˜¤çš„æ•ˆç”¨æœ‰é™ã€‚</li>
<li>18F-DOPAä¸ºç¥ç»å†…åˆ†æ³Œè‚¿ç˜¤å’Œç¥ç»ç–¾ç—…æä¾›äº†æ›´é«˜çš„ç‰¹å¼‚æ€§ï¼Œä½†å…¶åˆæˆå¤æ‚ä¸”ä¸´åºŠåº”ç”¨å—é™ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†åŸºäºå…ˆéªŒå¼•å¯¼çš„è”åˆæ‰©æ•£æ¨¡å‹ï¼ˆPJDMï¼‰è¿›è¡Œå›¾åƒè½¬æ¢ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡è®­ç»ƒç²—ç•¥ä¼°ç®—æ¨¡å‹å’Œå…ˆéªŒä¼˜åŒ–æ¨¡å‹æ¥ç”Ÿæˆåˆå§‹çš„18F-DOPA PETæ­£å¼¦å›¾ã€‚</li>
<li>åˆå§‹åˆæˆçš„æ­£å¼¦å›¾ç»è¿‡é€€åŒ–å¤„ç†ï¼Œç”¨ä½œè¿­ä»£ä¼˜åŒ–è¿‡ç¨‹çš„å¼•å¯¼æ¡ä»¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd4ad6f5263fcfae5ff611831070f3af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f467edd818b7e69cd92668d0b5e6e1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5eb8065a429e026c7e409ba6e376fdbe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9db4b59dce8adc38173c6de63f6642f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-696b7d61e16c7c55d077c850bef9e735.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b2edd5814bd470f7dd08d8d3d41ea60.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DiffO-Single-step-Diffusion-for-Image-Compression-at-Ultra-Low-Bitrates"><a href="#DiffO-Single-step-Diffusion-for-Image-Compression-at-Ultra-Low-Bitrates" class="headerlink" title="DiffO: Single-step Diffusion for Image Compression at Ultra-Low Bitrates"></a>DiffO: Single-step Diffusion for Image Compression at Ultra-Low Bitrates</h2><p><strong>Authors:Chanung Park, Joo Chan Lee, Jong Hwan Ko</strong></p>
<p>Although image compression is fundamental to visual data processing and has inspired numerous standard and learned codecs, these methods still suffer severe quality degradation at extremely low bits per pixel. While recent diffusion based models provided enhanced generative performance at low bitrates, they still yields limited perceptual quality and prohibitive decoding latency due to multiple denoising steps. In this paper, we propose the first single step diffusion model for image compression (DiffO) that delivers high perceptual quality and fast decoding at ultra low bitrates. DiffO achieves these goals by coupling two key innovations: (i) VQ Residual training, which factorizes a structural base code and a learned residual in latent space, capturing both global geometry and high frequency details; and (ii) rate adaptive noise modulation, which tunes denoising strength on the fly to match the desired bitrate. Extensive experiments show that DiffO surpasses state of the art compression performance while improving decoding speed by about 50x compared to prior diffusion-based methods, greatly improving the practicality of generative codecs. The code will be available at <a target="_blank" rel="noopener" href="https://github.com/Freemasti/DiffO">https://github.com/Freemasti/DiffO</a>. </p>
<blockquote>
<p>å°½ç®¡å›¾åƒå‹ç¼©æ˜¯è§†è§‰æ•°æ®å¤„ç†çš„åŸºç¡€ï¼Œå¹¶æ¿€å‘äº†è®¸å¤šæ ‡å‡†å’Œå­¦ä¹ çš„ç¼–ç æŠ€æœ¯ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨æä½æ¯”ç‰¹ç‡ä¸‹ä»é¢ä¸´ä¸¥é‡çš„è´¨é‡ä¸‹é™é—®é¢˜ã€‚è™½ç„¶æœ€è¿‘çš„æ‰©æ•£æ¨¡å‹åœ¨ä½æ¯”ç‰¹ç‡ä¸‹æä¾›äº†å¢å¼ºçš„ç”Ÿæˆæ€§èƒ½ï¼Œä½†å®ƒä»¬ä»ç„¶å› å¤šæ­¥å»å™ªè€Œå…·æœ‰æœ‰é™çš„æ„ŸçŸ¥è´¨é‡å’Œè¾ƒé«˜çš„è§£ç å»¶è¿Ÿã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªç”¨äºå›¾åƒå‹ç¼©çš„å•æ­¥æ‰©æ•£æ¨¡å‹ï¼ˆDiffOï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹æä¾›é«˜è´¨é‡çš„æ„ŸçŸ¥å’Œå¿«é€Ÿè§£ç ã€‚DiffOé€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°å®ç°äº†è¿™äº›ç›®æ ‡ï¼šï¼ˆiï¼‰VQæ®‹å·®è®­ç»ƒï¼Œå®ƒåœ¨æ½œåœ¨ç©ºé—´ä¸­åˆ†è§£äº†ä¸€ä¸ªç»“æ„åŸºç¡€ç å’Œä¸€ä¸ªå­¦ä¹ åˆ°çš„æ®‹å·®ï¼ŒåŒæ—¶æ•æ‰å…¨å±€å‡ ä½•ä¿¡æ¯å’Œé«˜é¢‘ç»†èŠ‚ï¼›ï¼ˆiiï¼‰é€Ÿç‡è‡ªé€‚åº”å™ªå£°è°ƒåˆ¶ï¼Œå®ƒå³æ—¶è°ƒæ•´å»å™ªå¼ºåº¦ä»¥åŒ¹é…æ‰€éœ€çš„æ¯”ç‰¹ç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDiffOçš„å‹ç¼©æ€§èƒ½è¶…è¿‡äº†ç°æœ‰æŠ€æœ¯ï¼Œä¸å…ˆå‰çš„æ‰©æ•£æ–¹æ³•ç›¸æ¯”ï¼Œè§£ç é€Ÿåº¦æé«˜äº†çº¦50å€ï¼Œå¤§å¤§æé«˜äº†ç”Ÿæˆç¼–ç æŠ€æœ¯çš„å®ç”¨æ€§ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Freemasti/DiffO%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Freemasti/DiffOä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16572v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å•æ­¥å›¾åƒå‹ç¼©æ–¹æ³•ï¼ˆDiffOï¼‰ï¼Œå¯åœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹å®ç°é«˜æ„ŸçŸ¥è´¨é‡å’Œå¿«é€Ÿè§£ç ã€‚DiffOé€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°å®ç°ç›®æ ‡ï¼šä¸€æ˜¯VQæ®‹å·®è®­ç»ƒï¼Œå®ƒåˆ†è§£äº†ä¸€ä¸ªç»“æ„åŸºç¡€ç å’Œæ½œåœ¨ç©ºé—´ä¸­çš„å­¦ä¹ æ®‹å·®ï¼Œæ•æ‰å…¨å±€å‡ ä½•ç»“æ„å’Œé«˜é¢‘ç»†èŠ‚ï¼›äºŒæ˜¯é€Ÿç‡è‡ªé€‚åº”å™ªå£°è°ƒåˆ¶ï¼Œå®ƒå¯å®æ—¶è°ƒæ•´å»å™ªå¼ºåº¦ï¼Œä»¥é€‚åº”æ‰€éœ€çš„æ¯”ç‰¹ç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒDiffOåœ¨å‹ç¼©æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œå¹¶å°†è§£ç é€Ÿåº¦æé«˜äº†çº¦50å€ï¼Œå¤§å¤§æé«˜äº†ç”Ÿæˆç¼–ç å™¨çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„å•æ­¥å›¾åƒå‹ç¼©æ–¹æ³•ï¼ˆDiffOï¼‰ã€‚</li>
<li>DiffOèƒ½åœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹å·¥ä½œï¼Œå®ç°é«˜æ„ŸçŸ¥è´¨é‡å’Œå¿«é€Ÿè§£ç ã€‚</li>
<li>VQæ®‹å·®è®­ç»ƒç”¨äºåˆ†è§£ç»“æ„åŸºç¡€ç å’Œå­¦ä¹ æ®‹å·®ï¼Œæ•æ‰å…¨å±€å‡ ä½•å’Œé«˜é¢‘ç»†èŠ‚ã€‚</li>
<li>é€Ÿç‡è‡ªé€‚åº”å™ªå£°è°ƒåˆ¶å¯å®æ—¶è°ƒæ•´å»å™ªå¼ºåº¦ï¼Œé€‚åº”æ‰€éœ€æ¯”ç‰¹ç‡ã€‚</li>
<li>DiffOåœ¨å‹ç¼©æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
<li>ä¸å…ˆå‰çš„æ‰©æ•£æ–¹æ³•ç›¸æ¯”ï¼ŒDiffOå°†è§£ç é€Ÿåº¦æé«˜äº†çº¦50å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16572">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c31e8814ecb4128e9dc3335ff6da696a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-daa6f7b5db0e5405c2bb37152b703f9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc49398c619863d219623bd5279de4f0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Hunyuan3D-2-5-Towards-High-Fidelity-3D-Assets-Generation-with-Ultimate-Details"><a href="#Hunyuan3D-2-5-Towards-High-Fidelity-3D-Assets-Generation-with-Ultimate-Details" class="headerlink" title="Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate   Details"></a>Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate   Details</h2><p><strong>Authors:Zeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang, Mingxin Yang, Shuhui Yang, Yifei Feng, Sheng Zhang, Xin Huang, Di Luo, Fan Yang, Fang Yang, Lifu Wang, Sicong Liu, Yixuan Tang, Yulin Cai, Zebin He, Tian Liu, Yuhong Liu, Jie Jiang,  Linus, Jingwei Huang, Chunchao Guo</strong></p>
<p>In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating high-fidelity and detailed textured 3D assets. Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D 2.0, while demonstrating substantial advancements in both shape and texture generation. In terms of shape generation, we introduce a new shape foundation model â€“ LATTICE, which is trained with scaled high-quality datasets, model-size, and compute. Our largest model reaches 10B parameters and generates sharp and detailed 3D shape with precise image-3D following while keeping mesh surface clean and smooth, significantly closing the gap between generated and handcrafted 3D shapes. In terms of texture generation, it is upgraded with phyiscal-based rendering (PBR) via a novel multi-view architecture extended from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D 2.5 significantly outperforms previous methods in both shape and end-to-end texture generation. </p>
<blockquote>
<p>åœ¨è¿™ä»½æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†äº¨æº3D 2.5ç‰ˆæœ¬ï¼Œè¿™æ˜¯ä¸€å¥—ç¨³å¥çš„3Dæ‰©æ•£æ¨¡å‹å¥—ä»¶ï¼Œæ—¨åœ¨ç”Ÿæˆé«˜ä¿çœŸå’Œå…·æœ‰çº¹ç†ç»†èŠ‚çš„3Dèµ„äº§ã€‚äº¨æº3D 2.5éµå¾ªå…¶å‰ç‰ˆäº¨æº3D 2.0çš„ä¸¤é˜¶æ®µæµç¨‹ï¼ŒåŒæ—¶åœ¨å½¢çŠ¶å’Œçº¹ç†ç”Ÿæˆæ–¹é¢å®ç°äº†é‡å¤§è¿›å±•ã€‚åœ¨å½¢çŠ¶ç”Ÿæˆæ–¹é¢ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„å½¢çŠ¶åŸºç¡€æ¨¡å‹â€”â€”ç‚¹é˜µæ¨¡å‹ï¼ˆLATTICEï¼‰ï¼Œå®ƒé‡‡ç”¨è§„æ¨¡åŒ–é«˜è´¨é‡æ•°æ®é›†ã€æ¨¡å‹å¤§å°å’Œè®¡ç®—èƒ½åŠ›è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„å¤§å‹æ¨¡å‹è¾¾åˆ°10äº¿ä¸ªå‚æ•°ï¼Œå¯ä»¥ç”Ÿæˆæ¸…æ™°ä¸”è¯¦ç»†çš„3Då½¢çŠ¶ï¼Œç²¾ç¡®è·Ÿéšå›¾åƒ-3Dï¼ŒåŒæ—¶ä¿æŒç½‘æ ¼è¡¨é¢æ¸…æ´å…‰æ»‘ï¼Œæ˜¾è‘—ç¼©å°äº†ç”Ÿæˆçš„å’Œæ‰‹å·¥åˆ¶ä½œçš„3Då½¢çŠ¶ä¹‹é—´çš„å·®è·ã€‚åœ¨çº¹ç†ç”Ÿæˆæ–¹é¢ï¼Œå®ƒé€šè¿‡ä»äº¨æº3D 2.0çš„ç”»ç¬”æ¨¡å‹æ‰©å±•å‡ºæ¥çš„æ–°å‹å¤šè§†å›¾æ¶æ„å‡çº§ä¸ºåŸºäºç‰©ç†çš„æ¸²æŸ“ï¼ˆPBRï¼‰ã€‚æˆ‘ä»¬çš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºï¼Œäº¨æº3D 2.5åœ¨å½¢çŠ¶å’Œç«¯åˆ°ç«¯çº¹ç†ç”Ÿæˆæ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16504v1">PDF</a> Technical report</p>
<p><strong>Summary</strong></p>
<p>èƒ¡å›­ä¸‰ç»´æŠ€æœ¯å›¢é˜Ÿæ¨å‡ºçš„æ–°ç‰ˆè½¯ä»¶Hunyuan3D 2.5é‡‡ç”¨äº†å‡çº§ç‰ˆçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ä½“ç³»ã€‚ä¸æ—§ç‰ˆæœ¬ç›¸æ¯”ï¼Œä¸ä»…ç»§ç»­ä¼˜åŒ–äº†æ•´ä½“çš„ä¸¤é˜¶æ®µå¤„ç†æµç¨‹ï¼Œè¿˜è¿›è¡Œäº†æ›´æ·±åº¦çš„ç³»ç»Ÿæå‡ä¸æ›´æ–°ï¼Œä½¿ä¸‰ç»´å½¢çŠ¶ç”Ÿæˆä¸çº¹ç†æ¸²æŸ“éƒ½å®ç°äº†æ›´é«˜çš„ç²¾åº¦ä¸æ¸…æ™°åº¦ã€‚èƒ¡å›­3D 2.5çš„å…¨æ–°æ¨¡å‹ç”Ÿæˆçš„ä¸‰ç»´å½¢çŠ¶å…·æœ‰æ›´æ¸…æ™°å’Œæ›´ç²¾ç»†çš„ç»†èŠ‚ï¼Œæ›´å‡†ç¡®çš„å›¾åƒåˆ°ä¸‰ç»´çš„æ˜ å°„æ•ˆæœï¼ŒåŒæ—¶ä¿æŒç½‘æ ¼è¡¨é¢æ¸…æ´å…‰æ»‘ã€‚æ­¤å¤–ï¼Œå…¶ç‰©ç†æ¸²æŸ“æŠ€æœ¯ä¹Ÿé€šè¿‡å…¨æ–°çš„å¤šè§†è§’æ¶æ„å¾—åˆ°äº†æå‡ï¼Œæ˜¾è‘—æå‡äº†çº¹ç†æ¸²æŸ“çš„è´¨é‡ã€‚æ•´ä½“è€Œè¨€ï¼Œèƒ¡å›­ä¸‰ç»´æŠ€æœ¯å›¢é˜Ÿçš„æ–°ç‰ˆè½¯ä»¶æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f6c7adb02f436f1238e3a95290df497.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27fcb870398c21c87bce76666499e021.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28d93f476179fcb1bc31ea54b7c9504a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c2aa49b67a33f667ccac91e522f4807.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78bf272524c10e0fdf36346c23cfa3e2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Category-based-Galaxy-Image-Generation-via-Diffusion-Models"><a href="#Category-based-Galaxy-Image-Generation-via-Diffusion-Models" class="headerlink" title="Category-based Galaxy Image Generation via Diffusion Models"></a>Category-based Galaxy Image Generation via Diffusion Models</h2><p><strong>Authors:Xingzhong Fan, Hongming Tang, Yue Zeng, M. B. N. Kouwenhoven, Guangquan Zeng</strong></p>
<p>Conventional galaxy generation methods rely on semi-analytical models and hydrodynamic simulations, which are highly dependent on physical assumptions and parameter tuning. In contrast, data-driven generative models do not have explicit physical parameters pre-determined, and instead learn them efficiently from observational data, making them alternative solutions to galaxy generation. Among these, diffusion models outperform Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) in quality and diversity. Leveraging physical prior knowledge to these models can further enhance their capabilities. In this work, we present GalCatDiff, the first framework in astronomy to leverage both galaxy image features and astrophysical properties in the network design of diffusion models. GalCatDiff incorporates an enhanced U-Net and a novel block entitled Astro-RAB (Residual Attention Block), which dynamically combines attention mechanisms with convolution operations to ensure global consistency and local feature fidelity. Moreover, GalCatDiff uses category embeddings for class-specific galaxy generation, avoiding the high computational costs of training separate models for each category. Our experimental results demonstrate that GalCatDiff significantly outperforms existing methods in terms of the consistency of sample color and size distributions, and the generated galaxies are both visually realistic and physically consistent. This framework will enhance the reliability of galaxy simulations and can potentially serve as a data augmentor to support future galaxy classification algorithm development. </p>
<blockquote>
<p>ä¼ ç»Ÿæ˜Ÿç³»ç”Ÿæˆæ–¹æ³•ä¾èµ–äºåŠè§£ææ¨¡å‹å’Œæ°´åŠ¨åŠ›å­¦æ¨¡æ‹Ÿï¼Œè¿™äº›æ–¹æ³•é«˜åº¦ä¾èµ–äºç‰©ç†å‡è®¾å’Œå‚æ•°è°ƒæ•´ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ•°æ®é©±åŠ¨çš„ç”Ÿæˆæ¨¡å‹æ²¡æœ‰é¢„å…ˆç¡®å®šçš„æ˜ç¡®ç‰©ç†å‚æ•°ï¼Œè€Œæ˜¯ä»è§‚æµ‹æ•°æ®ä¸­æœ‰æ•ˆåœ°å­¦ä¹ è¿™äº›å‚æ•°ï¼Œä½¿å…¶æˆä¸ºæ˜Ÿç³»ç”Ÿæˆçš„æ›¿ä»£è§£å†³æ–¹æ¡ˆã€‚åœ¨è¿™äº›æ¨¡å‹ä¸­ï¼Œæ‰©æ•£æ¨¡å‹åœ¨è´¨é‡å’Œå¤šæ ·æ€§æ–¹é¢ä¼˜äºå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEsï¼‰å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€‚åˆ©ç”¨ç‰©ç†å…ˆéªŒçŸ¥è¯†å¯ä»¥è¿›ä¸€æ­¥æé«˜è¿™äº›æ¨¡å‹çš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GalCatDiffï¼Œè¿™æ˜¯å¤©æ–‡å­¦ä¸­ç¬¬ä¸€ä¸ªåˆ©ç”¨æ˜Ÿç³»å›¾åƒç‰¹å¾å’Œå¤©æ–‡ç‰©ç†å±æ€§æ¥è®¾è®¡æ‰©æ•£æ¨¡å‹çš„ç½‘ç»œæ¡†æ¶ã€‚GalCatDiffé›†æˆäº†å¢å¼ºå‹U-Netå’Œä¸€ä¸ªåä¸ºAstro-RABï¼ˆæ®‹å·®æ³¨æ„åŠ›å—ï¼‰çš„æ–°é¢–æ¨¡å—ï¼Œå®ƒåŠ¨æ€åœ°å°†æ³¨æ„åŠ›æœºåˆ¶ä¸å·ç§¯æ“ä½œç›¸ç»“åˆï¼Œä»¥ç¡®ä¿å…¨å±€ä¸€è‡´æ€§å’Œå±€éƒ¨ç‰¹å¾ä¿çœŸåº¦ã€‚æ­¤å¤–ï¼ŒGalCatDiffä½¿ç”¨ç±»åˆ«åµŒå…¥è¿›è¡Œç‰¹å®šç±»åˆ«çš„æ˜Ÿç³»ç”Ÿæˆï¼Œé¿å…äº†ä¸ºæ¯ä¸ªç±»åˆ«è®­ç»ƒå•ç‹¬æ¨¡å‹çš„é«˜è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ ·æœ¬é¢œè‰²å’Œå¤§å°åˆ†å¸ƒçš„ä¸€è‡´æ€§æ–¹é¢ï¼ŒGalCatDiffæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”ç”Ÿæˆçš„æ˜Ÿç³»åœ¨è§†è§‰ä¸Šå¾ˆçœŸå®ä¸”ç‰©ç†ä¸Šä¸€è‡´ã€‚è¯¥æ¡†æ¶å°†æé«˜æ˜Ÿç³»æ¨¡æ‹Ÿçš„å¯é æ€§ï¼Œå¹¶å¯èƒ½ä½œä¸ºæ•°æ®å¢å¼ºå™¨æ”¯æŒæœªæ¥æ˜Ÿç³»åˆ†ç±»ç®—æ³•çš„å¼€å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16255v1">PDF</a> 18 pages, 6 figures. Submitted to AAS Astronomical Journal (AJ) and   is under revision. See another indenpdent work for furthur reference â€“ Can   AI Dream of Unseen Galaxies? Conditional Diffusion Model for Galaxy   Morphology Augmentation (Ma, Sun et al.). Comments are welcome</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ•°æ®é©±åŠ¨çš„ç”Ÿæˆæ¨¡å‹å·²æˆä¸ºä¸€ç§æ–°å…´çš„æ›¿ä»£æ–¹æ³•ç”¨äºæ˜Ÿç³»ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯å…¶ä¸­çš„æ‰©æ•£æ¨¡å‹ç›¸è¾ƒäºå…¶ä»–ç”Ÿæˆæ¨¡å‹æœ‰æ›´å¥½çš„æ€§èƒ½å’Œå¤šæ ·æ€§ã€‚æœ¬æ–‡å°†ç‰©ç†å…ˆéªŒçŸ¥è¯†åº”ç”¨äºæ‰©æ•£æ¨¡å‹çš„è®¾è®¡ï¼Œæ„å»ºäº†ä¸€ä¸ªåä¸ºGalCatDiffçš„æ–°æ¡†æ¶ï¼Œç»“åˆäº†æ˜Ÿç³»å›¾åƒç‰¹å¾å’Œå¤©ä½“ç‰©ç†å±æ€§ã€‚è¯¥æ¡†æ¶ä½¿ç”¨å¢å¼ºå‹U-Netå’Œåˆ›æ–°çš„Astro-RABæ¨¡å—è¿›è¡ŒåŠ¨æ€ç‰¹å¾å¤„ç†ï¼Œå¹¶ä¸”é€šè¿‡ä½¿ç”¨ç±»åˆ«åµŒå…¥å®ç°ç±»åˆ«ç‰¹å®šæ˜Ÿç³»ç”Ÿæˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGalCatDiffæ˜¾è‘—æé«˜äº†æ ·æœ¬é¢œè‰²ä¸å¤§å°åˆ†å¸ƒçš„ä¸€è‡´æ€§ï¼Œç”Ÿæˆçš„æ˜Ÿç³»æ—¢å…·æœ‰è§†è§‰çœŸå®æ€§åˆç¬¦åˆç‰©ç†ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®é©±åŠ¨ç”Ÿæˆæ¨¡å‹å·²æˆä¸ºæ˜Ÿç³»ç”Ÿæˆçš„æ–°å…´æ›¿ä»£æ–¹æ³•ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ€§èƒ½å’Œå¤šæ ·æ€§ä¸Šä¼˜äºå…¶ä»–ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>GalCatDiffç»“åˆäº†æ˜Ÿç³»å›¾åƒç‰¹å¾å’Œå¤©ä½“ç‰©ç†å±æ€§è¿›è¡Œæ‰©æ•£æ¨¡å‹è®¾è®¡ã€‚</li>
<li>GalCatDiffä½¿ç”¨å¢å¼ºå‹U-Netå’Œåˆ›æ–°æ¨¡å—Astro-RABè¿›è¡Œç‰¹å¾å¤„ç†ã€‚</li>
<li>GalCatDiffé€šè¿‡ç±»åˆ«åµŒå…¥å®ç°ç‰¹å®šç±»åˆ«çš„æ˜Ÿç³»ç”Ÿæˆï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºGalCatDiffæé«˜äº†æ ·æœ¬é¢œè‰²ä¸å¤§å°åˆ†å¸ƒçš„ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-744a6de829bd383d53f27b4a99f01e63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8419adba746c8deae43fcdf7dc3e73c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f52a1239fb0fb74a928875bea0369d5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Can-AI-Dream-of-Unseen-Galaxies-Conditional-Diffusion-Model-for-Galaxy-Morphology-Augmentation"><a href="#Can-AI-Dream-of-Unseen-Galaxies-Conditional-Diffusion-Model-for-Galaxy-Morphology-Augmentation" class="headerlink" title="Can AI Dream of Unseen Galaxies? Conditional Diffusion Model for Galaxy   Morphology Augmentation"></a>Can AI Dream of Unseen Galaxies? Conditional Diffusion Model for Galaxy   Morphology Augmentation</h2><p><strong>Authors:Chenrui Ma, Zechang Sun, Tao Jing, Zheng Cai, Yuan-Sen Ting, Song Huang, Mingyu Li</strong></p>
<p>Observational astronomy relies on visual feature identification to detect critical astrophysical phenomena. While machine learning (ML) increasingly automates this process, models often struggle with generalization in large-scale surveys due to the limited representativeness of labeled datasets â€“ whether from simulations or human annotation â€“ a challenge pronounced for rare yet scientifically valuable objects. To address this, we propose a conditional diffusion model to synthesize realistic galaxy images for augmenting ML training data. Leveraging the Galaxy Zoo 2 dataset which contains visual feature â€“ galaxy image pairs from volunteer annotation, we demonstrate that our model generates diverse, high-fidelity galaxy images closely adhere to the specified morphological feature conditions. Moreover, this model enables generative extrapolation to project well-annotated data into unseen domains and advancing rare object detection. Integrating synthesized images into ML pipelines improves performance in standard morphology classification, boosting completeness and purity by up to 30% across key metrics. For rare object detection, using early-type galaxies with prominent dust lane features ( $\sim$0.1% in GZ2 dataset) as a test case, our approach doubled the number of detected instances from 352 to 872, compared to previous studies based on visual inspection. This study highlights the power of generative models to bridge gaps between scarce labeled data and the vast, uncharted parameter space of observational astronomy and sheds insight for future astrophysical foundation model developments. Our project homepage is available at <a target="_blank" rel="noopener" href="https://galaxysd-webpage.streamlit.app/">https://galaxysd-webpage.streamlit.app/</a>. </p>
<blockquote>
<p>è§‚æµ‹å¤©æ–‡å­¦ä¾èµ–äºè§†è§‰ç‰¹å¾è¯†åˆ«æ¥æ£€æµ‹å…³é”®çš„å¤©ä½“ç‰©ç†ç°è±¡ã€‚è™½ç„¶æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰è¶Šæ¥è¶Šè‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œä½†ç”±äºæ¨¡æ‹Ÿæˆ–äººå·¥æ ‡æ³¨çš„æ•°æ®é›†ä»£è¡¨æ€§æœ‰é™ï¼Œæ¨¡å‹åœ¨å¤§è§„æ¨¡è°ƒæŸ¥ä¸­çš„æ³›åŒ–èƒ½åŠ›å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜â€”â€”è¿™å¯¹äºç¨€å°‘ä½†å…·æœ‰ç§‘å­¦ä»·å€¼çš„ç‰©ä½“æ¥è¯´æ˜¯ä¸€ä¸ªæ›´å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåˆæˆé€¼çœŸçš„æ˜Ÿç³»å›¾åƒï¼Œä»¥æ‰©å……æœºå™¨å­¦ä¹ çš„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬åˆ©ç”¨Galaxy Zoo 2æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªå¿—æ„¿è€…æ ‡æ³¨çš„è§†è§‰ç‰¹å¾â€”â€”æ˜Ÿç³»å›¾åƒå¯¹ï¼Œæ¼”ç¤ºäº†æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ã€é«˜ä¿çœŸåº¦çš„æ˜Ÿç³»å›¾åƒï¼Œç´§å¯†ç¬¦åˆæŒ‡å®šçš„å½¢æ€ç‰¹å¾æ¡ä»¶ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œç”Ÿæˆå¤–æ¨ï¼Œå°†æ ‡æ³¨è‰¯å¥½çš„æ•°æ®æŠ•å½±åˆ°æœªè§è¿‡çš„é¢†åŸŸï¼Œå¹¶æ¨åŠ¨ç¨€æœ‰ç‰©ä½“çš„æ£€æµ‹ã€‚å°†åˆæˆå›¾åƒé›†æˆåˆ°æœºå™¨å­¦ä¹ ç®¡é“ä¸­ï¼Œæé«˜äº†æ ‡å‡†å½¢æ€åˆ†ç±»çš„æ€§èƒ½ï¼Œåœ¨å…³é”®æŒ‡æ ‡ä¸Šï¼Œå®Œæ€§å’Œçº¯åº¦æé«˜äº†é«˜è¾¾30%ã€‚åœ¨æ£€æµ‹ç¨€æœ‰ç‰©ä½“æ–¹é¢ï¼Œä»¥å…·æœ‰æ˜æ˜¾å°˜åŸƒé€šé“ç‰¹å¾çš„æ—©æœŸæ˜Ÿç³»ï¼ˆåœ¨GZ2æ•°æ®é›†ä¸­çº¦ä¸º0.1%ï¼‰ä¸ºè¯•éªŒæ¡ˆä¾‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†æ£€æµ‹åˆ°çš„å®ä¾‹æ•°é‡ä»352ä¸ªå¢åŠ åˆ°872ä¸ªï¼Œä¸ä¹‹å‰çš„åŸºäºäººå·¥æ£€æŸ¥çš„ç ”ç©¶ç›¸æ¯”ç¿»äº†ä¸€ç•ªã€‚è¯¥ç ”ç©¶çªæ˜¾äº†ç”Ÿæˆæ¨¡å‹åœ¨ç¨€ç¼ºæ ‡æ³¨æ•°æ®å’Œè§‚æµ‹å¤©æ–‡å­¦å¹¿é˜”çš„ã€æœªæ¢ç´¢çš„å‚æ•°ç©ºé—´ä¹‹é—´çš„æ¡¥æ¢ä½œç”¨ï¼Œå¹¶ä¸ºæœªæ¥çš„å¤©ä½“ç‰©ç†åŸºç¡€æ¨¡å‹å¼€å‘æä¾›äº†è§è§£ã€‚æˆ‘ä»¬çš„é¡¹ç›®ä¸»é¡µå¯åœ¨<a target="_blank" rel="noopener" href="https://galaxysd-webpage.streamlit.app/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://galaxysd-webpage.streamlit.app/ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16233v1">PDF</a> We have submitted to AAS journals. See another independent work for   further reference â€“ Category-based Galaxy Image Generation via Diffusion   Models (Fan, Tang et al.). Comments are welcome</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹åˆæˆé€¼çœŸçš„æ˜Ÿç³»å›¾åƒï¼Œä»¥æ‰©å……æœºå™¨å­¦ä¹ è®­ç»ƒæ•°æ®ã€‚ç ”ç©¶ä½¿ç”¨Galaxy Zoo 2æ•°æ®é›†ï¼Œå±•ç¤ºäº†æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æŒ‡å®šçš„å½¢æ€ç‰¹å¾æ¡ä»¶ç”Ÿæˆå¤šæ ·ã€é«˜ä¿çœŸåº¦çš„æ˜Ÿç³»å›¾åƒã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œç”Ÿæˆå¤–æ¨ï¼Œå°†å·²æ ‡æ³¨çš„æ•°æ®æŠ•å½±åˆ°æœªçŸ¥é¢†åŸŸï¼Œå¹¶æ¨åŠ¨ç¨€æœ‰ç›®æ ‡æ£€æµ‹çš„å‘å±•ã€‚é›†æˆåˆæˆå›¾åƒåˆ°æœºå™¨å­¦ä¹ ç®¡é“ä¸­ï¼Œæé«˜äº†æ ‡å‡†å½¢æ€åˆ†ç±»çš„æ€§èƒ½ï¼Œå¹¶åœ¨å…³é”®æŒ‡æ ‡ä¸Šæé«˜äº†å®Œæ•´æ€§å’Œçº¯åº¦è¾¾30%ã€‚å¯¹äºç½•è§ç›®æ ‡æ£€æµ‹ï¼Œä»¥å…·æœ‰çªå‡ºå°˜åŸƒé€šé“ç‰¹å¾çš„æ—©æœŸç±»å‹æ˜Ÿç³»ä¸ºä¾‹ï¼ˆåœ¨GZ2æ•°æ®é›†ä¸­çº¦å 0.1%ï¼‰ï¼Œè¯¥ç ”ç©¶å°†æ£€æµ‹åˆ°çš„å®ä¾‹æ•°é‡ä»ä¹‹å‰çš„è§†è§‰æ£€æŸ¥ä¸­çš„352ä¸ªå¢åŠ åˆ°872ä¸ªã€‚è¿™å‡¸æ˜¾äº†ç”Ÿæˆæ¨¡å‹åœ¨ç¼©å°ç¨€ç¼ºæ ‡è®°æ•°æ®ä¸å¹¿é˜”çš„æœªè¢«æ¢ç´¢çš„å‚æ•°ç©ºé—´ä¹‹é—´çš„å·®è·ä¸­çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥çš„å¤©ä½“ç‰©ç†åŸºç¡€æ¨¡å‹å‘å±•æä¾›äº†å¯ç¤ºã€‚é¡¹ç›®ä¸»é¡µä½äºï¼š[é“¾æ¥åœ°å€]ã€‚æ­¤æ‘˜è¦å…¨æ–‡ä¸è¶…è¿‡ç™¾å­—é™åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ˜Ÿç³»å›¾åƒåˆæˆæ–¹æ³•ï¼Œç”¨äºæ‰©å……æœºå™¨å­¦ä¹ è®­ç»ƒæ•°æ®ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æŒ‡å®šçš„å½¢æ€ç‰¹å¾æ¡ä»¶ç”Ÿæˆå¤šæ ·ã€é«˜ä¿çœŸåº¦çš„æ˜Ÿç³»å›¾åƒã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¤–æ¨ï¼Œå°†æ ‡æ³¨æ•°æ®æŠ•å½±åˆ°æœªçŸ¥é¢†åŸŸã€‚</li>
<li>é›†æˆåˆæˆå›¾åƒæé«˜äº†æœºå™¨å­¦ä¹ åœ¨æ ‡å‡†å½¢æ€åˆ†ç±»ä¸Šçš„æ€§èƒ½ï¼Œå¹¶æå‡äº†æ£€æµ‹ç»“æœçš„å®Œæ•´æ€§å’Œçº¯åº¦ã€‚</li>
<li>åœ¨ç½•è§ç›®æ ‡æ£€æµ‹æ–¹é¢ï¼Œæ¨¡å‹æ˜¾è‘—å¢åŠ äº†æ£€æµ‹åˆ°å…·æœ‰ç‰¹å®šç‰¹å¾çš„æ˜Ÿç³»æ•°é‡ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†ç”Ÿæˆæ¨¡å‹åœ¨è¿æ¥ç¨€ç¼ºæ•°æ®å’Œæœªè¢«æ¢ç´¢çš„å‚æ•°ç©ºé—´ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16233">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-edf1a5f9bd3ddbfac9e6576805d26eae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81a73dd89fb7661a02a8de1a64d81573.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cb5b3c01c387349dae46bd3341d2ffe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-272036f7eda7d90a1920ff231837d801.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LoRA-Edit-Controllable-First-Frame-Guided-Video-Editing-via-Mask-Aware-LoRA-Fine-Tuning"><a href="#LoRA-Edit-Controllable-First-Frame-Guided-Video-Editing-via-Mask-Aware-LoRA-Fine-Tuning" class="headerlink" title="LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware   LoRA Fine-Tuning"></a>LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware   LoRA Fine-Tuning</h2><p><strong>Authors:Chenjian Gao, Lihe Ding, Xin Cai, Zhanpeng Huang, Zibin Wang, Tianfan Xue</strong></p>
<p>Video editing using diffusion models has achieved remarkable results in generating high-quality edits for videos. However, current methods often rely on large-scale pretraining, limiting flexibility for specific edits. First-frame-guided editing provides control over the first frame, but lacks flexibility over subsequent frames. To address this, we propose a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models for flexible video editing. Our approach preserves background regions while enabling controllable edits propagation. This solution offers efficient and adaptable video editing without altering the model architecture. To better steer this process, we incorporate additional references, such as alternate viewpoints or representative scene states, which serve as visual anchors for how content should unfold. We address the control challenge using a mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model to the editing context. The model must learn from two distinct sources: the input video provides spatial structure and motion cues, while reference images offer appearance guidance. A spatial mask enables region-specific learning by dynamically modulating what the model attends to, ensuring that each area draws from the appropriate source. Experimental results show our method achieves superior video editing performance compared to state-of-the-art methods. Project Page: <a target="_blank" rel="noopener" href="https://cjeen.github.io/LoraEditPaper">https://cjeen.github.io/LoraEditPaper</a> </p>
<blockquote>
<p>ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†é¢‘ç¼–è¾‘å·²ç»åœ¨ä¸ºé«˜è´¨é‡è§†é¢‘ç”Ÿæˆç¼–è¾‘æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œè¿™é™åˆ¶äº†ç‰¹å®šç¼–è¾‘çš„çµæ´»æ€§ã€‚è™½ç„¶é¦–å¸§å¼•å¯¼ç¼–è¾‘å¯ä»¥æ§åˆ¶é¦–å¸§ï¼Œä½†å¯¹åç»­å¸§çš„æ§åˆ¶ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ©è†œçš„LoRAï¼ˆä½ç§©é€‚åº”ï¼‰è°ƒä¼˜æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯é€‚åº”é¢„è®­ç»ƒçš„å›¾åˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ¨¡å‹ï¼Œç”¨äºçµæ´»çš„è§†é¢‘ç¼–è¾‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä¿ç•™èƒŒæ™¯åŒºåŸŸï¼ŒåŒæ—¶å®ç°å¯æ§çš„ç¼–è¾‘ä¼ æ’­ã€‚è¿™ä¸€è§£å†³æ–¹æ¡ˆæä¾›äº†é«˜æ•ˆä¸”å¯é€‚åº”çš„è§†é¢‘ç¼–è¾‘ï¼Œè€Œä¸ä¼šæ”¹å˜æ¨¡å‹æ¶æ„ã€‚ä¸ºäº†æ›´å¥½åœ°å¼•å¯¼è¿™ä¸€è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢å¤–çš„å‚è€ƒï¼Œå¦‚ä¸åŒçš„è§‚ç‚¹æˆ–ä»£è¡¨æ€§çš„åœºæ™¯çŠ¶æ€ï¼Œå®ƒä»¬ä½œä¸ºå†…å®¹å¦‚ä½•å±•å¼€çš„å¯è§†é”šç‚¹ã€‚æˆ‘ä»¬é‡‡ç”¨æ©è†œé©±åŠ¨çš„LoRAè°ƒä¼˜ç­–ç•¥æ¥è§£å†³æ§åˆ¶æŒ‘æˆ˜ï¼Œè¯¥ç­–ç•¥ä½¿é¢„è®­ç»ƒçš„å›¾åˆ°è§†é¢‘æ¨¡å‹é€‚åº”ç¼–è¾‘ä¸Šä¸‹æ–‡ã€‚æ¨¡å‹å¿…é¡»ä»ä¸¤ä¸ªç‹¬ç‰¹çš„ä¿¡æ¯æºä¸­å­¦ä¹ ï¼šè¾“å…¥è§†é¢‘æä¾›ç©ºé—´ç»“æ„å’Œè¿åŠ¨çº¿ç´¢ï¼Œè€Œå‚è€ƒå›¾åƒæä¾›å¤–è§‚æŒ‡å¯¼ã€‚ç©ºé—´æ©è†œé€šè¿‡åŠ¨æ€è°ƒåˆ¶æ¨¡å‹æ‰€å…³æ³¨çš„ç‚¹ï¼Œå®ç°ç‰¹å®šåŒºåŸŸçš„å­¦ä¹ ï¼Œç¡®ä¿æ¯ä¸ªåŒºåŸŸéƒ½ä»é€‚å½“çš„æºä¸­æ±²å–ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨è§†é¢‘ç¼–è¾‘æ€§èƒ½æ–¹é¢è¡¨ç°æ›´ä¼˜è¶Šã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cjeen.github.io/LoraEditPaper">https://cjeen.github.io/LoraEditPaper</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10082v2">PDF</a> 12 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†é¢‘ç¼–è¾‘çš„æœ€æ–°ç ”ç©¶æˆæœã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•é¢„è®­ç»ƒè§„æ¨¡å¤§ã€ç‰¹å®šç¼–è¾‘çµæ´»æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºæ©è†œçš„LoRAï¼ˆä½ç§©é€‚åº”ï¼‰è°ƒä¼˜æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯é€‚åº”é¢„è®­ç»ƒå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ¨¡å‹ï¼Œå®ç°çµæ´»è§†é¢‘ç¼–è¾‘ã€‚è¯¥æ–¹æ³•å¯åœ¨ä¿ç•™èƒŒæ™¯åŒºåŸŸçš„åŒæ—¶ï¼Œå®ç°å¯æ§çš„ç¼–è¾‘ä¼ æ’­ï¼Œæä¾›é«˜æ•ˆã€å¯é€‚åº”çš„è§†é¢‘ç¼–è¾‘ï¼Œè€Œæ— éœ€æ”¹å˜æ¨¡å‹æ¶æ„ã€‚ä¸ºæ›´å¥½åœ°å¼•å¯¼è¿™ä¸€è¿‡ç¨‹ï¼Œå¼•å…¥äº†é¢å¤–çš„å‚è€ƒå›¾åƒï¼Œä½œä¸ºå†…å®¹å±•å¼€çš„è§†è§‰é”šç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç¼–è¾‘ä¸­è¡¨ç°å“è¶Šï¼Œèƒ½ç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ç¼–è¾‘ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¾èµ–å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œé™åˆ¶ç‰¹å®šç¼–è¾‘çš„çµæ´»æ€§ã€‚</li>
<li>æå‡ºåŸºäºæ©è†œçš„LoRAï¼ˆä½ç§©é€‚åº”ï¼‰è°ƒä¼˜æ–¹æ³•ï¼Œé€‚åº”é¢„è®­ç»ƒå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ¨¡å‹ï¼Œå®ç°çµæ´»è§†é¢‘ç¼–è¾‘ã€‚</li>
<li>æ–¹æ³•å¯ä¿ç•™èƒŒæ™¯åŒºåŸŸï¼Œå®ç°å¯æ§çš„ç¼–è¾‘ä¼ æ’­ã€‚</li>
<li>æä¾›é«˜æ•ˆã€å¯é€‚åº”çš„è§†é¢‘ç¼–è¾‘ï¼Œæ— éœ€æ”¹å˜æ¨¡å‹æ¶æ„ã€‚</li>
<li>å¼•å…¥é¢å¤–å‚è€ƒå›¾åƒä½œä¸ºè§†è§‰é”šç‚¹ï¼Œæ›´å¥½åœ°å¼•å¯¼ç¼–è¾‘è¿‡ç¨‹ã€‚</li>
<li>é‡‡ç”¨æ©è†œé©±åŠ¨çš„LoRAè°ƒä¼˜ç­–ç•¥ï¼Œä»è¾“å…¥è§†é¢‘å’Œå‚è€ƒå›¾åƒä¸­å­¦ä¹ ï¼Œå®ç°åŒºåŸŸç‰¹å®šçš„å­¦ä¹ ï¼Œç¡®ä¿ä»é€‚å½“æ¥æºè·å–ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-be18c38135b1c6192436ad8cd666f81e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53ab42514b300b74be0259d543e6d0e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d6fcf3b0ac9b7f7e8a2c2220138e3b9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CryoCCD-Conditional-Cycle-consistent-Diffusion-with-Biophysical-Modeling-for-Cryo-EM-Synthesis"><a href="#CryoCCD-Conditional-Cycle-consistent-Diffusion-with-Biophysical-Modeling-for-Cryo-EM-Synthesis" class="headerlink" title="CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical   Modeling for Cryo-EM Synthesis"></a>CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical   Modeling for Cryo-EM Synthesis</h2><p><strong>Authors:Runmin Jiang, Genpei Zhang, Yuntian Yang, Siqi Wu, Yuheng Zhang, Wanyue Feng, Yizhou Zhao, Xi Xiao, Xiao Wang, Tianyang Wang, Xingjian Li, Min Xu</strong></p>
<p>Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of macromolecules, but developing robust models for downstream analysis is hindered by the scarcity of high-quality annotated data. While synthetic data generation has emerged as a potential solution, existing methods often fail to capture both the structural diversity of biological specimens and the complex, spatially varying noise inherent in cryo-EM imaging. To overcome these limitations, we propose CryoCCD, a synthesis framework that integrates biophysical modeling with generative techniques. Specifically, CryoCCD produces multi-scale cryo-EM micrographs that reflect realistic biophysical variability through compositional heterogeneity, cellular context, and physics-informed imaging. To generate realistic noise, we employ a conditional diffusion model, enhanced by cycle consistency to preserve structural fidelity and mask-aware contrastive learning to capture spatially adaptive noise patterns. Extensive experiments show that CryoCCD generates structurally accurate micrographs and enhances performance in downstream tasks, outperforming state-of-the-art baselines in both particle picking and reconstruction. </p>
<blockquote>
<p>å†·å†»ç”µå­æ˜¾å¾®é•œï¼ˆcryo-EMï¼‰èƒ½å¤Ÿä¸ºå®è§‚åˆ†å­æä¾›æ¥è¿‘åŸå­åˆ†è¾¨ç‡çš„æˆåƒï¼Œä½†æ˜¯é«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºé˜»ç¢äº†ä¸‹æ¸¸åˆ†æç¨³å¥æ¨¡å‹çš„å‘å±•ã€‚è™½ç„¶åˆæˆæ•°æ®ç”Ÿæˆå·²ç»å‡ºç°ä¸ºæ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æ•æ‰åˆ°ç”Ÿç‰©æ ·æœ¬çš„ç»“æ„å¤šæ ·æ€§å’Œå†·å†»ç”µå­æ˜¾å¾®é•œæˆåƒä¸­å›ºæœ‰çš„å¤æ‚ã€ç©ºé—´å˜åŒ–çš„å™ªå£°ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†CryoCCDï¼Œä¸€ä¸ªå°†ç”Ÿç‰©ç‰©ç†å»ºæ¨¡ä¸ç”ŸæˆæŠ€æœ¯ç›¸ç»“åˆçš„åˆæˆæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒCryoCCDäº§ç”Ÿå¤šå°ºåº¦çš„å†·å†»ç”µå­æ˜¾å¾®é•œå¾®å›¾ï¼Œé€šè¿‡ç»„æˆå¼‚è´¨æ€§ã€ç»†èƒä¸Šä¸‹æ–‡å’Œç‰©ç†ä¿¡æ¯æˆåƒåæ˜ ç°å®ç”Ÿç‰©ç‰©ç†çš„å˜å¼‚æ€§ã€‚ä¸ºäº†ç”ŸæˆçœŸå®çš„å™ªå£°ï¼Œæˆ‘ä»¬é‡‡ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¾ªç¯ä¸€è‡´æ€§å¢å¼ºæ¥ä¿æŒç»“æ„ä¿çœŸåº¦ï¼Œå¹¶é€šè¿‡æ©è†œæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ æ¥æ•æ‰ç©ºé—´è‡ªé€‚åº”å™ªå£°æ¨¡å¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCryoCCDç”Ÿæˆçš„å¾®å›¾ç»“æ„å‡†ç¡®ï¼Œæé«˜äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œåœ¨ç²’å­æŒ‘é€‰å’Œé‡å»ºæ–¹é¢éƒ½è¶…è¶Šäº†æœ€å…ˆè¿›çš„åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23444v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªåä¸ºCryoCCDçš„åˆæˆæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºè§£å†³ä½æ¸©ç”µå­æ˜¾å¾®é•œï¼ˆcryo-EMï¼‰ä¸‹æ¸¸åˆ†æç¼ºä¹é«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„éš¾é¢˜ã€‚é€šè¿‡æ•´åˆç”Ÿç‰©ç‰©ç†å»ºæ¨¡ä¸ç”ŸæˆæŠ€æœ¯ï¼ŒCryoCCDèƒ½å¤Ÿç”Ÿæˆåæ˜ çœŸå®ç”Ÿç‰©ç‰©ç†å˜å¼‚æ€§çš„å¤šå°ºåº¦ä½æ¸©ç”µå­æ˜¾å¾®é•œå›¾åƒï¼ŒåŒ…æ‹¬ç»„æˆå¼‚è´¨æ€§ã€ç»†èƒä¸Šä¸‹æ–‡ä»¥åŠç‰©ç†æˆåƒä¿¡æ¯ã€‚é‡‡ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”ŸæˆçœŸå®å™ªå£°ï¼Œé€šè¿‡å¾ªç¯ä¸€è‡´æ€§å¢å¼ºç»“æ„ä¿çœŸåº¦ï¼Œå¹¶é€šè¿‡æ©è†œæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ æ•æ‰ç©ºé—´è‡ªé€‚åº”å™ªå£°æ¨¡å¼ã€‚å®éªŒè¯æ˜ï¼ŒCryoCCDç”Ÿæˆçš„å›¾åƒç»“æ„å‡†ç¡®ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨ç²’å­æŒ‘é€‰å’Œé‡å»ºæ–¹é¢è¶…è¶Šç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½æ¸©ç”µå­æ˜¾å¾®é•œï¼ˆcryo-EMï¼‰æˆåƒä¸­ç¼ºä¹é«˜è´¨é‡æ ‡æ³¨æ•°æ®ï¼Œé˜»ç¢äº†ä¸‹æ¸¸åˆ†æçš„æ¨¡å‹å¼€å‘ã€‚</li>
<li>åˆæˆæ•°æ®ç”Ÿæˆæ˜¯è§£å†³æ­¤é—®é¢˜çš„ä¸€ç§æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æ•æ‰ç”Ÿç‰©æ ‡æœ¬çš„ç»“æ„å¤šæ ·æ€§å’Œä½æ¸©ç”µå­æ˜¾å¾®é•œæˆåƒä¸­çš„å¤æ‚ç©ºé—´å˜åŒ–å™ªå£°ã€‚</li>
<li>CryoCCDæ¡†æ¶æ•´åˆç”Ÿç‰©ç‰©ç†å»ºæ¨¡ä¸ç”ŸæˆæŠ€æœ¯æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”¨äºç”ŸæˆçœŸå®å™ªå£°ï¼Œä¿æŒç»“æ„ä¿çœŸåº¦å¹¶é€šè¿‡æ©è†œæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ æ•æ‰ç©ºé—´è‡ªé€‚åº”å™ªå£°æ¨¡å¼ã€‚</li>
<li>å®éªŒè¯æ˜CryoCCDç”Ÿæˆçš„å›¾åƒç»“æ„å‡†ç¡®å¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-624fda518a5b7bef7196ad86f14ede59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1aa255c9975bf4d18546af38f0ed4ae4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-838fdc5db52adab5246cb64cf7868c88.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GenLit-Reformulating-Single-Image-Relighting-as-Video-Generation"><a href="#GenLit-Reformulating-Single-Image-Relighting-as-Video-Generation" class="headerlink" title="GenLit: Reformulating Single-Image Relighting as Video Generation"></a>GenLit: Reformulating Single-Image Relighting as Video Generation</h2><p><strong>Authors:Shrisha Bharadwaj, Haiwen Feng, Giorgio Becherini, Victoria Fernandez Abrevaya, Michael J. Black</strong></p>
<p>Manipulating the illumination of a 3D scene within a single image represents a fundamental challenge in computer vision and graphics. This problem has traditionally been addressed using inverse rendering techniques, which involve explicit 3D asset reconstruction and costly ray-tracing simulations. Meanwhile, recent advancements in visual foundation models suggest that a new paradigm could soon be possible â€“ one that replaces explicit physical models with networks that are trained on large amounts of image and video data. In this paper, we exploit the physical world understanding of a video diffusion model, particularly Stable Video Diffusion, to relight a single image. We introduce GenLit, a framework that distills the ability of a graphics engine to perform light manipulation into a video-generation model, enabling users to directly insert and manipulate a point light in the 3D world within a given image, and generate results directly as a video sequence. We find that a model fine-tuned on only a small synthetic dataset generalizes to real-world scenes, enabling single-image relighting with plausible and convincing shadows. Our results highlight the ability of video foundation models to capture rich information about lighting, material, and, shape and our findings indicate that such models, with minimal training, can be used to perform relighting without explicit asset reconstruction or complex ray tracing. Project page: <a target="_blank" rel="noopener" href="https://genlit.is.tue.mpg.de/">https://genlit.is.tue.mpg.de/</a>. </p>
<blockquote>
<p>åœ¨å•å¹…å›¾åƒå†…æ“çºµ3Dåœºæ™¯çš„ç…§æ˜æ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦ä¸­çš„ä¸€é¡¹åŸºæœ¬æŒ‘æˆ˜ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™ä¸ªé—®é¢˜æ˜¯é€šè¿‡é€†å‘æ¸²æŸ“æŠ€æœ¯æ¥è§£å†³çš„ï¼Œè¿™æ¶‰åŠåˆ°æ˜ç¡®çš„3Dèµ„äº§é‡å»ºå’Œæ˜‚è´µçš„å…‰çº¿è¿½è¸ªæ¨¡æ‹Ÿã€‚åŒæ—¶ï¼Œè§†è§‰åŸºç¡€æ¨¡å‹çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œä¸€ç§æ–°çš„èŒƒå¼å³å°†åˆ°æ¥â€”â€”ç”¨å¤§é‡å›¾åƒå’Œè§†é¢‘æ•°æ®è®­ç»ƒçš„ç½‘ç»œå–ä»£æ˜ç¡®çš„ç‰©ç†æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯ç¨³å®šçš„è§†é¢‘æ‰©æ•£ï¼‰å¯¹ç‰©ç†ä¸–ç•Œçš„ç†è§£æ¥å¯¹å•å¹…å›¾åƒè¿›è¡Œé‡æ–°ç…§æ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†GenLitæ¡†æ¶ï¼Œå®ƒå°†å›¾å½¢å¼•æ“è¿›è¡Œå…‰çº¿æ“çºµçš„èƒ½åŠ›æç‚¼æˆè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåœ¨ç»™å®šçš„å›¾åƒä¸­ç›´æ¥åœ¨3Dä¸–ç•Œä¸­æ’å…¥å¹¶æ“çºµç‚¹å…‰æºï¼Œå¹¶ç›´æ¥ç”Ÿæˆè§†é¢‘åºåˆ—çš„ç»“æœã€‚æˆ‘ä»¬å‘ç°ä»…åœ¨å°å‹åˆæˆæ•°æ®é›†ä¸Šå¾®è°ƒè¿‡çš„æ¨¡å‹å¯ä»¥æ¨å¹¿åˆ°çœŸå®åœºæ™¯ï¼Œèƒ½å¤Ÿå®ç°å…·æœ‰åˆç†æ€§å’Œè¯´æœåŠ›çš„é˜´å½±çš„å•å›¾åƒé‡æ–°ç…§æ˜ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†è§†é¢‘åŸºç¡€æ¨¡å‹åœ¨æ•æ‰å…³äºç…§æ˜ã€æè´¨å’Œå½¢çŠ¶çš„ä¸°å¯Œä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æœ€å°çš„è®­ç»ƒï¼Œè¿™ç§æ¨¡å‹å¯ç”¨äºæ‰§è¡Œé‡æ–°ç…§æ˜ï¼Œæ— éœ€æ˜ç¡®çš„èµ„äº§é‡å»ºæˆ–å¤æ‚çš„å…‰çº¿è¿½è¸ªã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://genlit.is.tue.mpg.de/%EF%BC%88%E9%93%BE%E6%8E%A5%E6%9C%AA%E8%83%BD%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%EF%BC%89%E3%80%82">https://genlit.is.tue.mpg.de/ï¼ˆé“¾æ¥æ— æ³•ç›´æ¥æ‰“å¼€ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11224v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œå•å›¾åƒç…§æ˜æ“æ§çš„æ–°æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥GenLitæ¡†æ¶ï¼Œç»“åˆå›¾å½¢å¼•æ“çš„èƒ½åŠ›ï¼Œå°†å…‰ç…§æ“æ§èƒ½åŠ›èå…¥è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå®ç°åœ¨ç»™å®šå›¾åƒä¸­ç›´æ¥æ’å…¥å’Œæ“æ§ç‚¹å…‰æºï¼Œå¹¶ç”Ÿæˆè§†é¢‘åºåˆ—ç»“æœã€‚ç ”ç©¶å‘ç°åœ¨å°å‹åˆæˆæ•°æ®é›†ä¸Šå¾®è°ƒåçš„æ¨¡å‹èƒ½å¤Ÿæ³›åŒ–åˆ°çœŸå®åœºæ™¯ï¼Œå®ç°å…·æœ‰å¯ä¿¡é˜´å½±çš„å•å›¾åƒé‡ç…§æ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹çš„æ–°èŒƒå¼ï¼Œé€šè¿‡å¤§å‹å›¾åƒå’Œè§†é¢‘æ•°æ®è®­ç»ƒç½‘ç»œï¼Œæ›¿æ¢ä¼ ç»Ÿæ˜¾å¼ç‰©ç†æ¨¡å‹è§£å†³ç…§æ˜æ“æ§é—®é¢˜ã€‚</li>
<li>æå‡ºäº†GenLitæ¡†æ¶ï¼Œèƒ½å¤Ÿæç‚¼å›¾å½¢å¼•æ“çš„å…‰ç…§æ“æ§èƒ½åŠ›ï¼Œå¹¶å°†å…¶èå…¥è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>å¯åœ¨ç»™å®šå›¾åƒä¸­ç›´æ¥æ’å…¥å’Œæ“æ§ç‚¹å…‰æºï¼Œç”Ÿæˆè§†é¢‘åºåˆ—ç»“æœã€‚</li>
<li>æ¨¡å‹åœ¨å°å‹åˆæˆæ•°æ®é›†ä¸Šå¾®è°ƒåï¼Œèƒ½å¤Ÿæ³›åŒ–åˆ°çœŸå®åœºæ™¯ã€‚</li>
<li>å®ç°å•å›¾åƒé‡ç…§æ˜ï¼Œå…·æœ‰å¯ä¿¡çš„é˜´å½±æ•ˆæœã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ•æ‰å…³äºç…§æ˜ã€æè´¨ã€å½¢çŠ¶ç­‰ä¸°å¯Œä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4c4069bc57517dcf9dbd58ebebe0526.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4b01c1083a9473497c2bcea16f566c1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-001ed77159d94175a37eafb357873301.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba513cc6366cda54eb7598ed1e749247.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a799f04e6f9a1dee2d9c0fab87800e50.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-24  Co-Seg++ Mutual Prompt-Guided Collaborative Learning for Versatile   Medical Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-fcc5c756079868a6184d93ac098a2f64.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-24  R3eVision A Survey on Robust Rendering, Restoration, and Enhancement   for 3D Low-Level Vision
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25691.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
