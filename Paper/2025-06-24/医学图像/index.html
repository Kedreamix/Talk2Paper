<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-06-24  Co-Seg++ Mutual Prompt-Guided Collaborative Learning for Versatile   Medical Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-f68155fc37015d37eb9f1f5515653bc2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    17.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    70 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-24-更新"><a href="#2025-06-24-更新" class="headerlink" title="2025-06-24 更新"></a>2025-06-24 更新</h1><h2 id="Co-Seg-Mutual-Prompt-Guided-Collaborative-Learning-for-Versatile-Medical-Segmentation"><a href="#Co-Seg-Mutual-Prompt-Guided-Collaborative-Learning-for-Versatile-Medical-Segmentation" class="headerlink" title="Co-Seg++: Mutual Prompt-Guided Collaborative Learning for Versatile   Medical Segmentation"></a>Co-Seg++: Mutual Prompt-Guided Collaborative Learning for Versatile   Medical Segmentation</h2><p><strong>Authors:Qing Xu, Yuxiang Luo, Wenting Duan, Zhen Chen</strong></p>
<p>Medical image analysis is critical yet challenged by the need of jointly segmenting organs or tissues, and numerous instances for anatomical structures and tumor microenvironment analysis. Existing studies typically formulated different segmentation tasks in isolation, which overlooks the fundamental interdependencies between these tasks, leading to suboptimal segmentation performance and insufficient medical image understanding. To address this issue, we propose a Co-Seg++ framework for versatile medical segmentation. Specifically, we introduce a novel co-segmentation paradigm, allowing semantic and instance segmentation tasks to mutually enhance each other. We first devise a spatio-temporal prompt encoder (STP-Encoder) to capture long-range spatial and temporal relationships between segmentation regions and image embeddings as prior spatial constraints. Moreover, we devise a multi-task collaborative decoder (MTC-Decoder) that leverages cross-guidance to strengthen the contextual consistency of both tasks, jointly computing semantic and instance segmentation masks. Extensive experiments on diverse CT and histopathology datasets demonstrate that the proposed Co-Seg++ outperforms state-of-the-arts in the semantic, instance, and panoptic segmentation of dental anatomical structures, histopathology tissues, and nuclei instances. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/xq141839/Co-Seg-Plus">https://github.com/xq141839/Co-Seg-Plus</a>. </p>
<blockquote>
<p>医学图像分析非常重要，但面临着需要联合分割器官或组织以及多个解剖结构和肿瘤微环境分析的实例的挑战。现有研究通常孤立地制定不同的分割任务，这忽略了这些任务之间的基本相互依赖性，导致分割性能不佳和医学图像理解不足。为了解决这一问题，我们提出了一个用于通用医学分割的Co-Seg++框架。具体来说，我们引入了一种新的协同分割范式，允许语义分割和实例分割任务相互增强。我们首先设计了一种时空提示编码器（STP-Encoder），以捕获分割区域和图像嵌入之间的远程空间和时间关系，作为先验空间约束。此外，我们还设计了一种多任务协作解码器（MTC-Decoder），它利用交叉指导来加强两个任务上下文的一致性，联合计算语义和实例分割掩膜。在多种CT和病理数据集上的大量实验表明，所提出的Co-Seg++在牙科解剖结构、病理组织和细胞核实例的语义、实例和全景分割方面优于最新技术。源代码可在<a target="_blank" rel="noopener" href="https://github.com/xq141839/Co-Seg-Plus%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xq141839/Co-Seg-Plus上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17159v1">PDF</a> Under Review</p>
<p><strong>Summary</strong><br>医学图像分析面临器官或组织联合分割以及解剖结构、肿瘤微环境分析的多个实例需求。现有研究通常孤立地制定不同的分割任务，忽略了这些任务之间的根本相互依赖性，导致分割性能不佳和医学图像理解不足。为解决这一问题，我们提出Co-Seg++框架进行通用医学分割，引入一种新的协同分割范式，使语义和实例分割任务能够相互促进。采用时空提示编码器（STP-Encoder）捕捉分割区域和图像嵌入之间的远程空间和时间关系，作为先验空间约束。同时，我们设计了多任务协作解码器（MTC-Decoder），利用跨指导增强两个任务上下文一致性，联合计算语义和实例分割掩膜。在多种CT和病理数据集上的实验表明，Co-Seg++在牙齿解剖结构、病理组织以及细胞核实例的语义、实例和全景分割方面优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分析需要同时处理器官或组织的联合分割以及多个实例分割任务。</li>
<li>现有研究孤立处理不同分割任务，导致性能不足和医学图像理解受限。</li>
<li>提出Co-Seg++框架，通过引入协同分割范式解决这一问题。</li>
<li>STP-Encoder用于捕捉分割区域与图像嵌入之间的远程空间和时间关系。</li>
<li>MTC-Decoder利用跨指导增强语义和实例分割任务的上下文一致性。</li>
<li>Co-Seg++在多种医学图像数据集上表现优越，优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17159">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2cc30c8857a0ea0f90366965618b8fbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71f8435849b24e05172f9a79f7181cb9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-388513905ae60b3d74fa644011d80bf8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88e339f07eac77082d75d845cff878ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a799f04e6f9a1dee2d9c0fab87800e50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f13dc68f07911430b8cee7c48568e61.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Semi-Supervised-Multi-Modal-Medical-Image-Segmentation-for-Complex-Situations"><a href="#Semi-Supervised-Multi-Modal-Medical-Image-Segmentation-for-Complex-Situations" class="headerlink" title="Semi-Supervised Multi-Modal Medical Image Segmentation for Complex   Situations"></a>Semi-Supervised Multi-Modal Medical Image Segmentation for Complex   Situations</h2><p><strong>Authors:Dongdong Meng, Sheng Li, Hao Wu, Guoping Wang, Xueqing Yan</strong></p>
<p>Semi-supervised learning addresses the issue of limited annotations in medical images effectively, but its performance is often inadequate for complex backgrounds and challenging tasks. Multi-modal fusion methods can significantly improve the accuracy of medical image segmentation by providing complementary information. However, they face challenges in achieving significant improvements under semi-supervised conditions due to the challenge of effectively leveraging unlabeled data. There is a significant need to create an effective and reliable multi-modal learning strategy for leveraging unlabeled data in semi-supervised segmentation. To address these issues, we propose a novel semi-supervised multi-modal medical image segmentation approach, which leverages complementary multi-modal information to enhance performance with limited labeled data. Our approach employs a multi-stage multi-modal fusion and enhancement strategy to fully utilize complementary multi-modal information, while reducing feature discrepancies and enhancing feature sharing and alignment. Furthermore, we effectively introduce contrastive mutual learning to constrain prediction consistency across modalities, thereby facilitating the robustness of segmentation results in semi-supervised tasks. Experimental results on two multi-modal datasets demonstrate the superior performance and robustness of the proposed framework, establishing its valuable potential for solving medical image segmentation tasks in complex scenarios. </p>
<blockquote>
<p>半监督学习有效地解决了医学图像标注数据有限的问题，但在复杂背景和困难任务下，其性能往往不足。多模态融合方法可以通过提供互补信息来显著提高医学图像分割的准确性。然而，由于有效利用无标签数据的挑战，它们在半监督条件下实现显著改进面临困难。因此，迫切需要一个有效可靠的多模态学习策略，以在半监督分割中利用无标签数据。为了解决这些问题，我们提出了一种新型半监督多模态医学图像分割方法，该方法利用互补的多模态信息，在有限的标记数据下提高性能。我们的方法采用多阶段多模态融合和增强策略，以充分利用互补的多模态信息，同时减少特征差异，增强特征共享和对齐。此外，我们有效地引入了对比互助学习，以约束跨模态的预测一致性，从而促进半监督任务中分割结果的稳健性。在两个多模态数据集上的实验结果证明了所提框架的优越性能和稳健性，为复杂场景下的医学图像分割任务提供了有价值的潜力解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17136v1">PDF</a> 10 pages, 2 figures, accepted at MICCAI 2025</p>
<p><strong>Summary</strong><br>医学图像半监督学习中存在标注数据有限的问题，背景和任务复杂时性能可能不足。多模态融合方法通过提供互补信息提高了医学图像分割的准确性。但在半监督条件下利用未标注数据仍存在挑战。本文提出一种半监督多模态医学图像分割方法，通过融合多模态信息提高性能，采用多阶段多模态融合策略，减少特征差异，增强特征共享和对齐。同时引入对比互学习，约束模态间预测一致性，提高半监督任务分割结果的鲁棒性。实验结果表明，该方法在复杂场景下表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>半监督学习在医学图像中解决了标注数据有限的问题，但在复杂背景和任务下性能可能不足。</li>
<li>多模态融合方法能提供互补信息，提高医学图像分割的准确性。</li>
<li>半监督条件下利用未标注数据存在挑战。</li>
<li>提出一种半监督多模态医学图像分割方法，融合多模态信息提高性能。</li>
<li>采用多阶段多模态融合策略，减少特征差异，增强特征共享和对齐。</li>
<li>引入对比互学习，约束模态间预测一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17136">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e82d9f9816f8517e274eed8ad91c6331.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63425df4582834cc6a9252ecf69f7d2c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Frequently-Used-References-For-Atomic-Data-In-X-ray-Spectroscopy"><a href="#Frequently-Used-References-For-Atomic-Data-In-X-ray-Spectroscopy" class="headerlink" title="Frequently Used References For Atomic Data In X-ray Spectroscopy"></a>Frequently Used References For Atomic Data In X-ray Spectroscopy</h2><p><strong>Authors:N. Hell, G. V. Brown, M. E. Eckart, A. J. Fairchild, C. A. Kilbourne, M. A. Leutenegger, F. S. Porter, M. C. Witthoeft</strong></p>
<p>Accurate atomic physics reference data are a crucial requirement for analysis and interpretation of observed spectra, even more so for observations with high spectral resolution. This document provides a curated list of atomic physics references frequently used for plasma diagnostics in X-ray spectroscopy, outside of comprehensive plasma models that typically come with their own underlying atomic databases. The list includes references to physical constants, laboratory benchmarks, transition energies, position and line shapes of neutral fluorescence lines, radiative branching ratios, and commonly used notation for prominent transitions. Quick-look tables for transition energies in H-, He-, and Li-like ions and line positions and shapes for fluorescence lines in neutrals. The main focus is on K-shell transitions. For the H- and He-like tables, we cite state-of-the art calculations that we consider currently the best available reference energies, which are considered high accuracy and thus typically used for energy scale calibration in laboratory measurements. Omissions in these tables are due to the lack of availability in the chosen references, and are not a statement about the relevance of these lines. Due to their complex and highly source-dependent line shape, the atomic data for neutrals is of lower accuracy than that for the highly charged ions, and the best reference data for these line shapes typically consist of empirical models derived from very high-resolution laboratory measurements. The table for neutrals provided here is consistent with the reference used for the energy gain scale calibration of XRISM&#x2F;Resolve. This document is meant to serve as a resource to help find relevant references and conveniently formatted overview tables. When making use of the information found in these papers, credit should be given to their original authors by citing the appropriate references. </p>
<blockquote>
<p>准确的原子物理学参考数据对于分析和解释观察到的光谱至关重要，对于高光谱分辨率的观察更是如此。本文提供了一份经过精心挑选的原子物理学参考文献列表，这些文献通常用于X射线光谱中的等离子体诊断，并且不包括通常带有自己基础原子数据库的全面等离子体模型。列表包括对物理常数、实验室基准、跃迁能、中性荧光线的位置和线形、辐射分支比以及突出跃迁的常用符号的引用。针对H-、He-和Li-类离子的跃迁能以及中性荧光线的线位置和线形的快速查看表，主要关注K层跃迁。对于H-和He-类表格，我们引用了当前认为的最佳可用参考能量值，这些值具有高精度，通常用于实验室测量的能量标度校准。这些表格中的遗漏是由于所选参考文献中缺少相关信息，并不代表这些线的不重要性。由于中性原子数据的线型复杂且高度依赖于源，其精度低于高度带电离子的精度，而线型最佳的参考数据通常由来自超高分辨率实验室测量的经验模型构成。此处提供的中性表格与XRISM&#x2F;Resolve的能量增益标度校准所参考的文献一致。本文旨在作为帮助查找相关参考文献和方便格式化的概述表格的资源。在使用这些论文中找到的信息时，应通过引用适当的参考文献向原始作者致谢。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17106v1">PDF</a> 18 pages, 5 tables</p>
<p><strong>Summary</strong><br>     此文本提供了一系列针对X射线光谱中离子和中性原子物理参数的参考数据列表，包括物理常数、实验室基准值、过渡能量等。数据主要关注K层跃迁，旨在为高能谱分析提供准确参考数据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本提供了一系列针对X射线光谱的离子和中性原子的原子物理参考数据。</li>
<li>这些数据对于高光谱分辨率的观察至关重要，并包括物理常数、实验室基准值、过渡能量等内容。</li>
<li>列表专注于K层跃迁。</li>
<li>数据来源于最新的高精度计算，被认为是目前最佳可用的参考能量。</li>
<li>中性原子的数据准确性较低，主要因为它们的线形状复杂且高度依赖于源。</li>
<li>提供的数据表与XRISM&#x2F;Resolve的能量增益尺度校准参考一致。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17106">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6e3b42e5366fae251af26b447cf1d2c2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Client-Selection-Strategies-for-Federated-Semantic-Communications-in-Heterogeneous-IoT-Networks"><a href="#Client-Selection-Strategies-for-Federated-Semantic-Communications-in-Heterogeneous-IoT-Networks" class="headerlink" title="Client Selection Strategies for Federated Semantic Communications in   Heterogeneous IoT Networks"></a>Client Selection Strategies for Federated Semantic Communications in   Heterogeneous IoT Networks</h2><p><strong>Authors:Samer Lahoud, Kinda Khawam</strong></p>
<p>The exponential growth of IoT devices presents critical challenges in bandwidth-constrained wireless networks, particularly regarding efficient data transmission and privacy preservation. This paper presents a novel federated semantic communication (SC) framework that enables collaborative training of bandwidth-efficient models for image reconstruction across heterogeneous IoT devices. By leveraging SC principles to transmit only semantic features, our approach dramatically reduces communication overhead while preserving reconstruction quality. We address the fundamental challenge of client selection in federated learning environments where devices exhibit significant disparities in dataset sizes and data distributions. Our framework implements three distinct client selection strategies that explore different trade-offs between system performance and fairness in resource allocation. The system employs an end-to-end SC architecture with semantic bottlenecks, coupled with a loss-based aggregation mechanism that naturally adapts to client heterogeneity. Experimental evaluation on image data demonstrates that while Utilitarian selection achieves the highest reconstruction quality, Proportional Fairness maintains competitive performance while significantly reducing participation inequality and improving computational efficiency. These results establish that federated SC can successfully balance reconstruction quality, resource efficiency, and fairness in heterogeneous IoT deployments, paving the way for sustainable and privacy-preserving edge intelligence applications. </p>
<blockquote>
<p>物联网设备的指数级增长给带宽受限的无线网络带来了重大挑战，特别是在高效数据传输和隐私保护方面。本文针对这一背景，提出了一种新型的联邦语义通信（SC）框架，该框架能够在异构物联网设备之间实现带宽高效模型的协同训练，用于图像重建。通过利用SC原理仅传输语义特征，我们的方法可以在保持重建质量的同时，大幅降低通信开销。我们解决了联邦学习环境中客户选择的根本挑战，在此环境中，设备在数据集大小和数据分布方面存在显著差异。我们的框架实现了三种不同的客户选择策略，在系统性能和资源分配的公平性之间进行不同的权衡。该系统采用了一种端到端的SC架构，带有语义瓶颈，并结合了一种基于损失的聚合机制，该机制能够自然地适应客户端的异构性。在图像数据上的实验评估表明，尽管效用选择达到了最高的重建质量，但比例公平策略在保持竞争力的情况下显著减少了参与不平等并提高了计算效率。这些结果证明了联邦语义通信能够在异构物联网部署中成功平衡重建质量、资源效率和公平性，为可持续和隐私保护的边缘智能应用铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17063v1">PDF</a> </p>
<p><strong>Summary</strong><br>     物联网设备的指数级增长给带宽受限的无线网络带来了重大挑战，特别是在数据高效传输和隐私保护方面。本文提出了一种新型的联邦语义通信（SC）框架，该框架能够在异构物联网设备上实现带宽高效模型的协同训练，用于图像重建。通过利用SC原理只传输语义特征，该方法在保持重建质量的同时，大幅降低了通信开销。本文还解决了联邦学习环境中客户选择的根本挑战，该环境中的设备在数据集大小和数据分布上存在显著差异。该框架实现了三种不同的客户选择策略，在系统性能和资源分配的公平性之间进行不同的权衡。系统采用端到端的SC架构，配合基于损失的聚合机制，自然适应客户异质性。在图像数据上的实验评估表明，效用选择策略实现了最高的重建质量，而比例公平性则在保持竞争性能的同时，显著减少了参与不平等并提高了计算效率。这些结果表明，联邦SC能够成功平衡异构物联网部署中的重建质量、资源效率和公平性，为可持续和隐私保护的边缘智能应用铺平了道路。</p>
<p> <strong>Key Takeaways</strong></p>
<ol>
<li>物联网设备的增长给带宽受限的无线网络带来了挑战，需要解决高效数据传输和隐私保护的问题。</li>
<li>提出了一种新型的联邦语义通信（SC）框架，该框架能够在异构物联网设备上实现带宽高效的模型协同训练，用于图像重建。</li>
<li>利用SC原理只传输语义特征，降低通信开销的同时保持重建质量。</li>
<li>解决了联邦学习环境中客户选择的挑战，该环境中的设备在数据集大小和分布上存在显著差异。</li>
<li>实施了三种客户选择策略，在系统性能和资源分配的公平性之间进行权衡。</li>
<li>系统采用端到端的SC架构和基于损失的聚合机制，以适应客户异质性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17063">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-73b6dccca58aada826d379c8429c44a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6438c2a2e9dabac108980a0bf336891.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3191a759b3f98f954fd29685d0260312.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-803de40b29e8a328fef2710935c5c09f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Directional-Dark-Field-for-Nanoscale-Full-Field-Transmission-X-Ray-Microscopy"><a href="#Directional-Dark-Field-for-Nanoscale-Full-Field-Transmission-X-Ray-Microscopy" class="headerlink" title="Directional Dark Field for Nanoscale Full-Field Transmission X-Ray   Microscopy"></a>Directional Dark Field for Nanoscale Full-Field Transmission X-Ray   Microscopy</h2><p><strong>Authors:Sami Wirtensohn, Silja Flenner, Dominik John, Peng Qi, Christian David, Julia Herzen, Kritika Singh, Gudrun Lotze, Imke Greving</strong></p>
<p>Dark-field X-ray imaging offers unique insights into material structures by visualizing X-ray scattering rather than attenuation, revealing features invisible to conventional imaging techniques. While established approaches like grating-based and speckle-based imaging have demonstrated the utility of dark-field contrast in medical diagnostics and materials science, these methods have been primarily limited to laboratory and micro-CT systems. Building on the recent demonstration of dark-field imaging at the nanoscale using transmission X-ray microscopy, we extend this technique to retrieve directional small-angle scattering information. By analyzing both a test object and human primary tooth enamel, we show that our transmission X-ray microscopy setup can successfully retrieve directional scattering information with minimal modifications of existing systems. This advancement expands the capabilities of nanoscale dark-field imaging, offering new opportunities for investigating structural properties in a wide range of scientific fields. </p>
<blockquote>
<p>暗场X射线成像通过可视化X射线的散射而非衰减，为材料结构提供了独特的洞察视角，揭示出传统成像技术无法观察到的特征。虽然基于光栅和基于斑点成像等现有方法已经证明了暗场对比在医学诊断和材料科学中的实用性，但这些方法主要局限于实验室和显微CT系统。基于最近在透射X射线显微镜下纳米尺度暗场成像的展示，我们将该技术扩展到获取定向小角度散射信息。通过对测试对象和人体乳牙釉质的分析，我们证明我们的透射X射线显微镜装置能够成功获取定向散射信息，且只需对现有的系统进行最小的改造。这一进展扩大了纳米尺度暗场成像的功能，为探索一系列科学领域的结构特性提供了新的机会。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16998v1">PDF</a> </p>
<p><strong>Summary</strong><br>     暗场X射线成像通过可视化X射线散射而非衰减，为材料结构提供了独特见解，暗场成像技术已在医学诊断和材料科学中展现出实用价值，但主要局限于实验室和显微CT系统。近期，基于透射X射线显微镜的纳米尺度暗场成像技术展示，可获取方向性小角度散射信息。通过对测试对象和人类乳牙釉质的实验分析，证明改造现有系统后能够成功获取方向性散射信息，这一进展扩大了纳米尺度暗场成像的能力，为探究广泛科学领域的结构特性提供了新的机遇。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>暗场X射线成像技术可视化X射线散射，为材料结构提供独特见解。</li>
<li>传统成像技术无法观察到的特性可通过暗场成像揭示。</li>
<li>暗场成像技术在医学诊断和材料科学中具备实用价值。</li>
<li>该技术主要局限于实验室和显微CT系统。</li>
<li>通过透射X射线显微镜实现的纳米尺度暗场成像技术能够获取方向性小角度散射信息。</li>
<li>通过测试对象和人类乳牙釉质的实验分析证明了技术的有效性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16998">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8fda964b1ed8a50c0a82d15a56a89e01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d1c869a69ba095e5c5630114dc0b8d4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TextBraTS-Text-Guided-Volumetric-Brain-Tumor-Segmentation-with-Innovative-Dataset-Development-and-Fusion-Module-Exploration"><a href="#TextBraTS-Text-Guided-Volumetric-Brain-Tumor-Segmentation-with-Innovative-Dataset-Development-and-Fusion-Module-Exploration" class="headerlink" title="TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with   Innovative Dataset Development and Fusion Module Exploration"></a>TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with   Innovative Dataset Development and Fusion Module Exploration</h2><p><strong>Authors:Xiaoyu Shi, Rahul Kumar Jain, Yinhao Li, Ruibo Hou, Jingliang Cheng, Jie Bai, Guohua Zhao, Lanfen Lin, Rui Xu, Yen-wei Chen</strong></p>
<p>Deep learning has demonstrated remarkable success in medical image segmentation and computer-aided diagnosis. In particular, numerous advanced methods have achieved state-of-the-art performance in brain tumor segmentation from MRI scans. While recent studies in other medical imaging domains have revealed that integrating textual reports with visual data can enhance segmentation accuracy, the field of brain tumor analysis lacks a comprehensive dataset that combines radiological images with corresponding textual annotations. This limitation has hindered the exploration of multimodal approaches that leverage both imaging and textual data.   To bridge this critical gap, we introduce the TextBraTS dataset, the first publicly available volume-level multimodal dataset that contains paired MRI volumes and rich textual annotations, derived from the widely adopted BraTS2020 benchmark. Building upon this novel dataset, we propose a novel baseline framework and sequential cross-attention method for text-guided volumetric medical image segmentation. Through extensive experiments with various text-image fusion strategies and templated text formulations, our approach demonstrates significant improvements in brain tumor segmentation accuracy, offering valuable insights into effective multimodal integration techniques.   Our dataset, implementation code, and pre-trained models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Jupitern52/TextBraTS">https://github.com/Jupitern52/TextBraTS</a>. </p>
<blockquote>
<p>深度学习在医学图像分割和计算机辅助诊断方面取得了显著的成功。特别是，许多先进的方法在MRI扫描的脑肿瘤分割方面达到了最先进的技术性能。虽然最近在其他医学成像领域的研究表明，将文本报告与视觉数据相结合可以提高分割精度，但脑肿瘤分析领域缺乏一个综合数据集，该数据集结合了放射图像和相应的文本注释。这一局限性阻碍了利用成像和文本数据的多模式方法的探索。为了弥补这一关键差距，我们推出了TextBraTS数据集，这是第一个公开可用的体积级多模式数据集，包含配对的MRI体积和丰富的文本注释，这些注释来源于广泛采用的BraTS2020基准测试。基于这个新颖的数据集，我们提出了一个基线框架和顺序交叉注意方法，用于文本引导的体积医学图像分割。通过采用各种文本图像融合策略和模板文本公式的大量实验，我们的方法在脑肿瘤分割精度方面取得了显著的改进，为有效的多模式集成技术提供了有价值的见解。我们的数据集、实现代码和预训练模型可在[<a target="_blank" rel="noopener" href="https://github.com/Jupitern52/TextBraTS%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Jupitern52/TextBraTS公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16784v1">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像深度学习在医学图像分割和计算机辅助诊断方面取得了显著成果。针对脑部肿瘤分割的MRI扫描，先进的方法已达到最新性能水平。尽管其他医学影像领域的研究表明，将文本报告与视觉数据相结合可以提高分割精度，但脑肿瘤分析领域缺乏一个结合放射图像和相应文本注释的综合数据集。为弥补这一关键差距，我们推出了TextBraTS数据集，这是第一个公开可用的体积级多模式数据集，包含配对的MRI体积和丰富的文本注释，来源于广泛采用的BraTS2020基准测试。基于此新型数据集，我们提出了一种基线框架和顺序交叉注意方法，用于文本引导的体积医学图像分割。通过采用各种文本图像融合策略和模板文本制定，我们的方法在脑肿瘤分割精度方面取得了显著提高，为有效的多模式集成技术提供了有价值的见解。我们的数据集、实施代码和预先训练的模型可在公开渠道访问：<a target="_blank" rel="noopener" href="https://github.com/Jupitern52/TextBraTS">https://github.com/Jupitern52/TextBraTS</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>深度学习在医学图像分割和计算机辅助诊断中表现卓越，特别是在脑部肿瘤MRI扫描分割方面达到最新水平。</li>
<li>尽管结合文本报告和视觉数据在其他医学影像领域已提高分割精度，但脑肿瘤分析领域缺乏综合数据集。</li>
<li>TextBraTS数据集是首个公开可用的体积级多模式数据集，包含MRI体积和丰富的文本注释。</li>
<li>提出了一种新的基线框架和顺序交叉注意方法，利用TextBraTS数据集进行文本引导的医学图像体积分割。</li>
<li>通过不同的文本-图像融合策略和模板文本制定进行广泛实验，显著提高脑肿瘤分割精度。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16784">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ce7ff2a33473e7e0ed6301c63d898276.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-335d38880db4eebf6ff2c31ee3e11e3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e03c812650b7f83e8e7f2d20cb426582.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9547ca13d079db59a6b535c340c9e1c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2888a8e27ec14b490d056858f588275d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Prior-Guided-Joint-Diffusion-Model-in-Projection-Domain-for-PET-Tracer-Conversion"><a href="#A-Prior-Guided-Joint-Diffusion-Model-in-Projection-Domain-for-PET-Tracer-Conversion" class="headerlink" title="A Prior-Guided Joint Diffusion Model in Projection Domain for PET Tracer   Conversion"></a>A Prior-Guided Joint Diffusion Model in Projection Domain for PET Tracer   Conversion</h2><p><strong>Authors:Fang Chen, Weifeng Zhang, Xingyu Ai, BingXuan Li, An Li, Qiegen Liu</strong></p>
<p>Positron emission tomography (PET) is widely used to assess metabolic activity, but its application is limited by the availability of radiotracers. 18F-labeled fluorodeoxyglucose (18F-FDG) is the most commonly used tracer but shows limited effectiveness for certain tumors. In contrast, 6-18F-fluoro-3,4-dihydroxy-L-phenylalanine (18F-DOPA) offers higher specificity for neuroendocrine tumors and neurological disorders. However, its complex synthesis and limitations in transportation and clinical use hinder widespread adoption. During PET imaging, the sinogram represents a form of raw data acquired by the scanner. Therefore, modeling in projection domain enables more direct utilization of the original information, potentially reducing the accumulation of errors introduced during the image reconstruction process. Inspired by these factors, this study proposes a prior-guided joint diffusion model (PJDM) for transforming 18F-FDG PET images into 18F-DOPA PET images in projection domain. Specifically, a coarse estimation model and a prior refinement model are trained independently. During inference, an initial synthetic 18F-DOPA PET sinogram is generated using a higher-order hybrid sampler. This sinogram is then degraded and serves as an additional condition to guide the iterative refinement process using learned prior. Experimental results demonstrated that PJDM effectively improved both sinogram quality and synthetic outcomes. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/yqx7150/PJDM">https://github.com/yqx7150/PJDM</a>. </p>
<blockquote>
<p>正电子发射断层扫描（PET）广泛用于评估代谢活动，但其应用受限于放射性示踪剂的可用性。18F标记的氟脱氧葡萄糖（18F-FDG）是最常用的示踪剂，但对于某些肿瘤，其效果有限。相比之下，6-18F-氟-3,4-二羟基-L-苯丙氨酸（18F-DOPA）对神经内分泌肿瘤和神经疾病具有更高的特异性。然而，其复杂的合成以及运输和临床使用的限制阻碍了其广泛应用。在PET成像中，辛图（Sinogram）是由扫描仪获取的一种原始数据形式。因此，在投影域中进行建模能够更直接地利用原始信息，可能减少在图像重建过程中引入的误差积累。受这些因素启发，本研究提出了一种先验引导联合扩散模型（PJDM），用于在投影域中将18F-FDG PET图像转换为18F-DOPA PET图像。具体而言，分别训练了粗略估计模型和先验细化模型。在推理过程中，使用高阶混合采样器生成初始合成18F-DOPA PET辛图。然后，将此辛图退化并作为附加条件，引导使用学习先验的迭代细化过程。实验结果表明，PJDM有效提高辛图质量和合成结果。相关代码可通过以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/yqx7150/PJDM">https://github.com/yqx7150/PJDM</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16733v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于先验引导联合扩散模型（PJDM）将PET中的¹⁸F-FDG图像转化为¹⁸F-DOPA图像的技术。该技术在投影域进行建模，通过训练粗估计模型和先验优化模型，在推理阶段生成初始的¹⁸F-DOPA PET正弦图，并对其进行退化处理以指导迭代优化过程。实验结果表明，PJDM可有效提高正弦图质量和合成效果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PET常用于评估代谢活动，但受限于放射示踪剂的可获得性。</li>
<li>¹⁸F-FDG是最常用的示踪剂，但对某些肿瘤的效用有限。</li>
<li>¹⁸F-DOPA对神经内分泌肿瘤和神经障碍具有更高的特异性，但其合成复杂且运输和使用受限。</li>
<li>研究提出了一种基于先验引导的联合扩散模型（PJDM）来转换PET图像的技术。</li>
<li>该技术在投影域建模，直接利用原始信息，减少图像重建过程中的误差积累。</li>
<li>使用粗估计模型和先验优化模型进行训练，生成初始的¹⁸F-DOPA PET正弦图并通过迭代优化过程提高其质量。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16733">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dd4ad6f5263fcfae5ff611831070f3af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f467edd818b7e69cd92668d0b5e6e1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5eb8065a429e026c7e409ba6e376fdbe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9db4b59dce8adc38173c6de63f6642f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-696b7d61e16c7c55d077c850bef9e735.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b2edd5814bd470f7dd08d8d3d41ea60.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="The-Vela-pulsar-and-its-pulsar-wind-nebula-Vela-X-using-13-years-of-Fermi-LAT-Observations"><a href="#The-Vela-pulsar-and-its-pulsar-wind-nebula-Vela-X-using-13-years-of-Fermi-LAT-Observations" class="headerlink" title="The Vela pulsar and its pulsar wind nebula Vela-X using 13 years of   Fermi-LAT Observations"></a>The Vela pulsar and its pulsar wind nebula Vela-X using 13 years of   Fermi-LAT Observations</h2><p><strong>Authors:Alexander Lange, J. Eagle, O. Kargaltsev, Lucien Kuiper, Jeremy Hare</strong></p>
<p>We present results of more than 13 years of Fermi-LAT data analysis for the Vela pulsar from 60 MeV to 100 GeV and its pulsar wind nebula (PWN), Vela-X, for E &gt; 1 GeV in the off-pulse phases. We find the Vela-X PWN can be best characterized using two extended components: a large radial Gaussian accompanied by an off-set, compact radial disk, both with a similar spectral index, \Gamma \sim 2.3. The common spectral properties support a common PWN origin, but a supernova remnant component is plausible for the compact radial disk. With an updated Vela-X model, the phase resolved spectral properties of the Vela pulsar are explored through a phase-resolved analysis. The phase-resolved spectral properties of the pulsar are presented, such as the SED peak energy E$_p$, the width of the SED at its peak, d$_p$, and the asymptotic (low-energy) spectral index, $\Gamma_0$, are presented. The best-fit spectral models for each LAT pulse peak (Peak 1 and Peak 2) are extrapolated to UV energies and compared to archival, phase-resolved spectra at UV, X-ray, soft \gamma-ray and TeV energies. We also discuss the physical implications of our modeling and the data comparisons. </p>
<blockquote>
<p>我们对Vela脉冲星和其脉冲风星云（PWN）Vela-X进行了超过13年的费米-LAT数据分析，能量范围从60MeV到100GeV，并针对非脉冲阶段的E &gt; 1 GeV进行了讨论。我们发现Vela-X的PWN最好用两个扩展成分来描述：一个大的径向高斯分布，伴随着一个偏移的紧凑径向盘，它们的谱指数相似，Gamma约为2.3。相同的谱特性支持它们来源于同一个PWN，但对于紧凑径向盘来说，超新星遗迹成分也是可能的。通过更新的Vela-X模型，我们对Vela脉冲星的相位解析谱特性进行了探究。展示了脉冲星的相位解析谱特性，如SED峰值能量Ep、SED峰值的宽度dp、渐近（低能）谱指数Γ0等。将每个LAT脉冲峰值（Peak 1和Peak 2）的最佳拟合谱模型外推到紫外能量，并与档案中的紫外、X射线、软γ射线和TeV能量的相位解析光谱进行了比较。我们还讨论了我们的建模和数据比较的物理意义。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16687v1">PDF</a> Accepted for publication in the Astrophysical Journal. 24 pages, 15   figures</p>
<p><strong>Summary</strong><br>     本文报道了对超过13年的费米LAT数据分析结果，研究了Vela脉冲星和其脉冲风星云Vela-X在60MeV至100GeV的能量范围内的表现。发现Vela-X的最佳特征是由两个扩展成分组成：一个大的径向高斯分布和一个偏移的紧凑径向圆盘，两者具有相似的光谱指数Γ≈2.3。更新的Vela-X模型被用于探索Vela脉冲星的相位解析光谱特性。本文还探讨了模型的物理含义和数据比较。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>报道了对Vela脉冲星超过13年的费米LAT数据分析结果。</li>
<li>研究了Vela脉冲星和其脉冲风星云Vela-X在特定能量范围内的表现。</li>
<li>Vela-X可以被最好地描述为两个扩展成分：一个大的径向高斯分布和一个偏移的紧凑径向圆盘。</li>
<li>这两个成分具有相似的光谱指数Γ≈2.3，支持它们来自同一PWN的起源。</li>
<li>提出了一个可能是由超新星残骸成分构成的紧凑径向圆盘。</li>
<li>通过更新的Vela-X模型，探索了Vela脉冲星的相位解析光谱特性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16687">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-58178dcf37f1062f8d83e5de7df20ce0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca653148de7bd8bfe9bdebcb9924b691.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb5b719ae8dc19e82d5e0795b0719d0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d97d8cc79facc39e47a10f8e43782556.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ab7c08bb95ba4a27f1fed94093ed72a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-788ef7ae569eb999967b64bc7b46d6d0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Hybrid-Attention-Network-for-Accurate-Breast-Tumor-Segmentation-in-Ultrasound-Images"><a href="#Hybrid-Attention-Network-for-Accurate-Breast-Tumor-Segmentation-in-Ultrasound-Images" class="headerlink" title="Hybrid Attention Network for Accurate Breast Tumor Segmentation in   Ultrasound Images"></a>Hybrid Attention Network for Accurate Breast Tumor Segmentation in   Ultrasound Images</h2><p><strong>Authors:Muhammad Azeem Aslam, Asim Naveed, Nisar Ahmed</strong></p>
<p>Breast ultrasound imaging is a valuable tool for early breast cancer detection, but automated tumor segmentation is challenging due to inherent noise, variations in scale of lesions, and fuzzy boundaries. To address these challenges, we propose a novel hybrid attention-based network for lesion segmentation. Our proposed architecture integrates a pre-trained DenseNet121 in the encoder part for robust feature extraction with a multi-branch attention-enhanced decoder tailored for breast ultrasound images. The bottleneck incorporates Global Spatial Attention (GSA), Position Encoding (PE), and Scaled Dot-Product Attention (SDPA) to learn global context, spatial relationships, and relative positional features. The Spatial Feature Enhancement Block (SFEB) is embedded at skip connections to refine and enhance spatial features, enabling the network to focus more effectively on tumor regions. A hybrid loss function combining Binary Cross-Entropy (BCE) and Jaccard Index loss optimizes both pixel-level accuracy and region-level overlap metrics, enhancing robustness to class imbalance and irregular tumor shapes. Experiments on public datasets demonstrate that our method outperforms existing approaches, highlighting its potential to assist radiologists in early and accurate breast cancer diagnosis. </p>
<blockquote>
<p>乳腺超声成像在早期乳腺癌检测中是一种有价值的工具，但由于其固有的噪声、病灶尺度的变化和模糊的边界，自动化肿瘤分割仍然面临挑战。为了应对这些挑战，我们提出了一种新型混合注意力网络用于病灶分割。我们的提议架构将预训练的DenseNet121集成到编码器部分进行稳健的特征提取，并结合一个针对乳腺超声图像的多分支注意力增强解码器。瓶颈层结合了全局空间注意力（GSA）、位置编码（PE）和缩放点积注意力（SDPA）来学习全局上下文、空间关系和相对位置特征。空间特征增强块（SFEB）嵌入到跳跃连接中以细化和增强空间特征，使网络更有效地关注肿瘤区域。结合二元交叉熵（BCE）和Jaccard指数损失的混合损失函数优化了像素级精度和区域级重叠指标，提高了对类别不平衡和不规则肿瘤形状的鲁棒性。在公共数据集上的实验表明，我们的方法优于现有方法，突显其在帮助放射科医生进行早期和准确的乳腺癌诊断方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16592v1">PDF</a> </p>
<p><strong>Summary</strong><br>     针对乳腺超声影像中肿瘤分割的挑战，提出一种新型混合注意力网络。该网络结合预训练的DenseNet121进行特征提取，并利用多分支注意力增强解码器处理乳腺超声图像。通过全局空间注意力、位置编码和缩放点积注意力学习全局上下文、空间关系和相对位置特征。实验证明，该方法在公共数据集上的表现优于现有方法，有助于提高乳腺癌的早期诊断准确率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>乳腺超声成像对早期乳腺癌检测具有价值。</li>
<li>自动化肿瘤分割在乳腺超声图像中面临挑战，如噪声、病变规模变化和模糊边界。</li>
<li>提出一种新型混合注意力网络，结合预训练的DenseNet121进行特征提取。</li>
<li>网络利用多分支注意力增强解码器处理乳腺超声图像。</li>
<li>通过全局空间注意力、位置编码和缩放点积注意力提高网络性能。</li>
<li>网络的实验表现优于现有方法，在公共数据集上取得良好效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16592">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9cf07af7cd924e9101b27ba76e86456e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7cc7a4e0dd7f91b57e6c94ed3532ae7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61390f4adae71bc408d7007fb6f5e89a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Pixel-wise-Modulated-Dice-Loss-for-Medical-Image-Segmentation"><a href="#Pixel-wise-Modulated-Dice-Loss-for-Medical-Image-Segmentation" class="headerlink" title="Pixel-wise Modulated Dice Loss for Medical Image Segmentation"></a>Pixel-wise Modulated Dice Loss for Medical Image Segmentation</h2><p><strong>Authors:Seyed Mohsen Hosseini</strong></p>
<p>Class imbalance and the difficulty imbalance are the two types of data imbalance that affect the performance of neural networks in medical segmentation tasks. In class imbalance the loss is dominated by the majority classes and in difficulty imbalance the loss is dominated by easy to classify pixels. This leads to an ineffective training. Dice loss, which is based on a geometrical metric, is very effective in addressing the class imbalance compared to the cross entropy (CE) loss, which is adopted directly from classification tasks. To address the difficulty imbalance, the common approach is employing a re-weighted CE loss or a modified Dice loss to focus the training on difficult to classify areas. The existing modification methods are computationally costly and with limited success. In this study we propose a simple modification to the Dice loss with minimal computational cost. With a pixel level modulating term, we take advantage of the effectiveness of Dice loss in handling the class imbalance to also handle the difficulty imbalance. Results on three commonly used medical segmentation tasks show that the proposed Pixel-wise Modulated Dice loss (PM Dice loss) outperforms other methods, which are designed to tackle the difficulty imbalance problem. </p>
<blockquote>
<p>在医学分割任务中，类别不平衡和难度不平衡是影响神经网络性能的两种数据不平衡问题。在类别不平衡中，损失主要由多数类别主导；而在难度不平衡中，损失主要由容易分类的像素主导。这导致了训练效果不佳。Dice损失基于几何度量，在解决类别不平衡问题时相较于直接从分类任务中采纳的交叉熵（CE）损失非常有效。为了解决难度不平衡问题，常见的做法是采用加权CE损失或修改后的Dice损失，以将训练重点放在难以分类的区域。现有的修改方法计算成本较高且成功有限。本研究中，我们提出了对Dice损失的简单修改，且计算成本较低。通过像素级调制项，我们利用Dice损失在处理类别不平衡问题时的有效性，同时也处理难度不平衡问题。在三个常用的医学分割任务上的结果表明，所提出的像素级调制Dice损失（PM Dice损失）优于其他旨在解决难度不平衡问题的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15744v1">PDF</a> </p>
<p><strong>Summary</strong><br>     针对医学分割任务中神经网络性能受影响的类别不平衡和难度不平衡问题，本研究提出了一种简单的Dice损失修改方法，以较低的计算成本同时解决两类数据不平衡问题。通过像素级调制项，利用Dice损失在处理类别不平衡方面的有效性，也解决了难度不平衡问题。在三种常用的医学分割任务上，所提的像素级调制Dice损失（PM Dice损失）表现优于其他解决难度不平衡问题的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学分割任务中，神经网络性能受两类数据不平衡影响：类别不平衡和难度不平衡。</li>
<li>类别不平衡中，损失主要由多数类别主导；难度不平衡中，损失主要由易分类像素主导。</li>
<li>Dice损失在处理类别不平衡问题上比交叉熵损失更有效。</li>
<li>现有解决难度不平衡问题的方法计算成本高且效果有限。</li>
<li>本研究提出一种简单的Dice损失修改方法，以低计算成本同时处理两类数据不平衡问题。</li>
<li>通过像素级调制项，利用Dice损失在处理类别不平衡方面的优势，也解决了难度不平衡问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15744">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e489b0311cb1710c5c2004553ae5534c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2022c50334fada85ce28d8cd9977f16e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97b60eb1369a2232e9bac146e81fb826.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf95d73d17a01737ef4eab7700ae69b2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SynPo-Boosting-Training-Free-Few-Shot-Medical-Segmentation-via-High-Quality-Negative-Prompts"><a href="#SynPo-Boosting-Training-Free-Few-Shot-Medical-Segmentation-via-High-Quality-Negative-Prompts" class="headerlink" title="SynPo: Boosting Training-Free Few-Shot Medical Segmentation via   High-Quality Negative Prompts"></a>SynPo: Boosting Training-Free Few-Shot Medical Segmentation via   High-Quality Negative Prompts</h2><p><strong>Authors:Yufei Liu, Haoke Xiao, Jiaxing Chai, Yongcun Zhang, Rong Wang, Zijie Meng, Zhiming Luo</strong></p>
<p>The advent of Large Vision Models (LVMs) offers new opportunities for few-shot medical image segmentation. However, existing training-free methods based on LVMs fail to effectively utilize negative prompts, leading to poor performance on low-contrast medical images. To address this issue, we propose SynPo, a training-free few-shot method based on LVMs (e.g., SAM), with the core insight: improving the quality of negative prompts. To select point prompts in a more reliable confidence map, we design a novel Confidence Map Synergy Module by combining the strengths of DINOv2 and SAM. Based on the confidence map, we select the top-k pixels as the positive points set and choose the negative points set using a Gaussian distribution, followed by independent K-means clustering for both sets. Then, these selected points are leveraged as high-quality prompts for SAM to get the segmentation results. Extensive experiments demonstrate that SynPo achieves performance comparable to state-of-the-art training-based few-shot methods. </p>
<blockquote>
<p>大规模视觉模型（LVMs）的出现为少数医学图像分割提供了新的机会。然而，基于LVMs的无训练方法无法有效利用负提示，导致在低对比度医学图像上的性能不佳。为了解决这个问题，我们提出了SynPo，这是一种基于LVMs（例如SAM）的无训练少数方法，其核心是改善负提示的质量。为了在更可靠的置信图中选择点提示，我们结合了DINOv2和SAM的优点，设计了一种新型的置信图协同模块。基于置信图，我们选择前k个像素作为正点集，使用高斯分布选择负点集，然后对这两个集合进行独立的K-means聚类。然后，这些选定的点被用作高质量提示，供SAM获取分割结果。大量实验表明，SynPo的性能与基于训练的少数方法相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15153v2">PDF</a> MICCAI 2025 Early Accept. Project Page:   <a target="_blank" rel="noopener" href="https://liu-yufei.github.io/synpo-project-page/">https://liu-yufei.github.io/synpo-project-page/</a></p>
<p><strong>Summary</strong></p>
<p>大型视觉模型（LVMs）为少数医疗图像分割提供了新的机会，但现有基于LVMs的无训练方法无法有效利用负提示，导致在低对比度医疗图像上的表现不佳。为解决这一问题，我们提出了基于LVMs的无训练少数方法SynPo，其核心是提高负提示的质量。通过结合DINOv2和SAM的优点，我们设计了一种新型的置信图协同模块，以更可靠的置信图选择点提示。基于置信图，我们选择前k个像素作为正点集，使用高斯分布选择负点集，然后对这两组进行独立的K-means聚类。然后，这些选定的点被用作高质量提示，供SAM进行分割。实验表明，SynPo的性能与基于训练的少数方法相当。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型视觉模型（LVMs）在医疗图像分割中具有潜力。</li>
<li>现有基于LVMs的无训练方法在低对比度医疗图像上的表现不佳。</li>
<li>SynPo方法旨在解决这一问题，通过提高负提示的质量来优化性能。</li>
<li>SynPo设计了一种新型的置信图协同模块，结合DINOv2和SAM的优点。</li>
<li>基于置信图，选择正点集和负点集，然后进行独立的K-means聚类。</li>
<li>选定的点被用作高质量提示，供SAM进行分割。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15153">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-96dbe471f0ef20ab45c582024ead24a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a64112893cb38c03306e6d6cd7e48c88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3b9f688b027c6b4228079a26d761301.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="BreastDCEDL-Curating-a-Comprehensive-DCE-MRI-Dataset-and-developing-a-Transformer-Implementation-for-Breast-Cancer-Treatment-Response-Prediction"><a href="#BreastDCEDL-Curating-a-Comprehensive-DCE-MRI-Dataset-and-developing-a-Transformer-Implementation-for-Breast-Cancer-Treatment-Response-Prediction" class="headerlink" title="BreastDCEDL: Curating a Comprehensive DCE-MRI Dataset and developing a   Transformer Implementation for Breast Cancer Treatment Response Prediction"></a>BreastDCEDL: Curating a Comprehensive DCE-MRI Dataset and developing a   Transformer Implementation for Breast Cancer Treatment Response Prediction</h2><p><strong>Authors:Naomi Fridman, Bubby Solway, Tomer Fridman, Itamar Barnea, Anat Goldstein</strong></p>
<p>Breast cancer remains a leading cause of cancer-related mortality worldwide, making early detection and accurate treatment response monitoring critical priorities. We present BreastDCEDL, a curated, deep learning-ready dataset comprising pre-treatment 3D Dynamic Contrast-Enhanced MRI (DCE-MRI) scans from 2,070 breast cancer patients drawn from the I-SPY1, I-SPY2, and Duke cohorts, all sourced from The Cancer Imaging Archive. The raw DICOM imaging data were rigorously converted into standardized 3D NIfTI volumes with preserved signal integrity, accompanied by unified tumor annotations and harmonized clinical metadata including pathologic complete response (pCR), hormone receptor (HR), and HER2 status. Although DCE-MRI provides essential diagnostic information and deep learning offers tremendous potential for analyzing such complex data, progress has been limited by lack of accessible, public, multicenter datasets. BreastDCEDL addresses this gap by enabling development of advanced models, including state-of-the-art transformer architectures that require substantial training data. To demonstrate its capacity for robust modeling, we developed the first transformer-based model for breast DCE-MRI, leveraging Vision Transformer (ViT) architecture trained on RGB-fused images from three contrast phases (pre-contrast, early post-contrast, and late post-contrast). Our ViT model achieved state-of-the-art pCR prediction performance in HR+&#x2F;HER2- patients (AUC 0.94, accuracy 0.93). BreastDCEDL includes predefined benchmark splits, offering a framework for reproducible research and enabling clinically meaningful modeling in breast cancer imaging. </p>
<blockquote>
<p>乳腺癌仍然是全球癌症相关死亡的主要原因之一，因此早期检测和准确的治疗反应监测成为至关重要的优先事项。我们推出了BreastDCEDL，这是一个精心策划、准备好用于深度学习数据集，包含来自I-SPY1、I-SPY2和Duke队列的2070名乳腺癌患者的治疗前3D动态增强MRI（DCE-MRI）扫描。所有数据均来自癌症成像存档。原始的DICOM成像数据被严格转换为标准化的3DNIfTI体积，同时保留了信号完整性，并配有统一的肿瘤注释和协调一致的临床元数据，包括病理完全反应（pCR）、激素受体（HR）和HER2状态。尽管DCE-MRI提供了重要的诊断信息，深度学习在分析此类复杂数据方面拥有巨大潜力，但由于缺乏可访问的公共多中心数据集，进展一直受到限制。BreastDCEDL通过支持开发先进模型来解决这一差距，包括需要大训练数据的最新变压器架构。为了展示其稳健建模的能力，我们开发了基于变压器的首个乳腺癌DCE-MRI模型，利用在三个对比阶段（预对比、早期后对比和晚期后对比）的RGB融合图像上训练的Vision Transformer（ViT）架构。我们的ViT模型在HR+&#x2F;HER2-患者中实现了最先进的pCR预测性能（AUC 0.94，准确率0.93）。BreastDCEDL包括预定义的基准分割，为可重复的研究提供了一个框架，并在乳腺癌成像中实现了临床意义建模。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12190v2">PDF</a> </p>
<p><strong>Summary</strong><br>    乳腺癌是全球癌症死亡的主要原因之一，早期发现和准确监测治疗反应是关键。本研究推出BreastDCEDL数据集，包含来自I-SPY1、I-SPY2和Duke等队列的2,070例乳腺癌患者的预治疗3D动态增强MRI扫描数据，数据来自癌症成像档案。数据集包括标准化的3DNIfTI体积图像、统一的肿瘤注释和协调的临床元数据。本研究开发了基于Vision Transformer的模型，预测HR+&#x2F;HER2-患者的pCR，性能达到最新水平。BreastDCEDL包括预设的基准分割，为可重复研究和乳腺癌成像的临床意义建模提供了框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>乳腺癌仍是全球癌症死亡的主要原因，早期发现和准确监测治疗反应至关重要。</li>
<li>研究推出BreastDCEDL数据集，包含来自多个队列的乳腺癌患者的预治疗3D DCE-MRI扫描数据。</li>
<li>数据集包括标准化的图像数据、统一的肿瘤注释和临床元数据。</li>
<li>深度学习和DCE-MRI在乳腺癌诊断和治疗中潜力巨大，但缺乏公共多中心数据集限制了进展。</li>
<li>BreastDCEDL填补了这一空白，支持开发先进模型，包括基于Transformer的架构。</li>
<li>研究人员开发了基于Vision Transformer的模型，用于预测HR+&#x2F;HER2-患者的pCR，性能达到最新水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12190">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3835c0516c0ba853d6f3b0d1fb0d1ba1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-100bd057869bb8cf967db06d3455a640.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9f0ec1c0db9c5ca2eebe0ec3ac097f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b64260b9af43fcc9c67030971d20f4c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Statistical-microlocal-analysis-in-two-dimensional-X-ray-CT"><a href="#Statistical-microlocal-analysis-in-two-dimensional-X-ray-CT" class="headerlink" title="Statistical microlocal analysis in two-dimensional X-ray CT"></a>Statistical microlocal analysis in two-dimensional X-ray CT</h2><p><strong>Authors:Anuj Abhishek, Alexander Katsevich, James W. Webber</strong></p>
<p>In many imaging applications it is important to assess how well the edges of the original object, $f$, are resolved in an image, $f^\text{rec}$, reconstructed from the measured data, $g$. In this paper we consider the case of image reconstruction in 2D X-ray Computed Tomography (CT). Let $f$ be a function describing the object being scanned, and $g&#x3D;Rf + \eta$ be the Radon transform data in $\mathbb{R}^2$ corrupted by noise, $\eta$, and sampled with step size $\sim\epsilon$. Conventional microlocal analysis provides conditions for edge detectability based on the scanner geometry in the case of continuous, noiseless data (when $\eta &#x3D; 0$), but does not account for noise and finite sampling step size. We develop a novel technique called Statistical Microlocal Analysis (SMA), which uses a statistical hypothesis testing framework to determine if an image edge (singularity) of $f$ is detectable from $f^\text{rec}$, and we quantify edge detectability using the statistical power of the test. Our approach is based on the theory we developed in previous work, which provides a characterization of $f^\text{rec}$ in local $O(\epsilon)$-size neighborhoods when $\eta \neq 0$. We derive a statistical test for the presence and direction of an edge microlocally given the magnitude of $\eta$ and data sampling step size. Using the properties of the null distribution of the test, we quantify the uncertainty of the edge magnitude and direction. We validate our theory using simulations, which show strong agreement between our predictions and experimental observations. Our work is not only of practical value, but of theoretical value as well. SMA is a natural extension of classical microlocal analysis theory which accounts for practical measurement imperfections, such as noise and finite step size, at the highest possible resolution compatible with the data. </p>
<blockquote>
<p>在许多成像应用中，评估原始对象$f$的边缘在由测量数据$g$重建的图像$f^\text{rec}$中解析得有多好是非常重要的。本文考虑二维X射线计算机断层扫描（CT）中的图像重建情况。设$f$为描述被扫描对象的函数，$g&#x3D;Rf+\eta$为受到噪声$\eta$影响的Radon变换数据在$\mathbb{R}^2$中的表示，并且以步长$\sim\epsilon$进行采样。传统的微局部分析为连续、无噪声数据（当$\eta &#x3D; 0$时）的扫描仪几何提供了边缘可检测性的条件，但不考虑噪声和有限的采样步长。我们开发了一种称为统计微局部分析（SMA）的新技术，它使用统计假设检验框架来确定从$f^\text{rec}$是否可检测图像边缘（奇异性），并且我们使用检验的统计效力来量化边缘的可检测性。我们的方法基于我们之前的工作所发展的理论，该理论在$\eta \neq 0$的情况下，对$f^\text{rec}$在局部$O(\epsilon)$大小邻域内的特性进行了描述。我们针对边缘的存在性和方向性微局部地推导了一个统计检验，给定$\eta$的幅度和数据采样步长。利用检验的空分布属性，我们对边缘幅度和方向的不确定性进行了量化。我们通过模拟验证了我们的理论，模拟结果显示我们的预测与实验观察结果之间具有很强的一致性。我们的工作不仅具有实用价值，而且具有理论价值。SMA是经典微局部分析理论的自然扩展，它考虑了实际测量中的不完美之处，例如噪声和有限的步长，并在与数据兼容的最高可能分辨率下进行。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05113v3">PDF</a> 27 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>该论文关注二维X射线计算机断层扫描（CT）中的图像重建问题。针对传统微局部分析在噪声和有限采样步长方面的不足，提出了一种名为统计微局部分析（SMA）的新技术。该技术使用统计假设检验框架来确定从重建图像$f^{rec}$中是否可检测到对象函数$f$的图像边缘（奇点），并利用检验的统计效力量化边缘检测能力。此外，该研究还利用先前的理论工作，对$\eta \neq 0$时$f^{rec}$在局部$O(\epsilon)$大小邻域内的特性进行了表征。通过模拟验证，该研究的结果与实验观察结果高度一致。统计微局部分析不仅是实践中的有价值工具，也是对传统微局部分析理论的有益补充，能够解决实际测量中的不完美问题，如噪声和有限的步长，并兼容数据实现最高分辨率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文研究了二维X射线CT中的图像重建问题，关注边缘分辨率的评估。</li>
<li>提出了统计微局部分析（SMA）技术，利用统计假设检验框架评估图像边缘的检测性。</li>
<li>量化边缘检测能力，通过统计效力评估检验。</li>
<li>考虑噪声和有限采样步长等实际测量不完美因素。</li>
<li>基于先前的理论工作，对$f^{rec}$在局部邻域的特性进行了表征。</li>
<li>通过模拟验证了理论预测与实验观察的高度一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05113">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4235ed3fa3012b660e63eb8b302adb95.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Automatic-dataset-shift-identification-to-support-safe-deployment-of-medical-imaging-AI"><a href="#Automatic-dataset-shift-identification-to-support-safe-deployment-of-medical-imaging-AI" class="headerlink" title="Automatic dataset shift identification to support safe deployment of   medical imaging AI"></a>Automatic dataset shift identification to support safe deployment of   medical imaging AI</h2><p><strong>Authors:Mélanie Roschewitz, Raghav Mehta, Charles Jones, Ben Glocker</strong></p>
<p>Shifts in data distribution can substantially harm the performance of clinical AI models and lead to misdiagnosis. Hence, various methods have been developed to detect the presence of such shifts at deployment time. However, the root causes of dataset shifts are diverse, and the choice of shift mitigation strategies is highly dependent on the precise type of shift encountered at test time. As such, detecting test-time dataset shift is not sufficient: precisely identifying which type of shift has occurred is critical. In this work, we propose the first unsupervised dataset shift identification framework for imaging datasets, effectively distinguishing between prevalence shift (caused by a change in the label distribution), covariate shift (caused by a change in input characteristics) and mixed shifts (simultaneous prevalence and covariate shifts). We discuss the importance of self-supervised encoders for detecting subtle covariate shifts and propose a novel shift detector leveraging both self-supervised encoders and task model outputs for improved shift detection. We show the effectiveness of the proposed shift identification framework across three different imaging modalities (chest radiography, digital mammography, and retinal fundus images) on five types of real-world dataset shifts using five large publicly available datasets. </p>
<blockquote>
<p>数据分布的变迁可能会显著影响临床人工智能模型的性能，并导致误诊。因此，已经开发了各种方法来检测部署时的这种变迁是否存在。然而，数据集变迁的根源是多样的，所选的变迁缓解策略高度依赖于测试时遇到的精确变迁类型。因此，仅在测试时检测数据集变迁是不够的：精确识别哪种类型的变迁已经发生至关重要。在这项工作中，我们为成像数据集提出了第一个无监督数据集变迁识别框架，能够有效区分由标签分布变化引起的普及变迁、由输入特征变化引起的协变量变迁以及同时发生的普及和协变量混合变迁。我们讨论了自监督编码器在检测细微协变量变迁中的重要性，并提出了一种新型变迁检测器，该检测器利用自监督编码器和任务模型输出进行改进，以提高变迁检测的准确性。我们展示了所提出的数据集变迁识别框架在三种不同成像模式（胸部放射摄影、数字乳腺摄影和视网膜眼底图像）上的有效性，使用了五种大型公开数据集和五种现实世界的数据集变迁类型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07940v3">PDF</a> Accepted at MICCAI 2025. This version is an extended version with   additional experimental results. Code available at   <a target="_blank" rel="noopener" href="https://github.com/biomedia-mira/shift_identification">https://github.com/biomedia-mira/shift_identification</a></p>
<p><strong>Summary</strong><br>     临床AI模型在数据分布发生变化时性能会大幅下降，可能导致误诊。为检测部署时的数据分布变化，已开发多种方法。然而，数据集变化的根本原因多样，选择何种变化缓解策略高度依赖于测试时遇到的具体变化类型。因此，仅在测试时检测数据集变化是不够的，精确识别出哪种变化至关重要。本研究提出首个针对成像数据集的无监督数据集变化识别框架，有效区分由标签分布变化引起的普遍变化、由输入特征变化引起的协变量变化和混合变化（同时发生普遍变化和协变量变化）。本文讨论自监督编码器在检测细微协变量变化中的重要性，并提出一种新型变化检测器，该检测器利用自监督编码器和任务模型输出以提高变化检测效果。实验证明，该变化识别框架在三种不同的成像模式（胸部放射、数字乳腺摄影和眼底图像）上对五种现实世界数据集的五种类型变化均有效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据分布的变化可能对临床AI模型的性能产生重大影响，导致误诊断。</li>
<li>测试时的数据集变化检测是不够的，需要精确识别变化的类型。</li>
<li>普遍变化、协变量变化和混合变化是数据集变化的三种主要类型。</li>
<li>自监督编码器在检测细微协变量变化中起到重要作用。</li>
<li>提出的无监督数据集变化识别框架能有效区分这三种变化类型。</li>
<li>该框架利用自监督编码器和任务模型输出，提高了变化检测的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07940">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f68155fc37015d37eb9f1f5515653bc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc3c97454cdeef4d97089a0ef37666c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45d651516ad9b23343cde847910ced37.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Medical-Artificial-Intelligence-for-Early-Detection-of-Lung-Cancer-A-Survey"><a href="#Medical-Artificial-Intelligence-for-Early-Detection-of-Lung-Cancer-A-Survey" class="headerlink" title="Medical Artificial Intelligence for Early Detection of Lung Cancer: A   Survey"></a>Medical Artificial Intelligence for Early Detection of Lung Cancer: A   Survey</h2><p><strong>Authors:Guohui Cai, Ying Cai, Zeyu Zhang, Yuanzhouhan Cao, Lin Wu, Daji Ergu, Zhinbin Liao, Yang Zhao</strong></p>
<p>Lung cancer remains one of the leading causes of morbidity and mortality worldwide, making early diagnosis critical for improving therapeutic outcomes and patient prognosis. Computer-aided diagnosis systems, which analyze computed tomography images, have proven effective in detecting and classifying pulmonary nodules, significantly enhancing the detection rate of early-stage lung cancer. Although traditional machine learning algorithms have been valuable, they exhibit limitations in handling complex sample data. The recent emergence of deep learning has revolutionized medical image analysis, driving substantial advancements in this field. This review focuses on recent progress in deep learning for pulmonary nodule detection, segmentation, and classification. Traditional machine learning methods, such as support vector machines and k-nearest neighbors, have shown limitations, paving the way for advanced approaches like Convolutional Neural Networks, Recurrent Neural Networks, and Generative Adversarial Networks. The integration of ensemble models and novel techniques is also discussed, emphasizing the latest developments in lung cancer diagnosis. Deep learning algorithms, combined with various analytical techniques, have markedly improved the accuracy and efficiency of pulmonary nodule analysis, surpassing traditional methods, particularly in nodule classification. Although challenges remain, continuous technological advancements are expected to further strengthen the role of deep learning in medical diagnostics, especially for early lung cancer detection and diagnosis. A comprehensive list of lung cancer detection models reviewed in this work is available at <a target="_blank" rel="noopener" href="https://github.com/CaiGuoHui123/Awesome-Lung-Cancer-Detection">https://github.com/CaiGuoHui123/Awesome-Lung-Cancer-Detection</a>. </p>
<blockquote>
<p>肺癌仍然是全球发病率和死亡率的主要原因之一，因此早期诊断对于提高治疗效果和患者预后至关重要。计算机辅助诊断系统通过分析计算机断层扫描图像，在检测和分类肺结节方面表现出色，显著提高早期肺癌的检测率。虽然传统机器学习算法很有价值，但在处理复杂样本数据方面存在局限性。最近深度学习的出现彻底改变了医学图像分析，为这个领域带来了巨大的进步。这篇综述重点关注了深度学习在肺结节检测、分割和分类方面的最新进展。传统机器学习方法，如支持向量机和k近邻算法，已显示出其局限性，为卷积神经网络、循环神经网络和生成对抗网络等先进方法铺平了道路。还讨论了集成模型和新技术的一体化，强调了肺癌诊断领域的最新发展。深度学习算法结合各种分析技术，显著提高了肺结节分析的准确性和效率，超越了传统方法，特别是在结节分类方面。虽然仍存在挑战，但随着技术的不断进步，预计深度学习在医学诊断中的作用将得到进一步加强，特别是在早期肺癌检测和诊断方面。本工作所综述的肺癌检测模型的详细列表可在<a target="_blank" rel="noopener" href="https://github.com/CaiGuoHui123/Awesome-Lung-Cancer-Detection%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CaiGuoHui123/Awesome-Lung-Cancer-Detection上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14769v2">PDF</a> Accepted to Engineering Applications of Artificial Intelligence</p>
<p><strong>Summary</strong></p>
<p>本文介绍了肺癌仍是全球主要的疾病致死原因，计算机辅助诊断系统通过分析计算机断层扫描图像在检测早期肺癌方面表现出显著效果。传统机器学习算法在处理复杂样本数据时存在局限性，而深度学习在医学图像分析中的出现推动了该领域的重大进展。本文重点介绍了深度学习在肺结节检测、分割和分类方面的最新进展，以及集成模型和新技术在肺癌诊断中的应用。尽管仍有挑战，但技术的不断进步有望进一步加强深度学习在医学诊断中的作用，特别是在早期肺癌检测与诊断方面。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>肺癌仍是全球主要的健康威胁，早期诊断对改善治疗效果和患者预后至关重要。</li>
<li>计算机辅助诊断系统通过解析计算机断层扫描图像，在检测肺结节方面表现出显著效果，进而有助于早期肺癌的识别。</li>
<li>传统机器学习算法在处理复杂医学图像样本时存在局限性。</li>
<li>深度学习在医学图像分析领域带来革命性变化，推动肺结节检测、分割和分类的显著进展。</li>
<li>卷积神经网络、递归神经网络和生成对抗网络等先进深度学习技术应用于医学图像分析，提升诊断准确性。</li>
<li>集成模型和新技术在肺癌诊断中的最新发展受到关注。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14769">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-db06965bf7d7d5e9c93f36bc40073ec3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1aacd0b6aab50dae07859896952dc60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29e5862247db150ccd6210e984c1a512.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b71411a434517bb002f741b9f4525dd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89ff4f383db43870f6b2bc307f60c0ea.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Direct3γ-A-Pipeline-for-Direct-Three-gamma-PET-Image-Reconstruction"><a href="#Direct3γ-A-Pipeline-for-Direct-Three-gamma-PET-Image-Reconstruction" class="headerlink" title="Direct3γ: A Pipeline for Direct Three-gamma PET Image   Reconstruction"></a>Direct3γ: A Pipeline for Direct Three-gamma PET Image   Reconstruction</h2><p><strong>Authors:Youness Mellak, Alexandre Bousse, Thibaut Merlin, Debora Giovagnoli, Dimitris Visvikis</strong></p>
<p>This paper presents a novel image reconstruction pipeline for three-gamma (3-{\gamma}) positron emission tomography (PET) aimed at improving spatial resolution and reducing noise in nuclear medicine; the proposed Direct3{\gamma} pipeline addresses the inherent challenges in 3-{\gamma} PET systems, such as detector imperfections and uncertainty in photon interaction points, with a key feature being its ability to determine the order of interactions through a model trained on Monte Carlo (MC) simulations using the Geant4 Application for Tomography Emission (GATE) toolkit, thus providing the necessary information to construct Compton cones which intersect with the line of response (LOR) to estimate the emission point; the pipeline processes 3-{\gamma} PET raw data, reconstructs histoimages by propagating energy and spatial uncertainties along the LOR, and applies a 3-D convolutional neural network (CNN) to refine these intermediate images into high-quality reconstructions, further enhancing image quality through supervised learning and adversarial losses that preserve fine structural details; experimental results show that Direct3{\gamma} consistently outperforms conventional 200-ps time-of-flight (TOF) PET in terms of structural similarity index measure (SSIM) and peak signal-to-noise ratio (PSNR). </p>
<blockquote>
<p>本文提出了一种针对三伽马（3-γ）正电子发射断层扫描（PET）的新型图像重建流程，旨在提高核医学中的空间分辨率并降低噪声。所提出的Direct3γ流程解决了3-γ PET系统固有的挑战，如探测器的不完美性和光子交互点的不确定性。其主要功能是通过使用Geant4发射断层扫描应用程序（GATE）工具包进行蒙特卡洛（MC）模拟训练的模型来确定交互的顺序，从而提供构建交于响应线（LOR）的康普顿锥所必需的信息来估计发射点。该流程处理3-γ PET原始数据，通过沿LOR传播能量和空间不确定性来重建直方图像，并应用三维卷积神经网络（CNN）来将这些中间图像细化为高质量重建，通过保留精细结构细节的监督和对抗损失，进一步提高图像质量。实验结果表明，Direct3γ在结构相似性指数度量（SSIM）和峰值信噪比（PSNR）方面始终优于传统的200皮秒飞行时间（TOF）PET。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18337v7">PDF</a> 11 pages, 11 figures, 2 tables</p>
<p><strong>摘要</strong></p>
<p>本文介绍了一种针对三伽马（3-γ）正电子发射断层扫描（PET）的新型图像重建流程，旨在提高核医学中的空间分辨率并降低噪声。提出的Direct3γ流程解决了3-γ PET系统固有的挑战，如探测器缺陷和光子交互点的不确定性。其关键功能是通过使用Geant4发射断层扫描应用程序（GATE）工具包进行蒙特卡洛（MC）模拟训练模型来确定交互的顺序，从而提供构建与线性响应（LOR）相交的康普顿锥的必要信息，以估计发射点。该流程处理3-γ PET原始数据，通过沿LOR传播能量和空间不确定性来重建直方图像，并应用三维卷积神经网络（CNN）对这些中间图像进行精细化处理，生成高质量的重构图像。通过保留精细结构细节的监督学习和对抗性损失，进一步提高图像质量。实验结果表明，Direct3γ在结构相似性指数度量（SSIM）和峰值信噪比（PSNR）方面始终优于传统的200皮秒时间飞行（TOF）PET。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Direct3γ是一种针对三伽马（3-γ）PET的新型图像重建流程，旨在改善核医学中的空间分辨率和噪声问题。</li>
<li>该流程解决了3-γ PET系统的固有挑战，如探测器缺陷和光子交互点的不确定性。</li>
<li>Direct3γ通过训练模型确定光子交互的顺序，该模型使用GATE工具包进行蒙特卡洛模拟。</li>
<li>流程包括处理3-γ PET原始数据，重建直方图像，并通过CNN精细化处理生成高质量重构图像。</li>
<li>Direct3γ通过结合监督学习和对抗性损失，提高了图像质量并保留了精细结构细节。</li>
<li>实验结果表明，Direct3γ在结构相似性指数度量（SSIM）和峰值信噪比（PSNR）方面优于传统的时间飞行PET技术。</li>
<li>该流程有望为核医学中的图像重建提供更高的空间分辨率和更低的噪声水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18337">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a1154d9e286348dbfbd07f3f7c3b6d8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1931270b4ad292783e7b011a4495116.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e4395f32ce1e19efa528a2dbaf2e6f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf6181951dfb16932106658d26fa1577.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d3193083c913e5a2bbde118dd4d3bc2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Enhancing-Weakly-Supervised-3D-Medical-Image-Segmentation-through-Probabilistic-aware-Learning"><a href="#Enhancing-Weakly-Supervised-3D-Medical-Image-Segmentation-through-Probabilistic-aware-Learning" class="headerlink" title="Enhancing Weakly Supervised 3D Medical Image Segmentation through   Probabilistic-aware Learning"></a>Enhancing Weakly Supervised 3D Medical Image Segmentation through   Probabilistic-aware Learning</h2><p><strong>Authors:Runmin Jiang, Zhaoxin Fan, Junhao Wu, Lenghan Zhu, Xin Huang, Tianyang Wang, Heng Huang, Min Xu</strong></p>
<p>3D medical image segmentation is a challenging task with crucial implications for disease diagnosis and treatment planning. Recent advances in deep learning have significantly enhanced fully supervised medical image segmentation. However, this approach heavily relies on labor-intensive and time-consuming fully annotated ground-truth labels, particularly for 3D volumes. To overcome this limitation, we propose a novel probabilistic-aware weakly supervised learning pipeline, specifically designed for 3D medical imaging. Our pipeline integrates three innovative components: a Probability-based Pseudo Label Generation technique for synthesizing dense segmentation masks from sparse annotations, a Probabilistic Multi-head Self-Attention network for robust feature extraction within our Probabilistic Transformer Network, and a Probability-informed Segmentation Loss Function to enhance training with annotation confidence. Demonstrating significant advances, our approach not only rivals the performance of fully supervised methods but also surpasses existing weakly supervised methods in CT and MRI datasets, achieving up to 18.1% improvement in Dice scores for certain organs. The code is available at <a target="_blank" rel="noopener" href="https://github.com/runminjiang/PW4MedSeg">https://github.com/runminjiang/PW4MedSeg</a>. </p>
<blockquote>
<p>三维医学图像分割是一项具有挑战性的任务，对于疾病诊断和治疗计划具有重要意义。深度学习的最新进展极大地促进了完全监督的医学图像分割。然而，这种方法严重依赖于劳动密集型和耗时的大量真实标签，特别是对于三维体积数据。为了克服这一局限性，我们提出了一种新型的概率感知弱监督学习流程，专门用于三维医学影像。我们的流程集成了三个创新组件：一种基于概率的伪标签生成技术，用于从稀疏注释中合成密集分割掩膜；一个概率多头自注意力网络，用于在我们的概率变换网络中实现稳健的特征提取；以及一种基于概率的分割损失函数，利用注释置信度增强训练。通过展示显著进展，我们的方法不仅与全监督方法性能相当，而且在CT和MRI数据集上的表现超过了现有的弱监督方法，在某些器官的狄克得分上提高了高达18.1%。代码可在<a target="_blank" rel="noopener" href="https://github.com/runminjiang/PW4MedSeg%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/runminjiang/PW4MedSeg获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.02566v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种针对3D医学图像分割的新型概率感知弱监督学习管道，包括概率伪标签生成技术、概率多头自注意力网络和概率感知分割损失函数。该方法不仅与全监督方法性能相当，而且在CT和MRI数据集上超越了现有弱监督方法，某些器官的Dice得分提高了18.1%。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D医学图像分割是疾病诊断和治疗计划中的关键任务。</li>
<li>深度学习在全监督医学图像分割中取得了显著进展，但依赖大量手动标注数据。</li>
<li>本文提出了一种新型概率感知弱监督学习管道，用于3D医学图像分割。</li>
<li>管道包含概率伪标签生成技术、概率多头自注意力网络和概率感知分割损失函数。</li>
<li>所提方法性能与全监督方法相当，并在CT和MRI数据集上超越了现有弱监督方法。</li>
<li>方法的代码已公开，可在特定GitHub仓库中找到。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.02566">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-894b303d3859902e658cc319d83172e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-890edf6e66ee7d279b82231448e1ff45.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7ee744a958b32293531a11658bfc12aa.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-06-24  Joint Tensor-Train Parameterization for Efficient and Expressive   Low-Rank Adaptation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a799f04e6f9a1dee2d9c0fab87800e50.jpg" class="responsive-img" alt="牙齿修复">
                        
                        <span class="card-title">牙齿修复</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            牙齿修复 方向最新论文已更新，请持续关注 Update in 2025-06-24  Co-Seg++ Mutual Prompt-Guided Collaborative Learning for Versatile   Medical Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    牙齿修复
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">牙齿修复</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24231k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
