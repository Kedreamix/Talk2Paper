<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-24  Co-Seg++ Mutual Prompt-Guided Collaborative Learning for Versatile   Medical Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-f68155fc37015d37eb9f1f5515653bc2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    70 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-24-æ›´æ–°"><a href="#2025-06-24-æ›´æ–°" class="headerlink" title="2025-06-24 æ›´æ–°"></a>2025-06-24 æ›´æ–°</h1><h2 id="Co-Seg-Mutual-Prompt-Guided-Collaborative-Learning-for-Versatile-Medical-Segmentation"><a href="#Co-Seg-Mutual-Prompt-Guided-Collaborative-Learning-for-Versatile-Medical-Segmentation" class="headerlink" title="Co-Seg++: Mutual Prompt-Guided Collaborative Learning for Versatile   Medical Segmentation"></a>Co-Seg++: Mutual Prompt-Guided Collaborative Learning for Versatile   Medical Segmentation</h2><p><strong>Authors:Qing Xu, Yuxiang Luo, Wenting Duan, Zhen Chen</strong></p>
<p>Medical image analysis is critical yet challenged by the need of jointly segmenting organs or tissues, and numerous instances for anatomical structures and tumor microenvironment analysis. Existing studies typically formulated different segmentation tasks in isolation, which overlooks the fundamental interdependencies between these tasks, leading to suboptimal segmentation performance and insufficient medical image understanding. To address this issue, we propose a Co-Seg++ framework for versatile medical segmentation. Specifically, we introduce a novel co-segmentation paradigm, allowing semantic and instance segmentation tasks to mutually enhance each other. We first devise a spatio-temporal prompt encoder (STP-Encoder) to capture long-range spatial and temporal relationships between segmentation regions and image embeddings as prior spatial constraints. Moreover, we devise a multi-task collaborative decoder (MTC-Decoder) that leverages cross-guidance to strengthen the contextual consistency of both tasks, jointly computing semantic and instance segmentation masks. Extensive experiments on diverse CT and histopathology datasets demonstrate that the proposed Co-Seg++ outperforms state-of-the-arts in the semantic, instance, and panoptic segmentation of dental anatomical structures, histopathology tissues, and nuclei instances. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/xq141839/Co-Seg-Plus">https://github.com/xq141839/Co-Seg-Plus</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†æéå¸¸é‡è¦ï¼Œä½†é¢ä¸´ç€éœ€è¦è”åˆåˆ†å‰²å™¨å®˜æˆ–ç»„ç»‡ä»¥åŠå¤šä¸ªè§£å‰–ç»“æ„å’Œè‚¿ç˜¤å¾®ç¯å¢ƒåˆ†æçš„å®ä¾‹çš„æŒ‘æˆ˜ã€‚ç°æœ‰ç ”ç©¶é€šå¸¸å­¤ç«‹åœ°åˆ¶å®šä¸åŒçš„åˆ†å‰²ä»»åŠ¡ï¼Œè¿™å¿½ç•¥äº†è¿™äº›ä»»åŠ¡ä¹‹é—´çš„åŸºæœ¬ç›¸äº’ä¾èµ–æ€§ï¼Œå¯¼è‡´åˆ†å‰²æ€§èƒ½ä¸ä½³å’ŒåŒ»å­¦å›¾åƒç†è§£ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºé€šç”¨åŒ»å­¦åˆ†å‰²çš„Co-Seg++æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„ååŒåˆ†å‰²èŒƒå¼ï¼Œå…è®¸è¯­ä¹‰åˆ†å‰²å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ç›¸äº’å¢å¼ºã€‚æˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ç§æ—¶ç©ºæç¤ºç¼–ç å™¨ï¼ˆSTP-Encoderï¼‰ï¼Œä»¥æ•è·åˆ†å‰²åŒºåŸŸå’Œå›¾åƒåµŒå…¥ä¹‹é—´çš„è¿œç¨‹ç©ºé—´å’Œæ—¶é—´å…³ç³»ï¼Œä½œä¸ºå…ˆéªŒç©ºé—´çº¦æŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§å¤šä»»åŠ¡åä½œè§£ç å™¨ï¼ˆMTC-Decoderï¼‰ï¼Œå®ƒåˆ©ç”¨äº¤å‰æŒ‡å¯¼æ¥åŠ å¼ºä¸¤ä¸ªä»»åŠ¡ä¸Šä¸‹æ–‡çš„ä¸€è‡´æ€§ï¼Œè”åˆè®¡ç®—è¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²æ©è†œã€‚åœ¨å¤šç§CTå’Œç—…ç†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„Co-Seg++åœ¨ç‰™ç§‘è§£å‰–ç»“æ„ã€ç—…ç†ç»„ç»‡å’Œç»†èƒæ ¸å®ä¾‹çš„è¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²æ–¹é¢ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xq141839/Co-Seg-Plus%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xq141839/Co-Seg-Plusä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17159v1">PDF</a> Under Review</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†æé¢ä¸´å™¨å®˜æˆ–ç»„ç»‡è”åˆåˆ†å‰²ä»¥åŠè§£å‰–ç»“æ„ã€è‚¿ç˜¤å¾®ç¯å¢ƒåˆ†æçš„å¤šä¸ªå®ä¾‹éœ€æ±‚ã€‚ç°æœ‰ç ”ç©¶é€šå¸¸å­¤ç«‹åœ°åˆ¶å®šä¸åŒçš„åˆ†å‰²ä»»åŠ¡ï¼Œå¿½ç•¥äº†è¿™äº›ä»»åŠ¡ä¹‹é—´çš„æ ¹æœ¬ç›¸äº’ä¾èµ–æ€§ï¼Œå¯¼è‡´åˆ†å‰²æ€§èƒ½ä¸ä½³å’ŒåŒ»å­¦å›¾åƒç†è§£ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºCo-Seg++æ¡†æ¶è¿›è¡Œé€šç”¨åŒ»å­¦åˆ†å‰²ï¼Œå¼•å…¥ä¸€ç§æ–°çš„ååŒåˆ†å‰²èŒƒå¼ï¼Œä½¿è¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡èƒ½å¤Ÿç›¸äº’ä¿ƒè¿›ã€‚é‡‡ç”¨æ—¶ç©ºæç¤ºç¼–ç å™¨ï¼ˆSTP-Encoderï¼‰æ•æ‰åˆ†å‰²åŒºåŸŸå’Œå›¾åƒåµŒå…¥ä¹‹é—´çš„è¿œç¨‹ç©ºé—´å’Œæ—¶é—´å…³ç³»ï¼Œä½œä¸ºå…ˆéªŒç©ºé—´çº¦æŸã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¤šä»»åŠ¡åä½œè§£ç å™¨ï¼ˆMTC-Decoderï¼‰ï¼Œåˆ©ç”¨è·¨æŒ‡å¯¼å¢å¼ºä¸¤ä¸ªä»»åŠ¡ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼Œè”åˆè®¡ç®—è¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²æ©è†œã€‚åœ¨å¤šç§CTå’Œç—…ç†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCo-Seg++åœ¨ç‰™é½¿è§£å‰–ç»“æ„ã€ç—…ç†ç»„ç»‡ä»¥åŠç»†èƒæ ¸å®ä¾‹çš„è¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†æéœ€è¦åŒæ—¶å¤„ç†å™¨å®˜æˆ–ç»„ç»‡çš„è”åˆåˆ†å‰²ä»¥åŠå¤šä¸ªå®ä¾‹åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å­¤ç«‹å¤„ç†ä¸åŒåˆ†å‰²ä»»åŠ¡ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³å’ŒåŒ»å­¦å›¾åƒç†è§£å—é™ã€‚</li>
<li>æå‡ºCo-Seg++æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ååŒåˆ†å‰²èŒƒå¼è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>STP-Encoderç”¨äºæ•æ‰åˆ†å‰²åŒºåŸŸä¸å›¾åƒåµŒå…¥ä¹‹é—´çš„è¿œç¨‹ç©ºé—´å’Œæ—¶é—´å…³ç³»ã€‚</li>
<li>MTC-Decoderåˆ©ç”¨è·¨æŒ‡å¯¼å¢å¼ºè¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚</li>
<li>Co-Seg++åœ¨å¤šç§åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2cc30c8857a0ea0f90366965618b8fbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71f8435849b24e05172f9a79f7181cb9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-388513905ae60b3d74fa644011d80bf8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88e339f07eac77082d75d845cff878ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a799f04e6f9a1dee2d9c0fab87800e50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f13dc68f07911430b8cee7c48568e61.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Semi-Supervised-Multi-Modal-Medical-Image-Segmentation-for-Complex-Situations"><a href="#Semi-Supervised-Multi-Modal-Medical-Image-Segmentation-for-Complex-Situations" class="headerlink" title="Semi-Supervised Multi-Modal Medical Image Segmentation for Complex   Situations"></a>Semi-Supervised Multi-Modal Medical Image Segmentation for Complex   Situations</h2><p><strong>Authors:Dongdong Meng, Sheng Li, Hao Wu, Guoping Wang, Xueqing Yan</strong></p>
<p>Semi-supervised learning addresses the issue of limited annotations in medical images effectively, but its performance is often inadequate for complex backgrounds and challenging tasks. Multi-modal fusion methods can significantly improve the accuracy of medical image segmentation by providing complementary information. However, they face challenges in achieving significant improvements under semi-supervised conditions due to the challenge of effectively leveraging unlabeled data. There is a significant need to create an effective and reliable multi-modal learning strategy for leveraging unlabeled data in semi-supervised segmentation. To address these issues, we propose a novel semi-supervised multi-modal medical image segmentation approach, which leverages complementary multi-modal information to enhance performance with limited labeled data. Our approach employs a multi-stage multi-modal fusion and enhancement strategy to fully utilize complementary multi-modal information, while reducing feature discrepancies and enhancing feature sharing and alignment. Furthermore, we effectively introduce contrastive mutual learning to constrain prediction consistency across modalities, thereby facilitating the robustness of segmentation results in semi-supervised tasks. Experimental results on two multi-modal datasets demonstrate the superior performance and robustness of the proposed framework, establishing its valuable potential for solving medical image segmentation tasks in complex scenarios. </p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ æœ‰æ•ˆåœ°è§£å†³äº†åŒ»å­¦å›¾åƒæ ‡æ³¨æ•°æ®æœ‰é™çš„é—®é¢˜ï¼Œä½†åœ¨å¤æ‚èƒŒæ™¯å’Œå›°éš¾ä»»åŠ¡ä¸‹ï¼Œå…¶æ€§èƒ½å¾€å¾€ä¸è¶³ã€‚å¤šæ¨¡æ€èåˆæ–¹æ³•å¯ä»¥é€šè¿‡æä¾›äº’è¡¥ä¿¡æ¯æ¥æ˜¾è‘—æé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç”±äºæœ‰æ•ˆåˆ©ç”¨æ— æ ‡ç­¾æ•°æ®çš„æŒ‘æˆ˜ï¼Œå®ƒä»¬åœ¨åŠç›‘ç£æ¡ä»¶ä¸‹å®ç°æ˜¾è‘—æ”¹è¿›é¢ä¸´å›°éš¾ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦ä¸€ä¸ªæœ‰æ•ˆå¯é çš„å¤šæ¨¡æ€å­¦ä¹ ç­–ç•¥ï¼Œä»¥åœ¨åŠç›‘ç£åˆ†å‰²ä¸­åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹åŠç›‘ç£å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº’è¡¥çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œåœ¨æœ‰é™çš„æ ‡è®°æ•°æ®ä¸‹æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¤šé˜¶æ®µå¤šæ¨¡æ€èåˆå’Œå¢å¼ºç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨äº’è¡¥çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼ŒåŒæ—¶å‡å°‘ç‰¹å¾å·®å¼‚ï¼Œå¢å¼ºç‰¹å¾å…±äº«å’Œå¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°å¼•å…¥äº†å¯¹æ¯”äº’åŠ©å­¦ä¹ ï¼Œä»¥çº¦æŸè·¨æ¨¡æ€çš„é¢„æµ‹ä¸€è‡´æ€§ï¼Œä»è€Œä¿ƒè¿›åŠç›‘ç£ä»»åŠ¡ä¸­åˆ†å‰²ç»“æœçš„ç¨³å¥æ€§ã€‚åœ¨ä¸¤ä¸ªå¤šæ¨¡æ€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æ‰€ææ¡†æ¶çš„ä¼˜è¶Šæ€§èƒ½å’Œç¨³å¥æ€§ï¼Œä¸ºå¤æ‚åœºæ™¯ä¸‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡æä¾›äº†æœ‰ä»·å€¼çš„æ½œåŠ›è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17136v1">PDF</a> 10 pages, 2 figures, accepted at MICCAI 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåŠç›‘ç£å­¦ä¹ ä¸­å­˜åœ¨æ ‡æ³¨æ•°æ®æœ‰é™çš„é—®é¢˜ï¼ŒèƒŒæ™¯å’Œä»»åŠ¡å¤æ‚æ—¶æ€§èƒ½å¯èƒ½ä¸è¶³ã€‚å¤šæ¨¡æ€èåˆæ–¹æ³•é€šè¿‡æä¾›äº’è¡¥ä¿¡æ¯æé«˜äº†åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚ä½†åœ¨åŠç›‘ç£æ¡ä»¶ä¸‹åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŠç›‘ç£å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡èåˆå¤šæ¨¡æ€ä¿¡æ¯æé«˜æ€§èƒ½ï¼Œé‡‡ç”¨å¤šé˜¶æ®µå¤šæ¨¡æ€èåˆç­–ç•¥ï¼Œå‡å°‘ç‰¹å¾å·®å¼‚ï¼Œå¢å¼ºç‰¹å¾å…±äº«å’Œå¯¹é½ã€‚åŒæ—¶å¼•å…¥å¯¹æ¯”äº’å­¦ä¹ ï¼Œçº¦æŸæ¨¡æ€é—´é¢„æµ‹ä¸€è‡´æ€§ï¼Œæé«˜åŠç›‘ç£ä»»åŠ¡åˆ†å‰²ç»“æœçš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒä¸­è§£å†³äº†æ ‡æ³¨æ•°æ®æœ‰é™çš„é—®é¢˜ï¼Œä½†åœ¨å¤æ‚èƒŒæ™¯å’Œä»»åŠ¡ä¸‹æ€§èƒ½å¯èƒ½ä¸è¶³ã€‚</li>
<li>å¤šæ¨¡æ€èåˆæ–¹æ³•èƒ½æä¾›äº’è¡¥ä¿¡æ¯ï¼Œæé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚</li>
<li>åŠç›‘ç£æ¡ä»¶ä¸‹åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§åŠç›‘ç£å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œèåˆå¤šæ¨¡æ€ä¿¡æ¯æé«˜æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨å¤šé˜¶æ®µå¤šæ¨¡æ€èåˆç­–ç•¥ï¼Œå‡å°‘ç‰¹å¾å·®å¼‚ï¼Œå¢å¼ºç‰¹å¾å…±äº«å’Œå¯¹é½ã€‚</li>
<li>å¼•å…¥å¯¹æ¯”äº’å­¦ä¹ ï¼Œçº¦æŸæ¨¡æ€é—´é¢„æµ‹ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e82d9f9816f8517e274eed8ad91c6331.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63425df4582834cc6a9252ecf69f7d2c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Frequently-Used-References-For-Atomic-Data-In-X-ray-Spectroscopy"><a href="#Frequently-Used-References-For-Atomic-Data-In-X-ray-Spectroscopy" class="headerlink" title="Frequently Used References For Atomic Data In X-ray Spectroscopy"></a>Frequently Used References For Atomic Data In X-ray Spectroscopy</h2><p><strong>Authors:N. Hell, G. V. Brown, M. E. Eckart, A. J. Fairchild, C. A. Kilbourne, M. A. Leutenegger, F. S. Porter, M. C. Witthoeft</strong></p>
<p>Accurate atomic physics reference data are a crucial requirement for analysis and interpretation of observed spectra, even more so for observations with high spectral resolution. This document provides a curated list of atomic physics references frequently used for plasma diagnostics in X-ray spectroscopy, outside of comprehensive plasma models that typically come with their own underlying atomic databases. The list includes references to physical constants, laboratory benchmarks, transition energies, position and line shapes of neutral fluorescence lines, radiative branching ratios, and commonly used notation for prominent transitions. Quick-look tables for transition energies in H-, He-, and Li-like ions and line positions and shapes for fluorescence lines in neutrals. The main focus is on K-shell transitions. For the H- and He-like tables, we cite state-of-the art calculations that we consider currently the best available reference energies, which are considered high accuracy and thus typically used for energy scale calibration in laboratory measurements. Omissions in these tables are due to the lack of availability in the chosen references, and are not a statement about the relevance of these lines. Due to their complex and highly source-dependent line shape, the atomic data for neutrals is of lower accuracy than that for the highly charged ions, and the best reference data for these line shapes typically consist of empirical models derived from very high-resolution laboratory measurements. The table for neutrals provided here is consistent with the reference used for the energy gain scale calibration of XRISM&#x2F;Resolve. This document is meant to serve as a resource to help find relevant references and conveniently formatted overview tables. When making use of the information found in these papers, credit should be given to their original authors by citing the appropriate references. </p>
<blockquote>
<p>å‡†ç¡®çš„åŸå­ç‰©ç†å­¦å‚è€ƒæ•°æ®å¯¹äºåˆ†æå’Œè§£é‡Šè§‚å¯Ÿåˆ°çš„å…‰è°±è‡³å…³é‡è¦ï¼Œå¯¹äºé«˜å…‰è°±åˆ†è¾¨ç‡çš„è§‚å¯Ÿæ›´æ˜¯å¦‚æ­¤ã€‚æœ¬æ–‡æä¾›äº†ä¸€ä»½ç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„åŸå­ç‰©ç†å­¦å‚è€ƒæ–‡çŒ®åˆ—è¡¨ï¼Œè¿™äº›æ–‡çŒ®é€šå¸¸ç”¨äºXå°„çº¿å…‰è°±ä¸­çš„ç­‰ç¦»å­ä½“è¯Šæ–­ï¼Œå¹¶ä¸”ä¸åŒ…æ‹¬é€šå¸¸å¸¦æœ‰è‡ªå·±åŸºç¡€åŸå­æ•°æ®åº“çš„å…¨é¢ç­‰ç¦»å­ä½“æ¨¡å‹ã€‚åˆ—è¡¨åŒ…æ‹¬å¯¹ç‰©ç†å¸¸æ•°ã€å®éªŒå®¤åŸºå‡†ã€è·ƒè¿èƒ½ã€ä¸­æ€§è§å…‰çº¿çš„ä½ç½®å’Œçº¿å½¢ã€è¾å°„åˆ†æ”¯æ¯”ä»¥åŠçªå‡ºè·ƒè¿çš„å¸¸ç”¨ç¬¦å·çš„å¼•ç”¨ã€‚é’ˆå¯¹H-ã€He-å’ŒLi-ç±»ç¦»å­çš„è·ƒè¿èƒ½ä»¥åŠä¸­æ€§è§å…‰çº¿çš„çº¿ä½ç½®å’Œçº¿å½¢çš„å¿«é€ŸæŸ¥çœ‹è¡¨ï¼Œä¸»è¦å…³æ³¨Kå±‚è·ƒè¿ã€‚å¯¹äºH-å’ŒHe-ç±»è¡¨æ ¼ï¼Œæˆ‘ä»¬å¼•ç”¨äº†å½“å‰è®¤ä¸ºçš„æœ€ä½³å¯ç”¨å‚è€ƒèƒ½é‡å€¼ï¼Œè¿™äº›å€¼å…·æœ‰é«˜ç²¾åº¦ï¼Œé€šå¸¸ç”¨äºå®éªŒå®¤æµ‹é‡çš„èƒ½é‡æ ‡åº¦æ ¡å‡†ã€‚è¿™äº›è¡¨æ ¼ä¸­çš„é—æ¼æ˜¯ç”±äºæ‰€é€‰å‚è€ƒæ–‡çŒ®ä¸­ç¼ºå°‘ç›¸å…³ä¿¡æ¯ï¼Œå¹¶ä¸ä»£è¡¨è¿™äº›çº¿çš„ä¸é‡è¦æ€§ã€‚ç”±äºä¸­æ€§åŸå­æ•°æ®çš„çº¿å‹å¤æ‚ä¸”é«˜åº¦ä¾èµ–äºæºï¼Œå…¶ç²¾åº¦ä½äºé«˜åº¦å¸¦ç”µç¦»å­çš„ç²¾åº¦ï¼Œè€Œçº¿å‹æœ€ä½³çš„å‚è€ƒæ•°æ®é€šå¸¸ç”±æ¥è‡ªè¶…é«˜åˆ†è¾¨ç‡å®éªŒå®¤æµ‹é‡çš„ç»éªŒæ¨¡å‹æ„æˆã€‚æ­¤å¤„æä¾›çš„ä¸­æ€§è¡¨æ ¼ä¸XRISM&#x2F;Resolveçš„èƒ½é‡å¢ç›Šæ ‡åº¦æ ¡å‡†æ‰€å‚è€ƒçš„æ–‡çŒ®ä¸€è‡´ã€‚æœ¬æ–‡æ—¨åœ¨ä½œä¸ºå¸®åŠ©æŸ¥æ‰¾ç›¸å…³å‚è€ƒæ–‡çŒ®å’Œæ–¹ä¾¿æ ¼å¼åŒ–çš„æ¦‚è¿°è¡¨æ ¼çš„èµ„æºã€‚åœ¨ä½¿ç”¨è¿™äº›è®ºæ–‡ä¸­æ‰¾åˆ°çš„ä¿¡æ¯æ—¶ï¼Œåº”é€šè¿‡å¼•ç”¨é€‚å½“çš„å‚è€ƒæ–‡çŒ®å‘åŸå§‹ä½œè€…è‡´è°¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17106v1">PDF</a> 18 pages, 5 tables</p>
<p><strong>Summary</strong><br>     æ­¤æ–‡æœ¬æä¾›äº†ä¸€ç³»åˆ—é’ˆå¯¹Xå°„çº¿å…‰è°±ä¸­ç¦»å­å’Œä¸­æ€§åŸå­ç‰©ç†å‚æ•°çš„å‚è€ƒæ•°æ®åˆ—è¡¨ï¼ŒåŒ…æ‹¬ç‰©ç†å¸¸æ•°ã€å®éªŒå®¤åŸºå‡†å€¼ã€è¿‡æ¸¡èƒ½é‡ç­‰ã€‚æ•°æ®ä¸»è¦å…³æ³¨Kå±‚è·ƒè¿ï¼Œæ—¨åœ¨ä¸ºé«˜èƒ½è°±åˆ†ææä¾›å‡†ç¡®å‚è€ƒæ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æä¾›äº†ä¸€ç³»åˆ—é’ˆå¯¹Xå°„çº¿å…‰è°±çš„ç¦»å­å’Œä¸­æ€§åŸå­çš„åŸå­ç‰©ç†å‚è€ƒæ•°æ®ã€‚</li>
<li>è¿™äº›æ•°æ®å¯¹äºé«˜å…‰è°±åˆ†è¾¨ç‡çš„è§‚å¯Ÿè‡³å…³é‡è¦ï¼Œå¹¶åŒ…æ‹¬ç‰©ç†å¸¸æ•°ã€å®éªŒå®¤åŸºå‡†å€¼ã€è¿‡æ¸¡èƒ½é‡ç­‰å†…å®¹ã€‚</li>
<li>åˆ—è¡¨ä¸“æ³¨äºKå±‚è·ƒè¿ã€‚</li>
<li>æ•°æ®æ¥æºäºæœ€æ–°çš„é«˜ç²¾åº¦è®¡ç®—ï¼Œè¢«è®¤ä¸ºæ˜¯ç›®å‰æœ€ä½³å¯ç”¨çš„å‚è€ƒèƒ½é‡ã€‚</li>
<li>ä¸­æ€§åŸå­çš„æ•°æ®å‡†ç¡®æ€§è¾ƒä½ï¼Œä¸»è¦å› ä¸ºå®ƒä»¬çš„çº¿å½¢çŠ¶å¤æ‚ä¸”é«˜åº¦ä¾èµ–äºæºã€‚</li>
<li>æä¾›çš„æ•°æ®è¡¨ä¸XRISM&#x2F;Resolveçš„èƒ½é‡å¢ç›Šå°ºåº¦æ ¡å‡†å‚è€ƒä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e3b42e5366fae251af26b447cf1d2c2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Client-Selection-Strategies-for-Federated-Semantic-Communications-in-Heterogeneous-IoT-Networks"><a href="#Client-Selection-Strategies-for-Federated-Semantic-Communications-in-Heterogeneous-IoT-Networks" class="headerlink" title="Client Selection Strategies for Federated Semantic Communications in   Heterogeneous IoT Networks"></a>Client Selection Strategies for Federated Semantic Communications in   Heterogeneous IoT Networks</h2><p><strong>Authors:Samer Lahoud, Kinda Khawam</strong></p>
<p>The exponential growth of IoT devices presents critical challenges in bandwidth-constrained wireless networks, particularly regarding efficient data transmission and privacy preservation. This paper presents a novel federated semantic communication (SC) framework that enables collaborative training of bandwidth-efficient models for image reconstruction across heterogeneous IoT devices. By leveraging SC principles to transmit only semantic features, our approach dramatically reduces communication overhead while preserving reconstruction quality. We address the fundamental challenge of client selection in federated learning environments where devices exhibit significant disparities in dataset sizes and data distributions. Our framework implements three distinct client selection strategies that explore different trade-offs between system performance and fairness in resource allocation. The system employs an end-to-end SC architecture with semantic bottlenecks, coupled with a loss-based aggregation mechanism that naturally adapts to client heterogeneity. Experimental evaluation on image data demonstrates that while Utilitarian selection achieves the highest reconstruction quality, Proportional Fairness maintains competitive performance while significantly reducing participation inequality and improving computational efficiency. These results establish that federated SC can successfully balance reconstruction quality, resource efficiency, and fairness in heterogeneous IoT deployments, paving the way for sustainable and privacy-preserving edge intelligence applications. </p>
<blockquote>
<p>ç‰©è”ç½‘è®¾å¤‡çš„æŒ‡æ•°çº§å¢é•¿ç»™å¸¦å®½å—é™çš„æ— çº¿ç½‘ç»œå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜æ•ˆæ•°æ®ä¼ è¾“å’Œéšç§ä¿æŠ¤æ–¹é¢ã€‚æœ¬æ–‡é’ˆå¯¹è¿™ä¸€èƒŒæ™¯ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„è”é‚¦è¯­ä¹‰é€šä¿¡ï¼ˆSCï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å¼‚æ„ç‰©è”ç½‘è®¾å¤‡ä¹‹é—´å®ç°å¸¦å®½é«˜æ•ˆæ¨¡å‹çš„ååŒè®­ç»ƒï¼Œç”¨äºå›¾åƒé‡å»ºã€‚é€šè¿‡åˆ©ç”¨SCåŸç†ä»…ä¼ è¾“è¯­ä¹‰ç‰¹å¾ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ä¿æŒé‡å»ºè´¨é‡çš„åŒæ—¶ï¼Œå¤§å¹…é™ä½é€šä¿¡å¼€é”€ã€‚æˆ‘ä»¬è§£å†³äº†è”é‚¦å­¦ä¹ ç¯å¢ƒä¸­å®¢æˆ·é€‰æ‹©çš„æ ¹æœ¬æŒ‘æˆ˜ï¼Œåœ¨æ­¤ç¯å¢ƒä¸­ï¼Œè®¾å¤‡åœ¨æ•°æ®é›†å¤§å°å’Œæ•°æ®åˆ†å¸ƒæ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬çš„æ¡†æ¶å®ç°äº†ä¸‰ç§ä¸åŒçš„å®¢æˆ·é€‰æ‹©ç­–ç•¥ï¼Œåœ¨ç³»ç»Ÿæ€§èƒ½å’Œèµ„æºåˆ†é…çš„å…¬å¹³æ€§ä¹‹é—´è¿›è¡Œä¸åŒçš„æƒè¡¡ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨äº†ä¸€ç§ç«¯åˆ°ç«¯çš„SCæ¶æ„ï¼Œå¸¦æœ‰è¯­ä¹‰ç“¶é¢ˆï¼Œå¹¶ç»“åˆäº†ä¸€ç§åŸºäºæŸå¤±çš„èšåˆæœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿè‡ªç„¶åœ°é€‚åº”å®¢æˆ·ç«¯çš„å¼‚æ„æ€§ã€‚åœ¨å›¾åƒæ•°æ®ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œå°½ç®¡æ•ˆç”¨é€‰æ‹©è¾¾åˆ°äº†æœ€é«˜çš„é‡å»ºè´¨é‡ï¼Œä½†æ¯”ä¾‹å…¬å¹³ç­–ç•¥åœ¨ä¿æŒç«äº‰åŠ›çš„æƒ…å†µä¸‹æ˜¾è‘—å‡å°‘äº†å‚ä¸ä¸å¹³ç­‰å¹¶æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚è¿™äº›ç»“æœè¯æ˜äº†è”é‚¦è¯­ä¹‰é€šä¿¡èƒ½å¤Ÿåœ¨å¼‚æ„ç‰©è”ç½‘éƒ¨ç½²ä¸­æˆåŠŸå¹³è¡¡é‡å»ºè´¨é‡ã€èµ„æºæ•ˆç‡å’Œå…¬å¹³æ€§ï¼Œä¸ºå¯æŒç»­å’Œéšç§ä¿æŠ¤çš„è¾¹ç¼˜æ™ºèƒ½åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17063v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç‰©è”ç½‘è®¾å¤‡çš„æŒ‡æ•°çº§å¢é•¿ç»™å¸¦å®½å—é™çš„æ— çº¿ç½‘ç»œå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®é«˜æ•ˆä¼ è¾“å’Œéšç§ä¿æŠ¤æ–¹é¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è”é‚¦è¯­ä¹‰é€šä¿¡ï¼ˆSCï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å¼‚æ„ç‰©è”ç½‘è®¾å¤‡ä¸Šå®ç°å¸¦å®½é«˜æ•ˆæ¨¡å‹çš„ååŒè®­ç»ƒï¼Œç”¨äºå›¾åƒé‡å»ºã€‚é€šè¿‡åˆ©ç”¨SCåŸç†åªä¼ è¾“è¯­ä¹‰ç‰¹å¾ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé‡å»ºè´¨é‡çš„åŒæ—¶ï¼Œå¤§å¹…é™ä½äº†é€šä¿¡å¼€é”€ã€‚æœ¬æ–‡è¿˜è§£å†³äº†è”é‚¦å­¦ä¹ ç¯å¢ƒä¸­å®¢æˆ·é€‰æ‹©çš„æ ¹æœ¬æŒ‘æˆ˜ï¼Œè¯¥ç¯å¢ƒä¸­çš„è®¾å¤‡åœ¨æ•°æ®é›†å¤§å°å’Œæ•°æ®åˆ†å¸ƒä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¯¥æ¡†æ¶å®ç°äº†ä¸‰ç§ä¸åŒçš„å®¢æˆ·é€‰æ‹©ç­–ç•¥ï¼Œåœ¨ç³»ç»Ÿæ€§èƒ½å’Œèµ„æºåˆ†é…çš„å…¬å¹³æ€§ä¹‹é—´è¿›è¡Œä¸åŒçš„æƒè¡¡ã€‚ç³»ç»Ÿé‡‡ç”¨ç«¯åˆ°ç«¯çš„SCæ¶æ„ï¼Œé…åˆåŸºäºæŸå¤±çš„èšåˆæœºåˆ¶ï¼Œè‡ªç„¶é€‚åº”å®¢æˆ·å¼‚è´¨æ€§ã€‚åœ¨å›¾åƒæ•°æ®ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæ•ˆç”¨é€‰æ‹©ç­–ç•¥å®ç°äº†æœ€é«˜çš„é‡å»ºè´¨é‡ï¼Œè€Œæ¯”ä¾‹å…¬å¹³æ€§åˆ™åœ¨ä¿æŒç«äº‰æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†å‚ä¸ä¸å¹³ç­‰å¹¶æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè”é‚¦SCèƒ½å¤ŸæˆåŠŸå¹³è¡¡å¼‚æ„ç‰©è”ç½‘éƒ¨ç½²ä¸­çš„é‡å»ºè´¨é‡ã€èµ„æºæ•ˆç‡å’Œå…¬å¹³æ€§ï¼Œä¸ºå¯æŒç»­å’Œéšç§ä¿æŠ¤çš„è¾¹ç¼˜æ™ºèƒ½åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚</p>
<p> <strong>Key Takeaways</strong></p>
<ol>
<li>ç‰©è”ç½‘è®¾å¤‡çš„å¢é•¿ç»™å¸¦å®½å—é™çš„æ— çº¿ç½‘ç»œå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œéœ€è¦è§£å†³é«˜æ•ˆæ•°æ®ä¼ è¾“å’Œéšç§ä¿æŠ¤çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„è”é‚¦è¯­ä¹‰é€šä¿¡ï¼ˆSCï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å¼‚æ„ç‰©è”ç½‘è®¾å¤‡ä¸Šå®ç°å¸¦å®½é«˜æ•ˆçš„æ¨¡å‹ååŒè®­ç»ƒï¼Œç”¨äºå›¾åƒé‡å»ºã€‚</li>
<li>åˆ©ç”¨SCåŸç†åªä¼ è¾“è¯­ä¹‰ç‰¹å¾ï¼Œé™ä½é€šä¿¡å¼€é”€çš„åŒæ—¶ä¿æŒé‡å»ºè´¨é‡ã€‚</li>
<li>è§£å†³äº†è”é‚¦å­¦ä¹ ç¯å¢ƒä¸­å®¢æˆ·é€‰æ‹©çš„æŒ‘æˆ˜ï¼Œè¯¥ç¯å¢ƒä¸­çš„è®¾å¤‡åœ¨æ•°æ®é›†å¤§å°å’Œåˆ†å¸ƒä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>å®æ–½äº†ä¸‰ç§å®¢æˆ·é€‰æ‹©ç­–ç•¥ï¼Œåœ¨ç³»ç»Ÿæ€§èƒ½å’Œèµ„æºåˆ†é…çš„å…¬å¹³æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨ç«¯åˆ°ç«¯çš„SCæ¶æ„å’ŒåŸºäºæŸå¤±çš„èšåˆæœºåˆ¶ï¼Œä»¥é€‚åº”å®¢æˆ·å¼‚è´¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-73b6dccca58aada826d379c8429c44a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6438c2a2e9dabac108980a0bf336891.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3191a759b3f98f954fd29685d0260312.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-803de40b29e8a328fef2710935c5c09f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Directional-Dark-Field-for-Nanoscale-Full-Field-Transmission-X-Ray-Microscopy"><a href="#Directional-Dark-Field-for-Nanoscale-Full-Field-Transmission-X-Ray-Microscopy" class="headerlink" title="Directional Dark Field for Nanoscale Full-Field Transmission X-Ray   Microscopy"></a>Directional Dark Field for Nanoscale Full-Field Transmission X-Ray   Microscopy</h2><p><strong>Authors:Sami Wirtensohn, Silja Flenner, Dominik John, Peng Qi, Christian David, Julia Herzen, Kritika Singh, Gudrun Lotze, Imke Greving</strong></p>
<p>Dark-field X-ray imaging offers unique insights into material structures by visualizing X-ray scattering rather than attenuation, revealing features invisible to conventional imaging techniques. While established approaches like grating-based and speckle-based imaging have demonstrated the utility of dark-field contrast in medical diagnostics and materials science, these methods have been primarily limited to laboratory and micro-CT systems. Building on the recent demonstration of dark-field imaging at the nanoscale using transmission X-ray microscopy, we extend this technique to retrieve directional small-angle scattering information. By analyzing both a test object and human primary tooth enamel, we show that our transmission X-ray microscopy setup can successfully retrieve directional scattering information with minimal modifications of existing systems. This advancement expands the capabilities of nanoscale dark-field imaging, offering new opportunities for investigating structural properties in a wide range of scientific fields. </p>
<blockquote>
<p>æš—åœºXå°„çº¿æˆåƒé€šè¿‡å¯è§†åŒ–Xå°„çº¿çš„æ•£å°„è€Œéè¡°å‡ï¼Œä¸ºææ–™ç»“æ„æä¾›äº†ç‹¬ç‰¹çš„æ´å¯Ÿè§†è§’ï¼Œæ­ç¤ºå‡ºä¼ ç»ŸæˆåƒæŠ€æœ¯æ— æ³•è§‚å¯Ÿåˆ°çš„ç‰¹å¾ã€‚è™½ç„¶åŸºäºå…‰æ …å’ŒåŸºäºæ–‘ç‚¹æˆåƒç­‰ç°æœ‰æ–¹æ³•å·²ç»è¯æ˜äº†æš—åœºå¯¹æ¯”åœ¨åŒ»å­¦è¯Šæ–­å’Œææ–™ç§‘å­¦ä¸­çš„å®ç”¨æ€§ï¼Œä½†è¿™äº›æ–¹æ³•ä¸»è¦å±€é™äºå®éªŒå®¤å’Œæ˜¾å¾®CTç³»ç»Ÿã€‚åŸºäºæœ€è¿‘åœ¨é€å°„Xå°„çº¿æ˜¾å¾®é•œä¸‹çº³ç±³å°ºåº¦æš—åœºæˆåƒçš„å±•ç¤ºï¼Œæˆ‘ä»¬å°†è¯¥æŠ€æœ¯æ‰©å±•åˆ°è·å–å®šå‘å°è§’åº¦æ•£å°„ä¿¡æ¯ã€‚é€šè¿‡å¯¹æµ‹è¯•å¯¹è±¡å’Œäººä½“ä¹³ç‰™é‡‰è´¨çš„åˆ†æï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„é€å°„Xå°„çº¿æ˜¾å¾®é•œè£…ç½®èƒ½å¤ŸæˆåŠŸè·å–å®šå‘æ•£å°„ä¿¡æ¯ï¼Œä¸”åªéœ€å¯¹ç°æœ‰çš„ç³»ç»Ÿè¿›è¡Œæœ€å°çš„æ”¹é€ ã€‚è¿™ä¸€è¿›å±•æ‰©å¤§äº†çº³ç±³å°ºåº¦æš—åœºæˆåƒçš„åŠŸèƒ½ï¼Œä¸ºæ¢ç´¢ä¸€ç³»åˆ—ç§‘å­¦é¢†åŸŸçš„ç»“æ„ç‰¹æ€§æä¾›äº†æ–°çš„æœºä¼šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16998v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æš—åœºXå°„çº¿æˆåƒé€šè¿‡å¯è§†åŒ–Xå°„çº¿æ•£å°„è€Œéè¡°å‡ï¼Œä¸ºææ–™ç»“æ„æä¾›äº†ç‹¬ç‰¹è§è§£ï¼Œæš—åœºæˆåƒæŠ€æœ¯å·²åœ¨åŒ»å­¦è¯Šæ–­å’Œææ–™ç§‘å­¦ä¸­å±•ç°å‡ºå®ç”¨ä»·å€¼ï¼Œä½†ä¸»è¦å±€é™äºå®éªŒå®¤å’Œæ˜¾å¾®CTç³»ç»Ÿã€‚è¿‘æœŸï¼ŒåŸºäºé€å°„Xå°„çº¿æ˜¾å¾®é•œçš„çº³ç±³å°ºåº¦æš—åœºæˆåƒæŠ€æœ¯å±•ç¤ºï¼Œå¯è·å–æ–¹å‘æ€§å°è§’åº¦æ•£å°„ä¿¡æ¯ã€‚é€šè¿‡å¯¹æµ‹è¯•å¯¹è±¡å’Œäººç±»ä¹³ç‰™é‡‰è´¨çš„å®éªŒåˆ†æï¼Œè¯æ˜æ”¹é€ ç°æœ‰ç³»ç»Ÿåèƒ½å¤ŸæˆåŠŸè·å–æ–¹å‘æ€§æ•£å°„ä¿¡æ¯ï¼Œè¿™ä¸€è¿›å±•æ‰©å¤§äº†çº³ç±³å°ºåº¦æš—åœºæˆåƒçš„èƒ½åŠ›ï¼Œä¸ºæ¢ç©¶å¹¿æ³›ç§‘å­¦é¢†åŸŸçš„ç»“æ„ç‰¹æ€§æä¾›äº†æ–°çš„æœºé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æš—åœºXå°„çº¿æˆåƒæŠ€æœ¯å¯è§†åŒ–Xå°„çº¿æ•£å°„ï¼Œä¸ºææ–™ç»“æ„æä¾›ç‹¬ç‰¹è§è§£ã€‚</li>
<li>ä¼ ç»ŸæˆåƒæŠ€æœ¯æ— æ³•è§‚å¯Ÿåˆ°çš„ç‰¹æ€§å¯é€šè¿‡æš—åœºæˆåƒæ­ç¤ºã€‚</li>
<li>æš—åœºæˆåƒæŠ€æœ¯åœ¨åŒ»å­¦è¯Šæ–­å’Œææ–™ç§‘å­¦ä¸­å…·å¤‡å®ç”¨ä»·å€¼ã€‚</li>
<li>è¯¥æŠ€æœ¯ä¸»è¦å±€é™äºå®éªŒå®¤å’Œæ˜¾å¾®CTç³»ç»Ÿã€‚</li>
<li>é€šè¿‡é€å°„Xå°„çº¿æ˜¾å¾®é•œå®ç°çš„çº³ç±³å°ºåº¦æš—åœºæˆåƒæŠ€æœ¯èƒ½å¤Ÿè·å–æ–¹å‘æ€§å°è§’åº¦æ•£å°„ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡æµ‹è¯•å¯¹è±¡å’Œäººç±»ä¹³ç‰™é‡‰è´¨çš„å®éªŒåˆ†æè¯æ˜äº†æŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8fda964b1ed8a50c0a82d15a56a89e01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d1c869a69ba095e5c5630114dc0b8d4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TextBraTS-Text-Guided-Volumetric-Brain-Tumor-Segmentation-with-Innovative-Dataset-Development-and-Fusion-Module-Exploration"><a href="#TextBraTS-Text-Guided-Volumetric-Brain-Tumor-Segmentation-with-Innovative-Dataset-Development-and-Fusion-Module-Exploration" class="headerlink" title="TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with   Innovative Dataset Development and Fusion Module Exploration"></a>TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with   Innovative Dataset Development and Fusion Module Exploration</h2><p><strong>Authors:Xiaoyu Shi, Rahul Kumar Jain, Yinhao Li, Ruibo Hou, Jingliang Cheng, Jie Bai, Guohua Zhao, Lanfen Lin, Rui Xu, Yen-wei Chen</strong></p>
<p>Deep learning has demonstrated remarkable success in medical image segmentation and computer-aided diagnosis. In particular, numerous advanced methods have achieved state-of-the-art performance in brain tumor segmentation from MRI scans. While recent studies in other medical imaging domains have revealed that integrating textual reports with visual data can enhance segmentation accuracy, the field of brain tumor analysis lacks a comprehensive dataset that combines radiological images with corresponding textual annotations. This limitation has hindered the exploration of multimodal approaches that leverage both imaging and textual data.   To bridge this critical gap, we introduce the TextBraTS dataset, the first publicly available volume-level multimodal dataset that contains paired MRI volumes and rich textual annotations, derived from the widely adopted BraTS2020 benchmark. Building upon this novel dataset, we propose a novel baseline framework and sequential cross-attention method for text-guided volumetric medical image segmentation. Through extensive experiments with various text-image fusion strategies and templated text formulations, our approach demonstrates significant improvements in brain tumor segmentation accuracy, offering valuable insights into effective multimodal integration techniques.   Our dataset, implementation code, and pre-trained models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Jupitern52/TextBraTS">https://github.com/Jupitern52/TextBraTS</a>. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²å’Œè®¡ç®—æœºè¾…åŠ©è¯Šæ–­æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç‰¹åˆ«æ˜¯ï¼Œè®¸å¤šå…ˆè¿›çš„æ–¹æ³•åœ¨MRIæ‰«æçš„è„‘è‚¿ç˜¤åˆ†å‰²æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ã€‚è™½ç„¶æœ€è¿‘åœ¨å…¶ä»–åŒ»å­¦æˆåƒé¢†åŸŸçš„ç ”ç©¶è¡¨æ˜ï¼Œå°†æ–‡æœ¬æŠ¥å‘Šä¸è§†è§‰æ•°æ®ç›¸ç»“åˆå¯ä»¥æé«˜åˆ†å‰²ç²¾åº¦ï¼Œä½†è„‘è‚¿ç˜¤åˆ†æé¢†åŸŸç¼ºä¹ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç»“åˆäº†æ”¾å°„å›¾åƒå’Œç›¸åº”çš„æ–‡æœ¬æ³¨é‡Šã€‚è¿™ä¸€å±€é™æ€§é˜»ç¢äº†åˆ©ç”¨æˆåƒå’Œæ–‡æœ¬æ•°æ®çš„å¤šæ¨¡å¼æ–¹æ³•çš„æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å…³é”®å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TextBraTSæ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„ä½“ç§¯çº§å¤šæ¨¡å¼æ•°æ®é›†ï¼ŒåŒ…å«é…å¯¹çš„MRIä½“ç§¯å’Œä¸°å¯Œçš„æ–‡æœ¬æ³¨é‡Šï¼Œè¿™äº›æ³¨é‡Šæ¥æºäºå¹¿æ³›é‡‡ç”¨çš„BraTS2020åŸºå‡†æµ‹è¯•ã€‚åŸºäºè¿™ä¸ªæ–°é¢–çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºçº¿æ¡†æ¶å’Œé¡ºåºäº¤å‰æ³¨æ„æ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬å¼•å¯¼çš„ä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚é€šè¿‡é‡‡ç”¨å„ç§æ–‡æœ¬å›¾åƒèåˆç­–ç•¥å’Œæ¨¡æ¿æ–‡æœ¬å…¬å¼çš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è„‘è‚¿ç˜¤åˆ†å‰²ç²¾åº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä¸ºæœ‰æ•ˆçš„å¤šæ¨¡å¼é›†æˆæŠ€æœ¯æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€å®ç°ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/Jupitern52/TextBraTS%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Jupitern52/TextBraTSå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16784v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²å’Œè®¡ç®—æœºè¾…åŠ©è¯Šæ–­æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚é’ˆå¯¹è„‘éƒ¨è‚¿ç˜¤åˆ†å‰²çš„MRIæ‰«æï¼Œå…ˆè¿›çš„æ–¹æ³•å·²è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚å°½ç®¡å…¶ä»–åŒ»å­¦å½±åƒé¢†åŸŸçš„ç ”ç©¶è¡¨æ˜ï¼Œå°†æ–‡æœ¬æŠ¥å‘Šä¸è§†è§‰æ•°æ®ç›¸ç»“åˆå¯ä»¥æé«˜åˆ†å‰²ç²¾åº¦ï¼Œä½†è„‘è‚¿ç˜¤åˆ†æé¢†åŸŸç¼ºä¹ä¸€ä¸ªç»“åˆæ”¾å°„å›¾åƒå’Œç›¸åº”æ–‡æœ¬æ³¨é‡Šçš„ç»¼åˆæ•°æ®é›†ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€å…³é”®å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TextBraTSæ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„ä½“ç§¯çº§å¤šæ¨¡å¼æ•°æ®é›†ï¼ŒåŒ…å«é…å¯¹çš„MRIä½“ç§¯å’Œä¸°å¯Œçš„æ–‡æœ¬æ³¨é‡Šï¼Œæ¥æºäºå¹¿æ³›é‡‡ç”¨çš„BraTS2020åŸºå‡†æµ‹è¯•ã€‚åŸºäºæ­¤æ–°å‹æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºçº¿æ¡†æ¶å’Œé¡ºåºäº¤å‰æ³¨æ„æ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬å¼•å¯¼çš„ä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚é€šè¿‡é‡‡ç”¨å„ç§æ–‡æœ¬å›¾åƒèåˆç­–ç•¥å’Œæ¨¡æ¿æ–‡æœ¬åˆ¶å®šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è„‘è‚¿ç˜¤åˆ†å‰²ç²¾åº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—æé«˜ï¼Œä¸ºæœ‰æ•ˆçš„å¤šæ¨¡å¼é›†æˆæŠ€æœ¯æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€å®æ–½ä»£ç å’Œé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹å¯åœ¨å…¬å¼€æ¸ é“è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Jupitern52/TextBraTS">https://github.com/Jupitern52/TextBraTS</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²å’Œè®¡ç®—æœºè¾…åŠ©è¯Šæ–­ä¸­è¡¨ç°å“è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨è„‘éƒ¨è‚¿ç˜¤MRIæ‰«æåˆ†å‰²æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
<li>å°½ç®¡ç»“åˆæ–‡æœ¬æŠ¥å‘Šå’Œè§†è§‰æ•°æ®åœ¨å…¶ä»–åŒ»å­¦å½±åƒé¢†åŸŸå·²æé«˜åˆ†å‰²ç²¾åº¦ï¼Œä½†è„‘è‚¿ç˜¤åˆ†æé¢†åŸŸç¼ºä¹ç»¼åˆæ•°æ®é›†ã€‚</li>
<li>TextBraTSæ•°æ®é›†æ˜¯é¦–ä¸ªå…¬å¼€å¯ç”¨çš„ä½“ç§¯çº§å¤šæ¨¡å¼æ•°æ®é›†ï¼ŒåŒ…å«MRIä½“ç§¯å’Œä¸°å¯Œçš„æ–‡æœ¬æ³¨é‡Šã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºçº¿æ¡†æ¶å’Œé¡ºåºäº¤å‰æ³¨æ„æ–¹æ³•ï¼Œåˆ©ç”¨TextBraTSæ•°æ®é›†è¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„åŒ»å­¦å›¾åƒä½“ç§¯åˆ†å‰²ã€‚</li>
<li>é€šè¿‡ä¸åŒçš„æ–‡æœ¬-å›¾åƒèåˆç­–ç•¥å’Œæ¨¡æ¿æ–‡æœ¬åˆ¶å®šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œæ˜¾è‘—æé«˜è„‘è‚¿ç˜¤åˆ†å‰²ç²¾åº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce7ff2a33473e7e0ed6301c63d898276.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-335d38880db4eebf6ff2c31ee3e11e3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e03c812650b7f83e8e7f2d20cb426582.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9547ca13d079db59a6b535c340c9e1c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2888a8e27ec14b490d056858f588275d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Prior-Guided-Joint-Diffusion-Model-in-Projection-Domain-for-PET-Tracer-Conversion"><a href="#A-Prior-Guided-Joint-Diffusion-Model-in-Projection-Domain-for-PET-Tracer-Conversion" class="headerlink" title="A Prior-Guided Joint Diffusion Model in Projection Domain for PET Tracer   Conversion"></a>A Prior-Guided Joint Diffusion Model in Projection Domain for PET Tracer   Conversion</h2><p><strong>Authors:Fang Chen, Weifeng Zhang, Xingyu Ai, BingXuan Li, An Li, Qiegen Liu</strong></p>
<p>Positron emission tomography (PET) is widely used to assess metabolic activity, but its application is limited by the availability of radiotracers. 18F-labeled fluorodeoxyglucose (18F-FDG) is the most commonly used tracer but shows limited effectiveness for certain tumors. In contrast, 6-18F-fluoro-3,4-dihydroxy-L-phenylalanine (18F-DOPA) offers higher specificity for neuroendocrine tumors and neurological disorders. However, its complex synthesis and limitations in transportation and clinical use hinder widespread adoption. During PET imaging, the sinogram represents a form of raw data acquired by the scanner. Therefore, modeling in projection domain enables more direct utilization of the original information, potentially reducing the accumulation of errors introduced during the image reconstruction process. Inspired by these factors, this study proposes a prior-guided joint diffusion model (PJDM) for transforming 18F-FDG PET images into 18F-DOPA PET images in projection domain. Specifically, a coarse estimation model and a prior refinement model are trained independently. During inference, an initial synthetic 18F-DOPA PET sinogram is generated using a higher-order hybrid sampler. This sinogram is then degraded and serves as an additional condition to guide the iterative refinement process using learned prior. Experimental results demonstrated that PJDM effectively improved both sinogram quality and synthetic outcomes. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/yqx7150/PJDM">https://github.com/yqx7150/PJDM</a>. </p>
<blockquote>
<p>æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å¹¿æ³›ç”¨äºè¯„ä¼°ä»£è°¢æ´»åŠ¨ï¼Œä½†å…¶åº”ç”¨å—é™äºæ”¾å°„æ€§ç¤ºè¸ªå‰‚çš„å¯ç”¨æ€§ã€‚18Fæ ‡è®°çš„æ°Ÿè„±æ°§è‘¡è„ç³–ï¼ˆ18F-FDGï¼‰æ˜¯æœ€å¸¸ç”¨çš„ç¤ºè¸ªå‰‚ï¼Œä½†å¯¹äºæŸäº›è‚¿ç˜¤ï¼Œå…¶æ•ˆæœæœ‰é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ6-18F-æ°Ÿ-3,4-äºŒç¾ŸåŸº-L-è‹¯ä¸™æ°¨é…¸ï¼ˆ18F-DOPAï¼‰å¯¹ç¥ç»å†…åˆ†æ³Œè‚¿ç˜¤å’Œç¥ç»ç–¾ç—…å…·æœ‰æ›´é«˜çš„ç‰¹å¼‚æ€§ã€‚ç„¶è€Œï¼Œå…¶å¤æ‚çš„åˆæˆä»¥åŠè¿è¾“å’Œä¸´åºŠä½¿ç”¨çš„é™åˆ¶é˜»ç¢äº†å…¶å¹¿æ³›åº”ç”¨ã€‚åœ¨PETæˆåƒä¸­ï¼Œè¾›å›¾ï¼ˆSinogramï¼‰æ˜¯ç”±æ‰«æä»ªè·å–çš„ä¸€ç§åŸå§‹æ•°æ®å½¢å¼ã€‚å› æ­¤ï¼Œåœ¨æŠ•å½±åŸŸä¸­è¿›è¡Œå»ºæ¨¡èƒ½å¤Ÿæ›´ç›´æ¥åœ°åˆ©ç”¨åŸå§‹ä¿¡æ¯ï¼Œå¯èƒ½å‡å°‘åœ¨å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­å¼•å…¥çš„è¯¯å·®ç§¯ç´¯ã€‚å—è¿™äº›å› ç´ å¯å‘ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å…ˆéªŒå¼•å¯¼è”åˆæ‰©æ•£æ¨¡å‹ï¼ˆPJDMï¼‰ï¼Œç”¨äºåœ¨æŠ•å½±åŸŸä¸­å°†18F-FDG PETå›¾åƒè½¬æ¢ä¸º18F-DOPA PETå›¾åƒã€‚å…·ä½“è€Œè¨€ï¼Œåˆ†åˆ«è®­ç»ƒäº†ç²—ç•¥ä¼°è®¡æ¨¡å‹å’Œå…ˆéªŒç»†åŒ–æ¨¡å‹ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨é«˜é˜¶æ··åˆé‡‡æ ·å™¨ç”Ÿæˆåˆå§‹åˆæˆ18F-DOPA PETè¾›å›¾ã€‚ç„¶åï¼Œå°†æ­¤è¾›å›¾é€€åŒ–å¹¶ä½œä¸ºé™„åŠ æ¡ä»¶ï¼Œå¼•å¯¼ä½¿ç”¨å­¦ä¹ å…ˆéªŒçš„è¿­ä»£ç»†åŒ–è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPJDMæœ‰æ•ˆæé«˜è¾›å›¾è´¨é‡å’Œåˆæˆç»“æœã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/yqx7150/PJDM">https://github.com/yqx7150/PJDM</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16733v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå…ˆéªŒå¼•å¯¼è”åˆæ‰©æ•£æ¨¡å‹ï¼ˆPJDMï¼‰å°†PETä¸­çš„Â¹â¸F-FDGå›¾åƒè½¬åŒ–ä¸ºÂ¹â¸F-DOPAå›¾åƒçš„æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯åœ¨æŠ•å½±åŸŸè¿›è¡Œå»ºæ¨¡ï¼Œé€šè¿‡è®­ç»ƒç²—ä¼°è®¡æ¨¡å‹å’Œå…ˆéªŒä¼˜åŒ–æ¨¡å‹ï¼Œåœ¨æ¨ç†é˜¶æ®µç”Ÿæˆåˆå§‹çš„Â¹â¸F-DOPA PETæ­£å¼¦å›¾ï¼Œå¹¶å¯¹å…¶è¿›è¡Œé€€åŒ–å¤„ç†ä»¥æŒ‡å¯¼è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPJDMå¯æœ‰æ•ˆæé«˜æ­£å¼¦å›¾è´¨é‡å’Œåˆæˆæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PETå¸¸ç”¨äºè¯„ä¼°ä»£è°¢æ´»åŠ¨ï¼Œä½†å—é™äºæ”¾å°„ç¤ºè¸ªå‰‚çš„å¯è·å¾—æ€§ã€‚</li>
<li>Â¹â¸F-FDGæ˜¯æœ€å¸¸ç”¨çš„ç¤ºè¸ªå‰‚ï¼Œä½†å¯¹æŸäº›è‚¿ç˜¤çš„æ•ˆç”¨æœ‰é™ã€‚</li>
<li>Â¹â¸F-DOPAå¯¹ç¥ç»å†…åˆ†æ³Œè‚¿ç˜¤å’Œç¥ç»éšœç¢å…·æœ‰æ›´é«˜çš„ç‰¹å¼‚æ€§ï¼Œä½†å…¶åˆæˆå¤æ‚ä¸”è¿è¾“å’Œä½¿ç”¨å—é™ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå…ˆéªŒå¼•å¯¼çš„è”åˆæ‰©æ•£æ¨¡å‹ï¼ˆPJDMï¼‰æ¥è½¬æ¢PETå›¾åƒçš„æŠ€æœ¯ã€‚</li>
<li>è¯¥æŠ€æœ¯åœ¨æŠ•å½±åŸŸå»ºæ¨¡ï¼Œç›´æ¥åˆ©ç”¨åŸå§‹ä¿¡æ¯ï¼Œå‡å°‘å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­çš„è¯¯å·®ç§¯ç´¯ã€‚</li>
<li>ä½¿ç”¨ç²—ä¼°è®¡æ¨¡å‹å’Œå…ˆéªŒä¼˜åŒ–æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œç”Ÿæˆåˆå§‹çš„Â¹â¸F-DOPA PETæ­£å¼¦å›¾å¹¶é€šè¿‡è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹æé«˜å…¶è´¨é‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd4ad6f5263fcfae5ff611831070f3af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f467edd818b7e69cd92668d0b5e6e1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5eb8065a429e026c7e409ba6e376fdbe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9db4b59dce8adc38173c6de63f6642f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-696b7d61e16c7c55d077c850bef9e735.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b2edd5814bd470f7dd08d8d3d41ea60.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="The-Vela-pulsar-and-its-pulsar-wind-nebula-Vela-X-using-13-years-of-Fermi-LAT-Observations"><a href="#The-Vela-pulsar-and-its-pulsar-wind-nebula-Vela-X-using-13-years-of-Fermi-LAT-Observations" class="headerlink" title="The Vela pulsar and its pulsar wind nebula Vela-X using 13 years of   Fermi-LAT Observations"></a>The Vela pulsar and its pulsar wind nebula Vela-X using 13 years of   Fermi-LAT Observations</h2><p><strong>Authors:Alexander Lange, J. Eagle, O. Kargaltsev, Lucien Kuiper, Jeremy Hare</strong></p>
<p>We present results of more than 13 years of Fermi-LAT data analysis for the Vela pulsar from 60 MeV to 100 GeV and its pulsar wind nebula (PWN), Vela-X, for E &gt; 1 GeV in the off-pulse phases. We find the Vela-X PWN can be best characterized using two extended components: a large radial Gaussian accompanied by an off-set, compact radial disk, both with a similar spectral index, \Gamma \sim 2.3. The common spectral properties support a common PWN origin, but a supernova remnant component is plausible for the compact radial disk. With an updated Vela-X model, the phase resolved spectral properties of the Vela pulsar are explored through a phase-resolved analysis. The phase-resolved spectral properties of the pulsar are presented, such as the SED peak energy E$_p$, the width of the SED at its peak, d$_p$, and the asymptotic (low-energy) spectral index, $\Gamma_0$, are presented. The best-fit spectral models for each LAT pulse peak (Peak 1 and Peak 2) are extrapolated to UV energies and compared to archival, phase-resolved spectra at UV, X-ray, soft \gamma-ray and TeV energies. We also discuss the physical implications of our modeling and the data comparisons. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹Velaè„‰å†²æ˜Ÿå’Œå…¶è„‰å†²é£æ˜Ÿäº‘ï¼ˆPWNï¼‰Vela-Xè¿›è¡Œäº†è¶…è¿‡13å¹´çš„è´¹ç±³-LATæ•°æ®åˆ†æï¼Œèƒ½é‡èŒƒå›´ä»60MeVåˆ°100GeVï¼Œå¹¶é’ˆå¯¹éè„‰å†²é˜¶æ®µçš„E &gt; 1 GeVè¿›è¡Œäº†è®¨è®ºã€‚æˆ‘ä»¬å‘ç°Vela-Xçš„PWNæœ€å¥½ç”¨ä¸¤ä¸ªæ‰©å±•æˆåˆ†æ¥æè¿°ï¼šä¸€ä¸ªå¤§çš„å¾„å‘é«˜æ–¯åˆ†å¸ƒï¼Œä¼´éšç€ä¸€ä¸ªåç§»çš„ç´§å‡‘å¾„å‘ç›˜ï¼Œå®ƒä»¬çš„è°±æŒ‡æ•°ç›¸ä¼¼ï¼ŒGammaçº¦ä¸º2.3ã€‚ç›¸åŒçš„è°±ç‰¹æ€§æ”¯æŒå®ƒä»¬æ¥æºäºåŒä¸€ä¸ªPWNï¼Œä½†å¯¹äºç´§å‡‘å¾„å‘ç›˜æ¥è¯´ï¼Œè¶…æ–°æ˜Ÿé—è¿¹æˆåˆ†ä¹Ÿæ˜¯å¯èƒ½çš„ã€‚é€šè¿‡æ›´æ–°çš„Vela-Xæ¨¡å‹ï¼Œæˆ‘ä»¬å¯¹Velaè„‰å†²æ˜Ÿçš„ç›¸ä½è§£æè°±ç‰¹æ€§è¿›è¡Œäº†æ¢ç©¶ã€‚å±•ç¤ºäº†è„‰å†²æ˜Ÿçš„ç›¸ä½è§£æè°±ç‰¹æ€§ï¼Œå¦‚SEDå³°å€¼èƒ½é‡Epã€SEDå³°å€¼çš„å®½åº¦dpã€æ¸è¿‘ï¼ˆä½èƒ½ï¼‰è°±æŒ‡æ•°Î“0ç­‰ã€‚å°†æ¯ä¸ªLATè„‰å†²å³°å€¼ï¼ˆPeak 1å’ŒPeak 2ï¼‰çš„æœ€ä½³æ‹Ÿåˆè°±æ¨¡å‹å¤–æ¨åˆ°ç´«å¤–èƒ½é‡ï¼Œå¹¶ä¸æ¡£æ¡ˆä¸­çš„ç´«å¤–ã€Xå°„çº¿ã€è½¯Î³å°„çº¿å’ŒTeVèƒ½é‡çš„ç›¸ä½è§£æå…‰è°±è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†æˆ‘ä»¬çš„å»ºæ¨¡å’Œæ•°æ®æ¯”è¾ƒçš„ç‰©ç†æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16687v1">PDF</a> Accepted for publication in the Astrophysical Journal. 24 pages, 15   figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æŠ¥é“äº†å¯¹è¶…è¿‡13å¹´çš„è´¹ç±³LATæ•°æ®åˆ†æç»“æœï¼Œç ”ç©¶äº†Velaè„‰å†²æ˜Ÿå’Œå…¶è„‰å†²é£æ˜Ÿäº‘Vela-Xåœ¨60MeVè‡³100GeVçš„èƒ½é‡èŒƒå›´å†…çš„è¡¨ç°ã€‚å‘ç°Vela-Xçš„æœ€ä½³ç‰¹å¾æ˜¯ç”±ä¸¤ä¸ªæ‰©å±•æˆåˆ†ç»„æˆï¼šä¸€ä¸ªå¤§çš„å¾„å‘é«˜æ–¯åˆ†å¸ƒå’Œä¸€ä¸ªåç§»çš„ç´§å‡‘å¾„å‘åœ†ç›˜ï¼Œä¸¤è€…å…·æœ‰ç›¸ä¼¼çš„å…‰è°±æŒ‡æ•°Î“â‰ˆ2.3ã€‚æ›´æ–°çš„Vela-Xæ¨¡å‹è¢«ç”¨äºæ¢ç´¢Velaè„‰å†²æ˜Ÿçš„ç›¸ä½è§£æå…‰è°±ç‰¹æ€§ã€‚æœ¬æ–‡è¿˜æ¢è®¨äº†æ¨¡å‹çš„ç‰©ç†å«ä¹‰å’Œæ•°æ®æ¯”è¾ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŠ¥é“äº†å¯¹Velaè„‰å†²æ˜Ÿè¶…è¿‡13å¹´çš„è´¹ç±³LATæ•°æ®åˆ†æç»“æœã€‚</li>
<li>ç ”ç©¶äº†Velaè„‰å†²æ˜Ÿå’Œå…¶è„‰å†²é£æ˜Ÿäº‘Vela-Xåœ¨ç‰¹å®šèƒ½é‡èŒƒå›´å†…çš„è¡¨ç°ã€‚</li>
<li>Vela-Xå¯ä»¥è¢«æœ€å¥½åœ°æè¿°ä¸ºä¸¤ä¸ªæ‰©å±•æˆåˆ†ï¼šä¸€ä¸ªå¤§çš„å¾„å‘é«˜æ–¯åˆ†å¸ƒå’Œä¸€ä¸ªåç§»çš„ç´§å‡‘å¾„å‘åœ†ç›˜ã€‚</li>
<li>è¿™ä¸¤ä¸ªæˆåˆ†å…·æœ‰ç›¸ä¼¼çš„å…‰è°±æŒ‡æ•°Î“â‰ˆ2.3ï¼Œæ”¯æŒå®ƒä»¬æ¥è‡ªåŒä¸€PWNçš„èµ·æºã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªå¯èƒ½æ˜¯ç”±è¶…æ–°æ˜Ÿæ®‹éª¸æˆåˆ†æ„æˆçš„ç´§å‡‘å¾„å‘åœ†ç›˜ã€‚</li>
<li>é€šè¿‡æ›´æ–°çš„Vela-Xæ¨¡å‹ï¼Œæ¢ç´¢äº†Velaè„‰å†²æ˜Ÿçš„ç›¸ä½è§£æå…‰è°±ç‰¹æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58178dcf37f1062f8d83e5de7df20ce0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca653148de7bd8bfe9bdebcb9924b691.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb5b719ae8dc19e82d5e0795b0719d0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d97d8cc79facc39e47a10f8e43782556.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ab7c08bb95ba4a27f1fed94093ed72a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-788ef7ae569eb999967b64bc7b46d6d0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Hybrid-Attention-Network-for-Accurate-Breast-Tumor-Segmentation-in-Ultrasound-Images"><a href="#Hybrid-Attention-Network-for-Accurate-Breast-Tumor-Segmentation-in-Ultrasound-Images" class="headerlink" title="Hybrid Attention Network for Accurate Breast Tumor Segmentation in   Ultrasound Images"></a>Hybrid Attention Network for Accurate Breast Tumor Segmentation in   Ultrasound Images</h2><p><strong>Authors:Muhammad Azeem Aslam, Asim Naveed, Nisar Ahmed</strong></p>
<p>Breast ultrasound imaging is a valuable tool for early breast cancer detection, but automated tumor segmentation is challenging due to inherent noise, variations in scale of lesions, and fuzzy boundaries. To address these challenges, we propose a novel hybrid attention-based network for lesion segmentation. Our proposed architecture integrates a pre-trained DenseNet121 in the encoder part for robust feature extraction with a multi-branch attention-enhanced decoder tailored for breast ultrasound images. The bottleneck incorporates Global Spatial Attention (GSA), Position Encoding (PE), and Scaled Dot-Product Attention (SDPA) to learn global context, spatial relationships, and relative positional features. The Spatial Feature Enhancement Block (SFEB) is embedded at skip connections to refine and enhance spatial features, enabling the network to focus more effectively on tumor regions. A hybrid loss function combining Binary Cross-Entropy (BCE) and Jaccard Index loss optimizes both pixel-level accuracy and region-level overlap metrics, enhancing robustness to class imbalance and irregular tumor shapes. Experiments on public datasets demonstrate that our method outperforms existing approaches, highlighting its potential to assist radiologists in early and accurate breast cancer diagnosis. </p>
<blockquote>
<p>ä¹³è…ºè¶…å£°æˆåƒåœ¨æ—©æœŸä¹³è…ºç™Œæ£€æµ‹ä¸­æ˜¯ä¸€ç§æœ‰ä»·å€¼çš„å·¥å…·ï¼Œä½†ç”±äºå…¶å›ºæœ‰çš„å™ªå£°ã€ç—…ç¶å°ºåº¦çš„å˜åŒ–å’Œæ¨¡ç³Šçš„è¾¹ç•Œï¼Œè‡ªåŠ¨åŒ–è‚¿ç˜¤åˆ†å‰²ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ··åˆæ³¨æ„åŠ›ç½‘ç»œç”¨äºç—…ç¶åˆ†å‰²ã€‚æˆ‘ä»¬çš„æè®®æ¶æ„å°†é¢„è®­ç»ƒçš„DenseNet121é›†æˆåˆ°ç¼–ç å™¨éƒ¨åˆ†è¿›è¡Œç¨³å¥çš„ç‰¹å¾æå–ï¼Œå¹¶ç»“åˆä¸€ä¸ªé’ˆå¯¹ä¹³è…ºè¶…å£°å›¾åƒçš„å¤šåˆ†æ”¯æ³¨æ„åŠ›å¢å¼ºè§£ç å™¨ã€‚ç“¶é¢ˆå±‚ç»“åˆäº†å…¨å±€ç©ºé—´æ³¨æ„åŠ›ï¼ˆGSAï¼‰ã€ä½ç½®ç¼–ç ï¼ˆPEï¼‰å’Œç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆSDPAï¼‰æ¥å­¦ä¹ å…¨å±€ä¸Šä¸‹æ–‡ã€ç©ºé—´å…³ç³»å’Œç›¸å¯¹ä½ç½®ç‰¹å¾ã€‚ç©ºé—´ç‰¹å¾å¢å¼ºå—ï¼ˆSFEBï¼‰åµŒå…¥åˆ°è·³è·ƒè¿æ¥ä¸­ä»¥ç»†åŒ–å’Œå¢å¼ºç©ºé—´ç‰¹å¾ï¼Œä½¿ç½‘ç»œæ›´æœ‰æ•ˆåœ°å…³æ³¨è‚¿ç˜¤åŒºåŸŸã€‚ç»“åˆäºŒå…ƒäº¤å‰ç†µï¼ˆBCEï¼‰å’ŒJaccardæŒ‡æ•°æŸå¤±çš„æ··åˆæŸå¤±å‡½æ•°ä¼˜åŒ–äº†åƒç´ çº§ç²¾åº¦å’ŒåŒºåŸŸçº§é‡å æŒ‡æ ‡ï¼Œæé«˜äº†å¯¹ç±»åˆ«ä¸å¹³è¡¡å’Œä¸è§„åˆ™è‚¿ç˜¤å½¢çŠ¶çš„é²æ£’æ€§ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œçªæ˜¾å…¶åœ¨å¸®åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œæ—©æœŸå’Œå‡†ç¡®çš„ä¹³è…ºç™Œè¯Šæ–­æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16592v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ä¹³è…ºè¶…å£°å½±åƒä¸­è‚¿ç˜¤åˆ†å‰²çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§æ–°å‹æ··åˆæ³¨æ„åŠ›ç½‘ç»œã€‚è¯¥ç½‘ç»œç»“åˆé¢„è®­ç»ƒçš„DenseNet121è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶åˆ©ç”¨å¤šåˆ†æ”¯æ³¨æ„åŠ›å¢å¼ºè§£ç å™¨å¤„ç†ä¹³è…ºè¶…å£°å›¾åƒã€‚é€šè¿‡å…¨å±€ç©ºé—´æ³¨æ„åŠ›ã€ä½ç½®ç¼–ç å’Œç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›å­¦ä¹ å…¨å±€ä¸Šä¸‹æ–‡ã€ç©ºé—´å…³ç³»å’Œç›¸å¯¹ä½ç½®ç‰¹å¾ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæœ‰åŠ©äºæé«˜ä¹³è…ºç™Œçš„æ—©æœŸè¯Šæ–­å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºè¶…å£°æˆåƒå¯¹æ—©æœŸä¹³è…ºç™Œæ£€æµ‹å…·æœ‰ä»·å€¼ã€‚</li>
<li>è‡ªåŠ¨åŒ–è‚¿ç˜¤åˆ†å‰²åœ¨ä¹³è…ºè¶…å£°å›¾åƒä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å™ªå£°ã€ç—…å˜è§„æ¨¡å˜åŒ–å’Œæ¨¡ç³Šè¾¹ç•Œã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹æ··åˆæ³¨æ„åŠ›ç½‘ç»œï¼Œç»“åˆé¢„è®­ç»ƒçš„DenseNet121è¿›è¡Œç‰¹å¾æå–ã€‚</li>
<li>ç½‘ç»œåˆ©ç”¨å¤šåˆ†æ”¯æ³¨æ„åŠ›å¢å¼ºè§£ç å™¨å¤„ç†ä¹³è…ºè¶…å£°å›¾åƒã€‚</li>
<li>é€šè¿‡å…¨å±€ç©ºé—´æ³¨æ„åŠ›ã€ä½ç½®ç¼–ç å’Œç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æé«˜ç½‘ç»œæ€§èƒ½ã€‚</li>
<li>ç½‘ç»œçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å…¬å…±æ•°æ®é›†ä¸Šå–å¾—è‰¯å¥½æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9cf07af7cd924e9101b27ba76e86456e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7cc7a4e0dd7f91b57e6c94ed3532ae7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61390f4adae71bc408d7007fb6f5e89a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Pixel-wise-Modulated-Dice-Loss-for-Medical-Image-Segmentation"><a href="#Pixel-wise-Modulated-Dice-Loss-for-Medical-Image-Segmentation" class="headerlink" title="Pixel-wise Modulated Dice Loss for Medical Image Segmentation"></a>Pixel-wise Modulated Dice Loss for Medical Image Segmentation</h2><p><strong>Authors:Seyed Mohsen Hosseini</strong></p>
<p>Class imbalance and the difficulty imbalance are the two types of data imbalance that affect the performance of neural networks in medical segmentation tasks. In class imbalance the loss is dominated by the majority classes and in difficulty imbalance the loss is dominated by easy to classify pixels. This leads to an ineffective training. Dice loss, which is based on a geometrical metric, is very effective in addressing the class imbalance compared to the cross entropy (CE) loss, which is adopted directly from classification tasks. To address the difficulty imbalance, the common approach is employing a re-weighted CE loss or a modified Dice loss to focus the training on difficult to classify areas. The existing modification methods are computationally costly and with limited success. In this study we propose a simple modification to the Dice loss with minimal computational cost. With a pixel level modulating term, we take advantage of the effectiveness of Dice loss in handling the class imbalance to also handle the difficulty imbalance. Results on three commonly used medical segmentation tasks show that the proposed Pixel-wise Modulated Dice loss (PM Dice loss) outperforms other methods, which are designed to tackle the difficulty imbalance problem. </p>
<blockquote>
<p>åœ¨åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œç±»åˆ«ä¸å¹³è¡¡å’Œéš¾åº¦ä¸å¹³è¡¡æ˜¯å½±å“ç¥ç»ç½‘ç»œæ€§èƒ½çš„ä¸¤ç§æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨ç±»åˆ«ä¸å¹³è¡¡ä¸­ï¼ŒæŸå¤±ä¸»è¦ç”±å¤šæ•°ç±»åˆ«ä¸»å¯¼ï¼›è€Œåœ¨éš¾åº¦ä¸å¹³è¡¡ä¸­ï¼ŒæŸå¤±ä¸»è¦ç”±å®¹æ˜“åˆ†ç±»çš„åƒç´ ä¸»å¯¼ã€‚è¿™å¯¼è‡´äº†è®­ç»ƒæ•ˆæœä¸ä½³ã€‚DiceæŸå¤±åŸºäºå‡ ä½•åº¦é‡ï¼Œåœ¨è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜æ—¶ç›¸è¾ƒäºç›´æ¥ä»åˆ†ç±»ä»»åŠ¡ä¸­é‡‡çº³çš„äº¤å‰ç†µï¼ˆCEï¼‰æŸå¤±éå¸¸æœ‰æ•ˆã€‚ä¸ºäº†è§£å†³éš¾åº¦ä¸å¹³è¡¡é—®é¢˜ï¼Œå¸¸è§çš„åšæ³•æ˜¯é‡‡ç”¨åŠ æƒCEæŸå¤±æˆ–ä¿®æ”¹åçš„DiceæŸå¤±ï¼Œä»¥å°†è®­ç»ƒé‡ç‚¹æ”¾åœ¨éš¾ä»¥åˆ†ç±»çš„åŒºåŸŸã€‚ç°æœ‰çš„ä¿®æ”¹æ–¹æ³•è®¡ç®—æˆæœ¬è¾ƒé«˜ä¸”æˆåŠŸæœ‰é™ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¯¹DiceæŸå¤±çš„ç®€å•ä¿®æ”¹ï¼Œä¸”è®¡ç®—æˆæœ¬è¾ƒä½ã€‚é€šè¿‡åƒç´ çº§è°ƒåˆ¶é¡¹ï¼Œæˆ‘ä»¬åˆ©ç”¨DiceæŸå¤±åœ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜æ—¶çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä¹Ÿå¤„ç†éš¾åº¦ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨ä¸‰ä¸ªå¸¸ç”¨çš„åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„åƒç´ çº§è°ƒåˆ¶DiceæŸå¤±ï¼ˆPM DiceæŸå¤±ï¼‰ä¼˜äºå…¶ä»–æ—¨åœ¨è§£å†³éš¾åº¦ä¸å¹³è¡¡é—®é¢˜çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15744v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­ç¥ç»ç½‘ç»œæ€§èƒ½å—å½±å“çš„ç±»åˆ«ä¸å¹³è¡¡å’Œéš¾åº¦ä¸å¹³è¡¡é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç®€å•çš„DiceæŸå¤±ä¿®æ”¹æ–¹æ³•ï¼Œä»¥è¾ƒä½çš„è®¡ç®—æˆæœ¬åŒæ—¶è§£å†³ä¸¤ç±»æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚é€šè¿‡åƒç´ çº§è°ƒåˆ¶é¡¹ï¼Œåˆ©ç”¨DiceæŸå¤±åœ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿè§£å†³äº†éš¾åº¦ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨ä¸‰ç§å¸¸ç”¨çš„åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸Šï¼Œæ‰€æçš„åƒç´ çº§è°ƒåˆ¶DiceæŸå¤±ï¼ˆPM DiceæŸå¤±ï¼‰è¡¨ç°ä¼˜äºå…¶ä»–è§£å†³éš¾åº¦ä¸å¹³è¡¡é—®é¢˜çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œç¥ç»ç½‘ç»œæ€§èƒ½å—ä¸¤ç±»æ•°æ®ä¸å¹³è¡¡å½±å“ï¼šç±»åˆ«ä¸å¹³è¡¡å’Œéš¾åº¦ä¸å¹³è¡¡ã€‚</li>
<li>ç±»åˆ«ä¸å¹³è¡¡ä¸­ï¼ŒæŸå¤±ä¸»è¦ç”±å¤šæ•°ç±»åˆ«ä¸»å¯¼ï¼›éš¾åº¦ä¸å¹³è¡¡ä¸­ï¼ŒæŸå¤±ä¸»è¦ç”±æ˜“åˆ†ç±»åƒç´ ä¸»å¯¼ã€‚</li>
<li>DiceæŸå¤±åœ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ä¸Šæ¯”äº¤å‰ç†µæŸå¤±æ›´æœ‰æ•ˆã€‚</li>
<li>ç°æœ‰è§£å†³éš¾åº¦ä¸å¹³è¡¡é—®é¢˜çš„æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ä¸”æ•ˆæœæœ‰é™ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºä¸€ç§ç®€å•çš„DiceæŸå¤±ä¿®æ”¹æ–¹æ³•ï¼Œä»¥ä½è®¡ç®—æˆæœ¬åŒæ—¶å¤„ç†ä¸¤ç±»æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>é€šè¿‡åƒç´ çº§è°ƒåˆ¶é¡¹ï¼Œåˆ©ç”¨DiceæŸå¤±åœ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä¹Ÿè§£å†³äº†éš¾åº¦ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15744">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e489b0311cb1710c5c2004553ae5534c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2022c50334fada85ce28d8cd9977f16e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97b60eb1369a2232e9bac146e81fb826.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf95d73d17a01737ef4eab7700ae69b2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SynPo-Boosting-Training-Free-Few-Shot-Medical-Segmentation-via-High-Quality-Negative-Prompts"><a href="#SynPo-Boosting-Training-Free-Few-Shot-Medical-Segmentation-via-High-Quality-Negative-Prompts" class="headerlink" title="SynPo: Boosting Training-Free Few-Shot Medical Segmentation via   High-Quality Negative Prompts"></a>SynPo: Boosting Training-Free Few-Shot Medical Segmentation via   High-Quality Negative Prompts</h2><p><strong>Authors:Yufei Liu, Haoke Xiao, Jiaxing Chai, Yongcun Zhang, Rong Wang, Zijie Meng, Zhiming Luo</strong></p>
<p>The advent of Large Vision Models (LVMs) offers new opportunities for few-shot medical image segmentation. However, existing training-free methods based on LVMs fail to effectively utilize negative prompts, leading to poor performance on low-contrast medical images. To address this issue, we propose SynPo, a training-free few-shot method based on LVMs (e.g., SAM), with the core insight: improving the quality of negative prompts. To select point prompts in a more reliable confidence map, we design a novel Confidence Map Synergy Module by combining the strengths of DINOv2 and SAM. Based on the confidence map, we select the top-k pixels as the positive points set and choose the negative points set using a Gaussian distribution, followed by independent K-means clustering for both sets. Then, these selected points are leveraged as high-quality prompts for SAM to get the segmentation results. Extensive experiments demonstrate that SynPo achieves performance comparable to state-of-the-art training-based few-shot methods. </p>
<blockquote>
<p>å¤§è§„æ¨¡è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰çš„å‡ºç°ä¸ºå°‘æ•°åŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼ŒåŸºäºLVMsçš„æ— è®­ç»ƒæ–¹æ³•æ— æ³•æœ‰æ•ˆåˆ©ç”¨è´Ÿæç¤ºï¼Œå¯¼è‡´åœ¨ä½å¯¹æ¯”åº¦åŒ»å­¦å›¾åƒä¸Šçš„æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SynPoï¼Œè¿™æ˜¯ä¸€ç§åŸºäºLVMsï¼ˆä¾‹å¦‚SAMï¼‰çš„æ— è®­ç»ƒå°‘æ•°æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ˜¯æ”¹å–„è´Ÿæç¤ºçš„è´¨é‡ã€‚ä¸ºäº†åœ¨æ›´å¯é çš„ç½®ä¿¡å›¾ä¸­é€‰æ‹©ç‚¹æç¤ºï¼Œæˆ‘ä»¬ç»“åˆäº†DINOv2å’ŒSAMçš„ä¼˜ç‚¹ï¼Œè®¾è®¡äº†ä¸€ç§æ–°å‹çš„ç½®ä¿¡å›¾ååŒæ¨¡å—ã€‚åŸºäºç½®ä¿¡å›¾ï¼Œæˆ‘ä»¬é€‰æ‹©å‰kä¸ªåƒç´ ä½œä¸ºæ­£ç‚¹é›†ï¼Œä½¿ç”¨é«˜æ–¯åˆ†å¸ƒé€‰æ‹©è´Ÿç‚¹é›†ï¼Œç„¶åå¯¹è¿™ä¸¤ä¸ªé›†åˆè¿›è¡Œç‹¬ç«‹çš„K-meansèšç±»ã€‚ç„¶åï¼Œè¿™äº›é€‰å®šçš„ç‚¹è¢«ç”¨ä½œé«˜è´¨é‡æç¤ºï¼Œä¾›SAMè·å–åˆ†å‰²ç»“æœã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSynPoçš„æ€§èƒ½ä¸åŸºäºè®­ç»ƒçš„å°‘æ•°æ–¹æ³•ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15153v2">PDF</a> MICCAI 2025 Early Accept. Project Page:   <a target="_blank" rel="noopener" href="https://liu-yufei.github.io/synpo-project-page/">https://liu-yufei.github.io/synpo-project-page/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰ä¸ºå°‘æ•°åŒ»ç–—å›¾åƒåˆ†å‰²æä¾›äº†æ–°çš„æœºä¼šï¼Œä½†ç°æœ‰åŸºäºLVMsçš„æ— è®­ç»ƒæ–¹æ³•æ— æ³•æœ‰æ•ˆåˆ©ç”¨è´Ÿæç¤ºï¼Œå¯¼è‡´åœ¨ä½å¯¹æ¯”åº¦åŒ»ç–—å›¾åƒä¸Šçš„è¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºLVMsçš„æ— è®­ç»ƒå°‘æ•°æ–¹æ³•SynPoï¼Œå…¶æ ¸å¿ƒæ˜¯æé«˜è´Ÿæç¤ºçš„è´¨é‡ã€‚é€šè¿‡ç»“åˆDINOv2å’ŒSAMçš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹çš„ç½®ä¿¡å›¾ååŒæ¨¡å—ï¼Œä»¥æ›´å¯é çš„ç½®ä¿¡å›¾é€‰æ‹©ç‚¹æç¤ºã€‚åŸºäºç½®ä¿¡å›¾ï¼Œæˆ‘ä»¬é€‰æ‹©å‰kä¸ªåƒç´ ä½œä¸ºæ­£ç‚¹é›†ï¼Œä½¿ç”¨é«˜æ–¯åˆ†å¸ƒé€‰æ‹©è´Ÿç‚¹é›†ï¼Œç„¶åå¯¹è¿™ä¸¤ç»„è¿›è¡Œç‹¬ç«‹çš„K-meansèšç±»ã€‚ç„¶åï¼Œè¿™äº›é€‰å®šçš„ç‚¹è¢«ç”¨ä½œé«˜è´¨é‡æç¤ºï¼Œä¾›SAMè¿›è¡Œåˆ†å‰²ã€‚å®éªŒè¡¨æ˜ï¼ŒSynPoçš„æ€§èƒ½ä¸åŸºäºè®­ç»ƒçš„å°‘æ•°æ–¹æ³•ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>ç°æœ‰åŸºäºLVMsçš„æ— è®­ç»ƒæ–¹æ³•åœ¨ä½å¯¹æ¯”åº¦åŒ»ç–—å›¾åƒä¸Šçš„è¡¨ç°ä¸ä½³ã€‚</li>
<li>SynPoæ–¹æ³•æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡æé«˜è´Ÿæç¤ºçš„è´¨é‡æ¥ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>SynPoè®¾è®¡äº†ä¸€ç§æ–°å‹çš„ç½®ä¿¡å›¾ååŒæ¨¡å—ï¼Œç»“åˆDINOv2å’ŒSAMçš„ä¼˜ç‚¹ã€‚</li>
<li>åŸºäºç½®ä¿¡å›¾ï¼Œé€‰æ‹©æ­£ç‚¹é›†å’Œè´Ÿç‚¹é›†ï¼Œç„¶åè¿›è¡Œç‹¬ç«‹çš„K-meansèšç±»ã€‚</li>
<li>é€‰å®šçš„ç‚¹è¢«ç”¨ä½œé«˜è´¨é‡æç¤ºï¼Œä¾›SAMè¿›è¡Œåˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15153">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-96dbe471f0ef20ab45c582024ead24a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a64112893cb38c03306e6d6cd7e48c88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3b9f688b027c6b4228079a26d761301.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="BreastDCEDL-Curating-a-Comprehensive-DCE-MRI-Dataset-and-developing-a-Transformer-Implementation-for-Breast-Cancer-Treatment-Response-Prediction"><a href="#BreastDCEDL-Curating-a-Comprehensive-DCE-MRI-Dataset-and-developing-a-Transformer-Implementation-for-Breast-Cancer-Treatment-Response-Prediction" class="headerlink" title="BreastDCEDL: Curating a Comprehensive DCE-MRI Dataset and developing a   Transformer Implementation for Breast Cancer Treatment Response Prediction"></a>BreastDCEDL: Curating a Comprehensive DCE-MRI Dataset and developing a   Transformer Implementation for Breast Cancer Treatment Response Prediction</h2><p><strong>Authors:Naomi Fridman, Bubby Solway, Tomer Fridman, Itamar Barnea, Anat Goldstein</strong></p>
<p>Breast cancer remains a leading cause of cancer-related mortality worldwide, making early detection and accurate treatment response monitoring critical priorities. We present BreastDCEDL, a curated, deep learning-ready dataset comprising pre-treatment 3D Dynamic Contrast-Enhanced MRI (DCE-MRI) scans from 2,070 breast cancer patients drawn from the I-SPY1, I-SPY2, and Duke cohorts, all sourced from The Cancer Imaging Archive. The raw DICOM imaging data were rigorously converted into standardized 3D NIfTI volumes with preserved signal integrity, accompanied by unified tumor annotations and harmonized clinical metadata including pathologic complete response (pCR), hormone receptor (HR), and HER2 status. Although DCE-MRI provides essential diagnostic information and deep learning offers tremendous potential for analyzing such complex data, progress has been limited by lack of accessible, public, multicenter datasets. BreastDCEDL addresses this gap by enabling development of advanced models, including state-of-the-art transformer architectures that require substantial training data. To demonstrate its capacity for robust modeling, we developed the first transformer-based model for breast DCE-MRI, leveraging Vision Transformer (ViT) architecture trained on RGB-fused images from three contrast phases (pre-contrast, early post-contrast, and late post-contrast). Our ViT model achieved state-of-the-art pCR prediction performance in HR+&#x2F;HER2- patients (AUC 0.94, accuracy 0.93). BreastDCEDL includes predefined benchmark splits, offering a framework for reproducible research and enabling clinically meaningful modeling in breast cancer imaging. </p>
<blockquote>
<p>ä¹³è…ºç™Œä»ç„¶æ˜¯å…¨çƒç™Œç—‡ç›¸å…³æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œå› æ­¤æ—©æœŸæ£€æµ‹å’Œå‡†ç¡®çš„æ²»ç–—ååº”ç›‘æµ‹æˆä¸ºè‡³å…³é‡è¦çš„ä¼˜å…ˆäº‹é¡¹ã€‚æˆ‘ä»¬æ¨å‡ºäº†BreastDCEDLï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒç­–åˆ’ã€å‡†å¤‡å¥½ç”¨äºæ·±åº¦å­¦ä¹ æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªI-SPY1ã€I-SPY2å’ŒDukeé˜Ÿåˆ—çš„2070åä¹³è…ºç™Œæ‚£è€…çš„æ²»ç–—å‰3DåŠ¨æ€å¢å¼ºMRIï¼ˆDCE-MRIï¼‰æ‰«æã€‚æ‰€æœ‰æ•°æ®å‡æ¥è‡ªç™Œç—‡æˆåƒå­˜æ¡£ã€‚åŸå§‹çš„DICOMæˆåƒæ•°æ®è¢«ä¸¥æ ¼è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„3DNIfTIä½“ç§¯ï¼ŒåŒæ—¶ä¿ç•™äº†ä¿¡å·å®Œæ•´æ€§ï¼Œå¹¶é…æœ‰ç»Ÿä¸€çš„è‚¿ç˜¤æ³¨é‡Šå’Œåè°ƒä¸€è‡´çš„ä¸´åºŠå…ƒæ•°æ®ï¼ŒåŒ…æ‹¬ç—…ç†å®Œå…¨ååº”ï¼ˆpCRï¼‰ã€æ¿€ç´ å—ä½“ï¼ˆHRï¼‰å’ŒHER2çŠ¶æ€ã€‚å°½ç®¡DCE-MRIæä¾›äº†é‡è¦çš„è¯Šæ–­ä¿¡æ¯ï¼Œæ·±åº¦å­¦ä¹ åœ¨åˆ†ææ­¤ç±»å¤æ‚æ•°æ®æ–¹é¢æ‹¥æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ç”±äºç¼ºä¹å¯è®¿é—®çš„å…¬å…±å¤šä¸­å¿ƒæ•°æ®é›†ï¼Œè¿›å±•ä¸€ç›´å—åˆ°é™åˆ¶ã€‚BreastDCEDLé€šè¿‡æ”¯æŒå¼€å‘å…ˆè¿›æ¨¡å‹æ¥è§£å†³è¿™ä¸€å·®è·ï¼ŒåŒ…æ‹¬éœ€è¦å¤§è®­ç»ƒæ•°æ®çš„æœ€æ–°å˜å‹å™¨æ¶æ„ã€‚ä¸ºäº†å±•ç¤ºå…¶ç¨³å¥å»ºæ¨¡çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†åŸºäºå˜å‹å™¨çš„é¦–ä¸ªä¹³è…ºç™ŒDCE-MRIæ¨¡å‹ï¼Œåˆ©ç”¨åœ¨ä¸‰ä¸ªå¯¹æ¯”é˜¶æ®µï¼ˆé¢„å¯¹æ¯”ã€æ—©æœŸåå¯¹æ¯”å’Œæ™šæœŸåå¯¹æ¯”ï¼‰çš„RGBèåˆå›¾åƒä¸Šè®­ç»ƒçš„Vision Transformerï¼ˆViTï¼‰æ¶æ„ã€‚æˆ‘ä»¬çš„ViTæ¨¡å‹åœ¨HR+&#x2F;HER2-æ‚£è€…ä¸­å®ç°äº†æœ€å…ˆè¿›çš„pCRé¢„æµ‹æ€§èƒ½ï¼ˆAUC 0.94ï¼Œå‡†ç¡®ç‡0.93ï¼‰ã€‚BreastDCEDLåŒ…æ‹¬é¢„å®šä¹‰çš„åŸºå‡†åˆ†å‰²ï¼Œä¸ºå¯é‡å¤çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ¡†æ¶ï¼Œå¹¶åœ¨ä¹³è…ºç™Œæˆåƒä¸­å®ç°äº†ä¸´åºŠæ„ä¹‰å»ºæ¨¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12190v2">PDF</a> </p>
<p><strong>Summary</strong><br>    ä¹³è…ºç™Œæ˜¯å…¨çƒç™Œç—‡æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œæ—©æœŸå‘ç°å’Œå‡†ç¡®ç›‘æµ‹æ²»ç–—ååº”æ˜¯å…³é”®ã€‚æœ¬ç ”ç©¶æ¨å‡ºBreastDCEDLæ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªI-SPY1ã€I-SPY2å’ŒDukeç­‰é˜Ÿåˆ—çš„2,070ä¾‹ä¹³è…ºç™Œæ‚£è€…çš„é¢„æ²»ç–—3DåŠ¨æ€å¢å¼ºMRIæ‰«ææ•°æ®ï¼Œæ•°æ®æ¥è‡ªç™Œç—‡æˆåƒæ¡£æ¡ˆã€‚æ•°æ®é›†åŒ…æ‹¬æ ‡å‡†åŒ–çš„3DNIfTIä½“ç§¯å›¾åƒã€ç»Ÿä¸€çš„è‚¿ç˜¤æ³¨é‡Šå’Œåè°ƒçš„ä¸´åºŠå…ƒæ•°æ®ã€‚æœ¬ç ”ç©¶å¼€å‘äº†åŸºäºVision Transformerçš„æ¨¡å‹ï¼Œé¢„æµ‹HR+&#x2F;HER2-æ‚£è€…çš„pCRï¼Œæ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚BreastDCEDLåŒ…æ‹¬é¢„è®¾çš„åŸºå‡†åˆ†å‰²ï¼Œä¸ºå¯é‡å¤ç ”ç©¶å’Œä¹³è…ºç™Œæˆåƒçš„ä¸´åºŠæ„ä¹‰å»ºæ¨¡æä¾›äº†æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™Œä»æ˜¯å…¨çƒç™Œç—‡æ­»äº¡çš„ä¸»è¦åŸå› ï¼Œæ—©æœŸå‘ç°å’Œå‡†ç¡®ç›‘æµ‹æ²»ç–—ååº”è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶æ¨å‡ºBreastDCEDLæ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªå¤šä¸ªé˜Ÿåˆ—çš„ä¹³è…ºç™Œæ‚£è€…çš„é¢„æ²»ç–—3D DCE-MRIæ‰«ææ•°æ®ã€‚</li>
<li>æ•°æ®é›†åŒ…æ‹¬æ ‡å‡†åŒ–çš„å›¾åƒæ•°æ®ã€ç»Ÿä¸€çš„è‚¿ç˜¤æ³¨é‡Šå’Œä¸´åºŠå…ƒæ•°æ®ã€‚</li>
<li>æ·±åº¦å­¦ä¹ å’ŒDCE-MRIåœ¨ä¹³è…ºç™Œè¯Šæ–­å’Œæ²»ç–—ä¸­æ½œåŠ›å·¨å¤§ï¼Œä½†ç¼ºä¹å…¬å…±å¤šä¸­å¿ƒæ•°æ®é›†é™åˆ¶äº†è¿›å±•ã€‚</li>
<li>BreastDCEDLå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œæ”¯æŒå¼€å‘å…ˆè¿›æ¨¡å‹ï¼ŒåŒ…æ‹¬åŸºäºTransformerçš„æ¶æ„ã€‚</li>
<li>ç ”ç©¶äººå‘˜å¼€å‘äº†åŸºäºVision Transformerçš„æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹HR+&#x2F;HER2-æ‚£è€…çš„pCRï¼Œæ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3835c0516c0ba853d6f3b0d1fb0d1ba1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-100bd057869bb8cf967db06d3455a640.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9f0ec1c0db9c5ca2eebe0ec3ac097f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b64260b9af43fcc9c67030971d20f4c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Statistical-microlocal-analysis-in-two-dimensional-X-ray-CT"><a href="#Statistical-microlocal-analysis-in-two-dimensional-X-ray-CT" class="headerlink" title="Statistical microlocal analysis in two-dimensional X-ray CT"></a>Statistical microlocal analysis in two-dimensional X-ray CT</h2><p><strong>Authors:Anuj Abhishek, Alexander Katsevich, James W. Webber</strong></p>
<p>In many imaging applications it is important to assess how well the edges of the original object, $f$, are resolved in an image, $f^\text{rec}$, reconstructed from the measured data, $g$. In this paper we consider the case of image reconstruction in 2D X-ray Computed Tomography (CT). Let $f$ be a function describing the object being scanned, and $g&#x3D;Rf + \eta$ be the Radon transform data in $\mathbb{R}^2$ corrupted by noise, $\eta$, and sampled with step size $\sim\epsilon$. Conventional microlocal analysis provides conditions for edge detectability based on the scanner geometry in the case of continuous, noiseless data (when $\eta &#x3D; 0$), but does not account for noise and finite sampling step size. We develop a novel technique called Statistical Microlocal Analysis (SMA), which uses a statistical hypothesis testing framework to determine if an image edge (singularity) of $f$ is detectable from $f^\text{rec}$, and we quantify edge detectability using the statistical power of the test. Our approach is based on the theory we developed in previous work, which provides a characterization of $f^\text{rec}$ in local $O(\epsilon)$-size neighborhoods when $\eta \neq 0$. We derive a statistical test for the presence and direction of an edge microlocally given the magnitude of $\eta$ and data sampling step size. Using the properties of the null distribution of the test, we quantify the uncertainty of the edge magnitude and direction. We validate our theory using simulations, which show strong agreement between our predictions and experimental observations. Our work is not only of practical value, but of theoretical value as well. SMA is a natural extension of classical microlocal analysis theory which accounts for practical measurement imperfections, such as noise and finite step size, at the highest possible resolution compatible with the data. </p>
<blockquote>
<p>åœ¨è®¸å¤šæˆåƒåº”ç”¨ä¸­ï¼Œè¯„ä¼°åŸå§‹å¯¹è±¡$f$çš„è¾¹ç¼˜åœ¨ç”±æµ‹é‡æ•°æ®$g$é‡å»ºçš„å›¾åƒ$f^\text{rec}$ä¸­è§£æå¾—æœ‰å¤šå¥½æ˜¯éå¸¸é‡è¦çš„ã€‚æœ¬æ–‡è€ƒè™‘äºŒç»´Xå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸­çš„å›¾åƒé‡å»ºæƒ…å†µã€‚è®¾$f$ä¸ºæè¿°è¢«æ‰«æå¯¹è±¡çš„å‡½æ•°ï¼Œ$g&#x3D;Rf+\eta$ä¸ºå—åˆ°å™ªå£°$\eta$å½±å“çš„Radonå˜æ¢æ•°æ®åœ¨$\mathbb{R}^2$ä¸­çš„è¡¨ç¤ºï¼Œå¹¶ä¸”ä»¥æ­¥é•¿$\sim\epsilon$è¿›è¡Œé‡‡æ ·ã€‚ä¼ ç»Ÿçš„å¾®å±€éƒ¨åˆ†æä¸ºè¿ç»­ã€æ— å™ªå£°æ•°æ®ï¼ˆå½“$\eta &#x3D; 0$æ—¶ï¼‰çš„æ‰«æä»ªå‡ ä½•æä¾›äº†è¾¹ç¼˜å¯æ£€æµ‹æ€§çš„æ¡ä»¶ï¼Œä½†ä¸è€ƒè™‘å™ªå£°å’Œæœ‰é™çš„é‡‡æ ·æ­¥é•¿ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç§°ä¸ºç»Ÿè®¡å¾®å±€éƒ¨åˆ†æï¼ˆSMAï¼‰çš„æ–°æŠ€æœ¯ï¼Œå®ƒä½¿ç”¨ç»Ÿè®¡å‡è®¾æ£€éªŒæ¡†æ¶æ¥ç¡®å®šä»$f^\text{rec}$æ˜¯å¦å¯æ£€æµ‹å›¾åƒè¾¹ç¼˜ï¼ˆå¥‡å¼‚æ€§ï¼‰ï¼Œå¹¶ä¸”æˆ‘ä»¬ä½¿ç”¨æ£€éªŒçš„ç»Ÿè®¡æ•ˆåŠ›æ¥é‡åŒ–è¾¹ç¼˜çš„å¯æ£€æµ‹æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºæˆ‘ä»¬ä¹‹å‰çš„å·¥ä½œæ‰€å‘å±•çš„ç†è®ºï¼Œè¯¥ç†è®ºåœ¨$\eta \neq 0$çš„æƒ…å†µä¸‹ï¼Œå¯¹$f^\text{rec}$åœ¨å±€éƒ¨$O(\epsilon)$å¤§å°é‚»åŸŸå†…çš„ç‰¹æ€§è¿›è¡Œäº†æè¿°ã€‚æˆ‘ä»¬é’ˆå¯¹è¾¹ç¼˜çš„å­˜åœ¨æ€§å’Œæ–¹å‘æ€§å¾®å±€éƒ¨åœ°æ¨å¯¼äº†ä¸€ä¸ªç»Ÿè®¡æ£€éªŒï¼Œç»™å®š$\eta$çš„å¹…åº¦å’Œæ•°æ®é‡‡æ ·æ­¥é•¿ã€‚åˆ©ç”¨æ£€éªŒçš„ç©ºåˆ†å¸ƒå±æ€§ï¼Œæˆ‘ä»¬å¯¹è¾¹ç¼˜å¹…åº¦å’Œæ–¹å‘çš„ä¸ç¡®å®šæ€§è¿›è¡Œäº†é‡åŒ–ã€‚æˆ‘ä»¬é€šè¿‡æ¨¡æ‹ŸéªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºï¼Œæ¨¡æ‹Ÿç»“æœæ˜¾ç¤ºæˆ‘ä»¬çš„é¢„æµ‹ä¸å®éªŒè§‚å¯Ÿç»“æœä¹‹é—´å…·æœ‰å¾ˆå¼ºçš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ä»…å…·æœ‰å®ç”¨ä»·å€¼ï¼Œè€Œä¸”å…·æœ‰ç†è®ºä»·å€¼ã€‚SMAæ˜¯ç»å…¸å¾®å±€éƒ¨åˆ†æç†è®ºçš„è‡ªç„¶æ‰©å±•ï¼Œå®ƒè€ƒè™‘äº†å®é™…æµ‹é‡ä¸­çš„ä¸å®Œç¾ä¹‹å¤„ï¼Œä¾‹å¦‚å™ªå£°å’Œæœ‰é™çš„æ­¥é•¿ï¼Œå¹¶åœ¨ä¸æ•°æ®å…¼å®¹çš„æœ€é«˜å¯èƒ½åˆ†è¾¨ç‡ä¸‹è¿›è¡Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05113v3">PDF</a> 27 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡å…³æ³¨äºŒç»´Xå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸­çš„å›¾åƒé‡å»ºé—®é¢˜ã€‚é’ˆå¯¹ä¼ ç»Ÿå¾®å±€éƒ¨åˆ†æåœ¨å™ªå£°å’Œæœ‰é™é‡‡æ ·æ­¥é•¿æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§åä¸ºç»Ÿè®¡å¾®å±€éƒ¨åˆ†æï¼ˆSMAï¼‰çš„æ–°æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯ä½¿ç”¨ç»Ÿè®¡å‡è®¾æ£€éªŒæ¡†æ¶æ¥ç¡®å®šä»é‡å»ºå›¾åƒ$f^{rec}$ä¸­æ˜¯å¦å¯æ£€æµ‹åˆ°å¯¹è±¡å‡½æ•°$f$çš„å›¾åƒè¾¹ç¼˜ï¼ˆå¥‡ç‚¹ï¼‰ï¼Œå¹¶åˆ©ç”¨æ£€éªŒçš„ç»Ÿè®¡æ•ˆåŠ›é‡åŒ–è¾¹ç¼˜æ£€æµ‹èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜åˆ©ç”¨å…ˆå‰çš„ç†è®ºå·¥ä½œï¼Œå¯¹$\eta \neq 0$æ—¶$f^{rec}$åœ¨å±€éƒ¨$O(\epsilon)$å¤§å°é‚»åŸŸå†…çš„ç‰¹æ€§è¿›è¡Œäº†è¡¨å¾ã€‚é€šè¿‡æ¨¡æ‹ŸéªŒè¯ï¼Œè¯¥ç ”ç©¶çš„ç»“æœä¸å®éªŒè§‚å¯Ÿç»“æœé«˜åº¦ä¸€è‡´ã€‚ç»Ÿè®¡å¾®å±€éƒ¨åˆ†æä¸ä»…æ˜¯å®è·µä¸­çš„æœ‰ä»·å€¼å·¥å…·ï¼Œä¹Ÿæ˜¯å¯¹ä¼ ç»Ÿå¾®å±€éƒ¨åˆ†æç†è®ºçš„æœ‰ç›Šè¡¥å……ï¼Œèƒ½å¤Ÿè§£å†³å®é™…æµ‹é‡ä¸­çš„ä¸å®Œç¾é—®é¢˜ï¼Œå¦‚å™ªå£°å’Œæœ‰é™çš„æ­¥é•¿ï¼Œå¹¶å…¼å®¹æ•°æ®å®ç°æœ€é«˜åˆ†è¾¨ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡ç ”ç©¶äº†äºŒç»´Xå°„çº¿CTä¸­çš„å›¾åƒé‡å»ºé—®é¢˜ï¼Œå…³æ³¨è¾¹ç¼˜åˆ†è¾¨ç‡çš„è¯„ä¼°ã€‚</li>
<li>æå‡ºäº†ç»Ÿè®¡å¾®å±€éƒ¨åˆ†æï¼ˆSMAï¼‰æŠ€æœ¯ï¼Œåˆ©ç”¨ç»Ÿè®¡å‡è®¾æ£€éªŒæ¡†æ¶è¯„ä¼°å›¾åƒè¾¹ç¼˜çš„æ£€æµ‹æ€§ã€‚</li>
<li>é‡åŒ–è¾¹ç¼˜æ£€æµ‹èƒ½åŠ›ï¼Œé€šè¿‡ç»Ÿè®¡æ•ˆåŠ›è¯„ä¼°æ£€éªŒã€‚</li>
<li>è€ƒè™‘å™ªå£°å’Œæœ‰é™é‡‡æ ·æ­¥é•¿ç­‰å®é™…æµ‹é‡ä¸å®Œç¾å› ç´ ã€‚</li>
<li>åŸºäºå…ˆå‰çš„ç†è®ºå·¥ä½œï¼Œå¯¹$f^{rec}$åœ¨å±€éƒ¨é‚»åŸŸçš„ç‰¹æ€§è¿›è¡Œäº†è¡¨å¾ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹ŸéªŒè¯äº†ç†è®ºé¢„æµ‹ä¸å®éªŒè§‚å¯Ÿçš„é«˜åº¦ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4235ed3fa3012b660e63eb8b302adb95.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Automatic-dataset-shift-identification-to-support-safe-deployment-of-medical-imaging-AI"><a href="#Automatic-dataset-shift-identification-to-support-safe-deployment-of-medical-imaging-AI" class="headerlink" title="Automatic dataset shift identification to support safe deployment of   medical imaging AI"></a>Automatic dataset shift identification to support safe deployment of   medical imaging AI</h2><p><strong>Authors:MÃ©lanie Roschewitz, Raghav Mehta, Charles Jones, Ben Glocker</strong></p>
<p>Shifts in data distribution can substantially harm the performance of clinical AI models and lead to misdiagnosis. Hence, various methods have been developed to detect the presence of such shifts at deployment time. However, the root causes of dataset shifts are diverse, and the choice of shift mitigation strategies is highly dependent on the precise type of shift encountered at test time. As such, detecting test-time dataset shift is not sufficient: precisely identifying which type of shift has occurred is critical. In this work, we propose the first unsupervised dataset shift identification framework for imaging datasets, effectively distinguishing between prevalence shift (caused by a change in the label distribution), covariate shift (caused by a change in input characteristics) and mixed shifts (simultaneous prevalence and covariate shifts). We discuss the importance of self-supervised encoders for detecting subtle covariate shifts and propose a novel shift detector leveraging both self-supervised encoders and task model outputs for improved shift detection. We show the effectiveness of the proposed shift identification framework across three different imaging modalities (chest radiography, digital mammography, and retinal fundus images) on five types of real-world dataset shifts using five large publicly available datasets. </p>
<blockquote>
<p>æ•°æ®åˆ†å¸ƒçš„å˜è¿å¯èƒ½ä¼šæ˜¾è‘—å½±å“ä¸´åºŠäººå·¥æ™ºèƒ½æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶å¯¼è‡´è¯¯è¯Šã€‚å› æ­¤ï¼Œå·²ç»å¼€å‘äº†å„ç§æ–¹æ³•æ¥æ£€æµ‹éƒ¨ç½²æ—¶çš„è¿™ç§å˜è¿æ˜¯å¦å­˜åœ¨ã€‚ç„¶è€Œï¼Œæ•°æ®é›†å˜è¿çš„æ ¹æºæ˜¯å¤šæ ·çš„ï¼Œæ‰€é€‰çš„å˜è¿ç¼“è§£ç­–ç•¥é«˜åº¦ä¾èµ–äºæµ‹è¯•æ—¶é‡åˆ°çš„ç²¾ç¡®å˜è¿ç±»å‹ã€‚å› æ­¤ï¼Œä»…åœ¨æµ‹è¯•æ—¶æ£€æµ‹æ•°æ®é›†å˜è¿æ˜¯ä¸å¤Ÿçš„ï¼šç²¾ç¡®è¯†åˆ«å“ªç§ç±»å‹çš„å˜è¿å·²ç»å‘ç”Ÿè‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸ºæˆåƒæ•°æ®é›†æå‡ºäº†ç¬¬ä¸€ä¸ªæ— ç›‘ç£æ•°æ®é›†å˜è¿è¯†åˆ«æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†ç”±æ ‡ç­¾åˆ†å¸ƒå˜åŒ–å¼•èµ·çš„æ™®åŠå˜è¿ã€ç”±è¾“å…¥ç‰¹å¾å˜åŒ–å¼•èµ·çš„åå˜é‡å˜è¿ä»¥åŠåŒæ—¶å‘ç”Ÿçš„æ™®åŠå’Œåå˜é‡æ··åˆå˜è¿ã€‚æˆ‘ä»¬è®¨è®ºäº†è‡ªç›‘ç£ç¼–ç å™¨åœ¨æ£€æµ‹ç»†å¾®åå˜é‡å˜è¿ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹å˜è¿æ£€æµ‹å™¨ï¼Œè¯¥æ£€æµ‹å™¨åˆ©ç”¨è‡ªç›‘ç£ç¼–ç å™¨å’Œä»»åŠ¡æ¨¡å‹è¾“å‡ºè¿›è¡Œæ”¹è¿›ï¼Œä»¥æé«˜å˜è¿æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å±•ç¤ºäº†æ‰€æå‡ºçš„æ•°æ®é›†å˜è¿è¯†åˆ«æ¡†æ¶åœ¨ä¸‰ç§ä¸åŒæˆåƒæ¨¡å¼ï¼ˆèƒ¸éƒ¨æ”¾å°„æ‘„å½±ã€æ•°å­—ä¹³è…ºæ‘„å½±å’Œè§†ç½‘è†œçœ¼åº•å›¾åƒï¼‰ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œä½¿ç”¨äº†äº”ç§å¤§å‹å…¬å¼€æ•°æ®é›†å’Œäº”ç§ç°å®ä¸–ç•Œçš„æ•°æ®é›†å˜è¿ç±»å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07940v3">PDF</a> Accepted at MICCAI 2025. This version is an extended version with   additional experimental results. Code available at   <a target="_blank" rel="noopener" href="https://github.com/biomedia-mira/shift_identification">https://github.com/biomedia-mira/shift_identification</a></p>
<p><strong>Summary</strong><br>     ä¸´åºŠAIæ¨¡å‹åœ¨æ•°æ®åˆ†å¸ƒå‘ç”Ÿå˜åŒ–æ—¶æ€§èƒ½ä¼šå¤§å¹…ä¸‹é™ï¼Œå¯èƒ½å¯¼è‡´è¯¯è¯Šã€‚ä¸ºæ£€æµ‹éƒ¨ç½²æ—¶çš„æ•°æ®åˆ†å¸ƒå˜åŒ–ï¼Œå·²å¼€å‘å¤šç§æ–¹æ³•ã€‚ç„¶è€Œï¼Œæ•°æ®é›†å˜åŒ–çš„æ ¹æœ¬åŸå› å¤šæ ·ï¼Œé€‰æ‹©ä½•ç§å˜åŒ–ç¼“è§£ç­–ç•¥é«˜åº¦ä¾èµ–äºæµ‹è¯•æ—¶é‡åˆ°çš„å…·ä½“å˜åŒ–ç±»å‹ã€‚å› æ­¤ï¼Œä»…åœ¨æµ‹è¯•æ—¶æ£€æµ‹æ•°æ®é›†å˜åŒ–æ˜¯ä¸å¤Ÿçš„ï¼Œç²¾ç¡®è¯†åˆ«å‡ºå“ªç§å˜åŒ–è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºé¦–ä¸ªé’ˆå¯¹æˆåƒæ•°æ®é›†çš„æ— ç›‘ç£æ•°æ®é›†å˜åŒ–è¯†åˆ«æ¡†æ¶ï¼Œæœ‰æ•ˆåŒºåˆ†ç”±æ ‡ç­¾åˆ†å¸ƒå˜åŒ–å¼•èµ·çš„æ™®éå˜åŒ–ã€ç”±è¾“å…¥ç‰¹å¾å˜åŒ–å¼•èµ·çš„åå˜é‡å˜åŒ–å’Œæ··åˆå˜åŒ–ï¼ˆåŒæ—¶å‘ç”Ÿæ™®éå˜åŒ–å’Œåå˜é‡å˜åŒ–ï¼‰ã€‚æœ¬æ–‡è®¨è®ºè‡ªç›‘ç£ç¼–ç å™¨åœ¨æ£€æµ‹ç»†å¾®åå˜é‡å˜åŒ–ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºä¸€ç§æ–°å‹å˜åŒ–æ£€æµ‹å™¨ï¼Œè¯¥æ£€æµ‹å™¨åˆ©ç”¨è‡ªç›‘ç£ç¼–ç å™¨å’Œä»»åŠ¡æ¨¡å‹è¾“å‡ºä»¥æé«˜å˜åŒ–æ£€æµ‹æ•ˆæœã€‚å®éªŒè¯æ˜ï¼Œè¯¥å˜åŒ–è¯†åˆ«æ¡†æ¶åœ¨ä¸‰ç§ä¸åŒçš„æˆåƒæ¨¡å¼ï¼ˆèƒ¸éƒ¨æ”¾å°„ã€æ•°å­—ä¹³è…ºæ‘„å½±å’Œçœ¼åº•å›¾åƒï¼‰ä¸Šå¯¹äº”ç§ç°å®ä¸–ç•Œæ•°æ®é›†çš„äº”ç§ç±»å‹å˜åŒ–å‡æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®åˆ†å¸ƒçš„å˜åŒ–å¯èƒ½å¯¹ä¸´åºŠAIæ¨¡å‹çš„æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ï¼Œå¯¼è‡´è¯¯è¯Šæ–­ã€‚</li>
<li>æµ‹è¯•æ—¶çš„æ•°æ®é›†å˜åŒ–æ£€æµ‹æ˜¯ä¸å¤Ÿçš„ï¼Œéœ€è¦ç²¾ç¡®è¯†åˆ«å˜åŒ–çš„ç±»å‹ã€‚</li>
<li>æ™®éå˜åŒ–ã€åå˜é‡å˜åŒ–å’Œæ··åˆå˜åŒ–æ˜¯æ•°æ®é›†å˜åŒ–çš„ä¸‰ç§ä¸»è¦ç±»å‹ã€‚</li>
<li>è‡ªç›‘ç£ç¼–ç å™¨åœ¨æ£€æµ‹ç»†å¾®åå˜é‡å˜åŒ–ä¸­èµ·åˆ°é‡è¦ä½œç”¨ã€‚</li>
<li>æå‡ºçš„æ— ç›‘ç£æ•°æ®é›†å˜åŒ–è¯†åˆ«æ¡†æ¶èƒ½æœ‰æ•ˆåŒºåˆ†è¿™ä¸‰ç§å˜åŒ–ç±»å‹ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨è‡ªç›‘ç£ç¼–ç å™¨å’Œä»»åŠ¡æ¨¡å‹è¾“å‡ºï¼Œæé«˜äº†å˜åŒ–æ£€æµ‹çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07940">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f68155fc37015d37eb9f1f5515653bc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc3c97454cdeef4d97089a0ef37666c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45d651516ad9b23343cde847910ced37.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Medical-Artificial-Intelligence-for-Early-Detection-of-Lung-Cancer-A-Survey"><a href="#Medical-Artificial-Intelligence-for-Early-Detection-of-Lung-Cancer-A-Survey" class="headerlink" title="Medical Artificial Intelligence for Early Detection of Lung Cancer: A   Survey"></a>Medical Artificial Intelligence for Early Detection of Lung Cancer: A   Survey</h2><p><strong>Authors:Guohui Cai, Ying Cai, Zeyu Zhang, Yuanzhouhan Cao, Lin Wu, Daji Ergu, Zhinbin Liao, Yang Zhao</strong></p>
<p>Lung cancer remains one of the leading causes of morbidity and mortality worldwide, making early diagnosis critical for improving therapeutic outcomes and patient prognosis. Computer-aided diagnosis systems, which analyze computed tomography images, have proven effective in detecting and classifying pulmonary nodules, significantly enhancing the detection rate of early-stage lung cancer. Although traditional machine learning algorithms have been valuable, they exhibit limitations in handling complex sample data. The recent emergence of deep learning has revolutionized medical image analysis, driving substantial advancements in this field. This review focuses on recent progress in deep learning for pulmonary nodule detection, segmentation, and classification. Traditional machine learning methods, such as support vector machines and k-nearest neighbors, have shown limitations, paving the way for advanced approaches like Convolutional Neural Networks, Recurrent Neural Networks, and Generative Adversarial Networks. The integration of ensemble models and novel techniques is also discussed, emphasizing the latest developments in lung cancer diagnosis. Deep learning algorithms, combined with various analytical techniques, have markedly improved the accuracy and efficiency of pulmonary nodule analysis, surpassing traditional methods, particularly in nodule classification. Although challenges remain, continuous technological advancements are expected to further strengthen the role of deep learning in medical diagnostics, especially for early lung cancer detection and diagnosis. A comprehensive list of lung cancer detection models reviewed in this work is available at <a target="_blank" rel="noopener" href="https://github.com/CaiGuoHui123/Awesome-Lung-Cancer-Detection">https://github.com/CaiGuoHui123/Awesome-Lung-Cancer-Detection</a>. </p>
<blockquote>
<p>è‚ºç™Œä»ç„¶æ˜¯å…¨çƒå‘ç—…ç‡å’Œæ­»äº¡ç‡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œå› æ­¤æ—©æœŸè¯Šæ–­å¯¹äºæé«˜æ²»ç–—æ•ˆæœå’Œæ‚£è€…é¢„åè‡³å…³é‡è¦ã€‚è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿé€šè¿‡åˆ†æè®¡ç®—æœºæ–­å±‚æ‰«æå›¾åƒï¼Œåœ¨æ£€æµ‹å’Œåˆ†ç±»è‚ºç»“èŠ‚æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜æ—©æœŸè‚ºç™Œçš„æ£€æµ‹ç‡ã€‚è™½ç„¶ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•å¾ˆæœ‰ä»·å€¼ï¼Œä½†åœ¨å¤„ç†å¤æ‚æ ·æœ¬æ•°æ®æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æœ€è¿‘æ·±åº¦å­¦ä¹ çš„å‡ºç°å½»åº•æ”¹å˜äº†åŒ»å­¦å›¾åƒåˆ†æï¼Œä¸ºè¿™ä¸ªé¢†åŸŸå¸¦æ¥äº†å·¨å¤§çš„è¿›æ­¥ã€‚è¿™ç¯‡ç»¼è¿°é‡ç‚¹å…³æ³¨äº†æ·±åº¦å­¦ä¹ åœ¨è‚ºç»“èŠ‚æ£€æµ‹ã€åˆ†å‰²å’Œåˆ†ç±»æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå¦‚æ”¯æŒå‘é‡æœºå’Œkè¿‘é‚»ç®—æ³•ï¼Œå·²æ˜¾ç¤ºå‡ºå…¶å±€é™æ€§ï¼Œä¸ºå·ç§¯ç¥ç»ç½‘ç»œã€å¾ªç¯ç¥ç»ç½‘ç»œå’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œç­‰å…ˆè¿›æ–¹æ³•é“ºå¹³äº†é“è·¯ã€‚è¿˜è®¨è®ºäº†é›†æˆæ¨¡å‹å’Œæ–°æŠ€æœ¯çš„ä¸€ä½“åŒ–ï¼Œå¼ºè°ƒäº†è‚ºç™Œè¯Šæ–­é¢†åŸŸçš„æœ€æ–°å‘å±•ã€‚æ·±åº¦å­¦ä¹ ç®—æ³•ç»“åˆå„ç§åˆ†ææŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†è‚ºç»“èŠ‚åˆ†æçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»“èŠ‚åˆ†ç±»æ–¹é¢ã€‚è™½ç„¶ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä½†éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œé¢„è®¡æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„ä½œç”¨å°†å¾—åˆ°è¿›ä¸€æ­¥åŠ å¼ºï¼Œç‰¹åˆ«æ˜¯åœ¨æ—©æœŸè‚ºç™Œæ£€æµ‹å’Œè¯Šæ–­æ–¹é¢ã€‚æœ¬å·¥ä½œæ‰€ç»¼è¿°çš„è‚ºç™Œæ£€æµ‹æ¨¡å‹çš„è¯¦ç»†åˆ—è¡¨å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CaiGuoHui123/Awesome-Lung-Cancer-Detection%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CaiGuoHui123/Awesome-Lung-Cancer-Detectionä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14769v2">PDF</a> Accepted to Engineering Applications of Artificial Intelligence</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‚ºç™Œä»æ˜¯å…¨çƒä¸»è¦çš„ç–¾ç—…è‡´æ­»åŸå› ï¼Œè®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿé€šè¿‡åˆ†æè®¡ç®—æœºæ–­å±‚æ‰«æå›¾åƒåœ¨æ£€æµ‹æ—©æœŸè‚ºç™Œæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœã€‚ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•åœ¨å¤„ç†å¤æ‚æ ·æœ¬æ•°æ®æ—¶å­˜åœ¨å±€é™æ€§ï¼Œè€Œæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å‡ºç°æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„é‡å¤§è¿›å±•ã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»äº†æ·±åº¦å­¦ä¹ åœ¨è‚ºç»“èŠ‚æ£€æµ‹ã€åˆ†å‰²å’Œåˆ†ç±»æ–¹é¢çš„æœ€æ–°è¿›å±•ï¼Œä»¥åŠé›†æˆæ¨¡å‹å’Œæ–°æŠ€æœ¯åœ¨è‚ºç™Œè¯Šæ–­ä¸­çš„åº”ç”¨ã€‚å°½ç®¡ä»æœ‰æŒ‘æˆ˜ï¼Œä½†æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥æœ‰æœ›è¿›ä¸€æ­¥åŠ å¼ºæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—©æœŸè‚ºç™Œæ£€æµ‹ä¸è¯Šæ–­æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚ºç™Œä»æ˜¯å…¨çƒä¸»è¦çš„å¥åº·å¨èƒï¼Œæ—©æœŸè¯Šæ–­å¯¹æ”¹å–„æ²»ç–—æ•ˆæœå’Œæ‚£è€…é¢„åè‡³å…³é‡è¦ã€‚</li>
<li>è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿé€šè¿‡è§£æè®¡ç®—æœºæ–­å±‚æ‰«æå›¾åƒï¼Œåœ¨æ£€æµ‹è‚ºç»“èŠ‚æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœï¼Œè¿›è€Œæœ‰åŠ©äºæ—©æœŸè‚ºç™Œçš„è¯†åˆ«ã€‚</li>
<li>ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•åœ¨å¤„ç†å¤æ‚åŒ»å­¦å›¾åƒæ ·æœ¬æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå¸¦æ¥é©å‘½æ€§å˜åŒ–ï¼Œæ¨åŠ¨è‚ºç»“èŠ‚æ£€æµ‹ã€åˆ†å‰²å’Œåˆ†ç±»çš„æ˜¾è‘—è¿›å±•ã€‚</li>
<li>å·ç§¯ç¥ç»ç½‘ç»œã€é€’å½’ç¥ç»ç½‘ç»œå’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œç­‰å…ˆè¿›æ·±åº¦å­¦ä¹ æŠ€æœ¯åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†æï¼Œæå‡è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>é›†æˆæ¨¡å‹å’Œæ–°æŠ€æœ¯åœ¨è‚ºç™Œè¯Šæ–­ä¸­çš„æœ€æ–°å‘å±•å—åˆ°å…³æ³¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db06965bf7d7d5e9c93f36bc40073ec3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1aacd0b6aab50dae07859896952dc60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29e5862247db150ccd6210e984c1a512.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b71411a434517bb002f741b9f4525dd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89ff4f383db43870f6b2bc307f60c0ea.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Direct3Î³-A-Pipeline-for-Direct-Three-gamma-PET-Image-Reconstruction"><a href="#Direct3Î³-A-Pipeline-for-Direct-Three-gamma-PET-Image-Reconstruction" class="headerlink" title="Direct3Î³: A Pipeline for Direct Three-gamma PET Image   Reconstruction"></a>Direct3Î³: A Pipeline for Direct Three-gamma PET Image   Reconstruction</h2><p><strong>Authors:Youness Mellak, Alexandre Bousse, Thibaut Merlin, Debora Giovagnoli, Dimitris Visvikis</strong></p>
<p>This paper presents a novel image reconstruction pipeline for three-gamma (3-{\gamma}) positron emission tomography (PET) aimed at improving spatial resolution and reducing noise in nuclear medicine; the proposed Direct3{\gamma} pipeline addresses the inherent challenges in 3-{\gamma} PET systems, such as detector imperfections and uncertainty in photon interaction points, with a key feature being its ability to determine the order of interactions through a model trained on Monte Carlo (MC) simulations using the Geant4 Application for Tomography Emission (GATE) toolkit, thus providing the necessary information to construct Compton cones which intersect with the line of response (LOR) to estimate the emission point; the pipeline processes 3-{\gamma} PET raw data, reconstructs histoimages by propagating energy and spatial uncertainties along the LOR, and applies a 3-D convolutional neural network (CNN) to refine these intermediate images into high-quality reconstructions, further enhancing image quality through supervised learning and adversarial losses that preserve fine structural details; experimental results show that Direct3{\gamma} consistently outperforms conventional 200-ps time-of-flight (TOF) PET in terms of structural similarity index measure (SSIM) and peak signal-to-noise ratio (PSNR). </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸‰ä¼½é©¬ï¼ˆ3-Î³ï¼‰æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰çš„æ–°å‹å›¾åƒé‡å»ºæµç¨‹ï¼Œæ—¨åœ¨æé«˜æ ¸åŒ»å­¦ä¸­çš„ç©ºé—´åˆ†è¾¨ç‡å¹¶é™ä½å™ªå£°ã€‚æ‰€æå‡ºçš„Direct3Î³æµç¨‹è§£å†³äº†3-Î³ PETç³»ç»Ÿå›ºæœ‰çš„æŒ‘æˆ˜ï¼Œå¦‚æ¢æµ‹å™¨çš„ä¸å®Œç¾æ€§å’Œå…‰å­äº¤äº’ç‚¹çš„ä¸ç¡®å®šæ€§ã€‚å…¶ä¸»è¦åŠŸèƒ½æ˜¯é€šè¿‡ä½¿ç”¨Geant4å‘å°„æ–­å±‚æ‰«æåº”ç”¨ç¨‹åºï¼ˆGATEï¼‰å·¥å…·åŒ…è¿›è¡Œè’™ç‰¹å¡æ´›ï¼ˆMCï¼‰æ¨¡æ‹Ÿè®­ç»ƒçš„æ¨¡å‹æ¥ç¡®å®šäº¤äº’çš„é¡ºåºï¼Œä»è€Œæä¾›æ„å»ºäº¤äºå“åº”çº¿ï¼ˆLORï¼‰çš„åº·æ™®é¡¿é”¥æ‰€å¿…éœ€çš„ä¿¡æ¯æ¥ä¼°è®¡å‘å°„ç‚¹ã€‚è¯¥æµç¨‹å¤„ç†3-Î³ PETåŸå§‹æ•°æ®ï¼Œé€šè¿‡æ²¿LORä¼ æ’­èƒ½é‡å’Œç©ºé—´ä¸ç¡®å®šæ€§æ¥é‡å»ºç›´æ–¹å›¾åƒï¼Œå¹¶åº”ç”¨ä¸‰ç»´å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¥å°†è¿™äº›ä¸­é—´å›¾åƒç»†åŒ–ä¸ºé«˜è´¨é‡é‡å»ºï¼Œé€šè¿‡ä¿ç•™ç²¾ç»†ç»“æ„ç»†èŠ‚çš„ç›‘ç£å’Œå¯¹æŠ—æŸå¤±ï¼Œè¿›ä¸€æ­¥æé«˜å›¾åƒè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDirect3Î³åœ¨ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰å’Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æ–¹é¢å§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„200çš®ç§’é£è¡Œæ—¶é—´ï¼ˆTOFï¼‰PETã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18337v7">PDF</a> 11 pages, 11 figures, 2 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹ä¸‰ä¼½é©¬ï¼ˆ3-Î³ï¼‰æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰çš„æ–°å‹å›¾åƒé‡å»ºæµç¨‹ï¼Œæ—¨åœ¨æé«˜æ ¸åŒ»å­¦ä¸­çš„ç©ºé—´åˆ†è¾¨ç‡å¹¶é™ä½å™ªå£°ã€‚æå‡ºçš„Direct3Î³æµç¨‹è§£å†³äº†3-Î³ PETç³»ç»Ÿå›ºæœ‰çš„æŒ‘æˆ˜ï¼Œå¦‚æ¢æµ‹å™¨ç¼ºé™·å’Œå…‰å­äº¤äº’ç‚¹çš„ä¸ç¡®å®šæ€§ã€‚å…¶å…³é”®åŠŸèƒ½æ˜¯é€šè¿‡ä½¿ç”¨Geant4å‘å°„æ–­å±‚æ‰«æåº”ç”¨ç¨‹åºï¼ˆGATEï¼‰å·¥å…·åŒ…è¿›è¡Œè’™ç‰¹å¡æ´›ï¼ˆMCï¼‰æ¨¡æ‹Ÿè®­ç»ƒæ¨¡å‹æ¥ç¡®å®šäº¤äº’çš„é¡ºåºï¼Œä»è€Œæä¾›æ„å»ºä¸çº¿æ€§å“åº”ï¼ˆLORï¼‰ç›¸äº¤çš„åº·æ™®é¡¿é”¥çš„å¿…è¦ä¿¡æ¯ï¼Œä»¥ä¼°è®¡å‘å°„ç‚¹ã€‚è¯¥æµç¨‹å¤„ç†3-Î³ PETåŸå§‹æ•°æ®ï¼Œé€šè¿‡æ²¿LORä¼ æ’­èƒ½é‡å’Œç©ºé—´ä¸ç¡®å®šæ€§æ¥é‡å»ºç›´æ–¹å›¾åƒï¼Œå¹¶åº”ç”¨ä¸‰ç»´å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¯¹è¿™äº›ä¸­é—´å›¾åƒè¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œç”Ÿæˆé«˜è´¨é‡çš„é‡æ„å›¾åƒã€‚é€šè¿‡ä¿ç•™ç²¾ç»†ç»“æ„ç»†èŠ‚çš„ç›‘ç£å­¦ä¹ å’Œå¯¹æŠ—æ€§æŸå¤±ï¼Œè¿›ä¸€æ­¥æé«˜å›¾åƒè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDirect3Î³åœ¨ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰å’Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æ–¹é¢å§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„200çš®ç§’æ—¶é—´é£è¡Œï¼ˆTOFï¼‰PETã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Direct3Î³æ˜¯ä¸€ç§é’ˆå¯¹ä¸‰ä¼½é©¬ï¼ˆ3-Î³ï¼‰PETçš„æ–°å‹å›¾åƒé‡å»ºæµç¨‹ï¼Œæ—¨åœ¨æ”¹å–„æ ¸åŒ»å­¦ä¸­çš„ç©ºé—´åˆ†è¾¨ç‡å’Œå™ªå£°é—®é¢˜ã€‚</li>
<li>è¯¥æµç¨‹è§£å†³äº†3-Î³ PETç³»ç»Ÿçš„å›ºæœ‰æŒ‘æˆ˜ï¼Œå¦‚æ¢æµ‹å™¨ç¼ºé™·å’Œå…‰å­äº¤äº’ç‚¹çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>Direct3Î³é€šè¿‡è®­ç»ƒæ¨¡å‹ç¡®å®šå…‰å­äº¤äº’çš„é¡ºåºï¼Œè¯¥æ¨¡å‹ä½¿ç”¨GATEå·¥å…·åŒ…è¿›è¡Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿã€‚</li>
<li>æµç¨‹åŒ…æ‹¬å¤„ç†3-Î³ PETåŸå§‹æ•°æ®ï¼Œé‡å»ºç›´æ–¹å›¾åƒï¼Œå¹¶é€šè¿‡CNNç²¾ç»†åŒ–å¤„ç†ç”Ÿæˆé«˜è´¨é‡é‡æ„å›¾åƒã€‚</li>
<li>Direct3Î³é€šè¿‡ç»“åˆç›‘ç£å­¦ä¹ å’Œå¯¹æŠ—æ€§æŸå¤±ï¼Œæé«˜äº†å›¾åƒè´¨é‡å¹¶ä¿ç•™äº†ç²¾ç»†ç»“æ„ç»†èŠ‚ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDirect3Î³åœ¨ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰å’Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„æ—¶é—´é£è¡ŒPETæŠ€æœ¯ã€‚</li>
<li>è¯¥æµç¨‹æœ‰æœ›ä¸ºæ ¸åŒ»å­¦ä¸­çš„å›¾åƒé‡å»ºæä¾›æ›´é«˜çš„ç©ºé—´åˆ†è¾¨ç‡å’Œæ›´ä½çš„å™ªå£°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18337">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a1154d9e286348dbfbd07f3f7c3b6d8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1931270b4ad292783e7b011a4495116.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e4395f32ce1e19efa528a2dbaf2e6f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf6181951dfb16932106658d26fa1577.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d3193083c913e5a2bbde118dd4d3bc2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Enhancing-Weakly-Supervised-3D-Medical-Image-Segmentation-through-Probabilistic-aware-Learning"><a href="#Enhancing-Weakly-Supervised-3D-Medical-Image-Segmentation-through-Probabilistic-aware-Learning" class="headerlink" title="Enhancing Weakly Supervised 3D Medical Image Segmentation through   Probabilistic-aware Learning"></a>Enhancing Weakly Supervised 3D Medical Image Segmentation through   Probabilistic-aware Learning</h2><p><strong>Authors:Runmin Jiang, Zhaoxin Fan, Junhao Wu, Lenghan Zhu, Xin Huang, Tianyang Wang, Heng Huang, Min Xu</strong></p>
<p>3D medical image segmentation is a challenging task with crucial implications for disease diagnosis and treatment planning. Recent advances in deep learning have significantly enhanced fully supervised medical image segmentation. However, this approach heavily relies on labor-intensive and time-consuming fully annotated ground-truth labels, particularly for 3D volumes. To overcome this limitation, we propose a novel probabilistic-aware weakly supervised learning pipeline, specifically designed for 3D medical imaging. Our pipeline integrates three innovative components: a Probability-based Pseudo Label Generation technique for synthesizing dense segmentation masks from sparse annotations, a Probabilistic Multi-head Self-Attention network for robust feature extraction within our Probabilistic Transformer Network, and a Probability-informed Segmentation Loss Function to enhance training with annotation confidence. Demonstrating significant advances, our approach not only rivals the performance of fully supervised methods but also surpasses existing weakly supervised methods in CT and MRI datasets, achieving up to 18.1% improvement in Dice scores for certain organs. The code is available at <a target="_blank" rel="noopener" href="https://github.com/runminjiang/PW4MedSeg">https://github.com/runminjiang/PW4MedSeg</a>. </p>
<blockquote>
<p>ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¯¹äºç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ·±åº¦å­¦ä¹ çš„æœ€æ–°è¿›å±•æå¤§åœ°ä¿ƒè¿›äº†å®Œå…¨ç›‘ç£çš„åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸¥é‡ä¾èµ–äºåŠ³åŠ¨å¯†é›†å‹å’Œè€—æ—¶çš„å¤§é‡çœŸå®æ ‡ç­¾ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä¸‰ç»´ä½“ç§¯æ•°æ®ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ¦‚ç‡æ„ŸçŸ¥å¼±ç›‘ç£å­¦ä¹ æµç¨‹ï¼Œä¸“é—¨ç”¨äºä¸‰ç»´åŒ»å­¦å½±åƒã€‚æˆ‘ä»¬çš„æµç¨‹é›†æˆäº†ä¸‰ä¸ªåˆ›æ–°ç»„ä»¶ï¼šä¸€ç§åŸºäºæ¦‚ç‡çš„ä¼ªæ ‡ç­¾ç”ŸæˆæŠ€æœ¯ï¼Œç”¨äºä»ç¨€ç–æ³¨é‡Šä¸­åˆæˆå¯†é›†åˆ†å‰²æ©è†œï¼›ä¸€ä¸ªæ¦‚ç‡å¤šå¤´è‡ªæ³¨æ„åŠ›ç½‘ç»œï¼Œç”¨äºåœ¨æˆ‘ä»¬çš„æ¦‚ç‡å˜æ¢ç½‘ç»œä¸­å®ç°ç¨³å¥çš„ç‰¹å¾æå–ï¼›ä»¥åŠä¸€ç§åŸºäºæ¦‚ç‡çš„åˆ†å‰²æŸå¤±å‡½æ•°ï¼Œåˆ©ç”¨æ³¨é‡Šç½®ä¿¡åº¦å¢å¼ºè®­ç»ƒã€‚é€šè¿‡å±•ç¤ºæ˜¾è‘—è¿›å±•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¸å…¨ç›‘ç£æ–¹æ³•æ€§èƒ½ç›¸å½“ï¼Œè€Œä¸”åœ¨CTå’ŒMRIæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†ç°æœ‰çš„å¼±ç›‘ç£æ–¹æ³•ï¼Œåœ¨æŸäº›å™¨å®˜çš„ç‹„å…‹å¾—åˆ†ä¸Šæé«˜äº†é«˜è¾¾18.1%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/runminjiang/PW4MedSeg%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/runminjiang/PW4MedSegè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.02566v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹3DåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°å‹æ¦‚ç‡æ„ŸçŸ¥å¼±ç›‘ç£å­¦ä¹ ç®¡é“ï¼ŒåŒ…æ‹¬æ¦‚ç‡ä¼ªæ ‡ç­¾ç”ŸæˆæŠ€æœ¯ã€æ¦‚ç‡å¤šå¤´è‡ªæ³¨æ„åŠ›ç½‘ç»œå’Œæ¦‚ç‡æ„ŸçŸ¥åˆ†å‰²æŸå¤±å‡½æ•°ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¸å…¨ç›‘ç£æ–¹æ³•æ€§èƒ½ç›¸å½“ï¼Œè€Œä¸”åœ¨CTå’ŒMRIæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰å¼±ç›‘ç£æ–¹æ³•ï¼ŒæŸäº›å™¨å®˜çš„Diceå¾—åˆ†æé«˜äº†18.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DåŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ä¸­çš„å…³é”®ä»»åŠ¡ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨å…¨ç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä¾èµ–å¤§é‡æ‰‹åŠ¨æ ‡æ³¨æ•°æ®ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¦‚ç‡æ„ŸçŸ¥å¼±ç›‘ç£å­¦ä¹ ç®¡é“ï¼Œç”¨äº3DåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>ç®¡é“åŒ…å«æ¦‚ç‡ä¼ªæ ‡ç­¾ç”ŸæˆæŠ€æœ¯ã€æ¦‚ç‡å¤šå¤´è‡ªæ³¨æ„åŠ›ç½‘ç»œå’Œæ¦‚ç‡æ„ŸçŸ¥åˆ†å‰²æŸå¤±å‡½æ•°ã€‚</li>
<li>æ‰€ææ–¹æ³•æ€§èƒ½ä¸å…¨ç›‘ç£æ–¹æ³•ç›¸å½“ï¼Œå¹¶åœ¨CTå’ŒMRIæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰å¼±ç›‘ç£æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•çš„ä»£ç å·²å…¬å¼€ï¼Œå¯åœ¨ç‰¹å®šGitHubä»“åº“ä¸­æ‰¾åˆ°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.02566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-894b303d3859902e658cc319d83172e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-890edf6e66ee7d279b82231448e1ff45.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7ee744a958b32293531a11658bfc12aa.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-24  Joint Tensor-Train Parameterization for Efficient and Expressive   Low-Rank Adaptation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a799f04e6f9a1dee2d9c0fab87800e50.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-24  Co-Seg++ Mutual Prompt-Guided Collaborative Learning for Versatile   Medical Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24231k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
