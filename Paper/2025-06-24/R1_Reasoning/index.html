<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-24  Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models An Empirical Evaluation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-81785035976daa414d4c9fa7bf86cc8c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-24-æ›´æ–°"><a href="#2025-06-24-æ›´æ–°" class="headerlink" title="2025-06-24 æ›´æ–°"></a>2025-06-24 æ›´æ–°</h1><h2 id="Chain-of-Thought-Prompting-Obscures-Hallucination-Cues-in-Large-Language-Models-An-Empirical-Evaluation"><a href="#Chain-of-Thought-Prompting-Obscures-Hallucination-Cues-in-Large-Language-Models-An-Empirical-Evaluation" class="headerlink" title="Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models: An Empirical Evaluation"></a>Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models: An Empirical Evaluation</h2><p><strong>Authors:Jiahao Cheng, Tiancheng Su, Jia Yuan, Guoxiu He, Jiawei Liu, Xinqi Tao, Jingwen Xie, Huaxia Li</strong></p>
<p>Large Language Models (LLMs) often exhibit \textit{hallucinations}, generating factually incorrect or semantically irrelevant content in response to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by encouraging step-by-step reasoning, but its impact on hallucination detection remains underexplored. To bridge this gap, we conduct a systematic empirical evaluation. We begin with a pilot experiment, revealing that CoT reasoning significantly affects the LLMâ€™s internal states and token probability distributions. Building on this, we evaluate the impact of various CoT prompting methods on mainstream hallucination detection methods across both instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three key dimensions: changes in hallucination score distributions, variations in detection accuracy, and shifts in detection confidence. Our findings show that while CoT prompting helps reduce hallucination frequency, it also tends to obscure critical signals used for detection, impairing the effectiveness of various detection methods. Our study highlights an overlooked trade-off in the use of reasoning. Code is publicly available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/cot-hallu-detect">https://anonymous.4open.science/r/cot-hallu-detect</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å¸¸è¡¨ç°å‡ºâ€œå¹»è§‰â€ï¼Œå³å¯¹æç¤ºç”Ÿæˆäº‹å®é”™è¯¯æˆ–è¯­ä¹‰ä¸ç›¸å…³çš„å†…å®¹ã€‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå¯ä»¥é€šè¿‡é¼“åŠ±é€æ­¥æ¨ç†æ¥ç¼“è§£å¹»è§‰ï¼Œä½†å…¶å¯¹å¹»è§‰æ£€æµ‹çš„å½±å“ä»è¢«å¿½è§†ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯ç ”ç©¶ã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œäº†ä¸€é¡¹è¯•ç‚¹å®éªŒï¼Œç»“æœæ˜¾ç¤ºCoTæ¨ç†æ˜¾è‘—å½±å“äº†LLMçš„å†…éƒ¨çŠ¶æ€å’Œä»¤ç‰Œæ¦‚ç‡åˆ†å¸ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¯„ä¼°äº†å„ç§CoTæç¤ºæ–¹æ³•å¯¹ä¸»æµå¹»è§‰æ£€æµ‹æ–¹æ³•çš„æ£€æµ‹å½±å“ï¼Œæ¶‰åŠæŒ‡ä»¤è°ƒä¼˜å’Œé¢å‘æ¨ç†çš„LLMã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬è€ƒå¯Ÿä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šå¹»è§‰è¯„åˆ†åˆ†å¸ƒçš„å˜åŒ–ã€æ£€æµ‹å‡†ç¡®æ€§çš„å˜åŒ–ä»¥åŠæ£€æµ‹ä¿¡å¿ƒçš„è½¬å˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶CoTæç¤ºæœ‰åŠ©äºå‡å°‘å¹»è§‰é¢‘ç‡ï¼Œä½†å®ƒä¹Ÿå¯èƒ½æ©ç›–ç”¨äºæ£€æµ‹çš„å…³é”®ä¿¡å·ï¼Œä»è€ŒæŸå®³å„ç§æ£€æµ‹æ–¹æ³•çš„æ•ˆåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†åœ¨ä½¿ç”¨æ¨ç†æ—¶å¿½è§†çš„ä¸€ä¸ªæƒè¡¡ã€‚ä»£ç å…¬å¼€å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/cot-hallu-detect%E3%80%82">https://anonymous.4open.science/r/cot-hallu-detectã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17088v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®¹æ˜“å‡ºç°å¹»è§‰ç°è±¡ï¼Œå³ç”Ÿæˆä¸æç¤ºä¸ç¬¦çš„äº‹å®ä¸Šæˆ–è¯­ä¹‰ä¸Šæ— å…³çš„å†…å®¹ã€‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå¯ä»¥é€šè¿‡é¼“åŠ±é€æ­¥æ¨ç†æ¥ç¼“è§£å¹»è§‰ç°è±¡ï¼Œä½†å…¶å¯¹å¹»è§‰æ£€æµ‹çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯ç ”ç©¶ã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œäº†ä¸€é¡¹è¯•ç‚¹å®éªŒï¼Œå‘ç°CoTæ¨ç†æ˜¾è‘—å½±å“LLMçš„å†…éƒ¨çŠ¶æ€å’Œä»¤ç‰Œæ¦‚ç‡åˆ†å¸ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¯„ä¼°äº†å„ç§CoTæç¤ºæ–¹æ³•å¯¹ä¸»æµå¹»è§‰æ£€æµ‹æ–¹æ³•çš„å½±å“ï¼Œæ¶µç›–æŒ‡ä»¤è°ƒä¼˜å’Œé¢å‘æ¨ç†çš„LLMã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šå¹»è§‰åˆ†æ•°åˆ†å¸ƒçš„å˜åŒ–ã€æ£€æµ‹å‡†ç¡®æ€§çš„å˜åŒ–ä»¥åŠæ£€æµ‹ä¿¡å¿ƒçš„è½¬ç§»ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè™½ç„¶CoTæç¤ºæœ‰åŠ©äºå‡å°‘å¹»è§‰é¢‘ç‡ï¼Œä½†å®ƒä¹Ÿå¯èƒ½æ©ç›–ç”¨äºæ£€æµ‹çš„å…³é”®ä¿¡å·ï¼ŒæŸå®³å„ç§æ£€æµ‹æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‡¸æ˜¾äº†åœ¨ä½¿ç”¨æ¨ç†æ—¶çš„ä¸€ä¸ªè¢«å¿½è§†çš„æƒè¡¡ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r%2Fcot-hallu-detect%E3%80%82">https://anonymous.4open.science/r/cot-hallu-detectã€‚</a> </p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å›åº”æç¤ºæ—¶å¯èƒ½å‡ºç°å¹»è§‰ç°è±¡ï¼Œå³ç”Ÿæˆä¸äº‹å®ä¸ç¬¦æˆ–è¯­ä¹‰ä¸ç›¸å…³çš„å†…å®¹ã€‚</li>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºèƒ½å¤Ÿé€šè¿‡é¼“åŠ±é€æ­¥æ¨ç†æ¥å‡å°‘å¹»è§‰ç°è±¡ã€‚</li>
<li>å¯¹CoTæç¤ºå¯¹å¹»è§‰æ£€æµ‹çš„å½±å“è¿›è¡Œç³»ç»Ÿå®è¯ç ”ç©¶æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºè¿™ä¸€é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>CoTæ¨ç†æ˜¾è‘—å½±å“LLMçš„å†…éƒ¨çŠ¶æ€å’Œä»¤ç‰Œæ¦‚ç‡åˆ†å¸ƒï¼Œè¿™æ˜¯é€šè¿‡åˆæ­¥å®éªŒå‘ç°çš„ã€‚</li>
<li>CoTæç¤ºå¯èƒ½å½±å“ä¸»æµå¹»è§‰æ£€æµ‹æ–¹æ³•çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æ”¹å˜å¹»è§‰åˆ†æ•°åˆ†å¸ƒã€æ£€æµ‹å‡†ç¡®æ€§å’Œä¿¡å¿ƒã€‚</li>
<li>è™½ç„¶CoTæç¤ºæœ‰åŠ©äºå‡å°‘å¹»è§‰é¢‘ç‡ï¼Œä½†å®ƒä¹Ÿå¯èƒ½æ©ç›–ç”¨äºæ£€æµ‹çš„å…³é”®ä¿¡å·ï¼Œå¯¼è‡´æŸäº›æ£€æµ‹æ–¹æ³•æ•ˆæœä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-48a09b4bec85782fca355316fec8dccc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37e85487e3b3d22b59e1eb59ebf87266.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-796a4f0508590758ae8b352cb74ca8f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8470ed37decea58e3abb2b529d09897a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-30fe41e44e05c928711f130a781cb57a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Tower-Bridging-Generality-and-Translation-Specialization-in-Multilingual-LLMs"><a href="#Tower-Bridging-Generality-and-Translation-Specialization-in-Multilingual-LLMs" class="headerlink" title="Tower+: Bridging Generality and Translation Specialization in   Multilingual LLMs"></a>Tower+: Bridging Generality and Translation Specialization in   Multilingual LLMs</h2><p><strong>Authors:Ricardo Rei, Nuno M. Guerreiro, JosÃ© Pombal, JoÃ£o Alves, Pedro Teixeirinha, Amin Farajian, AndrÃ© F. T. Martins</strong></p>
<p>Fine-tuning pretrained LLMs has been shown to be an effective strategy for reaching state-of-the-art performance on specific tasks like machine translation. However, this process of adaptation often implies sacrificing general-purpose capabilities, such as conversational reasoning and instruction-following, hampering the utility of the system in real-world applications that require a mixture of skills. In this paper, we introduce Tower+, a suite of models designed to deliver strong performance across both translation and multilingual general-purpose text capabilities. We achieve a Pareto frontier between translation specialization and multilingual general-purpose capabilities by introducing a novel training recipe that builds on Tower (Alves et al., 2024), comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. At each stage of training, we carefully generate and curate data to strengthen performance on translation as well as general-purpose tasks involving code generation, mathematics problem solving, and general instruction-following. We develop models at multiple scales: 2B, 9B, and 72B. Our smaller models often outperform larger general-purpose open-weight and proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we introduce for evaluating both translation and instruction-following. Our findings highlight that it is possible to rival frontier models in general capabilities, while optimizing for specific business domains, such as translation and localization. </p>
<blockquote>
<p>å¯¹é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå¾®è°ƒï¼Œå·²è¢«è¯æ˜æ˜¯è¾¾åˆ°æœºå™¨ç¿»è¯‘ç­‰ç‰¹å®šä»»åŠ¡çš„æœ€å…ˆè¿›æ€§èƒ½çš„æœ‰æ•ˆç­–ç•¥ã€‚ç„¶è€Œï¼Œè¿™ä¸ªé€‚åº”è¿‡ç¨‹é€šå¸¸æ„å‘³ç€ç‰ºç‰²é€šç”¨èƒ½åŠ›ï¼Œå¦‚å¯¹è¯æ¨ç†å’ŒæŒ‡ä»¤éµå¾ªï¼Œä»è€Œé˜»ç¢äº†ç³»ç»Ÿåœ¨éœ€è¦æ··åˆæŠ€èƒ½çš„å®é™…åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Tower+æ¨¡å‹ç³»åˆ—ï¼Œæ—¨åœ¨å®ç°ç¿»è¯‘å’Œè·¨è¯­è¨€é€šç”¨æ–‡æœ¬èƒ½åŠ›çš„åŒé‡é«˜æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„è®­ç»ƒé…æ–¹ï¼Œåœ¨ç¿»è¯‘ä¸“ä¸šåŒ–å’Œè·¨è¯­è¨€é€šç”¨èƒ½åŠ›ä¹‹é—´å–å¾—äº†å¸•ç´¯æ‰˜å‰æ²¿ã€‚è¯¥é…æ–¹ä»¥Towerï¼ˆAlvesç­‰äººï¼Œ2024ï¼‰ä¸ºåŸºç¡€ï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒã€åå¥½ä¼˜åŒ–ä»¥åŠå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚åœ¨è®­ç»ƒçš„æ¯ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬éƒ½ä¼šç²¾å¿ƒç”Ÿæˆå’Œç­›é€‰æ•°æ®ï¼Œä»¥åŠ å¼ºç¿»è¯‘ä»¥åŠæ¶‰åŠä»£ç ç”Ÿæˆã€æ•°å­¦é—®é¢˜è§£å†³å’Œä¸€èˆ¬æŒ‡ä»¤éµå¾ªçš„é€šç”¨ä»»åŠ¡çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¼€å‘äº†å¤šç§è§„æ¨¡çš„æ¨¡å‹ï¼š2Bã€9Bå’Œ72Bã€‚æˆ‘ä»¬çš„è¾ƒå°æ¨¡å‹é€šå¸¸è¡¨ç°ä¼˜äºè¾ƒå¤§çš„é€šç”¨å…¬å¼€å’Œä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚Llama 3.3 70Bã€GPT-4oï¼‰ã€‚æˆ‘ä»¬çš„å¤§å‹æ¨¡å‹åœ¨é«˜èµ„æºè¯­è¨€ç¿»è¯‘æ–¹é¢è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨å¤šè¯­è¨€Arena Hardè¯„ä¼°å’Œæˆ‘ä»¬æ‰€å¼•å…¥çš„ç”¨äºè¯„ä¼°ç¿»è¯‘å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„IF-MTåŸºå‡†æµ‹è¯•ä¸­å–å¾—é¡¶å°–ç»“æœã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨é’ˆå¯¹ç‰¹å®šä¸šåŠ¡é¢†åŸŸè¿›è¡Œä¼˜åŒ–æ—¶ï¼Œå¯ä»¥åœ¨é€šç”¨èƒ½åŠ›æ–¹é¢è¾¾åˆ°å‰æ²¿æ¨¡å‹çš„æ°´å¹³ï¼Œå¦‚ç¿»è¯‘å’Œæœ¬åœ°åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17080v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Tower+æ¨¡å‹ç³»åˆ—ï¼Œè¯¥ç³»åˆ—æ¨¡å‹æ—¨åœ¨å®ç°ç¿»è¯‘å’Œå¤šè¯­ç§é€šç”¨æ–‡æœ¬èƒ½åŠ›çš„å¹³è¡¡ã€‚é€šè¿‡å¼•å…¥æ–°çš„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒã€åå¥½ä¼˜åŒ–å’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼Œè¾¾åˆ°ç¿»è¯‘ä¸“ä¸šåŒ–ä¸å¤šè¯­ç§é€šç”¨èƒ½åŠ›ä¹‹é—´çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚å¼€å‘ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬2Bã€9Bå’Œ72Bï¼Œå¹¶åœ¨ç¿»è¯‘å’Œé€šç”¨ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚æœ€å¤§æ¨¡å‹åœ¨é«˜èµ„æºè¯­è¨€ç¿»è¯‘æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œå¹¶åœ¨å¤šè¯­ç§Arena Hardè¯„ä¼°å’Œæ–°çš„IF-MTåŸºå‡†æµ‹è¯•ä¸­å–å¾—é¡¶å°–ç»“æœã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯ä»¥åœ¨ä¼˜åŒ–ç‰¹å®šä¸šåŠ¡é¢†åŸŸï¼ˆå¦‚ç¿»è¯‘å’Œæœ¬åœ°åŒ–ï¼‰çš„åŒæ—¶ï¼Œä¸å‰æ²¿æ¨¡å‹åœ¨é€šç”¨èƒ½åŠ›æ–¹é¢ç›¸åŒ¹æ•Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Tower+æ¨¡å‹ç³»åˆ—æ—¨åœ¨å¹³è¡¡ç¿»è¯‘å’Œå¤šç§è¯­è¨€é€šç”¨æ–‡æœ¬èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ–°çš„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒç­‰ï¼Œå®ç°æ¨¡å‹ä¼˜åŒ–ã€‚</li>
<li>æ¨¡å‹åœ¨ç¿»è¯‘å’Œé€šç”¨ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸åŒè§„æ¨¡çš„æ¨¡å‹å‡æœ‰è‰¯å¥½è¡¨ç°ã€‚</li>
<li>æœ€å¤§æ¨¡å‹åœ¨é«˜èµ„æºè¯­è¨€ç¿»è¯‘æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œå¹¶åœ¨å¤šè¯­ç§è¯„ä¼°ä¸­å–å¾—é¡¶å°–ç»“æœã€‚</li>
<li>å¼•å…¥æ–°çš„IF-MTåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°ç¿»è¯‘å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯ä»¥åœ¨ä¼˜åŒ–ç‰¹å®šä¸šåŠ¡é¢†åŸŸçš„åŒæ—¶ï¼Œä¿æŒæ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-938187e11a954f6183f36eab58d7d3a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cf2b924c33bc7987b47f277cebe22f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a52d10368cb4bebc80ade7724feb278.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a41e9aab2c6b929e9f64f064645d83a0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Universal-Music-Representations-Evaluating-Foundation-Models-on-World-Music-Corpora"><a href="#Universal-Music-Representations-Evaluating-Foundation-Models-on-World-Music-Corpora" class="headerlink" title="Universal Music Representations? Evaluating Foundation Models on World   Music Corpora"></a>Universal Music Representations? Evaluating Foundation Models on World   Music Corpora</h2><p><strong>Authors:Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos</strong></p>
<p>Foundation models have revolutionized music information retrieval, but questions remain about their ability to generalize across diverse musical traditions. This paper presents a comprehensive evaluation of five state-of-the-art audio foundation models across six musical corpora spanning Western popular, Greek, Turkish, and Indian classical traditions. We employ three complementary methodologies to investigate these modelsâ€™ cross-cultural capabilities: probing to assess inherent representations, targeted supervised fine-tuning of 1-2 layers, and multi-label few-shot learning for low-resource scenarios. Our analysis shows varying cross-cultural generalization, with larger models typically outperforming on non-Western music, though results decline for culturally distant traditions. Notably, our approaches achieve state-of-the-art performance on five out of six evaluated datasets, demonstrating the effectiveness of foundation models for world music understanding. We also find that our targeted fine-tuning approach does not consistently outperform probing across all settings, suggesting foundation models already encode substantial musical knowledge. Our evaluation framework and benchmarking results contribute to understanding how far current models are from achieving universal music representations while establishing metrics for future progress. </p>
<blockquote>
<p>æ¨¡å‹åŸºç¡€åœ¨éŸ³ä¹ä¿¡æ¯æ£€ç´¢é¢†åŸŸå·²ç»å‘ç”Ÿäº†é©å‘½æ€§çš„å˜é©ï¼Œä½†åœ¨ä¸åŒéŸ³ä¹ä¼ ç»Ÿä¸­çš„æ³›åŒ–èƒ½åŠ›ä»å­˜åœ¨ç–‘é—®ã€‚æœ¬æ–‡å…¨é¢è¯„ä¼°äº†äº”ç§æœ€æ–°éŸ³é¢‘æ¨¡å‹åŸºç¡€ï¼Œæ¶‰åŠå…­ç§éŸ³ä¹è¯­æ–™åº“ï¼ŒåŒ…æ‹¬è¥¿æ–¹æµè¡Œã€å¸Œè…Šã€åœŸè€³å…¶å’Œå°åº¦å¤å…¸ä¼ ç»Ÿã€‚æˆ‘ä»¬é‡‡ç”¨ä¸‰ç§äº’è¡¥æ–¹æ³•æ¥ç ”ç©¶è¿™äº›æ¨¡å‹çš„è·¨æ–‡åŒ–èƒ½åŠ›ï¼šæ¢æµ‹ä»¥è¯„ä¼°å†…åœ¨è¡¨ç¤ºã€å¯¹1-2å±‚è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„ç›‘ç£å¾®è°ƒï¼Œä»¥åŠç”¨äºä½èµ„æºåœºæ™¯çš„å¤šå…ƒæ ‡ç­¾å°æ ·æœ¬å­¦ä¹ ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè·¨æ–‡åŒ–æ³›åŒ–å­˜åœ¨å·®å¼‚ï¼Œæ›´å¤§çš„æ¨¡å‹é€šå¸¸åœ¨éè¥¿æ–¹éŸ³ä¹ä¸Šè¡¨ç°æ›´å¥½ï¼Œä½†å¯¹äºæ–‡åŒ–å·®å¼‚è¾ƒå¤§çš„ä¼ ç»Ÿï¼Œç»“æœä¼šæœ‰æ‰€ä¸‹é™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…­ä¸ªè¯„ä¼°æ•°æ®é›†ä¸­æœ‰äº”ä¸ªè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œè¯æ˜äº†æ¨¡å‹åŸºç¡€åœ¨ä¸–ç•ŒéŸ³ä¹ç†è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œæˆ‘ä»¬çš„æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒæ–¹æ³•å¹¶ä¸å§‹ç»ˆåœ¨æ‰€æœ‰è®¾ç½®ä¸­ä¼˜äºæ¢æµ‹æ–¹æ³•ï¼Œè¿™è¡¨æ˜æ¨¡å‹åŸºç¡€å·²ç»ç¼–ç äº†å¤§é‡çš„éŸ³ä¹çŸ¥è¯†ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•ç»“æœæœ‰åŠ©äºäº†è§£å½“å‰æ¨¡å‹è·ç¦»å®ç°é€šç”¨éŸ³ä¹è¡¨ç¤ºè¿˜æœ‰å¤šè¿œï¼ŒåŒæ—¶ä¸ºæœªæ¥è¿›æ­¥å»ºç«‹äº†æŒ‡æ ‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17055v1">PDF</a> Accepted at ISMIR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…¨é¢è¯„ä¼°äº†äº”æ¬¾å…ˆè¿›çš„éŸ³é¢‘åŸºç¡€æ¨¡å‹åœ¨æ¶µç›–è¥¿æ–¹æµè¡Œã€å¸Œè…Šã€åœŸè€³å…¶å’Œå°åº¦å¤å…¸ä¼ ç»Ÿç­‰å…­ç§éŸ³ä¹è¯­æ–™åº“ä¸­çš„è·¨æ–‡åŒ–æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡é‡‡ç”¨ä¸‰ç§äº’è¡¥æ–¹æ³•â€”â€”æ¢æµ‹å†…åœ¨è¡¨å¾ã€å¯¹ä¸€è‡³ä¸¤å±‚è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„ç›‘ç£å¾®è°ƒä»¥åŠç”¨äºä½èµ„æºåœºæ™¯çš„å¤šå…ƒæ ‡ç­¾å°‘æ ·æœ¬å­¦ä¹ ï¼Œç ”ç©¶å‘ç°è¿™äº›æ¨¡å‹çš„è·¨æ–‡åŒ–èƒ½åŠ›å­˜åœ¨ä¸åŒç¨‹åº¦çš„å·®å¼‚ï¼Œå¤§å‹æ¨¡å‹åœ¨éè¥¿æ–¹éŸ³ä¹ä¸Šçš„è¡¨ç°é€šå¸¸è¾ƒå¥½ï¼Œä½†åœ¨æ–‡åŒ–å·®å¼‚è¾ƒå¤§çš„ä¼ ç»Ÿä¸Šè¡¨ç°ä¸‹é™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ï¼Œè¯æ˜äº†åŸºç¡€æ¨¡å‹åœ¨ä¸–ç•ŒéŸ³ä¹ç†è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚åŒæ—¶ï¼Œå‘ç°æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒæ–¹æ³•å¹¶ä¸æ€»æ˜¯åœ¨æ‰€æœ‰è®¾ç½®ä¸­ä¼˜äºæ¢æµ‹æ–¹æ³•ï¼Œè¿™è¡¨æ˜åŸºç¡€æ¨¡å‹å·²ç»ç¼–ç äº†å¤§é‡çš„éŸ³ä¹çŸ¥è¯†ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•ç»“æœæœ‰åŠ©äºäº†è§£å½“å‰æ¨¡å‹è·ç¦»å®ç°é€šç”¨éŸ³ä¹è¡¨ç¤ºçš„ç¨‹åº¦ï¼Œå¹¶ä¸ºæœªæ¥è¿›æ­¥æä¾›äº†åº¦é‡æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘åŸºç¡€æ¨¡å‹åœ¨è·¨æ–‡åŒ–éŸ³ä¹ä¿¡æ¯æ£€ç´¢ä¸­è¡¨ç°å‡ºä¸€å®šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å­˜åœ¨ä¸åŒç¨‹åº¦çš„å·®å¼‚ã€‚</li>
<li>å¤§å‹æ¨¡å‹åœ¨éè¥¿æ–¹éŸ³ä¹ä¸Šçš„è¡¨ç°é€šå¸¸è¾ƒå¥½ã€‚</li>
<li>åœ¨æ–‡åŒ–å·®å¼‚è¾ƒå¤§çš„ä¼ ç»Ÿä¸Šï¼Œæ¨¡å‹è¡¨ç°å¯èƒ½ä¼šä¸‹é™ã€‚</li>
<li>æ¢æµ‹å†…åœ¨è¡¨å¾ã€ç›‘ç£å¾®è°ƒä»¥åŠå¤šå…ƒæ ‡ç­¾å°‘æ ·æœ¬å­¦ä¹ ç­‰æ–¹æ³•è¢«ç”¨äºè¯„ä¼°æ¨¡å‹èƒ½åŠ›ã€‚</li>
<li>é’ˆå¯¹æ€§çš„å¾®è°ƒæ–¹æ³•å¹¶ä¸æ€»æ˜¯ä¼˜äºæ¢æµ‹æ–¹æ³•ï¼Œè¡¨æ˜åŸºç¡€æ¨¡å‹å·²å…·å¤‡ä¸€å®šçš„éŸ³ä¹çŸ¥è¯†ç¼–ç èƒ½åŠ›ã€‚</li>
<li>åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ï¼Œè¯æ˜äº†åŸºç¡€æ¨¡å‹åœ¨ä¸–ç•ŒéŸ³ä¹ç†è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ef6b3ad7f0e46b926c06dd99107e3707.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4bc8c35aa40a515721127ed2f2eff5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f64a9f8dd2c8a8a3ec2556ee6cdf815f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55166605e64162bcc33a95edb4f4f24b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01e8305c9ecb63e3ba1683b0d0dfc759.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08c819e598b7abcc1ddb1935a60d39c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3c5000f235cf6a0c604f881f3cee922.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Enhancing-Step-by-Step-and-Verifiable-Medical-Reasoning-in-MLLMs"><a href="#Enhancing-Step-by-Step-and-Verifiable-Medical-Reasoning-in-MLLMs" class="headerlink" title="Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs"></a>Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs</h2><p><strong>Authors:Haoran Sun, Yankai Jiang, Wenjie Lou, Yujie Zhang, Wenjie Li, Lilong Wang, Mianxin Liu, Lei Liu, Xiaosong Wang</strong></p>
<p>Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing chain-of-thought (CoT) training data is essential for bolstering the reasoning abilities of medical MLLMs. However, existing approaches exhibit a deficiency in offering a comprehensive framework for searching and evaluating effective reasoning paths towards critical diagnosis. To address this challenge, we propose Mentor-Intern Collaborative Search (MICS), a novel reasoning-path searching scheme to generate rigorous and effective medical CoT data. MICS first leverages mentor models to initialize the reasoning, one step at a time, then prompts each intern model to continue the thinking along those initiated paths, and finally selects the optimal reasoning path according to the overall reasoning performance of multiple intern models. The reasoning performance is determined by an MICS-Score, which assesses the quality of generated reasoning paths. Eventually, we construct MMRP, a multi-task medical reasoning dataset with ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum learning strategy, with robust visual question-answering and generalizable reasoning capabilities. Extensive experiments demonstrate that Chiron-o1, trained on our CoT dataset constructed using MICS, achieves state-of-the-art performance across a list of medical visual question answering and reasoning benchmarks. Codes are available at GitHub - manglu097&#x2F;Chiron-o1: Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¼€å§‹åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸­å±•ç°å‡ºç¨³å¥çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ä»å¤„äºåˆçº§é˜¶æ®µã€‚æ„å»ºæ€ç»´é“¾ï¼ˆCoTï¼‰è®­ç»ƒæ•°æ®å¯¹äºå¢å¼ºåŒ»ç–—MLLMsçš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨æœç´¢å’Œè¯„ä¼°æœ‰æ•ˆæ¨ç†è·¯å¾„ä»¥è¿›è¡Œå…³é”®è¯Šæ–­æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¯¼å¸ˆå®ä¹ ç”Ÿåä½œæœç´¢ï¼ˆMICSï¼‰è¿™ä¸€æ–°é¢–çš„æ¨ç†è·¯å¾„æœç´¢æ–¹æ¡ˆï¼Œä»¥ç”Ÿæˆä¸¥è°¨æœ‰æ•ˆçš„åŒ»ç–—CoTæ•°æ®ã€‚MICSé¦–å…ˆåˆ©ç”¨å¯¼å¸ˆæ¨¡å‹é€æ­¥è¿›è¡Œæ¨ç†åˆå§‹åŒ–ï¼Œç„¶åæç¤ºæ¯ä¸ªå®ä¹ ç”Ÿæ¨¡å‹ç»§ç»­æ²¿ç€è¿™äº›åˆå§‹åŒ–çš„è·¯å¾„è¿›è¡Œæ€è€ƒï¼Œå¹¶æœ€ç»ˆæ ¹æ®å¤šä¸ªå®ä¹ ç”Ÿæ¨¡å‹çš„æ€»ä½“æ¨ç†æ€§èƒ½é€‰æ‹©æœ€ä½³æ¨ç†è·¯å¾„ã€‚æ¨ç†æ€§èƒ½ç”±MICSè¯„åˆ†ç¡®å®šï¼Œè¯¥è¯„åˆ†è¯„ä¼°ç”Ÿæˆæ¨ç†è·¯å¾„çš„è´¨é‡ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æ„å»ºäº†MMRPï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æ’åéš¾åº¦çš„å¤šä»»åŠ¡åŒ»ç–—æ¨ç†æ•°æ®é›†ï¼Œä»¥åŠé€šè¿‡è¯¾ç¨‹å­¦ä¹ ç­–ç•¥å¼€å‘çš„æ–°å‹åŒ»ç–—MLLMâ€”â€”å¥‡ä¼¦o1ï¼Œå…·æœ‰å¼ºå¤§çš„è§†è§‰é—®ç­”å’Œé€šç”¨æ¨ç†èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨æˆ‘ä»¬ä½¿ç”¨MICSæ„å»ºçš„CoTæ•°æ®é›†ä¸Šè®­ç»ƒçš„å¥‡ä¼¦o1ï¼Œåœ¨åŒ»ç–—è§†è§‰é—®ç­”å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨GitHubï¼ˆmanglu099&#x2F;Chiron-o1: Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMsï¼‰ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16962v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—é¢†åŸŸä¸­çš„åº”ç”¨ã€‚ä¸ºæé«˜åŒ»ç–—MLLLsçš„æ¨ç†èƒ½åŠ›ï¼Œæ„å»ºæ€ç»´é“¾ï¼ˆCoTï¼‰è®­ç»ƒæ•°æ®è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹å…¨é¢æœç´¢å’Œè¯„ä¼°æœ‰æ•ˆæ¨ç†è·¯å¾„ä»¥è¿›è¡Œå…³é”®è¯Šæ–­çš„æ¡†æ¶ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†å¯¼å¸ˆå®ä¹ ç”Ÿåä½œæœç´¢ï¼ˆMICSï¼‰è¿™ä¸€æ–°çš„æ¨ç†è·¯å¾„æœç´¢æ–¹æ¡ˆï¼Œä»¥ç”Ÿæˆä¸¥è°¨æœ‰æ•ˆçš„åŒ»ç–—CoTæ•°æ®ã€‚é€šè¿‡å¯¼å¸ˆæ¨¡å‹é€æ­¥åˆå§‹åŒ–æ¨ç†ï¼Œç„¶åæç¤ºå®ä¹ ç”Ÿæ¨¡å‹ç»§ç»­è¿™äº›åˆå§‹åŒ–çš„è·¯å¾„è¿›è¡Œæ€è€ƒï¼Œå¹¶æ ¹æ®å¤šä¸ªå®ä¹ ç”Ÿæ¨¡å‹çš„æ€»ä½“æ¨ç†æ€§èƒ½é€‰æ‹©æœ€ä½³æ¨ç†è·¯å¾„ã€‚æœ€ç»ˆæ„å»ºäº†å¤šä»»åŠ¡åŒ»ç–—æ¨ç†æ•°æ®é›†MMRPå’Œå…·æœ‰å¼ºå¤§è§†è§‰é—®ç­”å’Œé€šç”¨æ¨ç†èƒ½åŠ›çš„æ–°å‹åŒ»ç–—MLLLMâ€”â€”å¥‡è½®o1ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨åˆ©ç”¨MICSæ„å»ºçš„CoTæ•°æ®é›†ä¸Šè®­ç»ƒçš„å¥‡è½®o1åœ¨åŒ»ç–—è§†è§‰é—®ç­”å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLLMsï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„æ¨ç†èƒ½åŠ›å°šå¤„äºæ—©æœŸé˜¶æ®µã€‚</li>
<li>æ€ç»´é“¾ï¼ˆCoTï¼‰è®­ç»ƒæ•°æ®å¯¹æå‡åŒ»ç–—MLLLMsçš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨æœç´¢å’Œè¯„ä¼°æœ‰æ•ˆæ¨ç†è·¯å¾„ä»¥è¿›è¡Œå…³é”®è¯Šæ–­æ—¶å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æå‡ºå¯¼å¸ˆå®ä¹ ç”Ÿåä½œæœç´¢ï¼ˆMICSï¼‰æ–¹æ¡ˆæ¥ç”ŸæˆåŒ»ç–—CoTæ•°æ®ï¼Œä»¥æé«˜æ¨ç†çš„ä¸¥è°¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>MICSæ–¹æ¡ˆé€šè¿‡å¯¼å¸ˆæ¨¡å‹é€æ­¥åˆå§‹åŒ–æ¨ç†ï¼Œå¹¶æç¤ºå®ä¹ ç”Ÿæ¨¡å‹ç»§ç»­æ€è€ƒï¼Œé€‰æ‹©æœ€ä½³æ¨ç†è·¯å¾„ã€‚</li>
<li>æ„å»ºäº†å¤šä»»åŠ¡åŒ»ç–—æ¨ç†æ•°æ®é›†MMRPå’Œæ–°å‹åŒ»ç–—MLLLMâ€”â€”å¥‡è½®o1ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a6bc32eb3156854ee01da8d20b02048a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-947e92b3bbf23d67bd1dcf20a21fee21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6407ca05391c3adeaa23b491d1b1a746.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Co-VisiON-Co-Visibility-ReasONing-on-Sparse-Image-Sets-of-Indoor-Scenes"><a href="#Co-VisiON-Co-Visibility-ReasONing-on-Sparse-Image-Sets-of-Indoor-Scenes" class="headerlink" title="Co-VisiON: Co-Visibility ReasONing on Sparse Image Sets of Indoor Scenes"></a>Co-VisiON: Co-Visibility ReasONing on Sparse Image Sets of Indoor Scenes</h2><p><strong>Authors:Chao Chen, Nobel Dang, Juexiao Zhang, Wenkai Sun, Pengfei Zheng, Xuhang He, Yimeng Ye, Taarun Srinivas, Chen Feng</strong></p>
<p>Humans exhibit a remarkable ability to recognize co-visibility-the overlapping regions visible in multiple images-even when these images are sparsely distributed across a complex scene. This capability is foundational in 3D vision and robotic perception. Despite significant progress in vision learning, it remains unclear whether current vision models have reached human-level proficiency in co-visibility analysis. In this work, we introduce the Co-Visibility reasONing (Co-VisiON) benchmark, designed to directly evaluate co-visibility reasoning on sparse image sets across over 1000 indoor scenarios. Our experiments reveal that while co-visibility is typically treated as a low-level feature matching task, it poses a significant challenge for existing vision models under sparse conditions. Notably, a proprietary vision-language model outperforms all purely vision-based approaches, with all models lagging substantially behind human performance. This gap underscores the need for more than basic pairwise vision processing-it calls for a comprehensive spatial understanding through high-level reasoning across multiple views. Inspired by human visual cognition, we propose a novel multi-view baseline, Covis, which achieves top performance among pure vision models and narrows the gap to the proprietary VLM. We hope our benchmark and findings will spur further advancements in developing vision models capable of robust, high-level reasoning in challenging, sparse environments. Our dataset and source code can be found at: <a target="_blank" rel="noopener" href="https://ai4ce.github.io/CoVISION">https://ai4ce.github.io/CoVISION</a> </p>
<blockquote>
<p>äººç±»è¡¨ç°å‡ºä¸€ç§ä»¤äººå°è±¡æ·±åˆ»çš„è¯†åˆ«å…±è§†èƒ½åŠ›â€”â€”å³ä½¿åœ¨å¤æ‚çš„åœºæ™¯ä¸­ï¼Œå¤šå¼ å›¾åƒç¨€ç–åˆ†å¸ƒæ—¶ï¼Œä¹Ÿèƒ½è¯†åˆ«å‡ºå›¾åƒä¹‹é—´å¯è§çš„é‡å åŒºåŸŸã€‚è¿™ä¸€èƒ½åŠ›åœ¨3Dè§†è§‰å’Œæœºå™¨äººæ„ŸçŸ¥ä¸­æ˜¯åŸºç¡€æ€§çš„ã€‚å°½ç®¡è§†è§‰å­¦ä¹ æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å½“å‰è§†è§‰æ¨¡å‹åœ¨å…±è§†åˆ†ææ–¹é¢æ˜¯å¦è¾¾åˆ°äº†äººç±»æ°´å¹³çš„èƒ½åŠ›ä»ä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…±è§†æ¨ç†ï¼ˆCo- VisiONï¼‰åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç›´æ¥è¯„ä¼°è·¨è¶Š1000ä¸ªå®¤å†…åœºæ™¯çš„ç¨€ç–å›¾åƒé›†ä¸Šçš„å…±è§†æ¨ç†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè™½ç„¶å…±è§†é€šå¸¸è¢«è§†ä¸ºä½çº§çš„ç‰¹å¾åŒ¹é…ä»»åŠ¡ï¼Œä½†åœ¨ç¨€ç–æ¡ä»¶ä¸‹ï¼Œå®ƒå¯¹ç°æœ‰çš„è§†è§‰æ¨¡å‹æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸€ä¸ªä¸“æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ‰€æœ‰çº¯è§†è§‰æ–¹æ³•ä¸­éƒ½è¡¨ç°æœ€å¥½ï¼Œè€Œæ‰€æœ‰æ¨¡å‹ä¸äººç±»æ€§èƒ½éƒ½æœ‰å¾ˆå¤§çš„å·®è·ã€‚è¿™ä¸€å·®è·å‡¸æ˜¾äº†ä¸ä»…éœ€è¦åŸºæœ¬çš„é…å¯¹è§†è§‰å¤„ç†ï¼Œè€Œä¸”éœ€è¦é€šè¿‡è·¨å¤šä¸ªè§†å›¾çš„é«˜çº§æ¨ç†è¿›è¡Œå…¨é¢ç©ºé—´ç†è§£ã€‚å—äººç±»è§†è§‰è®¤çŸ¥çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šè§†å›¾åŸºçº¿æ–¹æ³•Covisï¼Œå®ƒåœ¨çº¯è§†è§‰æ¨¡å‹ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½å¹¶ç¼©å°äº†ä¸ä¸“æœ‰è§†è§‰è¯­è¨€æ¨¡å‹çš„å·®è·ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œç ”ç©¶æˆæœå°†ä¿ƒè¿›åœ¨å¼€å‘èƒ½å¤Ÿåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¨€ç–ç¯å¢ƒä¸­è¿›è¡Œç¨³å¥é«˜çº§æ¨ç†çš„è§†è§‰æ¨¡å‹æ–¹é¢å–å¾—è¿›ä¸€æ­¥è¿›å±•ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œæºä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://ai4ce.github.io/CoVISION">https://ai4ce.github.io/CoVISION</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16805v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹å…³äºå…±å¯è§æ€§æ¨ç†ï¼ˆCo-Visibility reasoningï¼‰çš„ç ”ç©¶ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å…±å¯è§æ€§æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆCo-VisONï¼‰ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ç¨€ç–å›¾åƒé›†ä¸Šçš„å…±å¯è§æ€§åˆ†æèƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨ç¨€ç–æ¡ä»¶ä¸‹çš„å…±å¯è§æ€§æ¨ç†å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸€ç§ç»“åˆè§†è§‰å’Œè¯­è¨€æ¨¡å‹çš„æ€§èƒ½è¡¨ç°æœ€ä½³ï¼Œä½†ä»è½åäºäººç±»è¡¨ç°ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å¤šè§†è§’åŸºçº¿æ–¹æ³•Covisï¼Œåœ¨çº¯è§†è§‰æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»èƒ½å¤Ÿè¯†åˆ«å›¾åƒä¸­çš„å…±å¯è§æ€§ï¼Œè¿™åœ¨3Dè§†è§‰å’Œæœºå™¨äººæ„ŸçŸ¥ä¸­éå¸¸é‡è¦ã€‚</li>
<li>å½“å‰è§†è§‰æ¨¡å‹åœ¨å…±å¯è§æ€§åˆ†ææ–¹é¢å°šæœªè¾¾åˆ°äººç±»æ°´å¹³ã€‚</li>
<li>å¼•å…¥Co-Visibility reasONingï¼ˆCo-VisiONï¼‰åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°ç¨€ç–å›¾åƒé›†ä¸Šçš„å…±å¯è§æ€§æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰è§†è§‰æ¨¡å‹åœ¨ç¨€ç–æ¡ä»¶ä¸‹çš„å…±å¯è§æ€§æ¨ç†å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ä¸€ç§ç»“åˆè§†è§‰å’Œè¯­è¨€æ¨¡å‹çš„æ€§èƒ½æœ€ä½³ï¼Œä½†ä»è¿œè¿œè½åäºäººç±»è¡¨ç°ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šè§†è§’åŸºçº¿æ–¹æ³•Covisï¼Œåœ¨çº¯è§†è§‰æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>è¯¥ç ”ç©¶å’ŒåŸºå‡†æµ‹è¯•å°†ä¿ƒè¿›åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¨€ç–ç¯å¢ƒä¸­å¼€å‘å…·å¤‡ç¨³å¥ã€é«˜çº§æ¨ç†èƒ½åŠ›çš„è§†è§‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16805">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f877c1786b0e5ae556a56d70d169a87b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b93d245063f0f7247cf02a675f81770.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81785035976daa414d4c9fa7bf86cc8c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23aff7aca313abfdfbf430214d6adfef.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Language-Informed-Synthesis-of-Rational-Agent-Models-for-Grounded-Theory-of-Mind-Reasoning-On-The-Fly"><a href="#Language-Informed-Synthesis-of-Rational-Agent-Models-for-Grounded-Theory-of-Mind-Reasoning-On-The-Fly" class="headerlink" title="Language-Informed Synthesis of Rational Agent Models for Grounded   Theory-of-Mind Reasoning On-The-Fly"></a>Language-Informed Synthesis of Rational Agent Models for Grounded   Theory-of-Mind Reasoning On-The-Fly</h2><p><strong>Authors:Lance Ying, Ryan Truong, Katherine M. Collins, Cedegao E. Zhang, Megan Wei, Tyler Brooke-Wilson, Tan Zhi-Xuan, Lionel Wong, Joshua B. Tenenbaum</strong></p>
<p>Drawing real world social inferences usually requires taking into account information from multiple modalities. Language is a particularly powerful source of information in social settings, especially in novel situations where language can provide both abstract information about the environment dynamics and concrete specifics about an agent that cannot be easily visually observed. In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a framework for drawing context-specific social inferences that integrate linguistic and visual inputs. LIRAS frames multimodal social reasoning as a process of constructing structured but situation-specific agent and environment representations - leveraging multimodal language models to parse language and visual inputs into unified symbolic representations, over which a Bayesian inverse planning engine can be run to produce granular probabilistic judgments. On a range of existing and new social reasoning tasks derived from cognitive science experiments, we find that our model (instantiated with a comparatively lightweight VLM) outperforms ablations and state-of-the-art models in capturing human judgments across all domains. </p>
<blockquote>
<p>æŠ½å–ç°å®ç¤¾ä¼šä¸­çš„æ¨æ–­é€šå¸¸éœ€è¦ç»¼åˆè€ƒè™‘æ¥è‡ªå¤šç§æ¨¡å¼çš„ä¿¡æ¯ã€‚åœ¨è¯­è¨€ç¯å¢ƒä¸­ï¼Œè¯­è¨€æ˜¯ä¸€ä¸ªç‰¹åˆ«å¼ºå¤§çš„ä¿¡æ¯æ¥æºï¼Œå°¤å…¶æ˜¯åœ¨æ–°æƒ…å†µä¸‹ï¼Œè¯­è¨€å¯ä»¥æä¾›å…³äºç¯å¢ƒåŠ¨æ€æŠ½è±¡ä¿¡æ¯å’Œå…³äºæ— æ³•è½»æ˜“è§‚å¯Ÿåˆ°çš„å®ä½“çš„å…·ä½“ç»†èŠ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­è¨€ä¿¡æ¯ç†æ€§ä»£ç†åˆæˆï¼ˆLIRASï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆè¯­è¨€å’Œè§†è§‰è¾“å…¥çš„ã€æŠ½å–ç‰¹å®šæƒ…å¢ƒç¤¾ä¼šæ¨æ–­çš„æ¡†æ¶ã€‚LIRASå°†å¤šæ¨¡å¼ç¤¾ä¼šæ¨ç†è§†ä¸ºä¸€ä¸ªè¿‡ç¨‹ï¼Œå³æ„å»ºç»“æ„åŒ–ä½†ç‰¹å®šäºæƒ…å¢ƒçš„ä»£ç†å’Œç¯å¢ƒè¡¨ç¤ºâ€”â€”åˆ©ç”¨å¤šæ¨¡å¼è¯­è¨€æ¨¡å‹è§£æè¯­è¨€å’Œè§†è§‰è¾“å…¥ä¸ºç»Ÿä¸€çš„ç¬¦å·è¡¨ç¤ºï¼Œåœ¨æ­¤åŸºç¡€ä¸Šè¿è¡Œè´å¶æ–¯é€†å‘è§„åˆ’å¼•æ“ï¼Œä»¥äº§ç”Ÿç²¾ç»†çš„æ¦‚ç‡åˆ¤æ–­ã€‚åœ¨æ¥è‡ªè®¤çŸ¥ç§‘å­¦å®éªŒçš„ä¸€ç³»åˆ—ç°æœ‰å’Œæ–°ç¤¾ä¼šæ¨ç†ä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„æ¨¡å‹ï¼ˆç”¨ç›¸å¯¹è½»é‡çº§çš„VLMå®ä¾‹åŒ–ï¼‰åœ¨æ•è·æ‰€æœ‰é¢†åŸŸçš„äººç±»åˆ¤æ–­æ–¹é¢ä¼˜äºåºŸé™¤å’Œæœ€æ–°æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16755v1">PDF</a> 5 figures, 19 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆè¯­è¨€å’Œè§†è§‰è¾“å…¥çš„ç¤¾ä¼šæ¨ç†æ¡†æ¶â€”â€”è¯­è¨€ä¿¡æ¯ç†æ€§ä»£ç†äººåˆæˆï¼ˆLIRASï¼‰ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å°†è¯­è¨€å’Œè§†è§‰è¾“å…¥è§£æä¸ºç»Ÿä¸€çš„ç¬¦å·è¡¨ç¤ºï¼Œå¹¶é€šè¿‡è´å¶æ–¯é€†è§„åˆ’å¼•æ“è¿è¡Œï¼Œä»¥äº§ç”Ÿç²¾ç»†çš„æ¦‚ç‡åˆ¤æ–­ï¼Œä»è€Œè¿›è¡Œæƒ…å¢ƒç‰¹å®šçš„ç¤¾ä¼šæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ¥è‡ªè®¤çŸ¥ç§‘å­¦å®éªŒçš„ç¤¾ä¼šæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¤¾ä¼šæ¨ç†æ¡†æ¶â€”â€”Language-Informed Rational Agent Synthesis (LIRAS)ã€‚</li>
<li>LIRASç»“åˆäº†è¯­è¨€å’Œè§†è§‰è¾“å…¥ï¼Œç”¨äºè¿›è¡Œç¤¾ä¼šæ¨ç†ã€‚</li>
<li>å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å°†è¯­è¨€å’Œè§†è§‰è¾“å…¥è§£æä¸ºç»Ÿä¸€çš„ç¬¦å·è¡¨ç¤ºã€‚</li>
<li>ä½¿ç”¨è´å¶æ–¯é€†è§„åˆ’å¼•æ“è¿›è¡Œæ¦‚ç‡åˆ¤æ–­ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªç¤¾ä¼šæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¿‡äº†ç°æœ‰æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹å¼ºè°ƒäº†è¯­è¨€åœ¨ç¤¾ä¼šæ¨ç†ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éš¾ä»¥é€šè¿‡è§†è§‰è§‚å¯Ÿè·å¾—å…·ä½“ä¿¡æ¯çš„æ–°æƒ…å¢ƒä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16755">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8f60dffdf7c2f7553a56b6d4801abf7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91f619e893807cfbb01d3a5d56410838.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26cdd1e9f2ecff3174a4a7aa5b03868a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Off-Policy-Actor-Critic-for-Adversarial-Observation-Robustness-Virtual-Alternative-Training-via-Symmetric-Policy-Evaluation"><a href="#Off-Policy-Actor-Critic-for-Adversarial-Observation-Robustness-Virtual-Alternative-Training-via-Symmetric-Policy-Evaluation" class="headerlink" title="Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual   Alternative Training via Symmetric Policy Evaluation"></a>Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual   Alternative Training via Symmetric Policy Evaluation</h2><p><strong>Authors:Kosuke Nakanishi, Akihiro Kubo, Yuji Yasui, Shin Ishii</strong></p>
<p>Recently, robust reinforcement learning (RL) methods designed to handle adversarial input observations have received significant attention, motivated by RLâ€™s inherent vulnerabilities. While existing approaches have demonstrated reasonable success, addressing worst-case scenarios over long time horizons requires both minimizing the agentâ€™s cumulative rewards for adversaries and training agents to counteract them through alternating learning. However, this process introduces mutual dependencies between the agent and the adversary, making interactions with the environment inefficient and hindering the development of off-policy methods. In this work, we propose a novel off-policy method that eliminates the need for additional environmental interactions by reformulating adversarial learning as a soft-constrained optimization problem. Our approach is theoretically supported by the symmetric property of policy evaluation between the agent and the adversary. The implementation is available at <a target="_blank" rel="noopener" href="https://github.com/nakanakakosuke/VALT_SAC">https://github.com/nakanakakosuke/VALT_SAC</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œä¸ºäº†å¤„ç†å¯¹æŠ—æ€§è¾“å…¥è§‚å¯Ÿè€Œè®¾è®¡çš„ç¨³å¥çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•å—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œè¿™æºäºRLå›ºæœ‰çš„è„†å¼±æ€§ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•å·²ç»å–å¾—äº†åˆç†çš„æˆåŠŸï¼Œä½†åœ¨é•¿æ—¶é—´èŒƒå›´å†…åº”å¯¹æœ€åæƒ…å†µåœºæ™¯éœ€è¦æœ€å°åŒ–ä»£ç†å¯¹å¯¹æ‰‹çš„ç´¯ç§¯å¥–åŠ±ï¼Œå¹¶è®­ç»ƒä»£ç†é€šè¿‡äº¤æ›¿å­¦ä¹ æ¥å¯¹æŠ—å®ƒä»¬ã€‚ç„¶è€Œï¼Œè¿™ä¸ªè¿‡ç¨‹åœ¨ä»£ç†å’Œå¯¹æ‰‹ä¹‹é—´å¼•å…¥äº†ç›¸äº’ä¾èµ–ï¼Œä½¿ä¸ç¯å¢ƒä¹‹é—´çš„äº’åŠ¨æ•ˆç‡ä½ä¸‹ï¼Œå¹¶é˜»ç¢äº†ç¦»çº¿æ–¹æ³•çš„å‘å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç¦»çº¿æ–¹æ³•ï¼Œå®ƒé€šè¿‡é‡æ–°å°†å¯¹å†³å­¦ä¹ è¡¨è¿°ä¸ºè½¯çº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹ç¯å¢ƒè¿›è¡Œé¢å¤–äº’åŠ¨çš„éœ€è¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¾—åˆ°äº†ä»£ç†å’Œå¯¹æ‰‹ä¹‹é—´æ”¿ç­–è¯„ä»·å¯¹ç§°æ€§çš„ç†è®ºæ”¯æŒã€‚å®æ–½ç»†èŠ‚å¯è§äº <a target="_blank" rel="noopener" href="https://github.com/nakanakakosuke/VALT_SAC%E3%80%82">https://github.com/nakanakakosuke/VALT_SACã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16753v1">PDF</a> ICML2025 poster, 39 pages, 6 figures, 13 tables. arXiv admin note:   text overlap with arXiv:2409.00418</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•åœ¨å¤„ç†å¯¹æŠ—æ€§è¾“å…¥è§‚å¯Ÿæ–¹é¢å—åˆ°å…³æ³¨ï¼Œå› RLå›ºæœ‰çš„è„†å¼±æ€§è€Œå¤‡å—æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•è™½å–å¾—ä¸€å®šæˆåŠŸï¼Œä½†åœ¨é•¿æ—¶é—´èŒƒå›´å†…åº”å¯¹æœ€ååœºæ™¯æ—¶ï¼Œéœ€è¦æœ€å°åŒ–ä»£ç†çš„ç´¯ç§¯å¥–åŠ±å¹¶ä¸ºä»£ç†è¿›è¡Œå¯¹æŠ—è®­ç»ƒã€‚ç„¶è€Œï¼Œè¿™ä¸€è¿‡ç¨‹åœ¨ä»£ç†å’Œå¯¹æ‰‹ä¹‹é—´å¼•å…¥äº†ç›¸äº’ä¾èµ–ï¼Œå¯¼è‡´ä¸ç¯å¢ƒäº’åŠ¨æ•ˆç‡ä½ä¸‹å¹¶é˜»ç¢ç¦»çº¿ç­–ç•¥æ–¹æ³•çš„å‘å±•ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç¦»çº¿ç­–ç•¥æ–¹æ³•ï¼Œé€šè¿‡å°†å¯¹æˆ˜å­¦ä¹ é‡æ–°è¡¨è¿°ä¸ºè½¯çº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œä»è€Œæ— éœ€é¢å¤–çš„ç¯å¢ƒäº’åŠ¨ã€‚æ­¤æ–¹æ³•çš„ç†è®ºæ”¯æŒæ¥è‡ªä»£ç†å’Œå¯¹æ‰‹ä¹‹é—´æ”¿ç­–è¯„ä»·çš„å¯¹ç§°æ€§ã€‚ä»£ç å®ç°å¯è§äº<a target="_blank" rel="noopener" href="https://github.com/nakanakakosuke/VALT_SAC">https://github.com/nakanakakosuke/VALT_SAC</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤„ç†å¯¹æŠ—æ€§è¾“å…¥è§‚å¯Ÿæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦åº”å¯¹é•¿æ—¶é—´èŒƒå›´å†…çš„æœ€ååœºæ™¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éœ€è¦åœ¨æœ€å°åŒ–ä»£ç†çš„ç´¯ç§¯å¥–åŠ±å’Œä¸ºä»£ç†è¿›è¡Œå¯¹æŠ—è®­ç»ƒä¹‹é—´å–å¾—å¹³è¡¡ã€‚</li>
<li>ä»£ç†å’Œå¯¹æ‰‹ä¹‹é—´çš„ç›¸äº’ä½œç”¨å¼•å…¥äº†ç›¸äº’ä¾èµ–ï¼Œé™ä½äº†ä¸ç¯å¢ƒäº’åŠ¨çš„æ•ˆç‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç¦»çº¿ç­–ç•¥æ–¹æ³•ï¼Œé€šè¿‡å¯¹æŠ—å­¦ä¹ è½¯çº¦æŸä¼˜åŒ–æ¥æ¶ˆé™¤å¯¹ç¯å¢ƒé¢å¤–äº’åŠ¨çš„éœ€æ±‚ã€‚</li>
<li>è¯¥æ–¹æ³•çš„ç†è®ºæ”¯æŒåŸºäºä»£ç†å’Œå¯¹æ‰‹åœ¨æ”¿ç­–è¯„ä»·ä¸Šçš„å¯¹ç§°æ€§ã€‚</li>
<li>æ–¹æ³•å®ç°çš„ä»£ç å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2cd611a8974e453c2a9241615858ffc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c32ee5f99b3baf2f07dc9be45983efe.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ReasonGRM-Enhancing-Generative-Reward-Models-through-Large-Reasoning-Models"><a href="#ReasonGRM-Enhancing-Generative-Reward-Models-through-Large-Reasoning-Models" class="headerlink" title="ReasonGRM: Enhancing Generative Reward Models through Large Reasoning   Models"></a>ReasonGRM: Enhancing Generative Reward Models through Large Reasoning   Models</h2><p><strong>Authors:Bin Chen, Xinzge Gao, Chuanrui Hu, Penghang Yu, Hua Zhang, Bing-Kun Bao</strong></p>
<p>Generative Reward Models (GRMs) provide greater flexibility than scalar reward models in capturing human preferences, but their effectiveness is limited by poor reasoning capabilities. This often results in incomplete or overly speculative reasoning paths, leading to hallucinations or missing key information in complex tasks. We address this challenge with ReasonGRM, a three-stage generative reward modeling framework. In the first stage, Zero-RL is used to generate concise, outcome-directed reasoning paths that reduce the likelihood of critical omissions. In the second stage, we introduce a novel evaluation metric, $R^\star$, which scores reasoning paths based on their generation likelihood. This favors paths that reach correct answers with minimal exploration, helping to reduce hallucination-prone data during training. In the final stage, the model is further refined through reinforcement learning on challenging examples to enhance its preference discrimination capabilities. Experiments on three public benchmarks show that ReasonGRM achieves competitive or state-of-the-art performance, outperforming previous best GRMs by 1.8% on average and surpassing proprietary models such as GPT-4o by up to 5.6%. These results demonstrate the effectiveness of reasoning-aware training and highlight the importance of high-quality rationale selection for reliable preference modeling. </p>
<blockquote>
<p>ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGRMsï¼‰åœ¨æ•æ‰äººç±»åå¥½æ–¹é¢æä¾›äº†æ¯”æ ‡é‡å¥–åŠ±æ¨¡å‹æ›´å¤§çš„çµæ´»æ€§ï¼Œä½†å…¶æœ‰æ•ˆæ€§å—åˆ°æ¨ç†èƒ½åŠ›ä¸è¶³çš„åˆ¶çº¦ã€‚è¿™é€šå¸¸ä¼šå¯¼è‡´æ¨ç†è·¯å¾„ä¸å®Œæ•´æˆ–è¿‡äºæŠ•æœºå–å·§ï¼Œä»è€Œåœ¨å¤æ‚ä»»åŠ¡ä¸­å‡ºç°å¹»è§‰æˆ–é—æ¼å…³é”®ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReasonGRMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µçš„ç”Ÿæˆå¥–åŠ±å»ºæ¨¡æ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨Zero-RLç”Ÿæˆç®€æ´ã€ä»¥ç»“æœä¸ºå¯¼å‘çš„æ¨ç†è·¯å¾„ï¼Œå‡å°‘å…³é”®é—æ¼çš„å¯èƒ½æ€§ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡$R^\star$ï¼Œæ ¹æ®æ¨ç†è·¯å¾„çš„ç”Ÿæˆå¯èƒ½æ€§å¯¹å…¶è¿›è¡Œè¯„åˆ†ã€‚è¿™æœ‰åˆ©äºä»¥æœ€å°æ¢ç´¢æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„è·¯å¾„ï¼Œæœ‰åŠ©äºå‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¹»è§‰å€¾å‘æ•°æ®ã€‚åœ¨æœ€åä¸€ä¸ªé˜¶æ®µï¼Œé€šè¿‡å¯¹æŒ‘æˆ˜æ ·æœ¬è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œè¿›ä¸€æ­¥ç²¾ç‚¼æ¨¡å‹ï¼Œæé«˜å…¶åå¥½è¾¨åˆ«èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒReasonGRMå…·æœ‰ç«äº‰åŠ›æˆ–è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹³å‡æ¯”ä¹‹å‰çš„æœ€ä½³GRMé«˜å‡º1.8%ï¼Œå¹¶è¶…è¿‡ä¸“æœ‰æ¨¡å‹å¦‚GPT-4oé«˜è¾¾5.6%ã€‚è¿™äº›ç»“æœè¯æ˜äº†æ¨ç†æ„ŸçŸ¥è®­ç»ƒçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¼ºè°ƒäº†é«˜è´¨é‡æ¨ç†é€‰æ‹©å¯¹äºå¯é åå¥½å»ºæ¨¡çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16712v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGRMsï¼‰åœ¨æ•æ‰äººç±»åå¥½æ—¶çµæ´»æ€§æ›´é«˜çš„äº‹å®ï¼Œæ–‡ç« æŒ‡å‡ºäº†å…¶å—é™äºè¾ƒå·®æ¨ç†èƒ½åŠ›çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ReasonGRMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µçš„ç”Ÿæˆå¥–åŠ±å»ºæ¨¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä¸ä»…ä½¿ç”¨äº†Zero-RLç”Ÿæˆç®€æ´ã€ç»“æœå¯¼å‘çš„æ¨ç†è·¯å¾„ä»¥å‡å°‘å…³é”®é—æ¼çš„å¯èƒ½æ€§ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡$R^\star$ï¼Œä»¥åŸºäºç”Ÿæˆå¯èƒ½æ€§å¯¹æ¨ç†è·¯å¾„è¿›è¡Œè¯„åˆ†ã€‚æœ€ç»ˆé˜¶æ®µåˆ™é€šè¿‡å¼ºåŒ–å­¦ä¹ å¯¹æ¨¡å‹è¿›è¡Œç²¾ç‚¼ï¼Œæå‡å…¶åå¥½è¾¨åˆ«èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReasonGRMåœ¨ä¸‰ä¸ªå…¬å¼€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ç«äº‰æ€§æˆ–é¢†å…ˆæ€§èƒ½ï¼Œå¹³å‡æé«˜äº†GRMsæ€§èƒ½çº¦1.8%ï¼Œå¹¶ä¸”ç›¸å¯¹äºGPT-4oç­‰ä¸“æœ‰æ¨¡å‹æ€§èƒ½æå‡æœ€é«˜å¯è¾¾5.6%ã€‚è¿™è¯æ˜äº†æ¨ç†æ„ŸçŸ¥è®­ç»ƒçš„é‡è¦æ€§ä»¥åŠé«˜è´¨é‡æ¨ç†é€‰æ‹©å¯¹äºå¯é åå¥½å»ºæ¨¡çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGRMsï¼‰ç›¸è¾ƒäºæ ‡é‡å¥–åŠ±æ¨¡å‹å…·æœ‰æ›´é«˜çš„çµæ´»æ€§æ¥æ•æ‰äººç±»åå¥½ï¼Œä½†å…¶æ•ˆæœå—é™äºæ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>é—®é¢˜å¯èƒ½å¼•å‘ä¸å®Œæ•´çš„æ¨ç†è·¯å¾„æˆ–è€…è¿‡äºæ¨æµ‹çš„å‡è®¾ï¼Œå¯¼è‡´ç”Ÿæˆé”™è¯¯çš„ç»“è®ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e519f5e5f71d7ec20fc1869a67ef0c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4c5f557931c55360f08fcd46d42d9ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-564781b6059fd64eb25e7d12f254b0b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7501de4a14a39fa8a5da7f0426aa55c8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GeoGuess-Multimodal-Reasoning-based-on-Hierarchy-of-Visual-Information-in-Street-View"><a href="#GeoGuess-Multimodal-Reasoning-based-on-Hierarchy-of-Visual-Information-in-Street-View" class="headerlink" title="GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information   in Street View"></a>GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information   in Street View</h2><p><strong>Authors:Fenghua Cheng, Jinxiang Wang, Sen Wang, Zi Huang, Xue Li</strong></p>
<p>Multimodal reasoning is a process of understanding, integrating and inferring information across different data modalities. It has recently attracted surging academic attention as a benchmark for Artificial Intelligence (AI). Although there are various tasks for evaluating multimodal reasoning ability, they still have limitations. Lack of reasoning on hierarchical visual clues at different levels of granularity, e.g., local details and global context, is of little discussion, despite its frequent involvement in real scenarios. To bridge the gap, we introduce a novel and challenging task for multimodal reasoning, namely GeoGuess. Given a street view image, the task is to identify its location and provide a detailed explanation. A system that succeeds in GeoGuess should be able to detect tiny visual clues, perceive the broader landscape, and associate with vast geographic knowledge. Therefore, GeoGuess would require the ability to reason between hierarchical visual information and geographic knowledge. In this work, we establish a benchmark for GeoGuess by introducing a specially curated dataset GeoExplain which consists of panoramas-geocoordinates-explanation tuples. Additionally, we present a multimodal and multilevel reasoning method, namely SightSense which can make prediction and generate comprehensive explanation based on hierarchy of visual information and external knowledge. Our analysis and experiments demonstrate their outstanding performance in GeoGuess. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ¨ç†æ˜¯ç†è§£å’Œæ¨æ–­ä¸åŒæ•°æ®æ¨¡æ€ä¿¡æ¯çš„è¿‡ç¨‹ã€‚æœ€è¿‘ï¼Œå®ƒä½œä¸ºäººå·¥æ™ºèƒ½çš„åŸºå‡†æµ‹è¯•ï¼Œå¼•èµ·äº†å­¦æœ¯ç•Œçš„å¹¿æ³›å…³æ³¨ã€‚å°½ç®¡å­˜åœ¨å„ç§è¯„ä¼°å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„ä»»åŠ¡ï¼Œä½†å®ƒä»¬ä»æœ‰ä¸€å®šçš„å±€é™æ€§ã€‚å¯¹äºä¸åŒç²’åº¦å±‚æ¬¡ä¸Šçš„è§†è§‰çº¿ç´¢çš„æ¨ç†ç¼ºä¹è®¨è®ºï¼Œä¾‹å¦‚å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œå°½ç®¡å…¶åœ¨ç°å®åœºæ™¯ä¸­ç»å¸¸æ¶‰åŠã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§å’Œåˆ›æ–°æ€§çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ï¼Œå³åœ°ç†çŒœæµ‹ï¼ˆGeoGuessï¼‰ã€‚ç»™å®šä¸€å¼ è¡—é“è§†å›¾å›¾åƒï¼Œä»»åŠ¡æ˜¯è¯†åˆ«å…¶ä½ç½®å¹¶æä¾›è¯¦ç»†çš„è§£é‡Šã€‚èƒ½å¤Ÿåœ¨GeoGuessä¸­æˆåŠŸçš„ç³»ç»Ÿåº”è¯¥èƒ½å¤Ÿæ£€æµ‹åˆ°å¾®å°çš„è§†è§‰çº¿ç´¢ï¼Œæ„ŸçŸ¥æ›´å¹¿é˜”çš„æ™¯è§‚ï¼Œå¹¶ä¸ä¸°å¯Œçš„åœ°ç†çŸ¥è¯†ç›¸å…³è”ã€‚å› æ­¤ï¼ŒGeoGuesså°†éœ€è¦å¤„ç†åˆ†å±‚è§†è§‰ä¿¡æ¯å’Œåœ°ç†çŸ¥è¯†çš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸“é—¨å®šåˆ¶çš„æ•°æ®é›†GeoExplainï¼Œå»ºç«‹äº†GeoGuessçš„åŸºå‡†æµ‹è¯•ï¼Œè¯¥æ•°æ®é›†ç”±å…¨æ™¯å›¾-åœ°ç†åæ ‡-è§£é‡Šå…ƒç»„ç»„æˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€ã€å¤šå±‚æ¬¡çš„æ¨ç†æ–¹æ³•ï¼Œå³SightSenseï¼Œå®ƒå¯ä»¥åŸºäºè§†è§‰ä¿¡æ¯çš„å±‚æ¬¡ç»“æ„å’Œå¤–éƒ¨çŸ¥è¯†æ¥è¿›è¡Œé¢„æµ‹å’Œç”Ÿæˆç»¼åˆè§£é‡Šã€‚æˆ‘ä»¬çš„åˆ†æå’Œå®éªŒè¯æ˜äº†ä»–ä»¬åœ¨GeoGuessä¸­çš„å‡ºè‰²è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16633v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€æ¨ç†çš„æ–°æŒ‘æˆ˜ä»»åŠ¡â€”â€”GeoGuessã€‚è¯¥ä»»åŠ¡è¦æ±‚ç³»ç»Ÿé€šè¿‡è¯†åˆ«è¡—é“æ™¯è§‚å›¾åƒä¸­çš„ç»†å¾®è§†è§‰çº¿ç´¢ã€æ„ŸçŸ¥æ›´å¹¿æ³›çš„æ™¯è§‚ï¼Œå¹¶ç»“åˆä¸°å¯Œçš„åœ°ç†çŸ¥è¯†æ¥ç¡®å®šå…¶ä½ç½®å¹¶æä¾›è¯¦ç»†è§£é‡Šã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªåä¸ºGeoExplainçš„ä¸“é—¨æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºSightSenseçš„å¤šæ¨¡æ€å¤šå±‚æ¬¡æ¨ç†æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®è§†è§‰ä¿¡æ¯çš„å±‚æ¬¡ç»“æ„å’Œå¤–éƒ¨çŸ¥è¯†è¿›è¡Œé¢„æµ‹å’Œç”Ÿæˆç»¼åˆè§£é‡Šã€‚åœ¨GeoGuessä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ¨ç†æ˜¯ç†è§£å’Œæ¨æ–­ä¸åŒæ•°æ®æ¨¡æ€ä¿¡æ¯çš„è¿‡ç¨‹ï¼Œè¿‘å¹´æ¥å·²æˆä¸ºäººå·¥æ™ºèƒ½çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å½“å‰çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡è¯„ä¼°å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å±‚æ¬¡åŒ–è§†è§‰çº¿ç´¢ä¸Šçš„æ¨ç†èƒ½åŠ›æ–¹é¢è®¨è®ºè¾ƒå°‘ã€‚</li>
<li>å¼•å…¥æ–°çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡â€”â€”GeoGuessï¼Œè¦æ±‚ç³»ç»Ÿç»“åˆç»†å¾®è§†è§‰çº¿ç´¢ã€æ•´ä½“æ™¯è§‚ä»¥åŠåœ°ç†çŸ¥è¯†æ¥è¯†åˆ«è¡—é“å›¾åƒçš„ä½ç½®ã€‚</li>
<li>å»ºç«‹GeoExplainæ•°æ®é›†ï¼ŒåŒ…å«å…¨æ™¯å›¾-åœ°ç†åæ ‡-è§£é‡Šçš„ä¸‰å…ƒç»„ï¼Œä¸ºGeoGuessä»»åŠ¡æä¾›åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æå‡ºåä¸ºSightSenseçš„å¤šæ¨¡æ€å¤šå±‚æ¬¡æ¨ç†æ–¹æ³•ï¼Œèƒ½å¤ŸåŸºäºè§†è§‰ä¿¡æ¯çš„å±‚æ¬¡ç»“æ„å’Œå¤–éƒ¨çŸ¥è¯†è¿›è¡Œé¢„æµ‹å’Œè¯¦ç»†è§£é‡Šã€‚</li>
<li>GeoGuessä»»åŠ¡éœ€è¦ç³»ç»Ÿå…·å¤‡åœ¨å±‚æ¬¡åŒ–è§†è§‰ä¿¡æ¯å’Œåœ°ç†çŸ¥è¯†ä¹‹é—´è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a589babead56952adac2dbe4b099de93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0389c07608241a5a7683cb236747070e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6ccac2c3e5efb646637d128184c87be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99d46e1b5f224328492e2460ee1b50d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-14d4d0e18f7454b6d01c25cd8644ed3c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="eCAV-An-Edge-Assisted-Evaluation-Platform-for-Connected-Autonomous-Vehicles"><a href="#eCAV-An-Edge-Assisted-Evaluation-Platform-for-Connected-Autonomous-Vehicles" class="headerlink" title="eCAV: An Edge-Assisted Evaluation Platform for Connected Autonomous   Vehicles"></a>eCAV: An Edge-Assisted Evaluation Platform for Connected Autonomous   Vehicles</h2><p><strong>Authors:Tyler Landle, Jordan Rapp, Dean Blank, Chandramouli Amarnath, Abhijit Chatterjee, Alex Daglis, Umakishore Ramachandran</strong></p>
<p>As autonomous vehicles edge closer to widespread adoption, enhancing road safety through collision avoidance and minimization of collateral damage becomes imperative. Vehicle-to-everything (V2X) technologies, which include vehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), and vehicle-to-cloud (V2C), are being proposed as mechanisms to achieve this safety improvement.   Simulation-based testing is crucial for early-stage evaluation of Connected Autonomous Vehicle (CAV) control systems, offering a safer and more cost-effective alternative to real-world tests. However, simulating large 3D environments with many complex single- and multi-vehicle sensors and controllers is computationally intensive. There is currently no evaluation framework that can effectively evaluate realistic scenarios involving large numbers of autonomous vehicles.   We propose eCAV â€“ an efficient, modular, and scalable evaluation platform to facilitate both functional validation of algorithmic approaches to increasing road safety, as well as performance prediction of algorithms of various V2X technologies, including a futuristic Vehicle-to-Edge control plane and correspondingly designed control algorithms. eCAV can model up to 256 vehicles running individual control algorithms without perception enabled, which is $8\times$ more vehicles than what is possible with state-of-the-art alternatives. %faster than state-of-the-art alternatives that can simulate $8\times$ fewer vehicles. With perception enabled, eCAV simulates up to 64 vehicles with a step time under 800ms, which is $4\times$ more and $1.5\times$ faster than the state-of-the-art OpenCDA framework. </p>
<blockquote>
<p>éšç€è‡ªåŠ¨é©¾é©¶æ±½è½¦è¶Šæ¥è¶Šæ¥è¿‘å¹¿æ³›é‡‡ç”¨ï¼Œé€šè¿‡é¿å…ç¢°æ’å’Œæœ€å°åŒ–é™„å¸¦æŸå¤±æ¥æé«˜é“è·¯å®‰å…¨å˜å¾—è‡³å…³é‡è¦ã€‚è½¦è¾†å¯¹ä¸€åˆ‡ï¼ˆV2Xï¼‰æŠ€æœ¯ï¼ŒåŒ…æ‹¬è½¦è¾†å¯¹è½¦è¾†ï¼ˆV2Vï¼‰ã€è½¦è¾†å¯¹åŸºç¡€è®¾æ–½ï¼ˆV2Iï¼‰å’Œè½¦è¾†å¯¹äº‘ï¼ˆV2Cï¼‰ï¼Œè¢«æè®®ä½œä¸ºå®ç°è¿™ç§å®‰å…¨æ”¹è¿›çš„æœºåˆ¶ã€‚åŸºäºæ¨¡æ‹Ÿçš„æµ‹è¯•å¯¹äºæ—©æœŸé˜¶æ®µè¯„ä¼°è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆCAVï¼‰æ§åˆ¶ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œå®ƒä¸ºçœŸå®ä¸–ç•Œæµ‹è¯•æä¾›äº†æ›´å®‰å…¨ã€æ›´ç»æµçš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæ¨¡æ‹Ÿå…·æœ‰è®¸å¤šå¤æ‚å•è½¦å’Œå¤šè½¦ä¼ æ„Ÿå™¨å’Œæ§åˆ¶å™¨çš„å¤§å‹ä¸‰ç»´ç¯å¢ƒåœ¨è®¡ç®—ä¸Šå¾ˆå¯†é›†ã€‚ç›®å‰è¿˜æ²¡æœ‰ä¸€ä¸ªè¯„ä¼°æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°æ¶‰åŠå¤§é‡è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„ç°å®åœºæ™¯ã€‚æˆ‘ä»¬æå‡ºäº†eCAVâ€”â€”ä¸€ä¸ªé«˜æ•ˆã€æ¨¡å—åŒ–ã€å¯æ‰©å±•çš„è¯„ä¼°å¹³å°ï¼Œå®ƒæ—¢èƒ½éªŒè¯æé«˜é“è·¯å®‰å…¨çš„ç®—æ³•æ–¹æ³•çš„åŠŸèƒ½æ€§ï¼Œä¹Ÿèƒ½é¢„æµ‹å„ç§V2XæŠ€æœ¯çš„ç®—æ³•æ€§èƒ½ï¼ŒåŒ…æ‹¬å…ˆè¿›çš„è½¦è¾†å¯¹è¾¹ç¼˜æ§åˆ¶å¹³é¢å’Œç›¸åº”è®¾è®¡çš„æ§åˆ¶ç®—æ³•ã€‚åœ¨æ²¡æœ‰å¯ç”¨æ„ŸçŸ¥çš„æƒ…å†µä¸‹ï¼ŒeCAVå¯ä»¥æ¨¡æ‹Ÿå¤šè¾¾256è¾†æ±½è½¦è¿è¡Œå„è‡ªçš„æ§åˆ¶ç®—æ³•ï¼Œè¿™æ˜¯ç›®å‰å…¶ä»–é€‰æ‹©å¯èƒ½æ¨¡æ‹Ÿçš„è½¦è¾†æ•°çš„8å€ã€‚é€šè¿‡å¯ç”¨æ„ŸçŸ¥åŠŸèƒ½ï¼ŒeCAVèƒ½åœ¨ä¸åˆ°800æ¯«ç§’çš„æ—¶é—´å†…æ¨¡æ‹Ÿå¤šè¾¾64è¾†æ±½è½¦çš„æƒ…å†µï¼Œè¿™æ¯”ç°æœ‰æœ€å…ˆè¿›çš„OpenCDAæ¡†æ¶é«˜å‡º4å€ï¼Œé€Ÿåº¦é«˜å‡º1.5å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16535v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è®¨è®ºäº†éšç€è‡ªåŠ¨é©¾é©¶æ±½è½¦çš„æ™®åŠï¼Œé€šè¿‡V2XæŠ€æœ¯æé«˜é“è·¯å®‰å…¨çš„é‡è¦æ€§ã€‚ä»¿çœŸæµ‹è¯•åœ¨è¯„ä¼°è¿æ¥è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆCAVï¼‰æ§åˆ¶ç³»ç»Ÿæ–¹é¢å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œæ¨¡æ‹Ÿå¤§å‹3Dç¯å¢ƒå­˜åœ¨è®¡ç®—å¯†é›†çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆã€æ¨¡å—åŒ–å’Œå¯æ‰©å±•çš„è¯„ä¼°å¹³å°eCAVï¼Œè¯¥å¹³å°ä¸ä»…å¯ä»¥éªŒè¯æé«˜é“è·¯å®‰å…¨çš„ç®—æ³•æ–¹æ³•çš„åŠŸèƒ½æ€§ï¼Œè¿˜å¯ä»¥é¢„æµ‹å„ç§V2XæŠ€æœ¯çš„ç®—æ³•æ€§èƒ½ã€‚eCAVèƒ½å¤Ÿæ¨¡æ‹Ÿè¿è¡Œç‹¬ç«‹æ§åˆ¶ç®—æ³•çš„æœ€å¤š256è¾†æ±½è½¦ï¼ˆåœ¨ä¸å¯ç”¨æ„ŸçŸ¥çš„æƒ…å†µä¸‹ï¼‰ï¼Œæ˜¯ç°æœ‰æ›¿ä»£æ–¹æ¡ˆçš„8å€ã€‚å¯ç”¨æ„ŸçŸ¥åï¼ŒeCAVå¯ä»¥åœ¨800æ¯«ç§’çš„æ­¥éª¤æ—¶é—´å†…æ¨¡æ‹Ÿæœ€å¤š64è¾†æ±½è½¦ï¼Œè¿™æ˜¯ç°æœ‰OpenCDAæ¡†æ¶çš„4å€å’Œ1.5å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶æ±½è½¦çš„æ™®åŠå¯¹é“è·¯å®‰å…¨æå‡ºäº†æ›´é«˜çš„è¦æ±‚ï¼Œéœ€è¦é€šè¿‡V2XæŠ€æœ¯å¢å¼ºå®‰å…¨æ€§ã€‚</li>
<li>ä»¿çœŸæµ‹è¯•åœ¨è¯„ä¼°CAVæ§åˆ¶ç³»ç»Ÿæ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä½œä¸ºç°å®ä¸–ç•Œæµ‹è¯•çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>æ¨¡æ‹Ÿå¤§å‹3Dç¯å¢ƒæ¶‰åŠè®¡ç®—å¯†é›†å‹ä»»åŠ¡ï¼Œå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹èƒ½å¤Ÿè¯„ä¼°æ¶‰åŠå¤§é‡è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„ç°å®åœºæ™¯çš„è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>æå‡ºäº†eCAVè¯„ä¼°å¹³å°ï¼Œè¯¥å¹³å°å…·æœ‰é«˜æ•ˆã€æ¨¡å—åŒ–å’Œå¯æ‰©å±•æ€§ç‰¹ç‚¹ã€‚</li>
<li>eCAVèƒ½å¤Ÿæ¨¡æ‹Ÿè¿è¡Œç‹¬ç«‹æ§åˆ¶ç®—æ³•çš„æœ€å¤š256è¾†æ±½è½¦ï¼ˆåœ¨ä¸å¯ç”¨æ„ŸçŸ¥çš„æƒ…å†µä¸‹ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16535">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e0a225acb8d7d2cb4f217650289280f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d651144b6e69f22781d52fa1332138ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdf22c23043001230c81761d7de434a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02e9de8e6d74c89610c7cd8c15aa8993.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a9508a8583561d14a41630b59186c3c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="From-LLM-anation-to-LLM-orchestrator-Coordinating-Small-Models-for-Data-Labeling"><a href="#From-LLM-anation-to-LLM-orchestrator-Coordinating-Small-Models-for-Data-Labeling" class="headerlink" title="From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data   Labeling"></a>From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data   Labeling</h2><p><strong>Authors:Yao Lu, Zhaiyuan Ji, Jiawei Du, Yu Shanqing, Qi Xuan, Tianyi Zhou</strong></p>
<p>Although the annotation paradigm based on Large Language Models (LLMs) has made significant breakthroughs in recent years, its actual deployment still has two core bottlenecks: first, the cost of calling commercial APIs in large-scale annotation is very expensive; second, in scenarios that require fine-grained semantic understanding, such as sentiment classification and toxicity classification, the annotation accuracy of LLMs is even lower than that of Small Language Models (SLMs) dedicated to this field. To address these problems, we propose a new paradigm of multi-model cooperative annotation and design a fully automatic annotation framework AutoAnnotator based on this. Specifically, AutoAnnotator consists of two layers. The upper-level meta-controller layer uses the generation and reasoning capabilities of LLMs to select SLMs for annotation, automatically generate annotation code and verify difficult samples; the lower-level task-specialist layer consists of multiple SLMs that perform annotation through multi-model voting. In addition, we use the difficult samples obtained by the secondary review of the meta-controller layer as the reinforcement learning set and fine-tune the SLMs in stages through a continual learning strategy, thereby improving the generalization of SLMs. Extensive experiments show that AutoAnnotator outperforms existing open-source&#x2F;API LLMs in zero-shot, one-shot, CoT, and majority voting settings. Notably, AutoAnnotator reduces the annotation cost by 74.15% compared to directly annotating with GPT-3.5-turbo, while still improving the accuracy by 6.21%. Project page: <a target="_blank" rel="noopener" href="https://github.com/Zhaiyuan-Ji/AutoAnnotator">https://github.com/Zhaiyuan-Ji/AutoAnnotator</a>. </p>
<blockquote>
<p>å°½ç®¡åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ‡æ³¨èŒƒå¼è¿‘å¹´æ¥å–å¾—äº†é‡å¤§çªç ´ï¼Œä½†å…¶å®é™…éƒ¨ç½²ä»é¢ä¸´ä¸¤ä¸ªæ ¸å¿ƒç“¶é¢ˆï¼šé¦–å…ˆï¼Œå¤§è§„æ¨¡æ ‡æ³¨ä¸­è°ƒç”¨å•†ä¸šAPIçš„æˆæœ¬éå¸¸é«˜æ˜‚ï¼›å…¶æ¬¡ï¼Œåœ¨æƒ…æ„Ÿåˆ†ç±»å’Œæ¯’æ€§åˆ†ç±»ç­‰éœ€è¦ç²¾ç»†ç²’åº¦è¯­ä¹‰ç†è§£çš„åœºæ™¯ä¸­ï¼ŒLLMçš„æ ‡æ³¨å‡†ç¡®åº¦ç”šè‡³ä½äºä¸“é—¨ç”¨äºè¯¥é¢†åŸŸçš„å°è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡å‹ååŒæ ‡æ³¨èŒƒå¼ï¼Œå¹¶åŸºäºæ­¤è®¾è®¡äº†ä¸€ä¸ªå…¨è‡ªåŠ¨æ ‡æ³¨æ¡†æ¶AutoAnnotatorã€‚å…·ä½“æ¥è¯´ï¼ŒAutoAnnotatorç”±ä¸¤ä¸ªå±‚æ¬¡ç»„æˆã€‚ä¸Šå±‚å…ƒæ§åˆ¶å™¨å±‚åˆ©ç”¨LLMçš„ç”Ÿæˆå’Œæ¨ç†èƒ½åŠ›æ¥é€‰æ‹©ç”¨äºæ ‡æ³¨çš„SLMï¼Œè‡ªåŠ¨ç”Ÿæˆæ ‡æ³¨ä»£ç å¹¶éªŒè¯å›°éš¾æ ·æœ¬ï¼›ä¸‹å±‚ä»»åŠ¡ä¸“å®¶å±‚ç”±å¤šä¸ªSLMç»„æˆï¼Œé€šè¿‡å¤šæ¨¡å‹æŠ•ç¥¨è¿›è¡Œæ ‡æ³¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»¥å…ƒæ§åˆ¶å™¨å±‚äºŒæ¬¡å®¡æŸ¥è·å¾—çš„å›°éš¾æ ·æœ¬ä½œä¸ºå¼ºåŒ–å­¦ä¹ é›†ï¼Œé€šè¿‡æŒç»­å­¦ä¹ ç­–ç•¥åˆ†é˜¶æ®µå¾®è°ƒSLMï¼Œä»è€Œæé«˜SLMçš„æ³›åŒ–èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAutoAnnotatoråœ¨é›¶æ ·æœ¬ã€å•æ ·æœ¬ã€é“¾å¼æ¨ç†å’Œå¤šæ•°æŠ•ç¥¨è®¾ç½®ä¸­ä¼˜äºç°æœ‰çš„å¼€æº&#x2F;API LLMã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸ç›´æ¥ä½¿ç”¨GPT-3.5-turboè¿›è¡Œæ ‡æ³¨ç›¸æ¯”ï¼ŒAutoAnnotatoré™ä½äº†74.15%çš„æ ‡æ³¨æˆæœ¬ï¼ŒåŒæ—¶æé«˜äº†6.21%çš„å‡†ç¡®åº¦ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/Zhaiyuan-Ji/AutoAnnotator%E3%80%82">https://github.com/Zhaiyuan-Ji/AutoAnnotatorã€‚</a></p>
</blockquote>
<p><strong>ç®€åŒ–ç‰ˆ</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16393v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ‡æ³¨èŒƒå¼è™½ç„¶è¿‘å¹´æ¥å–å¾—äº†é‡å¤§çªç ´ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»å­˜åœ¨ä¸¤å¤§ç“¶é¢ˆï¼šä¸€æ˜¯å¤§è§„æ¨¡æ ‡æ³¨è°ƒç”¨å•†ä¸šAPIçš„æˆæœ¬é«˜æ˜‚ï¼›äºŒæ˜¯åœ¨éœ€è¦ç²¾ç»†è¯­ä¹‰ç†è§£ï¼ˆå¦‚æƒ…æ„Ÿåˆ†ç±»å’Œæ¯’æ€§åˆ†ç±»ï¼‰çš„åœºæ™¯ä¸‹ï¼ŒLLMçš„æ ‡æ³¨ç²¾åº¦ä½äºä¸“æ³¨äºè¯¥é¢†åŸŸçš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡å‹ååŒæ ‡æ³¨èŒƒå¼ï¼Œå¹¶åŸºäºæ­¤è®¾è®¡äº†ä¸€ä¸ªå…¨è‡ªåŠ¨æ ‡æ³¨æ¡†æ¶AutoAnnotatorã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤å±‚ï¼šä¸Šå±‚æ˜¯å…ƒæ§åˆ¶å™¨å±‚ï¼Œåˆ©ç”¨LLMçš„ç”Ÿæˆå’Œæ¨ç†èƒ½åŠ›æ¥é€‰æ‹©SLMè¿›è¡Œæ ‡æ³¨ã€è‡ªåŠ¨ç”Ÿæˆæ ‡æ³¨ä»£ç å’ŒéªŒè¯å›°éš¾æ ·æœ¬ï¼›ä¸‹å±‚æ˜¯ä»»åŠ¡ä¸“å®¶å±‚ï¼Œç”±å¤šä¸ªSLMç»„æˆï¼Œé€šè¿‡å¤šæ¨¡å‹æŠ•ç¥¨è¿›è¡Œæ ‡æ³¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨å…ƒæ§åˆ¶å™¨å±‚äºŒæ¬¡å®¡æŸ¥è·å¾—çš„å›°éš¾æ ·æœ¬ä½œä¸ºå¼ºåŒ–å­¦ä¹ é›†ï¼Œé€šè¿‡æŒç»­å­¦ä¹ ç­–ç•¥åˆ†é˜¶æ®µå¾®è°ƒSLMï¼Œæé«˜SLMçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒAutoAnnotatoråœ¨é›¶æ ·æœ¬ã€å•æ ·æœ¬ã€CoTå’Œå¤šæ•°æŠ•ç¥¨è®¾ç½®ä¸‹ä¼˜äºç°æœ‰çš„å¼€æº&#x2F;API LLMsã€‚ç‰¹åˆ«åœ°ï¼Œç›¸è¾ƒäºç›´æ¥ä½¿ç”¨GPT-3.5-turboè¿›è¡Œæ ‡æ³¨ï¼ŒAutoAnnotatoré™ä½äº†74.15%çš„æ ‡æ³¨æˆæœ¬ï¼ŒåŒæ—¶æé«˜äº†6.21%çš„ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šé«˜æ˜‚çš„å•†ä¸šAPIè°ƒç”¨æˆæœ¬å’Œåœ¨ç²¾ç»†è¯­ä¹‰ç†è§£åœºæ™¯ä¸‹çš„æ ‡æ³¨ç²¾åº¦é—®é¢˜ã€‚</li>
<li>é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†å¤šæ¨¡å‹ååŒæ ‡æ³¨çš„æ–°èŒƒå¼å’Œå…¨è‡ªåŠ¨æ ‡æ³¨æ¡†æ¶AutoAnnotatorã€‚</li>
<li>AutoAnnotatoråŒ…æ‹¬å…ƒæ§åˆ¶å™¨å±‚å’Œä»»åŠ¡ä¸“å®¶å±‚ï¼Œåˆ†åˆ«åˆ©ç”¨LLMå’ŒSLMçš„ä¼˜åŠ¿è¿›è¡Œæ ‡æ³¨å’Œå›°éš¾æ ·æœ¬å¤„ç†ã€‚</li>
<li>å…ƒæ§åˆ¶å™¨å±‚èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæ ‡æ³¨ä»£ç å’ŒéªŒè¯å›°éš¾æ ·æœ¬ï¼Œä»»åŠ¡ä¸“å®¶å±‚åˆ™é€šè¿‡å¤šæ¨¡å‹æŠ•ç¥¨è¿›è¡Œæ ‡æ³¨ã€‚</li>
<li>ä½¿ç”¨å›°éš¾æ ·æœ¬ä½œä¸ºå¼ºåŒ–å­¦ä¹ é›†ï¼Œé€šè¿‡æŒç»­å­¦ä¹ ç­–ç•¥æé«˜å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜AutoAnnotatoråœ¨å¤šç§è®¾ç½®ä¸‹æ€§èƒ½ä¼˜äºç°æœ‰LLMsã€‚</li>
<li>AutoAnnotatoré™ä½äº†æ ‡æ³¨æˆæœ¬å¹¶æé«˜äº†æ ‡æ³¨ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16393">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8b974b45c2e713b20d8a2fab8cda8b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03a8db59580c637b050b66d820af0594.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9321308867846014bc7791e1e42fbf43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a24f2907b58d522994525f753de0453f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa38bd0ad53374d3643296b3c3b1e1fa.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GRPO-CARE-Consistency-Aware-Reinforcement-Learning-for-Multimodal-Reasoning"><a href="#GRPO-CARE-Consistency-Aware-Reinforcement-Learning-for-Multimodal-Reasoning" class="headerlink" title="GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal   Reasoning"></a>GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal   Reasoning</h2><p><strong>Authors:Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Junhao Cheng, Ying Shan, Xihui Liu</strong></p>
<p>Recent reinforcement learning approaches, such as outcome-supervised GRPO, have advanced Chain-of-Thought reasoning in large language models (LLMs), yet their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack of rigorous evaluation for MLLM post-training methods, we introduce SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced perception and reasoning. It offers a large training set and evaluates generalization across three escalating challenges: in-distribution, cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1, we find that standard GRPO, while improving answer accuracy, often reduces logical coherence between reasoning steps and answers, with only a 57.9% consistency rate. This stems from reward signals focusing solely on final answers, encouraging shortcuts, and strict KL penalties limiting exploration.To address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing both answer correctness and reasoning coherence without explicit supervision. GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer correctness, and (2) an adaptive consistency bonus, computed by comparing the modelâ€™s reasoning-to-answer likelihood (via a slowly-evolving reference model) against group peers.This dual mechanism amplifies rewards for reasoning paths that are both correct and logically consistent. Replacing KL penalties with this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1, achieving a 6.7% performance gain on the hardest evaluation level and a 24.5% improvement in consistency. It also shows strong transferability, improving model performance across diverse video understanding benchmarks. Our work contributes a systematically designed benchmark and a generalizable post-training framework, advancing the development of more interpretable and robust MLLMs. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œå¦‚ç»“æœç›‘ç£çš„GRPOï¼Œå·²ç»æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„é“¾å¼æ€ç»´æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­çš„é€‚åº”å°šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†è§£å†³MLLMåè®­ç»ƒæ–¹æ³•çš„ä¸¥æ ¼è¯„ä¼°ç¼ºä¹çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SEED-Bench-R1åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•åŒ…å«å¤æ‚çš„ç°å®è§†é¢‘ï¼Œéœ€è¦å¹³è¡¡çš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚å®ƒæä¾›äº†ä¸€ä¸ªå¤§å‹è®­ç»ƒé›†ï¼Œå¹¶è¯„ä¼°äº†åœ¨ä¸‰ä¸ªé€’å¢æŒ‘æˆ˜ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼šå†…éƒ¨åˆ†å¸ƒã€è·¨ç¯å¢ƒå’Œè·¨ç¯å¢ƒä»»åŠ¡åœºæ™¯ã€‚ä½¿ç”¨SEED-Bench-R1ï¼Œæˆ‘ä»¬å‘ç°æ ‡å‡†çš„GRPOè™½ç„¶æé«˜äº†ç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œä½†å¾€å¾€é™ä½äº†æ¨ç†æ­¥éª¤å’Œç­”æ¡ˆä¹‹é—´çš„é€»è¾‘è¿è´¯æ€§ï¼Œåªæœ‰57.9%çš„ä¸€è‡´æ€§ç‡ã€‚è¿™æºäºå¥–åŠ±ä¿¡å·åªå…³æ³¨æœ€ç»ˆç­”æ¡ˆï¼Œé¼“åŠ±èµ°æ·å¾„ï¼Œä»¥åŠä¸¥æ ¼çš„KLæƒ©ç½šé™åˆ¶æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GRPO-CAREï¼Œè¿™æ˜¯ä¸€ä¸ªä¸€è‡´æ€§æ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒä¼˜åŒ–äº†ç­”æ¡ˆæ­£ç¡®æ€§å’Œæ¨ç†è¿è´¯æ€§ï¼Œè€Œæ— éœ€æ˜ç¡®çš„ç›‘ç£ã€‚GRPO-CAREå¼•å…¥äº†ä¸¤å±‚å¥–åŠ±ï¼šï¼ˆ1ï¼‰åŸºäºç­”æ¡ˆæ­£ç¡®æ€§çš„åŸºç¡€å¥–åŠ±ï¼›ï¼ˆ2ï¼‰è‡ªé€‚åº”ä¸€è‡´æ€§å¥–é‡‘ï¼Œé€šè¿‡æ¯”è¾ƒæ¨¡å‹ä»æ¨ç†åˆ°ç­”æ¡ˆçš„å¯èƒ½æ€§ï¼ˆé€šè¿‡ä¸€ä¸ªç¼“æ…¢æ¼”åŒ–çš„å‚è€ƒæ¨¡å‹ï¼‰ä¸ç¾¤ä½“åŒè¡Œæ¥è®¡ç®—ã€‚è¿™ç§åŒé‡æœºåˆ¶æ”¾å¤§äº†æ—¢æ­£ç¡®åˆé€»è¾‘è¿è´¯çš„æ¨ç†è·¯å¾„çš„å¥–åŠ±ã€‚ç”¨è‡ªé€‚åº”å¥–é‡‘ä»£æ›¿KLæƒ©ç½šï¼ŒGRPO-CAREåœ¨SEED-Bench-R1ä¸Šçš„è¡¨ç°ä¼˜äºæ ‡å‡†GRPOï¼Œåœ¨æœ€éš¾çš„è¯„ä»·å±‚é¢ä¸Šå®ç°äº†6.7%çš„æ€§èƒ½æå‡å’Œ24.5%çš„ä¸€è‡´æ€§æ”¹è¿›ã€‚å®ƒè¿˜æ˜¾ç¤ºå‡ºå¼ºå¤§çš„å¯è¿ç§»æ€§ï¼Œæé«˜äº†åœ¨ä¸åŒè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­çš„æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œè´¡çŒ®äº†ä¸€ä¸ªç³»ç»Ÿè®¾è®¡çš„åŸºå‡†æµ‹è¯•å’Œä¸€ç§é€šç”¨çš„åè®­ç»ƒæ¡†æ¶ï¼Œæ¨åŠ¨äº†æ›´å…·è§£é‡Šæ€§å’Œé²æ£’æ€§çš„MLLMçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16141v1">PDF</a> Code released at: <a target="_blank" rel="noopener" href="https://github.com/TencentARC/GRPO-CARE">https://github.com/TencentARC/GRPO-CARE</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•å¦‚GRPOåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é“¾å¼æ€ç»´æ¨ç†ï¼ˆChain-of-Thoughtï¼‰ä¸Šæœ‰æ‰€è¿›å±•ï¼Œä½†å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é€‚åº”å°šæœªæ¢ç´¢ã€‚ä¸ºè§£å†³MLLMåè®­ç»ƒæ–¹æ³•çš„ä¸¥æ ¼è¯„ä¼°é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SEED-Bench-R1åŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…å«å¤æ‚çš„ç°å®è§†é¢‘ï¼Œè¦æ±‚æ„ŸçŸ¥å’Œæ¨ç†çš„å¹³è¡¡ã€‚é€šè¿‡ä½¿ç”¨SEED-Bench-R1ï¼Œæˆ‘ä»¬å‘ç°æ ‡å‡†GRPOè™½ç„¶æé«˜äº†å›ç­”çš„å‡†ç¡®æ€§ï¼Œä½†å¾€å¾€ä¼šé™ä½æ¨ç†æ­¥éª¤å’Œç­”æ¡ˆä¹‹é—´çš„é€»è¾‘è¿è´¯æ€§ï¼Œä¸€è‡´æ€§ä»…ä¸º57.9%ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GRPO-CAREè¿™ä¸€ä¸€è‡´æ€§æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¼˜åŒ–äº†ç­”æ¡ˆæ­£ç¡®æ€§å’Œæ¨ç†è¿è´¯æ€§çš„ä¼˜åŒ–ï¼Œæ— éœ€æ˜¾å¼ç›‘ç£ã€‚GRPO-CAREå¼•å…¥äº†ä¸¤å±‚å¥–åŠ±æœºåˆ¶ï¼Œå¹¶åœ¨SEED-Bench-R1ä¸Šè¡¨ç°å‡ºä¼˜äºæ ‡å‡†GRPOçš„æ€§èƒ½ï¼Œåœ¨æœ€éš¾çš„è¯„ä¼°å±‚é¢ä¸Šå®ç°äº†6.7%çš„æ€§èƒ½æå‡å’Œ24.5%çš„ä¸€è‡´æ€§æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é“¾å¼æ€ç»´æ¨ç†æœ‰è¿›å±•ï¼Œä½†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„é€‚åº”ä»æ˜¯æœªæ¢ç´¢é¢†åŸŸã€‚</li>
<li>SEED-Bench-R1åŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒæ–¹æ³•ï¼ŒåŒ…å«è¦æ±‚æ„ŸçŸ¥å’Œæ¨ç†å¹³è¡¡çš„çœŸå®è§†é¢‘ã€‚</li>
<li>æ ‡å‡†GRPOæé«˜å›ç­”å‡†ç¡®æ€§ï¼Œä½†ä¼šé™ä½æ¨ç†æ­¥éª¤é—´çš„é€»è¾‘è¿è´¯æ€§ï¼Œä¸€è‡´æ€§ä»…ä¸º57.9%ã€‚</li>
<li>GRPO-CAREæ¡†æ¶æ—¨åœ¨ä¼˜åŒ–ç­”æ¡ˆæ­£ç¡®æ€§å’Œæ¨ç†è¿è´¯æ€§ï¼Œé€šè¿‡ä¸¤å±‚å¥–åŠ±æœºåˆ¶å’Œè‡ªé€‚åº”ä¸€è‡´æ€§å¥–é‡‘å®ç°ã€‚</li>
<li>GRPO-CAREåœ¨SEED-Bench-R1ä¸Šè¡¨ç°ä¼˜äºæ ‡å‡†GRPOï¼Œæœ€éš¾çš„è¯„ä¼°å±‚é¢æ€§èƒ½æå‡6.7%ï¼Œä¸€è‡´æ€§æ”¹è¿›24.5%ã€‚</li>
<li>GRPO-CAREæ¡†æ¶å…·æœ‰å¼ºå¤§çš„å¯è½¬ç§»æ€§ï¼Œèƒ½æ”¹å–„ä¸åŒè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•çš„æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3ca95875b20e1e462a0ee89162d055f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f52f6b34df3495fc8946fa0667777da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f499a803ccd8e826a7439a2fdbfd06da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f1d19bea27be3970f023c8cac490c47.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Seeing-is-Fixing-Cross-Modal-Reasoning-with-Multimodal-LLMs-for-Visual-Software-Issue-Fixing"><a href="#Seeing-is-Fixing-Cross-Modal-Reasoning-with-Multimodal-LLMs-for-Visual-Software-Issue-Fixing" class="headerlink" title="Seeing is Fixing: Cross-Modal Reasoning with Multimodal LLMs for Visual   Software Issue Fixing"></a>Seeing is Fixing: Cross-Modal Reasoning with Multimodal LLMs for Visual   Software Issue Fixing</h2><p><strong>Authors:Kai Huang, Jian Zhang, Xiaofei Xie, Chunyang Chen</strong></p>
<p>Large language model-(LLM) based automated program repair (APR) techniques have shown promising results in resolving real-world GitHub issue tasks. Existing APR systems are primarily evaluated in unimodal settings (e.g., SWE-bench). However, these autonomous systems struggle to resolve multimodal problem scenarios (e.g., SWE-bench M) due to limitations in interpreting and leveraging visual information. In multimodal scenarios, LLMs need to rely on visual information in the graphical user interface (GUI) to understand bugs and generate fixes. To bridge this gap, we propose GUIRepair, a cross-modal reasoning approach for resolving multimodal issue scenarios by understanding and capturing visual information. Specifically, GUIRepair integrates two key components, Image2Code and Code2Image, to enhance fault comprehension and patch validation. Image2Code extracts relevant project documents based on the issue report, then applies this domain knowledge to generate the reproduced code responsible for the visual symptoms, effectively translating GUI images into executable context for better fault comprehension. Code2Image replays the visual issue scenario using the reproduced code and captures GUI renderings of the patched program to assess whether the fix visually resolves the issue, providing feedback for patch validation. We evaluate GUIRepair on SWE-bench M, and the approach demonstrates significant effectiveness. When utilizing GPT-4o as the base model, GUIRepair solves 157 instances, outperforming the best open-source baseline by 26 instances. Furthermore, when using o4-mini as the base model, GUIRepair can achieve even better results and solve 175 instances, outperforming the top commercial system by 22 instances. This emphasizes the success of our new perspective on incorporating cross-modal reasoning by understanding and capturing visual information to resolve multimodal issues. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰æŠ€æœ¯åœ¨è§£å†³ç°å®ä¸–ç•ŒGitHubé—®é¢˜ä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœã€‚ç°æœ‰çš„APRç³»ç»Ÿä¸»è¦æ˜¯åœ¨å•æ¨¡æ€è®¾ç½®ï¼ˆä¾‹å¦‚SWE-benchï¼‰ä¸­è¿›è¡Œè¯„ä¼°çš„ã€‚ç„¶è€Œï¼Œè¿™äº›è‡ªåŠ¨ç³»ç»Ÿåœ¨å¤„ç†å¤šæ¨¡æ€é—®é¢˜åœºæ™¯ï¼ˆä¾‹å¦‚SWE-bench Mï¼‰æ—¶å´é‡åˆ°å›°éš¾ï¼Œè¿™æ˜¯ç”±äºå®ƒä»¬åœ¨è§£é‡Šå’Œåˆ©ç”¨è§†è§‰ä¿¡æ¯æ–¹é¢çš„å±€é™æ€§ã€‚åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹éœ€è¦ä¾èµ–å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä¸­çš„è§†è§‰ä¿¡æ¯æ¥ç†è§£é”™è¯¯å¹¶ç”Ÿæˆä¿®å¤æ–¹æ¡ˆã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†GUIRepairï¼Œè¿™æ˜¯ä¸€ç§è·¨æ¨¡æ€æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡ç†è§£å’Œæ•è·è§†è§‰ä¿¡æ¯æ¥è§£å†³å¤šæ¨¡æ€é—®é¢˜åœºæ™¯ã€‚å…·ä½“æ¥è¯´ï¼ŒGUIRepairé›†æˆäº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šImage2Codeå’ŒCode2Imageï¼Œä»¥å¢å¼ºæ•…éšœç†è§£å’Œè¡¥ä¸éªŒè¯ã€‚Image2Codeæ ¹æ®é—®é¢˜æŠ¥å‘Šæå–ç›¸å…³çš„é¡¹ç›®æ–‡æ¡£ï¼Œç„¶ååº”ç”¨è¿™äº›é¢†åŸŸçŸ¥è¯†æ¥ç”Ÿæˆè´Ÿè´£è§†è§‰ç—‡çŠ¶çš„å†ç°ä»£ç ï¼Œæœ‰æ•ˆåœ°å°†GUIå›¾åƒç¿»è¯‘æˆå¯æ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œä»¥æ›´å¥½åœ°ç†è§£æ•…éšœã€‚Code2Imageä½¿ç”¨å†ç°çš„ä»£ç é‡æ”¾è§†è§‰é—®é¢˜åœºæ™¯ï¼Œå¹¶æ•è·ä¿®è¡¥ç¨‹åºçš„GUIæ¸²æŸ“ï¼Œä»¥è¯„ä¼°ä¿®å¤æ˜¯å¦è§†è§‰ä¸Šè§£å†³äº†é—®é¢˜ï¼Œä¸ºè¡¥ä¸éªŒè¯æä¾›åé¦ˆã€‚æˆ‘ä»¬åœ¨SWE-bench Mä¸Šè¯„ä¼°äº†GUIRepairï¼Œè¯¥æ–¹æ³•æ˜¾ç¤ºäº†æ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚å½“ä½¿ç”¨GPT-4oä½œä¸ºåŸºç¡€æ¨¡å‹æ—¶ï¼ŒGUIRepairè§£å†³äº†157ä¸ªå®ä¾‹ï¼Œæ¯”æœ€ä½³å¼€æºåŸºå‡†é«˜å‡º26ä¸ªå®ä¾‹ã€‚æ­¤å¤–ï¼Œå½“ä½¿ç”¨o4-miniä½œä¸ºåŸºç¡€æ¨¡å‹æ—¶ï¼ŒGUIRepairå¯ä»¥å–å¾—æ›´å¥½çš„ç»“æœï¼Œè§£å†³175ä¸ªå®ä¾‹ï¼Œæ¯”é¡¶çº§å•†ä¸šç³»ç»Ÿé«˜å‡º22ä¸ªå®ä¾‹ã€‚è¿™å¼ºè°ƒäº†æˆ‘ä»¬ç»“åˆè·¨æ¨¡æ€æ¨ç†çš„æ–°è§†è§’ï¼Œé€šè¿‡ç†è§£å’Œæ•è·è§†è§‰ä¿¡æ¯æ¥è§£å†³å¤šæ¨¡æ€é—®é¢˜çš„æˆåŠŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16136v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰æŠ€æœ¯åœ¨è§£å†³ç°å®ä¸–ç•ŒGitHubé—®é¢˜ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºè‰¯å¥½æ•ˆæœã€‚ç„¶è€Œï¼Œç°æœ‰çš„APRç³»ç»Ÿåœ¨å¤„ç†å¤šæ¨¡å¼é—®é¢˜åœºæ™¯ï¼ˆå¦‚SWE-bench Mï¼‰æ—¶é¢ä¸´å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬éš¾ä»¥è§£é‡Šå’Œåˆ©ç”¨è§†è§‰ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºGUIRepairï¼Œä¸€ç§é€šè¿‡ç†è§£å’Œæ•è·è§†è§‰ä¿¡æ¯çš„è·¨æ¨¡å¼æ¨ç†æ–¹æ³•æ¥è§£å†³å¤šæ¨¡å¼é—®é¢˜ã€‚GUIRepairåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šImage2Codeå’ŒCode2Imageï¼Œåˆ†åˆ«ç”¨äºå¢å¼ºæ•…éšœç†è§£å’Œè¡¥ä¸éªŒè¯ã€‚Image2Codeæ ¹æ®é—®é¢˜æŠ¥å‘Šæå–ç›¸å…³é¡¹ç›®æ–‡æ¡£ï¼Œå¹¶åº”ç”¨è¿™äº›é¢†åŸŸçŸ¥è¯†ç”Ÿæˆå†ç°ä»£ç ï¼Œå°†GUIå›¾åƒè½¬åŒ–ä¸ºå¯æ‰§è¡Œä¸Šä¸‹æ–‡ä»¥æ›´å¥½åœ°ç†è§£æ•…éšœã€‚Code2Imageä½¿ç”¨å†ç°çš„ä»£ç é‡æ”¾è§†è§‰é—®é¢˜åœºæ™¯ï¼Œå¹¶æ•è·ä¿®è¡¥ç¨‹åºçš„GUIæ¸²æŸ“ä»¥è¯„ä¼°ä¿®å¤æ˜¯å¦è§†è§‰è§£å†³äº†é—®é¢˜ï¼Œä¸ºè¡¥ä¸éªŒè¯æä¾›åé¦ˆã€‚æˆ‘ä»¬åœ¨SWE-bench Mä¸Šè¯„ä¼°GUIRepairï¼Œè¯¥æ–¹æ³•è¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚å½“ä½¿ç”¨GPT-4oä½œä¸ºåŸºæœ¬æ¨¡å‹æ—¶ï¼ŒGUIRepairè§£å†³äº†157ä¸ªå®ä¾‹ï¼Œæ¯”æœ€ä½³å¼€æºåŸºå‡†é«˜å‡º26ä¸ªå®ä¾‹ã€‚æ­¤å¤–ï¼Œå½“ä½¿ç”¨o4-miniä½œä¸ºåŸºæœ¬æ¨¡å‹æ—¶ï¼ŒGUIRepairèƒ½å–å¾—æ›´å¥½çš„ç»“æœï¼Œè§£å†³175ä¸ªå®ä¾‹ï¼Œæ¯”é¡¶çº§å•†ä¸šç³»ç»Ÿé«˜å‡º22ä¸ªå®ä¾‹ã€‚è¿™å¼ºè°ƒäº†é€šè¿‡ç†è§£å’Œæ•è·è§†è§‰ä¿¡æ¯èå…¥è·¨æ¨¡å¼æ¨ç†çš„æ–°è§†è§’åœ¨è§£å†³å¤šæ¨¡å¼é—®é¢˜ä¸­çš„æˆåŠŸã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ä¸­å±•ç°æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è§£å†³GitHubé—®é¢˜ä»»åŠ¡æ–¹é¢ã€‚</li>
<li>ç°æœ‰APRç³»ç»Ÿåœ¨å¤„ç†å¤šæ¨¡å¼é—®é¢˜åœºæ™¯æ—¶å­˜åœ¨å›°éš¾ï¼Œéœ€è¦è§£é‡Šå’Œåˆ©ç”¨è§†è§‰ä¿¡æ¯ã€‚</li>
<li>GUIRepairé€šè¿‡è·¨æ¨¡å¼æ¨ç†è§£å†³å¤šæ¨¡å¼é—®é¢˜ï¼ŒåŒ…å«ç†è§£å’Œæ•è·è§†è§‰ä¿¡æ¯çš„ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>Image2Codeèƒ½å¤Ÿä»é—®é¢˜æŠ¥å‘Šä¸­æå–ç›¸å…³é¡¹ç›®æ–‡æ¡£ï¼Œå¹¶å°†GUIå›¾åƒè½¬åŒ–ä¸ºå¯æ‰§è¡Œä¸Šä¸‹æ–‡ä»¥å¢å¼ºæ•…éšœç†è§£ã€‚</li>
<li>Code2Imageé€šè¿‡å†ç°ä»£ç é‡æ”¾è§†è§‰é—®é¢˜åœºæ™¯ï¼Œå¹¶æä¾›è¡¥ä¸éªŒè¯çš„åé¦ˆã€‚</li>
<li>GUIRepairåœ¨SWE-bench Mä¸Šçš„è¯„ä¼°ç»“æœè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1770cdb9276aa2e627afc1128bf37fa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c983f354db252aeee0b7722c7765442.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f5922d7ace749899b09a181dc11cbfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0146f10f7f24cb9295f01363ffb8c7a7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LazyEviction-Lagged-KV-Eviction-with-Attention-Pattern-Observation-for-Efficient-Long-Reasoning"><a href="#LazyEviction-Lagged-KV-Eviction-with-Attention-Pattern-Observation-for-Efficient-Long-Reasoning" class="headerlink" title="LazyEviction: Lagged KV Eviction with Attention Pattern Observation for   Efficient Long Reasoning"></a>LazyEviction: Lagged KV Eviction with Attention Pattern Observation for   Efficient Long Reasoning</h2><p><strong>Authors:Haoyue Zhang, Hualei Zhang, Xiaosong Ma, Jie Zhang, Song Guo</strong></p>
<p>Large Language Models (LLMs) exhibit enhanced reasoning capabilities by employing Chain-of-Thought (CoT). However, the extended reasoning sequences introduce significant GPU memory overhead due to increased key-value (KV) cache size, particularly in tasks requiring long reasoning sequences, such as mathematics and programming. Existing KV cache compression methods mitigate memory bottlenecks but struggle in long reasoning tasks. In this paper, we analyze attention patterns in reasoning tasks and reveal a Token Importance Recurrence phenomenon: a large proportion of tokens receive renewed attention after multiple decoding steps, which is failed to capture by existing works and may lead to unpredictable eviction on such periodically critical tokens. To address this, we propose LazyEviction, a lagged KV eviction framework designed to maintain reasoning performance while reducing KV memory. LazyEviction is an Observation Window-based Lagged Eviction Mechanism retaining latent recurring tokens by performing lagged evictions across decoding steps, which contains two key components: (1) Recurrence Interval Tracking for capturing temporal variations in token importance, and (2) an Maximum Recurrence Interval-Centric Eviction Policy that prioritizes eviction based on tokensâ€™ recurrence patterns. Extensive experiments demonstrate that LazyEviction reduces KV cache size by 50% while maintaining comparable accuracy on mathematics reasoning datasets, outperforming state-of-the-art methods. Our findings highlight the importance of preserving recurring tokens, which are critical for maintaining knowledge continuity in multi-step reasoning tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é‡‡ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰è¡¨ç°å‡ºå¢å¼ºçš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºé”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å¤§å°çš„å¢åŠ ï¼Œæ‰©å±•çš„æ¨ç†åºåˆ—ä¼šå¼•å‘é‡å¤§çš„GPUå†…å­˜å¼€é”€ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é•¿æ¨ç†åºåˆ—çš„ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œç¼–ç¨‹ï¼‰ä¸­ã€‚ç°æœ‰çš„KVç¼“å­˜å‹ç¼©æ–¹æ³•å¯ä»¥ç¼“è§£å†…å­˜ç“¶é¢ˆï¼Œä½†åœ¨é•¿æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°æŒ£æ‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†æ¨ç†ä»»åŠ¡ä¸­çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œå¹¶æ­ç¤ºäº†Tokené‡è¦æ€§å¤å‘ç°è±¡ï¼šåœ¨å¤šæ¬¡è§£ç æ­¥éª¤åï¼Œå¾ˆå¤§ä¸€éƒ¨åˆ†ä»¤ç‰Œä¼šè·å¾—æ–°çš„æ³¨æ„åŠ›ï¼Œè¿™å¯èƒ½è¢«ç°æœ‰å·¥ä½œæ‰€å¿½è§†ï¼Œå¹¶å¯èƒ½å¯¼è‡´å¯¹è¿™ç±»å‘¨æœŸæ€§å…³é”®ä»¤ç‰Œçš„ä¸å¯é¢„æµ‹é©±é€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LazyEvictionï¼Œè¿™æ˜¯ä¸€ç§å»¶è¿ŸKVé©±é€æ¡†æ¶ï¼Œæ—¨åœ¨ä¿æŒæ¨ç†æ€§èƒ½çš„åŒæ—¶å‡å°‘KVå†…å­˜ã€‚LazyEvictionæ˜¯ä¸€ç§åŸºäºè§‚å¯Ÿçª—å£çš„å»¶è¿Ÿé©±é€æœºåˆ¶ï¼Œé€šè¿‡è·¨è§£ç æ­¥éª¤æ‰§è¡Œå»¶è¿Ÿé©±é€æ¥ä¿ç•™æ½œåœ¨çš„å¤ç°ä»¤ç‰Œï¼Œå…¶ä¸­åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰å¤å‘é—´éš”è·Ÿè¸ªï¼Œç”¨äºæ•æ‰ä»¤ç‰Œé‡è¦æ€§çš„æ—¶é—´å˜åŒ–ï¼›ï¼ˆ2ï¼‰ä»¥æœ€å¤§å¤å‘é—´éš”ä¸ºä¸­å¿ƒçš„é©±é€ç­–ç•¥ï¼Œæ ¹æ®ä»¤ç‰Œçš„å¤å‘æ¨¡å¼ä¼˜å…ˆè¿›è¡Œé©±é€ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLazyEvictionå°†KVç¼“å­˜å¤§å°å‡å°‘äº†50%ï¼ŒåŒæ—¶åœ¨æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šä¿æŒäº†å¯¹æ ‡æ–¹æ³•çš„å‡†ç¡®æ€§ï¼Œç”šè‡³è¡¨ç°æ›´ä¼˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†ä¿ç•™å¤ç°ä»¤ç‰Œçš„é‡è¦æ€§ï¼Œè¿™å¯¹äºç»´æŒå¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­çš„çŸ¥è¯†è¿ç»­æ€§è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15969v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é‡‡ç”¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å±•ç°å‡ºå¢å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œä½†è¾ƒé•¿çš„æ¨ç†åºåˆ—ä¼šå¼•å‘GPUå†…å­˜è´Ÿæ‹…ã€‚å› ä¸ºä»»åŠ¡éœ€è¦æ›´å¤§çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ç©ºé—´ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰éœ€è¦é•¿æ¨ç†åºåˆ—çš„ä»»åŠ¡ä¸­æ›´ä¸ºæ˜æ˜¾ã€‚ç°æœ‰çš„KVç¼“å­˜å‹ç¼©æ–¹æ³•å¯ä»¥ç¼“è§£å†…å­˜ç“¶é¢ˆï¼Œä½†åœ¨é•¿æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡åˆ†æäº†æ¨ç†ä»»åŠ¡ä¸­çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œå¹¶æ­ç¤ºäº†Token Importance Recurrenceç°è±¡ï¼šåœ¨å¤šæ¬¡è§£ç æ­¥éª¤åï¼Œå¤§é‡æ¯”ä¾‹æ ‡è®°ä¼šå†æ¬¡è·å¾—å…³æ³¨ã€‚ç°æœ‰çš„å·¥ä½œæ²¡æœ‰æ•æ‰åˆ°è¿™ä¸€ç°è±¡ï¼Œå¯èƒ½å¯¼è‡´å¯¹è¿™äº›å®šæœŸå…³é”®æ ‡è®°çš„ä¸å¯é¢„æµ‹é€å‡ºã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†LazyEvictionï¼Œè¿™æ˜¯ä¸€ç§å»¶è¿ŸKVé€å‡ºæ¡†æ¶ï¼Œæ—¨åœ¨ç»´æŒæ¨ç†æ€§èƒ½çš„åŒæ—¶å‡å°‘KVå†…å­˜ä½¿ç”¨ã€‚LazyEvictionåŸºäºè§‚å¯Ÿçª—å£çš„å»¶è¿Ÿé€å‡ºæœºåˆ¶ï¼Œé€šè¿‡è·¨è§£ç æ­¥éª¤è¿›è¡Œå»¶è¿Ÿé€å‡ºä¿ç•™æ½œåœ¨çš„é‡å¤æ ‡è®°ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰ç”¨äºæ•è·æ ‡è®°é‡è¦æ€§çš„æ—¶é—´å˜åŒ–çš„Recurrence Interval Trackingï¼Œï¼ˆ2ï¼‰ä»¥æœ€å¤§é‡å¤é—´éš”ä¸ºä¸­å¿ƒçš„é€å‡ºç­–ç•¥ï¼Œæ ¹æ®æ ‡è®°çš„é‡å¤æ¨¡å¼ä¼˜å…ˆè¿›è¡Œé€å‡ºã€‚å®éªŒè¡¨æ˜ï¼ŒLazyEvictionå°†KVç¼“å­˜å¤§å°å‡å°‘äº†50%ï¼ŒåŒæ—¶åœ¨æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šä¿æŒè¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†ä¿å­˜é‡å¤æ ‡è®°çš„é‡è¦æ€§ï¼Œè¿™å¯¹äºç»´æŒå¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸­çš„çŸ¥è¯†è¿ç»­æ€§è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ä¸‹å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†é¢ä¸´GPUå†…å­˜è´Ÿæ‹…é—®é¢˜ã€‚</li>
<li>åœ¨éœ€è¦é•¿æ¨ç†åºåˆ—çš„ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œç¼–ç¨‹ï¼‰ä¸­ï¼Œé”®å€¼ï¼ˆKVï¼‰ç¼“å­˜éœ€æ±‚æ›´å¤§ã€‚</li>
<li>ç°æœ‰KVç¼“å­˜å‹ç¼©æ–¹æ³•åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ã€‚</li>
<li>æœ¬æ–‡å‘ç°Token Importance Recurrenceç°è±¡ï¼šåœ¨å¤šæ¬¡è§£ç æ­¥éª¤åï¼Œè®¸å¤šæ ‡è®°ä¼šå†æ¬¡è·å¾—å…³æ³¨ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†LazyEvictionæœºåˆ¶ï¼Œè¯¥æœºåˆ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šRecurrence Interval Trackingå’Œæœ€å¤§é‡å¤é—´éš”ä¸ºä¸­å¿ƒçš„é€å‡ºç­–ç•¥ã€‚</li>
<li>å®éªŒè¡¨æ˜LazyEvictionåœ¨å‡å°‘KVç¼“å­˜å¤§å°çš„åŒæ—¶ä¿æŒæ•°å­¦æ¨ç†ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15969">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dde855ae7e868c1fac8b7efd4c72344b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99ffa914fe805e72ca285ba78c3097b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8fc2236eaaf82af129c48a0dc541a58.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ba01f87326925f0f0435a12dd41f47b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Beyond-Audio-and-Pose-A-General-Purpose-Framework-for-Video-Synchronization"><a href="#Beyond-Audio-and-Pose-A-General-Purpose-Framework-for-Video-Synchronization" class="headerlink" title="Beyond Audio and Pose: A General-Purpose Framework for Video   Synchronization"></a>Beyond Audio and Pose: A General-Purpose Framework for Video   Synchronization</h2><p><strong>Authors:Yosub Shin, Igor Molybog</strong></p>
<p>Video synchronization-aligning multiple video streams capturing the same event from different angles-is crucial for applications such as reality TV show production, sports analysis, surveillance, and autonomous systems. Prior work has heavily relied on audio cues or specific visual events, limiting applicability in diverse settings where such signals may be unreliable or absent. Additionally, existing benchmarks for video synchronization lack generality and reproducibility, restricting progress in the field. In this work, we introduce VideoSync, a video synchronization framework that operates independently of specific feature extraction methods, such as human pose estimation, enabling broader applicability across different content types. We evaluate our system on newly composed datasets covering single-human, multi-human, and non-human scenarios, providing both the methodology and code for dataset creation to establish reproducible benchmarks. Our analysis reveals biases in prior SOTA work, particularly in SeSyn-Netâ€™s preprocessing pipeline, leading to inflated performance claims. We correct these biases and propose a more rigorous evaluation framework, demonstrating that VideoSync outperforms existing approaches, including SeSyn-Net, under fair experimental conditions. Additionally, we explore various synchronization offset prediction methods, identifying a convolutional neural network (CNN)-based model as the most effective. Our findings advance video synchronization beyond domain-specific constraints, making it more generalizable and robust for real-world applications. </p>
<blockquote>
<p>è§†é¢‘åŒæ­¥â€”â€”å¯¹é½ä»ä¸åŒè§’åº¦æ•æ‰åŒä¸€äº‹ä»¶çš„å¤šä¸ªè§†é¢‘æµâ€”â€”å¯¹äºç°å®ç”µè§†èŠ‚ç›®åˆ¶ä½œã€è¿åŠ¨åˆ†æã€ç›‘æ§å’Œè‡ªä¸»ç³»ç»Ÿç­‰é¢†åŸŸçš„åº”ç”¨è‡³å…³é‡è¦ã€‚æ—©æœŸçš„å·¥ä½œä¸»è¦ä¾èµ–äºéŸ³é¢‘çº¿ç´¢æˆ–ç‰¹å®šçš„è§†è§‰äº‹ä»¶ï¼Œè¿™åœ¨å¤šæ ·ç¯å¢ƒä¸­é™åˆ¶äº†å…¶é€‚ç”¨æ€§ï¼Œåœ¨è¿™äº›ç¯å¢ƒä¸­ï¼Œæ­¤ç±»ä¿¡å·å¯èƒ½ä¸å¯é æˆ–ä¸å­˜åœ¨ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„è§†é¢‘åŒæ­¥åŸºå‡†ç¼ºä¹é€šç”¨æ€§å’Œå¯é‡å¤æ€§ï¼Œé™åˆ¶äº†è¯¥é¢†åŸŸçš„è¿›å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†VideoSyncï¼Œä¸€ä¸ªç‹¬ç«‹äºç‰¹å®šç‰¹å¾æå–æ–¹æ³•çš„è§†é¢‘åŒæ­¥æ¡†æ¶ï¼Œä¾‹å¦‚äººä½“å§¿æ€ä¼°è®¡ï¼Œä½¿å…¶åœ¨ä¸åŒç±»å‹çš„å†…å®¹ä¸Šå…·æœ‰æ›´å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬åœ¨æ–°ç»„æˆçš„æ¶µç›–å•äººã€å¤šäººã€éäººåœºæ™¯çš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„ç³»ç»Ÿï¼Œå¹¶æä¾›äº†æ•°æ®é›†åˆ›å»ºçš„æ–¹æ³•å’Œä»£ç æ¥å»ºç«‹å¯é‡å¤çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†æ—©æœŸæœ€å…ˆè¿›å·¥ä½œå­˜åœ¨çš„åè§ï¼Œç‰¹åˆ«æ˜¯åœ¨SeSyn-Netçš„é¢„å¤„ç†ç®¡é“ä¸­ï¼Œå¯¼è‡´æ€§èƒ½å£°æ˜å¤¸å¤§ã€‚æˆ‘ä»¬çº æ­£äº†è¿™äº›åè§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ›´ä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯æ˜åœ¨å…¬å¹³çš„å®éªŒæ¡ä»¶ä¸‹ï¼ŒVideoSyncä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬SeSyn-Netã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†å„ç§åŒæ­¥åç§»é¢„æµ‹æ–¹æ³•ï¼Œå¹¶ç¡®å®šåŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ¨¡å‹æœ€ä¸ºæœ‰æ•ˆã€‚æˆ‘ä»¬çš„ç ”ç©¶å°†è§†é¢‘åŒæ­¥æŠ€æœ¯æ¨è¿›åˆ°äº†è¶…è¶Šç‰¹å®šé¢†åŸŸçš„çº¦æŸä¹‹å¤–ï¼Œä½¿å…¶åœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­æ›´å…·é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15937v1">PDF</a> </p>
<p><strong>Summary</strong><br>è§†é¢‘åŒæ­¥æŠ€æœ¯å¯¹äºç°å®ç”µè§†èŠ‚ç›®åˆ¶ä½œã€è¿åŠ¨åˆ†æã€ç›‘æ§å’Œè‡ªä¸»ç³»ç»Ÿç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ä»¥å‰çš„ç ”ç©¶ä¸»è¦ä¾èµ–äºéŸ³é¢‘çº¿ç´¢æˆ–ç‰¹å®šè§†è§‰äº‹ä»¶ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸åŒåœºæ™¯ä¸‹çš„é€‚ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºVideoSyncæ¡†æ¶ï¼Œç‹¬ç«‹äºç‰¹å®šç‰¹å¾æå–æ–¹æ³•ï¼Œå¦‚äººä½“å§¿æ€ä¼°è®¡ï¼Œå¯åœ¨ä¸åŒç±»å‹å†…å®¹ä¸Šæ›´å¹¿æ³›åº”ç”¨ã€‚æˆ‘ä»¬åœ¨æ–°ç»„æˆçš„å•ä¸€äººç±»ã€å¤šäººç±»å’Œéäººç±»åœºæ™¯æ•°æ®é›†ä¸Šè¯„ä¼°äº†ç³»ç»Ÿæ€§èƒ½ï¼Œå¹¶æä¾›æ•°æ®é›†åˆ›å»ºçš„æ–¹æ³•å’Œä»£ç ï¼Œä»¥å»ºç«‹å¯é‡å¤ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ã€‚åˆ†ææ˜¾ç¤ºå…ˆå‰çš„å·¥ä½œå­˜åœ¨åè§ï¼Œç‰¹åˆ«æ˜¯åœ¨SeSyn-Netçš„é¢„å¤„ç†ç®¡é“ä¸­ï¼Œå¯¼è‡´æ€§èƒ½å£°æ˜å¤¸å¤§ã€‚æˆ‘ä»¬çº æ­£è¿™äº›åè§ï¼Œæå‡ºæ›´ä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯æ˜VideoSyncåœ¨å…¬å¹³çš„å®éªŒæ¡ä»¶ä¸‹ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬SeSyn-Netã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†å„ç§åŒæ­¥åç§»é¢„æµ‹æ–¹æ³•ï¼Œå¹¶ç¡®å®šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹æœ€ä¸ºæœ‰æ•ˆã€‚æˆ‘ä»¬çš„ç ”ç©¶å°†è§†é¢‘åŒæ­¥æŠ€æœ¯æ¨å‘äº†è¶…è¶Šç‰¹å®šé¢†åŸŸçš„çº¦æŸï¼Œä½¿å…¶æ›´å…·é€šç”¨æ€§å’Œç¨³å¥æ€§ï¼Œé€‚ç”¨äºå®é™…åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘åŒæ­¥æŠ€æœ¯å¯¹äºå¤šç§åº”ç”¨å¦‚ç°å®ç”µè§†åˆ¶ä½œã€è¿åŠ¨åˆ†æã€ç›‘æ§å’Œè‡ªä¸»ç³»ç»Ÿè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰è§†é¢‘åŒæ­¥æŠ€æœ¯ä¾èµ–äºéŸ³é¢‘æˆ–ç‰¹å®šè§†è§‰äº‹ä»¶çº¿ç´¢ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é€‚ç”¨æ€§ã€‚</li>
<li>å¼•å…¥VideoSyncæ¡†æ¶ï¼Œå¯ç‹¬ç«‹äºç‰¹å®šç‰¹å¾æå–æ–¹æ³•å·¥ä½œï¼Œæé«˜æŠ€æœ¯çš„é€šç”¨æ€§ã€‚</li>
<li>åœ¨æ–°ç»„æˆçš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†VideoSyncæ€§èƒ½ï¼ŒåŒ…æ‹¬å•ä¸€ã€å¤šä¸ªäººç±»å’Œéäººç±»åœºæ™¯ã€‚</li>
<li>åˆ†ææ˜¾ç¤ºå…ˆå‰çš„å·¥ä½œå­˜åœ¨åè§ï¼Œæ€§èƒ½å£°æ˜å¯èƒ½å¤¸å¤§ï¼Œéœ€è¦æ›´ä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>VideoSyncåœ¨å…¬å¹³æ¡ä»¶ä¸‹ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬SeSyn-Netã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-23b052f41871eea87fcb37ed252a5ee9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d7e93e8cbddadf9b3b11a0638b4a115e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e6897716da37a1077770ad24bfe7be4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3701acd2c10fa721c996d73633d9f35d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00313f2a24502e5045eec3415feb14a5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Exploring-Big-Five-Personality-and-AI-Capability-Effects-in-LLM-Simulated-Negotiation-Dialogues"><a href="#Exploring-Big-Five-Personality-and-AI-Capability-Effects-in-LLM-Simulated-Negotiation-Dialogues" class="headerlink" title="Exploring Big Five Personality and AI Capability Effects in   LLM-Simulated Negotiation Dialogues"></a>Exploring Big Five Personality and AI Capability Effects in   LLM-Simulated Negotiation Dialogues</h2><p><strong>Authors:Myke C. Cohen, Zhe Su, Hsien-Te Kao, Daniel Nguyen, Spencer Lynch, Maarten Sap, Svitlana Volkova</strong></p>
<p>This paper presents an evaluation framework for agentic AI systems in mission-critical negotiation contexts, addressing the need for AI agents that can adapt to diverse human operators and stakeholders. Using Sotopia as a simulation testbed, we present two experiments that systematically evaluated how personality traits and AI agent characteristics influence LLM-simulated social negotiation outcomesâ€“a capability essential for a variety of applications involving cross-team coordination and civil-military interactions. Experiment 1 employs causal discovery methods to measure how personality traits impact price bargaining negotiations, through which we found that Agreeableness and Extraversion significantly affect believability, goal achievement, and knowledge acquisition outcomes. Sociocognitive lexical measures extracted from team communications detected fine-grained differences in agentsâ€™ empathic communication, moral foundations, and opinion patterns, providing actionable insights for agentic AI systems that must operate reliably in high-stakes operational scenarios. Experiment 2 evaluates human-AI job negotiations by manipulating both simulated human personality and AI system characteristics, specifically transparency, competence, adaptability, demonstrating how AI agent trustworthiness impact mission effectiveness. These findings establish a repeatable evaluation methodology for experimenting with AI agent reliability across diverse operator personalities and human-agent team dynamics, directly supporting operational requirements for reliable AI systems. Our work advances the evaluation of agentic AI workflows by moving beyond standard performance metrics to incorporate social dynamics essential for mission success in complex operations. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°å…³é”®ä»»åŠ¡è°ˆåˆ¤ç¯å¢ƒä¸­çš„æ™ºèƒ½ä½“AIç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ï¼Œæ»¡è¶³äº†AIä»£ç†èƒ½å¤Ÿé€‚åº”ä¸åŒçš„äººç±»æ“ä½œå‘˜å’Œåˆ©ç›Šç›¸å…³è€…çš„éœ€æ±‚ã€‚ä½¿ç”¨Sotopiaä½œä¸ºä»¿çœŸæµ‹è¯•å¹³å°ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸¤é¡¹å®éªŒï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†äººæ ¼ç‰¹è´¨å’ŒAIä»£ç†ç‰¹å¾å¦‚ä½•å½±å“LLMæ¨¡æ‹Ÿçš„ç¤¾ä¼šè°ˆåˆ¤ç»“æœâ€”â€”è¿™ä¸€èƒ½åŠ›å¯¹äºæ¶‰åŠè·¨å›¢é˜Ÿåè°ƒå’Œå†›æ°‘äº’åŠ¨çš„å„ç§åº”ç”¨ç¨‹åºè‡³å…³é‡è¦ã€‚å®éªŒä¸€é‡‡ç”¨å› æœå‘ç°æ–¹æ³•æ¥è¡¡é‡äººæ ¼ç‰¹è´¨å¯¹è®®ä»·è°ˆåˆ¤çš„å½±å“ï¼Œæˆ‘ä»¬å‘ç°å®œäººæ€§å’Œå¤–å‘æ€§æ˜¾è‘—å½±å“å¯ä¿¡åº¦ã€ç›®æ ‡å®ç°å’ŒçŸ¥è¯†è·å–ç»“æœã€‚ä»å›¢é˜Ÿæ²Ÿé€šä¸­æå–çš„ç¤¾ä¼šè®¤çŸ¥è¯æ±‡åº¦é‡å‘ç°äº†ä»£ç†äººåœ¨ç§»æƒ…æ²Ÿé€šã€é“å¾·åŸºç¡€å’Œæ„è§æ¨¡å¼ä¸Šçš„ç»†å¾®å·®å¼‚ï¼Œä¸ºå¿…é¡»åœ¨é«˜é£é™©æ“ä½œåœºæ™¯ä¸­å¯é è¿è¡Œçš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†å¯æ“ä½œæ€§çš„è§è§£ã€‚å®éªŒäºŒé€šè¿‡æ“çºµæ¨¡æ‹Ÿäººç±»ä¸ªæ€§å’ŒAIç³»ç»Ÿç‰¹æ€§ï¼ˆç‰¹åˆ«æ˜¯é€æ˜åº¦ã€èƒ½åŠ›å’Œé€‚åº”æ€§ï¼‰æ¥è¯„ä¼°äººæœºå·¥ä½œè°ˆåˆ¤ï¼Œå±•ç¤ºäº†AIä»£ç†çš„å¯ä¿¡åº¦å¦‚ä½•å½±å“ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›å‘ç°å»ºç«‹äº†ä¸€ç§å¯é‡å¤çš„è¯„ä¼°æ–¹æ³•ï¼Œç”¨äºåœ¨ä¸åŒæ“ä½œå‘˜ä¸ªæ€§å’Œäººæœºå›¢é˜ŸåŠ¨æ€æƒ…å†µä¸‹å®éªŒAIä»£ç†çš„å¯é æ€§ï¼Œç›´æ¥æ”¯æŒå¯¹å¯é AIç³»ç»Ÿçš„æ“ä½œè¦æ±‚ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡è¶…è¶Šæ ‡å‡†æ€§èƒ½æŒ‡æ ‡æ¥è¯„ä¼°æ™ºèƒ½ä½“AIçš„å·¥ä½œæµç¨‹ï¼Œçº³å…¥å¯¹å¤æ‚æ“ä½œä»»åŠ¡æˆåŠŸè‡³å…³é‡è¦çš„ç¤¾ä¼šåŠ¨æ€å› ç´ ï¼Œä»è€Œæ¨åŠ¨äº†æ™ºèƒ½ä½“AIè¯„ä¼°çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15928v1">PDF</a> Under review for KDD 2025 Workshop on Evaluation and Trustworthiness   of Agentic and Generative AI Models</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶è¯„ä¼°äº†ç”¨äºå…³é”®è°ˆåˆ¤ä»»åŠ¡çš„æ™ºèƒ½AIç³»ç»Ÿçš„è¡¨ç°æ¡†æ¶ã€‚è¯¥ç ”ç©¶ä½¿ç”¨Sotopiaä½œä¸ºæµ‹è¯•å¹³å°ï¼Œé€šè¿‡ä¸¤ä¸ªå®éªŒæ¢è®¨äº†äººæ ¼ç‰¹è´¨å’ŒAIç‰¹æ€§å¦‚ä½•å½±å“æ¨¡æ‹Ÿçš„ç¤¾ä¼šè°ˆåˆ¤ç»“æœã€‚ç¬¬ä¸€ä¸ªå®éªŒæµ‹é‡äº†äººæ ¼ç‰¹è´¨å¦‚ä½•å½±å“ä»·æ ¼è°ˆåˆ¤çš„ç»“æœï¼Œå‘ç°åˆä½œå’Œå¼€æ”¾æ€§å¯¹å¯ä¿¡åº¦å’Œç›®æ ‡å®ç°æœ‰æ˜¾è‘—å½±å“ã€‚ç¬¬äºŒä¸ªå®éªŒåˆ™è¯„ä¼°äº†äººæœºä¹‹é—´çš„ä»»åŠ¡è°ˆåˆ¤ï¼Œå¹¶æ¢è®¨äº†AIç³»ç»Ÿçš„é€æ˜åº¦ã€èƒ½åŠ›å’Œé€‚åº”æ€§ç­‰å› ç´ å¦‚ä½•å½±å“ä»»åŠ¡æ•ˆç‡ã€‚è¯¥ç ”ç©¶ä¸ºæ™ºèƒ½AIç³»ç»Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°æä¾›äº†é‡è¦çš„è¯„ä¼°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½AIç³»ç»Ÿåœ¨å…³é”®è°ˆåˆ¤ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
<li>ä½¿ç”¨Sotopiaä½œä¸ºæµ‹è¯•å¹³å°ï¼Œè¿›è¡Œäº†ä¸¤ä¸ªå®éªŒæ¥æ¢è®¨äººæ ¼ç‰¹è´¨å’ŒAIç‰¹æ€§å¯¹æ¨¡æ‹Ÿç¤¾ä¼šè°ˆåˆ¤ç»“æœçš„å½±å“ã€‚</li>
<li>å®éªŒä¸€å‘ç°åˆä½œå’Œå¼€æ”¾æ€§äººæ ¼ç‰¹è´¨å¯¹è°ˆåˆ¤ç»“æœçš„å¯ä¿¡åº¦å’Œç›®æ ‡å®ç°æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>é€šè¿‡åˆ†æå›¢é˜Ÿæ²Ÿé€šä¸­çš„ç¤¾ä¼šè®¤çŸ¥è¯æ±‡ï¼Œç ”ç©¶å‘ç°äº†AIç³»ç»Ÿåœ¨å¤æ‚ä»»åŠ¡åœºæ™¯ä¸­éœ€è¦å…·å¤‡çš„ç»†å¾®å·®åˆ«ç‰¹è´¨ï¼Œå¦‚å…±æƒ…æ²Ÿé€šã€é“å¾·åŸºç¡€å’Œæ„è§æ¨¡å¼ç­‰ã€‚</li>
<li>å®éªŒäºŒæ¢è®¨äº†äººæœºé—´ä»»åŠ¡è°ˆåˆ¤çš„å½±å“å› ç´ ï¼Œå¦‚æ¨¡æ‹Ÿäººç±»äººæ ¼å’ŒAIç³»ç»Ÿçš„é€æ˜åº¦ã€èƒ½åŠ›å’Œé€‚åº”æ€§ç­‰ï¼Œè¿™äº›å› ç´ å¯èƒ½å½±å“ä»»åŠ¡çš„æ•ˆç‡å’Œå¯é æ€§ã€‚åŒæ—¶æŒ‡å‡ºä¿¡ä»»æ˜¯å½±å“ä»»åŠ¡æœ‰æ•ˆæ€§çš„å…³é”®å› ç´ ä¹‹ä¸€ã€‚è¿™é¡¹ç ”ç©¶æä¾›äº†ä¸€ç§å¯é‡å¤çš„å®éªŒæ–¹æ³•ï¼Œç”¨äºæµ‹è¯•ä¸åŒæ“ä½œè€…ä¸ªæ€§å’Œäººæœºå›¢é˜ŸåŠ¨æ€ä¸‹çš„AIç³»ç»Ÿå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15928">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dbfd9a0142b19e26f647e233db1bbb75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-037301d6b270bbfb6e4f041274167468.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0021b500b4f76c72aadd1e71016c5ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5498bbac13cac2fc6e8ee52ca6217c24.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Fractional-Reasoning-via-Latent-Steering-Vectors-Improves-Inference-Time-Compute"><a href="#Fractional-Reasoning-via-Latent-Steering-Vectors-Improves-Inference-Time-Compute" class="headerlink" title="Fractional Reasoning via Latent Steering Vectors Improves Inference Time   Compute"></a>Fractional Reasoning via Latent Steering Vectors Improves Inference Time   Compute</h2><p><strong>Authors:Sheng Liu, Tianlang Chen, Pan Lu, Haotian Ye, Yizheng Chen, Lei Xing, James Zou</strong></p>
<p>Test-time compute has emerged as a powerful paradigm for improving the performance of large language models (LLMs), where generating multiple outputs or refining individual chains can significantly boost answer accuracy. However, existing methods like Best-of-N, majority voting, and self-reflection typically apply reasoning in a uniform way across inputs, overlooking the fact that different problems may require different levels of reasoning depth. In this work, we propose Fractional Reasoning, a training-free and model-agnostic framework that enables continuous control over reasoning intensity at inference time, going beyond the limitations of fixed instructional prompts. Our method operates by extracting the latent steering vector associated with deeper reasoning and reapplying it with a tunable scaling factor, allowing the model to tailor its reasoning process to the complexity of each input. This supports two key modes of test-time scaling: (1) improving output quality in breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing the correctness of individual reasoning chains in depth-based strategies (e.g., self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that Fractional Reasoning consistently improves performance across diverse reasoning tasks and models. </p>
<blockquote>
<p>æµ‹è¯•æ—¶çš„è®¡ç®—å·²ç»æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½çš„ä¸€ç§å¼ºå¤§èŒƒå¼ï¼Œå…¶ä¸­ç”Ÿæˆå¤šä¸ªè¾“å‡ºæˆ–ç²¾ç‚¼å•ä¸ªé“¾å¯ä»¥æ˜¾è‘—æé«˜ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚é€‰æœ€ä¼˜è§£ã€å¤šæ•°æŠ•ç¥¨å’Œè‡ªæˆ‘åæ€ç­‰ï¼Œé€šå¸¸åœ¨æ•´ä¸ªè¾“å…¥ä¸Šä»¥ä¸€ç§ç»Ÿä¸€çš„æ–¹å¼åº”ç”¨æ¨ç†ï¼Œå¿½ç•¥äº†ä¸åŒé—®é¢˜å¯èƒ½éœ€è¦ä¸åŒå±‚æ¬¡çš„æ¨ç†æ·±åº¦è¿™ä¸€äº‹å®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œåˆ†æ•°æ¨ç†â€ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒä¸”é€‚ç”¨äºå„ç§æ¨¡å‹çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†æ—¶å®ç°æ¨ç†å¼ºåº¦çš„è¿ç»­æ§åˆ¶ï¼Œè¶…è¶Šäº†å›ºå®šæŒ‡ä»¤æç¤ºçš„é™åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æå–ä¸æ·±å±‚æ¨ç†ç›¸å…³çš„æ½œåœ¨å¼•å¯¼å‘é‡ï¼Œå¹¶é‡æ–°åº”ç”¨ä¸€ä¸ªå¯è°ƒçš„æ¯”ä¾‹å› å­æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ¯ä¸ªè¾“å…¥çš„å¤æ‚æ€§è°ƒæ•´å…¶æ¨ç†è¿‡ç¨‹ã€‚è¿™æ”¯æŒäº†ä¸¤ç§å…³é”®çš„æµ‹è¯•æ—¶ç¼©æ”¾æ¨¡å¼ï¼šï¼ˆ1ï¼‰åœ¨åŸºäºå¹¿åº¦çš„ç­–ç•¥ï¼ˆå¦‚é€‰æœ€ä¼˜è§£ã€å¤šæ•°æŠ•ç¥¨ç­‰ï¼‰ä¸­æé«˜è¾“å‡ºè´¨é‡ï¼›ï¼ˆ2ï¼‰åœ¨åŸºäºæ·±åº¦çš„ç­–ç•¥ï¼ˆå¦‚è‡ªæˆ‘åæ€ï¼‰ä¸­æé«˜å•ä¸ªæ¨ç†é“¾çš„æ­£ç¡®æ€§ã€‚åœ¨GSM8Kã€MATH500å’ŒGPQAä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåˆ†æ•°æ¨ç†åœ¨å¤šç§æ¨ç†ä»»åŠ¡å’Œæ¨¡å‹ä¸Šéƒ½èƒ½æŒç»­æé«˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15882v1">PDF</a> 18 pages, 5 figures, Project website:   <a target="_blank" rel="noopener" href="https://shengliu66.github.io/fractreason/">https://shengliu66.github.io/fractreason/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•æ—¶çš„è®¡ç®—å·²ç»æˆä¸ºä¸€ç§å¼ºå¤§çš„èŒƒå¼ï¼Œç”¨ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¿½ç•¥äº†ä¸åŒé—®é¢˜å¯èƒ½éœ€è¦ä¸åŒå±‚æ¬¡çš„æ¨ç†æ·±åº¦ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ— éœ€è®­ç»ƒä¸”é€‚ç”¨äºæ‰€æœ‰æ¨¡å‹çš„æ¡†æ¶â€”â€”åˆ†æ•°æ¨ç†ï¼Œå®ƒèƒ½åœ¨æ¨ç†æ—¶å®ç°æ¨ç†å¼ºåº¦çš„è¿ç»­æ§åˆ¶ï¼Œçªç ´äº†å›ºå®šæŒ‡ä»¤æç¤ºçš„å±€é™ã€‚é€šè¿‡æå–ä¸æ·±å±‚æ¨ç†ç›¸å…³çš„æ½œåœ¨å¼•å¯¼å‘é‡å¹¶é‡æ–°åº”ç”¨ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®æ¯ä¸ªè¾“å…¥çš„å¤æ‚æ€§è°ƒæ•´è‡ªå·±çš„æ¨ç†è¿‡ç¨‹ã€‚åˆ†æ•°æ¨ç†å¯åº”ç”¨äºä¸¤ç§æµ‹è¯•æ—¶çš„ç¼©æ”¾æ¨¡å¼ï¼šä¸€æ˜¯æé«˜å¹¿åº¦ç­–ç•¥çš„è¾“å‡ºè´¨é‡ï¼ˆå¦‚â€œNä¸­é€‰ä¸€â€ã€å¤šæ•°æŠ•ç¥¨ï¼‰ï¼ŒäºŒæ˜¯æé«˜æ·±åº¦ç­–ç•¥çš„æ¨ç†é“¾çš„æ­£ç¡®æ€§ï¼ˆå¦‚è‡ªæˆ‘åæ€ï¼‰ã€‚å®éªŒè¯æ˜ï¼Œåˆ†æ•°æ¨ç†åœ¨å¤šæ ·åŒ–çš„æ¨ç†ä»»åŠ¡å’Œæ¨¡å‹ä¸Šéƒ½èƒ½æŒç»­æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æµ‹è¯•æ—¶çš„è®¡ç®—å·²æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„é‡è¦èŒƒå¼ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥äº†ä¸åŒé—®é¢˜å¯èƒ½éœ€è¦ä¸åŒå±‚æ¬¡çš„æ¨ç†æ·±åº¦ã€‚</li>
<li>åˆ†æ•°æ¨ç†æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨¡å‹æ— å…³æ¡†æ¶ï¼Œå¯ä»¥åœ¨æ¨ç†æ—¶è¿ç»­æ§åˆ¶æ¨ç†å¼ºåº¦ã€‚</li>
<li>åˆ†æ•°æ¨ç†é€šè¿‡æå–ä¸æ·±å±‚æ¨ç†ç›¸å…³çš„æ½œåœ¨å¼•å¯¼å‘é‡å¹¶é‡æ–°åº”ç”¨æ¥å®ç°å¯¹æ¨ç†å¼ºåº¦çš„æ§åˆ¶ã€‚</li>
<li>æ¨¡å‹å¯ä»¥æ ¹æ®æ¯ä¸ªè¾“å…¥çš„å¤æ‚æ€§è°ƒæ•´å…¶æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>åˆ†æ•°æ¨ç†å¯æé«˜å¹¿åº¦ç­–ç•¥å’Œæ·±åº¦ç­–ç•¥çš„æ¨ç†æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15882">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17ae690e9ab19acdad106a4866c0bdcb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85250fcfb0a83afa12341ec4898268e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29468104eb8ec7033b146c4c7e4acda0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4fdac6e9b83c3bd633e3acc61430f688.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Reinforcing-Spatial-Reasoning-in-Vision-Language-Models-with-Interwoven-Thinking-and-Visual-Drawing"><a href="#Reinforcing-Spatial-Reasoning-in-Vision-Language-Models-with-Interwoven-Thinking-and-Visual-Drawing" class="headerlink" title="Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven   Thinking and Visual Drawing"></a>Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven   Thinking and Visual Drawing</h2><p><strong>Authors:Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan</strong></p>
<p>As textual reasoning with large language models (LLMs) has advanced significantly, there has been growing interest in enhancing the multimodal reasoning capabilities of large vision-language models (LVLMs). However, existing methods primarily approach multimodal reasoning in a straightforward, text-centric manner, where both reasoning and answer derivation are conducted purely through text, with the only difference being the presence of multimodal input. As a result, these methods often encounter fundamental limitations in spatial reasoning tasks that demand precise geometric understanding and continuous spatial tracking-capabilities that humans achieve through mental visualization and manipulation. To address the limitations, we propose drawing to reason in space, a novel paradigm that enables LVLMs to reason through elementary drawing operations in the visual space. By equipping models with basic drawing operations, including annotating bounding boxes and drawing auxiliary lines, we empower them to express and analyze spatial relationships through direct visual manipulation, meanwhile avoiding the performance ceiling imposed by specialized perception tools in previous tool-integrated reasoning approaches. To cultivate this capability, we develop a three-stage training framework: cold-start training with synthetic data to establish basic drawing abilities, reflective rejection sampling to enhance self-reflection behaviors, and reinforcement learning to directly optimize for target rewards. Extensive experiments demonstrate that our model, named VILASR, consistently outperforms existing methods across diverse spatial reasoning benchmarks, involving maze navigation, static spatial reasoning, video-based reasoning, and multi-view-based reasoning tasks, with an average improvement of 18.4%. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬æ¨ç†èƒ½åŠ›æ˜¾è‘—æé«˜ï¼Œå¢å¼ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä¸»è¦é‡‡å–ç›´æ¥ã€ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€æ¨ç†æ–¹å¼ï¼Œæ¨ç†å’Œç­”æ¡ˆæ¨å¯¼éƒ½çº¯ç²¹é€šè¿‡æ–‡æœ¬è¿›è¡Œï¼Œä¸åŒä»…åœ¨äºæ˜¯å¦å­˜åœ¨å¤šæ¨¡æ€è¾“å…¥ã€‚å› æ­¤ï¼Œè¿™äº›æ–¹æ³•åœ¨éœ€è¦ç²¾ç¡®å‡ ä½•ç†è§£å’Œè¿ç»­ç©ºé—´è·Ÿè¸ªèƒ½åŠ›çš„ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­ç»å¸¸é‡åˆ°æ ¹æœ¬æ€§çš„å±€é™ï¼Œè€Œäººç±»åˆ™é€šè¿‡å¿ƒç†å¯è§†åŒ–å’Œæ“ä½œæ¥å®ç°è¿™äº›èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ–¹å¼â€”â€”ç©ºé—´ç»˜å›¾æ¨ç†ã€‚è¿™ç§æ–¹å¼è®©LVLMèƒ½å¤Ÿé€šè¿‡è§†è§‰ç©ºé—´çš„åŸºæœ¬ç»˜å›¾æ“ä½œè¿›è¡Œæ¨ç†ã€‚é€šè¿‡ä¸ºæ¨¡å‹é…å¤‡åŸºæœ¬çš„ç»˜å›¾æ“ä½œï¼ŒåŒ…æ‹¬æ ‡æ³¨è¾¹ç•Œæ¡†å’Œç»˜åˆ¶è¾…åŠ©çº¿ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿé€šè¿‡ç›´æ¥çš„è§†è§‰æ“ä½œæ¥è¡¨è¾¾å’Œåˆ†æç©ºé—´å…³ç³»ï¼ŒåŒæ—¶é¿å…äº†ä¹‹å‰å·¥å…·é›†æˆæ¨ç†æ–¹æ³•ä¸­ä¸“ç”¨æ„ŸçŸ¥å·¥å…·æ‰€å¸¦æ¥çš„æ€§èƒ½ä¸Šé™ã€‚ä¸ºäº†åŸ¹å…»è¿™ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼šä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œå†·å¯åŠ¨è®­ç»ƒä»¥å»ºç«‹åŸºæœ¬ç»˜å›¾èƒ½åŠ›ã€é€šè¿‡åå°„æ‹’ç»é‡‡æ ·å¢å¼ºè‡ªæˆ‘åæ€è¡Œä¸ºã€ä»¥åŠä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç›´æ¥ä¼˜åŒ–ç›®æ ‡å¥–åŠ±ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬å‘½åä¸ºVILASRçš„æ¨¡å‹åœ¨å„ç§ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ¶‰åŠè¿·å®«å¯¼èˆªã€é™æ€ç©ºé—´æ¨ç†ã€åŸºäºè§†é¢‘çš„ç†ç”±å’Œå¤šè§†è§’æ¨ç†ä»»åŠ¡ï¼Œå¹³å‡æé«˜äº†18.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09965v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡çº¯æ–‡æœ¬è¿›è¡Œæ¨ç†å’Œç­”æ¡ˆæ¨å¯¼ï¼Œå¯¼è‡´åœ¨å¤„ç†éœ€è¦ç²¾ç¡®å‡ ä½•ç†è§£å’Œè¿ç»­ç©ºé—´è¿½è¸ªèƒ½åŠ›çš„ç©ºé—´æ¨ç†ä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œä»¥ç»˜å›¾è¿›è¡Œç©ºé—´æ¨ç†â€çš„æ–°èŒƒå¼ï¼Œé€šè¿‡åŸºæœ¬çš„ç»˜å›¾æ“ä½œï¼ˆå¦‚æ ‡æ³¨è¾¹ç•Œæ¡†å’Œç»˜åˆ¶è¾…åŠ©çº¿ï¼‰æ¥èµ‹äºˆæ¨¡å‹ç›´æ¥è§†è§‰æ“æ§èƒ½åŠ›ï¼Œä»¥è¡¨è¾¾å’Œè§£æç©ºé—´å…³ç³»ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶æ¥åŸ¹å…»è¿™ç§èƒ½åŠ›ï¼Œå¹¶åœ¨è¿·å®«å¯¼èˆªã€é™æ€ç©ºé—´æ¨ç†ã€è§†é¢‘æ¨ç†å’Œå¤šè§†è§’æ¨ç†ç­‰å¤šæ ·åŒ–ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚è¯¥æ¨¡å‹è¢«ç§°ä¸ºVILASRï¼Œå¹³å‡æ”¹è¿›ç‡ä¸º18.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­çš„å…´è¶£å¢é•¿ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–çº¯æ–‡æœ¬è¿›è¡Œæ¨ç†å’Œç­”æ¡ˆæ¨å¯¼ã€‚</li>
<li>ç©ºé—´æ¨ç†ä»»åŠ¡éœ€è¦ç²¾ç¡®å‡ ä½•ç†è§£å’Œè¿ç»­ç©ºé—´è¿½è¸ªèƒ½åŠ›ï¼Œè¿™æ˜¯ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºâ€œä»¥ç»˜å›¾è¿›è¡Œç©ºé—´æ¨ç†â€çš„æ–°èŒƒå¼ï¼Œé€šè¿‡åŸºæœ¬ç»˜å›¾æ“ä½œæ¥å¢å¼ºæ¨¡å‹çš„è§†è§‰æ“æ§èƒ½åŠ›ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶æ¥åŸ¹å…»æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>VILASRæ¨¡å‹åœ¨å„ç§ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-412c95e227cf94f8864b28ffaa57d5e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f096d0b7fd10edd3976099a74ae8172f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51dad34af7e3694e2389b328032cf602.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SafeGenBench-A-Benchmark-Framework-for-Security-Vulnerability-Detection-in-LLM-Generated-Code"><a href="#SafeGenBench-A-Benchmark-Framework-for-Security-Vulnerability-Detection-in-LLM-Generated-Code" class="headerlink" title="SafeGenBench: A Benchmark Framework for Security Vulnerability Detection   in LLM-Generated Code"></a>SafeGenBench: A Benchmark Framework for Security Vulnerability Detection   in LLM-Generated Code</h2><p><strong>Authors:Xinghang Li, Jingzhe Ding, Chao Peng, Bing Zhao, Xiang Gao, Hongwan Gao, Xinchen Gu</strong></p>
<p>The code generation capabilities of large language models(LLMs) have emerged as a critical dimension in evaluating their overall performance. However, prior research has largely overlooked the security risks inherent in the generated code. In this work, we introduce SafeGenBench, a benchmark specifically designed to assess the security of LLM-generated code. The dataset encompasses a wide range of common software development scenarios and vulnerability types. Building upon this benchmark, we develop an automatic evaluation framework that leverages both static application security testing(SAST) and LLM-based judging to assess the presence of security vulnerabilities in model-generated code. Through the empirical evaluation of state-of-the-art LLMs on SafeGenBench, we reveal notable deficiencies in their ability to produce vulnerability-free code. Our findings highlight pressing challenges and offer actionable insights for future advancements in the secure code generation performance of LLMs. The data and code will be released soon. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç ç”Ÿæˆèƒ½åŠ›å·²æˆä¸ºè¯„ä¼°å…¶æ•´ä½“æ€§èƒ½çš„å…³é”®ç»´åº¦ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†ç”Ÿæˆä»£ç ä¸­çš„å®‰å…¨é£é™©ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SafeGenBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°LLMç”Ÿæˆä»£ç å®‰å…¨æ€§çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å„ç§å¸¸è§çš„è½¯ä»¶å¼€å‘åœºæ™¯å’Œæ¼æ´ç±»å‹ã€‚åŸºäºè¿™ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨é™æ€åº”ç”¨ç¨‹åºå®‰å…¨æµ‹è¯•ï¼ˆSASTï¼‰å’ŒåŸºäºLLMçš„åˆ¤æ–­æ¥è¯„ä¼°æ¨¡å‹ç”Ÿæˆä»£ç ä¸­å­˜åœ¨çš„å®‰å…¨æ¼æ´ã€‚é€šè¿‡å¯¹æœ€æ–°LLMåœ¨SafeGenBenchä¸Šçš„å®è¯è¯„ä¼°ï¼Œæˆ‘ä»¬æ­ç¤ºäº†å®ƒä»¬åœ¨ç”Ÿæˆæ— æ¼æ´ä»£ç æ–¹é¢çš„æ˜¾è‘—ç¼ºé™·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†ç´§è¿«çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥æé«˜LLMçš„å®‰å…¨ä»£ç ç”Ÿæˆæ€§èƒ½æä¾›äº†å¯è¡Œçš„è§è§£ã€‚æ•°æ®å’Œæ–¹æ³•å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05692v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç ç”Ÿæˆèƒ½åŠ›å·²æˆä¸ºè¯„ä¼°å…¶æ•´ä½“æ€§èƒ½çš„å…³é”®ç»´åº¦ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶å¤§å¤šå¿½ç•¥äº†ç”Ÿæˆä»£ç ä¸­çš„å®‰å…¨é£é™©ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SafeGenBenchï¼Œä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°LLMç”Ÿæˆä»£ç å®‰å…¨æ€§çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å„ç§å¸¸è§çš„è½¯ä»¶å¼€å‘åœºæ™¯å’Œæ¼æ´ç±»å‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨é™æ€åº”ç”¨ç¨‹åºå®‰å…¨æµ‹è¯•ï¼ˆSASTï¼‰å’ŒLLMåˆ¤æ–­æ¥è¯„ä¼°æ¨¡å‹ç”Ÿæˆä»£ç ä¸­æ˜¯å¦å­˜åœ¨å®‰å…¨éšæ‚£ã€‚é€šè¿‡å¯¹æœ€æ–°LLMåœ¨SafeGenBenchä¸Šçš„å®è¯è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å®ƒä»¬åœ¨ç”Ÿæˆæ— æ¼æ´ä»£ç æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†æœªæ¥æå‡LLMå®‰å…¨ä»£ç ç”Ÿæˆæ€§èƒ½çš„ç´§è¿«æŒ‘æˆ˜ï¼Œå¹¶æä¾›äº†å¯æ“ä½œçš„è§è§£ã€‚æ•°æ®å’Œæ–¹æ³•é›†å³å°†å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç ç”Ÿæˆèƒ½åŠ›å·²æˆä¸ºè¯„ä¼°å…¶æ€§èƒ½çš„é‡è¦æ–¹é¢ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¿½è§†äº†LLMç”Ÿæˆä»£ç ä¸­çš„å®‰å…¨é£é™©ã€‚</li>
<li>SafeGenBenchæ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LLMç”Ÿæˆä»£ç å®‰å…¨æ€§çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>è¯¥æ•°æ®é›†æ¶µç›–äº†å¤šç§è½¯ä»¶å¼€å‘åœºæ™¯å’Œæ¼æ´ç±»å‹ã€‚</li>
<li>æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆSASTå’ŒLLMåˆ¤æ–­æ¥è¯„ä¼°æ¨¡å‹ç”Ÿæˆä»£ç çš„å®‰å…¨æ€§ã€‚</li>
<li>å®è¯è¯„ä¼°å‘ç°LLMåœ¨ç”Ÿæˆæ— æ¼æ´ä»£ç æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05692">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e027d90c8513d8d228e4a2bf32748727.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d032adc2870e4bb936a697adb24471a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad29ba42f3fcdb2d5186345e875b8d18.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-379c4fea709546f50ecda95569e5845c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-232ce13ae0d1bc17ef022d679b9be68c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Rewarding-the-Unlikely-Lifting-GRPO-Beyond-Distribution-Sharpening"><a href="#Rewarding-the-Unlikely-Lifting-GRPO-Beyond-Distribution-Sharpening" class="headerlink" title="Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening"></a>Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening</h2><p><strong>Authors:Andre He, Daniel Fried, Sean Welleck</strong></p>
<p>Reinforcement learning is emerging as a primary driver for improving language model reasoning capabilities. A fundamental question is whether current reinforcement learning algorithms â€“ such as Group Relative Policy Optimization (GRPO), the de facto standard algorithm used to improve language model reasoning â€“ merely sharpen the base modelâ€™s distribution around problems it can already solve. We investigate this question in the context of formal theorem proving, which has access to a perfect verifier. We identify a degenerate rank bias in GRPO in which highly probable trajectories are reinforced and rare ones are neglected. This results in distribution sharpening: the model can solve some problems with fewer samples, but underperforms simply sampling more solutions from the original model. To overcome GRPOâ€™s rank bias we introduce unlikeliness reward, a simple method for explicitly up-weighting rare but correct solutions. We show that unlikeliness reward mitigates rank bias and improves pass@$N$ across a large range of $N$ in both synthetic and real theorem proving settings. We also uncover an unexpected link between rank bias and a seemingly mundane hyperparameter â€“ the number of updates per batch â€“ that leads to a second, complementary mitigation. We combine our insights into a revised GRPO training recipe for formal theorem proving, yielding an open pipeline that achieves competitive performance to DeepSeek-Prover-V1.5-RL on the miniF2F-test benchmark. We release our implementation at <a target="_blank" rel="noopener" href="https://github.com/AndreHe02/rewarding-unlikely-release">https://github.com/AndreHe02/rewarding-unlikely-release</a> </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ æ­£åœ¨æˆä¸ºæé«˜è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ä¸»è¦é©±åŠ¨åŠ›ã€‚ä¸€ä¸ªåŸºæœ¬çš„é—®é¢˜æ˜¯ï¼Œå½“å‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¦‚ç”¨äºæé«˜è¯­è¨€æ¨¡å‹æ¨ç†çš„é»˜è®¤æ ‡å‡†ç®—æ³•â€”â€”é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæ˜¯å¦åªæ˜¯ä½¿åŸºç¡€æ¨¡å‹åœ¨èƒ½å¤Ÿè§£å†³çš„é—®é¢˜å‘¨å›´åˆ†å¸ƒæ›´åŠ é›†ä¸­ã€‚æˆ‘ä»¬åœ¨å½¢å¼åŒ–å®šç†è¯æ˜çš„èƒŒæ™¯ä¸‹ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œå½¢å¼åŒ–å®šç†è¯æ˜å¯ä»¥ä½¿ç”¨å®Œç¾çš„éªŒè¯å™¨ã€‚æˆ‘ä»¬å‘ç°äº†GRPOä¸­çš„é€€åŒ–ç­‰çº§åè§ï¼Œå…¶ä¸­é«˜åº¦å¯èƒ½çš„è½¨è¿¹å¾—åˆ°åŠ å¼ºï¼Œè€Œç½•è§çš„è½¨è¿¹è¢«å¿½è§†ã€‚è¿™å¯¼è‡´åˆ†å¸ƒé›†ä¸­åŒ–ï¼šæ¨¡å‹å¯ä»¥åœ¨è¾ƒå°‘çš„æ ·æœ¬ä¸‹è§£å†³ä¸€äº›é—®é¢˜ï¼Œä½†åœ¨åŸå§‹æ¨¡å‹ä¸­ç®€å•åœ°é€šè¿‡é‡‡æ ·æ›´å¤šè§£å†³æ–¹æ¡ˆè¡¨ç°å¾—è¾ƒå·®ã€‚ä¸ºäº†å…‹æœGRPOçš„ç­‰çº§åè§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸åƒå¥–åŠ±çš„æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ˜ç¡®æé«˜ç½•è§ä½†æ­£ç¡®è§£å†³æ–¹æ¡ˆæƒé‡çš„ç®€å•æ–¹æ³•ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä¸åƒå¥–åŠ±å¯ä»¥å‡è½»ç­‰çº§åè§ï¼Œå¹¶åœ¨åˆæˆå’ŒçœŸå®å®šç†è¯æ˜ç¯å¢ƒçš„å¹¿æ³›èŒƒå›´å†…æé«˜äº†pass@Nçš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å‘ç°äº†ç­‰çº§åè§å’Œä¸€ä¸ªçœ‹ä¼¼æ™®é€šçš„è¶…å‚æ•°ä¹‹é—´çš„æ„å¤–è”ç³»â€”â€”æ¯æ‰¹æ›´æ–°æ¬¡æ•°ï¼Œè¿™å¯¼è‡´äº†ç¬¬äºŒç§è¡¥å……æ€§çš„ç¼“è§£æ–¹æ³•ã€‚æˆ‘ä»¬å°†è¿™äº›è§è§£ç»“åˆåˆ°å½¢å¼åŒ–å®šç†è¯æ˜çš„ä¿®è®¢GRPOè®­ç»ƒé…æ–¹ä¸­ï¼Œå½¢æˆäº†ä¸€ä¸ªå¼€æ”¾çš„ç®¡é“ï¼Œè¯¥ç®¡é“åœ¨miniF2Fæµ‹è¯•åŸºå‡†ä¸Šå®ç°äº†ä¸DeepSeek-Prover-V1.5-RLçš„ç«äº‰æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®ç°å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/AndreHe02/rewarding-unlikely-release%E4%B8%8A%E3%80%82">https://github.com/AndreHe02/rewarding-unlikely-releaseä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02355v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ æ­£åœ¨æˆä¸ºæå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ä¸»è¦é©±åŠ¨åŠ›ã€‚ç ”ç©¶å…³æ³¨GRPOç®—æ³•åœ¨å¤„ç†å½¢å¼åŒ–å®šç†è¯æ˜æ—¶å­˜åœ¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¥–åŠ±æ–¹æ³•ä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½¿å¾—ç¨€æœ‰ä¸”æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆå¾—ä»¥å¼ºåŒ–ã€‚è¿™ç§å¥–åŠ±æ–¹æ³•æœ‰æ•ˆç¼“è§£äº†æ’ååè§é—®é¢˜ï¼Œå¹¶æé«˜äº†æ¨¡å‹åœ¨åˆæˆå’ŒçœŸå®å®šç†è¯æ˜ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚ç ”ç©¶è¿˜æ­ç¤ºäº†ä¸€ä¸ªä¸çœ‹ä¼¼æ™®é€šçš„è¶…å‚æ•°æœ‰å…³çš„æ„å¤–è”ç³»ï¼Œæä¾›äº†ç¬¬äºŒç§è§£å†³é€”å¾„ã€‚å¯¹å®šç†è¯æ˜æ¨¡å‹æ€§èƒ½çš„ä¼˜åŒ–æˆæœä¸DeepSeek-Prover-V1.5-RLç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„å®ç°å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ æ˜¯æ¨åŠ¨è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ä¸»è¦æŠ€æœ¯é©±åŠ¨åŠ›ã€‚</li>
<li>åœ¨å½¢å¼åŒ–å®šç†è¯æ˜ç¯å¢ƒä¸­è¯„ä¼°äº†Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•çš„è¡¨ç°ã€‚</li>
<li>ç ”ç©¶å‘ç°äº†GRPOç®—æ³•çš„æ’ååè§é—®é¢˜ï¼Œå¼ºè°ƒå¯¹é«˜åº¦å¯èƒ½çš„è½¨è¿¹çš„å¼ºåŒ–è€Œå¿½è§†ç¨€æœ‰è·¯å¾„ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºâ€œunlikeliness rewardâ€çš„æ–¹æ³•æ¥è§£å†³æ’ååè§é—®é¢˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜ç¡®åœ°ä¸Šè°ƒç¨€æœ‰ä½†æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆçš„æƒé‡ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†ä¸€ä¸ªä¸è¶…å‚æ•°æœ‰å…³çš„æ„å¤–è”ç³»ï¼Œè¿™æœ‰åŠ©äºæ›´æ·±å…¥åœ°ç†è§£ç®—æ³•æ€§èƒ½çš„å½±å“å› ç´ ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf2545cd96f63ca42203ee8a26f8f094.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d1800fdd1f96b4c19c371ff64377132.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-014b685cb8fe3ff0b73a9f5495efd04e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a088e8b7cd5b2e1f01327ab0fe45e8bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2edd19bcde5df037d6991878a5eb29e1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c3545f79463c4933987b03be47936731.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-24  Confidence Scoring for LLM-Generated SQL in Supply Chain Data Extraction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-23/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4c1c73982632542b13bac20264fd9e87.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-23  A Hybrid ConvNeXt-EfficientNet AI Solution for Precise Falcon Disease   Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25219.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
