<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-06-24  Simultaneous Translation with Offline Speech and LLM Models in CUNI   Submission to IWSLT 2025">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-22d7dc073feae479256cafe7efc0b72b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    24 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-24-更新"><a href="#2025-06-24-更新" class="headerlink" title="2025-06-24 更新"></a>2025-06-24 更新</h1><h2 id="Simultaneous-Translation-with-Offline-Speech-and-LLM-Models-in-CUNI-Submission-to-IWSLT-2025"><a href="#Simultaneous-Translation-with-Offline-Speech-and-LLM-Models-in-CUNI-Submission-to-IWSLT-2025" class="headerlink" title="Simultaneous Translation with Offline Speech and LLM Models in CUNI   Submission to IWSLT 2025"></a>Simultaneous Translation with Offline Speech and LLM Models in CUNI   Submission to IWSLT 2025</h2><p><strong>Authors:Dominik Macháček, Peter Polák</strong></p>
<p>This paper describes Charles University submission to the Simultaneous Speech Translation Task of the IWSLT 2025. We cover all four language pairs with a direct or cascade approach. The backbone of our systems is the offline Whisper speech model, which we use for both translation and transcription in simultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We further improve the performance by prompting to inject in-domain terminology, and we accommodate context. Our cascaded systems further use EuroLLM for unbounded simultaneous translation. Compared to the Organizers’ baseline, our systems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on English to German, Chinese and Japanese on the development sets. Additionally, we also propose a new enhanced measure of speech recognition latency. </p>
<blockquote>
<p>本文描述了查尔斯大学对IWSLT 2025同步语音识别任务的提交内容。我们采用直接或级联的方法涵盖了所有四种语言对。我们系统的核心是离线whisper语音模型，我们将其用于最新的同步策略AlignAtt的同步模式和转录。通过提示注入领域专业术语和适应语境，我们进一步提高了性能。我们的级联系统还进一步使用EuroLLM进行无界限的同步翻译。与组织者的基线相比，我们的系统在开发集上捷克语到英语的BLEU得分提高了2分，英语到德语、中文和日语的BLEU得分提高了13到22分。此外，我们还提出了一种新的改进的语音识别延迟度量方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17077v1">PDF</a> IWSLT 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Charles大学在IWSLT 2025同步语音识别任务中的提交内容。文章涵盖了四种语言对的直接或级联方法。系统以离线Whisper语音模型为核心，采用最新同步策略AlignAtt进行翻译和转录。通过提示注入领域专业术语和适应语境，进一步提高了性能。级联系统还使用EuroLLM进行无界同步翻译。相较于组织者设定的基线，该系统在捷克语到英语的翻译上提高了2个BLEU点，在发展到英语的中文和日语翻译上提高了13到22个BLEU点。此外，还提出了一种新的语音识别的延迟度量方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Charles大学参与了IWSLT 2025的同步语音识别任务提交，涵盖四种语言对的翻译。</li>
<li>系统基于离线Whisper语音模型和最新的同步策略AlignAtt。</li>
<li>通过提示注入领域专业术语和适应语境来提高性能。</li>
<li>级联系统使用EuroLLM进行无界同步翻译。</li>
<li>与基线相比，系统在多种语言对的翻译任务上有显著的提升效果。</li>
<li>提高的量化指标具体为BLEU分数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17077">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-799366d1360e8d86ed3a73912a3a3ffc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfd32ed6ef97092d778005d88b869361.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c68dbede2536384458138ecc052758c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e964ed6a0684aaa0d13fa63be341cd69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0505ee424bfaa03e228eadc2efc68857.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="State-Space-Models-in-Efficient-Whispered-and-Multi-dialect-Speech-Recognition"><a href="#State-Space-Models-in-Efficient-Whispered-and-Multi-dialect-Speech-Recognition" class="headerlink" title="State-Space Models in Efficient Whispered and Multi-dialect Speech   Recognition"></a>State-Space Models in Efficient Whispered and Multi-dialect Speech   Recognition</h2><p><strong>Authors:Aref Farhadipour, Homayoon Beigi, Volker Dellwo, Hadi Veisi</strong></p>
<p>Whispered speech recognition presents significant challenges for conventional automatic speech recognition systems, particularly when combined with dialect variation. However, utilizing an efficient method to solve this problem using a low-range dataset and processing load is beneficial. This paper proposes a solution using a Mamba-based state-space model and four fine-tuned self-supervised models consisting of Wav2Vec2, WavLM, HuBERT, and Whisper to address the dual challenges of whispered speech and dialect diversity. Based on our knowledge, this represents the best performance reported on the wTIMIT and CHAINS datasets for whispered speech recognition. We trained the models using whispered and normal speech data across Singaporean, US, and Irish dialects. The findings demonstrated that utilizing the proposed Mamba-based model could work as a highly efficient model trained with low amounts of whispered data to simultaneously work on whispered and normal speech recognition. The code for this work is freely available. </p>
<blockquote>
<p>耳语语音识别对传统自动语音识别系统提出了重大挑战，特别是在与方言变化相结合时。然而，利用有效的方法解决此问题，使用低范围数据集和处理负载是非常有益的。本文提出了一种基于Mamba的状态空间模型和四个经过精细调整的包括Wav2Vec2、WavLM、HuBERT和whisper的自我监督模型，以应对耳语语音和方言多样性的双重挑战。基于我们目前的知识，这是wTIMIT和CHAINS数据集中报告的耳语语音识别性能最佳的一次。我们利用新加坡、美国和爱尔兰方言的耳语和正常语音数据进行了模型训练。实验结果表明，使用基于Mamba的模型可以作为高度有效的模型，用少量的耳语数据进行训练，可同时用于耳语和正常语音识别。该工作的代码可免费获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16969v1">PDF</a> paper is in 4+1 pages</p>
<p><strong>摘要</strong></p>
<p>本文提出一种基于Mamba的状态空间模型，结合四个精细调整的自监督模型（Wav2Vec2、WavLM、HuBERT和Whisper），解决低声说话和方言多样性双重挑战的问题。通过在新加坡方言、美国方言和爱尔兰方言的轻声和正常语音数据上训练模型，该模型展现出优秀的性能，是在wTIMIT和CHAINS数据集上轻声语音识别任务的最新最佳表现。研究结果表明，使用基于Mamba的模型能够以高度有效的方式，用少量的轻声数据训练，同时处理轻声和正常语音识别。该工作的代码已公开发布。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>低声语音识别对传统的自动语音识别系统存在挑战，特别是与方言变化相结合时。</li>
<li>提出一种基于Mamba的状态空间模型来解决这个问题，该模型能够处理低声语音和方言多样性。</li>
<li>在wTIMIT和CHAINS数据集上，该模型展现出最佳性能，为轻声语音识别任务的最新最佳表现。</li>
<li>模型在多种方言（包括新加坡方言、美国方言和爱尔兰方言）的轻声和正常语音数据上进行训练。</li>
<li>研究表明，使用基于Mamba的模型可以用低量的轻声数据进行训练，并同时处理轻声和正常语音识别。</li>
<li>该模型的代码已经公开发布，便于其他研究者使用和改进。</li>
<li>此方法对于解决低声语音识别问题具有实际应用的潜力，特别是在需要处理不同方言和挑战环境的应用中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16969">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e87aeb19cd8dee083496f9e3cc42109f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6825ce753a64743372f192d89b96a29a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1786c14193d2daf1dc3c905431c89501.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39d8aab1a26b701d5c05883a9b23f6d4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Optimizing-Multilingual-Text-To-Speech-with-Accents-Emotions"><a href="#Optimizing-Multilingual-Text-To-Speech-with-Accents-Emotions" class="headerlink" title="Optimizing Multilingual Text-To-Speech with Accents &amp; Emotions"></a>Optimizing Multilingual Text-To-Speech with Accents &amp; Emotions</h2><p><strong>Authors:Pranav Pawar, Akshansh Dwivedi, Jenish Boricha, Himanshu Gohil, Aditya Dubey</strong></p>
<p>State-of-the-art text-to-speech (TTS) systems realize high naturalness in monolingual environments, synthesizing speech with correct multilingual accents (especially for Indic languages) and context-relevant emotions still poses difficulty owing to cultural nuance discrepancies in current frameworks. This paper introduces a new TTS architecture integrating accent along with preserving transliteration with multi-scale emotion modelling, in particularly tuned for Hindi and Indian English accent. Our approach extends the Parler-TTS model by integrating A language-specific phoneme alignment hybrid encoder-decoder architecture, and culture-sensitive emotion embedding layers trained on native speaker corpora, as well as incorporating a dynamic accent code switching with residual vector quantization. Quantitative tests demonstrate 23.7% improvement in accent accuracy (Word Error Rate reduction from 15.4% to 11.8%) and 85.3% emotion recognition accuracy from native listeners, surpassing METTS and VECL-TTS baselines. The novelty of the system is that it can mix code in real time - generating statements such as “Namaste, let’s talk about <Hindi phrase>“ with uninterrupted accent shifts while preserving emotional consistency. Subjective evaluation with 200 users reported a mean opinion score (MOS) of 4.2&#x2F;5 for cultural correctness, much better than existing multilingual systems (p&lt;0.01). This research makes cross-lingual synthesis more feasible by showcasing scalable accent-emotion disentanglement, with direct application in South Asian EdTech and accessibility software. </p>
<blockquote>
<p>当前先进的文本转语音（TTS）系统在单语环境中实现了高度自然性，能够合成带有正确多语言口音（尤其是印度语言）的语音。然而，由于当前框架中的文化细微差别，在合成与语境相关的情绪时仍面临困难。本文介绍了一种新的TTS架构，它集成了口音并保留了多尺度情感建模中的音译。该架构特别针对印度文和印度英语口音进行了调整。我们的方法通过整合语言特定的音素对齐混合编码器-解码器架构、在本地语音语料库上训练的文化敏感情感嵌入层、以及具有残差向量量化的动态口音代码切换，来扩展Parler-TTS模型。定量测试显示，口音准确度提高了23.7%（单词错误率从15.4%降至11.8%），本地听众的情绪识别准确率为85.3%，超过了METTS和VECL-TTS基线。该系统的新颖之处在于它能够实时混合代码——生成语句，例如“你好，我们来谈谈&lt;印度短语&gt;”，口音不间断地转换，同时保持情感一致性。对200名用户的主观评估报告称，文化正确性的平均意见得分（MOS）为4.2&#x2F;5，远高于现有的多语言系统（p&lt;0.01）。该研究通过展示可扩展的口音情感分离，使跨语言合成更加可行，可直接应用于南亚教育技术软件和辅助软件。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16310v1">PDF</a> 12 pages, 8 figures</p>
<p><strong>摘要</strong></p>
<p>文本到语音（TTS）系统在现代单语环境中已经能够实现高度自然化。尽管系统能够合成带有正确多语种口音（尤其是印度语言）的语音，但在结合语境相关的情绪表达方面仍存在挑战，因为当前框架中的文化细微差别。本文介绍了一种新的TTS架构，该架构整合口音并保留多尺度情感建模的翻译，特别适合印度英语口音。我们的方法通过整合语言特定的音素对齐混合编码器-解码器架构，训练本地说话者语料库的情感嵌入层以感知文化敏感性，并结合动态口音代码切换与剩余向量量化技术，改进了口音准确性和情感识别能力。定量测试显示，口音准确性提高了23.7%（单词错误率从15.4%降至11.8%），情感识别准确率达到了本地听众的85.3%，超过了METTS和VECL-TTS基线系统。该系统的独特之处在于可以实时混合代码——生成语句如“你好，让我们谈谈&lt;印度语句&gt;”，口音转变流畅而情感保持一致。主观评价结果显示，与现有多语种系统相比，文化正确性平均意见得分（MOS）为4.2&#x2F;5（p&lt;0.01），这一研究通过展示可扩展的口音情感分离使跨语言合成更加可行，可直接应用于南亚教育技术软件和辅助软件中。</p>
<p><strong>要点提炼</strong></p>
<ol>
<li>当前TTS系统在合成带有语境相关情绪的语音时面临困难，尤其是在处理多语种口音和文化细微差别方面。</li>
<li>本文介绍了一种新的TTS架构，融合了口音和情绪建模，特别针对印度英语口音进行了优化。</li>
<li>通过整合语言特定的音素对齐混合编码器-解码器架构、文化敏感的情感嵌入层以及动态口音代码切换技术，提高了口音准确性和情感识别能力。</li>
<li>定量测试表明，该系统在口音和情感识别方面表现出色，超越了现有基线系统。</li>
<li>系统能够实时生成带有流畅口音转变的语句，并保持情感一致性。</li>
<li>主观评价显示，该系统的文化正确性得分较高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16310">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ddf4b49f7f5a5ebacda0ea09c54c2264.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dd93086121461feadd62fd97d521cef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ee744a958b32293531a11658bfc12aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4946f20f38f01e39808aed5eb2b24379.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Double-Entendre-Robust-Audio-Based-AI-Generated-Lyrics-Detection-via-Multi-View-Fusion"><a href="#Double-Entendre-Robust-Audio-Based-AI-Generated-Lyrics-Detection-via-Multi-View-Fusion" class="headerlink" title="Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via   Multi-View Fusion"></a>Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via   Multi-View Fusion</h2><p><strong>Authors:Markus Frohmann, Gabriel Meseguer-Brocal, Markus Schedl, Elena V. Epure</strong></p>
<p>The rapid advancement of AI-based music generation tools is revolutionizing the music industry but also posing challenges to artists, copyright holders, and providers alike. This necessitates reliable methods for detecting such AI-generated content. However, existing detectors, relying on either audio or lyrics, face key practical limitations: audio-based detectors fail to generalize to new or unseen generators and are vulnerable to audio perturbations; lyrics-based methods require cleanly formatted and accurate lyrics, unavailable in practice. To overcome these limitations, we propose a novel, practically grounded approach: a multimodal, modular late-fusion pipeline that combines automatically transcribed sung lyrics and speech features capturing lyrics-related information within the audio. By relying on lyrical aspects directly from audio, our method enhances robustness, mitigates susceptibility to low-level artifacts, and enables practical applicability. Experiments show that our method, DE-detect, outperforms existing lyrics-based detectors while also being more robust to audio perturbations. Thus, it offers an effective, robust solution for detecting AI-generated music in real-world scenarios. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/deezer/robust-AI-lyrics-detection">https://github.com/deezer/robust-AI-lyrics-detection</a>. </p>
<blockquote>
<p>基于人工智能的音乐生成工具的快速发展正在革命性地改变音乐行业，同时也给艺术家、版权所有者和提供商带来了挑战。这迫切需要可靠的方法来检测这种AI生成的内容。然而，现有的检测器，无论是基于音频还是歌词的，都面临关键的实际局限性：基于音频的检测器无法推广到新的或未见过的生成器，并容易受到音频干扰的影响；基于歌词的方法需要整洁格式化和准确的歌词，这在实践中是无法获得的。为了克服这些局限性，我们提出了一种新颖且实用的方法：多模态、模块化延迟融合管道，它结合了自动转录的歌唱歌词和捕捉音频中歌词相关信息的语音特征。通过直接从音频中依赖歌词方面，我们的方法提高了稳健性，减轻了对低级伪影的易感性，并实现了实际应用的可行性。实验表明，我们的方法DE-detect优于现有的基于歌词的检测器，同时对音频干扰具有更强的鲁棒性。因此，它为检测现实场景中的AI生成音乐提供了有效且稳健的解决方案。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/deezer/robust-AI-lyrics-detection%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/deezer/robust-AI-lyrics-detection上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15981v1">PDF</a> Accepted to ACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>AI音乐生成工具的快速发展正在革命性地改变音乐行业，但同时也给艺术家、版权持有者和提供商带来了挑战。为了检测这种AI生成的内容，需要可靠的方法。然而，现有的检测器存在实用上的局限性。为了克服这些局限性，提出了一种新的实用方法：多模态、模块化后期融合管道，它结合了自动转录的歌词和捕捉音频中歌词信息的语音特征。该方法直接依赖音频中的歌词方面，增强了稳健性，减轻了低级伪影的影响，并实现了实际应用。实验表明，该方法在检测AI生成的音乐方面表现出优于现有歌词检测器的性能，同时对音频扰动具有更强的稳健性。因此，它为真实场景中的AI生成音乐检测提供了有效且稳健的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI音乐生成工具的快速发展对音乐行业产生了重大影响，同时也带来了对艺术家、版权持有者和提供商的挑战。</li>
<li>现有的音乐检测器存在实用上的局限性，如音频检测器无法推广到新的生成器并易受音频扰动影响，而歌词检测器则需要整洁格式的准确歌词。</li>
<li>为了解决这些问题，提出了一种新的多模态、模块化后期融合检测方法，结合了自动转录的歌词和音频中的语音特征。</li>
<li>该方法直接从音频中提取歌词信息，增强了稳健性，并减轻了低级伪影的影响。</li>
<li>实验表明，该方法在检测AI生成的音乐方面优于现有的歌词检测器，并且对音频扰动具有更强的稳健性。</li>
<li>该方法提供了一个有效且稳健的解决方案，可用于真实场景中检测AI生成的音乐。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15981">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-36b59ac8c412bb38bb6e2c873c3dc1f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9808712ce0187a711cf09d8ab596bfe8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1139b943f1a203ea6ed1eed1606b96f2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c9486cb32440fa60084adfdaa2b4e587.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22d7dc073feae479256cafe7efc0b72b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Explainable-speech-emotion-recognition-through-attentive-pooling-insights-from-attention-based-temporal-localization"><a href="#Explainable-speech-emotion-recognition-through-attentive-pooling-insights-from-attention-based-temporal-localization" class="headerlink" title="Explainable speech emotion recognition through attentive pooling:   insights from attention-based temporal localization"></a>Explainable speech emotion recognition through attentive pooling:   insights from attention-based temporal localization</h2><p><strong>Authors:Tahitoa Leygue, Astrid Sabourin, Christian Bolzmacher, Sylvain Bouchigny, Margarita Anastassova, Quoc-Cuong Pham</strong></p>
<p>State-of-the-art transformer models for Speech Emotion Recognition (SER) rely on temporal feature aggregation, yet advanced pooling methods remain underexplored. We systematically benchmark pooling strategies, including Multi-Query Multi-Head Attentive Statistics Pooling, which achieves a 3.5 percentage point macro F1 gain over average pooling. Attention analysis shows 15 percent of frames capture 80 percent of emotion cues, revealing a localized pattern of emotional information. Analysis of high-attention frames reveals that non-linguistic vocalizations and hyperarticulated phonemes are disproportionately prioritized during pooling, mirroring human perceptual strategies. Our findings position attentive pooling as both a performant SER mechanism and a biologically plausible tool for explainable emotion localization. On Interspeech 2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, our approach obtained a macro F1 score of 0.3649. </p>
<blockquote>
<p>最先进的用于语音情感识别（SER）的变压器模型依赖于时间特征聚合，但先进的池化方法仍然被研究得不够深入。我们系统地评估了池化策略，包括多查询多头注意力统计池化，该方法相对于平均池化实现了3.5个百分点的宏观F1增益。注意力分析显示，15%的帧捕获了80%的情感线索，揭示了情感信息的局部化模式。高注意力帧的分析表明，在池化过程中，非语言性发声和夸张的语音音素被优先处理的比例过高，这反映了人类的感知策略。我们的研究结果表明，注意力池化是一种有效的SER机制和一种用于解释情感定位的生物学上合理的工具。在Interspeech 2025自然条件下语音情感识别挑战赛上，我们的方法获得了宏观F1分数为0.3649。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15754v1">PDF</a> </p>
<p><strong>总结</strong></p>
<p>最新前沿的转换器模型在语音情感识别（SER）中依赖于时序特征聚合，但先进的池化方法仍被忽略。系统性地对池化策略进行了评估，包括多查询多头条约统计池化，该方法相较于平均池化在宏观F1得分上提高了3.5个百分点。注意力分析显示，仅有15%的帧捕捉到了80%的情感线索，揭示了情感信息的局部化模式。高关注度框架的分析表明，非语言性发声和夸张发音的语音在池化过程中被优先处理，这与人类感知策略相呼应。我们的研究结果表明，注意力池化是一种有效的SER机制和情感定位的可解释工具。在Interspeech 2025自然条件下语音情感识别挑战中，我们的方法取得了宏观F1得分0.3649。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>先进的转换器模型在语音情感识别中依赖时序特征聚合。</li>
<li>多查询多头条约统计池化方法能有效提高性能，相对平均池化有3.5个百分点的宏观F1得分提升。</li>
<li>注意力分析显示情感线索集中在少量帧内，大部分信息由一小部分帧（15%）捕获。</li>
<li>高注意力框架分析揭示了非语言性发声和夸张发音的语音在情感识别中的重要性。</li>
<li>注意力池化方法不仅能提高SER性能，还可用于解释情感定位。</li>
<li>研究结果与人类的感知策略相呼应，表明模型在处理情感信息时具有生物合理性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15754">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-60848679665ed8f8ccff4d17882350dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88a86f56feb1ea0b612f6aa121b75de2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bac111559c2a0cf70bf7c5200bf072ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40cc6c39ede92b4439b75678cdc84d20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bafbae845bf74dba8edc7bf987c5c11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ef8cd6ec000d8e8ac8b298d731d3e1f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Microphone-Array-Geometry-Independent-Multi-Talker-Distant-ASR-NTT-System-for-the-DASR-Task-of-the-CHiME-8-Challenge"><a href="#Microphone-Array-Geometry-Independent-Multi-Talker-Distant-ASR-NTT-System-for-the-DASR-Task-of-the-CHiME-8-Challenge" class="headerlink" title="Microphone Array Geometry Independent Multi-Talker Distant ASR: NTT   System for the DASR Task of the CHiME-8 Challenge"></a>Microphone Array Geometry Independent Multi-Talker Distant ASR: NTT   System for the DASR Task of the CHiME-8 Challenge</h2><p><strong>Authors:Naoyuki Kamo, Naohiro Tawara, Atsushi Ando, Takatomo Kano, Hiroshi Sato, Rintaro Ikeshita, Takafumi Moriya, Shota Horiguchi, Kohei Matsuura, Atsunori Ogawa, Alexis Plaquet, Takanori Ashihara, Tsubasa Ochiai, Masato Mimura, Marc Delcroix, Tomohiro Nakatani, Taichi Asami, Shoko Araki</strong></p>
<p>In this paper, we introduce a multi-talker distant automatic speech recognition (DASR) system we designed for the DASR task 1 of the CHiME-8 challenge. Our system performs speaker counting, diarization, and ASR. It handles various recording conditions, from diner parties to professional meetings and from two to eight speakers. We perform diarization first, followed by speech enhancement, and then ASR as the challenge baseline. However, we introduced several key refinements. First, we derived a powerful speaker diarization relying on end-to-end speaker diarization with vector clustering (EEND-VC), multi-channel speaker counting using enhanced embeddings from EEND-VC, and target-speaker voice activity detection (TS-VAD). For speech enhancement, we introduced a novel microphone selection rule to better select the most relevant microphones among the distributed microphones and investigated improvements to beamforming. Finally, for ASR, we developed several models exploiting Whisper and WavLM speech foundation models. We present the results we submitted to the challenge and updated results we obtained afterward. Our strongest system achieves a 63% relative macro tcpWER improvement over the baseline and outperforms the challenge best results on the NOTSOFAR-1 meeting evaluation data among geometry-independent systems. </p>
<blockquote>
<p>在这篇论文中，我们介绍了一种为CHiME-8挑战赛的DASR任务1设计的多说话者远距离自动语音识别（DASR）系统。我们的系统执行说话人计数、发音人识别及ASR。它能够处理各种录音环境，从晚餐聚会到专业会议，以及从两名到八名发言人的情况。我们首先进行发音人识别，接着是语音增强，最后是ASR作为基线挑战。然而，我们介绍了一些关键改进。首先，我们采用了一种强大的发音人识别方法，依赖于基于端到端技术的发音人识别与向量聚类（EEND-VC）、利用EEND-VC增强的嵌入物的多通道说话人数统计以及目标说话人的语音活动检测（TS-VAD）。对于语音增强部分，我们引入了一种新颖的麦克风选择规则，以更好地选择分布式麦克风中最相关的麦克风，并对波束形成进行了改进。最后，对于ASR部分，我们开发了一些利用Whisper和WavLM语音基础模型的模型。我们展示了提交给挑战的结果以及之后获得的更新结果。我们最强大的系统相对于基线实现了63%的相对宏观tcpWER改进，并在几何独立系统中超越了挑战赛在NOTSOFAR-1会议评估数据上的最佳结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09859v2">PDF</a> 55 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种针对CHiME-8挑战DASR任务1的多说话者远程自动语音识别（DASR）系统。该系统执行说话人计数、语音识别和ASR任务，能适应从餐厅聚会到专业会议的不同录音环境，支持从两名到八名说话者。通过一系列关键技术改进，该系统在EEND-VC的基础上实现端对端说话者摘要，并采用多通道说话者计数增强嵌入和目标说话者语音活动检测。在语音增强方面，引入了一种新型麦克风选择规则，改善了分布式麦克风的选择，并对波束形成进行了改进。在ASR方面，开发了利用Whisper和WavLM语音基础模型的多个模型。最终系统相对于基线实现了63%的相对宏观tcpWER改善，并在几何独立系统中超越了挑战的最佳结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了针对CHiME-8挑战DASR任务1的多说话者DASR系统。</li>
<li>系统能处理不同录音环境和多说话者场景。</li>
<li>通过EEND-VC技术实现端对端说话者摘要。</li>
<li>引入新型麦克风选择规则和改进的波束形成技术来提升语音增强。</li>
<li>在ASR方面，利用Whisper和WavLM语音基础模型开发多个模型。</li>
<li>系统相对于基线有显著的性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09859">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bcca51c26460219b9ae3b9a4cf3cbfa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5934d75211e330d73b45137c4d2b575.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-24/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-edd01bc760dba039e19394547c766cc8.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-06-24  Category-based Galaxy Image Generation via Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-61390f4adae71bc408d7007fb6f5e89a.jpg" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-06-24  Hybrid Attention Network for Accurate Breast Tumor Segmentation in   Ultrasound Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">22950.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
