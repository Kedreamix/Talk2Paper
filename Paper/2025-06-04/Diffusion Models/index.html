<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-06-04  OpenUni A Simple Baseline for Unified Multimodal Understanding and   Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-32f95dfd51ea03a183c4ed355570c4e2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    39 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-04-更新"><a href="#2025-06-04-更新" class="headerlink" title="2025-06-04 更新"></a>2025-06-04 更新</h1><h2 id="OpenUni-A-Simple-Baseline-for-Unified-Multimodal-Understanding-and-Generation"><a href="#OpenUni-A-Simple-Baseline-for-Unified-Multimodal-Understanding-and-Generation" class="headerlink" title="OpenUni: A Simple Baseline for Unified Multimodal Understanding and   Generation"></a>OpenUni: A Simple Baseline for Unified Multimodal Understanding and   Generation</h2><p><strong>Authors:Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy</strong></p>
<p>In this report, we present OpenUni, a simple, lightweight, and fully open-source baseline for unifying multimodal understanding and generation. Inspired by prevailing practices in unified model learning, we adopt an efficient training strategy that minimizes the training complexity and overhead by bridging the off-the-shelf multimodal large language models (LLMs) and diffusion models through a set of learnable queries and a light-weight transformer-based connector. With a minimalist choice of architecture, we demonstrate that OpenUni can: 1) generate high-quality and instruction-aligned images, and 2) achieve exceptional performance on standard benchmarks such as GenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To support open research and community advancement, we release all model weights, training code, and our curated training datasets (including 23M image-text pairs) at <a target="_blank" rel="noopener" href="https://github.com/wusize/OpenUni">https://github.com/wusize/OpenUni</a>. </p>
<blockquote>
<p>在这份报告中，我们介绍了OpenUni，这是一个简单、轻量级、完全开源的多模态理解和生成统一基准。我们受到统一模型学习流行实践的启发，采用了一种高效的训练策略，通过一组可学习的查询和一个基于轻量级变压器的连接器，将现成的多模态大型语言模型（LLMs）和扩散模型联系起来，从而最小化训练复杂性和开销。通过选择最简洁的架构，我们证明OpenUni可以：1）生成高质量且符合指令的图像；2）在GenEval、DPG-Bench和WISE等标准基准测试上实现卓越性能，仅需1.1B和3.1B激活参数。为了支持开放研究和社区发展，我们在<a target="_blank" rel="noopener" href="https://github.com/wusize/OpenUni">https://github.com/wusize/OpenUni</a>上发布了所有模型权重、训练代码和我们精选的训练数据集（包括2300万张图像文本对）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23661v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>OpenUni是一个简单、轻量级、完全开源的多模态理解和生成统一基准。它通过高效的训练策略，采用现成的多模态大型语言模型和扩散模型，通过一组可学习的查询和轻量级基于变压器的连接器，以减小训练复杂性和开销。OpenUni能够生成高质量、指令对齐的图像，并在GenEval、DPG-Bench和WISE等标准基准测试中实现卓越性能，仅需1.1B和3.1B激活参数。所有模型权重、训练代码和训练数据集已在GitHub上发布，以支持开放研究和社区发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenUni是一个统一多模态理解和生成任务的开源框架。</li>
<li>它通过轻量级设计实现了高效训练策略，降低了训练复杂性和开销。</li>
<li>OpenUni结合了现成的多模态大型语言模型和扩散模型。</li>
<li>通过可学习的查询和基于变压器的连接器实现模型间的桥梁。</li>
<li>OpenUni能生成高质量、与指令对齐的图像。</li>
<li>在多个标准基准测试中表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23661">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-03d54df6a0a77105f39c534998910b1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2bbcb0e9d34e13c915cfb04cef56811.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2246c41aef27268222410d66d168b5e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-509b0b1f9d90cd173b67a77a448fb623.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-740b5c6b356ed58fbab5c59791c4e552.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ITA-MDT-Image-Timestep-Adaptive-Masked-Diffusion-Transformer-Framework-for-Image-Based-Virtual-Try-On"><a href="#ITA-MDT-Image-Timestep-Adaptive-Masked-Diffusion-Transformer-Framework-for-Image-Based-Virtual-Try-On" class="headerlink" title="ITA-MDT: Image-Timestep-Adaptive Masked Diffusion Transformer Framework   for Image-Based Virtual Try-On"></a>ITA-MDT: Image-Timestep-Adaptive Masked Diffusion Transformer Framework   for Image-Based Virtual Try-On</h2><p><strong>Authors:Ji Woo Hong, Tri Ton, Trung X. Pham, Gwanhyeong Koo, Sunjae Yoon, Chang D. Yoo</strong></p>
<p>This paper introduces ITA-MDT, the Image-Timestep-Adaptive Masked Diffusion Transformer Framework for Image-Based Virtual Try-On (IVTON), designed to overcome the limitations of previous approaches by leveraging the Masked Diffusion Transformer (MDT) for improved handling of both global garment context and fine-grained details. The IVTON task involves seamlessly superimposing a garment from one image onto a person in another, creating a realistic depiction of the person wearing the specified garment. Unlike conventional diffusion-based virtual try-on models that depend on large pre-trained U-Net architectures, ITA-MDT leverages a lightweight, scalable transformer-based denoising diffusion model with a mask latent modeling scheme, achieving competitive results while reducing computational overhead. A key component of ITA-MDT is the Image-Timestep Adaptive Feature Aggregator (ITAFA), a dynamic feature aggregator that combines all of the features from the image encoder into a unified feature of the same size, guided by diffusion timestep and garment image complexity. This enables adaptive weighting of features, allowing the model to emphasize either global information or fine-grained details based on the requirements of the denoising stage. Additionally, the Salient Region Extractor (SRE) module is presented to identify complex region of the garment to provide high-resolution local information to the denoising model as an additional condition alongside the global information of the full garment image. This targeted conditioning strategy enhances detail preservation of fine details in highly salient garment regions, optimizing computational resources by avoiding unnecessarily processing entire garment image. Comparative evaluations confirms that ITA-MDT improves efficiency while maintaining strong performance, reaching state-of-the-art results in several metrics. </p>
<blockquote>
<p>本文介绍了针对基于图像的虚拟试穿（IVTON）设计的图像时序自适应掩膜扩散变换框架ITA-MDT。该框架旨在克服以前方法的局限性，利用掩膜扩散变换器（MDT）改进全局服装上下文和精细细节的处理。IVTON任务涉及将一张图像中的服装无缝地叠加到另一张图像中的人物身上，创建人物穿着指定服装的现实主义描绘。不同于依赖大型预训练U-Net架构的传统基于扩散的虚拟试穿模型，ITA-MDT利用了一个轻量级、可扩展的基于变换器的去噪扩散模型，并采用了掩膜潜在建模方案，在降低计算开销的同时实现了具有竞争力的结果。ITA-MDT的关键组件是图像时序自适应特征聚合器（ITAFA），这是一个动态特征聚合器，它将来自图像编码器的所有特征合并为具有相同大小的统一特征，受扩散步骤和服装图像复杂性的引导。这实现了特征的自适应加权，使模型能够根据去噪阶段的需要在全局信息或精细细节之间进行权衡。此外，还介绍了显著区域提取器（SRE）模块，用于识别服装的复杂区域，为去噪模型提供高分辨率的局部信息，作为全局信息的附加条件。这种有针对性的条件策略有助于保留服装高度显著区域的细节，并通过避免不必要地处理整个服装图像来优化计算资源。比较评估证实，ITA-MDT在提高效率的同时保持了强大的性能，在多个指标上达到了最新水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20418v2">PDF</a> CVPR 2025, Project Page: <a target="_blank" rel="noopener" href="https://jiwoohong93.github.io/ita-mdt/">https://jiwoohong93.github.io/ita-mdt/</a></p>
<p><strong>Summary</strong></p>
<p>该论文提出了基于图像时序自适应掩膜扩散转换器框架（ITA-MDT）的图像虚拟试穿（IVTON）新方法。它克服了先前方法的局限性，利用掩膜扩散转换器（MDT）处理全局服装上下文和精细细节的能力。IVTON任务是将一张图像中的服装无缝地叠加到另一张图像中的人物上，创造出人物穿着指定服装的现实主义描绘。不同于传统的基于扩散的虚拟试穿模型依赖大型预训练U-Net架构，ITA-MDT采用了一种轻量级、可扩展的基于变换器的去噪扩散模型，并结合掩膜潜在建模方案，在降低计算开销的同时实现了竞争性的结果。ITA-MDT的关键组件包括图像时序自适应特征聚合器（ITAFA）和显著区域提取器（SRE）模块，它们分别通过自适应地加权特征和识别服装的复杂区域，提高了模型在降噪阶段的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ITA-MDT框架结合了掩膜扩散转换器（MDT）用于图像虚拟试穿（IVTON），提高了处理全局服装上下文和精细细节的能力。</li>
<li>不同于传统的扩散虚拟试穿模型依赖大型U-Net架构，ITA-MDT采用轻量级、可扩展的基于变换器的去噪扩散模型，降低了计算开销。</li>
<li>ITA-MDT引入了图像时序自适应特征聚合器（ITAFA），能够根据扩散步骤和服装图像复杂度动态聚合特征。</li>
<li>显著区域提取器（SRE）模块能够识别服装的复杂区域，为去噪模型提供高分辨率的局部信息，提高细节保留能力。</li>
<li>ITA-MDT通过针对显著区域的条件策略，优化了资源分配，避免了不必要地处理整个服装图像。</li>
<li>评估结果表明，ITA-MDT在提高效率的同时保持了强大的性能，达到了多个指标的最先进水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20418">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-32f95dfd51ea03a183c4ed355570c4e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5c65a9a21e551cbc0c0b026bdebf7bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0817bc3128d28e3e0b5881607aca546.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2019ea280ffbeb288ea84251e8dfa417.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TSD-SR-One-Step-Diffusion-with-Target-Score-Distillation-for-Real-World-Image-Super-Resolution"><a href="#TSD-SR-One-Step-Diffusion-with-Target-Score-Distillation-for-Real-World-Image-Super-Resolution" class="headerlink" title="TSD-SR: One-Step Diffusion with Target Score Distillation for Real-World   Image Super-Resolution"></a>TSD-SR: One-Step Diffusion with Target Score Distillation for Real-World   Image Super-Resolution</h2><p><strong>Authors:Linwei Dong, Qingnan Fan, Yihong Guo, Zhonghao Wang, Qi Zhang, Jinwei Chen, Yawei Luo, Changqing Zou</strong></p>
<p>Pre-trained text-to-image diffusion models are increasingly applied to real-world image super-resolution (Real-ISR) task. Given the iterative refinement nature of diffusion models, most existing approaches are computationally expensive. While methods such as SinSR and OSEDiff have emerged to condense inference steps via distillation, their performance in image restoration or details recovery is not satisfied. To address this, we propose TSD-SR, a novel distillation framework specifically designed for real-world image super-resolution, aiming to construct an efficient and effective one-step model. We first introduce the Target Score Distillation, which leverages the priors of diffusion models and real image references to achieve more realistic image restoration. Secondly, we propose a Distribution-Aware Sampling Module to make detail-oriented gradients more readily accessible, addressing the challenge of recovering fine details. Extensive experiments demonstrate that our TSD-SR has superior restoration results (most of the metrics perform the best) and the fastest inference speed (e.g. 40 times faster than SeeSR) compared to the past Real-ISR approaches based on pre-trained diffusion priors. </p>
<blockquote>
<p>预训练的文本到图像扩散模型越来越多地应用于现实世界图像超分辨率（Real-ISR）任务。由于扩散模型的迭代细化特性，大多数现有方法计算量大。虽然SinSR和OSEDiff等方法已经出现，通过蒸馏来精简推理步骤，但它们在图像恢复或细节恢复方面的性能并不令人满意。为了解决这一问题，我们提出了TSD-SR，这是一种专门用于现实世界图像超分辨率的新型蒸馏框架，旨在构建高效且有效的一步模型。首先，我们引入了目标分数蒸馏，它利用扩散模型和真实图像引用的先验知识来实现更真实的图像恢复。其次，我们提出了分布感知采样模块，使细节导向的梯度更容易获取，解决恢复精细细节的挑战。大量实验表明，与基于预训练扩散先验的过去Real-ISR方法相比，我们的TSD-SR具有优越的复原效果（大多数指标表现最佳）和最快的推理速度（例如，比SeeSR快40倍）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18263v4">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>预训练文本转图像扩散模型越来越多地应用于现实图像超分辨率（Real-ISR）任务。虽然现有方法如SinSR和OSEDiff通过蒸馏技术简化了推理步骤，但在图像恢复或细节恢复方面的性能尚不满足要求。为解决此问题，我们提出TSD-SR，一种专为现实图像超分辨率设计的全新蒸馏框架，旨在构建高效且有效的一步模型。首先，我们引入目标分数蒸馏技术，利用扩散模型的先验知识和真实图像参考来实现更逼真的图像恢复。其次，我们提出分布感知采样模块，使细节导向的梯度更容易获取，解决恢复精细细节的挑战。大量实验表明，我们的TSD-SR具有优异的恢复效果（多数指标表现最佳）和最快的推理速度（例如，比SeeSR快40倍），相较于基于预训练扩散先验的过往Real-ISR方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>预训练文本转图像扩散模型在Real-ISR任务中的应用越来越广泛。</li>
<li>现有方法如SinSR和OSEDiff虽然简化了推理步骤，但在图像恢复细节方面性能不足。</li>
<li>TSD-SR框架旨在构建高效且有效的一步模型，用于现实图像超分辨率。</li>
<li>目标分数蒸馏技术利用扩散模型先验和真实图像参考，实现更逼真的图像恢复。</li>
<li>分布感知采样模块使细节导向的梯度更易获取，改善精细细节的恢复。</li>
<li>TSD-SR在恢复效果上表现优异，多数指标达到最佳。</li>
<li>TSD-SR具有快速的推理速度，相较于其他方法有明显优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18263">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a784edc48467dacba05fb267ec88226d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd23620f041b52442392f284d869c353.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15889efae840cede118dfb149267a970.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-563936f99f598115b76546470b27e6ae.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HandCraft-Anatomically-Correct-Restoration-of-Malformed-Hands-in-Diffusion-Generated-Images"><a href="#HandCraft-Anatomically-Correct-Restoration-of-Malformed-Hands-in-Diffusion-Generated-Images" class="headerlink" title="HandCraft: Anatomically Correct Restoration of Malformed Hands in   Diffusion Generated Images"></a>HandCraft: Anatomically Correct Restoration of Malformed Hands in   Diffusion Generated Images</h2><p><strong>Authors:Zhenyue Qin, Yiqun Zhang, Yang Liu, Dylan Campbell</strong></p>
<p>Generative text-to-image models, such as Stable Diffusion, have demonstrated a remarkable ability to generate diverse, high-quality images. However, they are surprisingly inept when it comes to rendering human hands, which are often anatomically incorrect or reside in the “uncanny valley”. In this paper, we propose a method HandCraft for restoring such malformed hands. This is achieved by automatically constructing masks and depth images for hands as conditioning signals using a parametric model, allowing a diffusion-based image editor to fix the hand’s anatomy and adjust its pose while seamlessly integrating the changes into the original image, preserving pose, color, and style. Our plug-and-play hand restoration solution is compatible with existing pretrained diffusion models, and the restoration process facilitates adoption by eschewing any fine-tuning or training requirements for the diffusion models. We also contribute MalHand datasets that contain generated images with a wide variety of malformed hands in several styles for hand detector training and hand restoration benchmarking, and demonstrate through qualitative and quantitative evaluation that HandCraft not only restores anatomical correctness but also maintains the integrity of the overall image. </p>
<blockquote>
<p>生成式文本到图像模型，如Stable Diffusion，已经显示出生成多样、高质量图像的显著能力。然而，当涉及到呈现人类手部时，它们却表现出惊人的无能，手部通常解剖结构不正确或处于“诡异谷”之中。在本文中，我们提出了一种名为HandCraft的方法，用于修复这种畸形的手部。这是通过利用参数模型自动构建手部掩膜和深度图像作为条件信号来实现的，这使得基于扩散的图像编辑器能够修复手部的解剖结构并调整其姿态，同时无缝地将更改集成到原始图像中，保持姿态、颜色和风格。我们的即插即用式手部修复解决方案与现有的预训练扩散模型兼容，修复过程通过避免对扩散模型的任何微调或培训要求，促进了采用。我们还贡献了MalHand数据集，其中包含多种风格和多种畸形手部的生成图像，用于手部检测器训练和手部修复基准测试，并通过定性和定量评估证明，HandCraft不仅恢复了解剖结构的正确性，还保持了整体图像的完整性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04332v3">PDF</a> 2025 IEEE&#x2F;CVF Winter Conference on Applications of Computer Vision   (WACV)</p>
<p><strong>Summary</strong></p>
<p>文本介绍了Stable Diffusion等生成式文本转图像模型在生成图像时对手部渲染的不足，提出了HandCraft方法来解决这一问题。HandCraft通过自动构建手部掩膜和深度图像作为条件信号，利用参数模型实现扩散式图像编辑器的修复手部解剖结构和调整手部姿态的功能，同时无缝集成更改到原始图像中。该方法兼容现有预训练的扩散模型，无需对其进行微调或训练要求。此外，该研究还提供了MalHand数据集，用于手检测器训练和手修复基准测试，并通过定性和定量评估证明了HandCraft方法不仅可恢复手部解剖正确性，还能保持整体图像的完整性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成式文本转图像模型如Stable Diffusion在渲染手部时存在缺陷。</li>
<li>HandCraft方法旨在解决这一问题，通过构建手部掩膜和深度图像作为条件信号，利用参数模型修复手部解剖结构和调整姿态。</li>
<li>HandCraft方法与现有预训练的扩散模型兼容，无需进行额外的训练或微调。</li>
<li>HandCraft能够在保持整体图像完整性的同时，恢复手部的解剖正确性。</li>
<li>研究提供了MalHand数据集，包含多种风格和形态异常的手部图像，用于手检测器训练和手修复基准测试。</li>
<li>通过定性和定量评估，证明了HandCraft方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04332">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1e66585e4732eefd2adaa8ac467ac81e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31ce498138ec528755ed81846a8237ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f559207be7b17548e754a3efd0f51897.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9495ee625d7e44e45b49c467f72e072b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Flex3D-Feed-Forward-3D-Generation-with-Flexible-Reconstruction-Model-and-Input-View-Curation"><a href="#Flex3D-Feed-Forward-3D-Generation-with-Flexible-Reconstruction-Model-and-Input-View-Curation" class="headerlink" title="Flex3D: Feed-Forward 3D Generation with Flexible Reconstruction Model   and Input View Curation"></a>Flex3D: Feed-Forward 3D Generation with Flexible Reconstruction Model   and Input View Curation</h2><p><strong>Authors:Junlin Han, Jianyuan Wang, Andrea Vedaldi, Philip Torr, Filippos Kokkinos</strong></p>
<p>Generating high-quality 3D content from text, single images, or sparse view images remains a challenging task with broad applications. Existing methods typically employ multi-view diffusion models to synthesize multi-view images, followed by a feed-forward process for 3D reconstruction. However, these approaches are often constrained by a small and fixed number of input views, limiting their ability to capture diverse viewpoints and, even worse, leading to suboptimal generation results if the synthesized views are of poor quality. To address these limitations, we propose Flex3D, a novel two-stage framework capable of leveraging an arbitrary number of high-quality input views. The first stage consists of a candidate view generation and curation pipeline. We employ a fine-tuned multi-view image diffusion model and a video diffusion model to generate a pool of candidate views, enabling a rich representation of the target 3D object. Subsequently, a view selection pipeline filters these views based on quality and consistency, ensuring that only the high-quality and reliable views are used for reconstruction. In the second stage, the curated views are fed into a Flexible Reconstruction Model (FlexRM), built upon a transformer architecture that can effectively process an arbitrary number of inputs. FlemRM directly outputs 3D Gaussian points leveraging a tri-plane representation, enabling efficient and detailed 3D generation. Through extensive exploration of design and training strategies, we optimize FlexRM to achieve superior performance in both reconstruction and generation tasks. Our results demonstrate that Flex3D achieves state-of-the-art performance, with a user study winning rate of over 92% in 3D generation tasks when compared to several of the latest feed-forward 3D generative models. </p>
<blockquote>
<p>从文本、单张图像或稀疏视图图像生成高质量3D内容仍然是一个具有广泛应用挑战性的任务。现有方法通常采用多视图扩散模型合成多视图图像，然后进行前馈过程进行3D重建。然而，这些方法通常受限于输入视图的小而固定的数量，导致它们无法捕捉多种视点，更糟糕的是，如果合成的视图质量较差，还会导致生成结果不理想。为了解决这个问题，我们提出了Flex3D，这是一个新型的两阶段框架，能够利用任意数量的高质量输入视图。第一阶段包括候选视图生成和筛选管道。我们采用微调的多视图图像扩散模型和视频扩散模型来生成候选视图池，实现对目标3D对象的丰富表示。随后，视图选择管道根据质量和一致性过滤这些视图，确保只使用高质量和可靠的视图进行重建。在第二阶段，精选的视图被输入到灵活重建模型（FlexRM）中，该模型基于能够有效处理任意数量输入的变压器架构。FlemRM直接输出利用三重平面表示的3D高斯点，实现高效且详细的3D生成。通过设计和训练策略的广泛探索，我们优化了FlexRM，在重建和生成任务中都实现了卓越的性能。我们的结果表明，Flex3D在性能上达到了最新水平，在与几种最新的前馈3D生成模型的比较中，用户研究获胜率高达92%以上。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.00890v3">PDF</a> ICML 25. Project page: <a target="_blank" rel="noopener" href="https://junlinhan.github.io/projects/flex3d/">https://junlinhan.github.io/projects/flex3d/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Flex3D的两阶段框架，用于从文本、单张图像或稀疏视图图像生成高质量3D内容。第一阶段通过微调的多视图图像扩散模型和视频扩散模型生成候选视图，并进行筛选和筛选。第二阶段使用基于transformer架构的灵活重建模型（FlexRM）处理任意数量的输入视图，直接输出基于tri-plane表示的3D高斯点云。该方法优化了FlexRM的设计和培训策略，在重建和生成任务中表现出卓越性能。用户研究结果显示，Flex3D在与其他最新前馈3D生成模型的比较中赢得了超过92%的胜率。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是关于Flex3D框架的关键见解：</p>
<ol>
<li>Flex3D是一个两阶段框架，可以充分利用任意数量的高质量输入视图生成高质量的3D内容。</li>
<li>第一阶段包括候选视图生成和筛选流程，通过微调的多视图图像扩散模型和视频扩散模型来生成丰富的目标3D对象表示。</li>
<li>第二阶段使用灵活重建模型（FlexRM）处理筛选后的视图，并直接输出基于tri-plane表示的3D高斯点云。该模型可以有效处理任意数量的输入，并实现了高效的详细3D生成。</li>
<li>FlexRM的优化设计包括培训策略的调整，使其在重建和生成任务中表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.00890">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-90b6efa6b95dc58dfdf4f7618662e83e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf07cc1d61f98f2658b4a5cb3de6e227.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54e208d4337574a47319e32da5be3849.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dc69b67b56980918bed3f8de9aed560.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-950c93122d34728c65a73de25dc9adbb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Conditional-Image-Synthesis-with-Diffusion-Models-A-Survey"><a href="#Conditional-Image-Synthesis-with-Diffusion-Models-A-Survey" class="headerlink" title="Conditional Image Synthesis with Diffusion Models: A Survey"></a>Conditional Image Synthesis with Diffusion Models: A Survey</h2><p><strong>Authors:Zheyuan Zhan, Defang Chen, Jian-Ping Mei, Zhenghe Zhao, Jiawei Chen, Chun Chen, Siwei Lyu, Can Wang</strong></p>
<p>Conditional image synthesis based on user-specified requirements is a key component in creating complex visual content. In recent years, diffusion-based generative modeling has become a highly effective way for conditional image synthesis, leading to exponential growth in the literature. However, the complexity of diffusion-based modeling, the wide range of image synthesis tasks, and the diversity of conditioning mechanisms present significant challenges for researchers to keep up with rapid developments and to understand the core concepts on this topic. In this survey, we categorize existing works based on how conditions are integrated into the two fundamental components of diffusion-based modeling, $\textit{i.e.}$, the denoising network and the sampling process. We specifically highlight the underlying principles, advantages, and potential challenges of various conditioning approaches during the training, re-purposing, and specialization stages to construct a desired denoising network. We also summarize six mainstream conditioning mechanisms in the sampling process. All discussions are centered around popular applications. Finally, we pinpoint several critical yet still unsolved problems and suggest some possible solutions for future research. Our reviewed works are itemized at <a target="_blank" rel="noopener" href="https://github.com/zju-pi/Awesome-Conditional-Diffusion-Models">https://github.com/zju-pi/Awesome-Conditional-Diffusion-Models</a>. </p>
<blockquote>
<p>基于用户指定要求的有条件图像合成是创建复杂视觉内容的关键组成部分。近年来，基于扩散的生成模型已成为有条件图像合成的高效方式，相关文献呈指数级增长。然而，基于扩散的建模复杂性、图像合成任务的广泛性以及调节机制的多样性，为研究者带来了跟上快速发展和了解核心概念的重大挑战。在这项调查中，我们根据条件是如何集成到基于扩散建模的两个基本组成部分（即去噪网络和采样过程）来对现有工作进行归类。我们特别强调在训练、重新利用和专业化阶段各种调节方法的内在原理、优点和潜在挑战，以构建所需的去噪网络。我们还总结了采样过程中六种主流的调节机制。所有的讨论都围绕热门应用展开。最后，我们指出了几个关键但尚未解决的问题，并为未来的研究提出了可能的解决方案。我们评述的作品详列在<a target="_blank" rel="noopener" href="https://github.com/zju-pi/Awesome-Conditional-Diffusion-Models%E3%80%82">https://github.com/zju-pi/Awesome-Conditional-Diffusion-Models。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19365v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文综述了基于扩散模型的图像合成技术，详细介绍了如何将条件因素融入扩散模型的两个基本组成部分——去噪网络和采样过程。文章重点介绍了不同条件下的训练、再利用和专业化阶段的原理、优势和潜在挑战，并总结了六种主流的采样过程中的条件机制。本文旨在帮助读者了解扩散模型在图像合成领域的应用和发展现状，同时提供了一些未来研究可能的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>扩散模型已成为条件图像合成的一种高效方法，驱动了大量文献的出现。</li>
<li>条件因素被融入扩散模型的两个基本组成部分：去噪网络和采样过程。</li>
<li>文章详细阐述了不同条件下训练、再利用和专业化阶段的原理、优势和潜在挑战。</li>
<li>总结了六种主流的采样过程中的条件机制。</li>
<li>文章围绕流行应用进行讨论。</li>
<li>指出了一些关键但尚未解决的问题，并为未来研究提供了可能的解决方案。</li>
<li>推荐的审查工作列表可在<a target="_blank" rel="noopener" href="https://github.com/zju-pi/Awesome-Conditional-Diffusion-Models%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zju-pi/Awesome-Conditional-Diffusion-Models找到。</a></li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.19365">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d8d027299fae999a1870cb0b97b4f828.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f282647e6696d641d0be298ba23a462e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06d48b0750aed60e959633abf388f971.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LaWa-Using-Latent-Space-for-In-Generation-Image-Watermarking"><a href="#LaWa-Using-Latent-Space-for-In-Generation-Image-Watermarking" class="headerlink" title="LaWa: Using Latent Space for In-Generation Image Watermarking"></a>LaWa: Using Latent Space for In-Generation Image Watermarking</h2><p><strong>Authors:Ahmad Rezaei, Mohammad Akbari, Saeed Ranjbar Alvar, Arezou Fatemi, Yong Zhang</strong></p>
<p>With generative models producing high quality images that are indistinguishable from real ones, there is growing concern regarding the malicious usage of AI-generated images. Imperceptible image watermarking is one viable solution towards such concerns. Prior watermarking methods map the image to a latent space for adding the watermark. Moreover, Latent Diffusion Models (LDM) generate the image in the latent space of a pre-trained autoencoder. We argue that this latent space can be used to integrate watermarking into the generation process. To this end, we present LaWa, an in-generation image watermarking method designed for LDMs. By using coarse-to-fine watermark embedding modules, LaWa modifies the latent space of pre-trained autoencoders and achieves high robustness against a wide range of image transformations while preserving perceptual quality of the image. We show that LaWa can also be used as a general image watermarking method. Through extensive experiments, we demonstrate that LaWa outperforms previous works in perceptual quality, robustness against attacks, and computational complexity, while having very low false positive rate. Code is available here. </p>
<blockquote>
<p>随着生成模型产生的图像质量越来越高，与真实图像无法区分，人们越来越担心AI生成图像被恶意使用的问题。不易察觉的图像水印技术是一种可行的解决方案。以往的水印方法将图像映射到潜在空间以添加水印。此外，潜在扩散模型（LDM）在预训练自编码器的潜在空间中生成图像。我们认为，可以利用这个潜在空间将水印集成到生成过程中。为此，我们提出了LaWa，这是一种为LDM设计的在生成图像过程中的水印方法。通过使用从粗糙到精细的水印嵌入模块，LaWa修改了预训练自编码器的潜在空间，并在对抗各种图像转换时实现了高度的稳健性，同时保持了图像的感知质量。我们展示LaWa也可以作为一种通用的图像水印方法使用。通过大量实验，我们证明LaWa在感知质量、对抗攻击的稳健性和计算复杂性方面优于以前的工作，同时误报率非常低。代码可在此处获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.05868v3">PDF</a> Accepted to ECCV 2024</p>
<p><strong>Summary</strong><br>     随着生成模型产出高质量、难以辨别真伪的图像，人们越来越担忧AI生成图像被恶意使用的问题。隐形图像水印技术是一种可行的解决方案。以前的水印方法将图像映射到潜在空间以添加水印。此外，潜在扩散模型（LDM）在预训练自编码器的潜在空间中生成图像。我们认为可以利用这个潜在空间将水印集成到生成过程中。为此，我们提出了面向LDM的生成中图像水印方法LaWa。通过使用粗细结合的水印嵌入模块，LaWa修改了预训练自编码器的潜在空间，对各种图像变换具有高度的鲁棒性，同时保持了图像感知质量。我们还证明了LaWa也可以作为一种通用的图像水印方法使用。通过大量实验，我们证明了LaWa在感知质量、抗攻击性、计算复杂性方面优于以前的工作，并且具有极低的误报率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成模型产出的高质量图像难以辨别真伪，引发对恶意使用的担忧。</li>
<li>隐形图像水印技术是解决这一问题的可行方案。</li>
<li>LaWa是一种面向潜在扩散模型（LDM）的生成中图像水印方法。</li>
<li>LaWa利用粗细结合的水印嵌入模块修改预训练自编码器的潜在空间。</li>
<li>LaWa对各种图像变换具有高度的鲁棒性，同时保持图像感知质量。</li>
<li>LaWa可作为通用图像水印方法使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.05868">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8ba9a30eda06ce466b136906dbb06ef8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aabf8bcf63f3cffe3abd553f29f3bc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fbea1d802bc4934f95f063d2242a526.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f01d92700a1f2c299a8452272c3ac9b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SemanticDraw-Towards-Real-Time-Interactive-Content-Creation-from-Image-Diffusion-Models"><a href="#SemanticDraw-Towards-Real-Time-Interactive-Content-Creation-from-Image-Diffusion-Models" class="headerlink" title="SemanticDraw: Towards Real-Time Interactive Content Creation from Image   Diffusion Models"></a>SemanticDraw: Towards Real-Time Interactive Content Creation from Image   Diffusion Models</h2><p><strong>Authors:Jaerin Lee, Daniel Sungho Jung, Kanggeon Lee, Kyoung Mu Lee</strong></p>
<p>We introduce SemanticDraw, a new paradigm of interactive content creation where high-quality images are generated in near real-time from given multiple hand-drawn regions, each encoding prescribed semantic meaning. In order to maximize the productivity of content creators and to fully realize their artistic imagination, it requires both quick interactive interfaces and fine-grained regional controls in their tools. Despite astonishing generation quality from recent diffusion models, we find that existing approaches for regional controllability are very slow (52 seconds for $512 \times 512$ image) while not compatible with acceleration methods such as LCM, blocking their huge potential in interactive content creation. From this observation, we build our solution for interactive content creation in two steps: (1) we establish compatibility between region-based controls and acceleration techniques for diffusion models, maintaining high fidelity of multi-prompt image generation with $\times 10$ reduced number of inference steps, (2) we increase the generation throughput with our new multi-prompt stream batch pipeline, enabling low-latency generation from multiple, region-based text prompts on a single RTX 2080 Ti GPU. Our proposed framework is generalizable to any existing diffusion models and acceleration schedulers, allowing sub-second (0.64 seconds) image content creation application upon well-established image diffusion models. Our project page is: <a target="_blank" rel="noopener" href="https://jaerinlee.com/research/semantic-draw">https://jaerinlee.com/research/semantic-draw</a> </p>
<blockquote>
<p>我们介绍了SemanticDraw，这是一种新的交互式内容创建范式，它可以从给定的多个手绘区域中实时生成高质量图像，每个区域都编码了规定的语义含义。为了最大化内容创作者的生产力并实现他们的艺术想象力，这需要快速的交互式接口和工具中的精细区域控制。尽管最近的扩散模型产生了惊人的生成质量，但我们发现现有的区域可控性方法非常缓慢（对于512x512图像需要52秒），而且不兼容如LCM等加速方法，这阻碍了它们在交互式内容创建中的巨大潜力。基于这些观察，我们将交互式内容创建的解决方案分为两步构建：首先，我们建立了基于区域的控制与扩散模型的加速技术之间的兼容性，通过减少推理步骤的数量（减少10倍），保持多提示图像生成的高保真度；其次，我们使用了新的多提示流批处理管道，提高了生成吞吐量，能够在单个RTX 2080 Ti GPU上实现多个基于区域的文本提示的低延迟生成。我们提出的框架可以应用于任何现有的扩散模型和加速调度器，允许在成熟的图像扩散模型上进行子秒级（0.64秒）的图像内容创建应用。我们的项目页面是：<a target="_blank" rel="noopener" href="https://jaerinlee.com/research/semantic-draw">https://jaerinlee.com/research/semantic-draw</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.09055v4">PDF</a> CVPR 2025 camera ready</p>
<p><strong>Summary</strong></p>
<p>本文介绍了SemanticDraw，这是一种新的交互式内容创建范式。它利用扩散模型，根据多个手绘区域生成高质量图像，每个区域都有规定的语义意义。为了提高内容创作者的效率和实现他们的艺术想象力，需要快速交互式接口和工具中的精细区域控制。现有扩散模型的区域控制方法虽然生成质量惊人，但速度较慢，且不兼容加速方法，如LCM，阻碍了它们在交互式内容创建中的巨大潜力。因此，该研究通过建立区域控制与加速技术之间的兼容性，减少推理步骤数量，提高多提示流批量管道生成吞吐量，实现了低延迟的多区域文本提示生成。该研究提出的框架可广泛应用于现有扩散模型和加速调度器，允许在成熟的图像扩散模型上实现子秒级（0.64秒）的图像内容创建应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SemanticDraw是一种基于扩散模型的新的交互式内容创建方法，可以从多个手绘区域生成高质量图像。</li>
<li>现有扩散模型的区域控制方法虽然生成质量高，但速度慢，且不兼容现有加速方法。</li>
<li>研究者通过建立区域控制与加速技术之间的兼容性来提高效率。</li>
<li>通过减少推理步骤数量，维持多提示图像生成的高保真度。</li>
<li>新的多提示流批量管道提高了生成吞吐量，实现了低延迟的多区域文本提示生成。</li>
<li>该研究提出的框架可广泛应用于现有的扩散模型和加速调度器。</li>
<li>在成熟的图像扩散模型上，能够实现子秒级（0.64秒）的图像内容创建应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.09055">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f7b91bfcdbb84be89cfb9ed2b92a6bfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38067a34fd290635b6da1362323bf686.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccd86768c8b44865ee38c47109727b1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7db3232ba790c6d630c9416c8bfc890.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2ad0e342308f900da738ab691f88959.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Detecting-Multimedia-Generated-by-Large-AI-Models-A-Survey"><a href="#Detecting-Multimedia-Generated-by-Large-AI-Models-A-Survey" class="headerlink" title="Detecting Multimedia Generated by Large AI Models: A Survey"></a>Detecting Multimedia Generated by Large AI Models: A Survey</h2><p><strong>Authors:Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding, Xin Wang, Xin Li, Luisa Verdoliva, Shu Hu</strong></p>
<p>The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) and beyond detection (adding attributes like generalizability, robustness, and interpretability to detectors). Additionally, we have presented a brief overview of generation mechanisms, public datasets, online detection tools, and evaluation metrics to provide a valuable resource for researchers and practitioners in this field. Most importantly, we offer a focused analysis from a social media perspective to highlight their broader societal impact. Furthermore, we identify current challenges in detection and propose directions for future research that address unexplored, ongoing, and emerging issues in detecting multimedia generated by LAIMs. Our aim for this survey is to fill an academic gap and contribute to global AI security efforts, helping to ensure the integrity of information in the digital realm. The project link is <a target="_blank" rel="noopener" href="https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey">https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey</a>. </p>
<blockquote>
<p>大型人工智能模型（LAIMs）的快速发展，特别是扩散模型和大语言模型，标志着一个人工智能生成多媒体日益融入日常生活的全新时代的来临。尽管这些技术在许多领域都带来了好处，但这些内容也存在重大风险，包括潜在误用、社会混乱和道德担忧。因此，检测由LAIM生成的多媒体内容变得至关重要，相关研究的增长也极为显著。尽管如此，专门针对检测LAIM生成多媒体的系统性综述仍存在明显差距。为了解决这个问题，我们提供了第一篇全面涵盖现有检测LAIM生成多媒体（如文本、图像、视频、音频和多模态内容）的研究综述。具体来说，我们引入了一种新的检测方法论分类，按媒体模态分类，并从两个角度进行阐述：纯检测（旨在提高检测性能）和超越检测（向检测器添加通用性、稳健性和可解释性等属性）。此外，我们还简要概述了生成机制、公开数据集、在线检测工具和评估指标，为这一领域的研究人员和实践者提供了有价值的资源。最重要的是，我们从社交媒体的角度进行了深入分析，以突出其更广泛的社会影响。此外，我们还确定了当前检测的挑战，并提出了未来研究的方向，以解决在检测由LAIM生成的多媒体方面尚未探索、正在出现和新兴的问题。本综述旨在填补学术空白，为全球人工智能安全努力做出贡献，帮助确保数字领域的信息完整性。项目链接为：<a target="_blank" rel="noopener" href="https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey%E3%80%82">https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.00045v6">PDF</a> </p>
<p><strong>摘要</strong><br>扩散模型等大型AI模型（LAIMs）的快速发展标志着AI生成多媒体内容在日常生活中的融入程度越来越高。虽然这些技术在许多领域具有应用价值，但同时也存在潜在的误用风险、社会混乱和伦理问题。因此，检测LAIM生成的多媒体内容变得至关重要，相关研究也日渐增多。本文首次全面回顾了检测LAIM生成的多媒体（如文本、图像、视频、音频和多模态内容）的现有研究。介绍了新颖的检测方法分类，分为纯检测（旨在提高检测性能）和超越检测（增加通用性、鲁棒性和检测器的可解释性）。此外，本文简要概述了生成机制、公开数据集、在线检测工具和评估指标，为研究人员和实践者提供有价值的资源。本文从社交媒体的角度进行了深入分析，强调了其更广泛的社会影响。同时，本文还指出了当前检测面临的挑战，并探讨了未来研究的方向，以解决检测LAIM生成的多媒体中的未探索、持续和新兴问题。本文旨在填补学术空白，为全球AI安全做出贡献，确保数字领域的资讯完整性。</p>
<p><strong>关键见解</strong></p>
<ul>
<li>大型AI模型（LAIMs）的快速发展推动了AI生成多媒体内容的广泛应用。</li>
<li>AI生成的多媒体内容在日常生活中的融入带来了一系列风险，包括潜在误用、社会混乱和伦理问题。</li>
<li>检测LAIM生成的多媒体内容已成为关键议题，相关研究逐渐增多。</li>
<li>本文首次全面综述了检测LAIM生成的多媒体的现有研究，涉及纯检测和超越检测的分类方法。</li>
<li>介绍了生成机制、公开数据集、在线检测工具和评估指标的概述。</li>
<li>从社交媒体角度深入分析了社会影响。</li>
<li>指出了当前检测面临的挑战，并提出了未来研究的方向，以应对未探索、持续和新兴问题。</li>
<li>本文旨在填补学术空白，为AI安全做出贡献，确保数字资讯的完整性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.00045">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0f4420a0e524daa3fee95ef84ca8c592.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4449c26a66fa7efc16665b06f1085c45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e36dde2ecf62f34529e06bdea81c9bac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55ce1bca568eabaeb44b6b66e4bb1a9e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2c958068ec93320796409d041fee6521.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-06-04  NMCSE Noise-Robust Multi-Modal Coupling Signal Estimation Method via   Optimal Transport for Cardiovascular Disease Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bc3ae20ded0fbbf0abafdafd8f321e01.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-06-04  RenderBender A Survey on Adversarial Attacks Using Differentiable   Rendering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">20064.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
