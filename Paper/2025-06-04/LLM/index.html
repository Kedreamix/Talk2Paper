<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-04  AutoChemSchematic AI A Closed-Loop, Physics-Aware Agentic Framework for   Auto-Generating Chemical Process and Instrumentation Diagrams">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-d3c03365170299397c44c4faec81cd22.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-04-æ›´æ–°"><a href="#2025-06-04-æ›´æ–°" class="headerlink" title="2025-06-04 æ›´æ–°"></a>2025-06-04 æ›´æ–°</h1><h2 id="AutoChemSchematic-AI-A-Closed-Loop-Physics-Aware-Agentic-Framework-for-Auto-Generating-Chemical-Process-and-Instrumentation-Diagrams"><a href="#AutoChemSchematic-AI-A-Closed-Loop-Physics-Aware-Agentic-Framework-for-Auto-Generating-Chemical-Process-and-Instrumentation-Diagrams" class="headerlink" title="AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for   Auto-Generating Chemical Process and Instrumentation Diagrams"></a>AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for   Auto-Generating Chemical Process and Instrumentation Diagrams</h2><p><strong>Authors:Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana</strong></p>
<p>Recent advancements in generative AI have accelerated the discovery of novel chemicals and materials; however, transitioning these discoveries to industrial-scale production remains a critical bottleneck, as it requires the development of entirely new chemical manufacturing processes. Current AI methods cannot auto-generate PFDs or PIDs, despite their critical role in scaling chemical processes, while adhering to engineering constraints. We present a closed loop, physics aware framework for the automated generation of industrially viable PFDs and PIDs. The framework integrates domain specialized small scale language models (SLMs) (trained for chemical process QA tasks) with first principles simulation, leveraging three key components: (1) a hierarchical knowledge graph of process flow and instrumentation descriptions for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure feasibility. To improve both runtime efficiency and model compactness, the framework incorporates advanced inference time optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test Time Inference Scaling and independently applies structural pruning techniques (width and depth) guided by importance heuristics to reduce model size with minimal accuracy loss. Experiments demonstrate that the framework generates simulator-validated process descriptions with high fidelity, outperforms baseline methods in correctness, and generalizes to unseen chemicals. By bridging AI-driven design with industrial-scale feasibility, this work significantly reduces R&amp;D timelines from lab discovery to plant deployment. </p>
<blockquote>
<p>æœ€è¿‘ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è¿›æ­¥åŠ é€Ÿäº†æ–°å‹åŒ–å­¦ç‰©è´¨å’Œææ–™çš„å‘ç°ï¼Œç„¶è€Œï¼Œå°†è¿™äº›å‘ç°è½¬åŒ–ä¸ºå·¥ä¸šè§„æ¨¡çš„ç”Ÿäº§ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®çš„ç“¶é¢ˆï¼Œå› ä¸ºè¿™éœ€è¦å¼€å‘å…¨æ–°çš„åŒ–å­¦åˆ¶é€ å·¥è‰ºã€‚å°½ç®¡å½“å‰çš„AIæ–¹æ³•åœ¨æ‰©å±•åŒ–å­¦è¿‡ç¨‹ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œä½†å®ƒä»¬æ— æ³•è‡ªåŠ¨ç”Ÿæˆå·¥è‰ºæµç¨‹å›¾ï¼ˆPFDsï¼‰æˆ–å·¥è‰ºæµç¨‹è¯´æ˜ï¼ˆPIDsï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé—­ç¯çš„ç‰©ç†æ„ŸçŸ¥æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆå¯è¡Œçš„å·¥è‰ºæµç¨‹å›¾ï¼ˆPFDsï¼‰å’Œå·¥è‰ºæµç¨‹è¯´æ˜ï¼ˆPIDsï¼‰ã€‚è¯¥æ¡†æ¶ç»“åˆäº†é’ˆå¯¹åŒ–å­¦å·¥è‰ºé—®ç­”ä»»åŠ¡è®­ç»ƒçš„é¢†åŸŸä¸“ä¸šåŒ–å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä¸åŸºäºåŸºæœ¬åŸç†çš„ä»¿çœŸï¼Œåˆ©ç”¨ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰åŒ…å«è¶…è¿‡1020ç§åŒ–å­¦ç‰©è´¨çš„å·¥è‰ºæµç¨‹å’Œä»ªå™¨æè¿°å±‚æ¬¡çŸ¥è¯†å›¾è°±ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªå¤šé˜¶æ®µè®­ç»ƒç®¡é“ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œæ£€ç´¢å¢å¼ºæŒ‡ä»¤è°ƒæ•´ï¼ˆRAITï¼‰åœ¨åˆæˆæ•°æ®é›†ä¸Šå¾®è°ƒé¢†åŸŸä¸“ä¸šåŒ–SLMï¼›ï¼ˆ3ï¼‰åŸºäºDWSIMçš„ä»¿çœŸå™¨å¾ªç¯éªŒè¯ä»¥ç¡®ä¿å¯è¡Œæ€§ã€‚ä¸ºäº†æé«˜è¿è¡Œæ•ˆç‡å’Œæ¨¡å‹ç´§å‡‘æ€§ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†å…ˆè¿›çš„æ¨ç†æ—¶é—´ä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬FlashAttentionã€å‰ç»æ€§è§£ç ã€åˆ†é¡µæ³¨æ„åŠ›ä¸KVç¼“å­˜é‡åŒ–ä»¥åŠæµ‹è¯•æ—¶é—´æ¨ç†ç¼©æ”¾æŠ€æœ¯ï¼Œå¹¶ç‹¬ç«‹åº”ç”¨ç»“æ„ä¿®å‰ªæŠ€æœ¯ï¼ˆå®½åº¦å’Œæ·±åº¦ï¼‰ï¼Œä»¥é‡è¦æ€§å¯å‘å¼ä¸ºæŒ‡å¯¼å‡å°‘æ¨¡å‹å¤§å°ï¼ŒåŒæ—¶å°½é‡å‡å°‘ç²¾åº¦æŸå¤±ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆäº†ä»¿çœŸéªŒè¯çš„é«˜ä¿çœŸå·¥è‰ºæµç¨‹æè¿°ï¼Œåœ¨æ­£ç¡®æ€§æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶èƒ½æ¨å¹¿åˆ°æœªè§è¿‡çš„åŒ–å­¦ç‰©è´¨ã€‚é€šè¿‡å¼¥åˆäº†äººå·¥æ™ºèƒ½é©±åŠ¨çš„è®¾è®¡ä¸å·¥ä¸šè§„æ¨¡å¯è¡Œæ€§ä¹‹é—´çš„é¸¿æ²Ÿï¼Œè¿™é¡¹å·¥ä½œæ˜¾è‘—ç¼©çŸ­äº†ä»å®éªŒå®¤å‘ç°åˆ°å·¥å‚éƒ¨ç½²çš„ç ”å‘æ—¶é—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24584v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨å…ˆè¿›çš„ç”Ÿæˆå¼AIæŠ€æœ¯åŠ é€Ÿæ–°å‹åŒ–å­¦ç‰©è´¨å’Œææ–™å‘ç°çš„è¿‡ç¨‹ï¼Œä½†å·¥ä¸šè§„æ¨¡ç”Ÿäº§é˜¶æ®µä»æ˜¯ç“¶é¢ˆã€‚å½“å‰AIæ–¹æ³•æ— æ³•è‡ªåŠ¨ç”Ÿæˆå·¥è‰ºæµç¨‹å›¾ï¼ˆPFDsï¼‰å’Œå·¥è‰ºæµç¨‹æŒ‡ç¤ºå›¾ï¼ˆPIDsï¼‰ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é—­ç¯ã€ç‰©ç†æ„ŸçŸ¥çš„æ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ä¸“ä¸šé¢†åŸŸçš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰å’ŒåŸºäºç¬¬ä¸€åŸç†çš„ä»¿çœŸæŠ€æœ¯ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å®ç°è‡ªåŠ¨åŒ–ç”Ÿæˆå…·æœ‰å·¥ä¸šå¯è¡Œæ€§çš„PFDså’ŒPIDsã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„å·¥è‰ºæµç¨‹æè¿°å…·æœ‰é«˜åº¦çš„ä¿çœŸåº¦å’Œæ­£ç¡®æ€§ï¼Œå¹¶èƒ½æ¨å¹¿åˆ°æœªè§è¿‡çš„åŒ–å­¦ç‰©è´¨ã€‚è¿™ä¸€å·¥ä½œæ˜¾è‘—ç¼©çŸ­äº†ä»å®éªŒå®¤å‘ç°åˆ°å·¥å‚éƒ¨ç½²çš„ç ”å‘æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼AIåœ¨æ–°å‹åŒ–å­¦ç‰©è´¨å’Œææ–™å‘ç°æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†å·¥ä¸šè§„æ¨¡ç”Ÿäº§ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰AIæ–¹æ³•æ— æ³•è‡ªåŠ¨ç”Ÿæˆå·¥è‰ºæµç¨‹å›¾ï¼ˆPFDsï¼‰å’Œå·¥è‰ºæµç¨‹æŒ‡ç¤ºå›¾ï¼ˆPIDsï¼‰ã€‚</li>
<li>æå‡ºä¸€ç§é—­ç¯ã€ç‰©ç†æ„ŸçŸ¥çš„æ¡†æ¶ï¼Œç»“åˆå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰å’ŒåŸºäºç¬¬ä¸€åŸç†çš„ä»¿çœŸæŠ€æœ¯ï¼Œè‡ªåŠ¨åŒ–ç”Ÿæˆå…·æœ‰å·¥ä¸šå¯è¡Œæ€§çš„PFDså’ŒPIDsã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå±‚æ¬¡åŒ–çš„çŸ¥è¯†å›¾è°±ã€å¤šé˜¶æ®µè®­ç»ƒç®¡é“å’ŒDWSIMæ¨¡æ‹Ÿå™¨éªŒè¯ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨å¤šç§æ¨ç†æ—¶é—´ä¼˜åŒ–æŠ€æœ¯å’Œç»“æ„å‰ªææŠ€æœ¯ï¼Œä»¥æé«˜è¿è¡Œæ•ˆç‡å’Œæ¨¡å‹ç´§å‡‘æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„å·¥è‰ºæµç¨‹æè¿°å…·æœ‰é«˜åº¦çš„ä¿çœŸåº¦å’Œæ­£ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-238b0504040d6251fc4793018f33f2e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8124b1329bf708d3356c5d77642bf90d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3187f64448014e03deaf4a610f26608.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="OpenUni-A-Simple-Baseline-for-Unified-Multimodal-Understanding-and-Generation"><a href="#OpenUni-A-Simple-Baseline-for-Unified-Multimodal-Understanding-and-Generation" class="headerlink" title="OpenUni: A Simple Baseline for Unified Multimodal Understanding and   Generation"></a>OpenUni: A Simple Baseline for Unified Multimodal Understanding and   Generation</h2><p><strong>Authors:Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy</strong></p>
<p>In this report, we present OpenUni, a simple, lightweight, and fully open-source baseline for unifying multimodal understanding and generation. Inspired by prevailing practices in unified model learning, we adopt an efficient training strategy that minimizes the training complexity and overhead by bridging the off-the-shelf multimodal large language models (LLMs) and diffusion models through a set of learnable queries and a light-weight transformer-based connector. With a minimalist choice of architecture, we demonstrate that OpenUni can: 1) generate high-quality and instruction-aligned images, and 2) achieve exceptional performance on standard benchmarks such as GenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To support open research and community advancement, we release all model weights, training code, and our curated training datasets (including 23M image-text pairs) at <a target="_blank" rel="noopener" href="https://github.com/wusize/OpenUni">https://github.com/wusize/OpenUni</a>. </p>
<blockquote>
<p>åœ¨æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OpenUniï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•ã€è½»é‡çº§ã€å®Œå…¨å¼€æºçš„åŸºçº¿ç³»ç»Ÿï¼Œç”¨äºç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚æˆ‘ä»¬å€Ÿé‰´äº†å½“å‰æµè¡Œçš„ç»Ÿä¸€æ¨¡å‹å­¦ä¹ å®è·µï¼Œé‡‡ç”¨äº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡ä¸€ç»„å¯å­¦ä¹ çš„æŸ¥è¯¢å’Œä¸€ä¸ªåŸºäºè½»é‡çº§å˜å‹å™¨çš„è¿æ¥å™¨ï¼Œå°†ç°æˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ‰©æ•£æ¨¡å‹è¿æ¥èµ·æ¥ï¼Œä»è€Œé™ä½äº†è®­ç»ƒå¤æ‚æ€§å’Œå¼€é”€ã€‚åœ¨æ¶æ„ä¸Šé€‰æ‹©ç®€æ´æ˜äº†ï¼Œæˆ‘ä»¬è¯æ˜OpenUniå¯ä»¥ï¼š1ï¼‰ç”Ÿæˆé«˜è´¨é‡ã€æŒ‡ä»¤ä¸€è‡´çš„å›¾åƒï¼›2ï¼‰åœ¨GenEvalã€DPG-Benchå’ŒWISEç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šå®ç°å“è¶Šæ€§èƒ½ï¼Œæ¿€æ´»å‚æ•°åªæœ‰1.1Bå’Œ3.1Bã€‚ä¸ºäº†æ”¯æŒå¼€æ”¾ç ”ç©¶å’Œç¤¾åŒºå‘å±•ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/wusize/OpenUni">https://github.com/wusize/OpenUni</a>ä¸Šå‘å¸ƒäº†æ‰€æœ‰æ¨¡å‹æƒé‡ã€è®­ç»ƒä»£ç å’Œæˆ‘ä»¬ç²¾é€‰çš„è®­ç»ƒæ•°æ®é›†ï¼ˆåŒ…æ‹¬2300ä¸‡å¼ å›¾åƒæ–‡æœ¬å¯¹ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23661v3">PDF</a> </p>
<p><strong>Summary</strong><br>å¼€æºé¡¹ç›®OpenUniæ—¨åœ¨å®ç°å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„ä¸€ä½“åŒ–ã€‚é€šè¿‡æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œè¯¥é¡¹ç›®å……åˆ†åˆ©ç”¨ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¯å­¦ä¹ çš„æŸ¥è¯¢å’Œè½»é‡çº§åŸºäºè½¬æ¢å™¨çš„è¿æ¥å™¨å®ç°è®­ç»ƒå¤æ‚æ€§å’Œå¼€é”€çš„æœ€å°åŒ–ã€‚è¯¥é¡¹ç›®å±•ç¤ºäº†å…¶èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€ç¬¦åˆæŒ‡ä»¤çš„å›¾åƒï¼Œå¹¶åœ¨GenEvalã€DPG Benchå’ŒWISEç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ‰€æœ‰æ¨¡å‹æƒé‡ã€è®­ç»ƒä»£ç å’Œç²¾é€‰çš„è®­ç»ƒæ•°æ®é›†ï¼ˆåŒ…æ‹¬2.3äº¿ä¸ªå›¾åƒæ–‡æœ¬å¯¹ï¼‰å‡å·²å‘å¸ƒåœ¨GitHubä¸Šï¼Œä»¥æ”¯æŒå¼€æ”¾ç ”ç©¶å’Œç¤¾åŒºå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>OpenUniæ˜¯ä¸€ä¸ªç®€å•ã€è½»é‡çº§ã€å®Œå…¨å¼€æºçš„é¡¹ç›®ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚</li>
<li>å®ƒé‡‡ç”¨äº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡è¿æ¥ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ‰©æ•£æ¨¡å‹æ¥å®ç°æ€§èƒ½ä¼˜åŒ–ã€‚</li>
<li>è¯¥é¡¹ç›®å…·æœ‰ç”Ÿæˆé«˜è´¨é‡ã€ç¬¦åˆæŒ‡ä»¤çš„å›¾åƒçš„èƒ½åŠ›ã€‚</li>
<li>OpenUniåœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¦‚GenEvalã€DPG Benchå’ŒWISEã€‚</li>
<li>å®ƒæ”¯æŒé€šè¿‡å¯å­¦ä¹ çš„æŸ¥è¯¢å’Œè½»é‡çº§åŸºäºè½¬æ¢å™¨çš„è¿æ¥å™¨è¿›è¡Œçµæ´»åº”ç”¨ã€‚</li>
<li>è¯¥é¡¹ç›®å°†æ‰€æœ‰æ¨¡å‹æƒé‡ã€è®­ç»ƒä»£ç å’Œè®­ç»ƒæ•°æ®é›†å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šï¼Œä»¥ä¿ƒè¿›å¼€æ”¾ç ”ç©¶å’Œç¤¾åŒºå‘å±•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-03d54df6a0a77105f39c534998910b1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2bbcb0e9d34e13c915cfb04cef56811.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2246c41aef27268222410d66d168b5e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-509b0b1f9d90cd173b67a77a448fb623.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-740b5c6b356ed58fbab5c59791c4e552.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Distill-CLIP-DCLIP-Enhancing-Image-Text-Retrieval-via-Cross-Modal-Transformer-Distillation"><a href="#Distill-CLIP-DCLIP-Enhancing-Image-Text-Retrieval-via-Cross-Modal-Transformer-Distillation" class="headerlink" title="Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal   Transformer Distillation"></a>Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal   Transformer Distillation</h2><p><strong>Authors:Daniel Csizmadia, Andrei Codreanu, Victor Sim, Vighnesh Prabhu, Michael Lu, Kevin Zhu, Sean Oâ€™Brien, Vasu Sharma</strong></p>
<p>We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that enhances multimodal image-text retrieval while preserving the original modelâ€™s strong zero-shot classification capabilities. CLIP models are typically constrained by fixed image resolutions and limited context, which can hinder their effectiveness in retrieval tasks that require fine-grained cross-modal understanding. DCLIP addresses these challenges through a meta teacher-student distillation framework, where a cross-modal transformer teacher is fine-tuned to produce enriched embeddings via bidirectional cross-attention between YOLO-extracted image regions and corresponding textual spans. These semantically and spatially aligned global representations guide the training of a lightweight student model using a hybrid loss that combines contrastive learning and cosine similarity objectives. Despite being trained on only ~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a fraction of CLIPâ€™s original dataset-DCLIP significantly improves image-text retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIPâ€™s zero-shot classification performance. These results demonstrate that DCLIP effectively mitigates the trade-off between task specialization and generalization, offering a resource-efficient, domain-adaptive, and detail-sensitive solution for advanced vision-language tasks. Code available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DCLIP-B772/README.md">https://anonymous.4open.science/r/DCLIP-B772/README.md</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Distill CLIPï¼ˆDCLIPï¼‰ï¼Œå®ƒæ˜¯CLIPæ¨¡å‹çš„ä¸€ç§å¾®è°ƒå˜ä½“ï¼Œå¢å¼ºäº†å¤šæ¨¡æ€å›¾åƒæ–‡æœ¬æ£€ç´¢åŠŸèƒ½ï¼ŒåŒæ—¶ä¿ç•™äº†åŸå§‹æ¨¡å‹å¼ºå¤§çš„é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›ã€‚CLIPæ¨¡å‹é€šå¸¸å—åˆ°å›ºå®šå›¾åƒåˆ†è¾¨ç‡å’Œæœ‰é™ä¸Šä¸‹æ–‡çš„é™åˆ¶ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢å®ƒä»¬åœ¨éœ€è¦ç²¾ç»†è·¨æ¨¡æ€ç†è§£çš„ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚DCLIPé€šè¿‡å…ƒæ•™å¸ˆå­¦ç”Ÿè’¸é¦æ¡†æ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå…¶ä¸­è·¨æ¨¡æ€å˜å‹å™¨æ•™å¸ˆç»è¿‡å¾®è°ƒï¼Œé€šè¿‡YOLOæå–çš„å›¾åƒåŒºåŸŸå’Œç›¸åº”æ–‡æœ¬è·¨åº¦ä¹‹é—´çš„åŒå‘äº¤å‰æ³¨æ„åŠ›äº§ç”Ÿä¸°å¯Œçš„åµŒå…¥ã€‚è¿™äº›è¯­ä¹‰å’Œç©ºé—´å¯¹é½çš„å…¨å±€è¡¨ç¤ºé€šè¿‡ä½¿ç”¨ç»“åˆå¯¹æ¯”å­¦ä¹ å’Œä½™å¼¦ç›¸ä¼¼æ€§ç›®æ ‡çš„æ··åˆæŸå¤±æ¥æŒ‡å¯¼è½»é‡çº§å­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒã€‚å°½ç®¡ä»…ä½¿ç”¨ä»MSCOCOã€Flickr30kå’ŒConceptual Captionsä¸­ç²¾é€‰çš„çº¦67,500ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œè¿™äº›æ ·æœ¬ä»…å CLIPåŸå§‹æ•°æ®é›†çš„ä¸€éƒ¨åˆ†ï¼Œä½†DCLIPæ˜¾è‘—æé«˜äº†å›¾åƒæ–‡æœ¬æ£€ç´¢æŒ‡æ ‡ï¼ˆRecall@Kï¼ŒMAPï¼‰ï¼ŒåŒæ—¶ä¿ç•™äº†CLIPå¤§çº¦94%çš„é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDCLIPæœ‰æ•ˆåœ°ç¼“è§£äº†ä»»åŠ¡ä¸“é—¨åŒ–ä¸é€šç”¨åŒ–ä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºé«˜çº§è§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†èµ„æºé«˜æ•ˆã€é¢†åŸŸè‡ªé€‚åº”å’Œç»†èŠ‚æ•æ„Ÿçš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯ç”¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DCLIP-B772/README.md%E3%80%82">https://anonymous.4open.science/r/DCLIP-B772/README.mdã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21549v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Distill CLIPï¼ˆDCLIPï¼‰æ˜¯CLIPæ¨¡å‹çš„ç²¾ç»†è°ƒæ•´å˜ä½“ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å›¾åƒæ–‡æœ¬æ£€ç´¢åŠŸèƒ½ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹æ¨¡å‹çš„å¼ºå¤§é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›ã€‚DCLIPé€šè¿‡å…ƒæ•™å¸ˆ-å­¦ç”Ÿè’¸é¦æ¡†æ¶è§£å†³CLIPæ¨¡å‹åœ¨å›¾åƒåˆ†è¾¨ç‡å’Œä¸Šä¸‹æ–‡æ–¹é¢çš„å±€é™æ€§ï¼Œé€šè¿‡è·¨æ¨¡æ€å˜å‹å™¨æ•™å¸ˆæ¨¡å‹äº§ç”Ÿä¸°å¯Œçš„åµŒå…¥ï¼Œé€šè¿‡YOLOæå–çš„å›¾åƒåŒºåŸŸå’Œç›¸åº”æ–‡æœ¬è·¨åº¦ä¹‹é—´çš„åŒå‘äº¤å‰æ³¨æ„åŠ›å®ç°ã€‚è¿™äº›è¯­ä¹‰å’Œç©ºé—´å¯¹é½çš„å…¨å±€è¡¨ç¤ºé€šè¿‡æ··åˆæŸå¤±å¼•å¯¼è½»é‡çº§å­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒï¼Œè¯¥æŸå¤±ç»“åˆäº†å¯¹æ¯”å­¦ä¹ å’Œä½™å¼¦ç›¸ä¼¼æ€§ç›®æ ‡ã€‚å°½ç®¡ä»…åœ¨MSCOCOã€Flickr30kå’ŒConceptual Captionsç­‰æ•°æ®é›†çš„éƒ¨åˆ†æ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†DCLIPæ˜¾è‘—æé«˜äº†å›¾åƒæ–‡æœ¬æ£€ç´¢æŒ‡æ ‡ï¼ˆRecall@Kï¼ŒMAPï¼‰ï¼ŒåŒæ—¶ä¿ç•™äº†CLIPçš„é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½çš„çº¦94%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDCLIPæœ‰æ•ˆåœ°ç¼“è§£äº†ä»»åŠ¡ç‰¹å¼‚æ€§å’Œé€šç”¨æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºé«˜çº§è§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†èµ„æºé«˜æ•ˆã€åŸŸè‡ªé€‚åº”å’Œç»†èŠ‚æ•æ„Ÿçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DCLIPæ˜¯CLIPæ¨¡å‹çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å›¾åƒæ–‡æœ¬æ£€ç´¢èƒ½åŠ›ã€‚</li>
<li>DCLIPè§£å†³äº†CLIPæ¨¡å‹åœ¨å›¾åƒåˆ†è¾¨ç‡å’Œä¸Šä¸‹æ–‡æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>DCLIPä½¿ç”¨è·¨æ¨¡æ€å˜å‹å™¨æ•™å¸ˆæ¨¡å‹äº§ç”Ÿä¸°å¯Œçš„åµŒå…¥ã€‚</li>
<li>YOLOæå–çš„å›¾åƒåŒºåŸŸå’Œæ–‡æœ¬è·¨åº¦ä¹‹é—´çš„åŒå‘äº¤å‰æ³¨æ„åŠ›å¢å¼ºäº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>DCLIPé€šè¿‡æ··åˆæŸå¤±å‡½æ•°è®­ç»ƒè½»é‡çº§å­¦ç”Ÿæ¨¡å‹ã€‚</li>
<li>DCLIPåœ¨å°‘é‡æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œä½†æ˜¾è‘—æé«˜äº†å›¾åƒæ–‡æœ¬æ£€ç´¢æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fc9b63e0e1b498395f698ef53d3770f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64fc51ed96f60f5dfbe42773329d3a1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3c03365170299397c44c4faec81cd22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2a39fdf816a19d9a53b77d2690e9b54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c28462a539d4efc44dd09b4584be2188.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TAG-INSTRUCT-Controlled-Instruction-Complexity-Enhancement-through-Structure-based-Augmentation"><a href="#TAG-INSTRUCT-Controlled-Instruction-Complexity-Enhancement-through-Structure-based-Augmentation" class="headerlink" title="TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through   Structure-based Augmentation"></a>TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through   Structure-based Augmentation</h2><p><strong>Authors:He Zhu, Zhiwen Ruan, Junyou Su, Xingwei He, Yun Chen, Wenjia Zhang, Guanhua Chen</strong></p>
<p>High-quality instruction data is crucial for developing large language models (LLMs), yet existing approaches struggle to effectively control instruction complexity. We present TAG-INSTRUCT, a novel framework that enhances instruction complexity through structured semantic compression and controlled difficulty augmentation. Unlike previous prompt-based methods operating on raw text, TAG-INSTRUCT compresses instructions into a compact tag space and systematically enhances complexity through RL-guided tag expansion. Through extensive experiments, we show that TAG-INSTRUCT outperforms existing instruction complexity augmentation approaches. Our analysis reveals that operating in tag space provides superior controllability and stability across different instruction synthesis frameworks. </p>
<blockquote>
<p>é«˜è´¨é‡æŒ‡ä»¤æ•°æ®å¯¹äºå¼€å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ï¼Œç„¶è€Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ§åˆ¶æŒ‡ä»¤å¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†TAG-INSTRUCTè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“æ„åŒ–è¯­ä¹‰å‹ç¼©å’Œå—æ§éš¾åº¦å¢å¼ºæ¥æå‡æŒ‡ä»¤å¤æ‚æ€§ã€‚ä¸ä¹‹å‰åœ¨åŸå§‹æ–‡æœ¬ä¸Šæ“ä½œçš„åŸºäºæç¤ºçš„æ–¹æ³•ä¸åŒï¼ŒTAG-INSTRUCTå°†æŒ‡ä»¤å‹ç¼©åˆ°ä¸€ä¸ªç´§å‡‘çš„æ ‡ç­¾ç©ºé—´ä¸­ï¼Œå¹¶é€šè¿‡RLå¼•å¯¼çš„æ ‡ç­¾æ‰©å±•æ¥ç³»ç»Ÿåœ°å¢å¼ºå¤æ‚æ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†TAG-INSTRUCTä¼˜äºç°æœ‰çš„æŒ‡ä»¤å¤æ‚æ€§å¢å¼ºæ–¹æ³•ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œåœ¨æ ‡ç­¾ç©ºé—´ä¸­è¿›è¡Œæ“ä½œæä¾›äº†å‡ºè‰²çš„å¯æ§æ€§å’Œç¨³å®šæ€§ï¼Œé€‚ç”¨äºä¸åŒçš„æŒ‡ä»¤åˆæˆæ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18557v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å¼€å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­é«˜è´¨é‡æŒ‡ä»¤æ•°æ®çš„é‡è¦æ€§ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ§åˆ¶æŒ‡ä»¤å¤æ‚æ€§ï¼Œå› æ­¤æå‡ºäº†TAG-INSTRUCTè¿™ä¸€æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“æ„åŒ–è¯­ä¹‰å‹ç¼©å’Œå—æ§éš¾åº¦å¢å¼ºæ¥æé«˜æŒ‡ä»¤å¤æ‚æ€§ã€‚ä¸åŸºäºæç¤ºçš„æ–¹æ³•ä¸åŒï¼Œå®ƒåœ¨ç´§å‡‘çš„æ ‡ç­¾ç©ºé—´å†…å‹ç¼©æŒ‡ä»¤ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ å¼•å¯¼æ ‡ç­¾æ‰©å±•è¿›è¡Œç³»ç»Ÿæ€§çš„å¢å¼ºã€‚å®éªŒè¯æ˜ï¼ŒTAG-INSTRUCTåœ¨æŒ‡ä»¤å¤æ‚æ€§å¢å¼ºæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚åˆ†æè¡¨æ˜ï¼Œåœ¨æ ‡ç­¾ç©ºé—´ä¸­è¿›è¡Œæ“ä½œæä¾›äº†å‡ºè‰²çš„å¯æ§æ€§å’Œç¨³å®šæ€§ï¼Œé€‚ç”¨äºä¸åŒçš„æŒ‡ä»¤åˆæˆæ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡æŒ‡ä»¤æ•°æ®å¯¹å¼€å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ§åˆ¶æŒ‡ä»¤å¤æ‚æ€§ã€‚</li>
<li>TAG-INSTRUCTæ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æŒ‡ä»¤å¤æ‚æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ç»“æ„åŒ–è¯­ä¹‰å‹ç¼©å’Œå—æ§éš¾åº¦å¢å¼ºæ¥å®ç°ç›®æ ‡ã€‚</li>
<li>ä¸åŸºäºæç¤ºçš„æ–¹æ³•ä¸åŒï¼ŒTAG-INSTRUCTåœ¨ç´§å‡‘çš„æ ‡ç­¾ç©ºé—´å†…æ“ä½œã€‚</li>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ å¼•å¯¼æ ‡ç­¾æ‰©å±•ï¼Œå¢å¼ºæŒ‡ä»¤å¤æ‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18557">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-77b527ffead2e38a7da081c0c64db267.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a57daf5ed4c9dc01da06546a950c175.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8303d316b238945132a4dd0995c0f064.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Attributing-Response-to-Context-A-Jensen-Shannon-Divergence-Driven-Mechanistic-Study-of-Context-Attribution-in-Retrieval-Augmented-Generation"><a href="#Attributing-Response-to-Context-A-Jensen-Shannon-Divergence-Driven-Mechanistic-Study-of-Context-Attribution-in-Retrieval-Augmented-Generation" class="headerlink" title="Attributing Response to Context: A Jensen-Shannon Divergence Driven   Mechanistic Study of Context Attribution in Retrieval-Augmented Generation"></a>Attributing Response to Context: A Jensen-Shannon Divergence Driven   Mechanistic Study of Context Attribution in Retrieval-Augmented Generation</h2><p><strong>Authors:Ruizhe Li, Chen Chen, Yuchen Hu, Yanjun Gao, Xi Wang, Emine Yilmaz</strong></p>
<p>Retrieval-Augmented Generation (RAG) leverages large language models (LLMs) combined with external contexts to enhance the accuracy and reliability of generated responses. However, reliably attributing generated content to specific context segments, context attribution, remains challenging due to the computationally intensive nature of current methods, which often require extensive fine-tuning or human annotation. In this work, we introduce a novel Jensen-Shannon Divergence driven method to Attribute Response to Context (ARC-JSD), enabling efficient and accurate identification of essential context sentences without additional fine-tuning or surrogate modelling. Evaluations on a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using instruction-tuned LLMs in different scales demonstrate superior accuracy and significant computational efficiency improvements compared to the previous surrogate-based method. Furthermore, our mechanistic analysis reveals specific attention heads and multilayer perceptron (MLP) layers responsible for context attribution, providing valuable insights into the internal workings of RAG models. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ruizheliUOA/ARC_JSD">https://github.com/ruizheliUOA/ARC_JSD</a> </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤–éƒ¨ä¸Šä¸‹æ–‡ç›¸ç»“åˆï¼Œæé«˜ç”Ÿæˆå“åº”çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚ç„¶è€Œï¼Œç”±äºå½“å‰æ–¹æ³•çš„è®¡ç®—å¯†é›†ç‰¹æ€§ï¼Œå°†ç”Ÿæˆå†…å®¹å¯é åœ°å½’å› äºç‰¹å®šçš„ä¸Šä¸‹æ–‡ç‰‡æ®µï¼Œå³ä¸Šä¸‹æ–‡å½’å› ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™é€šå¸¸éœ€è¦å¤§é‡çš„å¾®è°ƒæˆ–äººå·¥æ ‡æ³¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸºäºJensen-Shannonæ•£åº¦çš„æ–¹æ³•â€”â€”åŸºäºè¯­å¢ƒå“åº”å½’å› ï¼ˆARC-JSDï¼‰ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–å¾®è°ƒæˆ–æ›¿ä»£å»ºæ¨¡çš„æƒ…å†µä¸‹ï¼Œé«˜æ•ˆå‡†ç¡®åœ°è¯†åˆ«å‡ºå…³é”®ä¸Šä¸‹æ–‡å¥å­ã€‚åœ¨å¤šç§RAGåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ï¼Œå¦‚TyDi QAã€Hotpot QAå’ŒMusiqueï¼Œä½¿ç”¨ä¸åŒè§„æ¨¡æŒ‡ä»¤è°ƒæ•´LLMçš„è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºç›¸è¾ƒäºä¹‹å‰çš„åŸºäºæ›¿ä»£æ–¹æ³•å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ˜¾è‘—çš„è®¡ç®—æ•ˆç‡æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æœºåˆ¶åˆ†ææ­ç¤ºäº†è´Ÿè´£ä¸Šä¸‹æ–‡å½’å› çš„ç‰¹å®šæ³¨æ„åŠ›å¤´å’Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å±‚ï¼Œä¸ºç†è§£RAGæ¨¡å‹çš„å†…éƒ¨å·¥ä½œåŸç†æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ruizheliUOA/ARC_JSD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ruizheliUOA/ARC_JSDæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16415v2">PDF</a> Work in process</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨ç¯å¢ƒç»“åˆç”Ÿæˆå“åº”ï¼Œä½†å…¶è®¡ç®—é‡å¤§ä½¿å¾—åŒºåˆ†å“åº”å†…å®¹å¯¹åº”çš„ç‰¹å®šè¯­å¢ƒå˜å¾—å›°éš¾ã€‚ç°ä»‹ç»ä¸€ç§æ–°å‹åŸºäºæ°æ£®-é¦™å†œæ•£åº¦é©±åŠ¨çš„æ–¹æ³•ARC-JSDè¿›è¡Œè¯­å¢ƒè¯†åˆ«ï¼Œå…¶å‡†ç¡®é«˜æ•ˆæ— éœ€é¢å¤–çš„å¾®è°ƒæˆ–æ›¿ä»£å»ºæ¨¡ã€‚å…¶åœ¨å„ç±»æ£€ç´¢å¢å¼ºå‹ç”Ÿæˆä»»åŠ¡ä¸Šå±•ç°å‡ºè‰²ä¼˜åŠ¿ã€‚é€šè¿‡è§£ææ³¨æ„åŠ›å±‚å’Œå¤šå±‚æ„ŸçŸ¥æœºå±‚æ­ç¤ºäº†è¯­å¢ƒè¯†åˆ«çš„å†…éƒ¨æœºåˆ¶ã€‚å…·ä½“å®ç°æ–¹æ³•è§ä»£ç ä»“åº“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RAGæŠ€æœ¯ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ä¸å¤–éƒ¨ä¸Šä¸‹æ–‡æé«˜ç”Ÿæˆå“åº”çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
<li>è¯­å¢ƒè¯†åˆ«æ˜¯RAGçš„ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•è®¡ç®—é‡å¤§ä¸”æ•ˆç‡ä¸é«˜ã€‚</li>
<li>ARC-JSDæ–¹æ³•ä½¿ç”¨æ°æ£®-é¦™å†œæ•£åº¦æŠ€æœ¯è§£å†³è¯­å¢ƒè¯†åˆ«é—®é¢˜ï¼Œå‡†ç¡®é«˜æ•ˆä¸”æ— éœ€é¢å¤–å¾®è°ƒæˆ–æ›¿ä»£å»ºæ¨¡ã€‚</li>
<li>ARC-JSDåœ¨å¤šç§RAGä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬TyDi QAã€Hotpot QAå’ŒMusiqueç­‰ã€‚</li>
<li>é€šè¿‡å†…éƒ¨æœºåˆ¶åˆ†æï¼Œæ­ç¤ºäº†ç‰¹å®šæ³¨æ„åŠ›å±‚å’Œå¤šå±‚æ„ŸçŸ¥æœºå±‚åœ¨è¯­å¢ƒè¯†åˆ«ä¸­çš„ä½œç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16415">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-879fb02c0cc0b3f2fb88c1873eb5235a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b76cc1728bda2137c84d7e3e8e30e36e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18419d4250a2e2a4122090d7d5740c50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b3aecf2b5a7171c613711aa6b5026df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f3944b26a402227ec9c69b4b5775781.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Omni-R1-Do-You-Really-Need-Audio-to-Fine-Tune-Your-Audio-LLM"><a href="#Omni-R1-Do-You-Really-Need-Audio-to-Fine-Tune-Your-Audio-LLM" class="headerlink" title="Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?"></a>Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?</h2><p><strong>Authors:Andrew Rouditchenko, Saurabhchand Bhati, Edson Araujo, Samuel Thomas, Hilde Kuehne, Rogerio Feris, James Glass</strong></p>
<p>We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU and MMAR benchmarks. Omni-R1 achieves the highest accuracies on the sounds, music, speech, and overall average categories, both on the Test-mini and Test-full splits. To understand the performance improvement, we tested models both with and without audio and found that much of the performance improvement from GRPO could be attributed to better text-based reasoning. We also made a surprising discovery that fine-tuning without audio on a text-only dataset was effective at improving the audio-based performance. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Omni-R1ï¼Œå®ƒé€šè¿‡å¼ºåŒ–å­¦ä¹ æ³•GRPOå¯¹è¿‘æœŸçš„å¤šæ¨¡å¼LLMï¼ŒQwen2.5-Omniè¿›è¡Œå¾®è°ƒï¼Œè¯¥æ¨¡å‹åœ¨éŸ³é¢‘é—®ç­”æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¿™å®ç°äº†åœ¨æœ€æ–°çš„MMAUå’ŒMMARåŸºå‡†æµ‹è¯•ä¸Šçš„æœ€æ–°å›½å®¶æŠ€æœ¯æ°´å‡†æ€§èƒ½ã€‚Omni-R1åœ¨å£°éŸ³ã€éŸ³ä¹ã€æ¼”è®²å’Œæ€»ä½“å¹³å‡ç±»åˆ«ä¸Šå‡è¾¾åˆ°äº†æœ€é«˜çš„å‡†ç¡®åº¦ï¼Œæ— è®ºæ˜¯åœ¨Test-miniè¿˜æ˜¯Test-fullåˆ†å‰²ä¸Šã€‚ä¸ºäº†äº†è§£æ€§èƒ½æå‡çš„åŸå› ï¼Œæˆ‘ä»¬å¯¹å¸¦æœ‰å’Œä¸å¸¦éŸ³é¢‘çš„æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ï¼Œå‘ç°GRPOçš„å¤§éƒ¨åˆ†æ€§èƒ½æå‡æ˜¯ç”±äºåŸºäºæ–‡æœ¬æ¨ç†çš„æ”¹è¿›ã€‚æˆ‘ä»¬è¿˜æ„å¤–åœ°å‘ç°ï¼Œåœ¨çº¯æ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œä¸å¸¦éŸ³é¢‘çš„å¾®è°ƒï¼Œå¯¹äºæé«˜åŸºäºéŸ³é¢‘çš„æ€§èƒ½ä¹Ÿæ˜¯æœ‰æ•ˆçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09439v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Omni-R1é€šè¿‡å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹Qwen2.5-Omniï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ ç®—æ³•GRPOï¼Œåœ¨éŸ³é¢‘é—®ç­”æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚åœ¨MMAUå’ŒMMARåŸºå‡†æµ‹è¯•ä¸­ï¼ŒOmni-R1åœ¨å£°éŸ³ã€éŸ³ä¹ã€è¯­éŸ³å’Œæ€»ä½“å¹³å‡ç±»åˆ«ä¸Šå‡è¾¾åˆ°äº†æœ€é«˜å‡†ç¡®ç‡ï¼Œæ— è®ºæ˜¯åœ¨æµ‹è¯•é›†çš„å°å‹åˆ†å‰²è¿˜æ˜¯å…¨åˆ†å‰²ä¸­éƒ½è¡¨ç°ä¼˜å¼‚ã€‚é€šè¿‡å¯¹æ¨¡å‹æœ‰æ— éŸ³é¢‘çš„æµ‹è¯•ï¼Œå‘ç°GRPOçš„å¤§éƒ¨åˆ†æ€§èƒ½æå‡ä¸»è¦å½’åŠŸäºåŸºäºæ–‡æœ¬æ¨ç†çš„æ”¹è¿›ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå³ä½¿åœ¨åªæœ‰æ–‡æœ¬çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä¹Ÿèƒ½æœ‰æ•ˆæé«˜åŸºäºéŸ³é¢‘çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Omni-R1é€šè¿‡å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ åœ¨éŸ³é¢‘é—®ç­”æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>Omni-R1åœ¨å£°éŸ³ã€éŸ³ä¹ã€è¯­éŸ³å’Œæ€»ä½“å¹³å‡ç±»åˆ«ä¸Šè¾¾åˆ°äº†æœ€é«˜å‡†ç¡®ç‡ã€‚</li>
<li>åœ¨æµ‹è¯•æ¨¡å‹æœ‰æ— éŸ³é¢‘çš„æƒ…å†µä¸‹ï¼Œå‘ç°å¤§éƒ¨åˆ†æ€§èƒ½æå‡æ¥æºäºåŸºäºæ–‡æœ¬æ¨ç†çš„æ”¹è¿›ã€‚</li>
<li>GRPOç®—æ³•å¯¹æ¨¡å‹æ€§èƒ½çš„æå‡èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚</li>
<li>åœ¨åªæœ‰æ–‡æœ¬çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒä¹Ÿèƒ½æœ‰æ•ˆæé«˜åŸºäºéŸ³é¢‘çš„æ€§èƒ½ã€‚</li>
<li>Omni-R1çš„æˆåŠŸéªŒè¯äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘é¢†åŸŸçš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-780f7c1b8537015d7064e8633f626424.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-693681007963562b5f998521aabf45bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df550e399e9fab0fd375ddb7ee33c7c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c752319722c0611336b63342a14eebde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec4f26bf2a25baa4e19db5a928423d49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4e57ab730fa3529100905cddebd16e2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LEGO-Puzzles-How-Good-Are-MLLMs-at-Multi-Step-Spatial-Reasoning"><a href="#LEGO-Puzzles-How-Good-Are-MLLMs-at-Multi-Step-Spatial-Reasoning" class="headerlink" title="LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?"></a>LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?</h2><p><strong>Authors:Kexian Tang, Junyao Gao, Yanhong Zeng, Haodong Duan, Yanan Sun, Zhening Xing, Wenran Liu, Kaifeng Lyu, Kai Chen</strong></p>
<p>Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multimodal Large Language Models (MLLMs) have acquired this fundamental capability, we introduce LEGO-Puzzles, a scalable benchmark designed to evaluate both spatial understanding and sequential reasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100 carefully curated visual question-answering (VQA) samples spanning 11 distinct tasks, ranging from basic spatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of 20 state-of-the-art MLLMs and uncover significant limitations in their spatial reasoning capabilities: even the most powerful MLLMs can answer only about half of the test cases, whereas human participants achieve over 90% accuracy. Furthermore, based on LEGO-Puzzles, we design generation tasks to investigate whether MLLMs can transfer their spatial understanding and reasoning abilities to image generation. Our experiments show that only GPT-4o and Gemini-2.0-Flash exhibit a limited ability to follow these instructions, while other MLLMs either replicate the input image or generate completely irrelevant outputs. Overall, LEGO-Puzzles exposes critical deficiencies in existing MLLMsâ€™ spatial understanding and sequential reasoning capabilities, and underscores the need for further advancements in multimodal spatial reasoning. </p>
<blockquote>
<p>å¤šæ­¥ç©ºé—´æ¨ç†æ¶‰åŠç†è§£å’Œæ¨ç†è·¨å¤šä¸ªè¿ç»­æ­¥éª¤çš„ç©ºé—´å…³ç³»ï¼Œè¿™å¯¹äºè§£å†³å¤æ‚ç°å®ä¸–ç•Œåº”ç”¨è‡³å…³é‡è¦ï¼Œä¾‹å¦‚æœºå™¨äººæ“ä½œã€è‡ªä¸»å¯¼èˆªå’Œè‡ªåŠ¨åŒ–ç»„è£…ã€‚ä¸ºäº†è¯„ä¼°å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è·å–è¿™ç§åŸºæœ¬èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¹é«˜æ‹¼å›¾ï¼ˆLEGO-Puzzlesï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡åŸºäºä¹é«˜çš„ä»»åŠ¡æ¥è¯„ä¼°MLLMsçš„ç©ºé—´ç†è§£å’Œé¡ºåºæ¨ç†èƒ½åŠ›ã€‚ä¹é«˜æ‹¼å›¾åŒ…å«1100ä¸ªç²¾å¿ƒç­–åˆ’çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ ·æœ¬ï¼Œæ¶µç›–11ä¸ªä¸åŒçš„ä»»åŠ¡ï¼Œä»åŸºæœ¬ç©ºé—´ç†è§£åˆ°å¤æ‚çš„å¤šæ­¥æ¨ç†ã€‚åŸºäºä¹é«˜æ‹¼å›¾ï¼Œæˆ‘ä»¬å¯¹20ä¸ªæœ€æ–°MLLMsè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶å‘ç°äº†å®ƒä»¬åœ¨ç©ºé—´æ¨ç†èƒ½åŠ›æ–¹é¢çš„é‡å¤§å±€é™æ€§ï¼šå³ä½¿æ˜¯æœ€å¼ºå¤§çš„MLLMä¹Ÿåªèƒ½å›ç­”å¤§çº¦ä¸€åŠçš„æµ‹è¯•ç”¨ä¾‹ï¼Œè€Œäººç±»å‚ä¸è€…çš„å‡†ç¡®ç‡è¶…è¿‡90%ã€‚æ­¤å¤–ï¼ŒåŸºäºä¹é«˜æ‹¼å›¾ï¼Œæˆ‘ä»¬è®¾è®¡äº†ç”Ÿæˆä»»åŠ¡æ¥è°ƒæŸ¥MLLMsæ˜¯å¦èƒ½å¤Ÿå°†ä»–ä»¬çš„ç©ºé—´ç†è§£å’Œæ¨ç†èƒ½åŠ›è½¬ç§»åˆ°å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåªæœ‰GPT-4oå’ŒGemini-2.0-Flashè¡¨ç°å‡ºæœ‰é™çš„èƒ½åŠ›æ¥éµå¾ªè¿™äº›æŒ‡ä»¤ï¼Œè€Œå…¶ä»–MLLMsè¦ä¹ˆå¤åˆ¶è¾“å…¥å›¾åƒï¼Œè¦ä¹ˆç”Ÿæˆå®Œå…¨ä¸ç›¸å…³çš„è¾“å‡ºã€‚æ€»ä½“è€Œè¨€ï¼Œä¹é«˜æ‹¼å›¾æ­ç¤ºäº†ç°æœ‰MLLMsåœ¨ç©ºé—´ç†è§£å’Œé¡ºåºæ¨ç†èƒ½åŠ›æ–¹é¢çš„å…³é”®ç¼ºé™·ï¼Œå¹¶å¼ºè°ƒäº†åœ¨å¤šæ¨¡æ€ç©ºé—´æ¨ç†æ–¹é¢éœ€è¦è¿›ä¸€æ­¥çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19990v2">PDF</a> 11 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>LEGO-Puzzlesæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç©ºé—´ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ã€‚å®ƒç”±åŒ…å«å¤šç§å¤æ‚ç©ºé—´ä»»åŠ¡çš„1100ä¸ªè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ ·æœ¬ç»„æˆã€‚è¯„ä¼°å‘ç°ï¼Œå½“å‰æœ€å…ˆè¿›çš„MLLMsåªèƒ½å›ç­”ä¸€åŠå·¦å³çš„é—®é¢˜ï¼Œè€Œäººç±»å‚ä¸è€…çš„å‡†ç¡®ç‡è¶…è¿‡90%ã€‚åŒæ—¶ï¼Œåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œä»…æœ‰GPT-4oå’ŒGemini-2.0-Flashèƒ½æœ‰é™åœ°éµå¾ªç©ºé—´ç†è§£å’Œæ¨ç†æŒ‡ä»¤ã€‚æ€»ä½“æ¥è¯´ï¼ŒLEGO-Puzzlesæ­ç¤ºäº†ç°æœ‰MLLMsåœ¨ç©ºé—´ç†è§£å’Œé¡ºåºæ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œå¹¶å¼ºè°ƒäº†è¿›ä¸€æ­¥æ”¹è¿›å¤šæ¨¡æ€ç©ºé—´æ¨ç†çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ­¥ç©ºé—´æ¨ç†å¯¹äºå¤„ç†ç°å®ä¸–ç•Œåº”ç”¨å¦‚æœºå™¨äººæ“æ§ã€è‡ªä¸»å¯¼èˆªå’Œè‡ªåŠ¨åŒ–è£…é…è‡³å…³é‡è¦ã€‚</li>
<li>LEGO-Puzzlesæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°MLLMsç©ºé—´ç†è§£å’Œé¡ºåºæ¨ç†èƒ½åŠ›çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„MLLMsåœ¨LEGO-Puzzlesæµ‹è¯•ä¸­åªèƒ½å›ç­”ä¸€åŠå·¦å³çš„é—®é¢˜ï¼Œè¡¨æ˜å…¶åœ¨ç©ºé—´æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>äººç±»å‚ä¸è€…åœ¨LEGO-Puzzlesæµ‹è¯•ä¸­çš„å‡†ç¡®ç‡è¶…è¿‡90%ã€‚</li>
<li>åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œä»…æœ‰éƒ¨åˆ†MLLMsï¼ˆå¦‚GPT-4oå’ŒGemini-2.0-Flashï¼‰èƒ½éµå¾ªç©ºé—´ç†è§£å’Œæ¨ç†æŒ‡ä»¤ã€‚</li>
<li>LEGO-Puzzlesæ­ç¤ºäº†ç°æœ‰MLLMsåœ¨ç©ºé—´ç†è§£å’Œæ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19990">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-841a55be744b6ea05b0d9f79c4403e76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84fcd9ff01216070bef53808223ccd03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6bc7c3af0ae9ad83f7adab96bb82ffa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d092568a1e77cdd4984b63b2a5fce873.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VecTrans-Enhancing-Compiler-Auto-Vectorization-through-LLM-Assisted-Code-Transformations"><a href="#VecTrans-Enhancing-Compiler-Auto-Vectorization-through-LLM-Assisted-Code-Transformations" class="headerlink" title="VecTrans: Enhancing Compiler Auto-Vectorization through LLM-Assisted   Code Transformations"></a>VecTrans: Enhancing Compiler Auto-Vectorization through LLM-Assisted   Code Transformations</h2><p><strong>Authors:Zhongchun Zheng, Kan Wu, Long Cheng, Lu Li, Rodrigo C. O. Rocha, Tianyi Liu, Wei Wei, Jianjiang Zeng, Xianwei Zhang, Yaoqing Gao</strong></p>
<p>Auto-vectorization is a fundamental optimization for modern compilers to exploit SIMD parallelism. However, state-of-the-art approaches still struggle to handle intricate code patterns, often requiring manual hints or domain-specific expertise. Large language models (LLMs), with their ability to capture intricate patterns, provide a promising solution, yet their effective application in compiler optimizations remains an open challenge due to issues such as hallucinations and a lack of domain-specific reasoning. In this paper, we present VecTrans, a novel framework that leverages LLMs to enhance compiler-based code vectorization. VecTrans first employs compiler analysis to identify potentially vectorizable code regions. It then utilizes an LLM to refactor these regions into patterns that are more amenable to the compilers auto-vectorization. To ensure semantic correctness, VecTrans further integrates a hybrid validation mechanism at the intermediate representation (IR) level. With the above efforts, VecTrans combines the adaptability of LLMs with the precision of compiler vectorization, thereby effectively opening up the vectorization opportunities. experimental results show that among all TSVC functions unvectorizable by GCC, ICC, Clang, and BiSheng Compiler, VecTrans achieves an geomean speedup of 1.77x and successfully vectorizes 24 of 51 test cases. This marks a significant advancement over state-of-the-art approaches while maintaining a cost efficiency of $0.012 per function optimization for LLM API usage. </p>
<blockquote>
<p>è‡ªåŠ¨çŸ¢é‡åŒ–æ˜¯ç°ä»£ç¼–è¯‘å™¨åˆ©ç”¨SIMDå¹¶è¡Œæ€§çš„åŸºæœ¬ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„æŠ€æœ¯åœ¨å¤„ç†å¤æ‚çš„ä»£ç æ¨¡å¼æ—¶ä»é¢ä¸´å›°éš¾ï¼Œé€šå¸¸éœ€è¦æ‰‹åŠ¨æç¤ºæˆ–ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰æ•æ‰å¤æ‚æ¨¡å¼çš„èƒ½åŠ›ï¼Œæä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç”±äºå¹»æƒ³å’Œç¼ºä¹ç‰¹å®šé¢†åŸŸçš„æ¨ç†ç­‰é—®é¢˜ï¼Œå®ƒä»¬åœ¨ç¼–è¯‘å™¨ä¼˜åŒ–ä¸­çš„æœ‰æ•ˆåº”ç”¨ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†VecTransï¼Œä¸€ä¸ªåˆ©ç”¨LLMå¢å¼ºåŸºäºç¼–è¯‘å™¨çš„ä»£ç çŸ¢é‡åŒ–çš„æ–°å‹æ¡†æ¶ã€‚VecTransé¦–å…ˆä½¿ç”¨ç¼–è¯‘å™¨åˆ†ææ¥è¯†åˆ«å¯èƒ½å¯å‘é‡åŒ–çš„ä»£ç åŒºåŸŸã€‚ç„¶åï¼Œå®ƒåˆ©ç”¨LLMå°†è¿™äº›åŒºåŸŸé‡æ„ä¸ºæ›´æ˜“äºç¼–è¯‘å™¨è‡ªåŠ¨çŸ¢é‡åŒ–çš„æ¨¡å¼ã€‚ä¸ºäº†ç¡®ä¿è¯­ä¹‰æ­£ç¡®æ€§ï¼ŒVecTransè¿›ä¸€æ­¥åœ¨ä¸­é—´è¡¨ç¤ºï¼ˆIRï¼‰çº§åˆ«é›†æˆäº†ä¸€ç§æ··åˆéªŒè¯æœºåˆ¶ã€‚é€šè¿‡ä»¥ä¸ŠåŠªåŠ›ï¼ŒVecTransç»“åˆäº†LLMçš„é€‚åº”æ€§å’Œç¼–è¯‘å™¨çŸ¢é‡åŒ–çš„ç²¾ç¡®æ€§ï¼Œä»è€Œæœ‰æ•ˆåœ°å¼€è¾Ÿäº†çŸ¢é‡åŒ–æœºä¼šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ‰€æœ‰ä¸è¢«GCCã€ICCã€Clangå’ŒBiShengç¼–è¯‘å™¨çŸ¢é‡åŒ–çš„TSVCå‡½æ•°ä¸­ï¼ŒVecTranså®ç°äº†1.77å€çš„å‡ ä½•å¹³å‡åŠ é€Ÿï¼Œå¹¶æˆåŠŸçŸ¢é‡åŒ–äº†51ä¸ªæµ‹è¯•ç”¨ä¾‹ä¸­çš„24ä¸ªã€‚è¿™æ ‡å¿—ç€åœ¨ä¿æŒæˆæœ¬æ•ˆç›Šçš„åŒæ—¶ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„æŠ€æœ¯å–å¾—äº†é‡å¤§è¿›å±•ï¼Œæ¯ä¸ªå‡½æ•°ä¼˜åŒ–çš„LLM APIä½¿ç”¨æˆæœ¬ä¸º0.012ç¾å…ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19449v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†VecTransæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºåŸºäºç¼–è¯‘å™¨çš„ä»£ç å‘é‡åŒ–ã€‚VecTransé¦–å…ˆé€šè¿‡ç¼–è¯‘å™¨åˆ†æè¯†åˆ«æ½œåœ¨çš„å¯å‘é‡åŒ–ä»£ç åŒºåŸŸï¼Œç„¶åä½¿ç”¨LLMå¯¹è¿™äº›åŒºåŸŸè¿›è¡Œé‡æ„ï¼Œä»¥ä¾¿ç¼–è¯‘å™¨è‡ªåŠ¨è¿›è¡Œå‘é‡åŒ–ã€‚ä¸ºç¡®ä¿è¯­ä¹‰æ­£ç¡®æ€§ï¼ŒVecTransåœ¨ä¸­é—´è¡¨ç¤ºï¼ˆIRï¼‰çº§åˆ«è¿›ä¸€æ­¥é›†æˆäº†æ··åˆéªŒè¯æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVecTransåœ¨æ‰€æœ‰æœªè¢«GCCã€ICCã€Clangå’ŒBiShengç¼–è¯‘å™¨å‘é‡åŒ–çš„TSVCå‡½æ•°ä¸­å®ç°äº†å¹³å‡åŠ é€Ÿæ¯”æé«˜1.77å€ï¼ŒæˆåŠŸå‘é‡åŒ–24ä¸ªæµ‹è¯•æ¡ˆä¾‹ä¸­çš„51ä¸ªã€‚ç›¸è¾ƒäºå½“å‰æœ€ä½³æ–¹æ³•ï¼Œè¿™ä¸€è¿›å±•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶ä¸”æ¯æ¬¡å‡½æ•°ä¼˜åŒ–çš„LLM APIä½¿ç”¨æˆæœ¬ä¸ºæ¯äººèŠ‚çœç¾å…ƒçš„è´¹ç”¨æˆæœ¬è¾¾åˆ°æ§åˆ¶çš„ç›®æ ‡æ ‡å‡†æ°´å‡†æ¯äººä½å·¥æ—¶ä»˜å‡ºè¶³å¤Ÿå»‰ä»·çš„å¼€æ”¯çº§åˆ«æœ€ä½äººåŠ›æˆæœ¬æ”¯å‡ºæ•ˆç‡ä¹Ÿæé«˜ã€‚æ€»ä½“è€Œè¨€ï¼ŒVecTransç»“åˆäº†LLMçš„é€‚åº”æ€§å’Œç¼–è¯‘å™¨å‘é‡åŒ–çš„ç²¾ç¡®æ€§ï¼Œæœ‰æ•ˆå¼€å¯äº†å‘é‡åŒ–çš„å¯èƒ½æ€§ã€‚å®ƒä¸ä»…å¯¹æ”¹å–„ç¼–ç¨‹æ•ˆç‡å’Œè½¯ä»¶æ€§èƒ½æœ‰é‡è¦æ„ä¹‰ï¼Œè€Œä¸”å…·æœ‰å·¨å¤§çš„åº”ç”¨æ½œåŠ›ã€‚è¿™é¡¹ç ”ç©¶å¯¹äºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¼–è¯‘å™¨ä¼˜åŒ–æ–¹é¢çš„åº”ç”¨å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥å’Œåº”ç”¨çš„æ¨å¹¿ï¼Œæˆ‘ä»¬æœŸå¾…çœ‹åˆ°æ›´å¤šå…³äºæ­¤ç±»æŠ€æœ¯çš„åˆ›æ–°å’Œæ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b3caf4bb472c2d7fe822af26d984385d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7775444ecbce5e131129bbff6a5401c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7380c7c050b27627e9f76f903025e3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-183e6d73d4cf40bbfcfce4fc2d227003.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8826d3138ffef65662fb28fcd86eb4a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50b7154d2425688685afe2923c7625a9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SPILL-Domain-Adaptive-Intent-Clustering-based-on-Selection-and-Pooling-with-Large-Language-Models"><a href="#SPILL-Domain-Adaptive-Intent-Clustering-based-on-Selection-and-Pooling-with-Large-Language-Models" class="headerlink" title="SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling   with Large Language Models"></a>SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling   with Large Language Models</h2><p><strong>Authors:I-Fan Lin, Faegheh Hasibi, Suzan Verberne</strong></p>
<p>In this paper, we propose Selection and Pooling with Large Language Models (SPILL), an intuitive and domain-adaptive method for intent clustering without fine-tuning. Existing embeddings-based clustering methods rely on a few labeled examples or unsupervised fine-tuning to optimize results for each new dataset, which makes them less generalizable to multiple datasets. Our goal is to make these existing embedders more generalizable to new domain datasets without further fine-tuning. Inspired by our theoretical derivation and simulation results on the effectiveness of sampling and pooling techniques, we view the clustering task as a small-scale selection problem. A good solution to this problem is associated with better clustering performance. Accordingly, we propose a two-stage approach: First, for each utterance (referred to as the seed), we derive its embedding using an existing embedder. Then, we apply a distance metric to select a pool of candidates close to the seed. Because the embedder is not optimized for new datasets, in the second stage, we use an LLM to further select utterances from these candidates that share the same intent as the seed. Finally, we pool these selected candidates with the seed to derive a refined embedding for the seed. We found that our method generally outperforms directly using an embedder, and it achieves comparable results to other state-of-the-art studies, even those that use much larger models and require fine-tuning, showing its strength and efficiency. Our results indicate that our method enables existing embedders to be further improved without additional fine-tuning, making them more adaptable to new domain datasets. Additionally, viewing the clustering task as a small-scale selection problem gives the potential of using LLMs to customize clustering tasks according to the userâ€™s goals. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é€‰æ‹©æ± åŒ–ï¼ˆSPILLï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç›´è§‚ä¸”é€‚ç”¨äºç‰¹å®šé¢†åŸŸçš„æ„å›¾èšç±»æ–¹æ³•ï¼Œæ— éœ€å¾®è°ƒã€‚ç°æœ‰çš„åŸºäºåµŒå…¥çš„èšç±»æ–¹æ³•ä¾èµ–äºå°‘é‡å¸¦æ ‡ç­¾çš„æ ·æœ¬æˆ–åŸºäºæ— ç›‘ç£å­¦ä¹ çš„å¾®è°ƒï¼Œä»¥ä¾¿ä¸ºæ¯æ–°æ•°æ®é›†ä¼˜åŒ–ç»“æœï¼Œè¿™å¯¼è‡´å®ƒä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„é€šç”¨æ€§è¾ƒå·®ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨æ— éœ€è¿›ä¸€æ­¥å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œä½¿ç°æœ‰åµŒå…¥æŠ€æœ¯æ›´å…·é€šç”¨æ€§ï¼Œä»¥é€‚åº”æ–°çš„é¢†åŸŸæ•°æ®é›†ã€‚é€šè¿‡æˆ‘ä»¬å¯¹é‡‡æ ·å’Œæ± æŠ€æœ¯çš„æœ‰æ•ˆæ€§è¿›è¡Œç†è®ºæ¨å¯¼å’Œæ¨¡æ‹ŸéªŒè¯å¾—åˆ°çš„å¯å‘ï¼Œæˆ‘ä»¬å°†èšç±»ä»»åŠ¡è§†ä¸ºä¸€ä¸ªå°è§„æ¨¡çš„é€‰æ‹©é—®é¢˜ã€‚è‰¯å¥½çš„è§£å†³æ–¹æ¡ˆä¸æ›´å¥½çš„èšç±»æ€§èƒ½ç›¸å…³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ–¹æ³•ï¼šé¦–å…ˆï¼Œé’ˆå¯¹æ¯ä¸ªå¥å­ï¼ˆç§°ä¸ºç§å­ï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨ç°æœ‰çš„åµŒå…¥æŠ€æœ¯å¾—åˆ°å…¶åµŒå…¥è¡¨ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è·ç¦»åº¦é‡é€‰æ‹©ä¸€ä¸ªä¸ç§å­æ¥è¿‘çš„å€™é€‰æ± ã€‚ç”±äºåµŒå…¥æŠ€æœ¯å¹¶ä¸æ˜¯é’ˆå¯¹æ–°æ•°æ®é›†è¿›è¡Œä¼˜åŒ–çš„ï¼Œåœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä»è¿™äº›å€™é€‰ä¸­è¿›ä¸€æ­¥é€‰æ‹©ä¸ç§å­å…·æœ‰ç›¸åŒæ„å›¾çš„å¥å­ã€‚æœ€åï¼Œæˆ‘ä»¬å°†è¿™äº›é€‰æ‹©çš„å€™é€‰ä¸ç§å­åˆå¹¶ï¼Œå¾—åˆ°ç§å­çš„ç²¾ç»†åµŒå…¥è¡¨ç¤ºã€‚æˆ‘ä»¬å‘ç°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šå¸¸ä¼˜äºç›´æ¥ä½¿ç”¨åµŒå…¥æŠ€æœ¯çš„æ–¹æ³•ï¼Œå¹¶ä¸”ä¸å…¶ä»–æœ€æ–°ç ”ç©¶çš„ç»“æœç›¸å½“ï¼Œå³ä½¿è¿™äº›ç ”ç©¶ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å¹¶éœ€è¦å¾®è°ƒã€‚è¿™æ˜¾ç¤ºäº†å…¶å¼ºå¤§å’Œé«˜æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–å¾®è°ƒçš„æƒ…å†µä¸‹è¿›ä¸€æ­¥æ”¹è¿›ç°æœ‰åµŒå…¥æŠ€æœ¯ï¼Œä½¿å…¶æ›´é€‚åº”æ–°çš„é¢†åŸŸæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œå°†èšç±»ä»»åŠ¡è§†ä¸ºå°è§„æ¨¡çš„é€‰æ‹©é—®é¢˜å…·æœ‰ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ ¹æ®ç”¨æˆ·ç›®æ ‡å®šåˆ¶èšç±»ä»»åŠ¡çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15351v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹è¾…åŠ©çš„æ„å›¾èšç±»æ–¹æ³•æ— éœ€å¾®è°ƒã€‚æœ¬æ–‡æå‡ºäº†åŸºäºé€‰æ‹©æ± åŒ–çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆSPILLï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´è§‚ä¸”é€‚åº”äºé¢†åŸŸéœ€æ±‚ï¼Œæ— éœ€å¾®è°ƒå³å¯è¿›è¡Œæ„å›¾èšç±»ã€‚é€šè¿‡å¯¹é‡‡æ ·å’Œæ± åŒ–æŠ€æœ¯çš„ç†è®ºæ¨å¯¼å’Œä»¿çœŸéªŒè¯ï¼Œå°†èšç±»ä»»åŠ¡è§†ä¸ºå°è§„æ¨¡é€‰æ‹©é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨ç°æœ‰åµŒå…¥å™¨å’Œå¤§è¯­è¨€æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæé«˜èšç±»æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¸€èˆ¬ä¼˜äºç›´æ¥ä½¿ç”¨åµŒå…¥å™¨çš„æ–¹æ³•ï¼Œä¸”ä¸ç°æœ‰çš„é¡¶å°–ç ”ç©¶æˆæœè¡¨ç°ç›¸å½“ã€‚æ­¤æ–¹æ³•çš„ä¼˜ç‚¹åœ¨äºå¯è¿›ä¸€æ­¥æé«˜ç°æœ‰åµŒå…¥å™¨çš„æ€§èƒ½è€Œæ— éœ€é¢å¤–çš„å¾®è°ƒï¼Œä½¿å…¶æ›´èƒ½é€‚åº”æ–°é¢†åŸŸæ•°æ®é›†ï¼Œå…·æœ‰å¹¿æ³›çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPILLæ–¹æ³•åŸºäºé€‰æ‹©æ± åŒ–æŠ€æœ¯ï¼Œé€šè¿‡ç†è®ºæ¨å¯¼å’Œä»¿çœŸéªŒè¯å®ç°æ— éœ€å¾®è°ƒè¿›è¡Œæ„å›¾èšç±»ã€‚</li>
<li>æå‡ºä¸€ç§ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œåˆ©ç”¨ç°æœ‰åµŒå…¥å™¨å’Œå¤§è¯­è¨€æ¨¡å‹çš„ä¼˜åŠ¿æé«˜èšç±»æ€§èƒ½ã€‚</li>
<li>SPILLæ–¹æ³•é€šè¿‡é€‰æ‹©æ¥è¿‘ç§å­çš„å€™é€‰é›†è¿›è¡Œæ± åŒ–ï¼Œè¿›ä¸€æ­¥æé«˜äº†ç§å­åµŒå…¥çš„ç²¾åº¦ã€‚</li>
<li>ä¸ç›´æ¥ä½¿ç”¨åµŒå…¥å™¨çš„æ–¹æ³•ç›¸æ¯”ï¼ŒSPILLæ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œä¸”ä¸é¡¶å°–ç ”ç©¶æˆæœè¡¨ç°ç›¸å½“ã€‚</li>
<li>SPILLæ–¹æ³•èƒ½è¿›ä¸€æ­¥æå‡ç°æœ‰åµŒå…¥å™¨çš„æ€§èƒ½ï¼Œæ— éœ€é¢å¤–å¾®è°ƒï¼Œå¯¹æ–°é¢†åŸŸæ•°æ®é›†çš„é€‚åº”èƒ½åŠ›æ›´å¼ºã€‚</li>
<li>é€šè¿‡å°†èšç±»ä»»åŠ¡è§†ä¸ºå°è§„æ¨¡é€‰æ‹©é—®é¢˜ï¼ŒSPILLæ–¹æ³•å…·æœ‰å¹¿æ³›çš„å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6aeccc9c2ce253057eda035b57b2c43d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccfa5abf3a60f7ed28a4aeab64e5b23e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4d6d1030b2f7fc7158d452970eebfcb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Implicit-Reasoning-in-Transformers-is-Reasoning-through-Shortcuts"><a href="#Implicit-Reasoning-in-Transformers-is-Reasoning-through-Shortcuts" class="headerlink" title="Implicit Reasoning in Transformers is Reasoning through Shortcuts"></a>Implicit Reasoning in Transformers is Reasoning through Shortcuts</h2><p><strong>Authors:Tianhe Lin, Jian Xie, Siyu Yuan, Deqing Yang</strong></p>
<p>Test-time compute is emerging as a new paradigm for enhancing language modelsâ€™ complex multi-step reasoning capabilities, as demonstrated by the success of OpenAIâ€™s o1 and o3, as well as DeepSeekâ€™s R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on unfixed-pattern data tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization. </p>
<blockquote>
<p>æµ‹è¯•æ—¶çš„è®¡ç®—æ­£æˆä¸ºä¸€ç§æ–°å…´çš„æ¨¡å¼ï¼Œç”¨äºå¢å¼ºè¯­è¨€æ¨¡å‹çš„å¤æ‚å¤šæ­¥æ¨ç†èƒ½åŠ›ï¼ŒOpenAIçš„o1å’Œo3ä»¥åŠDeepSeekçš„R1çš„æˆåŠŸä¹Ÿè¯æ˜äº†è¿™ä¸€ç‚¹ã€‚ä¸æµ‹è¯•æ—¶çš„æ˜¾å¼æ¨ç†ç›¸æ¯”ï¼Œéšå¼æ¨ç†çš„æ¨ç†æ•ˆç‡æ›´é«˜ï¼Œäº§ç”Ÿçš„æ ‡è®°æ›´å°‘ã€‚ç„¶è€Œï¼Œä¸ºä»€ä¹ˆé«˜çº§æ¨ç†èƒ½åŠ›æ²¡æœ‰åœ¨éšå¼æ¨ç†é£æ ¼ä¸­å‡ºç°å‘¢ï¼Ÿåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»é›¶å¼€å§‹è®­ç»ƒGPT-2ï¼Œç›®æ ‡æ˜¯åœ¨ç­›é€‰è¿‡çš„å¤šæ­¥æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šï¼Œå¹¶é€šè¿‡åˆ†æå®éªŒæ¥ç ”ç©¶è¯­è¨€æ¨¡å‹åœ¨å¤šæ­¥ä»»åŠ¡ä¸­è¿›è¡Œéšå¼æ¨ç†çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†ä»¥ä¸‹å‡ ç‚¹ï¼š1ï¼‰è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡éšå¼æ¨ç†è¿›è¡Œé€æ­¥æ¨ç†ï¼Œå¹¶åœ¨é¢†åŸŸå†…å¤–æµ‹è¯•ä¸­å®ç°é«˜å‡†ç¡®ç‡ã€‚ä½†è¿™ç§èƒ½åŠ›åªåœ¨å›ºå®šæ¨¡å¼çš„æ•°æ®è®­ç»ƒä¸‹æ‰ä¼šå‡ºç°ã€‚2ï¼‰ç›¸åï¼Œä»éå›ºå®šæ¨¡å¼æ•°æ®è®­ç»ƒä¸­å‡ºç°çš„éšå¼æ¨ç†èƒ½åŠ›å¾€å¾€è¿‡äºé€‚åº”ç‰¹å®šæ¨¡å¼ï¼Œæ— æ³•è¿›ä¸€æ­¥æ¨å¹¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸€å±€é™æ€§åœ¨æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä¹Ÿè¢«è§‚å¯Ÿåˆ°ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œè¯­è¨€æ¨¡å‹é€šè¿‡æ·å¾„å­¦ä¹ è·å¾—éšå¼æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å…·æœ‰ç›¸ä¼¼æ¨¡å¼çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07604v3">PDF</a> ACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†æµ‹è¯•æ—¶è®¡ç®—ï¼ˆtest-time computeï¼‰è¿™ä¸€æ–°å…´èŒƒå¼å¯¹å¢å¼ºè¯­è¨€æ¨¡å‹å¤æ‚å¤šæ­¥éª¤æ¨ç†èƒ½åŠ›çš„ä½œç”¨ã€‚é€šè¿‡å®éªŒåˆ†æï¼Œå‘ç°è¯­è¨€æ¨¡å‹åœ¨å›ºå®šæ¨¡å¼æ•°æ®è®­ç»ƒä¸‹å¯é€šè¿‡éšå¼æ¨ç†å®Œæˆæ­¥éª¤å¼æ¨ç†ï¼Œä¸”åœ¨ä¸åŒé¢†åŸŸæµ‹è¯•ä¸­å‡è¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ã€‚ç„¶è€Œï¼Œåœ¨éæ ‡å®šæ¨¡å¼æ•°æ®è®­ç»ƒä¸‹ï¼Œéšå¼æ¨ç†èƒ½åŠ›å¾€å¾€è¿‡äºæ‹Ÿåˆç‰¹å®šæ¨¡å¼ï¼Œç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶è®¡ç®—æ˜¯ä¸€ç§æ–°å…´èŒƒå¼ï¼Œç”¨äºå¢å¼ºè¯­è¨€æ¨¡å‹çš„å¤æ‚å¤šæ­¥éª¤æ¨ç†èƒ½åŠ›ã€‚</li>
<li>OpenAIçš„o1å’Œo3ä»¥åŠDeepSeekçš„R1çš„æˆåŠŸæ¼”ç¤ºäº†è¿™ä¸€ç‚¹ã€‚</li>
<li>éšå¼æ¨ç†ç›¸æ¯”æ˜¾å¼æ¨ç†æ›´åŠ æ¨ç†é«˜æ•ˆï¼Œéœ€è¦ç”Ÿæˆçš„æ ‡è®°æ›´å°‘ã€‚</li>
<li>è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨å›ºå®šæ¨¡å¼æ•°æ®è®­ç»ƒä¸‹é€šè¿‡éšå¼æ¨ç†è¿›è¡Œé€æ­¥æ¨ç†ï¼Œå¹¶åœ¨åŸŸå†…å’ŒåŸŸå¤–æµ‹è¯•ä¸­è·å¾—é«˜å‡†ç¡®ç‡ã€‚</li>
<li>åœ¨éæ ‡å®šæ¨¡å¼æ•°æ®è®­ç»ƒä¸‹ï¼Œéšå¼æ¨ç†èƒ½åŠ›å€¾å‘äºè¿‡åº¦æ‹Ÿåˆç‰¹å®šæ¨¡å¼ï¼Œç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯­è¨€æ¨¡å‹çš„éšå¼æ¨ç†èƒ½åŠ›æ˜¯é€šè¿‡â€œæ·å¾„å­¦ä¹ â€è·å¾—çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd9cc5d0151a74398329091b454dc2c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1af07f45cdd44186f18cfa6060a2a5db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee67fc939e16c689e91575ead8b3d269.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b98837df2adf1f3400f236f0a3c66ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af4f0939d8ec9f0ff1a085362aacaf72.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TESS-2-A-Large-Scale-Generalist-Diffusion-Language-Model"><a href="#TESS-2-A-Large-Scale-Generalist-Diffusion-Language-Model" class="headerlink" title="TESS 2: A Large-Scale Generalist Diffusion Language Model"></a>TESS 2: A Large-Scale Generalist Diffusion Language Model</h2><p><strong>Authors:Jaesung Tae, Hamish Ivison, Sachin Kumar, Arman Cohan</strong></p>
<p>We introduce TESS 2, a general instruction-following diffusion language model that outperforms contemporary instruction-tuned diffusion models, as well as matches and sometimes exceeds strong autoregressive (AR) models. We train TESS 2 by first adapting a strong AR model via continued pretraining with the usual cross-entropy as diffusion loss, and then performing further instruction tuning. We find that adaptation training as well as the choice of the base model is crucial for training good instruction-following diffusion models. We further propose reward guidance, a novel and modular inference-time guidance procedure to align model outputs without needing to train the underlying model. Finally, we show that TESS 2 further improves with increased inference-time compute, highlighting the utility of diffusion LMs in having fine-grained controllability over the amount of compute used at inference time. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/hamishivi/tess-2">https://github.com/hamishivi/tess-2</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†TESS 2ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æŒ‡ä»¤éµå¾ªæ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œå®ƒè¶…è¶Šäº†å½“ä»£æŒ‡ä»¤ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ï¼Œä¸å¼ºå¤§çš„è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ç›¸åŒ¹é…ï¼Œæœ‰æ—¶ç”šè‡³è¶…è¿‡å®ƒä»¬ã€‚æˆ‘ä»¬é€šè¿‡é¦–å…ˆä½¿ç”¨å¼ºå¤§çš„ARæ¨¡å‹è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼Œä»¥å¸¸è§„çš„äº¤å‰ç†µä½œä¸ºæ‰©æ•£æŸå¤±ï¼Œç„¶åè¿›ä¸€æ­¥è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´æ¥è®­ç»ƒTESS 2ã€‚æˆ‘ä»¬å‘ç°é€‚åº”è®­ç»ƒä»¥åŠåŸºç¡€æ¨¡å‹çš„é€‰æ‹©å¯¹äºè®­ç»ƒè‰¯å¥½çš„æŒ‡ä»¤éµå¾ªæ‰©æ•£æ¨¡å‹è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†å¥–åŠ±æŒ‡å¯¼ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¨¡å—åŒ–æ¨ç†æ—¶é—´æŒ‡å¯¼ç¨‹åºï¼Œå¯ä»¥åœ¨ä¸éœ€è¦è®­ç»ƒåº•å±‚æ¨¡å‹çš„æƒ…å†µä¸‹å¯¹é½æ¨¡å‹è¾“å‡ºã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜TESS 2éšç€æ¨ç†æ—¶é—´çš„è®¡ç®—å¢åŠ å¯ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œè¿™çªæ˜¾äº†æ‰©æ•£LMåœ¨æ¨ç†æ—¶é—´ä½¿ç”¨çš„è®¡ç®—é‡æ–¹é¢å…·æœ‰ç²¾ç»†æ§åˆ¶çš„æœ‰ç”¨æ€§ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hamishivi/tess-2%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hamishivi/tess-2æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13917v2">PDF</a> ACL 2025 camera-ready</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†TESS 2ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨æŒ‡ä»¤éµå¾ªæ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œå®ƒè¶…è¶Šäº†å½“ä»£æŒ‡ä»¤è°ƒæ•´æ‰©æ•£æ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶å¯ä¸å¼ºå¤§çš„è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ç›¸åŒ¹æ•Œï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°æ›´ä½³ã€‚æˆ‘ä»¬é€šè¿‡å…ˆä½¿ç”¨å¸¸è§„äº¤å‰ç†µä½œä¸ºæ‰©æ•£æŸå¤±å¯¹å¼ºå¤§ARæ¨¡å‹è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼Œç„¶åè¿›è¡Œè¿›ä¸€æ­¥çš„æŒ‡ä»¤è°ƒæ•´æ¥è®­ç»ƒTESS 2ã€‚æˆ‘ä»¬å‘ç°é€‚åº”è®­ç»ƒä»¥åŠåŸºç¡€æ¨¡å‹çš„é€‰æ‹©å¯¹äºè®­ç»ƒè‰¯å¥½çš„æŒ‡ä»¤éµå¾ªæ‰©æ•£æ¨¡å‹è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†å¥–åŠ±æŒ‡å¯¼ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¨¡å—åŒ–æ¨ç†æ—¶é—´æŒ‡å¯¼ç¨‹åºï¼Œå¯åœ¨æ— éœ€è®­ç»ƒåº•å±‚æ¨¡å‹çš„æƒ…å†µä¸‹å¯¹é½æ¨¡å‹è¾“å‡ºã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†TESS 2éšç€æ¨ç†æ—¶é—´è®¡ç®—é‡çš„å¢åŠ è€Œè¿›ä¸€æ­¥æ”¹è¿›ï¼Œè¿™å‡¸æ˜¾äº†æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶é—´ä½¿ç”¨è®¡ç®—é‡çš„ç²¾ç»†æ§åˆ¶èƒ½åŠ›ã€‚æ¨¡å‹å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/hamishivi/tess-2%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hamishivi/tess-2è·å–ã€‚</a></p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>TESS 2æ˜¯ä¸€ç§é€šç”¨æŒ‡ä»¤éµå¾ªæ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œè¡¨ç°è¶…è¶Šå½“ä»£æŒ‡ä»¤è°ƒæ•´æ‰©æ•£æ¨¡å‹ï¼Œå¯ä¸å¼ºå¤§çš„è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ç›¸åŒ¹æ•Œã€‚</li>
<li>TESS 2é€šè¿‡æŒç»­é¢„è®­ç»ƒå’Œè¿›ä¸€æ­¥çš„æŒ‡ä»¤è°ƒæ•´è¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­é€‚åº”è®­ç»ƒå’ŒåŸºç¡€æ¨¡å‹çš„é€‰æ‹©æ˜¯å…³é”®ã€‚</li>
<li>æå‡ºäº†å¥–åŠ±æŒ‡å¯¼ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¨ç†æ—¶é—´æŒ‡å¯¼ç¨‹åºï¼Œå¯åœ¨æ— éœ€è®­ç»ƒåº•å±‚æ¨¡å‹çš„æƒ…å†µä¸‹å¯¹é½æ¨¡å‹è¾“å‡ºã€‚</li>
<li>TESS 2éšç€æ¨ç†æ—¶é—´è®¡ç®—é‡çš„å¢åŠ è¿›ä¸€æ­¥æ”¹è¿›ï¼Œä½“ç°äº†æ‰©æ•£è¯­è¨€æ¨¡å‹çš„ç²¾ç»†å¯æ§æ€§ã€‚</li>
<li>TESS 2çš„å‘å¸ƒå’Œä»£ç åº“å¯ç”¨äºè¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘ã€‚</li>
<li>æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸Šå…·æœ‰æ½œåœ¨ä¼˜åŠ¿ï¼Œæœªæ¥ç ”ç©¶å¯èƒ½è¿›ä¸€æ­¥ä¼˜åŒ–è¿™ç±»æ¨¡å‹çš„æ€§èƒ½å’Œç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13917">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1bddce529e69444655867983329a7558.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d32cf2d92496e77d73f1e898ef223e67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fc9ae3f047b99c66caa5117792359ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12b95e63f58a80cecd196e9d93e9a380.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GLTW-Joint-Improved-Graph-Transformer-and-LLM-via-Three-Word-Language-for-Knowledge-Graph-Completion"><a href="#GLTW-Joint-Improved-Graph-Transformer-and-LLM-via-Three-Word-Language-for-Knowledge-Graph-Completion" class="headerlink" title="GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language   for Knowledge Graph Completion"></a>GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language   for Knowledge Graph Completion</h2><p><strong>Authors:Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Cunliang Kong, Wenhao Li, Yufei Huang, Ye Tian, Xuantang Xiong, Lei Han, Maosong Sun</strong></p>
<p>Knowledge Graph Completion (KGC), which aims to infer missing or incomplete facts, is a crucial task for KGs. However, integrating the vital structural information of KGs into Large Language Models (LLMs) and outputting predictions deterministically remains challenging. To address this, we propose a new method called GLTW, which encodes the structural information of KGs and merges it with LLMs to enhance KGC performance. Specifically, we introduce an improved Graph Transformer (iGT) that effectively encodes subgraphs with both local and global structural information and inherits the characteristics of language model, bypassing training from scratch. Also, we develop a subgraph-based multi-classification training objective, using all entities within KG as classification objects, to boost learning efficiency.Importantly, we combine iGT with an LLM that takes KG language prompts as input.Our extensive experiments on various KG datasets show that GLTW achieves significant performance gains compared to SOTA baselines. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ—¨åœ¨æ¨æ–­ç¼ºå¤±æˆ–ä¸å®Œæ•´çš„äº‹å®ï¼Œæ˜¯çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå°†çŸ¥è¯†å›¾è°±ä¸­çš„é‡è¦ç»“æ„ä¿¡æ¯æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¹¶ç¡®å®šæ€§åœ°è¾“å‡ºé¢„æµ‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºGLTWï¼Œå®ƒå°†çŸ¥è¯†å›¾è°±çš„ç»“æ„ä¿¡æ¯ç¼–ç å¹¶ä¸LLMåˆå¹¶ï¼Œä»¥æé«˜KGCçš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ”¹è¿›çš„å›¾å˜æ¢å™¨ï¼ˆiGTï¼‰ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆç¼–ç åŒ…å«å±€éƒ¨å’Œå…¨å±€ç»“æ„ä¿¡æ¯çš„å­å›¾ï¼Œå¹¶ç»§æ‰¿è¯­è¨€æ¨¡å‹çš„ç‰¹æ€§ï¼Œæ— éœ€ä»é›¶å¼€å§‹è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºå­å›¾çš„å¤šå…ƒåˆ†ç±»è®­ç»ƒç›®æ ‡ï¼Œä½¿ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„æ‰€æœ‰å®ä½“ä½œä¸ºåˆ†ç±»å¯¹è±¡ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å°†iGTä¸æ¥å—çŸ¥è¯†å›¾è°±è¯­è¨€æç¤ºä½œä¸ºè¾“å…¥çš„LLMç›¸ç»“åˆã€‚æˆ‘ä»¬åœ¨å„ç§çŸ¥è¯†å›¾è°±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°åŸºçº¿ç›¸æ¯”ï¼ŒGLTWå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11471v4">PDF</a> Accepted by ACL2025(Findings)</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰ä¸­èå…¥çŸ¥è¯†å›¾è°±çš„ç»“æ„ä¿¡æ¯å¹¶ç¡®å®šæ€§è¾“å‡ºé¢„æµ‹çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºGLTWçš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ”¹è¿›çš„å›¾å˜å‹å™¨ï¼ˆiGTï¼‰æœ‰æ•ˆç¼–ç å­å›¾çš„ç»“æ„ä¿¡æ¯ï¼Œå¹¶ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆï¼Œæé«˜KGCæ€§èƒ½ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºå­å›¾çš„å¤šåˆ†ç±»è®­ç»ƒç›®æ ‡ï¼Œåˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„æ‰€æœ‰å®ä½“ä½œä¸ºåˆ†ç±»å¯¹è±¡ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚ç»“åˆè¯­è¨€æç¤ºçš„LLMä¸iGTç›¸ç»“åˆï¼Œåœ¨å„ç§çŸ¥è¯†å›¾è°±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGLTWç›¸æ¯”æœ€æ–°åŸºçº¿å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ˜¯çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨æ¨æ–­ç¼ºå¤±æˆ–ä¸å®Œæ•´çš„äº‹å®ã€‚</li>
<li>æ•´åˆçŸ¥è¯†å›¾è°±çš„ç»“æ„ä¿¡æ¯åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹¶ç¡®å®šæ€§è¾“å‡ºé¢„æµ‹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºçš„æ–°æ–¹æ³•GLTWé€šè¿‡æ”¹è¿›çš„å›¾å˜å‹å™¨ï¼ˆiGTï¼‰ç¼–ç å­å›¾çš„ç»“æ„ä¿¡æ¯ï¼Œå¹¶ä¸LLMç»“åˆï¼Œä»¥æé«˜KGCæ€§èƒ½ã€‚</li>
<li>iGTèƒ½æœ‰æ•ˆç¼–ç å­å›¾çš„å±€éƒ¨å’Œå…¨å±€ç»“æ„ä¿¡æ¯ï¼Œå¹¶ç»§æ‰¿è¯­è¨€æ¨¡å‹çš„ç‰¹æ€§ï¼Œæ— éœ€ä»é›¶å¼€å§‹è®­ç»ƒã€‚</li>
<li>å¼€å‘äº†ä¸€ç§åŸºäºå­å›¾çš„å¤šåˆ†ç±»è®­ç»ƒç›®æ ‡ï¼Œåˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„æ‰€æœ‰å®ä½“æé«˜å­¦ä¹ æ•ˆç‡ã€‚</li>
<li>GLTWç»“åˆè¯­è¨€æç¤ºçš„LLMä¸iGTã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5885156e6501ba8f5d2a1814cb714151.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69a3d32cc07e8061fb0609a024a152f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-605490cdb89c420172112db58baada0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c5075bf6580cd30eef7a3d15d161b14.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AtmosSci-Bench-Evaluating-the-Recent-Advance-of-Large-Language-Model-for-Atmospheric-Science"><a href="#AtmosSci-Bench-Evaluating-the-Recent-Advance-of-Large-Language-Model-for-Atmospheric-Science" class="headerlink" title="AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model   for Atmospheric Science"></a>AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model   for Atmospheric Science</h2><p><strong>Authors:Chenyue Li, Wen Deng, Mengqian Lu, Binhang Yuan</strong></p>
<p>The rapid advancements in large language models (LLMs), particularly in their reasoning capabilities, hold transformative potential for addressing complex challenges in atmospheric science. However, leveraging LLMs effectively in this domain requires a robust and comprehensive evaluation benchmark. Toward this end, we present AtmosSci-Bench, a novel benchmark designed to systematically assess LLM performance across five core categories of atmospheric science problems: hydrology, atmospheric dynamics, atmospheric physics, geophysics, and physical oceanography. AtmosSci-Bench features a dual-format design comprising both multiple-choice questions (MCQs) and open-ended questions (OEQs), enabling scalable automated evaluation alongside deeper analysis of conceptual understanding. We employ a template-based MCQ generation framework to create diverse, graduate-level problems with symbolic perturbation, while OEQs are used to probe open-ended reasoning. We conduct a comprehensive evaluation of representative LLMs, categorized into four groups: instruction-tuned models, advanced reasoning models, math-augmented models, and domain-specific climate models. Our analysis provides some interesting insights into the reasoning and problem-solving capabilities of LLMs in atmospheric science. We believe AtmosSci-Bench can serve as a critical step toward advancing LLM applications in climate service by offering a standard and rigorous evaluation framework. Our source codes are currently available at Our source codes are currently available at <a target="_blank" rel="noopener" href="https://github.com/Relaxed-System-Lab/AtmosSci-Bench">https://github.com/Relaxed-System-Lab/AtmosSci-Bench</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¶æ¨ç†èƒ½åŠ›æ–¹é¢ï¼Œä¸ºè§£å†³å¤§æ°”ç§‘å­¦ä¸­çš„å¤æ‚æŒ‘æˆ˜å¸¦æ¥äº†å˜é©æ€§æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¦åœ¨è¿™ä¸€é¢†åŸŸæœ‰æ•ˆåœ°åˆ©ç”¨LLMï¼Œéœ€è¦è¿›è¡Œç¨³å¥è€Œå…¨é¢çš„è¯„ä¼°åŸºå‡†æµ‹è¯•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AtmosSci-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°LLMåœ¨äº”ä¸ªå¤§æ°”ç§‘å­¦æ ¸å¿ƒç±»åˆ«çš„é—®é¢˜ä¸Šçš„æ€§èƒ½ï¼šæ°´æ–‡ã€å¤§æ°”åŠ¨åŠ›å­¦ã€å¤§æ°”ç‰©ç†å­¦ã€åœ°çƒç‰©ç†å­¦å’Œæµ·æ´‹ç‰©ç†å­¦ã€‚AtmosSci-Benché‡‡ç”¨åŒæ ¼å¼è®¾è®¡ï¼ŒåŒ…æ‹¬å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰å’Œå¼€æ”¾æ€§é—®é¢˜ï¼ˆOEQsï¼‰ï¼Œæ—¢èƒ½å¤Ÿè¿›è¡Œå¯æ‰©å±•çš„è‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œåˆèƒ½å¯¹æ¦‚å¿µç†è§£è¿›è¡Œæ·±å…¥åˆ†æã€‚æˆ‘ä»¬é‡‡ç”¨åŸºäºæ¨¡æ¿çš„å¤šé¡¹é€‰æ‹©é¢˜ç”Ÿæˆæ¡†æ¶ï¼Œåˆ›å»ºå…·æœ‰ç¬¦å·æ‰°åŠ¨æ€§çš„å¤šæ ·åŒ–ã€ç ”ç©¶ç”Ÿæ°´å¹³çš„é—®é¢˜ï¼Œè€Œå¼€æ”¾æ€§é—®é¢˜åˆ™ç”¨äºæ¢ç´¢å¼€æ”¾å¼æ¨ç†ã€‚æˆ‘ä»¬å¯¹å…·æœ‰ä»£è¡¨æ€§çš„LLMè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå°†å®ƒä»¬åˆ†ä¸ºå››ä¸ªç±»åˆ«ï¼šæŒ‡ä»¤è°ƒæ•´æ¨¡å‹ã€é«˜çº§æ¨ç†æ¨¡å‹ã€æ•°å­¦å¢å¼ºæ¨¡å‹å’Œç‰¹å®šé¢†åŸŸçš„æ°”å€™æ¨¡å‹ã€‚æˆ‘ä»¬çš„åˆ†æå¯¹LLMåœ¨å¤§æ°”ç§‘å­¦ä¸­çš„æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›æä¾›äº†ä¸€äº›æœ‰è¶£çš„è§è§£ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œé€šè¿‡æä¾›æ ‡å‡†å’Œä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ï¼ŒAtmosSci-Benchå°†æˆä¸ºæ¨åŠ¨LLMåœ¨æ°”å€™æœåŠ¡ä¸­åº”ç”¨çš„å…³é”®æ­¥éª¤ã€‚æˆ‘ä»¬çš„æºä»£ç ç›®å‰å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Relaxed-System-Lab/AtmosSci-Bench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Relaxed-System-Lab/AtmosSci-Benchè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01159v2">PDF</a> 33 pages, 4 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ°”è±¡ç§‘å­¦é¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä½†éœ€è¦ä¸€ä¸ªå…¨é¢è¯„ä¼°åŸºå‡†æ¥å®ç°å…¶æœ‰æ•ˆåº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†AtmosSci-BenchåŸºå‡†ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°LLMåœ¨äº”å¤§æ ¸å¿ƒæ°”è±¡ç§‘å­¦é—®é¢˜é¢†åŸŸçš„èƒ½åŠ›ã€‚AtmosSci-Benché‡‡ç”¨åŒé‡æ ¼å¼è®¾è®¡ï¼ŒåŒ…æ‹¬å¤šé¡¹é€‰æ‹©é¢˜å’Œå¼€æ”¾æ€§é—®é¢˜ï¼Œä»¥å®ç°è§„æ¨¡åŒ–è‡ªåŠ¨è¯„ä¼°å’Œå¯¹æ¦‚å¿µç†è§£çš„æ·±å…¥åˆ†æã€‚å¯¹ç°æœ‰LLMçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå…¶åœ¨æ°”è±¡ç§‘å­¦ä¸­çš„æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›é¢‡å…·è§è§£ã€‚æˆ‘ä»¬ç›¸ä¿¡AtmosSci-Benchèƒ½ä¸ºæ°”å€™æœåŠ¡ä¸­çš„LLMåº”ç”¨æä¾›ä¸€ä¸ªæ ‡å‡†å’Œä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ°”è±¡ç§‘å­¦é¢†åŸŸå…·æœ‰å˜é©æ€§æ½œåŠ›ã€‚</li>
<li>AtmosSci-Benchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°LLMåœ¨æ°”è±¡ç§‘å­¦é¢†åŸŸèƒ½åŠ›çš„å…¨æ–°åŸºå‡†ã€‚</li>
<li>AtmosSci-BenchåŒ…å«äº”å¤§æ ¸å¿ƒæ°”è±¡ç§‘å­¦é—®é¢˜é¢†åŸŸã€‚</li>
<li>è¯¥åŸºå‡†é‡‡ç”¨åŒé‡æ ¼å¼è®¾è®¡ï¼ŒåŒ…æ‹¬å¤šé¡¹é€‰æ‹©é¢˜å’Œå¼€æ”¾æ€§é—®é¢˜ã€‚</li>
<li>AtmosSci-Benchå¯å®ç°è§„æ¨¡åŒ–è‡ªåŠ¨è¯„ä¼°å’Œå¯¹æ¦‚å¿µç†è§£çš„æ·±å…¥åˆ†æã€‚</li>
<li>å¯¹ç°æœ‰LLMçš„è¯„ä¼°æ˜¾ç¤ºå‡ºå…¶åœ¨æ°”è±¡ç§‘å­¦ä¸­çš„æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-940bd2f10b9bc39da78948c2b699361d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ad630707b2182bffedec24bbc910247.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-572c2c9fea6f4377606d04356b8c7088.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Brain-network-science-modelling-of-sparse-neural-networks-enables-Transformers-and-LLMs-to-perform-as-fully-connected"><a href="#Brain-network-science-modelling-of-sparse-neural-networks-enables-Transformers-and-LLMs-to-perform-as-fully-connected" class="headerlink" title="Brain network science modelling of sparse neural networks enables   Transformers and LLMs to perform as fully connected"></a>Brain network science modelling of sparse neural networks enables   Transformers and LLMs to perform as fully connected</h2><p><strong>Authors:Yingtao Zhang, Diego Cerretti, Jialin Zhao, Wenjing Wu, Ziheng Liao, Umberto Michieli, Carlo Vittorio Cannistraci</strong></p>
<p>Dynamic sparse training (DST) can reduce the computational demands in ANNs, but faces difficulties in keeping peak performance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a brain-inspired method for growing connectivity in DST. CHT leverages a gradient-free, topology-driven link regrowth, which has shown ultra-sparse (less than 1% connectivity) advantage across various tasks compared to fully connected networks. Yet, CHT suffers two main drawbacks: (i) its time complexity is $O(Nd^3)$ - N node network size, d node degree - restricting it to ultra-sparse regimes. (ii) it selects top link prediction scores, which is inappropriate for the early training epochs, when the network presents unreliable connections. Here, we design the first brain-inspired network model - termed bipartite receptive field (BRF) - to initialize the connectivity of sparse artificial neural networks. We further introduce a GPU-friendly matrix-based approximation of CH link prediction, reducing complexity to $O(N^3)$. We introduce the Cannistraci-Hebb training soft rule (CHTs), which adopts a flexible strategy for sampling connections in both link removal and regrowth, balancing the exploration and exploitation of network topology. Additionally, we integrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results show that BRF offers performance advantages over previous network science models. Using 1% of connections, CHTs outperforms fully connected networks in MLP architectures on image classification tasks, compressing some networks to less than 30% of the nodes. Using 5% of the connections, CHTss outperforms fully connected networks in two Transformer-based machine translation tasks. Finally, at 30% connectivity, both CHTs and CHTss outperform other DST methods in language modeling and even exceed fully connected baselines in zero-shot tasks. </p>
<blockquote>
<p>åŠ¨æ€ç¨€ç–è®­ç»ƒï¼ˆDSTï¼‰å¯ä»¥å‡å°‘äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰çš„è®¡ç®—éœ€æ±‚ï¼Œä½†åœ¨ä¿æŒé«˜ç¨€ç–çº§åˆ«çš„å³°å€¼æ€§èƒ½æ—¶é¢ä¸´å›°éš¾ã€‚Cannistraci-Hebbè®­ç»ƒï¼ˆCHTï¼‰æ˜¯ä¸€ç§ç”¨äºåœ¨DSTä¸­å¢åŠ è¿æ¥çš„å¤§è„‘å¯å‘æ–¹æ³•ã€‚CHTåˆ©ç”¨æ— æ¢¯åº¦ã€æ‹“æ‰‘é©±åŠ¨çš„è¿æ¥å†ç”Ÿé•¿ï¼Œä¸å…¨è¿æ¥ç½‘ç»œç›¸æ¯”ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºè¶…ç¨€ç–ï¼ˆå°äº1%çš„è¿æ¥ç‡ï¼‰çš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼ŒCHTå­˜åœ¨ä¸¤ä¸ªä¸»è¦ç¼ºç‚¹ï¼šï¼ˆiï¼‰å…¶æ—¶é—´å¤æ‚åº¦ä¸ºO(Nd^3)ï¼ˆNä¸ºèŠ‚ç‚¹ç½‘ç»œå¤§å°ï¼Œdä¸ºèŠ‚ç‚¹åº¦ï¼‰ï¼Œé™åˆ¶å…¶åœ¨è¶…ç¨€ç–ç¯å¢ƒä¸­çš„ä½¿ç”¨ã€‚ï¼ˆiiï¼‰å®ƒé€‰æ‹©é¡¶éƒ¨è¿æ¥é¢„æµ‹åˆ†æ•°ï¼Œè¿™åœ¨ç½‘ç»œå‘ˆç°ä¸å¯é è¿æ¥çš„æ—©æœŸè®­ç»ƒå‘¨æœŸä¸­æ˜¯ä¸åˆé€‚çš„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è®¾è®¡äº†ç¬¬ä¸€ä¸ªå¤§è„‘å¯å‘ç½‘ç»œæ¨¡å‹â€”â€”ç§°ä¸ºäºŒéƒ¨æ¥æ”¶åœºï¼ˆBRFï¼‰â€”â€”æ¥åˆå§‹åŒ–ç¨€ç–äººå·¥ç¥ç»ç½‘ç»œçš„è¿æ¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†åŸºäºGPUå‹å¥½çš„çŸ©é˜µè¿‘ä¼¼CHè¿æ¥é¢„æµ‹ï¼Œå°†å¤æ‚åº¦é™ä½åˆ°O(N^3)ã€‚æˆ‘ä»¬å¼•å…¥äº†Cannistraci-Hebbè®­ç»ƒçš„è½¯è§„åˆ™ï¼ˆCHTsï¼‰ï¼Œå®ƒé‡‡ç”¨çµæ´»çš„ç­–ç•¥è¿›è¡Œè¿æ¥åˆ é™¤å’Œå†ç”Ÿé•¿çš„é‡‡æ ·ï¼Œå¹³è¡¡ç½‘ç»œæ‹“æ‰‘çš„æ¢ç´¢å’Œåˆ©ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†CHTsä¸Sigmoidé€æ¸å¯†åº¦è¡°å‡ï¼ˆCHTssï¼‰ç›¸ç»“åˆã€‚ç»éªŒç»“æœè¡¨æ˜ï¼ŒBRFåœ¨ä¹‹å‰çš„ç½‘ç»œç§‘å­¦æ¨¡å‹ä¸­è¡¨ç°å‡ºæ€§èƒ½ä¼˜åŠ¿ã€‚ä½¿ç”¨1%çš„è¿æ¥ï¼ŒCHTsåœ¨å¤šå±‚æ„ŸçŸ¥å™¨æ¶æ„çš„å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šä¼˜äºå…¨è¿æ¥ç½‘ç»œï¼Œå‹ç¼©æŸäº›ç½‘ç»œè‡³ä¸åˆ°30%çš„èŠ‚ç‚¹ã€‚ä½¿ç”¨5%çš„è¿æ¥ï¼ŒCHTssåœ¨åŸºäºTransformerçš„æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ä¼˜äºå…¨è¿æ¥ç½‘ç»œã€‚æœ€åï¼Œåœ¨30%çš„è¿æ¥ç‡ä¸‹ï¼ŒCHTså’ŒCHTssåœ¨è¯­è¨€å»ºæ¨¡ä¸­éƒ½ä¼˜äºå…¶ä»–DSTæ–¹æ³•ï¼Œç”šè‡³åœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­è¶…è¿‡å…¨è¿æ¥åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19107v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŠ¨æ€ç¨€ç–è®­ç»ƒï¼ˆDSTï¼‰å¯ä»¥é™ä½äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰çš„è®¡ç®—éœ€æ±‚ï¼Œä½†åœ¨ä¿æŒé«˜ç¨€ç–æ€§æ°´å¹³çš„å³°å€¼æ€§èƒ½æ–¹é¢é¢ä¸´å›°éš¾ã€‚Cannistraci-Hebbè®­ç»ƒï¼ˆCHTï¼‰æ˜¯ä¸€ç§å—å¤§è„‘å¯å‘çš„åœ¨DSTä¸­å¢åŠ è¿æ¥æ€§çš„æ–¹æ³•ã€‚CHTåˆ©ç”¨æ¢¯åº¦è‡ªç”±ã€æ‹“æ‰‘é©±åŠ¨çš„è¿æ¥å†ç”Ÿï¼Œä¸å…¨è¿æ¥ç½‘ç»œç›¸æ¯”ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºè¶…ç¨€ç–ï¼ˆå°‘äº1%çš„è¿æ¥ï¼‰çš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼ŒCHTå­˜åœ¨ä¸¤ä¸ªä¸»è¦ç¼ºç‚¹ï¼šä¸€æ˜¯å…¶æ—¶é—´å¤æ‚åº¦ä¸ºO(Nd^3)ï¼Œé™åˆ¶äº†å…¶åœ¨è¶…ç¨€ç–çŠ¶æ€ä¸‹çš„åº”ç”¨ï¼›äºŒæ˜¯åœ¨ç½‘ç»œå‘ˆç°ä¸å¯é è¿æ¥æ—¶ï¼Œé€‰æ‹©é¡¶éƒ¨é“¾æ¥é¢„æµ‹åˆ†æ•°æ˜¯ä¸æ°å½“çš„ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†é¦–ä¸ªå—å¤§è„‘å¯å‘çš„ç½‘ç»œæ¨¡å‹â€”â€”äºŒåˆ†æ¥æ”¶åœºï¼ˆBRFï¼‰ï¼Œä»¥åˆå§‹åŒ–ç¨€ç–äººå·¥ç¥ç»ç½‘ç»œçš„è¿æ¥æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†CHé“¾æ¥é¢„æµ‹çš„GPUå‹å¥½çŸ©é˜µè¿‘ä¼¼æ–¹æ³•ï¼Œå°†å¤æ‚åº¦é™ä½åˆ°O(N^3)ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†Cannistraci-Hebbè®­ç»ƒè½¯è§„åˆ™ï¼ˆCHTsï¼‰ï¼Œé‡‡ç”¨çµæ´»çš„ç­–ç•¥è¿›è¡Œé“¾æ¥çš„åˆ é™¤å’Œå†ç”Ÿé‡‡æ ·ï¼Œå¹³è¡¡ç½‘ç»œæ‹“æ‰‘çš„æ¢ç´¢ä¸åˆ©ç”¨ã€‚å°†CHTsä¸Sigmoidé€æ­¥è¡°å‡ï¼ˆCHTsSï¼‰ç›¸ç»“åˆåï¼Œå®è¯ç»“æœè¡¨æ˜BRFä¼˜äºä¹‹å‰çš„ç½‘ç»œç§‘å­¦æ¨¡å‹ã€‚åœ¨ä»…ä½¿ç”¨1%çš„è¿æ¥æƒ…å†µä¸‹ï¼ŒCHTsåœ¨å¤šå±‚æ„ŸçŸ¥å™¨æ¶æ„ä¸Šçš„å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºè¶…è¶Šå…¨è¿æ¥ç½‘ç»œçš„ä¼˜åŠ¿ï¼Œå‹ç¼©éƒ¨åˆ†ç½‘ç»œè‡³ä¸åˆ°30%çš„èŠ‚ç‚¹ã€‚ä½¿ç”¨5%çš„è¿æ¥æ—¶ï¼ŒCHTsSåœ¨åŸºäºTransformerçš„æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ä¼˜äºå…¨è¿æ¥ç½‘ç»œã€‚æœ€åï¼Œåœ¨30%çš„è¿æ¥æ€§ä¸‹ï¼ŒCHTså’ŒCHTsSåœ¨è¯­è¨€å»ºæ¨¡ä¸­éƒ½ä¼˜äºå…¶ä»–DSTæ–¹æ³•ï¼Œç”šè‡³åœ¨é›¶å°„å‡»ä»»åŠ¡ä¸­è¶…è¿‡å…¨è¿æ¥åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DSTèƒ½æœ‰æ•ˆé™ä½ANNçš„è®¡ç®—éœ€æ±‚ï¼Œä½†åœ¨é«˜ç¨€ç–æ€§æ—¶é¢ä¸´æ€§èƒ½æŒ‘æˆ˜ã€‚</li>
<li>CHTæ˜¯ä¸€ç§å—å¤§è„‘å¯å‘çš„è®­ç»ƒæ–¹å¼ï¼Œé€‚ç”¨äºDSTä¸­çš„è¿æ¥å¢é•¿ã€‚</li>
<li>CHTå…·æœ‰ä¸¤ä¸ªä¸»è¦ç¼ºç‚¹ï¼šé«˜æ—¶é—´å¤æ‚åº¦å’Œä¸é€‚å½“çš„æ—©æœŸè®­ç»ƒè¿æ¥é€‰æ‹©ã€‚</li>
<li>BRFæ¨¡å‹è¢«è®¾è®¡æ¥è§£å†³CHTçš„é—®é¢˜ï¼Œé€šè¿‡åˆå§‹åŒ–ç¨€ç–ANNçš„è¿é€šæ€§ã€‚</li>
<li>å¼•å…¥CHTè½¯è§„åˆ™å’ŒSigmoidé€æ­¥è¡°å‡ç­–ç•¥ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>BRFå’ŒCHTsåœ¨å›¾åƒåˆ†ç±»å’Œè¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b37a8cf7b390bb4a05303685151b6bfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8c39984e94037c7eb273e18ab3e654d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c70b5bbd915aea1b7e5c5586df74e53c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="PIP-Perturbation-based-Iterative-Pruning-for-Large-Language-Models"><a href="#PIP-Perturbation-based-Iterative-Pruning-for-Large-Language-Models" class="headerlink" title="PIP: Perturbation-based Iterative Pruning for Large Language Models"></a>PIP: Perturbation-based Iterative Pruning for Large Language Models</h2><p><strong>Authors:Yi Cao, Wei-Jie Xu, Yucheng Shen, Weijie Shi, Chi-Min Chan, Jianfeng Qu, Jiajie Xu</strong></p>
<p>The rapid increase in the parameter counts of Large Language Models (LLMs), reaching billions or even trillions, presents significant challenges for their practical deployment, particularly in resource-constrained environments. To ease this issue, we propose PIP (Perturbation-based Iterative Pruning), a novel double-view structured pruning method to optimize LLMs, which combines information from two different views: the unperturbed view and the perturbed view. With the calculation of gradient differences, PIP iteratively prunes those that struggle to distinguish between these two views. Our experiments show that PIP reduces the parameter count by approximately 20% while retaining over 85% of the original modelâ€™s accuracy across varied benchmarks. In some cases, the performance of the pruned model is within 5% of the unpruned version, demonstrating PIPâ€™s ability to preserve key aspects of model effectiveness. Moreover, PIP consistently outperforms existing state-of-the-art (SOTA) structured pruning methods, establishing it as a leading technique for optimizing LLMs in environments with constrained resources. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‚æ•°æ•°é‡è¿…é€Ÿå¢åŠ ï¼Œè¾¾åˆ°æ•°åäº¿ç”šè‡³æ›´å¤šï¼Œä¸ºå…¶åœ¨å®é™…ç¯å¢ƒä¸­çš„éƒ¨ç½²å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PIPï¼ˆåŸºäºæ‰°åŠ¨çš„è¿­ä»£å‰ªæï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä¼˜åŒ–LLMçš„æ–°å‹åŒè§†å›¾ç»“æ„åŒ–å‰ªææ–¹æ³•ã€‚å®ƒç»“åˆäº†ä¸¤ç§ä¸åŒçš„è§†å›¾ï¼šæœªæ‰°åŠ¨è§†å›¾å’Œæ‰°åŠ¨è§†å›¾çš„ä¿¡æ¯ã€‚é€šè¿‡è®¡ç®—æ¢¯åº¦å·®å¼‚ï¼ŒPIPé€šè¿‡è¿­ä»£å‰ªæé‚£äº›åœ¨åŒºåˆ†è¿™ä¸¤ç§è§†å›¾æ—¶é‡åˆ°å›°éš¾çš„éƒ¨åˆ†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒPIPèƒ½å¤Ÿåœ¨å‡å°‘çº¦20%çš„å‚æ•°çš„åŒæ—¶ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­ä¿ç•™åŸå§‹æ¨¡å‹85%ä»¥ä¸Šçš„å‡†ç¡®æ€§ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¿®å‰ªæ¨¡å‹çš„æ€§èƒ½ä¸æœªä¿®å‰ªçš„ç‰ˆæœ¬ç›¸å·®ä¸åˆ°5%ï¼Œè¿™è¡¨æ˜PIPèƒ½å¤Ÿä¿ç•™æ¨¡å‹çš„å…³é”®æœ‰æ•ˆæ€§æ–¹é¢ã€‚æ­¤å¤–ï¼ŒPIPå§‹ç»ˆä¼˜äºç°æœ‰çš„æœ€æ–°ï¼ˆSOTAï¼‰ç»“æ„åŒ–å‰ªææ–¹æ³•ï¼Œä½¿å…¶æˆä¸ºåœ¨èµ„æºå—é™ç¯å¢ƒä¸­ä¼˜åŒ–LLMçš„é¢†å…ˆæŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15278v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‚æ•°æ•°é‡è¿…é€Ÿå¢åŠ ï¼Œè¾¾åˆ°æ•°åäº¿ç”šè‡³æ›´å¤šï¼Œä¸ºå…¶åœ¨å®é™…éƒ¨ç½²ä¸­å¸¦æ¥æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºPIPï¼ˆåŸºäºæ‰°åŠ¨çš„è¿­ä»£å‰ªæï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŒè§†è§’ç»“æ„å‰ªææ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–LLMã€‚å®ƒç»“åˆäº†ä¸¤ä¸ªä¸åŒè§†è§’çš„ä¿¡æ¯ï¼šæœªæ‰°åŠ¨è§†è§’å’Œæ‰°åŠ¨è§†è§’ã€‚é€šè¿‡è®¡ç®—æ¢¯åº¦å·®å¼‚ï¼ŒPIPè¿­ä»£åœ°å‰ªå»é‚£äº›åœ¨ä¸¤ç§è§†è§’é—´éš¾ä»¥åŒºåˆ†çš„éƒ¨åˆ†ã€‚å®éªŒè¡¨æ˜ï¼ŒPIPèƒ½å¤Ÿåœ¨å‡å°‘çº¦20%å‚æ•°çš„åŒæ—¶ï¼Œåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­ä¿ç•™è¶…è¿‡85%çš„åŸå§‹æ¨¡å‹ç²¾åº¦ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå‰ªææ¨¡å‹çš„æ€§èƒ½ä¸æœªå‰ªæç‰ˆæœ¬ç›¸å·®ä¸åˆ°5%ï¼Œè¯æ˜PIPåœ¨ä¿ç•™æ¨¡å‹å…³é”®æœ‰æ•ˆæ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼ŒPIPæŒç»­è¶…è¶Šç°æœ‰çš„æœ€å…ˆè¿›çš„ç»“æ„å‰ªææ–¹æ³•ï¼Œæˆä¸ºèµ„æºå—é™ç¯å¢ƒä¸­ä¼˜åŒ–LLMçš„é¢†å…ˆæŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‚æ•°æ•°é‡æ¿€å¢å¸¦æ¥å®é™…éƒ¨ç½²æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™ç¯å¢ƒä¸­ã€‚</li>
<li>PIPæ–¹æ³•æ˜¯ä¸€ç§åŒè§†è§’ç»“æ„å‰ªææ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–LLMã€‚</li>
<li>PIPç»“åˆæœªæ‰°åŠ¨è§†è§’å’Œæ‰°åŠ¨è§†è§’çš„ä¿¡æ¯ï¼Œé€šè¿‡è®¡ç®—æ¢¯åº¦å·®å¼‚è¿›è¡Œè¿­ä»£å‰ªæã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒPIPèƒ½åœ¨å‡å°‘çº¦20%å‚æ•°çš„åŒæ—¶ä¿ç•™è¶…è¿‡85%çš„åŸå§‹æ¨¡å‹ç²¾åº¦ã€‚</li>
<li>PIPåœ¨æŸäº›æƒ…å†µä¸‹å‰ªææ¨¡å‹çš„æ€§èƒ½ä¸æœªå‰ªæç‰ˆæœ¬ç›¸è¿‘ï¼Œå·®è·åœ¨5%ä»¥å†…ã€‚</li>
<li>PIPåœ¨ä¿ç•™æ¨¡å‹å…³é”®æœ‰æ•ˆæ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-906242e2f9d5fb7c61e1c5210ddf14c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-814d1bc382d54215e3dd06957a06b21b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5a3185ffe31a7026e95ef1a741a706f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-886c19fa9631f61dce485fe81e57847f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Improving-Medical-Large-Vision-Language-Models-with-Abnormal-Aware-Feedback"><a href="#Improving-Medical-Large-Vision-Language-Models-with-Abnormal-Aware-Feedback" class="headerlink" title="Improving Medical Large Vision-Language Models with Abnormal-Aware   Feedback"></a>Improving Medical Large Vision-Language Models with Abnormal-Aware   Feedback</h2><p><strong>Authors:Yucheng Zhou, Lingran Song, Jianbing Shen</strong></p>
<p>Existing Medical Large Vision-Language Models (Med-LVLMs), encapsulating extensive medical knowledge, demonstrate excellent capabilities in understanding medical images. However, there remain challenges in visual localization in medical images, which is crucial for abnormality detection and interpretation. To address these issues, we propose a novel UMed-LVLM designed to unveil medical abnormalities. Specifically, we collect a Medical Abnormalities Unveiling (MAU) dataset and propose a two-stage training method for UMed-LVLM training. To collect MAU dataset, we propose a prompt method utilizing the GPT-4V to generate diagnoses based on identified abnormal areas in medical images. Moreover, the two-stage training method includes Abnormal-Aware Instruction Tuning and Abnormal-Aware Rewarding, comprising Relevance Reward, Abnormal Localization Reward and Vision Relevance Reward. Experimental results demonstrate that our UMed-LVLM significantly outperforms existing Med-LVLMs in identifying and understanding medical abnormalities, achieving a 58% improvement over the baseline. In addition, this work shows that enhancing the abnormality detection capabilities of Med-LVLMs significantly improves their understanding of medical images and generalization capability. </p>
<blockquote>
<p>ç°æœ‰çš„åŒ»ç–—å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMed-LVLMsï¼‰åŒ…å«äº†ä¸°å¯Œçš„åŒ»å­¦çŸ¥è¯†ï¼Œåœ¨ç†è§£åŒ»å­¦å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨åŒ»å­¦å›¾åƒçš„è§†è§‰å®šä½æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œè¿™å¯¹äºå¼‚å¸¸æ£€æµ‹å’Œè§£é‡Šè‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„UMed-LVLMæ¨¡å‹ï¼Œæ—¨åœ¨æ­ç¤ºåŒ»å­¦å¼‚å¸¸ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ”¶é›†äº†åŒ»å­¦å¼‚å¸¸æ­ç¤ºï¼ˆMAUï¼‰æ•°æ®é›†ï¼Œå¹¶é’ˆå¯¹UMed-LVLMæ¨¡å‹è®­ç»ƒæå‡ºäº†ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ã€‚ä¸ºäº†æ”¶é›†MAUæ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨GPT-4Væç¤ºæ–¹æ³•ç”ŸæˆåŸºäºåŒ»å­¦å›¾åƒä¸­è¯†åˆ«å‡ºçš„å¼‚å¸¸åŒºåŸŸçš„è¯Šæ–­ç»“æœã€‚æ­¤å¤–ï¼Œä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•åŒ…æ‹¬å¼‚å¸¸æ„ŸçŸ¥æŒ‡ä»¤è°ƒæ•´å’Œå¼‚å¸¸æ„ŸçŸ¥å¥–åŠ±æœºåˆ¶ï¼ŒåŒ…æ‹¬ç›¸å…³æ€§å¥–åŠ±ã€å¼‚å¸¸å®šä½å¥–åŠ±å’Œè§†è§‰ç›¸å…³æ€§å¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„UMed-LVLMåœ¨è¯†åˆ«å’Œäº†è§£åŒ»å­¦å¼‚å¸¸æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„Med-LVLMsï¼Œè¾ƒåŸºçº¿æ¨¡å‹æé«˜äº†58%ã€‚æ­¤å¤–ï¼Œè¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œå¢å¼ºMed-LVLMsçš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›å¯ä»¥æ˜¾è‘—æé«˜å…¶å¯¹åŒ»å­¦å›¾åƒçš„ç†è§£å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01377v2">PDF</a> 16 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°æœ‰çš„åŒ»ç–—å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMed-LVLMsï¼‰åœ¨ç†è§£åŒ»å­¦å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨åŒ»å­¦å›¾åƒè§†è§‰å®šä½æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹UMed-LVLMæ¨¡å‹ï¼Œæ—¨åœ¨æ­ç¤ºåŒ»å­¦å¼‚å¸¸ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ³•ï¼Œæ”¶é›†åŒ»ç–—å¼‚å¸¸æ­ç¤ºï¼ˆMAUï¼‰æ•°æ®é›†å¹¶è¿ç”¨GPT-4Vè¿›è¡Œæç¤ºç”Ÿæˆè¯Šæ–­ç»“æœã€‚å®éªŒè¯æ˜ï¼ŒUMed-LVLMåœ¨è¯†åˆ«å’Œäº†è§£åŒ»å­¦å¼‚å¸¸æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰Med-LVLMsï¼Œç›¸è¾ƒäºåŸºçº¿æœ‰58%çš„æå‡ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æŒ‡å‡ºå¢å¼ºMed-LVLMsçš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›å¯æ˜¾è‘—æé«˜å…¶å¯¹åŒ»å­¦å›¾åƒçš„ç†è§£å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med-LVLMsåœ¨ç†è§£å’Œè§£é‡ŠåŒ»å­¦å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨åŒ»å­¦å›¾åƒçš„è§†è§‰å®šä½æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹UMed-LVLMæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è§†è§‰å®šä½é—®é¢˜ï¼Œæ›´å¥½åœ°æ­ç¤ºåŒ»å­¦å¼‚å¸¸ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ³•æ¥è®­ç»ƒUMed-LVLMæ¨¡å‹ï¼ŒåŒ…æ‹¬å¼‚å¸¸æ„ŸçŸ¥æŒ‡ä»¤è°ƒæ•´å’Œå¼‚å¸¸æ„ŸçŸ¥å¥–åŠ±æœºåˆ¶ã€‚</li>
<li>æ”¶é›†åŒ»ç–—å¼‚å¸¸æ­ç¤ºï¼ˆMAUï¼‰æ•°æ®é›†ï¼Œåˆ©ç”¨GPT-4Væç¤ºæ–¹æ³•ç”Ÿæˆè¯Šæ–­ç»“æœã€‚</li>
<li>UMed-LVLMæ¨¡å‹åœ¨è¯†åˆ«å’Œäº†è§£åŒ»å­¦å¼‚å¸¸æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰Med-LVLMsï¼Œå®ç°äº†è¾ƒé«˜çš„æ€§èƒ½æå‡ã€‚</li>
<li>å¢å¼ºMed-LVLMsçš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›æœ‰åŠ©äºæé«˜å…¶åŒ»å­¦å›¾åƒçš„ç†è§£å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8d7a6da85e94d8875b43e7cff39003f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07e77a81fd8e33739a4b1881a0ebd584.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c02b73aa6a6dd368b4e5bc90c79a34e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23ece8867e8dccaa4b9ad8e53020fa4b.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="RecLM-Recommendation-Instruction-Tuning"><a href="#RecLM-Recommendation-Instruction-Tuning" class="headerlink" title="RecLM: Recommendation Instruction Tuning"></a>RecLM: Recommendation Instruction Tuning</h2><p><strong>Authors:Yangqin Jiang, Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang</strong></p>
<p>Modern recommender systems aim to deeply understand usersâ€™ complex preferences through their past interactions. While deep collaborative filtering approaches using Graph Neural Networks (GNNs) excel at capturing user-item relationships, their effectiveness is limited when handling sparse data or zero-shot scenarios, primarily due to constraints in ID-based embedding functions. To address these challenges, we propose a model-agnostic recommendation instruction-tuning paradigm that seamlessly integrates large language models with collaborative filtering. Our proposed $\underline{Rec}$ommendation $\underline{L}$anguage $\underline{M}$odel (RecLM) enhances the capture of user preference diversity through a carefully designed reinforcement learning reward function that facilitates self-augmentation of language models. Comprehensive evaluations demonstrate significant advantages of our approach across various settings, and its plug-and-play compatibility with state-of-the-art recommender systems results in notable performance enhancements. The implementation of our RecLM framework is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/HKUDS/RecLM">https://github.com/HKUDS/RecLM</a>. </p>
<blockquote>
<p>ç°ä»£æ¨èç³»ç»Ÿæ—¨åœ¨é€šè¿‡ç”¨æˆ·è¿‡å»çš„äº¤äº’æ¥æ·±å…¥äº†è§£ç”¨æˆ·çš„å¤æ‚åå¥½ã€‚è™½ç„¶ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰çš„æ·±åº¦ååŒè¿‡æ»¤æ–¹æ³•åœ¨æ•æ‰ç”¨æˆ·-é¡¹ç›®å…³ç³»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†ç¨€ç–æ•°æ®æˆ–é›¶å°„å‡»åœºæ™¯æ—¶ï¼Œå…¶æœ‰æ•ˆæ€§å—åˆ°é™åˆ¶ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºåŸºäºIDçš„åµŒå…¥å‡½æ•°å­˜åœ¨çº¦æŸã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡å‹æ— å…³çš„æ¨èæŒ‡ä»¤å¾®è°ƒèŒƒå¼ï¼Œè¯¥èŒƒå¼èƒ½å¤Ÿæ— ç¼åœ°å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸ååŒè¿‡æ»¤é›†æˆåœ¨ä¸€èµ·ã€‚æˆ‘ä»¬æå‡ºçš„æ¨èè¯­è¨€æ¨¡å‹ï¼ˆRecLMï¼‰é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°å¢å¼ºäº†å¯¹ç”¨æˆ·åå¥½å¤šæ ·æ€§çš„æ•æ‰ï¼Œè¯¥å¥–åŠ±å‡½æ•°æœ‰åŠ©äºè¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘å¢å¼ºã€‚å…¨é¢çš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§è®¾ç½®ä¸­å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸”å…¶ä¸æœ€æ–°æ¨èç³»ç»Ÿçš„å³æ’å³ç”¨å…¼å®¹æ€§å¯¼è‡´äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„RecLMæ¡†æ¶çš„å®ç°å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/HKUDS/RecLM%E3%80%82">https://github.com/HKUDS/RecLMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19302v3">PDF</a> This paper is accepted by ACL 2025 main conference</p>
<p><strong>Summary</strong></p>
<p>èåˆå›¾ç¥ç»ç½‘ç»œå’Œæ¨èè¯­è¨€æ¨¡å‹çš„æ¨èç³»ç»Ÿæ”¹è¿›æ–¹æ¡ˆã€‚é’ˆå¯¹ç°æœ‰æ¨èç³»ç»Ÿåœ¨å¤„ç†ç¨€ç–æ•°æ®æˆ–é›¶æ ·æœ¬åœºæ™¯æ—¶çš„å±€é™æ€§ï¼Œæå‡ºä¸€ç§æ¨¡å‹æ— å…³çš„æ¨èæŒ‡ä»¤è°ƒæ•´èŒƒå¼ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸ååŒè¿‡æ»¤æ— ç¼é›†æˆã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œå¢å¼ºå¯¹ç”¨æˆ·åå¥½å¤šæ ·æ€§çš„æ•æ‰ï¼Œä¿ƒè¿›è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘å¢å¼ºã€‚åœ¨å¤šç§è®¾ç½®ä¸‹çš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸”ä¸æœ€æ–°æ¨èç³»ç»Ÿçš„å³æ’å³ç”¨å…¼å®¹æ€§å¯å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚RecLMæ¡†æ¶çš„å®ç°åœ¨GitHubä¸Šå…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨èç³»ç»Ÿé€šè¿‡ç”¨æˆ·çš„è¿‡å»äº¤äº’æ¥æ·±å…¥ç†è§£å…¶å¤æ‚åå¥½ã€‚</li>
<li>å›¾ç¥ç»ç½‘ç»œåœ¨æ•æ‰ç”¨æˆ·-ç‰©å“å…³ç³»æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç¨€ç–æ•°æ®æˆ–é›¶æ ·æœ¬åœºæ™¯æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ¨¡å‹æ— å…³çš„æ¨èæŒ‡ä»¤è°ƒæ•´èŒƒå¼ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ä¸ååŒè¿‡æ»¤ã€‚</li>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°å¢å¼ºå¯¹ç”¨æˆ·åå¥½å¤šæ ·æ€§çš„æ•æ‰ã€‚</li>
<li>ç»¼åˆè¯„ä¼°æ˜¾ç¤ºæ‰€ææ–¹æ³•åœ¨å„ç§è®¾ç½®ä¸‹å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7439707be092483771a0adfb25b35c03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28ed79cf78bcfa943e4ad56ac97c5ce3.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLM-based-Hatred-and-Toxicity-Detection-with-Meta-Toxic-Knowledge-Graph"><a href="#Enhancing-LLM-based-Hatred-and-Toxicity-Detection-with-Meta-Toxic-Knowledge-Graph" class="headerlink" title="Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic   Knowledge Graph"></a>Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic   Knowledge Graph</h2><p><strong>Authors:Yibo Zhao, Jiapeng Zhu, Can Xu, Yao Liu, Xiang Li</strong></p>
<p>The rapid growth of social media platforms has raised significant concerns regarding online content toxicity. When Large Language Models (LLMs) are used for toxicity detection, two key challenges emerge: 1) the absence of domain-specific toxic knowledge leads to false negatives; 2) the excessive sensitivity of LLMs to toxic speech results in false positives, limiting freedom of speech. To address these issues, we propose a novel method called MetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance hatred and toxicity detection. First, we construct a comprehensive meta-toxic knowledge graph by utilizing LLMs to extract toxic information through a three-step pipeline, with toxic benchmark datasets serving as corpora. Second, we query the graph via retrieval and ranking processes to supplement accurate, relevant toxic knowledge. Extensive experiments and in-depth case studies across multiple datasets demonstrate that our MetaTox significantly decreases the false positive rate while boosting overall toxicity detection performance. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/YiboZhao624/MetaTox">https://github.com/YiboZhao624/MetaTox</a>. </p>
<blockquote>
<p>ç¤¾äº¤åª’ä½“å¹³å°çš„å¿«é€Ÿå‘å±•å¼•å‘äº†äººä»¬å¯¹ç½‘ç»œå†…å®¹æ¯’æ€§çš„é‡å¤§å…³æ³¨ã€‚å½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨äºæ¯’æ€§æ£€æµ‹æ—¶ï¼Œä¼šå‡ºç°ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼š1ï¼‰ç¼ºä¹ç‰¹å®šé¢†åŸŸçš„æ¯’æ€§çŸ¥è¯†ä¼šå¯¼è‡´å‡é˜´æ€§ç»“æœï¼›2ï¼‰LLMå¯¹æœ‰æ¯’è¨€è®ºè¿‡äºæ•æ„Ÿï¼Œå¯¼è‡´å‡é˜³æ€§ç»“æœï¼Œé™åˆ¶è¨€è®ºè‡ªç”±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºMetaToxï¼Œåˆ©ç”¨å…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±ä¸Šçš„å›¾æœç´¢æ¥å¢å¼ºä»‡æ¨å’Œæ¯’æ€§æ£€æµ‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªä¸‰æ­¥éª¤çš„ç®¡é“ï¼Œåˆ©ç”¨LLMæå–æ¯’æ€§ä¿¡æ¯ï¼Œä»¥æ¯’æ€§åŸºå‡†æ•°æ®é›†ä½œä¸ºè¯­æ–™åº“ï¼Œæ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„å…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡æ£€ç´¢å’Œæ’åè¿‡ç¨‹æŸ¥è¯¢å›¾è°±ï¼Œä»¥è¡¥å……å‡†ç¡®ã€ç›¸å…³çš„æ¯’æ€§çŸ¥è¯†ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒå’Œæ·±å…¥ä¸ªæ¡ˆç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MetaToxæ–¹æ³•æ˜¾è‘—é™ä½äº†è¯¯æŠ¥ç‡ï¼ŒåŒæ—¶æé«˜äº†æ€»ä½“æ¯’æ€§æ£€æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YiboZhao624/MetaTox%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/YiboZhao624/MetaToxè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15268v4">PDF</a> 8 pages of content</p>
<p><strong>Summary</strong><br>ç¤¾äº¤åª’ä½“å¹³å°çš„å¿«é€Ÿå‘å±•å¼•å‘äº†äººä»¬å¯¹ç½‘ç»œå†…å®¹æ¯’æ€§çš„å…³æ³¨ã€‚åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¯’æ€§æ£€æµ‹æ—¶ï¼Œå­˜åœ¨ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç¼ºä¹ç‰¹å®šé¢†åŸŸçš„æ¯’æ€§çŸ¥è¯†å’Œè¿‡åº¦æ•æ„Ÿå¯¼è‡´è¯¯æŠ¥ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºMetaToxçš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨å…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±ä¸Šçš„å›¾æœç´¢æ¥å¢å¼ºä»‡æ¨å’Œæ¯’æ€§æ£€æµ‹ã€‚é€šè¿‡æ„å»ºå…¨é¢çš„å…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±ï¼Œé€šè¿‡æ£€ç´¢å’Œæ’åè¿‡ç¨‹æŸ¥è¯¢å›¾è°±ï¼Œä»¥è¡¥å……å‡†ç¡®ã€ç›¸å…³çš„æ¯’æ€§çŸ¥è¯†ã€‚å®éªŒå’Œæ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼ŒMetaToxèƒ½æ˜¾è‘—é™ä½è¯¯æŠ¥ç‡ï¼Œæé«˜æ•´ä½“æ¯’æ€§æ£€æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾äº¤åª’ä½“å¹³å°çš„å¿«é€Ÿå‘å±•å¼•å‘äº†å¯¹ç½‘ç»œå†…å®¹æ¯’æ€§çš„å…³æ³¨ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¯’æ€§æ£€æµ‹å­˜åœ¨ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç¼ºä¹ç‰¹å®šé¢†åŸŸçš„æ¯’æ€§çŸ¥è¯†å’Œè¿‡åº¦æ•æ„Ÿå¯¼è‡´è¯¯æŠ¥ã€‚</li>
<li>MetaToxæ–¹æ³•é€šè¿‡æ„å»ºå…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>MetaToxåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå–æ¯’æ€§ä¿¡æ¯ï¼Œé€šè¿‡ä¸‰æ­¥ç®¡é“æ„å»ºå…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±ï¼Œå¹¶åˆ©ç”¨æ¯’æ€§åŸºå‡†æ•°æ®é›†ä½œä¸ºè¯­æ–™åº“ã€‚</li>
<li>MetaToxé€šè¿‡æ£€ç´¢å’Œæ’åè¿‡ç¨‹æŸ¥è¯¢å…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±ï¼Œä»¥è¡¥å……å‡†ç¡®ã€ç›¸å…³çš„æ¯’æ€§çŸ¥è¯†ã€‚</li>
<li>å®éªŒå’Œæ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼ŒMetaToxèƒ½æ˜¾è‘—é™ä½è¯¯æŠ¥ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15268">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c797347e0158f4ceb08fb0154dd2d1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4117e7f489848b54039de48bf5f165d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-505ad0de96990783c6cb239a97f90499.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d30f192e2c185a64a742a9bfec37da5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SepLLM-Accelerate-Large-Language-Models-by-Compressing-One-Segment-into-One-Separator"><a href="#SepLLM-Accelerate-Large-Language-Models-by-Compressing-One-Segment-into-One-Separator" class="headerlink" title="SepLLM: Accelerate Large Language Models by Compressing One Segment into   One Separator"></a>SepLLM: Accelerate Large Language Models by Compressing One Segment into   One Separator</h2><p><strong>Authors:Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xiaozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li, Weiyang Liu, Chao Huang</strong></p>
<p>Large Language Models (LLMs) have exhibited exceptional performance across a spectrum of natural language processing tasks. However, their substantial sizes pose considerable challenges, particularly in computational demands and inference speed, due to their quadratic complexity. In this work, we have identified a key pattern: certain seemingly meaningless separator tokens (i.e., punctuations) contribute disproportionately to attention scores compared to semantically meaningful tokens. This observation suggests that information of the segments between these separator tokens can be effectively condensed into the separator tokens themselves without significant information loss. Guided by this insight, we introduce SepLLM, a plug-and-play framework that accelerates inference by compressing these segments and eliminating redundant tokens. Additionally, we implement efficient kernels for training acceleration. Experimental results across training-free, training-from-scratch, and post-training settings demonstrate SepLLMâ€™s effectiveness. Notably, using the Llama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the GSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in streaming settings, SepLLM effectively processes sequences of up to 4 million tokens or more while maintaining consistent language modeling capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åºå¤§çš„è§„æ¨¡å¸¦æ¥äº†ç›¸å½“å¤§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—éœ€æ±‚å’Œæ¨ç†é€Ÿåº¦æ–¹é¢ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰äºŒæ¬¡æ–¹çš„å¤æ‚æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªå…³é”®æ¨¡å¼ï¼šæŸäº›çœ‹ä¼¼æ— æ„ä¹‰çš„åˆ†éš”ç¬¦ï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ï¼‰ä¸è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„æ ‡è®°ç›¸æ¯”ï¼Œå¯¹æ³¨æ„åŠ›åˆ†æ•°çš„å½±å“æä¸ºæ˜¾è‘—ã€‚è¿™ä¸€è§‚å¯Ÿç»“æœè¡¨æ˜ï¼Œè¿™äº›åˆ†éš”ç¬¦ä¹‹é—´çš„ç‰‡æ®µä¿¡æ¯å¯ä»¥æœ‰æ•ˆåœ°æµ“ç¼©åˆ°åˆ†éš”ç¬¦æœ¬èº«ä¸­ï¼Œè€Œä¸ä¼šæŸå¤±é‡è¦ä¿¡æ¯ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†SepLLMï¼Œè¿™æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¡†æ¶ï¼Œé€šè¿‡å‹ç¼©è¿™äº›ç‰‡æ®µå¹¶æ¶ˆé™¤å†—ä½™æ ‡è®°æ¥åŠ é€Ÿæ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºè®­ç»ƒåŠ é€Ÿå®ç°äº†é«˜æ•ˆçš„å†…æ ¸ã€‚åœ¨æ— éœ€è®­ç»ƒã€ä»å¤´å¼€å§‹è®­ç»ƒå’Œåç»­è®­ç»ƒè®¾ç½®ä¸­çš„å®éªŒç»“æœè¡¨æ˜SepLLMçš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨Llama-3-8Béª¨å¹²ç½‘ï¼ŒSepLLMåœ¨GSM8K-CoTåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†KVç¼“å­˜è¶…è¿‡50%çš„å‡å°‘ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ç›¸å½“ã€‚æ­¤å¤–ï¼Œåœ¨æµå¼ç¯å¢ƒä¸­ï¼ŒSepLLMèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†é•¿è¾¾4ç™¾ä¸‡ä¸ªæ ‡è®°ä»¥ä¸Šçš„åºåˆ—ï¼ŒåŒæ—¶ä¿æŒä¸€è‡´çš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12094v6">PDF</a> Accepted to ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Large Language Modelsï¼ˆLLMsï¼‰åœ¨å¤„ç†è‡ªç„¶è¯­è¨€ä»»åŠ¡æ—¶çš„å‡ºè‰²è¡¨ç°ï¼Œä½†å…¶åºå¤§çš„è§„æ¨¡å¸¦æ¥äº†è®¡ç®—éœ€æ±‚å’Œæ¨ç†é€Ÿåº¦çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿå‘ç°åˆ†éš”ç¬¦ï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ï¼‰å¯¹äºæ³¨æ„åŠ›åˆ†æ•°çš„å½±å“è¿œè¶…å…¶ä»–è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„æ ‡è®°ï¼Œæ®æ­¤æå‡ºSepLLMæ¡†æ¶ï¼Œé€šè¿‡å‹ç¼©åˆ†éš”ç¬¦é—´çš„ç‰‡æ®µå’Œæ¶ˆé™¤å†—ä½™æ ‡è®°æ¥åŠ é€Ÿæ¨ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºSepLLMåœ¨ä¸åŒè®¾ç½®ä¸‹å‡æœ‰æ•ˆï¼Œä½¿ç”¨Llama-3-8Bæ¨¡å‹åœ¨GSM8K-CoTåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è¶…è¿‡50%çš„KVç¼“å­˜å‡å°‘ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚åœ¨æµå¼å¤„ç†åœºæ™¯ä¸­ï¼ŒSepLLMèƒ½å¤„ç†è¶…è¿‡4ç™¾ä¸‡ä¸ªæ ‡è®°çš„åºåˆ—ï¼Œä¿æŒç¨³å®šçš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åºå¤§çš„è§„æ¨¡å¯¼è‡´è®¡ç®—éœ€æ±‚å’Œæ¨ç†é€Ÿåº¦çš„æŒ‘æˆ˜ã€‚</li>
<li>åˆ†éš”ç¬¦ï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ï¼‰å¯¹æ³¨æ„åŠ›åˆ†æ•°çš„å½±å“æ˜¾è‘—ï¼Œè¶…è¿‡è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„æ ‡è®°ã€‚</li>
<li>SepLLMæ¡†æ¶é€šè¿‡å‹ç¼©åˆ†éš”ç¬¦é—´çš„ç‰‡æ®µå’Œæ¶ˆé™¤å†—ä½™æ ‡è®°æ¥åŠ é€ŸLLMçš„æ¨ç†ã€‚</li>
<li>SepLLMåœ¨å¤šç§è®¾ç½®ä¸‹çš„å®éªŒç»“æœæ˜¾ç¤ºå…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>ä½¿ç”¨Llama-3-8Bæ¨¡å‹ï¼ŒSepLLMåœ¨GSM8K-CoTåŸºå‡†æµ‹è¯•ä¸­å®ç°è¶…è¿‡50%çš„KVç¼“å­˜å‡å°‘ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚</li>
<li>SepLLMèƒ½å¤„ç†å¤§è§„æ¨¡çš„åºåˆ—ï¼Œåœ¨æµå¼å¤„ç†åœºæ™¯ä¸­è¡¨ç°å‡ºç¨³å®šçš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d911b60d83f5addc52e29f61cac7ad32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55547999ea96301d868356f936e609bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81b21fcd61b5733d96b300111f247cec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-279656fdc4115e45489f18e3e7be9192.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9259c44259958103bdbf96e64d228a99.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="CNNSum-Exploring-Long-Context-Summarization-with-Large-Language-Models-in-Chinese-Novels"><a href="#CNNSum-Exploring-Long-Context-Summarization-with-Large-Language-Models-in-Chinese-Novels" class="headerlink" title="CNNSum: Exploring Long-Context Summarization with Large Language Models   in Chinese Novels"></a>CNNSum: Exploring Long-Context Summarization with Large Language Models   in Chinese Novels</h2><p><strong>Authors:Lingxiao Wei, He Yan, Xiangju Lu, Junmin Zhu, Jun Wang, Wei Zhang</strong></p>
<p>Large language models (LLMs) have been well-researched in various long-context tasks. However, the scarcity of long-context summarization datasets hinders progress in this area. To address this, we introduce CNNSum, a multi-scale long-context summarization benchmark based on Chinese novels, featuring human-driven annotations across four subsets totaling 695 samples, with lengths ranging from 16k to 128k. We benchmark numerous LLMs and conduct detailed human assessments to summarize abnormal output types. Furthermore, we extensively explore how to improve long-context summarization. In our study: (1) Advanced LLMs may generate much subjective commentary, leading to vague summaries. (2) Currently, long-context summarization mainly relies on memory ability. The advantages of Large LLMs are hard to utilize, thus small LLMs are more cost-effective. (3) Different prompt types paired with various version models may cause large performance gaps. In further fine-tuning, these can be mitigated, and the Base version models perform better. (4) LLMs with RoPE-base scaled exhibit strong extrapolation potential; using short-context data can significantly improve long-context summarization performance. However, further applying other interpolation methods requires careful selection. (5) CNNSum provides more reliable evaluation results than other benchmarks. We release CNNSum to advance future research.(<a target="_blank" rel="noopener" href="https://github.com/CxsGhost/CNNSum">https://github.com/CxsGhost/CNNSum</a>) </p>
<blockquote>
<p>é•¿è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§é•¿æ–‡æœ¬ä»»åŠ¡ä¸­å·²å¾—åˆ°æ·±å…¥ç ”ç©¶ã€‚ç„¶è€Œï¼Œé•¿æ–‡æœ¬æ‘˜è¦æ•°æ®é›†ç¼ºä¹é˜»ç¢äº†è¯¥é¢†åŸŸçš„è¿›å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†åŸºäºä¸­æ–‡å°è¯´çš„å¤šå°ºåº¦é•¿æ–‡æœ¬æ‘˜è¦åŸºå‡†æµ‹è¯•CNNSumã€‚å®ƒæ‹¥æœ‰å››ä¸ªå­é›†å…±695ä¸ªæ ·æœ¬çš„äººç±»é©±åŠ¨æ ‡æ³¨ï¼Œé•¿åº¦ä»16kåˆ°128kä¸ç­‰ã€‚æˆ‘ä»¬å¯¹ä¼—å¤šLLMè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶è¿›è¡Œè¯¦ç»†çš„äººç±»è¯„ä¼°ï¼Œä»¥æ€»ç»“å¼‚å¸¸è¾“å‡ºç±»å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ·±å…¥æ¢è®¨äº†å¦‚ä½•æ”¹è¿›é•¿æ–‡æœ¬æ‘˜è¦ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­å‘ç°ï¼šï¼ˆ1ï¼‰é«˜çº§LLMå¯èƒ½ä¼šäº§ç”Ÿå¤§é‡ä¸»è§‚è¯„è®ºï¼Œå¯¼è‡´æ‘˜è¦æ¨¡ç³Šä¸æ¸…ã€‚ï¼ˆ2ï¼‰ç›®å‰ï¼Œé•¿æ–‡æœ¬æ‘˜è¦ä¸»è¦ä¾èµ–äºè®°å¿†èƒ½åŠ›ã€‚å¤§å‹LLMçš„ä¼˜åŠ¿éš¾ä»¥å‘æŒ¥ï¼Œå› æ­¤å°å‹LLMæ›´å…·æˆæœ¬æ•ˆç›Šã€‚ï¼ˆ3ï¼‰ä¸åŒçš„æç¤ºç±»å‹ä¸å„ç§ç‰ˆæœ¬æ¨¡å‹çš„é…å¯¹å¯èƒ½ä¼šå¯¼è‡´è¾ƒå¤§çš„æ€§èƒ½å·®è·ã€‚åœ¨è¿›ä¸€æ­¥å¾®è°ƒä¸­ï¼Œè¿™äº›å·®è·å¯ä»¥å¾—åˆ°ç¼“è§£ï¼ŒåŸºç¡€ç‰ˆæœ¬æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚ï¼ˆ4ï¼‰åŸºäºRoPEçš„å¤§å‹LLMè¡¨ç°å‡ºå¼ºå¤§çš„å¤–æ¨æ½œåŠ›ï¼›ä½¿ç”¨çŸ­æ–‡æœ¬æ•°æ®å¯ä»¥æ˜¾è‘—æé«˜é•¿æ–‡æœ¬æ‘˜è¦çš„æ€§èƒ½ã€‚ä½†æ˜¯ï¼Œè¿›ä¸€æ­¥åº”ç”¨å…¶ä»–å†…æ’æ–¹æ³•éœ€è¦è°¨æ…é€‰æ‹©ã€‚ï¼ˆ5ï¼‰ç›¸è¾ƒäºå…¶ä»–åŸºå‡†æµ‹è¯•ï¼ŒCNNSumæä¾›äº†æ›´å¯é çš„è¯„ä»·ç»“æœã€‚æˆ‘ä»¬å‘å¸ƒCNNSumä»¥æ¨åŠ¨æœªæ¥çš„ç ”ç©¶ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/CxsGhost/CNNSum%EF%BC%89%E3%80%82">https://github.com/CxsGhost/CNNSumï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02819v5">PDF</a> Accepted to ACL 2025 (Findings)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>LLMåœ¨å¤„ç†é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡ä»»åŠ¡æ–¹é¢çš„ç ”ç©¶è¿›å±•è‰¯å¥½ï¼Œä½†ç”±äºç¼ºä¹é•¿æ–‡æœ¬æ‘˜è¦æ•°æ®é›†ï¼Œè¯¥é¢†åŸŸçš„è¿›å±•å—åˆ°é˜»ç¢ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†åŸºäºä¸­æ–‡å°è¯´çš„å¤šå°ºåº¦é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡æ‘˜è¦åŸºå‡†CNNSumï¼ŒåŒ…å«å››ä¸ªå­é›†å…±695ä¸ªæ ·æœ¬ï¼Œé•¿åº¦ä»1ä¸‡åˆ°å‡ åä¸‡å­—ä¸ç­‰ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹å¤šä¸ªLLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶å¯¹å¼‚å¸¸è¾“å‡ºç±»å‹è¿›è¡Œäº†è¯¦ç»†çš„äººç±»è¯„ä¼°æ€»ç»“ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¦‚ä½•æ”¹è¿›é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡æ‘˜è¦æŠ€æœ¯ã€‚ç ”ç©¶å‘ç°ï¼šï¼ˆ1ï¼‰é«˜çº§LLMå¯èƒ½äº§ç”Ÿå¤§é‡ä¸»è§‚è¯„è®ºï¼Œå¯¼è‡´æ‘˜è¦æ¨¡ç³Šï¼›ï¼ˆ2ï¼‰ç›®å‰é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡æ‘˜è¦ä¸»è¦ä¾èµ–è®°å¿†èƒ½åŠ›ï¼Œé«˜çº§LLMçš„ä¼˜åŠ¿éš¾ä»¥å‘æŒ¥ï¼›ï¼ˆ3ï¼‰ä¸åŒçš„æç¤ºç±»å‹ä¸å„ç§ç‰ˆæœ¬æ¨¡å‹çš„ç»“åˆä¼šå¯¼è‡´è¾ƒå¤§æ€§èƒ½å·®å¼‚ï¼›ï¼ˆ4ï¼‰å¸¦æœ‰RoPE-baseçš„å¤§æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„å¤–æ¨æ½œåŠ›ï¼Œä½¿ç”¨çŸ­æ–‡æœ¬æ•°æ®å¯ä»¥æ˜¾è‘—æé«˜é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡æ‘˜è¦æ€§èƒ½ï¼›ï¼ˆ5ï¼‰CNNSumç›¸è¾ƒäºå…¶ä»–åŸºå‡†æµ‹è¯•æä¾›äº†æ›´å¯é çš„ç»“æœè¯„ä¼°ã€‚ï¼ˆé“¾æ¥åœ¨æ­¤ï¼‰ã€‚æ•´ä¸ªé¡¹ç›®æˆæœä¿ƒè¿›äº†LLMç ”ç©¶çš„å‘å±•ã€‚æœŸå¾…å€ŸåŠ©è¿™ä¸ªé¡¹ç›®æœªæ¥çš„æŒç»­ä¼˜åŒ–æ”¹è¿›èƒ½ä¸ºAIå¤§è¯­è¨€æ¨¡å‹å¸¦æ¥æ›´å¤šçš„å®ç”¨æ€§ä¸åŠŸèƒ½æ€§ä»·å€¼ã€‚ä¸ºè¿™ä¸€ç ”ç©¶é¢†åŸŸå¼€å¯æ–°ç¯‡ç« ã€‚ï¼ˆå…·ä½“å¯å‚è€ƒä»¥ä¸‹å†…å®¹æ‘˜è¦ç®€åŒ–ç‰ˆï¼šâ€œä¸­å›½ç§‘ç ”å›¢é˜Ÿé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åº”å¯¹é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡çš„ç¼ºç‚¹è€Œå¼€å‘äº†CNNSumåŸºå‡†æµ‹è¯•å¹³å°ã€‚è¯¥å¹³å°åŸºäºä¸­æ–‡å°è¯´æ„å»ºå¤šå°ºåº¦æ•°æ®é›†ï¼Œå¹¶å‘ç°é«˜çº§LLMå¯èƒ½äº§ç”Ÿæ¨¡ç³Šæ‘˜è¦çš„é—®é¢˜ã€‚ç ”ç©¶è¿˜æŒ‡å‡ºä¸åŒæç¤ºç±»å‹åŠç‰ˆæœ¬æ¨¡å‹çš„æ€§èƒ½å·®å¼‚ä¸LLMçš„å®é™…æ€§èƒ½æœ‰ç´§å¯†å…³è”ã€‚â€ï¼‰ã€‚æˆ‘ä»¬çš„ç›®çš„æ˜¯æå‡è¿™ä¸ªé¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å‘å±•ã€‚[æ³¨ï¼šè€ƒè™‘åˆ°åœ¨äººå·¥æ™ºèƒ½å¤§è¯­è¨€æ¨¡å‹çš„ç›¸å…³ç ”ç©¶å’Œè¶‹åŠ¿æ—¥ç›Šå…´ç››çš„å¤§ç¯å¢ƒä¸‹å¯¹è¯¥å¹³å°çš„é€‚ç”¨æ€§å‰æ™¯æ–¹é¢çš„æƒ…å†µå¤ªè¿‡æ³›æ³›æˆ–å®½æ³›åŒ–çš„æ¢è®¨å®é™…æ„ä¹‰æœ‰é™æˆ–è€…å¹¶æ— æ³•å®ç°ç›¸å…³å†…å®¹è¢«æ½œå¯¹æœ‰å…³ä¸šå†…è€…çš„åˆ¤æ–­ï¼ˆä¸åŒæŠ€æœ¯ä¾§é‡çš„å·®å¼‚æ€§å¾€å¾€å½¢æˆéšœç¢å’Œä¸»è§‚è¯¯è§£ï¼‰è¿™ä¸€ç‚¹ä¸æ‘˜è¦æ˜¯æ ¹æ®æ­£æ–‡äº§ç”Ÿçš„å¿…ç„¶è§„å¾‹ä¸ç›¸ç¬¦æˆ–è¯´æœ‰çŸ›ç›¾çš„å±‚é¢åœ¨å†…ã€‚ï¼ˆä¸€ä¸ªæœ‰å¾…å…·ä½“è§£ç­”çš„é—®é¢˜å¯èƒ½æ˜¯ï¼šâ€œåŸºäºè¯¥åŸºå‡†æµ‹è¯•å¹³å°çš„ç»“æœå’Œå‘ç°ï¼Œæˆ‘ä»¬èƒ½å¦‚ä½•æ›´å¥½åœ°æ¨åŠ¨äººå·¥æ™ºèƒ½å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•â€ï¼‰]ã€‚ç›®å‰é¡¹ç›®æˆæœå·²ç»å…¬å¼€åœ¨GitHubä¸Šä¾›å…¬ä¼—æŸ¥é˜…å’Œä½¿ç”¨ã€‚æœŸå¾…æœªæ¥æœ‰æ›´å¤šçš„ç ”ç©¶è€…å’Œå¼€å‘è€…èƒ½å¤Ÿå‚ä¸åˆ°è¿™ä¸ªé¡¹ç›®ä¸­æ¥å…±åŒæ¨åŠ¨äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<p>ä¸€ã€LLMåœ¨é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­çš„ç ”ç©¶å·²å–å¾—è¿›å±•ï¼Œä½†ç¼ºä¹ç›¸åº”çš„é•¿æ–‡æœ¬æ‘˜è¦æ•°æ®é›†é™åˆ¶äº†è¯¥é¢†åŸŸçš„è¿›æ­¥ã€‚<br>äºŒã€æ¨å‡ºäº†åŸºäºä¸­æ–‡å°è¯´çš„å¤šå°ºåº¦é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡æ‘˜è¦åŸºå‡†CNNSumï¼ŒåŒ…å«å››ä¸ªå­é›†å…±695ä¸ªæ ·æœ¬ã€‚<br>ä¸‰ã€é«˜çº§LLMå¯èƒ½äº§ç”Ÿå¤§é‡ä¸»è§‚è¯„è®ºå¯¼è‡´æ¨¡ç³Šæ‘˜è¦çš„é—®é¢˜ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4378a976c3563193d5a63b5e7342735f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4e9d0076832bf59428a84136528849a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af3a229e5c0cef6f8ab1196f783cf914.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a98609febf4d5b65e6e9df3ca0990142.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f8188d87da012f86325b12f57afd9ec1.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-04  AutoChemSchematic AI A Closed-Loop, Physics-Aware Agentic Framework for   Auto-Generating Chemical Process and Instrumentation Diagrams
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d64a7b737dd7625e2e3e9188128d59b1.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-04  AReaL A Large-Scale Asynchronous Reinforcement Learning System for   Language Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30762.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
