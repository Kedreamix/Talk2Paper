<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-04  MSDA Combining Pseudo-labeling and Self-Supervision for Unsupervised   Domain Adaptation in ASR">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-64f566e231dd16babc980ce9ea30a2ef.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    54 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-04-æ›´æ–°"><a href="#2025-06-04-æ›´æ–°" class="headerlink" title="2025-06-04 æ›´æ–°"></a>2025-06-04 æ›´æ–°</h1><h2 id="MSDA-Combining-Pseudo-labeling-and-Self-Supervision-for-Unsupervised-Domain-Adaptation-in-ASR"><a href="#MSDA-Combining-Pseudo-labeling-and-Self-Supervision-for-Unsupervised-Domain-Adaptation-in-ASR" class="headerlink" title="MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised   Domain Adaptation in ASR"></a>MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised   Domain Adaptation in ASR</h2><p><strong>Authors:Dimitrios Damianos, Georgios Paraskevopoulos, Alexandros Potamianos</strong></p>
<p>In this work, we investigate the Meta PL unsupervised domain adaptation framework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage Domain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation approach that integrates self-supervised learning with semi-supervised techniques. MSDA is designed to enhance the robustness and generalization of ASR models, making them more adaptable to diverse conditions. It is particularly effective for low-resource languages like Greek and in weakly supervised scenarios where labeled data is scarce or noisy. Through extensive experiments, we demonstrate that Meta PL can be applied effectively to ASR tasks, achieving state-of-the-art results, significantly outperforming state-of-the-art methods, and providing more robust solutions for unsupervised domain adaptation in ASR. Our ablations highlight the necessity of utilizing a cascading approach when combining self-supervision with self-training. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„Meta PLæ— ç›‘ç£åŸŸè‡ªé€‚åº”æ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šé˜¶æ®µåŸŸè‡ªé€‚åº”ç®¡é“ï¼ˆMSDAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ ·æœ¬é«˜æ•ˆçš„ã€ä¸¤é˜¶æ®µçš„è‡ªé€‚åº”æ–¹æ³•ï¼Œå®ƒå°†è‡ªç›‘ç£å­¦ä¹ ä¸åŠç›‘ç£æŠ€æœ¯ç›¸ç»“åˆã€‚MSDAæ—¨åœ¨æé«˜ASRæ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å®ƒä»¬æ›´èƒ½é€‚åº”å„ç§æ¡ä»¶ã€‚å®ƒå¯¹ä½èµ„æºè¯­è¨€ï¼ˆå¦‚å¸Œè…Šè¯­ï¼‰ä»¥åŠåœ¨æ ‡ç­¾æ•°æ®ç¨€ç¼ºæˆ–å˜ˆæ‚çš„å¼±ç›‘ç£åœºæ™¯ä¸­ç‰¹åˆ«æœ‰æ•ˆã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†Meta PLå¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºASRä»»åŠ¡ï¼Œå®ç°æœ€å…ˆè¿›çš„æˆæœï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸ºASRä¸­çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”æä¾›æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶å¼ºè°ƒäº†å°†è‡ªç›‘ç£ä¸è‡ªè®­ç»ƒç›¸ç»“åˆæ—¶é‡‡ç”¨çº§è”æ–¹æ³•çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24656v2">PDF</a> Submitted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†Meta PLæ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”æ¡†æ¶åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„åº”ç”¨ã€‚æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µé¢†åŸŸè‡ªé€‚åº”ç®¡é“ï¼ˆMSDAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ ·æœ¬é«˜æ•ˆçš„ã€ä¸¤é˜¶æ®µçš„è‡ªé€‚åº”æ–¹æ³•ï¼Œç»“åˆäº†è‡ªç›‘ç£å­¦ä¹ ä¸åŠç›‘ç£æŠ€æœ¯ã€‚MSDAæ—¨åœ¨æé«˜ASRæ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶æ›´é€‚åº”ä¸åŒçš„æ¡ä»¶ã€‚åœ¨èµ„æºæœ‰é™çš„è¯­ç§å¦‚å¸Œè…Šè¯­ä»¥åŠæ ‡ç­¾æ•°æ®ç¨€ç¼ºæˆ–å˜ˆæ‚çš„å¼±ç›‘ç£åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚å®éªŒè¯æ˜ï¼ŒMeta PLå¯æœ‰æ•ˆåº”ç”¨äºASRä»»åŠ¡ï¼Œå®ç°æœ€æ–°æŠ€æœ¯æˆæœï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºASRçš„æ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”æä¾›æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†Meta PLæ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”æ¡†æ¶åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„åº”ç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šé˜¶æ®µé¢†åŸŸè‡ªé€‚åº”ç®¡é“ï¼ˆMSDAï¼‰ï¼Œç»“åˆäº†è‡ªç›‘ç£å­¦ä¹ ä¸åŠç›‘ç£æŠ€æœ¯ã€‚</li>
<li>MSDAå¢å¼ºäº†ASRæ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œé€‚åº”ä¸åŒçš„æ¡ä»¶ã€‚</li>
<li>MSDAåœ¨èµ„æºæœ‰é™çš„è¯­ç§å’Œå¼±ç›‘ç£åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å®éªŒè¯æ˜Meta PLåœ¨ASRä»»åŠ¡ä¸Šå®ç°äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚</li>
<li>MSDAæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24656">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6919ae5663441e597993137ccb66a956.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f4890306a3099cba8a4d16b51184d9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a62e73aef6235c153db5fde894612e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36c92477180313d34eb3075562b193aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e23ab06e38a341c09415fc5b6e7e918.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c49d3df1086fc48f4a5d2b8916cb52a4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Interspeech-2025-URGENT-Speech-Enhancement-Challenge"><a href="#Interspeech-2025-URGENT-Speech-Enhancement-Challenge" class="headerlink" title="Interspeech 2025 URGENT Speech Enhancement Challenge"></a>Interspeech 2025 URGENT Speech Enhancement Challenge</h2><p><strong>Authors:Kohei Saijo, Wangyou Zhang, Samuele Cornell, Robin Scheibler, Chenda Li, Zhaoheng Ni, Anurag Kumar, Marvin Sach, Yihui Fu, Wei Wang, Tim Fingscheidt, Shinji Watanabe</strong></p>
<p>There has been a growing effort to develop universal speech enhancement (SE) to handle inputs with various speech distortions and recording conditions. The URGENT Challenge series aims to foster such universal SE by embracing a broad range of distortion types, increasing data diversity, and incorporating extensive evaluation metrics. This work introduces the Interspeech 2025 URGENT Challenge, the second edition of the series, to explore several aspects that have received limited attention so far: language dependency, universality for more distortion types, data scalability, and the effectiveness of using noisy training data. We received 32 submissions, where the best system uses a discriminative model, while most other competitive ones are hybrid methods. Analysis reveals some key findings: (i) some generative or hybrid approaches are preferred in subjective evaluations over the top discriminative model, and (ii) purely generative SE models can exhibit language dependency. </p>
<blockquote>
<p>åœ¨å¼€å‘èƒ½å¤Ÿå¤„ç†å„ç§è¯­éŸ³å¤±çœŸå’Œå½•éŸ³æ¡ä»¶çš„é€šç”¨è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹é¢ï¼Œå·²ç»ä»˜å‡ºäº†è¶Šæ¥è¶Šå¤šçš„åŠªåŠ›ã€‚URGENT Challengeç³»åˆ—æ—¨åœ¨é€šè¿‡æ¥çº³å¹¿æ³›çš„å¤±çœŸç±»å‹ã€å¢åŠ æ•°æ®å¤šæ ·æ€§å’Œå¼•å…¥å¹¿æ³›çš„è¯„ä¼°æŒ‡æ ‡æ¥ä¿ƒè¿›è¿™ç§é€šç”¨SEçš„å‘å±•ã€‚è¿™é¡¹å·¥ä½œä»‹ç»äº†Interspeech 2025 URGENT Challengeï¼Œå³è¯¥ç³»åˆ—çš„ç¬¬äºŒç‰ˆï¼Œæ—¨åœ¨æ¢ç´¢è¿„ä»Šä¸ºæ­¢å—åˆ°æœ‰é™å…³æ³¨çš„å‡ ä¸ªæ–¹é¢ï¼šè¯­è¨€ä¾èµ–æ€§ã€æ›´å¤šå¤±çœŸç±»å‹çš„æ™®éæ€§ã€æ•°æ®å¯æ‰©å±•æ€§ä»¥åŠä½¿ç”¨å¸¦å™ªå£°è®­ç»ƒæ•°æ®çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æ”¶åˆ°äº†32ä»½æäº¤ï¼Œå…¶ä¸­æœ€ä½³ç³»ç»Ÿä½¿ç”¨çš„æ˜¯åˆ¤åˆ«æ¨¡å‹ï¼Œè€Œå…¶ä»–å¤§å¤šæ•°æœ‰ç«äº‰åŠ›çš„ç³»ç»Ÿéƒ½æ˜¯æ··åˆæ–¹æ³•ã€‚åˆ†ææ­ç¤ºäº†ä¸€äº›å…³é”®å‘ç°ï¼šï¼ˆiï¼‰åœ¨æŸäº›ä¸»è§‚è¯„ä»·ä¸­ï¼Œä¸€äº›ç”Ÿæˆæ€§æˆ–æ··åˆæ–¹æ³•ä¼˜äºé¡¶çº§åˆ¤åˆ«æ¨¡å‹ï¼›ï¼ˆiiï¼‰çº¯ç²¹çš„ç”Ÿæˆæ€§SEæ¨¡å‹å¯èƒ½ä¼šè¡¨ç°å‡ºè¯­è¨€ä¾èµ–æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23212v2">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ®µæ–‡æœ¬ä¸»è¦ä»‹ç»äº†Interspeech 2025 URGENT Challengeç³»åˆ—çš„ç›®æ ‡å’Œæˆæœã€‚è¯¥æŒ‘æˆ˜æ—¨åœ¨å¼€å‘é€šç”¨è¯­éŸ³å¢å¼ºæŠ€æœ¯ï¼Œåº”å¯¹å„ç§è¯­éŸ³å¤±çœŸå’Œå½•éŸ³æ¡ä»¶çš„é—®é¢˜ã€‚å…¶ä¸­å¼•å…¥äº†ç¬¬äºŒæ¬¾æŒ‘æˆ˜ï¼šæ¢ç´¢è¯­è¨€ä¾èµ–æ€§ã€æ›´å¤šå¤±çœŸç±»å‹çš„æ™®éæ€§ã€æ•°æ®å¯æ‰©å±•æ€§ä»¥åŠä½¿ç”¨å™ªå£°è®­ç»ƒæ•°æ®çš„æœ‰æ•ˆæ€§ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼Œä¸€äº›ç”Ÿæˆæ€§æˆ–æ··åˆæ–¹æ³•åœ¨æŸäº›ä¸»è§‚è¯„ä»·ä¸­ä¼˜äºé¡¶çº§åˆ¤åˆ«æ¨¡å‹ï¼Œè€Œçº¯ç²¹çš„ç”Ÿæˆæ€§è¯­éŸ³å¢å¼ºæ¨¡å‹å¯èƒ½å­˜åœ¨è¯­è¨€ä¾èµ–æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Interspeech 2025 URGENT Challengeæ—¨åœ¨å¼€å‘é€šç”¨è¯­éŸ³å¢å¼ºæŠ€æœ¯ï¼Œä»¥å¤„ç†å„ç§è¯­éŸ³å¤±çœŸå’Œå½•éŸ³æ¡ä»¶çš„é—®é¢˜ã€‚</li>
<li>è¯¥æŒ‘æˆ˜çš„ç¬¬äºŒç‰ˆæ¢ç´¢äº†è¯­è¨€ä¾èµ–æ€§ã€æ›´å¤šå¤±çœŸç±»å‹çš„æ™®éæ€§ã€æ•°æ®å¯æ‰©å±•æ€§å’Œä½¿ç”¨å™ªå£°è®­ç»ƒæ•°æ®çš„æœ‰æ•ˆæ€§ç­‰å‡ ä¸ªæ–¹é¢ã€‚</li>
<li>ç›®å‰æ”¶åˆ°çš„32ä»½æäº¤ä¸­ï¼Œæœ€ä½³ç³»ç»Ÿä½¿ç”¨çš„æ˜¯åˆ¤åˆ«æ¨¡å‹ï¼Œè€Œå…¶ä»–æœ‰ç«äº‰åŠ›çš„ç³»ç»Ÿå¤šä¸ºæ··åˆæ–¹æ³•ã€‚</li>
<li>åˆ†æå‘ç°ï¼Œåœ¨æŸäº›ä¸»è§‚è¯„ä»·ä¸­ï¼Œä¸€äº›ç”Ÿæˆæ€§æˆ–æ··åˆæ–¹æ³•å¯èƒ½ä¼˜äºé¡¶çº§åˆ¤åˆ«æ¨¡å‹ã€‚</li>
<li>çº¯ç²¹çš„ç”Ÿæˆæ€§è¯­éŸ³å¢å¼ºæ¨¡å‹å¯èƒ½å­˜åœ¨è¯­è¨€ä¾èµ–æ€§ã€‚</li>
<li>è¯¥æŒ‘æˆ˜æ—¨åœ¨æ‹¥æŠ±å¹¿æ³›çš„å¤±çœŸç±»å‹ï¼Œå¢åŠ æ•°æ®å¤šæ ·æ€§ï¼Œå¹¶èå…¥å¹¿æ³›çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥æ¨åŠ¨é€šç”¨è¯­éŸ³å¢å¼ºæŠ€æœ¯çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23212">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5f12c5f74507f6c5a1bcee56d33d3b64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2e4c333298de0703dfcd6675857f29db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d176c61b24639b6fc514b7f6664d382a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Developing-a-Top-tier-Framework-in-Naturalistic-Conditions-Challenge-for-Categorized-Emotion-Prediction-From-Speech-Foundation-Models-and-Learning-Objective-to-Data-Augmentation-and-Engineering-Choices"><a href="#Developing-a-Top-tier-Framework-in-Naturalistic-Conditions-Challenge-for-Categorized-Emotion-Prediction-From-Speech-Foundation-Models-and-Learning-Objective-to-Data-Augmentation-and-Engineering-Choices" class="headerlink" title="Developing a Top-tier Framework in Naturalistic Conditions Challenge for   Categorized Emotion Prediction: From Speech Foundation Models and Learning   Objective to Data Augmentation and Engineering Choices"></a>Developing a Top-tier Framework in Naturalistic Conditions Challenge for   Categorized Emotion Prediction: From Speech Foundation Models and Learning   Objective to Data Augmentation and Engineering Choices</h2><p><strong>Authors:Tiantian Feng, Thanathai Lertpetchpun, Dani Byrd, Shrikanth Narayanan</strong></p>
<p>Speech emotion recognition (SER), particularly for naturally expressed emotions, remains a challenging computational task. Key challenges include the inherent subjectivity in emotion annotation and the imbalanced distribution of emotion labels in datasets. This paper introduces the \texttt{SAILER} system developed for participation in the INTERSPEECH 2025 Emotion Recognition Challenge (Task 1). The challenge dataset, which contains natural emotional speech from podcasts, serves as a valuable resource for studying imbalanced and subjective emotion annotations. Our system is designed to be simple, reproducible, and effective, highlighting critical choices in modeling, learning objectives, data augmentation, and engineering choices. Results show that even a single system (without ensembling) can outperform more than 95% of the submissions, with a Macro-F1 score exceeding 0.4. Moreover, an ensemble of three systems further improves performance, achieving a competitively ranked score (top-3 performing team). Our model is at: <a target="_blank" rel="noopener" href="https://github.com/tiantiaf0627/vox-profile-release">https://github.com/tiantiaf0627/vox-profile-release</a>. </p>
<blockquote>
<p>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ï¼Œå°¤å…¶æ˜¯å¯¹è‡ªç„¶è¡¨è¾¾æƒ…æ„Ÿçš„è¯†åˆ«ï¼Œä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„è®¡ç®—ä»»åŠ¡ã€‚ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬æƒ…æ„Ÿæ³¨é‡Šä¸­çš„å›ºæœ‰ä¸»è§‚æ€§å’Œæ•°æ®é›†ä¸­æƒ…æ„Ÿæ ‡ç­¾çš„ä¸å¹³è¡¡åˆ†å¸ƒã€‚æœ¬æ–‡ä»‹ç»äº†ä¸ºå‚ä¸INTERSPEECH 2025æƒ…æ„Ÿè¯†åˆ«æŒ‘æˆ˜èµ›ï¼ˆä»»åŠ¡1ï¼‰è€Œå¼€å‘çš„&#96;SAILERâ€™ç³»ç»Ÿã€‚æŒ‘æˆ˜èµ›æ•°æ®é›†åŒ…å«æ¥è‡ªæ’­å®¢çš„è‡ªç„¶æƒ…æ„Ÿè¯­éŸ³ï¼Œæ˜¯ç ”ç©¶ä¸å¹³è¡¡å’Œä¸»è§‚æƒ…æ„Ÿæ³¨é‡Šçš„å®è´µèµ„æºã€‚æˆ‘ä»¬çš„ç³»ç»Ÿè®¾è®¡ç®€å•ã€å¯å¤åˆ¶ã€æœ‰æ•ˆï¼Œçªå‡ºæ˜¾ç¤ºå»ºæ¨¡ã€å­¦ä¹ ç›®æ ‡ã€æ•°æ®å¢å¼ºå’Œå·¥ç¨‹é€‰æ‹©ä¸­çš„å…³é”®é€‰æ‹©ã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯ä¸€ä¸ªç³»ç»Ÿï¼ˆæ— éœ€é›†åˆï¼‰ä¹Ÿèƒ½è¡¨ç°å‡ºè¶…è¿‡95%æäº¤çš„ç»“æœï¼Œå®è§‚F1åˆ†æ•°è¶…è¿‡0.4ã€‚æ­¤å¤–ï¼Œä¸‰ä¸ªç³»ç»Ÿçš„ç»„åˆè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œå–å¾—äº†ç«äº‰æ’ååˆ†æ•°ï¼ˆæ’åå‰ä¸‰çš„å›¢é˜Ÿï¼‰ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/tiantiaf0627/vox-profile-release%E3%80%82">https://github.com/tiantiaf0627/vox-profile-releaseã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22133v2">PDF</a> Accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡é’ˆå¯¹è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æƒ…æ„Ÿæ ‡æ³¨çš„ä¸»è§‚æ€§å’Œæ•°æ®é›†æƒ…æ„Ÿæ ‡ç­¾çš„ä¸å¹³è¡¡åˆ†å¸ƒï¼Œæå‡ºäº†ä¸ºå‚ä¸INTERSPEECH 2025æƒ…æ„Ÿè¯†åˆ«æŒ‘æˆ˜èµ›ï¼ˆä»»åŠ¡ä¸€ï¼‰è€Œå¼€å‘çš„<code>SAILER</code>ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿæ—¨åœ¨è®¾è®¡ç®€å•ã€å¯å¤åˆ¶å’Œé«˜æ•ˆï¼Œå¼ºè°ƒå»ºæ¨¡ã€å­¦ä¹ ç›®æ ‡ã€æ•°æ®å¢å¼ºå’Œå·¥ç¨‹é€‰æ‹©çš„é‡è¦æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå•ä¸€ç³»ç»Ÿæ€§èƒ½å·²è¶…è¶Šå¤§å¤šæ•°æäº¤ç»“æœï¼Œå®è§‚F1åˆ†æ•°è¶…è¿‡0.4ã€‚é€šè¿‡é›†æˆä¸‰ä¸ªç³»ç»Ÿï¼Œæ€§èƒ½è¿›ä¸€æ­¥æå‡ï¼Œæˆä¸ºæ’åå‰ä¸‰çš„å›¢é˜Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æŒ‡å‡ºè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿæ ‡æ³¨çš„ä¸»è§‚æ€§å’Œæ•°æ®é›†æƒ…æ„Ÿæ ‡ç­¾çš„ä¸å¹³è¡¡åˆ†å¸ƒã€‚</li>
<li>ä»‹ç»äº†ä¸ºINTERSPEECH 2025æƒ…æ„Ÿè¯†åˆ«æŒ‘æˆ˜èµ›å¼€å‘çš„<code>SAILER</code>ç³»ç»Ÿã€‚</li>
<li><code>SAILER</code>ç³»ç»Ÿçš„è®¾è®¡é‡ç‚¹æ˜¯ç®€å•æ€§ã€å¯å¤åˆ¶æ€§å’Œé«˜æ•ˆæ€§ã€‚</li>
<li>è®ºæ–‡å¼ºè°ƒäº†å»ºæ¨¡ã€å­¦ä¹ ç›®æ ‡ã€æ•°æ®å¢å¼ºå’Œå·¥ç¨‹é€‰æ‹©çš„é‡è¦æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå•ä¸€ç³»ç»Ÿçš„æ€§èƒ½å·²è¶…è¶Šå¤§å¤šæ•°æäº¤çš„ç»“æœï¼Œå®è§‚F1åˆ†æ•°è¶…è¿‡0.4ã€‚</li>
<li>é€šè¿‡é›†æˆä¸‰ä¸ªç³»ç»Ÿï¼Œæ€§èƒ½è¿›ä¸€æ­¥æå‡ï¼Œæˆä¸ºç«èµ›ä¸­æ’åé å‰çš„å›¢é˜Ÿã€‚</li>
<li>è®ºæ–‡æä¾›äº†å…¶æ¨¡å‹çš„å…¬å¼€è®¿é—®é“¾æ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22133">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-49f9436a519b0100b6b1602b8cf38d3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c33138f2f85cfc98882eb1c1055f187d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-537229c6ae9fd890d7603f5de6d96722.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77e9fcbe821c9847bc482c87adefc4c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64f566e231dd16babc980ce9ea30a2ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b99b4edcf6885b3f8abf53f34225cc5d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ClearSphere-Multi-Earphone-Synergy-for-Enhanced-Conversational-Clarity"><a href="#ClearSphere-Multi-Earphone-Synergy-for-Enhanced-Conversational-Clarity" class="headerlink" title="ClearSphere: Multi-Earphone Synergy for Enhanced Conversational Clarity"></a>ClearSphere: Multi-Earphone Synergy for Enhanced Conversational Clarity</h2><p><strong>Authors:Lixing He</strong></p>
<p>In crowded places such as conferences, background noise, overlapping voices, and lively interactions make it difficult to have clear conversations. This situation often worsens the phenomenon known as â€œcocktail party deafness.â€ We present ClearSphere, the collaborative system that enhances speech at the conversation level with multi-earphones. Real-time conversation enhancement requires a holistic modeling of all the members in the conversation, and an effective way to extract the speech from the mixture. ClearSphere bridges the acoustic sensor system and state-of-the-art deep learning for target speech extraction by making two key contributions: 1) a conversation-driven network protocol, and 2) a robust target conversation extraction model. Our networking protocol enables mobile, infrastructure-free coordination among earphone devices. Our conversation extraction model can leverage the relay audio in a bandwidth-efficient way. ClearSphere is evaluated in both real-world experiments and simulations. Results show that our conversation network obtains more than 90% accuracy in group formation, improves the speech quality by up to 8.8 dB over state-of-the-art baselines, and demonstrates real-time performance on a mobile device. In a user study with 20 participants, ClearSphere has a much higher score than baseline with good usability. </p>
<blockquote>
<p>åœ¨ä¼šè®®ç­‰æ‹¥æŒ¤åœºæ‰€ï¼ŒèƒŒæ™¯å™ªéŸ³ã€å£°éŸ³é‡å ä»¥åŠæ´»è·ƒçš„äº’åŠ¨ä½¿å¾—è¿›è¡Œæ¸…æ™°çš„å¯¹è¯å˜å¾—å›°éš¾ã€‚è¿™ç§æƒ…å†µä¼šåŠ å‰§æ‰€è°“çš„â€œé¸¡å°¾é…’ä¼šè€³è‹â€ç°è±¡ã€‚æˆ‘ä»¬æ¨å‡ºäº†ClearSphereï¼Œè¿™æ˜¯ä¸€æ¬¾åä½œç³»ç»Ÿï¼Œé€šè¿‡å¤šè€³æœºå¢å¼ºå¯¹è¯çº§åˆ«çš„è¯­éŸ³ã€‚å®æ—¶å¯¹è¯å¢å¼ºéœ€è¦å¯¹æ‰€æœ‰å¯¹è¯æˆå‘˜è¿›è¡Œæ•´ä½“å»ºæ¨¡ï¼Œå¹¶éœ€è¦ä¸€ç§ä»æ··åˆè¯­éŸ³ä¸­æå–ç›®æ ‡è¯­éŸ³çš„æœ‰æ•ˆæ–¹æ³•ã€‚ClearSphereé€šè¿‡ä¸¤é¡¹å…³é”®è´¡çŒ®â€”â€”å¯¹è¯é©±åŠ¨çš„ç½‘ç»œåè®®å’Œç¨³å¥çš„ç›®æ ‡å¯¹è¯æå–æ¨¡å‹ï¼Œæ­å»ºäº†å£°å­¦ä¼ æ„Ÿå™¨ç³»ç»Ÿå’Œæœ€æ–°çš„æ·±åº¦å­¦ä¹ ç›®æ ‡è¯­éŸ³æå–ä¹‹é—´çš„æ¡¥æ¢ã€‚æˆ‘ä»¬çš„ç½‘ç»œåè®®å®ç°äº†è€³æœºè®¾å¤‡ä¹‹é—´æ— éœ€åŸºç¡€è®¾æ–½çš„ç§»åŠ¨åè°ƒã€‚æˆ‘ä»¬çš„å¯¹è¯æå–æ¨¡å‹èƒ½å¤Ÿä»¥å¸¦å®½æœ‰æ•ˆçš„æ–¹å¼åˆ©ç”¨ä¸­ç»§éŸ³é¢‘ã€‚ClearSphereåœ¨çœŸå®å®éªŒå’Œæ¨¡æ‹Ÿä¸­éƒ½å¾—åˆ°äº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¯¹è¯ç½‘ç»œåœ¨ç»„é˜Ÿæ–¹é¢çš„å‡†ç¡®ç‡è¶…è¿‡90%ï¼Œåœ¨æœ€æ–°åŸºçº¿çš„åŸºç¡€ä¸Šæ”¹å–„äº†é«˜è¾¾8.8åˆ†è´çš„è¯­éŸ³è´¨é‡ï¼Œå¹¶åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®ç°äº†å®æ—¶æ€§èƒ½ã€‚åœ¨20åå‚ä¸è€…çš„ä¸€é¡¹ç”¨æˆ·ç ”ç©¶ä¸­ï¼ŒClearSphereçš„å¾—åˆ†è¿œé«˜äºåŸºçº¿ï¼Œå…·æœ‰è‰¯å¥½çš„å¯ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21004v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>åœ¨ä¼šè®®ç­‰æ‹¥æŒ¤åœºåˆï¼ŒèƒŒæ™¯å™ªéŸ³ã€å£°éŸ³é‡å åŠæ´»è·ƒäº’åŠ¨ä½¿å¾—æ¸…æ™°äº¤æµå˜å¾—å›°éš¾ï¼ŒåŠ å‰§äº†â€œé¸¡å°¾é…’ä¼šè€³è‹â€ç°è±¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ClearSphereåä½œç³»ç»Ÿï¼Œé€šè¿‡å¤šè€³æœºå¢å¼ºå¯¹è¯çº§åˆ«çš„è¯­éŸ³ã€‚å®æ—¶å¯¹è¯å¢å¼ºéœ€è¦å…¨é¢å»ºæ¨¡æ‰€æœ‰å‚ä¸è€…çš„å£°éŸ³ï¼Œå¹¶æœ‰æ•ˆåœ°ä»æ··åˆå£°éŸ³ä¸­æå–ç›®æ ‡è¯­éŸ³ã€‚ClearSphereé€šè¿‡ä¸¤é¡¹å…³é”®è´¡çŒ®â€”â€”å¯¹è¯é©±åŠ¨çš„ç½‘ç»œåè®®å’Œç¨³å¥çš„ç›®æ ‡å¯¹è¯æå–æ¨¡å‹ï¼Œå°†å£°å­¦ä¼ æ„Ÿå™¨ç³»ç»Ÿå’Œæœ€æ–°çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ç›¸ç»“åˆï¼Œå®ç°ç›®æ ‡è¯­éŸ³æå–ã€‚ç½‘ç»œåè®®å¯å®ç°è€³æœºè®¾å¤‡ä¹‹é—´çš„ç§»åŠ¨ã€æ— åŸºç¡€è®¾æ–½åè°ƒã€‚å¯¹è¯æå–æ¨¡å‹èƒ½å¤Ÿä»¥å¸¦å®½æœ‰æ•ˆçš„æ–¹å¼åˆ©ç”¨ä¸­ç»§éŸ³é¢‘ã€‚ClearSphereåœ¨çœŸå®å®éªŒå’Œæ¨¡æ‹Ÿä¸­å‡å¾—åˆ°è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºï¼Œå…¶å¯¹è¯ç½‘ç»œåœ¨ç¾¤ä½“å½¢æˆæ–¹é¢çš„å‡†ç¡®ç‡è¶…è¿‡90%ï¼Œåœ¨è¯­éŸ³è´¨é‡ä¸Šæ¯”æœ€æ–°åŸºçº¿æé«˜äº†é«˜è¾¾8.8åˆ†è´ï¼Œå¹¶åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®ç°äº†å®æ—¶æ€§èƒ½ã€‚åœ¨ç”¨æˆ·ç ”ç©¶ä¸­ï¼ŒClearSphereè¾ƒåŸºçº¿äº§å“å…·æœ‰æ›´é«˜å¾—åˆ†å’Œè‰¯å¥½çš„å¯ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åœ¨æ‹¥æŒ¤åœºæ‰€å¦‚ä¼šè®®ä¸­ï¼ŒèƒŒæ™¯å™ªéŸ³å’Œå£°éŸ³é‡å ä½¿å¾—æ¸…æ™°äº¤æµå˜å¾—å›°éš¾ï¼ŒåŠ å‰§äº†â€œé¸¡å°¾é…’ä¼šè€³è‹â€ç°è±¡ã€‚</li>
<li>ClearSphereæ˜¯ä¸€ä¸ªåä½œç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡å¤šè€³æœºå¢å¼ºå¯¹è¯çº§åˆ«çš„è¯­éŸ³ã€‚</li>
<li>å®æ—¶å¯¹è¯å¢å¼ºéœ€è¦å…¨é¢å»ºæ¨¡æ‰€æœ‰å‚ä¸è€…çš„å£°éŸ³å¹¶æœ‰æ•ˆæå–ç›®æ ‡è¯­éŸ³ã€‚</li>
<li>ClearSphereé€šè¿‡å¯¹è¯é©±åŠ¨çš„ç½‘ç»œåè®®å’Œç¨³å¥çš„ç›®æ ‡å¯¹è¯æå–æ¨¡å‹å®ç°ç›®æ ‡è¯­éŸ³æå–ã€‚</li>
<li>ç½‘ç»œåè®®æ”¯æŒè€³æœºè®¾å¤‡é—´çš„ç§»åŠ¨ã€æ— åŸºç¡€è®¾æ–½åè°ƒã€‚</li>
<li>ClearSphereåœ¨çœŸå®å®éªŒå’Œæ¨¡æ‹Ÿä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¯¹è¯ç½‘ç»œå‡†ç¡®ç‡è¶…è¿‡90%ï¼Œè¯­éŸ³è´¨é‡æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1efff593247fc488a9a688acb88b9274.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4a831015983902fb22023d7590083c5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SoloSpeech-Enhancing-Intelligibility-and-Quality-in-Target-Speech-Extraction-through-a-Cascaded-Generative-Pipeline"><a href="#SoloSpeech-Enhancing-Intelligibility-and-Quality-in-Target-Speech-Extraction-through-a-Cascaded-Generative-Pipeline" class="headerlink" title="SoloSpeech: Enhancing Intelligibility and Quality in Target Speech   Extraction through a Cascaded Generative Pipeline"></a>SoloSpeech: Enhancing Intelligibility and Quality in Target Speech   Extraction through a Cascaded Generative Pipeline</h2><p><strong>Authors:Helin Wang, Jiarui Hai, Dongchao Yang, Chen Chen, Kai Li, Junyi Peng, Thomas Thebaud, Laureano Moro Velazquez, Jesus Villalba, Najim Dehak</strong></p>
<p>Target Speech Extraction (TSE) aims to isolate a target speakerâ€™s voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high perceptual quality, these models often introduce unwanted artifacts, reduce naturalness, and are sensitive to discrepancies between training and testing environments. On the other hand, generative models for TSE lag in perceptual quality and intelligibility. To address these challenges, we present SoloSpeech, a novel cascaded generative pipeline that integrates compression, extraction, reconstruction, and correction processes. SoloSpeech features a speaker-embedding-free target extractor that utilizes conditional information from the cue audioâ€™s latent space, aligning it with the mixture audioâ€™s latent space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset, SoloSpeech achieves the new state-of-the-art intelligibility and quality in target speech extraction and speech separation tasks while demonstrating exceptional generalization on out-of-domain data and real-world scenarios. </p>
<blockquote>
<p>ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰æ—¨åœ¨åˆ©ç”¨ç‰¹å®šäºè¯´è¯è€…çš„çº¿ç´¢ï¼ˆé€šå¸¸ä½œä¸ºè¾…åŠ©éŸ³é¢‘ï¼ˆå³çº¿ç´¢éŸ³é¢‘ï¼‰æä¾›ï¼‰ä»å¤šä¸ªè¯´è¯è€…çš„æ··åˆå£°éŸ³ä¸­åˆ†ç¦»å‡ºç›®æ ‡è¯´è¯è€…çš„å£°éŸ³ã€‚å°½ç®¡æœ€è¿‘çš„TSEè¿›å±•ä¸»è¦é‡‡ç”¨äº†æä¾›é«˜æ„ŸçŸ¥è´¨é‡çš„åˆ¤åˆ«æ¨¡å‹ï¼Œä½†è¿™äº›æ¨¡å‹å¾€å¾€å¼•å…¥äº†ä¸éœ€è¦çš„ä¼ªå½±ï¼Œé™ä½äº†è‡ªç„¶åº¦ï¼Œå¹¶ä¸”å¯¹è®­ç»ƒå’Œæµ‹è¯•ç¯å¢ƒä¹‹é—´çš„å·®å¼‚å¾ˆæ•æ„Ÿã€‚å¦ä¸€æ–¹é¢ï¼ŒTSEçš„ç”Ÿæˆæ¨¡å‹åœ¨æ„ŸçŸ¥è´¨é‡å’Œæ¸…æ™°åº¦æ–¹é¢æœ‰æ‰€ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SoloSpeechï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çº§è”ç”Ÿæˆç®¡é“ï¼Œå®ƒé›†æˆäº†å‹ç¼©ã€æå–ã€é‡å»ºå’Œæ ¡æ­£è¿‡ç¨‹ã€‚SoloSpeechçš„ç‰¹ç‚¹æ˜¯æ— éœ€è¯´è¯è€…åµŒå…¥çš„ç›®æ ‡æå–å™¨ï¼Œå®ƒåˆ©ç”¨çº¿ç´¢éŸ³é¢‘çš„æ½œåœ¨ç©ºé—´çš„æ¡ä»¶ä¿¡æ¯ï¼Œå°†å…¶ä¸æ··åˆéŸ³é¢‘çš„æ½œåœ¨ç©ºé—´å¯¹é½ï¼Œä»¥é˜²æ­¢ä¸åŒ¹é…ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„Libri2Mixæ•°æ®é›†ä¸Šï¼ŒSoloSpeechåœ¨ç›®æ ‡è¯­éŸ³æå–å’Œè¯­éŸ³åˆ†ç¦»ä»»åŠ¡ä¸­å®ç°äº†æœ€æ–°çš„æœ€é«˜æ¸…æ™°åº¦å’Œè´¨é‡ï¼ŒåŒæ—¶åœ¨åŸŸå¤–æ•°æ®å’Œç°å®åœºæ™¯ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19314v2">PDF</a> </p>
<p><strong>Summary</strong><br>ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰æ—¨åœ¨åˆ©ç”¨è¯´è¯äººç‰¹å®šçº¿ç´¢ä»å¤šä¸ªè¯´è¯äººçš„æ··åˆè¯­éŸ³ä¸­åˆ†ç¦»å‡ºç›®æ ‡è¯´è¯äººçš„å£°éŸ³ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶ä¸»è¦é‡‡ç”¨æä¾›é«˜æ„ŸçŸ¥è´¨é‡çš„åˆ¤åˆ«æ¨¡å‹ï¼Œä½†è¿™äº›æ¨¡å‹å¸¸å¼•å…¥ä¸å¿…è¦çš„ä¼ªå½±ã€é™ä½è‡ªç„¶åº¦ï¼Œå¹¶å¯¹è®­ç»ƒå’Œæµ‹è¯•ç¯å¢ƒçš„å·®å¼‚æ•æ„Ÿã€‚å¦ä¸€æ–¹é¢ï¼Œç”Ÿæˆå¼TSEæ¨¡å‹åœ¨æ„ŸçŸ¥è´¨é‡å’Œæ¸…æ™°åº¦æ–¹é¢æ»åã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ— è¯´è¯äººåµŒå…¥çš„SoloSpeechï¼Œè¿™æ˜¯ä¸€ä¸ªé›†æˆå‹ç¼©ã€æå–ã€é‡å»ºå’Œæ ¡æ­£è¿‡ç¨‹çš„æ–°å‹çº§è”ç”Ÿæˆç®¡é“ã€‚å®ƒåœ¨å¹¿æ³›ä½¿ç”¨çš„Libri2Mixæ•°æ®é›†ä¸Šå®ç°äº†ç›®æ ‡è¯­éŸ³æå–å’Œè¯­éŸ³åˆ†ç¦»ä»»åŠ¡çš„æ–°é¢–æ€§å’Œå“è¶Šæ€§ï¼ŒåŒæ—¶åœ¨åŸŸå¤–æ•°æ®å’ŒçœŸå®ä¸–ç•Œåœºæ™¯ä¸Šè¡¨ç°å‡ºå‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰æ—¨åœ¨ä»æ··åˆè¯­éŸ³ä¸­åˆ†ç¦»å‡ºç›®æ ‡è¯´è¯äººçš„å£°éŸ³ã€‚</li>
<li>ç°æœ‰æ¨¡å‹é¢ä¸´å¼•å…¥ä¼ªå½±ã€é™ä½è‡ªç„¶åº¦åŠå¯¹ç¯å¢ƒå·®å¼‚æ•æ„Ÿçš„æŒ‘æˆ˜ã€‚</li>
<li>ç”Ÿæˆå¼TSEæ¨¡å‹åœ¨æ„ŸçŸ¥è´¨é‡å’Œæ¸…æ™°åº¦æ–¹é¢æœ‰å¾…æé«˜ã€‚</li>
<li>SoloSpeechæ˜¯ä¸€ä¸ªæ–°å‹çº§è”ç”Ÿæˆç®¡é“ï¼Œé›†æˆå‹ç¼©ã€æå–ã€é‡å»ºå’Œæ ¡æ­£è¿‡ç¨‹ã€‚</li>
<li>SoloSpeechåˆ©ç”¨æ¡ä»¶ä¿¡æ¯ï¼Œé€šè¿‡æ½œåœ¨ç©ºé—´å¯¹é½ï¼Œå®ç°ç›®æ ‡è¯­éŸ³çš„ç²¾å‡†æå–ã€‚</li>
<li>åœ¨Libri2Mixæ•°æ®é›†ä¸Šï¼ŒSoloSpeechè¾¾åˆ°äº†ç›®æ ‡è¯­éŸ³æå–å’Œè¯­éŸ³åˆ†ç¦»ä»»åŠ¡çš„æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19314">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-052d8aaa1f2a8e254c974da79f4e20ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e800b7845030112c089c4dee17be2d3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d4b3411393f5cd9d4a494f56120efb4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f7a660eb64f8d76b8fc7c66d5640d567.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b1a4ab8a735330ac8c22f03b1727d5d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Effect-of-Segmentation-and-Vocabulary-Size-on-Speech-Tokenization-for-Speech-Language-Models"><a href="#Exploring-the-Effect-of-Segmentation-and-Vocabulary-Size-on-Speech-Tokenization-for-Speech-Language-Models" class="headerlink" title="Exploring the Effect of Segmentation and Vocabulary Size on Speech   Tokenization for Speech Language Models"></a>Exploring the Effect of Segmentation and Vocabulary Size on Speech   Tokenization for Speech Language Models</h2><p><strong>Authors:Shunsuke Kando, Yusuke Miyao, Shinnosuke Takamichi</strong></p>
<p>The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed&#x2F;variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding. </p>
<blockquote>
<p>è¯­éŸ³åˆ‡è¯çš„ç›®çš„æ˜¯å°†è¯­éŸ³ä¿¡å·è½¬æ¢ä¸ºä¸€ç³»åˆ—ç¦»æ•£è¡¨ç¤ºï¼Œä½œä¸ºè¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„åŸºç¡€ã€‚è™½ç„¶è¯­éŸ³åˆ‡è¯æœ‰å¾ˆå¤šé€‰é¡¹ï¼Œä½†å®ƒä»¬å¯¹SLMæ€§èƒ½çš„å½±å“ä»ä¸æ¸…æ¥šã€‚æœ¬æ–‡ç ”ç©¶äº†è¯­éŸ³åˆ‡è¯çš„ä¸¤ä¸ªæ–¹é¢ï¼šåˆ†æ®µå®½åº¦å’Œç¦»æ•£å•å…ƒç°‡çš„å¤§å°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†è¯­éŸ³ä¿¡å·åˆ†æ®µä¸ºå›ºå®š&#x2F;å¯å˜å®½åº¦å’Œæ± åŒ–è¡¨ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªç°‡å¤§å°ä¸Šè®­ç»ƒK-meansæ¨¡å‹ã€‚é€šè¿‡å¯¹é›¶æ ·æœ¬å£è¯­ç†è§£åŸºå‡†çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°é€‚åº¦ç²—ç³™çš„åˆ†æ®µå’Œè¾ƒå¤§çš„ç°‡å¤§å°å…·æœ‰ç§¯æçš„å½±å“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¡¨ç°æœ€ä½³çš„æ¨¡å‹ä¸­ï¼Œæœ€æœ‰æ•ˆçš„æ¨¡å‹å®ç°äº†è®­ç»ƒæ•°æ®50%çš„å‡å°‘å’Œè®­ç»ƒè¿è¡Œæ—¶é—´70%çš„å‡å°‘ã€‚æˆ‘ä»¬çš„åˆ†æå¼ºè°ƒäº†ç»“åˆå¤šä¸ªä»¤ç‰Œä»¥å¢å¼ºç²¾ç»†å£è¯­ç†è§£çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17446v2">PDF</a> Accepted to Interspeech2025</p>
<p><strong>Summary</strong><br>è¯­éŸ³æ ‡è®°åŒ–çš„ç›®çš„æ˜¯å°†è¯­éŸ³ä¿¡å·è½¬æ¢ä¸ºç¦»æ•£è¡¨ç¤ºçš„åºåˆ—ï¼Œä¸ºè¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰æ‰“ä¸‹åŸºç¡€ã€‚æœ¬æ–‡æ¢è®¨äº†è¯­éŸ³æ ‡è®°åŒ–çš„ä¸¤ä¸ªå…³é”®æ–¹é¢ï¼šåˆ†æ®µå®½åº¦å’Œç¦»æ•£å•å…ƒçš„ç°‡å¤§å°ã€‚ç ”ç©¶å‘ç°ï¼Œé€‚åº¦ç²—ç³™çš„åˆ†æ®µå’Œè¾ƒå¤§çš„ç°‡å¤§å°å¯¹æå‡æ¨¡å‹æ€§èƒ½æœ‰æ­£é¢æ•ˆæœï¼Œæœ€é«˜æ•ˆçš„æ¨¡å‹èƒ½åœ¨è®­ç»ƒæ•°æ®å’Œè®­ç»ƒæ—¶é—´ä¸Šåˆ†åˆ«å‡å°‘50%å’Œ70%ã€‚åˆ†æå¼ºè°ƒäº†ç»“åˆå¤šä¸ªæ ‡è®°ä»¥å¢å¼ºç²¾ç»†ç²’åº¦è¯­éŸ³è¯­è¨€ç†è§£çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æ ‡è®°åŒ–æ˜¯å°†è¯­éŸ³ä¿¡å·è½¬æ¢ä¸ºç¦»æ•£è¡¨ç¤ºçš„åºåˆ—çš„è¿‡ç¨‹ï¼Œä¸ºè¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰å¥ å®šåŸºç¡€ã€‚</li>
<li>æœ¬æ–‡ç ”ç©¶äº†è¯­éŸ³æ ‡è®°åŒ–çš„ä¸¤ä¸ªå…³é”®å‚æ•°ï¼šåˆ†æ®µå®½åº¦å’Œç°‡å¤§å°ã€‚</li>
<li>é€‚åº¦ç²—ç³™çš„åˆ†æ®µå’Œè¾ƒå¤§çš„ç°‡å¤§å°å¯¹SLMçš„æ€§èƒ½æœ‰ç§¯æå½±å“ã€‚</li>
<li>æœ€é«˜æ•ˆçš„æ¨¡å‹åœ¨å‡å°‘è®­ç»ƒæ•°æ®å’Œè®­ç»ƒæ—¶é—´æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
<li>ç»“åˆå¤šä¸ªæ ‡è®°æœ‰åŠ©äºå¢å¼ºç²¾ç»†ç²’åº¦çš„è¯­éŸ³è¯­è¨€ç†è§£ã€‚</li>
<li>è¯­éŸ³æ ‡è®°åŒ–çš„ç ”ç©¶å¯¹äºæé«˜SLMçš„æ€§èƒ½å…·æœ‰æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡è¯­éŸ³æ•°æ®æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c516cf5a9658057f366122ef25bb1538.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd5fdc9a7fc3eedf4fdf004361f2788a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb2c99cf1566ee39243d28fbf980bee3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-257579c8a30101a6150724a5728da600.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-975bc3cccba45a17f24ae9ec37c77142.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c76bc3715c57c5168db5014fe616f533.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="An-End-to-End-Approach-for-Child-Reading-Assessment-in-the-Xhosa-Language"><a href="#An-End-to-End-Approach-for-Child-Reading-Assessment-in-the-Xhosa-Language" class="headerlink" title="An End-to-End Approach for Child Reading Assessment in the Xhosa   Language"></a>An End-to-End Approach for Child Reading Assessment in the Xhosa   Language</h2><p><strong>Authors:Sergio Chevtchenko, Nikhil Navas, Rafaella Vale, Franco Ubaudi, Sipumelele Lucwaba, Cally Ardington, Soheil Afshar, Mark Antoniou, Saeed Afshar</strong></p>
<p>Child literacy is a strong predictor of life outcomes at the subsequent stages of an individualâ€™s life. This points to a need for targeted interventions in vulnerable low and middle income populations to help bridge the gap between literacy levels in these regions and high income ones. In this effort, reading assessments provide an important tool to measure the effectiveness of these programs and AI can be a reliable and economical tool to support educators with this task. Developing accurate automatic reading assessment systems for child speech in low-resource languages poses significant challenges due to limited data and the unique acoustic properties of childrenâ€™s voices. This study focuses on Xhosa, a language spoken in South Africa, to advance child speech recognition capabilities. We present a novel dataset composed of child speech samples in Xhosa. The dataset is available upon request and contains ten words and letters, which are part of the Early Grade Reading Assessment (EGRA) system. Each recording is labeled with an online and cost-effective approach by multiple markers and a subsample is validated by an independent EGRA reviewer. This dataset is evaluated with three fine-tuned state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The results indicate that the performance of these models can be significantly influenced by the amount and balancing of the available training data, which is fundamental for cost-effective large dataset collection. Furthermore, our experiments indicate that the wav2vec 2.0 performance is improved by training on multiple classes at a time, even when the number of available samples is constrained. </p>
<blockquote>
<p>å„¿ç«¥è¯†å­—èƒ½åŠ›å¯¹å…¶åç»­ç”Ÿå‘½é˜¶æ®µçš„ç”Ÿæ´»æˆæœå…·æœ‰å¾ˆå¼ºçš„é¢„æµ‹ä½œç”¨ã€‚è¿™æŒ‡å‡ºäº†é’ˆå¯¹è„†å¼±çš„ä¸­ä½æ”¶å…¥äººç¾¤è¿›è¡Œé’ˆå¯¹æ€§å¹²é¢„çš„å¿…è¦æ€§ï¼Œä»¥å¸®åŠ©ç¼©å°è¿™äº›åœ°åŒºè¯†å­—æ°´å¹³ä¸é«˜æ”¶å…¥åœ°åŒºä¹‹é—´çš„å·®è·ã€‚åœ¨è¿™æ–¹é¢ï¼Œé˜…è¯»è¯„ä¼°æ˜¯è¡¡é‡è¿™äº›èŠ‚ç›®æ•ˆæœçš„é‡è¦å·¥å…·ï¼Œäººå·¥æ™ºèƒ½å¯ä»¥æˆä¸ºæ”¯æŒæ•™è‚²å·¥ä½œè€…å®Œæˆè¿™é¡¹ä»»åŠ¡çš„å¯é ä¸”ç»æµçš„å·¥å…·ã€‚ä¸ºä½èµ„æºè¯­è¨€çš„å­©å­å¼€å‘å‡†ç¡®çš„è‡ªåŠ¨é˜…è¯»è¯„ä¼°ç³»ç»Ÿé¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºæ•°æ®æœ‰é™ä¸”å„¿ç«¥çš„å£°éŸ³å…·æœ‰ç‹¬ç‰¹çš„å£°å­¦ç‰¹æ€§ã€‚è¿™é¡¹ç ”ç©¶ä¸“æ³¨äºå—éä½¿ç”¨çš„è¯­è¨€â€”â€”ç§‘è¨è¯­ï¼Œä»¥æå‡å„¿ç«¥è¯­éŸ³è¯†åˆ«èƒ½åŠ›ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªç”±ç§‘è¨è¯­å„¿ç«¥è¯­éŸ³æ ·æœ¬ç»„æˆçš„æ–°æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†å¯åœ¨è¯·æ±‚åè·å¾—ï¼ŒåŒ…å«åä¸ªå•è¯å’Œå­—æ¯ï¼Œè¿™äº›å•è¯å’Œå­—æ¯æ˜¯åˆçº§é˜…è¯»è¯„ä¼°ï¼ˆEGRAï¼‰ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†ã€‚æ¯ä¸ªå½•éŸ³éƒ½é€šè¿‡å¤šä¸ªæ ‡è®°å™¨ä»¥åœ¨çº¿å’Œä½æˆæœ¬çš„æ–¹å¼è¿›è¡Œæ ‡è®°ï¼Œå¹¶ç”±ç‹¬ç«‹çš„EGRAå®¡æŸ¥è€…å¯¹æ ·æœ¬è¿›è¡ŒéªŒè¯ã€‚è¯¥æ•°æ®é›†ç»è¿‡ä¸‰ç§ç»è¿‡å¾®è°ƒçš„æœ€å…ˆè¿›ç«¯åˆ°ç«¯æ¨¡å‹çš„è¯„ä¼°ï¼šwav2vec 2.0ã€HuBERTå’ŒWhisperã€‚ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹çš„æ€§èƒ½å¯èƒ½ä¼šå—åˆ°å¯ç”¨è®­ç»ƒæ•°æ®é‡å’Œå¹³è¡¡æ€§çš„æ˜¾è‘—å½±å“ï¼Œè¿™å¯¹äºä½æˆæœ¬å¤§è§„æ¨¡æ•°æ®é›†æ”¶é›†è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å¯ç”¨æ ·æœ¬æ•°é‡å—é™çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä¸€æ¬¡è®­ç»ƒå¤šä¸ªç±»åˆ«ä¹Ÿå¯ä»¥æé«˜wav2vec 2.0çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17371v2">PDF</a> Paper accepted on AIED 2025 containing 14 pages, 6 figures and 4   tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å„¿ç«¥è¯†å­—èƒ½åŠ›å¯¹å…¶åç»­ç”Ÿæ´»é˜¶æ®µçš„å½±å“å…·æœ‰æ˜¾è‘—é¢„æµ‹ä½œç”¨ï¼Œè¿™çªæ˜¾äº†å¯¹è„†å¼±ä½æ”¶å…¥å’Œä¸­æ”¶å…¥ç¾¤ä½“è¿›è¡Œé’ˆå¯¹æ€§å¹²é¢„çš„å¿…è¦æ€§ï¼Œä»¥å¸®åŠ©ç¼©å°è¿™äº›åœ°åŒºä¸é«˜æ”¶å…¥åœ°åŒºè¯†å­—æ°´å¹³çš„å·®è·ã€‚é˜…è¯»è¯„ä¼°æ˜¯è¡¡é‡è¿™äº›é¡¹ç›®æˆæ•ˆçš„é‡è¦å·¥å…·ï¼Œè€Œäººå·¥æ™ºèƒ½å¯ä¸ºæ•™è‚²å·¥ä½œè€…æä¾›å¯é ä¸”ç»æµçš„æ”¯æŒã€‚åœ¨ä¸ºä½èµ„æºè¯­è¨€å¼€å‘å‡†ç¡®çš„è‡ªåŠ¨é˜…è¯»è¯„ä¼°ç³»ç»Ÿæ—¶ï¼Œç”±äºæ•°æ®æœ‰é™ä»¥åŠå„¿ç«¥å£°éŸ³çš„ç‹¬ç‰¹å£°å­¦ç‰¹æ€§ï¼Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å…³æ³¨å—éä½¿ç”¨çš„è¯­è¨€â€”â€”ç§‘è¨è¯­ï¼Œä»¥æå‡å„¿ç«¥è¯­éŸ³è¯†åˆ«èƒ½åŠ›ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä»½æ–°æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ç§‘è¨è¯­å„¿ç«¥è¯­éŸ³æ ·æœ¬ã€‚è¯¥æ•°æ®é›†ç»è¯·æ±‚å¯è·å¾—ï¼ŒåŒ…å«æ—©æœŸé˜…è¯»èƒ½åŠ›è¯„ä¼°ç³»ç»Ÿä¸­çš„åä¸ªå•è¯å’Œå­—æ¯ã€‚æ¯ä¸ªè®°å½•éƒ½é‡‡ç”¨äº†åœ¨çº¿ç»æµçš„å¤šé‡æ ‡è®°æ–¹æ³•è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶ç”±ç‹¬ç«‹çš„æ—©æœŸé˜…è¯»èƒ½åŠ›è¯„ä¼°å®¡æŸ¥è€…å¯¹æ ·æœ¬è¿›è¡Œäº†éªŒè¯ã€‚è¯¥æ•°æ®é›†ç»è¿‡wav2vec 2.0ã€HuBERTå’ŒWhisperä¸‰æ¬¾å…ˆè¿›çš„ç«¯åˆ°ç«¯æ¨¡å‹çš„å¾®è°ƒè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œå¯ç”¨è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œå¹³è¡¡æ€§å¯¹è¿™äº›æ¨¡å‹çš„æ€§èƒ½æœ‰é‡å¤§å½±å“ï¼Œè¿™å¯¹äºç»æµçš„å¤§è§„æ¨¡æ•°æ®é›†æ”¶é›†è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å¯ç”¨æ ·æœ¬æ•°é‡æœ‰é™çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡åŒæ—¶è®­ç»ƒå¤šä¸ªç±»åˆ«ï¼Œwav2vec 2.0çš„æ€§èƒ½ä¹Ÿèƒ½å¾—åˆ°æå‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å„¿ç«¥è¯†å­—èƒ½åŠ›å¯¹ä¸ªä½“åç»­ç”Ÿæ´»é˜¶æ®µçš„å½±å“å…·æœ‰é¢„æµ‹æ€§ã€‚</li>
<li>éœ€è¦é’ˆå¯¹è„†å¼±ä½æ”¶å…¥å’Œä¸­æ”¶å…¥ç¾¤ä½“è¿›è¡Œé’ˆå¯¹æ€§çš„é˜…è¯»å¹²é¢„ã€‚</li>
<li>é˜…è¯»è¯„ä¼°æ˜¯è¡¡é‡é˜…è¯»é¡¹ç›®æ•ˆæœçš„é‡è¦å·¥å…·ï¼ŒAIå¯æ”¯æŒæ­¤ä»»åŠ¡ã€‚</li>
<li>å¼€å‘è‡ªåŠ¨é˜…è¯»è¯„ä¼°ç³»ç»Ÿæ—¶é¢ä¸´æ•°æ®é™åˆ¶å’Œå„¿ç«¥å£°éŸ³ç‹¬ç‰¹æ€§æŒ‘æˆ˜ã€‚</li>
<li>ç§‘è¨è¯­å„¿ç«¥è¯­éŸ³æ•°æ®é›†ç”¨äºæå‡è¯­éŸ³è¯†åˆ«èƒ½åŠ›ç ”ç©¶ã€‚</li>
<li>æ•°æ®é›†è´¨é‡åŠå¹³è¡¡æ€§å¯¹æ¨¡å‹æ€§èƒ½å½±å“æ˜¾è‘—ï¼Œå¼ºè°ƒå¤§è§„æ¨¡æ•°æ®é›†æ”¶é›†çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a6b8a63d7791b4cdedbb151d9f9f93ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4e33aa3709d3ae11ca3f52047076849.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-798f6a03b9ca85b10edafe60ab63b3f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1b41946b7c8ba9d8f0e60d82315dd88.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Counterspeech-the-ultimate-shield-Multi-Conditioned-Counterspeech-Generation-through-Attributed-Prefix-Learning"><a href="#Counterspeech-the-ultimate-shield-Multi-Conditioned-Counterspeech-Generation-through-Attributed-Prefix-Learning" class="headerlink" title="Counterspeech the ultimate shield! Multi-Conditioned Counterspeech   Generation through Attributed Prefix Learning"></a>Counterspeech the ultimate shield! Multi-Conditioned Counterspeech   Generation through Attributed Prefix Learning</h2><p><strong>Authors:Aswini Kumar, Anil Bandhakavi, Tanmoy Chakraborty</strong></p>
<p>Counterspeech has proven to be a powerful tool to combat hate speech online. Previous studies have focused on generating counterspeech conditioned only on specific intents (single attributed). However, a holistic approach considering multiple attributes simultaneously can yield more nuanced and effective responses. Here, we introduce HiPPrO, Hierarchical Prefix learning with Preference Optimization, a novel two-stage framework that utilizes the effectiveness of attribute-specific prefix embedding spaces hierarchically optimized during the counterspeech generation process in the first phase. Thereafter, we incorporate both reference and reward-free preference optimization to generate more constructive counterspeech. Furthermore, we extend IntentCONANv2 by annotating all 13,973 counterspeech instances with emotion labels by five annotators. HiPPrO leverages hierarchical prefix optimization to integrate these dual attributes effectively. An extensive evaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent conformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L, respectively, compared to several baseline models. Human evaluations further substantiate the superiority of our approach, highlighting the enhanced relevance and appropriateness of the generated counterspeech. This work underscores the potential of multi-attribute conditioning in advancing the efficacy of counterspeech generation systems. Our code is available on Github and dataset is open-sourced on Hugging-face. </p>
<blockquote>
<p>åè¯å·²è¢«è¯æ˜æ˜¯æ‰“å‡»ç½‘ä¸Šä»‡æ¨è¨€è®ºçš„æœ‰åŠ›å·¥å…·ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä»…é’ˆå¯¹ç‰¹å®šæ„å›¾ï¼ˆå•ä¸€å±æ€§ï¼‰ç”Ÿæˆåè¯ã€‚ç„¶è€Œï¼ŒåŒæ—¶è€ƒè™‘å¤šä¸ªå±æ€§çš„æ•´ä½“æ–¹æ³•å¯èƒ½ä¼šäº§ç”Ÿæ›´å¾®å¦™å’Œæœ‰æ•ˆçš„å›åº”ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†HiPPrOï¼Œå³å¸¦æœ‰åå¥½ä¼˜åŒ–çš„åˆ†å±‚å‰ç¼€å­¦ä¹ ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å±æ€§ç‰¹å®šå‰ç¼€åµŒå…¥ç©ºé—´åœ¨åè¯ç”Ÿæˆè¿‡ç¨‹ä¸­çš„åˆ†å±‚ä¼˜åŒ–æ•ˆæœã€‚æ­¤åï¼Œæˆ‘ä»¬å°†å‚è€ƒå’Œæ— å¥–åŠ±åå¥½ä¼˜åŒ–ç›¸ç»“åˆï¼Œä»¥ç”Ÿæˆæ›´å…·å»ºè®¾æ€§çš„åè¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰13973ä¸ªåè¯å®ä¾‹è¿›è¡Œäº†æƒ…æ„Ÿæ ‡ç­¾æ ‡æ³¨ï¼Œç”±äº”ä½æ ‡æ³¨è€…å®Œæˆã€‚HiPPrOåˆ©ç”¨åˆ†å±‚å‰ç¼€ä¼˜åŒ–æœ‰æ•ˆåœ°æ•´åˆäº†è¿™äº›åŒé‡å±æ€§ã€‚å¤§é‡è¯„ä¼°è¡¨æ˜ï¼Œä¸å‡ ä¸ªåŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼ŒHiPPrOåœ¨æ„å›¾ä¸€è‡´æ€§ä¸Šæé«˜äº†çº¦38%ï¼Œåœ¨Rouge-1ã€Rouge-2å’ŒRouge-Lä¸Šåˆ†åˆ«æé«˜äº†çº¦3%ã€2%ã€3%ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œçªå‡ºäº†ç”Ÿæˆåè¯çš„ç›¸å…³æ€§å’Œé€‚å½“æ€§çš„æé«˜ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†å¤šå±æ€§æ¡ä»¶åœ¨æå‡åè¯ç”Ÿæˆç³»ç»Ÿæ•ˆç‡æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨Githubä¸Šæä¾›ï¼Œæ•°æ®é›†å·²åœ¨Hugging-faceä¸Šå¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11958v3">PDF</a> Accepted in ACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹æŠ—ç½‘ç»œä»‡æ¨è¨€è®ºçš„æœ‰åŠ›å·¥å…·â€”â€”åè¯ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦å…³æ³¨åŸºäºç‰¹å®šæ„å›¾çš„åè¯ç”Ÿæˆï¼Œè€Œæœ¬æ–‡æå‡ºä¸€ç§ç»¼åˆè€ƒè™‘å¤šç§å±æ€§çš„å…¨æ¯æ–¹æ³•ï¼Œèƒ½ç”Ÿæˆæ›´ç»†è…»å’Œæœ‰æ•ˆçš„å›åº”ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥HiPPrOæ¡†æ¶ï¼Œé‡‡ç”¨å±æ€§ç‰¹å®šçš„å‰ç¼€åµŒå…¥ç©ºé—´è¿›è¡Œå±‚æ¬¡ä¼˜åŒ–ï¼Œç”Ÿæˆæ›´å…·å»ºè®¾æ€§çš„åè¯ã€‚å®éªŒè¯æ˜ï¼ŒHiPPrOåœ¨æ„å›¾ç¬¦åˆåº¦ä¸Šæé«˜äº†çº¦38%ï¼Œåœ¨RougeæŒ‡æ ‡ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚åè¯ç”Ÿæˆç³»ç»Ÿçš„å¤šå±æ€§è°ƒèŠ‚å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åè¯æ˜¯æ‰“å‡»ç½‘ç»œä»‡æ¨è¨€è®ºçš„æœ‰æ•ˆå·¥å…·ã€‚</li>
<li>ä»¥å¾€ç ”ç©¶ä¸»è¦å…³æ³¨åŸºäºå•ä¸€æ„å›¾çš„åè¯ç”Ÿæˆï¼Œæœ¬æ–‡æå‡ºç»¼åˆè€ƒè™‘å¤šç§å±æ€§çš„å…¨æ¯æ–¹æ³•ã€‚</li>
<li>HiPPrOæ¡†æ¶åˆ©ç”¨å±æ€§ç‰¹å®šçš„å‰ç¼€åµŒå…¥ç©ºé—´è¿›è¡Œå±‚æ¬¡ä¼˜åŒ–ï¼Œç”Ÿæˆæ›´å…·å»ºè®¾æ€§çš„åè¯ã€‚</li>
<li>HiPPrOåœ¨æ„å›¾ç¬¦åˆåº¦ä¸Šè¾ƒåŸºçº¿æ¨¡å‹æé«˜äº†çº¦38%ã€‚</li>
<li>åœ¨RougeæŒ‡æ ‡ä¸Šï¼ŒHiPPrOè¾ƒåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>äººç±»è¯„ä¼°è¿›ä¸€æ­¥è¯æ˜äº†HiPPrOæ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-115ac15b7f4e5de46f40791fddd9b4be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2114ef1160b95f2589e0b548c8ff433.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-535b6242b793eb6b4c6568ecf9145119.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48a570a92b74fe981ae2f28f0c2f181b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLM-based-Hatred-and-Toxicity-Detection-with-Meta-Toxic-Knowledge-Graph"><a href="#Enhancing-LLM-based-Hatred-and-Toxicity-Detection-with-Meta-Toxic-Knowledge-Graph" class="headerlink" title="Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic   Knowledge Graph"></a>Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic   Knowledge Graph</h2><p><strong>Authors:Yibo Zhao, Jiapeng Zhu, Can Xu, Yao Liu, Xiang Li</strong></p>
<p>The rapid growth of social media platforms has raised significant concerns regarding online content toxicity. When Large Language Models (LLMs) are used for toxicity detection, two key challenges emerge: 1) the absence of domain-specific toxic knowledge leads to false negatives; 2) the excessive sensitivity of LLMs to toxic speech results in false positives, limiting freedom of speech. To address these issues, we propose a novel method called MetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance hatred and toxicity detection. First, we construct a comprehensive meta-toxic knowledge graph by utilizing LLMs to extract toxic information through a three-step pipeline, with toxic benchmark datasets serving as corpora. Second, we query the graph via retrieval and ranking processes to supplement accurate, relevant toxic knowledge. Extensive experiments and in-depth case studies across multiple datasets demonstrate that our MetaTox significantly decreases the false positive rate while boosting overall toxicity detection performance. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/YiboZhao624/MetaTox">https://github.com/YiboZhao624/MetaTox</a>. </p>
<blockquote>
<p>ç¤¾äº¤åª’ä½“å¹³å°çš„å¿«é€Ÿå‘å±•å¼•å‘äº†äººä»¬å¯¹ç½‘ç»œå†…å®¹æ¯’æ€§çš„é‡å¤§å…³æ³¨ã€‚å½“ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¯’æ€§æ£€æµ‹æ—¶ï¼Œä¼šå‡ºç°ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼š1ï¼‰ç¼ºä¹ç‰¹å®šé¢†åŸŸçš„æ¯’æ€§çŸ¥è¯†ä¼šå¯¼è‡´å‡é˜´æ€§ç»“æœï¼›2ï¼‰LLMå¯¹æœ‰æ¯’è¨€è®ºè¿‡äºæ•æ„Ÿï¼Œå¯¼è‡´å‡é˜³æ€§ç»“æœï¼Œé™åˆ¶è¨€è®ºè‡ªç”±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºMetaToxçš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨å…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±ä¸Šçš„å›¾æœç´¢æ¥å¢å¼ºä»‡æ¨å’Œæ¯’æ€§æ£€æµ‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªä¸‰æ­¥éª¤çš„ç®¡é“ï¼Œåˆ©ç”¨LLMæå–æ¯’æ€§ä¿¡æ¯ï¼Œä»¥æ¯’æ€§åŸºå‡†æ•°æ®é›†ä½œä¸ºè¯­æ–™åº“ï¼Œæ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„å…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡æ£€ç´¢å’Œæ’åè¿‡ç¨‹æŸ¥è¯¢å›¾è°±ï¼Œä»¥è¡¥å……å‡†ç¡®ä¸”ç›¸å…³çš„æ¯’æ€§çŸ¥è¯†ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒå’Œæ·±å…¥æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œæˆ‘ä»¬çš„MetaToxæ–¹æ³•æ˜¾è‘—é™ä½äº†è¯¯æŠ¥ç‡ï¼ŒåŒæ—¶æé«˜äº†æ€»ä½“æ¯’æ€§æ£€æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/YiboZhao624/MetaTox%E8%8E%B7%E5%8F%96%E3%80%82]">https://github.com/YiboZhao624/MetaToxè·å–ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15268v4">PDF</a> 8 pages of content</p>
<p><strong>Summary</strong></p>
<p>ç¤¾äº¤åª’ä½“å¹³å°çš„å¿«é€Ÿå‘å±•å¼•å‘äº†äººä»¬å¯¹ç½‘ç»œå†…å®¹æ¯’æ€§çš„å…³æ³¨ã€‚é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¯’æ€§æ£€æµ‹ä¸­çš„ä¸¤å¤§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•MetaToxï¼Œåˆ©ç”¨å…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±çš„å›¾å½¢æœç´¢æ¥å¢å¼ºä»‡æ¨å’Œæ¯’æ€§æ£€æµ‹ã€‚é€šè¿‡æ„å»ºå…¨é¢çš„å…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±ï¼Œä»¥åŠé€šè¿‡æŸ¥è¯¢å›¾è°±ä»¥è¡¥å……å‡†ç¡®ä¸”ç›¸å…³çš„æ¯’æ€§çŸ¥è¯†æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å®éªŒç»“æœè¯æ˜äº†MetaToxåœ¨é™ä½è¯¯æŠ¥ç‡çš„åŒæ—¶ï¼Œæé«˜äº†æ¯’æ€§æ£€æµ‹çš„æ€»ä½“æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾äº¤åª’ä½“å†…å®¹çš„æ¯’æ€§æ£€æµ‹é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šç¼ºä¹é¢†åŸŸç‰¹å®šçš„æ¯’æ€§çŸ¥è¯†å’Œè¿‡åº¦æ•æ„Ÿå¯¼è‡´çš„è¯¯æŠ¥ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•MetaToxï¼Œåˆ©ç”¨å…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±çš„å›¾å½¢æœç´¢å¢å¼ºæ¯’æ€§æ£€æµ‹ã€‚</li>
<li>MetaToxé€šè¿‡æ„å»ºå…¨é¢çš„å…ƒæ¯’æ€§çŸ¥è¯†å›¾è°±æ¥è§£å†³ç¼ºä¹é¢†åŸŸç‰¹å®šçŸ¥è¯†çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡æŸ¥è¯¢çŸ¥è¯†å›¾è°±æ¥è¡¥å……å‡†ç¡®ä¸”ç›¸å…³çš„æ¯’æ€§çŸ¥è¯†ï¼Œä»¥æé«˜æ£€æµ‹å‡†ç¡®æ€§å¹¶é™ä½è¯¯æŠ¥ç‡ã€‚</li>
<li>MetaToxèƒ½å¤Ÿæ˜¾è‘—é™ä½è¯¯æŠ¥ç‡ï¼ŒåŒæ—¶æé«˜æ¯’æ€§æ£€æµ‹çš„æ€»ä½“æ€§èƒ½ã€‚</li>
<li>è¿›è¡Œäº†å¤§é‡çš„å®éªŒå’Œæ·±å…¥çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯æ˜äº†MetaToxçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15268">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c797347e0158f4ceb08fb0154dd2d1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4117e7f489848b54039de48bf5f165d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-505ad0de96990783c6cb239a97f90499.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d30f192e2c185a64a742a9bfec37da5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Efficient-Generative-Modeling-with-Residual-Vector-Quantization-Based-Tokens"><a href="#Efficient-Generative-Modeling-with-Residual-Vector-Quantization-Based-Tokens" class="headerlink" title="Efficient Generative Modeling with Residual Vector Quantization-Based   Tokens"></a>Efficient Generative Modeling with Residual Vector Quantization-Based   Tokens</h2><p><strong>Authors:Jaehyeon Kim, Taehong Moon, Keon Lee, Jaewoong Cho</strong></p>
<p>We introduce ResGen, an efficient Residual Vector Quantization (RVQ)-based generative model for high-fidelity generation with fast sampling. RVQ improves data fidelity by increasing the number of quantization steps, referred to as depth, but deeper quantization typically increases inference steps in generative models. To address this, ResGen directly predicts the vector embedding of collective tokens rather than individual ones, ensuring that inference steps remain independent of RVQ depth. Additionally, we formulate token masking and multi-token prediction within a probabilistic framework using discrete diffusion and variational inference. We validate the efficacy and generalizability of the proposed method on two challenging tasks across different modalities: conditional image generation on ImageNet 256x256 and zero-shot text-to-speech synthesis. Experimental results demonstrate that ResGen outperforms autoregressive counterparts in both tasks, delivering superior performance without compromising sampling speed. Furthermore, as we scale the depth of RVQ, our generative models exhibit enhanced generation fidelity or faster sampling speeds compared to similarly sized baseline models. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ResGenï¼Œè¿™æ˜¯ä¸€ç§åŸºäºé«˜æ•ˆçš„æ®‹å·®çŸ¢é‡é‡åŒ–ï¼ˆRVQï¼‰çš„ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºé«˜ä¿çœŸç”Ÿæˆå’Œå¿«é€Ÿé‡‡æ ·ã€‚RVQé€šè¿‡å¢åŠ é‡åŒ–æ­¥éª¤çš„æ•°é‡ï¼ˆç§°ä¸ºæ·±åº¦ï¼‰æ¥æé«˜æ•°æ®ä¿çœŸåº¦ï¼Œä½†æ›´æ·±çš„é‡åŒ–é€šå¸¸ä¼šå¢åŠ ç”Ÿæˆæ¨¡å‹ä¸­çš„æ¨ç†æ­¥éª¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒResGenç›´æ¥é¢„æµ‹é›†ä½“æ ‡è®°çš„å‘é‡åµŒå…¥ï¼Œè€Œä¸æ˜¯å•ä¸ªæ ‡è®°çš„å‘é‡åµŒå…¥ï¼Œä»è€Œç¡®ä¿æ¨ç†æ­¥éª¤ä¸RVQæ·±åº¦æ— å…³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨ç¦»æ•£æ‰©æ•£å’Œå˜åˆ†æ¨ç†ï¼Œåœ¨æ¦‚ç‡æ¡†æ¶ä¸‹åˆ¶å®šæ ‡è®°æ©ç å’Œå¤šæ ‡è®°é¢„æµ‹ã€‚æˆ‘ä»¬åœ¨ä¸åŒæ¨¡æ€çš„ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸ŠéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼šImageNet 256x256ä¸Šçš„æ¡ä»¶å›¾åƒç”Ÿæˆå’Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³çš„åˆæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒResGenåœ¨è¿™ä¸¤ä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºè‡ªå›å½’æ¨¡å‹ï¼Œåœ¨ä¸å½±å“é‡‡æ ·é€Ÿåº¦çš„æƒ…å†µä¸‹å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œéšç€æˆ‘ä»¬æ‰©å±•RVQçš„æ·±åº¦ï¼Œæˆ‘ä»¬çš„ç”Ÿæˆæ¨¡å‹ä¸ç±»ä¼¼è§„æ¨¡çš„åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„ç”Ÿæˆä¿çœŸåº¦æˆ–æ›´å¿«çš„é‡‡æ ·é€Ÿåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10208v3">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>ResGenæ˜¯ä¸€ç§åŸºäºResidual Vector Quantizationï¼ˆRVQï¼‰çš„é«˜æ•ˆç”Ÿæˆæ¨¡å‹ï¼Œå¯å®ç°é«˜ä¿çœŸç”Ÿæˆå’Œå¿«é€Ÿé‡‡æ ·ã€‚å®ƒé€šè¿‡å¢åŠ é‡åŒ–æ­¥éª¤çš„æ•°é‡ï¼ˆç§°ä¸ºæ·±åº¦ï¼‰æ¥æé«˜æ•°æ®ä¿çœŸåº¦ï¼Œè€Œæ— éœ€å¢åŠ æ¨ç†æ­¥éª¤ã€‚ResGenç›´æ¥é¢„æµ‹é›†ä½“æ ‡è®°çš„å‘é‡åµŒå…¥ï¼Œè€Œä¸æ˜¯å•ç‹¬çš„æ ‡è®°ï¼Œç¡®ä¿äº†æ¨ç†æ­¥éª¤ä¸RVQæ·±åº¦æ— å…³ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æ¦‚ç‡æ¡†æ¶ä¸‹åˆ¶å®šäº†æ ‡è®°æ©ç å’Œå¤šæ ‡è®°é¢„æµ‹ï¼Œé‡‡ç”¨ç¦»æ•£æ‰©æ•£å’Œå˜åˆ†æ¨æ–­æ–¹æ³•ã€‚åœ¨ImageNet 256x256ä¸Šçš„æ¡ä»¶å›¾åƒç”Ÿæˆå’Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆç­‰ä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒResGenåœ¨æ€§èƒ½ä¸Šä¼˜äºè‡ªå›å½’æ¨¡å‹ï¼Œæé«˜äº†ç”Ÿæˆä¿çœŸåº¦æˆ–æ›´å¿«çš„é‡‡æ ·é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ResGenæ˜¯ä¸€ä¸ªåŸºäºResidual Vector Quantizationï¼ˆRVQï¼‰çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¯å®ç°é«˜ä¿çœŸç”Ÿæˆå’Œå¿«é€Ÿé‡‡æ ·ã€‚</li>
<li>RVQé€šè¿‡å¢åŠ é‡åŒ–æ­¥éª¤çš„æ•°é‡æé«˜äº†æ•°æ®ä¿çœŸåº¦ï¼Œè€ŒResGençš„è®¾è®¡ä¿è¯äº†æ¨ç†æ­¥éª¤ä¸é‡åŒ–æ·±åº¦æ— å…³ã€‚</li>
<li>ResGenç›´æ¥é¢„æµ‹é›†ä½“æ ‡è®°çš„å‘é‡åµŒå…¥ï¼Œæé«˜äº†æ¨¡å‹æ•ˆç‡ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨æ¦‚ç‡æ¡†æ¶è¿›è¡Œæ ‡è®°æ©ç å’Œå¤šæ ‡è®°é¢„æµ‹ï¼Œç»“åˆç¦»æ•£æ‰©æ•£å’Œå˜åˆ†æ¨æ–­æ–¹æ³•ã€‚</li>
<li>åœ¨æ¡ä»¶å›¾åƒç”Ÿæˆå’Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä»»åŠ¡ä¸Šï¼ŒResGenè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
<li>ResGenç›¸è¾ƒäºè‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿæé«˜ç”Ÿæˆä¿çœŸåº¦æˆ–åŠ å¿«é‡‡æ ·é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10208">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d011611d18708640fd1edf813a638d91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b60915cd1b56567d2223dace85d26489.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="BanTH-A-Multi-label-Hate-Speech-Detection-Dataset-for-Transliterated-Bangla"><a href="#BanTH-A-Multi-label-Hate-Speech-Detection-Dataset-for-Transliterated-Bangla" class="headerlink" title="BanTH: A Multi-label Hate Speech Detection Dataset for Transliterated   Bangla"></a>BanTH: A Multi-label Hate Speech Detection Dataset for Transliterated   Bangla</h2><p><strong>Authors:Fabiha Haider, Fariha Tanjim Shifat, Md Farhan Ishmam, Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, Md Fahim, Md Farhad Alam</strong></p>
<p>The proliferation of transliterated texts in digital spaces has emphasized the need for detecting and classifying hate speech in languages beyond English, particularly in low-resource languages. As online discourse can perpetuate discrimination based on target groups, e.g. gender, religion, and origin, multi-label classification of hateful content can help in comprehending hate motivation and enhance content moderation. While previous efforts have focused on monolingual or binary hate classification tasks, no work has yet addressed the challenge of multi-label hate speech classification in transliterated Bangla. We introduce BanTH, the first multi-label transliterated Bangla hate speech dataset comprising 37.3k samples. The samples are sourced from YouTube comments, where each instance is labeled with one or more target groups, reflecting the regional demographic. We establish novel transformer encoder-based baselines by further pre-training on transliterated Bangla corpus. We also propose a novel translation-based LLM prompting strategy for transliterated text. Experiments reveal that our further pre-trained encoders are achieving state-of-the-art performance on the BanTH dataset, while our translation-based prompting outperforms other strategies in the zero-shot setting. The introduction of BanTH not only fills a critical gap in hate speech research for Bangla but also sets the stage for future exploration into code-mixed and multi-label classification challenges in underrepresented languages. </p>
<blockquote>
<p>æ•°å­—ç©ºé—´ä¸­éŸ³è¯‘æ–‡æœ¬çš„æ¿€å¢å¼ºè°ƒäº†æ£€æµ‹å’Œéè‹±è¯­è¯­è¨€çš„ä»‡æ¨è¨€è®ºåˆ†ç±»çš„éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºè¾ƒå°‘çš„è¯­è¨€ä¸­ã€‚ç”±äºåœ¨çº¿è¨€è®ºå¯èƒ½ä¼šä½¿åŸºäºç›®æ ‡ç¾¤ä½“çš„æ­§è§†æ°¸ä¹…åŒ–ï¼Œä¾‹å¦‚æ€§åˆ«ã€å®—æ•™å’Œå‡ºèº«ç­‰ï¼Œä»‡æ¨å†…å®¹çš„å¤šæ ‡ç­¾åˆ†ç±»æœ‰åŠ©äºç†è§£ä»‡æ¨åŠ¨æœºå¹¶å¢å¼ºå†…å®¹ç®¡ç†ã€‚è™½ç„¶ä»¥å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºå•è¯­æˆ–äºŒå…ƒä»‡æ¨åˆ†ç±»ä»»åŠ¡ï¼Œä½†å°šæœªæœ‰å·¥ä½œåœ¨éŸ³è¯‘å­ŸåŠ æ‹‰è¯­çš„å¤šæ ‡ç­¾ä»‡æ¨è¨€è®ºåˆ†ç±»æ–¹é¢æå‡ºæŒ‘æˆ˜ã€‚æˆ‘ä»¬ä»‹ç»äº†BanTHï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤šæ ‡ç­¾éŸ³è¯‘å­ŸåŠ æ‹‰ä»‡æ¨è¨€è®ºæ•°æ®é›†ï¼ŒåŒ…å«37.3kæ ·æœ¬ã€‚è¿™äº›æ ·æœ¬æ¥æºäºYouTubeè¯„è®ºï¼Œæ¯ä¸ªå®ä¾‹éƒ½è¢«æ ‡è®°ä¸ºä¸€ä¸ªæˆ–å¤šä¸ªç›®æ ‡ç¾¤ä½“ï¼Œåæ˜ äº†åœ°åŒºäººå£çŠ¶å†µã€‚æˆ‘ä»¬é€šè¿‡è¿›ä¸€æ­¥åœ¨éŸ³è¯‘å­ŸåŠ æ‹‰è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå»ºç«‹äº†åŸºäºæ–°å‹è½¬æ¢å™¨ç¼–ç å™¨çš„åŸºçº¿ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºç¿»è¯‘çš„å¤§å‹è¯­è¨€æ¨¡å‹æç¤ºç­–ç•¥ï¼Œç”¨äºå¤„ç†éŸ³è¯‘æ–‡æœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è¿›ä¸€æ­¥é¢„è®­ç»ƒç¼–ç å™¨åœ¨BanTHæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬çš„åŸºäºç¿»è¯‘çš„æ–¹æ³•çš„æç¤ºæ•ˆæœä¼˜äºå…¶ä»–ç­–ç•¥ã€‚BanTHçš„å¼•å…¥ä¸ä»…å¡«è¡¥äº†å­ŸåŠ æ‹‰ä»‡æ¨è¨€è®ºç ”ç©¶ä¸­çš„å…³é”®ç©ºç™½ï¼Œè€Œä¸”ä¸ºæ¢ç´¢æœªæ¥ä½ä»£è¡¨æ€§è¯­è¨€çš„æ··åˆä»£ç å’Œå¤šæ ‡ç­¾åˆ†ç±»æŒ‘æˆ˜å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13281v4">PDF</a> Published in NAACL Findings 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€æ•°å­—ç©ºé—´ä¸­è½¬è¯‘æ–‡æœ¬çš„æ™®åŠï¼Œå¯¹éè‹±è¯­è¯­è¨€çš„ä»‡æ¨è¨€è®ºæ£€æµ‹å’Œåˆ†ç±»å˜å¾—å°¤ä¸ºé‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºè¾ƒå°‘çš„è¯­è¨€ä¸­ã€‚å¤šæ ‡ç­¾ä»‡æ¨å†…å®¹åˆ†ç±»æœ‰åŠ©äºç†è§£ä»‡æ¨åŠ¨æœºå¹¶å¢å¼ºå†…å®¹ç®¡ç†ã€‚ç›®å‰å°šæœªæœ‰é’ˆå¯¹è½¬è¯‘å­ŸåŠ æ‹‰è¯­çš„å¤šæ ‡ç­¾ä»‡æ¨è¨€è®ºåˆ†ç±»æŒ‘æˆ˜çš„å·¥ä½œã€‚æˆ‘ä»¬å¼•å…¥äº†BanTHï¼Œé¦–ä¸ªå¤šæ ‡ç­¾è½¬è¯‘å­ŸåŠ æ‹‰è¯­ä»‡æ¨è¨€è®ºæ•°æ®é›†ï¼ŒåŒ…å«37.3kæ ·æœ¬ã€‚æ ·æœ¬æ¥æºäºYouTubeè¯„è®ºï¼Œæ¯ä¸ªå®ä¾‹éƒ½è¢«æ ‡è®°ä¸ºä¸€ä¸ªæˆ–å¤šä¸ªç›®æ ‡ç¾¤ä½“ï¼Œåæ˜ äº†åŒºåŸŸäººå£ç»Ÿè®¡ç‰¹å¾ã€‚æˆ‘ä»¬å»ºç«‹äº†åŸºäºè½¬æ¢å™¨ç¼–ç å™¨çš„æ–°åŸºçº¿ï¼Œé€šè¿‡è½¬è¯‘å­ŸåŠ æ‹‰è¯­è¯­æ–™åº“è¿›è¡Œè¿›ä¸€æ­¥é¢„è®­ç»ƒã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºç¿»è¯‘çš„å¤§å‹è¯­è¨€æ¨¡å‹æç¤ºç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è¿›ä¸€æ­¥é¢„è®­ç»ƒç¼–ç å™¨åœ¨BanTHæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼Œè€Œæˆ‘ä»¬çš„åŸºäºç¿»è¯‘æç¤ºçš„ç­–ç•¥åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­è¡¨ç°ä¼˜äºå…¶ä»–ç­–ç•¥ã€‚BanTHçš„å¼•å…¥ä¸ä»…å¡«è¡¥äº†å­ŸåŠ æ‹‰è¯­ä»‡æ¨è¨€è®ºç ”ç©¶ä¸­çš„å…³é”®ç©ºç™½ï¼Œè€Œä¸”ä¸ºæ¢ç´¢ä»£ç æ··åˆå’Œå¤šæ ‡ç­¾åˆ†ç±»æŒ‘æˆ˜åœ¨ä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€ä¸­å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½¬è¯‘æ–‡æœ¬çš„æ™®åŠå¼ºè°ƒäº†å¯¹éè‹±è¯­è¯­è¨€çš„ä»‡æ¨è¨€è®ºæ£€æµ‹ä¸åˆ†ç±»çš„å¿…è¦æ€§ã€‚</li>
<li>å¤šæ ‡ç­¾ä»‡æ¨å†…å®¹åˆ†ç±»æœ‰åŠ©äºç†è§£ä»‡æ¨åŠ¨æœºå¹¶å¢å¼ºå†…å®¹ç®¡ç†ã€‚</li>
<li>ç›®å‰å°šæœªæœ‰é’ˆå¯¹è½¬è¯‘å­ŸåŠ æ‹‰è¯­çš„å¤šæ ‡ç­¾ä»‡æ¨è¨€è®ºåˆ†ç±»ç ”ç©¶ã€‚</li>
<li>å¼•å…¥äº†é¦–ä¸ªå¤šæ ‡ç­¾è½¬è¯‘å­ŸåŠ æ‹‰è¯­ä»‡æ¨è¨€è®ºæ•°æ®é›†BanTHï¼ŒåŒ…å«37.3kæ ·æœ¬ã€‚</li>
<li>æ ·æœ¬æ¥æºäºYouTubeè¯„è®ºï¼Œåæ˜ åŒºåŸŸäººå£ç‰¹å¾ã€‚</li>
<li>å»ºç«‹äº†åŸºäºè½¬æ¢å™¨ç¼–ç å™¨çš„åŸºçº¿æ¨¡å‹ï¼Œé€šè¿‡è¿›ä¸€æ­¥é¢„è®­ç»ƒæå‡æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†åŸºäºç¿»è¯‘çš„å¤§å‹è¯­è¨€æ¨¡å‹æç¤ºç­–ç•¥ï¼Œå¹¶åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13281">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40837887e729a3ea05e1d70312d6d21f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-187b74e9cfdf07f86cd75d54ce5da9b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc1efb82c38adc5abcd9ebfb1d73034b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-845ba45fbcfef19034dccbbbac3e98fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0848ba1a83c31a150ba938c91a5b61f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c797a70db9ce510a75839f27b122a82b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-704373d8c00b5a1921df79d2c11490b2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1e455a764a195af5b137de1400ae49a4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Text-To-Speech-Synthesis-In-The-Wild"><a href="#Text-To-Speech-Synthesis-In-The-Wild" class="headerlink" title="Text-To-Speech Synthesis In The Wild"></a>Text-To-Speech Synthesis In The Wild</h2><p><strong>Authors:Jee-weon Jung, Wangyou Zhang, Soumi Maiti, Yihan Wu, Xin Wang, Ji-Hoon Kim, Yuta Matsunaga, Seyun Um, Jinchuan Tian, Hye-jin Shim, Nicholas Evans, Joon Son Chung, Shinnosuke Takamichi, Shinji Watanabe</strong></p>
<p>Traditional Text-to-Speech (TTS) systems rely on studio-quality speech recorded in controlled settings.a Recently, an effort known as noisy-TTS training has emerged, aiming to utilize in-the-wild data. However, the lack of dedicated datasets has been a significant limitation. We introduce the TTS In the Wild (TITW) dataset, which is publicly available, created through a fully automated pipeline applied to the VoxCeleb1 dataset. It comprises two training sets: TITW-Hard, derived from the transcription, segmentation, and selection of raw VoxCeleb1 data, and TITW-Easy, which incorporates additional enhancement and data selection based on DNSMOS. State-of-the-art TTS models achieve over 3.0 UTMOS score with TITW-Easy, while TITW-Hard remains difficult showing UTMOS below 2.8. </p>
<blockquote>
<p>ä¼ ç»Ÿæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¾èµ–äºå—æ§ç¯å¢ƒä¸‹çš„é«˜è´¨é‡è¯­éŸ³å½•åˆ¶ã€‚è¿‘æœŸï¼Œä¸€ç§è¢«ç§°ä¸ºå™ªå£°TTSè®­ç»ƒçš„ç ”ç©¶æ–¹æ³•åº”è¿è€Œç”Ÿï¼Œæ—¨åœ¨åˆ©ç”¨é‡å¤–æ•°æ®ã€‚ç„¶è€Œï¼Œç¼ºä¹ä¸“ç”¨æ•°æ®é›†æ˜¯ä¸€ä¸ªé‡å¤§é™åˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†é‡å¤–æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTITWï¼‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ˜¯å…¬å¼€çš„ï¼Œé€šè¿‡åº”ç”¨äºVoxCeleb1æ•°æ®é›†çš„å®Œå…¨è‡ªåŠ¨åŒ–ç®¡é“åˆ›å»ºã€‚å®ƒåŒ…å«ä¸¤ä¸ªè®­ç»ƒé›†ï¼šTITW-Hardï¼Œé€šè¿‡å¯¹åŸå§‹VoxCeleb1æ•°æ®è¿›è¡Œè½¬å½•ã€åˆ†æ®µå’Œé€‰æ‹©è€Œç”Ÿæˆï¼›TITW-Easyåˆ™åŸºäºDNSMOSè¿›è¡Œé¢å¤–çš„å¢å¼ºå’Œæ•°æ®é€‰æ‹©ã€‚æœ€æ–°çš„TTSæ¨¡å‹åœ¨TITW-Easyä¸Šå–å¾—äº†è¶…è¿‡3.0çš„UTMOSåˆ†æ•°ï¼Œè€ŒTITW-Hardä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼ŒUTMOSä½äº2.8ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08711v2">PDF</a> 5 pages, Interspeech 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä¼ ç»Ÿæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¾èµ–äºåœ¨å—æ§ç¯å¢ƒä¸­å½•åˆ¶çš„è¯­éŸ³æ•°æ®ã€‚è¿‘æœŸï¼Œä¸€ç§åä¸ºå™ªå£°TTSè®­ç»ƒçš„ç ”ç©¶åŠªåŠ›å·²ç»å‡ºç°ï¼Œæ—¨åœ¨åˆ©ç”¨é‡å¤–æ•°æ®ã€‚ç„¶è€Œï¼Œç¼ºä¹ä¸“ç”¨æ•°æ®é›†ä¸€ç›´æ˜¯å…¶é‡è¦é™åˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†é‡å¤–æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTITWï¼‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å·²å…¬å¼€å¯ç”¨ï¼Œé€šè¿‡å¯¹VoxCeleb1æ•°æ®é›†åº”ç”¨å…¨è‡ªåŠ¨ç®¡é“åˆ›å»ºè€Œæˆã€‚å®ƒåŒ…å«ä¸¤ä¸ªè®­ç»ƒé›†ï¼šTITW-Hardï¼Œç”±VoxCeleb1åŸå§‹æ•°æ®çš„è½¬å½•ã€åˆ†æ®µå’Œé€‰æ‹©å¾—å‡ºï¼›TITW-Easyåˆ™åŸºäºDNSMOSè¿›è¡Œäº†é¢å¤–çš„å¢å¼ºå’Œæ•°æ®é€‰æ‹©ã€‚ä½¿ç”¨TITW-Easyçš„æœ€æ–°TTSæ¨¡å‹UTMOSå¾—åˆ†è¶…è¿‡3.0ï¼Œè€ŒTITW-Hardä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼ŒUTMOSä½äº2.8ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ul>
<li>ä¼ ç»ŸTTSç³»ç»Ÿä¾èµ–å—æ§ç¯å¢ƒä¸­çš„é«˜è´¨é‡è¯­éŸ³æ•°æ®ã€‚</li>
<li>å™ªå£°TTSè®­ç»ƒæ—¨åœ¨åˆ©ç”¨é‡å¤–æ•°æ®ã€‚</li>
<li>ç¼ºä¹ä¸“ç”¨æ•°æ®é›†æ˜¯å™ªå£°TTSè®­ç»ƒçš„ä¸€ä¸ªé‡è¦é™åˆ¶ã€‚</li>
<li>å¼•å…¥äº†TITWæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é€šè¿‡å…¨è‡ªåŠ¨ç®¡é“ä»VoxCeleb1åˆ›å»ºè€Œæˆã€‚</li>
<li>TITWæ•°æ®é›†åŒ…å«ä¸¤ä¸ªè®­ç»ƒé›†ï¼šTITW-Hardå’ŒTITW-Easyã€‚</li>
<li>TITW-EasyåŸºäºDNSMOSè¿›è¡Œäº†é¢å¤–çš„å¢å¼ºå’Œæ•°æ®é€‰æ‹©ã€‚</li>
<li>æœ€æ–°TTSæ¨¡å‹åœ¨TITW-Easyä¸Šçš„UTMOSå¾—åˆ†è¶…è¿‡3.0ï¼Œè€ŒTITW-Hardå…·æœ‰æŒ‘æˆ˜æ€§ï¼ŒUTMOSä½äº2.8ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08711">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2741ff5bd278376c19eae77be11eae51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90fc6648529f1291b5b5aad6e044da16.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3663f140ca164000b802e7fe6f9a230e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc61520be108ad9dce89cc2aeccc7209.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="BERP-A-Blind-Estimator-of-Room-Parameters-for-Single-Channel-Noisy-Speech-Signals"><a href="#BERP-A-Blind-Estimator-of-Room-Parameters-for-Single-Channel-Noisy-Speech-Signals" class="headerlink" title="BERP: A Blind Estimator of Room Parameters for Single-Channel Noisy   Speech Signals"></a>BERP: A Blind Estimator of Room Parameters for Single-Channel Noisy   Speech Signals</h2><p><strong>Authors:Lijun Wang, Yixian Lu, Ziyan Gao, Kai Li, Jianqiang Huang, Yuntao Kong, Shogo Okada</strong></p>
<p>Room acoustical parameters (RAPs), room geometrical parameters (RGPs) and instantaneous occupancy level are essential metrics for parameterizing the room acoustical characteristics (RACs) of a sound field around a listenerâ€™s local environment, offering comprehensive indications for various applications. Current blind estimation methods either fail to cover a broad range of real-world acoustic environments in the context of real background noise or estimate only a few RAPs and RGPs from noisy single-channel speech signals. In addition, they are limited in their ability to estimate the instantaneous occupancy level. In this paper, we propose a new universal blind estimation framework called the blind estimator of room parameters (BERP) to estimate RAPs, RGPs and occupancy level via a unified methodology. It consists of two modules: a unified room feature encoder that combines attention mechanisms with convolutional layers to learn common features across room parameters, and multiple separate parametric predictors for continuous estimation of each parameter in parallel. The combination of attention and convolutions enables the model to capture acoustic features locally and globally from speech, yielding more robust and multitask generalizable common features. Separate predictors allow the model to independently optimize for each room parameter to reduce task learning conflict and improve per-task performance. This estimation framework enables universal and efficient estimation of room parameters while maintaining satisfactory performance. To evaluate the effectiveness of the proposed framework, we compile a task-specific dataset from several publicly available datasets, including synthetic and real reverberant recordings. The results reveal that BERP achieves state-of-the-art (SOTA) performance and excellent adaptability to real-world scenarios. The code and weights are available on GitHub. </p>
<blockquote>
<p>æˆ¿é—´å£°å­¦å‚æ•°ï¼ˆRAPsï¼‰ã€æˆ¿é—´å‡ ä½•å‚æ•°ï¼ˆRGPsï¼‰å’Œç¬æ—¶å ç”¨æ°´å¹³æ˜¯è¡¨å¾å¬ä¼—å‘¨å›´å£°éŸ³åœºçš„ç¯å¢ƒå£°å­¦ç‰¹æ€§ï¼ˆRACsï¼‰çš„é‡è¦æŒ‡æ ‡ï¼Œä¸ºå„ç§åº”ç”¨æä¾›äº†å…¨é¢çš„æŒ‡ç¤ºã€‚å½“å‰çš„ç›²ä¼°è®¡æ–¹æ³•è¦ä¹ˆæ— æ³•è¦†ç›–çœŸå®èƒŒæ™¯å™ªå£°ç¯å¢ƒä¸­çš„å¹¿æ³›ç°å®ä¸–ç•Œå£°å­¦ç¯å¢ƒï¼Œè¦ä¹ˆåªèƒ½ä»å˜ˆæ‚çš„å•é€šé“è¯­éŸ³ä¿¡å·ä¸­ä¼°è®¡å‡ºä¸€äº›RAPså’ŒRGPsã€‚æ­¤å¤–ï¼Œå®ƒä»¬åœ¨ä¼°è®¡ç¬æ—¶å ç”¨æ°´å¹³æ–¹é¢çš„èƒ½åŠ›ä¹Ÿæœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é€šç”¨ç›²ä¼°è®¡æ¡†æ¶ï¼Œç§°ä¸ºæˆ¿é—´å‚æ•°ç›²ä¼°è®¡å™¨ï¼ˆBERPï¼‰ï¼Œé€šè¿‡ç»Ÿä¸€çš„æ–¹æ³•ä¼°è®¡RAPsã€RGPså’Œå ç”¨æ°´å¹³ã€‚å®ƒç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼šä¸€ä¸ªç»Ÿä¸€æˆ¿é—´ç‰¹å¾ç¼–ç å™¨ï¼Œå®ƒå°†æ³¨æ„åŠ›æœºåˆ¶ä¸å·ç§¯å±‚ç›¸ç»“åˆï¼Œä»¥å­¦ä¹ æˆ¿é—´å‚æ•°ä¹‹é—´çš„å…±åŒç‰¹å¾ï¼›ä»¥åŠå¤šä¸ªå•ç‹¬çš„å‚æ•°é¢„æµ‹å™¨ï¼Œç”¨äºå¹¶è¡Œè¿ç»­ä¼°è®¡æ¯ä¸ªå‚æ•°ã€‚æ³¨æ„åŠ›å’Œå·ç§¯çš„ç»“åˆä½¿æ¨¡å‹èƒ½å¤Ÿä»è¯­éŸ³ä¸­æ•è·å±€éƒ¨å’Œå…¨å±€çš„å£°å­¦ç‰¹å¾ï¼Œä»è€Œäº§ç”Ÿæ›´ç¨³å¥ã€å¤šä»»åŠ¡é€šç”¨çš„å…±åŒç‰¹å¾ã€‚å•ç‹¬çš„é¢„æµ‹å™¨å…è®¸æ¨¡å‹é’ˆå¯¹æ¯ä¸ªæˆ¿é—´å‚æ•°è¿›è¡Œç‹¬ç«‹ä¼˜åŒ–ï¼Œä»¥å‡å°‘ä»»åŠ¡å­¦ä¹ å†²çªå¹¶æé«˜æ¯é¡¹ä»»åŠ¡çš„æ€§èƒ½ã€‚è¯¥ä¼°è®¡æ¡†æ¶èƒ½å¤Ÿåœ¨ä¿æŒä»¤äººæ»¡æ„æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°æˆ¿é—´å‚æ•°çš„é€šç”¨å’Œæœ‰æ•ˆä¼°è®¡ã€‚ä¸ºäº†è¯„ä¼°æ‰€æå‡ºæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä»å‡ ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®é›†ï¼ˆåŒ…æ‹¬åˆæˆå’ŒçœŸå®çš„æ··å“å½•éŸ³ï¼‰ä¸­ç¼–åˆ¶äº†ä¸€ä¸ªç‰¹å®šä»»åŠ¡çš„æ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼ŒBERPè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”å¯¹çœŸå®ä¸–ç•Œåœºæ™¯å…·æœ‰å‡ºè‰²çš„é€‚åº”æ€§ã€‚ä»£ç å’Œæƒé‡å¯åœ¨GitHubä¸Šè·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.04476v7">PDF</a> 16-page with supplementary materials, Accepted to IEEE Transaction on   Audio Speech and Language Processing (TASLP 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºBERPï¼ˆæˆ¿é—´å‚æ•°ç›²ä¼°è®¡å™¨ï¼‰çš„æ–°é€šç”¨ç›²ä¼°è®¡æ¡†æ¶ï¼Œç”¨äºä¼°è®¡æˆ¿é—´çš„å£°å­¦å‚æ•°ï¼ˆRAPsï¼‰ã€å‡ ä½•å‚æ•°ï¼ˆRGPsï¼‰å’Œç¬æ—¶å ç”¨çº§åˆ«ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆæ³¨æ„åŠ›æœºåˆ¶å’Œå·ç§¯å±‚ï¼Œå­¦ä¹ æˆ¿é—´å‚æ•°çš„é€šç”¨ç‰¹å¾ï¼Œå¹¶åˆ†åˆ«å¯¹æ¯ä¸ªå‚æ•°è¿›è¡Œå¹¶è¡Œè¿ç»­ä¼°è®¡ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°æˆ¿é—´å‚æ•°çš„é€šç”¨å’Œé«˜æ•ˆä¼°è®¡ï¼ŒåŒæ—¶ä¿æŒä»¤äººæ»¡æ„çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æˆ¿é—´å£°å­¦å‚æ•°ï¼ˆRAPsï¼‰å’Œæˆ¿é—´å‡ ä½•å‚æ•°ï¼ˆRGPsï¼‰æ˜¯æè¿°å£°éŸ³åœºç‰¹æ€§çš„å…³é”®æŒ‡æ ‡ã€‚</li>
<li>å½“å‰ç›²ä¼°è®¡æ–¹æ³•æ— æ³•è¦†ç›–å¹¿æ³›çš„å®é™…å£°å­¦ç¯å¢ƒæˆ–ä»…èƒ½ä»å¸¦å™ªå£°çš„å•é€šé“è¯­éŸ³ä¿¡å·ä¸­ä¼°è®¡æœ‰é™çš„RAPså’ŒRGPsã€‚</li>
<li>BERPæ¡†æ¶ç»“åˆäº†æ³¨æ„åŠ›æœºåˆ¶å’Œå·ç§¯å±‚ï¼Œå¯ä»¥æœ¬åœ°å’Œå…¨å±€æ•æ‰è¯­éŸ³çš„å£°å­¦ç‰¹å¾ã€‚</li>
<li>é€šè¿‡ç‹¬ç«‹ä¼˜åŒ–æ¯ä¸ªæˆ¿é—´å‚æ•°ï¼ŒBERPå‡å°‘äº†ä»»åŠ¡å­¦ä¹ å†²çªå¹¶æé«˜äº†æ¯ä¸ªä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>BERPæ¡†æ¶åœ¨ä»»åŠ¡ä¸“ç”¨æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„é€‚åº”çœŸå®ä¸–ç•Œåœºæ™¯çš„èƒ½åŠ›ã€‚</li>
<li>BERPä»£ç å’Œæƒé‡å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.04476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-938435dcfc492e0d00abd44a315bf530.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a798b74c8c6d84785eb51a785db92bb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be76a48d6ee8171c61b6b2a4adedb9aa.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Enabling-Differentially-Private-Federated-Learning-for-Speech-Recognition-Benchmarks-Adaptive-Optimizers-and-Gradient-Clipping"><a href="#Enabling-Differentially-Private-Federated-Learning-for-Speech-Recognition-Benchmarks-Adaptive-Optimizers-and-Gradient-Clipping" class="headerlink" title="Enabling Differentially Private Federated Learning for Speech   Recognition: Benchmarks, Adaptive Optimizers and Gradient Clipping"></a>Enabling Differentially Private Federated Learning for Speech   Recognition: Benchmarks, Adaptive Optimizers and Gradient Clipping</h2><p><strong>Authors:Martin Pelikan, Sheikh Shams Azam, Vitaly Feldman, Jan â€œHonzaâ€ Silovsky, Kunal Talwar, Christopher G. Brinton, Tatiana Likhomanenko</strong></p>
<p>While federated learning (FL) and differential privacy (DP) have been extensively studied, their application to automatic speech recognition (ASR) remains largely unexplored due to the challenges in training large transformer models. Specifically, large models further exacerbate issues in FL as they are particularly susceptible to gradient heterogeneity across layers, unlike the relatively uniform gradient behavior observed in shallow models. As a result, prior works struggle to converge with standard optimization techniques, even in the absence of DP mechanisms. To the best of our knowledge, no existing work establishes a competitive, practical recipe for FL with DP in the context of ASR. To address this gap, we establish \textbf{the first benchmark for FL with DP in end-to-end ASR}. Our approach centers on per-layer clipping and layer-wise gradient normalization: theoretical analysis reveals that these techniques together mitigate clipping bias and gradient heterogeneity across layers in deeper models. Consistent with these theoretical insights, our empirical results show that FL with DP is viable under strong privacy guarantees, provided a population of at least several million users. Specifically, we achieve user-level (7.2, $10^{-9}$)-DP (resp. (4.5, $10^{-9}$)-DP) with only a 1.3% (resp. 4.6%) absolute drop in word error rate when extrapolating to high (resp. low) population scales for FL with DP in ASR. Although our experiments focus on ASR, the underlying principles we uncover - particularly those concerning gradient heterogeneity and layer-wise gradient normalization - offer broader guidance for designing scalable, privacy-preserving FL algorithms for large models across domains. </p>
<blockquote>
<p>å°½ç®¡è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰å’Œå·®åˆ†éšç§ï¼ˆDPï¼‰å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†å®ƒä»¬åœ¨å®é™…è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„åº”ç”¨ä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ï¼Œè¿™æ˜¯ç”±äºè®­ç»ƒå¤§å‹å˜å‹å™¨æ¨¡å‹å¸¦æ¥çš„æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œå¤§å‹æ¨¡å‹ä¼šè¿›ä¸€æ­¥åŠ å‰§è”é‚¦å­¦ä¹ çš„é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬ç‰¹åˆ«å®¹æ˜“å—åˆ°å„å±‚æ¢¯åº¦å¼‚è´¨æ€§çš„å½±å“ï¼Œè¿™ä¸æµ…å±‚æ¨¡å‹ä¸­è§‚å¯Ÿåˆ°çš„ç›¸å¯¹ç»Ÿä¸€çš„æ¢¯åº¦è¡Œä¸ºå½¢æˆå¯¹æ¯”ã€‚å› æ­¤ï¼Œå…ˆå‰çš„å·¥ä½œå³ä½¿åœ¨æ²¡æœ‰ä»»ä½•DPæœºåˆ¶çš„æƒ…å†µä¸‹ï¼Œä¹Ÿå¾ˆéš¾ç”¨æ ‡å‡†ä¼˜åŒ–æŠ€æœ¯å®ç°æ”¶æ•›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œç›®å‰æ²¡æœ‰ä»»ä½•å·¥ä½œå»ºç«‹äº†åœ¨ASRèƒŒæ™¯ä¸‹å…·æœ‰DPçš„FLçš„ç«äº‰æ€§å®ç”¨æ–¹æ¡ˆã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬ä¸ºå…·æœ‰DPçš„FLåœ¨ç«¯åˆ°ç«¯çš„ASRä¸­å»ºç«‹äº†<strong>ç¬¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•</strong>ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥æ¯å±‚å‰ªè£å’Œé€å±‚æ¢¯åº¦å½’ä¸€åŒ–ä¸ºä¸­å¿ƒï¼šç†è®ºåˆ†æè¡¨æ˜ï¼Œè¿™äº›æŠ€æœ¯ç›¸ç»“åˆå‡è½»äº†æ·±å±‚æ¨¡å‹ä¸­å‰ªè£åè§å’Œå„å±‚æ¢¯åº¦çš„å¼‚è´¨æ€§ã€‚ä¸è¿™äº›ç†è®ºè§è§£ä¸€è‡´ï¼Œæˆ‘ä»¬çš„å®è¯ç»“æœè¡¨æ˜ï¼Œåœ¨å¼ºå¤§çš„éšç§ä¿è¯ä¸‹ï¼Œå…·æœ‰DPçš„FLæ˜¯å¯è¡Œçš„ï¼Œå‰ææ˜¯æœ‰è‡³å°‘æ•°ç™¾ä¸‡ç”¨æˆ·ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨ç”¨æˆ·å±‚é¢å®ç°äº†ï¼ˆ7.2ï¼Œ10^-9ï¼‰-DPï¼ˆåˆ†åˆ«åœ¨é«˜ä½ç”¨æˆ·ç¾¤ä½“è§„æ¨¡å¤–æ¨æ—¶å®ç°ï¼ˆ4.5ï¼Œ10^-9ï¼‰-DPï¼‰ï¼Œè¯æ±‡é”™è¯¯ç‡ä»…ç»å¯¹ä¸‹é™1.3%ï¼ˆæˆ–4.6%ï¼‰ã€‚å°½ç®¡æˆ‘ä»¬çš„å®éªŒä¸“æ³¨äºASRï¼Œä½†æˆ‘ä»¬æ‰€æ­ç¤ºçš„åŸºæœ¬åŸç†ï¼Œç‰¹åˆ«æ˜¯å…³äºæ¢¯åº¦å¼‚è´¨æ€§å’Œé€å±‚æ¢¯åº¦å½’ä¸€åŒ–çš„åŸç†ï¼Œä¸ºè®¾è®¡è·¨é¢†åŸŸçš„å¤§å‹æ¨¡å‹çš„éšç§ä¿æŠ¤è”é‚¦å­¦ä¹ ç®—æ³•æä¾›äº†æ›´å¹¿æ³›çš„æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00098v2">PDF</a> Under review</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†è”é‚¦å­¦ä¹ ä¸å·®åˆ†éšç§åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­çš„åº”ç”¨ï¼Œé’ˆå¯¹å¤§å‹æ¨¡å‹åœ¨è”é‚¦å­¦ä¹ ä¸­çš„æ¢¯åº¦å¼‚è´¨æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†åŸºäºæ¯å±‚è£å‰ªå’Œå±‚é—´æ¢¯åº¦å½’ä¸€åŒ–çš„æ–¹æ³•ã€‚å»ºç«‹äº†ä¸€ä¸ªå…·æœ‰å·®åˆ†éšç§çš„è”é‚¦å­¦ä¹ åœ¨ç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­çš„é¦–ä¸ªåŸºå‡†æµ‹è¯•ï¼Œå¹¶é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯ç ”ç©¶éªŒè¯äº†è¯¥æ–¹æ³•çš„å¯è¡Œæ€§ã€‚åœ¨å¼ºå¤§çš„éšç§ä¿éšœä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨è‡³å°‘æ•°ç™¾ä¸‡ç”¨æˆ·çš„æƒ…å†µä¸‹æ˜¯å¯è¡Œçš„ã€‚åœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„åŒæ—¶ï¼Œå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡çš„å½±å“è¾ƒå°ã€‚æ­¤å¤–ï¼Œæ–‡ä¸­æ­ç¤ºçš„æ¢¯åº¦å¼‚è´¨æ€§å’Œå±‚é—´æ¢¯åº¦å½’ä¸€åŒ–ç­‰åŸç†ä¸ºè®¾è®¡è·¨é¢†åŸŸçš„å¤§è§„æ¨¡ã€éšç§ä¿æŠ¤çš„è”é‚¦å­¦ä¹ ç®—æ³•æä¾›äº†æ›´å¹¿æ³›çš„æŒ‡å¯¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ ä¸å·®åˆ†éšç§åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­çš„åº”ç”¨ä»å­˜åœ¨å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒå¤§å‹æ¨¡å‹æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹æ¨¡å‹åœ¨è”é‚¦å­¦ä¹ ä¸­é¢ä¸´æ¢¯åº¦å¼‚è´¨æ€§çš„é—®é¢˜ï¼Œè¿™ä¸æµ…å±‚æ¨¡å‹ä¸­ç›¸å¯¹ç»Ÿä¸€çš„æ¢¯åº¦è¡Œä¸ºä¸åŒã€‚</li>
<li>ç°æœ‰å·¥ä½œéš¾ä»¥åœ¨æ ‡å‡†ä¼˜åŒ–æŠ€æœ¯ä¸‹æ”¶æ•›ï¼Œç”šè‡³åœ¨ä¸å­˜åœ¨å·®åˆ†éšç§æœºåˆ¶çš„æƒ…å†µä¸‹ã€‚</li>
<li>æœ¬æ–‡å»ºç«‹äº†é¦–ä¸ªåœ¨ç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­å…·æœ‰å·®åˆ†éšç§çš„è”é‚¦å­¦ä¹ åŸºå‡†æµ‹è¯•ã€‚</li>
<li>é€šè¿‡æ¯å±‚è£å‰ªå’Œå±‚é—´æ¢¯åº¦å½’ä¸€åŒ–çš„æ–¹æ³•ï¼Œç¼“è§£äº†è£å‰ªåå·®å’Œæ·±å±‚æ¨¡å‹ä¸­çš„æ¢¯åº¦å¼‚è´¨æ€§ã€‚</li>
<li>åœ¨å¼ºå¤§çš„éšç§ä¿éšœä¸‹ï¼Œè¯¥æ–¹æ³•çš„å¯è¡Œæ€§å·²ç»å¾—åˆ°å®è¯ç ”ç©¶éªŒè¯ï¼Œå¹¶ä¸”åœ¨è‡³å°‘æ•°ç™¾ä¸‡ç”¨æˆ·çš„æƒ…å†µä¸‹ï¼Œå¯¹ç”¨æˆ·çº§åˆ«çš„éšç§ä¿æŠ¤è¡¨ç°è‰¯å¥½ã€‚</li>
<li>æœ¬æ–‡æ­ç¤ºçš„åŸç†ï¼Œå¦‚æ¢¯åº¦å¼‚è´¨æ€§å’Œå±‚é—´æ¢¯åº¦å½’ä¸€åŒ–ç­‰ï¼Œä¸ºè®¾è®¡è·¨é¢†åŸŸçš„å¤§è§„æ¨¡ã€éšç§ä¿æŠ¤çš„è”é‚¦å­¦ä¹ ç®—æ³•æä¾›äº†æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.00098">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c8b0657c14e426d3f633daeab0890a8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e63189a006d43140e98f8e456f9ed9c3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f16759ce1920af57a496df9762396cc3.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-04  Beyond Face Swapping A Diffusion-Based Digital Human Benchmark for   Multimodal Deepfake Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e03de1558239401138166a510a0d9825.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-04  MFCLIP Multi-modal Fine-grained CLIP for Generalizable Diffusion Face   Forgery Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31686.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
