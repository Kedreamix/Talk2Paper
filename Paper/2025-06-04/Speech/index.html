<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-06-04  MSDA Combining Pseudo-labeling and Self-Supervision for Unsupervised   Domain Adaptation in ASR">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-64f566e231dd16babc980ce9ea30a2ef.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    54 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-04-更新"><a href="#2025-06-04-更新" class="headerlink" title="2025-06-04 更新"></a>2025-06-04 更新</h1><h2 id="MSDA-Combining-Pseudo-labeling-and-Self-Supervision-for-Unsupervised-Domain-Adaptation-in-ASR"><a href="#MSDA-Combining-Pseudo-labeling-and-Self-Supervision-for-Unsupervised-Domain-Adaptation-in-ASR" class="headerlink" title="MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised   Domain Adaptation in ASR"></a>MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised   Domain Adaptation in ASR</h2><p><strong>Authors:Dimitrios Damianos, Georgios Paraskevopoulos, Alexandros Potamianos</strong></p>
<p>In this work, we investigate the Meta PL unsupervised domain adaptation framework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage Domain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation approach that integrates self-supervised learning with semi-supervised techniques. MSDA is designed to enhance the robustness and generalization of ASR models, making them more adaptable to diverse conditions. It is particularly effective for low-resource languages like Greek and in weakly supervised scenarios where labeled data is scarce or noisy. Through extensive experiments, we demonstrate that Meta PL can be applied effectively to ASR tasks, achieving state-of-the-art results, significantly outperforming state-of-the-art methods, and providing more robust solutions for unsupervised domain adaptation in ASR. Our ablations highlight the necessity of utilizing a cascading approach when combining self-supervision with self-training. </p>
<blockquote>
<p>在这项工作中，我们研究了用于自动语音识别（ASR）的Meta PL无监督域自适应框架。我们引入了一种多阶段域自适应管道（MSDA），这是一种样本高效的、两阶段的自适应方法，它将自监督学习与半监督技术相结合。MSDA旨在提高ASR模型的鲁棒性和泛化能力，使它们更能适应各种条件。它对低资源语言（如希腊语）以及在标签数据稀缺或嘈杂的弱监督场景中特别有效。通过大量实验，我们证明了Meta PL可以有效地应用于ASR任务，实现最先进的成果，显著优于现有方法，并为ASR中的无监督域自适应提供更稳健的解决方案。我们的消融研究强调了将自监督与自训练相结合时采用级联方法的必要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24656v2">PDF</a> Submitted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文研究了Meta PL无监督领域自适应框架在自动语音识别（ASR）中的应用。提出了一种多阶段领域自适应管道（MSDA），这是一种样本高效的、两阶段的自适应方法，结合了自监督学习与半监督技术。MSDA旨在提高ASR模型的鲁棒性和泛化能力，使其更适应不同的条件。在资源有限的语种如希腊语以及标签数据稀缺或嘈杂的弱监督场景中，该方法表现尤为出色。实验证明，Meta PL可有效应用于ASR任务，实现最新技术成果，显著优于现有方法，为ASR的无监督领域自适应提供更稳健的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了Meta PL无监督领域自适应框架在自动语音识别（ASR）中的应用。</li>
<li>提出了一种新的多阶段领域自适应管道（MSDA），结合了自监督学习与半监督技术。</li>
<li>MSDA增强了ASR模型的鲁棒性和泛化能力，适应不同的条件。</li>
<li>MSDA在资源有限的语种和弱监督场景中表现优异。</li>
<li>实验证明Meta PL在ASR任务上实现了最新技术成果。</li>
<li>MSDA显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24656">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6919ae5663441e597993137ccb66a956.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f4890306a3099cba8a4d16b51184d9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a62e73aef6235c153db5fde894612e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36c92477180313d34eb3075562b193aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e23ab06e38a341c09415fc5b6e7e918.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c49d3df1086fc48f4a5d2b8916cb52a4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Interspeech-2025-URGENT-Speech-Enhancement-Challenge"><a href="#Interspeech-2025-URGENT-Speech-Enhancement-Challenge" class="headerlink" title="Interspeech 2025 URGENT Speech Enhancement Challenge"></a>Interspeech 2025 URGENT Speech Enhancement Challenge</h2><p><strong>Authors:Kohei Saijo, Wangyou Zhang, Samuele Cornell, Robin Scheibler, Chenda Li, Zhaoheng Ni, Anurag Kumar, Marvin Sach, Yihui Fu, Wei Wang, Tim Fingscheidt, Shinji Watanabe</strong></p>
<p>There has been a growing effort to develop universal speech enhancement (SE) to handle inputs with various speech distortions and recording conditions. The URGENT Challenge series aims to foster such universal SE by embracing a broad range of distortion types, increasing data diversity, and incorporating extensive evaluation metrics. This work introduces the Interspeech 2025 URGENT Challenge, the second edition of the series, to explore several aspects that have received limited attention so far: language dependency, universality for more distortion types, data scalability, and the effectiveness of using noisy training data. We received 32 submissions, where the best system uses a discriminative model, while most other competitive ones are hybrid methods. Analysis reveals some key findings: (i) some generative or hybrid approaches are preferred in subjective evaluations over the top discriminative model, and (ii) purely generative SE models can exhibit language dependency. </p>
<blockquote>
<p>在开发能够处理各种语音失真和录音条件的通用语音增强（SE）方面，已经付出了越来越多的努力。URGENT Challenge系列旨在通过接纳广泛的失真类型、增加数据多样性和引入广泛的评估指标来促进这种通用SE的发展。这项工作介绍了Interspeech 2025 URGENT Challenge，即该系列的第二版，旨在探索迄今为止受到有限关注的几个方面：语言依赖性、更多失真类型的普遍性、数据可扩展性以及使用带噪声训练数据的有效性。我们收到了32份提交，其中最佳系统使用的是判别模型，而其他大多数有竞争力的系统都是混合方法。分析揭示了一些关键发现：（i）在某些主观评价中，一些生成性或混合方法优于顶级判别模型；（ii）纯粹的生成性SE模型可能会表现出语言依赖性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23212v2">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本段文本主要介绍了Interspeech 2025 URGENT Challenge系列的目标和成果。该挑战旨在开发通用语音增强技术，应对各种语音失真和录音条件的问题。其中引入了第二款挑战：探索语言依赖性、更多失真类型的普遍性、数据可扩展性以及使用噪声训练数据的有效性。分析结果显示，一些生成性或混合方法在某些主观评价中优于顶级判别模型，而纯粹的生成性语音增强模型可能存在语言依赖性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Interspeech 2025 URGENT Challenge旨在开发通用语音增强技术，以处理各种语音失真和录音条件的问题。</li>
<li>该挑战的第二版探索了语言依赖性、更多失真类型的普遍性、数据可扩展性和使用噪声训练数据的有效性等几个方面。</li>
<li>目前收到的32份提交中，最佳系统使用的是判别模型，而其他有竞争力的系统多为混合方法。</li>
<li>分析发现，在某些主观评价中，一些生成性或混合方法可能优于顶级判别模型。</li>
<li>纯粹的生成性语音增强模型可能存在语言依赖性。</li>
<li>该挑战旨在拥抱广泛的失真类型，增加数据多样性，并融入广泛的评估指标，以推动通用语音增强技术的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23212">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5f12c5f74507f6c5a1bcee56d33d3b64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2e4c333298de0703dfcd6675857f29db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d176c61b24639b6fc514b7f6664d382a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Developing-a-Top-tier-Framework-in-Naturalistic-Conditions-Challenge-for-Categorized-Emotion-Prediction-From-Speech-Foundation-Models-and-Learning-Objective-to-Data-Augmentation-and-Engineering-Choices"><a href="#Developing-a-Top-tier-Framework-in-Naturalistic-Conditions-Challenge-for-Categorized-Emotion-Prediction-From-Speech-Foundation-Models-and-Learning-Objective-to-Data-Augmentation-and-Engineering-Choices" class="headerlink" title="Developing a Top-tier Framework in Naturalistic Conditions Challenge for   Categorized Emotion Prediction: From Speech Foundation Models and Learning   Objective to Data Augmentation and Engineering Choices"></a>Developing a Top-tier Framework in Naturalistic Conditions Challenge for   Categorized Emotion Prediction: From Speech Foundation Models and Learning   Objective to Data Augmentation and Engineering Choices</h2><p><strong>Authors:Tiantian Feng, Thanathai Lertpetchpun, Dani Byrd, Shrikanth Narayanan</strong></p>
<p>Speech emotion recognition (SER), particularly for naturally expressed emotions, remains a challenging computational task. Key challenges include the inherent subjectivity in emotion annotation and the imbalanced distribution of emotion labels in datasets. This paper introduces the \texttt{SAILER} system developed for participation in the INTERSPEECH 2025 Emotion Recognition Challenge (Task 1). The challenge dataset, which contains natural emotional speech from podcasts, serves as a valuable resource for studying imbalanced and subjective emotion annotations. Our system is designed to be simple, reproducible, and effective, highlighting critical choices in modeling, learning objectives, data augmentation, and engineering choices. Results show that even a single system (without ensembling) can outperform more than 95% of the submissions, with a Macro-F1 score exceeding 0.4. Moreover, an ensemble of three systems further improves performance, achieving a competitively ranked score (top-3 performing team). Our model is at: <a target="_blank" rel="noopener" href="https://github.com/tiantiaf0627/vox-profile-release">https://github.com/tiantiaf0627/vox-profile-release</a>. </p>
<blockquote>
<p>语音情感识别（SER），尤其是对自然表达情感的识别，仍然是一项具有挑战性的计算任务。主要挑战包括情感注释中的固有主观性和数据集中情感标签的不平衡分布。本文介绍了为参与INTERSPEECH 2025情感识别挑战赛（任务1）而开发的&#96;SAILER’系统。挑战赛数据集包含来自播客的自然情感语音，是研究不平衡和主观情感注释的宝贵资源。我们的系统设计简单、可复制、有效，突出显示建模、学习目标、数据增强和工程选择中的关键选择。结果表明，即使是一个系统（无需集合）也能表现出超过95%提交的结果，宏观F1分数超过0.4。此外，三个系统的组合进一步提高了性能，取得了竞争排名分数（排名前三的团队）。我们的模型位于：<a target="_blank" rel="noopener" href="https://github.com/tiantiaf0627/vox-profile-release%E3%80%82">https://github.com/tiantiaf0627/vox-profile-release。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22133v2">PDF</a> Accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>该论文针对语音情感识别（SER）面临的挑战，如情感标注的主观性和数据集情感标签的不平衡分布，提出了为参与INTERSPEECH 2025情感识别挑战赛（任务一）而开发的<code>SAILER</code>系统。该系统旨在设计简单、可复制和高效，强调建模、学习目标、数据增强和工程选择的重要性。实验结果显示，单一系统性能已超越大多数提交结果，宏观F1分数超过0.4。通过集成三个系统，性能进一步提升，成为排名前三的团队。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文指出语音情感识别（SER）面临的挑战，包括情感标注的主观性和数据集情感标签的不平衡分布。</li>
<li>介绍了为INTERSPEECH 2025情感识别挑战赛开发的<code>SAILER</code>系统。</li>
<li><code>SAILER</code>系统的设计重点是简单性、可复制性和高效性。</li>
<li>论文强调了建模、学习目标、数据增强和工程选择的重要性。</li>
<li>实验结果显示，单一系统的性能已超越大多数提交的结果，宏观F1分数超过0.4。</li>
<li>通过集成三个系统，性能进一步提升，成为竞赛中排名靠前的团队。</li>
<li>论文提供了其模型的公开访问链接。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22133">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-49f9436a519b0100b6b1602b8cf38d3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c33138f2f85cfc98882eb1c1055f187d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-537229c6ae9fd890d7603f5de6d96722.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77e9fcbe821c9847bc482c87adefc4c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64f566e231dd16babc980ce9ea30a2ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b99b4edcf6885b3f8abf53f34225cc5d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ClearSphere-Multi-Earphone-Synergy-for-Enhanced-Conversational-Clarity"><a href="#ClearSphere-Multi-Earphone-Synergy-for-Enhanced-Conversational-Clarity" class="headerlink" title="ClearSphere: Multi-Earphone Synergy for Enhanced Conversational Clarity"></a>ClearSphere: Multi-Earphone Synergy for Enhanced Conversational Clarity</h2><p><strong>Authors:Lixing He</strong></p>
<p>In crowded places such as conferences, background noise, overlapping voices, and lively interactions make it difficult to have clear conversations. This situation often worsens the phenomenon known as “cocktail party deafness.” We present ClearSphere, the collaborative system that enhances speech at the conversation level with multi-earphones. Real-time conversation enhancement requires a holistic modeling of all the members in the conversation, and an effective way to extract the speech from the mixture. ClearSphere bridges the acoustic sensor system and state-of-the-art deep learning for target speech extraction by making two key contributions: 1) a conversation-driven network protocol, and 2) a robust target conversation extraction model. Our networking protocol enables mobile, infrastructure-free coordination among earphone devices. Our conversation extraction model can leverage the relay audio in a bandwidth-efficient way. ClearSphere is evaluated in both real-world experiments and simulations. Results show that our conversation network obtains more than 90% accuracy in group formation, improves the speech quality by up to 8.8 dB over state-of-the-art baselines, and demonstrates real-time performance on a mobile device. In a user study with 20 participants, ClearSphere has a much higher score than baseline with good usability. </p>
<blockquote>
<p>在会议等拥挤场所，背景噪音、声音重叠以及活跃的互动使得进行清晰的对话变得困难。这种情况会加剧所谓的“鸡尾酒会耳聋”现象。我们推出了ClearSphere，这是一款协作系统，通过多耳机增强对话级别的语音。实时对话增强需要对所有对话成员进行整体建模，并需要一种从混合语音中提取目标语音的有效方法。ClearSphere通过两项关键贡献——对话驱动的网络协议和稳健的目标对话提取模型，搭建了声学传感器系统和最新的深度学习目标语音提取之间的桥梁。我们的网络协议实现了耳机设备之间无需基础设施的移动协调。我们的对话提取模型能够以带宽有效的方式利用中继音频。ClearSphere在真实实验和模拟中都得到了评估。结果表明，我们的对话网络在组队方面的准确率超过90%，在最新基线的基础上改善了高达8.8分贝的语音质量，并在移动设备上实现了实时性能。在20名参与者的一项用户研究中，ClearSphere的得分远高于基线，具有良好的可用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21004v2">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>在会议等拥挤场合，背景噪音、声音重叠及活跃互动使得清晰交流变得困难，加剧了“鸡尾酒会耳聋”现象。为此，我们推出了ClearSphere协作系统，通过多耳机增强对话级别的语音。实时对话增强需要全面建模所有参与者的声音，并有效地从混合声音中提取目标语音。ClearSphere通过两项关键贡献——对话驱动的网络协议和稳健的目标对话提取模型，将声学传感器系统和最新的深度学习技术相结合，实现目标语音提取。网络协议可实现耳机设备之间的移动、无基础设施协调。对话提取模型能够以带宽有效的方式利用中继音频。ClearSphere在真实实验和模拟中均得到评估，结果显示，其对话网络在群体形成方面的准确率超过90%，在语音质量上比最新基线提高了高达8.8分贝，并在移动设备上实现了实时性能。在用户研究中，ClearSphere较基线产品具有更高得分和良好的可用性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>在拥挤场所如会议中，背景噪音和声音重叠使得清晰交流变得困难，加剧了“鸡尾酒会耳聋”现象。</li>
<li>ClearSphere是一个协作系统，旨在通过多耳机增强对话级别的语音。</li>
<li>实时对话增强需要全面建模所有参与者的声音并有效提取目标语音。</li>
<li>ClearSphere通过对话驱动的网络协议和稳健的目标对话提取模型实现目标语音提取。</li>
<li>网络协议支持耳机设备间的移动、无基础设施协调。</li>
<li>ClearSphere在真实实验和模拟中表现优异，对话网络准确率超过90%，语音质量显著提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21004">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1efff593247fc488a9a688acb88b9274.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4a831015983902fb22023d7590083c5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SoloSpeech-Enhancing-Intelligibility-and-Quality-in-Target-Speech-Extraction-through-a-Cascaded-Generative-Pipeline"><a href="#SoloSpeech-Enhancing-Intelligibility-and-Quality-in-Target-Speech-Extraction-through-a-Cascaded-Generative-Pipeline" class="headerlink" title="SoloSpeech: Enhancing Intelligibility and Quality in Target Speech   Extraction through a Cascaded Generative Pipeline"></a>SoloSpeech: Enhancing Intelligibility and Quality in Target Speech   Extraction through a Cascaded Generative Pipeline</h2><p><strong>Authors:Helin Wang, Jiarui Hai, Dongchao Yang, Chen Chen, Kai Li, Junyi Peng, Thomas Thebaud, Laureano Moro Velazquez, Jesus Villalba, Najim Dehak</strong></p>
<p>Target Speech Extraction (TSE) aims to isolate a target speaker’s voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high perceptual quality, these models often introduce unwanted artifacts, reduce naturalness, and are sensitive to discrepancies between training and testing environments. On the other hand, generative models for TSE lag in perceptual quality and intelligibility. To address these challenges, we present SoloSpeech, a novel cascaded generative pipeline that integrates compression, extraction, reconstruction, and correction processes. SoloSpeech features a speaker-embedding-free target extractor that utilizes conditional information from the cue audio’s latent space, aligning it with the mixture audio’s latent space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset, SoloSpeech achieves the new state-of-the-art intelligibility and quality in target speech extraction and speech separation tasks while demonstrating exceptional generalization on out-of-domain data and real-world scenarios. </p>
<blockquote>
<p>目标语音提取（TSE）旨在利用特定于说话者的线索（通常作为辅助音频（即线索音频）提供）从多个说话者的混合声音中分离出目标说话者的声音。尽管最近的TSE进展主要采用了提供高感知质量的判别模型，但这些模型往往引入了不需要的伪影，降低了自然度，并且对训练和测试环境之间的差异很敏感。另一方面，TSE的生成模型在感知质量和清晰度方面有所不足。为了解决这些挑战，我们提出了SoloSpeech，这是一种新型级联生成管道，它集成了压缩、提取、重建和校正过程。SoloSpeech的特点是无需说话者嵌入的目标提取器，它利用线索音频的潜在空间的条件信息，将其与混合音频的潜在空间对齐，以防止不匹配。在广泛使用的Libri2Mix数据集上，SoloSpeech在目标语音提取和语音分离任务中实现了最新的最高清晰度和质量，同时在域外数据和现实场景上表现出卓越的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19314v2">PDF</a> </p>
<p><strong>Summary</strong><br>目标语音提取（TSE）旨在利用说话人特定线索从多个说话人的混合语音中分离出目标说话人的声音。虽然最近的研究主要采用提供高感知质量的判别模型，但这些模型常引入不必要的伪影、降低自然度，并对训练和测试环境的差异敏感。另一方面，生成式TSE模型在感知质量和清晰度方面滞后。为解决这些挑战，我们提出了无说话人嵌入的SoloSpeech，这是一个集成压缩、提取、重建和校正过程的新型级联生成管道。它在广泛使用的Libri2Mix数据集上实现了目标语音提取和语音分离任务的新颖性和卓越性，同时在域外数据和真实世界场景上表现出出色的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>目标语音提取（TSE）旨在从混合语音中分离出目标说话人的声音。</li>
<li>现有模型面临引入伪影、降低自然度及对环境差异敏感的挑战。</li>
<li>生成式TSE模型在感知质量和清晰度方面有待提高。</li>
<li>SoloSpeech是一个新型级联生成管道，集成压缩、提取、重建和校正过程。</li>
<li>SoloSpeech利用条件信息，通过潜在空间对齐，实现目标语音的精准提取。</li>
<li>在Libri2Mix数据集上，SoloSpeech达到了目标语音提取和语音分离任务的新水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19314">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-052d8aaa1f2a8e254c974da79f4e20ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e800b7845030112c089c4dee17be2d3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d4b3411393f5cd9d4a494f56120efb4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f7a660eb64f8d76b8fc7c66d5640d567.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b1a4ab8a735330ac8c22f03b1727d5d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Effect-of-Segmentation-and-Vocabulary-Size-on-Speech-Tokenization-for-Speech-Language-Models"><a href="#Exploring-the-Effect-of-Segmentation-and-Vocabulary-Size-on-Speech-Tokenization-for-Speech-Language-Models" class="headerlink" title="Exploring the Effect of Segmentation and Vocabulary Size on Speech   Tokenization for Speech Language Models"></a>Exploring the Effect of Segmentation and Vocabulary Size on Speech   Tokenization for Speech Language Models</h2><p><strong>Authors:Shunsuke Kando, Yusuke Miyao, Shinnosuke Takamichi</strong></p>
<p>The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed&#x2F;variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding. </p>
<blockquote>
<p>语音切词的目的是将语音信号转换为一系列离散表示，作为语音语言模型（SLM）的基础。虽然语音切词有很多选项，但它们对SLM性能的影响仍不清楚。本文研究了语音切词的两个方面：分段宽度和离散单元簇的大小。首先，我们将语音信号分段为固定&#x2F;可变宽度和池化表示。然后，我们在多个簇大小上训练K-means模型。通过对零样本口语理解基准的评估，我们发现适度粗糙的分段和较大的簇大小具有积极的影响。值得注意的是，在表现最佳的模型中，最有效的模型实现了训练数据50%的减少和训练运行时间70%的减少。我们的分析强调了结合多个令牌以增强精细口语理解的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17446v2">PDF</a> Accepted to Interspeech2025</p>
<p><strong>Summary</strong><br>语音标记化的目的是将语音信号转换为离散表示的序列，为语音语言模型（SLMs）打下基础。本文探讨了语音标记化的两个关键方面：分段宽度和离散单元的簇大小。研究发现，适度粗糙的分段和较大的簇大小对提升模型性能有正面效果，最高效的模型能在训练数据和训练时间上分别减少50%和70%。分析强调了结合多个标记以增强精细粒度语音语言理解的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音标记化是将语音信号转换为离散表示的序列的过程，为语音语言模型（SLMs）奠定基础。</li>
<li>本文研究了语音标记化的两个关键参数：分段宽度和簇大小。</li>
<li>适度粗糙的分段和较大的簇大小对SLM的性能有积极影响。</li>
<li>最高效的模型在减少训练数据和训练时间方面取得了显著成果。</li>
<li>结合多个标记有助于增强精细粒度的语音语言理解。</li>
<li>语音标记化的研究对于提高SLM的性能具有潜力，特别是在处理大规模语音数据方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17446">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c516cf5a9658057f366122ef25bb1538.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd5fdc9a7fc3eedf4fdf004361f2788a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb2c99cf1566ee39243d28fbf980bee3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-257579c8a30101a6150724a5728da600.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-975bc3cccba45a17f24ae9ec37c77142.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c76bc3715c57c5168db5014fe616f533.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="An-End-to-End-Approach-for-Child-Reading-Assessment-in-the-Xhosa-Language"><a href="#An-End-to-End-Approach-for-Child-Reading-Assessment-in-the-Xhosa-Language" class="headerlink" title="An End-to-End Approach for Child Reading Assessment in the Xhosa   Language"></a>An End-to-End Approach for Child Reading Assessment in the Xhosa   Language</h2><p><strong>Authors:Sergio Chevtchenko, Nikhil Navas, Rafaella Vale, Franco Ubaudi, Sipumelele Lucwaba, Cally Ardington, Soheil Afshar, Mark Antoniou, Saeed Afshar</strong></p>
<p>Child literacy is a strong predictor of life outcomes at the subsequent stages of an individual’s life. This points to a need for targeted interventions in vulnerable low and middle income populations to help bridge the gap between literacy levels in these regions and high income ones. In this effort, reading assessments provide an important tool to measure the effectiveness of these programs and AI can be a reliable and economical tool to support educators with this task. Developing accurate automatic reading assessment systems for child speech in low-resource languages poses significant challenges due to limited data and the unique acoustic properties of children’s voices. This study focuses on Xhosa, a language spoken in South Africa, to advance child speech recognition capabilities. We present a novel dataset composed of child speech samples in Xhosa. The dataset is available upon request and contains ten words and letters, which are part of the Early Grade Reading Assessment (EGRA) system. Each recording is labeled with an online and cost-effective approach by multiple markers and a subsample is validated by an independent EGRA reviewer. This dataset is evaluated with three fine-tuned state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The results indicate that the performance of these models can be significantly influenced by the amount and balancing of the available training data, which is fundamental for cost-effective large dataset collection. Furthermore, our experiments indicate that the wav2vec 2.0 performance is improved by training on multiple classes at a time, even when the number of available samples is constrained. </p>
<blockquote>
<p>儿童识字能力对其后续生命阶段的生活成果具有很强的预测作用。这指出了针对脆弱的中低收入人群进行针对性干预的必要性，以帮助缩小这些地区识字水平与高收入地区之间的差距。在这方面，阅读评估是衡量这些节目效果的重要工具，人工智能可以成为支持教育工作者完成这项任务的可靠且经济的工具。为低资源语言的孩子开发准确的自动阅读评估系统面临着重大挑战，因为数据有限且儿童的声音具有独特的声学特性。这项研究专注于南非使用的语言——科萨语，以提升儿童语音识别能力。我们展示了一个由科萨语儿童语音样本组成的新数据集。该数据集可在请求后获得，包含十个单词和字母，这些单词和字母是初级阅读评估（EGRA）系统的一部分。每个录音都通过多个标记器以在线和低成本的方式进行标记，并由独立的EGRA审查者对样本进行验证。该数据集经过三种经过微调的最先进端到端模型的评估：wav2vec 2.0、HuBERT和Whisper。结果表明，这些模型的性能可能会受到可用训练数据量和平衡性的显著影响，这对于低成本大规模数据集收集至关重要。此外，我们的实验表明，即使在可用样本数量受限的情况下，通过一次训练多个类别也可以提高wav2vec 2.0的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17371v2">PDF</a> Paper accepted on AIED 2025 containing 14 pages, 6 figures and 4   tables</p>
<p><strong>摘要</strong></p>
<p>儿童识字能力对其后续生活阶段的影响具有显著预测作用，这突显了对脆弱低收入和中收入群体进行针对性干预的必要性，以帮助缩小这些地区与高收入地区识字水平的差距。阅读评估是衡量这些项目成效的重要工具，而人工智能可为教育工作者提供可靠且经济的支持。在为低资源语言开发准确的自动阅读评估系统时，由于数据有限以及儿童声音的独特声学特性，面临重大挑战。本研究关注南非使用的语言——科萨语，以提升儿童语音识别能力。我们提供了一份新数据集，其中包含科萨语儿童语音样本。该数据集经请求可获得，包含早期阅读能力评估系统中的十个单词和字母。每个记录都采用了在线经济的多重标记方法进行标注，并由独立的早期阅读能力评估审查者对样本进行了验证。该数据集经过wav2vec 2.0、HuBERT和Whisper三款先进的端到端模型的微调评估。结果表明，可用训练数据的质量和平衡性对这些模型的性能有重大影响，这对于经济的大规模数据集收集至关重要。此外，我们的实验表明，即使在可用样本数量有限的情况下，通过同时训练多个类别，wav2vec 2.0的性能也能得到提升。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>儿童识字能力对个体后续生活阶段的影响具有预测性。</li>
<li>需要针对脆弱低收入和中收入群体进行针对性的阅读干预。</li>
<li>阅读评估是衡量阅读项目效果的重要工具，AI可支持此任务。</li>
<li>开发自动阅读评估系统时面临数据限制和儿童声音独特性挑战。</li>
<li>科萨语儿童语音数据集用于提升语音识别能力研究。</li>
<li>数据集质量及平衡性对模型性能影响显著，强调大规模数据集收集的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17371">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a6b8a63d7791b4cdedbb151d9f9f93ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4e33aa3709d3ae11ca3f52047076849.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-798f6a03b9ca85b10edafe60ab63b3f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1b41946b7c8ba9d8f0e60d82315dd88.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Counterspeech-the-ultimate-shield-Multi-Conditioned-Counterspeech-Generation-through-Attributed-Prefix-Learning"><a href="#Counterspeech-the-ultimate-shield-Multi-Conditioned-Counterspeech-Generation-through-Attributed-Prefix-Learning" class="headerlink" title="Counterspeech the ultimate shield! Multi-Conditioned Counterspeech   Generation through Attributed Prefix Learning"></a>Counterspeech the ultimate shield! Multi-Conditioned Counterspeech   Generation through Attributed Prefix Learning</h2><p><strong>Authors:Aswini Kumar, Anil Bandhakavi, Tanmoy Chakraborty</strong></p>
<p>Counterspeech has proven to be a powerful tool to combat hate speech online. Previous studies have focused on generating counterspeech conditioned only on specific intents (single attributed). However, a holistic approach considering multiple attributes simultaneously can yield more nuanced and effective responses. Here, we introduce HiPPrO, Hierarchical Prefix learning with Preference Optimization, a novel two-stage framework that utilizes the effectiveness of attribute-specific prefix embedding spaces hierarchically optimized during the counterspeech generation process in the first phase. Thereafter, we incorporate both reference and reward-free preference optimization to generate more constructive counterspeech. Furthermore, we extend IntentCONANv2 by annotating all 13,973 counterspeech instances with emotion labels by five annotators. HiPPrO leverages hierarchical prefix optimization to integrate these dual attributes effectively. An extensive evaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent conformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L, respectively, compared to several baseline models. Human evaluations further substantiate the superiority of our approach, highlighting the enhanced relevance and appropriateness of the generated counterspeech. This work underscores the potential of multi-attribute conditioning in advancing the efficacy of counterspeech generation systems. Our code is available on Github and dataset is open-sourced on Hugging-face. </p>
<blockquote>
<p>反话已被证明是打击网上仇恨言论的有力工具。以往的研究主要集中在仅针对特定意图（单一属性）生成反话。然而，同时考虑多个属性的整体方法可能会产生更微妙和有效的回应。在这里，我们介绍了HiPPrO，即带有偏好优化的分层前缀学习，这是一种新的两阶段框架，它利用属性特定前缀嵌入空间在反话生成过程中的分层优化效果。此后，我们将参考和无奖励偏好优化相结合，以生成更具建设性的反话。此外，我们对所有13973个反话实例进行了情感标签标注，由五位标注者完成。HiPPrO利用分层前缀优化有效地整合了这些双重属性。大量评估表明，与几个基准模型相比，HiPPrO在意图一致性上提高了约38%，在Rouge-1、Rouge-2和Rouge-L上分别提高了约3%、2%、3%。人类评估进一步证实了我们方法的优越性，突出了生成反话的相关性和适当性的提高。这项工作强调了多属性条件在提升反话生成系统效率方面的潜力。我们的代码已在Github上提供，数据集已在Hugging-face上开源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11958v3">PDF</a> Accepted in ACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>本文介绍了对抗网络仇恨言论的有力工具——反话。以往的研究主要关注基于特定意图的反话生成，而本文提出一种综合考虑多种属性的全息方法，能生成更细腻和有效的回应。为此，本文引入HiPPrO框架，采用属性特定的前缀嵌入空间进行层次优化，生成更具建设性的反话。实验证明，HiPPrO在意图符合度上提高了约38%，在Rouge指标上也有显著提升。反话生成系统的多属性调节具有巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>反话是打击网络仇恨言论的有效工具。</li>
<li>以往研究主要关注基于单一意图的反话生成，本文提出综合考虑多种属性的全息方法。</li>
<li>HiPPrO框架利用属性特定的前缀嵌入空间进行层次优化，生成更具建设性的反话。</li>
<li>HiPPrO在意图符合度上较基线模型提高了约38%。</li>
<li>在Rouge指标上，HiPPrO较基线模型有显著提升。</li>
<li>人类评估进一步证明了HiPPrO方法的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11958">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-115ac15b7f4e5de46f40791fddd9b4be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2114ef1160b95f2589e0b548c8ff433.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-535b6242b793eb6b4c6568ecf9145119.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48a570a92b74fe981ae2f28f0c2f181b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLM-based-Hatred-and-Toxicity-Detection-with-Meta-Toxic-Knowledge-Graph"><a href="#Enhancing-LLM-based-Hatred-and-Toxicity-Detection-with-Meta-Toxic-Knowledge-Graph" class="headerlink" title="Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic   Knowledge Graph"></a>Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic   Knowledge Graph</h2><p><strong>Authors:Yibo Zhao, Jiapeng Zhu, Can Xu, Yao Liu, Xiang Li</strong></p>
<p>The rapid growth of social media platforms has raised significant concerns regarding online content toxicity. When Large Language Models (LLMs) are used for toxicity detection, two key challenges emerge: 1) the absence of domain-specific toxic knowledge leads to false negatives; 2) the excessive sensitivity of LLMs to toxic speech results in false positives, limiting freedom of speech. To address these issues, we propose a novel method called MetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance hatred and toxicity detection. First, we construct a comprehensive meta-toxic knowledge graph by utilizing LLMs to extract toxic information through a three-step pipeline, with toxic benchmark datasets serving as corpora. Second, we query the graph via retrieval and ranking processes to supplement accurate, relevant toxic knowledge. Extensive experiments and in-depth case studies across multiple datasets demonstrate that our MetaTox significantly decreases the false positive rate while boosting overall toxicity detection performance. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/YiboZhao624/MetaTox">https://github.com/YiboZhao624/MetaTox</a>. </p>
<blockquote>
<p>社交媒体平台的快速发展引发了人们对网络内容毒性的重大关注。当使用大型语言模型（LLM）进行毒性检测时，会出现两个关键挑战：1）缺乏特定领域的毒性知识会导致假阴性结果；2）LLM对有毒言论过于敏感，导致假阳性结果，限制言论自由。为了解决这些问题，我们提出了一种名为MetaTox的新方法，利用元毒性知识图谱上的图搜索来增强仇恨和毒性检测。首先，我们通过一个三步骤的管道，利用LLM提取毒性信息，以毒性基准数据集作为语料库，构建了一个全面的元毒性知识图谱。其次，我们通过检索和排名过程查询图谱，以补充准确且相关的毒性知识。在多个数据集上的广泛实验和深入案例研究表明，我们的MetaTox方法显著降低了误报率，同时提高了总体毒性检测性能。我们的代码可在[<a target="_blank" rel="noopener" href="https://github.com/YiboZhao624/MetaTox%E8%8E%B7%E5%8F%96%E3%80%82]">https://github.com/YiboZhao624/MetaTox获取。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15268v4">PDF</a> 8 pages of content</p>
<p><strong>Summary</strong></p>
<p>社交媒体平台的快速发展引发了人们对网络内容毒性的关注。针对大型语言模型（LLMs）在毒性检测中的两大挑战，提出了一种新的方法MetaTox，利用元毒性知识图谱的图形搜索来增强仇恨和毒性检测。通过构建全面的元毒性知识图谱，以及通过查询图谱以补充准确且相关的毒性知识来解决这些问题。实验结果证明了MetaTox在降低误报率的同时，提高了毒性检测的总体性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>社交媒体内容的毒性检测面临两大挑战：缺乏领域特定的毒性知识和过度敏感导致的误报。</li>
<li>提出了一种新的方法MetaTox，利用元毒性知识图谱的图形搜索增强毒性检测。</li>
<li>MetaTox通过构建全面的元毒性知识图谱来解决缺乏领域特定知识的问题。</li>
<li>通过查询知识图谱来补充准确且相关的毒性知识，以提高检测准确性并降低误报率。</li>
<li>MetaTox能够显著降低误报率，同时提高毒性检测的总体性能。</li>
<li>进行了大量的实验和深入的案例研究，证明了MetaTox的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15268">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4c797347e0158f4ceb08fb0154dd2d1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4117e7f489848b54039de48bf5f165d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-505ad0de96990783c6cb239a97f90499.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d30f192e2c185a64a742a9bfec37da5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Efficient-Generative-Modeling-with-Residual-Vector-Quantization-Based-Tokens"><a href="#Efficient-Generative-Modeling-with-Residual-Vector-Quantization-Based-Tokens" class="headerlink" title="Efficient Generative Modeling with Residual Vector Quantization-Based   Tokens"></a>Efficient Generative Modeling with Residual Vector Quantization-Based   Tokens</h2><p><strong>Authors:Jaehyeon Kim, Taehong Moon, Keon Lee, Jaewoong Cho</strong></p>
<p>We introduce ResGen, an efficient Residual Vector Quantization (RVQ)-based generative model for high-fidelity generation with fast sampling. RVQ improves data fidelity by increasing the number of quantization steps, referred to as depth, but deeper quantization typically increases inference steps in generative models. To address this, ResGen directly predicts the vector embedding of collective tokens rather than individual ones, ensuring that inference steps remain independent of RVQ depth. Additionally, we formulate token masking and multi-token prediction within a probabilistic framework using discrete diffusion and variational inference. We validate the efficacy and generalizability of the proposed method on two challenging tasks across different modalities: conditional image generation on ImageNet 256x256 and zero-shot text-to-speech synthesis. Experimental results demonstrate that ResGen outperforms autoregressive counterparts in both tasks, delivering superior performance without compromising sampling speed. Furthermore, as we scale the depth of RVQ, our generative models exhibit enhanced generation fidelity or faster sampling speeds compared to similarly sized baseline models. </p>
<blockquote>
<p>我们介绍了ResGen，这是一种基于高效的残差矢量量化（RVQ）的生成模型，用于高保真生成和快速采样。RVQ通过增加量化步骤的数量（称为深度）来提高数据保真度，但更深的量化通常会增加生成模型中的推理步骤。为了解决这一问题，ResGen直接预测集体标记的向量嵌入，而不是单个标记的向量嵌入，从而确保推理步骤与RVQ深度无关。此外，我们利用离散扩散和变分推理，在概率框架下制定标记掩码和多标记预测。我们在不同模态的两个具有挑战性的任务上验证了所提出方法的有效性和通用性：ImageNet 256x256上的条件图像生成和零样本文本到语音的合成。实验结果表明，ResGen在这两个任务上的表现均优于自回归模型，在不影响采样速度的情况下实现了卓越的性能。此外，随着我们扩展RVQ的深度，我们的生成模型与类似规模的基准模型相比，表现出更高的生成保真度或更快的采样速度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10208v3">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>ResGen是一种基于Residual Vector Quantization（RVQ）的高效生成模型，可实现高保真生成和快速采样。它通过增加量化步骤的数量（称为深度）来提高数据保真度，而无需增加推理步骤。ResGen直接预测集体标记的向量嵌入，而不是单独的标记，确保了推理步骤与RVQ深度无关。此外，它在概率框架下制定了标记掩码和多标记预测，采用离散扩散和变分推断方法。在ImageNet 256x256上的条件图像生成和零样本文本到语音合成等任务上验证了该方法的有效性和通用性。实验结果表明，ResGen在性能上优于自回归模型，提高了生成保真度或更快的采样速度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ResGen是一个基于Residual Vector Quantization（RVQ）的生成模型，可实现高保真生成和快速采样。</li>
<li>RVQ通过增加量化步骤的数量提高了数据保真度，而ResGen的设计保证了推理步骤与量化深度无关。</li>
<li>ResGen直接预测集体标记的向量嵌入，提高了模型效率。</li>
<li>该模型采用概率框架进行标记掩码和多标记预测，结合离散扩散和变分推断方法。</li>
<li>在条件图像生成和零样本文本到语音合成任务上，ResGen表现出优越的性能。</li>
<li>ResGen相较于自回归模型，能够提高生成保真度或加快采样速度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10208">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d011611d18708640fd1edf813a638d91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b60915cd1b56567d2223dace85d26489.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="BanTH-A-Multi-label-Hate-Speech-Detection-Dataset-for-Transliterated-Bangla"><a href="#BanTH-A-Multi-label-Hate-Speech-Detection-Dataset-for-Transliterated-Bangla" class="headerlink" title="BanTH: A Multi-label Hate Speech Detection Dataset for Transliterated   Bangla"></a>BanTH: A Multi-label Hate Speech Detection Dataset for Transliterated   Bangla</h2><p><strong>Authors:Fabiha Haider, Fariha Tanjim Shifat, Md Farhan Ishmam, Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, Md Fahim, Md Farhad Alam</strong></p>
<p>The proliferation of transliterated texts in digital spaces has emphasized the need for detecting and classifying hate speech in languages beyond English, particularly in low-resource languages. As online discourse can perpetuate discrimination based on target groups, e.g. gender, religion, and origin, multi-label classification of hateful content can help in comprehending hate motivation and enhance content moderation. While previous efforts have focused on monolingual or binary hate classification tasks, no work has yet addressed the challenge of multi-label hate speech classification in transliterated Bangla. We introduce BanTH, the first multi-label transliterated Bangla hate speech dataset comprising 37.3k samples. The samples are sourced from YouTube comments, where each instance is labeled with one or more target groups, reflecting the regional demographic. We establish novel transformer encoder-based baselines by further pre-training on transliterated Bangla corpus. We also propose a novel translation-based LLM prompting strategy for transliterated text. Experiments reveal that our further pre-trained encoders are achieving state-of-the-art performance on the BanTH dataset, while our translation-based prompting outperforms other strategies in the zero-shot setting. The introduction of BanTH not only fills a critical gap in hate speech research for Bangla but also sets the stage for future exploration into code-mixed and multi-label classification challenges in underrepresented languages. </p>
<blockquote>
<p>数字空间中音译文本的激增强调了检测和非英语语言的仇恨言论分类的需求，特别是在资源较少的语言中。由于在线言论可能会使基于目标群体的歧视永久化，例如性别、宗教和出身等，仇恨内容的多标签分类有助于理解仇恨动机并增强内容管理。虽然以前的研究主要集中于单语或二元仇恨分类任务，但尚未有工作在音译孟加拉语的多标签仇恨言论分类方面提出挑战。我们介绍了BanTH，这是第一个多标签音译孟加拉仇恨言论数据集，包含37.3k样本。这些样本来源于YouTube评论，每个实例都被标记为一个或多个目标群体，反映了地区人口状况。我们通过进一步在音译孟加拉语料库上进行预训练，建立了基于新型转换器编码器的基线。我们还提出了一种基于翻译的大型语言模型提示策略，用于处理音译文本。实验表明，我们的进一步预训练编码器在BanTH数据集上达到了最先进的性能，而在零样本设置中，我们的基于翻译的方法的提示效果优于其他策略。BanTH的引入不仅填补了孟加拉仇恨言论研究中的关键空白，而且为探索未来低代表性语言的混合代码和多标签分类挑战奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13281v4">PDF</a> Published in NAACL Findings 2025</p>
<p><strong>Summary</strong></p>
<p>随着数字空间中转译文本的普及，对非英语语言的仇恨言论检测和分类变得尤为重要，尤其是在资源较少的语言中。多标签仇恨内容分类有助于理解仇恨动机并增强内容管理。目前尚未有针对转译孟加拉语的多标签仇恨言论分类挑战的工作。我们引入了BanTH，首个多标签转译孟加拉语仇恨言论数据集，包含37.3k样本。样本来源于YouTube评论，每个实例都被标记为一个或多个目标群体，反映了区域人口统计特征。我们建立了基于转换器编码器的新基线，通过转译孟加拉语语料库进行进一步预训练。我们还提出了一种基于翻译的大型语言模型提示策略。实验表明，我们的进一步预训练编码器在BanTH数据集上实现了最新技术性能，而我们的基于翻译提示的策略在零样本设置中表现优于其他策略。BanTH的引入不仅填补了孟加拉语仇恨言论研究中的关键空白，而且为探索代码混合和多标签分类挑战在代表性不足的语言中奠定了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>转译文本的普及强调了对非英语语言的仇恨言论检测与分类的必要性。</li>
<li>多标签仇恨内容分类有助于理解仇恨动机并增强内容管理。</li>
<li>目前尚未有针对转译孟加拉语的多标签仇恨言论分类研究。</li>
<li>引入了首个多标签转译孟加拉语仇恨言论数据集BanTH，包含37.3k样本。</li>
<li>样本来源于YouTube评论，反映区域人口特征。</li>
<li>建立了基于转换器编码器的基线模型，通过进一步预训练提升性能。</li>
<li>提出了基于翻译的大型语言模型提示策略，并在零样本设置中表现出优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13281">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-40837887e729a3ea05e1d70312d6d21f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-187b74e9cfdf07f86cd75d54ce5da9b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc1efb82c38adc5abcd9ebfb1d73034b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-845ba45fbcfef19034dccbbbac3e98fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0848ba1a83c31a150ba938c91a5b61f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c797a70db9ce510a75839f27b122a82b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-704373d8c00b5a1921df79d2c11490b2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1e455a764a195af5b137de1400ae49a4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Text-To-Speech-Synthesis-In-The-Wild"><a href="#Text-To-Speech-Synthesis-In-The-Wild" class="headerlink" title="Text-To-Speech Synthesis In The Wild"></a>Text-To-Speech Synthesis In The Wild</h2><p><strong>Authors:Jee-weon Jung, Wangyou Zhang, Soumi Maiti, Yihan Wu, Xin Wang, Ji-Hoon Kim, Yuta Matsunaga, Seyun Um, Jinchuan Tian, Hye-jin Shim, Nicholas Evans, Joon Son Chung, Shinnosuke Takamichi, Shinji Watanabe</strong></p>
<p>Traditional Text-to-Speech (TTS) systems rely on studio-quality speech recorded in controlled settings.a Recently, an effort known as noisy-TTS training has emerged, aiming to utilize in-the-wild data. However, the lack of dedicated datasets has been a significant limitation. We introduce the TTS In the Wild (TITW) dataset, which is publicly available, created through a fully automated pipeline applied to the VoxCeleb1 dataset. It comprises two training sets: TITW-Hard, derived from the transcription, segmentation, and selection of raw VoxCeleb1 data, and TITW-Easy, which incorporates additional enhancement and data selection based on DNSMOS. State-of-the-art TTS models achieve over 3.0 UTMOS score with TITW-Easy, while TITW-Hard remains difficult showing UTMOS below 2.8. </p>
<blockquote>
<p>传统文本转语音（TTS）系统依赖于受控环境下的高质量语音录制。近期，一种被称为噪声TTS训练的研究方法应运而生，旨在利用野外数据。然而，缺乏专用数据集是一个重大限制。我们引入了野外文本转语音（TITW）数据集，该数据集是公开的，通过应用于VoxCeleb1数据集的完全自动化管道创建。它包含两个训练集：TITW-Hard，通过对原始VoxCeleb1数据进行转录、分段和选择而生成；TITW-Easy则基于DNSMOS进行额外的增强和数据选择。最新的TTS模型在TITW-Easy上取得了超过3.0的UTMOS分数，而TITW-Hard仍然具有挑战性，UTMOS低于2.8。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08711v2">PDF</a> 5 pages, Interspeech 2025</p>
<p><strong>摘要</strong></p>
<p>传统文本转语音（TTS）系统依赖于在受控环境中录制的语音数据。近期，一种名为噪声TTS训练的研究努力已经出现，旨在利用野外数据。然而，缺乏专用数据集一直是其重要限制。我们引入了野外文本转语音（TITW）数据集，该数据集已公开可用，通过对VoxCeleb1数据集应用全自动管道创建而成。它包含两个训练集：TITW-Hard，由VoxCeleb1原始数据的转录、分段和选择得出；TITW-Easy则基于DNSMOS进行了额外的增强和数据选择。使用TITW-Easy的最新TTS模型UTMOS得分超过3.0，而TITW-Hard仍然具有挑战性，UTMOS低于2.8。</p>
<p><strong>要点提炼</strong></p>
<ul>
<li>传统TTS系统依赖受控环境中的高质量语音数据。</li>
<li>噪声TTS训练旨在利用野外数据。</li>
<li>缺乏专用数据集是噪声TTS训练的一个重要限制。</li>
<li>引入了TITW数据集，该数据集通过全自动管道从VoxCeleb1创建而成。</li>
<li>TITW数据集包含两个训练集：TITW-Hard和TITW-Easy。</li>
<li>TITW-Easy基于DNSMOS进行了额外的增强和数据选择。</li>
<li>最新TTS模型在TITW-Easy上的UTMOS得分超过3.0，而TITW-Hard具有挑战性，UTMOS低于2.8。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08711">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2741ff5bd278376c19eae77be11eae51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90fc6648529f1291b5b5aad6e044da16.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3663f140ca164000b802e7fe6f9a230e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc61520be108ad9dce89cc2aeccc7209.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="BERP-A-Blind-Estimator-of-Room-Parameters-for-Single-Channel-Noisy-Speech-Signals"><a href="#BERP-A-Blind-Estimator-of-Room-Parameters-for-Single-Channel-Noisy-Speech-Signals" class="headerlink" title="BERP: A Blind Estimator of Room Parameters for Single-Channel Noisy   Speech Signals"></a>BERP: A Blind Estimator of Room Parameters for Single-Channel Noisy   Speech Signals</h2><p><strong>Authors:Lijun Wang, Yixian Lu, Ziyan Gao, Kai Li, Jianqiang Huang, Yuntao Kong, Shogo Okada</strong></p>
<p>Room acoustical parameters (RAPs), room geometrical parameters (RGPs) and instantaneous occupancy level are essential metrics for parameterizing the room acoustical characteristics (RACs) of a sound field around a listener’s local environment, offering comprehensive indications for various applications. Current blind estimation methods either fail to cover a broad range of real-world acoustic environments in the context of real background noise or estimate only a few RAPs and RGPs from noisy single-channel speech signals. In addition, they are limited in their ability to estimate the instantaneous occupancy level. In this paper, we propose a new universal blind estimation framework called the blind estimator of room parameters (BERP) to estimate RAPs, RGPs and occupancy level via a unified methodology. It consists of two modules: a unified room feature encoder that combines attention mechanisms with convolutional layers to learn common features across room parameters, and multiple separate parametric predictors for continuous estimation of each parameter in parallel. The combination of attention and convolutions enables the model to capture acoustic features locally and globally from speech, yielding more robust and multitask generalizable common features. Separate predictors allow the model to independently optimize for each room parameter to reduce task learning conflict and improve per-task performance. This estimation framework enables universal and efficient estimation of room parameters while maintaining satisfactory performance. To evaluate the effectiveness of the proposed framework, we compile a task-specific dataset from several publicly available datasets, including synthetic and real reverberant recordings. The results reveal that BERP achieves state-of-the-art (SOTA) performance and excellent adaptability to real-world scenarios. The code and weights are available on GitHub. </p>
<blockquote>
<p>房间声学参数（RAPs）、房间几何参数（RGPs）和瞬时占用水平是表征听众周围声音场的环境声学特性（RACs）的重要指标，为各种应用提供了全面的指示。当前的盲估计方法要么无法覆盖真实背景噪声环境中的广泛现实世界声学环境，要么只能从嘈杂的单通道语音信号中估计出一些RAPs和RGPs。此外，它们在估计瞬时占用水平方面的能力也有限。在本文中，我们提出了一种新的通用盲估计框架，称为房间参数盲估计器（BERP），通过统一的方法估计RAPs、RGPs和占用水平。它由两个模块组成：一个统一房间特征编码器，它将注意力机制与卷积层相结合，以学习房间参数之间的共同特征；以及多个单独的参数预测器，用于并行连续估计每个参数。注意力和卷积的结合使模型能够从语音中捕获局部和全局的声学特征，从而产生更稳健、多任务通用的共同特征。单独的预测器允许模型针对每个房间参数进行独立优化，以减少任务学习冲突并提高每项任务的性能。该估计框架能够在保持令人满意性能的同时，实现房间参数的通用和有效估计。为了评估所提出框架的有效性，我们从几个公开可用的数据集（包括合成和真实的混响录音）中编制了一个特定任务的数据集。结果表明，BERP达到了最先进的性能，并且对真实世界场景具有出色的适应性。代码和权重可在GitHub上获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.04476v7">PDF</a> 16-page with supplementary materials, Accepted to IEEE Transaction on   Audio Speech and Language Processing (TASLP 2025)</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为BERP（房间参数盲估计器）的新通用盲估计框架，用于估计房间的声学参数（RAPs）、几何参数（RGPs）和瞬时占用级别。该框架通过结合注意力机制和卷积层，学习房间参数的通用特征，并分别对每个参数进行并行连续估计。该框架能够实现房间参数的通用和高效估计，同时保持令人满意的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>房间声学参数（RAPs）和房间几何参数（RGPs）是描述声音场特性的关键指标。</li>
<li>当前盲估计方法无法覆盖广泛的实际声学环境或仅能从带噪声的单通道语音信号中估计有限的RAPs和RGPs。</li>
<li>BERP框架结合了注意力机制和卷积层，可以本地和全局捕捉语音的声学特征。</li>
<li>通过独立优化每个房间参数，BERP减少了任务学习冲突并提高了每个任务性能。</li>
<li>BERP框架在任务专用数据集上的性能达到最新水平，并具有良好的适应真实世界场景的能力。</li>
<li>BERP代码和权重已发布在GitHub上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.04476">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-938435dcfc492e0d00abd44a315bf530.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a798b74c8c6d84785eb51a785db92bb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be76a48d6ee8171c61b6b2a4adedb9aa.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Enabling-Differentially-Private-Federated-Learning-for-Speech-Recognition-Benchmarks-Adaptive-Optimizers-and-Gradient-Clipping"><a href="#Enabling-Differentially-Private-Federated-Learning-for-Speech-Recognition-Benchmarks-Adaptive-Optimizers-and-Gradient-Clipping" class="headerlink" title="Enabling Differentially Private Federated Learning for Speech   Recognition: Benchmarks, Adaptive Optimizers and Gradient Clipping"></a>Enabling Differentially Private Federated Learning for Speech   Recognition: Benchmarks, Adaptive Optimizers and Gradient Clipping</h2><p><strong>Authors:Martin Pelikan, Sheikh Shams Azam, Vitaly Feldman, Jan “Honza” Silovsky, Kunal Talwar, Christopher G. Brinton, Tatiana Likhomanenko</strong></p>
<p>While federated learning (FL) and differential privacy (DP) have been extensively studied, their application to automatic speech recognition (ASR) remains largely unexplored due to the challenges in training large transformer models. Specifically, large models further exacerbate issues in FL as they are particularly susceptible to gradient heterogeneity across layers, unlike the relatively uniform gradient behavior observed in shallow models. As a result, prior works struggle to converge with standard optimization techniques, even in the absence of DP mechanisms. To the best of our knowledge, no existing work establishes a competitive, practical recipe for FL with DP in the context of ASR. To address this gap, we establish \textbf{the first benchmark for FL with DP in end-to-end ASR}. Our approach centers on per-layer clipping and layer-wise gradient normalization: theoretical analysis reveals that these techniques together mitigate clipping bias and gradient heterogeneity across layers in deeper models. Consistent with these theoretical insights, our empirical results show that FL with DP is viable under strong privacy guarantees, provided a population of at least several million users. Specifically, we achieve user-level (7.2, $10^{-9}$)-DP (resp. (4.5, $10^{-9}$)-DP) with only a 1.3% (resp. 4.6%) absolute drop in word error rate when extrapolating to high (resp. low) population scales for FL with DP in ASR. Although our experiments focus on ASR, the underlying principles we uncover - particularly those concerning gradient heterogeneity and layer-wise gradient normalization - offer broader guidance for designing scalable, privacy-preserving FL algorithms for large models across domains. </p>
<blockquote>
<p>尽管联邦学习（FL）和差分隐私（DP）已经得到了广泛的研究，但它们在实际语音识别（ASR）中的应用仍然在很大程度上未被探索，这是由于训练大型变压器模型带来的挑战。具体来说，大型模型会进一步加剧联邦学习的问题，因为它们特别容易受到各层梯度异质性的影响，这与浅层模型中观察到的相对统一的梯度行为形成对比。因此，先前的工作即使在没有任何DP机制的情况下，也很难用标准优化技术实现收敛。据我们所知，目前没有任何工作建立了在ASR背景下具有DP的FL的竞争性实用方案。为了弥补这一空白，我们为具有DP的FL在端到端的ASR中建立了<strong>第一个基准测试</strong>。我们的方法以每层剪裁和逐层梯度归一化为中心：理论分析表明，这些技术相结合减轻了深层模型中剪裁偏见和各层梯度的异质性。与这些理论见解一致，我们的实证结果表明，在强大的隐私保证下，具有DP的FL是可行的，前提是有至少数百万用户。具体来说，我们在用户层面实现了（7.2，10^-9）-DP（分别在高低用户群体规模外推时实现（4.5，10^-9）-DP），词汇错误率仅绝对下降1.3%（或4.6%）。尽管我们的实验专注于ASR，但我们所揭示的基本原理，特别是关于梯度异质性和逐层梯度归一化的原理，为设计跨领域的大型模型的隐私保护联邦学习算法提供了更广泛的指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00098v2">PDF</a> Under review</p>
<p><strong>摘要</strong></p>
<p>本文探索了联邦学习与差分隐私在自动语音识别中的应用，针对大型模型在联邦学习中的梯度异质性挑战，提出了基于每层裁剪和层间梯度归一化的方法。建立了一个具有差分隐私的联邦学习在端到端自动语音识别中的首个基准测试，并通过理论分析和实证研究验证了该方法的可行性。在强大的隐私保障下，该方法在至少数百万用户的情况下是可行的。在保护用户隐私的同时，对自动语音识别任务的影响较小。此外，文中揭示的梯度异质性和层间梯度归一化等原理为设计跨领域的大规模、隐私保护的联邦学习算法提供了更广泛的指导。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>联邦学习与差分隐私在自动语音识别中的应用仍存在差距，尤其是在训练大型模型方面的挑战。</li>
<li>大型模型在联邦学习中面临梯度异质性的问题，这与浅层模型中相对统一的梯度行为不同。</li>
<li>现有工作难以在标准优化技术下收敛，甚至在不存在差分隐私机制的情况下。</li>
<li>本文建立了首个在端到端自动语音识别中具有差分隐私的联邦学习基准测试。</li>
<li>通过每层裁剪和层间梯度归一化的方法，缓解了裁剪偏差和深层模型中的梯度异质性。</li>
<li>在强大的隐私保障下，该方法的可行性已经得到实证研究验证，并且在至少数百万用户的情况下，对用户级别的隐私保护表现良好。</li>
<li>本文揭示的原理，如梯度异质性和层间梯度归一化等，为设计跨领域的大规模、隐私保护的联邦学习算法提供了指导。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.00098">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c8b0657c14e426d3f633daeab0890a8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e63189a006d43140e98f8e456f9ed9c3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f16759ce1920af57a496df9762396cc3.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-06-04  Beyond Face Swapping A Diffusion-Based Digital Human Benchmark for   Multimodal Deepfake Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e03de1558239401138166a510a0d9825.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-06-04  MFCLIP Multi-modal Fine-grained CLIP for Generalizable Diffusion Face   Forgery Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23901.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
