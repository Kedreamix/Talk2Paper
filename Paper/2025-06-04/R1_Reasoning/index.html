<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-06-04  AReaL A Large-Scale Asynchronous Reinforcement Learning System for   Language Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-d64a7b737dd7625e2e3e9188128d59b1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    85 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-04-更新"><a href="#2025-06-04-更新" class="headerlink" title="2025-06-04 更新"></a>2025-06-04 更新</h1><h2 id="AReaL-A-Large-Scale-Asynchronous-Reinforcement-Learning-System-for-Language-Reasoning"><a href="#AReaL-A-Large-Scale-Asynchronous-Reinforcement-Learning-System-for-Language-Reasoning" class="headerlink" title="AReaL: A Large-Scale Asynchronous Reinforcement Learning System for   Language Reasoning"></a>AReaL: A Large-Scale Asynchronous Reinforcement Learning System for   Language Reasoning</h2><p><strong>Authors:Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, Yi Wu</strong></p>
<p>Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a \emph{fully asynchronous} RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves \textbf{up to 2.57$\times$ training speedup} compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at <a target="_blank" rel="noopener" href="https://github.com/inclusionAI/AReaL/">https://github.com/inclusionAI/AReaL/</a>. </p>
<blockquote>
<p>强化学习（RL）已成为训练大型语言模型（LLM）的流行趋势，特别是在推理任务中。对于LLM的有效RL需要大规模并行化，并迫切需要对高效的训练系统提出需求。大多数现有的用于LLM的大规模RL系统是同步的，通过在批量设置中交替生成和培训来稳定RL训练。在每个训练批次中生成的滚动数据由同一（或最新）模型生成。这虽然稳定了RL训练，但导致了系统层面的严重低效。生成必须等待批次中最长的输出完成才能进行模型更新，导致GPU利用率不足。我们提出了AReaL，一个完全异步的RL系统，它将生成与训练完全解耦。AReaL中的滚动工作器可以连续生成新的输出而无需等待，而训练工作器则会在每次收集到一批数据时更新模型。AReaL还采用了一系列系统层面的优化，大大提高了GPU的利用率。为了稳定RL训练，AReaL平衡了滚动和训练工作器的负载，以控制数据的陈旧程度，并采用了增强陈旧性的PPO变体来更好地处理过时的训练样本。在数学和代码推理基准测试上的大量实验表明，与使用相同数量GPU的最佳同步系统相比，AReaL实现了高达2.57倍的训练加速，并实现了相匹配甚至更好的最终性能。AReaL的代码可在<a target="_blank" rel="noopener" href="https://github.com/inclusionAI/AReaL/%E3%80%82">https://github.com/inclusionAI/AReaL/获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24298v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模语言模型（LLM）的强化学习（RL）训练需要高效的系统支持。现有同步RL系统虽然稳定，但存在系统级效率低下问题。为此，提出一种全新的异步RL系统AReaL，将生成与训练过程完全解耦，提高GPU利用率。通过工作负载平衡和PPO算法改进，AReaL在数学和代码推理基准测试中实现了高效训练加速，且性能与最佳同步系统相比有所提高。代码已开源。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>强化学习在大规模语言模型训练中越来越受欢迎，特别是在推理任务中。</li>
<li>现有同步强化学习系统在训练语言模型时存在系统级效率低下问题。</li>
<li>AReaL系统通过完全异步的强化学习方式解决此问题，生成与训练过程解耦，提高GPU利用率。</li>
<li>AReaL系统采用一系列系统级优化措施，包括工作负载平衡和PPO算法的改进。</li>
<li>在数学和代码推理测试中，AReaL系统相比最佳同步系统实现了最高达2.57倍的训练加速。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24298">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-579aa7e9768d0d5ce5d51f2796d64cb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3800501961f7e4f527105ff0f88237c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4aa208b05258fb160835e564f4d9b17f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-14c152cab10b8255feac839aa31c38aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28a5f4330eb91a7f145c578aaa39d633.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CodeV-R1-Reasoning-Enhanced-Verilog-Generation"><a href="#CodeV-R1-Reasoning-Enhanced-Verilog-Generation" class="headerlink" title="CodeV-R1: Reasoning-Enhanced Verilog Generation"></a>CodeV-R1: Reasoning-Enhanced Verilog Generation</h2><p><strong>Authors:Yaoyu Zhu, Di Huang, Hanqi Lyu, Xiaoyun Zhang, Chongxiao Li, Wenxuan Shi, Yutong Wu, Jianan Mu, Jinghua Wang, Yang Zhao, Pengwei Jin, Shuyao Cheng, Shengwen Liang, Xishan Zhang, Rui Zhang, Zidong Du, Qi Guo, Xing Hu, Yunji Chen</strong></p>
<p>Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage “distill-then-RL” training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities. </p>
<blockquote>
<p>通过结合强化学习与可验证奖励（RLVR）训练的大型语言模型（LLM）在具有明确、可自动化的验证任务上取得了突破，例如软件编程和数学问题。然而，将RLVR扩展到电子设计自动化（EDA），特别是从自然语言（NL）规范自动生成硬件描述语言（HDL）如Verilog，面临三大挑战：缺乏自动化和准确的验证环境、高质量NL-代码对稀缺，以及RLVR的计算成本高昂。为此，我们引入了CodeV-R1，这是一个用于训练Verilog生成LLM的RLVR框架。首先，我们开发了一种基于规则的测试台生成器，它可以通过对照金参考进行稳健的等价性检查。其次，我们提出了一种往返数据合成方法，将开源Verilog片段与LLM生成的自然语言描述配对，通过生成的测试台验证代码-NL-代码的的一致性，并过滤出不等价的例子，从而生成高质量的数据集。第三，我们采用了两阶段“蒸馏然后强化学习”的训练流程：先进行推理能力的冷启动蒸馏，然后采用我们新型的自适应DAPO RLVR算法，通过自适应调整采样率来降低训练成本。最终模型CodeV-R1-7B在VerilogEval v2和RTLLM v1.1上的pass@1分别达到了68.6%和72.9%，较之前的最先进水平提高了12~20%，同时匹配甚至超越了671B DeepSeek-R1的性能。我们将发布我们的模型、训练流程和数据集，以促进EDA和LLM社区的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24183v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>采用强化学习与可验证奖励（RLVR）训练的大型语言模型（LLM）在具有明确、可自动化验证的任务上取得了突破，如软件编程和数学问题。然而，将其扩展到电子设计自动化（EDA）领域，尤其是从自然语言（NL）规范自动生成硬件描述语言（HDL）如Verilog时，面临缺乏自动化和准确的验证环境、高质量NL-代码对稀缺以及RLVR计算成本高昂等三大挑战。为此，我们引入了CodeV-R1，这是一个用于训练Verilog生成LLM的RLVR框架。我们开发了一个基于规则的测试平台生成器，它能够对黄金参考进行稳健的等价性检查；提出了一种往返数据合成方法，将开源Verilog片段与LLM生成的NL描述配对，通过生成的测试平台进行代码-NL-代码的一致性验证，并过滤出不相当的例子以形成高质量数据集；采用两阶段“蒸馏后RL”训练管道：先进行蒸馏以启动推理能力，然后采用我们新型RLVR算法自适应DAPO，通过自适应调整采样率来降低训练成本。结果模型CodeV-R1-7B在VerilogEval v2和RTLLVM v1.1上的pass@1分别达到了68.6%和72.9%，较之前的最优状态提高了12~20%，同时匹配甚至超越了671B DeepSeek-R1的性能。我们将发布我们的模型、训练管道和数据集，以促进EDA和LLM社区的研究。</p>
<p><strong>要点提炼</strong></p>
<ol>
<li>大型语言模型（LLMs）通过强化学习与可验证奖励（RLVR）训练在特定任务上取得突破。</li>
<li>在电子设计自动化（EDA）领域，尤其是Verilog自动生成面临三大挑战。</li>
<li>引入CodeV-R1框架来解决这些挑战，包括测试平台生成器、往返数据合成方法和两阶段训练管道。</li>
<li>CodeV-R1-7B模型在Verilog评价上表现优异，超过现有技术12~20%。</li>
<li>模型、训练管道和数据集将公开发布，以促进EDA和LLM领域的研究。</li>
<li>RLVR的新型算法自适应DAPO在降低训练成本方面表现出色。</li>
<li>模型在匹配或超越671B DeepSeek-R1性能的同时实现了较高的准确率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24183">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1cc4c09c1fefd21a414fe0df953c98e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d106afc022ea2f5fa9ac33b509963af1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74da7c0b73414cc15dc46acd7d45a94d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Mixed-R1-Unified-Reward-Perspective-For-Reasoning-Capability-in-Multimodal-Large-Language-Models"><a href="#Mixed-R1-Unified-Reward-Perspective-For-Reasoning-Capability-in-Multimodal-Large-Language-Models" class="headerlink" title="Mixed-R1: Unified Reward Perspective For Reasoning Capability in   Multimodal Large Language Models"></a>Mixed-R1: Unified Reward Perspective For Reasoning Capability in   Multimodal Large Language Models</h2><p><strong>Authors:Shilin Xu, Yanwei Li, Rui Yang, Tao Zhang, Yueyi Sun, Wei Chow, Linfeng Li, Hang Song, Qi Xu, Yunhai Tong, Xiangtai Li, Hao Fei</strong></p>
<p>Recent works on large language models (LLMs) have successfully demonstrated the emergence of reasoning capabilities via reinforcement learning (RL). Although recent efforts leverage group relative policy optimization (GRPO) for MLLMs post-training, they constantly explore one specific aspect, such as grounding tasks, math problems, or chart analysis. There are no works that can leverage multi-source MLLM tasks for stable reinforcement learning. In this work, we present a unified perspective to solve this problem. We present Mixed-R1, a unified yet straightforward framework that contains a mixed reward function design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K). We first design a data engine to select high-quality examples to build the Mixed-45K post-training dataset. Then, we present a Mixed-Reward design, which contains various reward functions for various MLLM tasks. In particular, it has four different reward functions: matching reward for binary answer or multiple-choice problems, chart reward for chart-aware datasets, IoU reward for grounding problems, and open-ended reward for long-form text responses such as caption datasets. To handle the various long-form text content, we propose a new open-ended reward named Bidirectional Max-Average Similarity (BMAS) by leveraging tokenizer embedding matching between the generated response and the ground truth. Extensive experiments show the effectiveness of our proposed method on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes. Our dataset and model are available at <a target="_blank" rel="noopener" href="https://github.com/xushilin1/mixed-r1">https://github.com/xushilin1/mixed-r1</a>. </p>
<blockquote>
<p>关于大型语言模型（LLMs）的最新研究已经成功地证明了通过强化学习（RL）涌现出的推理能力。尽管最近的努力利用群体相对策略优化（GRPO）对MLLMs进行后训练，但它们不断地探索一个特定的方面，如接地任务、数学问题或图表分析。还没有工作能够利用多源MLLM任务来进行稳定的强化学习。在这项工作中，我们从一个统一的角度来解决这个问题。我们提出了Mixed-R1，这是一个统一而简单的框架，它包含一个混合奖励函数设计（Mixed-Reward）和一个混合后训练数据集（Mixed-45K）。我们首先设计一个数据引擎来选择高质量的样例来构建Mixed-45K后训练数据集。然后，我们提出了Mixed-Reward设计，它包含各种LLMM任务的奖励函数。特别是，它有四种不同的奖励功能：针对二进制答案或多选问题的匹配奖励、针对图表感知数据集的图表奖励、针对接地问题的IoU奖励，以及针对长文本响应的开放奖励，如标题数据集。为了处理各种长文本内容，我们提出了一种新的开放奖励，称为双向最大平均相似性（BMAS），它通过生成响应和真实标签之间的分词器嵌入匹配来实现。大量实验证明，我们的方法在各种MLLMs上的有效性，包括Qwen2.5-VL和Intern-VL在各种规模上的有效性。我们的数据集和模型可在<a target="_blank" rel="noopener" href="https://github.com/xushilin1/mixed-r1%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/xushilin1/mixed-r1上获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24164v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一个统一的框架Mixed-R1，用于解决大型语言模型（LLM）在强化学习中的稳定性问题。该框架包含混合奖励函数设计（Mixed-Reward）和混合后训练数据集（Mixed-45K）。通过数据引擎筛选高质量样本构建Mixed-45K数据集，并为不同的LLM任务设计不同的奖励函数。实验证明，该方法在多种MLLMs上均有效。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型（LLMs）通过强化学习展现出推理能力。</li>
<li>当前工作主要集中在特定方面的任务，如接地任务、数学问题或图表分析等。</li>
<li>尚无工作能够利用多源MLLM任务进行稳定的强化学习。</li>
<li>提出Mixed-R1框架，包含混合奖励函数和混合后训练数据集。</li>
<li>数据引擎用于筛选高质量样本构建Mixed-45K数据集。</li>
<li>为不同的MLLM任务设计了不同的奖励函数，包括匹配奖励、图表奖励、IoU奖励和开放奖励。</li>
<li>提出了一种新的开放奖励函数——双向最大平均相似度（BMAS），用于处理长文本内容。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24164">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6f4dfc90749855191d25bf1c06229f8d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d0533fb37156d26421deda955930be4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac3ff81a77e01e629acd89a75fdd4123.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f1a6826d5e788f7479fd23d3059879e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DSR-Bench-Evaluating-the-Structural-Reasoning-Abilities-of-LLMs-via-Data-Structures"><a href="#DSR-Bench-Evaluating-the-Structural-Reasoning-Abilities-of-LLMs-via-Data-Structures" class="headerlink" title="DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via   Data Structures"></a>DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via   Data Structures</h2><p><strong>Authors:Yu He, Yingxi Li, Colin White, Ellen Vitercik</strong></p>
<p>Large language models (LLMs) are increasingly deployed for real-world tasks that fundamentally involve data manipulation. A core requirement across these tasks is the ability to perform structural reasoning–that is, to understand and reason about data relationships. For example, customer requests require a temporal ordering, which can be represented by data structures such as queues. However, existing benchmarks primarily focus on high-level, application-driven evaluations without isolating this fundamental capability. To address this gap, we introduce DSR-Bench, a novel benchmark evaluating LLMs’ structural reasoning capabilities through data structures, which provide interpretable representations of data relationships. DSR-Bench includes 20 data structures, 35 operations, and 4,140 problem instances, organized hierarchically for fine-grained analysis of reasoning limitations. Our evaluation pipeline is fully automated and deterministic, eliminating subjective human or model-based judgments. Its synthetic nature also ensures scalability and minimizes data contamination risks. We benchmark nine state-of-the-art LLMs. Our analysis shows that instruction-tuned models struggle with basic multi-attribute and multi-hop reasoning. Furthermore, while reasoning-oriented models perform better, they remain fragile on complex and hybrid structures, with the best model achieving an average score of only 47% on the challenge subset. Crucially, models often perform poorly on multi-dimensional data and natural language task descriptions, highlighting a critical gap for real-world deployment. </p>
<blockquote>
<p>大型语言模型（LLM）越来越多地被用于涉及数据操作的真实世界任务。这些任务的核心要求是进行结构推理的能力，即理解和推理数据关系。例如，客户请求需要时序排序，可以通过队列等数据结构来表示。然而，现有的基准测试主要关注高级、应用驱动的评价，而没有孤立这种基本能力。为了解决这一差距，我们引入了DSR-Bench，这是一个新的基准测试，通过数据结构评估LLM的结构推理能力，提供数据关系的可解释表示。DSR-Bench包括20个数据结构、35个操作和4140个问题实例，按层次结构组织，用于精细分析推理局限性。我们的评估管道是完全自动化和确定的，消除了主观人为或模型基础的判断。其合成性质还确保了可扩展性并降低了数据污染风险。我们对九个最新的大型语言模型进行了基准测试。我们的分析表明，指令调整模型在基本的多属性和多跳推理方面表现挣扎。此外，虽然以推理为导向的模型表现更好，但在复杂和混合结构上仍然脆弱，最佳模型在挑战子集上的平均得分仅为47%。关键的是，模型在多维数据和自然语言任务描述方面的表现往往不佳，这突出了在真实世界部署中的关键差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24069v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了大型语言模型（LLMs）在处理涉及数据操作的真实任务时的核心需求，即进行结构性推理的能力。针对这一需求，提出了一种新型评估基准——DSR-Bench，用于评估LLMs处理数据结构的能力，该基准通过数据结构提供了数据关系的可解释表示。DSR-Bench包含20个数据结构、35个操作和4140个问题实例，并进行了层次结构组织，以精细分析推理的局限性。评估流程完全自动化且确定性高，消除了主观人为或模型判断的干扰。通过对比九种最先进的LLMs表现，发现指令微调模型在基本多属性和多跳推理方面遇到困难，而面向推理的模型虽表现较好，但在复杂和混合结构上仍显脆弱，最佳模型挑战子集的平均得分仅为47%。模型在多维度数据和自然语言任务描述方面的表现不佳，凸显了实际部署中的关键差距。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在处理真实任务时需要具备结构性推理能力，这涉及理解并推理数据关系。</li>
<li>现有评估基准主要关注高级、应用驱动的评价，未针对结构性推理能力进行专门评估。</li>
<li>提出了一种新型评估基准DSR-Bench，旨在评估LLMs处理数据结构的能力。</li>
<li>DSR-Bench包含丰富的问题实例，能够精细分析推理的局限性。</li>
<li>评估流程自动化且确定性强，消除主观判断，确保评估公正性。</li>
<li>九种最先进的LLMs在结构性推理方面存在差距，表现在基本多属性和多跳推理方面遇到困难。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24069">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b6ce9e802d3963f5fbbbdc24bef4b3d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10cb66a901c2d0edfece7d67c1381383.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28d1ca5d38791ee0c0f2114a60f192f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5450150ec019815032c89e6679faca0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LlamaRL-A-Distributed-Asynchronous-Reinforcement-Learning-Framework-for-Efficient-Large-scale-LLM-Training"><a href="#LlamaRL-A-Distributed-Asynchronous-Reinforcement-Learning-Framework-for-Efficient-Large-scale-LLM-Training" class="headerlink" title="LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for   Efficient Large-scale LLM Training"></a>LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for   Efficient Large-scale LLM Training</h2><p><strong>Authors:Bo Wu, Sid Wang, Yunhao Tang, Jia Ding, Eryk Helenowski, Liang Tan, Tengyu Xu, Tushar Gowda, Zhengxing Chen, Chen Zhu, Xiaocheng Tang, Yundi Qian, Beibei Zhu, Rui Hou</strong></p>
<p>Reinforcement Learning (RL) has become the most effective post-training approach for improving the capabilities of Large Language Models (LLMs). In practice, because of the high demands on latency and memory, it is particularly challenging to develop an efficient RL framework that reliably manages policy models with hundreds to thousands of billions of parameters.   In this paper, we present LlamaRL, a fully distributed, asynchronous RL framework optimized for efficient training of large-scale LLMs with various model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a handful to thousands of devices. LlamaRL introduces a streamlined, single-controller architecture built entirely on native PyTorch, enabling modularity, ease of use, and seamless scalability to thousands of GPUs. We also provide a theoretical analysis of LlamaRL’s efficiency, including a formal proof that its asynchronous design leads to strict RL speed-up. Empirically during the Llama 3 post-training, by leveraging best practices such as colocated model offloading, asynchronous off-policy training, and distributed direct memory access for weight synchronization, LlamaRL achieves significant efficiency gains – up to 10.7x speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy model. Furthermore, the efficiency advantage continues to grow with increasing model scale, demonstrating the framework’s suitability for future large-scale RL training. </p>
<blockquote>
<p>强化学习（RL）已成为提高大型语言模型（LLM）能力最有效的后训练方法。然而，由于其对延迟和内存的极高要求，开发一个能够可靠管理拥有数万亿参数的策略模型的高效RL框架是一项特别具有挑战性的任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24034v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了LlamaRL，一个针对大规模语言模型（LLM）的高效强化学习（RL）框架。LlamaRL采用全分布式、异步设计，支持从少数到数千个设备的GPU集群训练。其优化LLM的实战训练效果显著提升，尤其在参数规模更大的模型上优势明显。LlamaRL利用PyTorch原生特性构建单一控制器架构，实现模块化、易用性和无缝扩展到数千个GPU。通过理论分析和实证验证，证明了其异步设计能显著提高RL速度。在Llama 3的后训练中，LlamaRL通过最佳实践实现了显著效率提升，相较于DeepSpeed-Chat类系统，对405B参数模型的速度提升达10.7倍，且随着模型规模的增加，效率优势持续增长。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LlamaRL是一个用于训练大规模语言模型（LLM）的强化学习（RL）框架，支持全分布式和异步训练。</li>
<li>LlamaRL具有优化的架构，能在GPU集群上实现高效训练，支持从少数到数千个设备的扩展。</li>
<li>LlamaRL利用PyTorch原生特性构建单一控制器，实现模块化、易用性和无缝扩展到数千GPU。</li>
<li>LlamaRL通过理论分析和实证验证证明了其异步设计能显著提高强化学习速度。</li>
<li>在Llama 3的后训练中，LlamaRL通过一系列最佳实践实现了显著效率提升。</li>
<li>LlamaRL相较于其他系统，对405B参数模型的速度提升达10.7倍。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24034">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-03f3389b50b5aa8b3673d7d9ed8afb61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-585cb18128e097a0f59cfd95a738acd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f9e0e263eec426aa675bf53c0f0f8b2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DINO-R1-Incentivizing-Reasoning-Capability-in-Vision-Foundation-Models"><a href="#DINO-R1-Incentivizing-Reasoning-Capability-in-Vision-Foundation-Models" class="headerlink" title="DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models"></a>DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models</h2><p><strong>Authors:Chenbin Pan, Wenbin He, Zhengzhong Tu, Liu Ren</strong></p>
<p>The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representation models like the DINO series. In this work, we propose \textbf{DINO-R1}, the first such attempt to incentivize visual in-context reasoning capabilities of vision foundation models using reinforcement learning. Specifically, DINO-R1 introduces \textbf{Group Relative Query Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly designed for query-based representation models, which computes query-level rewards based on group-normalized alignment quality. We also apply KL-regularization to stabilize the objectness distribution to reduce the training instability. This joint optimization enables dense and expressive supervision across queries while mitigating overfitting and distributional drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models that integrate a visual prompt encoder and a visual-guided query selection mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving strong generalization in both open-vocabulary and closed-set visual prompting scenarios. </p>
<blockquote>
<p>最近对大型语言模型（如DeepSeek-R1）的推理能力的研究兴趣激增，通过基于强化学习的微调框架（如群体相对策略优化（GRPO）方法）取得了显著的成功。然而，这种推理能力在视觉基础模型（包括DINO系列等表示模型）中仍然被探索得不够深入，甚至明显缺失。在这项工作中，我们提出了使用强化学习激励视觉基础模型的视觉上下文推理能力的首个尝试——<strong>DINO-R1</strong>。具体来说，DINO-R1引入了<strong>群体相对查询优化（GRQO）</strong>，这是一种针对基于查询的表示模型的新型强化训练策略，它根据群体归一化的对齐质量计算查询级别的奖励。我们还应用KL正则化来稳定对象性分布，以减少训练的不稳定性。这种联合优化可以在查询之间实现密集且富有表现力的监督，同时减轻过拟合和分布漂移。在基于Grounding-DINO的基础上，我们训练了一系列DINO-R1家族模型，这些模型集成了视觉提示编码器和视觉引导查询选择机制。在COCO、LVIS和ODinW上的大量实验表明，DINO-R1显著优于监督微调基线，在开放词汇表和封闭集视觉提示场景中实现了强大的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24025v1">PDF</a> </p>
<p><strong>Summary</strong>：大型语言模型如DeepSeek-R1的推理能力得到了广泛研究，取得了显著成效。然而，这一成功尚未应用于视觉基础模型（如DINO系列模型）。在这篇文章中，提出了使用强化学习的激励方法（DINO-R1），以增强视觉基础模型的视觉上下文推理能力。具体来说，DINO-R1引入了基于强化学习的训练策略Group Relative Query Optimization (GRQO)，适用于查询型表示模型。同时应用KL正则化稳定对象分布，减少训练的不稳定性。通过一系列实验证明，DINO-R1在COCO、LVIS和ODinW等数据集上的表现优于监督微调基线模型，在开放词汇表和封闭集视觉提示场景中实现了强大的泛化能力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型的推理能力通过强化学习得到了显著的提升。</li>
<li>目前视觉基础模型（如DINO系列）缺乏类似的推理能力研究。</li>
<li>DINO-R1模型首次尝试使用强化学习激励视觉基础模型的视觉上下文推理能力。</li>
<li>DINO-R1引入了Group Relative Query Optimization (GRQO)这一新型强化学习训练策略，适用于查询型表示模型。</li>
<li>通过KL正则化来稳定对象分布，以减少训练过程中的不稳定性。</li>
<li>DINO-R1模型在多个数据集上的表现优于监督微调基线模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24025">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1333ec1844d9d1050f5ed21d1d7b0e81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7c48eae20ab063a0e892b40e9d38aa8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca21d7ba3a7840ca70b75ff9fd8d2083.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2837543202367367086f583c64e665c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Measuring-Sycophancy-of-Language-Models-in-Multi-turn-Dialogues"><a href="#Measuring-Sycophancy-of-Language-Models-in-Multi-turn-Dialogues" class="headerlink" title="Measuring Sycophancy of Language Models in Multi-turn Dialogues"></a>Measuring Sycophancy of Language Models in Multi-turn Dialogues</h2><p><strong>Authors:Jiseung Hong, Grace Byun, Seungone Kim, Kai Shu</strong></p>
<p>Large Language Models (LLMs) are expected to provide helpful and harmless responses, yet they often exhibit sycophancy–conforming to user beliefs regardless of factual accuracy or ethical soundness. Prior research on sycophancy has primarily focused on single-turn factual correctness, overlooking the dynamics of real-world interactions. In this work, we introduce SYCON Bench, a novel benchmark for evaluating sycophantic behavior in multi-turn, free-form conversational settings. Our benchmark measures how quickly a model conforms to the user (Turn of Flip) and how frequently it shifts its stance under sustained user pressure (Number of Flip). Applying SYCON Bench to 17 LLMs across three real-world scenarios, we find that sycophancy remains a prevalent failure mode. Our analysis shows that alignment tuning amplifies sycophantic behavior, whereas model scaling and reasoning optimization strengthen the model’s ability to resist undesirable user views. Reasoning models generally outperform instruction-tuned models but often fail when they over-index on logical exposition instead of directly addressing the user’s underlying beliefs. Finally, we evaluate four additional prompting strategies and demonstrate that adopting a third-person perspective reduces sycophancy by up to 63.8% in debate scenario. We release our code and data at <a target="_blank" rel="noopener" href="https://github.com/JiseungHong/SYCON-Bench">https://github.com/JiseungHong/SYCON-Bench</a>. </p>
<blockquote>
<p>大规模语言模型（LLMs）本应在问答系统中给出有益且无伤害的反应，然而它们通常表现出一种媚俗行为，即无论事实准确性或道德合理性如何，都会迎合用户的信念。之前关于媚俗的研究主要集中在单轮事实正确性上，忽视了现实互动中的动态变化。在这项研究中，我们引入了SYCON Bench，这是一个用于评估多轮自由形式对话设置中媚俗行为的新型基准测试。我们的基准测试衡量的是模型迎合用户的速度（Flip Turn）以及在持续的用户压力下改变立场的频率（Flip Number）。通过对三种现实场景中的17个大型语言模型应用SYCON Bench基准测试，我们发现媚俗行为仍然是一种普遍存在的失败模式。我们的分析表明，对齐调整放大了媚俗行为，而模型规模和推理优化则增强了模型抵抗不良用户观点的能力。推理模型通常表现优于指令训练模型，但在过于强调逻辑阐述而非直接应对用户基本信念时往往会失败。最后，我们评估了四种额外的提示策略，并证明采用第三人称视角可以减少辩论场景中的媚俗行为高达63.8%。我们在<a target="_blank" rel="noopener" href="https://github.com/JiseungHong/SYCON-Bench">https://github.com/JiseungHong/SYCON-Bench</a>公开了我们的代码和数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23840v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在对话中常表现出顺应力（sycophancy），即无论事实准确性或道德合理性如何，都会迎合用户的信念。过去的研究主要关注单轮对话中的事实正确性，忽略了现实互动中的动态变化。本研究介绍了一种新型评估工具SYCON Bench，用于评估多轮自由形式对话中的顺应力行为。该工具衡量了模型迎合用户的速度和频率。对17个LLMs的三大现实场景应用SYCON Bench发现，顺应力仍然是一种普遍存在的失败模式。分析显示，调整对齐放大了顺应力行为，而模型规模和推理优化增强了抵制不良用户观点的能力。推理模型通常表现优于指令调优模型，但在过度重视逻辑展示而非直接解决用户基础信念时往往会失败。最后，评估了四种额外的提示策略，并证明采用第三人称视角可以减少辩论场景中的顺应力达63.8%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在对话中常表现出顺应力（sycophancy），迎合用户信念。</li>
<li>SYCON Bench是一种新型评估工具，用于衡量LLMs在多轮对话中的顺应力行为。</li>
<li>顺应力是LLMs的一种普遍失败模式，与模型的对齐调整、规模及推理优化有关。</li>
<li>推理模型在解决用户基础信念时表现复杂，过度重视逻辑展示可能导致失败。</li>
<li>采用第三人称视角的提示策略能有效减少辩论场景中的顺应力。</li>
<li>SYCON Bench代码和数据已公开发布，供研究使用。</li>
<li>LLMs需要进一步提高在对话中的自主性，以更好地适应现实世界的复杂互动。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23840">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-25e9f59545360a71e8e0c7f46df22ff6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d31d26419e74a6e682a3e8522d7955e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7e7cfcd0a0ec687ff5a0d72db2ad48a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa5e95e0d4c3dba72ac8d59fb38fbf27.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-efdb253e837ee4f45da0d1bc3b402e30.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Jigsaw-R1-A-Study-of-Rule-based-Visual-Reinforcement-Learning-with-Jigsaw-Puzzles"><a href="#Jigsaw-R1-A-Study-of-Rule-based-Visual-Reinforcement-Learning-with-Jigsaw-Puzzles" class="headerlink" title="Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with   Jigsaw Puzzles"></a>Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with   Jigsaw Puzzles</h2><p><strong>Authors:Zifu Wang, Junyi Zhu, Bo Tang, Zhiyu Li, Feiyu Xiong, Jiaqian Yu, Matthew B. Blaschko</strong></p>
<p>The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL, using jigsaw puzzles as a structured experimental framework. Jigsaw puzzles offer inherent ground truth, adjustable difficulty, and demand complex decision-making, making them ideal for this study. Our research reveals several key findings: \textit{Firstly,} we find that MLLMs, initially performing near to random guessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. \textit{Secondly,} training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. \textit{Thirdly,} MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. \textit{Fourthly,} we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. \textit{Finally,} our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/zifuwanggg/Jigsaw-R1">https://github.com/zifuwanggg/Jigsaw-R1</a>. </p>
<blockquote>
<p>将基于规则的强化学习（RL）应用于多模态大型语言模型（MLLMs）为文本领域以外的领域带来了独特的挑战和可能的偏差，尤其是对于感知密集的任务。本文全面研究了基于规则的视觉强化学习，以拼图作为结构化实验框架。拼图提供了固有的真实情况、可调整的难度和复杂的决策需求，使其成为这项研究的理想选择。我们的研究发现了几个关键观点：首先，我们发现MLLM在最简单的拼图上开始时近乎随机猜测，但通过微调达到近乎完美的准确度并推广到复杂的未见过的配置。其次，在拼图上训练可以推广到其他视觉任务，其有效性取决于特定的任务配置。第三，MLLM可以在有或没有显式推理的情况下学习和推广，尽管开源模型通常倾向于直接回答。因此，即使经过逐步推理的训练，它们也可能会忽略得出最终答案的思考过程。第四，我们观察到复杂的推理模式似乎是预先存在的而不是突发的，其频率随训练和任务难度的增加而增加。最后，我们的结果表明，强化学习比监督微调（SFT）表现出更有效的泛化能力，而初始的SFT冷启动阶段可能会阻碍后续的RL优化。尽管这些观察是基于拼图游戏的，并可能在其他视觉任务中有所不同，但此研究为集体理解基于规则的视觉强化学习及其在多模态学习中的潜力这一更大的谜题贡献了一块有价值的拼图。代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/zifuwanggg/Jigsaw-R1">https://github.com/zifuwanggg/Jigsaw-R1</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23590v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>基于规则强化学习（RL）在多模态大型语言模型（MLLMs）中的应用，对于感知密集型任务，面临着独特的挑战和潜在的偏差。本文全面研究了基于规则的视觉RL，以拼图作为结构化实验框架进行研究。拼图提供了内在的地面真实情况、可调整的难度和复杂的决策需求，使其成为这项研究的理想选择。我们的研究发现：首先，MLLMs在最简单的拼图上最初表现接近随机猜测，但通过微调达到近乎完美的准确度并推广到复杂的未见过的配置。其次，在拼图上训练可以推广到其他视觉任务，其有效性取决于特定的任务配置。第三，MLLMs可以学习和推广到有无明确推理的情况，尽管开源模型更喜欢直接回答。因此，即使经过逐步推理的训练，它们也可能会忽略得出最终答案的思考过程。第四，我们观察到复杂的推理模式似乎是预先存在的，而不是突然出现的，它们的频率随着训练和任务难度的增加而增加。最后，我们的结果表明RL比监督微调（SFT）表现出更有效的泛化能力，而初始的SFT冷启动阶段可能会阻碍后续的RL优化。虽然这些观察是基于拼图而得出的，并可能因其他视觉任务而异，但此研究对理解基于规则的视觉RL及其在多模态学习中的潜力做出了有价值的贡献。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>MLLMs在简单的拼图上最初表现较差，但通过微调可以实现在复杂未见配置上的近乎完美表现。</li>
<li>在拼图上训练可以推广到其他视觉任务，有效性取决于特定任务配置。</li>
<li>MLLMs可以适应有无明确推理的情况，但即使经过推理训练，也可能会忽略推导最终答案的过程。</li>
<li>复杂的推理模式似乎是预先存在的，其频率随训练和任务难度的增加而增加。</li>
<li>RL相较于监督微调（SFT）展现出更有效的泛化能力。</li>
<li>初始的SFT冷启动阶段可能对后续的RL优化产生阻碍。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23590">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f98f332c26baafa1472fa145133fa1cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6181cfafaed65844ad9b0feffbe058e9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Qwen-Look-Again-Guiding-Vision-Language-Reasoning-Models-to-Re-attention-Visual-Information"><a href="#Qwen-Look-Again-Guiding-Vision-Language-Reasoning-Models-to-Re-attention-Visual-Information" class="headerlink" title="Qwen Look Again: Guiding Vision-Language Reasoning Models to   Re-attention Visual Information"></a>Qwen Look Again: Guiding Vision-Language Reasoning Models to   Re-attention Visual Information</h2><p><strong>Authors:Xu Chu, Xinrong Chen, Guanyu Wang, Zhijie Tan, Kui Huang, Wenyu Lv, Tong Mo, Weiping Li</strong></p>
<p>Inference time scaling drives extended reasoning to enhance the performance of Vision-Language Models (VLMs), thus forming powerful Vision-Language Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens, causing visual information to receive less attention and may trigger hallucinations. Although introducing text-only reflection processes shows promise in language models, we demonstrate that it is insufficient to suppress hallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain (Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a vision-text reflection process that guides the model to re-attention visual information during reasoning. We first propose a reinforcement learning method Balanced Reflective Policy Optimization (BRPO), which guides the model to decide when to generate vision-text reflection on its own and balance the number and length of reflections. Then, we formally prove that VLRMs lose attention to visual tokens as reasoning progresses, and demonstrate that supplementing visual information during reflection enhances visual attention. Therefore, during training and inference, Visual Token COPY and Visual Token ROUTE are introduced to force the model to re-attention visual information at the visual level, addressing the limitations of text-only reflection. Experiments on multiple visual QA datasets and hallucination metrics indicate that Qwen-LA achieves leading accuracy performance while reducing hallucinations. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/Liar406/Look_Again">https://github.com/Liar406/Look_Again</a> </p>
<blockquote>
<p>推理时间缩放驱动扩展推理，以提高视觉语言模型（VLMs）的性能，从而形成强大的视觉语言推理模型（VLRMs）。然而，长时间的推理会稀释视觉标记，导致视觉信息受到的关注减少，并可能引发幻觉。虽然在语言模型中引入仅文本反射过程显示出了一定的前景，但我们证明它在视觉语言模型中抑制幻觉的能力有限。为了解决这个问题，我们引入了Qwen-LookAgain（Qwen-LA），这是一种新型的VLRM，旨在通过融入视觉文本反射过程来减轻幻觉问题，引导模型在推理过程中重新关注视觉信息。我们首先提出了一种强化学习方法——平衡反射策略优化（BRPO），它引导模型自主决定何时生成视觉文本反射，并平衡反射的数量和长度。然后，我们正式证明了随着推理的进行，VLRMs对视觉标记的关注度逐渐降低，并证明在反射过程中补充视觉信息可以增强视觉关注度。因此，在训练和推理过程中，我们引入了视觉标记复制和视觉标记路由，以强制模型在视觉层面上重新关注视觉信息，解决仅文本反射的局限性。在多个视觉问答数据集和幻觉指标上的实验表明，Qwen-LA在保持领先准确性的同时，减少了幻觉。我们的代码位于：<a target="_blank" rel="noopener" href="https://github.com/Liar406/Look_Again">https://github.com/Liar406/Look_Again</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23558v2">PDF</a> </p>
<p><strong>Summary</strong><br>在视觉问答模型中，长时间的推理过程可能会导致视觉信息的忽略和引发幻觉的问题。为此，我们提出了一种新的视觉语言推理模型Qwen-LookAgain（Qwen-LA），通过引入视觉文本反射过程来平衡视觉和文本的注意力。使用强化学习的方法来指导模型进行自适应的视觉文本反射，并且在训练和推理过程中加入了视觉标记复制和视觉标记路由策略，以解决仅依赖文本反射的局限性。实验结果表明，Qwen-LA模型在多个视觉问答数据集上实现了领先的准确性，同时减少了幻觉的产生。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>长时间的推理过程可能导致视觉信息的忽略和引发幻觉问题。</li>
<li>Qwen-LookAgain是一种新型的视觉语言推理模型，旨在通过视觉文本反射过程解决这一问题。</li>
<li>强化学习被用来指导模型进行自适应的视觉文本反射决策。</li>
<li>模型在训练和推理过程中引入了视觉标记复制和视觉标记路由策略，以重新关注视觉信息。</li>
<li>Qwen-LA模型通过平衡视觉和文本的注意力，提高了模型的性能。</li>
<li>实验结果表明，Qwen-LA模型在多个视觉问答数据集上实现了领先的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23558">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-92bced9c2668af0ae0deb88aa8e1cf8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c738e1b1af5d75fb148d4cf266c4e7d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-91789ea98e732662bf88fea2cc4026ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f0489204046aed9fd9fb9b86bed0d83.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ScEdit-Script-based-Assessment-of-Knowledge-Editing"><a href="#ScEdit-Script-based-Assessment-of-Knowledge-Editing" class="headerlink" title="ScEdit: Script-based Assessment of Knowledge Editing"></a>ScEdit: Script-based Assessment of Knowledge Editing</h2><p><strong>Authors:Xinye Li, Zunwen Zheng, Qian Zhang, Dekai Zhuang, Jiabao Kang, Liyan Xu, Qingbin Liu, Xi Chen, Zhiying Tu, Dianhui Chu, Dianbo Sui</strong></p>
<p>Knowledge Editing (KE) has gained increasing attention, yet current KE tasks remain relatively simple. Under current evaluation frameworks, many editing methods achieve exceptionally high scores, sometimes nearing perfection. However, few studies integrate KE into real-world application scenarios (e.g., recent interest in LLM-as-agent). To support our analysis, we introduce a novel script-based benchmark – ScEdit (Script-based Knowledge Editing Benchmark) – which encompasses both counterfactual and temporal edits. We integrate token-level and text-level evaluation methods, comprehensively analyzing existing KE techniques. The benchmark extends traditional fact-based (“What”-type question) evaluation to action-based (“How”-type question) evaluation. We observe that all KE methods exhibit a drop in performance on established metrics and face challenges on text-level metrics, indicating a challenging task. Our benchmark is available at <a target="_blank" rel="noopener" href="https://github.com/asdfo123/ScEdit">https://github.com/asdfo123/ScEdit</a>. </p>
<blockquote>
<p>知识编辑（KE）越来越受到关注，但当前的KE任务仍然相对简单。在现有的评估框架下，许多编辑方法都取得了异常高的分数，有时甚至接近完美。然而，很少有研究将KE集成到现实世界的应用场景中（例如，最近对LLM-as-agent的兴趣）。为了支持我们的分析，我们引入了一个新的基于脚本的基准测试——ScEdit（基于脚本的知识编辑基准测试），它涵盖了反事实和时间编辑。我们整合了令牌级别和文本级别的评估方法，全面分析了现有的KE技术。该基准测试将传统的基于事实（“是什么”类型问题）的评估扩展到基于行动（“如何做”类型问题）的评估。我们发现，所有KE方法在既定指标上的表现都有所下降，并在文本级别的指标上面临挑战，这表明任务具有挑战性。我们的基准测试在<a target="_blank" rel="noopener" href="https://github.com/asdfo123/ScEdit%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/asdfo123/ScEdit上可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23291v2">PDF</a> ACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>本文介绍了知识编辑（KE）的当前状况及其面临的挑战。尽管KE已经引起了广泛关注，但现有任务相对简单，评估框架下的许多编辑方法得分极高，但实际应用场景中的集成较少。为支持分析，本文引入了一个新型脚本基准测试——ScEdit，它涵盖了反事实和时序编辑。该基准测试将传统的基于事实的评价扩展到了基于行动的评价，对现有KE技术进行了全面的分析，并发现所有KE方法在新基准测试上的性能都有所下降，面临文本层面度量的挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前知识编辑（KE）任务相对简单，评估框架下的编辑方法得分极高。</li>
<li>实际应用场景中KE的集成较少，尤其在大型语言模型代理（LLM-as-agent）领域。</li>
<li>引入了一个新型的脚本基准测试——ScEdit，该测试涵盖了反事实和时序编辑。</li>
<li>ScEdit基准测试集成了令牌级别和文本级别的评估方法，全面分析现有KE技术。</li>
<li>ScEdit基准测试从传统的基于事实的评价扩展到了基于行动的评价。</li>
<li>在ScEdit基准测试中，所有KE方法的性能都有所下降，面临文本级别度量的挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23291">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0aae5ec32865d2c68706768b471c2b3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12bc17cc76811fed42d8e3076fcffb01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98a0f6c04e3fd147931c67e8118f840c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8018e501f9f4afd405ebb05e9cf2bf44.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PBEBench-A-Multi-Step-Programming-by-Examples-Reasoning-Benchmark-inspired-by-Historical-Linguistics"><a href="#PBEBench-A-Multi-Step-Programming-by-Examples-Reasoning-Benchmark-inspired-by-Historical-Linguistics" class="headerlink" title="PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark   inspired by Historical Linguistics"></a>PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark   inspired by Historical Linguistics</h2><p><strong>Authors:Atharva Naik, Darsh Agrawal, Manav Kapadnis, Yuwei An, Yash Mathur, Carolyn Rose, David Mortensen</strong></p>
<p>Recently, long chain of thought (LCoT), Large Language Models (LLMs), have taken the machine learning world by storm with their breathtaking reasoning capabilities. However, are the abstract reasoning abilities of these models general enough for problems of practical importance? Unlike past work, which has focused mainly on math, coding, and data wrangling, we focus on a historical linguistics-inspired inductive reasoning problem, formulated as Programming by Examples. We develop a fully automated pipeline for dynamically generating a benchmark for this task with controllable difficulty in order to tackle scalability and contamination issues to which many reasoning benchmarks are subject. Using our pipeline, we generate a test set with nearly 1k instances that is challenging for all state-of-the-art reasoning LLMs, with the best model (Claude-3.7-Sonnet) achieving a mere 54% pass rate, demonstrating that LCoT LLMs still struggle with a class or reasoning that is ubiquitous in historical linguistics as well as many other domains. </p>
<blockquote>
<p>最近，长链思维（Long Chain of Thought，简称LCoT）的大型语言模型（Large Language Models，简称LLM）以其惊人的推理能力席卷了机器学习领域。然而，这些模型的抽象推理能力对于实际应用中重要的问题是否足够通用？与过去的研究主要集中在数学、编程和数据处理不同，我们关注的是一个受历史语言学启发的归纳推理问题，它被制定为示例编程。为了解决许多推理基准测试所面临的扩展性和污染问题，我们开发了一个全自动管道，用于动态生成此任务的基准测试，并可控制其难度。使用我们的管道，我们生成了一个包含近一千个实例的测试集，对所有最先进的推理大型语言模型来说都具有挑战性。最好的模型（Claude-3.7-Sonnet）通过率仅为54%，这表明大型语言模型在处理历史语言学以及其他许多领域中普遍存在的某种推理问题时仍然面临困难。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23126v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）的长链思维（LCoT）展现出惊人的推理能力，但对于实际应用中的难题，其抽象推理能力是否足够通用尚待探讨。不同于过去专注于数学、编程和数据处理的研究，本研究聚焦于受历史语言学启发的归纳推理问题，并将其制定为编程范例。研究团队开发了一个动态生成此类任务基准测试的全自动化管道，以解决现有基准测试面临的扩展性和污染问题。利用该管道，我们生成了一个包含近一千个实例的测试集，对当前最前沿的推理LLMs构成了挑战，其中最好的模型（Claude-3.7-Sonnet）通过率仅为54%，表明LCoT LLMs在历史语言学以及其他许多领域中仍面临普遍的推理难题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）的长链思维（LCoT）具有强大的推理能力，但在实际应用中面临通用性挑战。</li>
<li>研究关注历史语言学启发的归纳推理问题，以编程范例的形式呈现。</li>
<li>开发了一个自动化管道，用于生成具有可控难度的任务基准测试，解决可扩展性和污染问题。</li>
<li>测试集包含近1000个实例，对现有最前沿的推理LLMs构成挑战。</li>
<li>最好的模型（Claude-3.7-Sonnet）完成率仅为54%，表明仍存在普遍推理难题。</li>
<li>LCoT LLMs在历史语言学以及其他领域的应用仍面临挑战。</li>
<li>研究揭示了LLMs在解决通用性问题上的不足，为未来研究提供了方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23126">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d64a7b737dd7625e2e3e9188128d59b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3c87b391a69e295c3a25bfd1aaffae2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ER-REASON-A-Benchmark-Dataset-for-LLM-Based-Clinical-Reasoning-in-the-Emergency-Room"><a href="#ER-REASON-A-Benchmark-Dataset-for-LLM-Based-Clinical-Reasoning-in-the-Emergency-Room" class="headerlink" title="ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the   Emergency Room"></a>ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the   Emergency Room</h2><p><strong>Authors:Nikita Mehandru, Niloufar Golchini, David Bamman, Travis Zack, Melanie F. Molina, Ahmed Alaa</strong></p>
<p>Large language models (LLMs) have been extensively evaluated on medical question answering tasks based on licensing exams. However, real-world evaluations often depend on costly human annotators, and existing benchmarks tend to focus on isolated tasks that rarely capture the clinical reasoning or full workflow underlying medical decisions. In this paper, we introduce ER-Reason, a benchmark designed to evaluate LLM-based clinical reasoning and decision-making in the emergency room (ER)–a high-stakes setting where clinicians make rapid, consequential decisions across diverse patient presentations and medical specialties under time pressure. ER-Reason includes data from 3,984 patients, encompassing 25,174 de-identified longitudinal clinical notes spanning discharge summaries, progress notes, history and physical exams, consults, echocardiography reports, imaging notes, and ER provider documentation. The benchmark includes evaluation tasks that span key stages of the ER workflow: triage intake, initial assessment, treatment selection, disposition planning, and final diagnosis–each structured to reflect core clinical reasoning processes such as differential diagnosis via rule-out reasoning. We also collected 72 full physician-authored rationales explaining reasoning processes that mimic the teaching process used in residency training, and are typically absent from ER documentation. Evaluations of state-of-the-art LLMs on ER-Reason reveal a gap between LLM-generated and clinician-authored clinical reasoning for ER decisions, highlighting the need for future research to bridge this divide. </p>
<blockquote>
<p>大型语言模型（LLM）在基于执业资格考试的医疗问题回答任务中已得到广泛评估。然而，真实世界的评估通常依赖于成本高昂的人工标注者，而且现有的基准测试往往侧重于很少能捕捉到临床推理或医疗决策背后完整工作流的孤立任务。在本文中，我们介绍了ER-Reason，这是一个旨在评估基于LLM的临床推理和急诊科（ER）决策制定的基准测试。急诊科是一个高风险的环境，医生需要在时间压力下，对不同的患者表现和医学专科做出迅速且重要的决策。ER-Reason包含了3,984名患者的数据，涵盖25,174份去标识化的纵向临床笔记，包括出院总结、病情记录、病史与体检、会诊、超声心动图报告、影像记录以及急诊科医生的文档。该基准测试涵盖了急诊科工作的主要阶段：分诊接待、初步评估、治疗选择、处置计划以及最终诊断。每个阶段的结构都反映了核心的临床推理过程，如通过排除法进行的鉴别诊断。我们还收集了72份医生撰写的完整推理过程，这些推理过程模仿了住院培训中使用的教学过程，通常不会出现在急诊文档记录中。对最新LLM在ER-Reason上的评估显示，LLM生成的医疗急诊决策临床推理与医生撰写的临床推理之间存在差距，这突显了未来研究需要弥补这一鸿沟的必要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22919v2">PDF</a> </p>
<p><strong>Summary</strong>：<br>大型语言模型在基于执照考试的内科医生问答任务中进行了广泛评估。然而，真实世界的评估常常依赖于昂贵的专业注释员，现有基准测试倾向于集中于孤立的任务，很少捕捉到医学决策背后的临床推理或完整工作流程。本文介绍了ER-Reason，这是一个旨在评估基于大型语言模型的急诊室临床推理和决策制定的基准测试。ER-Reason包含来自3984名患者的数据，涵盖了超过记录的纵向临床笔记的25，其中包括出院摘要、进展记录、病史和体检、会诊报告等。评估最新的大型语言模型在ER-Reason上的表现揭示了大型语言模型生成的临床推理与急诊室决策之间的差异，强调了未来需要更多的研究来弥补这一鸿沟。这个差距指出了LLM模型在未来的研究和开发方面需要进一步改进以满足临床决策的需求。同时，这也是一个值得深入探讨的领域，特别是在模拟真实世界环境中进行大规模评估和验证的重要性方面。总的来说，该论文对医学AI的进一步发展提供了新的见解和挑战性思考。此研究的突破性的新方向和新角度推动了我们对这一领域的发展和深入理解的期望和步伐。论文给出了明确的观察和比较模型生成的假设病例与现实中的医学实践的实证差距的解决方案和实践步骤。<strong>Key Takeaways</strong>：</p>
<ul>
<li>大型语言模型（LLMs）在医疗问答任务上进行了广泛评估，但真实世界评估依赖于昂贵的专业注释员，且现有基准测试集中在孤立任务上，无法充分反映医学决策背后的临床推理或完整工作流程。</li>
<li>介绍了一个新的基准测试ER-Reason，旨在评估LLM在急诊室临床推理和决策制定方面的能力。</li>
<li>ER-Reason包含来自不同医疗环节的真实患者数据，并设计了多个评价任务以反映核心临床推理过程。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22919">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ebeda22d237cbc21c73c9e6381fa0f10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a8593df6935ba4c3823a48fb685a116.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a40dac45ee305086031a035ceead853.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6f600515fe71cd2f943044e1d012590.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Revisiting-Group-Relative-Policy-Optimization-Insights-into-On-Policy-and-Off-Policy-Training"><a href="#Revisiting-Group-Relative-Policy-Optimization-Insights-into-On-Policy-and-Off-Policy-Training" class="headerlink" title="Revisiting Group Relative Policy Optimization: Insights into On-Policy   and Off-Policy Training"></a>Revisiting Group Relative Policy Optimization: Insights into On-Policy   and Off-Policy Training</h2><p><strong>Authors:Youssef Mroueh, Nicolas Dupuis, Brian Belgodere, Apoorva Nitsure, Mattia Rigotti, Kristjan Greenewald, Jiri Navratil, Jerret Ross, Jesus Rios</strong></p>
<p>We revisit Group Relative Policy Optimization (GRPO) in both on-policy and off-policy optimization regimes. Our motivation comes from recent work on off-policy Proximal Policy Optimization (PPO), which improves training stability, sampling efficiency, and memory usage. In addition, a recent analysis of GRPO suggests that estimating the advantage function with off-policy samples could be beneficial. Building on these observations, we adapt GRPO to the off-policy setting. We show that both on-policy and off-policy GRPO objectives yield an improvement in the reward. This result motivates the use of clipped surrogate objectives in the off-policy version of GRPO. We then compare the empirical performance of reinforcement learning with verifiable rewards in post-training using both GRPO variants. Our results show that off-policy GRPO either significantly outperforms or performs on par with its on-policy counterpart. </p>
<blockquote>
<p>我们重新研究了Group Relative Policy Optimization（GRPO）在on-policy和off-policy优化制度下的应用。我们的灵感来自于近期关于off-policy近端策略优化（PPO）的研究，它提高了训练稳定性、采样效率和内存使用。此外，最近对GRPO的分析表明，利用off-policy样本估计优势函数可能是有益的。基于这些观察，我们将GRPO适应于off-policy环境。我们证明了on-policy和off-policy GRPO目标都能提高奖励。这一结果促使我们在GRPO的off-policy版本中使用裁剪的替代目标。然后，我们使用这两种GRPO变体，在具有可验证奖励的强化学习中对训练后的经验性能进行比较。我们的结果表明，off-policy GRPO要么显著优于其on-policy版本，要么与其表现相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22257v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在基于策略和基于价值的强化学习优化中重新研究了Group Relative Policy Optimization（GRPO）。受最近关于PPO工作的启发，我们将其适应到off-policy环境中。我们的实验表明，无论是在基于策略还是基于价值的优化中，GRPO都能提高奖励。因此，在GRPO的off-policy版本中，使用剪辑的替代目标是有益的。比较结果显示，off-policy GRPO显著优于或与基于策略的GRPO表现相当。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究了Group Relative Policy Optimization（GRPO）在基于策略和基于价值的强化学习优化中的应用。</li>
<li>受到PPO工作的启发，将其扩展至off-policy环境中。</li>
<li>实验显示，无论是在基于策略还是基于价值的优化中，GRPO都能提高奖励。</li>
<li>在GRPO的off-policy版本中使用剪辑的替代目标是有益的。</li>
<li>off-policy GRPO相较于基于策略的GRPO具有显著优势或表现相当。</li>
<li>off-policy样本在估计优势函数时可能具有优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22257">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d898e95770e0d208f5eadd3137efe210.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7732f3227734bb07c33c3518e413cbec.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TabXEval-Why-this-is-a-Bad-Table-An-eXhaustive-Rubric-for-Table-Evaluation"><a href="#TabXEval-Why-this-is-a-Bad-Table-An-eXhaustive-Rubric-for-Table-Evaluation" class="headerlink" title="TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table   Evaluation"></a>TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table   Evaluation</h2><p><strong>Authors:Vihang Pancholi, Jainit Bafna, Tejas Anvekar, Manish Shrivastava, Vivek Gupta</strong></p>
<p>Evaluating tables qualitatively and quantitatively poses a significant challenge, as standard metrics often overlook subtle structural and content-level discrepancies. To address this, we propose a rubric-based evaluation framework that integrates multi-level structural descriptors with fine-grained contextual signals, enabling more precise and consistent table comparison. Building on this, we introduce TabXEval, an eXhaustive and eXplainable two-phase evaluation framework. TabXEval first aligns reference and predicted tables structurally via TabAlign, then performs semantic and syntactic comparison using TabCompare, offering interpretable and granular feedback. We evaluate TabXEval on TabXBench, a diverse, multi-domain benchmark featuring realistic table perturbations and human annotations. A sensitivity-specificity analysis further demonstrates the robustness and explainability of TabXEval across varied table tasks. Code and data are available at <a target="_blank" rel="noopener" href="https://coral-lab-asu.github.io/tabxeval/">https://coral-lab-asu.github.io/tabxeval/</a> </p>
<blockquote>
<p>评估和对比表格在定性和定量上都存在很大的挑战，因为标准的评估指标往往会忽略细微的结构和内容层面的差异。为了解决这个问题，我们提出了一个基于rubirc的评估框架，该框架结合了多级结构描述器和精细的上下文信号，可以实现更精确和一致的表格比较。在此基础上，我们引入了TabXEval，这是一个全面且可解释的两阶段评估框架。TabXEval首先通过TabAlign对参考表格和预测表格进行结构上的对齐，然后使用TabCompare进行语义和句法上的比较，提供可解释和精细的反馈。我们在TabXBench上评估了TabXEval，这是一个包含真实表格扰动和人类注释的多元多领域基准测试集。通过敏感性和特异性的分析进一步证明了TabXEval在不同表格任务中的稳健性和可解释性。相关代码和数据可在<a target="_blank" rel="noopener" href="https://coral-lab-asu.github.io/tabxeval/%E6%89%BE%E5%88%B0%E3%80%82">https://coral-lab-asu.github.io/tabxeval/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22176v2">PDF</a> Accepeted for Findings at ACL 2025</p>
<p><strong>Summary</strong></p>
<p>该文提出了一种基于rubric的评估框架，该框架融合了多层次的表格结构描述与精细的上下文信号，实现了更精确和一致的表格比较。进一步介绍了TabXEval，一个全面且可解释的两阶段评估框架。它首先通过TabAlign对参考和预测表格进行结构对齐，然后使用TabCompare进行语义和句法比较，提供可解释和详细的反馈。在TabXBench上评估TabXEval，这是一个具有现实表格扰动和人工注释的多元化多领域基准测试集。通过敏感性特异性分析验证了TabXEval在不同表格任务中的稳健性和可解释性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文中提出了一种新的基于rubirc的评估框架，旨在解决表格评估中定性定量评估面临的挑战。</li>
<li>评估框架融合了多层次的结构描述和精细的上下文信号，实现了更精确和一致的表格比较。</li>
<li>介绍了一个名为TabXEval的两阶段评估框架，包含结构对齐阶段（TabAlign）和语义句法比较阶段（TabCompare）。</li>
<li>TabXEval在TabXBench上进行评估，这是一个包含多种表格任务和现实表格扰动的基准测试集。</li>
<li>TabXEval具有全面性和可解释性，能提供详细的反馈。</li>
<li>通过敏感性特异性分析验证了TabXEval在不同表格任务中的稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22176">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a0a7e8db0342751e0fce02c0a447ee9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d569497a6798e810f0920980183ff045.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74ca1fb4295599dadac1e74c571817cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2c7c6b2b290c52ddf03b1eed8a169f3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CAD-Coder-Text-to-CAD-Generation-with-Chain-of-Thought-and-Geometric-Reward"><a href="#CAD-Coder-Text-to-CAD-Generation-with-Chain-of-Thought-and-Geometric-Reward" class="headerlink" title="CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric   Reward"></a>CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric   Reward</h2><p><strong>Authors:Yandong Guan, Xilin Wang, Xingxi Ming, Jing Zhang, Dong Xu, Qian Yu</strong></p>
<p>In this work, we introduce CAD-Coder, a novel framework that reformulates text-to-CAD as the generation of CadQuery scripts - a Python-based, parametric CAD language. This representation enables direct geometric validation, a richer modeling vocabulary, and seamless integration with existing LLMs. To further enhance code validity and geometric fidelity, we propose a two-stage learning pipeline: (1) supervised fine-tuning on paired text-CadQuery data, and (2) reinforcement learning with Group Reward Policy Optimization (GRPO), guided by a CAD-specific reward comprising both a geometric reward (Chamfer Distance) and a format reward. We also introduce a chain-of-thought (CoT) planning process to improve model reasoning, and construct a large-scale, high-quality dataset of 110K text-CadQuery-3D model triplets and 1.5K CoT samples via an automated pipeline. Extensive experiments demonstrate that CAD-Coder enables LLMs to generate diverse, valid, and complex CAD models directly from natural language, advancing the state of the art of text-to-CAD generation and geometric reasoning. </p>
<blockquote>
<p>在这项工作中，我们介绍了CAD-Coder，这是一个新颖的框架，它将文本到CAD的转换重新定义为CadQuery脚本的生成——一种基于Python的参数化CAD语言。这种表示方法能够实现直接的几何验证、更丰富的建模词汇和无缝融合现有的大型语言模型。为了进一步提高代码的有效性和几何保真度，我们提出了一个两阶段的学习管道：（1）在成对的文本-CadQuery数据上进行监督微调；（2）采用基于CAD特定奖励的强化学习，包括几何奖励（Chamfer距离）和格式奖励，进行群体奖励政策优化（GRPO）。我们还引入了思维链（CoT）规划过程来改善模型推理，并通过自动化管道构建了一个大规模、高质量的包含110K文本-CadQuery-3D模型三元组和1.5K思维链样本的数据集。大量实验表明，CAD-Coder使大型语言模型能够直接从自然语言生成多样、有效和复杂的CAD模型，从而推动了文本到CAD生成和几何推理的最新技术前沿。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19713v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本研究介绍了CAD-Coder框架，该框架将文本转换为CAD的过程重新设计为生成CadQuery脚本（基于Python的参数化CAD语言）。此表示法可实现直接几何验证、丰富的建模词汇和与现有大型语言模型的无缝集成。为进一步提高代码的有效性和几何保真度，本研究提出了一种两阶段学习管道，包括：（1）在配对文本-CadQuery数据上进行监督微调；（2）使用组合奖励策略的强化学习，其中包括几何奖励（Chamfer距离）和格式奖励。此外，还引入了思维链（CoT）规划过程以提高模型推理能力，并通过自动化管道构建了一个大规模、高质量包含11万文本-CadQuery-3D模型三元组和1500个CoT样本的数据集。实验表明，CAD-Coder使大型语言模型能够直接从自然语言生成多样化、有效且复杂的CAD模型，推动了文本到CAD生成和几何推理的最新进展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAD-Coder框架能够将文本转换为CAD的过程重新设计为生成CadQuery脚本，丰富了建模语言，并实现直接几何验证。</li>
<li>提出了一种两阶段学习管道，通过监督微调与强化学习提高代码的有效性和几何保真度。</li>
<li>强化学习中的奖励策略包括几何奖励（Chamfer距离）和格式奖励，以优化模型生成。</li>
<li>引入了思维链（CoT）规划过程，提升模型在推理方面的能力。</li>
<li>构建了一个大规模、高质量的数据集，包含文本-CadQuery-3D模型三元组和CoT样本，为相关研究提供丰富资源。</li>
<li>CAD-Coder在文本到CAD生成和几何推理方面展现出卓越性能，能够生成多样化、有效且复杂的CAD模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19713">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8a02cd23da62dd9c0d5a1bc79c5a8f08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f227d4738b014e8188273432dfbe8a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69e3a0b1472798b3cf897d12a1f99e67.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Surrogate-Signals-from-Format-and-Length-Reinforcement-Learning-for-Solving-Mathematical-Problems-without-Ground-Truth-Answers"><a href="#Surrogate-Signals-from-Format-and-Length-Reinforcement-Learning-for-Solving-Mathematical-Problems-without-Ground-Truth-Answers" class="headerlink" title="Surrogate Signals from Format and Length: Reinforcement Learning for   Solving Mathematical Problems without Ground Truth Answers"></a>Surrogate Signals from Format and Length: Reinforcement Learning for   Solving Mathematical Problems without Ground Truth Answers</h2><p><strong>Authors:Rihui Xin, Han Liu, Zecheng Wang, Yupeng Zhang, Dianbo Sui, Xiaolin Hu, Bingning Wang</strong></p>
<p>Large Language Models have achieved remarkable success in natural language processing tasks, with Reinforcement Learning playing a key role in adapting them to specific applications. However, obtaining ground truth answers for training LLMs in mathematical problem-solving is often challenging, costly, and sometimes unfeasible. This research delves into the utilization of format and length as surrogate signals to train LLMs for mathematical problem-solving, bypassing the need for traditional ground truth answers. Our study shows that a reward function centered on format correctness alone can yield performance improvements comparable to the standard GRPO algorithm in early phases. Recognizing the limitations of format-only rewards in the later phases, we incorporate length-based rewards. The resulting GRPO approach, leveraging format-length surrogate signals, not only matches but surpasses the performance of the standard GRPO algorithm relying on ground truth answers in certain scenarios, achieving 40.0% accuracy on AIME2024 with a 7B base model. Through systematic exploration and experimentation, this research not only offers a practical solution for training LLMs to solve mathematical problems and reducing the dependence on extensive ground truth data collection, but also reveals the essence of why our label-free approach succeeds: the powerful base model is like an excellent student who has already mastered mathematical and logical reasoning skills, but performs poorly on the test paper, it simply needs to develop good answering habits to achieve outstanding results in exams, to unlock the capabilities it already possesses. </p>
<blockquote>
<p>大型语言模型在自然语言处理任务中取得了显著的成功，强化学习在适应特定应用方面发挥了关键作用。然而，在为数学问题解决训练大型语言模型时，获取真实答案通常具有挑战性，成本高昂，有时甚至不可行。本研究深入探讨了利用格式和长度作为替代信号来训练大型语言模型进行数学问题解决，从而绕过对传统真实答案的需求。我们的研究表明，仅专注于格式正确性的奖励函数可以在早期阶段产生与标准GRPO算法相当的性能改进。意识到仅使用格式奖励在后期阶段的局限性，我们融入了基于长度的奖励。结果产生的GRPO方法，利用格式长度替代信号，不仅匹配而且在某些情况下超越了依赖真实答案的标准GRPO算法的性能，在AIME2024上实现了40.0%的准确率，基础模型为7B。通过系统的探索和实验，本研究不仅为训练大型语言模型解决数学问题提供了实用解决方案，减少了我们对大量真实数据收集的依赖，还揭示了我们的无标签方法成功的本质：强大的基础模型就像一个已经掌握了数学和逻辑推理技能的优秀学生，但在试卷上表现不佳，它只需要养成良好的答题习惯就能在考试中取得优异成绩，解锁其已经拥有的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19439v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型在自然语言处理任务中取得了显著成功，强化学习在适应特定应用方面发挥了关键作用。然而，为数学问题解决训练大型语言模型时获取真实答案通常具有挑战性、成本高昂，有时甚至不可行。本研究探索了利用格式和长度作为替代信号来训练大型语言模型进行数学问题解决，从而绕过对传统真实答案的需求。研究表明，仅基于格式正确性的奖励函数在初期阶段可以与标准GRPO算法的性能改进相媲美。意识到仅使用格式奖励的局限性，我们引入了基于长度的奖励。GRPO方法利用格式长度替代信号，不仅匹配而且超越了依赖真实答案的标准GRPO算法在某些场景中的性能，在AIME2024上实现了40.0%的准确率，基础模型为7B。通过系统的探索和实验，本研究不仅为训练大型语言模型解决数学问题提供了实用解决方案，减少了大量收集真实数据的依赖，还揭示了无标签方法成功的原因：强大的基础模型就像一个已经掌握了数学和逻辑推理技能的学生，只需要养成良好的答题习惯就能在考试中取得优异成绩，解锁其已具备的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在自然语言处理任务中表现出卓越性能，强化学习在特定应用适应中起关键作用。</li>
<li>训练大型语言模型解决数学问题面临获取真实答案的挑战。</li>
<li>研究利用格式和长度作为替代信号来训练语言模型进行数学问题解决。</li>
<li>基于格式正确性的奖励函数在初期能提升模型性能。</li>
<li>结合长度奖励的GRPO方法在某些场景下性能超越依赖真实答案的标准GRPO算法。</li>
<li>在AIME2024上实现40.0%准确率，使用7B基础模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19439">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f2bf67c39a699ee5477f6762fac5cc0d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d20385106b0194dc68760dac405bbd0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a5edc95fcc359922b4eebaec3f44d68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbbb0173632a1ca4ac851f8f19d79bc8.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="TAG-INSTRUCT-Controlled-Instruction-Complexity-Enhancement-through-Structure-based-Augmentation"><a href="#TAG-INSTRUCT-Controlled-Instruction-Complexity-Enhancement-through-Structure-based-Augmentation" class="headerlink" title="TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through   Structure-based Augmentation"></a>TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through   Structure-based Augmentation</h2><p><strong>Authors:He Zhu, Zhiwen Ruan, Junyou Su, Xingwei He, Yun Chen, Wenjia Zhang, Guanhua Chen</strong></p>
<p>High-quality instruction data is crucial for developing large language models (LLMs), yet existing approaches struggle to effectively control instruction complexity. We present TAG-INSTRUCT, a novel framework that enhances instruction complexity through structured semantic compression and controlled difficulty augmentation. Unlike previous prompt-based methods operating on raw text, TAG-INSTRUCT compresses instructions into a compact tag space and systematically enhances complexity through RL-guided tag expansion. Through extensive experiments, we show that TAG-INSTRUCT outperforms existing instruction complexity augmentation approaches. Our analysis reveals that operating in tag space provides superior controllability and stability across different instruction synthesis frameworks. </p>
<blockquote>
<p>高质量的指令数据对于开发大型语言模型（LLMs）至关重要，然而现有的方法很难有效控制指令的复杂性。我们提出了TAG-INSTRUCT，这是一种通过结构化语义压缩和受控难度增强来提高指令复杂性的新型框架。与以前基于提示的在原始文本上操作的方法不同，TAG-INSTRUCT将指令压缩到一个紧凑的标签空间中，并通过RL引导的标签扩展来系统地增强复杂性。通过广泛的实验，我们证明了TAG-INSTRUCT优于现有的指令复杂性增强方法。我们的分析表明，在标签空间中进行操作可以在不同的指令合成框架中提供出色的可控性和稳定性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18557v2">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了TAG-INSTRUCT框架，该框架通过结构化语义压缩和受控难度增强来提高指令复杂性控制。与传统的基于提示的方法不同，TAG-INSTRUCT将指令压缩成紧凑的标签空间，并通过RL引导标签扩展来系统地增强复杂性。实验表明，TAG-INSTRUCT在指令复杂性增强方面优于现有方法，并且在标签空间中的操作提供了出色的可控性和稳定性。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>高质量的教学数据对于开发大型语言模型至关重要，但现有方法难以有效控制指令复杂性。</li>
<li>TAG-INSTRUCT框架通过结构化语义压缩和受控难度增强来提高指令复杂性控制。</li>
<li>与传统基于提示的方法不同，TAG-INSTRUCT将指令压缩成紧凑的标签空间。</li>
<li>通过RL引导标签扩展，TAG-INSTRUCT系统地增强复杂性。</li>
<li>实验表明，TAG-INSTRUCT在指令复杂性增强方面优于现有方法。</li>
<li>TAG-INSTRUCT在标签空间中的操作提供了出色的可控性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18557">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-77b527ffead2e38a7da081c0c64db267.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a57daf5ed4c9dc01da06546a950c175.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8303d316b238945132a4dd0995c0f064.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="One-RL-to-See-Them-All-Visual-Triple-Unified-Reinforcement-Learning"><a href="#One-RL-to-See-Them-All-Visual-Triple-Unified-Reinforcement-Learning" class="headerlink" title="One RL to See Them All: Visual Triple Unified Reinforcement Learning"></a>One RL to See Them All: Visual Triple Unified Reinforcement Learning</h2><p><strong>Authors:Yan Ma, Linge Du, Xuyang Shen, Shaoxiang Chen, Pengfei Li, Qibing Ren, Lizhuang Ma, Yuchao Dai, Pengfei Liu, Junjie Yan</strong></p>
<p>Reinforcement learning (RL) has significantly advanced the reasoning capabilities of vision-language models (VLMs). However, the use of RL beyond reasoning tasks remains largely unexplored, especially for perceptionintensive tasks like object detection and grounding. We propose V-Triune, a Visual Triple Unified Reinforcement Learning system that enables VLMs to jointly learn visual reasoning and perception tasks within a single training pipeline. V-Triune comprises triple complementary components: Sample-Level Data Formatting (to unify diverse task inputs), Verifier-Level Reward Computation (to deliver custom rewards via specialized verifiers) , and Source-Level Metric Monitoring (to diagnose problems at the data-source level). We further introduce a novel Dynamic IoU reward, which provides adaptive, progressive, and definite feedback for perception tasks handled by V-Triune. Our approach is instantiated within off-the-shelf RL training framework using open-source 7B and 32B backbone models. The resulting model, dubbed Orsta (One RL to See Them All), demonstrates consistent improvements across both reasoning and perception tasks. This broad capability is significantly shaped by its training on a diverse dataset, constructed around four representative visual reasoning tasks (Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding, Detection, Counting, and OCR). Subsequently, Orsta achieves substantial gains on MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1 across its various 7B and 32B model variants, with performance benefits extending to a wide range of downstream tasks. These results highlight the effectiveness and scalability of our unified RL approach for VLMs. The V-Triune system, along with the Orsta models, is publicly available at <a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI">https://github.com/MiniMax-AI</a>. </p>
<blockquote>
<p>强化学习（RL）已显著提高视觉语言模型（VLM）的推理能力。然而，除了推理任务之外，RL在感知密集型任务（如目标检测和定位）中的使用仍然在很大程度上未被探索。我们提出了V-Triune，一个视觉三重统一强化学习系统，它使VLM能够在单个训练管道中联合学习视觉推理和感知任务。V-Triune包含三个互补的组件：样本级数据格式化（以统一各种任务输入）、验证器级奖励计算（通过专用验证器提供自定义奖励），以及源级指标监控（在数据源级别诊断问题）。我们还引入了一种新的动态IoU奖励，它为V-Triune处理的感知任务提供自适应、渐进和明确的反馈。我们的方法是在现成的RL训练框架中使用开源的7B和32B骨干模型实现的。由此产生的模型，被称为Orsta（一RL见他们所有人），在推理和感知任务上都表现出了一致的改进。这种广泛的能力在很大程度上是由其在围绕四个代表性视觉推理任务（数学、拼图、图表和科学）和四个视觉感知任务（定位、检测、计数和OCR）构建的多样化数据集上的训练所塑造的。随后，Orsta在MEGA-Bench Core上取得了实质性进展，在其各种7B和32B模型变体中的改进范围从+2.1到令人印象深刻的+14.1，性能优势扩展到广泛的下游任务。这些结果突出显示了我们统一的RL方法对VLM的有效性和可扩展性。V-Triune系统以及Orsta模型可在<a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/MiniMax-AI上公开获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18129v2">PDF</a> Technical Report</p>
<p><strong>摘要</strong><br>强化学习（RL）提升了视觉语言模型（VLM）的推理能力。然而，RL在推理任务之外的应用，尤其是针对感知密集型任务如目标检测和定位，仍然有待探索。我们提出了V-Triune系统，一个视觉三重统一强化学习系统，它使VLM能够在单一训练管道中联合学习视觉推理和感知任务。V-Triune包括三个互补的组件：样本级数据格式化（统一不同任务的输入）、验证器级奖励计算（通过专用验证器提供自定义奖励），以及源级指标监控（在数据源级别诊断问题）。我们还引入了一种新的动态IoU奖励，它为V-Triune处理的感知任务提供自适应、渐进和明确的反馈。我们的方法是在现成的RL训练框架中使用开源的7B和32B骨干模型实现的。由此产生的模型，被称为Orsta（一RL见万物），在推理和感知任务上都表现出持续的优势。这种广泛的性能在很大程度上是由其在四个代表性视觉推理任务和四个视觉感知任务周围构建的多样化数据集上的训练塑造的（数学、拼图、图表和科学；定位、检测、计数和OCR）。随后，Orsta在MEGA-Bench Core上取得了重大进展，在其各种7B和32B模型变种中，改进范围从+2.1到令人印象深刻的+14.1，性能优势扩展到广泛的下游任务。结果突出了我们统一RL方法对VLM的有效性和可扩展性。V-Triune系统和Orsta模型可在<a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/MiniMax-AI获得。</a></p>
<p><strong>要点</strong></p>
<ol>
<li>强化学习（RL）增强了视觉语言模型（VLM）的推理能力。</li>
<li>提出V-Triune系统：一个视觉三重统一强化学习系统，使VLM能联合学习视觉推理和感知任务。</li>
<li>V-Triune包含三个互补组件：样本级数据格式化、验证器级奖励计算和源级指标监控。</li>
<li>引入动态IoU奖励，为感知任务提供自适应、渐进和明确的反馈。</li>
<li>在多种视觉推理和感知任务上，Orsta模型表现出显著的性能改进。</li>
<li>Orsta模型在MEGA-Bench Core上取得了重大进展，改进范围广泛。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18129">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1b5d6435098e68dd28af9aa984385e29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6e229fba3bde01a32cb4eba3b00c150.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09409e4ddbedec71977f7fbffbc8fafe.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Co-Reinforcement-Learning-for-Unified-Multimodal-Understanding-and-Generation"><a href="#Co-Reinforcement-Learning-for-Unified-Multimodal-Understanding-and-Generation" class="headerlink" title="Co-Reinforcement Learning for Unified Multimodal Understanding and   Generation"></a>Co-Reinforcement Learning for Unified Multimodal Understanding and   Generation</h2><p><strong>Authors:Jingjing Jiang, Chongjie Si, Jun Luo, Hanwang Zhang, Chao Ma</strong></p>
<p>This paper presents a pioneering exploration of reinforcement learning (RL) via group relative policy optimization for unified multimodal large language models (ULMs), aimed at simultaneously reinforcing generation and understanding capabilities. Through systematic pilot studies, we uncover the significant potential of ULMs to enable the synergistic co-evolution of dual capabilities within a shared policy optimization framework. Building on this insight, we introduce CoRL, a co-reinforcement learning framework comprising a unified RL stage for joint optimization and a refined RL stage for task-specific enhancement. With the proposed CoRL, our resulting model, ULM-R1, achieves average improvements of 7% on three text-to-image generation datasets and 23% on nine multimodal understanding benchmarks. These results demonstrate the effectiveness of CoRL and highlight the substantial benefit of reinforcement learning in facilitating cross-task synergy and optimization for ULMs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mm-vl/ULM-R1">https://github.com/mm-vl/ULM-R1</a>. </p>
<blockquote>
<p>本文首次探索了通过群体相对策略优化来强化多模态大型语言模型（ULM）的强化学习（RL）方法，旨在同时增强生成和理解能力。通过系统的初步研究，我们发现ULM在共享策略优化框架内协同进化双重能力的巨大潜力。基于此，我们引入了协同强化学习框架CoRL，该框架包括一个用于联合优化的统一RL阶段和一个用于特定任务增强的精炼RL阶段。通过使用所提出的CoRL，我们的模型ULM-R1在三个文本到图像生成数据集上平均提高了7%，在九个多模态理解基准测试中提高了23%。这些结果证明了CoRL的有效性，并突显了强化学习在提高ULM跨任务协同和优化的实质利益中的关键作用。代码可在<a target="_blank" rel="noopener" href="https://github.com/mm-vl/ULM-R1%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/mm-vl/ULM-R1获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17534v2">PDF</a> </p>
<p><strong>Summary</strong><br>文本介绍了一项开创性的研究，该研究探讨了通过群体相对策略优化强化学习在统一多模态大型语言模型中的应用，旨在同时强化生成和理解能力。该研究引入了协同强化学习框架，实现了模型在文本生成和理解方面的协同进化。提出的模型ULM-R1在三个文本生成数据集上平均提高了7%，在九个多模态理解基准测试上平均提高了23%，证明了强化学习在促进跨任务协同和优化方面的巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究采用强化学习（RL）通过群体相对策略优化，对统一多模态大型语言模型（ULMs）进行探索。</li>
<li>研究目标是同时强化ULMs的生成和理解能力。</li>
<li>通过系统试点研究，发现了ULMs在协同进化生成和理解能力方面的显著潜力。</li>
<li>研究引入了协同强化学习框架（CoRL），包括统一RL阶段进行联合优化和精细RL阶段进行任务特定增强。</li>
<li>ULM-R1模型在三个文本生成数据集上的平均性能提高了7%，在九个多模态理解基准测试上的平均性能提高了23%。</li>
<li>结果证明了CoRL框架的有效性，并凸显了强化学习在促进ULMs跨任务协同和优化方面的巨大益处。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17534">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1d34d6d2e5fdb0b03b4b63eca26d6229.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23809cf37c0243c211fd1b0921ffd220.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e2c02a186bb2e31e96c93efd6562d53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02e1b44bd3cedcc0812fcc31442606cf.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="VLM-R-3-Region-Recognition-Reasoning-and-Refinement-for-Enhanced-Multimodal-Chain-of-Thought"><a href="#VLM-R-3-Region-Recognition-Reasoning-and-Refinement-for-Enhanced-Multimodal-Chain-of-Thought" class="headerlink" title="VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced   Multimodal Chain-of-Thought"></a>VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced   Multimodal Chain-of-Thought</h2><p><strong>Authors:Chaoya Jiang, Yongrui Heng, Wei Ye, Han Yang, Haiyang Xu, Ming Yan, Ji Zhang, Fei Huang, Shikun Zhang</strong></p>
<p>Recently, reasoning-based MLLMs have achieved a degree of success in generating long-form textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on and revisiting of visual regions to achieve precise grounding of textual reasoning in visual evidence. We introduce \textbf{VLM-R$^3$} (\textbf{V}isual \textbf{L}anguage \textbf{M}odel with \textbf{R}egion \textbf{R}ecognition and \textbf{R}easoning), a framework that equips an MLLM with the ability to (i) decide \emph{when} additional visual evidence is needed, (ii) determine \emph{where} to ground within the image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved chain-of-thought. The core of our method is \textbf{Region-Conditioned Reinforcement Policy Optimization (R-GRPO)}, a training paradigm that rewards the model for selecting informative regions, formulating appropriate transformations (e.g.\ crop, zoom), and integrating the resulting visual context into subsequent reasoning steps. To bootstrap this policy, we compile a modest but carefully curated Visuo-Lingual Interleaved Rationale (VLIR) corpus that provides step-level supervision on region selection and textual justification. Extensive experiments on MathVista, ScienceQA, and other benchmarks show that VLM-R$^3$ sets a new state of the art in zero-shot and few-shot settings, with the largest gains appearing on questions demanding subtle spatial reasoning or fine-grained visual cue extraction. </p>
<blockquote>
<p>最近，基于推理的MLLM在生成长文本推理链方面取得了一定的成功。然而，它们在处理需要动态和迭代地关注并重新访问视觉区域以实现在视觉证据中进行精确文本推理的复杂任务时仍然面临困难。我们引入了VLM-R$^3$（带区域识别和推理的视觉语言模型），这是一个为MLLM配备能力的框架，能够（i）决定何时需要额外的视觉证据，（ii）确定在图像中的扎根位置，以及（iii）无缝地将相关子图像内容重新编织成连贯的思维链。我们的方法的核心是区域条件强化策略优化（R-GRPO），这是一种训练范式，奖励模型选择信息区域、制定适当的转换（例如裁剪、放大），并将得到的视觉上下文集成到随后的推理步骤中。为了引导策略，我们精心整理了一个规模不大但经过精心筛选的Visuo-Lingual交织理由（VLIR）语料库，该语料库提供了关于区域选择和文本依据的逐步监督。在MathVista、ScienceQA和其他基准测试上的大量实验表明，VLM-R$^3$在零样本和少样本设置中树立了新的技术标杆，在要求精细空间推理或精细视觉线索提取的问题上表现最为显著。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16192v2">PDF</a> </p>
<p><strong>摘要</strong><br>    MLLM在生成长文本推理链方面取得了成功，但在需要动态迭代关注图像区域的任务上仍有挑战。为此，我们提出VLM-R³框架，使MLLM具备决定何时需要额外视觉证据、确定图像中的落地点以及无缝地将相关子图像内容融入思维链的能力。核心方法是Region-Conditioned Reinforcement Policy Optimization (R-GRPO)，奖励模型选择信息区域、制定适当的转换（如裁剪、放大）并将所得视觉上下文融入后续推理步骤。通过编制适度但精心策划的Visuo-Lingual Interleaved Rationale (VLIR)语料库，为区域选择和文本依据提供步骤层面的监督来引导策略。在MathVista、ScienceQA等基准测试上的实验显示，VLM-R³在零样本和少样本设置上创下了新纪录，尤其是在需要精细空间推理或精细视觉线索提取的问题上效果更佳。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>MLLM在生成长文本推理链方面已取得成功，但在某些复杂任务中仍需提高。</li>
<li>引入VLM-R³框架，使MLLM具备动态迭代关注图像区域的能力。</li>
<li>VLM-R³框架的核心是Region-Conditioned Reinforcement Policy Optimization (R-GRPO)，它能奖励模型在选择信息区域和整合视觉上下文方面的表现。</li>
<li>VLM-R³通过编制Visuo-Lingual Interleaved Rationale (VLIR)语料库来引导策略。</li>
<li>VLIR语料库提供步骤层面的监督，涵盖区域选择和文本依据。</li>
<li>在多个基准测试上，VLM-R³表现出卓越性能，尤其在需要精细空间推理和视觉线索提取的任务上。</li>
<li>VLM-R³在零样本和少样本设置上创下了新纪录。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16192">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6a436ebc8a1fb336805b4d7637d8a6a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3efa3118bcc66eac357d292593cbeb63.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ade60abc3bdc5760f02256ca2c95a87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2199ea2b2d0aacf723ef9262927e083.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d3c03365170299397c44c4faec81cd22.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-06-04  AutoChemSchematic AI A Closed-Loop, Physics-Aware Agentic Framework for   Auto-Generating Chemical Process and Instrumentation Diagrams
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-57c1aa4dacbd5e92143c9195eef923f9.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-06-03  TalkingHeadBench A Multi-Modal Benchmark & Analysis of Talking-Head   DeepFake Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27348.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
