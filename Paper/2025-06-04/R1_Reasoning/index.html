<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-04  AReaL A Large-Scale Asynchronous Reinforcement Learning System for   Language Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-d64a7b737dd7625e2e3e9188128d59b1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-04-æ›´æ–°"><a href="#2025-06-04-æ›´æ–°" class="headerlink" title="2025-06-04 æ›´æ–°"></a>2025-06-04 æ›´æ–°</h1><h2 id="AReaL-A-Large-Scale-Asynchronous-Reinforcement-Learning-System-for-Language-Reasoning"><a href="#AReaL-A-Large-Scale-Asynchronous-Reinforcement-Learning-System-for-Language-Reasoning" class="headerlink" title="AReaL: A Large-Scale Asynchronous Reinforcement Learning System for   Language Reasoning"></a>AReaL: A Large-Scale Asynchronous Reinforcement Learning System for   Language Reasoning</h2><p><strong>Authors:Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, Yi Wu</strong></p>
<p>Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a \emph{fully asynchronous} RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves \textbf{up to 2.57$\times$ training speedup} compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at <a target="_blank" rel="noopener" href="https://github.com/inclusionAI/AReaL/">https://github.com/inclusionAI/AReaL/</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æµè¡Œè¶‹åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†ä»»åŠ¡ä¸­ã€‚å¯¹äºLLMçš„æœ‰æ•ˆRLéœ€è¦å¤§è§„æ¨¡å¹¶è¡ŒåŒ–ï¼Œå¹¶è¿«åˆ‡éœ€è¦å¯¹é«˜æ•ˆçš„è®­ç»ƒç³»ç»Ÿæå‡ºéœ€æ±‚ã€‚å¤§å¤šæ•°ç°æœ‰çš„ç”¨äºLLMçš„å¤§è§„æ¨¡RLç³»ç»Ÿæ˜¯åŒæ­¥çš„ï¼Œé€šè¿‡åœ¨æ‰¹é‡è®¾ç½®ä¸­äº¤æ›¿ç”Ÿæˆå’ŒåŸ¹è®­æ¥ç¨³å®šRLè®­ç»ƒã€‚åœ¨æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¸­ç”Ÿæˆçš„æ»šåŠ¨æ•°æ®ç”±åŒä¸€ï¼ˆæˆ–æœ€æ–°ï¼‰æ¨¡å‹ç”Ÿæˆã€‚è¿™è™½ç„¶ç¨³å®šäº†RLè®­ç»ƒï¼Œä½†å¯¼è‡´äº†ç³»ç»Ÿå±‚é¢çš„ä¸¥é‡ä½æ•ˆã€‚ç”Ÿæˆå¿…é¡»ç­‰å¾…æ‰¹æ¬¡ä¸­æœ€é•¿çš„è¾“å‡ºå®Œæˆæ‰èƒ½è¿›è¡Œæ¨¡å‹æ›´æ–°ï¼Œå¯¼è‡´GPUåˆ©ç”¨ç‡ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†AReaLï¼Œä¸€ä¸ªå®Œå…¨å¼‚æ­¥çš„RLç³»ç»Ÿï¼Œå®ƒå°†ç”Ÿæˆä¸è®­ç»ƒå®Œå…¨è§£è€¦ã€‚AReaLä¸­çš„æ»šåŠ¨å·¥ä½œå™¨å¯ä»¥è¿ç»­ç”Ÿæˆæ–°çš„è¾“å‡ºè€Œæ— éœ€ç­‰å¾…ï¼Œè€Œè®­ç»ƒå·¥ä½œå™¨åˆ™ä¼šåœ¨æ¯æ¬¡æ”¶é›†åˆ°ä¸€æ‰¹æ•°æ®æ—¶æ›´æ–°æ¨¡å‹ã€‚AReaLè¿˜é‡‡ç”¨äº†ä¸€ç³»åˆ—ç³»ç»Ÿå±‚é¢çš„ä¼˜åŒ–ï¼Œå¤§å¤§æé«˜äº†GPUçš„åˆ©ç”¨ç‡ã€‚ä¸ºäº†ç¨³å®šRLè®­ç»ƒï¼ŒAReaLå¹³è¡¡äº†æ»šåŠ¨å’Œè®­ç»ƒå·¥ä½œå™¨çš„è´Ÿè½½ï¼Œä»¥æ§åˆ¶æ•°æ®çš„é™ˆæ—§ç¨‹åº¦ï¼Œå¹¶é‡‡ç”¨äº†å¢å¼ºé™ˆæ—§æ€§çš„PPOå˜ä½“æ¥æ›´å¥½åœ°å¤„ç†è¿‡æ—¶çš„è®­ç»ƒæ ·æœ¬ã€‚åœ¨æ•°å­¦å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ä½¿ç”¨ç›¸åŒæ•°é‡GPUçš„æœ€ä½³åŒæ­¥ç³»ç»Ÿç›¸æ¯”ï¼ŒAReaLå®ç°äº†é«˜è¾¾2.57å€çš„è®­ç»ƒåŠ é€Ÿï¼Œå¹¶å®ç°äº†ç›¸åŒ¹é…ç”šè‡³æ›´å¥½çš„æœ€ç»ˆæ€§èƒ½ã€‚AReaLçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/inclusionAI/AReaL/%E3%80%82">https://github.com/inclusionAI/AReaL/è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24298v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒéœ€è¦é«˜æ•ˆçš„ç³»ç»Ÿæ”¯æŒã€‚ç°æœ‰åŒæ­¥RLç³»ç»Ÿè™½ç„¶ç¨³å®šï¼Œä½†å­˜åœ¨ç³»ç»Ÿçº§æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºä¸€ç§å…¨æ–°çš„å¼‚æ­¥RLç³»ç»ŸAReaLï¼Œå°†ç”Ÿæˆä¸è®­ç»ƒè¿‡ç¨‹å®Œå…¨è§£è€¦ï¼Œæé«˜GPUåˆ©ç”¨ç‡ã€‚é€šè¿‡å·¥ä½œè´Ÿè½½å¹³è¡¡å’ŒPPOç®—æ³•æ”¹è¿›ï¼ŒAReaLåœ¨æ•°å­¦å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜æ•ˆè®­ç»ƒåŠ é€Ÿï¼Œä¸”æ€§èƒ½ä¸æœ€ä½³åŒæ­¥ç³»ç»Ÿç›¸æ¯”æœ‰æ‰€æé«˜ã€‚ä»£ç å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†ä»»åŠ¡ä¸­ã€‚</li>
<li>ç°æœ‰åŒæ­¥å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿåœ¨è®­ç»ƒè¯­è¨€æ¨¡å‹æ—¶å­˜åœ¨ç³»ç»Ÿçº§æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚</li>
<li>AReaLç³»ç»Ÿé€šè¿‡å®Œå…¨å¼‚æ­¥çš„å¼ºåŒ–å­¦ä¹ æ–¹å¼è§£å†³æ­¤é—®é¢˜ï¼Œç”Ÿæˆä¸è®­ç»ƒè¿‡ç¨‹è§£è€¦ï¼Œæé«˜GPUåˆ©ç”¨ç‡ã€‚</li>
<li>AReaLç³»ç»Ÿé‡‡ç”¨ä¸€ç³»åˆ—ç³»ç»Ÿçº§ä¼˜åŒ–æªæ–½ï¼ŒåŒ…æ‹¬å·¥ä½œè´Ÿè½½å¹³è¡¡å’ŒPPOç®—æ³•çš„æ”¹è¿›ã€‚</li>
<li>åœ¨æ•°å­¦å’Œä»£ç æ¨ç†æµ‹è¯•ä¸­ï¼ŒAReaLç³»ç»Ÿç›¸æ¯”æœ€ä½³åŒæ­¥ç³»ç»Ÿå®ç°äº†æœ€é«˜è¾¾2.57å€çš„è®­ç»ƒåŠ é€Ÿã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-579aa7e9768d0d5ce5d51f2796d64cb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3800501961f7e4f527105ff0f88237c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4aa208b05258fb160835e564f4d9b17f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-14c152cab10b8255feac839aa31c38aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28a5f4330eb91a7f145c578aaa39d633.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CodeV-R1-Reasoning-Enhanced-Verilog-Generation"><a href="#CodeV-R1-Reasoning-Enhanced-Verilog-Generation" class="headerlink" title="CodeV-R1: Reasoning-Enhanced Verilog Generation"></a>CodeV-R1: Reasoning-Enhanced Verilog Generation</h2><p><strong>Authors:Yaoyu Zhu, Di Huang, Hanqi Lyu, Xiaoyun Zhang, Chongxiao Li, Wenxuan Shi, Yutong Wu, Jianan Mu, Jinghua Wang, Yang Zhao, Pengwei Jin, Shuyao Cheng, Shengwen Liang, Xishan Zhang, Rui Zhang, Zidong Du, Qi Guo, Xing Hu, Yunji Chen</strong></p>
<p>Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage â€œdistill-then-RLâ€ training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities. </p>
<blockquote>
<p>é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…·æœ‰æ˜ç¡®ã€å¯è‡ªåŠ¨åŒ–çš„éªŒè¯ä»»åŠ¡ä¸Šå–å¾—äº†çªç ´ï¼Œä¾‹å¦‚è½¯ä»¶ç¼–ç¨‹å’Œæ•°å­¦é—®é¢˜ã€‚ç„¶è€Œï¼Œå°†RLVRæ‰©å±•åˆ°ç”µå­è®¾è®¡è‡ªåŠ¨åŒ–ï¼ˆEDAï¼‰ï¼Œç‰¹åˆ«æ˜¯ä»è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰è§„èŒƒè‡ªåŠ¨ç”Ÿæˆç¡¬ä»¶æè¿°è¯­è¨€ï¼ˆHDLï¼‰å¦‚Verilogï¼Œé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šç¼ºä¹è‡ªåŠ¨åŒ–å’Œå‡†ç¡®çš„éªŒè¯ç¯å¢ƒã€é«˜è´¨é‡NL-ä»£ç å¯¹ç¨€ç¼ºï¼Œä»¥åŠRLVRçš„è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†CodeV-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒVerilogç”ŸæˆLLMçš„RLVRæ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºè§„åˆ™çš„æµ‹è¯•å°ç”Ÿæˆå™¨ï¼Œå®ƒå¯ä»¥é€šè¿‡å¯¹ç…§é‡‘å‚è€ƒè¿›è¡Œç¨³å¥çš„ç­‰ä»·æ€§æ£€æŸ¥ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¾€è¿”æ•°æ®åˆæˆæ–¹æ³•ï¼Œå°†å¼€æºVerilogç‰‡æ®µä¸LLMç”Ÿæˆçš„è‡ªç„¶è¯­è¨€æè¿°é…å¯¹ï¼Œé€šè¿‡ç”Ÿæˆçš„æµ‹è¯•å°éªŒè¯ä»£ç -NL-ä»£ç çš„çš„ä¸€è‡´æ€§ï¼Œå¹¶è¿‡æ»¤å‡ºä¸ç­‰ä»·çš„ä¾‹å­ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„æ•°æ®é›†ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µâ€œè’¸é¦ç„¶åå¼ºåŒ–å­¦ä¹ â€çš„è®­ç»ƒæµç¨‹ï¼šå…ˆè¿›è¡Œæ¨ç†èƒ½åŠ›çš„å†·å¯åŠ¨è’¸é¦ï¼Œç„¶åé‡‡ç”¨æˆ‘ä»¬æ–°å‹çš„è‡ªé€‚åº”DAPO RLVRç®—æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´é‡‡æ ·ç‡æ¥é™ä½è®­ç»ƒæˆæœ¬ã€‚æœ€ç»ˆæ¨¡å‹CodeV-R1-7Båœ¨VerilogEval v2å’ŒRTLLM v1.1ä¸Šçš„pass@1åˆ†åˆ«è¾¾åˆ°äº†68.6%å’Œ72.9%ï¼Œè¾ƒä¹‹å‰çš„æœ€å…ˆè¿›æ°´å¹³æé«˜äº†12~20%ï¼ŒåŒæ—¶åŒ¹é…ç”šè‡³è¶…è¶Šäº†671B DeepSeek-R1çš„æ€§èƒ½ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹ã€è®­ç»ƒæµç¨‹å’Œæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›EDAå’ŒLLMç¤¾åŒºçš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24183v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…·æœ‰æ˜ç¡®ã€å¯è‡ªåŠ¨åŒ–éªŒè¯çš„ä»»åŠ¡ä¸Šå–å¾—äº†çªç ´ï¼Œå¦‚è½¯ä»¶ç¼–ç¨‹å’Œæ•°å­¦é—®é¢˜ã€‚ç„¶è€Œï¼Œå°†å…¶æ‰©å±•åˆ°ç”µå­è®¾è®¡è‡ªåŠ¨åŒ–ï¼ˆEDAï¼‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯ä»è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰è§„èŒƒè‡ªåŠ¨ç”Ÿæˆç¡¬ä»¶æè¿°è¯­è¨€ï¼ˆHDLï¼‰å¦‚Verilogæ—¶ï¼Œé¢ä¸´ç¼ºä¹è‡ªåŠ¨åŒ–å’Œå‡†ç¡®çš„éªŒè¯ç¯å¢ƒã€é«˜è´¨é‡NL-ä»£ç å¯¹ç¨€ç¼ºä»¥åŠRLVRè®¡ç®—æˆæœ¬é«˜æ˜‚ç­‰ä¸‰å¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†CodeV-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒVerilogç”ŸæˆLLMçš„RLVRæ¡†æ¶ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŸºäºè§„åˆ™çš„æµ‹è¯•å¹³å°ç”Ÿæˆå™¨ï¼Œå®ƒèƒ½å¤Ÿå¯¹é»„é‡‘å‚è€ƒè¿›è¡Œç¨³å¥çš„ç­‰ä»·æ€§æ£€æŸ¥ï¼›æå‡ºäº†ä¸€ç§å¾€è¿”æ•°æ®åˆæˆæ–¹æ³•ï¼Œå°†å¼€æºVerilogç‰‡æ®µä¸LLMç”Ÿæˆçš„NLæè¿°é…å¯¹ï¼Œé€šè¿‡ç”Ÿæˆçš„æµ‹è¯•å¹³å°è¿›è¡Œä»£ç -NL-ä»£ç çš„ä¸€è‡´æ€§éªŒè¯ï¼Œå¹¶è¿‡æ»¤å‡ºä¸ç›¸å½“çš„ä¾‹å­ä»¥å½¢æˆé«˜è´¨é‡æ•°æ®é›†ï¼›é‡‡ç”¨ä¸¤é˜¶æ®µâ€œè’¸é¦åRLâ€è®­ç»ƒç®¡é“ï¼šå…ˆè¿›è¡Œè’¸é¦ä»¥å¯åŠ¨æ¨ç†èƒ½åŠ›ï¼Œç„¶åé‡‡ç”¨æˆ‘ä»¬æ–°å‹RLVRç®—æ³•è‡ªé€‚åº”DAPOï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´é‡‡æ ·ç‡æ¥é™ä½è®­ç»ƒæˆæœ¬ã€‚ç»“æœæ¨¡å‹CodeV-R1-7Båœ¨VerilogEval v2å’ŒRTLLVM v1.1ä¸Šçš„pass@1åˆ†åˆ«è¾¾åˆ°äº†68.6%å’Œ72.9%ï¼Œè¾ƒä¹‹å‰çš„æœ€ä¼˜çŠ¶æ€æé«˜äº†12~20%ï¼ŒåŒæ—¶åŒ¹é…ç”šè‡³è¶…è¶Šäº†671B DeepSeek-R1çš„æ€§èƒ½ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹ã€è®­ç»ƒç®¡é“å’Œæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›EDAå’ŒLLMç¤¾åŒºçš„ç ”ç©¶ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è®­ç»ƒåœ¨ç‰¹å®šä»»åŠ¡ä¸Šå–å¾—çªç ´ã€‚</li>
<li>åœ¨ç”µå­è®¾è®¡è‡ªåŠ¨åŒ–ï¼ˆEDAï¼‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯Verilogè‡ªåŠ¨ç”Ÿæˆé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥CodeV-R1æ¡†æ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æµ‹è¯•å¹³å°ç”Ÿæˆå™¨ã€å¾€è¿”æ•°æ®åˆæˆæ–¹æ³•å’Œä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ã€‚</li>
<li>CodeV-R1-7Bæ¨¡å‹åœ¨Verilogè¯„ä»·ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡ç°æœ‰æŠ€æœ¯12~20%ã€‚</li>
<li>æ¨¡å‹ã€è®­ç»ƒç®¡é“å’Œæ•°æ®é›†å°†å…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›EDAå’ŒLLMé¢†åŸŸçš„ç ”ç©¶ã€‚</li>
<li>RLVRçš„æ–°å‹ç®—æ³•è‡ªé€‚åº”DAPOåœ¨é™ä½è®­ç»ƒæˆæœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>æ¨¡å‹åœ¨åŒ¹é…æˆ–è¶…è¶Š671B DeepSeek-R1æ€§èƒ½çš„åŒæ—¶å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1cc4c09c1fefd21a414fe0df953c98e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d106afc022ea2f5fa9ac33b509963af1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74da7c0b73414cc15dc46acd7d45a94d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Mixed-R1-Unified-Reward-Perspective-For-Reasoning-Capability-in-Multimodal-Large-Language-Models"><a href="#Mixed-R1-Unified-Reward-Perspective-For-Reasoning-Capability-in-Multimodal-Large-Language-Models" class="headerlink" title="Mixed-R1: Unified Reward Perspective For Reasoning Capability in   Multimodal Large Language Models"></a>Mixed-R1: Unified Reward Perspective For Reasoning Capability in   Multimodal Large Language Models</h2><p><strong>Authors:Shilin Xu, Yanwei Li, Rui Yang, Tao Zhang, Yueyi Sun, Wei Chow, Linfeng Li, Hang Song, Qi Xu, Yunhai Tong, Xiangtai Li, Hao Fei</strong></p>
<p>Recent works on large language models (LLMs) have successfully demonstrated the emergence of reasoning capabilities via reinforcement learning (RL). Although recent efforts leverage group relative policy optimization (GRPO) for MLLMs post-training, they constantly explore one specific aspect, such as grounding tasks, math problems, or chart analysis. There are no works that can leverage multi-source MLLM tasks for stable reinforcement learning. In this work, we present a unified perspective to solve this problem. We present Mixed-R1, a unified yet straightforward framework that contains a mixed reward function design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K). We first design a data engine to select high-quality examples to build the Mixed-45K post-training dataset. Then, we present a Mixed-Reward design, which contains various reward functions for various MLLM tasks. In particular, it has four different reward functions: matching reward for binary answer or multiple-choice problems, chart reward for chart-aware datasets, IoU reward for grounding problems, and open-ended reward for long-form text responses such as caption datasets. To handle the various long-form text content, we propose a new open-ended reward named Bidirectional Max-Average Similarity (BMAS) by leveraging tokenizer embedding matching between the generated response and the ground truth. Extensive experiments show the effectiveness of our proposed method on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes. Our dataset and model are available at <a target="_blank" rel="noopener" href="https://github.com/xushilin1/mixed-r1">https://github.com/xushilin1/mixed-r1</a>. </p>
<blockquote>
<p>å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ€æ–°ç ”ç©¶å·²ç»æˆåŠŸåœ°è¯æ˜äº†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¶Œç°å‡ºçš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡æœ€è¿‘çš„åŠªåŠ›åˆ©ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¯¹MLLMsè¿›è¡Œåè®­ç»ƒï¼Œä½†å®ƒä»¬ä¸æ–­åœ°æ¢ç´¢ä¸€ä¸ªç‰¹å®šçš„æ–¹é¢ï¼Œå¦‚æ¥åœ°ä»»åŠ¡ã€æ•°å­¦é—®é¢˜æˆ–å›¾è¡¨åˆ†æã€‚è¿˜æ²¡æœ‰å·¥ä½œèƒ½å¤Ÿåˆ©ç”¨å¤šæºMLLMä»»åŠ¡æ¥è¿›è¡Œç¨³å®šçš„å¼ºåŒ–å­¦ä¹ ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªç»Ÿä¸€çš„è§’åº¦æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†Mixed-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€è€Œç®€å•çš„æ¡†æ¶ï¼Œå®ƒåŒ…å«ä¸€ä¸ªæ··åˆå¥–åŠ±å‡½æ•°è®¾è®¡ï¼ˆMixed-Rewardï¼‰å’Œä¸€ä¸ªæ··åˆåè®­ç»ƒæ•°æ®é›†ï¼ˆMixed-45Kï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆè®¾è®¡ä¸€ä¸ªæ•°æ®å¼•æ“æ¥é€‰æ‹©é«˜è´¨é‡çš„æ ·ä¾‹æ¥æ„å»ºMixed-45Kåè®­ç»ƒæ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†Mixed-Rewardè®¾è®¡ï¼Œå®ƒåŒ…å«å„ç§LLMMä»»åŠ¡çš„å¥–åŠ±å‡½æ•°ã€‚ç‰¹åˆ«æ˜¯ï¼Œå®ƒæœ‰å››ç§ä¸åŒçš„å¥–åŠ±åŠŸèƒ½ï¼šé’ˆå¯¹äºŒè¿›åˆ¶ç­”æ¡ˆæˆ–å¤šé€‰é—®é¢˜çš„åŒ¹é…å¥–åŠ±ã€é’ˆå¯¹å›¾è¡¨æ„ŸçŸ¥æ•°æ®é›†çš„å›¾è¡¨å¥–åŠ±ã€é’ˆå¯¹æ¥åœ°é—®é¢˜çš„IoUå¥–åŠ±ï¼Œä»¥åŠé’ˆå¯¹é•¿æ–‡æœ¬å“åº”çš„å¼€æ”¾å¥–åŠ±ï¼Œå¦‚æ ‡é¢˜æ•°æ®é›†ã€‚ä¸ºäº†å¤„ç†å„ç§é•¿æ–‡æœ¬å†…å®¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¼€æ”¾å¥–åŠ±ï¼Œç§°ä¸ºåŒå‘æœ€å¤§å¹³å‡ç›¸ä¼¼æ€§ï¼ˆBMASï¼‰ï¼Œå®ƒé€šè¿‡ç”Ÿæˆå“åº”å’ŒçœŸå®æ ‡ç­¾ä¹‹é—´çš„åˆ†è¯å™¨åµŒå…¥åŒ¹é…æ¥å®ç°ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§MLLMsä¸Šçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬Qwen2.5-VLå’ŒIntern-VLåœ¨å„ç§è§„æ¨¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xushilin1/mixed-r1%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/xushilin1/mixed-r1ä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24164v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶Mixed-R1ï¼Œç”¨äºè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„ç¨³å®šæ€§é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…å«æ··åˆå¥–åŠ±å‡½æ•°è®¾è®¡ï¼ˆMixed-Rewardï¼‰å’Œæ··åˆåè®­ç»ƒæ•°æ®é›†ï¼ˆMixed-45Kï¼‰ã€‚é€šè¿‡æ•°æ®å¼•æ“ç­›é€‰é«˜è´¨é‡æ ·æœ¬æ„å»ºMixed-45Kæ•°æ®é›†ï¼Œå¹¶ä¸ºä¸åŒçš„LLMä»»åŠ¡è®¾è®¡ä¸åŒçš„å¥–åŠ±å‡½æ•°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§MLLMsä¸Šå‡æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ å±•ç°å‡ºæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨ç‰¹å®šæ–¹é¢çš„ä»»åŠ¡ï¼Œå¦‚æ¥åœ°ä»»åŠ¡ã€æ•°å­¦é—®é¢˜æˆ–å›¾è¡¨åˆ†æç­‰ã€‚</li>
<li>å°šæ— å·¥ä½œèƒ½å¤Ÿåˆ©ç”¨å¤šæºMLLMä»»åŠ¡è¿›è¡Œç¨³å®šçš„å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>æå‡ºMixed-R1æ¡†æ¶ï¼ŒåŒ…å«æ··åˆå¥–åŠ±å‡½æ•°å’Œæ··åˆåè®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>æ•°æ®å¼•æ“ç”¨äºç­›é€‰é«˜è´¨é‡æ ·æœ¬æ„å»ºMixed-45Kæ•°æ®é›†ã€‚</li>
<li>ä¸ºä¸åŒçš„MLLMä»»åŠ¡è®¾è®¡äº†ä¸åŒçš„å¥–åŠ±å‡½æ•°ï¼ŒåŒ…æ‹¬åŒ¹é…å¥–åŠ±ã€å›¾è¡¨å¥–åŠ±ã€IoUå¥–åŠ±å’Œå¼€æ”¾å¥–åŠ±ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¼€æ”¾å¥–åŠ±å‡½æ•°â€”â€”åŒå‘æœ€å¤§å¹³å‡ç›¸ä¼¼åº¦ï¼ˆBMASï¼‰ï¼Œç”¨äºå¤„ç†é•¿æ–‡æœ¬å†…å®¹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f4dfc90749855191d25bf1c06229f8d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d0533fb37156d26421deda955930be4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac3ff81a77e01e629acd89a75fdd4123.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f1a6826d5e788f7479fd23d3059879e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DSR-Bench-Evaluating-the-Structural-Reasoning-Abilities-of-LLMs-via-Data-Structures"><a href="#DSR-Bench-Evaluating-the-Structural-Reasoning-Abilities-of-LLMs-via-Data-Structures" class="headerlink" title="DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via   Data Structures"></a>DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via   Data Structures</h2><p><strong>Authors:Yu He, Yingxi Li, Colin White, Ellen Vitercik</strong></p>
<p>Large language models (LLMs) are increasingly deployed for real-world tasks that fundamentally involve data manipulation. A core requirement across these tasks is the ability to perform structural reasoningâ€“that is, to understand and reason about data relationships. For example, customer requests require a temporal ordering, which can be represented by data structures such as queues. However, existing benchmarks primarily focus on high-level, application-driven evaluations without isolating this fundamental capability. To address this gap, we introduce DSR-Bench, a novel benchmark evaluating LLMsâ€™ structural reasoning capabilities through data structures, which provide interpretable representations of data relationships. DSR-Bench includes 20 data structures, 35 operations, and 4,140 problem instances, organized hierarchically for fine-grained analysis of reasoning limitations. Our evaluation pipeline is fully automated and deterministic, eliminating subjective human or model-based judgments. Its synthetic nature also ensures scalability and minimizes data contamination risks. We benchmark nine state-of-the-art LLMs. Our analysis shows that instruction-tuned models struggle with basic multi-attribute and multi-hop reasoning. Furthermore, while reasoning-oriented models perform better, they remain fragile on complex and hybrid structures, with the best model achieving an average score of only 47% on the challenge subset. Crucially, models often perform poorly on multi-dimensional data and natural language task descriptions, highlighting a critical gap for real-world deployment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºæ¶‰åŠæ•°æ®æ“ä½œçš„çœŸå®ä¸–ç•Œä»»åŠ¡ã€‚è¿™äº›ä»»åŠ¡çš„æ ¸å¿ƒè¦æ±‚æ˜¯è¿›è¡Œç»“æ„æ¨ç†çš„èƒ½åŠ›ï¼Œå³ç†è§£å’Œæ¨ç†æ•°æ®å…³ç³»ã€‚ä¾‹å¦‚ï¼Œå®¢æˆ·è¯·æ±‚éœ€è¦æ—¶åºæ’åºï¼Œå¯ä»¥é€šè¿‡é˜Ÿåˆ—ç­‰æ•°æ®ç»“æ„æ¥è¡¨ç¤ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨é«˜çº§ã€åº”ç”¨é©±åŠ¨çš„è¯„ä»·ï¼Œè€Œæ²¡æœ‰å­¤ç«‹è¿™ç§åŸºæœ¬èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†DSR-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡æ•°æ®ç»“æ„è¯„ä¼°LLMçš„ç»“æ„æ¨ç†èƒ½åŠ›ï¼Œæä¾›æ•°æ®å…³ç³»çš„å¯è§£é‡Šè¡¨ç¤ºã€‚DSR-BenchåŒ…æ‹¬20ä¸ªæ•°æ®ç»“æ„ã€35ä¸ªæ“ä½œå’Œ4140ä¸ªé—®é¢˜å®ä¾‹ï¼ŒæŒ‰å±‚æ¬¡ç»“æ„ç»„ç»‡ï¼Œç”¨äºç²¾ç»†åˆ†ææ¨ç†å±€é™æ€§ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç®¡é“æ˜¯å®Œå…¨è‡ªåŠ¨åŒ–å’Œç¡®å®šçš„ï¼Œæ¶ˆé™¤äº†ä¸»è§‚äººä¸ºæˆ–æ¨¡å‹åŸºç¡€çš„åˆ¤æ–­ã€‚å…¶åˆæˆæ€§è´¨è¿˜ç¡®ä¿äº†å¯æ‰©å±•æ€§å¹¶é™ä½äº†æ•°æ®æ±¡æŸ“é£é™©ã€‚æˆ‘ä»¬å¯¹ä¹ä¸ªæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹åœ¨åŸºæœ¬çš„å¤šå±æ€§å’Œå¤šè·³æ¨ç†æ–¹é¢è¡¨ç°æŒ£æ‰ã€‚æ­¤å¤–ï¼Œè™½ç„¶ä»¥æ¨ç†ä¸ºå¯¼å‘çš„æ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œä½†åœ¨å¤æ‚å’Œæ··åˆç»“æ„ä¸Šä»ç„¶è„†å¼±ï¼Œæœ€ä½³æ¨¡å‹åœ¨æŒ‘æˆ˜å­é›†ä¸Šçš„å¹³å‡å¾—åˆ†ä»…ä¸º47%ã€‚å…³é”®çš„æ˜¯ï¼Œæ¨¡å‹åœ¨å¤šç»´æ•°æ®å’Œè‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°æ–¹é¢çš„è¡¨ç°å¾€å¾€ä¸ä½³ï¼Œè¿™çªå‡ºäº†åœ¨çœŸå®ä¸–ç•Œéƒ¨ç½²ä¸­çš„å…³é”®å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24069v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†æ¶‰åŠæ•°æ®æ“ä½œçš„çœŸå®ä»»åŠ¡æ—¶çš„æ ¸å¿ƒéœ€æ±‚ï¼Œå³è¿›è¡Œç»“æ„æ€§æ¨ç†çš„èƒ½åŠ›ã€‚é’ˆå¯¹è¿™ä¸€éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹è¯„ä¼°åŸºå‡†â€”â€”DSR-Benchï¼Œç”¨äºè¯„ä¼°LLMså¤„ç†æ•°æ®ç»“æ„çš„èƒ½åŠ›ï¼Œè¯¥åŸºå‡†é€šè¿‡æ•°æ®ç»“æ„æä¾›äº†æ•°æ®å…³ç³»çš„å¯è§£é‡Šè¡¨ç¤ºã€‚DSR-BenchåŒ…å«20ä¸ªæ•°æ®ç»“æ„ã€35ä¸ªæ“ä½œå’Œ4140ä¸ªé—®é¢˜å®ä¾‹ï¼Œå¹¶è¿›è¡Œäº†å±‚æ¬¡ç»“æ„ç»„ç»‡ï¼Œä»¥ç²¾ç»†åˆ†ææ¨ç†çš„å±€é™æ€§ã€‚è¯„ä¼°æµç¨‹å®Œå…¨è‡ªåŠ¨åŒ–ä¸”ç¡®å®šæ€§é«˜ï¼Œæ¶ˆé™¤äº†ä¸»è§‚äººä¸ºæˆ–æ¨¡å‹åˆ¤æ–­çš„å¹²æ‰°ã€‚é€šè¿‡å¯¹æ¯”ä¹ç§æœ€å…ˆè¿›çš„LLMsè¡¨ç°ï¼Œå‘ç°æŒ‡ä»¤å¾®è°ƒæ¨¡å‹åœ¨åŸºæœ¬å¤šå±æ€§å’Œå¤šè·³æ¨ç†æ–¹é¢é‡åˆ°å›°éš¾ï¼Œè€Œé¢å‘æ¨ç†çš„æ¨¡å‹è™½è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨å¤æ‚å’Œæ··åˆç»“æ„ä¸Šä»æ˜¾è„†å¼±ï¼Œæœ€ä½³æ¨¡å‹æŒ‘æˆ˜å­é›†çš„å¹³å‡å¾—åˆ†ä»…ä¸º47%ã€‚æ¨¡å‹åœ¨å¤šç»´åº¦æ•°æ®å’Œè‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°æ–¹é¢çš„è¡¨ç°ä¸ä½³ï¼Œå‡¸æ˜¾äº†å®é™…éƒ¨ç½²ä¸­çš„å…³é”®å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†çœŸå®ä»»åŠ¡æ—¶éœ€è¦å…·å¤‡ç»“æ„æ€§æ¨ç†èƒ½åŠ›ï¼Œè¿™æ¶‰åŠç†è§£å¹¶æ¨ç†æ•°æ®å…³ç³»ã€‚</li>
<li>ç°æœ‰è¯„ä¼°åŸºå‡†ä¸»è¦å…³æ³¨é«˜çº§ã€åº”ç”¨é©±åŠ¨çš„è¯„ä»·ï¼Œæœªé’ˆå¯¹ç»“æ„æ€§æ¨ç†èƒ½åŠ›è¿›è¡Œä¸“é—¨è¯„ä¼°ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹è¯„ä¼°åŸºå‡†DSR-Benchï¼Œæ—¨åœ¨è¯„ä¼°LLMså¤„ç†æ•°æ®ç»“æ„çš„èƒ½åŠ›ã€‚</li>
<li>DSR-BenchåŒ…å«ä¸°å¯Œçš„é—®é¢˜å®ä¾‹ï¼Œèƒ½å¤Ÿç²¾ç»†åˆ†ææ¨ç†çš„å±€é™æ€§ã€‚</li>
<li>è¯„ä¼°æµç¨‹è‡ªåŠ¨åŒ–ä¸”ç¡®å®šæ€§å¼ºï¼Œæ¶ˆé™¤ä¸»è§‚åˆ¤æ–­ï¼Œç¡®ä¿è¯„ä¼°å…¬æ­£æ€§ã€‚</li>
<li>ä¹ç§æœ€å…ˆè¿›çš„LLMsåœ¨ç»“æ„æ€§æ¨ç†æ–¹é¢å­˜åœ¨å·®è·ï¼Œè¡¨ç°åœ¨åŸºæœ¬å¤šå±æ€§å’Œå¤šè·³æ¨ç†æ–¹é¢é‡åˆ°å›°éš¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24069">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b6ce9e802d3963f5fbbbdc24bef4b3d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10cb66a901c2d0edfece7d67c1381383.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28d1ca5d38791ee0c0f2114a60f192f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5450150ec019815032c89e6679faca0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LlamaRL-A-Distributed-Asynchronous-Reinforcement-Learning-Framework-for-Efficient-Large-scale-LLM-Training"><a href="#LlamaRL-A-Distributed-Asynchronous-Reinforcement-Learning-Framework-for-Efficient-Large-scale-LLM-Training" class="headerlink" title="LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for   Efficient Large-scale LLM Training"></a>LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for   Efficient Large-scale LLM Training</h2><p><strong>Authors:Bo Wu, Sid Wang, Yunhao Tang, Jia Ding, Eryk Helenowski, Liang Tan, Tengyu Xu, Tushar Gowda, Zhengxing Chen, Chen Zhu, Xiaocheng Tang, Yundi Qian, Beibei Zhu, Rui Hou</strong></p>
<p>Reinforcement Learning (RL) has become the most effective post-training approach for improving the capabilities of Large Language Models (LLMs). In practice, because of the high demands on latency and memory, it is particularly challenging to develop an efficient RL framework that reliably manages policy models with hundreds to thousands of billions of parameters.   In this paper, we present LlamaRL, a fully distributed, asynchronous RL framework optimized for efficient training of large-scale LLMs with various model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a handful to thousands of devices. LlamaRL introduces a streamlined, single-controller architecture built entirely on native PyTorch, enabling modularity, ease of use, and seamless scalability to thousands of GPUs. We also provide a theoretical analysis of LlamaRLâ€™s efficiency, including a formal proof that its asynchronous design leads to strict RL speed-up. Empirically during the Llama 3 post-training, by leveraging best practices such as colocated model offloading, asynchronous off-policy training, and distributed direct memory access for weight synchronization, LlamaRL achieves significant efficiency gains â€“ up to 10.7x speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy model. Furthermore, the efficiency advantage continues to grow with increasing model scale, demonstrating the frameworkâ€™s suitability for future large-scale RL training. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½åŠ›æœ€æœ‰æ•ˆçš„åè®­ç»ƒæ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºå…¶å¯¹å»¶è¿Ÿå’Œå†…å­˜çš„æé«˜è¦æ±‚ï¼Œå¼€å‘ä¸€ä¸ªèƒ½å¤Ÿå¯é ç®¡ç†æ‹¥æœ‰æ•°ä¸‡äº¿å‚æ•°çš„ç­–ç•¥æ¨¡å‹çš„é«˜æ•ˆRLæ¡†æ¶æ˜¯ä¸€é¡¹ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24034v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LlamaRLï¼Œä¸€ä¸ªé’ˆå¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é«˜æ•ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ã€‚LlamaRLé‡‡ç”¨å…¨åˆ†å¸ƒå¼ã€å¼‚æ­¥è®¾è®¡ï¼Œæ”¯æŒä»å°‘æ•°åˆ°æ•°åƒä¸ªè®¾å¤‡çš„GPUé›†ç¾¤è®­ç»ƒã€‚å…¶ä¼˜åŒ–LLMçš„å®æˆ˜è®­ç»ƒæ•ˆæœæ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨å‚æ•°è§„æ¨¡æ›´å¤§çš„æ¨¡å‹ä¸Šä¼˜åŠ¿æ˜æ˜¾ã€‚LlamaRLåˆ©ç”¨PyTorchåŸç”Ÿç‰¹æ€§æ„å»ºå•ä¸€æ§åˆ¶å™¨æ¶æ„ï¼Œå®ç°æ¨¡å—åŒ–ã€æ˜“ç”¨æ€§å’Œæ— ç¼æ‰©å±•åˆ°æ•°åƒä¸ªGPUã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯éªŒè¯ï¼Œè¯æ˜äº†å…¶å¼‚æ­¥è®¾è®¡èƒ½æ˜¾è‘—æé«˜RLé€Ÿåº¦ã€‚åœ¨Llama 3çš„åè®­ç»ƒä¸­ï¼ŒLlamaRLé€šè¿‡æœ€ä½³å®è·µå®ç°äº†æ˜¾è‘—æ•ˆç‡æå‡ï¼Œç›¸è¾ƒäºDeepSpeed-Chatç±»ç³»ç»Ÿï¼Œå¯¹405Bå‚æ•°æ¨¡å‹çš„é€Ÿåº¦æå‡è¾¾10.7å€ï¼Œä¸”éšç€æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œæ•ˆç‡ä¼˜åŠ¿æŒç»­å¢é•¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LlamaRLæ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œæ”¯æŒå…¨åˆ†å¸ƒå¼å’Œå¼‚æ­¥è®­ç»ƒã€‚</li>
<li>LlamaRLå…·æœ‰ä¼˜åŒ–çš„æ¶æ„ï¼Œèƒ½åœ¨GPUé›†ç¾¤ä¸Šå®ç°é«˜æ•ˆè®­ç»ƒï¼Œæ”¯æŒä»å°‘æ•°åˆ°æ•°åƒä¸ªè®¾å¤‡çš„æ‰©å±•ã€‚</li>
<li>LlamaRLåˆ©ç”¨PyTorchåŸç”Ÿç‰¹æ€§æ„å»ºå•ä¸€æ§åˆ¶å™¨ï¼Œå®ç°æ¨¡å—åŒ–ã€æ˜“ç”¨æ€§å’Œæ— ç¼æ‰©å±•åˆ°æ•°åƒGPUã€‚</li>
<li>LlamaRLé€šè¿‡ç†è®ºåˆ†æå’Œå®è¯éªŒè¯è¯æ˜äº†å…¶å¼‚æ­¥è®¾è®¡èƒ½æ˜¾è‘—æé«˜å¼ºåŒ–å­¦ä¹ é€Ÿåº¦ã€‚</li>
<li>åœ¨Llama 3çš„åè®­ç»ƒä¸­ï¼ŒLlamaRLé€šè¿‡ä¸€ç³»åˆ—æœ€ä½³å®è·µå®ç°äº†æ˜¾è‘—æ•ˆç‡æå‡ã€‚</li>
<li>LlamaRLç›¸è¾ƒäºå…¶ä»–ç³»ç»Ÿï¼Œå¯¹405Bå‚æ•°æ¨¡å‹çš„é€Ÿåº¦æå‡è¾¾10.7å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-03f3389b50b5aa8b3673d7d9ed8afb61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-585cb18128e097a0f59cfd95a738acd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f9e0e263eec426aa675bf53c0f0f8b2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DINO-R1-Incentivizing-Reasoning-Capability-in-Vision-Foundation-Models"><a href="#DINO-R1-Incentivizing-Reasoning-Capability-in-Vision-Foundation-Models" class="headerlink" title="DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models"></a>DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models</h2><p><strong>Authors:Chenbin Pan, Wenbin He, Zhengzhong Tu, Liu Ren</strong></p>
<p>The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representation models like the DINO series. In this work, we propose \textbf{DINO-R1}, the first such attempt to incentivize visual in-context reasoning capabilities of vision foundation models using reinforcement learning. Specifically, DINO-R1 introduces \textbf{Group Relative Query Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly designed for query-based representation models, which computes query-level rewards based on group-normalized alignment quality. We also apply KL-regularization to stabilize the objectness distribution to reduce the training instability. This joint optimization enables dense and expressive supervision across queries while mitigating overfitting and distributional drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models that integrate a visual prompt encoder and a visual-guided query selection mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving strong generalization in both open-vocabulary and closed-set visual prompting scenarios. </p>
<blockquote>
<p>æœ€è¿‘å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1ï¼‰çš„æ¨ç†èƒ½åŠ›çš„ç ”ç©¶å…´è¶£æ¿€å¢ï¼Œé€šè¿‡åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒæ¡†æ¶ï¼ˆå¦‚ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ–¹æ³•ï¼‰å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™ç§æ¨ç†èƒ½åŠ›åœ¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆåŒ…æ‹¬DINOç³»åˆ—ç­‰è¡¨ç¤ºæ¨¡å‹ï¼‰ä¸­ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ï¼Œç”šè‡³æ˜æ˜¾ç¼ºå¤±ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¿€åŠ±è§†è§‰åŸºç¡€æ¨¡å‹çš„è§†è§‰ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›çš„é¦–ä¸ªå°è¯•â€”â€”<strong>DINO-R1</strong>ã€‚å…·ä½“æ¥è¯´ï¼ŒDINO-R1å¼•å…¥äº†<strong>ç¾¤ä½“ç›¸å¯¹æŸ¥è¯¢ä¼˜åŒ–ï¼ˆGRQOï¼‰</strong>ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹åŸºäºæŸ¥è¯¢çš„è¡¨ç¤ºæ¨¡å‹çš„æ–°å‹å¼ºåŒ–è®­ç»ƒç­–ç•¥ï¼Œå®ƒæ ¹æ®ç¾¤ä½“å½’ä¸€åŒ–çš„å¯¹é½è´¨é‡è®¡ç®—æŸ¥è¯¢çº§åˆ«çš„å¥–åŠ±ã€‚æˆ‘ä»¬è¿˜åº”ç”¨KLæ­£åˆ™åŒ–æ¥ç¨³å®šå¯¹è±¡æ€§åˆ†å¸ƒï¼Œä»¥å‡å°‘è®­ç»ƒçš„ä¸ç¨³å®šæ€§ã€‚è¿™ç§è”åˆä¼˜åŒ–å¯ä»¥åœ¨æŸ¥è¯¢ä¹‹é—´å®ç°å¯†é›†ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„ç›‘ç£ï¼ŒåŒæ—¶å‡è½»è¿‡æ‹Ÿåˆå’Œåˆ†å¸ƒæ¼‚ç§»ã€‚åœ¨åŸºäºGrounding-DINOçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ç³»åˆ—DINO-R1å®¶æ—æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹é›†æˆäº†è§†è§‰æç¤ºç¼–ç å™¨å’Œè§†è§‰å¼•å¯¼æŸ¥è¯¢é€‰æ‹©æœºåˆ¶ã€‚åœ¨COCOã€LVISå’ŒODinWä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDINO-R1æ˜¾è‘—ä¼˜äºç›‘ç£å¾®è°ƒåŸºçº¿ï¼Œåœ¨å¼€æ”¾è¯æ±‡è¡¨å’Œå°é—­é›†è§†è§‰æç¤ºåœºæ™¯ä¸­å®ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24025v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹å¦‚DeepSeek-R1çš„æ¨ç†èƒ½åŠ›å¾—åˆ°äº†å¹¿æ³›ç ”ç©¶ï¼Œå–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚ç„¶è€Œï¼Œè¿™ä¸€æˆåŠŸå°šæœªåº”ç”¨äºè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚DINOç³»åˆ—æ¨¡å‹ï¼‰ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæå‡ºäº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„æ¿€åŠ±æ–¹æ³•ï¼ˆDINO-R1ï¼‰ï¼Œä»¥å¢å¼ºè§†è§‰åŸºç¡€æ¨¡å‹çš„è§†è§‰ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒDINO-R1å¼•å…¥äº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒç­–ç•¥Group Relative Query Optimization (GRQO)ï¼Œé€‚ç”¨äºæŸ¥è¯¢å‹è¡¨ç¤ºæ¨¡å‹ã€‚åŒæ—¶åº”ç”¨KLæ­£åˆ™åŒ–ç¨³å®šå¯¹è±¡åˆ†å¸ƒï¼Œå‡å°‘è®­ç»ƒçš„ä¸ç¨³å®šæ€§ã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒè¯æ˜ï¼ŒDINO-R1åœ¨COCOã€LVISå’ŒODinWç­‰æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç›‘ç£å¾®è°ƒåŸºçº¿æ¨¡å‹ï¼Œåœ¨å¼€æ”¾è¯æ±‡è¡¨å’Œå°é—­é›†è§†è§‰æç¤ºåœºæ™¯ä¸­å®ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›é€šè¿‡å¼ºåŒ–å­¦ä¹ å¾—åˆ°äº†æ˜¾è‘—çš„æå‡ã€‚</li>
<li>ç›®å‰è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚DINOç³»åˆ—ï¼‰ç¼ºä¹ç±»ä¼¼çš„æ¨ç†èƒ½åŠ›ç ”ç©¶ã€‚</li>
<li>DINO-R1æ¨¡å‹é¦–æ¬¡å°è¯•ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¿€åŠ±è§†è§‰åŸºç¡€æ¨¡å‹çš„è§†è§‰ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚</li>
<li>DINO-R1å¼•å…¥äº†Group Relative Query Optimization (GRQO)è¿™ä¸€æ–°å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒç­–ç•¥ï¼Œé€‚ç”¨äºæŸ¥è¯¢å‹è¡¨ç¤ºæ¨¡å‹ã€‚</li>
<li>é€šè¿‡KLæ­£åˆ™åŒ–æ¥ç¨³å®šå¯¹è±¡åˆ†å¸ƒï¼Œä»¥å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šæ€§ã€‚</li>
<li>DINO-R1æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç›‘ç£å¾®è°ƒåŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24025">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1333ec1844d9d1050f5ed21d1d7b0e81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7c48eae20ab063a0e892b40e9d38aa8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca21d7ba3a7840ca70b75ff9fd8d2083.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2837543202367367086f583c64e665c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Measuring-Sycophancy-of-Language-Models-in-Multi-turn-Dialogues"><a href="#Measuring-Sycophancy-of-Language-Models-in-Multi-turn-Dialogues" class="headerlink" title="Measuring Sycophancy of Language Models in Multi-turn Dialogues"></a>Measuring Sycophancy of Language Models in Multi-turn Dialogues</h2><p><strong>Authors:Jiseung Hong, Grace Byun, Seungone Kim, Kai Shu</strong></p>
<p>Large Language Models (LLMs) are expected to provide helpful and harmless responses, yet they often exhibit sycophancyâ€“conforming to user beliefs regardless of factual accuracy or ethical soundness. Prior research on sycophancy has primarily focused on single-turn factual correctness, overlooking the dynamics of real-world interactions. In this work, we introduce SYCON Bench, a novel benchmark for evaluating sycophantic behavior in multi-turn, free-form conversational settings. Our benchmark measures how quickly a model conforms to the user (Turn of Flip) and how frequently it shifts its stance under sustained user pressure (Number of Flip). Applying SYCON Bench to 17 LLMs across three real-world scenarios, we find that sycophancy remains a prevalent failure mode. Our analysis shows that alignment tuning amplifies sycophantic behavior, whereas model scaling and reasoning optimization strengthen the modelâ€™s ability to resist undesirable user views. Reasoning models generally outperform instruction-tuned models but often fail when they over-index on logical exposition instead of directly addressing the userâ€™s underlying beliefs. Finally, we evaluate four additional prompting strategies and demonstrate that adopting a third-person perspective reduces sycophancy by up to 63.8% in debate scenario. We release our code and data at <a target="_blank" rel="noopener" href="https://github.com/JiseungHong/SYCON-Bench">https://github.com/JiseungHong/SYCON-Bench</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ¬åº”åœ¨é—®ç­”ç³»ç»Ÿä¸­ç»™å‡ºæœ‰ç›Šä¸”æ— ä¼¤å®³çš„ååº”ï¼Œç„¶è€Œå®ƒä»¬é€šå¸¸è¡¨ç°å‡ºä¸€ç§åªšä¿—è¡Œä¸ºï¼Œå³æ— è®ºäº‹å®å‡†ç¡®æ€§æˆ–é“å¾·åˆç†æ€§å¦‚ä½•ï¼Œéƒ½ä¼šè¿åˆç”¨æˆ·çš„ä¿¡å¿µã€‚ä¹‹å‰å…³äºåªšä¿—çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•è½®äº‹å®æ­£ç¡®æ€§ä¸Šï¼Œå¿½è§†äº†ç°å®äº’åŠ¨ä¸­çš„åŠ¨æ€å˜åŒ–ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†SYCON Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šè½®è‡ªç”±å½¢å¼å¯¹è¯è®¾ç½®ä¸­åªšä¿—è¡Œä¸ºçš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•è¡¡é‡çš„æ˜¯æ¨¡å‹è¿åˆç”¨æˆ·çš„é€Ÿåº¦ï¼ˆFlip Turnï¼‰ä»¥åŠåœ¨æŒç»­çš„ç”¨æˆ·å‹åŠ›ä¸‹æ”¹å˜ç«‹åœºçš„é¢‘ç‡ï¼ˆFlip Numberï¼‰ã€‚é€šè¿‡å¯¹ä¸‰ç§ç°å®åœºæ™¯ä¸­çš„17ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨SYCON BenchåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°åªšä¿—è¡Œä¸ºä»ç„¶æ˜¯ä¸€ç§æ™®éå­˜åœ¨çš„å¤±è´¥æ¨¡å¼ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå¯¹é½è°ƒæ•´æ”¾å¤§äº†åªšä¿—è¡Œä¸ºï¼Œè€Œæ¨¡å‹è§„æ¨¡å’Œæ¨ç†ä¼˜åŒ–åˆ™å¢å¼ºäº†æ¨¡å‹æŠµæŠ—ä¸è‰¯ç”¨æˆ·è§‚ç‚¹çš„èƒ½åŠ›ã€‚æ¨ç†æ¨¡å‹é€šå¸¸è¡¨ç°ä¼˜äºæŒ‡ä»¤è®­ç»ƒæ¨¡å‹ï¼Œä½†åœ¨è¿‡äºå¼ºè°ƒé€»è¾‘é˜è¿°è€Œéç›´æ¥åº”å¯¹ç”¨æˆ·åŸºæœ¬ä¿¡å¿µæ—¶å¾€å¾€ä¼šå¤±è´¥ã€‚æœ€åï¼Œæˆ‘ä»¬è¯„ä¼°äº†å››ç§é¢å¤–çš„æç¤ºç­–ç•¥ï¼Œå¹¶è¯æ˜é‡‡ç”¨ç¬¬ä¸‰äººç§°è§†è§’å¯ä»¥å‡å°‘è¾©è®ºåœºæ™¯ä¸­çš„åªšä¿—è¡Œä¸ºé«˜è¾¾63.8%ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/JiseungHong/SYCON-Bench">https://github.com/JiseungHong/SYCON-Bench</a>å…¬å¼€äº†æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23840v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯¹è¯ä¸­å¸¸è¡¨ç°å‡ºé¡ºåº”åŠ›ï¼ˆsycophancyï¼‰ï¼Œå³æ— è®ºäº‹å®å‡†ç¡®æ€§æˆ–é“å¾·åˆç†æ€§å¦‚ä½•ï¼Œéƒ½ä¼šè¿åˆç”¨æˆ·çš„ä¿¡å¿µã€‚è¿‡å»çš„ç ”ç©¶ä¸»è¦å…³æ³¨å•è½®å¯¹è¯ä¸­çš„äº‹å®æ­£ç¡®æ€§ï¼Œå¿½ç•¥äº†ç°å®äº’åŠ¨ä¸­çš„åŠ¨æ€å˜åŒ–ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°å‹è¯„ä¼°å·¥å…·SYCON Benchï¼Œç”¨äºè¯„ä¼°å¤šè½®è‡ªç”±å½¢å¼å¯¹è¯ä¸­çš„é¡ºåº”åŠ›è¡Œä¸ºã€‚è¯¥å·¥å…·è¡¡é‡äº†æ¨¡å‹è¿åˆç”¨æˆ·çš„é€Ÿåº¦å’Œé¢‘ç‡ã€‚å¯¹17ä¸ªLLMsçš„ä¸‰å¤§ç°å®åœºæ™¯åº”ç”¨SYCON Benchå‘ç°ï¼Œé¡ºåº”åŠ›ä»ç„¶æ˜¯ä¸€ç§æ™®éå­˜åœ¨çš„å¤±è´¥æ¨¡å¼ã€‚åˆ†ææ˜¾ç¤ºï¼Œè°ƒæ•´å¯¹é½æ”¾å¤§äº†é¡ºåº”åŠ›è¡Œä¸ºï¼Œè€Œæ¨¡å‹è§„æ¨¡å’Œæ¨ç†ä¼˜åŒ–å¢å¼ºäº†æŠµåˆ¶ä¸è‰¯ç”¨æˆ·è§‚ç‚¹çš„èƒ½åŠ›ã€‚æ¨ç†æ¨¡å‹é€šå¸¸è¡¨ç°ä¼˜äºæŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼Œä½†åœ¨è¿‡åº¦é‡è§†é€»è¾‘å±•ç¤ºè€Œéç›´æ¥è§£å†³ç”¨æˆ·åŸºç¡€ä¿¡å¿µæ—¶å¾€å¾€ä¼šå¤±è´¥ã€‚æœ€åï¼Œè¯„ä¼°äº†å››ç§é¢å¤–çš„æç¤ºç­–ç•¥ï¼Œå¹¶è¯æ˜é‡‡ç”¨ç¬¬ä¸‰äººç§°è§†è§’å¯ä»¥å‡å°‘è¾©è®ºåœºæ™¯ä¸­çš„é¡ºåº”åŠ›è¾¾63.8%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯¹è¯ä¸­å¸¸è¡¨ç°å‡ºé¡ºåº”åŠ›ï¼ˆsycophancyï¼‰ï¼Œè¿åˆç”¨æˆ·ä¿¡å¿µã€‚</li>
<li>SYCON Benchæ˜¯ä¸€ç§æ–°å‹è¯„ä¼°å·¥å…·ï¼Œç”¨äºè¡¡é‡LLMsåœ¨å¤šè½®å¯¹è¯ä¸­çš„é¡ºåº”åŠ›è¡Œä¸ºã€‚</li>
<li>é¡ºåº”åŠ›æ˜¯LLMsçš„ä¸€ç§æ™®éå¤±è´¥æ¨¡å¼ï¼Œä¸æ¨¡å‹çš„å¯¹é½è°ƒæ•´ã€è§„æ¨¡åŠæ¨ç†ä¼˜åŒ–æœ‰å…³ã€‚</li>
<li>æ¨ç†æ¨¡å‹åœ¨è§£å†³ç”¨æˆ·åŸºç¡€ä¿¡å¿µæ—¶è¡¨ç°å¤æ‚ï¼Œè¿‡åº¦é‡è§†é€»è¾‘å±•ç¤ºå¯èƒ½å¯¼è‡´å¤±è´¥ã€‚</li>
<li>é‡‡ç”¨ç¬¬ä¸‰äººç§°è§†è§’çš„æç¤ºç­–ç•¥èƒ½æœ‰æ•ˆå‡å°‘è¾©è®ºåœºæ™¯ä¸­çš„é¡ºåº”åŠ›ã€‚</li>
<li>SYCON Benchä»£ç å’Œæ•°æ®å·²å…¬å¼€å‘å¸ƒï¼Œä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
<li>LLMséœ€è¦è¿›ä¸€æ­¥æé«˜åœ¨å¯¹è¯ä¸­çš„è‡ªä¸»æ€§ï¼Œä»¥æ›´å¥½åœ°é€‚åº”ç°å®ä¸–ç•Œçš„å¤æ‚äº’åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23840">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25e9f59545360a71e8e0c7f46df22ff6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d31d26419e74a6e682a3e8522d7955e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7e7cfcd0a0ec687ff5a0d72db2ad48a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa5e95e0d4c3dba72ac8d59fb38fbf27.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-efdb253e837ee4f45da0d1bc3b402e30.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Jigsaw-R1-A-Study-of-Rule-based-Visual-Reinforcement-Learning-with-Jigsaw-Puzzles"><a href="#Jigsaw-R1-A-Study-of-Rule-based-Visual-Reinforcement-Learning-with-Jigsaw-Puzzles" class="headerlink" title="Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with   Jigsaw Puzzles"></a>Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with   Jigsaw Puzzles</h2><p><strong>Authors:Zifu Wang, Junyi Zhu, Bo Tang, Zhiyu Li, Feiyu Xiong, Jiaqian Yu, Matthew B. Blaschko</strong></p>
<p>The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL, using jigsaw puzzles as a structured experimental framework. Jigsaw puzzles offer inherent ground truth, adjustable difficulty, and demand complex decision-making, making them ideal for this study. Our research reveals several key findings: \textit{Firstly,} we find that MLLMs, initially performing near to random guessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. \textit{Secondly,} training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. \textit{Thirdly,} MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. \textit{Fourthly,} we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. \textit{Finally,} our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/zifuwanggg/Jigsaw-R1">https://github.com/zifuwanggg/Jigsaw-R1</a>. </p>
<blockquote>
<p>å°†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸ºæ–‡æœ¬é¢†åŸŸä»¥å¤–çš„é¢†åŸŸå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜å’Œå¯èƒ½çš„åå·®ï¼Œå°¤å…¶æ˜¯å¯¹äºæ„ŸçŸ¥å¯†é›†çš„ä»»åŠ¡ã€‚æœ¬æ–‡å…¨é¢ç ”ç©¶äº†åŸºäºè§„åˆ™çš„è§†è§‰å¼ºåŒ–å­¦ä¹ ï¼Œä»¥æ‹¼å›¾ä½œä¸ºç»“æ„åŒ–å®éªŒæ¡†æ¶ã€‚æ‹¼å›¾æä¾›äº†å›ºæœ‰çš„çœŸå®æƒ…å†µã€å¯è°ƒæ•´çš„éš¾åº¦å’Œå¤æ‚çš„å†³ç­–éœ€æ±‚ï¼Œä½¿å…¶æˆä¸ºè¿™é¡¹ç ”ç©¶çš„ç†æƒ³é€‰æ‹©ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†å‡ ä¸ªå…³é”®è§‚ç‚¹ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å‘ç°MLLMåœ¨æœ€ç®€å•çš„æ‹¼å›¾ä¸Šå¼€å§‹æ—¶è¿‘ä¹éšæœºçŒœæµ‹ï¼Œä½†é€šè¿‡å¾®è°ƒè¾¾åˆ°è¿‘ä¹å®Œç¾çš„å‡†ç¡®åº¦å¹¶æ¨å¹¿åˆ°å¤æ‚çš„æœªè§è¿‡çš„é…ç½®ã€‚å…¶æ¬¡ï¼Œåœ¨æ‹¼å›¾ä¸Šè®­ç»ƒå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–è§†è§‰ä»»åŠ¡ï¼Œå…¶æœ‰æ•ˆæ€§å–å†³äºç‰¹å®šçš„ä»»åŠ¡é…ç½®ã€‚ç¬¬ä¸‰ï¼ŒMLLMå¯ä»¥åœ¨æœ‰æˆ–æ²¡æœ‰æ˜¾å¼æ¨ç†çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ¨å¹¿ï¼Œå°½ç®¡å¼€æºæ¨¡å‹é€šå¸¸å€¾å‘äºç›´æ¥å›ç­”ã€‚å› æ­¤ï¼Œå³ä½¿ç»è¿‡é€æ­¥æ¨ç†çš„è®­ç»ƒï¼Œå®ƒä»¬ä¹Ÿå¯èƒ½ä¼šå¿½ç•¥å¾—å‡ºæœ€ç»ˆç­”æ¡ˆçš„æ€è€ƒè¿‡ç¨‹ã€‚ç¬¬å››ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å¤æ‚çš„æ¨ç†æ¨¡å¼ä¼¼ä¹æ˜¯é¢„å…ˆå­˜åœ¨çš„è€Œä¸æ˜¯çªå‘çš„ï¼Œå…¶é¢‘ç‡éšè®­ç»ƒå’Œä»»åŠ¡éš¾åº¦çš„å¢åŠ è€Œå¢åŠ ã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ æ¯”ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¡¨ç°å‡ºæ›´æœ‰æ•ˆçš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œåˆå§‹çš„SFTå†·å¯åŠ¨é˜¶æ®µå¯èƒ½ä¼šé˜»ç¢åç»­çš„RLä¼˜åŒ–ã€‚å°½ç®¡è¿™äº›è§‚å¯Ÿæ˜¯åŸºäºæ‹¼å›¾æ¸¸æˆçš„ï¼Œå¹¶å¯èƒ½åœ¨å…¶ä»–è§†è§‰ä»»åŠ¡ä¸­æœ‰æ‰€ä¸åŒï¼Œä½†æ­¤ç ”ç©¶ä¸ºé›†ä½“ç†è§£åŸºäºè§„åˆ™çš„è§†è§‰å¼ºåŒ–å­¦ä¹ åŠå…¶åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ½œåŠ›è¿™ä¸€æ›´å¤§çš„è°œé¢˜è´¡çŒ®äº†ä¸€å—æœ‰ä»·å€¼çš„æ‹¼å›¾ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/zifuwanggg/Jigsaw-R1">https://github.com/zifuwanggg/Jigsaw-R1</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23590v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„åº”ç”¨ï¼Œå¯¹äºæ„ŸçŸ¥å¯†é›†å‹ä»»åŠ¡ï¼Œé¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜å’Œæ½œåœ¨çš„åå·®ã€‚æœ¬æ–‡å…¨é¢ç ”ç©¶äº†åŸºäºè§„åˆ™çš„è§†è§‰RLï¼Œä»¥æ‹¼å›¾ä½œä¸ºç»“æ„åŒ–å®éªŒæ¡†æ¶è¿›è¡Œç ”ç©¶ã€‚æ‹¼å›¾æä¾›äº†å†…åœ¨çš„åœ°é¢çœŸå®æƒ…å†µã€å¯è°ƒæ•´çš„éš¾åº¦å’Œå¤æ‚çš„å†³ç­–éœ€æ±‚ï¼Œä½¿å…¶æˆä¸ºè¿™é¡¹ç ”ç©¶çš„ç†æƒ³é€‰æ‹©ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼šé¦–å…ˆï¼ŒMLLMsåœ¨æœ€ç®€å•çš„æ‹¼å›¾ä¸Šæœ€åˆè¡¨ç°æ¥è¿‘éšæœºçŒœæµ‹ï¼Œä½†é€šè¿‡å¾®è°ƒè¾¾åˆ°è¿‘ä¹å®Œç¾çš„å‡†ç¡®åº¦å¹¶æ¨å¹¿åˆ°å¤æ‚çš„æœªè§è¿‡çš„é…ç½®ã€‚å…¶æ¬¡ï¼Œåœ¨æ‹¼å›¾ä¸Šè®­ç»ƒå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–è§†è§‰ä»»åŠ¡ï¼Œå…¶æœ‰æ•ˆæ€§å–å†³äºç‰¹å®šçš„ä»»åŠ¡é…ç½®ã€‚ç¬¬ä¸‰ï¼ŒMLLMså¯ä»¥å­¦ä¹ å’Œæ¨å¹¿åˆ°æœ‰æ— æ˜ç¡®æ¨ç†çš„æƒ…å†µï¼Œå°½ç®¡å¼€æºæ¨¡å‹æ›´å–œæ¬¢ç›´æ¥å›ç­”ã€‚å› æ­¤ï¼Œå³ä½¿ç»è¿‡é€æ­¥æ¨ç†çš„è®­ç»ƒï¼Œå®ƒä»¬ä¹Ÿå¯èƒ½ä¼šå¿½ç•¥å¾—å‡ºæœ€ç»ˆç­”æ¡ˆçš„æ€è€ƒè¿‡ç¨‹ã€‚ç¬¬å››ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å¤æ‚çš„æ¨ç†æ¨¡å¼ä¼¼ä¹æ˜¯é¢„å…ˆå­˜åœ¨çš„ï¼Œè€Œä¸æ˜¯çªç„¶å‡ºç°çš„ï¼Œå®ƒä»¬çš„é¢‘ç‡éšç€è®­ç»ƒå’Œä»»åŠ¡éš¾åº¦çš„å¢åŠ è€Œå¢åŠ ã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜RLæ¯”ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¡¨ç°å‡ºæ›´æœ‰æ•ˆçš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œåˆå§‹çš„SFTå†·å¯åŠ¨é˜¶æ®µå¯èƒ½ä¼šé˜»ç¢åç»­çš„RLä¼˜åŒ–ã€‚è™½ç„¶è¿™äº›è§‚å¯Ÿæ˜¯åŸºäºæ‹¼å›¾è€Œå¾—å‡ºçš„ï¼Œå¹¶å¯èƒ½å› å…¶ä»–è§†è§‰ä»»åŠ¡è€Œå¼‚ï¼Œä½†æ­¤ç ”ç©¶å¯¹ç†è§£åŸºäºè§„åˆ™çš„è§†è§‰RLåŠå…¶åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ½œåŠ›åšå‡ºäº†æœ‰ä»·å€¼çš„è´¡çŒ®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>MLLMsåœ¨ç®€å•çš„æ‹¼å›¾ä¸Šæœ€åˆè¡¨ç°è¾ƒå·®ï¼Œä½†é€šè¿‡å¾®è°ƒå¯ä»¥å®ç°åœ¨å¤æ‚æœªè§é…ç½®ä¸Šçš„è¿‘ä¹å®Œç¾è¡¨ç°ã€‚</li>
<li>åœ¨æ‹¼å›¾ä¸Šè®­ç»ƒå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–è§†è§‰ä»»åŠ¡ï¼Œæœ‰æ•ˆæ€§å–å†³äºç‰¹å®šä»»åŠ¡é…ç½®ã€‚</li>
<li>MLLMså¯ä»¥é€‚åº”æœ‰æ— æ˜ç¡®æ¨ç†çš„æƒ…å†µï¼Œä½†å³ä½¿ç»è¿‡æ¨ç†è®­ç»ƒï¼Œä¹Ÿå¯èƒ½ä¼šå¿½ç•¥æ¨å¯¼æœ€ç»ˆç­”æ¡ˆçš„è¿‡ç¨‹ã€‚</li>
<li>å¤æ‚çš„æ¨ç†æ¨¡å¼ä¼¼ä¹æ˜¯é¢„å…ˆå­˜åœ¨çš„ï¼Œå…¶é¢‘ç‡éšè®­ç»ƒå’Œä»»åŠ¡éš¾åº¦çš„å¢åŠ è€Œå¢åŠ ã€‚</li>
<li>RLç›¸è¾ƒäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å±•ç°å‡ºæ›´æœ‰æ•ˆçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åˆå§‹çš„SFTå†·å¯åŠ¨é˜¶æ®µå¯èƒ½å¯¹åç»­çš„RLä¼˜åŒ–äº§ç”Ÿé˜»ç¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23590">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f98f332c26baafa1472fa145133fa1cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6181cfafaed65844ad9b0feffbe058e9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Qwen-Look-Again-Guiding-Vision-Language-Reasoning-Models-to-Re-attention-Visual-Information"><a href="#Qwen-Look-Again-Guiding-Vision-Language-Reasoning-Models-to-Re-attention-Visual-Information" class="headerlink" title="Qwen Look Again: Guiding Vision-Language Reasoning Models to   Re-attention Visual Information"></a>Qwen Look Again: Guiding Vision-Language Reasoning Models to   Re-attention Visual Information</h2><p><strong>Authors:Xu Chu, Xinrong Chen, Guanyu Wang, Zhijie Tan, Kui Huang, Wenyu Lv, Tong Mo, Weiping Li</strong></p>
<p>Inference time scaling drives extended reasoning to enhance the performance of Vision-Language Models (VLMs), thus forming powerful Vision-Language Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens, causing visual information to receive less attention and may trigger hallucinations. Although introducing text-only reflection processes shows promise in language models, we demonstrate that it is insufficient to suppress hallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain (Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a vision-text reflection process that guides the model to re-attention visual information during reasoning. We first propose a reinforcement learning method Balanced Reflective Policy Optimization (BRPO), which guides the model to decide when to generate vision-text reflection on its own and balance the number and length of reflections. Then, we formally prove that VLRMs lose attention to visual tokens as reasoning progresses, and demonstrate that supplementing visual information during reflection enhances visual attention. Therefore, during training and inference, Visual Token COPY and Visual Token ROUTE are introduced to force the model to re-attention visual information at the visual level, addressing the limitations of text-only reflection. Experiments on multiple visual QA datasets and hallucination metrics indicate that Qwen-LA achieves leading accuracy performance while reducing hallucinations. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/Liar406/Look_Again">https://github.com/Liar406/Look_Again</a> </p>
<blockquote>
<p>æ¨ç†æ—¶é—´ç¼©æ”¾é©±åŠ¨æ‰©å±•æ¨ç†ï¼Œä»¥æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ€§èƒ½ï¼Œä»è€Œå½¢æˆå¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨ç†æ¨¡å‹ï¼ˆVLRMsï¼‰ã€‚ç„¶è€Œï¼Œé•¿æ—¶é—´çš„æ¨ç†ä¼šç¨€é‡Šè§†è§‰æ ‡è®°ï¼Œå¯¼è‡´è§†è§‰ä¿¡æ¯å—åˆ°çš„å…³æ³¨å‡å°‘ï¼Œå¹¶å¯èƒ½å¼•å‘å¹»è§‰ã€‚è™½ç„¶åœ¨è¯­è¨€æ¨¡å‹ä¸­å¼•å…¥ä»…æ–‡æœ¬åå°„è¿‡ç¨‹æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„å‰æ™¯ï¼Œä½†æˆ‘ä»¬è¯æ˜å®ƒåœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æŠ‘åˆ¶å¹»è§‰çš„èƒ½åŠ›æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Qwen-LookAgainï¼ˆQwen-LAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„VLRMï¼Œæ—¨åœ¨é€šè¿‡èå…¥è§†è§‰æ–‡æœ¬åå°„è¿‡ç¨‹æ¥å‡è½»å¹»è§‰é—®é¢˜ï¼Œå¼•å¯¼æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡æ–°å…³æ³¨è§†è§‰ä¿¡æ¯ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•â€”â€”å¹³è¡¡åå°„ç­–ç•¥ä¼˜åŒ–ï¼ˆBRPOï¼‰ï¼Œå®ƒå¼•å¯¼æ¨¡å‹è‡ªä¸»å†³å®šä½•æ—¶ç”Ÿæˆè§†è§‰æ–‡æœ¬åå°„ï¼Œå¹¶å¹³è¡¡åå°„çš„æ•°é‡å’Œé•¿åº¦ã€‚ç„¶åï¼Œæˆ‘ä»¬æ­£å¼è¯æ˜äº†éšç€æ¨ç†çš„è¿›è¡Œï¼ŒVLRMså¯¹è§†è§‰æ ‡è®°çš„å…³æ³¨åº¦é€æ¸é™ä½ï¼Œå¹¶è¯æ˜åœ¨åå°„è¿‡ç¨‹ä¸­è¡¥å……è§†è§‰ä¿¡æ¯å¯ä»¥å¢å¼ºè§†è§‰å…³æ³¨åº¦ã€‚å› æ­¤ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§†è§‰æ ‡è®°å¤åˆ¶å’Œè§†è§‰æ ‡è®°è·¯ç”±ï¼Œä»¥å¼ºåˆ¶æ¨¡å‹åœ¨è§†è§‰å±‚é¢ä¸Šé‡æ–°å…³æ³¨è§†è§‰ä¿¡æ¯ï¼Œè§£å†³ä»…æ–‡æœ¬åå°„çš„å±€é™æ€§ã€‚åœ¨å¤šä¸ªè§†è§‰é—®ç­”æ•°æ®é›†å’Œå¹»è§‰æŒ‡æ ‡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒQwen-LAåœ¨ä¿æŒé¢†å…ˆå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå‡å°‘äº†å¹»è§‰ã€‚æˆ‘ä»¬çš„ä»£ç ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/Liar406/Look_Again">https://github.com/Liar406/Look_Again</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23558v2">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨è§†è§‰é—®ç­”æ¨¡å‹ä¸­ï¼Œé•¿æ—¶é—´çš„æ¨ç†è¿‡ç¨‹å¯èƒ½ä¼šå¯¼è‡´è§†è§‰ä¿¡æ¯çš„å¿½ç•¥å’Œå¼•å‘å¹»è§‰çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€æ¨ç†æ¨¡å‹Qwen-LookAgainï¼ˆQwen-LAï¼‰ï¼Œé€šè¿‡å¼•å…¥è§†è§‰æ–‡æœ¬åå°„è¿‡ç¨‹æ¥å¹³è¡¡è§†è§‰å’Œæ–‡æœ¬çš„æ³¨æ„åŠ›ã€‚ä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•æ¥æŒ‡å¯¼æ¨¡å‹è¿›è¡Œè‡ªé€‚åº”çš„è§†è§‰æ–‡æœ¬åå°„ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­åŠ å…¥äº†è§†è§‰æ ‡è®°å¤åˆ¶å’Œè§†è§‰æ ‡è®°è·¯ç”±ç­–ç•¥ï¼Œä»¥è§£å†³ä»…ä¾èµ–æ–‡æœ¬åå°„çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQwen-LAæ¨¡å‹åœ¨å¤šä¸ªè§†è§‰é—®ç­”æ•°æ®é›†ä¸Šå®ç°äº†é¢†å…ˆçš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶å‡å°‘äº†å¹»è§‰çš„äº§ç”Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•¿æ—¶é—´çš„æ¨ç†è¿‡ç¨‹å¯èƒ½å¯¼è‡´è§†è§‰ä¿¡æ¯çš„å¿½ç•¥å’Œå¼•å‘å¹»è§‰é—®é¢˜ã€‚</li>
<li>Qwen-LookAgainæ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰è¯­è¨€æ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡è§†è§‰æ–‡æœ¬åå°„è¿‡ç¨‹è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ è¢«ç”¨æ¥æŒ‡å¯¼æ¨¡å‹è¿›è¡Œè‡ªé€‚åº”çš„è§†è§‰æ–‡æœ¬åå°„å†³ç­–ã€‚</li>
<li>æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥äº†è§†è§‰æ ‡è®°å¤åˆ¶å’Œè§†è§‰æ ‡è®°è·¯ç”±ç­–ç•¥ï¼Œä»¥é‡æ–°å…³æ³¨è§†è§‰ä¿¡æ¯ã€‚</li>
<li>Qwen-LAæ¨¡å‹é€šè¿‡å¹³è¡¡è§†è§‰å’Œæ–‡æœ¬çš„æ³¨æ„åŠ›ï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒQwen-LAæ¨¡å‹åœ¨å¤šä¸ªè§†è§‰é—®ç­”æ•°æ®é›†ä¸Šå®ç°äº†é¢†å…ˆçš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-92bced9c2668af0ae0deb88aa8e1cf8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c738e1b1af5d75fb148d4cf266c4e7d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-91789ea98e732662bf88fea2cc4026ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f0489204046aed9fd9fb9b86bed0d83.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ScEdit-Script-based-Assessment-of-Knowledge-Editing"><a href="#ScEdit-Script-based-Assessment-of-Knowledge-Editing" class="headerlink" title="ScEdit: Script-based Assessment of Knowledge Editing"></a>ScEdit: Script-based Assessment of Knowledge Editing</h2><p><strong>Authors:Xinye Li, Zunwen Zheng, Qian Zhang, Dekai Zhuang, Jiabao Kang, Liyan Xu, Qingbin Liu, Xi Chen, Zhiying Tu, Dianhui Chu, Dianbo Sui</strong></p>
<p>Knowledge Editing (KE) has gained increasing attention, yet current KE tasks remain relatively simple. Under current evaluation frameworks, many editing methods achieve exceptionally high scores, sometimes nearing perfection. However, few studies integrate KE into real-world application scenarios (e.g., recent interest in LLM-as-agent). To support our analysis, we introduce a novel script-based benchmark â€“ ScEdit (Script-based Knowledge Editing Benchmark) â€“ which encompasses both counterfactual and temporal edits. We integrate token-level and text-level evaluation methods, comprehensively analyzing existing KE techniques. The benchmark extends traditional fact-based (â€œWhatâ€-type question) evaluation to action-based (â€œHowâ€-type question) evaluation. We observe that all KE methods exhibit a drop in performance on established metrics and face challenges on text-level metrics, indicating a challenging task. Our benchmark is available at <a target="_blank" rel="noopener" href="https://github.com/asdfo123/ScEdit">https://github.com/asdfo123/ScEdit</a>. </p>
<blockquote>
<p>çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œä½†å½“å‰çš„KEä»»åŠ¡ä»ç„¶ç›¸å¯¹ç®€å•ã€‚åœ¨ç°æœ‰çš„è¯„ä¼°æ¡†æ¶ä¸‹ï¼Œè®¸å¤šç¼–è¾‘æ–¹æ³•éƒ½å–å¾—äº†å¼‚å¸¸é«˜çš„åˆ†æ•°ï¼Œæœ‰æ—¶ç”šè‡³æ¥è¿‘å®Œç¾ã€‚ç„¶è€Œï¼Œå¾ˆå°‘æœ‰ç ”ç©¶å°†KEé›†æˆåˆ°ç°å®ä¸–ç•Œçš„åº”ç”¨åœºæ™¯ä¸­ï¼ˆä¾‹å¦‚ï¼Œæœ€è¿‘å¯¹LLM-as-agentçš„å…´è¶£ï¼‰ã€‚ä¸ºäº†æ”¯æŒæˆ‘ä»¬çš„åˆ†æï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºäºè„šæœ¬çš„åŸºå‡†æµ‹è¯•â€”â€”ScEditï¼ˆåŸºäºè„šæœ¬çš„çŸ¥è¯†ç¼–è¾‘åŸºå‡†æµ‹è¯•ï¼‰ï¼Œå®ƒæ¶µç›–äº†åäº‹å®å’Œæ—¶é—´ç¼–è¾‘ã€‚æˆ‘ä»¬æ•´åˆäº†ä»¤ç‰Œçº§åˆ«å’Œæ–‡æœ¬çº§åˆ«çš„è¯„ä¼°æ–¹æ³•ï¼Œå…¨é¢åˆ†æäº†ç°æœ‰çš„KEæŠ€æœ¯ã€‚è¯¥åŸºå‡†æµ‹è¯•å°†ä¼ ç»Ÿçš„åŸºäºäº‹å®ï¼ˆâ€œæ˜¯ä»€ä¹ˆâ€ç±»å‹é—®é¢˜ï¼‰çš„è¯„ä¼°æ‰©å±•åˆ°åŸºäºè¡ŒåŠ¨ï¼ˆâ€œå¦‚ä½•åšâ€ç±»å‹é—®é¢˜ï¼‰çš„è¯„ä¼°ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ‰€æœ‰KEæ–¹æ³•åœ¨æ—¢å®šæŒ‡æ ‡ä¸Šçš„è¡¨ç°éƒ½æœ‰æ‰€ä¸‹é™ï¼Œå¹¶åœ¨æ–‡æœ¬çº§åˆ«çš„æŒ‡æ ‡ä¸Šé¢ä¸´æŒ‘æˆ˜ï¼Œè¿™è¡¨æ˜ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åœ¨<a target="_blank" rel="noopener" href="https://github.com/asdfo123/ScEdit%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/asdfo123/ScEditä¸Šå¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23291v2">PDF</a> ACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰çš„å½“å‰çŠ¶å†µåŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚å°½ç®¡KEå·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œä½†ç°æœ‰ä»»åŠ¡ç›¸å¯¹ç®€å•ï¼Œè¯„ä¼°æ¡†æ¶ä¸‹çš„è®¸å¤šç¼–è¾‘æ–¹æ³•å¾—åˆ†æé«˜ï¼Œä½†å®é™…åº”ç”¨åœºæ™¯ä¸­çš„é›†æˆè¾ƒå°‘ã€‚ä¸ºæ”¯æŒåˆ†æï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªæ–°å‹è„šæœ¬åŸºå‡†æµ‹è¯•â€”â€”ScEditï¼Œå®ƒæ¶µç›–äº†åäº‹å®å’Œæ—¶åºç¼–è¾‘ã€‚è¯¥åŸºå‡†æµ‹è¯•å°†ä¼ ç»Ÿçš„åŸºäºäº‹å®çš„è¯„ä»·æ‰©å±•åˆ°äº†åŸºäºè¡ŒåŠ¨çš„è¯„ä»·ï¼Œå¯¹ç°æœ‰KEæŠ€æœ¯è¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼Œå¹¶å‘ç°æ‰€æœ‰KEæ–¹æ³•åœ¨æ–°åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½éƒ½æœ‰æ‰€ä¸‹é™ï¼Œé¢ä¸´æ–‡æœ¬å±‚é¢åº¦é‡çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰ä»»åŠ¡ç›¸å¯¹ç®€å•ï¼Œè¯„ä¼°æ¡†æ¶ä¸‹çš„ç¼–è¾‘æ–¹æ³•å¾—åˆ†æé«˜ã€‚</li>
<li>å®é™…åº”ç”¨åœºæ™¯ä¸­KEçš„é›†æˆè¾ƒå°‘ï¼Œå°¤å…¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼ˆLLM-as-agentï¼‰é¢†åŸŸã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°å‹çš„è„šæœ¬åŸºå‡†æµ‹è¯•â€”â€”ScEditï¼Œè¯¥æµ‹è¯•æ¶µç›–äº†åäº‹å®å’Œæ—¶åºç¼–è¾‘ã€‚</li>
<li>ScEditåŸºå‡†æµ‹è¯•é›†æˆäº†ä»¤ç‰Œçº§åˆ«å’Œæ–‡æœ¬çº§åˆ«çš„è¯„ä¼°æ–¹æ³•ï¼Œå…¨é¢åˆ†æç°æœ‰KEæŠ€æœ¯ã€‚</li>
<li>ScEditåŸºå‡†æµ‹è¯•ä»ä¼ ç»Ÿçš„åŸºäºäº‹å®çš„è¯„ä»·æ‰©å±•åˆ°äº†åŸºäºè¡ŒåŠ¨çš„è¯„ä»·ã€‚</li>
<li>åœ¨ScEditåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ‰€æœ‰KEæ–¹æ³•çš„æ€§èƒ½éƒ½æœ‰æ‰€ä¸‹é™ï¼Œé¢ä¸´æ–‡æœ¬çº§åˆ«åº¦é‡çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23291">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0aae5ec32865d2c68706768b471c2b3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12bc17cc76811fed42d8e3076fcffb01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98a0f6c04e3fd147931c67e8118f840c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8018e501f9f4afd405ebb05e9cf2bf44.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PBEBench-A-Multi-Step-Programming-by-Examples-Reasoning-Benchmark-inspired-by-Historical-Linguistics"><a href="#PBEBench-A-Multi-Step-Programming-by-Examples-Reasoning-Benchmark-inspired-by-Historical-Linguistics" class="headerlink" title="PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark   inspired by Historical Linguistics"></a>PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark   inspired by Historical Linguistics</h2><p><strong>Authors:Atharva Naik, Darsh Agrawal, Manav Kapadnis, Yuwei An, Yash Mathur, Carolyn Rose, David Mortensen</strong></p>
<p>Recently, long chain of thought (LCoT), Large Language Models (LLMs), have taken the machine learning world by storm with their breathtaking reasoning capabilities. However, are the abstract reasoning abilities of these models general enough for problems of practical importance? Unlike past work, which has focused mainly on math, coding, and data wrangling, we focus on a historical linguistics-inspired inductive reasoning problem, formulated as Programming by Examples. We develop a fully automated pipeline for dynamically generating a benchmark for this task with controllable difficulty in order to tackle scalability and contamination issues to which many reasoning benchmarks are subject. Using our pipeline, we generate a test set with nearly 1k instances that is challenging for all state-of-the-art reasoning LLMs, with the best model (Claude-3.7-Sonnet) achieving a mere 54% pass rate, demonstrating that LCoT LLMs still struggle with a class or reasoning that is ubiquitous in historical linguistics as well as many other domains. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œé•¿é“¾æ€ç»´ï¼ˆLong Chain of Thoughtï¼Œç®€ç§°LCoTï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelsï¼Œç®€ç§°LLMï¼‰ä»¥å…¶æƒŠäººçš„æ¨ç†èƒ½åŠ›å¸­å·äº†æœºå™¨å­¦ä¹ é¢†åŸŸã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„æŠ½è±¡æ¨ç†èƒ½åŠ›å¯¹äºå®é™…åº”ç”¨ä¸­é‡è¦çš„é—®é¢˜æ˜¯å¦è¶³å¤Ÿé€šç”¨ï¼Ÿä¸è¿‡å»çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ•°å­¦ã€ç¼–ç¨‹å’Œæ•°æ®å¤„ç†ä¸åŒï¼Œæˆ‘ä»¬å…³æ³¨çš„æ˜¯ä¸€ä¸ªå—å†å²è¯­è¨€å­¦å¯å‘çš„å½’çº³æ¨ç†é—®é¢˜ï¼Œå®ƒè¢«åˆ¶å®šä¸ºç¤ºä¾‹ç¼–ç¨‹ã€‚ä¸ºäº†è§£å†³è®¸å¤šæ¨ç†åŸºå‡†æµ‹è¯•æ‰€é¢ä¸´çš„æ‰©å±•æ€§å’Œæ±¡æŸ“é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå…¨è‡ªåŠ¨ç®¡é“ï¼Œç”¨äºåŠ¨æ€ç”Ÿæˆæ­¤ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶å¯æ§åˆ¶å…¶éš¾åº¦ã€‚ä½¿ç”¨æˆ‘ä»¬çš„ç®¡é“ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«è¿‘ä¸€åƒä¸ªå®ä¾‹çš„æµ‹è¯•é›†ï¼Œå¯¹æ‰€æœ‰æœ€å…ˆè¿›çš„æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯´éƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ€å¥½çš„æ¨¡å‹ï¼ˆClaude-3.7-Sonnetï¼‰é€šè¿‡ç‡ä»…ä¸º54%ï¼Œè¿™è¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å†å²è¯­è¨€å­¦ä»¥åŠå…¶ä»–è®¸å¤šé¢†åŸŸä¸­æ™®éå­˜åœ¨çš„æŸç§æ¨ç†é—®é¢˜æ—¶ä»ç„¶é¢ä¸´å›°éš¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23126v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é•¿é“¾æ€ç»´ï¼ˆLCoTï¼‰å±•ç°å‡ºæƒŠäººçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¯¹äºå®é™…åº”ç”¨ä¸­çš„éš¾é¢˜ï¼Œå…¶æŠ½è±¡æ¨ç†èƒ½åŠ›æ˜¯å¦è¶³å¤Ÿé€šç”¨å°šå¾…æ¢è®¨ã€‚ä¸åŒäºè¿‡å»ä¸“æ³¨äºæ•°å­¦ã€ç¼–ç¨‹å’Œæ•°æ®å¤„ç†çš„ç ”ç©¶ï¼Œæœ¬ç ”ç©¶èšç„¦äºå—å†å²è¯­è¨€å­¦å¯å‘çš„å½’çº³æ¨ç†é—®é¢˜ï¼Œå¹¶å°†å…¶åˆ¶å®šä¸ºç¼–ç¨‹èŒƒä¾‹ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªåŠ¨æ€ç”Ÿæˆæ­¤ç±»ä»»åŠ¡åŸºå‡†æµ‹è¯•çš„å…¨è‡ªåŠ¨åŒ–ç®¡é“ï¼Œä»¥è§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•é¢ä¸´çš„æ‰©å±•æ€§å’Œæ±¡æŸ“é—®é¢˜ã€‚åˆ©ç”¨è¯¥ç®¡é“ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«è¿‘ä¸€åƒä¸ªå®ä¾‹çš„æµ‹è¯•é›†ï¼Œå¯¹å½“å‰æœ€å‰æ²¿çš„æ¨ç†LLMsæ„æˆäº†æŒ‘æˆ˜ï¼Œå…¶ä¸­æœ€å¥½çš„æ¨¡å‹ï¼ˆClaude-3.7-Sonnetï¼‰é€šè¿‡ç‡ä»…ä¸º54%ï¼Œè¡¨æ˜LCoT LLMsåœ¨å†å²è¯­è¨€å­¦ä»¥åŠå…¶ä»–è®¸å¤šé¢†åŸŸä¸­ä»é¢ä¸´æ™®éçš„æ¨ç†éš¾é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é•¿é“¾æ€ç»´ï¼ˆLCoTï¼‰å…·æœ‰å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´é€šç”¨æ€§æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶å…³æ³¨å†å²è¯­è¨€å­¦å¯å‘çš„å½’çº³æ¨ç†é—®é¢˜ï¼Œä»¥ç¼–ç¨‹èŒƒä¾‹çš„å½¢å¼å‘ˆç°ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“ï¼Œç”¨äºç”Ÿæˆå…·æœ‰å¯æ§éš¾åº¦çš„ä»»åŠ¡åŸºå‡†æµ‹è¯•ï¼Œè§£å†³å¯æ‰©å±•æ€§å’Œæ±¡æŸ“é—®é¢˜ã€‚</li>
<li>æµ‹è¯•é›†åŒ…å«è¿‘1000ä¸ªå®ä¾‹ï¼Œå¯¹ç°æœ‰æœ€å‰æ²¿çš„æ¨ç†LLMsæ„æˆæŒ‘æˆ˜ã€‚</li>
<li>æœ€å¥½çš„æ¨¡å‹ï¼ˆClaude-3.7-Sonnetï¼‰å®Œæˆç‡ä»…ä¸º54%ï¼Œè¡¨æ˜ä»å­˜åœ¨æ™®éæ¨ç†éš¾é¢˜ã€‚</li>
<li>LCoT LLMsåœ¨å†å²è¯­è¨€å­¦ä»¥åŠå…¶ä»–é¢†åŸŸçš„åº”ç”¨ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†LLMsåœ¨è§£å†³é€šç”¨æ€§é—®é¢˜ä¸Šçš„ä¸è¶³ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23126">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d64a7b737dd7625e2e3e9188128d59b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3c87b391a69e295c3a25bfd1aaffae2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ER-REASON-A-Benchmark-Dataset-for-LLM-Based-Clinical-Reasoning-in-the-Emergency-Room"><a href="#ER-REASON-A-Benchmark-Dataset-for-LLM-Based-Clinical-Reasoning-in-the-Emergency-Room" class="headerlink" title="ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the   Emergency Room"></a>ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the   Emergency Room</h2><p><strong>Authors:Nikita Mehandru, Niloufar Golchini, David Bamman, Travis Zack, Melanie F. Molina, Ahmed Alaa</strong></p>
<p>Large language models (LLMs) have been extensively evaluated on medical question answering tasks based on licensing exams. However, real-world evaluations often depend on costly human annotators, and existing benchmarks tend to focus on isolated tasks that rarely capture the clinical reasoning or full workflow underlying medical decisions. In this paper, we introduce ER-Reason, a benchmark designed to evaluate LLM-based clinical reasoning and decision-making in the emergency room (ER)â€“a high-stakes setting where clinicians make rapid, consequential decisions across diverse patient presentations and medical specialties under time pressure. ER-Reason includes data from 3,984 patients, encompassing 25,174 de-identified longitudinal clinical notes spanning discharge summaries, progress notes, history and physical exams, consults, echocardiography reports, imaging notes, and ER provider documentation. The benchmark includes evaluation tasks that span key stages of the ER workflow: triage intake, initial assessment, treatment selection, disposition planning, and final diagnosisâ€“each structured to reflect core clinical reasoning processes such as differential diagnosis via rule-out reasoning. We also collected 72 full physician-authored rationales explaining reasoning processes that mimic the teaching process used in residency training, and are typically absent from ER documentation. Evaluations of state-of-the-art LLMs on ER-Reason reveal a gap between LLM-generated and clinician-authored clinical reasoning for ER decisions, highlighting the need for future research to bridge this divide. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŸºäºæ‰§ä¸šèµ„æ ¼è€ƒè¯•çš„åŒ»ç–—é—®é¢˜å›ç­”ä»»åŠ¡ä¸­å·²å¾—åˆ°å¹¿æ³›è¯„ä¼°ã€‚ç„¶è€Œï¼ŒçœŸå®ä¸–ç•Œçš„è¯„ä¼°é€šå¸¸ä¾èµ–äºæˆæœ¬é«˜æ˜‚çš„äººå·¥æ ‡æ³¨è€…ï¼Œè€Œä¸”ç°æœ‰çš„åŸºå‡†æµ‹è¯•å¾€å¾€ä¾§é‡äºå¾ˆå°‘èƒ½æ•æ‰åˆ°ä¸´åºŠæ¨ç†æˆ–åŒ»ç–—å†³ç­–èƒŒåå®Œæ•´å·¥ä½œæµçš„å­¤ç«‹ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ER-Reasonï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°åŸºäºLLMçš„ä¸´åºŠæ¨ç†å’Œæ€¥è¯Šç§‘ï¼ˆERï¼‰å†³ç­–åˆ¶å®šçš„åŸºå‡†æµ‹è¯•ã€‚æ€¥è¯Šç§‘æ˜¯ä¸€ä¸ªé«˜é£é™©çš„ç¯å¢ƒï¼ŒåŒ»ç”Ÿéœ€è¦åœ¨æ—¶é—´å‹åŠ›ä¸‹ï¼Œå¯¹ä¸åŒçš„æ‚£è€…è¡¨ç°å’ŒåŒ»å­¦ä¸“ç§‘åšå‡ºè¿…é€Ÿä¸”é‡è¦çš„å†³ç­–ã€‚ER-ReasonåŒ…å«äº†3,984åæ‚£è€…çš„æ•°æ®ï¼Œæ¶µç›–25,174ä»½å»æ ‡è¯†åŒ–çš„çºµå‘ä¸´åºŠç¬”è®°ï¼ŒåŒ…æ‹¬å‡ºé™¢æ€»ç»“ã€ç—…æƒ…è®°å½•ã€ç—…å²ä¸ä½“æ£€ã€ä¼šè¯Šã€è¶…å£°å¿ƒåŠ¨å›¾æŠ¥å‘Šã€å½±åƒè®°å½•ä»¥åŠæ€¥è¯Šç§‘åŒ»ç”Ÿçš„æ–‡æ¡£ã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†æ€¥è¯Šç§‘å·¥ä½œçš„ä¸»è¦é˜¶æ®µï¼šåˆ†è¯Šæ¥å¾…ã€åˆæ­¥è¯„ä¼°ã€æ²»ç–—é€‰æ‹©ã€å¤„ç½®è®¡åˆ’ä»¥åŠæœ€ç»ˆè¯Šæ–­ã€‚æ¯ä¸ªé˜¶æ®µçš„ç»“æ„éƒ½åæ˜ äº†æ ¸å¿ƒçš„ä¸´åºŠæ¨ç†è¿‡ç¨‹ï¼Œå¦‚é€šè¿‡æ’é™¤æ³•è¿›è¡Œçš„é‰´åˆ«è¯Šæ–­ã€‚æˆ‘ä»¬è¿˜æ”¶é›†äº†72ä»½åŒ»ç”Ÿæ’°å†™çš„å®Œæ•´æ¨ç†è¿‡ç¨‹ï¼Œè¿™äº›æ¨ç†è¿‡ç¨‹æ¨¡ä»¿äº†ä½é™¢åŸ¹è®­ä¸­ä½¿ç”¨çš„æ•™å­¦è¿‡ç¨‹ï¼Œé€šå¸¸ä¸ä¼šå‡ºç°åœ¨æ€¥è¯Šæ–‡æ¡£è®°å½•ä¸­ã€‚å¯¹æœ€æ–°LLMåœ¨ER-Reasonä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒLLMç”Ÿæˆçš„åŒ»ç–—æ€¥è¯Šå†³ç­–ä¸´åºŠæ¨ç†ä¸åŒ»ç”Ÿæ’°å†™çš„ä¸´åºŠæ¨ç†ä¹‹é—´å­˜åœ¨å·®è·ï¼Œè¿™çªæ˜¾äº†æœªæ¥ç ”ç©¶éœ€è¦å¼¥è¡¥è¿™ä¸€é¸¿æ²Ÿçš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22919v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŸºäºæ‰§ç…§è€ƒè¯•çš„å†…ç§‘åŒ»ç”Ÿé—®ç­”ä»»åŠ¡ä¸­è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚ç„¶è€Œï¼ŒçœŸå®ä¸–ç•Œçš„è¯„ä¼°å¸¸å¸¸ä¾èµ–äºæ˜‚è´µçš„ä¸“ä¸šæ³¨é‡Šå‘˜ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•å€¾å‘äºé›†ä¸­äºå­¤ç«‹çš„ä»»åŠ¡ï¼Œå¾ˆå°‘æ•æ‰åˆ°åŒ»å­¦å†³ç­–èƒŒåçš„ä¸´åºŠæ¨ç†æˆ–å®Œæ•´å·¥ä½œæµç¨‹ã€‚æœ¬æ–‡ä»‹ç»äº†ER-Reasonï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€¥è¯Šå®¤ä¸´åºŠæ¨ç†å’Œå†³ç­–åˆ¶å®šçš„åŸºå‡†æµ‹è¯•ã€‚ER-ReasonåŒ…å«æ¥è‡ª3984åæ‚£è€…çš„æ•°æ®ï¼Œæ¶µç›–äº†è¶…è¿‡è®°å½•çš„çºµå‘ä¸´åºŠç¬”è®°çš„25ï¼Œå…¶ä¸­åŒ…æ‹¬å‡ºé™¢æ‘˜è¦ã€è¿›å±•è®°å½•ã€ç—…å²å’Œä½“æ£€ã€ä¼šè¯ŠæŠ¥å‘Šç­‰ã€‚è¯„ä¼°æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ER-Reasonä¸Šçš„è¡¨ç°æ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä¸´åºŠæ¨ç†ä¸æ€¥è¯Šå®¤å†³ç­–ä¹‹é—´çš„å·®å¼‚ï¼Œå¼ºè°ƒäº†æœªæ¥éœ€è¦æ›´å¤šçš„ç ”ç©¶æ¥å¼¥è¡¥è¿™ä¸€é¸¿æ²Ÿã€‚è¿™ä¸ªå·®è·æŒ‡å‡ºäº†LLMæ¨¡å‹åœ¨æœªæ¥çš„ç ”ç©¶å’Œå¼€å‘æ–¹é¢éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ä»¥æ»¡è¶³ä¸´åºŠå†³ç­–çš„éœ€æ±‚ã€‚åŒæ—¶ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªå€¼å¾—æ·±å…¥æ¢è®¨çš„é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œç¯å¢ƒä¸­è¿›è¡Œå¤§è§„æ¨¡è¯„ä¼°å’ŒéªŒè¯çš„é‡è¦æ€§æ–¹é¢ã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥è®ºæ–‡å¯¹åŒ»å­¦AIçš„è¿›ä¸€æ­¥å‘å±•æä¾›äº†æ–°çš„è§è§£å’ŒæŒ‘æˆ˜æ€§æ€è€ƒã€‚æ­¤ç ”ç©¶çš„çªç ´æ€§çš„æ–°æ–¹å‘å’Œæ–°è§’åº¦æ¨åŠ¨äº†æˆ‘ä»¬å¯¹è¿™ä¸€é¢†åŸŸçš„å‘å±•å’Œæ·±å…¥ç†è§£çš„æœŸæœ›å’Œæ­¥ä¼ã€‚è®ºæ–‡ç»™å‡ºäº†æ˜ç¡®çš„è§‚å¯Ÿå’Œæ¯”è¾ƒæ¨¡å‹ç”Ÿæˆçš„å‡è®¾ç—…ä¾‹ä¸ç°å®ä¸­çš„åŒ»å­¦å®è·µçš„å®è¯å·®è·çš„è§£å†³æ–¹æ¡ˆå’Œå®è·µæ­¥éª¤ã€‚<strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—é—®ç­”ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œä½†çœŸå®ä¸–ç•Œè¯„ä¼°ä¾èµ–äºæ˜‚è´µçš„ä¸“ä¸šæ³¨é‡Šå‘˜ï¼Œä¸”ç°æœ‰åŸºå‡†æµ‹è¯•é›†ä¸­åœ¨å­¤ç«‹ä»»åŠ¡ä¸Šï¼Œæ— æ³•å……åˆ†åæ˜ åŒ»å­¦å†³ç­–èƒŒåçš„ä¸´åºŠæ¨ç†æˆ–å®Œæ•´å·¥ä½œæµç¨‹ã€‚</li>
<li>ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ER-Reasonï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨æ€¥è¯Šå®¤ä¸´åºŠæ¨ç†å’Œå†³ç­–åˆ¶å®šæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ER-ReasonåŒ…å«æ¥è‡ªä¸åŒåŒ»ç–—ç¯èŠ‚çš„çœŸå®æ‚£è€…æ•°æ®ï¼Œå¹¶è®¾è®¡äº†å¤šä¸ªè¯„ä»·ä»»åŠ¡ä»¥åæ˜ æ ¸å¿ƒä¸´åºŠæ¨ç†è¿‡ç¨‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22919">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ebeda22d237cbc21c73c9e6381fa0f10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a8593df6935ba4c3823a48fb685a116.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a40dac45ee305086031a035ceead853.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6f600515fe71cd2f943044e1d012590.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Revisiting-Group-Relative-Policy-Optimization-Insights-into-On-Policy-and-Off-Policy-Training"><a href="#Revisiting-Group-Relative-Policy-Optimization-Insights-into-On-Policy-and-Off-Policy-Training" class="headerlink" title="Revisiting Group Relative Policy Optimization: Insights into On-Policy   and Off-Policy Training"></a>Revisiting Group Relative Policy Optimization: Insights into On-Policy   and Off-Policy Training</h2><p><strong>Authors:Youssef Mroueh, Nicolas Dupuis, Brian Belgodere, Apoorva Nitsure, Mattia Rigotti, Kristjan Greenewald, Jiri Navratil, Jerret Ross, Jesus Rios</strong></p>
<p>We revisit Group Relative Policy Optimization (GRPO) in both on-policy and off-policy optimization regimes. Our motivation comes from recent work on off-policy Proximal Policy Optimization (PPO), which improves training stability, sampling efficiency, and memory usage. In addition, a recent analysis of GRPO suggests that estimating the advantage function with off-policy samples could be beneficial. Building on these observations, we adapt GRPO to the off-policy setting. We show that both on-policy and off-policy GRPO objectives yield an improvement in the reward. This result motivates the use of clipped surrogate objectives in the off-policy version of GRPO. We then compare the empirical performance of reinforcement learning with verifiable rewards in post-training using both GRPO variants. Our results show that off-policy GRPO either significantly outperforms or performs on par with its on-policy counterpart. </p>
<blockquote>
<p>æˆ‘ä»¬é‡æ–°ç ”ç©¶äº†Group Relative Policy Optimizationï¼ˆGRPOï¼‰åœ¨on-policyå’Œoff-policyä¼˜åŒ–åˆ¶åº¦ä¸‹çš„åº”ç”¨ã€‚æˆ‘ä»¬çš„çµæ„Ÿæ¥è‡ªäºè¿‘æœŸå…³äºoff-policyè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„ç ”ç©¶ï¼Œå®ƒæé«˜äº†è®­ç»ƒç¨³å®šæ€§ã€é‡‡æ ·æ•ˆç‡å’Œå†…å­˜ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œæœ€è¿‘å¯¹GRPOçš„åˆ†æè¡¨æ˜ï¼Œåˆ©ç”¨off-policyæ ·æœ¬ä¼°è®¡ä¼˜åŠ¿å‡½æ•°å¯èƒ½æ˜¯æœ‰ç›Šçš„ã€‚åŸºäºè¿™äº›è§‚å¯Ÿï¼Œæˆ‘ä»¬å°†GRPOé€‚åº”äºoff-policyç¯å¢ƒã€‚æˆ‘ä»¬è¯æ˜äº†on-policyå’Œoff-policy GRPOç›®æ ‡éƒ½èƒ½æé«˜å¥–åŠ±ã€‚è¿™ä¸€ç»“æœä¿ƒä½¿æˆ‘ä»¬åœ¨GRPOçš„off-policyç‰ˆæœ¬ä¸­ä½¿ç”¨è£å‰ªçš„æ›¿ä»£ç›®æ ‡ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸¤ç§GRPOå˜ä½“ï¼Œåœ¨å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ä¸­å¯¹è®­ç»ƒåçš„ç»éªŒæ€§èƒ½è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œoff-policy GRPOè¦ä¹ˆæ˜¾è‘—ä¼˜äºå…¶on-policyç‰ˆæœ¬ï¼Œè¦ä¹ˆä¸å…¶è¡¨ç°ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22257v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨åŸºäºç­–ç•¥å’ŒåŸºäºä»·å€¼çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä¸­é‡æ–°ç ”ç©¶äº†Group Relative Policy Optimizationï¼ˆGRPOï¼‰ã€‚å—æœ€è¿‘å…³äºPPOå·¥ä½œçš„å¯å‘ï¼Œæˆ‘ä»¬å°†å…¶é€‚åº”åˆ°off-policyç¯å¢ƒä¸­ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨åŸºäºç­–ç•¥è¿˜æ˜¯åŸºäºä»·å€¼çš„ä¼˜åŒ–ä¸­ï¼ŒGRPOéƒ½èƒ½æé«˜å¥–åŠ±ã€‚å› æ­¤ï¼Œåœ¨GRPOçš„off-policyç‰ˆæœ¬ä¸­ï¼Œä½¿ç”¨å‰ªè¾‘çš„æ›¿ä»£ç›®æ ‡æ˜¯æœ‰ç›Šçš„ã€‚æ¯”è¾ƒç»“æœæ˜¾ç¤ºï¼Œoff-policy GRPOæ˜¾è‘—ä¼˜äºæˆ–ä¸åŸºäºç­–ç•¥çš„GRPOè¡¨ç°ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†Group Relative Policy Optimizationï¼ˆGRPOï¼‰åœ¨åŸºäºç­–ç•¥å’ŒåŸºäºä»·å€¼çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä¸­çš„åº”ç”¨ã€‚</li>
<li>å—åˆ°PPOå·¥ä½œçš„å¯å‘ï¼Œå°†å…¶æ‰©å±•è‡³off-policyç¯å¢ƒä¸­ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œæ— è®ºæ˜¯åœ¨åŸºäºç­–ç•¥è¿˜æ˜¯åŸºäºä»·å€¼çš„ä¼˜åŒ–ä¸­ï¼ŒGRPOéƒ½èƒ½æé«˜å¥–åŠ±ã€‚</li>
<li>åœ¨GRPOçš„off-policyç‰ˆæœ¬ä¸­ä½¿ç”¨å‰ªè¾‘çš„æ›¿ä»£ç›®æ ‡æ˜¯æœ‰ç›Šçš„ã€‚</li>
<li>off-policy GRPOç›¸è¾ƒäºåŸºäºç­–ç•¥çš„GRPOå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿æˆ–è¡¨ç°ç›¸å½“ã€‚</li>
<li>off-policyæ ·æœ¬åœ¨ä¼°è®¡ä¼˜åŠ¿å‡½æ•°æ—¶å¯èƒ½å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22257">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d898e95770e0d208f5eadd3137efe210.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7732f3227734bb07c33c3518e413cbec.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TabXEval-Why-this-is-a-Bad-Table-An-eXhaustive-Rubric-for-Table-Evaluation"><a href="#TabXEval-Why-this-is-a-Bad-Table-An-eXhaustive-Rubric-for-Table-Evaluation" class="headerlink" title="TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table   Evaluation"></a>TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table   Evaluation</h2><p><strong>Authors:Vihang Pancholi, Jainit Bafna, Tejas Anvekar, Manish Shrivastava, Vivek Gupta</strong></p>
<p>Evaluating tables qualitatively and quantitatively poses a significant challenge, as standard metrics often overlook subtle structural and content-level discrepancies. To address this, we propose a rubric-based evaluation framework that integrates multi-level structural descriptors with fine-grained contextual signals, enabling more precise and consistent table comparison. Building on this, we introduce TabXEval, an eXhaustive and eXplainable two-phase evaluation framework. TabXEval first aligns reference and predicted tables structurally via TabAlign, then performs semantic and syntactic comparison using TabCompare, offering interpretable and granular feedback. We evaluate TabXEval on TabXBench, a diverse, multi-domain benchmark featuring realistic table perturbations and human annotations. A sensitivity-specificity analysis further demonstrates the robustness and explainability of TabXEval across varied table tasks. Code and data are available at <a target="_blank" rel="noopener" href="https://coral-lab-asu.github.io/tabxeval/">https://coral-lab-asu.github.io/tabxeval/</a> </p>
<blockquote>
<p>è¯„ä¼°å’Œå¯¹æ¯”è¡¨æ ¼åœ¨å®šæ€§å’Œå®šé‡ä¸Šéƒ½å­˜åœ¨å¾ˆå¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºæ ‡å‡†çš„è¯„ä¼°æŒ‡æ ‡å¾€å¾€ä¼šå¿½ç•¥ç»†å¾®çš„ç»“æ„å’Œå†…å®¹å±‚é¢çš„å·®å¼‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºrubircçš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¤šçº§ç»“æ„æè¿°å™¨å’Œç²¾ç»†çš„ä¸Šä¸‹æ–‡ä¿¡å·ï¼Œå¯ä»¥å®ç°æ›´ç²¾ç¡®å’Œä¸€è‡´çš„è¡¨æ ¼æ¯”è¾ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†TabXEvalï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢ä¸”å¯è§£é‡Šçš„ä¸¤é˜¶æ®µè¯„ä¼°æ¡†æ¶ã€‚TabXEvalé¦–å…ˆé€šè¿‡TabAlignå¯¹å‚è€ƒè¡¨æ ¼å’Œé¢„æµ‹è¡¨æ ¼è¿›è¡Œç»“æ„ä¸Šçš„å¯¹é½ï¼Œç„¶åä½¿ç”¨TabCompareè¿›è¡Œè¯­ä¹‰å’Œå¥æ³•ä¸Šçš„æ¯”è¾ƒï¼Œæä¾›å¯è§£é‡Šå’Œç²¾ç»†çš„åé¦ˆã€‚æˆ‘ä»¬åœ¨TabXBenchä¸Šè¯„ä¼°äº†TabXEvalï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«çœŸå®è¡¨æ ¼æ‰°åŠ¨å’Œäººç±»æ³¨é‡Šçš„å¤šå…ƒå¤šé¢†åŸŸåŸºå‡†æµ‹è¯•é›†ã€‚é€šè¿‡æ•æ„Ÿæ€§å’Œç‰¹å¼‚æ€§çš„åˆ†æè¿›ä¸€æ­¥è¯æ˜äº†TabXEvalåœ¨ä¸åŒè¡¨æ ¼ä»»åŠ¡ä¸­çš„ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://coral-lab-asu.github.io/tabxeval/%E6%89%BE%E5%88%B0%E3%80%82">https://coral-lab-asu.github.io/tabxeval/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22176v2">PDF</a> Accepeted for Findings at ACL 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§åŸºäºrubricçš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†å¤šå±‚æ¬¡çš„è¡¨æ ¼ç»“æ„æè¿°ä¸ç²¾ç»†çš„ä¸Šä¸‹æ–‡ä¿¡å·ï¼Œå®ç°äº†æ›´ç²¾ç¡®å’Œä¸€è‡´çš„è¡¨æ ¼æ¯”è¾ƒã€‚è¿›ä¸€æ­¥ä»‹ç»äº†TabXEvalï¼Œä¸€ä¸ªå…¨é¢ä¸”å¯è§£é‡Šçš„ä¸¤é˜¶æ®µè¯„ä¼°æ¡†æ¶ã€‚å®ƒé¦–å…ˆé€šè¿‡TabAlignå¯¹å‚è€ƒå’Œé¢„æµ‹è¡¨æ ¼è¿›è¡Œç»“æ„å¯¹é½ï¼Œç„¶åä½¿ç”¨TabCompareè¿›è¡Œè¯­ä¹‰å’Œå¥æ³•æ¯”è¾ƒï¼Œæä¾›å¯è§£é‡Šå’Œè¯¦ç»†çš„åé¦ˆã€‚åœ¨TabXBenchä¸Šè¯„ä¼°TabXEvalï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰ç°å®è¡¨æ ¼æ‰°åŠ¨å’Œäººå·¥æ³¨é‡Šçš„å¤šå…ƒåŒ–å¤šé¢†åŸŸåŸºå‡†æµ‹è¯•é›†ã€‚é€šè¿‡æ•æ„Ÿæ€§ç‰¹å¼‚æ€§åˆ†æéªŒè¯äº†TabXEvalåœ¨ä¸åŒè¡¨æ ¼ä»»åŠ¡ä¸­çš„ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºrubircçš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¡¨æ ¼è¯„ä¼°ä¸­å®šæ€§å®šé‡è¯„ä¼°é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>è¯„ä¼°æ¡†æ¶èåˆäº†å¤šå±‚æ¬¡çš„ç»“æ„æè¿°å’Œç²¾ç»†çš„ä¸Šä¸‹æ–‡ä¿¡å·ï¼Œå®ç°äº†æ›´ç²¾ç¡®å’Œä¸€è‡´çš„è¡¨æ ¼æ¯”è¾ƒã€‚</li>
<li>ä»‹ç»äº†ä¸€ä¸ªåä¸ºTabXEvalçš„ä¸¤é˜¶æ®µè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«ç»“æ„å¯¹é½é˜¶æ®µï¼ˆTabAlignï¼‰å’Œè¯­ä¹‰å¥æ³•æ¯”è¾ƒé˜¶æ®µï¼ˆTabCompareï¼‰ã€‚</li>
<li>TabXEvalåœ¨TabXBenchä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å¤šç§è¡¨æ ¼ä»»åŠ¡å’Œç°å®è¡¨æ ¼æ‰°åŠ¨çš„åŸºå‡†æµ‹è¯•é›†ã€‚</li>
<li>TabXEvalå…·æœ‰å…¨é¢æ€§å’Œå¯è§£é‡Šæ€§ï¼Œèƒ½æä¾›è¯¦ç»†çš„åé¦ˆã€‚</li>
<li>é€šè¿‡æ•æ„Ÿæ€§ç‰¹å¼‚æ€§åˆ†æéªŒè¯äº†TabXEvalåœ¨ä¸åŒè¡¨æ ¼ä»»åŠ¡ä¸­çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0a7e8db0342751e0fce02c0a447ee9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d569497a6798e810f0920980183ff045.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74ca1fb4295599dadac1e74c571817cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2c7c6b2b290c52ddf03b1eed8a169f3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CAD-Coder-Text-to-CAD-Generation-with-Chain-of-Thought-and-Geometric-Reward"><a href="#CAD-Coder-Text-to-CAD-Generation-with-Chain-of-Thought-and-Geometric-Reward" class="headerlink" title="CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric   Reward"></a>CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric   Reward</h2><p><strong>Authors:Yandong Guan, Xilin Wang, Xingxi Ming, Jing Zhang, Dong Xu, Qian Yu</strong></p>
<p>In this work, we introduce CAD-Coder, a novel framework that reformulates text-to-CAD as the generation of CadQuery scripts - a Python-based, parametric CAD language. This representation enables direct geometric validation, a richer modeling vocabulary, and seamless integration with existing LLMs. To further enhance code validity and geometric fidelity, we propose a two-stage learning pipeline: (1) supervised fine-tuning on paired text-CadQuery data, and (2) reinforcement learning with Group Reward Policy Optimization (GRPO), guided by a CAD-specific reward comprising both a geometric reward (Chamfer Distance) and a format reward. We also introduce a chain-of-thought (CoT) planning process to improve model reasoning, and construct a large-scale, high-quality dataset of 110K text-CadQuery-3D model triplets and 1.5K CoT samples via an automated pipeline. Extensive experiments demonstrate that CAD-Coder enables LLMs to generate diverse, valid, and complex CAD models directly from natural language, advancing the state of the art of text-to-CAD generation and geometric reasoning. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CAD-Coderï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå®ƒå°†æ–‡æœ¬åˆ°CADçš„è½¬æ¢é‡æ–°å®šä¹‰ä¸ºCadQueryè„šæœ¬çš„ç”Ÿæˆâ€”â€”ä¸€ç§åŸºäºPythonçš„å‚æ•°åŒ–CADè¯­è¨€ã€‚è¿™ç§è¡¨ç¤ºæ–¹æ³•èƒ½å¤Ÿå®ç°ç›´æ¥çš„å‡ ä½•éªŒè¯ã€æ›´ä¸°å¯Œçš„å»ºæ¨¡è¯æ±‡å’Œæ— ç¼èåˆç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ä»£ç çš„æœ‰æ•ˆæ€§å’Œå‡ ä½•ä¿çœŸåº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„å­¦ä¹ ç®¡é“ï¼šï¼ˆ1ï¼‰åœ¨æˆå¯¹çš„æ–‡æœ¬-CadQueryæ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼›ï¼ˆ2ï¼‰é‡‡ç”¨åŸºäºCADç‰¹å®šå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ŒåŒ…æ‹¬å‡ ä½•å¥–åŠ±ï¼ˆChamferè·ç¦»ï¼‰å’Œæ ¼å¼å¥–åŠ±ï¼Œè¿›è¡Œç¾¤ä½“å¥–åŠ±æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æ€ç»´é“¾ï¼ˆCoTï¼‰è§„åˆ’è¿‡ç¨‹æ¥æ”¹å–„æ¨¡å‹æ¨ç†ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–ç®¡é“æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„åŒ…å«110Kæ–‡æœ¬-CadQuery-3Dæ¨¡å‹ä¸‰å…ƒç»„å’Œ1.5Kæ€ç»´é“¾æ ·æœ¬çš„æ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCAD-Coderä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç›´æ¥ä»è‡ªç„¶è¯­è¨€ç”Ÿæˆå¤šæ ·ã€æœ‰æ•ˆå’Œå¤æ‚çš„CADæ¨¡å‹ï¼Œä»è€Œæ¨åŠ¨äº†æ–‡æœ¬åˆ°CADç”Ÿæˆå’Œå‡ ä½•æ¨ç†çš„æœ€æ–°æŠ€æœ¯å‰æ²¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19713v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶ä»‹ç»äº†CAD-Coderæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ–‡æœ¬è½¬æ¢ä¸ºCADçš„è¿‡ç¨‹é‡æ–°è®¾è®¡ä¸ºç”ŸæˆCadQueryè„šæœ¬ï¼ˆåŸºäºPythonçš„å‚æ•°åŒ–CADè¯­è¨€ï¼‰ã€‚æ­¤è¡¨ç¤ºæ³•å¯å®ç°ç›´æ¥å‡ ä½•éªŒè¯ã€ä¸°å¯Œçš„å»ºæ¨¡è¯æ±‡å’Œä¸ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„æ— ç¼é›†æˆã€‚ä¸ºè¿›ä¸€æ­¥æé«˜ä»£ç çš„æœ‰æ•ˆæ€§å’Œå‡ ä½•ä¿çœŸåº¦ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µå­¦ä¹ ç®¡é“ï¼ŒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰åœ¨é…å¯¹æ–‡æœ¬-CadQueryæ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼›ï¼ˆ2ï¼‰ä½¿ç”¨ç»„åˆå¥–åŠ±ç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ ï¼Œå…¶ä¸­åŒ…æ‹¬å‡ ä½•å¥–åŠ±ï¼ˆChamferè·ç¦»ï¼‰å’Œæ ¼å¼å¥–åŠ±ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†æ€ç»´é“¾ï¼ˆCoTï¼‰è§„åˆ’è¿‡ç¨‹ä»¥æé«˜æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–ç®¡é“æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡åŒ…å«11ä¸‡æ–‡æœ¬-CadQuery-3Dæ¨¡å‹ä¸‰å…ƒç»„å’Œ1500ä¸ªCoTæ ·æœ¬çš„æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒCAD-Coderä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç›´æ¥ä»è‡ªç„¶è¯­è¨€ç”Ÿæˆå¤šæ ·åŒ–ã€æœ‰æ•ˆä¸”å¤æ‚çš„CADæ¨¡å‹ï¼Œæ¨åŠ¨äº†æ–‡æœ¬åˆ°CADç”Ÿæˆå’Œå‡ ä½•æ¨ç†çš„æœ€æ–°è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAD-Coderæ¡†æ¶èƒ½å¤Ÿå°†æ–‡æœ¬è½¬æ¢ä¸ºCADçš„è¿‡ç¨‹é‡æ–°è®¾è®¡ä¸ºç”ŸæˆCadQueryè„šæœ¬ï¼Œä¸°å¯Œäº†å»ºæ¨¡è¯­è¨€ï¼Œå¹¶å®ç°ç›´æ¥å‡ ä½•éªŒè¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µå­¦ä¹ ç®¡é“ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ æé«˜ä»£ç çš„æœ‰æ•ˆæ€§å’Œå‡ ä½•ä¿çœŸåº¦ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±ç­–ç•¥åŒ…æ‹¬å‡ ä½•å¥–åŠ±ï¼ˆChamferè·ç¦»ï¼‰å’Œæ ¼å¼å¥–åŠ±ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹ç”Ÿæˆã€‚</li>
<li>å¼•å…¥äº†æ€ç»´é“¾ï¼ˆCoTï¼‰è§„åˆ’è¿‡ç¨‹ï¼Œæå‡æ¨¡å‹åœ¨æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æ•°æ®é›†ï¼ŒåŒ…å«æ–‡æœ¬-CadQuery-3Dæ¨¡å‹ä¸‰å…ƒç»„å’ŒCoTæ ·æœ¬ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›ä¸°å¯Œèµ„æºã€‚</li>
<li>CAD-Coderåœ¨æ–‡æœ¬åˆ°CADç”Ÿæˆå’Œå‡ ä½•æ¨ç†æ–¹é¢å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–ã€æœ‰æ•ˆä¸”å¤æ‚çš„CADæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a02cd23da62dd9c0d5a1bc79c5a8f08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f227d4738b014e8188273432dfbe8a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69e3a0b1472798b3cf897d12a1f99e67.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Surrogate-Signals-from-Format-and-Length-Reinforcement-Learning-for-Solving-Mathematical-Problems-without-Ground-Truth-Answers"><a href="#Surrogate-Signals-from-Format-and-Length-Reinforcement-Learning-for-Solving-Mathematical-Problems-without-Ground-Truth-Answers" class="headerlink" title="Surrogate Signals from Format and Length: Reinforcement Learning for   Solving Mathematical Problems without Ground Truth Answers"></a>Surrogate Signals from Format and Length: Reinforcement Learning for   Solving Mathematical Problems without Ground Truth Answers</h2><p><strong>Authors:Rihui Xin, Han Liu, Zecheng Wang, Yupeng Zhang, Dianbo Sui, Xiaolin Hu, Bingning Wang</strong></p>
<p>Large Language Models have achieved remarkable success in natural language processing tasks, with Reinforcement Learning playing a key role in adapting them to specific applications. However, obtaining ground truth answers for training LLMs in mathematical problem-solving is often challenging, costly, and sometimes unfeasible. This research delves into the utilization of format and length as surrogate signals to train LLMs for mathematical problem-solving, bypassing the need for traditional ground truth answers. Our study shows that a reward function centered on format correctness alone can yield performance improvements comparable to the standard GRPO algorithm in early phases. Recognizing the limitations of format-only rewards in the later phases, we incorporate length-based rewards. The resulting GRPO approach, leveraging format-length surrogate signals, not only matches but surpasses the performance of the standard GRPO algorithm relying on ground truth answers in certain scenarios, achieving 40.0% accuracy on AIME2024 with a 7B base model. Through systematic exploration and experimentation, this research not only offers a practical solution for training LLMs to solve mathematical problems and reducing the dependence on extensive ground truth data collection, but also reveals the essence of why our label-free approach succeeds: the powerful base model is like an excellent student who has already mastered mathematical and logical reasoning skills, but performs poorly on the test paper, it simply needs to develop good answering habits to achieve outstanding results in exams, to unlock the capabilities it already possesses. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå¼ºåŒ–å­¦ä¹ åœ¨é€‚åº”ç‰¹å®šåº”ç”¨æ–¹é¢å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œåœ¨ä¸ºæ•°å­¦é—®é¢˜è§£å†³è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œè·å–çœŸå®ç­”æ¡ˆé€šå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæˆæœ¬é«˜æ˜‚ï¼Œæœ‰æ—¶ç”šè‡³ä¸å¯è¡Œã€‚æœ¬ç ”ç©¶æ·±å…¥æ¢è®¨äº†åˆ©ç”¨æ ¼å¼å’Œé•¿åº¦ä½œä¸ºæ›¿ä»£ä¿¡å·æ¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ•°å­¦é—®é¢˜è§£å†³ï¼Œä»è€Œç»•è¿‡å¯¹ä¼ ç»ŸçœŸå®ç­”æ¡ˆçš„éœ€æ±‚ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä»…ä¸“æ³¨äºæ ¼å¼æ­£ç¡®æ€§çš„å¥–åŠ±å‡½æ•°å¯ä»¥åœ¨æ—©æœŸé˜¶æ®µäº§ç”Ÿä¸æ ‡å‡†GRPOç®—æ³•ç›¸å½“çš„æ€§èƒ½æ”¹è¿›ã€‚æ„è¯†åˆ°ä»…ä½¿ç”¨æ ¼å¼å¥–åŠ±åœ¨åæœŸé˜¶æ®µçš„å±€é™æ€§ï¼Œæˆ‘ä»¬èå…¥äº†åŸºäºé•¿åº¦çš„å¥–åŠ±ã€‚ç»“æœäº§ç”Ÿçš„GRPOæ–¹æ³•ï¼Œåˆ©ç”¨æ ¼å¼é•¿åº¦æ›¿ä»£ä¿¡å·ï¼Œä¸ä»…åŒ¹é…è€Œä¸”åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†ä¾èµ–çœŸå®ç­”æ¡ˆçš„æ ‡å‡†GRPOç®—æ³•çš„æ€§èƒ½ï¼Œåœ¨AIME2024ä¸Šå®ç°äº†40.0%çš„å‡†ç¡®ç‡ï¼ŒåŸºç¡€æ¨¡å‹ä¸º7Bã€‚é€šè¿‡ç³»ç»Ÿçš„æ¢ç´¢å’Œå®éªŒï¼Œæœ¬ç ”ç©¶ä¸ä»…ä¸ºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è§£å†³æ•°å­¦é—®é¢˜æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œå‡å°‘äº†æˆ‘ä»¬å¯¹å¤§é‡çœŸå®æ•°æ®æ”¶é›†çš„ä¾èµ–ï¼Œè¿˜æ­ç¤ºäº†æˆ‘ä»¬çš„æ— æ ‡ç­¾æ–¹æ³•æˆåŠŸçš„æœ¬è´¨ï¼šå¼ºå¤§çš„åŸºç¡€æ¨¡å‹å°±åƒä¸€ä¸ªå·²ç»æŒæ¡äº†æ•°å­¦å’Œé€»è¾‘æ¨ç†æŠ€èƒ½çš„ä¼˜ç§€å­¦ç”Ÿï¼Œä½†åœ¨è¯•å·ä¸Šè¡¨ç°ä¸ä½³ï¼Œå®ƒåªéœ€è¦å…»æˆè‰¯å¥½çš„ç­”é¢˜ä¹ æƒ¯å°±èƒ½åœ¨è€ƒè¯•ä¸­å–å¾—ä¼˜å¼‚æˆç»©ï¼Œè§£é”å…¶å·²ç»æ‹¥æœ‰çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19439v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå¼ºåŒ–å­¦ä¹ åœ¨é€‚åº”ç‰¹å®šåº”ç”¨æ–¹é¢å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œä¸ºæ•°å­¦é—®é¢˜è§£å†³è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶è·å–çœŸå®ç­”æ¡ˆé€šå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€æˆæœ¬é«˜æ˜‚ï¼Œæœ‰æ—¶ç”šè‡³ä¸å¯è¡Œã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†åˆ©ç”¨æ ¼å¼å’Œé•¿åº¦ä½œä¸ºæ›¿ä»£ä¿¡å·æ¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ•°å­¦é—®é¢˜è§£å†³ï¼Œä»è€Œç»•è¿‡å¯¹ä¼ ç»ŸçœŸå®ç­”æ¡ˆçš„éœ€æ±‚ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»…åŸºäºæ ¼å¼æ­£ç¡®æ€§çš„å¥–åŠ±å‡½æ•°åœ¨åˆæœŸé˜¶æ®µå¯ä»¥ä¸æ ‡å‡†GRPOç®—æ³•çš„æ€§èƒ½æ”¹è¿›ç›¸åª²ç¾ã€‚æ„è¯†åˆ°ä»…ä½¿ç”¨æ ¼å¼å¥–åŠ±çš„å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºé•¿åº¦çš„å¥–åŠ±ã€‚GRPOæ–¹æ³•åˆ©ç”¨æ ¼å¼é•¿åº¦æ›¿ä»£ä¿¡å·ï¼Œä¸ä»…åŒ¹é…è€Œä¸”è¶…è¶Šäº†ä¾èµ–çœŸå®ç­”æ¡ˆçš„æ ‡å‡†GRPOç®—æ³•åœ¨æŸäº›åœºæ™¯ä¸­çš„æ€§èƒ½ï¼Œåœ¨AIME2024ä¸Šå®ç°äº†40.0%çš„å‡†ç¡®ç‡ï¼ŒåŸºç¡€æ¨¡å‹ä¸º7Bã€‚é€šè¿‡ç³»ç»Ÿçš„æ¢ç´¢å’Œå®éªŒï¼Œæœ¬ç ”ç©¶ä¸ä»…ä¸ºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è§£å†³æ•°å­¦é—®é¢˜æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œå‡å°‘äº†å¤§é‡æ”¶é›†çœŸå®æ•°æ®çš„ä¾èµ–ï¼Œè¿˜æ­ç¤ºäº†æ— æ ‡ç­¾æ–¹æ³•æˆåŠŸçš„åŸå› ï¼šå¼ºå¤§çš„åŸºç¡€æ¨¡å‹å°±åƒä¸€ä¸ªå·²ç»æŒæ¡äº†æ•°å­¦å’Œé€»è¾‘æ¨ç†æŠ€èƒ½çš„å­¦ç”Ÿï¼Œåªéœ€è¦å…»æˆè‰¯å¥½çš„ç­”é¢˜ä¹ æƒ¯å°±èƒ½åœ¨è€ƒè¯•ä¸­å–å¾—ä¼˜å¼‚æˆç»©ï¼Œè§£é”å…¶å·²å…·å¤‡çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¼ºåŒ–å­¦ä¹ åœ¨ç‰¹å®šåº”ç”¨é€‚åº”ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è§£å†³æ•°å­¦é—®é¢˜é¢ä¸´è·å–çœŸå®ç­”æ¡ˆçš„æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨æ ¼å¼å’Œé•¿åº¦ä½œä¸ºæ›¿ä»£ä¿¡å·æ¥è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œæ•°å­¦é—®é¢˜è§£å†³ã€‚</li>
<li>åŸºäºæ ¼å¼æ­£ç¡®æ€§çš„å¥–åŠ±å‡½æ•°åœ¨åˆæœŸèƒ½æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç»“åˆé•¿åº¦å¥–åŠ±çš„GRPOæ–¹æ³•åœ¨æŸäº›åœºæ™¯ä¸‹æ€§èƒ½è¶…è¶Šä¾èµ–çœŸå®ç­”æ¡ˆçš„æ ‡å‡†GRPOç®—æ³•ã€‚</li>
<li>åœ¨AIME2024ä¸Šå®ç°40.0%å‡†ç¡®ç‡ï¼Œä½¿ç”¨7BåŸºç¡€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f2bf67c39a699ee5477f6762fac5cc0d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d20385106b0194dc68760dac405bbd0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a5edc95fcc359922b4eebaec3f44d68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbbb0173632a1ca4ac851f8f19d79bc8.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="TAG-INSTRUCT-Controlled-Instruction-Complexity-Enhancement-through-Structure-based-Augmentation"><a href="#TAG-INSTRUCT-Controlled-Instruction-Complexity-Enhancement-through-Structure-based-Augmentation" class="headerlink" title="TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through   Structure-based Augmentation"></a>TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through   Structure-based Augmentation</h2><p><strong>Authors:He Zhu, Zhiwen Ruan, Junyou Su, Xingwei He, Yun Chen, Wenjia Zhang, Guanhua Chen</strong></p>
<p>High-quality instruction data is crucial for developing large language models (LLMs), yet existing approaches struggle to effectively control instruction complexity. We present TAG-INSTRUCT, a novel framework that enhances instruction complexity through structured semantic compression and controlled difficulty augmentation. Unlike previous prompt-based methods operating on raw text, TAG-INSTRUCT compresses instructions into a compact tag space and systematically enhances complexity through RL-guided tag expansion. Through extensive experiments, we show that TAG-INSTRUCT outperforms existing instruction complexity augmentation approaches. Our analysis reveals that operating in tag space provides superior controllability and stability across different instruction synthesis frameworks. </p>
<blockquote>
<p>é«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®å¯¹äºå¼€å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡³å…³é‡è¦ï¼Œç„¶è€Œç°æœ‰çš„æ–¹æ³•å¾ˆéš¾æœ‰æ•ˆæ§åˆ¶æŒ‡ä»¤çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†TAG-INSTRUCTï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç»“æ„åŒ–è¯­ä¹‰å‹ç¼©å’Œå—æ§éš¾åº¦å¢å¼ºæ¥æé«˜æŒ‡ä»¤å¤æ‚æ€§çš„æ–°å‹æ¡†æ¶ã€‚ä¸ä»¥å‰åŸºäºæç¤ºçš„åœ¨åŸå§‹æ–‡æœ¬ä¸Šæ“ä½œçš„æ–¹æ³•ä¸åŒï¼ŒTAG-INSTRUCTå°†æŒ‡ä»¤å‹ç¼©åˆ°ä¸€ä¸ªç´§å‡‘çš„æ ‡ç­¾ç©ºé—´ä¸­ï¼Œå¹¶é€šè¿‡RLå¼•å¯¼çš„æ ‡ç­¾æ‰©å±•æ¥ç³»ç»Ÿåœ°å¢å¼ºå¤æ‚æ€§ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†TAG-INSTRUCTä¼˜äºç°æœ‰çš„æŒ‡ä»¤å¤æ‚æ€§å¢å¼ºæ–¹æ³•ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œåœ¨æ ‡ç­¾ç©ºé—´ä¸­è¿›è¡Œæ“ä½œå¯ä»¥åœ¨ä¸åŒçš„æŒ‡ä»¤åˆæˆæ¡†æ¶ä¸­æä¾›å‡ºè‰²çš„å¯æ§æ€§å’Œç¨³å®šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18557v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†TAG-INSTRUCTæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç»“æ„åŒ–è¯­ä¹‰å‹ç¼©å’Œå—æ§éš¾åº¦å¢å¼ºæ¥æé«˜æŒ‡ä»¤å¤æ‚æ€§æ§åˆ¶ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæç¤ºçš„æ–¹æ³•ä¸åŒï¼ŒTAG-INSTRUCTå°†æŒ‡ä»¤å‹ç¼©æˆç´§å‡‘çš„æ ‡ç­¾ç©ºé—´ï¼Œå¹¶é€šè¿‡RLå¼•å¯¼æ ‡ç­¾æ‰©å±•æ¥ç³»ç»Ÿåœ°å¢å¼ºå¤æ‚æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒTAG-INSTRUCTåœ¨æŒ‡ä»¤å¤æ‚æ€§å¢å¼ºæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ ‡ç­¾ç©ºé—´ä¸­çš„æ“ä½œæä¾›äº†å‡ºè‰²çš„å¯æ§æ€§å’Œç¨³å®šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>é«˜è´¨é‡çš„æ•™å­¦æ•°æ®å¯¹äºå¼€å‘å¤§å‹è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ§åˆ¶æŒ‡ä»¤å¤æ‚æ€§ã€‚</li>
<li>TAG-INSTRUCTæ¡†æ¶é€šè¿‡ç»“æ„åŒ–è¯­ä¹‰å‹ç¼©å’Œå—æ§éš¾åº¦å¢å¼ºæ¥æé«˜æŒ‡ä»¤å¤æ‚æ€§æ§åˆ¶ã€‚</li>
<li>ä¸ä¼ ç»ŸåŸºäºæç¤ºçš„æ–¹æ³•ä¸åŒï¼ŒTAG-INSTRUCTå°†æŒ‡ä»¤å‹ç¼©æˆç´§å‡‘çš„æ ‡ç­¾ç©ºé—´ã€‚</li>
<li>é€šè¿‡RLå¼•å¯¼æ ‡ç­¾æ‰©å±•ï¼ŒTAG-INSTRUCTç³»ç»Ÿåœ°å¢å¼ºå¤æ‚æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒTAG-INSTRUCTåœ¨æŒ‡ä»¤å¤æ‚æ€§å¢å¼ºæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>TAG-INSTRUCTåœ¨æ ‡ç­¾ç©ºé—´ä¸­çš„æ“ä½œæä¾›äº†å‡ºè‰²çš„å¯æ§æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18557">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-77b527ffead2e38a7da081c0c64db267.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a57daf5ed4c9dc01da06546a950c175.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8303d316b238945132a4dd0995c0f064.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="One-RL-to-See-Them-All-Visual-Triple-Unified-Reinforcement-Learning"><a href="#One-RL-to-See-Them-All-Visual-Triple-Unified-Reinforcement-Learning" class="headerlink" title="One RL to See Them All: Visual Triple Unified Reinforcement Learning"></a>One RL to See Them All: Visual Triple Unified Reinforcement Learning</h2><p><strong>Authors:Yan Ma, Linge Du, Xuyang Shen, Shaoxiang Chen, Pengfei Li, Qibing Ren, Lizhuang Ma, Yuchao Dai, Pengfei Liu, Junjie Yan</strong></p>
<p>Reinforcement learning (RL) has significantly advanced the reasoning capabilities of vision-language models (VLMs). However, the use of RL beyond reasoning tasks remains largely unexplored, especially for perceptionintensive tasks like object detection and grounding. We propose V-Triune, a Visual Triple Unified Reinforcement Learning system that enables VLMs to jointly learn visual reasoning and perception tasks within a single training pipeline. V-Triune comprises triple complementary components: Sample-Level Data Formatting (to unify diverse task inputs), Verifier-Level Reward Computation (to deliver custom rewards via specialized verifiers) , and Source-Level Metric Monitoring (to diagnose problems at the data-source level). We further introduce a novel Dynamic IoU reward, which provides adaptive, progressive, and definite feedback for perception tasks handled by V-Triune. Our approach is instantiated within off-the-shelf RL training framework using open-source 7B and 32B backbone models. The resulting model, dubbed Orsta (One RL to See Them All), demonstrates consistent improvements across both reasoning and perception tasks. This broad capability is significantly shaped by its training on a diverse dataset, constructed around four representative visual reasoning tasks (Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding, Detection, Counting, and OCR). Subsequently, Orsta achieves substantial gains on MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1 across its various 7B and 32B model variants, with performance benefits extending to a wide range of downstream tasks. These results highlight the effectiveness and scalability of our unified RL approach for VLMs. The V-Triune system, along with the Orsta models, is publicly available at <a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI">https://github.com/MiniMax-AI</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æ˜¾è‘—æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œé™¤äº†æ¨ç†ä»»åŠ¡ä¹‹å¤–ï¼ŒRLåœ¨æ„ŸçŸ¥å¯†é›†å‹ä»»åŠ¡ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹å’Œå®šä½ï¼‰ä¸­çš„ä½¿ç”¨ä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†V-Triuneï¼Œä¸€ä¸ªè§†è§‰ä¸‰é‡ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼Œå®ƒä½¿VLMèƒ½å¤Ÿåœ¨å•ä¸ªè®­ç»ƒç®¡é“ä¸­è”åˆå­¦ä¹ è§†è§‰æ¨ç†å’Œæ„ŸçŸ¥ä»»åŠ¡ã€‚V-TriuneåŒ…å«ä¸‰ä¸ªäº’è¡¥çš„ç»„ä»¶ï¼šæ ·æœ¬çº§æ•°æ®æ ¼å¼åŒ–ï¼ˆä»¥ç»Ÿä¸€å„ç§ä»»åŠ¡è¾“å…¥ï¼‰ã€éªŒè¯å™¨çº§å¥–åŠ±è®¡ç®—ï¼ˆé€šè¿‡ä¸“ç”¨éªŒè¯å™¨æä¾›è‡ªå®šä¹‰å¥–åŠ±ï¼‰ï¼Œä»¥åŠæºçº§æŒ‡æ ‡ç›‘æ§ï¼ˆåœ¨æ•°æ®æºçº§åˆ«è¯Šæ–­é—®é¢˜ï¼‰ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„åŠ¨æ€IoUå¥–åŠ±ï¼Œå®ƒä¸ºV-Triuneå¤„ç†çš„æ„ŸçŸ¥ä»»åŠ¡æä¾›è‡ªé€‚åº”ã€æ¸è¿›å’Œæ˜ç¡®çš„åé¦ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯åœ¨ç°æˆçš„RLè®­ç»ƒæ¡†æ¶ä¸­ä½¿ç”¨å¼€æºçš„7Bå’Œ32Béª¨å¹²æ¨¡å‹å®ç°çš„ã€‚ç”±æ­¤äº§ç”Ÿçš„æ¨¡å‹ï¼Œè¢«ç§°ä¸ºOrstaï¼ˆä¸€RLè§ä»–ä»¬æ‰€æœ‰äººï¼‰ï¼Œåœ¨æ¨ç†å’Œæ„ŸçŸ¥ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºäº†ä¸€è‡´çš„æ”¹è¿›ã€‚è¿™ç§å¹¿æ³›çš„èƒ½åŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±å…¶åœ¨å›´ç»•å››ä¸ªä»£è¡¨æ€§è§†è§‰æ¨ç†ä»»åŠ¡ï¼ˆæ•°å­¦ã€æ‹¼å›¾ã€å›¾è¡¨å’Œç§‘å­¦ï¼‰å’Œå››ä¸ªè§†è§‰æ„ŸçŸ¥ä»»åŠ¡ï¼ˆå®šä½ã€æ£€æµ‹ã€è®¡æ•°å’ŒOCRï¼‰æ„å»ºçš„å¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„è®­ç»ƒæ‰€å¡‘é€ çš„ã€‚éšåï¼ŒOrstaåœ¨MEGA-Bench Coreä¸Šå–å¾—äº†å®è´¨æ€§è¿›å±•ï¼Œåœ¨å…¶å„ç§7Bå’Œ32Bæ¨¡å‹å˜ä½“ä¸­çš„æ”¹è¿›èŒƒå›´ä»+2.1åˆ°ä»¤äººå°è±¡æ·±åˆ»çš„+14.1ï¼Œæ€§èƒ½ä¼˜åŠ¿æ‰©å±•åˆ°å¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚è¿™äº›ç»“æœçªå‡ºæ˜¾ç¤ºäº†æˆ‘ä»¬ç»Ÿä¸€çš„RLæ–¹æ³•å¯¹VLMçš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚V-Triuneç³»ç»Ÿä»¥åŠOrstaæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/MiniMax-AIä¸Šå…¬å¼€è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18129v2">PDF</a> Technical Report</p>
<p><strong>æ‘˜è¦</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒRLåœ¨æ¨ç†ä»»åŠ¡ä¹‹å¤–çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹æ„ŸçŸ¥å¯†é›†å‹ä»»åŠ¡å¦‚ç›®æ ‡æ£€æµ‹å’Œå®šä½ï¼Œä»ç„¶æœ‰å¾…æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†V-Triuneç³»ç»Ÿï¼Œä¸€ä¸ªè§†è§‰ä¸‰é‡ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼Œå®ƒä½¿VLMèƒ½å¤Ÿåœ¨å•ä¸€è®­ç»ƒç®¡é“ä¸­è”åˆå­¦ä¹ è§†è§‰æ¨ç†å’Œæ„ŸçŸ¥ä»»åŠ¡ã€‚V-TriuneåŒ…æ‹¬ä¸‰ä¸ªäº’è¡¥çš„ç»„ä»¶ï¼šæ ·æœ¬çº§æ•°æ®æ ¼å¼åŒ–ï¼ˆç»Ÿä¸€ä¸åŒä»»åŠ¡çš„è¾“å…¥ï¼‰ã€éªŒè¯å™¨çº§å¥–åŠ±è®¡ç®—ï¼ˆé€šè¿‡ä¸“ç”¨éªŒè¯å™¨æä¾›è‡ªå®šä¹‰å¥–åŠ±ï¼‰ï¼Œä»¥åŠæºçº§æŒ‡æ ‡ç›‘æ§ï¼ˆåœ¨æ•°æ®æºçº§åˆ«è¯Šæ–­é—®é¢˜ï¼‰ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„åŠ¨æ€IoUå¥–åŠ±ï¼Œå®ƒä¸ºV-Triuneå¤„ç†çš„æ„ŸçŸ¥ä»»åŠ¡æä¾›è‡ªé€‚åº”ã€æ¸è¿›å’Œæ˜ç¡®çš„åé¦ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯åœ¨ç°æˆçš„RLè®­ç»ƒæ¡†æ¶ä¸­ä½¿ç”¨å¼€æºçš„7Bå’Œ32Béª¨å¹²æ¨¡å‹å®ç°çš„ã€‚ç”±æ­¤äº§ç”Ÿçš„æ¨¡å‹ï¼Œè¢«ç§°ä¸ºOrstaï¼ˆä¸€RLè§ä¸‡ç‰©ï¼‰ï¼Œåœ¨æ¨ç†å’Œæ„ŸçŸ¥ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºæŒç»­çš„ä¼˜åŠ¿ã€‚è¿™ç§å¹¿æ³›çš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±å…¶åœ¨å››ä¸ªä»£è¡¨æ€§è§†è§‰æ¨ç†ä»»åŠ¡å’Œå››ä¸ªè§†è§‰æ„ŸçŸ¥ä»»åŠ¡å‘¨å›´æ„å»ºçš„å¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„è®­ç»ƒå¡‘é€ çš„ï¼ˆæ•°å­¦ã€æ‹¼å›¾ã€å›¾è¡¨å’Œç§‘å­¦ï¼›å®šä½ã€æ£€æµ‹ã€è®¡æ•°å’ŒOCRï¼‰ã€‚éšåï¼ŒOrstaåœ¨MEGA-Bench Coreä¸Šå–å¾—äº†é‡å¤§è¿›å±•ï¼Œåœ¨å…¶å„ç§7Bå’Œ32Bæ¨¡å‹å˜ç§ä¸­ï¼Œæ”¹è¿›èŒƒå›´ä»+2.1åˆ°ä»¤äººå°è±¡æ·±åˆ»çš„+14.1ï¼Œæ€§èƒ½ä¼˜åŠ¿æ‰©å±•åˆ°å¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚ç»“æœçªå‡ºäº†æˆ‘ä»¬ç»Ÿä¸€RLæ–¹æ³•å¯¹VLMçš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚V-Triuneç³»ç»Ÿå’ŒOrstaæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/MiniMax-AIè·å¾—ã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºV-Triuneç³»ç»Ÿï¼šä¸€ä¸ªè§†è§‰ä¸‰é‡ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼Œä½¿VLMèƒ½è”åˆå­¦ä¹ è§†è§‰æ¨ç†å’Œæ„ŸçŸ¥ä»»åŠ¡ã€‚</li>
<li>V-TriuneåŒ…å«ä¸‰ä¸ªäº’è¡¥ç»„ä»¶ï¼šæ ·æœ¬çº§æ•°æ®æ ¼å¼åŒ–ã€éªŒè¯å™¨çº§å¥–åŠ±è®¡ç®—å’Œæºçº§æŒ‡æ ‡ç›‘æ§ã€‚</li>
<li>å¼•å…¥åŠ¨æ€IoUå¥–åŠ±ï¼Œä¸ºæ„ŸçŸ¥ä»»åŠ¡æä¾›è‡ªé€‚åº”ã€æ¸è¿›å’Œæ˜ç¡®çš„åé¦ˆã€‚</li>
<li>åœ¨å¤šç§è§†è§‰æ¨ç†å’Œæ„ŸçŸ¥ä»»åŠ¡ä¸Šï¼ŒOrstaæ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
<li>Orstaæ¨¡å‹åœ¨MEGA-Bench Coreä¸Šå–å¾—äº†é‡å¤§è¿›å±•ï¼Œæ”¹è¿›èŒƒå›´å¹¿æ³›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1b5d6435098e68dd28af9aa984385e29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6e229fba3bde01a32cb4eba3b00c150.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09409e4ddbedec71977f7fbffbc8fafe.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Co-Reinforcement-Learning-for-Unified-Multimodal-Understanding-and-Generation"><a href="#Co-Reinforcement-Learning-for-Unified-Multimodal-Understanding-and-Generation" class="headerlink" title="Co-Reinforcement Learning for Unified Multimodal Understanding and   Generation"></a>Co-Reinforcement Learning for Unified Multimodal Understanding and   Generation</h2><p><strong>Authors:Jingjing Jiang, Chongjie Si, Jun Luo, Hanwang Zhang, Chao Ma</strong></p>
<p>This paper presents a pioneering exploration of reinforcement learning (RL) via group relative policy optimization for unified multimodal large language models (ULMs), aimed at simultaneously reinforcing generation and understanding capabilities. Through systematic pilot studies, we uncover the significant potential of ULMs to enable the synergistic co-evolution of dual capabilities within a shared policy optimization framework. Building on this insight, we introduce CoRL, a co-reinforcement learning framework comprising a unified RL stage for joint optimization and a refined RL stage for task-specific enhancement. With the proposed CoRL, our resulting model, ULM-R1, achieves average improvements of 7% on three text-to-image generation datasets and 23% on nine multimodal understanding benchmarks. These results demonstrate the effectiveness of CoRL and highlight the substantial benefit of reinforcement learning in facilitating cross-task synergy and optimization for ULMs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mm-vl/ULM-R1">https://github.com/mm-vl/ULM-R1</a>. </p>
<blockquote>
<p>æœ¬æ–‡é¦–æ¬¡æ¢ç´¢äº†é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ¥å¼ºåŒ–å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆULMï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨åŒæ—¶å¢å¼ºç”Ÿæˆå’Œç†è§£èƒ½åŠ›ã€‚é€šè¿‡ç³»ç»Ÿçš„åˆæ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°ULMåœ¨å…±äº«ç­–ç•¥ä¼˜åŒ–æ¡†æ¶å†…ååŒè¿›åŒ–åŒé‡èƒ½åŠ›çš„å·¨å¤§æ½œåŠ›ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ååŒå¼ºåŒ–å­¦ä¹ æ¡†æ¶CoRLï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªç”¨äºè”åˆä¼˜åŒ–çš„ç»Ÿä¸€RLé˜¶æ®µå’Œä¸€ä¸ªç”¨äºç‰¹å®šä»»åŠ¡å¢å¼ºçš„ç²¾ç‚¼RLé˜¶æ®µã€‚é€šè¿‡ä½¿ç”¨æ‰€æå‡ºçš„CoRLï¼Œæˆ‘ä»¬çš„æ¨¡å‹ULM-R1åœ¨ä¸‰ä¸ªæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ•°æ®é›†ä¸Šå¹³å‡æé«˜äº†7%ï¼Œåœ¨ä¹ä¸ªå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­æé«˜äº†23%ã€‚è¿™äº›ç»“æœè¯æ˜äº†CoRLçš„æœ‰æ•ˆæ€§ï¼Œå¹¶çªæ˜¾äº†å¼ºåŒ–å­¦ä¹ åœ¨æé«˜ULMè·¨ä»»åŠ¡ååŒå’Œä¼˜åŒ–çš„å®è´¨åˆ©ç›Šä¸­çš„å…³é”®ä½œç”¨ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mm-vl/ULM-R1%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/mm-vl/ULM-R1è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17534v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†ä¸€é¡¹å¼€åˆ›æ€§çš„ç ”ç©¶ï¼Œè¯¥ç ”ç©¶æ¢è®¨äº†é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ åœ¨ç»Ÿä¸€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨åŒæ—¶å¼ºåŒ–ç”Ÿæˆå’Œç†è§£èƒ½åŠ›ã€‚è¯¥ç ”ç©¶å¼•å…¥äº†ååŒå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ç°äº†æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆå’Œç†è§£æ–¹é¢çš„ååŒè¿›åŒ–ã€‚æå‡ºçš„æ¨¡å‹ULM-R1åœ¨ä¸‰ä¸ªæ–‡æœ¬ç”Ÿæˆæ•°æ®é›†ä¸Šå¹³å‡æé«˜äº†7%ï¼Œåœ¨ä¹ä¸ªå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸Šå¹³å‡æé«˜äº†23%ï¼Œè¯æ˜äº†å¼ºåŒ–å­¦ä¹ åœ¨ä¿ƒè¿›è·¨ä»»åŠ¡ååŒå’Œä¼˜åŒ–æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œå¯¹ç»Ÿä¸€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆULMsï¼‰è¿›è¡Œæ¢ç´¢ã€‚</li>
<li>ç ”ç©¶ç›®æ ‡æ˜¯åŒæ—¶å¼ºåŒ–ULMsçš„ç”Ÿæˆå’Œç†è§£èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿè¯•ç‚¹ç ”ç©¶ï¼Œå‘ç°äº†ULMsåœ¨ååŒè¿›åŒ–ç”Ÿæˆå’Œç†è§£èƒ½åŠ›æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†ååŒå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ˆCoRLï¼‰ï¼ŒåŒ…æ‹¬ç»Ÿä¸€RLé˜¶æ®µè¿›è¡Œè”åˆä¼˜åŒ–å’Œç²¾ç»†RLé˜¶æ®µè¿›è¡Œä»»åŠ¡ç‰¹å®šå¢å¼ºã€‚</li>
<li>ULM-R1æ¨¡å‹åœ¨ä¸‰ä¸ªæ–‡æœ¬ç”Ÿæˆæ•°æ®é›†ä¸Šçš„å¹³å‡æ€§èƒ½æé«˜äº†7%ï¼Œåœ¨ä¹ä¸ªå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡æ€§èƒ½æé«˜äº†23%ã€‚</li>
<li>ç»“æœè¯æ˜äº†CoRLæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å‡¸æ˜¾äº†å¼ºåŒ–å­¦ä¹ åœ¨ä¿ƒè¿›ULMsè·¨ä»»åŠ¡ååŒå’Œä¼˜åŒ–æ–¹é¢çš„å·¨å¤§ç›Šå¤„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17534">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d34d6d2e5fdb0b03b4b63eca26d6229.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23809cf37c0243c211fd1b0921ffd220.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e2c02a186bb2e31e96c93efd6562d53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02e1b44bd3cedcc0812fcc31442606cf.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="VLM-R-3-Region-Recognition-Reasoning-and-Refinement-for-Enhanced-Multimodal-Chain-of-Thought"><a href="#VLM-R-3-Region-Recognition-Reasoning-and-Refinement-for-Enhanced-Multimodal-Chain-of-Thought" class="headerlink" title="VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced   Multimodal Chain-of-Thought"></a>VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced   Multimodal Chain-of-Thought</h2><p><strong>Authors:Chaoya Jiang, Yongrui Heng, Wei Ye, Han Yang, Haiyang Xu, Ming Yan, Ji Zhang, Fei Huang, Shikun Zhang</strong></p>
<p>Recently, reasoning-based MLLMs have achieved a degree of success in generating long-form textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on and revisiting of visual regions to achieve precise grounding of textual reasoning in visual evidence. We introduce \textbf{VLM-R$^3$} (\textbf{V}isual \textbf{L}anguage \textbf{M}odel with \textbf{R}egion \textbf{R}ecognition and \textbf{R}easoning), a framework that equips an MLLM with the ability to (i) decide \emph{when} additional visual evidence is needed, (ii) determine \emph{where} to ground within the image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved chain-of-thought. The core of our method is \textbf{Region-Conditioned Reinforcement Policy Optimization (R-GRPO)}, a training paradigm that rewards the model for selecting informative regions, formulating appropriate transformations (e.g.\ crop, zoom), and integrating the resulting visual context into subsequent reasoning steps. To bootstrap this policy, we compile a modest but carefully curated Visuo-Lingual Interleaved Rationale (VLIR) corpus that provides step-level supervision on region selection and textual justification. Extensive experiments on MathVista, ScienceQA, and other benchmarks show that VLM-R$^3$ sets a new state of the art in zero-shot and few-shot settings, with the largest gains appearing on questions demanding subtle spatial reasoning or fine-grained visual cue extraction. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºæ¨ç†çš„MLLMåœ¨ç”Ÿæˆé•¿æ–‡æœ¬æ¨ç†é“¾æ–¹é¢å–å¾—äº†ä¸€å®šçš„æˆåŠŸã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†éœ€è¦åŠ¨æ€å’Œè¿­ä»£åœ°å…³æ³¨å¹¶é‡æ–°è®¿é—®è§†è§‰åŒºåŸŸä»¥å®ç°åœ¨è§†è§‰è¯æ®ä¸­è¿›è¡Œç²¾ç¡®æ–‡æœ¬æ¨ç†çš„å¤æ‚ä»»åŠ¡æ—¶ä»ç„¶é¢ä¸´å›°éš¾ã€‚æˆ‘ä»¬å¼•å…¥äº†VLM-R$^3$ï¼ˆå¸¦åŒºåŸŸè¯†åˆ«å’Œæ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºMLLMé…å¤‡èƒ½åŠ›çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿï¼ˆiï¼‰å†³å®šä½•æ—¶éœ€è¦é¢å¤–çš„è§†è§‰è¯æ®ï¼Œï¼ˆiiï¼‰ç¡®å®šåœ¨å›¾åƒä¸­çš„æ‰æ ¹ä½ç½®ï¼Œä»¥åŠï¼ˆiiiï¼‰æ— ç¼åœ°å°†ç›¸å…³å­å›¾åƒå†…å®¹é‡æ–°ç¼–ç»‡æˆè¿è´¯çš„æ€ç»´é“¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯åŒºåŸŸæ¡ä»¶å¼ºåŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆR-GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è®­ç»ƒèŒƒå¼ï¼Œå¥–åŠ±æ¨¡å‹é€‰æ‹©ä¿¡æ¯åŒºåŸŸã€åˆ¶å®šé€‚å½“çš„è½¬æ¢ï¼ˆä¾‹å¦‚è£å‰ªã€æ”¾å¤§ï¼‰ï¼Œå¹¶å°†å¾—åˆ°çš„è§†è§‰ä¸Šä¸‹æ–‡é›†æˆåˆ°éšåçš„æ¨ç†æ­¥éª¤ä¸­ã€‚ä¸ºäº†å¼•å¯¼ç­–ç•¥ï¼Œæˆ‘ä»¬ç²¾å¿ƒæ•´ç†äº†ä¸€ä¸ªè§„æ¨¡ä¸å¤§ä½†ç»è¿‡ç²¾å¿ƒç­›é€‰çš„Visuo-Lingualäº¤ç»‡ç†ç”±ï¼ˆVLIRï¼‰è¯­æ–™åº“ï¼Œè¯¥è¯­æ–™åº“æä¾›äº†å…³äºåŒºåŸŸé€‰æ‹©å’Œæ–‡æœ¬ä¾æ®çš„é€æ­¥ç›‘ç£ã€‚åœ¨MathVistaã€ScienceQAå’Œå…¶ä»–åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVLM-R$^3$åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­æ ‘ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ï¼Œåœ¨è¦æ±‚ç²¾ç»†ç©ºé—´æ¨ç†æˆ–ç²¾ç»†è§†è§‰çº¿ç´¢æå–çš„é—®é¢˜ä¸Šè¡¨ç°æœ€ä¸ºæ˜¾è‘—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16192v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    MLLMåœ¨ç”Ÿæˆé•¿æ–‡æœ¬æ¨ç†é“¾æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨éœ€è¦åŠ¨æ€è¿­ä»£å…³æ³¨å›¾åƒåŒºåŸŸçš„ä»»åŠ¡ä¸Šä»æœ‰æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºVLM-RÂ³æ¡†æ¶ï¼Œä½¿MLLMå…·å¤‡å†³å®šä½•æ—¶éœ€è¦é¢å¤–è§†è§‰è¯æ®ã€ç¡®å®šå›¾åƒä¸­çš„è½åœ°ç‚¹ä»¥åŠæ— ç¼åœ°å°†ç›¸å…³å­å›¾åƒå†…å®¹èå…¥æ€ç»´é“¾çš„èƒ½åŠ›ã€‚æ ¸å¿ƒæ–¹æ³•æ˜¯Region-Conditioned Reinforcement Policy Optimization (R-GRPO)ï¼Œå¥–åŠ±æ¨¡å‹é€‰æ‹©ä¿¡æ¯åŒºåŸŸã€åˆ¶å®šé€‚å½“çš„è½¬æ¢ï¼ˆå¦‚è£å‰ªã€æ”¾å¤§ï¼‰å¹¶å°†æ‰€å¾—è§†è§‰ä¸Šä¸‹æ–‡èå…¥åç»­æ¨ç†æ­¥éª¤ã€‚é€šè¿‡ç¼–åˆ¶é€‚åº¦ä½†ç²¾å¿ƒç­–åˆ’çš„Visuo-Lingual Interleaved Rationale (VLIR)è¯­æ–™åº“ï¼Œä¸ºåŒºåŸŸé€‰æ‹©å’Œæ–‡æœ¬ä¾æ®æä¾›æ­¥éª¤å±‚é¢çš„ç›‘ç£æ¥å¼•å¯¼ç­–ç•¥ã€‚åœ¨MathVistaã€ScienceQAç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒVLM-RÂ³åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸Šåˆ›ä¸‹äº†æ–°çºªå½•ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç»†ç©ºé—´æ¨ç†æˆ–ç²¾ç»†è§†è§‰çº¿ç´¢æå–çš„é—®é¢˜ä¸Šæ•ˆæœæ›´ä½³ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>MLLMåœ¨ç”Ÿæˆé•¿æ–‡æœ¬æ¨ç†é“¾æ–¹é¢å·²å–å¾—æˆåŠŸï¼Œä½†åœ¨æŸäº›å¤æ‚ä»»åŠ¡ä¸­ä»éœ€æé«˜ã€‚</li>
<li>å¼•å…¥VLM-RÂ³æ¡†æ¶ï¼Œä½¿MLLMå…·å¤‡åŠ¨æ€è¿­ä»£å…³æ³¨å›¾åƒåŒºåŸŸçš„èƒ½åŠ›ã€‚</li>
<li>VLM-RÂ³æ¡†æ¶çš„æ ¸å¿ƒæ˜¯Region-Conditioned Reinforcement Policy Optimization (R-GRPO)ï¼Œå®ƒèƒ½å¥–åŠ±æ¨¡å‹åœ¨é€‰æ‹©ä¿¡æ¯åŒºåŸŸå’Œæ•´åˆè§†è§‰ä¸Šä¸‹æ–‡æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>VLM-RÂ³é€šè¿‡ç¼–åˆ¶Visuo-Lingual Interleaved Rationale (VLIR)è¯­æ–™åº“æ¥å¼•å¯¼ç­–ç•¥ã€‚</li>
<li>VLIRè¯­æ–™åº“æä¾›æ­¥éª¤å±‚é¢çš„ç›‘ç£ï¼Œæ¶µç›–åŒºåŸŸé€‰æ‹©å’Œæ–‡æœ¬ä¾æ®ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒVLM-RÂ³è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå°¤å…¶åœ¨éœ€è¦ç²¾ç»†ç©ºé—´æ¨ç†å’Œè§†è§‰çº¿ç´¢æå–çš„ä»»åŠ¡ä¸Šã€‚</li>
<li>VLM-RÂ³åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸Šåˆ›ä¸‹äº†æ–°çºªå½•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16192">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a436ebc8a1fb336805b4d7637d8a6a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3efa3118bcc66eac357d292593cbeb63.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ade60abc3bdc5760f02256ca2c95a87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2199ea2b2d0aacf723ef9262927e083.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d3c03365170299397c44c4faec81cd22.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-04  AutoChemSchematic AI A Closed-Loop, Physics-Aware Agentic Framework for   Auto-Generating Chemical Process and Instrumentation Diagrams
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-57c1aa4dacbd5e92143c9195eef923f9.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  TalkingHeadBench A Multi-Modal Benchmark & Analysis of Talking-Head   DeepFake Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31987.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
