<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-04  NMCSE Noise-Robust Multi-Modal Coupling Signal Estimation Method via   Optimal Transport for Cardiovascular Disease Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2c958068ec93320796409d041fee6521.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    66 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-04-æ›´æ–°"><a href="#2025-06-04-æ›´æ–°" class="headerlink" title="2025-06-04 æ›´æ–°"></a>2025-06-04 æ›´æ–°</h1><h2 id="NMCSE-Noise-Robust-Multi-Modal-Coupling-Signal-Estimation-Method-via-Optimal-Transport-for-Cardiovascular-Disease-Detection"><a href="#NMCSE-Noise-Robust-Multi-Modal-Coupling-Signal-Estimation-Method-via-Optimal-Transport-for-Cardiovascular-Disease-Detection" class="headerlink" title="NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via   Optimal Transport for Cardiovascular Disease Detection"></a>NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via   Optimal Transport for Cardiovascular Disease Detection</h2><p><strong>Authors:Peihong Zhang, Zhixin Li, Rui Sang, Yuxuan Liu, Yiqiang Cai, Yizhou Tan, Shengchen Li</strong></p>
<p>Electrocardiogram (ECG) and Phonocardiogram (PCG) signals are linked by a latent coupling signal representing the electrical-to-mechanical cardiac transformation. While valuable for cardiovascular disease (CVD) detection, this coupling signal is traditionally estimated using deconvolution methods that amplify noise, limiting clinical utility. In this paper, we propose Noise-Robust Multi-Modal Coupling Signal Estimation (NMCSE), which reformulates the problem as distribution matching via optimal transport theory. By jointly optimizing amplitude and temporal alignment, NMCSE mitigates noise amplification without additional preprocessing. Integrated with our Temporal-Spatial Feature Extraction network, NMCSE enables robust multi-modal CVD detection. Experiments on the PhysioNet 2016 dataset with realistic hospital noise demonstrate that NMCSE reduces estimation errors by approximately 30% in Mean Squared Error while maintaining higher Pearson Correlation Coefficients across all tested signal-to-noise ratios. Our approach achieves 97.38% accuracy and 0.98 AUC in CVD detection, outperforming state-of-the-art methods and demonstrating robust performance for real-world clinical applications. </p>
<blockquote>
<p>å¿ƒç”µå›¾ï¼ˆECGï¼‰å’Œå¿ƒéŸ³å›¾ï¼ˆPCGï¼‰ä¿¡å·é€šè¿‡ä»£è¡¨ç”µ-æœºæ¢°å¿ƒè„è½¬æ¢çš„æ½œåœ¨è€¦åˆä¿¡å·ç›¸å…³è”ã€‚è™½ç„¶è¿™ç§è€¦åˆä¿¡å·å¯¹äºæ£€æµ‹å¿ƒè¡€ç®¡ç–¾ç—…ï¼ˆCVDï¼‰å¾ˆæœ‰ä»·å€¼ï¼Œä½†ä¼ ç»Ÿä¸Šæ˜¯é€šè¿‡åå·ç§¯æ–¹æ³•æ¥ä¼°è®¡çš„ï¼Œè¿™ä¼šå¢åŠ å™ªå£°ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠä¸Šçš„å®ç”¨ä»·å€¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å™ªå£°é²æ£’çš„è·¨æ¨¡æ€è€¦åˆä¿¡å·ä¼°è®¡ï¼ˆNMCSEï¼‰ï¼Œå®ƒå°†é—®é¢˜é‡æ–°å®šä¹‰ä¸ºé€šè¿‡æœ€ä¼˜ä¼ è¾“ç†è®ºè¿›è¡Œåˆ†å¸ƒåŒ¹é…ã€‚é€šè¿‡è”åˆä¼˜åŒ–å¹…åº¦å’Œæ—¶é—´å¯¹é½ï¼ŒNMCSEå¯ä»¥åœ¨æ— éœ€é¢å¤–é¢„å¤„ç†çš„æƒ…å†µä¸‹å‡è½»å™ªå£°æ”¾å¤§ã€‚ç»“åˆæˆ‘ä»¬çš„æ—¶ç©ºç‰¹å¾æå–ç½‘ç»œï¼ŒNMCSEå¯å®ç°ç¨³å¥çš„è·¨æ¨¡æ€CVDæ£€æµ‹ã€‚åœ¨å…·æœ‰ç°å®åŒ»é™¢å™ªå£°çš„PhysioNet 2016æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨å‡æ–¹è¯¯å·®æ–¹é¢ï¼ŒNMCSEå°†ä¼°è®¡è¯¯å·®é™ä½äº†çº¦30%ï¼Œå¹¶ä¸”åœ¨æ‰€æœ‰æµ‹è¯•çš„ä¿¡å™ªæ¯”ä¸­ä¿æŒäº†æ›´é«˜çš„çš®å°”é€Šç›¸å…³ç³»æ•°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨CVDæ£€æµ‹ä¸­è¾¾åˆ°äº†97.38%çš„å‡†ç¡®åº¦å’Œ0.98çš„AUCå€¼ï¼Œä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†åœ¨ç°å®ä¸´åºŠåº”ç”¨ä¸­çš„ç¨³å¥æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18174v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§å™ªå£°é²æ£’çš„è·¨æ¨¡æ€è€¦åˆä¿¡å·ä¼°è®¡æ–¹æ³•ï¼ˆNMCSEï¼‰ï¼Œç”¨äºä¼°è®¡å¿ƒç”µå›¾ï¼ˆECGï¼‰å’Œå¿ƒéŸ³å›¾ï¼ˆPCGï¼‰ä¹‹é—´çš„æ½œåœ¨è€¦åˆä¿¡å·ã€‚è¯¥æ–¹æ³•åŸºäºæœ€ä¼˜ä¼ è¾“ç†è®ºé‡æ–°æ„å»ºé—®é¢˜ï¼Œè”åˆä¼˜åŒ–æŒ¯å¹…å’Œæ—¶é—´å¯¹é½ï¼Œé™ä½äº†å™ªå£°æ”¾å¤§é—®é¢˜ï¼Œå¹¶ä¸æˆ‘ä»¬çš„æ—¶ç©ºç‰¹å¾æå–ç½‘ç»œç›¸ç»“åˆï¼Œå®ç°äº†ç¨³å¥çš„å¤šæ¨¡æ€å¿ƒè¡€ç®¡ç–¾ç—…ï¼ˆCVDï¼‰æ£€æµ‹ã€‚åœ¨PhysioNet 2016æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNMCSEåœ¨å¹³å‡è¯¯å·®å¹³æ–¹ä¸Šå‡å°‘äº†å¤§çº¦30%çš„ä¼°è®¡è¯¯å·®ï¼ŒåŒæ—¶åœ¨æ‰€æœ‰æµ‹è¯•çš„ä¿¡å·å™ªå£°æ¯”ä¸Šéƒ½ä¿æŒäº†è¾ƒé«˜çš„çš®å°”é€Šç›¸å…³ç³»æ•°ã€‚åœ¨å¿ƒè¡€ç®¡ç–¾ç—…æ£€æµ‹ä¸­ï¼Œå…¶å‡†ç¡®æ€§è¾¾åˆ°97.38%ï¼ŒAUCè¾¾åˆ°0.98ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨ç°å®çš„ä¸´åºŠåº”ç”¨ä¸­è¡¨ç°å‡ºç¨³å¥æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¿ƒç”µå›¾ï¼ˆECGï¼‰å’Œå¿ƒéŸ³å›¾ï¼ˆPCGï¼‰ä¹‹é—´çš„è€¦åˆä¿¡å·ä»£è¡¨äº†å¿ƒè„çš„ç”µ-æœºæ¢°è½¬æ¢ã€‚</li>
<li>ä¼ ç»Ÿä¼°è®¡è€¦åˆä¿¡å·çš„æ–¹æ³•ä½¿ç”¨è§£å·ç§¯æ–¹æ³•ï¼Œä¼šæ”¾å¤§å™ªå£°ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠçš„å®ç”¨æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºå™ªå£°é²æ£’çš„è·¨æ¨¡æ€è€¦åˆä¿¡å·ä¼°è®¡æ–¹æ³•ï¼ˆNMCSEï¼‰ï¼Œé€šè¿‡æœ€ä¼˜ä¼ è¾“ç†è®ºé‡æ–°æ„å»ºé—®é¢˜æ¥è§£å†³å™ªå£°é—®é¢˜ã€‚</li>
<li>NMCSEè”åˆä¼˜åŒ–æŒ¯å¹…å’Œæ—¶é—´å¯¹é½ï¼Œé™ä½äº†å™ªå£°æ”¾å¤§é—®é¢˜ï¼Œæé«˜äº†ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>NMCSEä¸æ—¶ç©ºç‰¹å¾æå–ç½‘ç»œç»“åˆï¼Œå®ç°äº†å¤šæ¨¡æ€å¿ƒè¡€ç®¡ç–¾ç—…æ£€æµ‹ã€‚</li>
<li>åœ¨PhysioNetæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNMCSEæ˜¾è‘—å‡å°‘äº†ä¼°è®¡è¯¯å·®ï¼Œå¹¶æé«˜äº†æ£€æµ‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18174">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7f707c1c196e402693373dee692ea4da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7de23563f5b9efb12d8a50e605fef143.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="4D-CTA-Image-and-geometry-dataset-for-kinematic-analysis-of-abdominal-aortic-aneurysms"><a href="#4D-CTA-Image-and-geometry-dataset-for-kinematic-analysis-of-abdominal-aortic-aneurysms" class="headerlink" title="4D-CTA Image and geometry dataset for kinematic analysis of abdominal   aortic aneurysms"></a>4D-CTA Image and geometry dataset for kinematic analysis of abdominal   aortic aneurysms</h2><p><strong>Authors:Mostafa Jamshidian, Adam Wittek, Saeideh Sekhavat, Farah Alkhatib, Jens Carsten Ritter, Paul M. Parizel, Donatien Le Liepvre, Florian Bernard, Ludovic Minvielle, Antoine FondanÃ¨che, Jane Polce, Christopher Wood, Karol Miller</strong></p>
<p>This article presents a dataset used in the article â€œKinematics of Abdominal Aortic Aneurysmsâ€, published in the Journal of Biomechanics. The dataset is publicly available for download from the Zenodo data repository (<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.15477710">https://doi.org/10.5281/zenodo.15477710</a>). The dataset includes time-resolved 3D computed tomography angiography (4D-CTA) images of abdominal aortic aneurysm (AAA) captured throughout the cardiac cycle from ten patients diagnosed with AAA, along with ten patient-specific AAA geometries extracted from these images. Typically, the 4D-CTA dataset for each patient contains ten electrocardiogram (ECG)-gated 3D-CTA image frames acquired over a cardiac cycle, capturing both the systolic and diastolic phases of the AAA configuration. For method verification, the dataset also includes synthetic ground truth data generated from Patient 1â€™s 3D-CTA AAA image in the diastolic phase. The ground truth data includes the patient-specific finite element (FE) biomechanical model and a synthetic systolic 3D-CTA image. The synthetic systolic image was generated by warping Patient 1â€™s diastolic 3D-CTA image using the realistic displacement field obtained from the AAA biomechanical FE model. The images were acquired at Fiona Stanley Hospital in Western Australia and provided to the researchers at the Intelligent Systems for Medicine Laboratory at The University of Western Australia (ISML-UWA), where image-based AAA kinematic analysis was performed. Our dataset enabled the analysis of AAA wall displacement and strain throughout the cardiac cycle using a non-invasive, in vivo, image registration-based approach. The use of widely adopted, open-source file formats (NRRD for images and STL for geometries) facilitates broad applicability and reusability in AAA biomechanics studies that require patient-specific geometry and information about AAA kinematics during cardiac cycle. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåœ¨ã€Šç”Ÿç‰©åŠ›å­¦æ‚å¿—ã€‹ä¸Šå‘è¡¨çš„åä¸ºâ€œè…¹éƒ¨ä¸»åŠ¨è„‰ç˜¤çš„è¿åŠ¨å­¦â€çš„æ–‡ç« ä¸­ä½¿ç”¨çš„æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†å¯ä»Zenodoæ•°æ®ä»“åº“ï¼ˆ<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.15477710%EF%BC%89%E5%85%AC%E5%BC%80%E4%B8%8B%E8%BD%BD%E3%80%82%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8C%85%E5%90%AB%E9%80%9A%E8%BF%87%E5%BF%83%E8%84%8F%E5%91%A8%E6%9C%9F%E6%8D%95%E8%8E%B7%E7%9A%84%E8%85%B9%E9%83%A8%E4%B8%BB%E5%8A%A8%E8%84%89%E7%98%A4%EF%BC%88AAA%EF%BC%89%E7%9A%84%E6%97%B6%E9%97%B4%E5%88%86%E8%BE%A8%E4%B8%89%E7%BB%B4%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%96%AD%E5%B1%82%E8%A1%80%E7%AE%A1%E9%80%A0%E5%BD%B1%EF%BC%884D-CTA%EF%BC%89%E5%9B%BE%E5%83%8F%EF%BC%8C%E5%8C%85%E6%8B%AC10%E5%90%8D%E8%A2%AB%E8%AF%8A%E6%96%AD%E4%B8%BAAAA%E7%9A%84%E6%82%A3%E8%80%85%E7%9A%84AAA%E5%9B%BE%E5%83%8F%EF%BC%8C%E4%BB%A5%E5%8F%8A%E4%BB%8E%E8%BF%99%E4%BA%9B%E5%9B%BE%E5%83%8F%E4%B8%AD%E6%8F%90%E5%8F%96%E7%9A%8410%E4%B8%AA%E6%82%A3%E8%80%85%E7%89%B9%E5%AE%9A%E7%9A%84AAA%E5%87%A0%E4%BD%95%E7%BB%93%E6%9E%84%E3%80%82%E9%80%9A%E5%B8%B8%EF%BC%8C%E6%AF%8F%E4%B8%AA%E6%82%A3%E8%80%85%E7%9A%844D-CTA%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8C%85%E5%90%AB%E5%9C%A8%E6%95%B4%E4%B8%AA%E5%BF%83%E8%84%8F%E5%91%A8%E6%9C%9F%E5%86%85%E8%8E%B7%E5%8F%96%E7%9A%84%E5%BF%83%E7%94%B5%E5%9B%BE%E9%97%A8%E6%8E%A7%E7%9A%843D-CTA%E5%9B%BE%E5%83%8F%E5%B8%A7%EF%BC%8C%E8%BF%99%E4%BA%9B%E5%9B%BE%E5%83%8F%E6%8D%95%E6%8D%89AAA%E9%85%8D%E7%BD%AE%E7%9A%84%E6%94%B6%E7%BC%A9%E6%9C%9F%E5%92%8C%E8%88%92%E5%BC%A0%E6%9C%9F%E3%80%82%E4%B8%BA%E4%BA%86%E9%AA%8C%E8%AF%81%E6%96%B9%E6%B3%95%EF%BC%8C%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%98%E5%8C%85%E6%8B%AC%E7%94%B1%E6%82%A3%E8%80%851%E7%9A%84%E8%88%92%E5%BC%A0%E6%9C%9F3D-CTA">https://doi.org/10.5281/zenodo.15477710ï¼‰å…¬å¼€ä¸‹è½½ã€‚æ•°æ®é›†åŒ…å«é€šè¿‡å¿ƒè„å‘¨æœŸæ•è·çš„è…¹éƒ¨ä¸»åŠ¨è„‰ç˜¤ï¼ˆAAAï¼‰çš„æ—¶é—´åˆ†è¾¨ä¸‰ç»´è®¡ç®—æœºæ–­å±‚è¡€ç®¡é€ å½±ï¼ˆ4D-CTAï¼‰å›¾åƒï¼ŒåŒ…æ‹¬10åè¢«è¯Šæ–­ä¸ºAAAçš„æ‚£è€…çš„AAAå›¾åƒï¼Œä»¥åŠä»è¿™äº›å›¾åƒä¸­æå–çš„10ä¸ªæ‚£è€…ç‰¹å®šçš„AAAå‡ ä½•ç»“æ„ã€‚é€šå¸¸ï¼Œæ¯ä¸ªæ‚£è€…çš„4D-CTAæ•°æ®é›†åŒ…å«åœ¨æ•´ä¸ªå¿ƒè„å‘¨æœŸå†…è·å–çš„å¿ƒç”µå›¾é—¨æ§çš„3D-CTAå›¾åƒå¸§ï¼Œè¿™äº›å›¾åƒæ•æ‰AAAé…ç½®çš„æ”¶ç¼©æœŸå’Œèˆ’å¼ æœŸã€‚ä¸ºäº†éªŒè¯æ–¹æ³•ï¼Œè¯¥æ•°æ®é›†è¿˜åŒ…æ‹¬ç”±æ‚£è€…1çš„èˆ’å¼ æœŸ3D-CTA</a> AAAå›¾åƒç”Ÿæˆçš„åˆæˆåŸºå‡†æ•°æ®ã€‚åŸºå‡†æ•°æ®åŒ…æ‹¬æ‚£è€…ç‰¹å®šçš„æœ‰é™å…ƒï¼ˆFEï¼‰ç”Ÿç‰©åŠ›å­¦æ¨¡å‹å’Œåˆæˆçš„æ”¶ç¼©æœŸ3D-CTAå›¾åƒã€‚åˆæˆæ”¶ç¼©æœŸå›¾åƒæ˜¯é€šè¿‡ä½¿ç”¨æ¥è‡ªAAAç”Ÿç‰©åŠ›å­¦æœ‰é™å…ƒæ¨¡å‹çš„ç°å®ä½ç§»åœºå¯¹æ‚£è€…1çš„èˆ’å¼ æœŸ3D-CTAå›¾åƒè¿›è¡Œå˜å½¢å¤„ç†è€Œç”Ÿæˆçš„ã€‚è¿™äº›å›¾åƒæ˜¯åœ¨è¥¿æ¾³å¤§åˆ©äºšå·çš„è²æ¬§å¨œæ–¯å¦åˆ©åŒ»é™¢è·å–çš„ï¼Œå¹¶æä¾›ç»™è¥¿æ¾³å¤§åˆ©äºšå¤§å­¦æ™ºèƒ½åŒ»å­¦å®éªŒå®¤ï¼ˆISML-UWAï¼‰çš„ç ”ç©¶äººå‘˜ï¼Œåœ¨é‚£é‡Œè¿›è¡Œäº†åŸºäºå›¾åƒçš„AAAè¿åŠ¨å­¦åˆ†æã€‚æˆ‘ä»¬çš„æ•°æ®é›†èƒ½å¤Ÿä½¿ç”¨ä¸€ç§æ— åˆ›ã€ä½“å†…ã€åŸºäºå›¾åƒé…å‡†çš„æ–¹æ³•åˆ†ææ•´ä¸ªå¿ƒè„å‘¨æœŸå†…AAAå£ä½ç§»å’Œåº”å˜ã€‚ä½¿ç”¨å¹¿æ³›é‡‡ç”¨çš„å¼€æ”¾æºä»£ç æ–‡ä»¶æ ¼å¼ï¼ˆNRRDç”¨äºå›¾åƒå’ŒSTLç”¨äºå‡ ä½•å½¢çŠ¶ï¼‰æœ‰åŠ©äºåœ¨AAAç”Ÿç‰©åŠ›å­¦ç ”ç©¶ä¸­å®ç°å¹¿æ³›çš„é€‚ç”¨æ€§å’Œå¯é‡ç”¨æ€§ï¼Œè¿™äº›ç ”ç©¶éœ€è¦ç‰¹å®šäºæ‚£è€…çš„å‡ ä½•å½¢çŠ¶ä»¥åŠæœ‰å…³å¿ƒè„å‘¨æœŸå†…AAAè¿åŠ¨å­¦çš„ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17647v2">PDF</a> </p>
<p><strong>Summary</strong><br>è¯¥æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªç”¨äºç ”ç©¶è…¹éƒ¨ä¸»åŠ¨è„‰ç˜¤ï¼ˆAAAï¼‰è¿åŠ¨å­¦çš„æ•°æ®é›†ã€‚æ•°æ®é›†å¯ä»Zenodoæ•°æ®ä»“åº“ä¸‹è½½ï¼ŒåŒ…å«é€šè¿‡4D-CTAæŠ€æœ¯è·å–çš„AAAæ‚£è€…çš„å®æ—¶ä¸‰ç»´å›¾åƒä»¥åŠä»è¿™äº›å›¾åƒä¸­æå–çš„æ‚£è€…ç‰¹å¼‚æ€§AAAå‡ ä½•ç»“æ„ã€‚æ•°æ®é›†è¿˜åŒ…æ‹¬åˆæˆåŸºå‡†æ•°æ®ï¼Œç”¨äºéªŒè¯æ–¹æ³•ã€‚è¯¥æ•°æ®é›†æœ‰åŠ©äºåˆ†æAAAå£åœ¨æ•´ä¸ªå¿ƒåŠ¨å‘¨æœŸä¸­çš„ä½ç§»å’Œåº”å˜ï¼Œé‡‡ç”¨éä¾µå…¥æ€§ã€æ´»ä½“ã€åŸºäºå›¾åƒé…å‡†çš„æ–¹æ³•ã€‚æ•°æ®é›†ä½¿ç”¨å¹¿æ³›é‡‡çº³çš„å¼€æ”¾æºæ–‡ä»¶æ ¼å¼ï¼Œä¾¿äºåœ¨AAAç”Ÿç‰©åŠ›å­¦ç ”ç©¶ä¸­åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªå…³äºè…¹éƒ¨ä¸»åŠ¨è„‰ç˜¤ï¼ˆAAAï¼‰è¿åŠ¨å­¦ç ”ç©¶çš„å…¬å¼€æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«é€šè¿‡4D-CTAæŠ€æœ¯è·å–çš„AAAæ‚£è€…çš„å®æ—¶ä¸‰ç»´å›¾åƒã€‚</li>
<li>æ•°æ®é›†åŒ…å«æ‚£è€…ç‰¹å¼‚æ€§AAAå‡ ä½•ç»“æ„ä»¥åŠåˆæˆåŸºå‡†æ•°æ®ç”¨äºæ–¹æ³•éªŒè¯ã€‚</li>
<li>æ•°æ®é›†å¯ç”¨äºåˆ†æAAAå£åœ¨æ•´ä¸ªå¿ƒåŠ¨å‘¨æœŸä¸­çš„ä½ç§»å’Œåº”å˜ã€‚</li>
<li>æ•°æ®é›†é‡‡ç”¨äº†éä¾µå…¥æ€§ã€æ´»ä½“ã€åŸºäºå›¾åƒé…å‡†çš„æ–¹æ³•è¿›è¡Œåˆ†æã€‚</li>
<li>æ•°æ®é›†ä½¿ç”¨å¼€æ”¾æºæ–‡ä»¶æ ¼å¼ï¼Œä¾¿äºåœ¨AAAç”Ÿç‰©åŠ›å­¦ç ”ç©¶ä¸­ä½¿ç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4457b0ae3b24bff4cf456c06786b8291.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d57e6e2d36950001b6c16ac55de7305c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45b65f365cb44e7d5ed4bb63c4b44fcc.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RADAR-Enhancing-Radiology-Report-Generation-with-Supplementary-Knowledge-Injection"><a href="#RADAR-Enhancing-Radiology-Report-Generation-with-Supplementary-Knowledge-Injection" class="headerlink" title="RADAR: Enhancing Radiology Report Generation with Supplementary   Knowledge Injection"></a>RADAR: Enhancing Radiology Report Generation with Supplementary   Knowledge Injection</h2><p><strong>Authors:Wenjun Hou, Yi Cheng, Kaishuai Xu, Heng Li, Yan Hu, Wenjie Li, Jiang Liu</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities in various domains, including radiology report generation. Previous approaches have attempted to utilize multimodal LLMs for this task, enhancing their performance through the integration of domain-specific knowledge retrieval. However, these approaches often overlook the knowledge already embedded within the LLMs, leading to redundant information integration. To address this limitation, we propose Radar, a framework for enhancing radiology report generation with supplementary knowledge injection. Radar improves report generation by systematically leveraging both the internal knowledge of an LLM and externally retrieved information. Specifically, it first extracts the modelâ€™s acquired knowledge that aligns with expert image-based classification outputs. It then retrieves relevant supplementary knowledge to further enrich this information. Finally, by aggregating both sources, Radar generates more accurate and informative radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU X-ray demonstrate that our model outperforms state-of-the-art LLMs in both language quality and clinical accuracy. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆã€‚ä¹‹å‰çš„æ–¹æ³•è¯•å›¾ä½¿ç”¨å¤šæ¨¡æ€LLMæ¥å®Œæˆæ­¤ä»»åŠ¡ï¼Œé€šè¿‡æ•´åˆç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†æ£€ç´¢æ¥å¢å¼ºæ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ä¼šå¿½ç•¥LLMä¸­å·²ç»åµŒå…¥çš„çŸ¥è¯†ï¼Œå¯¼è‡´ä¿¡æ¯é›†æˆå†—ä½™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Radaræ¡†æ¶ï¼Œé€šè¿‡æ³¨å…¥è¡¥å……çŸ¥è¯†æ¥æé«˜æ”¾å°„å­¦æŠ¥å‘Šçš„ç”Ÿæˆè´¨é‡ã€‚Radaré€šè¿‡ç³»ç»Ÿåœ°åˆ©ç”¨LLMçš„å†…éƒ¨çŸ¥è¯†å’Œå¤–éƒ¨æ£€ç´¢ä¿¡æ¯æ¥æ”¹å–„æŠ¥å‘Šç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé¦–å…ˆæå–ä¸ä¸“å®¶åŸºäºå›¾åƒçš„åˆ†ç±»è¾“å‡ºç›¸ç¬¦çš„æ¨¡å‹è·å–çš„çŸ¥è¯†ã€‚ç„¶åï¼Œå®ƒæ£€ç´¢ç›¸å…³çš„è¡¥å……çŸ¥è¯†æ¥è¿›ä¸€æ­¥ä¸°å¯Œè¿™äº›ä¿¡æ¯ã€‚æœ€åï¼Œé€šè¿‡èšåˆè¿™ä¸¤ä¸ªæ¥æºï¼ŒRadarç”Ÿæˆæ›´å‡†ç¡®å’Œæ›´æœ‰ä¿¡æ¯çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚åœ¨MIMIC-CXRã€CheXpert-Pluså’ŒIU X-rayä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è¯­è¨€è´¨é‡å’Œä¸´åºŠå‡†ç¡®æ€§æ–¹é¢éƒ½ä¼˜äºæœ€æ–°çš„LLMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14318v2">PDF</a> Accepted to ACL 2025 main</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆã€‚å…ˆå‰çš„æ–¹æ³•è¯•å›¾ä½¿ç”¨å¤šæ¨¡å¼LLMså®Œæˆæ­¤ä»»åŠ¡ï¼Œå¹¶é€šè¿‡æ•´åˆé¢†åŸŸç‰¹å®šçŸ¥è¯†æ£€ç´¢å¢å¼ºå…¶æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€å¿½è§†äº†LLMsä¸­å·²ç»åµŒå…¥çš„çŸ¥è¯†ï¼Œå¯¼è‡´ä¿¡æ¯æ•´åˆå†—ä½™ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Radaræ¡†æ¶ï¼Œç”¨äºé€šè¿‡è¡¥å……çŸ¥è¯†æ³¨å…¥å¢å¼ºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆã€‚Radaré€šè¿‡ç³»ç»Ÿåœ°åˆ©ç”¨LLMçš„å†…éƒ¨çŸ¥è¯†å’Œå¤–éƒ¨æ£€ç´¢ä¿¡æ¯ï¼Œæ”¹è¿›äº†æŠ¥å‘Šç”Ÿæˆã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒé¦–å…ˆæå–ä¸ä¸“å®¶å›¾åƒåˆ†ç±»è¾“å‡ºå¯¹é½çš„æ¨¡å‹è·å–çš„çŸ¥è¯†ã€‚ç„¶åï¼Œå®ƒæ£€ç´¢ç›¸å…³çš„è¡¥å……çŸ¥è¯†ä»¥è¿›ä¸€æ­¥ä¸°å¯Œè¿™äº›ä¿¡æ¯ã€‚æœ€åï¼Œé€šè¿‡èšåˆè¿™ä¸¤ä¸ªæ¥æºï¼ŒRadarç”Ÿæˆæ›´å‡†ç¡®å’Œå…¨é¢çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚åœ¨MIMIC-CXRã€CheXpert-Pluså’ŒIU X-rayä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è¯­è¨€è´¨é‡å’Œä¸´åºŠå‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºæœ€æ–°LLMsã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆç­‰é¢†åŸŸå…·æœ‰æ˜¾è‘—èƒ½åŠ›ã€‚</li>
<li>å…ˆå‰çš„æ–¹æ³•é€šè¿‡æ•´åˆé¢†åŸŸç‰¹å®šçŸ¥è¯†æ£€ç´¢æ¥å¢å¼ºæ€§èƒ½ï¼Œä½†å¿½è§†äº†LLMsä¸­å·²åµŒå…¥çš„çŸ¥è¯†ã€‚</li>
<li>Radaræ¡†æ¶é€šè¿‡ç³»ç»Ÿåœ°åˆ©ç”¨LLMçš„å†…éƒ¨çŸ¥è¯†å’Œå¤–éƒ¨æ£€ç´¢ä¿¡æ¯ï¼Œæ”¹è¿›äº†æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆã€‚</li>
<li>Radaræå–ä¸ä¸“å®¶å›¾åƒåˆ†ç±»è¾“å‡ºå¯¹é½çš„æ¨¡å‹è·å–çš„çŸ¥è¯†ï¼Œå¹¶æ£€ç´¢ç›¸å…³è¡¥å……çŸ¥è¯†ã€‚</li>
<li>é€šè¿‡èšåˆæ¨¡å‹å†…éƒ¨çŸ¥è¯†å’Œå¤–éƒ¨æ£€ç´¢ä¿¡æ¯ï¼ŒRadarèƒ½ç”Ÿæˆæ›´å‡†ç¡®å’Œå…¨é¢çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜ï¼ŒRadaråœ¨è¯­è¨€å’Œä¸´åºŠå‡†ç¡®æ€§æ–¹é¢ä¼˜äºå…¶ä»–æœ€æ–°LLMsã€‚</li>
<li>Radaræ¡†æ¶æœ‰åŠ©äºå®ç°æ›´é«˜æ•ˆçš„æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼Œå…·æœ‰æ½œåœ¨çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4346ca7b68269d600966ea00eb5aae87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cf4a52ebd60501ee92d5754dea193b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79f8114c02f902fc03aa83ad2f83e385.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-972f09c4b05d732e9f0505f6468e106b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6710f06dbcd37ac7f3e2ad1fda6f4e41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edb754dd3cc987a3ac243e6ff27bd8ed.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SMURF-Scalable-method-for-unsupervised-reconstruction-of-flow-in-4D-flow-MRI"><a href="#SMURF-Scalable-method-for-unsupervised-reconstruction-of-flow-in-4D-flow-MRI" class="headerlink" title="SMURF: Scalable method for unsupervised reconstruction of flow in 4D   flow MRI"></a>SMURF: Scalable method for unsupervised reconstruction of flow in 4D   flow MRI</h2><p><strong>Authors:Atharva Hans, Abhishek Singh, Pavlos Vlachos, Ilias Bilionis</strong></p>
<p>We introduce SMURF, a scalable and unsupervised machine learning method for simultaneously segmenting vascular geometries and reconstructing velocity fields from 4D flow MRI data. SMURF models geometry and velocity fields using multilayer perceptron-based functions incorporating Fourier feature embeddings and random weight factorization to accelerate convergence. A measurement model connects these fields to the observed image magnitude and phase data. Maximum likelihood estimation and subsampling enable SMURF to process high-dimensional datasets efficiently. Evaluations on synthetic, in vitro, and in vivo datasets demonstrate SMURFâ€™s performance. On synthetic internal carotid artery aneurysm data derived from CFD, SMURF achieves a quarter-voxel segmentation accuracy across noise levels of up to 50%, outperforming the state-of-the-art segmentation method by up to double the accuracy. In an in vitro experiment on Poiseuille flow, SMURF reduces velocity reconstruction RMSE by approximately 34% compared to raw measurements. In in vivo internal carotid artery aneurysm data, SMURF attains nearly half-voxel segmentation accuracy relative to expert annotations and decreases median velocity divergence residuals by about 31%, with a 27% reduction in the interquartile range. These results indicate that SMURF is robust to noise, preserves flow structure, and identifies patient-specific morphological features. SMURF advances 4D flow MRI accuracy, potentially enhancing the diagnostic utility of 4D flow MRI in clinical applications. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»SMURFï¼Œè¿™æ˜¯ä¸€ç§å¯æ‰©å±•çš„æ— ç›‘ç£æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤ŸåŒæ—¶ä»4DæµMRIæ•°æ®ä¸­åˆ†å‰²è¡€ç®¡å‡ ä½•ç»“æ„å¹¶é‡å»ºé€Ÿåº¦åœºã€‚SMURFä½¿ç”¨å¤šå±‚æ„ŸçŸ¥å™¨å‡½æ•°å¯¹å‡ ä½•å½¢çŠ¶å’Œé€Ÿåº¦åœºè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ç»“åˆå‚…é‡Œå¶ç‰¹å¾åµŒå…¥å’Œéšæœºæƒé‡åˆ†è§£æ¥åŠ é€Ÿæ”¶æ•›ã€‚æµ‹é‡æ¨¡å‹å°†è¿™äº›åœºä¸è§‚å¯Ÿåˆ°çš„å›¾åƒå¹…åº¦å’Œç›¸ä½æ•°æ®è¿æ¥èµ·æ¥ã€‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡å’Œå­é‡‡æ ·ä½¿SMURFèƒ½å¤Ÿé«˜æ•ˆå¤„ç†é«˜ç»´æ•°æ®é›†ã€‚åœ¨åˆæˆã€ä½“å¤–å’Œä½“å†…æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†SMURFçš„æ€§èƒ½ã€‚åœ¨ç”±CFDæ´¾ç”Ÿçš„åˆæˆå†…éƒ¨é¢ˆåŠ¨è„‰åŠ¨è„‰ç˜¤æ•°æ®ä¸Šï¼ŒSMURFåœ¨é«˜è¾¾50%çš„å™ªå£°æ°´å¹³ä¸‹å®ç°äº†å››åˆ†ä¹‹ä¸€ä½“ç´ çš„åˆ†å‰²ç²¾åº¦ï¼Œæ¯”æœ€æ–°åˆ†å‰²æ–¹æ³•çš„ç²¾åº¦é«˜å‡ºä¸¤å€ã€‚åœ¨æ³Šæ¾æµçš„ä½“å¤–å®éªŒä¸­ï¼ŒSMURFä¸åŸå§‹æµ‹é‡ç›¸æ¯”ï¼Œé€Ÿåº¦é‡å»ºçš„RMSEé™ä½äº†çº¦34%ã€‚åœ¨ä½“å†…çš„å†…éƒ¨é¢ˆåŠ¨è„‰åŠ¨è„‰ç˜¤æ•°æ®ä¸­ï¼ŒSMURFç›¸å¯¹äºä¸“å®¶æ³¨é‡Šè¾¾åˆ°äº†è¿‘ä¸€åŠä½“ç´ çš„åˆ†å‰²ç²¾åº¦ï¼Œå¹¶å°†ä¸­ä½æ•°é€Ÿåº¦å‘æ•£æ®‹å·®å‡å°‘äº†çº¦31%ï¼Œå››åˆ†ä½è·å‡å°‘äº†27%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSMURFå¯¹å™ªå£°å…·æœ‰é²æ£’æ€§ï¼Œèƒ½å¤Ÿä¿ç•™æµåŠ¨ç»“æ„ï¼Œå¹¶è¯†åˆ«æ‚£è€…ç‰¹å®šçš„å½¢æ€ç‰¹å¾ã€‚SMURFæé«˜äº†4DæµMRIçš„å‡†ç¡®æ€§ï¼Œæœ‰æœ›å¢å¼º4DæµMRIåœ¨ä¸´åºŠåº”ç”¨ä¸­çš„è¯Šæ–­æ•ˆç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12494v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SMURFæ˜¯ä¸€ç§å¯æ‰©å±•çš„æ— ç›‘ç£æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå¯åŒæ—¶å®ç°è¡€ç®¡å‡ ä½•ç»“æ„çš„åˆ†å‰²å’Œé€Ÿåº¦åœºçš„é‡å»ºï¼Œé€‚ç”¨äºå¤„ç†å››ç»´è¡€æµMRIæ•°æ®ã€‚SMURFé‡‡ç”¨å¤šå±‚æ„ŸçŸ¥å™¨å‡½æ•°å»ºæ¨¡å‡ ä½•å’Œé€Ÿåº¦åœºï¼Œå¹¶ç»“åˆå‚…é‡Œå¶ç‰¹å¾åµŒå…¥å’Œéšæœºæƒé‡åˆ†è§£åŠ é€Ÿæ”¶æ•›ã€‚å…¶åœ¨åˆæˆã€ä½“å¤–å’Œä½“å†…æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†å…¶æ€§èƒ½ä¼˜åŠ¿ã€‚SMURFå…·æœ‰æŠ—å™ªæ€§å¼ºã€ä¿ç•™è¡€æµç»“æ„ä»¥åŠè¯†åˆ«æ‚£è€…ç‰¹å®šå½¢æ€ç‰¹å¾ç­‰ä¼˜ç‚¹ï¼Œæé«˜äº†å››ç»´è¡€æµMRIçš„å‡†ç¡®æ€§ï¼Œæœ‰æœ›å¢å¼ºå…¶åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„è¯Šæ–­ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SMURFæ˜¯ä¸€ç§ç”¨äºå¤„ç†å››ç»´è¡€æµMRIæ•°æ®çš„å¯æ‰©å±•æ— ç›‘ç£æœºå™¨å­¦ä¹ æŠ€æœ¯ã€‚</li>
<li>SMURFèƒ½åŒæ—¶åˆ†å‰²è¡€ç®¡å‡ ä½•ç»“æ„å’Œé‡å»ºé€Ÿåº¦åœºã€‚</li>
<li>SMURFé‡‡ç”¨å¤šå±‚æ„ŸçŸ¥å™¨ç»“åˆå‚…é‡Œå¶ç‰¹å¾åµŒå…¥å’Œéšæœºæƒé‡åˆ†è§£æ¥å»ºæ¨¡ã€‚</li>
<li>åœ¨åˆæˆæ•°æ®é›†ä¸Šï¼ŒSMURFåœ¨å™ªå£°æ°´å¹³é«˜è¾¾50%çš„æƒ…å†µä¸‹å®ç°äº†å››åˆ†ä¹‹ä¸€ä½“ç´ çš„åˆ†å‰²ç²¾åº¦ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>åœ¨ä½“å¤–å®éªŒä¸­ï¼ŒSMURFå°†é€Ÿåº¦é‡å»ºçš„RMSEé™ä½äº†çº¦34%ã€‚</li>
<li>åœ¨ä½“å†…æ•°æ®é›†ä¸Šï¼ŒSMURFè¾¾åˆ°äº†è¿‘ä¹ä¸€åŠä½“ç´ çš„åˆ†å‰²ç²¾åº¦ï¼Œå¹¶é™ä½äº†é€Ÿåº¦æ•£åº¦æ®‹å·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12494">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b58ec44a966ca418dafbd99cd67115a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2c3a49f416bb2c7b9d48e60c0240f2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edf19d7396ae7717ab10bb012669e6b8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RAFT-Robust-Augmentation-of-FeaTures-for-Image-Segmentation"><a href="#RAFT-Robust-Augmentation-of-FeaTures-for-Image-Segmentation" class="headerlink" title="RAFT: Robust Augmentation of FeaTures for Image Segmentation"></a>RAFT: Robust Augmentation of FeaTures for Image Segmentation</h2><p><strong>Authors:Edward Humes, Xiaomin Lin, Uttej Kallakuri, Tinoosh Mohsenin</strong></p>
<p>Image segmentation is a powerful computer vision technique for scene understanding. However, real-world deployment is stymied by the need for high-quality, meticulously labeled datasets. Synthetic data provides high-quality labels while reducing the need for manual data collection and annotation. However, deep neural networks trained on synthetic data often face the Syn2Real problem, leading to poor performance in real-world deployments.   To mitigate the aforementioned gap in image segmentation, we propose RAFT, a novel framework for adapting image segmentation models using minimal labeled real-world data through data and feature augmentations, as well as active learning. To validate RAFT, we perform experiments on the synthetic-to-real â€œSYNTHIA-&gt;Cityscapesâ€ and â€œGTAV-&gt;Cityscapesâ€ benchmarks. We managed to surpass the previous state of the art, HALO. SYNTHIA-&gt;Cityscapes experiences an improvement in mIoU* upon domain adaptation of 2.1%&#x2F;79.9%, and GTAV-&gt;Cityscapes experiences a 0.4%&#x2F;78.2% improvement in mIoU. Furthermore, we test our approach on the real-to-real benchmark of â€œCityscapes-&gt;ACDCâ€, and again surpass HALO, with a gain in mIoU upon adaptation of 1.3%&#x2F;73.2%. Finally, we examine the effect of the allocated annotation budget and various components of RAFT upon the final transfer mIoU. </p>
<blockquote>
<p>å›¾åƒåˆ†å‰²æ˜¯åœºæ™¯ç†è§£çš„ä¸€ç§å¼ºå¤§çš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå®é™…éƒ¨ç½²å´å—åˆ°éœ€è¦é«˜è´¨é‡ã€ç²¾ç»†æ ‡æ³¨æ•°æ®é›†çš„é™åˆ¶ã€‚åˆæˆæ•°æ®æä¾›äº†é«˜è´¨é‡æ ‡ç­¾ï¼ŒåŒæ—¶å‡å°‘äº†æ‰‹åŠ¨æ•°æ®æ”¶é›†å’Œæ ‡æ³¨çš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œåœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œå¸¸å¸¸é¢ä¸´Syn2Realé—®é¢˜ï¼Œå¯¼è‡´åœ¨å®é™…éƒ¨ç½²ä¸­çš„æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†å‡è½»ä¸Šè¿°å›¾åƒåˆ†å‰²ä¸­çš„å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†RAFTï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨æœ€å°é‡çš„çœŸå®ä¸–ç•Œæ•°æ®ï¼Œé€šè¿‡æ•°æ®å’Œç‰¹å¾å¢å¼ºä»¥åŠä¸»åŠ¨å­¦ä¹ ï¼Œé€‚åº”å›¾åƒåˆ†å‰²æ¨¡å‹çš„æ–°æ¡†æ¶ã€‚ä¸ºäº†éªŒè¯RAFTçš„æ•ˆæœï¼Œæˆ‘ä»¬åœ¨åˆæˆåˆ°çœŸå®çš„â€SYNTHIA-&gt;Cityscapesâ€å’Œâ€GTAV-&gt;Cityscapesâ€åŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚æˆ‘ä»¬è¶…è¶Šäº†ä¹‹å‰çš„æŠ€æœ¯æ°´å¹³HALOã€‚åœ¨SYNTHIA-&gt;Cityscapesä¸Šï¼Œæˆ‘ä»¬çš„åŸŸé€‚åº”æ–¹æ³•æé«˜äº†mIoU*ï¼ˆå¹³å‡äº¤å¹¶æ¯”ï¼‰çš„å‡†ç¡®åº¦è¾¾åˆ°2.1%&#x2F;79.9%ï¼Œè€Œåœ¨GTAV-&gt;Cityscapesä¸Šæé«˜äº†mIoUçš„å‡†ç¡®åº¦è¾¾åˆ°0.4%&#x2F;78.2%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨çœŸå®åˆ°çœŸå®çš„â€Cityscapes-&gt;ACDCâ€åŸºå‡†æµ‹è¯•é›†ä¸Šæµ‹è¯•äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶å†æ¬¡è¶…è¶Šäº†HALOï¼Œåœ¨é€‚åº”åæé«˜äº†mIoUçš„å‡†ç¡®åº¦è¾¾åˆ°1.3%&#x2F;73.2%ã€‚æœ€åï¼Œæˆ‘ä»¬ç ”ç©¶äº†åˆ†é…æ ‡æ³¨é¢„ç®—å¯¹RAFTæœ€ç»ˆè¿ç§»mIoUçš„å½±å“ä»¥åŠå„ç§ç»„ä»¶çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04529v2">PDF</a> </p>
<p><strong>Summary</strong><br>     å›¾åƒåˆ†å‰²æ˜¯ä¸€ç§å¼ºå¤§çš„è®¡ç®—æœºè§†è§‰åœºæ™¯ç†è§£æŠ€æœ¯ï¼Œä½†åœ¨å®é™…éƒ¨ç½²ä¸­éœ€è¦é«˜è´¨é‡ã€ç²¾ç»†æ ‡æ³¨çš„æ•°æ®é›†ã€‚åˆæˆæ•°æ®å¯ä»¥æä¾›é«˜è´¨é‡æ ‡ç­¾ï¼Œå‡å°‘æ‰‹åŠ¨æ”¶é›†å’Œæ ‡æ³¨çš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œåœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œå¸¸é¢ä¸´åˆæˆåˆ°çœŸå®ï¼ˆSyn2Realï¼‰é—®é¢˜ï¼Œå¯¼è‡´åœ¨çœŸå®ä¸–ç•Œéƒ¨ç½²ä¸­çš„æ€§èƒ½ä¸ä½³ã€‚ä¸ºç¼“è§£å›¾åƒåˆ†å‰²ä¸­çš„å‰è¿°å·®è·ï¼Œæˆ‘ä»¬æå‡ºRAFTæ¡†æ¶ï¼Œé€šè¿‡æœ€å°çœŸå®ä¸–ç•Œæ•°æ®ã€æ•°æ®å¢å¼ºå’Œç‰¹å¾å¢å¼ºä»¥åŠä¸»åŠ¨å­¦ä¹ æ–¹æ³•ï¼Œé€‚åº”å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚éªŒè¯å®éªŒè¡¨æ˜ï¼ŒRAFTåœ¨åˆæˆåˆ°çœŸå®åœºæ™¯çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è¶…è¶Šå…ˆå‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹é‡è¦æŠ€æœ¯ï¼Œç”¨äºåœºæ™¯ç†è§£ã€‚</li>
<li>å®é™…éƒ¨ç½²å›¾åƒåˆ†å‰²æŠ€æœ¯æ—¶ï¼Œéœ€è¦é«˜è´¨é‡ã€ç²¾ç»†æ ‡æ³¨çš„æ•°æ®é›†ã€‚</li>
<li>åˆæˆæ•°æ®å¯ä»¥å‡å°‘æ‰‹åŠ¨æ”¶é›†å’Œæ ‡æ³¨æ•°æ®çš„éœ€æ±‚ï¼Œä½†è®­ç»ƒçš„æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œéƒ¨ç½²ä¸­å¸¸é¢ä¸´æ€§èƒ½é—®é¢˜ã€‚</li>
<li>æå‡ºçš„RAFTæ¡†æ¶æ—¨åœ¨é€šè¿‡æœ€å°çœŸå®ä¸–ç•Œæ•°æ®ã€æ•°æ®å¢å¼ºå’Œç‰¹å¾å¢å¼ºä»¥åŠä¸»åŠ¨å­¦ä¹ æ–¹æ³•ï¼Œé€‚åº”å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚</li>
<li>RAFTåœ¨åˆæˆåˆ°çœŸå®åœºæ™¯çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è¶…è¶Šå…ˆå‰æŠ€æœ¯ã€‚</li>
<li>åœ¨SYNTHIA-&gt;Cityscapeså’ŒGTAV-&gt;Cityscapesçš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRAFTåœ¨mIoUæŒ‡æ ‡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04529">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-551b7af54f06f86899d8e8e560e75948.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6ef83331d867655fe12354178b25379.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40ad481c5b644787162f11a7fa58e51c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b8da820a548ac6af52a12f4411c248e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CSTRL-Context-Driven-Sequential-Transfer-Learning-for-Abstractive-Radiology-Report-Summarization"><a href="#CSTRL-Context-Driven-Sequential-Transfer-Learning-for-Abstractive-Radiology-Report-Summarization" class="headerlink" title="CSTRL: Context-Driven Sequential Transfer Learning for Abstractive   Radiology Report Summarization"></a>CSTRL: Context-Driven Sequential Transfer Learning for Abstractive   Radiology Report Summarization</h2><p><strong>Authors:Mst. Fahmida Sultana Naznin, Adnan Ibney Faruq, Mostafa Rifat Tazwar, Md Jobayer, Md. Mehedi Hasan Shawon, Md Rakibul Hasan</strong></p>
<p>A radiology report comprises several sections, including the Findings and Impression of the diagnosis. Automatically generating the Impression from the Findings is crucial for reducing radiologistsâ€™ workload and improving diagnostic accuracy. Pretrained models that excel in common abstractive summarization problems encounter challenges when applied to specialized medical domains largely due to the complex terminology and the necessity for accurate clinical context. Such tasks in medical domains demand extracting core information, avoiding context shifts, and maintaining proper flow. Misuse of medical terms can lead to drastic clinical errors. To address these issues, we introduce a sequential transfer learning that ensures key content extraction and coherent summarization. Sequential transfer learning often faces challenges like initial parameter decay and knowledge loss, which we resolve with the Fisher matrix regularization. Using MIMIC-CXR and Open-I datasets, our model, CSTRL - Context-driven Sequential TRansfer Learning - achieved state-of-the-art performance, showing 56.2% improvement in BLEU-1, 40.5% in BLEU-2, 84.3% in BLEU-3, 28.9% in ROUGE-1, 41.0% in ROUGE-2 and 26.5% in ROGUE-3 score over benchmark studies. We also analyze factual consistency scores while preserving the medical context. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/fahmidahossain/Report_Summarization">https://github.com/fahmidahossain/Report_Summarization</a>. </p>
<blockquote>
<p>åŒ»å­¦æ”¾å°„æŠ¥å‘Šç”±å‡ ä¸ªéƒ¨åˆ†ç»„æˆï¼ŒåŒ…æ‹¬è¯Šæ–­çš„æ£€æŸ¥ç»“æœå’Œå°è±¡éƒ¨åˆ†ã€‚è‡ªåŠ¨æ ¹æ®æ£€æŸ¥ç»“æœç”Ÿæˆè¯Šæ–­å°è±¡å¯¹å‡å°‘æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ã€æé«˜è¯Šæ–­å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚é¢„è®­ç»ƒæ¨¡å‹åœ¨å¸¸è§çš„æŠ½è±¡æ‘˜è¦é—®é¢˜ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†å½“åº”ç”¨äºä¸“ä¸šåŒ»å­¦é¢†åŸŸæ—¶ï¼Œç”±äºå¤æ‚çš„æœ¯è¯­å’Œå‡†ç¡®çš„ä¸´åºŠè¯­å¢ƒçš„å¿…è¦æ€§ï¼Œå®ƒä»¬é¢ä¸´æŒ‘æˆ˜ã€‚æ­¤ç±»åŒ»å­¦é¢†åŸŸçš„ä»»åŠ¡éœ€è¦æå–æ ¸å¿ƒä¿¡æ¯ï¼Œé¿å…ä¸Šä¸‹æ–‡åç§»ï¼Œå¹¶ä¿æŒé€‚å½“çš„æµç¨‹ã€‚åŒ»å­¦æœ¯è¯­çš„è¯¯ç”¨å¯èƒ½å¯¼è‡´ä¸¥é‡çš„ä¸´åºŠé”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é¡ºåºè¿ç§»å­¦ä¹ ï¼Œä»¥ç¡®ä¿å…³é”®å†…å®¹æå–å’Œè¿è´¯çš„æ‘˜è¦ã€‚é¡ºåºè¿ç§»å­¦ä¹ ç»å¸¸é¢ä¸´åˆå§‹å‚æ•°è¡°å‡å’ŒçŸ¥è¯†æŸå¤±ç­‰æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é€šè¿‡FisherçŸ©é˜µæ­£åˆ™åŒ–æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ä½¿ç”¨MIMIC-CXRå’ŒOpen-Iæ•°æ®é›†ï¼Œæˆ‘ä»¬çš„æ¨¡å‹CSTRLï¼ˆåŸºäºä¸Šä¸‹æ–‡é©±åŠ¨çš„åºåˆ—è¿ç§»å­¦ä¹ ï¼‰å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚ç›¸è¾ƒäºåŸºå‡†ç ”ç©¶ï¼ŒBLEU-1å¾—åˆ†æé«˜äº†56.2%ï¼ŒBLEU-2å¾—åˆ†æé«˜äº†40.5%ï¼ŒBLEU-3å¾—åˆ†æé«˜äº†84.3%ï¼Œåœ¨ROUGEå¾—åˆ†æ–¹é¢ä¹Ÿæœ‰æ‰€æé«˜ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†ä¿æŒåŒ»å­¦ä¸Šä¸‹æ–‡çš„ä¸€è‡´åº¦å¾—åˆ†ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/fahmidahossain/Report_Summarization">https://github.com/fahmidahossain/Report_Summarization</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05750v2">PDF</a> Accepted in ACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒæŠ¥å‘Šè‡ªåŠ¨æ‘˜è¦çš„æŠ€æœ¯ã€‚é€šè¿‡ä½¿ç”¨åºè´¯è¿ç§»å­¦ä¹ ï¼Œè¯¥æŠ€æœ¯åœ¨åŒ»å­¦é¢†åŸŸçš„æ–‡æœ¬æ‘˜è¦ä¸­å®ç°äº†æ˜¾è‘—æå‡ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘åŒ»ç”Ÿçš„å·¥ä½œé‡å¹¶æå‡è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚æ¨¡å‹çš„æ‘˜è¦ç»“æœå…¬å¼€å¯è®¿é—®ã€‚æ–‡ä¸­æåˆ°äº†æŒ‘æˆ˜ä¸è§£å†³ç­–ç•¥ä»¥åŠæ¨¡å‹æ•ˆæœè¯„ä¼°ç»“æœã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ä»‹ç»äº†ä¸€ç§åŸºäºåºè´¯è¿ç§»å­¦ä¹ çš„åŒ»å­¦å›¾åƒæŠ¥å‘Šè‡ªåŠ¨æ‘˜è¦æŠ€æœ¯ã€‚</li>
<li>è¯¥æŠ€æœ¯è§£å†³äº†ä¼ ç»Ÿæ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸæ–‡æœ¬æ‘˜è¦ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æœ¯è¯­å¤æ‚åº¦å’Œå‡†ç¡®ä¸´åºŠè¯­å¢ƒçš„æŠŠæ¡ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å…³é”®å†…å®¹æå–å’Œè¿è´¯æ€§æ€»ç»“ä¹‹é—´å®ç°å¹³è¡¡ï¼Œé¿å…å› åŒ»å­¦æœ¯è¯­è¯¯ç”¨å¯¼è‡´çš„ä¸´åºŠé”™è¯¯ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨FisherçŸ©é˜µæ­£åˆ™åŒ–ï¼Œè§£å†³äº†åºè´¯è¿ç§»å­¦ä¹ ä¸­åˆå§‹å‚æ•°è¡°å‡å’ŒçŸ¥è¯†æŸå¤±çš„é—®é¢˜ã€‚</li>
<li>æ¨¡å‹åœ¨MIMIC-CXRå’ŒOpen-Iæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œå–å¾—äº†æ˜¾è‘—çš„æå‡æ•ˆæœã€‚ç›¸è¾ƒäºåŸºå‡†ç ”ç©¶ï¼Œæ¨¡å‹åœ¨BLEUå’ŒROUGEå¾—åˆ†ä¸Šæœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>æ–‡ç« å¯¹æ¨¡å‹çš„æ€§èƒ½è¿›è¡Œäº†è¯¦ç»†è¯„ä¼°ï¼ŒåŒ…æ‹¬äº‹å®ä¸€è‡´æ€§è¯„ä¼°ï¼Œç¡®ä¿åœ¨ä¿ç•™åŒ»å­¦è¯­å¢ƒçš„åŒæ—¶æé«˜æ‘˜è¦çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05750">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ceba3cf403083361b765f1f7073222ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-231ba19dbc098c5a9f82d7f38e38db3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1a4e1f195849dcad5e3ca234a5087ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3384149a65a8a4fbdb5231c65c7e0be0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Fourier-Asymmetric-Attention-on-Domain-Generalization-for-Pan-Cancer-Drug-Response-Prediction"><a href="#Fourier-Asymmetric-Attention-on-Domain-Generalization-for-Pan-Cancer-Drug-Response-Prediction" class="headerlink" title="Fourier Asymmetric Attention on Domain Generalization for Pan-Cancer   Drug Response Prediction"></a>Fourier Asymmetric Attention on Domain Generalization for Pan-Cancer   Drug Response Prediction</h2><p><strong>Authors:Ran Song, Yinpu Bai, Hui Liu</strong></p>
<p>The accurate prediction of drug responses remains a formidable challenge, particularly at the single-cell level and in clinical treatment contexts. Some studies employ transfer learning techniques to predict drug responses in individual cells and patients, but they require access to target-domain data during training, which is often unavailable or only obtainable in future. In this study, we propose a novel domain generalization framework, termed FourierDrug, to address this challenge. Given the extracted feature from expression profile, we performed Fourier transforms and then introduced an asymmetric attention constraint that would cluster drug-sensitive samples into a compact group while drives resistant samples dispersed in the frequency domain. Our empirical experiments demonstrate that our model effectively learns task-relevant features from diverse source domains, and achieves accurate predictions of drug response for unseen cancer type. When evaluated on single-cell and patient-level drug response prediction tasks, FourierDrugâ€“trained solely on in vitro cell line data without access to target-domain dataâ€“consistently outperforms or, at least, matched the performance of current state-of-the-art methods. These findings underscore the potential of our method for real-world clinical applications. </p>
<blockquote>
<p>å‡†ç¡®é¢„æµ‹è¯ç‰©ååº”ä»ç„¶æ˜¯ä¸€é¡¹è‰°å·¨çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å•ç»†èƒæ°´å¹³å’Œä¸´åºŠæ²»ç–—ç¯å¢ƒä¸­ã€‚ä¸€äº›ç ”ç©¶é‡‡ç”¨è¿ç§»å­¦ä¹ æŠ€æœ¯æ¥é¢„æµ‹å•ä¸ªç»†èƒå’Œæ‚£è€…çš„è¯ç‰©ååº”ï¼Œä½†å®ƒä»¬éœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¿é—®ç›®æ ‡åŸŸæ•°æ®ï¼Œè€Œè¿™äº›æ•°æ®é€šå¸¸ä¸å¯ç”¨æˆ–åªèƒ½åœ¨å°†æ¥è·å¾—ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„é¢†åŸŸæ³›åŒ–æ¡†æ¶ï¼Œç§°ä¸ºFourierDrugï¼Œä»¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚æ ¹æ®è¡¨è¾¾è°±æå–çš„ç‰¹å¾ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å‚…é‡Œå¶å˜æ¢ï¼Œç„¶åå¼•å…¥äº†ä¸€ä¸ªä¸å¯¹ç§°æ³¨æ„åŠ›çº¦æŸï¼Œè¯¥çº¦æŸä¼šå°†è¯ç‰©æ•æ„Ÿæ ·æœ¬èšé›†æˆä¸€ä¸ªç´§å‡‘çš„ç¾¤ä½“ï¼ŒåŒæ—¶ä½¿è€è¯æ ·æœ¬åœ¨é¢‘åŸŸä¸­åˆ†æ•£ã€‚æˆ‘ä»¬çš„ç»éªŒå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æœ‰æ•ˆåœ°ä»å¤šä¸ªæºåŸŸå­¦ä¹ äº†ä»»åŠ¡ç›¸å…³ç‰¹å¾ï¼Œå¹¶å¯¹æœªè§è¿‡çš„ç™Œç—‡ç±»å‹å®ç°äº†å‡†ç¡®çš„è¯ç‰©ååº”é¢„æµ‹ã€‚åœ¨å•ç»†èƒå’Œæ‚£è€…æ°´å¹³çš„è¯ç‰©ååº”é¢„æµ‹ä»»åŠ¡ä¸Šè¯„ä¼°æ—¶ï¼ŒFourierDrugï¼ˆä»…åœ¨ä½“å¤–ç»†èƒç³»æ•°æ®ä¸Šè®­ç»ƒï¼Œæ— éœ€è®¿é—®ç›®æ ‡åŸŸæ•°æ®ï¼‰å§‹ç»ˆä¼˜äºæˆ–è‡³å°‘ä¸å½“å‰æœ€æ–°æ–¹æ³•çš„è¡¨ç°ç›¸åŒ¹é…ã€‚è¿™äº›å‘ç°çªå‡ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®é™…ä¸´åºŠåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04034v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦é¢†åŸŸé¢„æµ‹è¯ç‰©ååº”ä»æ˜¯é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å•ç»†èƒå±‚é¢å’Œä¸´åºŠæ²»ç–—ç¯å¢ƒä¸­ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºFourierDrugçš„æ–°å‹åŸŸæ³›åŒ–æ¡†æ¶ï¼Œé€šè¿‡å‚…é‡Œå¶å˜æ¢å’Œä¸å¯¹ç§°æ³¨æ„åŠ›çº¦æŸï¼Œèƒ½åœ¨æœªæ¥è§¦ç›®æ ‡åŸŸæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆå­¦ä¹ æ¥è‡ªä¸åŒæºåŸŸçš„ä»»åŠ¡ç›¸å…³ç‰¹å¾ï¼Œå¹¶å¯¹æœªè§è¿‡çš„ç™Œç—‡ç±»å‹è¿›è¡Œè¯ç‰©ååº”é¢„æµ‹ã€‚åœ¨å•ç»†èƒå’Œæ‚£è€…å±‚é¢çš„è¯ç‰©ååº”é¢„æµ‹ä»»åŠ¡ä¸­ï¼ŒFourierDrugçš„è¡¨ç°ä¼˜äºæˆ–è‡³å°‘ä¸å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸åŒ¹é…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦é¢†åŸŸé¢„æµ‹è¯ç‰©ååº”æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å•ç»†èƒå’Œä¸´åºŠæ²»ç–—ç¯å¢ƒä¸­ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†åä¸ºFourierDrugçš„æ–°å‹åŸŸæ³›åŒ–æ¡†æ¶æ¥è§£å†³æ­¤æŒ‘æˆ˜ã€‚</li>
<li>FourierDrugåˆ©ç”¨å‚…é‡Œå¶å˜æ¢å’Œä¸å¯¹ç§°æ³¨æ„åŠ›çº¦æŸè¿›è¡Œç‰¹å¾æå–å’Œåˆ†ç±»ã€‚</li>
<li>è¯¥æ¨¡å‹å¯åœ¨æœªæ¥è§¦ç›®æ ‡åŸŸæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå­¦ä¹ æ¥è‡ªä¸åŒæºåŸŸçš„ä»»åŠ¡ç›¸å…³ç‰¹å¾ã€‚</li>
<li>FourierDrugåœ¨æœªè§è¿‡çš„ç™Œç—‡ç±»å‹è¯ç‰©ååº”é¢„æµ‹ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>åœ¨å•ç»†èƒå’Œæ‚£è€…å±‚é¢çš„è¯ç‰©ååº”é¢„æµ‹ä»»åŠ¡ä¸­ï¼ŒFourierDrugçš„è¡¨ç°ä¼˜äºæˆ–è‡³å°‘ä¸å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-87ba2fc87d40c563c56bedd0a2998f0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7492705d46aded771d10b9b187e727d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ccf7bf5e7876eaa874b026e2ceaf90f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Parameter-Efficient-Fine-Tuning-of-Segment-Anything-Model-for-Biomedical-Imaging"><a href="#Parameter-Efficient-Fine-Tuning-of-Segment-Anything-Model-for-Biomedical-Imaging" class="headerlink" title="Parameter Efficient Fine-Tuning of Segment Anything Model for Biomedical   Imaging"></a>Parameter Efficient Fine-Tuning of Segment Anything Model for Biomedical   Imaging</h2><p><strong>Authors:Carolin Teuber, Anwai Archit, Constantin Pape</strong></p>
<p>Segmentation is an important analysis task for biomedical images, enabling the study of individual organelles, cells or organs. Deep learning has massively improved segmentation methods, but challenges remain in generalization to new conditions, requiring costly data annotation. Vision foundation models, such as Segment Anything Model (SAM), address this issue through improved generalization. However, these models still require finetuning on annotated data, although with less annotations, to achieve optimal results for new conditions. As a downside, they require more computational resources. This makes parameter-efficient finetuning (PEFT) relevant. We contribute the first comprehensive study of PEFT for SAM applied to biomedical images. We find that the placement of PEFT layers is more important for efficiency than the type of layer for vision transformers and we provide a recipe for resource-efficient finetuning. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/computational-cell-analytics/peft-sam">https://github.com/computational-cell-analytics/peft-sam</a>. </p>
<blockquote>
<p>åˆ†å‰²æ˜¯ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æçš„é‡è¦ä»»åŠ¡ï¼Œèƒ½å¤Ÿå¯¹å•ä¸ªç»†èƒå™¨ã€ç»†èƒæˆ–å™¨å®˜è¿›è¡Œç ”ç©¶ã€‚æ·±åº¦å­¦ä¹ æå¤§åœ°æ”¹å–„äº†åˆ†å‰²æ–¹æ³•ï¼Œä½†ä»ç„¶å­˜åœ¨å¯¹æ–°æ¡ä»¶æ³›åŒ–æ–¹é¢çš„æŒ‘æˆ˜ï¼Œè¿™éœ€è¦æ˜‚è´µçš„æ ‡æ³¨æ•°æ®ã€‚è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚ä»»ä½•åˆ†å‰²æ¨¡å‹ï¼ˆSAMï¼‰ï¼‰é€šè¿‡æé«˜æ³›åŒ–èƒ½åŠ›æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹è™½ç„¶ä½¿ç”¨è¾ƒå°‘çš„æ ‡æ³¨æ•°æ®ï¼Œä½†ä»éœ€åœ¨æ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°æ–°æ¡ä»¶ä¸‹çš„æœ€ä½³ç»“æœã€‚ä½œä¸ºä»£ä»·ï¼Œå®ƒä»¬éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚è¿™ä½¿å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å˜å¾—è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å¯¹åº”ç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒçš„SAMçš„PEFTè¿›è¡Œäº†é¦–æ¬¡å…¨é¢ç ”ç©¶ã€‚æˆ‘ä»¬å‘ç°å¯¹äºè§†è§‰è½¬æ¢å™¨è€Œè¨€ï¼ŒPEFTå±‚çš„æ”¾ç½®æ¯”å±‚çš„ç±»å‹å¯¹æ•ˆç‡æ›´é‡è¦ï¼Œå¹¶ä¸ºèµ„æºé«˜æ•ˆçš„å¾®è°ƒæä¾›äº†ä¸€ä¸ªé…æ–¹ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/computational-cell-analytics/peft-sam%E3%80%82">https://github.com/computational-cell-analytics/peft-samã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00418v2">PDF</a> Published in MIDL 2025</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ä¸­å‘æŒ¥äº†å·¨å¤§ä½œç”¨ï¼Œä½†ä»å­˜åœ¨å¯¹æ–°æ¡ä»¶æ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ï¼Œéœ€è¦æ˜‚è´µçš„æ•°æ®æ ‡æ³¨æˆæœ¬ã€‚Segment Anything Modelï¼ˆSAMï¼‰ç­‰è§†è§‰åŸºç¡€æ¨¡å‹é€šè¿‡æ”¹è¿›æ³›åŒ–èƒ½åŠ›æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†ä»éœ€åœ¨æ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒä»¥è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ˜¯ä¸€ç§è§£å†³è¯¥é—®é¢˜çš„æ–¹æ³•ï¼Œæœ¬ç ”ç©¶é¦–æ¬¡å…¨é¢ç ”ç©¶äº†PEFTåœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒä¸­åº”ç”¨äºSAMçš„æ•ˆæœï¼Œå‘ç°å¯¹äºè§†è§‰è½¬æ¢å™¨è€Œè¨€ï¼ŒPEFTå±‚çš„æ”¾ç½®æ¯”å±‚çš„ç±»å‹æ›´é‡è¦ï¼Œå¹¶æä¾›äº†ä¸€ç§èµ„æºé«˜æ•ˆçš„å¾®è°ƒé…æ–¹ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/computational-cell-analytics/peft-sam%E3%80%82">https://github.com/computational-cell-analytics/peft-samã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä½†ä»é¢ä¸´å¯¹æ–°æ¡ä»¶æ³›åŒ–èƒ½åŠ›å’Œæ•°æ®æ ‡æ³¨æˆæœ¬çš„é—®é¢˜ã€‚</li>
<li>Segment Anything Modelï¼ˆSAMï¼‰é€šè¿‡æ”¹è¿›æ³›åŒ–èƒ½åŠ›æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†ä»éœ€å¾®è°ƒä»¥è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚</li>
<li>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ˜¯ä¸€ç§è§£å†³å¾®è°ƒæ‰€éœ€å¤§é‡è®¡ç®—èµ„æºçš„æ–¹æ³•ã€‚</li>
<li>PEFTåœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒä¸­åº”ç”¨äºSAMçš„æ•ˆæœç ”ç©¶æŒ‡å‡ºï¼ŒPEFTå±‚çš„æ”¾ç½®æ¯”å±‚çš„ç±»å‹æ›´é‡è¦ã€‚</li>
<li>ç ”ç©¶æä¾›äº†ä¸€ç§èµ„æºé«˜æ•ˆçš„å¾®è°ƒé…æ–¹ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ç›¸å…³ä»£ç å·²å…¬å¼€ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-326d75029ed32c3e2a6955ae29db5211.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb3b330a8b90a45f3765a0d01676afc3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a5410ed2838d6848c04a95341447c780.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e8ae7e1c0d7a73ff4ca1e016e76cade.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Segment-Anything-for-Histopathology"><a href="#Segment-Anything-for-Histopathology" class="headerlink" title="Segment Anything for Histopathology"></a>Segment Anything for Histopathology</h2><p><strong>Authors:Titus Griebel, Anwai Archit, Constantin Pape</strong></p>
<p>Nucleus segmentation is an important analysis task in digital pathology. However, methods for automatic segmentation often struggle with new data from a different distribution, requiring users to manually annotate nuclei and retrain data-specific models. Vision foundation models (VFMs), such as the Segment Anything Model (SAM), offer a more robust alternative for automatic and interactive segmentation. Despite their success in natural images, a foundation model for nucleus segmentation in histopathology is still missing. Initial efforts to adapt SAM have shown some success, but did not yet introduce a comprehensive model for diverse segmentation tasks. To close this gap, we introduce PathoSAM, a VFM for nucleus segmentation, based on training SAM on a diverse dataset. Our extensive experiments show that it is the new state-of-the-art model for automatic and interactive nucleus instance segmentation in histopathology. We also demonstrate how it can be adapted for other segmentation tasks, including semantic nucleus segmentation. For this task, we show that it yields results better than popular methods, while not yet beating the state-of-the-art, CellViT. Our models are open-source and compatible with popular tools for data annotation. We also provide scripts for whole-slide image segmentation. Our code and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/computational-cell-analytics/patho-sam">https://github.com/computational-cell-analytics/patho-sam</a>. </p>
<blockquote>
<p>ç»†èƒæ ¸åˆ†å‰²æ˜¯æ•°å­—ç—…ç†å­¦ä¸­çš„é‡è¦åˆ†æä»»åŠ¡ã€‚ç„¶è€Œï¼Œè‡ªåŠ¨åˆ†å‰²æ–¹æ³•é€šå¸¸éš¾ä»¥å¤„ç†æ¥è‡ªä¸åŒåˆ†å¸ƒçš„æ–°æ•°æ®ï¼Œéœ€è¦ç”¨æˆ·æ‰‹åŠ¨æ³¨é‡Šç»†èƒæ ¸å¹¶é‡æ–°è®­ç»ƒç‰¹å®šæ•°æ®æ¨¡å‹ã€‚ä¾‹å¦‚åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰çš„è§†é‡åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ä¸ºè‡ªåŠ¨å’Œäº¤äº’å¼åˆ†å‰²æä¾›äº†æ›´ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆã€‚å°½ç®¡å®ƒä»¬åœ¨è‡ªç„¶å›¾åƒä¸­å–å¾—äº†æˆåŠŸï¼Œä½†ç”¨äºç—…ç†å­¦ç»†èƒæ ¸åˆ†å‰²çš„åŸºç¡€æ¨¡å‹ä»ç„¶ç¼ºå¤±ã€‚åˆæ­¥é€‚åº”SAMçš„åŠªåŠ›å·²ç»å–å¾—äº†ä¸€äº›æˆåŠŸï¼Œä½†å¹¶æ²¡æœ‰æå‡ºä¸€ä¸ªç”¨äºå¤šç§åˆ†å‰²ä»»åŠ¡çš„å…¨é¢æ¨¡å‹ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†PathoSAMï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ ·æ•°æ®é›†è®­ç»ƒçš„ç»†èƒæ ¸åˆ†å‰²çš„VFMã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå®ƒæ˜¯ç—…ç†å­¦è‡ªåŠ¨å’Œäº¤äº’å¼ç»†èƒæ ¸å®ä¾‹åˆ†å‰²çš„æœ€æ–°æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†å¦‚ä½•å°†å…¶é€‚åº”äºå…¶ä»–åˆ†å‰²ä»»åŠ¡ï¼ŒåŒ…æ‹¬è¯­ä¹‰ç»†èƒæ ¸åˆ†å‰²ã€‚å¯¹äºæ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬å±•ç¤ºå®ƒäº§ç”Ÿçš„ç»“æœä¼˜äºæµè¡Œçš„æ–¹æ³•ï¼Œä½†å°šæœªè¾¾åˆ°å½“å‰æœ€å…ˆè¿›çš„CellViTæ¨¡å‹çš„æ°´å¹³ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¼€æºçš„ï¼Œä¸æµè¡Œçš„æ•°æ®æ³¨é‡Šå·¥å…·å…¼å®¹ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ç”¨äºå…¨å¹»ç¯ç‰‡å›¾åƒåˆ†å‰²çš„è„šæœ¬ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/computational-cell-analytics/patho-sam%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/computational-cell-analytics/patho-samå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00408v2">PDF</a> Published in MIDL 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºè®­ç»ƒå¤šæ ·åŒ–æ•°æ®é›†çš„Vision Foundation Modelï¼ˆPathoSAMï¼‰ç”¨äºç»†èƒæ ¸åˆ†å‰²çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨æ•°å­—ç—…ç†å­¦é¢†åŸŸå®ç°äº†è‡ªåŠ¨å’Œäº¤äº’å¼ç»†èƒæ ¸å®ä¾‹åˆ†å‰²çš„æ–°æ°´å¹³ï¼Œå¹¶å¯é€‚åº”å…¶ä»–åˆ†å‰²ä»»åŠ¡ã€‚æ¨¡å‹å¼€æºï¼Œå…¼å®¹æ•°æ®æ ‡æ³¨å·¥å…·ï¼Œå¹¶æä¾›å…¨åˆ‡ç‰‡å›¾åƒåˆ†å‰²è„šæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Nucleus segmentationæ˜¯æ•°å­—ç—…ç†å­¦ä¸­çš„é‡è¦åˆ†æä»»åŠ¡ï¼Œä½†è‡ªåŠ¨åˆ†å‰²æ–¹æ³•åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ï¼Œéœ€è¦ç”¨æˆ·æ‰‹åŠ¨æ ‡æ³¨ç»†èƒæ ¸å¹¶é‡æ–°è®­ç»ƒç‰¹å®šæ•°æ®æ¨¡å‹ã€‚</li>
<li>Vision Foundation Modelsï¼ˆVFMsï¼‰å¦‚Segment Anything Modelï¼ˆSAMï¼‰ä¸ºè‡ªåŠ¨å’Œäº¤äº’å¼åˆ†å‰²æä¾›äº†æ›´ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†åœ¨æ•°å­—ç—…ç†å­¦ä¸­ç¼ºå°‘ä¸“é—¨é’ˆå¯¹ç»†èƒæ ¸åˆ†å‰²çš„æ¨¡å‹ã€‚</li>
<li>PathoSAMæ˜¯ä¸€ä¸ªåŸºäºSAMçš„VFMæ¨¡å‹ï¼Œç»è¿‡å¤šæ ·åŒ–æ•°æ®é›†è®­ç»ƒï¼Œç”¨äºç»†èƒæ ¸åˆ†å‰²ã€‚</li>
<li>PathoSAMå®ç°äº†è‡ªåŠ¨å’Œäº¤äº’å¼ç»†èƒæ ¸å®ä¾‹åˆ†å‰²çš„æ–°æ°´å¹³ï¼Œå¹¶å¯ä»¥é€‚åº”å…¶ä»–åˆ†å‰²ä»»åŠ¡ï¼ŒåŒ…æ‹¬è¯­ä¹‰ç»†èƒæ ¸åˆ†å‰²ã€‚</li>
<li>è™½ç„¶PathoSAMåœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜ç§€æ€§èƒ½ï¼Œä½†åœ¨æŸäº›ä»»åŠ¡ä¸Šä»æœªè¾¾åˆ°å½“å‰æœ€ä½³æ¨¡å‹CellViTçš„æ€§èƒ½ã€‚</li>
<li>PathoSAMæ¨¡å‹æ˜¯å¼€æºçš„ï¼Œä¸æµè¡Œçš„æ•°æ®æ ‡æ³¨å·¥å…·å…¼å®¹ï¼Œå¹¶æä¾›å…¨åˆ‡ç‰‡å›¾åƒåˆ†å‰²è„šæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00408">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-709805a43b57b9af0b29dead130fd918.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76490c7ed8229245d1d4c5a9994648b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0aede69817dee97f06ffe639ce5c5d1c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="In-the-Picture-Medical-Imaging-Datasets-Artifacts-and-their-Living-Review"><a href="#In-the-Picture-Medical-Imaging-Datasets-Artifacts-and-their-Living-Review" class="headerlink" title="In the Picture: Medical Imaging Datasets, Artifacts, and their Living   Review"></a>In the Picture: Medical Imaging Datasets, Artifacts, and their Living   Review</h2><p><strong>Authors:Amelia JimÃ©nez-SÃ¡nchez, Natalia-Rozalia Avlona, Sarah de Boer, VÃ­ctor M. Campello, Aasa Feragen, Enzo Ferrante, Melanie Ganz, Judy Wawira Gichoya, Camila GonzÃ¡lez, Steff Groefsema, Alessa Hering, Adam Hulman, Leo Joskowicz, Dovile Juodelyte, Melih Kandemir, Thijs Kooi, Jorge del Pozo LÃ©rida, Livie Yumeng Li, Andre Pacheco, Tim RÃ¤dsch, Mauricio Reyes, ThÃ©o Sourget, Bram van Ginneken, David Wen, Nina Weng, Jack Junchi Xu, Hubert Dariusz ZajÄ…c, Maria A. Zuluaga, Veronika Cheplygina</strong></p>
<p>Datasets play a critical role in medical imaging research, yet issues such as label quality, shortcuts, and metadata are often overlooked. This lack of attention may harm the generalizability of algorithms and, consequently, negatively impact patient outcomes. While existing medical imaging literature reviews mostly focus on machine learning (ML) methods, with only a few focusing on datasets for specific applications, these reviews remain static â€“ they are published once and not updated thereafter. This fails to account for emerging evidence, such as biases, shortcuts, and additional annotations that other researchers may contribute after the dataset is published. We refer to these newly discovered findings of datasets as research artifacts. To address this gap, we propose a living review that continuously tracks public datasets and their associated research artifacts across multiple medical imaging applications. Our approach includes a framework for the living review to monitor data documentation artifacts, and an SQL database to visualize the citation relationships between research artifact and dataset. Lastly, we discuss key considerations for creating medical imaging datasets, review best practices for data annotation, discuss the significance of shortcuts and demographic diversity, and emphasize the importance of managing datasets throughout their entire lifecycle. Our demo is publicly available at <a target="_blank" rel="noopener" href="http://inthepicture.itu.dk/">http://inthepicture.itu.dk/</a>. </p>
<blockquote>
<p>æ•°æ®é›†åœ¨åŒ»å­¦æˆåƒç ”ç©¶ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œç„¶è€Œæ ‡ç­¾è´¨é‡ã€æ·å¾„å’Œå…ƒæ•°æ®ç­‰é—®é¢˜å¾€å¾€è¢«å¿½è§†ã€‚è¿™ç§ç¼ºä¹å…³æ³¨å¯èƒ½ä¼šæŸå®³ç®—æ³•çš„é€šç”¨æ€§ï¼Œå¹¶å› æ­¤å¯¹æ‚£è€…ç»“æœäº§ç”Ÿè´Ÿé¢å½±å“ã€‚å°½ç®¡ç°æœ‰çš„åŒ»å­¦æˆåƒæ–‡çŒ®ç»¼è¿°ä¸»è¦é›†ä¸­åœ¨æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ–¹æ³•ä¸Šï¼Œåªæœ‰å°‘æ•°å…³æ³¨ç‰¹å®šåº”ç”¨çš„æ•°æ®é›†ï¼Œä½†è¿™äº›ç»¼è¿°æ˜¯é™æ€çš„â€”â€”ä¸€ç»å‘å¸ƒï¼Œå°±ä¸å†æ›´æ–°ã€‚è¿™æœªèƒ½è€ƒè™‘åˆ°æ–°å…´è¯æ®ï¼Œå¦‚åè§ã€æ·å¾„å’Œå…¶ä»–ç ”ç©¶äººå‘˜åœ¨æ•°æ®é›†å‘å¸ƒåå¯èƒ½åšå‡ºçš„é¢å¤–æ³¨é‡Šç­‰ã€‚æˆ‘ä»¬å°†è¿™äº›æ–°å‘ç°çš„æ•°æ®é›†ç§°ä¸ºç ”ç©¶äº§ç‰©ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æŒç»­è¿½è¸ªå¤šä¸ªåŒ»å­¦æˆåƒåº”ç”¨ä¸­çš„å…¬å…±æ•°æ®é›†åŠå…¶ç›¸å…³ç ”ç©¶äº§ç‰©çš„åŠ¨æ€ç»¼è¿°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªç”¨äºç›‘æ§æ•°æ®æ–‡æ¡£äº§ç‰©å’Œå¯è§†åŒ–æ•°æ®é›†ä¸ç ”ç©¶äº§ç‰©ä¹‹é—´å¼•æ–‡å…³ç³»çš„SQLæ•°æ®åº“æ¡†æ¶ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†åˆ›å»ºåŒ»å­¦æˆåƒæ•°æ®é›†çš„å…³é”®æ³¨æ„äº‹é¡¹ï¼Œå›é¡¾äº†æ•°æ®æ³¨é‡Šçš„æœ€ä½³å®è·µï¼Œè®¨è®ºäº†æ·å¾„å’Œäººå£å¤šæ ·æ€§çš„é‡è¦æ€§ï¼Œå¹¶å¼ºè°ƒäº†æ•´ä¸ªæ•°æ®ç”Ÿå‘½å‘¨æœŸä¸­ç®¡ç†æ•°æ®é›†çš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„æ¼”ç¤ºå…¬å¼€è®¿é—®åœ°å€ä¸º <a target="_blank" rel="noopener" href="http://inthepicture.itu.dk/">http://inthepicture.itu.dk/</a>.</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10727v2">PDF</a> ACM Conference on Fairness, Accountability, and Transparency - FAccT   2025</p>
<p><strong>Summary</strong><br>     åŒ»å­¦æˆåƒç ”ç©¶ä¸­æ•°æ®é›†è‡³å…³é‡è¦ï¼Œä½†æ ‡ç­¾è´¨é‡ã€æ·å¾„å’Œå…ƒæ•°æ®ç­‰é—®é¢˜å¸¸è¢«å¿½è§†ã€‚è¿™äº›é—®é¢˜å¯èƒ½å½±å“ç®—æ³•æ³›åŒ–èƒ½åŠ›ï¼Œè¿›è€Œå½±å“æ‚£è€…æ²»ç–—æ•ˆæœã€‚ç°æœ‰åŒ»å­¦æˆåƒæ–‡çŒ®ç»¼è¿°ä¸»è¦å…³æ³¨æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ–¹æ³•ï¼Œä»…å°‘æ•°å…³æ³¨ç‰¹å®šåº”ç”¨çš„æ•°æ®é›†ï¼Œä½†è¿™äº›ç»¼è¿°ä¸€ç»å‘å¸ƒä¾¿ä¸å†æ›´æ–°ï¼Œæ— æ³•æ¶µç›–æ–°å…´è¯æ®ï¼Œå¦‚åè§ã€æ·å¾„å’Œé™„åŠ æ³¨é‡Šç­‰ç ”ç©¶è€…è´¡çŒ®çš„ç ”ç©¶å·¥ä»¶ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæŒç»­è¿½è¸ªå¤šä¸ªåŒ»å­¦æˆåƒåº”ç”¨å…¬å…±æ•°æ®é›†åŠå…¶ç›¸å…³ç ”ç©¶å·¥ä»¶çš„åŠ¨æ€ç»¼è¿°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªç”¨äºç›‘æ§æ•°æ®æ–‡æ¡£å·¥ä»¶åŠ¨æ€ç»¼è¿°çš„æ¡†æ¶å’Œä¸€ä¸ªSQLæ•°æ®åº“ï¼Œä»¥å¯è§†åŒ–ç ”ç©¶å·¥ä»¶ä¸æ•°æ®é›†ä¹‹é—´çš„å¼•æ–‡å…³ç³»ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†åˆ›å»ºåŒ»å­¦æˆåƒæ•°æ®é›†çš„å…³é”®è€ƒé‡å› ç´ ï¼Œå›é¡¾äº†æ•°æ®æ³¨é‡Šçš„æœ€ä½³å®è·µï¼Œè®¨è®ºäº†æ·å¾„å’Œäººå£å¤šæ ·æ€§çš„é‡è¦æ€§ï¼Œå¹¶å¼ºè°ƒäº†æ•´ä¸ªæ•°æ®é›†ç”Ÿå‘½å‘¨æœŸä¸­ç®¡ç†æ•°æ®é›†çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>æ•°æ®é›†åœ¨åŒ»å­¦æˆåƒç ”ç©¶ä¸­çš„é‡è¦æ€§åŠå…¶å¸¸å¿½ç•¥çš„é—®é¢˜ï¼Œå¦‚æ ‡ç­¾è´¨é‡ã€æ·å¾„å’Œå…ƒæ•°æ®ã€‚</p>
</li>
<li><p>ç°æœ‰æ–‡çŒ®ç»¼è¿°çš„å±€é™æ€§ï¼Œå¯¹æ–°å…´è¯æ®ï¼ˆå¦‚åè§ã€æ·å¾„ç­‰ç ”ç©¶å·¥ä»¶ï¼‰æ— æ³•å……åˆ†è¦†ç›–çš„é—®é¢˜ã€‚ </p>
</li>
<li><p>æè®®çš„â€œåŠ¨æ€ç»¼è¿°â€çš„æ¦‚å¿µæ¥è·Ÿè¸ªå…¬å…±æ•°æ®é›†åŠå…¶ç›¸å…³ç ”ç©¶ç»“æœå·¥ä»¶çš„åº”ç”¨è¡¨ç°è·¨å¤šä¸ªåŒ»å­¦æˆåƒé¢†åŸŸçš„è§‚ç‚¹åŠå…¶ä¼˜åŠ¿ã€‚  </p>
</li>
<li><p>é€šè¿‡ç›‘æ§æ•°æ®æ–‡æ¡£å·¥ä»¶çš„æ–¹å¼æå‡ºæ¡†æ¶ä»¥åº”å¯¹åŠ¨æ€ç»¼è¿°çš„è¦æ±‚ä»¥åŠå¯è§†åŒ–æ•°æ®åº“æè¿°ç ”ç©¶å·¥ä»¶ä¸æ•°æ®é›†ä¹‹é—´çš„å…³ç³»çš„æè¿°ã€‚ </p>
</li>
<li><p>è®¨è®ºåˆ›å»ºåŒ»å­¦æˆåƒæ•°æ®é›†æ—¶çš„å…³é”®è€ƒé‡å› ç´ ã€‚  </p>
</li>
<li><p>ä»‹ç»å¹¶å›é¡¾æ•°æ®æ³¨é‡Šçš„æœ€ä½³å®è·µåŠå…¶æ„ä¹‰å’Œä»·å€¼é‡è¦æ€§å¢åŠ æ–°çš„è§’åº¦è€ƒé‡å¯¹ç°æœ‰çš„æ¦‚å¿µæœ‰å…¨é¢æ›´æ·±å…¥çš„ç†è§£å¯¹æ•°æ®å’Œç®—æ³•æœ‰æ›´å…¨é¢çš„äº†è§£å¯¹æ•°æ®å’Œç®—æ³•çš„ä¼˜åŒ–æœ‰æ›´æ¸…æ™°çš„æ€è·¯å¯¹ç§‘ç ”ç ”ç©¶æœ‰æ›´æ·±åˆ»çš„ç†è§£å’Œåº”ç”¨ä»·å€¼å¯¹ç§‘ç ”å®è·µæœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰å¼ºè°ƒæ·å¾„å’Œäººå£å¤šæ ·æ€§çš„é‡è¦æ€§åœ¨æ•´ä¸ªæ•°æ®é›†ä¸­å¦‚ä½•é¿å…å¹¶æœ‰æ•ˆåˆ©ç”¨è¿™äº›æ·å¾„å¢åŠ æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®å¤šæ ·æ€§çš„æ„ä¹‰å¯¹äºè§£å†³å®é™…åº”ç”¨ä¸­çš„é—®é¢˜æä¾›æ›´å…¨é¢æœ‰æ•ˆçš„æ€è·¯å’Œæ–¹å‘é€šè¿‡ç†è§£ä¸åŒæ•°æ®å’Œç®—æ³•çš„ä¼˜åŠ£åŠ¿å½¢æˆå…¨é¢çš„ç†è§£å’Œç§‘å­¦çš„å®è·µå»ºè®®å¯ä»¥å……åˆ†åˆ©ç”¨å¹¶è§„é¿æ•°æ®çš„å±€é™æé«˜ç ”ç©¶è´¨é‡å’Œæˆæœåº”ç”¨çš„å¹¿åº¦å¯¹å½“å‰å­¦æœ¯å’Œå®è·µçš„å½±å“ä¸ºè¿›ä¸€æ­¥çš„å­¦æœ¯ç ”ç©¶å’ŒæŠ€æœ¯åˆ›æ–°æä¾›äº†åŸºç¡€å’Œåº”ç”¨ä»·å€¼æœ‰åŠ©äºç›¸å…³é¢†åŸŸçš„ç§‘ç ”å·¥ä½œè€…æ›´å¥½åœ°ç†è§£å’Œåº”ç”¨åŒ»å­¦æˆåƒæ•°æ®é›†ã€‚</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5755be9d62321239b04de17a7fc3cd69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40c8200ee549b56e8a5d69727bac3284.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ade82dc7cec410670becac191b52cc0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MADUV-The-1st-INTERSPEECH-Mice-Autism-Detection-via-Ultrasound-Vocalization-Challenge"><a href="#MADUV-The-1st-INTERSPEECH-Mice-Autism-Detection-via-Ultrasound-Vocalization-Challenge" class="headerlink" title="MADUV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound   Vocalization Challenge"></a>MADUV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound   Vocalization Challenge</h2><p><strong>Authors:Zijiang Yang, Meishu Song, Xin Jing, Haojie Zhang, Kun Qian, Bin Hu, Kota Tamada, Toru Takumi, BjÃ¶rn W. Schuller, Yoshiharu Yamamoto</strong></p>
<p>The Mice Autism Detection via Ultrasound Vocalization (MADUV) Challenge introduces the first INTERSPEECH challenge focused on detecting autism spectrum disorder (ASD) in mice through their vocalizations. Participants are tasked with developing models to automatically classify mice as either wild-type or ASD models based on recordings with a high sampling rate. Our baseline system employs a simple CNN-based classification using three different spectrogram features. Results demonstrate the feasibility of automated ASD detection, with the considered audible-range features achieving the best performance (UAR of 0.600 for segment-level and 0.625 for subject-level classification). This challenge bridges speech technology and biomedical research, offering opportunities to advance our understanding of ASD models through machine learning approaches. The findings suggest promising directions for vocalization analysis and highlight the potential value of audible and ultrasound vocalizations in ASD detection. </p>
<blockquote>
<p>é€šè¿‡è¶…å£°æ³¢å‘å£°ï¼ˆMADUVï¼‰æ£€æµ‹å°é¼ è‡ªé—­ç—‡ï¼ˆMice Autism Detection via Ultrasound Vocalizationï¼Œç®€ç§°MADUVï¼‰æŒ‘æˆ˜å¼•å…¥äº†é¦–ä¸ªèšç„¦äºé€šè¿‡å°é¼ çš„å‘å£°æ¥æ£€æµ‹è‡ªé—­ç—‡è°±ç³»éšœç¢ï¼ˆASDï¼‰çš„INTERSPEECHæŒ‘æˆ˜ã€‚å‚èµ›è€…çš„ä»»åŠ¡æ˜¯å¼€å‘æ¨¡å‹ï¼Œæ ¹æ®é«˜é‡‡æ ·ç‡çš„å½•éŸ³è‡ªåŠ¨å°†å°é¼ åˆ†ç±»ä¸ºé‡ç”Ÿå‹æˆ–ASDæ¨¡å‹ã€‚æˆ‘ä»¬çš„åŸºçº¿ç³»ç»Ÿé‡‡ç”¨åŸºäºç®€å•å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„åˆ†ç±»æ–¹æ³•ï¼Œä½¿ç”¨ä¸‰ç§ä¸åŒçš„é¢‘è°±ç‰¹å¾ã€‚ç»“æœè¡¨æ˜ï¼Œè‡ªåŠ¨åŒ–æ£€æµ‹ASDæ˜¯å¯è¡Œçš„ï¼Œåœ¨è€ƒè™‘çš„éŸ³é¢‘èŒƒå›´å†…çš„ç‰¹å¾å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼ˆåˆ†æ®µçº§åˆ«çš„UARä¸º0.600ï¼Œä¸»ä½“çº§åˆ«çš„åˆ†ç±»ä¸º0.625ï¼‰ã€‚æ­¤æ¬¡æŒ‘æˆ˜å°†è¯­éŸ³æŠ€æœ¯ä¸ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ç›¸ç»“åˆï¼Œä¸ºé€šè¿‡æœºå™¨å­¦ä¹ æ–¹æ³•äº†è§£ASDæ¨¡å‹æä¾›äº†æœºä¼šã€‚ç ”ç©¶ç»“æœè¡¨æ˜äº†å‘å£°åˆ†æçš„å¸Œæœ›æ–¹å‘ï¼Œå¹¶çªå‡ºäº†å¬è§‰å’Œè¶…å£°æ³¢å‘å£°åœ¨ASDæ£€æµ‹ä¸­çš„æ½œåœ¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04292v3">PDF</a> 5 pages, 1 figure and 2 tables. Submitted to INTERSPEECH 2025. For   MADUV Challenge 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é€šè¿‡è¶…å£°å‘å£°æ£€æµ‹å°é¼ è‡ªé—­ç—‡ï¼ˆMADUVï¼‰æŒ‘æˆ˜ï¼Œè¿™æ˜¯é¦–ä¸ªèšç„¦äºé€šè¿‡å°é¼ å‘å£°æ£€æµ‹è‡ªé—­ç—‡è°±ç³»éšœç¢ï¼ˆASDï¼‰çš„INTERSPEECHæŒ‘æˆ˜ã€‚å‚ä¸è€…éœ€å¼€å‘æ¨¡å‹è‡ªåŠ¨å°†å°é¼ åˆ†ç±»ä¸ºé‡ç”Ÿå‹æˆ–ASDæ¨¡å‹ï¼Œä¾æ®çš„æ˜¯é«˜é‡‡æ ·ç‡å½•éŸ³ã€‚åŸºçº¿ç³»ç»Ÿé‡‡ç”¨ç®€å•çš„åŸºäºCNNçš„åˆ†ç±»æ–¹æ³•ï¼Œä½¿ç”¨ä¸‰ç§ä¸åŒçš„é¢‘è°±ç‰¹å¾ã€‚ç»“æœè¡¨æ˜è‡ªåŠ¨åŒ–ASDæ£€æµ‹çš„å¯è¡Œæ€§ï¼Œæ‰€è€ƒè™‘çš„å¬è§‰èŒƒå›´ç‰¹å¾å–å¾—æœ€ä½³æ€§èƒ½ï¼ˆåˆ†æ®µçº§åˆ«UARä¸º0.600ï¼Œä¸»é¢˜çº§åˆ«åˆ†ç±»ä¸º0.625ï¼‰ã€‚è¯¥æŒ‘æˆ˜å°†è¯­éŸ³è¯†åˆ«æŠ€æœ¯ä¸ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ç›¸ç»“åˆï¼Œé€šè¿‡æœºå™¨å­¦ä¹ çš„æ–¹æ³•æ¨åŠ¨å¯¹ASDæ¨¡å‹çš„ç†è§£ã€‚å‘ç°è¡¨æ˜å£°éŸ³åˆ†æçš„æ–¹å‘å…·æœ‰å¸Œæœ›ï¼Œå¹¶çªæ˜¾äº†å¬è§‰å’Œè¶…å£°æ³¢å‘å£°åœ¨ASDæ£€æµ‹ä¸­çš„æ½œåœ¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MADUVæŒ‘æˆ˜æ˜¯é¦–ä¸ªèšç„¦äºé€šè¿‡å°é¼ å‘å£°æ£€æµ‹è‡ªé—­ç—‡è°±ç³»éšœç¢ï¼ˆASDï¼‰çš„INTERSPEECHæŒ‘æˆ˜ã€‚</li>
<li>å‚ä¸è€…éœ€å¼€å‘æ¨¡å‹è‡ªåŠ¨åˆ†ç±»å°é¼ ï¼ŒåŒºåˆ†é‡ç”Ÿå‹å’ŒASDæ¨¡å‹ï¼Œä¾æ®é«˜é‡‡æ ·ç‡å½•éŸ³ã€‚</li>
<li>åŸºçº¿ç³»ç»Ÿé‡‡ç”¨ç®€å•çš„åŸºäºCNNçš„åˆ†ç±»æ–¹æ³•ï¼Œä½¿ç”¨ä¸‰ç§ä¸åŒçš„é¢‘è°±ç‰¹å¾è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>è‡ªåŠ¨åŒ–ASDæ£€æµ‹çš„å¯è¡Œæ€§å¾—åˆ°éªŒè¯ï¼Œå…¶ä¸­å¬è§‰èŒƒå›´ç‰¹å¾è¡¨ç°æœ€ä½³ã€‚</li>
<li>åˆ†æ®µçº§åˆ«å’Œä¸»é¢˜çº§åˆ«çš„åˆ†ç±»æ€§èƒ½åˆ†åˆ«è¾¾åˆ°äº†UAR 0.600å’Œ0.625ã€‚</li>
<li>è¯¥æŒ‘æˆ˜æ¨åŠ¨äº†è¯­éŸ³è¯†åˆ«æŠ€æœ¯ä¸ç”Ÿç‰©åŒ»å­¦ç ”ç©¶çš„ç»“åˆï¼Œé€šè¿‡æœºå™¨å­¦ä¹ çš„æ–¹æ³•åŠ æ·±å¯¹ASDæ¨¡å‹çš„ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04292">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-73b11d0f6c0d21128ff8ce2ab937a88f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-476223deedecb318184529c3e6fe089d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8491a401c899c97e5ecc91635dac954a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9321738a231992c91216abd139a8b05.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SCC-YOLO-An-Improved-Object-Detector-for-Assisting-in-Brain-Tumor-Diagnosis"><a href="#SCC-YOLO-An-Improved-Object-Detector-for-Assisting-in-Brain-Tumor-Diagnosis" class="headerlink" title="SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor   Diagnosis"></a>SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor   Diagnosis</h2><p><strong>Authors:Runci Bai, Guibao Xu, Yanze Shi</strong></p>
<p>Brain tumors can lead to neurological dysfunction, cognitive and psychological changes, increased intracranial pressure, and seizures, posing significant risks to health. The You Only Look Once (YOLO) series has shown superior accuracy in medical imaging object detection. This paper presents a novel SCC-YOLO architecture that integrates the SCConv module into YOLOv9. The SCConv module optimizes convolutional efficiency by reducing spatial and channel redundancy, enhancing image feature learning. We examine the effects of different attention mechanisms with YOLOv9 for brain tumor detection using the Br35H dataset and our custom dataset (Brain_Tumor_Dataset). Results indicate that SCC-YOLO improved mAP50 by 0.3% on the Br35H dataset and by 0.5% on our custom dataset compared to YOLOv9. SCC-YOLO achieves state-of-the-art performance in brain tumor detection. </p>
<blockquote>
<p>è„‘è‚¿ç˜¤å¯èƒ½å¯¼è‡´ç¥ç»åŠŸèƒ½éšœç¢ã€è®¤çŸ¥å’Œå¿ƒç†å­¦å˜åŒ–ã€é¢…å†…å‹å‡é«˜å’Œç™«ç—«å‘ä½œï¼Œå¯¹å¥åº·æ„æˆé‡å¤§é£é™©ã€‚You Only Look Onceï¼ˆYOLOï¼‰ç³»åˆ—å·²åœ¨åŒ»å­¦å½±åƒç›®æ ‡æ£€æµ‹ä¸­å±•ç°å‡ºå“è¶Šå‡†ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„SCC-YOLOæ¶æ„ï¼Œå®ƒå°†SCConvæ¨¡å—é›†æˆåˆ°YOLOv9ä¸­ã€‚SCConvæ¨¡å—é€šè¿‡å‡å°‘ç©ºé—´å†—ä½™å’Œé€šé“å†—ä½™æ¥ä¼˜åŒ–å·ç§¯æ•ˆç‡ï¼Œä»è€Œå¢å¼ºå›¾åƒç‰¹å¾å­¦ä¹ ã€‚æˆ‘ä»¬ä½¿ç”¨Br35Hæ•°æ®é›†å’Œæˆ‘ä»¬è‡ªå·±çš„æ•°æ®é›†ï¼ˆBrain_Tumor_Datasetï¼‰æ¥æ£€éªŒä¸åŒæ³¨æ„åŠ›æœºåˆ¶å¯¹YOLOv9åœ¨è„‘è‚¿ç˜¤æ£€æµ‹ä¸Šçš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œä¸YOLOv9ç›¸æ¯”ï¼ŒSCC-YOLOåœ¨Br35Hæ•°æ®é›†ä¸Šçš„mAP50æé«˜äº†0.3%ï¼Œåœ¨æˆ‘ä»¬è‡ªå·±çš„æ•°æ®é›†ä¸Šæé«˜äº†0.5%ã€‚SCC-YOLOåœ¨è„‘è‚¿ç˜¤æ£€æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03836v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„SCC-YOLOæ¶æ„ï¼Œå®ƒå°†SCConvæ¨¡å—é›†æˆåˆ°YOLOv9ä¸­ä»¥æé«˜åŒ»å­¦å›¾åƒä¸­ç›®æ ‡æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨è„‘è‚¿ç˜¤æ£€æµ‹æ–¹é¢ï¼ŒSCC-YOLOåœ¨Br35Hæ•°æ®é›†ä¸Šçš„mAP50æé«˜äº†0.3%ï¼Œåœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šçš„mAP50æé«˜äº†0.5%ï¼Œè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­æ¢è®¨äº†è„‘è‚¿ç˜¤å¯èƒ½å¼•å‘çš„å¥åº·é£é™©ï¼Œå¦‚ç¥ç»åŠŸèƒ½éšœç¢ã€è®¤çŸ¥å’Œå¿ƒç†å˜åŒ–ç­‰ã€‚</li>
<li>YOLOç³»åˆ—åœ¨åŒ»å­¦æˆåƒç‰©ä½“æ£€æµ‹ä¸­å±•ç°å‡ºé«˜ç²¾åº¦æ€§èƒ½ã€‚</li>
<li>æ–°æå‡ºçš„SCC-YOLOæ¶æ„é›†æˆäº†SCConvæ¨¡å—ä»¥æå‡å›¾åƒç‰¹å¾å­¦ä¹ èƒ½åŠ›å¹¶ä¼˜åŒ–å·ç§¯æ•ˆç‡ã€‚</li>
<li>ç ”ç©¶è€…é€šè¿‡Br35Hæ•°æ®é›†å’Œè‡ªå®šä¹‰æ•°æ®é›†ï¼ˆBrain_Tumor_Datasetï¼‰è¯„ä¼°äº†ä¸åŒæ³¨æ„åŠ›æœºåˆ¶å¯¹è„‘è‚¿ç˜¤æ£€æµ‹çš„å½±å“ã€‚</li>
<li>SCC-YOLOç›¸è¾ƒäºYOLOv9åœ¨Br35Hæ•°æ®é›†ä¸Šçš„mAP50æé«˜äº†0.3%ï¼Œåœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šæé«˜äº†0.5%ã€‚</li>
<li>SCC-YOLOåœ¨è„‘è‚¿ç˜¤æ£€æµ‹æ–¹é¢è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9bc8dc1d7eebd3b81f562305c90f90cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4f6d06940ae5bf0eafca03c04e52691.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acbfacca7b571a58bf0b0fa81abad8b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb6961d8099ccbc5926cd21d76083524.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5a030700c8e9ee0a682b707ec4d1c9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99fa394558c33c410fafaa9e299b78de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df32214ff3ee1d3949d63a78474639a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f6ffde02c70c0b5df5c3471db59e70f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Exploring-Compositional-Generalization-of-Multimodal-LLMs-for-Medical-Imaging"><a href="#Exploring-Compositional-Generalization-of-Multimodal-LLMs-for-Medical-Imaging" class="headerlink" title="Exploring Compositional Generalization of Multimodal LLMs for Medical   Imaging"></a>Exploring Compositional Generalization of Multimodal LLMs for Medical   Imaging</h2><p><strong>Authors:Zhenyang Cai, Junying Chen, Rongsheng Wang, Weihong Wang, Yonglin Deng, Dingjie Song, Yize Chen, Zixu Zhang, Benyou Wang</strong></p>
<p>Medical imaging provides essential visual insights for diagnosis, and multimodal large language models (MLLMs) are increasingly utilized for its analysis due to their strong generalization capabilities; however, the underlying factors driving this generalization remain unclear. Current research suggests that multi-task training outperforms single-task as different tasks can benefit each other, but they often overlook the internal relationships within these tasks. To analyze this phenomenon, we attempted to employ compositional generalization (CG), which refers to the modelsâ€™ ability to understand novel combinations by recombining learned elements, as a guiding framework. Since medical images can be precisely defined by Modality, Anatomical area, and Task, naturally providing an environment for exploring CG, we assembled 106 medical datasets to create Med-MAT for comprehensive experiments. The experiments confirmed that MLLMs can use CG to understand unseen medical images and identified CG as one of the main drivers of the generalization observed in multi-task training. Additionally, further studies demonstrated that CG effectively supports datasets with limited data and confirmed that MLLMs can achieve CG across classification and detection tasks, underscoring its broader generalization potential. Med-MAT is available at <a target="_blank" rel="noopener" href="https://github.com/FreedomIntelligence/Med-MAT">https://github.com/FreedomIntelligence/Med-MAT</a>. </p>
<blockquote>
<p>åŒ»å­¦æˆåƒä¸ºè¯Šæ–­æä¾›äº†å¿…è¦çš„è§†è§‰æ´å¯ŸåŠ›ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”±äºå…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›è€Œè¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºåŒ»å­¦å›¾åƒåˆ†æï¼›ç„¶è€Œï¼Œé©±åŠ¨è¿™ç§æ³›åŒ–çš„æ½œåœ¨å› ç´ å°šä¸æ¸…æ¥šã€‚å½“å‰çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤šä»»åŠ¡è®­ç»ƒä¼˜äºå•ä»»åŠ¡è®­ç»ƒï¼Œå› ä¸ºä¸åŒçš„ä»»åŠ¡å¯ä»¥ç›¸äº’å—ç›Šï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šå¿½ç•¥è¿™äº›ä»»åŠ¡ä¹‹é—´çš„å†…éƒ¨å…³ç³»ã€‚ä¸ºäº†åˆ†æè¿™ä¸€ç°è±¡ï¼Œæˆ‘ä»¬å°è¯•é‡‡ç”¨ç»„åˆæ³›åŒ–ï¼ˆCGï¼‰ä½œä¸ºæŒ‡å¯¼æ¡†æ¶ï¼Œç»„åˆæ³›åŒ–æ˜¯æŒ‡æ¨¡å‹é€šè¿‡é‡æ–°ç»„åˆå·²å­¦ä¹ çš„å…ƒç´ æ¥ç†è§£æ–°å‹ç»„åˆçš„èƒ½åŠ›ã€‚ç”±äºåŒ»å­¦å›¾åƒå¯ä»¥é€šè¿‡æ¨¡æ€ã€è§£å‰–éƒ¨ä½å’Œä»»åŠ¡è¿›è¡Œç²¾ç¡®å®šä¹‰ï¼Œè‡ªç„¶åœ°ä¸ºæ¢ç´¢CGæä¾›äº†ç¯å¢ƒï¼Œæˆ‘ä»¬æ”¶é›†äº†106ä¸ªåŒ»å­¦æ•°æ®é›†ï¼Œåˆ›å»ºäº†Med-MATç”¨äºè¿›è¡Œç»¼åˆå®éªŒã€‚å®éªŒè¯å®ï¼ŒMLLMså¯ä»¥åˆ©ç”¨CGæ¥ç†è§£æœªè§è¿‡çš„åŒ»å­¦å›¾åƒï¼Œå¹¶ç¡®å®šCGæ˜¯å¤šä»»åŠ¡è®­ç»ƒä¸­è§‚å¯Ÿåˆ°çš„æ³›åŒ–çš„ä¸»è¦é©±åŠ¨å› ç´ ä¹‹ä¸€ã€‚å¦å¤–ï¼Œè¿›ä¸€æ­¥çš„ç ”ç©¶è¡¨æ˜ï¼ŒCGæœ‰æ•ˆåœ°æ”¯æŒäº†æ•°æ®æœ‰é™çš„æ•°æ®é›†ï¼Œå¹¶è¯å®MLLMså¯ä»¥åœ¨åˆ†ç±»å’Œæ£€æµ‹ä»»åŠ¡ä¸­å®ç°CGï¼Œçªå‡ºäº†å…¶æ›´å¹¿æ³›çš„æ³›åŒ–æ½œåŠ›ã€‚Med-MATå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/FreedomIntelligence/Med-MAT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/FreedomIntelligence/Med-MATè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20070v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨å¹¿èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯å…¶é€šè¿‡ç»„åˆå­¦ä¹ åˆ°çš„å…ƒç´ ç†è§£æ–°é¢–ç»„åˆçš„èƒ½åŠ›â€”â€”å³ç»„åˆæ¦‚æ‹¬èƒ½åŠ›ï¼ˆCGï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMLLMså¯åˆ©ç”¨CGç†è§£æœªè§è¿‡çš„åŒ»å­¦å›¾åƒï¼Œç»„åˆæ¦‚æ‹¬èƒ½åŠ›æ˜¯å¤šä»»åŠ¡è®­ç»ƒä¸­çš„ä¸»è¦æ¨å¹¿é©±åŠ¨åŠ›ä¹‹ä¸€ã€‚æ­¤å¤–ï¼ŒCGæ”¯æŒæœ‰é™æ•°æ®é›†ï¼Œå¹¶å¯åœ¨åˆ†ç±»å’Œæ£€æµ‹ä»»åŠ¡ä¸­å®ç°è·¨ä»»åŠ¡æ¦‚æ‹¬ï¼Œå±•ç°å‡ºå…¶æ›´å¹¿æ³›çš„æ¨å¹¿æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å…·æœ‰å¼ºå¤§çš„æ¨å¹¿èƒ½åŠ›ã€‚</li>
<li>ç»„åˆæ¦‚æ‹¬èƒ½åŠ›ï¼ˆCGï¼‰æ˜¯ç†è§£æ–°é¢–ç»„åˆçš„å…³é”®ï¼Œæœ‰åŠ©äºæ¨¡å‹ç†è§£æœªè§è¿‡çš„åŒ»å­¦å›¾åƒã€‚</li>
<li>å¤šä»»åŠ¡è®­ç»ƒåœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å…·æœ‰ä¼˜åŠ¿ï¼Œä¸åŒä»»åŠ¡é—´å¯ä»¥ç›¸äº’å—ç›Šã€‚</li>
<li>CGæ˜¯å¤šä»»åŠ¡è®­ç»ƒä¸­çš„ä¸»è¦æ¨å¹¿é©±åŠ¨åŠ›ä¹‹ä¸€ã€‚</li>
<li>CGæ”¯æŒæœ‰é™æ•°æ®é›†ï¼Œæ˜¾ç¤ºå‡ºå…¶å¤„ç†èµ„æºæœ‰é™æƒ…å†µçš„æ½œåŠ›ã€‚</li>
<li>MLLMsèƒ½åœ¨åˆ†ç±»å’Œæ£€æµ‹ä»»åŠ¡ä¸­å®ç°è·¨ä»»åŠ¡çš„CGï¼Œè¡¨æ˜å…¶æ›´å¹¿æ³›çš„æ¨å¹¿æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e6f110d852c7ae28821b19e3b37b8cf3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c32f75d7a3d8438c02b46e472d029c47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3ab0435ba81162faf23c49a9fa49704.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74d57783de284d0983309f702cc9028c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa6a651e5764eed1d515b8cb9f82cb97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07616c23927a234140c2cc15add5727d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fc8cc766ff62d03972c01af82951fea.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SwiftEdit-Lightning-Fast-Text-Guided-Image-Editing-via-One-Step-Diffusion"><a href="#SwiftEdit-Lightning-Fast-Text-Guided-Image-Editing-via-One-Step-Diffusion" class="headerlink" title="SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step   Diffusion"></a>SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step   Diffusion</h2><p><strong>Authors:Trong-Tung Nguyen, Quang Nguyen, Khoi Nguyen, Anh Tran, Cuong Pham</strong></p>
<p>Recent advances in text-guided image editing enable users to perform image edits through simple text inputs, leveraging the extensive priors of multi-step diffusion-based text-to-image models. However, these methods often fall short of the speed demands required for real-world and on-device applications due to the costly multi-step inversion and sampling process involved. In response to this, we introduce SwiftEdit, a simple yet highly efficient editing tool that achieve instant text-guided image editing (in 0.23s). The advancement of SwiftEdit lies in its two novel contributions: a one-step inversion framework that enables one-step image reconstruction via inversion and a mask-guided editing technique with our proposed attention rescaling mechanism to perform localized image editing. Extensive experiments are provided to demonstrate the effectiveness and efficiency of SwiftEdit. In particular, SwiftEdit enables instant text-guided image editing, which is extremely faster than previous multi-step methods (at least 50 times faster) while maintain a competitive performance in editing results. Our project page is at: <a target="_blank" rel="noopener" href="https://swift-edit.github.io/">https://swift-edit.github.io/</a> </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æŠ€æœ¯å–å¾—äº†è¿›å±•ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡ç®€å•çš„æ–‡æœ¬è¾“å…¥è¿›è¡Œå›¾åƒç¼–è¾‘ï¼Œåˆ©ç”¨å¤šæ­¥æ‰©æ•£åŸºç¡€ä¸Šçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¹¿æ³›å…ˆéªŒçŸ¥è¯†ã€‚ç„¶è€Œï¼Œç”±äºæ¶‰åŠæ˜‚è´µçš„å¤šæ­¥åæ¼”å’Œé‡‡æ ·è¿‡ç¨‹ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€éš¾ä»¥æ»¡è¶³ç°å®ä¸–ç•Œå’Œåœ¨çº¿è®¾å¤‡åº”ç”¨çš„é€Ÿåº¦è¦æ±‚ã€‚å¯¹æ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SwiftEditï¼Œè¿™æ˜¯ä¸€æ¬¾ç®€å•è€Œé«˜æ•ˆç¼–è¾‘å·¥å…·ï¼Œå¯å®ç°å³æ—¶æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘ï¼ˆåœ¨0.23ç§’å†…ï¼‰ã€‚SwiftEditçš„è¿›å±•åœ¨äºå…¶ä¸¤ä¸ªæ–°é¢–çš„è´¡çŒ®ï¼šä¸€ä¸ªä¸€æ­¥åæ¼”æ¡†æ¶ï¼Œå¯é€šè¿‡åæ¼”å®ç°ä¸€æ­¥å›¾åƒé‡å»ºï¼Œä»¥åŠæˆ‘ä»¬æå‡ºçš„å¸¦æœ‰æ³¨æ„åŠ›è°ƒæ•´æœºåˆ¶çš„é®ç½©å¼•å¯¼ç¼–è¾‘æŠ€æœ¯ï¼Œä»¥æ‰§è¡Œå±€éƒ¨å›¾åƒç¼–è¾‘ã€‚æä¾›äº†å¹¿æ³›çš„å®éªŒæ¥è¯æ˜SwiftEditçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚ç‰¹åˆ«æ˜¯ï¼ŒSwiftEditèƒ½å¤Ÿå®ç°å³æ—¶æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘ï¼Œè¿™æ¯”ä»¥å‰çš„å¤šæ­¥æ–¹æ³•æ›´å¿«ï¼ˆè‡³å°‘å¿«50å€ï¼‰ï¼ŒåŒæ—¶åœ¨ç¼–è¾‘ç»“æœæ–¹é¢ä¿æŒç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯ï¼š<a target="_blank" rel="noopener" href="https://swift-edit.github.io/">https://swift-edit.github.io/</a></p>
</blockquote>
<p><strong>ç®€åŒ–è¯´æ˜</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04301v4">PDF</a> 17 pages, 15 figures</p>
<p><strong>Summary</strong><br>     è¿‘æœŸæ–‡æœ¬æŒ‡å¯¼å›¾åƒç¼–è¾‘æŠ€æœ¯é€šè¿‡ç®€å•çš„æ–‡æœ¬è¾“å…¥å®ç°äº†å›¾åƒç¼–è¾‘ï¼Œåˆ©ç”¨å¤šæ­¥æ‰©æ•£åŸºç¡€çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸ä¸èƒ½æ»¡è¶³ç°å®åº”ç”¨å’Œåœ¨çº¿è®¾å¤‡åº”ç”¨çš„é€Ÿåº¦éœ€æ±‚ï¼Œå› ä¸ºå®ƒä»¬æ¶‰åŠæˆæœ¬é«˜æ˜‚çš„å¤šæ­¥åè½¬å’Œé‡‡æ ·è¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SwiftEditï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•è€Œé«˜æ•ˆçš„ç¼–è¾‘å·¥å…·ï¼Œå®ç°äº†å³æ—¶æ–‡æœ¬æŒ‡å¯¼å›¾åƒç¼–è¾‘ï¼ˆä»…éœ€0.23ç§’ï¼‰ã€‚SwiftEditçš„è¿›æ­¥åœ¨äºå…¶ä¸¤ä¸ªæ–°é¢–çš„è´¡çŒ®ï¼šä¸€æ­¥åè½¬æ¡†æ¶ï¼Œå¯é€šè¿‡åè½¬å®ç°ä¸€æ­¥å›¾åƒé‡å»ºï¼Œä»¥åŠæˆ‘ä»¬æå‡ºçš„æ³¨æ„åŠ›é‡æ–°è°ƒæ•´æœºåˆ¶çš„æ©è†œæŒ‡å¯¼ç¼–è¾‘æŠ€æœ¯ï¼Œä»¥æ‰§è¡Œå±€éƒ¨å›¾åƒç¼–è¾‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æŒ‡å¯¼å›¾åƒç¼–è¾‘æŠ€æœ¯å…è®¸é€šè¿‡ç®€å•çš„æ–‡æœ¬è¾“å…¥è¿›è¡Œå›¾åƒç¼–è¾‘ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å› å¤šæ­¥åè½¬å’Œé‡‡æ ·è¿‡ç¨‹è€Œé€Ÿåº¦è¾ƒæ…¢ï¼Œéš¾ä»¥æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚</li>
<li>SwiftEditæ˜¯ä¸€ä¸ªé«˜æ•ˆç¼–è¾‘å·¥å…·ï¼Œå®ç°äº†å³æ—¶æ–‡æœ¬æŒ‡å¯¼å›¾åƒç¼–è¾‘ï¼ˆ0.23ç§’å†…ï¼‰ã€‚</li>
<li>SwiftEditå…·æœ‰ä¸¤ä¸ªæ–°é¢–è´¡çŒ®ï¼šä¸€æ­¥åè½¬æ¡†æ¶å’Œæ³¨æ„åŠ›é‡æ–°è°ƒæ•´æœºåˆ¶çš„æ©è†œæŒ‡å¯¼ç¼–è¾‘æŠ€æœ¯ã€‚</li>
<li>ä¸€æ­¥åè½¬æ¡†æ¶å¯é€šè¿‡åè½¬å®ç°å¿«é€Ÿå›¾åƒé‡å»ºã€‚</li>
<li>æ©è†œæŒ‡å¯¼ç¼–è¾‘æŠ€æœ¯å¯ä»¥æ‰§è¡Œå±€éƒ¨å›¾åƒç¼–è¾‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b75d638f05b51d8f696e81953d8d25d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce452239af890c743ca59fe66d8a6155.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e02dca5d4cd8544337247225d711c58e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4841c4114519305f6d722723a5958d8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60ecc2b114be44b97bea6af33e725251.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Domain-Agnostic-Stroke-Lesion-Segmentation-Using-Physics-Constrained-Synthetic-Data"><a href="#Domain-Agnostic-Stroke-Lesion-Segmentation-Using-Physics-Constrained-Synthetic-Data" class="headerlink" title="Domain-Agnostic Stroke Lesion Segmentation Using Physics-Constrained   Synthetic Data"></a>Domain-Agnostic Stroke Lesion Segmentation Using Physics-Constrained   Synthetic Data</h2><p><strong>Authors:Liam Chalcroft, Jenny Crinion, Cathy J. Price, John Ashburner</strong></p>
<p>Segmenting stroke lesions in MRI is challenging due to diverse acquisition protocols that limit model generalisability. In this work, we introduce two physics-constrained approaches to generate synthetic quantitative MRI (qMRI) images that improve segmentation robustness across heterogeneous domains. Our first method, $\texttt{qATLAS}$, trains a neural network to estimate qMRI maps from standard MPRAGE images, enabling the simulation of varied MRI sequences with realistic tissue contrasts. The second method, $\texttt{qSynth}$, synthesises qMRI maps directly from tissue labels using label-conditioned Gaussian mixture models, ensuring physical plausibility. Extensive experiments on multiple out-of-domain datasets show that both methods outperform a baseline UNet, with $\texttt{qSynth}$ notably surpassing previous synthetic data approaches. These results highlight the promise of integrating MRI physics into synthetic data generation for robust, generalisable stroke lesion segmentation. Code is available at <a target="_blank" rel="noopener" href="https://github.com/liamchalcroft/qsynth">https://github.com/liamchalcroft/qsynth</a> </p>
<blockquote>
<p>åœ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­ï¼Œå¯¹å’ä¸­ç—…å˜è¿›è¡Œåˆ†å‰²æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºä¸åŒçš„é‡‡é›†åè®®é™åˆ¶äº†æ¨¡å‹çš„é€šç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»ä¸¤ç§ç‰©ç†çº¦æŸæ–¹æ³•ï¼Œç”¨äºç”Ÿæˆåˆæˆå®šé‡ç£å…±æŒ¯æˆåƒï¼ˆqMRIï¼‰å›¾åƒï¼Œä»¥æé«˜ä¸åŒå¼‚è´¨é¢†åŸŸçš„åˆ†å‰²ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç¬¬ä¸€ç§æ–¹æ³•ï¼Œ<code>qATLAS</code>ï¼Œè®­ç»ƒç¥ç»ç½‘ç»œä»æ ‡å‡†MPRAGEå›¾åƒä¼°è®¡qMRIåœ°å›¾ï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿå…·æœ‰çœŸå®ç»„ç»‡å¯¹æ¯”åº¦çš„å„ç§MRIåºåˆ—ã€‚ç¬¬äºŒç§æ–¹æ³•ï¼Œ<code>qSynth</code>ï¼Œç›´æ¥ä»ç»„ç»‡æ ‡ç­¾åˆæˆqMRIåœ°å›¾ï¼Œé‡‡ç”¨æ ‡ç­¾æ¡ä»¶ä¸‹çš„é«˜æ–¯æ··åˆæ¨¡å‹ï¼Œç¡®ä¿ç‰©ç†åˆç†æ€§ã€‚åœ¨å¤šä¸ªè·¨é¢†åŸŸæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½ä¼˜äºåŸºçº¿UNetï¼Œå…¶ä¸­<code>qSynth</code>æ˜¾è‘—è¶…è¶Šäº†ä»¥å‰çš„åˆæˆæ•°æ®æ–¹æ³•ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å°†MRIç‰©ç†å­¦çº³å…¥åˆæˆæ•°æ®ç”Ÿæˆä¸­çš„æ½œåŠ›ï¼Œä»¥å®ç°ç¨³å¥ã€é€šç”¨çš„å’ä¸­ç—…å˜åˆ†å‰²ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/liamchalcroft/qsynth%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/liamchalcroft/qsynthæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03318v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸¤ç§åŸºäºç‰©ç†çº¦æŸçš„åˆæˆå®šé‡æ ¸ç£å…±æŒ¯æˆåƒï¼ˆqMRIï¼‰å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ä¸åŒå¼‚è´¨é¢†åŸŸä¸­çš„åˆ†å‰²ç¨³å¥æ€§ã€‚ç¬¬ä¸€ç§æ–¹æ³•qATLASä»æ ‡å‡†MPRAGEå›¾åƒä¼°è®¡qMRIå›¾ï¼Œæ¨¡æ‹Ÿå¤šç§MRIåºåˆ—ï¼Œå®ç°ç°å®ç»„ç»‡å¯¹æ¯”ã€‚ç¬¬äºŒç§æ–¹æ³•qSynthç›´æ¥ä»ç»„ç»‡æ ‡ç­¾åˆæˆqMRIå›¾ï¼Œç¡®ä¿ç‰©ç†åˆç†æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä¸¤ç§æ–¹æ³•å‡ä¼˜äºåŸºå‡†UNetï¼Œç‰¹åˆ«æ˜¯qSynthåœ¨åˆæˆæ•°æ®æ–¹æ³•ä¸Šè¡¨ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚é›†æˆMRIç‰©ç†çš„åˆæˆæ•°æ®ç”Ÿæˆå±•ç°å‡ºç¨³å¥ã€é€šç”¨çš„è„‘å’ä¸­ç—…ç¶åˆ†å‰²æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»MRIä¸­åˆ†å‰²è„‘å’ä¸­ç—…ç¶çš„æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºé‡‡é›†åè®®çš„å¤šæ ·æ€§é™åˆ¶äº†æ¨¡å‹çš„é€šç”¨æ€§ã€‚</li>
<li>æå‡ºä¸¤ç§ç‰©ç†çº¦æŸçš„åˆæˆå®šé‡æ ¸ç£å…±æŒ¯æˆåƒï¼ˆqMRIï¼‰å›¾åƒç”Ÿæˆæ–¹æ³•ä»¥æé«˜åˆ†å‰²ç¨³å¥æ€§ã€‚</li>
<li>ç¬¬ä¸€ç§æ–¹æ³•qATLASèƒ½å¤Ÿä»æ ‡å‡†MPRAGEå›¾åƒä¼°è®¡qMRIå›¾ï¼Œæ¨¡æ‹Ÿå¤šç§MRIåºåˆ—å¹¶å®ç°ç°å®çš„ç»„ç»‡å¯¹æ¯”ã€‚</li>
<li>ç¬¬äºŒç§æ–¹æ³•qSynthç›´æ¥ä»ç»„ç»‡æ ‡ç­¾åˆæˆqMRIå›¾ï¼Œç¡®ä¿åˆæˆçš„å›¾åƒç¬¦åˆç‰©ç†è§„å¾‹ã€‚</li>
<li>åœ¨å¤šä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ–¹æ³•å‡ä¼˜äºåŸºå‡†UNetæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯qSynthè¡¨ç°çªå‡ºã€‚</li>
<li>é›†æˆMRIç‰©ç†çš„åˆæˆæ•°æ®ç”Ÿæˆæœ‰åŠ©äºæé«˜æ¨¡å‹çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ï¼Œåœ¨è„‘å’ä¸­ç—…ç¶åˆ†å‰²æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-324b035746d85af60a419ee403012f72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe2c13c1843cb3030668132228568bad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d4b634061e49f660b98f11dec2c6a50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d3bb61c5f9d527de3bae81c46b3d3ea7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="FactCheXcker-Mitigating-Measurement-Hallucinations-in-Chest-X-ray-Report-Generation-Models"><a href="#FactCheXcker-Mitigating-Measurement-Hallucinations-in-Chest-X-ray-Report-Generation-Models" class="headerlink" title="FactCheXcker: Mitigating Measurement Hallucinations in Chest X-ray   Report Generation Models"></a>FactCheXcker: Mitigating Measurement Hallucinations in Chest X-ray   Report Generation Models</h2><p><strong>Authors:Alice Heiman, Xiaoman Zhang, Emma Chen, Sung Eun Kim, Pranav Rajpurkar</strong></p>
<p>Medical vision-language models often struggle with generating accurate quantitative measurements in radiology reports, leading to hallucinations that undermine clinical reliability. We introduce FactCheXcker, a modular framework that de-hallucinates radiology report measurements by leveraging an improved query-code-update paradigm. Specifically, FactCheXcker employs specialized modules and the code generation capabilities of large language models to solve measurement queries generated based on the original report. After extracting measurable findings, the results are incorporated into an updated report. We evaluate FactCheXcker on endotracheal tube placement, which accounts for an average of 78% of report measurements, using the MIMIC-CXR dataset and 11 medical report-generation models. Our results show that FactCheXcker significantly reduces hallucinations, improves measurement precision, and maintains the quality of the original reports. Specifically, FactCheXcker improves the performance of 10&#x2F;11 models and achieves an average improvement of 135.0% in reducing measurement hallucinations measured by mean absolute error. Code is available at <a target="_blank" rel="noopener" href="https://github.com/rajpurkarlab/FactCheXcker">https://github.com/rajpurkarlab/FactCheXcker</a>. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒè¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šä¸­çš„ç²¾ç¡®å®šé‡æµ‹é‡æ–¹é¢ç»å¸¸é‡åˆ°å›°éš¾ï¼Œè¿™å¯¼è‡´å‡ºç°å½±å“ä¸´åºŠå¯é æ€§çš„å¹»è§‰ã€‚æˆ‘ä»¬å¼•å…¥äº†FactCheXckerï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ”¹è¿›æŸ¥è¯¢-ç¼–ç -æ›´æ–°èŒƒå¼æ¥æ¶ˆé™¤æ”¾å°„å­¦æŠ¥å‘Šæµ‹é‡ä¸­çš„å¹»è§‰ã€‚å…·ä½“æ¥è¯´ï¼ŒFactCheXckeråˆ©ç”¨ä¸“ä¸šæ¨¡å—å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„ç¼–ç ç”Ÿæˆèƒ½åŠ›æ¥è§£å†³åŸºäºåŸå§‹æŠ¥å‘Šç”Ÿæˆçš„æµ‹é‡æŸ¥è¯¢é—®é¢˜ã€‚åœ¨æå–å¯æµ‹é‡çš„å‘ç°åï¼Œå°†ç»“æœçº³å…¥æ›´æ–°çš„æŠ¥å‘Šä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨MIMIC-CXRæ•°æ®é›†å’Œ11ä¸ªåŒ»å­¦æŠ¥å‘Šç”Ÿæˆæ¨¡å‹ï¼Œå¯¹æ°”ç®¡å¯¼ç®¡ç½®å…¥çš„æƒ…å†µè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯¥æƒ…å†µå æŠ¥å‘Šæµ‹é‡çš„å¹³å‡77%ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒFactCheXckeræ˜¾è‘—å‡å°‘äº†å¹»è§‰ï¼Œæé«˜äº†æµ‹é‡ç²¾åº¦ï¼Œå¹¶ä¿æŒäº†åŸå§‹æŠ¥å‘Šçš„è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼ŒFactCheXckeræé«˜äº†å…¶ä¸­åä¸ªæ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡å¹³å‡ç»å¯¹è¯¯å·®è¡¡é‡çš„æµ‹é‡å¹»è§‰å‡å°‘æ–¹é¢å®ç°äº†å¹³å‡135.0%çš„æ”¹è¿›ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/rajpurkarlab/FactCheXcker%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/rajpurkarlab/FactCheXckerè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18672v3">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šä¸­çš„å®šé‡æµ‹é‡æ—¶å¸¸å¸¸å‡ºç°è¯¯å·®ï¼Œå¯¼è‡´äº§ç”Ÿè¯¯å¯¼ä¸´åºŠçš„å¹»è§‰ã€‚æˆ‘ä»¬æ¨å‡ºFactCheXckerï¼Œä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œé€šè¿‡æ”¹è¿›æŸ¥è¯¢-ç¼–ç -æ›´æ–°èŒƒå¼æ¥æ¶ˆé™¤æ”¾å°„å­¦æŠ¥å‘Šä¸­çš„æµ‹é‡å¹»è§‰ã€‚å…·ä½“æ¥è¯´ï¼ŒFactCheXckeråˆ©ç”¨ä¸“ä¸šæ¨¡å—å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„ç¼–ç ç”Ÿæˆèƒ½åŠ›æ¥è§£å†³åŸºäºåŸå§‹æŠ¥å‘Šç”Ÿæˆçš„æµ‹é‡æŸ¥è¯¢é—®é¢˜ã€‚åœ¨æå–å‡ºå¯æµ‹é‡çš„å‘ç°åï¼Œå°†ç»“æœçº³å…¥æ›´æ–°çš„æŠ¥å‘Šä¸­ã€‚æˆ‘ä»¬åœ¨æ°”ç®¡æ’ç®¡æ”¾ç½®ä»»åŠ¡ä¸Šè¯„ä¼°äº†FactCheXckerï¼Œè¯¥ä»»åŠ¡å æŠ¥å‘Šæµ‹é‡çš„å¹³å‡78%ï¼Œä½¿ç”¨MIMIC-CXRæ•°æ®é›†å’Œ1 ç»“åŒ»å­¦æŠ¥å‘Šç”Ÿæˆæ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼ŒFactCheXckeræ˜¾è‘—å‡å°‘äº†å¹»è§‰ï¼Œæé«˜äº†æµ‹é‡ç²¾åº¦ï¼Œå¹¶ä¿æŒäº†åŸå§‹æŠ¥å‘Šçš„è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼ŒFactCheXckeræé«˜äº†10&#x2F;11æ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨å¹³å‡è¯¯å·®ç»å¯¹å€¼çš„æµ‹é‡ä¸‹ï¼Œå‡å°‘æµ‹é‡å¹»è§‰çš„å¹³å‡æ”¹å–„ç‡ä¸º135.0%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/rajpurkarlab/FactCheXcker%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/rajpurkarlab/FactCheXckeræ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šä¸­çš„å®šé‡æµ‹é‡æ—¶å­˜åœ¨è¯¯å·®é—®é¢˜ã€‚</li>
<li>FactCheXckeræ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æ¶ˆé™¤æ”¾å°„å­¦æŠ¥å‘Šä¸­çš„æµ‹é‡å¹»è§‰ã€‚</li>
<li>FactCheXckeré€šè¿‡æ”¹è¿›æŸ¥è¯¢-ç¼–ç -æ›´æ–°èŒƒå¼æ¥è§£å†³é—®é¢˜ï¼Œåˆ©ç”¨ä¸“ä¸šæ¨¡å—å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„ç¼–ç ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>FactCheXckerå¯ä»¥æå–å¹¶çº³å…¥å¯æµ‹é‡çš„å‘ç°äºæ›´æ–°çš„æŠ¥å‘Šä¸­ã€‚</li>
<li>åœ¨æ°”ç®¡æ’ç®¡æ”¾ç½®ä»»åŠ¡ä¸Šï¼ŒFactCheXckeræ˜¾è‘—å‡å°‘äº†å¹»è§‰å¹¶æé«˜äº†æµ‹é‡ç²¾åº¦ã€‚</li>
<li>FactCheXckerå¯¹å¤§å¤šæ•°åŒ»å­¦æŠ¥å‘Šç”Ÿæˆæ¨¡å‹éƒ½æœ‰æé«˜è¡¨ç°çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dedd6b3e610b8b7b725564f503b84876.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f64b166c55bbebfa4b8248464178354.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac54e9a83c52e83ced04124ff8d3a617.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2abf15d2f0b309fc4eb8c875aa6974ac.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Leveraging-Complementary-Attention-maps-in-vision-transformers-for-OCT-image-analysis"><a href="#Leveraging-Complementary-Attention-maps-in-vision-transformers-for-OCT-image-analysis" class="headerlink" title="Leveraging Complementary Attention maps in vision transformers for OCT   image analysis"></a>Leveraging Complementary Attention maps in vision transformers for OCT   image analysis</h2><p><strong>Authors:Haz Sameen Shahgir, Tanjeem Azwad Zaman, Khondker Salman Sayeed, Md. Asif Haider, Sheikh Saifur Rahman Jony, M. Sohel Rahman</strong></p>
<p>Optical Coherence Tomography (OCT) scan yields all possible cross-section images of a retina for detecting biomarkers linked to optical defects. Due to the high volume of data generated, an automated and reliable biomarker detection pipeline is necessary as a primary screening stage.   We outline our new state-of-the-art pipeline for identifying biomarkers from OCT scans. In collaboration with trained ophthalmologists, we identify local and global structures in biomarkers. Through a comprehensive and systematic review of existing vision architectures, we evaluate different convolution and attention mechanisms for biomarker detection. We find that MaxViT, a hybrid vision transformer combining convolution layers with strided attention, is better suited for local feature detection, while EVA-02, a standard vision transformer leveraging pure attention and large-scale knowledge distillation, excels at capturing global features. We ensemble the predictions of both models to achieve first place in the IEEE Video and Image Processing Cup 2023 competition on OCT biomarker detection, achieving a patient-wise F1 score of 0.8527 in the final phase of the competition, scoring 3.8% higher than the next best solution. Finally, we used knowledge distillation to train a single MaxViT to outperform our ensemble at a fraction of the computation cost. </p>
<blockquote>
<p>å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰æ‰«æå¯ä»¥è·å–è§†ç½‘è†œçš„æ‰€æœ‰å¯èƒ½çš„æ¨ªæˆªé¢å›¾åƒï¼Œä»è€Œæ£€æµ‹ä¸å…‰å­¦ç¼ºé™·ç›¸å…³çš„ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚ç”±äºäº§ç”Ÿçš„å¤§é‡æ•°æ®ï¼Œéœ€è¦ä¸€ä¸ªè‡ªåŠ¨åŒ–å’Œå¯é çš„ç”Ÿç‰©æ ‡å¿—ç‰©æ£€æµ‹æµç¨‹ä½œä¸ºåˆæ­¥ç­›æŸ¥é˜¶æ®µã€‚æˆ‘ä»¬æ¦‚è¿°äº†æˆ‘ä»¬æœ€æ–°çš„ä»OCTæ‰«æä¸­è¯†åˆ«ç”Ÿç‰©æ ‡å¿—ç‰©çš„å…ˆè¿›æµç¨‹ã€‚ä¸è®­ç»ƒæœ‰ç´ çš„çœ¼ç§‘åŒ»ç”Ÿåˆä½œï¼Œæˆ‘ä»¬è¯†åˆ«ç”Ÿç‰©æ ‡å¿—ç‰©çš„å±€éƒ¨å’Œå…¨å±€ç»“æ„ã€‚é€šè¿‡å¯¹ç°æœ‰è§†è§‰æ¶æ„çš„å…¨é¢ç³»ç»Ÿå®¡æŸ¥ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ç”¨äºç”Ÿç‰©æ ‡å¿—ç‰©æ£€æµ‹çš„ä¸åŒå·ç§¯å’Œæ³¨æ„åŠ›æœºåˆ¶ã€‚æˆ‘ä»¬å‘ç°MaxViTï¼Œä¸€ç§ç»“åˆå·ç§¯å±‚å’Œè·¨æ­¥æ³¨æ„åŠ›çš„æ··åˆè§†è§‰è½¬æ¢å™¨ï¼Œæ›´é€‚åˆäºå±€éƒ¨ç‰¹å¾æ£€æµ‹ï¼Œè€ŒEVA-02ï¼Œä¸€ç§åˆ©ç”¨çº¯æ³¨æ„åŠ›å’Œå¤§è§„æ¨¡çŸ¥è¯†è’¸é¦çš„æ ‡å‡†è§†è§‰è½¬æ¢å™¨ï¼Œæ“…é•¿æ•æ‰å…¨å±€ç‰¹å¾ã€‚æˆ‘ä»¬ç»“åˆäº†è¿™ä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹ï¼Œåœ¨IEEE 2023å¹´OCTç”Ÿç‰©æ ‡å¿—ç‰©æ£€æµ‹æ¯”èµ›ä¸­è·å¾—ç¬¬ä¸€åï¼Œåœ¨æ¯”èµ›çš„æœ€åé˜¶æ®µï¼Œæ‚£è€…çº§åˆ«çš„F1åˆ†æ•°è¾¾åˆ°0.8527ï¼Œæ¯”ç¬¬äºŒåé«˜å‡º3.8%ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨çŸ¥è¯†è’¸é¦è®­ç»ƒäº†ä¸€ä¸ªå•ç‹¬çš„MaxViTï¼Œä»¥è¶…è¿‡æˆ‘ä»¬ç»„åˆçš„è¡¨ç°åœ¨å¾ˆå°çš„è®¡ç®—æˆæœ¬ä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14005v3">PDF</a> Accepted in 2025 IEEE International Conference on Image Processing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„ç”Ÿç‰©æ ‡å¿—ç‰©æ£€æµ‹ç®¡é“ï¼Œè¯¥ç®¡é“ç»“åˆäº†å…‰å­¦ç›¸å¹²å±‚ææ‰«ææŠ€æœ¯ä¸å…ˆè¿›çš„è§†è§‰æŠ€æœ¯ï¼Œæ—¨åœ¨ä»OCTæ‰«æä¸­è‡ªåŠ¨è¯†åˆ«å’Œåˆ†ç±»ä¸å…‰å­¦ç¼ºé™·ç›¸å…³çš„ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚é€šè¿‡ä¸çœ¼ç§‘åŒ»ç”Ÿçš„åˆä½œï¼Œç ”ç©¶äººå‘˜æˆåŠŸå¼€å‘äº†ä¸€ç§é›†æˆçš„é¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨IEEEè§†é¢‘å’Œå›¾åƒå¤„ç†æ¯ç«èµ›ä¸­å–å¾—äº†ç¬¬ä¸€åï¼Œå¹¶åœ¨æœ€ç»ˆé˜¶æ®µå®ç°äº†æ‚£è€…çº§åˆ«çš„F1åˆ†æ•°ä¸º0.8527ã€‚æ­¤å¤–ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œç ”ç©¶äººå‘˜è¿˜è®­ç»ƒäº†ä¸€ä¸ªå•ä¸€çš„MaxViTæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨è®¡ç®—èƒ½åŠ›æ¶ˆè€—æ–¹é¢è¡¨ç°ä¼˜äºé›†æˆæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>OCTæ‰«æèƒ½å¤Ÿç”Ÿæˆè§†ç½‘è†œçš„æ‰€æœ‰å¯èƒ½æ¨ªæˆªé¢å›¾åƒï¼Œæœ‰åŠ©äºæ£€æµ‹ä¸å…‰å­¦ç¼ºé™·ç›¸å…³çš„ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚</li>
<li>ç”±äºæ•°æ®é‡å·¨å¤§ï¼Œéœ€è¦è‡ªåŠ¨åŒ–å’Œå¯é çš„ç”Ÿç‰©æ ‡å¿—ç‰©æ£€æµ‹ç®¡é“ä½œä¸ºåˆæ­¥ç­›é€‰é˜¶æ®µã€‚</li>
<li>é€šè¿‡å¯¹ç°æœ‰è§†è§‰æ¶æ„çš„å…¨é¢ç³»ç»Ÿå®¡æŸ¥ï¼Œè¯„ä¼°äº†ä¸åŒçš„å·ç§¯å’Œæ³¨æ„åŠ›æœºåˆ¶åœ¨ç”Ÿç‰©æ ‡å¿—ç‰©æ£€æµ‹ä¸­çš„åº”ç”¨ã€‚</li>
<li>MaxViTå’ŒEVA-02æ¨¡å‹åˆ†åˆ«åœ¨å±€éƒ¨ç‰¹å¾æ£€æµ‹å’Œå…¨å±€ç‰¹å¾æ•æ‰æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>é›†æˆMaxViTå’ŒEVA-02æ¨¡å‹çš„é¢„æµ‹åœ¨IEEE Video and Image Processing Cup 2023ç«èµ›ä¸­å–å¾—ç¬¬ä¸€åã€‚</li>
<li>æ‚£è€…çº§åˆ«çš„F1åˆ†æ•°è¾¾åˆ°0.8527ï¼Œè¾ƒå…¶ä»–æœ€ä½³è§£å†³æ–¹æ¡ˆé«˜å‡º3.8%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.14005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-14881e67b41822c093cf7a6011ea0eda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95e380de7b23c209744811a59bcd5ae0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97f97935fa43d0c292d9f217300bcbde.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c8276fc046158b3831f41fb6d3f1f19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0e57cb78963f64f215d4e8ea7a224bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c958068ec93320796409d041fee6521.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3663f140ca164000b802e7fe6f9a230e.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-04  Zero-Shot Streaming Text to Speech Synthesis with Transducer and   Auto-Regressive Modeling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-32f95dfd51ea03a183c4ed355570c4e2.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-04  OpenUni A Simple Baseline for Unified Multimodal Understanding and   Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
