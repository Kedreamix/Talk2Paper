<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-06-04  NMCSE Noise-Robust Multi-Modal Coupling Signal Estimation Method via   Optimal Transport for Cardiovascular Disease Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2c958068ec93320796409d041fee6521.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    66 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-04-更新"><a href="#2025-06-04-更新" class="headerlink" title="2025-06-04 更新"></a>2025-06-04 更新</h1><h2 id="NMCSE-Noise-Robust-Multi-Modal-Coupling-Signal-Estimation-Method-via-Optimal-Transport-for-Cardiovascular-Disease-Detection"><a href="#NMCSE-Noise-Robust-Multi-Modal-Coupling-Signal-Estimation-Method-via-Optimal-Transport-for-Cardiovascular-Disease-Detection" class="headerlink" title="NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via   Optimal Transport for Cardiovascular Disease Detection"></a>NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via   Optimal Transport for Cardiovascular Disease Detection</h2><p><strong>Authors:Peihong Zhang, Zhixin Li, Rui Sang, Yuxuan Liu, Yiqiang Cai, Yizhou Tan, Shengchen Li</strong></p>
<p>Electrocardiogram (ECG) and Phonocardiogram (PCG) signals are linked by a latent coupling signal representing the electrical-to-mechanical cardiac transformation. While valuable for cardiovascular disease (CVD) detection, this coupling signal is traditionally estimated using deconvolution methods that amplify noise, limiting clinical utility. In this paper, we propose Noise-Robust Multi-Modal Coupling Signal Estimation (NMCSE), which reformulates the problem as distribution matching via optimal transport theory. By jointly optimizing amplitude and temporal alignment, NMCSE mitigates noise amplification without additional preprocessing. Integrated with our Temporal-Spatial Feature Extraction network, NMCSE enables robust multi-modal CVD detection. Experiments on the PhysioNet 2016 dataset with realistic hospital noise demonstrate that NMCSE reduces estimation errors by approximately 30% in Mean Squared Error while maintaining higher Pearson Correlation Coefficients across all tested signal-to-noise ratios. Our approach achieves 97.38% accuracy and 0.98 AUC in CVD detection, outperforming state-of-the-art methods and demonstrating robust performance for real-world clinical applications. </p>
<blockquote>
<p>心电图（ECG）和心音图（PCG）信号通过代表电-机械心脏转换的潜在耦合信号相关联。虽然这种耦合信号对于检测心血管疾病（CVD）很有价值，但传统上是通过反卷积方法来估计的，这会增加噪声，限制了其在临床上的实用价值。在本文中，我们提出了噪声鲁棒的跨模态耦合信号估计（NMCSE），它将问题重新定义为通过最优传输理论进行分布匹配。通过联合优化幅度和时间对齐，NMCSE可以在无需额外预处理的情况下减轻噪声放大。结合我们的时空特征提取网络，NMCSE可实现稳健的跨模态CVD检测。在具有现实医院噪声的PhysioNet 2016数据集上的实验表明，在均方误差方面，NMCSE将估计误差降低了约30%，并且在所有测试的信噪比中保持了更高的皮尔逊相关系数。我们的方法在CVD检测中达到了97.38%的准确度和0.98的AUC值，优于最新方法，并展示了在现实临床应用中的稳健性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18174v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种噪声鲁棒的跨模态耦合信号估计方法（NMCSE），用于估计心电图（ECG）和心音图（PCG）之间的潜在耦合信号。该方法基于最优传输理论重新构建问题，联合优化振幅和时间对齐，降低了噪声放大问题，并与我们的时空特征提取网络相结合，实现了稳健的多模态心血管疾病（CVD）检测。在PhysioNet 2016数据集上的实验表明，NMCSE在平均误差平方上减少了大约30%的估计误差，同时在所有测试的信号噪声比上都保持了较高的皮尔逊相关系数。在心血管疾病检测中，其准确性达到97.38%，AUC达到0.98，优于现有方法，并在现实的临床应用中表现出稳健性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>心电图（ECG）和心音图（PCG）之间的耦合信号代表了心脏的电-机械转换。</li>
<li>传统估计耦合信号的方法使用解卷积方法，会放大噪声，限制了其在临床的实用性。</li>
<li>本文提出噪声鲁棒的跨模态耦合信号估计方法（NMCSE），通过最优传输理论重新构建问题来解决噪声问题。</li>
<li>NMCSE联合优化振幅和时间对齐，降低了噪声放大问题，提高了估计的准确性。</li>
<li>NMCSE与时空特征提取网络结合，实现了多模态心血管疾病检测。</li>
<li>在PhysioNet数据集上的实验结果显示，NMCSE显著减少了估计误差，并提高了检测性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18174">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7f707c1c196e402693373dee692ea4da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7de23563f5b9efb12d8a50e605fef143.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="4D-CTA-Image-and-geometry-dataset-for-kinematic-analysis-of-abdominal-aortic-aneurysms"><a href="#4D-CTA-Image-and-geometry-dataset-for-kinematic-analysis-of-abdominal-aortic-aneurysms" class="headerlink" title="4D-CTA Image and geometry dataset for kinematic analysis of abdominal   aortic aneurysms"></a>4D-CTA Image and geometry dataset for kinematic analysis of abdominal   aortic aneurysms</h2><p><strong>Authors:Mostafa Jamshidian, Adam Wittek, Saeideh Sekhavat, Farah Alkhatib, Jens Carsten Ritter, Paul M. Parizel, Donatien Le Liepvre, Florian Bernard, Ludovic Minvielle, Antoine Fondanèche, Jane Polce, Christopher Wood, Karol Miller</strong></p>
<p>This article presents a dataset used in the article “Kinematics of Abdominal Aortic Aneurysms”, published in the Journal of Biomechanics. The dataset is publicly available for download from the Zenodo data repository (<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.15477710">https://doi.org/10.5281/zenodo.15477710</a>). The dataset includes time-resolved 3D computed tomography angiography (4D-CTA) images of abdominal aortic aneurysm (AAA) captured throughout the cardiac cycle from ten patients diagnosed with AAA, along with ten patient-specific AAA geometries extracted from these images. Typically, the 4D-CTA dataset for each patient contains ten electrocardiogram (ECG)-gated 3D-CTA image frames acquired over a cardiac cycle, capturing both the systolic and diastolic phases of the AAA configuration. For method verification, the dataset also includes synthetic ground truth data generated from Patient 1’s 3D-CTA AAA image in the diastolic phase. The ground truth data includes the patient-specific finite element (FE) biomechanical model and a synthetic systolic 3D-CTA image. The synthetic systolic image was generated by warping Patient 1’s diastolic 3D-CTA image using the realistic displacement field obtained from the AAA biomechanical FE model. The images were acquired at Fiona Stanley Hospital in Western Australia and provided to the researchers at the Intelligent Systems for Medicine Laboratory at The University of Western Australia (ISML-UWA), where image-based AAA kinematic analysis was performed. Our dataset enabled the analysis of AAA wall displacement and strain throughout the cardiac cycle using a non-invasive, in vivo, image registration-based approach. The use of widely adopted, open-source file formats (NRRD for images and STL for geometries) facilitates broad applicability and reusability in AAA biomechanics studies that require patient-specific geometry and information about AAA kinematics during cardiac cycle. </p>
<blockquote>
<p>本文介绍了一个在《生物力学杂志》上发表的名为“腹部主动脉瘤的运动学”的文章中使用的数据集。该数据集可从Zenodo数据仓库（<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.15477710%EF%BC%89%E5%85%AC%E5%BC%80%E4%B8%8B%E8%BD%BD%E3%80%82%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8C%85%E5%90%AB%E9%80%9A%E8%BF%87%E5%BF%83%E8%84%8F%E5%91%A8%E6%9C%9F%E6%8D%95%E8%8E%B7%E7%9A%84%E8%85%B9%E9%83%A8%E4%B8%BB%E5%8A%A8%E8%84%89%E7%98%A4%EF%BC%88AAA%EF%BC%89%E7%9A%84%E6%97%B6%E9%97%B4%E5%88%86%E8%BE%A8%E4%B8%89%E7%BB%B4%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%96%AD%E5%B1%82%E8%A1%80%E7%AE%A1%E9%80%A0%E5%BD%B1%EF%BC%884D-CTA%EF%BC%89%E5%9B%BE%E5%83%8F%EF%BC%8C%E5%8C%85%E6%8B%AC10%E5%90%8D%E8%A2%AB%E8%AF%8A%E6%96%AD%E4%B8%BAAAA%E7%9A%84%E6%82%A3%E8%80%85%E7%9A%84AAA%E5%9B%BE%E5%83%8F%EF%BC%8C%E4%BB%A5%E5%8F%8A%E4%BB%8E%E8%BF%99%E4%BA%9B%E5%9B%BE%E5%83%8F%E4%B8%AD%E6%8F%90%E5%8F%96%E7%9A%8410%E4%B8%AA%E6%82%A3%E8%80%85%E7%89%B9%E5%AE%9A%E7%9A%84AAA%E5%87%A0%E4%BD%95%E7%BB%93%E6%9E%84%E3%80%82%E9%80%9A%E5%B8%B8%EF%BC%8C%E6%AF%8F%E4%B8%AA%E6%82%A3%E8%80%85%E7%9A%844D-CTA%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8C%85%E5%90%AB%E5%9C%A8%E6%95%B4%E4%B8%AA%E5%BF%83%E8%84%8F%E5%91%A8%E6%9C%9F%E5%86%85%E8%8E%B7%E5%8F%96%E7%9A%84%E5%BF%83%E7%94%B5%E5%9B%BE%E9%97%A8%E6%8E%A7%E7%9A%843D-CTA%E5%9B%BE%E5%83%8F%E5%B8%A7%EF%BC%8C%E8%BF%99%E4%BA%9B%E5%9B%BE%E5%83%8F%E6%8D%95%E6%8D%89AAA%E9%85%8D%E7%BD%AE%E7%9A%84%E6%94%B6%E7%BC%A9%E6%9C%9F%E5%92%8C%E8%88%92%E5%BC%A0%E6%9C%9F%E3%80%82%E4%B8%BA%E4%BA%86%E9%AA%8C%E8%AF%81%E6%96%B9%E6%B3%95%EF%BC%8C%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%98%E5%8C%85%E6%8B%AC%E7%94%B1%E6%82%A3%E8%80%851%E7%9A%84%E8%88%92%E5%BC%A0%E6%9C%9F3D-CTA">https://doi.org/10.5281/zenodo.15477710）公开下载。数据集包含通过心脏周期捕获的腹部主动脉瘤（AAA）的时间分辨三维计算机断层血管造影（4D-CTA）图像，包括10名被诊断为AAA的患者的AAA图像，以及从这些图像中提取的10个患者特定的AAA几何结构。通常，每个患者的4D-CTA数据集包含在整个心脏周期内获取的心电图门控的3D-CTA图像帧，这些图像捕捉AAA配置的收缩期和舒张期。为了验证方法，该数据集还包括由患者1的舒张期3D-CTA</a> AAA图像生成的合成基准数据。基准数据包括患者特定的有限元（FE）生物力学模型和合成的收缩期3D-CTA图像。合成收缩期图像是通过使用来自AAA生物力学有限元模型的现实位移场对患者1的舒张期3D-CTA图像进行变形处理而生成的。这些图像是在西澳大利亚州的菲欧娜斯坦利医院获取的，并提供给西澳大利亚大学智能医学实验室（ISML-UWA）的研究人员，在那里进行了基于图像的AAA运动学分析。我们的数据集能够使用一种无创、体内、基于图像配准的方法分析整个心脏周期内AAA壁位移和应变。使用广泛采用的开放源代码文件格式（NRRD用于图像和STL用于几何形状）有助于在AAA生物力学研究中实现广泛的适用性和可重用性，这些研究需要特定于患者的几何形状以及有关心脏周期内AAA运动学的信息。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17647v2">PDF</a> </p>
<p><strong>Summary</strong><br>该文章介绍了一个用于研究腹部主动脉瘤（AAA）运动学的数据集。数据集可从Zenodo数据仓库下载，包含通过4D-CTA技术获取的AAA患者的实时三维图像以及从这些图像中提取的患者特异性AAA几何结构。数据集还包括合成基准数据，用于验证方法。该数据集有助于分析AAA壁在整个心动周期中的位移和应变，采用非侵入性、活体、基于图像配准的方法。数据集使用广泛采纳的开放源文件格式，便于在AAA生物力学研究中应用。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>文章介绍了一个关于腹部主动脉瘤（AAA）运动学研究的公开数据集。</li>
<li>数据集包含通过4D-CTA技术获取的AAA患者的实时三维图像。</li>
<li>数据集包含患者特异性AAA几何结构以及合成基准数据用于方法验证。</li>
<li>数据集可用于分析AAA壁在整个心动周期中的位移和应变。</li>
<li>数据集采用了非侵入性、活体、基于图像配准的方法进行分析。</li>
<li>数据集使用开放源文件格式，便于在AAA生物力学研究中使用。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17647">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4457b0ae3b24bff4cf456c06786b8291.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d57e6e2d36950001b6c16ac55de7305c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45b65f365cb44e7d5ed4bb63c4b44fcc.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RADAR-Enhancing-Radiology-Report-Generation-with-Supplementary-Knowledge-Injection"><a href="#RADAR-Enhancing-Radiology-Report-Generation-with-Supplementary-Knowledge-Injection" class="headerlink" title="RADAR: Enhancing Radiology Report Generation with Supplementary   Knowledge Injection"></a>RADAR: Enhancing Radiology Report Generation with Supplementary   Knowledge Injection</h2><p><strong>Authors:Wenjun Hou, Yi Cheng, Kaishuai Xu, Heng Li, Yan Hu, Wenjie Li, Jiang Liu</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities in various domains, including radiology report generation. Previous approaches have attempted to utilize multimodal LLMs for this task, enhancing their performance through the integration of domain-specific knowledge retrieval. However, these approaches often overlook the knowledge already embedded within the LLMs, leading to redundant information integration. To address this limitation, we propose Radar, a framework for enhancing radiology report generation with supplementary knowledge injection. Radar improves report generation by systematically leveraging both the internal knowledge of an LLM and externally retrieved information. Specifically, it first extracts the model’s acquired knowledge that aligns with expert image-based classification outputs. It then retrieves relevant supplementary knowledge to further enrich this information. Finally, by aggregating both sources, Radar generates more accurate and informative radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU X-ray demonstrate that our model outperforms state-of-the-art LLMs in both language quality and clinical accuracy. </p>
<blockquote>
<p>大型语言模型（LLM）在多个领域表现出了卓越的能力，包括放射学报告生成。之前的方法试图使用多模态LLM来完成此任务，通过整合特定领域的知识检索来增强性能。然而，这些方法往往会忽略LLM中已经嵌入的知识，导致信息集成冗余。为了解决这一局限性，我们提出了Radar框架，通过注入补充知识来提高放射学报告的生成质量。Radar通过系统地利用LLM的内部知识和外部检索信息来改善报告生成。具体来说，它首先提取与专家基于图像的分类输出相符的模型获取的知识。然后，它检索相关的补充知识来进一步丰富这些信息。最后，通过聚合这两个来源，Radar生成更准确和更有信息的放射学报告。在MIMIC-CXR、CheXpert-Plus和IU X-ray上的广泛实验表明，我们的模型在语言质量和临床准确性方面都优于最新的LLM。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14318v2">PDF</a> Accepted to ACL 2025 main</p>
<p><strong>摘要</strong></p>
<p>大规模语言模型（LLMs）在多个领域表现出卓越的能力，包括放射学报告生成。先前的方法试图使用多模式LLMs完成此任务，并通过整合领域特定知识检索增强其性能。然而，这些方法往往忽视了LLMs中已经嵌入的知识，导致信息整合冗余。为解决这一局限性，我们提出了Radar框架，用于通过补充知识注入增强放射学报告生成。Radar通过系统地利用LLM的内部知识和外部检索信息，改进了报告生成。具体而言，它首先提取与专家图像分类输出对齐的模型获取的知识。然后，它检索相关的补充知识以进一步丰富这些信息。最后，通过聚合这两个来源，Radar生成更准确和全面的放射学报告。在MIMIC-CXR、CheXpert-Plus和IU X-ray上的广泛实验表明，我们的模型在语言质量和临床准确性方面均优于最新LLMs。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大规模语言模型在放射学报告生成等领域具有显著能力。</li>
<li>先前的方法通过整合领域特定知识检索来增强性能，但忽视了LLMs中已嵌入的知识。</li>
<li>Radar框架通过系统地利用LLM的内部知识和外部检索信息，改进了放射学报告生成。</li>
<li>Radar提取与专家图像分类输出对齐的模型获取的知识，并检索相关补充知识。</li>
<li>通过聚合模型内部知识和外部检索信息，Radar能生成更准确和全面的放射学报告。</li>
<li>广泛实验证明，Radar在语言和临床准确性方面优于其他最新LLMs。</li>
<li>Radar框架有助于实现更高效的放射学报告生成，具有潜在的临床应用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14318">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4346ca7b68269d600966ea00eb5aae87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cf4a52ebd60501ee92d5754dea193b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79f8114c02f902fc03aa83ad2f83e385.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-972f09c4b05d732e9f0505f6468e106b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6710f06dbcd37ac7f3e2ad1fda6f4e41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edb754dd3cc987a3ac243e6ff27bd8ed.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SMURF-Scalable-method-for-unsupervised-reconstruction-of-flow-in-4D-flow-MRI"><a href="#SMURF-Scalable-method-for-unsupervised-reconstruction-of-flow-in-4D-flow-MRI" class="headerlink" title="SMURF: Scalable method for unsupervised reconstruction of flow in 4D   flow MRI"></a>SMURF: Scalable method for unsupervised reconstruction of flow in 4D   flow MRI</h2><p><strong>Authors:Atharva Hans, Abhishek Singh, Pavlos Vlachos, Ilias Bilionis</strong></p>
<p>We introduce SMURF, a scalable and unsupervised machine learning method for simultaneously segmenting vascular geometries and reconstructing velocity fields from 4D flow MRI data. SMURF models geometry and velocity fields using multilayer perceptron-based functions incorporating Fourier feature embeddings and random weight factorization to accelerate convergence. A measurement model connects these fields to the observed image magnitude and phase data. Maximum likelihood estimation and subsampling enable SMURF to process high-dimensional datasets efficiently. Evaluations on synthetic, in vitro, and in vivo datasets demonstrate SMURF’s performance. On synthetic internal carotid artery aneurysm data derived from CFD, SMURF achieves a quarter-voxel segmentation accuracy across noise levels of up to 50%, outperforming the state-of-the-art segmentation method by up to double the accuracy. In an in vitro experiment on Poiseuille flow, SMURF reduces velocity reconstruction RMSE by approximately 34% compared to raw measurements. In in vivo internal carotid artery aneurysm data, SMURF attains nearly half-voxel segmentation accuracy relative to expert annotations and decreases median velocity divergence residuals by about 31%, with a 27% reduction in the interquartile range. These results indicate that SMURF is robust to noise, preserves flow structure, and identifies patient-specific morphological features. SMURF advances 4D flow MRI accuracy, potentially enhancing the diagnostic utility of 4D flow MRI in clinical applications. </p>
<blockquote>
<p>我们介绍SMURF，这是一种可扩展的无监督机器学习方法，能够同时从4D流MRI数据中分割血管几何结构并重建速度场。SMURF使用多层感知器函数对几何形状和速度场进行建模，并结合傅里叶特征嵌入和随机权重分解来加速收敛。测量模型将这些场与观察到的图像幅度和相位数据连接起来。最大似然估计和子采样使SMURF能够高效处理高维数据集。在合成、体外和体内数据集上的评估证明了SMURF的性能。在由CFD派生的合成内部颈动脉动脉瘤数据上，SMURF在高达50%的噪声水平下实现了四分之一体素的分割精度，比最新分割方法的精度高出两倍。在泊松流的体外实验中，SMURF与原始测量相比，速度重建的RMSE降低了约34%。在体内的内部颈动脉动脉瘤数据中，SMURF相对于专家注释达到了近一半体素的分割精度，并将中位数速度发散残差减少了约31%，四分位距减少了27%。这些结果表明，SMURF对噪声具有鲁棒性，能够保留流动结构，并识别患者特定的形态特征。SMURF提高了4D流MRI的准确性，有望增强4D流MRI在临床应用中的诊断效用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12494v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SMURF是一种可扩展的无监督机器学习方法，可同时实现血管几何结构的分割和速度场的重建，适用于处理四维血流MRI数据。SMURF采用多层感知器函数建模几何和速度场，并结合傅里叶特征嵌入和随机权重分解加速收敛。其在合成、体外和体内数据集上的评估证明了其性能优势。SMURF具有抗噪性强、保留血流结构以及识别患者特定形态特征等优点，提高了四维血流MRI的准确性，有望增强其在临床应用中的诊断价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SMURF是一种用于处理四维血流MRI数据的可扩展无监督机器学习技术。</li>
<li>SMURF能同时分割血管几何结构和重建速度场。</li>
<li>SMURF采用多层感知器结合傅里叶特征嵌入和随机权重分解来建模。</li>
<li>在合成数据集上，SMURF在噪声水平高达50%的情况下实现了四分之一体素的分割精度，优于现有技术。</li>
<li>在体外实验中，SMURF将速度重建的RMSE降低了约34%。</li>
<li>在体内数据集上，SMURF达到了近乎一半体素的分割精度，并降低了速度散度残差。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12494">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3b58ec44a966ca418dafbd99cd67115a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2c3a49f416bb2c7b9d48e60c0240f2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edf19d7396ae7717ab10bb012669e6b8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RAFT-Robust-Augmentation-of-FeaTures-for-Image-Segmentation"><a href="#RAFT-Robust-Augmentation-of-FeaTures-for-Image-Segmentation" class="headerlink" title="RAFT: Robust Augmentation of FeaTures for Image Segmentation"></a>RAFT: Robust Augmentation of FeaTures for Image Segmentation</h2><p><strong>Authors:Edward Humes, Xiaomin Lin, Uttej Kallakuri, Tinoosh Mohsenin</strong></p>
<p>Image segmentation is a powerful computer vision technique for scene understanding. However, real-world deployment is stymied by the need for high-quality, meticulously labeled datasets. Synthetic data provides high-quality labels while reducing the need for manual data collection and annotation. However, deep neural networks trained on synthetic data often face the Syn2Real problem, leading to poor performance in real-world deployments.   To mitigate the aforementioned gap in image segmentation, we propose RAFT, a novel framework for adapting image segmentation models using minimal labeled real-world data through data and feature augmentations, as well as active learning. To validate RAFT, we perform experiments on the synthetic-to-real “SYNTHIA-&gt;Cityscapes” and “GTAV-&gt;Cityscapes” benchmarks. We managed to surpass the previous state of the art, HALO. SYNTHIA-&gt;Cityscapes experiences an improvement in mIoU* upon domain adaptation of 2.1%&#x2F;79.9%, and GTAV-&gt;Cityscapes experiences a 0.4%&#x2F;78.2% improvement in mIoU. Furthermore, we test our approach on the real-to-real benchmark of “Cityscapes-&gt;ACDC”, and again surpass HALO, with a gain in mIoU upon adaptation of 1.3%&#x2F;73.2%. Finally, we examine the effect of the allocated annotation budget and various components of RAFT upon the final transfer mIoU. </p>
<blockquote>
<p>图像分割是场景理解的一种强大的计算机视觉技术。然而，实际部署却受到需要高质量、精细标注数据集的限制。合成数据提供了高质量标签，同时减少了手动数据收集和标注的需求。然而，在合成数据上训练的深度神经网络常常面临Syn2Real问题，导致在实际部署中的性能不佳。为了减轻上述图像分割中的差距，我们提出了RAFT，这是一个使用最小量的真实世界数据，通过数据和特征增强以及主动学习，适应图像分割模型的新框架。为了验证RAFT的效果，我们在合成到真实的”SYNTHIA-&gt;Cityscapes”和”GTAV-&gt;Cityscapes”基准测试集上进行了实验。我们超越了之前的技术水平HALO。在SYNTHIA-&gt;Cityscapes上，我们的域适应方法提高了mIoU*（平均交并比）的准确度达到2.1%&#x2F;79.9%，而在GTAV-&gt;Cityscapes上提高了mIoU的准确度达到0.4%&#x2F;78.2%。此外，我们在真实到真实的”Cityscapes-&gt;ACDC”基准测试集上测试了我们的方法，并再次超越了HALO，在适应后提高了mIoU的准确度达到1.3%&#x2F;73.2%。最后，我们研究了分配标注预算对RAFT最终迁移mIoU的影响以及各种组件的影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04529v2">PDF</a> </p>
<p><strong>Summary</strong><br>     图像分割是一种强大的计算机视觉场景理解技术，但在实际部署中需要高质量、精细标注的数据集。合成数据可以提供高质量标签，减少手动收集和标注的需求。然而，在合成数据上训练的深度神经网络常面临合成到真实（Syn2Real）问题，导致在真实世界部署中的性能不佳。为缓解图像分割中的前述差距，我们提出RAFT框架，通过最小真实世界数据、数据增强和特征增强以及主动学习方法，适应图像分割模型。验证实验表明，RAFT在合成到真实场景的基准测试中表现超越先前技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像分割是计算机视觉中的一项重要技术，用于场景理解。</li>
<li>实际部署图像分割技术时，需要高质量、精细标注的数据集。</li>
<li>合成数据可以减少手动收集和标注数据的需求，但训练的模型在真实世界部署中常面临性能问题。</li>
<li>提出的RAFT框架旨在通过最小真实世界数据、数据增强和特征增强以及主动学习方法，适应图像分割模型。</li>
<li>RAFT在合成到真实场景的基准测试中表现超越先前技术。</li>
<li>在SYNTHIA-&gt;Cityscapes和GTAV-&gt;Cityscapes的基准测试中，RAFT在mIoU指标上实现了显著的改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04529">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-551b7af54f06f86899d8e8e560e75948.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6ef83331d867655fe12354178b25379.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40ad481c5b644787162f11a7fa58e51c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b8da820a548ac6af52a12f4411c248e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CSTRL-Context-Driven-Sequential-Transfer-Learning-for-Abstractive-Radiology-Report-Summarization"><a href="#CSTRL-Context-Driven-Sequential-Transfer-Learning-for-Abstractive-Radiology-Report-Summarization" class="headerlink" title="CSTRL: Context-Driven Sequential Transfer Learning for Abstractive   Radiology Report Summarization"></a>CSTRL: Context-Driven Sequential Transfer Learning for Abstractive   Radiology Report Summarization</h2><p><strong>Authors:Mst. Fahmida Sultana Naznin, Adnan Ibney Faruq, Mostafa Rifat Tazwar, Md Jobayer, Md. Mehedi Hasan Shawon, Md Rakibul Hasan</strong></p>
<p>A radiology report comprises several sections, including the Findings and Impression of the diagnosis. Automatically generating the Impression from the Findings is crucial for reducing radiologists’ workload and improving diagnostic accuracy. Pretrained models that excel in common abstractive summarization problems encounter challenges when applied to specialized medical domains largely due to the complex terminology and the necessity for accurate clinical context. Such tasks in medical domains demand extracting core information, avoiding context shifts, and maintaining proper flow. Misuse of medical terms can lead to drastic clinical errors. To address these issues, we introduce a sequential transfer learning that ensures key content extraction and coherent summarization. Sequential transfer learning often faces challenges like initial parameter decay and knowledge loss, which we resolve with the Fisher matrix regularization. Using MIMIC-CXR and Open-I datasets, our model, CSTRL - Context-driven Sequential TRansfer Learning - achieved state-of-the-art performance, showing 56.2% improvement in BLEU-1, 40.5% in BLEU-2, 84.3% in BLEU-3, 28.9% in ROUGE-1, 41.0% in ROUGE-2 and 26.5% in ROGUE-3 score over benchmark studies. We also analyze factual consistency scores while preserving the medical context. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/fahmidahossain/Report_Summarization">https://github.com/fahmidahossain/Report_Summarization</a>. </p>
<blockquote>
<p>医学放射报告由几个部分组成，包括诊断的检查结果和印象部分。自动根据检查结果生成诊断印象对减少放射科医生的工作量、提高诊断准确性至关重要。预训练模型在常见的抽象摘要问题中表现优异，但当应用于专业医学领域时，由于复杂的术语和准确的临床语境的必要性，它们面临挑战。此类医学领域的任务需要提取核心信息，避免上下文偏移，并保持适当的流程。医学术语的误用可能导致严重的临床错误。为了解决这些问题，我们引入了一种顺序迁移学习，以确保关键内容提取和连贯的摘要。顺序迁移学习经常面临初始参数衰减和知识损失等挑战，我们通过Fisher矩阵正则化来解决这些问题。使用MIMIC-CXR和Open-I数据集，我们的模型CSTRL（基于上下文驱动的序列迁移学习）取得了最先进的性能表现。相较于基准研究，BLEU-1得分提高了56.2%，BLEU-2得分提高了40.5%，BLEU-3得分提高了84.3%，在ROUGE得分方面也有所提高。我们还分析了保持医学上下文的一致度得分。我们的代码公开在<a target="_blank" rel="noopener" href="https://github.com/fahmidahossain/Report_Summarization">https://github.com/fahmidahossain/Report_Summarization</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05750v2">PDF</a> Accepted in ACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了一种针对医学图像报告自动摘要的技术。通过使用序贯迁移学习，该技术在医学领域的文本摘要中实现了显著提升，可以有效减少医生的工作量并提升诊断的准确性。模型的摘要结果公开可访问。文中提到了挑战与解决策略以及模型效果评估结果。 </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章介绍了一种基于序贯迁移学习的医学图像报告自动摘要技术。</li>
<li>该技术解决了传统模型在医学领域文本摘要中面临的挑战，如术语复杂度和准确临床语境的把握。</li>
<li>该方法能够在关键内容提取和连贯性总结之间实现平衡，避免因医学术语误用导致的临床错误。</li>
<li>通过使用Fisher矩阵正则化，解决了序贯迁移学习中初始参数衰减和知识损失的问题。</li>
<li>模型在MIMIC-CXR和Open-I数据集上进行了实验验证，取得了显著的提升效果。相较于基准研究，模型在BLEU和ROUGE得分上有显著提高。</li>
<li>文章对模型的性能进行了详细评估，包括事实一致性评估，确保在保留医学语境的同时提高摘要的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05750">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ceba3cf403083361b765f1f7073222ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-231ba19dbc098c5a9f82d7f38e38db3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1a4e1f195849dcad5e3ca234a5087ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3384149a65a8a4fbdb5231c65c7e0be0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Fourier-Asymmetric-Attention-on-Domain-Generalization-for-Pan-Cancer-Drug-Response-Prediction"><a href="#Fourier-Asymmetric-Attention-on-Domain-Generalization-for-Pan-Cancer-Drug-Response-Prediction" class="headerlink" title="Fourier Asymmetric Attention on Domain Generalization for Pan-Cancer   Drug Response Prediction"></a>Fourier Asymmetric Attention on Domain Generalization for Pan-Cancer   Drug Response Prediction</h2><p><strong>Authors:Ran Song, Yinpu Bai, Hui Liu</strong></p>
<p>The accurate prediction of drug responses remains a formidable challenge, particularly at the single-cell level and in clinical treatment contexts. Some studies employ transfer learning techniques to predict drug responses in individual cells and patients, but they require access to target-domain data during training, which is often unavailable or only obtainable in future. In this study, we propose a novel domain generalization framework, termed FourierDrug, to address this challenge. Given the extracted feature from expression profile, we performed Fourier transforms and then introduced an asymmetric attention constraint that would cluster drug-sensitive samples into a compact group while drives resistant samples dispersed in the frequency domain. Our empirical experiments demonstrate that our model effectively learns task-relevant features from diverse source domains, and achieves accurate predictions of drug response for unseen cancer type. When evaluated on single-cell and patient-level drug response prediction tasks, FourierDrug–trained solely on in vitro cell line data without access to target-domain data–consistently outperforms or, at least, matched the performance of current state-of-the-art methods. These findings underscore the potential of our method for real-world clinical applications. </p>
<blockquote>
<p>准确预测药物反应仍然是一项艰巨的挑战，特别是在单细胞水平和临床治疗环境中。一些研究采用迁移学习技术来预测单个细胞和患者的药物反应，但它们需要在训练过程中访问目标域数据，而这些数据通常不可用或只能在将来获得。本研究提出了一种新的领域泛化框架，称为FourierDrug，以解决这一挑战。根据表达谱提取的特征，我们进行了傅里叶变换，然后引入了一个不对称注意力约束，该约束会将药物敏感样本聚集成一个紧凑的群体，同时使耐药样本在频域中分散。我们的经验实验表明，我们的模型有效地从多个源域学习了任务相关特征，并对未见过的癌症类型实现了准确的药物反应预测。在单细胞和患者水平的药物反应预测任务上评估时，FourierDrug（仅在体外细胞系数据上训练，无需访问目标域数据）始终优于或至少与当前最新方法的表现相匹配。这些发现突出了我们的方法在实际临床应用中的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04034v2">PDF</a> </p>
<p><strong>Summary</strong><br>医学领域预测药物反应仍是重大挑战，特别是在单细胞层面和临床治疗环境中。本研究提出了一种名为FourierDrug的新型域泛化框架，通过傅里叶变换和不对称注意力约束，能在未接触目标域数据的情况下，有效学习来自不同源域的任务相关特征，并对未见过的癌症类型进行药物反应预测。在单细胞和患者层面的药物反应预测任务中，FourierDrug的表现优于或至少与当前最先进的方法相匹配。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学领域预测药物反应是一大挑战，特别是在单细胞和临床治疗环境中。</li>
<li>研究提出了名为FourierDrug的新型域泛化框架来解决此挑战。</li>
<li>FourierDrug利用傅里叶变换和不对称注意力约束进行特征提取和分类。</li>
<li>该模型可在未接触目标域数据的情况下，学习来自不同源域的任务相关特征。</li>
<li>FourierDrug在未见过的癌症类型药物反应预测中表现出优异性能。</li>
<li>在单细胞和患者层面的药物反应预测任务中，FourierDrug的表现优于或至少与当前最先进的方法相匹配。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04034">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-87ba2fc87d40c563c56bedd0a2998f0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7492705d46aded771d10b9b187e727d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ccf7bf5e7876eaa874b026e2ceaf90f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Parameter-Efficient-Fine-Tuning-of-Segment-Anything-Model-for-Biomedical-Imaging"><a href="#Parameter-Efficient-Fine-Tuning-of-Segment-Anything-Model-for-Biomedical-Imaging" class="headerlink" title="Parameter Efficient Fine-Tuning of Segment Anything Model for Biomedical   Imaging"></a>Parameter Efficient Fine-Tuning of Segment Anything Model for Biomedical   Imaging</h2><p><strong>Authors:Carolin Teuber, Anwai Archit, Constantin Pape</strong></p>
<p>Segmentation is an important analysis task for biomedical images, enabling the study of individual organelles, cells or organs. Deep learning has massively improved segmentation methods, but challenges remain in generalization to new conditions, requiring costly data annotation. Vision foundation models, such as Segment Anything Model (SAM), address this issue through improved generalization. However, these models still require finetuning on annotated data, although with less annotations, to achieve optimal results for new conditions. As a downside, they require more computational resources. This makes parameter-efficient finetuning (PEFT) relevant. We contribute the first comprehensive study of PEFT for SAM applied to biomedical images. We find that the placement of PEFT layers is more important for efficiency than the type of layer for vision transformers and we provide a recipe for resource-efficient finetuning. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/computational-cell-analytics/peft-sam">https://github.com/computational-cell-analytics/peft-sam</a>. </p>
<blockquote>
<p>分割是生物医学图像分析的重要任务，能够对单个细胞器、细胞或器官进行研究。深度学习极大地改善了分割方法，但仍然存在对新条件泛化方面的挑战，这需要昂贵的标注数据。视觉基础模型（如任何分割模型（SAM））通过提高泛化能力来解决这个问题。然而，这些模型虽然使用较少的标注数据，但仍需在标注数据上进行微调，以实现新条件下的最佳结果。作为代价，它们需要更多的计算资源。这使参数高效微调（PEFT）变得至关重要。我们对应用于生物医学图像的SAM的PEFT进行了首次全面研究。我们发现对于视觉转换器而言，PEFT层的放置比层的类型对效率更重要，并为资源高效的微调提供了一个配方。我们的代码公开在<a target="_blank" rel="noopener" href="https://github.com/computational-cell-analytics/peft-sam%E3%80%82">https://github.com/computational-cell-analytics/peft-sam。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00418v2">PDF</a> Published in MIDL 2025</p>
<p><strong>Summary</strong></p>
<p>深度学习在生物医学图像分割方法中发挥了巨大作用，但仍存在对新条件泛化能力有限的问题，需要昂贵的数据标注成本。Segment Anything Model（SAM）等视觉基础模型通过改进泛化能力来解决这一问题，但仍需在标注数据上进行微调以达到最佳效果。参数高效微调（PEFT）是一种解决该问题的方法，本研究首次全面研究了PEFT在生物医学图像中应用于SAM的效果，发现对于视觉转换器而言，PEFT层的放置比层的类型更重要，并提供了一种资源高效的微调配方。相关代码已公开于<a target="_blank" rel="noopener" href="https://github.com/computational-cell-analytics/peft-sam%E3%80%82">https://github.com/computational-cell-analytics/peft-sam。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习在生物医学图像分割中发挥着重要作用，但仍面临对新条件泛化能力和数据标注成本的问题。</li>
<li>Segment Anything Model（SAM）通过改进泛化能力来解决这些问题，但仍需微调以达到最佳效果。</li>
<li>参数高效微调（PEFT）是一种解决微调所需大量计算资源的方法。</li>
<li>PEFT在生物医学图像中应用于SAM的效果研究指出，PEFT层的放置比层的类型更重要。</li>
<li>研究提供了一种资源高效的微调配方。</li>
<li>该研究的相关代码已公开，便于其他研究者使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00418">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-326d75029ed32c3e2a6955ae29db5211.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb3b330a8b90a45f3765a0d01676afc3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a5410ed2838d6848c04a95341447c780.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e8ae7e1c0d7a73ff4ca1e016e76cade.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Segment-Anything-for-Histopathology"><a href="#Segment-Anything-for-Histopathology" class="headerlink" title="Segment Anything for Histopathology"></a>Segment Anything for Histopathology</h2><p><strong>Authors:Titus Griebel, Anwai Archit, Constantin Pape</strong></p>
<p>Nucleus segmentation is an important analysis task in digital pathology. However, methods for automatic segmentation often struggle with new data from a different distribution, requiring users to manually annotate nuclei and retrain data-specific models. Vision foundation models (VFMs), such as the Segment Anything Model (SAM), offer a more robust alternative for automatic and interactive segmentation. Despite their success in natural images, a foundation model for nucleus segmentation in histopathology is still missing. Initial efforts to adapt SAM have shown some success, but did not yet introduce a comprehensive model for diverse segmentation tasks. To close this gap, we introduce PathoSAM, a VFM for nucleus segmentation, based on training SAM on a diverse dataset. Our extensive experiments show that it is the new state-of-the-art model for automatic and interactive nucleus instance segmentation in histopathology. We also demonstrate how it can be adapted for other segmentation tasks, including semantic nucleus segmentation. For this task, we show that it yields results better than popular methods, while not yet beating the state-of-the-art, CellViT. Our models are open-source and compatible with popular tools for data annotation. We also provide scripts for whole-slide image segmentation. Our code and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/computational-cell-analytics/patho-sam">https://github.com/computational-cell-analytics/patho-sam</a>. </p>
<blockquote>
<p>细胞核分割是数字病理学中的重要分析任务。然而，自动分割方法通常难以处理来自不同分布的新数据，需要用户手动注释细胞核并重新训练特定数据模型。例如分段任何模型（SAM）的视野基础模型（VFMs）为自动和交互式分割提供了更稳健的替代方案。尽管它们在自然图像中取得了成功，但用于病理学细胞核分割的基础模型仍然缺失。初步适应SAM的努力已经取得了一些成功，但并没有提出一个用于多种分割任务的全面模型。为了填补这一空白，我们引入了PathoSAM，这是一个基于多样数据集训练的细胞核分割的VFM。我们的广泛实验表明，它是病理学自动和交互式细胞核实例分割的最新最先进的模型。我们还展示了如何将其适应于其他分割任务，包括语义细胞核分割。对于此任务，我们展示它产生的结果优于流行的方法，但尚未达到当前最先进的CellViT模型的水平。我们的模型是开源的，与流行的数据注释工具兼容。我们还提供了用于全幻灯片图像分割的脚本。我们的代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/computational-cell-analytics/patho-sam%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/computational-cell-analytics/patho-sam公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00408v2">PDF</a> Published in MIDL 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于训练多样化数据集的Vision Foundation Model（PathoSAM）用于细胞核分割的方法。该方法在数字病理学领域实现了自动和交互式细胞核实例分割的新水平，并可适应其他分割任务。模型开源，兼容数据标注工具，并提供全切片图像分割脚本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Nucleus segmentation是数字病理学中的重要分析任务，但自动分割方法在新数据上表现不佳，需要用户手动标注细胞核并重新训练特定数据模型。</li>
<li>Vision Foundation Models（VFMs）如Segment Anything Model（SAM）为自动和交互式分割提供了更稳健的替代方案，但在数字病理学中缺少专门针对细胞核分割的模型。</li>
<li>PathoSAM是一个基于SAM的VFM模型，经过多样化数据集训练，用于细胞核分割。</li>
<li>PathoSAM实现了自动和交互式细胞核实例分割的新水平，并可以适应其他分割任务，包括语义细胞核分割。</li>
<li>虽然PathoSAM在某些任务上表现出优秀性能，但在某些任务上仍未达到当前最佳模型CellViT的性能。</li>
<li>PathoSAM模型是开源的，与流行的数据标注工具兼容，并提供全切片图像分割脚本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00408">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-709805a43b57b9af0b29dead130fd918.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76490c7ed8229245d1d4c5a9994648b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0aede69817dee97f06ffe639ce5c5d1c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="In-the-Picture-Medical-Imaging-Datasets-Artifacts-and-their-Living-Review"><a href="#In-the-Picture-Medical-Imaging-Datasets-Artifacts-and-their-Living-Review" class="headerlink" title="In the Picture: Medical Imaging Datasets, Artifacts, and their Living   Review"></a>In the Picture: Medical Imaging Datasets, Artifacts, and their Living   Review</h2><p><strong>Authors:Amelia Jiménez-Sánchez, Natalia-Rozalia Avlona, Sarah de Boer, Víctor M. Campello, Aasa Feragen, Enzo Ferrante, Melanie Ganz, Judy Wawira Gichoya, Camila González, Steff Groefsema, Alessa Hering, Adam Hulman, Leo Joskowicz, Dovile Juodelyte, Melih Kandemir, Thijs Kooi, Jorge del Pozo Lérida, Livie Yumeng Li, Andre Pacheco, Tim Rädsch, Mauricio Reyes, Théo Sourget, Bram van Ginneken, David Wen, Nina Weng, Jack Junchi Xu, Hubert Dariusz Zając, Maria A. Zuluaga, Veronika Cheplygina</strong></p>
<p>Datasets play a critical role in medical imaging research, yet issues such as label quality, shortcuts, and metadata are often overlooked. This lack of attention may harm the generalizability of algorithms and, consequently, negatively impact patient outcomes. While existing medical imaging literature reviews mostly focus on machine learning (ML) methods, with only a few focusing on datasets for specific applications, these reviews remain static – they are published once and not updated thereafter. This fails to account for emerging evidence, such as biases, shortcuts, and additional annotations that other researchers may contribute after the dataset is published. We refer to these newly discovered findings of datasets as research artifacts. To address this gap, we propose a living review that continuously tracks public datasets and their associated research artifacts across multiple medical imaging applications. Our approach includes a framework for the living review to monitor data documentation artifacts, and an SQL database to visualize the citation relationships between research artifact and dataset. Lastly, we discuss key considerations for creating medical imaging datasets, review best practices for data annotation, discuss the significance of shortcuts and demographic diversity, and emphasize the importance of managing datasets throughout their entire lifecycle. Our demo is publicly available at <a target="_blank" rel="noopener" href="http://inthepicture.itu.dk/">http://inthepicture.itu.dk/</a>. </p>
<blockquote>
<p>数据集在医学成像研究中扮演着至关重要的角色，然而标签质量、捷径和元数据等问题往往被忽视。这种缺乏关注可能会损害算法的通用性，并因此对患者结果产生负面影响。尽管现有的医学成像文献综述主要集中在机器学习（ML）方法上，只有少数关注特定应用的数据集，但这些综述是静态的——一经发布，就不再更新。这未能考虑到新兴证据，如偏见、捷径和其他研究人员在数据集发布后可能做出的额外注释等。我们将这些新发现的数据集称为研究产物。为了弥补这一空白，我们提出了一种持续追踪多个医学成像应用中的公共数据集及其相关研究产物的动态综述。我们的方法包括一个用于监控数据文档产物和可视化数据集与研究产物之间引文关系的SQL数据库框架。最后，我们讨论了创建医学成像数据集的关键注意事项，回顾了数据注释的最佳实践，讨论了捷径和人口多样性的重要性，并强调了整个数据生命周期中管理数据集的重要性。我们的演示公开访问地址为 <a target="_blank" rel="noopener" href="http://inthepicture.itu.dk/">http://inthepicture.itu.dk/</a>.</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10727v2">PDF</a> ACM Conference on Fairness, Accountability, and Transparency - FAccT   2025</p>
<p><strong>Summary</strong><br>     医学成像研究中数据集至关重要，但标签质量、捷径和元数据等问题常被忽视。这些问题可能影响算法泛化能力，进而影响患者治疗效果。现有医学成像文献综述主要关注机器学习（ML）方法，仅少数关注特定应用的数据集，但这些综述一经发布便不再更新，无法涵盖新兴证据，如偏见、捷径和附加注释等研究者贡献的研究工件。为解决此问题，我们提出了一个持续追踪多个医学成像应用公共数据集及其相关研究工件的动态综述。我们的方法包括一个用于监控数据文档工件动态综述的框架和一个SQL数据库，以可视化研究工件与数据集之间的引文关系。最后，我们讨论了创建医学成像数据集的关键考量因素，回顾了数据注释的最佳实践，讨论了捷径和人口多样性的重要性，并强调了整个数据集生命周期中管理数据集的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>数据集在医学成像研究中的重要性及其常忽略的问题，如标签质量、捷径和元数据。</p>
</li>
<li><p>现有文献综述的局限性，对新兴证据（如偏见、捷径等研究工件）无法充分覆盖的问题。 </p>
</li>
<li><p>提议的“动态综述”的概念来跟踪公共数据集及其相关研究结果工件的应用表现跨多个医学成像领域的观点及其优势。  </p>
</li>
<li><p>通过监控数据文档工件的方式提出框架以应对动态综述的要求以及可视化数据库描述研究工件与数据集之间的关系的描述。 </p>
</li>
<li><p>讨论创建医学成像数据集时的关键考量因素。  </p>
</li>
<li><p>介绍并回顾数据注释的最佳实践及其意义和价值重要性增加新的角度考量对现有的概念有全面更深入的理解对数据和算法有更全面的了解对数据和算法的优化有更清晰的思路对科研研究有更深刻的理解和应用价值对科研实践有重要指导意义强调捷径和人口多样性的重要性在整个数据集中如何避免并有效利用这些捷径增加模型训练过程中的数据多样性的意义对于解决实际应用中的问题提供更全面有效的思路和方向通过理解不同数据和算法的优劣势形成全面的理解和科学的实践建议可以充分利用并规避数据的局限提高研究质量和成果应用的广度对当前学术和实践的影响为进一步的学术研究和技术创新提供了基础和应用价值有助于相关领域的科研工作者更好地理解和应用医学成像数据集。</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10727">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5755be9d62321239b04de17a7fc3cd69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40c8200ee549b56e8a5d69727bac3284.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ade82dc7cec410670becac191b52cc0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MADUV-The-1st-INTERSPEECH-Mice-Autism-Detection-via-Ultrasound-Vocalization-Challenge"><a href="#MADUV-The-1st-INTERSPEECH-Mice-Autism-Detection-via-Ultrasound-Vocalization-Challenge" class="headerlink" title="MADUV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound   Vocalization Challenge"></a>MADUV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound   Vocalization Challenge</h2><p><strong>Authors:Zijiang Yang, Meishu Song, Xin Jing, Haojie Zhang, Kun Qian, Bin Hu, Kota Tamada, Toru Takumi, Björn W. Schuller, Yoshiharu Yamamoto</strong></p>
<p>The Mice Autism Detection via Ultrasound Vocalization (MADUV) Challenge introduces the first INTERSPEECH challenge focused on detecting autism spectrum disorder (ASD) in mice through their vocalizations. Participants are tasked with developing models to automatically classify mice as either wild-type or ASD models based on recordings with a high sampling rate. Our baseline system employs a simple CNN-based classification using three different spectrogram features. Results demonstrate the feasibility of automated ASD detection, with the considered audible-range features achieving the best performance (UAR of 0.600 for segment-level and 0.625 for subject-level classification). This challenge bridges speech technology and biomedical research, offering opportunities to advance our understanding of ASD models through machine learning approaches. The findings suggest promising directions for vocalization analysis and highlight the potential value of audible and ultrasound vocalizations in ASD detection. </p>
<blockquote>
<p>通过超声波发声（MADUV）检测小鼠自闭症（Mice Autism Detection via Ultrasound Vocalization，简称MADUV）挑战引入了首个聚焦于通过小鼠的发声来检测自闭症谱系障碍（ASD）的INTERSPEECH挑战。参赛者的任务是开发模型，根据高采样率的录音自动将小鼠分类为野生型或ASD模型。我们的基线系统采用基于简单卷积神经网络（CNN）的分类方法，使用三种不同的频谱特征。结果表明，自动化检测ASD是可行的，在考虑的音频范围内的特征取得了最佳性能（分段级别的UAR为0.600，主体级别的分类为0.625）。此次挑战将语音技术与生物医学研究相结合，为通过机器学习方法了解ASD模型提供了机会。研究结果表明了发声分析的希望方向，并突出了听觉和超声波发声在ASD检测中的潜在价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04292v3">PDF</a> 5 pages, 1 figure and 2 tables. Submitted to INTERSPEECH 2025. For   MADUV Challenge 2025</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了通过超声发声检测小鼠自闭症（MADUV）挑战，这是首个聚焦于通过小鼠发声检测自闭症谱系障碍（ASD）的INTERSPEECH挑战。参与者需开发模型自动将小鼠分类为野生型或ASD模型，依据的是高采样率录音。基线系统采用简单的基于CNN的分类方法，使用三种不同的频谱特征。结果表明自动化ASD检测的可行性，所考虑的听觉范围特征取得最佳性能（分段级别UAR为0.600，主题级别分类为0.625）。该挑战将语音识别技术与生物医学研究相结合，通过机器学习的方法推动对ASD模型的理解。发现表明声音分析的方向具有希望，并突显了听觉和超声波发声在ASD检测中的潜在价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MADUV挑战是首个聚焦于通过小鼠发声检测自闭症谱系障碍（ASD）的INTERSPEECH挑战。</li>
<li>参与者需开发模型自动分类小鼠，区分野生型和ASD模型，依据高采样率录音。</li>
<li>基线系统采用简单的基于CNN的分类方法，使用三种不同的频谱特征进行分类。</li>
<li>自动化ASD检测的可行性得到验证，其中听觉范围特征表现最佳。</li>
<li>分段级别和主题级别的分类性能分别达到了UAR 0.600和0.625。</li>
<li>该挑战推动了语音识别技术与生物医学研究的结合，通过机器学习的方法加深对ASD模型的理解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04292">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-73b11d0f6c0d21128ff8ce2ab937a88f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-476223deedecb318184529c3e6fe089d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8491a401c899c97e5ecc91635dac954a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9321738a231992c91216abd139a8b05.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SCC-YOLO-An-Improved-Object-Detector-for-Assisting-in-Brain-Tumor-Diagnosis"><a href="#SCC-YOLO-An-Improved-Object-Detector-for-Assisting-in-Brain-Tumor-Diagnosis" class="headerlink" title="SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor   Diagnosis"></a>SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor   Diagnosis</h2><p><strong>Authors:Runci Bai, Guibao Xu, Yanze Shi</strong></p>
<p>Brain tumors can lead to neurological dysfunction, cognitive and psychological changes, increased intracranial pressure, and seizures, posing significant risks to health. The You Only Look Once (YOLO) series has shown superior accuracy in medical imaging object detection. This paper presents a novel SCC-YOLO architecture that integrates the SCConv module into YOLOv9. The SCConv module optimizes convolutional efficiency by reducing spatial and channel redundancy, enhancing image feature learning. We examine the effects of different attention mechanisms with YOLOv9 for brain tumor detection using the Br35H dataset and our custom dataset (Brain_Tumor_Dataset). Results indicate that SCC-YOLO improved mAP50 by 0.3% on the Br35H dataset and by 0.5% on our custom dataset compared to YOLOv9. SCC-YOLO achieves state-of-the-art performance in brain tumor detection. </p>
<blockquote>
<p>脑肿瘤可能导致神经功能障碍、认知和心理学变化、颅内压升高和癫痫发作，对健康构成重大风险。You Only Look Once（YOLO）系列已在医学影像目标检测中展现出卓越准确性。本文提出了一种新型的SCC-YOLO架构，它将SCConv模块集成到YOLOv9中。SCConv模块通过减少空间冗余和通道冗余来优化卷积效率，从而增强图像特征学习。我们使用Br35H数据集和我们自己的数据集（Brain_Tumor_Dataset）来检验不同注意力机制对YOLOv9在脑肿瘤检测上的影响。结果表明，与YOLOv9相比，SCC-YOLO在Br35H数据集上的mAP50提高了0.3%，在我们自己的数据集上提高了0.5%。SCC-YOLO在脑肿瘤检测方面达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03836v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型的SCC-YOLO架构，它将SCConv模块集成到YOLOv9中以提高医学图像中目标检测的准确性。研究结果表明，在脑肿瘤检测方面，SCC-YOLO在Br35H数据集上的mAP50提高了0.3%，在自定义数据集上的mAP50提高了0.5%，达到了当前最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文中探讨了脑肿瘤可能引发的健康风险，如神经功能障碍、认知和心理变化等。</li>
<li>YOLO系列在医学成像物体检测中展现出高精度性能。</li>
<li>新提出的SCC-YOLO架构集成了SCConv模块以提升图像特征学习能力并优化卷积效率。</li>
<li>研究者通过Br35H数据集和自定义数据集（Brain_Tumor_Dataset）评估了不同注意力机制对脑肿瘤检测的影响。</li>
<li>SCC-YOLO相较于YOLOv9在Br35H数据集上的mAP50提高了0.3%，在自定义数据集上提高了0.5%。</li>
<li>SCC-YOLO在脑肿瘤检测方面达到了当前最先进的性能水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03836">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9bc8dc1d7eebd3b81f562305c90f90cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4f6d06940ae5bf0eafca03c04e52691.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acbfacca7b571a58bf0b0fa81abad8b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb6961d8099ccbc5926cd21d76083524.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5a030700c8e9ee0a682b707ec4d1c9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99fa394558c33c410fafaa9e299b78de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df32214ff3ee1d3949d63a78474639a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f6ffde02c70c0b5df5c3471db59e70f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Exploring-Compositional-Generalization-of-Multimodal-LLMs-for-Medical-Imaging"><a href="#Exploring-Compositional-Generalization-of-Multimodal-LLMs-for-Medical-Imaging" class="headerlink" title="Exploring Compositional Generalization of Multimodal LLMs for Medical   Imaging"></a>Exploring Compositional Generalization of Multimodal LLMs for Medical   Imaging</h2><p><strong>Authors:Zhenyang Cai, Junying Chen, Rongsheng Wang, Weihong Wang, Yonglin Deng, Dingjie Song, Yize Chen, Zixu Zhang, Benyou Wang</strong></p>
<p>Medical imaging provides essential visual insights for diagnosis, and multimodal large language models (MLLMs) are increasingly utilized for its analysis due to their strong generalization capabilities; however, the underlying factors driving this generalization remain unclear. Current research suggests that multi-task training outperforms single-task as different tasks can benefit each other, but they often overlook the internal relationships within these tasks. To analyze this phenomenon, we attempted to employ compositional generalization (CG), which refers to the models’ ability to understand novel combinations by recombining learned elements, as a guiding framework. Since medical images can be precisely defined by Modality, Anatomical area, and Task, naturally providing an environment for exploring CG, we assembled 106 medical datasets to create Med-MAT for comprehensive experiments. The experiments confirmed that MLLMs can use CG to understand unseen medical images and identified CG as one of the main drivers of the generalization observed in multi-task training. Additionally, further studies demonstrated that CG effectively supports datasets with limited data and confirmed that MLLMs can achieve CG across classification and detection tasks, underscoring its broader generalization potential. Med-MAT is available at <a target="_blank" rel="noopener" href="https://github.com/FreedomIntelligence/Med-MAT">https://github.com/FreedomIntelligence/Med-MAT</a>. </p>
<blockquote>
<p>医学成像为诊断提供了必要的视觉洞察力，多模态大型语言模型（MLLMs）由于其强大的泛化能力而越来越多地被用于医学图像分析；然而，驱动这种泛化的潜在因素尚不清楚。当前的研究表明，多任务训练优于单任务训练，因为不同的任务可以相互受益，但它们往往会忽略这些任务之间的内部关系。为了分析这一现象，我们尝试采用组合泛化（CG）作为指导框架，组合泛化是指模型通过重新组合已学习的元素来理解新型组合的能力。由于医学图像可以通过模态、解剖部位和任务进行精确定义，自然地为探索CG提供了环境，我们收集了106个医学数据集，创建了Med-MAT用于进行综合实验。实验证实，MLLMs可以利用CG来理解未见过的医学图像，并确定CG是多任务训练中观察到的泛化的主要驱动因素之一。另外，进一步的研究表明，CG有效地支持了数据有限的数据集，并证实MLLMs可以在分类和检测任务中实现CG，突出了其更广泛的泛化潜力。Med-MAT可在<a target="_blank" rel="noopener" href="https://github.com/FreedomIntelligence/Med-MAT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/FreedomIntelligence/Med-MAT获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20070v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探索了医学图像分析中的多模态大型语言模型（MLLMs）的推广能力，尤其是其通过组合学习到的元素理解新颖组合的能力——即组合概括能力（CG）。研究表明，MLLMs可利用CG理解未见过的医学图像，组合概括能力是多任务训练中的主要推广驱动力之一。此外，CG支持有限数据集，并可在分类和检测任务中实现跨任务概括，展现出其更广泛的推广潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在医学图像分析中具有强大的推广能力。</li>
<li>组合概括能力（CG）是理解新颖组合的关键，有助于模型理解未见过的医学图像。</li>
<li>多任务训练在医学图像分析中具有优势，不同任务间可以相互受益。</li>
<li>CG是多任务训练中的主要推广驱动力之一。</li>
<li>CG支持有限数据集，显示出其处理资源有限情况的潜力。</li>
<li>MLLMs能在分类和检测任务中实现跨任务的CG，表明其更广泛的推广潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20070">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e6f110d852c7ae28821b19e3b37b8cf3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c32f75d7a3d8438c02b46e472d029c47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3ab0435ba81162faf23c49a9fa49704.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74d57783de284d0983309f702cc9028c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa6a651e5764eed1d515b8cb9f82cb97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07616c23927a234140c2cc15add5727d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fc8cc766ff62d03972c01af82951fea.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SwiftEdit-Lightning-Fast-Text-Guided-Image-Editing-via-One-Step-Diffusion"><a href="#SwiftEdit-Lightning-Fast-Text-Guided-Image-Editing-via-One-Step-Diffusion" class="headerlink" title="SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step   Diffusion"></a>SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step   Diffusion</h2><p><strong>Authors:Trong-Tung Nguyen, Quang Nguyen, Khoi Nguyen, Anh Tran, Cuong Pham</strong></p>
<p>Recent advances in text-guided image editing enable users to perform image edits through simple text inputs, leveraging the extensive priors of multi-step diffusion-based text-to-image models. However, these methods often fall short of the speed demands required for real-world and on-device applications due to the costly multi-step inversion and sampling process involved. In response to this, we introduce SwiftEdit, a simple yet highly efficient editing tool that achieve instant text-guided image editing (in 0.23s). The advancement of SwiftEdit lies in its two novel contributions: a one-step inversion framework that enables one-step image reconstruction via inversion and a mask-guided editing technique with our proposed attention rescaling mechanism to perform localized image editing. Extensive experiments are provided to demonstrate the effectiveness and efficiency of SwiftEdit. In particular, SwiftEdit enables instant text-guided image editing, which is extremely faster than previous multi-step methods (at least 50 times faster) while maintain a competitive performance in editing results. Our project page is at: <a target="_blank" rel="noopener" href="https://swift-edit.github.io/">https://swift-edit.github.io/</a> </p>
<blockquote>
<p>近期文本引导的图像编辑技术取得了进展，使用户能够通过简单的文本输入进行图像编辑，利用多步扩散基础上的文本到图像模型的广泛先验知识。然而，由于涉及昂贵的多步反演和采样过程，这些方法往往难以满足现实世界和在线设备应用的速度要求。对此，我们推出了SwiftEdit，这是一款简单而高效编辑工具，可实现即时文本引导的图像编辑（在0.23秒内）。SwiftEdit的进展在于其两个新颖的贡献：一个一步反演框架，可通过反演实现一步图像重建，以及我们提出的带有注意力调整机制的遮罩引导编辑技术，以执行局部图像编辑。提供了广泛的实验来证明SwiftEdit的有效性和效率。特别是，SwiftEdit能够实现即时文本引导的图像编辑，这比以前的多步方法更快（至少快50倍），同时在编辑结果方面保持竞争力。我们的项目页面是：<a target="_blank" rel="noopener" href="https://swift-edit.github.io/">https://swift-edit.github.io/</a></p>
</blockquote>
<p><strong>简化说明</strong></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04301v4">PDF</a> 17 pages, 15 figures</p>
<p><strong>Summary</strong><br>     近期文本指导图像编辑技术通过简单的文本输入实现了图像编辑，利用多步扩散基础的文本到图像模型的先验知识。然而，这些方法常常不能满足现实应用和在线设备应用的速度需求，因为它们涉及成本高昂的多步反转和采样过程。为了解决这个问题，我们推出了SwiftEdit，这是一个简单而高效的编辑工具，实现了即时文本指导图像编辑（仅需0.23秒）。SwiftEdit的进步在于其两个新颖的贡献：一步反转框架，可通过反转实现一步图像重建，以及我们提出的注意力重新调整机制的掩膜指导编辑技术，以执行局部图像编辑。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本指导图像编辑技术允许通过简单的文本输入进行图像编辑。</li>
<li>现有方法因多步反转和采样过程而速度较慢，难以满足实际应用需求。</li>
<li>SwiftEdit是一个高效编辑工具，实现了即时文本指导图像编辑（0.23秒内）。</li>
<li>SwiftEdit具有两个新颖贡献：一步反转框架和注意力重新调整机制的掩膜指导编辑技术。</li>
<li>一步反转框架可通过反转实现快速图像重建。</li>
<li>掩膜指导编辑技术可以执行局部图像编辑。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04301">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b75d638f05b51d8f696e81953d8d25d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce452239af890c743ca59fe66d8a6155.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e02dca5d4cd8544337247225d711c58e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4841c4114519305f6d722723a5958d8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60ecc2b114be44b97bea6af33e725251.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Domain-Agnostic-Stroke-Lesion-Segmentation-Using-Physics-Constrained-Synthetic-Data"><a href="#Domain-Agnostic-Stroke-Lesion-Segmentation-Using-Physics-Constrained-Synthetic-Data" class="headerlink" title="Domain-Agnostic Stroke Lesion Segmentation Using Physics-Constrained   Synthetic Data"></a>Domain-Agnostic Stroke Lesion Segmentation Using Physics-Constrained   Synthetic Data</h2><p><strong>Authors:Liam Chalcroft, Jenny Crinion, Cathy J. Price, John Ashburner</strong></p>
<p>Segmenting stroke lesions in MRI is challenging due to diverse acquisition protocols that limit model generalisability. In this work, we introduce two physics-constrained approaches to generate synthetic quantitative MRI (qMRI) images that improve segmentation robustness across heterogeneous domains. Our first method, $\texttt{qATLAS}$, trains a neural network to estimate qMRI maps from standard MPRAGE images, enabling the simulation of varied MRI sequences with realistic tissue contrasts. The second method, $\texttt{qSynth}$, synthesises qMRI maps directly from tissue labels using label-conditioned Gaussian mixture models, ensuring physical plausibility. Extensive experiments on multiple out-of-domain datasets show that both methods outperform a baseline UNet, with $\texttt{qSynth}$ notably surpassing previous synthetic data approaches. These results highlight the promise of integrating MRI physics into synthetic data generation for robust, generalisable stroke lesion segmentation. Code is available at <a target="_blank" rel="noopener" href="https://github.com/liamchalcroft/qsynth">https://github.com/liamchalcroft/qsynth</a> </p>
<blockquote>
<p>在磁共振成像（MRI）中，对卒中病变进行分割是一项挑战，因为不同的采集协议限制了模型的通用性。在这项工作中，我们介绍两种物理约束方法，用于生成合成定量磁共振成像（qMRI）图像，以提高不同异质领域的分割稳健性。我们的第一种方法，<code>qATLAS</code>，训练神经网络从标准MPRAGE图像估计qMRI地图，能够模拟具有真实组织对比度的各种MRI序列。第二种方法，<code>qSynth</code>，直接从组织标签合成qMRI地图，采用标签条件下的高斯混合模型，确保物理合理性。在多个跨领域数据集上的大量实验表明，这两种方法都优于基线UNet，其中<code>qSynth</code>显著超越了以前的合成数据方法。这些结果突显了将MRI物理学纳入合成数据生成中的潜力，以实现稳健、通用的卒中病变分割。代码可在<a target="_blank" rel="noopener" href="https://github.com/liamchalcroft/qsynth%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/liamchalcroft/qsynth找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03318v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了两种基于物理约束的合成定量核磁共振成像（qMRI）图像生成方法，旨在提高不同异质领域中的分割稳健性。第一种方法qATLAS从标准MPRAGE图像估计qMRI图，模拟多种MRI序列，实现现实组织对比。第二种方法qSynth直接从组织标签合成qMRI图，确保物理合理性。实验表明，两种方法均优于基准UNet，特别是qSynth在合成数据方法上表现出明显优势。集成MRI物理的合成数据生成展现出稳健、通用的脑卒中病灶分割潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍MRI中分割脑卒中病灶的挑战，主要由于采集协议的多样性限制了模型的通用性。</li>
<li>提出两种物理约束的合成定量核磁共振成像（qMRI）图像生成方法以提高分割稳健性。</li>
<li>第一种方法qATLAS能够从标准MPRAGE图像估计qMRI图，模拟多种MRI序列并实现现实的组织对比。</li>
<li>第二种方法qSynth直接从组织标签合成qMRI图，确保合成的图像符合物理规律。</li>
<li>在多个跨域数据集上的实验表明，这两种方法均优于基准UNet模型，特别是qSynth表现突出。</li>
<li>集成MRI物理的合成数据生成有助于提高模型的稳健性和通用性，在脑卒中病灶分割方面具有潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03318">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-324b035746d85af60a419ee403012f72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe2c13c1843cb3030668132228568bad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d4b634061e49f660b98f11dec2c6a50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d3bb61c5f9d527de3bae81c46b3d3ea7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="FactCheXcker-Mitigating-Measurement-Hallucinations-in-Chest-X-ray-Report-Generation-Models"><a href="#FactCheXcker-Mitigating-Measurement-Hallucinations-in-Chest-X-ray-Report-Generation-Models" class="headerlink" title="FactCheXcker: Mitigating Measurement Hallucinations in Chest X-ray   Report Generation Models"></a>FactCheXcker: Mitigating Measurement Hallucinations in Chest X-ray   Report Generation Models</h2><p><strong>Authors:Alice Heiman, Xiaoman Zhang, Emma Chen, Sung Eun Kim, Pranav Rajpurkar</strong></p>
<p>Medical vision-language models often struggle with generating accurate quantitative measurements in radiology reports, leading to hallucinations that undermine clinical reliability. We introduce FactCheXcker, a modular framework that de-hallucinates radiology report measurements by leveraging an improved query-code-update paradigm. Specifically, FactCheXcker employs specialized modules and the code generation capabilities of large language models to solve measurement queries generated based on the original report. After extracting measurable findings, the results are incorporated into an updated report. We evaluate FactCheXcker on endotracheal tube placement, which accounts for an average of 78% of report measurements, using the MIMIC-CXR dataset and 11 medical report-generation models. Our results show that FactCheXcker significantly reduces hallucinations, improves measurement precision, and maintains the quality of the original reports. Specifically, FactCheXcker improves the performance of 10&#x2F;11 models and achieves an average improvement of 135.0% in reducing measurement hallucinations measured by mean absolute error. Code is available at <a target="_blank" rel="noopener" href="https://github.com/rajpurkarlab/FactCheXcker">https://github.com/rajpurkarlab/FactCheXcker</a>. </p>
<blockquote>
<p>医学影像语言模型在生成放射学报告中的精确定量测量方面经常遇到困难，这导致出现影响临床可靠性的幻觉。我们引入了FactCheXcker，这是一个模块化框架，它通过改进查询-编码-更新范式来消除放射学报告测量中的幻觉。具体来说，FactCheXcker利用专业模块和大型语言模型的编码生成能力来解决基于原始报告生成的测量查询问题。在提取可测量的发现后，将结果纳入更新的报告中。我们使用MIMIC-CXR数据集和11个医学报告生成模型，对气管导管置入的情况进行了评估，该情况占报告测量的平均77%。我们的结果表明，FactCheXcker显著减少了幻觉，提高了测量精度，并保持了原始报告的质量。具体来说，FactCheXcker提高了其中十个模型的性能，并通过平均绝对误差衡量的测量幻觉减少方面实现了平均135.0%的改进。代码可通过<a target="_blank" rel="noopener" href="https://github.com/rajpurkarlab/FactCheXcker%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/rajpurkarlab/FactCheXcker获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18672v3">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong><br>医学视觉语言模型在生成放射学报告中的定量测量时常常出现误差，导致产生误导临床的幻觉。我们推出FactCheXcker，一个模块化框架，通过改进查询-编码-更新范式来消除放射学报告中的测量幻觉。具体来说，FactCheXcker利用专业模块和大型语言模型的编码生成能力来解决基于原始报告生成的测量查询问题。在提取出可测量的发现后，将结果纳入更新的报告中。我们在气管插管放置任务上评估了FactCheXcker，该任务占报告测量的平均78%，使用MIMIC-CXR数据集和1 结医学报告生成模型。结果表明，FactCheXcker显著减少了幻觉，提高了测量精度，并保持了原始报告的质量。具体来说，FactCheXcker提高了10&#x2F;11模型的性能，在平均误差绝对值的测量下，减少测量幻觉的平均改善率为135.0%。代码可在<a target="_blank" rel="noopener" href="https://github.com/rajpurkarlab/FactCheXcker%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/rajpurkarlab/FactCheXcker找到。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学视觉语言模型在生成放射学报告中的定量测量时存在误差问题。</li>
<li>FactCheXcker是一个模块化框架，旨在消除放射学报告中的测量幻觉。</li>
<li>FactCheXcker通过改进查询-编码-更新范式来解决问题，利用专业模块和大型语言模型的编码生成能力。</li>
<li>FactCheXcker可以提取并纳入可测量的发现于更新的报告中。</li>
<li>在气管插管放置任务上，FactCheXcker显著减少了幻觉并提高了测量精度。</li>
<li>FactCheXcker对大多数医学报告生成模型都有提高表现的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18672">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dedd6b3e610b8b7b725564f503b84876.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f64b166c55bbebfa4b8248464178354.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac54e9a83c52e83ced04124ff8d3a617.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2abf15d2f0b309fc4eb8c875aa6974ac.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Leveraging-Complementary-Attention-maps-in-vision-transformers-for-OCT-image-analysis"><a href="#Leveraging-Complementary-Attention-maps-in-vision-transformers-for-OCT-image-analysis" class="headerlink" title="Leveraging Complementary Attention maps in vision transformers for OCT   image analysis"></a>Leveraging Complementary Attention maps in vision transformers for OCT   image analysis</h2><p><strong>Authors:Haz Sameen Shahgir, Tanjeem Azwad Zaman, Khondker Salman Sayeed, Md. Asif Haider, Sheikh Saifur Rahman Jony, M. Sohel Rahman</strong></p>
<p>Optical Coherence Tomography (OCT) scan yields all possible cross-section images of a retina for detecting biomarkers linked to optical defects. Due to the high volume of data generated, an automated and reliable biomarker detection pipeline is necessary as a primary screening stage.   We outline our new state-of-the-art pipeline for identifying biomarkers from OCT scans. In collaboration with trained ophthalmologists, we identify local and global structures in biomarkers. Through a comprehensive and systematic review of existing vision architectures, we evaluate different convolution and attention mechanisms for biomarker detection. We find that MaxViT, a hybrid vision transformer combining convolution layers with strided attention, is better suited for local feature detection, while EVA-02, a standard vision transformer leveraging pure attention and large-scale knowledge distillation, excels at capturing global features. We ensemble the predictions of both models to achieve first place in the IEEE Video and Image Processing Cup 2023 competition on OCT biomarker detection, achieving a patient-wise F1 score of 0.8527 in the final phase of the competition, scoring 3.8% higher than the next best solution. Finally, we used knowledge distillation to train a single MaxViT to outperform our ensemble at a fraction of the computation cost. </p>
<blockquote>
<p>光学相干断层扫描（OCT）扫描可以获取视网膜的所有可能的横截面图像，从而检测与光学缺陷相关的生物标志物。由于产生的大量数据，需要一个自动化和可靠的生物标志物检测流程作为初步筛查阶段。我们概述了我们最新的从OCT扫描中识别生物标志物的先进流程。与训练有素的眼科医生合作，我们识别生物标志物的局部和全局结构。通过对现有视觉架构的全面系统审查，我们评估了用于生物标志物检测的不同卷积和注意力机制。我们发现MaxViT，一种结合卷积层和跨步注意力的混合视觉转换器，更适合于局部特征检测，而EVA-02，一种利用纯注意力和大规模知识蒸馏的标准视觉转换器，擅长捕捉全局特征。我们结合了这两个模型的预测，在IEEE 2023年OCT生物标志物检测比赛中获得第一名，在比赛的最后阶段，患者级别的F1分数达到0.8527，比第二名高出3.8%。最后，我们使用知识蒸馏训练了一个单独的MaxViT，以超过我们组合的表现在很小的计算成本下。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14005v3">PDF</a> Accepted in 2025 IEEE International Conference on Image Processing</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型的生物标志物检测管道，该管道结合了光学相干层析扫描技术与先进的视觉技术，旨在从OCT扫描中自动识别和分类与光学缺陷相关的生物标志物。通过与眼科医生的合作，研究人员成功开发了一种集成的预测模型，该模型在IEEE视频和图像处理杯竞赛中取得了第一名，并在最终阶段实现了患者级别的F1分数为0.8527。此外，通过知识蒸馏技术，研究人员还训练了一个单一的MaxViT模型，该模型在计算能力消耗方面表现优于集成模型。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>OCT扫描能够生成视网膜的所有可能横截面图像，有助于检测与光学缺陷相关的生物标志物。</li>
<li>由于数据量巨大，需要自动化和可靠的生物标志物检测管道作为初步筛选阶段。</li>
<li>通过对现有视觉架构的全面系统审查，评估了不同的卷积和注意力机制在生物标志物检测中的应用。</li>
<li>MaxViT和EVA-02模型分别在局部特征检测和全局特征捕捉方面表现出优势。</li>
<li>集成MaxViT和EVA-02模型的预测在IEEE Video and Image Processing Cup 2023竞赛中取得第一名。</li>
<li>患者级别的F1分数达到0.8527，较其他最佳解决方案高出3.8%。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.14005">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-14881e67b41822c093cf7a6011ea0eda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95e380de7b23c209744811a59bcd5ae0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97f97935fa43d0c292d9f217300bcbde.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c8276fc046158b3831f41fb6d3f1f19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0e57cb78963f64f215d4e8ea7a224bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c958068ec93320796409d041fee6521.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3663f140ca164000b802e7fe6f9a230e.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-06-04  Zero-Shot Streaming Text to Speech Synthesis with Transducer and   Auto-Regressive Modeling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-32f95dfd51ea03a183c4ed355570c4e2.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-06-04  OpenUni A Simple Baseline for Unified Multimodal Understanding and   Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23154.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
