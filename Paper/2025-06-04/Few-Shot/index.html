<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-06-04  Data Whisperer Efficient Data Selection for Task-Specific LLM   Fine-Tuning via Few-Shot In-Context Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-93ec5c199a3111735cbe5faf45431d31.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    35 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-04-更新"><a href="#2025-06-04-更新" class="headerlink" title="2025-06-04 更新"></a>2025-06-04 更新</h1><h2 id="Data-Whisperer-Efficient-Data-Selection-for-Task-Specific-LLM-Fine-Tuning-via-Few-Shot-In-Context-Learning"><a href="#Data-Whisperer-Efficient-Data-Selection-for-Task-Specific-LLM-Fine-Tuning-via-Few-Shot-In-Context-Learning" class="headerlink" title="Data Whisperer: Efficient Data Selection for Task-Specific LLM   Fine-Tuning via Few-Shot In-Context Learning"></a>Data Whisperer: Efficient Data Selection for Task-Specific LLM   Fine-Tuning via Few-Shot In-Context Learning</h2><p><strong>Authors:Shaobo Wang, Xiangqi Jin, Ziming Wang, Jize Wang, Jiajun Zhang, Kaixin Li, Zichen Wen, Zhong Li, Conghui He, Xuming Hu, Linfeng Zhang</strong></p>
<p>Fine-tuning large language models (LLMs) on task-specific data is essential for their effective deployment. As dataset sizes grow, efficiently selecting optimal subsets for training becomes crucial to balancing performance and computational costs. Traditional data selection methods often require fine-tuning a scoring model on the target dataset, which is time-consuming and resource-intensive, or rely on heuristics that fail to fully leverage the model’s predictive capabilities. To address these challenges, we propose Data Whisperer, an efficient, training-free, attention-based method that leverages few-shot in-context learning with the model to be fine-tuned. Comprehensive evaluations were conducted on both raw and synthetic datasets across diverse tasks and models. Notably, Data Whisperer achieves superior performance compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just 10% of the data, and outperforms existing methods with a 3.1-point improvement and a 7.4$\times$ speedup. The code is available at <a target="_blank" rel="noopener" href="https://github.com/gszfwsb/Data-Whisperer">https://github.com/gszfwsb/Data-Whisperer</a>. </p>
<blockquote>
<p>对大型语言模型（LLM）进行针对特定任务的微调是有效部署它们的关键。随着数据集规模的扩大，高效地选择最佳子集进行训练对于平衡性能和计算成本变得至关重要。传统的数据选择方法通常需要针对目标数据集对评分模型进行微调，这既耗时又消耗资源，或者依赖于未能充分利用模型预测能力的启发式方法。为了解决这些挑战，我们提出了Data Whisperer，这是一种高效、无需训练、基于注意力的方法，利用少量上下文学习，对要微调的模型进行微调。我们在原始和合成数据集上进行了多样化的任务和模型的全面评估。值得注意的是，Data Whisperer在仅使用10%的数据的情况下，在Llama-3-8B-Instruct模型上实现了比GSM8K数据集更好的性能，并且在现有方法的基础上实现了3.1个点的改进和7.4倍的加速。代码可在<a target="_blank" rel="noopener" href="https://github.com/gszfwsb/Data-Whisperer%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/gszfwsb/Data-Whisperer获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12212v3">PDF</a> Accepted by ACL 2025 main, 18 pages, 8 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>大型语言模型的微调需要针对特定任务的数据集进行，而如何在日益增长的数据集中高效地选择优质子集成为了平衡性能和计算成本的关键。为解决这一问题，提出了Data Whisperer方法，它是一种高效、无需训练、基于注意力的方法，利用少量上下文数据进行学习，实现对模型的微调。其在不同任务和模型上的综合评估表现优异，使用仅10%的数据就能在Llama-3-8B-Instruct模型上超越使用完整GSM8K数据集的性能，且相较于现有方法速度提升了7.4倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型的微调依赖于任务特定数据集。</li>
<li>数据集的选择对于平衡性能和计算成本至关重要。</li>
<li>传统数据选择方法耗时且资源密集，或依赖不能完全发挥模型预测能力的启发式方法。</li>
<li>Data Whisperer是一种高效、无需训练的方法，基于注意力机制，利用少量上下文数据进行学习。</li>
<li>Data Whisperer在不同任务和模型上的综合评估表现优异。</li>
<li>使用仅10%的数据，Data Whisperer在Llama-3-8B-Instruct模型上的性能超越了使用完整GSM8K数据集的性能。</li>
<li>Data Whisperer相较于现有方法速度提升了7.4倍。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12212">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0d4e7e82acae08234a5970554a11de2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b45dc95a439d1d199d01bbda0ae96514.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d50cfb0cae1e151984528fcd02d7c52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd0fc55ae0634319a377e8c5f4453859.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AnomalyGFM-Graph-Foundation-Model-for-Zero-Few-shot-Anomaly-Detection"><a href="#AnomalyGFM-Graph-Foundation-Model-for-Zero-Few-shot-Anomaly-Detection" class="headerlink" title="AnomalyGFM: Graph Foundation Model for Zero&#x2F;Few-shot Anomaly Detection"></a>AnomalyGFM: Graph Foundation Model for Zero&#x2F;Few-shot Anomaly Detection</h2><p><strong>Authors:Hezhe Qiao, Chaoxi Niu, Ling Chen, Guansong Pang</strong></p>
<p>Graph anomaly detection (GAD) aims to identify abnormal nodes that differ from the majority of the nodes in a graph, which has been attracting significant attention in recent years. Existing generalist graph models have achieved remarkable success in different graph tasks but struggle to generalize to the GAD task. This limitation arises from their difficulty in learning generalized knowledge for capturing the inherently infrequent, irregular and heterogeneous abnormality patterns in graphs from different domains. To address this challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model that supports zero-shot inference and few-shot prompt tuning for GAD in diverse graph datasets. One key insight is that graph-agnostic representations for normal and abnormal classes are required to support effective zero&#x2F;few-shot GAD across different graphs. Motivated by this, AnomalyGFM is pre-trained to align data-independent, learnable normal and abnormal class prototypes with node representation residuals (i.e., representation deviation of a node from its neighbors). The residual features essentially project the node information into a unified feature space where we can effectively measure the abnormality of nodes from different graphs in a consistent way. This provides a driving force for the learning of graph-agnostic, discriminative prototypes for the normal and abnormal classes, which can be used to enable zero-shot GAD on new graphs, including very large-scale graphs. If there are few-shot labeled normal nodes available in the new graphs, AnomalyGFM can further support prompt tuning to leverage these nodes for better adaptation. Comprehensive experiments on 11 widely-used GAD datasets with real anomalies, demonstrate that AnomalyGFM significantly outperforms state-of-the-art competing methods under both zero- and few-shot GAD settings. </p>
<blockquote>
<p>图异常检测（GAD）旨在识别图中与大多数节点不同的异常节点，这在近年来引起了广泛关注。现有的通用图模型在不同的图任务中取得了显著的成功，但在GAD任务上却很难进行推广。这一局限性源于它们捕捉图中固有的不频繁、不规则和异质异常模式的困难，这些模式可能来源于不同的领域。为了解决这一挑战，我们提出了面向GAD的图基础模型AnomalyGFM，支持零样本推理和少量样本提示调整，用于处理各种图数据集中的GAD。一个重要的见解是需要针对正常和异常类别的图无关表示，以支持跨不同图的零&#x2F;少量样本GAD。受到这一认识的启发，AnomalyGFM通过预训练将独立于数据的可学习正常和异常类别原型与节点表示残差（即节点与其邻居的表示偏差）进行对齐。残差特征本质上将节点信息投影到一个统一的特征空间中，我们可以有效地衡量来自不同图的节点的异常性。这为学习正常和异常类别的图无关判别原型提供了动力，可用于在新图上实现零样本GAD，包括大规模图。如果新图中只有少量标记的正常节点可用，AnomalyGFM还可以进一步支持提示调整，以利用这些节点进行更好的适应。在具有真实异常的广泛使用的11个GAD数据集上的实验表明，AnomalyGFM在零样本和少量样本GAD设置下均显著优于最新的竞争方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09254v2">PDF</a> Accepted by KDD2025</p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了一种面向异常检测的图形基础模型AnomalyGFM，支持零样本推断和少量样本提示微调，用于处理不同图形数据集中的异常检测任务。该模型通过预训练对齐数据独立的正常和异常类别原型与节点表示残差，学习图无关的判别性表示，从而实现跨不同图形的有效零&#x2F;少量样本异常检测。同时，该模型还支持利用新图中少量标记的正常节点进行提示调整，以适应更好的性能。实验表明，AnomalyGFM在零样本和少量样本的异常检测设置下均显著优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Graph anomaly detection (GAD)旨在识别与图中大多数节点不同的异常节点，近年来备受关注。</li>
<li>现有通用图形模型在不同图形任务上取得了显著成功，但在GAD任务上难以推广。</li>
<li>AnomalyGFM是一个面向GAD的图形基础模型，支持零样本推断和少量样本提示微调。</li>
<li>模型需要图无关的关于正常和异常类别的表示，以支持跨不同图形的有效零&#x2F;少量样本GAD。</li>
<li>AnomalyGFM通过预训练对齐正常和异常类别原型与节点表示残差，学习图无关的判别性表示。</li>
<li>残差特征将节点信息投影到统一特征空间，从而有效地度量不同图形的节点异常性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09254">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a39ca23961a9fb0b000045625bea0357.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5750f21f2a0cdfba6339b190e5a5194a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca4d30f51c5b00e0ec580c688ddd45d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45287d8a2117e5656ad1e603a2366086.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44b3ebac02d2cb826798bdb7a8399c70.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Stepwise-Reasoning-Error-Disruption-Attack-of-LLMs"><a href="#Stepwise-Reasoning-Error-Disruption-Attack-of-LLMs" class="headerlink" title="Stepwise Reasoning Error Disruption Attack of LLMs"></a>Stepwise Reasoning Error Disruption Attack of LLMs</h2><p><strong>Authors:Jingyu Peng, Maolin Wang, Xiangyu Zhao, Kai Zhang, Wanyu Wang, Pengyue Jia, Qidong Liu, Ruocheng Guo, Qi Liu</strong></p>
<p>Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEED’s effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/SEED-Attack">https://github.com/Applied-Machine-Learning-Lab/SEED-Attack</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在复杂推理任务中取得了显著进展，但其在推理过程中的安全性和稳健性仍缺乏深入研究。现有针对LLM推理的攻击受到特定设置或隐蔽性不足的制约，限制了其可行性和通用性。为了解决这些挑战，我们提出了逐步推理误差干扰（SEED）攻击方法，该方法能微妙地将错误注入到先前的推理步骤中，误导模型产生错误的后续推理和最终答案。与以前的方法不同，SEED与零样本和少样本设置兼容，保持自然推理流程，确保在不修改指令的情况下秘密执行。在四个数据集上的四个不同模型的广泛实验证明了SEED的有效性，揭示了LLM对推理过程干扰的脆弱性。这些发现强调了在实践应用中确保LLM推理稳健性的重要性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/SEED-Attack%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Applied-Machine-Learning-Lab/SEED-Attack上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11934v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在复杂推理任务中取得了显著进展，但其安全性和推理过程的稳健性仍然未得到充分探索。针对LLM推理的攻击受限于特定场景或缺乏隐蔽性。为解决这些挑战，本文提出了逐步推理误差干扰（SEED）攻击方法，该方法通过微妙地注入错误来误导模型的后续推理和最终答案。SEED方法与零样本和小样本场景兼容，保持自然推理流程，确保在不修改指令的情况下秘密执行。在四个数据集和四个不同模型上的广泛实验证明了SEED的有效性，揭示了LLM对推理过程干扰的脆弱性。这强调了在实际应用中关注LLM推理稳健性的必要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在复杂推理任务中表现出色，但安全性和稳健性亟待探索。</li>
<li>现有对LLM推理的攻击方法存在局限性，如特定场景依赖和缺乏隐蔽性。</li>
<li>提出了Stepwise rEasoning Error Disruption (SEED)攻击方法，通过微妙地注入错误来误导模型的推理和答案。</li>
<li>SEED方法适用于零样本和小样本场景，保持自然推理流程，确保隐蔽执行。</li>
<li>广泛实验证明SEED的有效性，揭示了LLM对推理过程干扰的脆弱性。</li>
<li>需要关注LLM推理的稳健性，以确保实际应用中的安全性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11934">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-65f3e0e6b8b5e8b018d5891aef727612.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00ca2d3282bea65c458df2505f4b40e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87b1c6209c61a9ecdf478d89801d4273.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c845210b4c456a596c9b56ccbedf20ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb5963186429b3a8418db47170b4451d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Can-Graph-Neural-Networks-Learn-Language-with-Extremely-Weak-Text-Supervision"><a href="#Can-Graph-Neural-Networks-Learn-Language-with-Extremely-Weak-Text-Supervision" class="headerlink" title="Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?"></a>Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?</h2><p><strong>Authors:Zihao Li, Lecheng Zheng, Bowen Jin, Dongqi Fu, Baoyu Jing, Yikun Ban, Jingrui He, Jiawei Han</strong></p>
<p>While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we propose a multi-modal prompt learning paradigm to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. We demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Violet24K/Morpher">https://github.com/Violet24K/Morpher</a>. </p>
<blockquote>
<p>尽管在使用对比语言图像预训练（CLIP）在互联网规模的图像文本对上构建视觉模型方面取得了巨大成功，但使用CLIP管道构建可迁移的图神经网络（GNN）仍然具有挑战性，这是由于标记数据缺乏、文本监督不足、下游任务级别不同以及领域之间的概念差距造成的。针对这些问题，我们在工作中提出了一种多模式提示学习范式，以有效地适应预训练的GNN进行下游任务和数据的处理，仅使用少量语义标记样本，每个样本都有极其微弱的文本监督。我们的新范式通过将图直接在大型语言模型（LLM）的相同空间中嵌入，同时学习图提示和文本提示。我们在小样例、多任务级别和跨域设置中展示了范式的卓越性能。此外，我们构建了第一个CLIP风格的零样本分类原型，可以推广到未见类别，具有极其微弱的文本监督能力。代码可在<a target="_blank" rel="noopener" href="https://github.com/Violet24K/Morpher%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Violet24K/Morpher上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08174v3">PDF</a> ACL 2025 Main Conference, 27 pages</p>
<p><strong>Summary</strong></p>
<p>预训练Graph Neural Networks（GNNs）使用CLIP管道面临挑战，如缺乏标签数据、文本监督不足、下游任务级别不同以及领域间的概念差距等。本研究提出了一种多模式提示学习范式，旨在仅使用少量具有极弱文本监督的语义标记样本，有效适应预训练GNN到下游任务和数据的挑战。该范式通过将图直接嵌入与大型语言模型（LLMs）相同的空间，同时学习图形提示和文本提示，表现出在少样本、多任务级别和跨域设置中的卓越性能。此外，本研究构建了第一个具有CLIP风格的零样本分类原型，能够利用极弱的文本监督将GNNs推广到未见类别中。代码已发布在<a target="_blank" rel="noopener" href="https://github.com/Violet24K/Morpher%E4%B8%8A%E3%80%82">https://github.com/Violet24K/Morpher上。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>尽管CLIP在构建视觉模型方面取得了巨大成功，但使用CLIP管道构建可转移的Graph Neural Networks (GNNs)具有挑战性，主要原因是缺乏标签数据和文本监督，任务级别的差异以及领域间的概念差距。</li>
<li>研究提出了一种多模式提示学习范式来适应预训练GNN到下游任务和数据的挑战，使用仅少量的语义标记样本和极弱的文本监督。</li>
<li>这种新范式通过将图直接嵌入与大型语言模型（LLMs）相同的空间，同时学习图形提示和文本提示，展现了优越的性能。</li>
<li>研究在少样本、多任务级别和跨域设置下验证了范式的性能优势。</li>
<li>研究构建了第一个具有CLIP风格的零样本分类原型，能够实现未见类别中的GNNs推广，仅依赖于极弱的文本监督。</li>
<li>模型的代码已经发布在Violet24K的Morpher项目上，供公众访问和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.08174">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f4df9489bb1cece1cb5f4c71fa51694d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e68b8e72a5936cd0071cb0864243871.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-610d50eeed67b149f8b1b7de9c346feb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2e2c8e120affd8e37978f26658c59e8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PromptRefine-Enhancing-Few-Shot-Performance-on-Low-Resource-Indic-Languages-with-Example-Selection-from-Related-Example-Banks"><a href="#PromptRefine-Enhancing-Few-Shot-Performance-on-Low-Resource-Indic-Languages-with-Example-Selection-from-Related-Example-Banks" class="headerlink" title="PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic   Languages with Example Selection from Related Example Banks"></a>PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic   Languages with Example Selection from Related Example Banks</h2><p><strong>Authors:Soumya Suvra Ghosal, Soumyabrata Pal, Koyel Mukherjee, Dinesh Manocha</strong></p>
<p>Large Language Models (LLMs) have recently demonstrated impressive few-shot learning capabilities through in-context learning (ICL). However, ICL performance is highly dependent on the choice of few-shot demonstrations, making the selection of the most optimal examples a persistent research challenge. This issue is further amplified in low-resource Indic languages, where the scarcity of ground-truth data complicates the selection process. In this work, we propose PromptRefine, a novel Alternating Minimization approach for example selection that improves ICL performance on low-resource Indic languages. PromptRefine leverages auxiliary example banks from related high-resource Indic languages and employs multi-task learning techniques to align language-specific retrievers, enabling effective cross-language retrieval. Additionally, we incorporate diversity in the selected examples to enhance generalization and reduce bias. Through comprehensive evaluations on four text generation tasks – Cross-Lingual Question Answering, Multilingual Question Answering, Machine Translation, and Cross-Lingual Summarization using state-of-the-art LLMs such as LLAMA-3.1-8B, LLAMA-2-7B, Qwen-2-7B, and Qwen-2.5-7B, we demonstrate that PromptRefine significantly outperforms existing frameworks for retrieving examples. </p>
<blockquote>
<p>大型语言模型（LLM）最近通过上下文学习（ICL）展示了令人印象深刻的少量学习功能。然而，ICL的性能高度依赖于少数示范的选择，这使得选择最优质的例子成为一个持续的研究挑战。在资源贫乏的印度语言（Indic languages）中，这个问题进一步放大，因为缺乏真实数据的选择过程变得复杂。在这项工作中，我们提出了PromptRefine，这是一种用于示例选择的新型交替最小化方法，能够改善资源贫乏的印度语言上的ICL性能。PromptRefine利用相关的高资源印度语言的辅助示例库，并采用多任务学习技术对齐特定语言的检索器，实现有效的跨语言检索。此外，我们在选择的示例中融入了多样性，以增强泛化能力并减少偏见。通过四项文本生成任务的全面评估——跨语言问答、多语言问答、机器翻译和跨语言摘要，使用最前沿的大型语言模型如LLAMA-3.1-8B、LLAMA-2-7B、Qwen-2-7B和Qwen-2.5-7B等，我们证明了PromptRefine在检索示例方面显著优于现有框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05710v2">PDF</a> Accepted at NAACL 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）通过上下文学习（ICL）展现出令人印象深刻的少样本学习能力。然而，ICL性能高度依赖于少数样本示范的选择，使得选择最优示例成为持续的研究挑战。特别是在资源贫乏的印度语言环境中，由于缺乏真实数据，选择过程更加复杂。本研究提出一种名为PromptRefine的新型交替最小化方法，用于示例选择，可提高低资源印度语言环境下的ICL性能。PromptRefine利用来自相关高资源印度语言的辅助示例库，采用多任务学习技术，实现语言特定检索器的对齐，实现有效的跨语言检索。此外，通过增加所选示例的多样性来提高泛化能力和减少偏见。通过全面的评估，在四项文本生成任务上使用前沿的LLM，如LLAMA-3.1-8B、LLAMA-2-7B、Qwen-2-7B和Qwen-2.5-7B等模型，证明PromptRefine在检索示例方面显著优于现有框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型展现出强大的少样本学习能力，但示例选择是关键挑战。</li>
<li>在资源有限的印度语言中，示例选择过程更加复杂。</li>
<li>PromptRefine是一种新型的示例选择方法，通过交替最小化来提升低资源印度语言环境下的上下文学习性能。</li>
<li>PromptRefine利用辅助示例库和多任务学习技术，实现对语言特定检索器的有效跨语言检索。</li>
<li>通过增加所选示例的多样性来提高模型的泛化能力和减少偏见。</li>
<li>在多项文本生成任务上，PromptRefine显著优于现有框架。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05710">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3abc881f58cb45a8847a029d49d7700f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95756d1155eeb8effb115b7d8a3db523.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a24dbd9507325756ce88281a7ed99183.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d0a58e26a07145f0acaa0db6e131b10.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping"><a href="#MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping" class="headerlink" title="MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping"></a>MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping</h2><p><strong>Authors:Amirreza Fateh, Mohammad Reza Mohammadi, Mohammad Reza Jahed Motlagh</strong></p>
<p>Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the Transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve competitive results on benchmark datasets such as PASCAL-5^i and COCO-20^i in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies. </p>
<blockquote>
<p>少样本语义分割（Few-shot Semantic Segmentation）旨在解决仅使用少量标注样本对查询图像中的对象进行分割的挑战。然而，许多先前的前沿方法要么需要放弃复杂的局部语义特征，要么面临高计算复杂度的问题。为了解决这些挑战，我们提出了一种基于Transformer架构的少样本语义分割框架。我们的方法引入了空间变换解码器和上下文掩码生成模块，以提高支持图像和查询图像之间的关系理解。此外，我们引入了多尺度解码器，以分层的方式融入不同分辨率的特征来优化分割掩码。同时，我们的方法整合了中间编码器阶段的全局特征，以提高上下文理解，同时保持轻量级结构以降低复杂度。性能和效率之间的这种平衡使我们的方法在PASCAL-5^i和COCO-20^i等基准数据集上，无论是1次拍摄还是5次拍摄的环境中都能取得有竞争力的结果。值得注意的是，我们的模型仅有150万个参数，展现了有竞争力的性能，同时克服了现有方法的局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11316v3">PDF</a> </p>
<p><strong>摘要</strong><br>    基于Transformer架构的少数样本语义分割框架，通过引入空间变换解码器和上下文掩模生成模块，提高支持图像和查询图像之间的关系理解。采用多尺度解码器，以层次方式融合不同分辨率的特征，优化分割掩模。整合中间编码阶段的全局特征，提高上下文理解，同时保持轻量级结构，降低复杂性。在PASCAL-5^i和COCO-20^i等基准数据集上，1次和5次射击设置中表现优异。模型仅150万参数，具有竞争力，克服了现有方法的局限性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>该文本介绍了基于Transformer架构的少数样本语义分割框架。</li>
<li>引入空间变换解码器和上下文掩模生成模块以提高关系理解。</li>
<li>使用多尺度解码器融合不同分辨率的特征来优化分割掩模。</li>
<li>结合中间编码阶段的全球特征以提高上下文理解。</li>
<li>模型结构保持轻量级，以降低复杂性。</li>
<li>在PASCAL-5^i和COCO-20^i等基准数据集中具有竞争力。</li>
<li>模型具有较少的参数（仅150万），性能出色，克服了现有方法的局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11316">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a28d10d4e1e67b7c935fbf7a8b9ca0ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6d94b05da2662a71afb78c8b6fec8f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1232d75de621baac9ac76c250665a86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43b280a5d0cd66845ec22d0186712420.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="BMIKE-53-Investigating-Cross-Lingual-Knowledge-Editing-with-In-Context-Learning"><a href="#BMIKE-53-Investigating-Cross-Lingual-Knowledge-Editing-with-In-Context-Learning" class="headerlink" title="BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context   Learning"></a>BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context   Learning</h2><p><strong>Authors:Ercong Nie, Bo Shao, Zifeng Ding, Mingyang Wang, Helmut Schmid, Hinrich Schütze</strong></p>
<p>This paper introduces BMIKE-53, a comprehensive benchmark for cross-lingual in-context knowledge editing (IKE) across 53 languages, unifying three knowledge editing (KE) datasets: zsRE, CounterFact, and WikiFactDiff. Cross-lingual KE, which requires knowledge edited in one language to generalize across others while preserving unrelated knowledge, remains underexplored. To address this gap, we systematically evaluate IKE under zero-shot, one-shot, and few-shot setups, incorporating tailored metric-specific demonstrations. Our findings reveal that model scale and demonstration alignment critically govern cross-lingual IKE efficacy, with larger models and tailored demonstrations significantly improving performance. Linguistic properties, particularly script type, strongly influence performance variation across languages, with non-Latin languages underperforming due to issues like language confusion. Code and data are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ercong21/MultiKnow/">https://github.com/ercong21/MultiKnow/</a>. </p>
<blockquote>
<p>本文介绍了BMIKE-53，这是一个针对53种语言的跨语境知识编辑（IKE）的全面基准测试，它统一了三个知识编辑（KE）数据集：zsRE、CounterFact和WikiFactDiff。跨语言知识编辑（IKE）要求在一种语言中对知识进行编辑，并能在其他语言中推广，同时保留无关知识，这一领域的研究仍然不足。为了弥补这一空白，我们在零样本、一样本和少样本设置下系统地评估IKE，并纳入定制的特定指标演示。我们的研究发现，模型的规模和演示对齐对跨语言IKE的有效性至关重要，较大的模型和定制的演示能显著提高性能。语言特性，尤其是文字类型，对跨语言性能变化有很大影响，非拉丁语系的表现不佳主要是由于语言混淆等问题导致的。相关代码和数据已公开在：<a target="_blank" rel="noopener" href="https://github.com/ercong2sc=/%EF%BC%88%E5%BC%80%E6%BA%90%E5%B9%B3%E5%8F%B0%E7%9A%84%E8%B4%A6%E5%8F%B7%E4%B8%8D%E5%8F%AF%E7%94%A8%EF%BC%89%E3%80%82">https://github.com/ercong2sc=/（开源平台的账号不可用）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17764v3">PDF</a> Accepted to ACL 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了BMIKE-53，这是一个涵盖53种语言的跨语言上下文知识编辑（IKE）综合基准测试，它统一了三个知识编辑（KE）数据集：zsRE、CounterFact和WikiFactDiff。研究系统地评估了跨语言IKE在零样本、一示例和少样本设置下的表现，发现模型规模和演示与任务的匹配度对跨语言IKE的效果至关重要。此外，研究结果还表明语言特性对性能有显著影响，特别是非拉丁语言的表现相对较差。数据和代码已公开于GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BMIKE-53是一个跨语言上下文知识编辑的基准测试，涵盖了53种语言，统一了三个知识编辑数据集。</li>
<li>评估了跨语言IKE在零样本、一示例和少样本设置下的表现。</li>
<li>模型规模和演示与任务的匹配度是影响跨语言IKE效果的关键因素。</li>
<li>非拉丁语言在跨语言IKE中的表现相对较差。</li>
<li>语言特性（如脚本类型）对跨语言IKE性能有显著影响。</li>
<li>数据和代码已公开在GitHub上供公众使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.17764">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c1a02a9d96372d0211a8828ad516dd6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93ec5c199a3111735cbe5faf45431d31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4064a6f312fd03bd951cd7686c17e077.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f912800c0bd7e2ea52bd2aefc613c10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b83596af779e99edfe2986435a260ad0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bd3fb4a4f5c1a1329ba3cfb5b8a715c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6406e372a9d60bac0b41c5aec839f5d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RAEmoLLM-Retrieval-Augmented-LLMs-for-Cross-Domain-Misinformation-Detection-Using-In-Context-Learning-Based-on-Emotional-Information"><a href="#RAEmoLLM-Retrieval-Augmented-LLMs-for-Cross-Domain-Misinformation-Detection-Using-In-Context-Learning-Based-on-Emotional-Information" class="headerlink" title="RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation   Detection Using In-Context Learning Based on Emotional Information"></a>RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation   Detection Using In-Context Learning Based on Emotional Information</h2><p><strong>Authors:Zhiwei Liu, Kailai Yang, Qianqian Xie, Christine de Kock, Sophia Ananiadou, Eduard Hovy</strong></p>
<p>Misinformation is prevalent in various fields such as education, politics, health, etc., causing significant harm to society. However, current methods for cross-domain misinformation detection rely on effort- and resource-intensive fine-tuning and complex model structures. With the outstanding performance of LLMs, many studies have employed them for misinformation detection. Unfortunately, they focus on in-domain tasks and do not incorporate significant sentiment and emotion features (which we jointly call {\em affect}). In this paper, we propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework to address cross-domain misinformation detection using in-context learning based on affective information. RAEmoLLM includes three modules. (1) In the index construction module, we apply an emotional LLM to obtain affective embeddings from all domains to construct a retrieval database. (2) The retrieval module uses the database to recommend top K examples (text-label pairs) from source domain data for target domain contents. (3) These examples are adopted as few-shot demonstrations for the inference module to process the target domain content. The RAEmoLLM can effectively enhance the general performance of LLMs in cross-domain misinformation detection tasks through affect-based retrieval, without fine-tuning. We evaluate our framework on three misinformation benchmarks. Results show that RAEmoLLM achieves significant improvements compared to the other few-shot methods on three datasets, with the highest increases of 15.64%, 31.18%, and 15.73% respectively. This project is available at <a target="_blank" rel="noopener" href="https://github.com/lzw108/RAEmoLLM">https://github.com/lzw108/RAEmoLLM</a>. </p>
<blockquote>
<p>误信息在各个领域都很普遍，如教育、政治、卫生等，给社会造成重大危害。然而，现有的跨域误检测方法依赖于耗费努力和资源的精细调整及复杂的模型结构。随着大型语言模型（LLMs）的出色表现，许多研究已将其用于误检测。然而，它们主要关注领域内的任务，并没有融入重要的情感和情绪特征（我们称之为“情感”）。在本文中，我们提出了RAEmoLLM，这是第一个基于情感信息的上下文学习增强型（RAG）LLMs框架，用于解决跨域误检测问题。RAEmoLLM包括三个模块。（1）在索引构建模块中，我们应用情感LLM来获得所有领域的情感嵌入来构建检索数据库。（2）检索模块使用数据库为目标域内容推荐前K个示例（文本标签对）。（3）这些示例被用作推断模块的少数演示来处理目标域内容。通过基于情感的检索，RAEmoLLM能有效提升LLMs在跨域误检测任务中的总体性能，无需精细调整。我们在三个误检测基准测试上对框架进行了评估。结果表明，与其他少数方法相比，RAEmoLLM在这三个数据集上的改进效果显著，最高分别提高了15.64%、31.18%和15.73%。此项目可通过<a target="_blank" rel="noopener" href="https://github.com/lzw108/RAEmoLLM%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/lzw108/RAEmoLLM访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11093v2">PDF</a> Accepted by ACL 2025 (Main)</p>
<p><strong>摘要</strong></p>
<p>本文提出一种名为RAEmoLLM的跨域虚假信息检测框架，它利用情感嵌入增强大型语言模型（LLMs）的性能。该框架包括三个模块：构建情感嵌入索引、基于情感的检索，以及利用检索结果作为示范进行推断。RAEmoLLM能有效提高LLMs在跨域虚假信息检测任务中的性能，且无需微调。在三个虚假信息检测数据集上的实验结果表明，与现有方法相比，RAEmoLLM取得了显著的提升。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>跨域虚假信息检测是一个重要但具有挑战性的问题，需要新的解决方案。</li>
<li>当前的方法依赖于资源密集型的精细调整和复杂的模型结构。</li>
<li>LLMs在虚假信息检测方面表现出色，但缺乏跨域和融入情感特征的能力。</li>
<li>RAEmoLLM是首个利用情感信息增强LLMs的跨域虚假信息检测框架。</li>
<li>RAEmoLLM包括情感嵌入索引构建、基于情感的检索和示范推断三个模块。</li>
<li>RAEmoLLM在三个数据集上的实验结果表明其性能显著提升。</li>
<li>RAEmoLLM框架可用于多种领域，具有广泛的应用前景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.11093">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-badbe6951d5ec47af093d6af23bfee86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b5e631e1713eaddda9667980cd5a4ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1eafebc4cebf13e243f766c3c151fd42.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef8abac4ad22db776c9f2c00b0959e61.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Multi-Prompting-Decoder-Helps-Better-Language-Understanding"><a href="#Multi-Prompting-Decoder-Helps-Better-Language-Understanding" class="headerlink" title="Multi-Prompting Decoder Helps Better Language Understanding"></a>Multi-Prompting Decoder Helps Better Language Understanding</h2><p><strong>Authors:Zifeng Cheng, Zhaoling Chen, Zhiwei Jiang, Yafeng Yin, Cong Wang, Shiping Ge, Qing Gu</strong></p>
<p>Recent Pre-trained Language Models (PLMs) usually only provide users with the inference APIs, namely the emerging Model-as-a-Service (MaaS) setting. To adapt MaaS PLMs to downstream tasks without accessing their parameters and gradients, some existing methods focus on the output-side adaptation of PLMs, viewing the PLM as an encoder and then optimizing a task-specific decoder for decoding the output hidden states and class scores of the PLM. Despite the effectiveness of these methods, they only use a single prompt to query PLMs for decoding, leading to a heavy reliance on the quality of the adopted prompt. In this paper, we propose a simple yet effective Multi-Prompting Decoder (MPD) framework for MaaS adaptation. The core idea is to query PLMs with multiple different prompts for each sample, thereby obtaining multiple output hidden states and class scores for subsequent decoding. Such multi-prompting decoding paradigm can simultaneously mitigate reliance on the quality of a single prompt, alleviate the issue of data scarcity under the few-shot setting, and provide richer knowledge extracted from PLMs. Specifically, we propose two decoding strategies: multi-prompting decoding with optimal transport for hidden states and calibrated decoding for class scores. Extensive experiments demonstrate that our method achieves new state-of-the-art results on multiple natural language understanding datasets under the few-shot setting. </p>
<blockquote>
<p>最近的预训练语言模型（PLMs）通常只为用户提供推理API，即新兴的模型即服务（MaaS）设置。为了适应不需要访问其参数和梯度的下游任务的MaaS PLMs，一些现有方法专注于PLMs的输出端适应，将PLM视为编码器，然后优化针对解码PLM的输出隐藏状态和类别分数的特定任务解码器。尽管这些方法很有效，但它们仅使用单个提示来查询PLMs进行解码，这导致对采用提示的质量的严重依赖。在本文中，我们提出了一个简单的但有效的多提示解码（MPD）框架，用于MaaS适应。核心理念是为每个样本使用多个不同的提示来查询PLMs，从而获得多个输出隐藏状态和类别分数，以供后续解码。这种多提示解码范式可以同时减轻对单个提示质量的依赖，缓解少样本设置下的数据稀缺问题，并提供从PLMs中提取的更丰富的知识。具体来说，我们提出了两种解码策略：用于隐藏状态的最优传输多提示解码和用于类别分数的校准解码。大量实验表明，我们的方法在多个自然语言理解数据集上的少样本设置下达到了新的最先进的成果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06279v2">PDF</a> Findings of ACL 2025</p>
<p><strong>Summary</strong></p>
<p>预训练语言模型（PLM）通常采用模型即服务（MaaS）的方式提供推理接口。现有方法主要关注PLM输出端的适配，通过优化特定任务的解码器来解码PLM的输出隐藏状态和类别分数。然而，这些方法依赖于单一提示的质量。本文提出一种简单有效的多提示解码（MPD）框架，通过为每个样本使用多个不同提示来查询PLM，获得多个输出隐藏状态和类别分数进行后续解码。多提示解码范式能同时减轻对单一提示的依赖，缓解少样本设置下的数据稀缺问题，并提供更丰富地从PLM中提取的知识。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练语言模型（PLM）通常采用模型即服务（MaaS）方式，仅提供推理接口。</li>
<li>现有方法主要关注PLM输出端的适配，使用单一提示进行查询。</li>
<li>多提示解码（MPD）框架提出，通过为每个样本使用多个提示来查询PLM。</li>
<li>MPD框架通过获得多个输出隐藏状态和类别分数进行后续解码。</li>
<li>多提示解码能减轻对单一提示的依赖，缓解少样本设置下的数据稀缺问题。</li>
<li>MPD框架提供从PLM更丰富地提取的知识。</li>
<li>提出两种解码策略：基于最优传输的隐藏状态多提示解码和校准类别分数解码。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06279">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-10c39cd6aa16286fceb56863da20a14f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0d53d6b494572715f32ae577519ad73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28b6923886eb6c9ee080ec52b2d1791f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce5712a3dc294aa6da8a77fc60bf75bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9796e1e33bfae42360e8cbabbca470a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-02ea006e8e551367dc0c39d167de2e7f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-b37a8cf7b390bb4a05303685151b6bfd.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-06-04  Brain network science modelling of sparse neural networks enables   Transformers and LLMs to perform as fully connected
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f8188d87da012f86325b12f57afd9ec1.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-06-04  AutoChemSchematic AI A Closed-Loop, Physics-Aware Agentic Framework for   Auto-Generating Chemical Process and Instrumentation Diagrams
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
