<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-04  Data Whisperer Efficient Data Selection for Task-Specific LLM   Fine-Tuning via Few-Shot In-Context Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-93ec5c199a3111735cbe5faf45431d31.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    35 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-04-æ›´æ–°"><a href="#2025-06-04-æ›´æ–°" class="headerlink" title="2025-06-04 æ›´æ–°"></a>2025-06-04 æ›´æ–°</h1><h2 id="Data-Whisperer-Efficient-Data-Selection-for-Task-Specific-LLM-Fine-Tuning-via-Few-Shot-In-Context-Learning"><a href="#Data-Whisperer-Efficient-Data-Selection-for-Task-Specific-LLM-Fine-Tuning-via-Few-Shot-In-Context-Learning" class="headerlink" title="Data Whisperer: Efficient Data Selection for Task-Specific LLM   Fine-Tuning via Few-Shot In-Context Learning"></a>Data Whisperer: Efficient Data Selection for Task-Specific LLM   Fine-Tuning via Few-Shot In-Context Learning</h2><p><strong>Authors:Shaobo Wang, Xiangqi Jin, Ziming Wang, Jize Wang, Jiajun Zhang, Kaixin Li, Zichen Wen, Zhong Li, Conghui He, Xuming Hu, Linfeng Zhang</strong></p>
<p>Fine-tuning large language models (LLMs) on task-specific data is essential for their effective deployment. As dataset sizes grow, efficiently selecting optimal subsets for training becomes crucial to balancing performance and computational costs. Traditional data selection methods often require fine-tuning a scoring model on the target dataset, which is time-consuming and resource-intensive, or rely on heuristics that fail to fully leverage the modelâ€™s predictive capabilities. To address these challenges, we propose Data Whisperer, an efficient, training-free, attention-based method that leverages few-shot in-context learning with the model to be fine-tuned. Comprehensive evaluations were conducted on both raw and synthetic datasets across diverse tasks and models. Notably, Data Whisperer achieves superior performance compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just 10% of the data, and outperforms existing methods with a 3.1-point improvement and a 7.4$\times$ speedup. The code is available at <a target="_blank" rel="noopener" href="https://github.com/gszfwsb/Data-Whisperer">https://github.com/gszfwsb/Data-Whisperer</a>. </p>
<blockquote>
<p>å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒæ˜¯æœ‰æ•ˆéƒ¨ç½²å®ƒä»¬çš„å…³é”®ã€‚éšç€æ•°æ®é›†è§„æ¨¡çš„æ‰©å¤§ï¼Œé«˜æ•ˆåœ°é€‰æ‹©æœ€ä½³å­é›†è¿›è¡Œè®­ç»ƒå¯¹äºå¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬å˜å¾—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„æ•°æ®é€‰æ‹©æ–¹æ³•é€šå¸¸éœ€è¦é’ˆå¯¹ç›®æ ‡æ•°æ®é›†å¯¹è¯„åˆ†æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¿™æ—¢è€—æ—¶åˆæ¶ˆè€—èµ„æºï¼Œæˆ–è€…ä¾èµ–äºæœªèƒ½å……åˆ†åˆ©ç”¨æ¨¡å‹é¢„æµ‹èƒ½åŠ›çš„å¯å‘å¼æ–¹æ³•ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Data Whispererï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆã€æ— éœ€è®­ç»ƒã€åŸºäºæ³¨æ„åŠ›çš„æ–¹æ³•ï¼Œåˆ©ç”¨å°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå¯¹è¦å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬åœ¨åŸå§‹å’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤šæ ·åŒ–çš„ä»»åŠ¡å’Œæ¨¡å‹çš„å…¨é¢è¯„ä¼°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒData Whispereråœ¨ä»…ä½¿ç”¨10%çš„æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåœ¨Llama-3-8B-Instructæ¨¡å‹ä¸Šå®ç°äº†æ¯”GSM8Kæ•°æ®é›†æ›´å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ç°æœ‰æ–¹æ³•çš„åŸºç¡€ä¸Šå®ç°äº†3.1ä¸ªç‚¹çš„æ”¹è¿›å’Œ7.4å€çš„åŠ é€Ÿã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gszfwsb/Data-Whisperer%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/gszfwsb/Data-Whispererè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12212v3">PDF</a> Accepted by ACL 2025 main, 18 pages, 8 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒéœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ•°æ®é›†è¿›è¡Œï¼Œè€Œå¦‚ä½•åœ¨æ—¥ç›Šå¢é•¿çš„æ•°æ®é›†ä¸­é«˜æ•ˆåœ°é€‰æ‹©ä¼˜è´¨å­é›†æˆä¸ºäº†å¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬çš„å…³é”®ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†Data Whispereræ–¹æ³•ï¼Œå®ƒæ˜¯ä¸€ç§é«˜æ•ˆã€æ— éœ€è®­ç»ƒã€åŸºäºæ³¨æ„åŠ›çš„æ–¹æ³•ï¼Œåˆ©ç”¨å°‘é‡ä¸Šä¸‹æ–‡æ•°æ®è¿›è¡Œå­¦ä¹ ï¼Œå®ç°å¯¹æ¨¡å‹çš„å¾®è°ƒã€‚å…¶åœ¨ä¸åŒä»»åŠ¡å’Œæ¨¡å‹ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨ç°ä¼˜å¼‚ï¼Œä½¿ç”¨ä»…10%çš„æ•°æ®å°±èƒ½åœ¨Llama-3-8B-Instructæ¨¡å‹ä¸Šè¶…è¶Šä½¿ç”¨å®Œæ•´GSM8Kæ•°æ®é›†çš„æ€§èƒ½ï¼Œä¸”ç›¸è¾ƒäºç°æœ‰æ–¹æ³•é€Ÿåº¦æå‡äº†7.4å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒä¾èµ–äºä»»åŠ¡ç‰¹å®šæ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†çš„é€‰æ‹©å¯¹äºå¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ•°æ®é€‰æ‹©æ–¹æ³•è€—æ—¶ä¸”èµ„æºå¯†é›†ï¼Œæˆ–ä¾èµ–ä¸èƒ½å®Œå…¨å‘æŒ¥æ¨¡å‹é¢„æµ‹èƒ½åŠ›çš„å¯å‘å¼æ–¹æ³•ã€‚</li>
<li>Data Whispereræ˜¯ä¸€ç§é«˜æ•ˆã€æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ©ç”¨å°‘é‡ä¸Šä¸‹æ–‡æ•°æ®è¿›è¡Œå­¦ä¹ ã€‚</li>
<li>Data Whispereråœ¨ä¸åŒä»»åŠ¡å’Œæ¨¡å‹ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä½¿ç”¨ä»…10%çš„æ•°æ®ï¼ŒData Whispereråœ¨Llama-3-8B-Instructæ¨¡å‹ä¸Šçš„æ€§èƒ½è¶…è¶Šäº†ä½¿ç”¨å®Œæ•´GSM8Kæ•°æ®é›†çš„æ€§èƒ½ã€‚</li>
<li>Data Whispererç›¸è¾ƒäºç°æœ‰æ–¹æ³•é€Ÿåº¦æå‡äº†7.4å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12212">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d4e7e82acae08234a5970554a11de2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b45dc95a439d1d199d01bbda0ae96514.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d50cfb0cae1e151984528fcd02d7c52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd0fc55ae0634319a377e8c5f4453859.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AnomalyGFM-Graph-Foundation-Model-for-Zero-Few-shot-Anomaly-Detection"><a href="#AnomalyGFM-Graph-Foundation-Model-for-Zero-Few-shot-Anomaly-Detection" class="headerlink" title="AnomalyGFM: Graph Foundation Model for Zero&#x2F;Few-shot Anomaly Detection"></a>AnomalyGFM: Graph Foundation Model for Zero&#x2F;Few-shot Anomaly Detection</h2><p><strong>Authors:Hezhe Qiao, Chaoxi Niu, Ling Chen, Guansong Pang</strong></p>
<p>Graph anomaly detection (GAD) aims to identify abnormal nodes that differ from the majority of the nodes in a graph, which has been attracting significant attention in recent years. Existing generalist graph models have achieved remarkable success in different graph tasks but struggle to generalize to the GAD task. This limitation arises from their difficulty in learning generalized knowledge for capturing the inherently infrequent, irregular and heterogeneous abnormality patterns in graphs from different domains. To address this challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model that supports zero-shot inference and few-shot prompt tuning for GAD in diverse graph datasets. One key insight is that graph-agnostic representations for normal and abnormal classes are required to support effective zero&#x2F;few-shot GAD across different graphs. Motivated by this, AnomalyGFM is pre-trained to align data-independent, learnable normal and abnormal class prototypes with node representation residuals (i.e., representation deviation of a node from its neighbors). The residual features essentially project the node information into a unified feature space where we can effectively measure the abnormality of nodes from different graphs in a consistent way. This provides a driving force for the learning of graph-agnostic, discriminative prototypes for the normal and abnormal classes, which can be used to enable zero-shot GAD on new graphs, including very large-scale graphs. If there are few-shot labeled normal nodes available in the new graphs, AnomalyGFM can further support prompt tuning to leverage these nodes for better adaptation. Comprehensive experiments on 11 widely-used GAD datasets with real anomalies, demonstrate that AnomalyGFM significantly outperforms state-of-the-art competing methods under both zero- and few-shot GAD settings. </p>
<blockquote>
<p>å›¾å¼‚å¸¸æ£€æµ‹ï¼ˆGADï¼‰æ—¨åœ¨è¯†åˆ«å›¾ä¸­ä¸å¤§å¤šæ•°èŠ‚ç‚¹ä¸åŒçš„å¼‚å¸¸èŠ‚ç‚¹ï¼Œè¿™åœ¨è¿‘å¹´æ¥å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç°æœ‰çš„é€šç”¨å›¾æ¨¡å‹åœ¨ä¸åŒçš„å›¾ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨GADä»»åŠ¡ä¸Šå´å¾ˆéš¾è¿›è¡Œæ¨å¹¿ã€‚è¿™ä¸€å±€é™æ€§æºäºå®ƒä»¬æ•æ‰å›¾ä¸­å›ºæœ‰çš„ä¸é¢‘ç¹ã€ä¸è§„åˆ™å’Œå¼‚è´¨å¼‚å¸¸æ¨¡å¼çš„å›°éš¾ï¼Œè¿™äº›æ¨¡å¼å¯èƒ½æ¥æºäºä¸åŒçš„é¢†åŸŸã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘GADçš„å›¾åŸºç¡€æ¨¡å‹AnomalyGFMï¼Œæ”¯æŒé›¶æ ·æœ¬æ¨ç†å’Œå°‘é‡æ ·æœ¬æç¤ºè°ƒæ•´ï¼Œç”¨äºå¤„ç†å„ç§å›¾æ•°æ®é›†ä¸­çš„GADã€‚ä¸€ä¸ªé‡è¦çš„è§è§£æ˜¯éœ€è¦é’ˆå¯¹æ­£å¸¸å’Œå¼‚å¸¸ç±»åˆ«çš„å›¾æ— å…³è¡¨ç¤ºï¼Œä»¥æ”¯æŒè·¨ä¸åŒå›¾çš„é›¶&#x2F;å°‘é‡æ ·æœ¬GADã€‚å—åˆ°è¿™ä¸€è®¤è¯†çš„å¯å‘ï¼ŒAnomalyGFMé€šè¿‡é¢„è®­ç»ƒå°†ç‹¬ç«‹äºæ•°æ®çš„å¯å­¦ä¹ æ­£å¸¸å’Œå¼‚å¸¸ç±»åˆ«åŸå‹ä¸èŠ‚ç‚¹è¡¨ç¤ºæ®‹å·®ï¼ˆå³èŠ‚ç‚¹ä¸å…¶é‚»å±…çš„è¡¨ç¤ºåå·®ï¼‰è¿›è¡Œå¯¹é½ã€‚æ®‹å·®ç‰¹å¾æœ¬è´¨ä¸Šå°†èŠ‚ç‚¹ä¿¡æ¯æŠ•å½±åˆ°ä¸€ä¸ªç»Ÿä¸€çš„ç‰¹å¾ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ•ˆåœ°è¡¡é‡æ¥è‡ªä¸åŒå›¾çš„èŠ‚ç‚¹çš„å¼‚å¸¸æ€§ã€‚è¿™ä¸ºå­¦ä¹ æ­£å¸¸å’Œå¼‚å¸¸ç±»åˆ«çš„å›¾æ— å…³åˆ¤åˆ«åŸå‹æä¾›äº†åŠ¨åŠ›ï¼Œå¯ç”¨äºåœ¨æ–°å›¾ä¸Šå®ç°é›¶æ ·æœ¬GADï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡å›¾ã€‚å¦‚æœæ–°å›¾ä¸­åªæœ‰å°‘é‡æ ‡è®°çš„æ­£å¸¸èŠ‚ç‚¹å¯ç”¨ï¼ŒAnomalyGFMè¿˜å¯ä»¥è¿›ä¸€æ­¥æ”¯æŒæç¤ºè°ƒæ•´ï¼Œä»¥åˆ©ç”¨è¿™äº›èŠ‚ç‚¹è¿›è¡Œæ›´å¥½çš„é€‚åº”ã€‚åœ¨å…·æœ‰çœŸå®å¼‚å¸¸çš„å¹¿æ³›ä½¿ç”¨çš„11ä¸ªGADæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAnomalyGFMåœ¨é›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬GADè®¾ç½®ä¸‹å‡æ˜¾è‘—ä¼˜äºæœ€æ–°çš„ç«äº‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09254v2">PDF</a> Accepted by KDD2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ç§é¢å‘å¼‚å¸¸æ£€æµ‹çš„å›¾å½¢åŸºç¡€æ¨¡å‹AnomalyGFMï¼Œæ”¯æŒé›¶æ ·æœ¬æ¨æ–­å’Œå°‘é‡æ ·æœ¬æç¤ºå¾®è°ƒï¼Œç”¨äºå¤„ç†ä¸åŒå›¾å½¢æ•°æ®é›†ä¸­çš„å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ã€‚è¯¥æ¨¡å‹é€šè¿‡é¢„è®­ç»ƒå¯¹é½æ•°æ®ç‹¬ç«‹çš„æ­£å¸¸å’Œå¼‚å¸¸ç±»åˆ«åŸå‹ä¸èŠ‚ç‚¹è¡¨ç¤ºæ®‹å·®ï¼Œå­¦ä¹ å›¾æ— å…³çš„åˆ¤åˆ«æ€§è¡¨ç¤ºï¼Œä»è€Œå®ç°è·¨ä¸åŒå›¾å½¢çš„æœ‰æ•ˆé›¶&#x2F;å°‘é‡æ ·æœ¬å¼‚å¸¸æ£€æµ‹ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹è¿˜æ”¯æŒåˆ©ç”¨æ–°å›¾ä¸­å°‘é‡æ ‡è®°çš„æ­£å¸¸èŠ‚ç‚¹è¿›è¡Œæç¤ºè°ƒæ•´ï¼Œä»¥é€‚åº”æ›´å¥½çš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒAnomalyGFMåœ¨é›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬çš„å¼‚å¸¸æ£€æµ‹è®¾ç½®ä¸‹å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Graph anomaly detection (GAD)æ—¨åœ¨è¯†åˆ«ä¸å›¾ä¸­å¤§å¤šæ•°èŠ‚ç‚¹ä¸åŒçš„å¼‚å¸¸èŠ‚ç‚¹ï¼Œè¿‘å¹´æ¥å¤‡å—å…³æ³¨ã€‚</li>
<li>ç°æœ‰é€šç”¨å›¾å½¢æ¨¡å‹åœ¨ä¸åŒå›¾å½¢ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨GADä»»åŠ¡ä¸Šéš¾ä»¥æ¨å¹¿ã€‚</li>
<li>AnomalyGFMæ˜¯ä¸€ä¸ªé¢å‘GADçš„å›¾å½¢åŸºç¡€æ¨¡å‹ï¼Œæ”¯æŒé›¶æ ·æœ¬æ¨æ–­å’Œå°‘é‡æ ·æœ¬æç¤ºå¾®è°ƒã€‚</li>
<li>æ¨¡å‹éœ€è¦å›¾æ— å…³çš„å…³äºæ­£å¸¸å’Œå¼‚å¸¸ç±»åˆ«çš„è¡¨ç¤ºï¼Œä»¥æ”¯æŒè·¨ä¸åŒå›¾å½¢çš„æœ‰æ•ˆé›¶&#x2F;å°‘é‡æ ·æœ¬GADã€‚</li>
<li>AnomalyGFMé€šè¿‡é¢„è®­ç»ƒå¯¹é½æ­£å¸¸å’Œå¼‚å¸¸ç±»åˆ«åŸå‹ä¸èŠ‚ç‚¹è¡¨ç¤ºæ®‹å·®ï¼Œå­¦ä¹ å›¾æ— å…³çš„åˆ¤åˆ«æ€§è¡¨ç¤ºã€‚</li>
<li>æ®‹å·®ç‰¹å¾å°†èŠ‚ç‚¹ä¿¡æ¯æŠ•å½±åˆ°ç»Ÿä¸€ç‰¹å¾ç©ºé—´ï¼Œä»è€Œæœ‰æ•ˆåœ°åº¦é‡ä¸åŒå›¾å½¢çš„èŠ‚ç‚¹å¼‚å¸¸æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a39ca23961a9fb0b000045625bea0357.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5750f21f2a0cdfba6339b190e5a5194a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca4d30f51c5b00e0ec580c688ddd45d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45287d8a2117e5656ad1e603a2366086.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44b3ebac02d2cb826798bdb7a8399c70.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Stepwise-Reasoning-Error-Disruption-Attack-of-LLMs"><a href="#Stepwise-Reasoning-Error-Disruption-Attack-of-LLMs" class="headerlink" title="Stepwise Reasoning Error Disruption Attack of LLMs"></a>Stepwise Reasoning Error Disruption Attack of LLMs</h2><p><strong>Authors:Jingyu Peng, Maolin Wang, Xiangyu Zhao, Kai Zhang, Wanyu Wang, Pengyue Jia, Qidong Liu, Ruocheng Guo, Qi Liu</strong></p>
<p>Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEEDâ€™s effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/SEED-Attack">https://github.com/Applied-Machine-Learning-Lab/SEED-Attack</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„å®‰å…¨æ€§å’Œç¨³å¥æ€§ä»ç¼ºä¹æ·±å…¥ç ”ç©¶ã€‚ç°æœ‰é’ˆå¯¹LLMæ¨ç†çš„æ”»å‡»å—åˆ°ç‰¹å®šè®¾ç½®æˆ–éšè”½æ€§ä¸è¶³çš„åˆ¶çº¦ï¼Œé™åˆ¶äº†å…¶å¯è¡Œæ€§å’Œé€šç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é€æ­¥æ¨ç†è¯¯å·®å¹²æ‰°ï¼ˆSEEDï¼‰æ”»å‡»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¾®å¦™åœ°å°†é”™è¯¯æ³¨å…¥åˆ°å…ˆå‰çš„æ¨ç†æ­¥éª¤ä¸­ï¼Œè¯¯å¯¼æ¨¡å‹äº§ç”Ÿé”™è¯¯çš„åç»­æ¨ç†å’Œæœ€ç»ˆç­”æ¡ˆã€‚ä¸ä»¥å‰çš„æ–¹æ³•ä¸åŒï¼ŒSEEDä¸é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®å…¼å®¹ï¼Œä¿æŒè‡ªç„¶æ¨ç†æµç¨‹ï¼Œç¡®ä¿åœ¨ä¸ä¿®æ”¹æŒ‡ä»¤çš„æƒ…å†µä¸‹ç§˜å¯†æ‰§è¡Œã€‚åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å››ä¸ªä¸åŒæ¨¡å‹çš„å¹¿æ³›å®éªŒè¯æ˜äº†SEEDçš„æœ‰æ•ˆæ€§ï¼Œæ­ç¤ºäº†LLMå¯¹æ¨ç†è¿‡ç¨‹å¹²æ‰°çš„è„†å¼±æ€§ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨å®è·µåº”ç”¨ä¸­ç¡®ä¿LLMæ¨ç†ç¨³å¥æ€§çš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/SEED-Attack%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Applied-Machine-Learning-Lab/SEED-Attackä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11934v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å®‰å…¨æ€§å’Œæ¨ç†è¿‡ç¨‹çš„ç¨³å¥æ€§ä»ç„¶æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚é’ˆå¯¹LLMæ¨ç†çš„æ”»å‡»å—é™äºç‰¹å®šåœºæ™¯æˆ–ç¼ºä¹éšè”½æ€§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†é€æ­¥æ¨ç†è¯¯å·®å¹²æ‰°ï¼ˆSEEDï¼‰æ”»å‡»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¾®å¦™åœ°æ³¨å…¥é”™è¯¯æ¥è¯¯å¯¼æ¨¡å‹çš„åç»­æ¨ç†å’Œæœ€ç»ˆç­”æ¡ˆã€‚SEEDæ–¹æ³•ä¸é›¶æ ·æœ¬å’Œå°æ ·æœ¬åœºæ™¯å…¼å®¹ï¼Œä¿æŒè‡ªç„¶æ¨ç†æµç¨‹ï¼Œç¡®ä¿åœ¨ä¸ä¿®æ”¹æŒ‡ä»¤çš„æƒ…å†µä¸‹ç§˜å¯†æ‰§è¡Œã€‚åœ¨å››ä¸ªæ•°æ®é›†å’Œå››ä¸ªä¸åŒæ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†SEEDçš„æœ‰æ•ˆæ€§ï¼Œæ­ç¤ºäº†LLMå¯¹æ¨ç†è¿‡ç¨‹å¹²æ‰°çš„è„†å¼±æ€§ã€‚è¿™å¼ºè°ƒäº†åœ¨å®é™…åº”ç”¨ä¸­å…³æ³¨LLMæ¨ç†ç¨³å¥æ€§çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®‰å…¨æ€§å’Œç¨³å¥æ€§äºŸå¾…æ¢ç´¢ã€‚</li>
<li>ç°æœ‰å¯¹LLMæ¨ç†çš„æ”»å‡»æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ç‰¹å®šåœºæ™¯ä¾èµ–å’Œç¼ºä¹éšè”½æ€§ã€‚</li>
<li>æå‡ºäº†Stepwise rEasoning Error Disruption (SEED)æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡å¾®å¦™åœ°æ³¨å…¥é”™è¯¯æ¥è¯¯å¯¼æ¨¡å‹çš„æ¨ç†å’Œç­”æ¡ˆã€‚</li>
<li>SEEDæ–¹æ³•é€‚ç”¨äºé›¶æ ·æœ¬å’Œå°æ ·æœ¬åœºæ™¯ï¼Œä¿æŒè‡ªç„¶æ¨ç†æµç¨‹ï¼Œç¡®ä¿éšè”½æ‰§è¡Œã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜SEEDçš„æœ‰æ•ˆæ€§ï¼Œæ­ç¤ºäº†LLMå¯¹æ¨ç†è¿‡ç¨‹å¹²æ‰°çš„è„†å¼±æ€§ã€‚</li>
<li>éœ€è¦å…³æ³¨LLMæ¨ç†çš„ç¨³å¥æ€§ï¼Œä»¥ç¡®ä¿å®é™…åº”ç”¨ä¸­çš„å®‰å…¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11934">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-65f3e0e6b8b5e8b018d5891aef727612.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00ca2d3282bea65c458df2505f4b40e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87b1c6209c61a9ecdf478d89801d4273.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c845210b4c456a596c9b56ccbedf20ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb5963186429b3a8418db47170b4451d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Can-Graph-Neural-Networks-Learn-Language-with-Extremely-Weak-Text-Supervision"><a href="#Can-Graph-Neural-Networks-Learn-Language-with-Extremely-Weak-Text-Supervision" class="headerlink" title="Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?"></a>Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?</h2><p><strong>Authors:Zihao Li, Lecheng Zheng, Bowen Jin, Dongqi Fu, Baoyu Jing, Yikun Ban, Jingrui He, Jiawei Han</strong></p>
<p>While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we propose a multi-modal prompt learning paradigm to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. We demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Violet24K/Morpher">https://github.com/Violet24K/Morpher</a>. </p>
<blockquote>
<p>å°½ç®¡åœ¨ä½¿ç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨äº’è”ç½‘è§„æ¨¡çš„å›¾åƒæ–‡æœ¬å¯¹ä¸Šæ„å»ºè§†è§‰æ¨¡å‹æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†ä½¿ç”¨CLIPç®¡é“æ„å»ºå¯è¿ç§»çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™æ˜¯ç”±äºæ ‡è®°æ•°æ®ç¼ºä¹ã€æ–‡æœ¬ç›‘ç£ä¸è¶³ã€ä¸‹æ¸¸ä»»åŠ¡çº§åˆ«ä¸åŒä»¥åŠé¢†åŸŸä¹‹é—´çš„æ¦‚å¿µå·®è·é€ æˆçš„ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨å·¥ä½œä¸­æå‡ºäº†ä¸€ç§å¤šæ¨¡å¼æç¤ºå­¦ä¹ èŒƒå¼ï¼Œä»¥æœ‰æ•ˆåœ°é€‚åº”é¢„è®­ç»ƒçš„GNNè¿›è¡Œä¸‹æ¸¸ä»»åŠ¡å’Œæ•°æ®çš„å¤„ç†ï¼Œä»…ä½¿ç”¨å°‘é‡è¯­ä¹‰æ ‡è®°æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æœ‰æå…¶å¾®å¼±çš„æ–‡æœ¬ç›‘ç£ã€‚æˆ‘ä»¬çš„æ–°èŒƒå¼é€šè¿‡å°†å›¾ç›´æ¥åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç›¸åŒç©ºé—´ä¸­åµŒå…¥ï¼ŒåŒæ—¶å­¦ä¹ å›¾æç¤ºå’Œæ–‡æœ¬æç¤ºã€‚æˆ‘ä»¬åœ¨å°æ ·ä¾‹ã€å¤šä»»åŠ¡çº§åˆ«å’Œè·¨åŸŸè®¾ç½®ä¸­å±•ç¤ºäº†èŒƒå¼çš„å“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªCLIPé£æ ¼çš„é›¶æ ·æœ¬åˆ†ç±»åŸå‹ï¼Œå¯ä»¥æ¨å¹¿åˆ°æœªè§ç±»åˆ«ï¼Œå…·æœ‰æå…¶å¾®å¼±çš„æ–‡æœ¬ç›‘ç£èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Violet24K/Morpher%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Violet24K/Morpherä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08174v3">PDF</a> ACL 2025 Main Conference, 27 pages</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒGraph Neural Networksï¼ˆGNNsï¼‰ä½¿ç”¨CLIPç®¡é“é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ç¼ºä¹æ ‡ç­¾æ•°æ®ã€æ–‡æœ¬ç›‘ç£ä¸è¶³ã€ä¸‹æ¸¸ä»»åŠ¡çº§åˆ«ä¸åŒä»¥åŠé¢†åŸŸé—´çš„æ¦‚å¿µå·®è·ç­‰ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡å¼æç¤ºå­¦ä¹ èŒƒå¼ï¼Œæ—¨åœ¨ä»…ä½¿ç”¨å°‘é‡å…·æœ‰æå¼±æ–‡æœ¬ç›‘ç£çš„è¯­ä¹‰æ ‡è®°æ ·æœ¬ï¼Œæœ‰æ•ˆé€‚åº”é¢„è®­ç»ƒGNNåˆ°ä¸‹æ¸¸ä»»åŠ¡å’Œæ•°æ®çš„æŒ‘æˆ˜ã€‚è¯¥èŒƒå¼é€šè¿‡å°†å›¾ç›´æ¥åµŒå…¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸åŒçš„ç©ºé—´ï¼ŒåŒæ—¶å­¦ä¹ å›¾å½¢æç¤ºå’Œæ–‡æœ¬æç¤ºï¼Œè¡¨ç°å‡ºåœ¨å°‘æ ·æœ¬ã€å¤šä»»åŠ¡çº§åˆ«å’Œè·¨åŸŸè®¾ç½®ä¸­çš„å“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶æ„å»ºäº†ç¬¬ä¸€ä¸ªå…·æœ‰CLIPé£æ ¼çš„é›¶æ ·æœ¬åˆ†ç±»åŸå‹ï¼Œèƒ½å¤Ÿåˆ©ç”¨æå¼±çš„æ–‡æœ¬ç›‘ç£å°†GNNsæ¨å¹¿åˆ°æœªè§ç±»åˆ«ä¸­ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Violet24K/Morpher%E4%B8%8A%E3%80%82">https://github.com/Violet24K/Morpherä¸Šã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°½ç®¡CLIPåœ¨æ„å»ºè§†è§‰æ¨¡å‹æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†ä½¿ç”¨CLIPç®¡é“æ„å»ºå¯è½¬ç§»çš„Graph Neural Networks (GNNs)å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹æ ‡ç­¾æ•°æ®å’Œæ–‡æœ¬ç›‘ç£ï¼Œä»»åŠ¡çº§åˆ«çš„å·®å¼‚ä»¥åŠé¢†åŸŸé—´çš„æ¦‚å¿µå·®è·ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡å¼æç¤ºå­¦ä¹ èŒƒå¼æ¥é€‚åº”é¢„è®­ç»ƒGNNåˆ°ä¸‹æ¸¸ä»»åŠ¡å’Œæ•°æ®çš„æŒ‘æˆ˜ï¼Œä½¿ç”¨ä»…å°‘é‡çš„è¯­ä¹‰æ ‡è®°æ ·æœ¬å’Œæå¼±çš„æ–‡æœ¬ç›‘ç£ã€‚</li>
<li>è¿™ç§æ–°èŒƒå¼é€šè¿‡å°†å›¾ç›´æ¥åµŒå…¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸åŒçš„ç©ºé—´ï¼ŒåŒæ—¶å­¦ä¹ å›¾å½¢æç¤ºå’Œæ–‡æœ¬æç¤ºï¼Œå±•ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶åœ¨å°‘æ ·æœ¬ã€å¤šä»»åŠ¡çº§åˆ«å’Œè·¨åŸŸè®¾ç½®ä¸‹éªŒè¯äº†èŒƒå¼çš„æ€§èƒ½ä¼˜åŠ¿ã€‚</li>
<li>ç ”ç©¶æ„å»ºäº†ç¬¬ä¸€ä¸ªå…·æœ‰CLIPé£æ ¼çš„é›¶æ ·æœ¬åˆ†ç±»åŸå‹ï¼Œèƒ½å¤Ÿå®ç°æœªè§ç±»åˆ«ä¸­çš„GNNsæ¨å¹¿ï¼Œä»…ä¾èµ–äºæå¼±çš„æ–‡æœ¬ç›‘ç£ã€‚</li>
<li>æ¨¡å‹çš„ä»£ç å·²ç»å‘å¸ƒåœ¨Violet24Kçš„Morpheré¡¹ç›®ä¸Šï¼Œä¾›å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.08174">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f4df9489bb1cece1cb5f4c71fa51694d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e68b8e72a5936cd0071cb0864243871.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-610d50eeed67b149f8b1b7de9c346feb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2e2c8e120affd8e37978f26658c59e8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PromptRefine-Enhancing-Few-Shot-Performance-on-Low-Resource-Indic-Languages-with-Example-Selection-from-Related-Example-Banks"><a href="#PromptRefine-Enhancing-Few-Shot-Performance-on-Low-Resource-Indic-Languages-with-Example-Selection-from-Related-Example-Banks" class="headerlink" title="PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic   Languages with Example Selection from Related Example Banks"></a>PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic   Languages with Example Selection from Related Example Banks</h2><p><strong>Authors:Soumya Suvra Ghosal, Soumyabrata Pal, Koyel Mukherjee, Dinesh Manocha</strong></p>
<p>Large Language Models (LLMs) have recently demonstrated impressive few-shot learning capabilities through in-context learning (ICL). However, ICL performance is highly dependent on the choice of few-shot demonstrations, making the selection of the most optimal examples a persistent research challenge. This issue is further amplified in low-resource Indic languages, where the scarcity of ground-truth data complicates the selection process. In this work, we propose PromptRefine, a novel Alternating Minimization approach for example selection that improves ICL performance on low-resource Indic languages. PromptRefine leverages auxiliary example banks from related high-resource Indic languages and employs multi-task learning techniques to align language-specific retrievers, enabling effective cross-language retrieval. Additionally, we incorporate diversity in the selected examples to enhance generalization and reduce bias. Through comprehensive evaluations on four text generation tasks â€“ Cross-Lingual Question Answering, Multilingual Question Answering, Machine Translation, and Cross-Lingual Summarization using state-of-the-art LLMs such as LLAMA-3.1-8B, LLAMA-2-7B, Qwen-2-7B, and Qwen-2.5-7B, we demonstrate that PromptRefine significantly outperforms existing frameworks for retrieving examples. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„å°‘é‡å­¦ä¹ åŠŸèƒ½ã€‚ç„¶è€Œï¼ŒICLçš„æ€§èƒ½é«˜åº¦ä¾èµ–äºå°‘æ•°ç¤ºèŒƒçš„é€‰æ‹©ï¼Œè¿™ä½¿å¾—é€‰æ‹©æœ€ä¼˜è´¨çš„ä¾‹å­æˆä¸ºä¸€ä¸ªæŒç»­çš„ç ”ç©¶æŒ‘æˆ˜ã€‚åœ¨èµ„æºè´«ä¹çš„å°åº¦è¯­è¨€ï¼ˆIndic languagesï¼‰ä¸­ï¼Œè¿™ä¸ªé—®é¢˜è¿›ä¸€æ­¥æ”¾å¤§ï¼Œå› ä¸ºç¼ºä¹çœŸå®æ•°æ®çš„é€‰æ‹©è¿‡ç¨‹å˜å¾—å¤æ‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PromptRefineï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç¤ºä¾‹é€‰æ‹©çš„æ–°å‹äº¤æ›¿æœ€å°åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿæ”¹å–„èµ„æºè´«ä¹çš„å°åº¦è¯­è¨€ä¸Šçš„ICLæ€§èƒ½ã€‚PromptRefineåˆ©ç”¨ç›¸å…³çš„é«˜èµ„æºå°åº¦è¯­è¨€çš„è¾…åŠ©ç¤ºä¾‹åº“ï¼Œå¹¶é‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ æŠ€æœ¯å¯¹é½ç‰¹å®šè¯­è¨€çš„æ£€ç´¢å™¨ï¼Œå®ç°æœ‰æ•ˆçš„è·¨è¯­è¨€æ£€ç´¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨é€‰æ‹©çš„ç¤ºä¾‹ä¸­èå…¥äº†å¤šæ ·æ€§ï¼Œä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›å¹¶å‡å°‘åè§ã€‚é€šè¿‡å››é¡¹æ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„å…¨é¢è¯„ä¼°â€”â€”è·¨è¯­è¨€é—®ç­”ã€å¤šè¯­è¨€é—®ç­”ã€æœºå™¨ç¿»è¯‘å’Œè·¨è¯­è¨€æ‘˜è¦ï¼Œä½¿ç”¨æœ€å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹å¦‚LLAMA-3.1-8Bã€LLAMA-2-7Bã€Qwen-2-7Bå’ŒQwen-2.5-7Bç­‰ï¼Œæˆ‘ä»¬è¯æ˜äº†PromptRefineåœ¨æ£€ç´¢ç¤ºä¾‹æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05710v2">PDF</a> Accepted at NAACL 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒICLæ€§èƒ½é«˜åº¦ä¾èµ–äºå°‘æ•°æ ·æœ¬ç¤ºèŒƒçš„é€‰æ‹©ï¼Œä½¿å¾—é€‰æ‹©æœ€ä¼˜ç¤ºä¾‹æˆä¸ºæŒç»­çš„ç ”ç©¶æŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯åœ¨èµ„æºè´«ä¹çš„å°åº¦è¯­è¨€ç¯å¢ƒä¸­ï¼Œç”±äºç¼ºä¹çœŸå®æ•°æ®ï¼Œé€‰æ‹©è¿‡ç¨‹æ›´åŠ å¤æ‚ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºPromptRefineçš„æ–°å‹äº¤æ›¿æœ€å°åŒ–æ–¹æ³•ï¼Œç”¨äºç¤ºä¾‹é€‰æ‹©ï¼Œå¯æé«˜ä½èµ„æºå°åº¦è¯­è¨€ç¯å¢ƒä¸‹çš„ICLæ€§èƒ½ã€‚PromptRefineåˆ©ç”¨æ¥è‡ªç›¸å…³é«˜èµ„æºå°åº¦è¯­è¨€çš„è¾…åŠ©ç¤ºä¾‹åº“ï¼Œé‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ æŠ€æœ¯ï¼Œå®ç°è¯­è¨€ç‰¹å®šæ£€ç´¢å™¨çš„å¯¹é½ï¼Œå®ç°æœ‰æ•ˆçš„è·¨è¯­è¨€æ£€ç´¢ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¢åŠ æ‰€é€‰ç¤ºä¾‹çš„å¤šæ ·æ€§æ¥æé«˜æ³›åŒ–èƒ½åŠ›å’Œå‡å°‘åè§ã€‚é€šè¿‡å…¨é¢çš„è¯„ä¼°ï¼Œåœ¨å››é¡¹æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šä½¿ç”¨å‰æ²¿çš„LLMï¼Œå¦‚LLAMA-3.1-8Bã€LLAMA-2-7Bã€Qwen-2-7Bå’ŒQwen-2.5-7Bç­‰æ¨¡å‹ï¼Œè¯æ˜PromptRefineåœ¨æ£€ç´¢ç¤ºä¾‹æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œä½†ç¤ºä¾‹é€‰æ‹©æ˜¯å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>åœ¨èµ„æºæœ‰é™çš„å°åº¦è¯­è¨€ä¸­ï¼Œç¤ºä¾‹é€‰æ‹©è¿‡ç¨‹æ›´åŠ å¤æ‚ã€‚</li>
<li>PromptRefineæ˜¯ä¸€ç§æ–°å‹çš„ç¤ºä¾‹é€‰æ‹©æ–¹æ³•ï¼Œé€šè¿‡äº¤æ›¿æœ€å°åŒ–æ¥æå‡ä½èµ„æºå°åº¦è¯­è¨€ç¯å¢ƒä¸‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ€§èƒ½ã€‚</li>
<li>PromptRefineåˆ©ç”¨è¾…åŠ©ç¤ºä¾‹åº“å’Œå¤šä»»åŠ¡å­¦ä¹ æŠ€æœ¯ï¼Œå®ç°å¯¹è¯­è¨€ç‰¹å®šæ£€ç´¢å™¨çš„æœ‰æ•ˆè·¨è¯­è¨€æ£€ç´¢ã€‚</li>
<li>é€šè¿‡å¢åŠ æ‰€é€‰ç¤ºä¾‹çš„å¤šæ ·æ€§æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå‡å°‘åè§ã€‚</li>
<li>åœ¨å¤šé¡¹æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒPromptRefineæ˜¾è‘—ä¼˜äºç°æœ‰æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3abc881f58cb45a8847a029d49d7700f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95756d1155eeb8effb115b7d8a3db523.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a24dbd9507325756ce88281a7ed99183.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d0a58e26a07145f0acaa0db6e131b10.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping"><a href="#MSDNet-Multi-Scale-Decoder-for-Few-Shot-Semantic-Segmentation-via-Transformer-Guided-Prototyping" class="headerlink" title="MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping"></a>MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via   Transformer-Guided Prototyping</h2><p><strong>Authors:Amirreza Fateh, Mohammad Reza Mohammadi, Mohammad Reza Jahed Motlagh</strong></p>
<p>Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the Transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve competitive results on benchmark datasets such as PASCAL-5^i and COCO-20^i in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies. </p>
<blockquote>
<p>å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆFew-shot Semantic Segmentationï¼‰æ—¨åœ¨è§£å†³ä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å¯¹æŸ¥è¯¢å›¾åƒä¸­çš„å¯¹è±¡è¿›è¡Œåˆ†å‰²çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œè®¸å¤šå…ˆå‰çš„å‰æ²¿æ–¹æ³•è¦ä¹ˆéœ€è¦æ”¾å¼ƒå¤æ‚çš„å±€éƒ¨è¯­ä¹‰ç‰¹å¾ï¼Œè¦ä¹ˆé¢ä¸´é«˜è®¡ç®—å¤æ‚åº¦çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºTransformeræ¶æ„çš„å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ç©ºé—´å˜æ¢è§£ç å™¨å’Œä¸Šä¸‹æ–‡æ©ç ç”Ÿæˆæ¨¡å—ï¼Œä»¥æé«˜æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´çš„å…³ç³»ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šå°ºåº¦è§£ç å™¨ï¼Œä»¥åˆ†å±‚çš„æ–¹å¼èå…¥ä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾æ¥ä¼˜åŒ–åˆ†å‰²æ©ç ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ•´åˆäº†ä¸­é—´ç¼–ç å™¨é˜¶æ®µçš„å…¨å±€ç‰¹å¾ï¼Œä»¥æé«˜ä¸Šä¸‹æ–‡ç†è§£ï¼ŒåŒæ—¶ä¿æŒè½»é‡çº§ç»“æ„ä»¥é™ä½å¤æ‚åº¦ã€‚æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´çš„è¿™ç§å¹³è¡¡ä½¿æˆ‘ä»¬çš„æ–¹æ³•åœ¨PASCAL-5^iå’ŒCOCO-20^iç­‰åŸºå‡†æ•°æ®é›†ä¸Šï¼Œæ— è®ºæ˜¯1æ¬¡æ‹æ‘„è¿˜æ˜¯5æ¬¡æ‹æ‘„çš„ç¯å¢ƒä¸­éƒ½èƒ½å–å¾—æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»…æœ‰150ä¸‡ä¸ªå‚æ•°ï¼Œå±•ç°äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒåŒæ—¶å…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11316v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    åŸºäºTransformeræ¶æ„çš„å°‘æ•°æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ç©ºé—´å˜æ¢è§£ç å™¨å’Œä¸Šä¸‹æ–‡æ©æ¨¡ç”Ÿæˆæ¨¡å—ï¼Œæé«˜æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´çš„å…³ç³»ç†è§£ã€‚é‡‡ç”¨å¤šå°ºåº¦è§£ç å™¨ï¼Œä»¥å±‚æ¬¡æ–¹å¼èåˆä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾ï¼Œä¼˜åŒ–åˆ†å‰²æ©æ¨¡ã€‚æ•´åˆä¸­é—´ç¼–ç é˜¶æ®µçš„å…¨å±€ç‰¹å¾ï¼Œæé«˜ä¸Šä¸‹æ–‡ç†è§£ï¼ŒåŒæ—¶ä¿æŒè½»é‡çº§ç»“æ„ï¼Œé™ä½å¤æ‚æ€§ã€‚åœ¨PASCAL-5^iå’ŒCOCO-20^iç­‰åŸºå‡†æ•°æ®é›†ä¸Šï¼Œ1æ¬¡å’Œ5æ¬¡å°„å‡»è®¾ç½®ä¸­è¡¨ç°ä¼˜å¼‚ã€‚æ¨¡å‹ä»…150ä¸‡å‚æ•°ï¼Œå…·æœ‰ç«äº‰åŠ›ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯¥æ–‡æœ¬ä»‹ç»äº†åŸºäºTransformeræ¶æ„çš„å°‘æ•°æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¡†æ¶ã€‚</li>
<li>å¼•å…¥ç©ºé—´å˜æ¢è§£ç å™¨å’Œä¸Šä¸‹æ–‡æ©æ¨¡ç”Ÿæˆæ¨¡å—ä»¥æé«˜å…³ç³»ç†è§£ã€‚</li>
<li>ä½¿ç”¨å¤šå°ºåº¦è§£ç å™¨èåˆä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾æ¥ä¼˜åŒ–åˆ†å‰²æ©æ¨¡ã€‚</li>
<li>ç»“åˆä¸­é—´ç¼–ç é˜¶æ®µçš„å…¨çƒç‰¹å¾ä»¥æé«˜ä¸Šä¸‹æ–‡ç†è§£ã€‚</li>
<li>æ¨¡å‹ç»“æ„ä¿æŒè½»é‡çº§ï¼Œä»¥é™ä½å¤æ‚æ€§ã€‚</li>
<li>åœ¨PASCAL-5^iå’ŒCOCO-20^iç­‰åŸºå‡†æ•°æ®é›†ä¸­å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>æ¨¡å‹å…·æœ‰è¾ƒå°‘çš„å‚æ•°ï¼ˆä»…150ä¸‡ï¼‰ï¼Œæ€§èƒ½å‡ºè‰²ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11316">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a28d10d4e1e67b7c935fbf7a8b9ca0ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6d94b05da2662a71afb78c8b6fec8f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1232d75de621baac9ac76c250665a86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43b280a5d0cd66845ec22d0186712420.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="BMIKE-53-Investigating-Cross-Lingual-Knowledge-Editing-with-In-Context-Learning"><a href="#BMIKE-53-Investigating-Cross-Lingual-Knowledge-Editing-with-In-Context-Learning" class="headerlink" title="BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context   Learning"></a>BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context   Learning</h2><p><strong>Authors:Ercong Nie, Bo Shao, Zifeng Ding, Mingyang Wang, Helmut Schmid, Hinrich SchÃ¼tze</strong></p>
<p>This paper introduces BMIKE-53, a comprehensive benchmark for cross-lingual in-context knowledge editing (IKE) across 53 languages, unifying three knowledge editing (KE) datasets: zsRE, CounterFact, and WikiFactDiff. Cross-lingual KE, which requires knowledge edited in one language to generalize across others while preserving unrelated knowledge, remains underexplored. To address this gap, we systematically evaluate IKE under zero-shot, one-shot, and few-shot setups, incorporating tailored metric-specific demonstrations. Our findings reveal that model scale and demonstration alignment critically govern cross-lingual IKE efficacy, with larger models and tailored demonstrations significantly improving performance. Linguistic properties, particularly script type, strongly influence performance variation across languages, with non-Latin languages underperforming due to issues like language confusion. Code and data are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ercong21/MultiKnow/">https://github.com/ercong21/MultiKnow/</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†BMIKE-53ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹53ç§è¯­è¨€çš„è·¨è¯­å¢ƒçŸ¥è¯†ç¼–è¾‘ï¼ˆIKEï¼‰çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå®ƒç»Ÿä¸€äº†ä¸‰ä¸ªçŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰æ•°æ®é›†ï¼šzsREã€CounterFactå’ŒWikiFactDiffã€‚è·¨è¯­è¨€çŸ¥è¯†ç¼–è¾‘ï¼ˆIKEï¼‰è¦æ±‚åœ¨ä¸€ç§è¯­è¨€ä¸­å¯¹çŸ¥è¯†è¿›è¡Œç¼–è¾‘ï¼Œå¹¶èƒ½åœ¨å…¶ä»–è¯­è¨€ä¸­æ¨å¹¿ï¼ŒåŒæ—¶ä¿ç•™æ— å…³çŸ¥è¯†ï¼Œè¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬åœ¨é›¶æ ·æœ¬ã€ä¸€æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹ç³»ç»Ÿåœ°è¯„ä¼°IKEï¼Œå¹¶çº³å…¥å®šåˆ¶çš„ç‰¹å®šæŒ‡æ ‡æ¼”ç¤ºã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹çš„è§„æ¨¡å’Œæ¼”ç¤ºå¯¹é½å¯¹è·¨è¯­è¨€IKEçš„æœ‰æ•ˆæ€§è‡³å…³é‡è¦ï¼Œè¾ƒå¤§çš„æ¨¡å‹å’Œå®šåˆ¶çš„æ¼”ç¤ºèƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚è¯­è¨€ç‰¹æ€§ï¼Œå°¤å…¶æ˜¯æ–‡å­—ç±»å‹ï¼Œå¯¹è·¨è¯­è¨€æ€§èƒ½å˜åŒ–æœ‰å¾ˆå¤§å½±å“ï¼Œéæ‹‰ä¸è¯­ç³»çš„è¡¨ç°ä¸ä½³ä¸»è¦æ˜¯ç”±äºè¯­è¨€æ··æ·†ç­‰é—®é¢˜å¯¼è‡´çš„ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å·²å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ercong2sc=/%EF%BC%88%E5%BC%80%E6%BA%90%E5%B9%B3%E5%8F%B0%E7%9A%84%E8%B4%A6%E5%8F%B7%E4%B8%8D%E5%8F%AF%E7%94%A8%EF%BC%89%E3%80%82">https://github.com/ercong2sc=/ï¼ˆå¼€æºå¹³å°çš„è´¦å·ä¸å¯ç”¨ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17764v3">PDF</a> Accepted to ACL 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†BMIKE-53ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–53ç§è¯­è¨€çš„è·¨è¯­è¨€ä¸Šä¸‹æ–‡çŸ¥è¯†ç¼–è¾‘ï¼ˆIKEï¼‰ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œå®ƒç»Ÿä¸€äº†ä¸‰ä¸ªçŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰æ•°æ®é›†ï¼šzsREã€CounterFactå’ŒWikiFactDiffã€‚ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°äº†è·¨è¯­è¨€IKEåœ¨é›¶æ ·æœ¬ã€ä¸€ç¤ºä¾‹å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹çš„è¡¨ç°ï¼Œå‘ç°æ¨¡å‹è§„æ¨¡å’Œæ¼”ç¤ºä¸ä»»åŠ¡çš„åŒ¹é…åº¦å¯¹è·¨è¯­è¨€IKEçš„æ•ˆæœè‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶ç»“æœè¿˜è¡¨æ˜è¯­è¨€ç‰¹æ€§å¯¹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œç‰¹åˆ«æ˜¯éæ‹‰ä¸è¯­è¨€çš„è¡¨ç°ç›¸å¯¹è¾ƒå·®ã€‚æ•°æ®å’Œä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BMIKE-53æ˜¯ä¸€ä¸ªè·¨è¯­è¨€ä¸Šä¸‹æ–‡çŸ¥è¯†ç¼–è¾‘çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†53ç§è¯­è¨€ï¼Œç»Ÿä¸€äº†ä¸‰ä¸ªçŸ¥è¯†ç¼–è¾‘æ•°æ®é›†ã€‚</li>
<li>è¯„ä¼°äº†è·¨è¯­è¨€IKEåœ¨é›¶æ ·æœ¬ã€ä¸€ç¤ºä¾‹å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹çš„è¡¨ç°ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡å’Œæ¼”ç¤ºä¸ä»»åŠ¡çš„åŒ¹é…åº¦æ˜¯å½±å“è·¨è¯­è¨€IKEæ•ˆæœçš„å…³é”®å› ç´ ã€‚</li>
<li>éæ‹‰ä¸è¯­è¨€åœ¨è·¨è¯­è¨€IKEä¸­çš„è¡¨ç°ç›¸å¯¹è¾ƒå·®ã€‚</li>
<li>è¯­è¨€ç‰¹æ€§ï¼ˆå¦‚è„šæœ¬ç±»å‹ï¼‰å¯¹è·¨è¯­è¨€IKEæ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>æ•°æ®å’Œä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šä¾›å…¬ä¼—ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.17764">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c1a02a9d96372d0211a8828ad516dd6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93ec5c199a3111735cbe5faf45431d31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4064a6f312fd03bd951cd7686c17e077.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f912800c0bd7e2ea52bd2aefc613c10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b83596af779e99edfe2986435a260ad0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bd3fb4a4f5c1a1329ba3cfb5b8a715c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6406e372a9d60bac0b41c5aec839f5d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RAEmoLLM-Retrieval-Augmented-LLMs-for-Cross-Domain-Misinformation-Detection-Using-In-Context-Learning-Based-on-Emotional-Information"><a href="#RAEmoLLM-Retrieval-Augmented-LLMs-for-Cross-Domain-Misinformation-Detection-Using-In-Context-Learning-Based-on-Emotional-Information" class="headerlink" title="RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation   Detection Using In-Context Learning Based on Emotional Information"></a>RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation   Detection Using In-Context Learning Based on Emotional Information</h2><p><strong>Authors:Zhiwei Liu, Kailai Yang, Qianqian Xie, Christine de Kock, Sophia Ananiadou, Eduard Hovy</strong></p>
<p>Misinformation is prevalent in various fields such as education, politics, health, etc., causing significant harm to society. However, current methods for cross-domain misinformation detection rely on effort- and resource-intensive fine-tuning and complex model structures. With the outstanding performance of LLMs, many studies have employed them for misinformation detection. Unfortunately, they focus on in-domain tasks and do not incorporate significant sentiment and emotion features (which we jointly call {\em affect}). In this paper, we propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework to address cross-domain misinformation detection using in-context learning based on affective information. RAEmoLLM includes three modules. (1) In the index construction module, we apply an emotional LLM to obtain affective embeddings from all domains to construct a retrieval database. (2) The retrieval module uses the database to recommend top K examples (text-label pairs) from source domain data for target domain contents. (3) These examples are adopted as few-shot demonstrations for the inference module to process the target domain content. The RAEmoLLM can effectively enhance the general performance of LLMs in cross-domain misinformation detection tasks through affect-based retrieval, without fine-tuning. We evaluate our framework on three misinformation benchmarks. Results show that RAEmoLLM achieves significant improvements compared to the other few-shot methods on three datasets, with the highest increases of 15.64%, 31.18%, and 15.73% respectively. This project is available at <a target="_blank" rel="noopener" href="https://github.com/lzw108/RAEmoLLM">https://github.com/lzw108/RAEmoLLM</a>. </p>
<blockquote>
<p>è¯¯ä¿¡æ¯åœ¨å„ä¸ªé¢†åŸŸéƒ½å¾ˆæ™®éï¼Œå¦‚æ•™è‚²ã€æ”¿æ²»ã€å«ç”Ÿç­‰ï¼Œç»™ç¤¾ä¼šé€ æˆé‡å¤§å±å®³ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è·¨åŸŸè¯¯æ£€æµ‹æ–¹æ³•ä¾èµ–äºè€—è´¹åŠªåŠ›å’Œèµ„æºçš„ç²¾ç»†è°ƒæ•´åŠå¤æ‚çš„æ¨¡å‹ç»“æ„ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡ºè‰²è¡¨ç°ï¼Œè®¸å¤šç ”ç©¶å·²å°†å…¶ç”¨äºè¯¯æ£€æµ‹ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¸»è¦å…³æ³¨é¢†åŸŸå†…çš„ä»»åŠ¡ï¼Œå¹¶æ²¡æœ‰èå…¥é‡è¦çš„æƒ…æ„Ÿå’Œæƒ…ç»ªç‰¹å¾ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæƒ…æ„Ÿâ€ï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†RAEmoLLMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºæƒ…æ„Ÿä¿¡æ¯çš„ä¸Šä¸‹æ–‡å­¦ä¹ å¢å¼ºå‹ï¼ˆRAGï¼‰LLMsæ¡†æ¶ï¼Œç”¨äºè§£å†³è·¨åŸŸè¯¯æ£€æµ‹é—®é¢˜ã€‚RAEmoLLMåŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ã€‚ï¼ˆ1ï¼‰åœ¨ç´¢å¼•æ„å»ºæ¨¡å—ä¸­ï¼Œæˆ‘ä»¬åº”ç”¨æƒ…æ„ŸLLMæ¥è·å¾—æ‰€æœ‰é¢†åŸŸçš„æƒ…æ„ŸåµŒå…¥æ¥æ„å»ºæ£€ç´¢æ•°æ®åº“ã€‚ï¼ˆ2ï¼‰æ£€ç´¢æ¨¡å—ä½¿ç”¨æ•°æ®åº“ä¸ºç›®æ ‡åŸŸå†…å®¹æ¨èå‰Kä¸ªç¤ºä¾‹ï¼ˆæ–‡æœ¬æ ‡ç­¾å¯¹ï¼‰ã€‚ï¼ˆ3ï¼‰è¿™äº›ç¤ºä¾‹è¢«ç”¨ä½œæ¨æ–­æ¨¡å—çš„å°‘æ•°æ¼”ç¤ºæ¥å¤„ç†ç›®æ ‡åŸŸå†…å®¹ã€‚é€šè¿‡åŸºäºæƒ…æ„Ÿçš„æ£€ç´¢ï¼ŒRAEmoLLMèƒ½æœ‰æ•ˆæå‡LLMsåœ¨è·¨åŸŸè¯¯æ£€æµ‹ä»»åŠ¡ä¸­çš„æ€»ä½“æ€§èƒ½ï¼Œæ— éœ€ç²¾ç»†è°ƒæ•´ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªè¯¯æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸Šå¯¹æ¡†æ¶è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–å°‘æ•°æ–¹æ³•ç›¸æ¯”ï¼ŒRAEmoLLMåœ¨è¿™ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„æ”¹è¿›æ•ˆæœæ˜¾è‘—ï¼Œæœ€é«˜åˆ†åˆ«æé«˜äº†15.64%ã€31.18%å’Œ15.73%ã€‚æ­¤é¡¹ç›®å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/lzw108/RAEmoLLM%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/lzw108/RAEmoLLMè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11093v2">PDF</a> Accepted by ACL 2025 (Main)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºRAEmoLLMçš„è·¨åŸŸè™šå‡ä¿¡æ¯æ£€æµ‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨æƒ…æ„ŸåµŒå…¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šæ„å»ºæƒ…æ„ŸåµŒå…¥ç´¢å¼•ã€åŸºäºæƒ…æ„Ÿçš„æ£€ç´¢ï¼Œä»¥åŠåˆ©ç”¨æ£€ç´¢ç»“æœä½œä¸ºç¤ºèŒƒè¿›è¡Œæ¨æ–­ã€‚RAEmoLLMèƒ½æœ‰æ•ˆæé«˜LLMsåœ¨è·¨åŸŸè™šå‡ä¿¡æ¯æ£€æµ‹ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä¸”æ— éœ€å¾®è°ƒã€‚åœ¨ä¸‰ä¸ªè™šå‡ä¿¡æ¯æ£€æµ‹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒRAEmoLLMå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è·¨åŸŸè™šå‡ä¿¡æ¯æ£€æµ‹æ˜¯ä¸€ä¸ªé‡è¦ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œéœ€è¦æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å½“å‰çš„æ–¹æ³•ä¾èµ–äºèµ„æºå¯†é›†å‹çš„ç²¾ç»†è°ƒæ•´å’Œå¤æ‚çš„æ¨¡å‹ç»“æ„ã€‚</li>
<li>LLMsåœ¨è™šå‡ä¿¡æ¯æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹è·¨åŸŸå’Œèå…¥æƒ…æ„Ÿç‰¹å¾çš„èƒ½åŠ›ã€‚</li>
<li>RAEmoLLMæ˜¯é¦–ä¸ªåˆ©ç”¨æƒ…æ„Ÿä¿¡æ¯å¢å¼ºLLMsçš„è·¨åŸŸè™šå‡ä¿¡æ¯æ£€æµ‹æ¡†æ¶ã€‚</li>
<li>RAEmoLLMåŒ…æ‹¬æƒ…æ„ŸåµŒå…¥ç´¢å¼•æ„å»ºã€åŸºäºæƒ…æ„Ÿçš„æ£€ç´¢å’Œç¤ºèŒƒæ¨æ–­ä¸‰ä¸ªæ¨¡å—ã€‚</li>
<li>RAEmoLLMåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶æ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
<li>RAEmoLLMæ¡†æ¶å¯ç”¨äºå¤šç§é¢†åŸŸï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.11093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-badbe6951d5ec47af093d6af23bfee86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b5e631e1713eaddda9667980cd5a4ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1eafebc4cebf13e243f766c3c151fd42.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef8abac4ad22db776c9f2c00b0959e61.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Multi-Prompting-Decoder-Helps-Better-Language-Understanding"><a href="#Multi-Prompting-Decoder-Helps-Better-Language-Understanding" class="headerlink" title="Multi-Prompting Decoder Helps Better Language Understanding"></a>Multi-Prompting Decoder Helps Better Language Understanding</h2><p><strong>Authors:Zifeng Cheng, Zhaoling Chen, Zhiwei Jiang, Yafeng Yin, Cong Wang, Shiping Ge, Qing Gu</strong></p>
<p>Recent Pre-trained Language Models (PLMs) usually only provide users with the inference APIs, namely the emerging Model-as-a-Service (MaaS) setting. To adapt MaaS PLMs to downstream tasks without accessing their parameters and gradients, some existing methods focus on the output-side adaptation of PLMs, viewing the PLM as an encoder and then optimizing a task-specific decoder for decoding the output hidden states and class scores of the PLM. Despite the effectiveness of these methods, they only use a single prompt to query PLMs for decoding, leading to a heavy reliance on the quality of the adopted prompt. In this paper, we propose a simple yet effective Multi-Prompting Decoder (MPD) framework for MaaS adaptation. The core idea is to query PLMs with multiple different prompts for each sample, thereby obtaining multiple output hidden states and class scores for subsequent decoding. Such multi-prompting decoding paradigm can simultaneously mitigate reliance on the quality of a single prompt, alleviate the issue of data scarcity under the few-shot setting, and provide richer knowledge extracted from PLMs. Specifically, we propose two decoding strategies: multi-prompting decoding with optimal transport for hidden states and calibrated decoding for class scores. Extensive experiments demonstrate that our method achieves new state-of-the-art results on multiple natural language understanding datasets under the few-shot setting. </p>
<blockquote>
<p>æœ€è¿‘çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰é€šå¸¸åªä¸ºç”¨æˆ·æä¾›æ¨ç†APIï¼Œå³æ–°å…´çš„æ¨¡å‹å³æœåŠ¡ï¼ˆMaaSï¼‰è®¾ç½®ã€‚ä¸ºäº†é€‚åº”ä¸éœ€è¦è®¿é—®å…¶å‚æ•°å’Œæ¢¯åº¦çš„ä¸‹æ¸¸ä»»åŠ¡çš„MaaS PLMsï¼Œä¸€äº›ç°æœ‰æ–¹æ³•ä¸“æ³¨äºPLMsçš„è¾“å‡ºç«¯é€‚åº”ï¼Œå°†PLMè§†ä¸ºç¼–ç å™¨ï¼Œç„¶åä¼˜åŒ–é’ˆå¯¹è§£ç PLMçš„è¾“å‡ºéšè—çŠ¶æ€å’Œç±»åˆ«åˆ†æ•°çš„ç‰¹å®šä»»åŠ¡è§£ç å™¨ã€‚å°½ç®¡è¿™äº›æ–¹æ³•å¾ˆæœ‰æ•ˆï¼Œä½†å®ƒä»¬ä»…ä½¿ç”¨å•ä¸ªæç¤ºæ¥æŸ¥è¯¢PLMsè¿›è¡Œè§£ç ï¼Œè¿™å¯¼è‡´å¯¹é‡‡ç”¨æç¤ºçš„è´¨é‡çš„ä¸¥é‡ä¾èµ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•çš„ä½†æœ‰æ•ˆçš„å¤šæç¤ºè§£ç ï¼ˆMPDï¼‰æ¡†æ¶ï¼Œç”¨äºMaaSé€‚åº”ã€‚æ ¸å¿ƒç†å¿µæ˜¯ä¸ºæ¯ä¸ªæ ·æœ¬ä½¿ç”¨å¤šä¸ªä¸åŒçš„æç¤ºæ¥æŸ¥è¯¢PLMsï¼Œä»è€Œè·å¾—å¤šä¸ªè¾“å‡ºéšè—çŠ¶æ€å’Œç±»åˆ«åˆ†æ•°ï¼Œä»¥ä¾›åç»­è§£ç ã€‚è¿™ç§å¤šæç¤ºè§£ç èŒƒå¼å¯ä»¥åŒæ—¶å‡è½»å¯¹å•ä¸ªæç¤ºè´¨é‡çš„ä¾èµ–ï¼Œç¼“è§£å°‘æ ·æœ¬è®¾ç½®ä¸‹çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¹¶æä¾›ä»PLMsä¸­æå–çš„æ›´ä¸°å¯Œçš„çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§è§£ç ç­–ç•¥ï¼šç”¨äºéšè—çŠ¶æ€çš„æœ€ä¼˜ä¼ è¾“å¤šæç¤ºè§£ç å’Œç”¨äºç±»åˆ«åˆ†æ•°çš„æ ¡å‡†è§£ç ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€ç†è§£æ•°æ®é›†ä¸Šçš„å°‘æ ·æœ¬è®¾ç½®ä¸‹è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06279v2">PDF</a> Findings of ACL 2025</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰é€šå¸¸é‡‡ç”¨æ¨¡å‹å³æœåŠ¡ï¼ˆMaaSï¼‰çš„æ–¹å¼æä¾›æ¨ç†æ¥å£ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨PLMè¾“å‡ºç«¯çš„é€‚é…ï¼Œé€šè¿‡ä¼˜åŒ–ç‰¹å®šä»»åŠ¡çš„è§£ç å™¨æ¥è§£ç PLMçš„è¾“å‡ºéšè—çŠ¶æ€å’Œç±»åˆ«åˆ†æ•°ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºå•ä¸€æç¤ºçš„è´¨é‡ã€‚æœ¬æ–‡æå‡ºä¸€ç§ç®€å•æœ‰æ•ˆçš„å¤šæç¤ºè§£ç ï¼ˆMPDï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸ºæ¯ä¸ªæ ·æœ¬ä½¿ç”¨å¤šä¸ªä¸åŒæç¤ºæ¥æŸ¥è¯¢PLMï¼Œè·å¾—å¤šä¸ªè¾“å‡ºéšè—çŠ¶æ€å’Œç±»åˆ«åˆ†æ•°è¿›è¡Œåç»­è§£ç ã€‚å¤šæç¤ºè§£ç èŒƒå¼èƒ½åŒæ—¶å‡è½»å¯¹å•ä¸€æç¤ºçš„ä¾èµ–ï¼Œç¼“è§£å°‘æ ·æœ¬è®¾ç½®ä¸‹çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¹¶æä¾›æ›´ä¸°å¯Œåœ°ä»PLMä¸­æå–çš„çŸ¥è¯†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰é€šå¸¸é‡‡ç”¨æ¨¡å‹å³æœåŠ¡ï¼ˆMaaSï¼‰æ–¹å¼ï¼Œä»…æä¾›æ¨ç†æ¥å£ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨PLMè¾“å‡ºç«¯çš„é€‚é…ï¼Œä½¿ç”¨å•ä¸€æç¤ºè¿›è¡ŒæŸ¥è¯¢ã€‚</li>
<li>å¤šæç¤ºè§£ç ï¼ˆMPDï¼‰æ¡†æ¶æå‡ºï¼Œé€šè¿‡ä¸ºæ¯ä¸ªæ ·æœ¬ä½¿ç”¨å¤šä¸ªæç¤ºæ¥æŸ¥è¯¢PLMã€‚</li>
<li>MPDæ¡†æ¶é€šè¿‡è·å¾—å¤šä¸ªè¾“å‡ºéšè—çŠ¶æ€å’Œç±»åˆ«åˆ†æ•°è¿›è¡Œåç»­è§£ç ã€‚</li>
<li>å¤šæç¤ºè§£ç èƒ½å‡è½»å¯¹å•ä¸€æç¤ºçš„ä¾èµ–ï¼Œç¼“è§£å°‘æ ·æœ¬è®¾ç½®ä¸‹çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>MPDæ¡†æ¶æä¾›ä»PLMæ›´ä¸°å¯Œåœ°æå–çš„çŸ¥è¯†ã€‚</li>
<li>æå‡ºä¸¤ç§è§£ç ç­–ç•¥ï¼šåŸºäºæœ€ä¼˜ä¼ è¾“çš„éšè—çŠ¶æ€å¤šæç¤ºè§£ç å’Œæ ¡å‡†ç±»åˆ«åˆ†æ•°è§£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06279">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-10c39cd6aa16286fceb56863da20a14f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0d53d6b494572715f32ae577519ad73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28b6923886eb6c9ee080ec52b2d1791f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce5712a3dc294aa6da8a77fc60bf75bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9796e1e33bfae42360e8cbabbca470a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-02ea006e8e551367dc0c39d167de2e7f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-04/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-b37a8cf7b390bb4a05303685151b6bfd.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-04  Brain network science modelling of sparse neural networks enables   Transformers and LLMs to perform as fully connected
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f8188d87da012f86325b12f57afd9ec1.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-04  AutoChemSchematic AI A Closed-Loop, Physics-Aware Agentic Framework for   Auto-Generating Chemical Process and Instrumentation Diagrams
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
