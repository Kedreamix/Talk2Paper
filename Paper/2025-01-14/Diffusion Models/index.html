<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-01-14  Beyond Flat Text Dual Self-inherited Guidance for Visual Text   Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1498048b4d84cb56a8c04fe13cf4817d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    49 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-14-更新"><a href="#2025-01-14-更新" class="headerlink" title="2025-01-14 更新"></a>2025-01-14 更新</h1><h2 id="Beyond-Flat-Text-Dual-Self-inherited-Guidance-for-Visual-Text-Generation"><a href="#Beyond-Flat-Text-Dual-Self-inherited-Guidance-for-Visual-Text-Generation" class="headerlink" title="Beyond Flat Text: Dual Self-inherited Guidance for Visual Text   Generation"></a>Beyond Flat Text: Dual Self-inherited Guidance for Visual Text   Generation</h2><p><strong>Authors:Minxing Luo, Zixun Xia, Liaojun Chen, Zhenhang Li, Weichao Zeng, Jianye Wang, Wentao Cheng, Yaxing Wang, Yu Zhou, Jian Yang</strong></p>
<p>In real-world images, slanted or curved texts, especially those on cans, banners, or badges, appear as frequently, if not more so, than flat texts due to artistic design or layout constraints. While high-quality visual text generation has become available with the advanced generative capabilities of diffusion models, these models often produce distorted text and inharmonious text background when given slanted or curved text layouts due to training data limitation. In this paper, we introduce a new training-free framework, STGen, which accurately generates visual texts in challenging scenarios (\eg, slanted or curved text layouts) while harmonizing them with the text background. Our framework decomposes the visual text generation process into two branches: (i) \textbf{Semantic Rectification Branch}, which leverages the ability in generating flat but accurate visual texts of the model to guide the generation of challenging scenarios. The generated latent of flat text is abundant in accurate semantic information related both to the text itself and its background. By incorporating this, we rectify the semantic information of the texts and harmonize the integration of the text with its background in complex layouts. (ii) \textbf{Structure Injection Branch}, which reinforces the visual text structure during inference. We incorporate the latent information of the glyph image, rich in glyph structure, as a new condition to further strengthen the text structure. To enhance image harmony, we also apply an effective combination method to merge the priors, providing a solid foundation for generation. Extensive experiments across a variety of visual text layouts demonstrate that our framework achieves superior accuracy and outstanding quality. </p>
<blockquote>
<p>在现实世界的图像中，倾斜或弯曲的文本，特别是在罐头、横幅或徽章上的文本，由于艺术设计或布局限制，其出现的频率几乎与平面文本一样多，甚至可能更多。虽然扩散模型的高级生成能力已经实现了高质量视觉文本生成，但由于训练数据限制，这些模型在给定的倾斜或弯曲的文本布局时，往往会产生失真的文本和不协调的文本背景。在本文中，我们介绍了一种新的无需训练框架STGen，它可以在具有挑战性的场景（例如倾斜或弯曲的文本布局）中准确生成视觉文本，同时协调其与文本背景的融合。我们的框架将视觉文本生成过程分解为两个分支：（i）语义校正分支，它利用模型生成平面但准确的视觉文本的能力来指导生成具有挑战性的场景。平面文本生成的潜在特征包含与文本本身及其背景相关的丰富语义信息。通过融入这些特征，我们可以纠正文本的语义信息并协调文本与其在复杂布局中的背景融合。（ii）结构注入分支，在推理过程中强化视觉文本结构。我们融入字形图像的潜在信息（富含字形结构），作为一种新的条件来加强文本结构。为了提高图像和谐度，我们还应用了一种有效的组合方法来合并先验信息，为生成提供了坚实的基础。跨越多种视觉文本布局的大量实验表明，我们的框架达到了更高的准确性和卓越的质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05892v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种无需训练的新框架STGen，能准确生成具有挑战性的场景中的视觉文本（如斜体或曲文本布局），并和谐地融入文本背景。该框架通过语义校正分支和结构注入分支来实现，前者利用模型生成平面文本的语义信息来指导复杂布局中的文本生成，后者在推理过程中强化文本结构，并通过结合字形图像的潜在信息来增强文本结构。实验证明，该框架在多种视觉文本布局上实现了较高的准确性和优质的质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>斜体或曲文本在真实世界图像中频繁出现，但现有扩散模型在生成此类文本时存在局限。</li>
<li>新框架STGen可准确生成斜体或曲文本，并和谐融入文本背景。</li>
<li>STGen通过语义校正分支利用平面文本的语义信息，用于指导复杂布局中的文本生成。</li>
<li>结构注入分支在推理过程中强化文本结构，并结合字形图像的潜在信息。</li>
<li>STGen使用有效的组合方法来合并先验信息，为生成提供坚实基础。</li>
<li>跨多种视觉文本布局的实验证明STGen具有较高的准确性和优质质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05892">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6927757dc204adc03d430d1aabfaa896.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0feb8634320ace26d462ac0dae4573f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d57003e768b0a23492d295c0283cffc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88f9e4d09683814ec22baf74bf063d5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c982e0b5802726b74bea28dfa043d961.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="StarGen-A-Spatiotemporal-Autoregression-Framework-with-Video-Diffusion-Model-for-Scalable-and-Controllable-Scene-Generation"><a href="#StarGen-A-Spatiotemporal-Autoregression-Framework-with-Video-Diffusion-Model-for-Scalable-and-Controllable-Scene-Generation" class="headerlink" title="StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion   Model for Scalable and Controllable Scene Generation"></a>StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion   Model for Scalable and Controllable Scene Generation</h2><p><strong>Authors:Shangjin Zhai, Zhichao Ye, Jialin Liu, Weijian Xie, Jiaqi Hu, Zhen Peng, Hua Xue, Danpeng Chen, Xiaomeng Wang, Lei Yang, Nan Wang, Haomin Liu, Guofeng Zhang</strong></p>
<p>Recent advances in large reconstruction and generative models have significantly improved scene reconstruction and novel view generation. However, due to compute limitations, each inference with these large models is confined to a small area, making long-range consistent scene generation challenging. To address this, we propose StarGen, a novel framework that employs a pre-trained video diffusion model in an autoregressive manner for long-range scene generation. The generation of each video clip is conditioned on the 3D warping of spatially adjacent images and the temporally overlapping image from previously generated clips, improving spatiotemporal consistency in long-range scene generation with precise pose control. The spatiotemporal condition is compatible with various input conditions, facilitating diverse tasks, including sparse view interpolation, perpetual view generation, and layout-conditioned city generation. Quantitative and qualitative evaluations demonstrate StarGen’s superior scalability, fidelity, and pose accuracy compared to state-of-the-art methods. </p>
<blockquote>
<p>最近重建和生成模型方面的进展大大提高了场景重建和新颖视角生成的能力。然而，由于计算限制，这些大型模型的每次推理都局限于一个小区域，使得大范围一致的场景生成具有挑战性。为了解决这一问题，我们提出了StarGen，这是一个采用预训练视频扩散模型的新型框架，以自回归的方式进行大范围场景生成。每个视频剪辑的生成都基于空间相邻图像的3D扭曲和先前生成的剪辑中时间上重叠的图像，提高了大范围场景生成中的时空一致性，并实现了精确的姿态控制。这种时空条件与各种输入条件兼容，能够促进包括稀疏视图插值、永久视图生成和布局控制城市生成在内的各种任务。定量和定性评估表明，StarGen在可扩展性、保真度和姿态准确性方面优于最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05763v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型重建和生成模型的最新进展极大地促进了场景重建和新颖视角生成的能力。然而，由于计算限制，这些大型模型的每次推理都局限于小范围，使得长程一致场景生成面临挑战。为解决这一问题，我们提出StarGen框架，采用预训练视频扩散模型进行长程场景生成。每个视频剪辑的生成都基于空间相邻图像的3D变换和先前生成的剪辑中时间上重叠的图像，提高了长程场景生成的时空一致性并精确控制姿态。该时空条件与各种输入条件兼容，促进多种任务，包括稀疏视图插值、永久视图生成和布局控制城市生成。评估和测试表明，StarGen在可扩展性、保真度和姿态准确性方面优于最新方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型重建和生成模型的最新进展促进了场景重建和新颖视角生成。</li>
<li>由于计算限制，现有模型在长程一致场景生成方面面临挑战。</li>
<li>StarGen框架采用预训练视频扩散模型进行长程场景生成。</li>
<li>StarGen利用3D变换和时空条件提高长程场景生成的时空一致性。</li>
<li>StarGen支持多种任务，包括稀疏视图插值、永久视图生成和布局控制城市生成。</li>
<li>StarGen在可扩展性、保真度和姿态准确性方面优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05763">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f001961806ff3abb27537a1536e6a7f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e343836fcdae9f30fd34b3b33bf427eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a058d94655e7fdf703cdb98da41424f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a07ddcc66da0e1dcef8900c3a2fddbfa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d3069487a090c15d22a7cdb8cebba7d0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Network-Diffuser-for-Placing-Scheduling-Service-Function-Chains-with-Inverse-Demonstration"><a href="#Network-Diffuser-for-Placing-Scheduling-Service-Function-Chains-with-Inverse-Demonstration" class="headerlink" title="Network Diffuser for Placing-Scheduling Service Function Chains with   Inverse Demonstration"></a>Network Diffuser for Placing-Scheduling Service Function Chains with   Inverse Demonstration</h2><p><strong>Authors:Zuyuan Zhang, Vaneet Aggarwal, Tian Lan</strong></p>
<p>Network services are increasingly managed by considering chained-up virtual network functions and relevant traffic flows, known as the Service Function Chains (SFCs). To deal with sequential arrivals of SFCs in an online fashion, we must consider two closely-coupled problems - an SFC placement problem that maps SFCs to servers&#x2F;links in the network and an SFC scheduling problem that determines when each SFC is executed. Solving the whole SFC problem targeting these two optimizations jointly is extremely challenging. In this paper, we propose a novel network diffuser using conditional generative modeling for this SFC placing-scheduling optimization. Recent advances in generative AI and diffusion models have made it possible to generate high-quality images&#x2F;videos and decision trajectories from language description. We formulate the SFC optimization as a problem of generating a state sequence for planning and perform graph diffusion on the state trajectories to enable extraction of SFC decisions, with SFC optimization constraints and objectives as conditions. To address the lack of demonstration data due to NP-hardness and exponential problem space of the SFC optimization, we also propose a novel and somewhat maverick approach – Rather than solving instances of this difficult optimization, we start with randomly-generated solutions as input, and then determine appropriate SFC optimization problems that render these solutions feasible. This inverse demonstration enables us to obtain sufficient expert demonstrations, i.e., problem-solution pairs, through further optimization. In our numerical evaluations, the proposed network diffuser outperforms learning and heuristic baselines, by $\sim$20% improvement in SFC reward and $\sim$50% reduction in SFC waiting time and blocking rate. </p>
<blockquote>
<p>网络服务越来越多地通过考虑串联的虚拟网络功能和相关流量流（称为服务功能链（SFC））来进行管理。为了在线处理SFC的连续到达，我们必须考虑两个紧密耦合的问题——SFC放置问题，将SFC映射到网络中的服务器&#x2F;链接，以及SFC调度问题，确定每个SFC的执行时间。针对这两个优化的SFC整体问题解决起来极为具有挑战性。在本文中，我们提出了一种使用条件生成模型的新型网络扩散器来解决SFC放置调度优化问题。最近生成人工智能和扩散模型的进步使得可以从语言描述生成高质量图像&#x2F;视频和决策轨迹成为可能。我们将SFC优化公式化为生成状态序列的规划问题，并在状态轨迹上执行图扩散以启用SFC决策提取，将SFC优化约束和目标作为条件。为了解决由于NP难度和SFC优化的指数问题空间而导致的演示数据缺乏的问题，我们还提出了一种新颖且有些独特的方法——我们不是解决这个困难优化的实例，而是首先从随机生成的解决方案开始作为输入，然后确定使这些解决方案可行的适当SFC优化问题。这种逆向演示使我们能够通过进一步的优化获得充足的专业演示，即问题解决方案对。在我们的数值评估中，所提出的网络扩散器在SFC奖励方面提高了约20％，在SFC等待时间和阻塞率方面减少了约50％，超过了学习和启发式基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05673v1">PDF</a> Accepted to IEEE INFOCOM 2025</p>
<p><strong>摘要</strong></p>
<p>随着网络服务越来越多地通过考虑链式虚拟网络功能和相关的流量流（称为服务功能链SFCs）进行管理，如何在线处理SFCs的连续到达成为一个重要问题。这涉及到两个紧密耦合的问题：SFC放置问题和SFC调度问题。本文将SFC优化问题表述为生成状态序列的规划问题，并利用扩散模型进行状态轨迹的图形扩散，以提取SFC决策。为了弥补因NP难度和指数级问题空间而缺乏演示数据的问题，本文采用逆向演示的方法，从随机生成的解决方案出发，确定可行的SFC优化问题。数值评估表明，所提出的网络扩散器在SFC奖励方面提高了约20％，在SFC等待时间和阻塞率方面减少了约50％，优于学习和启发式基线方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>服务功能链（SFCs）是管理服务的一种新方法，涉及将网络功能链接成链条来处理流量。</li>
<li>SFC面临两个核心问题：SFC放置和SFC调度，这两个问题需要联合优化。</li>
<li>提出了一种基于条件生成建模的网络扩散器来解决SFC优化问题。</li>
<li>利用扩散模型在状态轨迹上进行图形扩散，以生成SFC决策。</li>
<li>为了解决因NP难度和指数级问题空间而缺乏演示数据的问题，采用逆向演示方法。</li>
<li>数值评估显示，网络扩散器在SFC奖励、等待时间和阻塞率方面表现优越。</li>
<li>这种方法的优点在于，即使面对复杂的网络环境和大量的SFC请求，也能有效地进行实时优化和响应。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05673">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ccfafcdc0a28574c8460d1bc3a10f6c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca77ed34070e1cb0bdcd5ba2bc7fc29c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe9e7d9059fbbe5cb319cb91ceabf038.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1498048b4d84cb56a8c04fe13cf4817d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HFMF-Hierarchical-Fusion-Meets-Multi-Stream-Models-for-Deepfake-Detection"><a href="#HFMF-Hierarchical-Fusion-Meets-Multi-Stream-Models-for-Deepfake-Detection" class="headerlink" title="HFMF: Hierarchical Fusion Meets Multi-Stream Models for Deepfake   Detection"></a>HFMF: Hierarchical Fusion Meets Multi-Stream Models for Deepfake   Detection</h2><p><strong>Authors:Anant Mehta, Bryant McArthur, Nagarjuna Kolloju, Zhengzhong Tu</strong></p>
<p>The rapid progress in deep generative models has led to the creation of incredibly realistic synthetic images that are becoming increasingly difficult to distinguish from real-world data. The widespread use of Variational Models, Diffusion Models, and Generative Adversarial Networks has made it easier to generate convincing fake images and videos, which poses significant challenges for detecting and mitigating the spread of misinformation. As a result, developing effective methods for detecting AI-generated fakes has become a pressing concern. In our research, we propose HFMF, a comprehensive two-stage deepfake detection framework that leverages both hierarchical cross-modal feature fusion and multi-stream feature extraction to enhance detection performance against imagery produced by state-of-the-art generative AI models. The first component of our approach integrates vision Transformers and convolutional nets through a hierarchical feature fusion mechanism. The second component of our framework combines object-level information and a fine-tuned convolutional net model. We then fuse the outputs from both components via an ensemble deep neural net, enabling robust classification performances. We demonstrate that our architecture achieves superior performance across diverse dataset benchmarks while maintaining calibration and interoperability. </p>
<blockquote>
<p>随着深度生成模型的快速发展，已经能够创造出非常逼真的合成图像，这些图像与现实世界的数据越来越难以区分。变分模型、扩散模型和生成对抗网络的广泛应用使得生成令人信服的虚假图像和视频变得更加容易，这给检测和缓解虚假信息的传播带来了重大挑战。因此，开发有效的检测AI生成虚假信息的方法已成为当务之急。在我们的研究中，我们提出了HFMF，这是一个全面的两阶段深度伪造检测框架，它利用分层跨模态特征融合和多流特征提取技术，增强了对由最新生成式AI模型生成的图像的检测性能。我们的方法的第一部分是通过分层特征融合机制将视觉Transformer和卷积网络集成在一起。我们框架的第二部分结合了对象级信息和经过精细调整的卷积网络模型。然后，我们通过集成深度神经网络融合了这两个组件的输出，实现了稳健的分类性能。我们证明，我们的架构在多种数据集基准测试中实现了卓越的性能，同时保持了校准和互操作性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05631v1">PDF</a> This work is accepted to WACV 2025 Workshop on AI for Multimedia   Forensics &amp; Disinformation Detection. Code is available at:   <a target="_blank" rel="noopener" href="https://github.com/taco-group/HFMF">https://github.com/taco-group/HFMF</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了深度生成模型的快速发展，使得生成逼真的合成图像变得越来越容易，这给检测和遏制虚假信息的传播带来了巨大挑战。针对这一问题，本文提出了一种全面的两阶段深度伪造检测框架HFMF，该框架采用分层跨模态特征融合和多流特征提取技术，以增强对先进生成AI模型生成的图像的检测性能。该框架结合了视觉Transformer和卷积网络，通过集成对象级信息和精细调整的卷积网络模型，实现了稳健的分类性能，并在不同的数据集上表现出卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度生成模型的快速发展导致合成图像越来越逼真，难以与现实数据区分。</li>
<li>广泛使用的变分模型、扩散模型和生成对抗网络使得生成虚假图像和视频变得更加容易。</li>
<li>检测AI生成的虚假内容已成为紧迫的需求。</li>
<li>提出的HFMF是一个两阶段的深度伪造检测框架。</li>
<li>HFMF采用分层跨模态特征融合和多流特征提取技术。</li>
<li>框架结合了视觉Transformer和卷积网络，通过集成对象级信息实现稳健分类。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05631">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-754aae288d986a59dda5339638641125.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2aecf98bc0533214b6be2bb27c31b4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-196e978c0634403010359fe7e46f9987.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e93ceb327822b36feb413a32d4386d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8577a63940ad18d4b938ca9d8a0df7c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06d10a829caf412dd5a5c02e7495de37.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc6d0b41e8af0b73cb44f336d3a21501.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e5df784c02956bd4f7efc87292501cc.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CROPS-Model-Agnostic-Training-Free-Framework-for-Safe-Image-Synthesis-with-Latent-Diffusion-Models"><a href="#CROPS-Model-Agnostic-Training-Free-Framework-for-Safe-Image-Synthesis-with-Latent-Diffusion-Models" class="headerlink" title="CROPS: Model-Agnostic Training-Free Framework for Safe Image Synthesis   with Latent Diffusion Models"></a>CROPS: Model-Agnostic Training-Free Framework for Safe Image Synthesis   with Latent Diffusion Models</h2><p><strong>Authors:Junha Park, Ian Ryu, Jaehui Hwang, Hyungkeun Park, Jiyoon Kim, Jong-Seok Lee</strong></p>
<p>With advances in diffusion models, image generation has shown significant performance improvements. This raises concerns about the potential abuse of image generation, such as the creation of explicit or violent images, commonly referred to as Not Safe For Work (NSFW) content. To address this, the Stable Diffusion model includes several safety checkers to censor initial text prompts and final output images generated from the model. However, recent research has shown that these safety checkers have vulnerabilities against adversarial attacks, allowing them to generate NSFW images. In this paper, we find that these adversarial attacks are not robust to small changes in text prompts or input latents. Based on this, we propose CROPS (Circular or RandOm Prompts for Safety), a model-agnostic framework that easily defends against adversarial attacks generating NSFW images without requiring additional training. Moreover, we develop an approach that utilizes one-step diffusion models for efficient NSFW detection (CROPS-1), further reducing computational resources. We demonstrate the superiority of our method in terms of performance and applicability. </p>
<blockquote>
<p>随着扩散模型的进步，图像生成在性能上已显示出显著的改进。这引发了关于图像生成可能被滥用的担忧，例如生成具有明确或暴力内容的图像，通常被称为不适合工作场所（NSFW）的内容。为了解决这一问题，Stable Diffusion模型包含多个安全检查器，以对初始文本提示和模型生成的最终输出图像进行审查。然而，最近的研究表明，这些安全检查器对对抗性攻击存在漏洞，允许它们生成NSFW图像。在本文中，我们发现这些对抗性攻击对文本提示或输入潜在空间中的微小变化并不稳健。基于此，我们提出了CROPS（用于安全的循环或随机提示），这是一个模型无关框架，可以轻松防御对抗性攻击生成的NSFW图像，而无需额外的训练。此外，我们开发了一种利用单步扩散模型进行有效NSFW检测的方法（CROPS-1），进一步减少了计算资源。我们在性能和适用性方面展示了我们的方法的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05359v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着扩散模型技术的进步，图像生成性能显著提升，但也引发了关于生成不适宜工作场合（NSFW）内容的潜在滥用问题。Stable Diffusion模型采用多个安全检查器对初始文本提示和最终生成的图像进行审查。但最新研究显示，这些安全检查器易受对抗性攻击的漏洞影响，能生成NSFW图像。本文提出一种模型无关框架——CROPS（用于安全的循环或随机提示），能轻松防范生成NSFW图像的对抗性攻击，无需额外训练。此外，我们还开发了一种利用单步扩散模型进行高效NSFW检测的方法（CROPS-1），进一步降低计算资源消耗，展现了出色的性能和应用潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型的进步带动了图像生成性能的大幅提升。</li>
<li>伴随性能提升，出现了关于生成不适宜工作场合（NSFW）内容的潜在滥用问题。</li>
<li>Stable Diffusion模型采用安全检查器审查内容，但存在易受对抗性攻击漏洞的风险。</li>
<li>对抗性的攻击对文本提示或输入潜变量的微小变化很敏感。</li>
<li>CROPS框架能轻松防范生成NSFW图像的对抗性攻击，且无需额外训练。</li>
<li>CROPS-1方法利用单步扩散模型进行高效NSFW检测，降低计算资源消耗。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05359">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b9899adecacec8191476bc6558b5a0fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac79dc5a9d1d0859a333be94b835182f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d8cfcb40b3058e31a4732631d5e8e80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37146373c17580c9c96e46655db93ffc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdc7bc17db80dd8452d0a64e132f03c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17767ed24f8cb9d3578a10039043d905.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de8a55be1b7ee954865452241e7aa98b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ResPanDiff-Diffusion-Model-for-Pansharpening-by-Inferring-Residual-Inference"><a href="#ResPanDiff-Diffusion-Model-for-Pansharpening-by-Inferring-Residual-Inference" class="headerlink" title="ResPanDiff: Diffusion Model for Pansharpening by Inferring Residual   Inference"></a>ResPanDiff: Diffusion Model for Pansharpening by Inferring Residual   Inference</h2><p><strong>Authors:Shiqi Cao, Liangjian Deng, Shangqi Deng</strong></p>
<p>The implementation of diffusion-based pansharpening task is predominantly constrained by its slow inference speed, which results from numerous sampling steps. Despite the existing techniques aiming to accelerate sampling, they often compromise performance when fusing multi-source images. To ease this limitation, we introduce a novel and efficient diffusion model named Diffusion Model for Pansharpening by Inferring Residual Inference (ResPanDiff), which significantly reduces the number of diffusion steps without sacrificing the performance to tackle pansharpening task. In ResPanDiff, we innovatively propose a Markov chain that transits from noisy residuals to the residuals between the LRMS and HRMS images, thereby reducing the number of sampling steps and enhancing performance. Additionally, we design the latent space to help model extract more features at the encoding stage, Shallow Cond-Injection<del>(SC-I) to help model fetch cond-injected hidden features with higher dimensions, and loss functions to give a better guidance for the residual generation task. enabling the model to achieve superior performance in residual generation. Furthermore, experimental evaluations on pansharpening datasets demonstrate that the proposed method achieves superior outcomes compared to recent state-of-the-art</del>(SOTA) techniques, requiring only 15 sampling steps, which reduces over $90%$ step compared with the benchmark diffusion models. Our experiments also include thorough discussions and ablation studies to underscore the effectiveness of our approach. </p>
<blockquote>
<p>基于扩散的锐化任务实施主要受到其缓慢推理速度的制约，这是由于需要大量的采样步骤。尽管现有的技术旨在加速采样，但在融合多源图像时往往会损害性能。为了缓解这一限制，我们引入了一种新型高效的扩散模型，名为通过推断残差推理的锐化扩散模型（ResPanDiff）。ResPanDiff显著减少了扩散步骤的数量，同时不牺牲性能来解决锐化任务。在ResPanDiff中，我们创新地提出了一个马尔可夫链，该链从噪声残差过渡到低分辨率图像（LRMS）和高分辨率图像（HRMS）之间的残差，从而减少了采样步骤的数量并提高了性能。此外，我们设计了潜在空间以帮助模型在编码阶段提取更多特征，浅层条件注入（SC-I）以帮助模型获取具有更高维度的条件注入隐藏特征，以及损失函数以更好地指导残差生成任务。这使得模型在残差生成方面实现卓越性能。此外，在锐化数据集上的实验评估表明，所提出的方法与最新的最先进的（SOTA）技术相比取得了优越的结果，仅需15个采样步骤，与基准扩散模型相比减少了超过90％的步骤。我们的实验还包括深入的讨论和消融研究，以强调我们方法的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05091v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为ResPanDiff的新型扩散模型，用于解决图像超分辨率中的pansharpening任务。该模型通过引入残差推断来加速采样过程，减少扩散步骤数量，同时不牺牲性能。此外，该模型还采用了一系列技术，如Markov链、潜空间设计、浅层条件注入（SC-I）和特定的损失函数，以优化特征提取和残差生成任务。实验证明，该模型在pansharpening数据集上取得了优于最新技术成果的结果，仅需要15个采样步骤，与基准扩散模型相比减少了超过90%的步骤。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ResPanDiff是一种高效的扩散模型，用于解决pansharpening任务。</li>
<li>该模型通过引入残差推断来加速采样过程。</li>
<li>ResPanDiff减少扩散步骤数量，同时不牺牲性能。</li>
<li>模型采用Markov链过渡从噪声残差到LRMS和HRMS图像之间的残差，增强性能。</li>
<li>潜空间设计和浅层条件注入（SC-I）技术帮助模型更有效地提取特征和条件注入隐藏特征。</li>
<li>特定的损失函数为残差生成任务提供更好的指导。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05091">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-69541765ebc044144a947967a4e2ca1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b10f3139bebbb9de90c659f17ee10e82.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-838894a85bc4d9c2b58d080c0d7830a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cc20dd622eee459319f25caecc2016d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d7f79a54d328e054d15009cc0fa485f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-356908a819b64b50f37ecde771c6e0a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cac2769d3f7cf4fa64308bda1dbd3ab3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12aa1c77950b230f683ceb076aad72b3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="D3RM-A-Discrete-Denoising-Diffusion-Refinement-Model-for-Piano-Transcription"><a href="#D3RM-A-Discrete-Denoising-Diffusion-Refinement-Model-for-Piano-Transcription" class="headerlink" title="D3RM: A Discrete Denoising Diffusion Refinement Model for Piano   Transcription"></a>D3RM: A Discrete Denoising Diffusion Refinement Model for Piano   Transcription</h2><p><strong>Authors:Hounsu Kim, Taegyun Kwon, Juhan Nam</strong></p>
<p>Diffusion models have been widely used in the generative domain due to their convincing performance in modeling complex data distributions. Moreover, they have shown competitive results on discriminative tasks, such as image segmentation. While diffusion models have also been explored for automatic music transcription, their performance has yet to reach a competitive level. In this paper, we focus on discrete diffusion model’s refinement capabilities and present a novel architecture for piano transcription. Our model utilizes Neighborhood Attention layers as the denoising module, gradually predicting the target high-resolution piano roll, conditioned on the finetuned features of a pretrained acoustic model. To further enhance refinement, we devise a novel strategy which applies distinct transition states during training and inference stage of discrete diffusion models. Experiments on the MAESTRO dataset show that our approach outperforms previous diffusion-based piano transcription models and the baseline model in terms of F1 score. Our code is available in <a target="_blank" rel="noopener" href="https://github.com/hanshounsu/d3rm">https://github.com/hanshounsu/d3rm</a>. </p>
<blockquote>
<p>扩散模型由于其在复杂数据分布建模方面的出色表现，在生成领域得到了广泛应用。此外，它们在判别任务（如图像分割）上也取得了具有竞争力的结果。虽然扩散模型也被探索用于自动音乐转录，但其性能尚未达到竞争水平。在本文中，我们重点关注离散扩散模型的细化能力，并为钢琴转录提出了一种新型架构。我们的模型使用邻域注意力层作为去噪模块，基于预训练声学模型的微调特征，逐步预测目标高分辨率钢琴卷。为了进一步提高细化能力，我们设计了一种新型策略，在离散扩散模型的训练和推理阶段应用不同的过渡状态。在MAESTRO数据集上的实验表明，我们的方法在F1分数方面优于之前的基于扩散的钢琴转录模型和基线模型。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/hanshounsu/d3rm%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hanshounsu/d3rm中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05068v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了扩散模型在生成领域的应用及其在复杂数据分布建模中的出色表现。此外，论文还重点探讨了离散扩散模型在钢琴音乐转录方面的优化能力，并提出了一种新型架构。该模型采用邻域注意力层作为去噪模块，逐渐预测目标高分辨率钢琴乐谱，并使用预训练声学模型的微调特征进行条件处理。实验表明，该方法在MAESTRO数据集上的表现优于之前的扩散模型钢琴转录方法和基线模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在生成领域具有广泛的应用，特别是在复杂数据分布的建模方面表现出令人信服的效果。</li>
<li>离散扩散模型在钢琴音乐转录方面展现出优化能力。</li>
<li>提出了一种新型钢琴音乐转录模型架构，采用邻域注意力层作为去噪模块。</li>
<li>模型能逐渐预测目标高分辨率钢琴乐谱，基于预训练声学模型的微调特征进行条件处理。</li>
<li>提出了一种在训练和推理阶段应用不同过渡状态的新型策略，以进一步增强模型的优化效果。</li>
<li>在MAESTRO数据集上的实验表明，该方法优于其他扩散模型钢琴转录方法和基线模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05068">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0964bffbf1c1d6192cc6180e46eb1f87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44a494d972de7b54258131ca8a6c6ba2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92d3dd22fcd9d11ebc0f706a7dfe86a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f2016f224be32580eb20bd95e37c89a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7425f69833b2da851f65418d3858aedf.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MC-VTON-Minimal-Control-Virtual-Try-On-Diffusion-Transformer"><a href="#MC-VTON-Minimal-Control-Virtual-Try-On-Diffusion-Transformer" class="headerlink" title="MC-VTON: Minimal Control Virtual Try-On Diffusion Transformer"></a>MC-VTON: Minimal Control Virtual Try-On Diffusion Transformer</h2><p><strong>Authors:Junsheng Luan, Guangyuan Li, Lei Zhao, Wei Xing</strong></p>
<p>Virtual try-on methods based on diffusion models achieve realistic try-on effects. They use an extra reference network or an additional image encoder to process multiple conditional image inputs, which adds complexity pre-processing and additional computational costs. Besides, they require more than 25 inference steps, bringing longer inference time. In this work, with the development of diffusion transformer (DiT), we rethink the necessity of additional reference network or image encoder and introduce MC-VTON, which leverages DiT’s intrinsic backbone to seamlessly integrate minimal conditional try-on inputs. Compared to existing methods, the superiority of MC-VTON is demonstrated in four aspects: (1) Superior detail fidelity. Our DiT-based MC-VTON exhibits superior fidelity in preserving fine-grained details. (2) Simplified network and inputs. We remove any extra reference network or image encoder. We also remove unnecessary conditions like the long prompt, pose estimation, human parsing, and depth map. We require only the masked person image and the garment image. (3) Parameter-efficient training. To process the try-on task, we fine-tune the FLUX.1-dev with only 39.7M additional parameters (0.33% of the backbone parameters). (4) Less inference steps. We apply distillation diffusion on MC-VTON and only need 8 steps to generate a realistic try-on image, with only 86.8M additional parameters (0.72% of the backbone parameters). Experiments show that MC-VTON achieves superior qualitative and quantitative results with fewer condition inputs, trainable parameters, and inference steps than baseline methods. </p>
<blockquote>
<p>基于扩散模型的虚拟试穿方法实现了逼真的试穿效果。它们使用额外的参考网络或图像编码器来处理多个条件图像输入，这增加了预处理和额外的计算成本。此外，它们需要超过25步推理，导致推理时间延长。在这项工作中，随着扩散变压器（DiT）的发展，我们重新思考了额外参考网络或图像编码器的必要性，并引入了MC-VTON，它利用DiT的内在骨干来无缝集成最少的条件试穿输入。与现有方法相比，MC-VTON在四个方面表现出优越性：（1）出色的细节保真度。我们基于DiT的MC-VTON在保持细节方面表现出优越的保真度。（2）简化的网络和输入。我们移除了任何额外的参考网络或图像编码器。我们还移除了不必要的条件，如长提示、姿势估计、人体解析和深度图。我们只需要遮罩的人物图像和服装图像。（3）参数高效的训练。为了处理试穿任务，我们只使用39.7M额外参数（占主干参数的0.33%）对FLUX.1-dev进行微调。（4）较少的推理步骤。我们对MC-VTON应用蒸馏扩散，只需8步即可生成逼真的试穿图像，只需额外的86.8M参数（占主干参数的0.72%）。实验表明，MC-VTON在条件输入、可训练参数和推理步骤方面比基准方法更少，但定性定量结果更优越。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03630v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于扩散模型的虚拟试穿方法能实现逼真的试穿效果。它们使用额外的参考网络或图像编码器来处理多个条件图像输入，增加了预处理和额外的计算成本。此外，它们需要超过25步推理，导致推理时间较长。本研究通过发展扩散转换器（DiT）重新思考了额外参考网络或图像编码器的必要性，并引入了MC-VTON，它利用DiT的内在骨架无缝集成了最少的条件试穿输入。相比现有方法，MC-VTON在四个方面表现出优越性：细节保真度更高、网络及输入简化、参数训练效率更高、推理步骤更少。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>虚拟试穿方法基于扩散模型实现逼真效果。</li>
<li>以往方法使用额外网络或编码器处理多条件图像，增加复杂性和计算成本。</li>
<li>MC-VTON引入扩散转换器（DiT），无需额外网络和编码器。</li>
<li>MC-VTON仅需要遮罩的人物图像和服装图像作为输入。</li>
<li>MC-VTON参数训练高效，只需39.7M额外参数。</li>
<li>MC-VTON通过蒸馏扩散技术，仅需8步生成现实试穿图像。</li>
<li>实验显示，MC-VTON在条件输入、可训练参数和推理步骤方面均优于基准方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03630">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a4e29958b99c66eb9f14c08d2d44b039.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fe3801031268c545ff0735cbb612cae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ed6d44f84b4a110fac6be547bf1af48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82488f36cdeeb2aebd92fe66f709ab10.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-911f2c2ef1648557b12dc043e124a168.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="K-space-Diffusion-Model-Based-MR-Reconstruction-Method-for-Simultaneous-Multislice-Imaging"><a href="#K-space-Diffusion-Model-Based-MR-Reconstruction-Method-for-Simultaneous-Multislice-Imaging" class="headerlink" title="K-space Diffusion Model Based MR Reconstruction Method for Simultaneous   Multislice Imaging"></a>K-space Diffusion Model Based MR Reconstruction Method for Simultaneous   Multislice Imaging</h2><p><strong>Authors:Ting Zhao, Zhuoxu Cui, Congcong Liu, Xingyang Wu, Yihang Zhou, Dong Liang, Haifeng Wang</strong></p>
<p>Simultaneous Multi-Slice(SMS) is a magnetic resonance imaging (MRI) technique which excites several slices concurrently using multiband radiofrequency pulses to reduce scanning time. However, due to its variable data structure and difficulty in acquisition, it is challenging to integrate SMS data as training data into deep learning frameworks.This study proposed a novel k-space diffusion model of SMS reconstruction that does not utilize SMS data for training. Instead, it incorporates Slice GRAPPA during the sampling process to reconstruct SMS data from different acquisition modes.Our results demonstrated that this method outperforms traditional SMS reconstruction methods and can achieve higher acceleration factors without in-plane aliasing. </p>
<blockquote>
<p>同步多切片（Simultaneous Multi-Slice，简称SMS）是一种磁共振成像（MRI）技术。它通过多频带射频脉冲同时激发多个切片，以减少扫描时间。然而，由于其数据结构多变，采集难度高，将SMS数据作为训练数据集成到深度学习框架中是一项挑战。本研究提出了一种新型的k空间扩散模型用于SMS重建，该模型不直接使用SMS数据进行训练。相反，它在采样过程中采用了切片GRAPPA（Slice GRAPPA），可以从不同的采集模式重建SMS数据。我们的结果表明，该方法优于传统的SMS重建方法，并且可以在没有平面混淆的情况下实现更高的加速因子。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03293v2">PDF</a> Accepted at the 2025 IEEE 22nd International Symposium on Biomedical   Imaging (ISBI)</p>
<p><strong>Summary</strong><br>     该研究提出了一种新型的k空间扩散模型用于SMS重建，该方法不直接使用SMS数据进行训练，而是在采样过程中融入Slice GRAPPA技术以从不同采集模式重建SMS数据。研究结果显示，该方法优于传统SMS重建方法，能够在不出现平面混叠的情况下实现更高的加速因子。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SMS是MRI的一种技术，通过多频带射频脉冲同时激发多个切片来缩短扫描时间。</li>
<li>将SMS数据作为训练数据集成到深度学习框架中面临挑战，因为其数据结构可变且采集困难。</li>
<li>本研究提出了一种新型的k空间扩散模型用于SMS重建。</li>
<li>该模型不直接使用SMS数据进行训练，而是在采样过程中融入Slice GRAPPA技术。</li>
<li>该方法实现了更高的加速因子，并且避免了平面混叠的问题。</li>
<li>研究结果表明，该方法在性能上超越了传统的SMS重建方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03293">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5946111ea1f95d72734c516345b2f0db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22aa40318d9f0fa2b883f57d7e45a9f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19c3eefe8af4e777ca1cde3b1570943d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b85ce8e7cd121bc67c268889b6930d21.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f65a0f70d67dfb12ff1a59d439836b1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Factorized-Diffusion-Perceptual-Illusions-by-Noise-Decomposition"><a href="#Factorized-Diffusion-Perceptual-Illusions-by-Noise-Decomposition" class="headerlink" title="Factorized Diffusion: Perceptual Illusions by Noise Decomposition"></a>Factorized Diffusion: Perceptual Illusions by Noise Decomposition</h2><p><strong>Authors:Daniel Geng, Inbum Park, Andrew Owens</strong></p>
<p>Given a factorization of an image into a sum of linear components, we present a zero-shot method to control each individual component through diffusion model sampling. For example, we can decompose an image into low and high spatial frequencies and condition these components on different text prompts. This produces hybrid images, which change appearance depending on viewing distance. By decomposing an image into three frequency subbands, we can generate hybrid images with three prompts. We also use a decomposition into grayscale and color components to produce images whose appearance changes when they are viewed in grayscale, a phenomena that naturally occurs under dim lighting. And we explore a decomposition by a motion blur kernel, which produces images that change appearance under motion blurring. Our method works by denoising with a composite noise estimate, built from the components of noise estimates conditioned on different prompts. We also show that for certain decompositions, our method recovers prior approaches to compositional generation and spatial control. Finally, we show that we can extend our approach to generate hybrid images from real images. We do this by holding one component fixed and generating the remaining components, effectively solving an inverse problem. </p>
<blockquote>
<p>给定一张图像被分解为一系列线性组件的总和，我们提出了一种零样本方法，通过扩散模型采样控制每个单独组件。例如，我们可以将图像分解为低空间频率和高空间频率，并根据不同的文本提示调整这些组件。这产生了混合图像，其外观会根据观看距离而变化。通过将图像分解为三个频率子带，我们可以使用三个提示生成混合图像。我们还通过使用灰度与彩色组件的分解来生成在灰度查看时外观会改变的图像，这是在昏暗光线下的自然发生的现象。我们还通过运动模糊核进行分解，产生在运动模糊下外观会改变的图像。我们的方法是通过使用基于不同提示条件组合的噪声估计去噪来工作的。我们还表明，对于某些分解，我们的方法能够恢复先前关于组合生成和空间控制的方法。最后，我们展示了我们能够将我们的方法扩展到从真实图像生成混合图像。这是通过将某个组件固定并生成其余组件来实现的，有效地解决了一个反问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.11615v2">PDF</a> ECCV 2024 camera ready version + more readable size</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于扩散模型的零样本方法，通过分解图像为多个线性组件，并利用文本提示控制各个组件，生成混合图像。这种技术可以分解图像为不同频率子带和灰度与彩色组件，并生成在不同观看条件下呈现不同外观的混合图像。此外，文章还探索了基于运动模糊核的分解方法，并展示了从真实图像生成混合图像的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种零样本方法，利用扩散模型控制图像的各个线性组件。</li>
<li>图像可以被分解为不同频率子带（如低、高空间频率），并依据不同的文本提示进行条件控制。</li>
<li>生成混合图像，其外观随观看距离变化。</li>
<li>使用灰度与彩色组件分解，生成在灰度模式下呈现不同外观的图像。</li>
<li>通过组合噪声估计值的分解方法，实现图像去噪。</li>
<li>在某些分解下，此方法能够恢复先前的组合生成方法和空间控制方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.11615">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fe5c5f256fca0c8512d959b56c2e558c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb722e73b6fcd94df9d01bac9fcdf0a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-145496b139981e04cb5f9cc28ddcd16b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Magic-Boost-Boost-3D-Generation-with-Multi-View-Conditioned-Diffusion"><a href="#Magic-Boost-Boost-3D-Generation-with-Multi-View-Conditioned-Diffusion" class="headerlink" title="Magic-Boost: Boost 3D Generation with Multi-View Conditioned Diffusion"></a>Magic-Boost: Boost 3D Generation with Multi-View Conditioned Diffusion</h2><p><strong>Authors:Fan Yang, Jianfeng Zhang, Yichun Shi, Bowen Chen, Chenxu Zhang, Huichao Zhang, Xiaofeng Yang, Xiu Li, Jiashi Feng, Guosheng Lin</strong></p>
<p>Benefiting from the rapid development of 2D diffusion models, 3D content generation has witnessed significant progress. One promising solution is to finetune the pre-trained 2D diffusion models to produce multi-view images and then reconstruct them into 3D assets via feed-forward sparse-view reconstruction models. However, limited by the 3D inconsistency in the generated multi-view images and the low reconstruction resolution of the feed-forward reconstruction models, the generated 3d assets are still limited to incorrect geometries and blurry textures. To address this problem, we present a multi-view based refine method, named Magic-Boost, to further refine the generation results. In detail, we first propose a novel multi-view conditioned diffusion model which extracts 3d prior from the synthesized multi-view images to synthesize high-fidelity novel view images and then introduce a novel iterative-update strategy to adopt it to provide precise guidance to refine the coarse generated results through a fast optimization process. Conditioned on the strong 3d priors extracted from the synthesized multi-view images, Magic-Boost is capable of providing precise optimization guidance that well aligns with the coarse generated 3D assets, enriching the local detail in both geometry and texture within a short time ($\sim15$min). Extensive experiments show Magic-Boost greatly enhances the coarse generated inputs, generates high-quality 3D assets with rich geometric and textural details. (Project Page: <a target="_blank" rel="noopener" href="https://magic-research.github.io/magic-boost/">https://magic-research.github.io/magic-boost/</a>) </p>
<blockquote>
<p>得益于二维扩散模型的快速发展，三维内容生成也取得了重大进展。一种有前途的解决方案是通过微调预训练的二维扩散模型来生成多视角图像，然后通过前馈稀疏视图重建模型将它们重建为三维资产。然而，由于生成的多视角图像中的三维不一致性和前馈重建模型较低的重建分辨率，生成的3D资产仍然存在几何错误和纹理模糊。为了解决这个问题，我们提出了一种基于多视角的细化方法，名为Magic-Boost，进一步改进了生成结果。具体来说，我们首先提出了一种新型的多视角条件扩散模型，该模型从合成的多视角图像中提取三维先验知识，合成高保真度的新视角图像，然后引入了一种新型迭代更新策略，将其应用于通过快速优化过程对粗略生成的结果进行精细指导。基于从合成多视角图像中提取的强大三维先验知识，Magic-Boost能够提供精确的优化指导，与粗略生成的3D资产高度对齐，在短时间内（~15分钟）丰富几何和纹理的局部细节。大量实验表明，Magic-Boost大大增强了粗略生成的输入，生成了高质量、具有丰富几何和纹理细节的3D资产。（项目页面：<a target="_blank" rel="noopener" href="https://magic-research.github.io/magic-boost/%EF%BC%89">https://magic-research.github.io/magic-boost/）</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.06429v3">PDF</a> </p>
<p><strong>Summary</strong><br>     得益于2D扩散模型的快速发展，3D内容生成领域已取得了显著进步。一种有前途的解决方案是通过微调预训练的2D扩散模型以生成多视角图像，然后通过前馈稀疏视图重建模型将其重建为3D资产。然而，由于生成的多视角图像中的3D不一致性和前馈重建模型的低重建分辨率限制，生成的3D资产仍存在几何不正确和纹理模糊的问题。为解决此问题，我们提出了一种基于多视角的细化方法Magic-Boost，以进一步优化生成结果。具体而言，我们首先提出一种新型的多视角条件扩散模型，从合成的多视角图像中提取3D先验知识来合成高保真度的新视角图像，并引入一种新型迭代更新策略来适应它，以通过快速优化过程为粗糙的生成结果提供精确的指导。基于从合成多视角图像中提取的强大3D先验知识，Magic-Boost能够提供与粗糙生成的3D资产高度吻合的精确优化指导，在短时间内（~15分钟）丰富几何和纹理的局部细节。大量实验表明，Magic-Boost极大地提高了粗糙的生成输入，生成了高质量、细节丰富的3D资产。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D内容生成领域受益于2D扩散模型的快速发展，已经取得显著进步。</li>
<li>通过微调预训练的2D扩散模型生成多视角图像，然后通过前馈稀疏视图重建模型将其转化为3D资产是一种有前途的解决方案。</li>
<li>当前方法面临生成的多视角图像中的3D不一致性和前馈重建模型的低重建分辨率的问题，导致生成的3D资产存在几何不正确和纹理模糊的问题。</li>
<li>Magic-Boost是一种基于多视角的细化方法，旨在解决上述问题，通过提取多视角图像的3D先验知识来优化生成结果。</li>
<li>Magic-Boost合成高保真度的新视角图像，并引入新型迭代更新策略来精确指导优化过程。</li>
<li>Magic-Boost利用强大的3D先验知识，在短时间内丰富几何和纹理的局部细节。</li>
<li>实验证明Magic-Boost能显著提高生成质量，生成细节丰富的3D资产。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.06429">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-df62ca8b7d54831419eafd26744effe2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c42c826143996d4d401b6904c54cbe66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b8988b07e744eb3dc19c9fc90f6a974.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a374cf29144fd1c01be3d35c827197a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f0f33a094a9658ff5689cf67fb9df83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e9340ded73d0a52eed397d9a332c8ff.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Discriminative-Class-Tokens-for-Text-to-Image-Diffusion-Models"><a href="#Discriminative-Class-Tokens-for-Text-to-Image-Diffusion-Models" class="headerlink" title="Discriminative Class Tokens for Text-to-Image Diffusion Models"></a>Discriminative Class Tokens for Text-to-Image Diffusion Models</h2><p><strong>Authors:Idan Schwartz, Vésteinn Snæbjarnarson, Hila Chefer, Ryan Cotterell, Serge Belongie, Lior Wolf, Sagie Benaim</strong></p>
<p>Recent advances in text-to-image diffusion models have enabled the generation of diverse and high-quality images. While impressive, the images often fall short of depicting subtle details and are susceptible to errors due to ambiguity in the input text. One way of alleviating these issues is to train diffusion models on class-labeled datasets. This approach has two disadvantages: (i) supervised datasets are generally small compared to large-scale scraped text-image datasets on which text-to-image models are trained, affecting the quality and diversity of the generated images, or (ii) the input is a hard-coded label, as opposed to free-form text, limiting the control over the generated images.   In this work, we propose a non-invasive fine-tuning technique that capitalizes on the expressive potential of free-form text while achieving high accuracy through discriminative signals from a pretrained classifier. This is done by iteratively modifying the embedding of an added input token of a text-to-image diffusion model, by steering generated images toward a given target class according to a classifier. Our method is fast compared to prior fine-tuning methods and does not require a collection of in-class images or retraining of a noise-tolerant classifier. We evaluate our method extensively, showing that the generated images are: (i) more accurate and of higher quality than standard diffusion models, (ii) can be used to augment training data in a low-resource setting, and (iii) reveal information about the data used to train the guiding classifier. The code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/idansc/discriminative_class_tokens%7D">https://github.com/idansc/discriminative_class_tokens}</a>. </p>
<blockquote>
<p>最近文本到图像扩散模型的进步使得能够生成多样且高质量的图像。虽然令人印象深刻，但这些图像往往无法描绘出细微的细节，并且由于输入文本的模糊性，容易出现错误。缓解这些问题的一种方法是使用类别标签数据集训练扩散模型。这种方法有两个缺点：一是与用于训练文本到图像模型的大规模爬取的文本图像数据集相比，监督数据集通常规模较小，影响生成图像的质量和多样性；二是输入是硬编码的标签，而不是自由形式的文本，限制了生成图像的控制性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2303.17155v4">PDF</a> ICCV 2023</p>
<p><strong>摘要</strong><br>    近期文本到图像扩散模型的进展使得生成多样且高质量的图像成为可能。然而，生成的图像往往缺乏细微的细节描绘，并且由于输入文本的歧义而容易出现错误。为缓解这些问题，一种方法是使用分类标签数据集训练扩散模型。但这种方法存在两个缺点：一是监督数据集通常比用于训练文本到图像模型的大规模抓取文本图像数据集要小，影响生成图像的质量和多样性；二是输入是硬编码标签，而非自由形式的文本，限制了生成图像的控制能力。在此工作中，我们提出了一种非侵入式微调技术，该技术利用自由形式文本的表达能力，并通过预训练分类器的判别信号实现高精度。这是通过迭代修改文本到图像扩散模型中添加输入标记的嵌入来实现的，将生成的图像导向给定的目标类别，并根据分类器进行调整。我们的方法与先前的微调方法相比速度更快，并且不需要收集同类图像或重新训练噪声容忍分类器。我们进行了广泛的方法评估，证明生成的图像比标准扩散模型更准确、质量更高，可用于增强低资源设置中的训练数据，并揭示用于训练指导分类器的数据的信息。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>文本到图像扩散模型的最新进展能够生成多样且高质量的图像，但在描绘细微细节方面存在不足，且易受输入文本歧义的影响。</li>
<li>使用分类标签数据集训练扩散模型的方法存在两个主要缺点：数据集大小有限，以及输入形式限制（硬编码标签而非自由形式文本），这限制了生成图像的控制能力。</li>
<li>提出了一种非侵入式微调技术，结合自由形式文本的表达能力与预训练分类器的判别信号，以提高生成图像的质量和准确性。</li>
<li>该方法通过迭代修改文本到图像扩散模型中增加输入标记的嵌入来实现，使生成的图像能够导向特定类别，并根据分类器进行调整。</li>
<li>与其他微调方法相比，该方法具有更快的速度，且不需要收集同类图像或重新训练噪声容忍分类器。</li>
<li>评估表明，该方法生成的图像具有更高的准确性和质量，可用于增强低资源环境中的训练数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2303.17155">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dfbb62be9f6ac0d1266dbea49808349f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efb094756f3ab73c14f670da0f0dec99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44b49bfb4f4d4f53dc7c5e70bfa8ea65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dabe42fb5508dd9b7d1736b982fb42ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e83d6710a2f661028c9e19dbe9e6a69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53e9c87c8e54fd33ce4ce3ae444e0eac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07d211abe5ac618ce65bbc60a108aa50.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-14/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-14/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-853b531abe72fe60dae09301d06339e9.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-01-14  Real-Time Textless Dialogue Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a5dd8467341555d03e377166d4f53cfd.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-01-14  Low-Resource Text-to-Speech Synthesis Using Noise-Augmented Training of   ForwardTacotron
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26254.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
