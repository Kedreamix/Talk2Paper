<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-14  Beyond Flat Text Dual Self-inherited Guidance for Visual Text   Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1498048b4d84cb56a8c04fe13cf4817d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    49 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-14-æ›´æ–°"><a href="#2025-01-14-æ›´æ–°" class="headerlink" title="2025-01-14 æ›´æ–°"></a>2025-01-14 æ›´æ–°</h1><h2 id="Beyond-Flat-Text-Dual-Self-inherited-Guidance-for-Visual-Text-Generation"><a href="#Beyond-Flat-Text-Dual-Self-inherited-Guidance-for-Visual-Text-Generation" class="headerlink" title="Beyond Flat Text: Dual Self-inherited Guidance for Visual Text   Generation"></a>Beyond Flat Text: Dual Self-inherited Guidance for Visual Text   Generation</h2><p><strong>Authors:Minxing Luo, Zixun Xia, Liaojun Chen, Zhenhang Li, Weichao Zeng, Jianye Wang, Wentao Cheng, Yaxing Wang, Yu Zhou, Jian Yang</strong></p>
<p>In real-world images, slanted or curved texts, especially those on cans, banners, or badges, appear as frequently, if not more so, than flat texts due to artistic design or layout constraints. While high-quality visual text generation has become available with the advanced generative capabilities of diffusion models, these models often produce distorted text and inharmonious text background when given slanted or curved text layouts due to training data limitation. In this paper, we introduce a new training-free framework, STGen, which accurately generates visual texts in challenging scenarios (\eg, slanted or curved text layouts) while harmonizing them with the text background. Our framework decomposes the visual text generation process into two branches: (i) \textbf{Semantic Rectification Branch}, which leverages the ability in generating flat but accurate visual texts of the model to guide the generation of challenging scenarios. The generated latent of flat text is abundant in accurate semantic information related both to the text itself and its background. By incorporating this, we rectify the semantic information of the texts and harmonize the integration of the text with its background in complex layouts. (ii) \textbf{Structure Injection Branch}, which reinforces the visual text structure during inference. We incorporate the latent information of the glyph image, rich in glyph structure, as a new condition to further strengthen the text structure. To enhance image harmony, we also apply an effective combination method to merge the priors, providing a solid foundation for generation. Extensive experiments across a variety of visual text layouts demonstrate that our framework achieves superior accuracy and outstanding quality. </p>
<blockquote>
<p>åœ¨ç°å®ä¸–ç•Œçš„å›¾åƒä¸­ï¼Œå€¾æ–œæˆ–å¼¯æ›²çš„æ–‡æœ¬ï¼Œç‰¹åˆ«æ˜¯åœ¨ç½å¤´ã€æ¨ªå¹…æˆ–å¾½ç« ä¸Šçš„æ–‡æœ¬ï¼Œç”±äºè‰ºæœ¯è®¾è®¡æˆ–å¸ƒå±€é™åˆ¶ï¼Œå…¶å‡ºç°çš„é¢‘ç‡å‡ ä¹ä¸å¹³é¢æ–‡æœ¬ä¸€æ ·å¤šï¼Œç”šè‡³å¯èƒ½æ›´å¤šã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹çš„é«˜çº§ç”Ÿæˆèƒ½åŠ›å·²ç»å®ç°äº†é«˜è´¨é‡è§†è§‰æ–‡æœ¬ç”Ÿæˆï¼Œä½†ç”±äºè®­ç»ƒæ•°æ®é™åˆ¶ï¼Œè¿™äº›æ¨¡å‹åœ¨ç»™å®šçš„å€¾æ–œæˆ–å¼¯æ›²çš„æ–‡æœ¬å¸ƒå±€æ—¶ï¼Œå¾€å¾€ä¼šäº§ç”Ÿå¤±çœŸçš„æ–‡æœ¬å’Œä¸åè°ƒçš„æ–‡æœ¬èƒŒæ™¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒæ¡†æ¶STGenï¼Œå®ƒå¯ä»¥åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ï¼ˆä¾‹å¦‚å€¾æ–œæˆ–å¼¯æ›²çš„æ–‡æœ¬å¸ƒå±€ï¼‰ä¸­å‡†ç¡®ç”Ÿæˆè§†è§‰æ–‡æœ¬ï¼ŒåŒæ—¶åè°ƒå…¶ä¸æ–‡æœ¬èƒŒæ™¯çš„èåˆã€‚æˆ‘ä»¬çš„æ¡†æ¶å°†è§†è§‰æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹åˆ†è§£ä¸ºä¸¤ä¸ªåˆ†æ”¯ï¼šï¼ˆiï¼‰è¯­ä¹‰æ ¡æ­£åˆ†æ”¯ï¼Œå®ƒåˆ©ç”¨æ¨¡å‹ç”Ÿæˆå¹³é¢ä½†å‡†ç¡®çš„è§†è§‰æ–‡æœ¬çš„èƒ½åŠ›æ¥æŒ‡å¯¼ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ã€‚å¹³é¢æ–‡æœ¬ç”Ÿæˆçš„æ½œåœ¨ç‰¹å¾åŒ…å«ä¸æ–‡æœ¬æœ¬èº«åŠå…¶èƒŒæ™¯ç›¸å…³çš„ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡èå…¥è¿™äº›ç‰¹å¾ï¼Œæˆ‘ä»¬å¯ä»¥çº æ­£æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯å¹¶åè°ƒæ–‡æœ¬ä¸å…¶åœ¨å¤æ‚å¸ƒå±€ä¸­çš„èƒŒæ™¯èåˆã€‚ï¼ˆiiï¼‰ç»“æ„æ³¨å…¥åˆ†æ”¯ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼ºåŒ–è§†è§‰æ–‡æœ¬ç»“æ„ã€‚æˆ‘ä»¬èå…¥å­—å½¢å›¾åƒçš„æ½œåœ¨ä¿¡æ¯ï¼ˆå¯Œå«å­—å½¢ç»“æ„ï¼‰ï¼Œä½œä¸ºä¸€ç§æ–°çš„æ¡ä»¶æ¥åŠ å¼ºæ–‡æœ¬ç»“æ„ã€‚ä¸ºäº†æé«˜å›¾åƒå’Œè°åº¦ï¼Œæˆ‘ä»¬è¿˜åº”ç”¨äº†ä¸€ç§æœ‰æ•ˆçš„ç»„åˆæ–¹æ³•æ¥åˆå¹¶å…ˆéªŒä¿¡æ¯ï¼Œä¸ºç”Ÿæˆæä¾›äº†åšå®çš„åŸºç¡€ã€‚è·¨è¶Šå¤šç§è§†è§‰æ–‡æœ¬å¸ƒå±€çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è¾¾åˆ°äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œå“è¶Šçš„è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05892v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°æ¡†æ¶STGenï¼Œèƒ½å‡†ç¡®ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­çš„è§†è§‰æ–‡æœ¬ï¼ˆå¦‚æ–œä½“æˆ–æ›²æ–‡æœ¬å¸ƒå±€ï¼‰ï¼Œå¹¶å’Œè°åœ°èå…¥æ–‡æœ¬èƒŒæ™¯ã€‚è¯¥æ¡†æ¶é€šè¿‡è¯­ä¹‰æ ¡æ­£åˆ†æ”¯å’Œç»“æ„æ³¨å…¥åˆ†æ”¯æ¥å®ç°ï¼Œå‰è€…åˆ©ç”¨æ¨¡å‹ç”Ÿæˆå¹³é¢æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯æ¥æŒ‡å¯¼å¤æ‚å¸ƒå±€ä¸­çš„æ–‡æœ¬ç”Ÿæˆï¼Œåè€…åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼ºåŒ–æ–‡æœ¬ç»“æ„ï¼Œå¹¶é€šè¿‡ç»“åˆå­—å½¢å›¾åƒçš„æ½œåœ¨ä¿¡æ¯æ¥å¢å¼ºæ–‡æœ¬ç»“æ„ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§è§†è§‰æ–‡æœ¬å¸ƒå±€ä¸Šå®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œä¼˜è´¨çš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–œä½“æˆ–æ›²æ–‡æœ¬åœ¨çœŸå®ä¸–ç•Œå›¾åƒä¸­é¢‘ç¹å‡ºç°ï¼Œä½†ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆæ­¤ç±»æ–‡æœ¬æ—¶å­˜åœ¨å±€é™ã€‚</li>
<li>æ–°æ¡†æ¶STGenå¯å‡†ç¡®ç”Ÿæˆæ–œä½“æˆ–æ›²æ–‡æœ¬ï¼Œå¹¶å’Œè°èå…¥æ–‡æœ¬èƒŒæ™¯ã€‚</li>
<li>STGené€šè¿‡è¯­ä¹‰æ ¡æ­£åˆ†æ”¯åˆ©ç”¨å¹³é¢æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯ï¼Œç”¨äºæŒ‡å¯¼å¤æ‚å¸ƒå±€ä¸­çš„æ–‡æœ¬ç”Ÿæˆã€‚</li>
<li>ç»“æ„æ³¨å…¥åˆ†æ”¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼ºåŒ–æ–‡æœ¬ç»“æ„ï¼Œå¹¶ç»“åˆå­—å½¢å›¾åƒçš„æ½œåœ¨ä¿¡æ¯ã€‚</li>
<li>STGenä½¿ç”¨æœ‰æ•ˆçš„ç»„åˆæ–¹æ³•æ¥åˆå¹¶å…ˆéªŒä¿¡æ¯ï¼Œä¸ºç”Ÿæˆæä¾›åšå®åŸºç¡€ã€‚</li>
<li>è·¨å¤šç§è§†è§‰æ–‡æœ¬å¸ƒå±€çš„å®éªŒè¯æ˜STGenå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œä¼˜è´¨è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05892">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6927757dc204adc03d430d1aabfaa896.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0feb8634320ace26d462ac0dae4573f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d57003e768b0a23492d295c0283cffc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88f9e4d09683814ec22baf74bf063d5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c982e0b5802726b74bea28dfa043d961.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="StarGen-A-Spatiotemporal-Autoregression-Framework-with-Video-Diffusion-Model-for-Scalable-and-Controllable-Scene-Generation"><a href="#StarGen-A-Spatiotemporal-Autoregression-Framework-with-Video-Diffusion-Model-for-Scalable-and-Controllable-Scene-Generation" class="headerlink" title="StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion   Model for Scalable and Controllable Scene Generation"></a>StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion   Model for Scalable and Controllable Scene Generation</h2><p><strong>Authors:Shangjin Zhai, Zhichao Ye, Jialin Liu, Weijian Xie, Jiaqi Hu, Zhen Peng, Hua Xue, Danpeng Chen, Xiaomeng Wang, Lei Yang, Nan Wang, Haomin Liu, Guofeng Zhang</strong></p>
<p>Recent advances in large reconstruction and generative models have significantly improved scene reconstruction and novel view generation. However, due to compute limitations, each inference with these large models is confined to a small area, making long-range consistent scene generation challenging. To address this, we propose StarGen, a novel framework that employs a pre-trained video diffusion model in an autoregressive manner for long-range scene generation. The generation of each video clip is conditioned on the 3D warping of spatially adjacent images and the temporally overlapping image from previously generated clips, improving spatiotemporal consistency in long-range scene generation with precise pose control. The spatiotemporal condition is compatible with various input conditions, facilitating diverse tasks, including sparse view interpolation, perpetual view generation, and layout-conditioned city generation. Quantitative and qualitative evaluations demonstrate StarGenâ€™s superior scalability, fidelity, and pose accuracy compared to state-of-the-art methods. </p>
<blockquote>
<p>æœ€è¿‘é‡å»ºå’Œç”Ÿæˆæ¨¡å‹æ–¹é¢çš„è¿›å±•å¤§å¤§æé«˜äº†åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’ç”Ÿæˆçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—é™åˆ¶ï¼Œè¿™äº›å¤§å‹æ¨¡å‹çš„æ¯æ¬¡æ¨ç†éƒ½å±€é™äºä¸€ä¸ªå°åŒºåŸŸï¼Œä½¿å¾—å¤§èŒƒå›´ä¸€è‡´çš„åœºæ™¯ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†StarGenï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ–°å‹æ¡†æ¶ï¼Œä»¥è‡ªå›å½’çš„æ–¹å¼è¿›è¡Œå¤§èŒƒå›´åœºæ™¯ç”Ÿæˆã€‚æ¯ä¸ªè§†é¢‘å‰ªè¾‘çš„ç”Ÿæˆéƒ½åŸºäºç©ºé—´ç›¸é‚»å›¾åƒçš„3Dæ‰­æ›²å’Œå…ˆå‰ç”Ÿæˆçš„å‰ªè¾‘ä¸­æ—¶é—´ä¸Šé‡å çš„å›¾åƒï¼Œæé«˜äº†å¤§èŒƒå›´åœºæ™¯ç”Ÿæˆä¸­çš„æ—¶ç©ºä¸€è‡´æ€§ï¼Œå¹¶å®ç°äº†ç²¾ç¡®çš„å§¿æ€æ§åˆ¶ã€‚è¿™ç§æ—¶ç©ºæ¡ä»¶ä¸å„ç§è¾“å…¥æ¡ä»¶å…¼å®¹ï¼Œèƒ½å¤Ÿä¿ƒè¿›åŒ…æ‹¬ç¨€ç–è§†å›¾æ’å€¼ã€æ°¸ä¹…è§†å›¾ç”Ÿæˆå’Œå¸ƒå±€æ§åˆ¶åŸå¸‚ç”Ÿæˆåœ¨å†…çš„å„ç§ä»»åŠ¡ã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒStarGenåœ¨å¯æ‰©å±•æ€§ã€ä¿çœŸåº¦å’Œå§¿æ€å‡†ç¡®æ€§æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05763v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹é‡å»ºå’Œç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è¿›å±•æå¤§åœ°ä¿ƒè¿›äº†åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’ç”Ÿæˆçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—é™åˆ¶ï¼Œè¿™äº›å¤§å‹æ¨¡å‹çš„æ¯æ¬¡æ¨ç†éƒ½å±€é™äºå°èŒƒå›´ï¼Œä½¿å¾—é•¿ç¨‹ä¸€è‡´åœºæ™¯ç”Ÿæˆé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºStarGenæ¡†æ¶ï¼Œé‡‡ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œé•¿ç¨‹åœºæ™¯ç”Ÿæˆã€‚æ¯ä¸ªè§†é¢‘å‰ªè¾‘çš„ç”Ÿæˆéƒ½åŸºäºç©ºé—´ç›¸é‚»å›¾åƒçš„3Då˜æ¢å’Œå…ˆå‰ç”Ÿæˆçš„å‰ªè¾‘ä¸­æ—¶é—´ä¸Šé‡å çš„å›¾åƒï¼Œæé«˜äº†é•¿ç¨‹åœºæ™¯ç”Ÿæˆçš„æ—¶ç©ºä¸€è‡´æ€§å¹¶ç²¾ç¡®æ§åˆ¶å§¿æ€ã€‚è¯¥æ—¶ç©ºæ¡ä»¶ä¸å„ç§è¾“å…¥æ¡ä»¶å…¼å®¹ï¼Œä¿ƒè¿›å¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç¨€ç–è§†å›¾æ’å€¼ã€æ°¸ä¹…è§†å›¾ç”Ÿæˆå’Œå¸ƒå±€æ§åˆ¶åŸå¸‚ç”Ÿæˆã€‚è¯„ä¼°å’Œæµ‹è¯•è¡¨æ˜ï¼ŒStarGenåœ¨å¯æ‰©å±•æ€§ã€ä¿çœŸåº¦å’Œå§¿æ€å‡†ç¡®æ€§æ–¹é¢ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é‡å»ºå’Œç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è¿›å±•ä¿ƒè¿›äº†åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’ç”Ÿæˆã€‚</li>
<li>ç”±äºè®¡ç®—é™åˆ¶ï¼Œç°æœ‰æ¨¡å‹åœ¨é•¿ç¨‹ä¸€è‡´åœºæ™¯ç”Ÿæˆæ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>StarGenæ¡†æ¶é‡‡ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œé•¿ç¨‹åœºæ™¯ç”Ÿæˆã€‚</li>
<li>StarGenåˆ©ç”¨3Då˜æ¢å’Œæ—¶ç©ºæ¡ä»¶æé«˜é•¿ç¨‹åœºæ™¯ç”Ÿæˆçš„æ—¶ç©ºä¸€è‡´æ€§ã€‚</li>
<li>StarGenæ”¯æŒå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç¨€ç–è§†å›¾æ’å€¼ã€æ°¸ä¹…è§†å›¾ç”Ÿæˆå’Œå¸ƒå±€æ§åˆ¶åŸå¸‚ç”Ÿæˆã€‚</li>
<li>StarGenåœ¨å¯æ‰©å±•æ€§ã€ä¿çœŸåº¦å’Œå§¿æ€å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05763">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f001961806ff3abb27537a1536e6a7f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e343836fcdae9f30fd34b3b33bf427eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a058d94655e7fdf703cdb98da41424f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a07ddcc66da0e1dcef8900c3a2fddbfa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d3069487a090c15d22a7cdb8cebba7d0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Network-Diffuser-for-Placing-Scheduling-Service-Function-Chains-with-Inverse-Demonstration"><a href="#Network-Diffuser-for-Placing-Scheduling-Service-Function-Chains-with-Inverse-Demonstration" class="headerlink" title="Network Diffuser for Placing-Scheduling Service Function Chains with   Inverse Demonstration"></a>Network Diffuser for Placing-Scheduling Service Function Chains with   Inverse Demonstration</h2><p><strong>Authors:Zuyuan Zhang, Vaneet Aggarwal, Tian Lan</strong></p>
<p>Network services are increasingly managed by considering chained-up virtual network functions and relevant traffic flows, known as the Service Function Chains (SFCs). To deal with sequential arrivals of SFCs in an online fashion, we must consider two closely-coupled problems - an SFC placement problem that maps SFCs to servers&#x2F;links in the network and an SFC scheduling problem that determines when each SFC is executed. Solving the whole SFC problem targeting these two optimizations jointly is extremely challenging. In this paper, we propose a novel network diffuser using conditional generative modeling for this SFC placing-scheduling optimization. Recent advances in generative AI and diffusion models have made it possible to generate high-quality images&#x2F;videos and decision trajectories from language description. We formulate the SFC optimization as a problem of generating a state sequence for planning and perform graph diffusion on the state trajectories to enable extraction of SFC decisions, with SFC optimization constraints and objectives as conditions. To address the lack of demonstration data due to NP-hardness and exponential problem space of the SFC optimization, we also propose a novel and somewhat maverick approach â€“ Rather than solving instances of this difficult optimization, we start with randomly-generated solutions as input, and then determine appropriate SFC optimization problems that render these solutions feasible. This inverse demonstration enables us to obtain sufficient expert demonstrations, i.e., problem-solution pairs, through further optimization. In our numerical evaluations, the proposed network diffuser outperforms learning and heuristic baselines, by $\sim$20% improvement in SFC reward and $\sim$50% reduction in SFC waiting time and blocking rate. </p>
<blockquote>
<p>ç½‘ç»œæœåŠ¡è¶Šæ¥è¶Šå¤šåœ°é€šè¿‡è€ƒè™‘ä¸²è”çš„è™šæ‹Ÿç½‘ç»œåŠŸèƒ½å’Œç›¸å…³æµé‡æµï¼ˆç§°ä¸ºæœåŠ¡åŠŸèƒ½é“¾ï¼ˆSFCï¼‰ï¼‰æ¥è¿›è¡Œç®¡ç†ã€‚ä¸ºäº†åœ¨çº¿å¤„ç†SFCçš„è¿ç»­åˆ°è¾¾ï¼Œæˆ‘ä»¬å¿…é¡»è€ƒè™‘ä¸¤ä¸ªç´§å¯†è€¦åˆçš„é—®é¢˜â€”â€”SFCæ”¾ç½®é—®é¢˜ï¼Œå°†SFCæ˜ å°„åˆ°ç½‘ç»œä¸­çš„æœåŠ¡å™¨&#x2F;é“¾æ¥ï¼Œä»¥åŠSFCè°ƒåº¦é—®é¢˜ï¼Œç¡®å®šæ¯ä¸ªSFCçš„æ‰§è¡Œæ—¶é—´ã€‚é’ˆå¯¹è¿™ä¸¤ä¸ªä¼˜åŒ–çš„SFCæ•´ä½“é—®é¢˜è§£å†³èµ·æ¥æä¸ºå…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨æ¡ä»¶ç”Ÿæˆæ¨¡å‹çš„æ–°å‹ç½‘ç»œæ‰©æ•£å™¨æ¥è§£å†³SFCæ”¾ç½®è°ƒåº¦ä¼˜åŒ–é—®é¢˜ã€‚æœ€è¿‘ç”Ÿæˆäººå·¥æ™ºèƒ½å’Œæ‰©æ•£æ¨¡å‹çš„è¿›æ­¥ä½¿å¾—å¯ä»¥ä»è¯­è¨€æè¿°ç”Ÿæˆé«˜è´¨é‡å›¾åƒ&#x2F;è§†é¢‘å’Œå†³ç­–è½¨è¿¹æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬å°†SFCä¼˜åŒ–å…¬å¼åŒ–ä¸ºç”ŸæˆçŠ¶æ€åºåˆ—çš„è§„åˆ’é—®é¢˜ï¼Œå¹¶åœ¨çŠ¶æ€è½¨è¿¹ä¸Šæ‰§è¡Œå›¾æ‰©æ•£ä»¥å¯ç”¨SFCå†³ç­–æå–ï¼Œå°†SFCä¼˜åŒ–çº¦æŸå’Œç›®æ ‡ä½œä¸ºæ¡ä»¶ã€‚ä¸ºäº†è§£å†³ç”±äºNPéš¾åº¦å’ŒSFCä¼˜åŒ–çš„æŒ‡æ•°é—®é¢˜ç©ºé—´è€Œå¯¼è‡´çš„æ¼”ç¤ºæ•°æ®ç¼ºä¹çš„é—®é¢˜ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”æœ‰äº›ç‹¬ç‰¹çš„æ–¹æ³•â€”â€”æˆ‘ä»¬ä¸æ˜¯è§£å†³è¿™ä¸ªå›°éš¾ä¼˜åŒ–çš„å®ä¾‹ï¼Œè€Œæ˜¯é¦–å…ˆä»éšæœºç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆå¼€å§‹ä½œä¸ºè¾“å…¥ï¼Œç„¶åç¡®å®šä½¿è¿™äº›è§£å†³æ–¹æ¡ˆå¯è¡Œçš„é€‚å½“SFCä¼˜åŒ–é—®é¢˜ã€‚è¿™ç§é€†å‘æ¼”ç¤ºä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡è¿›ä¸€æ­¥çš„ä¼˜åŒ–è·å¾—å……è¶³çš„ä¸“ä¸šæ¼”ç¤ºï¼Œå³é—®é¢˜è§£å†³æ–¹æ¡ˆå¯¹ã€‚åœ¨æˆ‘ä»¬çš„æ•°å€¼è¯„ä¼°ä¸­ï¼Œæ‰€æå‡ºçš„ç½‘ç»œæ‰©æ•£å™¨åœ¨SFCå¥–åŠ±æ–¹é¢æé«˜äº†çº¦20ï¼…ï¼Œåœ¨SFCç­‰å¾…æ—¶é—´å’Œé˜»å¡ç‡æ–¹é¢å‡å°‘äº†çº¦50ï¼…ï¼Œè¶…è¿‡äº†å­¦ä¹ å’Œå¯å‘å¼åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05673v1">PDF</a> Accepted to IEEE INFOCOM 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€ç½‘ç»œæœåŠ¡è¶Šæ¥è¶Šå¤šåœ°é€šè¿‡è€ƒè™‘é“¾å¼è™šæ‹Ÿç½‘ç»œåŠŸèƒ½å’Œç›¸å…³çš„æµé‡æµï¼ˆç§°ä¸ºæœåŠ¡åŠŸèƒ½é“¾SFCsï¼‰è¿›è¡Œç®¡ç†ï¼Œå¦‚ä½•åœ¨çº¿å¤„ç†SFCsçš„è¿ç»­åˆ°è¾¾æˆä¸ºä¸€ä¸ªé‡è¦é—®é¢˜ã€‚è¿™æ¶‰åŠåˆ°ä¸¤ä¸ªç´§å¯†è€¦åˆçš„é—®é¢˜ï¼šSFCæ”¾ç½®é—®é¢˜å’ŒSFCè°ƒåº¦é—®é¢˜ã€‚æœ¬æ–‡å°†SFCä¼˜åŒ–é—®é¢˜è¡¨è¿°ä¸ºç”ŸæˆçŠ¶æ€åºåˆ—çš„è§„åˆ’é—®é¢˜ï¼Œå¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡ŒçŠ¶æ€è½¨è¿¹çš„å›¾å½¢æ‰©æ•£ï¼Œä»¥æå–SFCå†³ç­–ã€‚ä¸ºäº†å¼¥è¡¥å› NPéš¾åº¦å’ŒæŒ‡æ•°çº§é—®é¢˜ç©ºé—´è€Œç¼ºä¹æ¼”ç¤ºæ•°æ®çš„é—®é¢˜ï¼Œæœ¬æ–‡é‡‡ç”¨é€†å‘æ¼”ç¤ºçš„æ–¹æ³•ï¼Œä»éšæœºç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆå‡ºå‘ï¼Œç¡®å®šå¯è¡Œçš„SFCä¼˜åŒ–é—®é¢˜ã€‚æ•°å€¼è¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç½‘ç»œæ‰©æ•£å™¨åœ¨SFCå¥–åŠ±æ–¹é¢æé«˜äº†çº¦20ï¼…ï¼Œåœ¨SFCç­‰å¾…æ—¶é—´å’Œé˜»å¡ç‡æ–¹é¢å‡å°‘äº†çº¦50ï¼…ï¼Œä¼˜äºå­¦ä¹ å’Œå¯å‘å¼åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœåŠ¡åŠŸèƒ½é“¾ï¼ˆSFCsï¼‰æ˜¯ç®¡ç†æœåŠ¡çš„ä¸€ç§æ–°æ–¹æ³•ï¼Œæ¶‰åŠå°†ç½‘ç»œåŠŸèƒ½é“¾æ¥æˆé“¾æ¡æ¥å¤„ç†æµé‡ã€‚</li>
<li>SFCé¢ä¸´ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šSFCæ”¾ç½®å’ŒSFCè°ƒåº¦ï¼Œè¿™ä¸¤ä¸ªé—®é¢˜éœ€è¦è”åˆä¼˜åŒ–ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶ç”Ÿæˆå»ºæ¨¡çš„ç½‘ç»œæ‰©æ•£å™¨æ¥è§£å†³SFCä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹åœ¨çŠ¶æ€è½¨è¿¹ä¸Šè¿›è¡Œå›¾å½¢æ‰©æ•£ï¼Œä»¥ç”ŸæˆSFCå†³ç­–ã€‚</li>
<li>ä¸ºäº†è§£å†³å› NPéš¾åº¦å’ŒæŒ‡æ•°çº§é—®é¢˜ç©ºé—´è€Œç¼ºä¹æ¼”ç¤ºæ•°æ®çš„é—®é¢˜ï¼Œé‡‡ç”¨é€†å‘æ¼”ç¤ºæ–¹æ³•ã€‚</li>
<li>æ•°å€¼è¯„ä¼°æ˜¾ç¤ºï¼Œç½‘ç»œæ‰©æ•£å™¨åœ¨SFCå¥–åŠ±ã€ç­‰å¾…æ—¶é—´å’Œé˜»å¡ç‡æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>è¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹åœ¨äºï¼Œå³ä½¿é¢å¯¹å¤æ‚çš„ç½‘ç»œç¯å¢ƒå’Œå¤§é‡çš„SFCè¯·æ±‚ï¼Œä¹Ÿèƒ½æœ‰æ•ˆåœ°è¿›è¡Œå®æ—¶ä¼˜åŒ–å’Œå“åº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05673">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ccfafcdc0a28574c8460d1bc3a10f6c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca77ed34070e1cb0bdcd5ba2bc7fc29c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe9e7d9059fbbe5cb319cb91ceabf038.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1498048b4d84cb56a8c04fe13cf4817d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HFMF-Hierarchical-Fusion-Meets-Multi-Stream-Models-for-Deepfake-Detection"><a href="#HFMF-Hierarchical-Fusion-Meets-Multi-Stream-Models-for-Deepfake-Detection" class="headerlink" title="HFMF: Hierarchical Fusion Meets Multi-Stream Models for Deepfake   Detection"></a>HFMF: Hierarchical Fusion Meets Multi-Stream Models for Deepfake   Detection</h2><p><strong>Authors:Anant Mehta, Bryant McArthur, Nagarjuna Kolloju, Zhengzhong Tu</strong></p>
<p>The rapid progress in deep generative models has led to the creation of incredibly realistic synthetic images that are becoming increasingly difficult to distinguish from real-world data. The widespread use of Variational Models, Diffusion Models, and Generative Adversarial Networks has made it easier to generate convincing fake images and videos, which poses significant challenges for detecting and mitigating the spread of misinformation. As a result, developing effective methods for detecting AI-generated fakes has become a pressing concern. In our research, we propose HFMF, a comprehensive two-stage deepfake detection framework that leverages both hierarchical cross-modal feature fusion and multi-stream feature extraction to enhance detection performance against imagery produced by state-of-the-art generative AI models. The first component of our approach integrates vision Transformers and convolutional nets through a hierarchical feature fusion mechanism. The second component of our framework combines object-level information and a fine-tuned convolutional net model. We then fuse the outputs from both components via an ensemble deep neural net, enabling robust classification performances. We demonstrate that our architecture achieves superior performance across diverse dataset benchmarks while maintaining calibration and interoperability. </p>
<blockquote>
<p>éšç€æ·±åº¦ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œå·²ç»èƒ½å¤Ÿåˆ›é€ å‡ºéå¸¸é€¼çœŸçš„åˆæˆå›¾åƒï¼Œè¿™äº›å›¾åƒä¸ç°å®ä¸–ç•Œçš„æ•°æ®è¶Šæ¥è¶Šéš¾ä»¥åŒºåˆ†ã€‚å˜åˆ†æ¨¡å‹ã€æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„å¹¿æ³›åº”ç”¨ä½¿å¾—ç”Ÿæˆä»¤äººä¿¡æœçš„è™šå‡å›¾åƒå’Œè§†é¢‘å˜å¾—æ›´åŠ å®¹æ˜“ï¼Œè¿™ç»™æ£€æµ‹å’Œç¼“è§£è™šå‡ä¿¡æ¯çš„ä¼ æ’­å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œå¼€å‘æœ‰æ•ˆçš„æ£€æµ‹AIç”Ÿæˆè™šå‡ä¿¡æ¯çš„æ–¹æ³•å·²æˆä¸ºå½“åŠ¡ä¹‹æ€¥ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†HFMFï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ä¸¤é˜¶æ®µæ·±åº¦ä¼ªé€ æ£€æµ‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨åˆ†å±‚è·¨æ¨¡æ€ç‰¹å¾èåˆå’Œå¤šæµç‰¹å¾æå–æŠ€æœ¯ï¼Œå¢å¼ºäº†å¯¹ç”±æœ€æ–°ç”Ÿæˆå¼AIæ¨¡å‹ç”Ÿæˆçš„å›¾åƒçš„æ£€æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ç¬¬ä¸€éƒ¨åˆ†æ˜¯é€šè¿‡åˆ†å±‚ç‰¹å¾èåˆæœºåˆ¶å°†è§†è§‰Transformerå’Œå·ç§¯ç½‘ç»œé›†æˆåœ¨ä¸€èµ·ã€‚æˆ‘ä»¬æ¡†æ¶çš„ç¬¬äºŒéƒ¨åˆ†ç»“åˆäº†å¯¹è±¡çº§ä¿¡æ¯å’Œç»è¿‡ç²¾ç»†è°ƒæ•´çš„å·ç§¯ç½‘ç»œæ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡é›†æˆæ·±åº¦ç¥ç»ç½‘ç»œèåˆäº†è¿™ä¸¤ä¸ªç»„ä»¶çš„è¾“å‡ºï¼Œå®ç°äº†ç¨³å¥çš„åˆ†ç±»æ€§èƒ½ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„æ¶æ„åœ¨å¤šç§æ•°æ®é›†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†æ ¡å‡†å’Œäº’æ“ä½œæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05631v1">PDF</a> This work is accepted to WACV 2025 Workshop on AI for Multimedia   Forensics &amp; Disinformation Detection. Code is available at:   <a target="_blank" rel="noopener" href="https://github.com/taco-group/HFMF">https://github.com/taco-group/HFMF</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ·±åº¦ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œä½¿å¾—ç”Ÿæˆé€¼çœŸçš„åˆæˆå›¾åƒå˜å¾—è¶Šæ¥è¶Šå®¹æ˜“ï¼Œè¿™ç»™æ£€æµ‹å’Œéåˆ¶è™šå‡ä¿¡æ¯çš„ä¼ æ’­å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨é¢çš„ä¸¤é˜¶æ®µæ·±åº¦ä¼ªé€ æ£€æµ‹æ¡†æ¶HFMFï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åˆ†å±‚è·¨æ¨¡æ€ç‰¹å¾èåˆå’Œå¤šæµç‰¹å¾æå–æŠ€æœ¯ï¼Œä»¥å¢å¼ºå¯¹å…ˆè¿›ç”ŸæˆAIæ¨¡å‹ç”Ÿæˆçš„å›¾åƒçš„æ£€æµ‹æ€§èƒ½ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è§†è§‰Transformerå’Œå·ç§¯ç½‘ç»œï¼Œé€šè¿‡é›†æˆå¯¹è±¡çº§ä¿¡æ¯å’Œç²¾ç»†è°ƒæ•´çš„å·ç§¯ç½‘ç»œæ¨¡å‹ï¼Œå®ç°äº†ç¨³å¥çš„åˆ†ç±»æ€§èƒ½ï¼Œå¹¶åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•å¯¼è‡´åˆæˆå›¾åƒè¶Šæ¥è¶Šé€¼çœŸï¼Œéš¾ä»¥ä¸ç°å®æ•°æ®åŒºåˆ†ã€‚</li>
<li>å¹¿æ³›ä½¿ç”¨çš„å˜åˆ†æ¨¡å‹ã€æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œä½¿å¾—ç”Ÿæˆè™šå‡å›¾åƒå’Œè§†é¢‘å˜å¾—æ›´åŠ å®¹æ˜“ã€‚</li>
<li>æ£€æµ‹AIç”Ÿæˆçš„è™šå‡å†…å®¹å·²æˆä¸ºç´§è¿«çš„éœ€æ±‚ã€‚</li>
<li>æå‡ºçš„HFMFæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ¡†æ¶ã€‚</li>
<li>HFMFé‡‡ç”¨åˆ†å±‚è·¨æ¨¡æ€ç‰¹å¾èåˆå’Œå¤šæµç‰¹å¾æå–æŠ€æœ¯ã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†è§†è§‰Transformerå’Œå·ç§¯ç½‘ç»œï¼Œé€šè¿‡é›†æˆå¯¹è±¡çº§ä¿¡æ¯å®ç°ç¨³å¥åˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-754aae288d986a59dda5339638641125.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2aecf98bc0533214b6be2bb27c31b4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-196e978c0634403010359fe7e46f9987.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e93ceb327822b36feb413a32d4386d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8577a63940ad18d4b938ca9d8a0df7c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06d10a829caf412dd5a5c02e7495de37.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc6d0b41e8af0b73cb44f336d3a21501.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e5df784c02956bd4f7efc87292501cc.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CROPS-Model-Agnostic-Training-Free-Framework-for-Safe-Image-Synthesis-with-Latent-Diffusion-Models"><a href="#CROPS-Model-Agnostic-Training-Free-Framework-for-Safe-Image-Synthesis-with-Latent-Diffusion-Models" class="headerlink" title="CROPS: Model-Agnostic Training-Free Framework for Safe Image Synthesis   with Latent Diffusion Models"></a>CROPS: Model-Agnostic Training-Free Framework for Safe Image Synthesis   with Latent Diffusion Models</h2><p><strong>Authors:Junha Park, Ian Ryu, Jaehui Hwang, Hyungkeun Park, Jiyoon Kim, Jong-Seok Lee</strong></p>
<p>With advances in diffusion models, image generation has shown significant performance improvements. This raises concerns about the potential abuse of image generation, such as the creation of explicit or violent images, commonly referred to as Not Safe For Work (NSFW) content. To address this, the Stable Diffusion model includes several safety checkers to censor initial text prompts and final output images generated from the model. However, recent research has shown that these safety checkers have vulnerabilities against adversarial attacks, allowing them to generate NSFW images. In this paper, we find that these adversarial attacks are not robust to small changes in text prompts or input latents. Based on this, we propose CROPS (Circular or RandOm Prompts for Safety), a model-agnostic framework that easily defends against adversarial attacks generating NSFW images without requiring additional training. Moreover, we develop an approach that utilizes one-step diffusion models for efficient NSFW detection (CROPS-1), further reducing computational resources. We demonstrate the superiority of our method in terms of performance and applicability. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥ï¼Œå›¾åƒç”Ÿæˆåœ¨æ€§èƒ½ä¸Šå·²æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚è¿™å¼•å‘äº†å…³äºå›¾åƒç”Ÿæˆå¯èƒ½è¢«æ»¥ç”¨çš„æ‹…å¿§ï¼Œä¾‹å¦‚ç”Ÿæˆå…·æœ‰æ˜ç¡®æˆ–æš´åŠ›å†…å®¹çš„å›¾åƒï¼Œé€šå¸¸è¢«ç§°ä¸ºä¸é€‚åˆå·¥ä½œåœºæ‰€ï¼ˆNSFWï¼‰çš„å†…å®¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒStable Diffusionæ¨¡å‹åŒ…å«å¤šä¸ªå®‰å…¨æ£€æŸ¥å™¨ï¼Œä»¥å¯¹åˆå§‹æ–‡æœ¬æç¤ºå’Œæ¨¡å‹ç”Ÿæˆçš„æœ€ç»ˆè¾“å‡ºå›¾åƒè¿›è¡Œå®¡æŸ¥ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›å®‰å…¨æ£€æŸ¥å™¨å¯¹å¯¹æŠ—æ€§æ”»å‡»å­˜åœ¨æ¼æ´ï¼Œå…è®¸å®ƒä»¬ç”ŸæˆNSFWå›¾åƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å‘ç°è¿™äº›å¯¹æŠ—æ€§æ”»å‡»å¯¹æ–‡æœ¬æç¤ºæˆ–è¾“å…¥æ½œåœ¨ç©ºé—´ä¸­çš„å¾®å°å˜åŒ–å¹¶ä¸ç¨³å¥ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†CROPSï¼ˆç”¨äºå®‰å…¨çš„å¾ªç¯æˆ–éšæœºæç¤ºï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å‹æ— å…³æ¡†æ¶ï¼Œå¯ä»¥è½»æ¾é˜²å¾¡å¯¹æŠ—æ€§æ”»å‡»ç”Ÿæˆçš„NSFWå›¾åƒï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åˆ©ç”¨å•æ­¥æ‰©æ•£æ¨¡å‹è¿›è¡Œæœ‰æ•ˆNSFWæ£€æµ‹çš„æ–¹æ³•ï¼ˆCROPS-1ï¼‰ï¼Œè¿›ä¸€æ­¥å‡å°‘äº†è®¡ç®—èµ„æºã€‚æˆ‘ä»¬åœ¨æ€§èƒ½å’Œé€‚ç”¨æ€§æ–¹é¢å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05359v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€æ‰©æ•£æ¨¡å‹æŠ€æœ¯çš„è¿›æ­¥ï¼Œå›¾åƒç”Ÿæˆæ€§èƒ½æ˜¾è‘—æå‡ï¼Œä½†ä¹Ÿå¼•å‘äº†å…³äºç”Ÿæˆä¸é€‚å®œå·¥ä½œåœºåˆï¼ˆNSFWï¼‰å†…å®¹çš„æ½œåœ¨æ»¥ç”¨é—®é¢˜ã€‚Stable Diffusionæ¨¡å‹é‡‡ç”¨å¤šä¸ªå®‰å…¨æ£€æŸ¥å™¨å¯¹åˆå§‹æ–‡æœ¬æç¤ºå’Œæœ€ç»ˆç”Ÿæˆçš„å›¾åƒè¿›è¡Œå®¡æŸ¥ã€‚ä½†æœ€æ–°ç ”ç©¶æ˜¾ç¤ºï¼Œè¿™äº›å®‰å…¨æ£€æŸ¥å™¨æ˜“å—å¯¹æŠ—æ€§æ”»å‡»çš„æ¼æ´å½±å“ï¼Œèƒ½ç”ŸæˆNSFWå›¾åƒã€‚æœ¬æ–‡æå‡ºä¸€ç§æ¨¡å‹æ— å…³æ¡†æ¶â€”â€”CROPSï¼ˆç”¨äºå®‰å…¨çš„å¾ªç¯æˆ–éšæœºæç¤ºï¼‰ï¼Œèƒ½è½»æ¾é˜²èŒƒç”ŸæˆNSFWå›¾åƒçš„å¯¹æŠ—æ€§æ”»å‡»ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§åˆ©ç”¨å•æ­¥æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜æ•ˆNSFWæ£€æµ‹çš„æ–¹æ³•ï¼ˆCROPS-1ï¼‰ï¼Œè¿›ä¸€æ­¥é™ä½è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œå±•ç°äº†å‡ºè‰²çš„æ€§èƒ½å’Œåº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥å¸¦åŠ¨äº†å›¾åƒç”Ÿæˆæ€§èƒ½çš„å¤§å¹…æå‡ã€‚</li>
<li>ä¼´éšæ€§èƒ½æå‡ï¼Œå‡ºç°äº†å…³äºç”Ÿæˆä¸é€‚å®œå·¥ä½œåœºåˆï¼ˆNSFWï¼‰å†…å®¹çš„æ½œåœ¨æ»¥ç”¨é—®é¢˜ã€‚</li>
<li>Stable Diffusionæ¨¡å‹é‡‡ç”¨å®‰å…¨æ£€æŸ¥å™¨å®¡æŸ¥å†…å®¹ï¼Œä½†å­˜åœ¨æ˜“å—å¯¹æŠ—æ€§æ”»å‡»æ¼æ´çš„é£é™©ã€‚</li>
<li>å¯¹æŠ—æ€§çš„æ”»å‡»å¯¹æ–‡æœ¬æç¤ºæˆ–è¾“å…¥æ½œå˜é‡çš„å¾®å°å˜åŒ–å¾ˆæ•æ„Ÿã€‚</li>
<li>CROPSæ¡†æ¶èƒ½è½»æ¾é˜²èŒƒç”ŸæˆNSFWå›¾åƒçš„å¯¹æŠ—æ€§æ”»å‡»ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
<li>CROPS-1æ–¹æ³•åˆ©ç”¨å•æ­¥æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜æ•ˆNSFWæ£€æµ‹ï¼Œé™ä½è®¡ç®—èµ„æºæ¶ˆè€—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05359">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b9899adecacec8191476bc6558b5a0fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac79dc5a9d1d0859a333be94b835182f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d8cfcb40b3058e31a4732631d5e8e80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37146373c17580c9c96e46655db93ffc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdc7bc17db80dd8452d0a64e132f03c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17767ed24f8cb9d3578a10039043d905.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de8a55be1b7ee954865452241e7aa98b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ResPanDiff-Diffusion-Model-for-Pansharpening-by-Inferring-Residual-Inference"><a href="#ResPanDiff-Diffusion-Model-for-Pansharpening-by-Inferring-Residual-Inference" class="headerlink" title="ResPanDiff: Diffusion Model for Pansharpening by Inferring Residual   Inference"></a>ResPanDiff: Diffusion Model for Pansharpening by Inferring Residual   Inference</h2><p><strong>Authors:Shiqi Cao, Liangjian Deng, Shangqi Deng</strong></p>
<p>The implementation of diffusion-based pansharpening task is predominantly constrained by its slow inference speed, which results from numerous sampling steps. Despite the existing techniques aiming to accelerate sampling, they often compromise performance when fusing multi-source images. To ease this limitation, we introduce a novel and efficient diffusion model named Diffusion Model for Pansharpening by Inferring Residual Inference (ResPanDiff), which significantly reduces the number of diffusion steps without sacrificing the performance to tackle pansharpening task. In ResPanDiff, we innovatively propose a Markov chain that transits from noisy residuals to the residuals between the LRMS and HRMS images, thereby reducing the number of sampling steps and enhancing performance. Additionally, we design the latent space to help model extract more features at the encoding stage, Shallow Cond-Injection<del>(SC-I) to help model fetch cond-injected hidden features with higher dimensions, and loss functions to give a better guidance for the residual generation task. enabling the model to achieve superior performance in residual generation. Furthermore, experimental evaluations on pansharpening datasets demonstrate that the proposed method achieves superior outcomes compared to recent state-of-the-art</del>(SOTA) techniques, requiring only 15 sampling steps, which reduces over $90%$ step compared with the benchmark diffusion models. Our experiments also include thorough discussions and ablation studies to underscore the effectiveness of our approach. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„é”åŒ–ä»»åŠ¡å®æ–½ä¸»è¦å—åˆ°å…¶ç¼“æ…¢æ¨ç†é€Ÿåº¦çš„åˆ¶çº¦ï¼Œè¿™æ˜¯ç”±äºéœ€è¦å¤§é‡çš„é‡‡æ ·æ­¥éª¤ã€‚å°½ç®¡ç°æœ‰çš„æŠ€æœ¯æ—¨åœ¨åŠ é€Ÿé‡‡æ ·ï¼Œä½†åœ¨èåˆå¤šæºå›¾åƒæ—¶å¾€å¾€ä¼šæŸå®³æ€§èƒ½ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹é«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹ï¼Œåä¸ºé€šè¿‡æ¨æ–­æ®‹å·®æ¨ç†çš„é”åŒ–æ‰©æ•£æ¨¡å‹ï¼ˆResPanDiffï¼‰ã€‚ResPanDiffæ˜¾è‘—å‡å°‘äº†æ‰©æ•£æ­¥éª¤çš„æ•°é‡ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æ€§èƒ½æ¥è§£å†³é”åŒ–ä»»åŠ¡ã€‚åœ¨ResPanDiffä¸­ï¼Œæˆ‘ä»¬åˆ›æ–°åœ°æå‡ºäº†ä¸€ä¸ªé©¬å°”å¯å¤«é“¾ï¼Œè¯¥é“¾ä»å™ªå£°æ®‹å·®è¿‡æ¸¡åˆ°ä½åˆ†è¾¨ç‡å›¾åƒï¼ˆLRMSï¼‰å’Œé«˜åˆ†è¾¨ç‡å›¾åƒï¼ˆHRMSï¼‰ä¹‹é—´çš„æ®‹å·®ï¼Œä»è€Œå‡å°‘äº†é‡‡æ ·æ­¥éª¤çš„æ•°é‡å¹¶æé«˜äº†æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†æ½œåœ¨ç©ºé—´ä»¥å¸®åŠ©æ¨¡å‹åœ¨ç¼–ç é˜¶æ®µæå–æ›´å¤šç‰¹å¾ï¼Œæµ…å±‚æ¡ä»¶æ³¨å…¥ï¼ˆSC-Iï¼‰ä»¥å¸®åŠ©æ¨¡å‹è·å–å…·æœ‰æ›´é«˜ç»´åº¦çš„æ¡ä»¶æ³¨å…¥éšè—ç‰¹å¾ï¼Œä»¥åŠæŸå¤±å‡½æ•°ä»¥æ›´å¥½åœ°æŒ‡å¯¼æ®‹å·®ç”Ÿæˆä»»åŠ¡ã€‚è¿™ä½¿å¾—æ¨¡å‹åœ¨æ®‹å·®ç”Ÿæˆæ–¹é¢å®ç°å“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨é”åŒ–æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¸æœ€æ–°çš„æœ€å…ˆè¿›çš„ï¼ˆSOTAï¼‰æŠ€æœ¯ç›¸æ¯”å–å¾—äº†ä¼˜è¶Šçš„ç»“æœï¼Œä»…éœ€15ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œä¸åŸºå‡†æ‰©æ•£æ¨¡å‹ç›¸æ¯”å‡å°‘äº†è¶…è¿‡90ï¼…çš„æ­¥éª¤ã€‚æˆ‘ä»¬çš„å®éªŒè¿˜åŒ…æ‹¬æ·±å…¥çš„è®¨è®ºå’Œæ¶ˆèç ”ç©¶ï¼Œä»¥å¼ºè°ƒæˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05091v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºResPanDiffçš„æ–°å‹æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè§£å†³å›¾åƒè¶…åˆ†è¾¨ç‡ä¸­çš„pansharpeningä»»åŠ¡ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥æ®‹å·®æ¨æ–­æ¥åŠ é€Ÿé‡‡æ ·è¿‡ç¨‹ï¼Œå‡å°‘æ‰©æ•£æ­¥éª¤æ•°é‡ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜é‡‡ç”¨äº†ä¸€ç³»åˆ—æŠ€æœ¯ï¼Œå¦‚Markové“¾ã€æ½œç©ºé—´è®¾è®¡ã€æµ…å±‚æ¡ä»¶æ³¨å…¥ï¼ˆSC-Iï¼‰å’Œç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–ç‰¹å¾æå–å’Œæ®‹å·®ç”Ÿæˆä»»åŠ¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨pansharpeningæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜äºæœ€æ–°æŠ€æœ¯æˆæœçš„ç»“æœï¼Œä»…éœ€è¦15ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œä¸åŸºå‡†æ‰©æ•£æ¨¡å‹ç›¸æ¯”å‡å°‘äº†è¶…è¿‡90%çš„æ­¥éª¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ResPanDiffæ˜¯ä¸€ç§é«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè§£å†³pansharpeningä»»åŠ¡ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥æ®‹å·®æ¨æ–­æ¥åŠ é€Ÿé‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>ResPanDiffå‡å°‘æ‰©æ•£æ­¥éª¤æ•°é‡ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨Markové“¾è¿‡æ¸¡ä»å™ªå£°æ®‹å·®åˆ°LRMSå’ŒHRMSå›¾åƒä¹‹é—´çš„æ®‹å·®ï¼Œå¢å¼ºæ€§èƒ½ã€‚</li>
<li>æ½œç©ºé—´è®¾è®¡å’Œæµ…å±‚æ¡ä»¶æ³¨å…¥ï¼ˆSC-Iï¼‰æŠ€æœ¯å¸®åŠ©æ¨¡å‹æ›´æœ‰æ•ˆåœ°æå–ç‰¹å¾å’Œæ¡ä»¶æ³¨å…¥éšè—ç‰¹å¾ã€‚</li>
<li>ç‰¹å®šçš„æŸå¤±å‡½æ•°ä¸ºæ®‹å·®ç”Ÿæˆä»»åŠ¡æä¾›æ›´å¥½çš„æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69541765ebc044144a947967a4e2ca1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b10f3139bebbb9de90c659f17ee10e82.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-838894a85bc4d9c2b58d080c0d7830a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cc20dd622eee459319f25caecc2016d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d7f79a54d328e054d15009cc0fa485f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-356908a819b64b50f37ecde771c6e0a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cac2769d3f7cf4fa64308bda1dbd3ab3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12aa1c77950b230f683ceb076aad72b3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="D3RM-A-Discrete-Denoising-Diffusion-Refinement-Model-for-Piano-Transcription"><a href="#D3RM-A-Discrete-Denoising-Diffusion-Refinement-Model-for-Piano-Transcription" class="headerlink" title="D3RM: A Discrete Denoising Diffusion Refinement Model for Piano   Transcription"></a>D3RM: A Discrete Denoising Diffusion Refinement Model for Piano   Transcription</h2><p><strong>Authors:Hounsu Kim, Taegyun Kwon, Juhan Nam</strong></p>
<p>Diffusion models have been widely used in the generative domain due to their convincing performance in modeling complex data distributions. Moreover, they have shown competitive results on discriminative tasks, such as image segmentation. While diffusion models have also been explored for automatic music transcription, their performance has yet to reach a competitive level. In this paper, we focus on discrete diffusion modelâ€™s refinement capabilities and present a novel architecture for piano transcription. Our model utilizes Neighborhood Attention layers as the denoising module, gradually predicting the target high-resolution piano roll, conditioned on the finetuned features of a pretrained acoustic model. To further enhance refinement, we devise a novel strategy which applies distinct transition states during training and inference stage of discrete diffusion models. Experiments on the MAESTRO dataset show that our approach outperforms previous diffusion-based piano transcription models and the baseline model in terms of F1 score. Our code is available in <a target="_blank" rel="noopener" href="https://github.com/hanshounsu/d3rm">https://github.com/hanshounsu/d3rm</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ç”±äºå…¶åœ¨å¤æ‚æ•°æ®åˆ†å¸ƒå»ºæ¨¡æ–¹é¢çš„å‡ºè‰²è¡¨ç°ï¼Œåœ¨ç”Ÿæˆé¢†åŸŸå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚æ­¤å¤–ï¼Œå®ƒä»¬åœ¨åˆ¤åˆ«ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†å‰²ï¼‰ä¸Šä¹Ÿå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹ä¹Ÿè¢«æ¢ç´¢ç”¨äºè‡ªåŠ¨éŸ³ä¹è½¬å½•ï¼Œä½†å…¶æ€§èƒ½å°šæœªè¾¾åˆ°ç«äº‰æ°´å¹³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„ç»†åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸ºé’¢ç´è½¬å½•æå‡ºäº†ä¸€ç§æ–°å‹æ¶æ„ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨é‚»åŸŸæ³¨æ„åŠ›å±‚ä½œä¸ºå»å™ªæ¨¡å—ï¼ŒåŸºäºé¢„è®­ç»ƒå£°å­¦æ¨¡å‹çš„å¾®è°ƒç‰¹å¾ï¼Œé€æ­¥é¢„æµ‹ç›®æ ‡é«˜åˆ†è¾¨ç‡é’¢ç´å·ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ç»†åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹ç­–ç•¥ï¼Œåœ¨ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†é˜¶æ®µåº”ç”¨ä¸åŒçš„è¿‡æ¸¡çŠ¶æ€ã€‚åœ¨MAESTROæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨F1åˆ†æ•°æ–¹é¢ä¼˜äºä¹‹å‰çš„åŸºäºæ‰©æ•£çš„é’¢ç´è½¬å½•æ¨¡å‹å’ŒåŸºçº¿æ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hanshounsu/d3rm%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hanshounsu/d3rmä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05068v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨åŠå…¶åœ¨å¤æ‚æ•°æ®åˆ†å¸ƒå»ºæ¨¡ä¸­çš„å‡ºè‰²è¡¨ç°ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é‡ç‚¹æ¢è®¨äº†ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨é’¢ç´éŸ³ä¹è½¬å½•æ–¹é¢çš„ä¼˜åŒ–èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹æ¶æ„ã€‚è¯¥æ¨¡å‹é‡‡ç”¨é‚»åŸŸæ³¨æ„åŠ›å±‚ä½œä¸ºå»å™ªæ¨¡å—ï¼Œé€æ¸é¢„æµ‹ç›®æ ‡é«˜åˆ†è¾¨ç‡é’¢ç´ä¹è°±ï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒå£°å­¦æ¨¡å‹çš„å¾®è°ƒç‰¹å¾è¿›è¡Œæ¡ä»¶å¤„ç†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨MAESTROæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„æ‰©æ•£æ¨¡å‹é’¢ç´è½¬å½•æ–¹æ³•å’ŒåŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æ•°æ®åˆ†å¸ƒçš„å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºä»¤äººä¿¡æœçš„æ•ˆæœã€‚</li>
<li>ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨é’¢ç´éŸ³ä¹è½¬å½•æ–¹é¢å±•ç°å‡ºä¼˜åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹é’¢ç´éŸ³ä¹è½¬å½•æ¨¡å‹æ¶æ„ï¼Œé‡‡ç”¨é‚»åŸŸæ³¨æ„åŠ›å±‚ä½œä¸ºå»å™ªæ¨¡å—ã€‚</li>
<li>æ¨¡å‹èƒ½é€æ¸é¢„æµ‹ç›®æ ‡é«˜åˆ†è¾¨ç‡é’¢ç´ä¹è°±ï¼ŒåŸºäºé¢„è®­ç»ƒå£°å­¦æ¨¡å‹çš„å¾®è°ƒç‰¹å¾è¿›è¡Œæ¡ä»¶å¤„ç†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µåº”ç”¨ä¸åŒè¿‡æ¸¡çŠ¶æ€çš„æ–°å‹ç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„ä¼˜åŒ–æ•ˆæœã€‚</li>
<li>åœ¨MAESTROæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–æ‰©æ•£æ¨¡å‹é’¢ç´è½¬å½•æ–¹æ³•å’ŒåŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0964bffbf1c1d6192cc6180e46eb1f87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44a494d972de7b54258131ca8a6c6ba2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92d3dd22fcd9d11ebc0f706a7dfe86a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f2016f224be32580eb20bd95e37c89a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7425f69833b2da851f65418d3858aedf.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MC-VTON-Minimal-Control-Virtual-Try-On-Diffusion-Transformer"><a href="#MC-VTON-Minimal-Control-Virtual-Try-On-Diffusion-Transformer" class="headerlink" title="MC-VTON: Minimal Control Virtual Try-On Diffusion Transformer"></a>MC-VTON: Minimal Control Virtual Try-On Diffusion Transformer</h2><p><strong>Authors:Junsheng Luan, Guangyuan Li, Lei Zhao, Wei Xing</strong></p>
<p>Virtual try-on methods based on diffusion models achieve realistic try-on effects. They use an extra reference network or an additional image encoder to process multiple conditional image inputs, which adds complexity pre-processing and additional computational costs. Besides, they require more than 25 inference steps, bringing longer inference time. In this work, with the development of diffusion transformer (DiT), we rethink the necessity of additional reference network or image encoder and introduce MC-VTON, which leverages DiTâ€™s intrinsic backbone to seamlessly integrate minimal conditional try-on inputs. Compared to existing methods, the superiority of MC-VTON is demonstrated in four aspects: (1) Superior detail fidelity. Our DiT-based MC-VTON exhibits superior fidelity in preserving fine-grained details. (2) Simplified network and inputs. We remove any extra reference network or image encoder. We also remove unnecessary conditions like the long prompt, pose estimation, human parsing, and depth map. We require only the masked person image and the garment image. (3) Parameter-efficient training. To process the try-on task, we fine-tune the FLUX.1-dev with only 39.7M additional parameters (0.33% of the backbone parameters). (4) Less inference steps. We apply distillation diffusion on MC-VTON and only need 8 steps to generate a realistic try-on image, with only 86.8M additional parameters (0.72% of the backbone parameters). Experiments show that MC-VTON achieves superior qualitative and quantitative results with fewer condition inputs, trainable parameters, and inference steps than baseline methods. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„è™šæ‹Ÿè¯•ç©¿æ–¹æ³•å®ç°äº†é€¼çœŸçš„è¯•ç©¿æ•ˆæœã€‚å®ƒä»¬ä½¿ç”¨é¢å¤–çš„å‚è€ƒç½‘ç»œæˆ–å›¾åƒç¼–ç å™¨æ¥å¤„ç†å¤šä¸ªæ¡ä»¶å›¾åƒè¾“å…¥ï¼Œè¿™å¢åŠ äº†é¢„å¤„ç†å’Œé¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œå®ƒä»¬éœ€è¦è¶…è¿‡25æ­¥æ¨ç†ï¼Œå¯¼è‡´æ¨ç†æ—¶é—´å»¶é•¿ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œéšç€æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰çš„å‘å±•ï¼Œæˆ‘ä»¬é‡æ–°æ€è€ƒäº†é¢å¤–å‚è€ƒç½‘ç»œæˆ–å›¾åƒç¼–ç å™¨çš„å¿…è¦æ€§ï¼Œå¹¶å¼•å…¥äº†MC-VTONï¼Œå®ƒåˆ©ç”¨DiTçš„å†…åœ¨éª¨å¹²æ¥æ— ç¼é›†æˆæœ€å°‘çš„æ¡ä»¶è¯•ç©¿è¾“å…¥ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMC-VTONåœ¨å››ä¸ªæ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼šï¼ˆ1ï¼‰å‡ºè‰²çš„ç»†èŠ‚ä¿çœŸåº¦ã€‚æˆ‘ä»¬åŸºäºDiTçš„MC-VTONåœ¨ä¿æŒç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šçš„ä¿çœŸåº¦ã€‚ï¼ˆ2ï¼‰ç®€åŒ–çš„ç½‘ç»œå’Œè¾“å…¥ã€‚æˆ‘ä»¬ç§»é™¤äº†ä»»ä½•é¢å¤–çš„å‚è€ƒç½‘ç»œæˆ–å›¾åƒç¼–ç å™¨ã€‚æˆ‘ä»¬è¿˜ç§»é™¤äº†ä¸å¿…è¦çš„æ¡ä»¶ï¼Œå¦‚é•¿æç¤ºã€å§¿åŠ¿ä¼°è®¡ã€äººä½“è§£æå’Œæ·±åº¦å›¾ã€‚æˆ‘ä»¬åªéœ€è¦é®ç½©çš„äººç‰©å›¾åƒå’Œæœè£…å›¾åƒã€‚ï¼ˆ3ï¼‰å‚æ•°é«˜æ•ˆçš„è®­ç»ƒã€‚ä¸ºäº†å¤„ç†è¯•ç©¿ä»»åŠ¡ï¼Œæˆ‘ä»¬åªä½¿ç”¨39.7Mé¢å¤–å‚æ•°ï¼ˆå ä¸»å¹²å‚æ•°çš„0.33%ï¼‰å¯¹FLUX.1-devè¿›è¡Œå¾®è°ƒã€‚ï¼ˆ4ï¼‰è¾ƒå°‘çš„æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬å¯¹MC-VTONåº”ç”¨è’¸é¦æ‰©æ•£ï¼Œåªéœ€8æ­¥å³å¯ç”Ÿæˆé€¼çœŸçš„è¯•ç©¿å›¾åƒï¼Œåªéœ€é¢å¤–çš„86.8Må‚æ•°ï¼ˆå ä¸»å¹²å‚æ•°çš„0.72%ï¼‰ã€‚å®éªŒè¡¨æ˜ï¼ŒMC-VTONåœ¨æ¡ä»¶è¾“å…¥ã€å¯è®­ç»ƒå‚æ•°å’Œæ¨ç†æ­¥éª¤æ–¹é¢æ¯”åŸºå‡†æ–¹æ³•æ›´å°‘ï¼Œä½†å®šæ€§å®šé‡ç»“æœæ›´ä¼˜è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03630v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„è™šæ‹Ÿè¯•ç©¿æ–¹æ³•èƒ½å®ç°é€¼çœŸçš„è¯•ç©¿æ•ˆæœã€‚å®ƒä»¬ä½¿ç”¨é¢å¤–çš„å‚è€ƒç½‘ç»œæˆ–å›¾åƒç¼–ç å™¨æ¥å¤„ç†å¤šä¸ªæ¡ä»¶å›¾åƒè¾“å…¥ï¼Œå¢åŠ äº†é¢„å¤„ç†å’Œé¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œå®ƒä»¬éœ€è¦è¶…è¿‡25æ­¥æ¨ç†ï¼Œå¯¼è‡´æ¨ç†æ—¶é—´è¾ƒé•¿ã€‚æœ¬ç ”ç©¶é€šè¿‡å‘å±•æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰é‡æ–°æ€è€ƒäº†é¢å¤–å‚è€ƒç½‘ç»œæˆ–å›¾åƒç¼–ç å™¨çš„å¿…è¦æ€§ï¼Œå¹¶å¼•å…¥äº†MC-VTONï¼Œå®ƒåˆ©ç”¨DiTçš„å†…åœ¨éª¨æ¶æ— ç¼é›†æˆäº†æœ€å°‘çš„æ¡ä»¶è¯•ç©¿è¾“å…¥ã€‚ç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼ŒMC-VTONåœ¨å››ä¸ªæ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼šç»†èŠ‚ä¿çœŸåº¦æ›´é«˜ã€ç½‘ç»œåŠè¾“å…¥ç®€åŒ–ã€å‚æ•°è®­ç»ƒæ•ˆç‡æ›´é«˜ã€æ¨ç†æ­¥éª¤æ›´å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è™šæ‹Ÿè¯•ç©¿æ–¹æ³•åŸºäºæ‰©æ•£æ¨¡å‹å®ç°é€¼çœŸæ•ˆæœã€‚</li>
<li>ä»¥å¾€æ–¹æ³•ä½¿ç”¨é¢å¤–ç½‘ç»œæˆ–ç¼–ç å™¨å¤„ç†å¤šæ¡ä»¶å›¾åƒï¼Œå¢åŠ å¤æ‚æ€§å’Œè®¡ç®—æˆæœ¬ã€‚</li>
<li>MC-VTONå¼•å…¥æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰ï¼Œæ— éœ€é¢å¤–ç½‘ç»œå’Œç¼–ç å™¨ã€‚</li>
<li>MC-VTONä»…éœ€è¦é®ç½©çš„äººç‰©å›¾åƒå’Œæœè£…å›¾åƒä½œä¸ºè¾“å…¥ã€‚</li>
<li>MC-VTONå‚æ•°è®­ç»ƒé«˜æ•ˆï¼Œåªéœ€39.7Mé¢å¤–å‚æ•°ã€‚</li>
<li>MC-VTONé€šè¿‡è’¸é¦æ‰©æ•£æŠ€æœ¯ï¼Œä»…éœ€8æ­¥ç”Ÿæˆç°å®è¯•ç©¿å›¾åƒã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒMC-VTONåœ¨æ¡ä»¶è¾“å…¥ã€å¯è®­ç»ƒå‚æ•°å’Œæ¨ç†æ­¥éª¤æ–¹é¢å‡ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a4e29958b99c66eb9f14c08d2d44b039.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fe3801031268c545ff0735cbb612cae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ed6d44f84b4a110fac6be547bf1af48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82488f36cdeeb2aebd92fe66f709ab10.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-911f2c2ef1648557b12dc043e124a168.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="K-space-Diffusion-Model-Based-MR-Reconstruction-Method-for-Simultaneous-Multislice-Imaging"><a href="#K-space-Diffusion-Model-Based-MR-Reconstruction-Method-for-Simultaneous-Multislice-Imaging" class="headerlink" title="K-space Diffusion Model Based MR Reconstruction Method for Simultaneous   Multislice Imaging"></a>K-space Diffusion Model Based MR Reconstruction Method for Simultaneous   Multislice Imaging</h2><p><strong>Authors:Ting Zhao, Zhuoxu Cui, Congcong Liu, Xingyang Wu, Yihang Zhou, Dong Liang, Haifeng Wang</strong></p>
<p>Simultaneous Multi-Slice(SMS) is a magnetic resonance imaging (MRI) technique which excites several slices concurrently using multiband radiofrequency pulses to reduce scanning time. However, due to its variable data structure and difficulty in acquisition, it is challenging to integrate SMS data as training data into deep learning frameworks.This study proposed a novel k-space diffusion model of SMS reconstruction that does not utilize SMS data for training. Instead, it incorporates Slice GRAPPA during the sampling process to reconstruct SMS data from different acquisition modes.Our results demonstrated that this method outperforms traditional SMS reconstruction methods and can achieve higher acceleration factors without in-plane aliasing. </p>
<blockquote>
<p>åŒæ­¥å¤šåˆ‡ç‰‡ï¼ˆSimultaneous Multi-Sliceï¼Œç®€ç§°SMSï¼‰æ˜¯ä¸€ç§ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æŠ€æœ¯ã€‚å®ƒé€šè¿‡å¤šé¢‘å¸¦å°„é¢‘è„‰å†²åŒæ—¶æ¿€å‘å¤šä¸ªåˆ‡ç‰‡ï¼Œä»¥å‡å°‘æ‰«ææ—¶é—´ã€‚ç„¶è€Œï¼Œç”±äºå…¶æ•°æ®ç»“æ„å¤šå˜ï¼Œé‡‡é›†éš¾åº¦é«˜ï¼Œå°†SMSæ•°æ®ä½œä¸ºè®­ç»ƒæ•°æ®é›†æˆåˆ°æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„kç©ºé—´æ‰©æ•£æ¨¡å‹ç”¨äºSMSé‡å»ºï¼Œè¯¥æ¨¡å‹ä¸ç›´æ¥ä½¿ç”¨SMSæ•°æ®è¿›è¡Œè®­ç»ƒã€‚ç›¸åï¼Œå®ƒåœ¨é‡‡æ ·è¿‡ç¨‹ä¸­é‡‡ç”¨äº†åˆ‡ç‰‡GRAPPAï¼ˆSlice GRAPPAï¼‰ï¼Œå¯ä»¥ä»ä¸åŒçš„é‡‡é›†æ¨¡å¼é‡å»ºSMSæ•°æ®ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä¼ ç»Ÿçš„SMSé‡å»ºæ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥åœ¨æ²¡æœ‰å¹³é¢æ··æ·†çš„æƒ…å†µä¸‹å®ç°æ›´é«˜çš„åŠ é€Ÿå› å­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03293v2">PDF</a> Accepted at the 2025 IEEE 22nd International Symposium on Biomedical   Imaging (ISBI)</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„kç©ºé—´æ‰©æ•£æ¨¡å‹ç”¨äºSMSé‡å»ºï¼Œè¯¥æ–¹æ³•ä¸ç›´æ¥ä½¿ç”¨SMSæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè€Œæ˜¯åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­èå…¥Slice GRAPPAæŠ€æœ¯ä»¥ä»ä¸åŒé‡‡é›†æ¨¡å¼é‡å»ºSMSæ•°æ®ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¼˜äºä¼ ç»ŸSMSé‡å»ºæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸å‡ºç°å¹³é¢æ··å çš„æƒ…å†µä¸‹å®ç°æ›´é«˜çš„åŠ é€Ÿå› å­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SMSæ˜¯MRIçš„ä¸€ç§æŠ€æœ¯ï¼Œé€šè¿‡å¤šé¢‘å¸¦å°„é¢‘è„‰å†²åŒæ—¶æ¿€å‘å¤šä¸ªåˆ‡ç‰‡æ¥ç¼©çŸ­æ‰«ææ—¶é—´ã€‚</li>
<li>å°†SMSæ•°æ®ä½œä¸ºè®­ç»ƒæ•°æ®é›†æˆåˆ°æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå…¶æ•°æ®ç»“æ„å¯å˜ä¸”é‡‡é›†å›°éš¾ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„kç©ºé—´æ‰©æ•£æ¨¡å‹ç”¨äºSMSé‡å»ºã€‚</li>
<li>è¯¥æ¨¡å‹ä¸ç›´æ¥ä½¿ç”¨SMSæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè€Œæ˜¯åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­èå…¥Slice GRAPPAæŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†æ›´é«˜çš„åŠ é€Ÿå› å­ï¼Œå¹¶ä¸”é¿å…äº†å¹³é¢æ··å çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„SMSé‡å»ºæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03293">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5946111ea1f95d72734c516345b2f0db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22aa40318d9f0fa2b883f57d7e45a9f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19c3eefe8af4e777ca1cde3b1570943d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b85ce8e7cd121bc67c268889b6930d21.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f65a0f70d67dfb12ff1a59d439836b1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Factorized-Diffusion-Perceptual-Illusions-by-Noise-Decomposition"><a href="#Factorized-Diffusion-Perceptual-Illusions-by-Noise-Decomposition" class="headerlink" title="Factorized Diffusion: Perceptual Illusions by Noise Decomposition"></a>Factorized Diffusion: Perceptual Illusions by Noise Decomposition</h2><p><strong>Authors:Daniel Geng, Inbum Park, Andrew Owens</strong></p>
<p>Given a factorization of an image into a sum of linear components, we present a zero-shot method to control each individual component through diffusion model sampling. For example, we can decompose an image into low and high spatial frequencies and condition these components on different text prompts. This produces hybrid images, which change appearance depending on viewing distance. By decomposing an image into three frequency subbands, we can generate hybrid images with three prompts. We also use a decomposition into grayscale and color components to produce images whose appearance changes when they are viewed in grayscale, a phenomena that naturally occurs under dim lighting. And we explore a decomposition by a motion blur kernel, which produces images that change appearance under motion blurring. Our method works by denoising with a composite noise estimate, built from the components of noise estimates conditioned on different prompts. We also show that for certain decompositions, our method recovers prior approaches to compositional generation and spatial control. Finally, we show that we can extend our approach to generate hybrid images from real images. We do this by holding one component fixed and generating the remaining components, effectively solving an inverse problem. </p>
<blockquote>
<p>ç»™å®šä¸€å¼ å›¾åƒè¢«åˆ†è§£ä¸ºä¸€ç³»åˆ—çº¿æ€§ç»„ä»¶çš„æ€»å’Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬æ–¹æ³•ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹é‡‡æ ·æ§åˆ¶æ¯ä¸ªå•ç‹¬ç»„ä»¶ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å°†å›¾åƒåˆ†è§£ä¸ºä½ç©ºé—´é¢‘ç‡å’Œé«˜ç©ºé—´é¢‘ç‡ï¼Œå¹¶æ ¹æ®ä¸åŒçš„æ–‡æœ¬æç¤ºè°ƒæ•´è¿™äº›ç»„ä»¶ã€‚è¿™äº§ç”Ÿäº†æ··åˆå›¾åƒï¼Œå…¶å¤–è§‚ä¼šæ ¹æ®è§‚çœ‹è·ç¦»è€Œå˜åŒ–ã€‚é€šè¿‡å°†å›¾åƒåˆ†è§£ä¸ºä¸‰ä¸ªé¢‘ç‡å­å¸¦ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‰ä¸ªæç¤ºç”Ÿæˆæ··åˆå›¾åƒã€‚æˆ‘ä»¬è¿˜é€šè¿‡ä½¿ç”¨ç°åº¦ä¸å½©è‰²ç»„ä»¶çš„åˆ†è§£æ¥ç”Ÿæˆåœ¨ç°åº¦æŸ¥çœ‹æ—¶å¤–è§‚ä¼šæ”¹å˜çš„å›¾åƒï¼Œè¿™æ˜¯åœ¨æ˜æš—å…‰çº¿ä¸‹çš„è‡ªç„¶å‘ç”Ÿçš„ç°è±¡ã€‚æˆ‘ä»¬è¿˜é€šè¿‡è¿åŠ¨æ¨¡ç³Šæ ¸è¿›è¡Œåˆ†è§£ï¼Œäº§ç”Ÿåœ¨è¿åŠ¨æ¨¡ç³Šä¸‹å¤–è§‚ä¼šæ”¹å˜çš„å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯é€šè¿‡ä½¿ç”¨åŸºäºä¸åŒæç¤ºæ¡ä»¶ç»„åˆçš„å™ªå£°ä¼°è®¡å»å™ªæ¥å·¥ä½œçš„ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œå¯¹äºæŸäº›åˆ†è§£ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ¢å¤å…ˆå‰å…³äºç»„åˆç”Ÿæˆå’Œç©ºé—´æ§åˆ¶çš„æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬èƒ½å¤Ÿå°†æˆ‘ä»¬çš„æ–¹æ³•æ‰©å±•åˆ°ä»çœŸå®å›¾åƒç”Ÿæˆæ··åˆå›¾åƒã€‚è¿™æ˜¯é€šè¿‡å°†æŸä¸ªç»„ä»¶å›ºå®šå¹¶ç”Ÿæˆå…¶ä½™ç»„ä»¶æ¥å®ç°çš„ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ä¸€ä¸ªåé—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.11615v2">PDF</a> ECCV 2024 camera ready version + more readable size</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬æ–¹æ³•ï¼Œé€šè¿‡åˆ†è§£å›¾åƒä¸ºå¤šä¸ªçº¿æ€§ç»„ä»¶ï¼Œå¹¶åˆ©ç”¨æ–‡æœ¬æç¤ºæ§åˆ¶å„ä¸ªç»„ä»¶ï¼Œç”Ÿæˆæ··åˆå›¾åƒã€‚è¿™ç§æŠ€æœ¯å¯ä»¥åˆ†è§£å›¾åƒä¸ºä¸åŒé¢‘ç‡å­å¸¦å’Œç°åº¦ä¸å½©è‰²ç»„ä»¶ï¼Œå¹¶ç”Ÿæˆåœ¨ä¸åŒè§‚çœ‹æ¡ä»¶ä¸‹å‘ˆç°ä¸åŒå¤–è§‚çš„æ··åˆå›¾åƒã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ¢ç´¢äº†åŸºäºè¿åŠ¨æ¨¡ç³Šæ ¸çš„åˆ†è§£æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†ä»çœŸå®å›¾åƒç”Ÿæˆæ··åˆå›¾åƒçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§é›¶æ ·æœ¬æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹æ§åˆ¶å›¾åƒçš„å„ä¸ªçº¿æ€§ç»„ä»¶ã€‚</li>
<li>å›¾åƒå¯ä»¥è¢«åˆ†è§£ä¸ºä¸åŒé¢‘ç‡å­å¸¦ï¼ˆå¦‚ä½ã€é«˜ç©ºé—´é¢‘ç‡ï¼‰ï¼Œå¹¶ä¾æ®ä¸åŒçš„æ–‡æœ¬æç¤ºè¿›è¡Œæ¡ä»¶æ§åˆ¶ã€‚</li>
<li>ç”Ÿæˆæ··åˆå›¾åƒï¼Œå…¶å¤–è§‚éšè§‚çœ‹è·ç¦»å˜åŒ–ã€‚</li>
<li>ä½¿ç”¨ç°åº¦ä¸å½©è‰²ç»„ä»¶åˆ†è§£ï¼Œç”Ÿæˆåœ¨ç°åº¦æ¨¡å¼ä¸‹å‘ˆç°ä¸åŒå¤–è§‚çš„å›¾åƒã€‚</li>
<li>é€šè¿‡ç»„åˆå™ªå£°ä¼°è®¡å€¼çš„åˆ†è§£æ–¹æ³•ï¼Œå®ç°å›¾åƒå»å™ªã€‚</li>
<li>åœ¨æŸäº›åˆ†è§£ä¸‹ï¼Œæ­¤æ–¹æ³•èƒ½å¤Ÿæ¢å¤å…ˆå‰çš„ç»„åˆç”Ÿæˆæ–¹æ³•å’Œç©ºé—´æ§åˆ¶æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.11615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe5c5f256fca0c8512d959b56c2e558c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb722e73b6fcd94df9d01bac9fcdf0a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-145496b139981e04cb5f9cc28ddcd16b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Magic-Boost-Boost-3D-Generation-with-Multi-View-Conditioned-Diffusion"><a href="#Magic-Boost-Boost-3D-Generation-with-Multi-View-Conditioned-Diffusion" class="headerlink" title="Magic-Boost: Boost 3D Generation with Multi-View Conditioned Diffusion"></a>Magic-Boost: Boost 3D Generation with Multi-View Conditioned Diffusion</h2><p><strong>Authors:Fan Yang, Jianfeng Zhang, Yichun Shi, Bowen Chen, Chenxu Zhang, Huichao Zhang, Xiaofeng Yang, Xiu Li, Jiashi Feng, Guosheng Lin</strong></p>
<p>Benefiting from the rapid development of 2D diffusion models, 3D content generation has witnessed significant progress. One promising solution is to finetune the pre-trained 2D diffusion models to produce multi-view images and then reconstruct them into 3D assets via feed-forward sparse-view reconstruction models. However, limited by the 3D inconsistency in the generated multi-view images and the low reconstruction resolution of the feed-forward reconstruction models, the generated 3d assets are still limited to incorrect geometries and blurry textures. To address this problem, we present a multi-view based refine method, named Magic-Boost, to further refine the generation results. In detail, we first propose a novel multi-view conditioned diffusion model which extracts 3d prior from the synthesized multi-view images to synthesize high-fidelity novel view images and then introduce a novel iterative-update strategy to adopt it to provide precise guidance to refine the coarse generated results through a fast optimization process. Conditioned on the strong 3d priors extracted from the synthesized multi-view images, Magic-Boost is capable of providing precise optimization guidance that well aligns with the coarse generated 3D assets, enriching the local detail in both geometry and texture within a short time ($\sim15$min). Extensive experiments show Magic-Boost greatly enhances the coarse generated inputs, generates high-quality 3D assets with rich geometric and textural details. (Project Page: <a target="_blank" rel="noopener" href="https://magic-research.github.io/magic-boost/">https://magic-research.github.io/magic-boost/</a>) </p>
<blockquote>
<p>å¾—ç›ŠäºäºŒç»´æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œä¸‰ç»´å†…å®¹ç”Ÿæˆä¹Ÿå–å¾—äº†é‡å¤§è¿›å±•ã€‚ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆæ˜¯é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå¤šè§†è§’å›¾åƒï¼Œç„¶åé€šè¿‡å‰é¦ˆç¨€ç–è§†å›¾é‡å»ºæ¨¡å‹å°†å®ƒä»¬é‡å»ºä¸ºä¸‰ç»´èµ„äº§ã€‚ç„¶è€Œï¼Œç”±äºç”Ÿæˆçš„å¤šè§†è§’å›¾åƒä¸­çš„ä¸‰ç»´ä¸ä¸€è‡´æ€§å’Œå‰é¦ˆé‡å»ºæ¨¡å‹è¾ƒä½çš„é‡å»ºåˆ†è¾¨ç‡ï¼Œç”Ÿæˆçš„3Dèµ„äº§ä»ç„¶å­˜åœ¨å‡ ä½•é”™è¯¯å’Œçº¹ç†æ¨¡ç³Šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šè§†è§’çš„ç»†åŒ–æ–¹æ³•ï¼Œåä¸ºMagic-Boostï¼Œè¿›ä¸€æ­¥æ”¹è¿›äº†ç”Ÿæˆç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šè§†è§’æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»åˆæˆçš„å¤šè§†è§’å›¾åƒä¸­æå–ä¸‰ç»´å…ˆéªŒçŸ¥è¯†ï¼Œåˆæˆé«˜ä¿çœŸåº¦çš„æ–°è§†è§’å›¾åƒï¼Œç„¶åå¼•å…¥äº†ä¸€ç§æ–°å‹è¿­ä»£æ›´æ–°ç­–ç•¥ï¼Œå°†å…¶åº”ç”¨äºé€šè¿‡å¿«é€Ÿä¼˜åŒ–è¿‡ç¨‹å¯¹ç²—ç•¥ç”Ÿæˆçš„ç»“æœè¿›è¡Œç²¾ç»†æŒ‡å¯¼ã€‚åŸºäºä»åˆæˆå¤šè§†è§’å›¾åƒä¸­æå–çš„å¼ºå¤§ä¸‰ç»´å…ˆéªŒçŸ¥è¯†ï¼ŒMagic-Boostèƒ½å¤Ÿæä¾›ç²¾ç¡®çš„ä¼˜åŒ–æŒ‡å¯¼ï¼Œä¸ç²—ç•¥ç”Ÿæˆçš„3Dèµ„äº§é«˜åº¦å¯¹é½ï¼Œåœ¨çŸ­æ—¶é—´å†…ï¼ˆ~15åˆ†é’Ÿï¼‰ä¸°å¯Œå‡ ä½•å’Œçº¹ç†çš„å±€éƒ¨ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMagic-Boostå¤§å¤§å¢å¼ºäº†ç²—ç•¥ç”Ÿæˆçš„è¾“å…¥ï¼Œç”Ÿæˆäº†é«˜è´¨é‡ã€å…·æœ‰ä¸°å¯Œå‡ ä½•å’Œçº¹ç†ç»†èŠ‚çš„3Dèµ„äº§ã€‚ï¼ˆé¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://magic-research.github.io/magic-boost/%EF%BC%89">https://magic-research.github.io/magic-boost/ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.06429v3">PDF</a> </p>
<p><strong>Summary</strong><br>     å¾—ç›Šäº2Dæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œ3Då†…å®¹ç”Ÿæˆé¢†åŸŸå·²å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆæ˜¯é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹ä»¥ç”Ÿæˆå¤šè§†è§’å›¾åƒï¼Œç„¶åé€šè¿‡å‰é¦ˆç¨€ç–è§†å›¾é‡å»ºæ¨¡å‹å°†å…¶é‡å»ºä¸º3Dèµ„äº§ã€‚ç„¶è€Œï¼Œç”±äºç”Ÿæˆçš„å¤šè§†è§’å›¾åƒä¸­çš„3Dä¸ä¸€è‡´æ€§å’Œå‰é¦ˆé‡å»ºæ¨¡å‹çš„ä½é‡å»ºåˆ†è¾¨ç‡é™åˆ¶ï¼Œç”Ÿæˆçš„3Dèµ„äº§ä»å­˜åœ¨å‡ ä½•ä¸æ­£ç¡®å’Œçº¹ç†æ¨¡ç³Šçš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šè§†è§’çš„ç»†åŒ–æ–¹æ³•Magic-Boostï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ç”Ÿæˆç»“æœã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºä¸€ç§æ–°å‹çš„å¤šè§†è§’æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œä»åˆæˆçš„å¤šè§†è§’å›¾åƒä¸­æå–3Då…ˆéªŒçŸ¥è¯†æ¥åˆæˆé«˜ä¿çœŸåº¦çš„æ–°è§†è§’å›¾åƒï¼Œå¹¶å¼•å…¥ä¸€ç§æ–°å‹è¿­ä»£æ›´æ–°ç­–ç•¥æ¥é€‚åº”å®ƒï¼Œä»¥é€šè¿‡å¿«é€Ÿä¼˜åŒ–è¿‡ç¨‹ä¸ºç²—ç³™çš„ç”Ÿæˆç»“æœæä¾›ç²¾ç¡®çš„æŒ‡å¯¼ã€‚åŸºäºä»åˆæˆå¤šè§†è§’å›¾åƒä¸­æå–çš„å¼ºå¤§3Då…ˆéªŒçŸ¥è¯†ï¼ŒMagic-Boostèƒ½å¤Ÿæä¾›ä¸ç²—ç³™ç”Ÿæˆçš„3Dèµ„äº§é«˜åº¦å»åˆçš„ç²¾ç¡®ä¼˜åŒ–æŒ‡å¯¼ï¼Œåœ¨çŸ­æ—¶é—´å†…ï¼ˆ~15åˆ†é’Ÿï¼‰ä¸°å¯Œå‡ ä½•å’Œçº¹ç†çš„å±€éƒ¨ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMagic-Boostæå¤§åœ°æé«˜äº†ç²—ç³™çš„ç”Ÿæˆè¾“å…¥ï¼Œç”Ÿæˆäº†é«˜è´¨é‡ã€ç»†èŠ‚ä¸°å¯Œçš„3Dèµ„äº§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Då†…å®¹ç”Ÿæˆé¢†åŸŸå—ç›Šäº2Dæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œå·²ç»å–å¾—æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šè§†è§’å›¾åƒï¼Œç„¶åé€šè¿‡å‰é¦ˆç¨€ç–è§†å›¾é‡å»ºæ¨¡å‹å°†å…¶è½¬åŒ–ä¸º3Dèµ„äº§æ˜¯ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å½“å‰æ–¹æ³•é¢ä¸´ç”Ÿæˆçš„å¤šè§†è§’å›¾åƒä¸­çš„3Dä¸ä¸€è‡´æ€§å’Œå‰é¦ˆé‡å»ºæ¨¡å‹çš„ä½é‡å»ºåˆ†è¾¨ç‡çš„é—®é¢˜ï¼Œå¯¼è‡´ç”Ÿæˆçš„3Dèµ„äº§å­˜åœ¨å‡ ä½•ä¸æ­£ç¡®å’Œçº¹ç†æ¨¡ç³Šçš„é—®é¢˜ã€‚</li>
<li>Magic-Boostæ˜¯ä¸€ç§åŸºäºå¤šè§†è§’çš„ç»†åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡æå–å¤šè§†è§’å›¾åƒçš„3Då…ˆéªŒçŸ¥è¯†æ¥ä¼˜åŒ–ç”Ÿæˆç»“æœã€‚</li>
<li>Magic-Booståˆæˆé«˜ä¿çœŸåº¦çš„æ–°è§†è§’å›¾åƒï¼Œå¹¶å¼•å…¥æ–°å‹è¿­ä»£æ›´æ–°ç­–ç•¥æ¥ç²¾ç¡®æŒ‡å¯¼ä¼˜åŒ–è¿‡ç¨‹ã€‚</li>
<li>Magic-Booståˆ©ç”¨å¼ºå¤§çš„3Då…ˆéªŒçŸ¥è¯†ï¼Œåœ¨çŸ­æ—¶é—´å†…ä¸°å¯Œå‡ ä½•å’Œçº¹ç†çš„å±€éƒ¨ç»†èŠ‚ã€‚</li>
<li>å®éªŒè¯æ˜Magic-Boostèƒ½æ˜¾è‘—æé«˜ç”Ÿæˆè´¨é‡ï¼Œç”Ÿæˆç»†èŠ‚ä¸°å¯Œçš„3Dèµ„äº§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.06429">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-df62ca8b7d54831419eafd26744effe2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c42c826143996d4d401b6904c54cbe66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b8988b07e744eb3dc19c9fc90f6a974.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a374cf29144fd1c01be3d35c827197a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f0f33a094a9658ff5689cf67fb9df83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e9340ded73d0a52eed397d9a332c8ff.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Discriminative-Class-Tokens-for-Text-to-Image-Diffusion-Models"><a href="#Discriminative-Class-Tokens-for-Text-to-Image-Diffusion-Models" class="headerlink" title="Discriminative Class Tokens for Text-to-Image Diffusion Models"></a>Discriminative Class Tokens for Text-to-Image Diffusion Models</h2><p><strong>Authors:Idan Schwartz, VÃ©steinn SnÃ¦bjarnarson, Hila Chefer, Ryan Cotterell, Serge Belongie, Lior Wolf, Sagie Benaim</strong></p>
<p>Recent advances in text-to-image diffusion models have enabled the generation of diverse and high-quality images. While impressive, the images often fall short of depicting subtle details and are susceptible to errors due to ambiguity in the input text. One way of alleviating these issues is to train diffusion models on class-labeled datasets. This approach has two disadvantages: (i) supervised datasets are generally small compared to large-scale scraped text-image datasets on which text-to-image models are trained, affecting the quality and diversity of the generated images, or (ii) the input is a hard-coded label, as opposed to free-form text, limiting the control over the generated images.   In this work, we propose a non-invasive fine-tuning technique that capitalizes on the expressive potential of free-form text while achieving high accuracy through discriminative signals from a pretrained classifier. This is done by iteratively modifying the embedding of an added input token of a text-to-image diffusion model, by steering generated images toward a given target class according to a classifier. Our method is fast compared to prior fine-tuning methods and does not require a collection of in-class images or retraining of a noise-tolerant classifier. We evaluate our method extensively, showing that the generated images are: (i) more accurate and of higher quality than standard diffusion models, (ii) can be used to augment training data in a low-resource setting, and (iii) reveal information about the data used to train the guiding classifier. The code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/idansc/discriminative_class_tokens%7D">https://github.com/idansc/discriminative_class_tokens}</a>. </p>
<blockquote>
<p>æœ€è¿‘æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¿›æ­¥ä½¿å¾—èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„å›¾åƒã€‚è™½ç„¶ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†è¿™äº›å›¾åƒå¾€å¾€æ— æ³•æç»˜å‡ºç»†å¾®çš„ç»†èŠ‚ï¼Œå¹¶ä¸”ç”±äºè¾“å…¥æ–‡æœ¬çš„æ¨¡ç³Šæ€§ï¼Œå®¹æ˜“å‡ºç°é”™è¯¯ã€‚ç¼“è§£è¿™äº›é—®é¢˜çš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨ç±»åˆ«æ ‡ç­¾æ•°æ®é›†è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•æœ‰ä¸¤ä¸ªç¼ºç‚¹ï¼šä¸€æ˜¯ä¸ç”¨äºè®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¤§è§„æ¨¡çˆ¬å–çš„æ–‡æœ¬å›¾åƒæ•°æ®é›†ç›¸æ¯”ï¼Œç›‘ç£æ•°æ®é›†é€šå¸¸è§„æ¨¡è¾ƒå°ï¼Œå½±å“ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¤šæ ·æ€§ï¼›äºŒæ˜¯è¾“å…¥æ˜¯ç¡¬ç¼–ç çš„æ ‡ç­¾ï¼Œè€Œä¸æ˜¯è‡ªç”±å½¢å¼çš„æ–‡æœ¬ï¼Œé™åˆ¶äº†ç”Ÿæˆå›¾åƒçš„æ§åˆ¶æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2303.17155v4">PDF</a> ICCV 2023</p>
<p><strong>æ‘˜è¦</strong><br>    è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¿›å±•ä½¿å¾—ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„å›¾åƒæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œç”Ÿæˆçš„å›¾åƒå¾€å¾€ç¼ºä¹ç»†å¾®çš„ç»†èŠ‚æç»˜ï¼Œå¹¶ä¸”ç”±äºè¾“å…¥æ–‡æœ¬çš„æ­§ä¹‰è€Œå®¹æ˜“å‡ºç°é”™è¯¯ã€‚ä¸ºç¼“è§£è¿™äº›é—®é¢˜ï¼Œä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨åˆ†ç±»æ ‡ç­¾æ•°æ®é›†è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚ä½†è¿™ç§æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªç¼ºç‚¹ï¼šä¸€æ˜¯ç›‘ç£æ•°æ®é›†é€šå¸¸æ¯”ç”¨äºè®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¤§è§„æ¨¡æŠ“å–æ–‡æœ¬å›¾åƒæ•°æ®é›†è¦å°ï¼Œå½±å“ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¤šæ ·æ€§ï¼›äºŒæ˜¯è¾“å…¥æ˜¯ç¡¬ç¼–ç æ ‡ç­¾ï¼Œè€Œéè‡ªç”±å½¢å¼çš„æ–‡æœ¬ï¼Œé™åˆ¶äº†ç”Ÿæˆå›¾åƒçš„æ§åˆ¶èƒ½åŠ›ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§éä¾µå…¥å¼å¾®è°ƒæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯åˆ©ç”¨è‡ªç”±å½¢å¼æ–‡æœ¬çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶é€šè¿‡é¢„è®­ç»ƒåˆ†ç±»å™¨çš„åˆ¤åˆ«ä¿¡å·å®ç°é«˜ç²¾åº¦ã€‚è¿™æ˜¯é€šè¿‡è¿­ä»£ä¿®æ”¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­æ·»åŠ è¾“å…¥æ ‡è®°çš„åµŒå…¥æ¥å®ç°çš„ï¼Œå°†ç”Ÿæˆçš„å›¾åƒå¯¼å‘ç»™å®šçš„ç›®æ ‡ç±»åˆ«ï¼Œå¹¶æ ¹æ®åˆ†ç±»å™¨è¿›è¡Œè°ƒæ•´ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸å…ˆå‰çš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”é€Ÿåº¦æ›´å¿«ï¼Œå¹¶ä¸”ä¸éœ€è¦æ”¶é›†åŒç±»å›¾åƒæˆ–é‡æ–°è®­ç»ƒå™ªå£°å®¹å¿åˆ†ç±»å™¨ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„æ–¹æ³•è¯„ä¼°ï¼Œè¯æ˜ç”Ÿæˆçš„å›¾åƒæ¯”æ ‡å‡†æ‰©æ•£æ¨¡å‹æ›´å‡†ç¡®ã€è´¨é‡æ›´é«˜ï¼Œå¯ç”¨äºå¢å¼ºä½èµ„æºè®¾ç½®ä¸­çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶æ­ç¤ºç”¨äºè®­ç»ƒæŒ‡å¯¼åˆ†ç±»å™¨çš„æ•°æ®çš„ä¿¡æ¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„å›¾åƒï¼Œä½†åœ¨æç»˜ç»†å¾®ç»†èŠ‚æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸”æ˜“å—è¾“å…¥æ–‡æœ¬æ­§ä¹‰çš„å½±å“ã€‚</li>
<li>ä½¿ç”¨åˆ†ç±»æ ‡ç­¾æ•°æ®é›†è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦ç¼ºç‚¹ï¼šæ•°æ®é›†å¤§å°æœ‰é™ï¼Œä»¥åŠè¾“å…¥å½¢å¼é™åˆ¶ï¼ˆç¡¬ç¼–ç æ ‡ç­¾è€Œéè‡ªç”±å½¢å¼æ–‡æœ¬ï¼‰ï¼Œè¿™é™åˆ¶äº†ç”Ÿæˆå›¾åƒçš„æ§åˆ¶èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§éä¾µå…¥å¼å¾®è°ƒæŠ€æœ¯ï¼Œç»“åˆè‡ªç”±å½¢å¼æ–‡æœ¬çš„è¡¨è¾¾èƒ½åŠ›ä¸é¢„è®­ç»ƒåˆ†ç±»å™¨çš„åˆ¤åˆ«ä¿¡å·ï¼Œä»¥æé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£ä¿®æ”¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­å¢åŠ è¾“å…¥æ ‡è®°çš„åµŒå…¥æ¥å®ç°ï¼Œä½¿ç”Ÿæˆçš„å›¾åƒèƒ½å¤Ÿå¯¼å‘ç‰¹å®šç±»åˆ«ï¼Œå¹¶æ ¹æ®åˆ†ç±»å™¨è¿›è¡Œè°ƒæ•´ã€‚</li>
<li>ä¸å…¶ä»–å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´å¿«çš„é€Ÿåº¦ï¼Œä¸”ä¸éœ€è¦æ”¶é›†åŒç±»å›¾åƒæˆ–é‡æ–°è®­ç»ƒå™ªå£°å®¹å¿åˆ†ç±»å™¨ã€‚</li>
<li>è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„å›¾åƒå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œè´¨é‡ï¼Œå¯ç”¨äºå¢å¼ºä½èµ„æºç¯å¢ƒä¸­çš„è®­ç»ƒæ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2303.17155">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dfbb62be9f6ac0d1266dbea49808349f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efb094756f3ab73c14f670da0f0dec99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44b49bfb4f4d4f53dc7c5e70bfa8ea65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dabe42fb5508dd9b7d1736b982fb42ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e83d6710a2f661028c9e19dbe9e6a69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53e9c87c8e54fd33ce4ce3ae444e0eac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07d211abe5ac618ce65bbc60a108aa50.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-14/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-14/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-853b531abe72fe60dae09301d06339e9.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-14  Real-Time Textless Dialogue Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a5dd8467341555d03e377166d4f53cfd.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-14  Low-Resource Text-to-Speech Synthesis Using Noise-Augmented Training of   ForwardTacotron
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26254.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
