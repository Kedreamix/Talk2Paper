<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-01-14  AgroGPT Efficient Agricultural Vision-Language Model with Expert Tuning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-cc41d6a4994aa580d50294bdd66e41dc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    35 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-14-更新"><a href="#2025-01-14-更新" class="headerlink" title="2025-01-14 更新"></a>2025-01-14 更新</h1><h2 id="AgroGPT-Efficient-Agricultural-Vision-Language-Model-with-Expert-Tuning"><a href="#AgroGPT-Efficient-Agricultural-Vision-Language-Model-with-Expert-Tuning" class="headerlink" title="AgroGPT: Efficient Agricultural Vision-Language Model with Expert Tuning"></a>AgroGPT: Efficient Agricultural Vision-Language Model with Expert Tuning</h2><p><strong>Authors:Muhammad Awais, Ali Husain Salem Abdulla Alharthi, Amandeep Kumar, Hisham Cholakkal, Rao Muhammad Anwer</strong></p>
<p>Significant progress has been made in advancing large multimodal conversational models (LMMs), capitalizing on vast repositories of image-text data available online. Despite this progress, these models often encounter substantial domain gaps, hindering their ability to engage in complex conversations across new domains. Recent efforts have aimed to mitigate this issue, albeit relying on domain-specific image-text data to curate instruction-tuning data. However, many domains, such as agriculture, lack such vision-language data. In this work, we propose an approach to construct instruction-tuning data that harnesses vision-only data for the agriculture domain. We utilize diverse agricultural datasets spanning multiple domains, curate class-specific information, and employ large language models (LLMs) to construct an expert-tuning set, resulting in a 70k expert-tuning dataset called AgroInstruct. Subsequently, we expert-tuned and created AgroGPT, an efficient LMM that can hold complex agriculture-related conversations and provide useful insights. We also develop AgroEvals for evaluation and compare {AgroGPT’s} performance with large open and closed-source models. {AgroGPT} excels at identifying fine-grained agricultural concepts, can act as an agriculture expert, and provides helpful information for multimodal agriculture questions. The code, datasets, and models are available at <a target="_blank" rel="noopener" href="https://github.com/awaisrauf/agroGPT">https://github.com/awaisrauf/agroGPT</a>. </p>
<blockquote>
<p>在推进大型多模态对话模型（LMMs）方面取得了重大进展，这得益于网络上可用的海量图像文本数据仓库。尽管取得了这些进展，但这些模型仍然经常遇到巨大的领域差距，阻碍了它们在新领域进行复杂对话的能力。最近的努力旨在缓解这个问题，尽管这依赖于特定领域的图像文本数据来策划指令微调数据。然而，许多领域，如农业，缺乏这样的视觉语言数据。在这项工作中，我们提出了一种构建指令微调数据的方法，该方法利用仅视觉数据针对农业领域。我们利用跨越多个领域的各种农业数据集，整理特定类别的信息，并采用大型语言模型（LLMs）来构建专家调整集，从而形成了名为AgroInstruct的70k专家调整数据集。随后，我们进行了专家调整并创建了AgrGPT，这是一个有效的LMM，可以进行复杂的农业相关对话并提供有用的见解。我们还开发了AgroEvals进行评估，并将AgrGPT的性能与大型开源和闭源模型进行比较。AgrGPT擅长识别农业中的细微概念，可以作为农业专家发挥作用，并为多模态农业问题提供有用的信息。代码、数据集和模型可在<a target="_blank" rel="noopener" href="https://github.com/awaisrauf/agroGPT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/awaisrauf/agroGPT找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.08405v2">PDF</a> Accepted at WACV, 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对农业领域的大型多模态对话模型（LMMs）的研究。研究团队利用仅有视觉数据构建了农业领域的指令调优数据集AgroInstruct，并以此为基础训练出能进行复杂农业相关对话并提供有用见解的专家级模型AgroGPT。该模型在农业领域的精细概念识别方面表现出色，可作为农业专家提供信息。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究进展：大型多模态对话模型（LMMs）在利用丰富的在线图像文本数据方面取得了显著进展。</li>
<li>领域差距问题：尽管有这些进展，但模型在新领域进行复杂对话时仍会遇到显著的领域差距问题。</li>
<li>现有解决方案的不足：尽管有努力通过域特定的图像文本数据来减轻这个问题，但在诸如农业等领域，由于缺乏此类视觉语言数据，这一方法并不适用。</li>
<li>新方法介绍：提出了一种利用仅有视觉数据构建农业领域的指令调优数据集的方法，并命名为AgroInstruct。</li>
<li>专家级模型的训练：基于AgroInstruct数据集训练出了名为AgroGPT的专家级模型，能够进行复杂的农业相关对话并提供有用信息。</li>
<li>模型性能评估：开发了AgroEvals用于评估AgroGPT的性能，并与大型开源和闭源模型进行了比较。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.08405">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4fb6bb063106a83d1b181828e153836b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae074963b022cb9e9b9e23706022a6c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46d778d88f41e24c0299f92e8fe3b503.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-888f0ac51d6936a853074049556d541e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fb55857815a09adfea2ba8e8c367e4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37af65f3d1437afd0a1118ae7c35ac8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e7c17ea3e65075ab0ef5a8d2cda2ccc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MacST-Multi-Accent-Speech-Synthesis-via-Text-Transliteration-for-Accent-Conversion"><a href="#MacST-Multi-Accent-Speech-Synthesis-via-Text-Transliteration-for-Accent-Conversion" class="headerlink" title="MacST: Multi-Accent Speech Synthesis via Text Transliteration for Accent   Conversion"></a>MacST: Multi-Accent Speech Synthesis via Text Transliteration for Accent   Conversion</h2><p><strong>Authors:Sho Inoue, Shuai Wang, Wanxing Wang, Pengcheng Zhu, Mengxiao Bi, Haizhou Li</strong></p>
<p>In accented voice conversion or accent conversion, we seek to convert the accent in speech from one another while preserving speaker identity and semantic content. In this study, we formulate a novel method for creating multi-accented speech samples, thus pairs of accented speech samples by the same speaker, through text transliteration for training accent conversion systems. We begin by generating transliterated text with Large Language Models (LLMs), which is then fed into multilingual TTS models to synthesize accented English speech. As a reference system, we built a sequence-to-sequence model on the synthetic parallel corpus for accent conversion. We validated the proposed method for both native and non-native English speakers. Subjective and objective evaluations further validate our dataset’s effectiveness in accent conversion studies. </p>
<blockquote>
<p>在口音转换或语音转换中，我们旨在将语音中的口音进行转换，同时保留说话者的身份和语义内容。本研究中，我们提出了一种新的方法，通过文本转译来创建多口音语音样本，从而为口音转换系统训练提供同一说话人的带口音语音样本对。我们首先使用大型语言模型（LLM）生成转译文本，然后将其输入多语言TTS模型，合成带有口音的英语语音。作为参考系统，我们在合成平行语料库上建立了序列到序列的模型进行口音转换。我们验证了所提出的方法对于英语母语者和非母语者都有效。主观和客观评估进一步验证了我们的数据集在口音转换研究中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09352v2">PDF</a> This is accepted to IEEE ICASSP 2025; Project page with Speech Demo:   <a target="_blank" rel="noopener" href="https://github.com/shinshoji01/MacST-project-page">https://github.com/shinshoji01/MacST-project-page</a></p>
<p><strong>Summary</strong></p>
<p>在口音转换研究中，本文提出了一种新型的多口音语音样本创建方法。通过大型语言模型（LLM）进行文本转译，生成口音样本对，用于训练口音转换系统。实验结果显示，该方法能有效实现口音转换，同时保留说话人身份和语义内容。对本地和非本地英语说话者均进行了验证。主观和客观评估进一步验证了数据集在口音转换研究中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文提出了使用大型语言模型（LLM）生成文本转译来创建多口音语音样本的方法。</li>
<li>创建的口音样本对用于训练口音转换系统。</li>
<li>说话人的身份和语义内容在口音转换过程中得以保留。</li>
<li>该方法既适用于本地英语说话者也适用于非本地英语说话者。</li>
<li>主观和客观评估验证了数据集的有效性。</li>
<li>本文采用了一种序列到序列模型进行口音转换。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09352">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bda998ee7c6539ce6d05781d2e0e4d1c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-081b421c361ba539819a9c6ba2a70296.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-255fe0b982a65b5a7f38cd3d865e82da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fbf0627b83804e6fad6f45ab6436e4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78b865ce5c3ba31b1c0a8168716a2d00.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="The-Mamba-in-the-Llama-Distilling-and-Accelerating-Hybrid-Models"><a href="#The-Mamba-in-the-Llama-Distilling-and-Accelerating-Hybrid-Models" class="headerlink" title="The Mamba in the Llama: Distilling and Accelerating Hybrid Models"></a>The Mamba in the Llama: Distilling and Accelerating Hybrid Models</h2><p><strong>Authors:Junxiong Wang, Daniele Paliotta, Avner May, Alexander M. Rush, Tri Dao</strong></p>
<p>Linear RNN architectures, like Mamba, can be competitive with Transformer models in language modeling while having advantageous deployment characteristics. Given the focus on training large-scale Transformer models, we consider the challenge of converting these pretrained models for deployment. We demonstrate that it is feasible to distill large Transformers into linear RNNs by reusing the linear projection weights from attention layers with academic GPU resources. The resulting hybrid model, which incorporates a quarter of the attention layers, achieves performance comparable to the original Transformer in chat benchmarks and outperforms open-source hybrid Mamba models trained from scratch with trillions of tokens in both chat benchmarks and general benchmarks. Moreover, we introduce a hardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid models. Overall we show how, with limited computation resources, we can remove many of the original attention layers and generate from the resulting model more efficiently. Our top-performing model, distilled from Llama3-8B-Instruct, achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and 7.35 on MT-Bench, surpassing the best 8B scale instruction-tuned linear RNN model. We also find that the distilled model has natural length extrapolation, showing almost perfect accuracy in the needle-in-a-haystack test at 20x the distillation length. Code and pre-trained checkpoints are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/jxiw/MambaInLlama">https://github.com/jxiw/MambaInLlama</a> and <a target="_blank" rel="noopener" href="https://github.com/itsdaniele/speculative_mamba">https://github.com/itsdaniele/speculative_mamba</a>. </p>
<blockquote>
<p>线性RNN架构（如Mamba）在语言建模方面可以与Transformer模型竞争，同时拥有优势部署特性。鉴于目前主要关注训练大规模Transformer模型，我们面临将这些预训练模型进行部署的挑战。我们证明了通过利用GPU资源重复使用注意力层的线性投影权重，将大型Transformer蒸馏成线性RNN是可行的。所得混合模型结合了四分之一的注意力层，在聊天基准测试中实现了与原始Transformer相当的性能，并且在聊天基准测试和一般基准测试中表现优于从头开始训练的开源混合Mamba模型（这些模型训练时使用了万亿个令牌）。此外，我们引入了一种硬件感知的投机解码算法，该算法加速了Mamba和混合模型的推理速度。总体而言，我们展示了如何在有限的计算资源下删除许多原始注意力层并更高效地生成结果模型。我们的高性能模型从Llama3-8B-Instruct中提炼出来，在AlpacaEval 2上相对于GPT-4达到了29.61的长度控制胜率，并在MT-Bench上达到了7.35，超过了最佳8B规模指令调整线性RNN模型。我们还发现，提炼后的模型具有自然的长度扩展能力，在蒸馏长度提高20倍的情况下，几乎达到了完美的准确率在“海底捞针”测试中。代码和预先训练的模型检查点已开源分享至：<a target="_blank" rel="noopener" href="https://github.com/jxiw/MambaInLlama%E5%92%8Chttps://github.com/itsdaniele/speculative_mamba%E3%80%82">https://github.com/jxiw/MambaInLlama和https://github.com/itsdaniele/speculative_mamba。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.15237v3">PDF</a> NeurIPS 2024. v3 updates: fix format errors</p>
<p><strong>摘要</strong></p>
<p>线性RNN架构如Mamba，在语言建模方面可与Transformer模型竞争，并具有优势部署特性。研究挑战在于将大型预训练Transformer模型进行部署转换。研究展示了用学术GPU资源通过复用注意力层的线性投影权重来蒸馏大型Transformer为线性RNN的可行性。得到的混合模型只使用了四分之一的注意力层，在聊天基准测试中表现与原始Transformer相当，并且在聊天基准测试和一般基准测试中表现优于从头开始训练的开源混合Mamba模型。此外，研究还引入了一种硬件感知的投机解码算法，提高了Mamba和混合模型的推理速度。总体上，研究展示了如何在有限的计算资源下减少许多原始注意力层并更高效地生成模型。最佳性能的模型是从Llama3-8B-Instruct中提炼出来的，在AlpacaEval 2上相对于GPT-4取得了29.61的长度控制胜率，在MT-Bench上取得了7.35的成绩，超过了最佳8B规模指令调整线性RNN模型。还发现提炼的模型具有自然长度外推能力，在蒸馏长度提高20倍的情况下，几乎达到了完美的准确率。代码和预先训练的模型检查点在GitHub上开源。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>线性RNN架构如Mamba在语言建模方面可与Transformer模型竞争，且具有优势部署特性。</li>
<li>通过复用注意力层的线性投影权重，可以将大型Transformer模型蒸馏为线性RNN模型。</li>
<li>混合模型使用四分之一的注意力层即可实现与原始Transformer相当的性能。</li>
<li>引入了一种硬件感知的投机解码算法，以提高模型推理速度。</li>
<li>最佳性能的模型是从大型预训练模型Llama3-8B-Instruct中提炼出来的。</li>
<li>提炼的模型在多个基准测试中表现出色，超过了其他类似规模的模型。</li>
<li>提炼的模型具有自然长度外推能力，表现出强大的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.15237">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cf99c5bab0e409ab07d7e12bef0dd9b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcee4de37c49a214575b87b42e5bef80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc41d6a4994aa580d50294bdd66e41dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-330d8a7004a7626191ebda5a54f9ebb9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Codebook-LLMs-Evaluating-LLMs-as-Measurement-Tools-for-Political-Science-Concepts"><a href="#Codebook-LLMs-Evaluating-LLMs-as-Measurement-Tools-for-Political-Science-Concepts" class="headerlink" title="Codebook LLMs: Evaluating LLMs as Measurement Tools for Political   Science Concepts"></a>Codebook LLMs: Evaluating LLMs as Measurement Tools for Political   Science Concepts</h2><p><strong>Authors:Andrew Halterman, Katherine A. Keith</strong></p>
<p>Codebooks – documents that operationalize concepts and outline annotation procedures – are used almost universally by social scientists when coding political texts. To code these texts automatically, researchers are increasing turning to generative large language models (LLMs). However, there is limited empirical evidence on whether “off-the-shelf” LLMs faithfully follow real-world codebook operationalizations and measure complex political constructs with sufficient accuracy. To address this, we gather and curate three real-world political science codebooks – covering protest events, political violence and manifestos – along with their unstructured texts and human labels. We also propose a five-stage framework for codebook-LLM measurement: preparing a codebook for both humans and LLMs, testing LLMs’ basic capabilities on a codebook, evaluating zero-shot measurement accuracy (i.e. off-the-shelf performance), analyzing errors, and further (parameter-efficient) supervised training of LLMs. We provide an empirical demonstration of this framework using our three codebook datasets and several pretrained 7-12 billion open-weight LLMs. We find current open-weight LLMs have limitations in following codebooks zero-shot, but that supervised instruction tuning can substantially improve performance. Rather than suggesting the “best” LLM, our contribution lies in our codebook datasets, evaluation framework, and guidance for applied researchers who wish to implement their own codebook-LLM measurement projects. </p>
<blockquote>
<p>概念操作化文档和概述注释程序的手册几乎被社会科学家在分析政治文本编码时普遍使用。为了自动对这些文本进行编码，研究人员正越来越多地转向生成大型语言模型（LLM）。然而，关于“现成的”LLM是否能够忠实遵循现实世界的手册操作化，并以足够的准确性衡量复杂的政治结构，现有的实证证据有限。为了解决这一问题，我们收集并整理了三个现实世界的政治科学手册数据集，涵盖抗议事件、政治暴力和宣言等内容及其非结构化文本和人类标签。我们还提出了一个五阶段的手册-LLM测量框架：为人工和LLM准备手册，测试LLM在手册上的基本能力，评估零启动测量精度（即现成性能），分析错误，以及对LLM进行进一步的（参数高效）监督训练。我们使用这三个手册数据集和几个预训练的7-12亿开放式权重LLM来实证展示这个框架。我们发现现有的开放式权重LLM在零启动遵循手册方面存在局限性，但监督指令微调可以显著提高性能。我们的贡献不在于建议“最佳”LLM，而在于我们的手册数据集、评估框架以及希望对自家进行手册-LLM测量项目的应用研究者提供的指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.10747v2">PDF</a> Version 2 (v1 Presented at PolMeth 2024)</p>
<p><strong>Summary</strong></p>
<p>本文探讨了社会科学家在进行政治文本编码时普遍使用的编码手册（codebooks）与大型语言模型（LLMs）的结合应用。针对现有研究中关于LLMs是否能忠实遵循现实编码手册操作并准确测量复杂政治概念的问题，作者收集并整理了三个真实世界的政治科学编码手册及其相关文本和人类标签。同时，作者提出了一个五阶段的编码手册-LLM测量框架，包括准备编码手册、测试LLM的基本能力、评估零样本测量精度、分析误差以及进一步对LLM进行参数有效的监督训练。通过实证演示，作者发现现有的大型语言模型在零样本遵循编码手册方面存在局限性，但通过监督指令微调可以显著提高性能。本文的贡献在于提供编码手册数据集、评估框架以及为希望实施自己的编码手册-LLM测量项目的应用研究者提供指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>社会科学家普遍使用编码手册（codebooks）对政治文本进行编码。</li>
<li>大型语言模型（LLMs）被越来越多地用于自动编码这些文本。</li>
<li>目前对于LLMs是否忠实遵循现实编码手册操作并准确测量复杂政治概念的研究证据有限。</li>
<li>作者提出了一个五阶段的编码手册-LLM测量框架，包括准备、测试、评估、分析和进一步训练LLM。</li>
<li>实证研究表明，现有LLMs在零样本遵循编码手册方面存在局限性。</li>
<li>通过监督指令微调可以显著提高LLMs的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.10747">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9f238a7514c29df2a3bf0dc93700f46f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51fd56598189b9beaf44475a61b3e21b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Uncertainty-Quantification-Methods-for-Large-Language-Models-with-LM-Polygraph"><a href="#Benchmarking-Uncertainty-Quantification-Methods-for-Large-Language-Models-with-LM-Polygraph" class="headerlink" title="Benchmarking Uncertainty Quantification Methods for Large Language   Models with LM-Polygraph"></a>Benchmarking Uncertainty Quantification Methods for Large Language   Models with LM-Polygraph</h2><p><strong>Authors:Roman Vashurin, Ekaterina Fadeeva, Artem Vazhentsev, Lyudmila Rvanova, Akim Tsvigun, Daniil Vasilev, Rui Xing, Abdelrahman Boda Sadallah, Kirill Grishchenkov, Sergey Petrakov, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov, Artem Shelmanov</strong></p>
<p>The rapid proliferation of large language models (LLMs) has stimulated researchers to seek effective and efficient approaches to deal with LLM hallucinations and low-quality outputs. Uncertainty quantification (UQ) is a key element of machine learning applications in dealing with such challenges. However, research to date on UQ for LLMs has been fragmented in terms of techniques and evaluation methodologies. In this work, we address this issue by introducing a novel benchmark that implements a collection of state-of-the-art UQ baselines and offers an environment for controllable and consistent evaluation of novel UQ techniques over various text generation tasks. Our benchmark also supports the assessment of confidence normalization methods in terms of their ability to provide interpretable scores. Using our benchmark, we conduct a large-scale empirical investigation of UQ and normalization techniques across eleven tasks, identifying the most effective approaches. Code: <a target="_blank" rel="noopener" href="https://github.com/IINemo/lm-polygraph">https://github.com/IINemo/lm-polygraph</a> Benchmark: <a target="_blank" rel="noopener" href="https://huggingface.co/LM-Polygraph">https://huggingface.co/LM-Polygraph</a> </p>
<blockquote>
<p>大型语言模型（LLM）的迅速增殖促使研究人员寻求有效且高效的方法来应对LLM幻觉和输出质量低的问题。不确定性量化（UQ）是机器学习应用中解决此类挑战的关键因素。然而，迄今为止关于LLM的不确定性量化的研究在技术和评估方法方面都是零碎的。在这项工作中，我们通过引入一个新型基准测试来解决这个问题，该测试实现了最先进的UQ基线技术的集合，并提供了对各种文本生成任务中新型UQ技术进行可控且一致评估的环境。我们的基准测试还支持对信心归一化方法的评估，以评估其提供可解释分数的能力。使用我们的基准测试，我们对UQ和归一化技术在十一个任务上进行了大规模实证研究，确定了最有效的方式。代码：<a target="_blank" rel="noopener" href="https://github.com/IINemo/lm-polygraph">https://github.com/IINemo/lm-polygraph</a> 基准测试：<a target="_blank" rel="noopener" href="https://huggingface.co/LM-Polygraph">https://huggingface.co/LM-Polygraph</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15627v3">PDF</a> Accepted to TACL 2025, pre-MIT Press publication version. Roman   Vashurin, Ekaterina Fadeeva, Artem Vazhentsev contributed equally</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的快速发展引发了研究人员对于处理LLM幻觉和低质量输出的有效和高效方法的探求。不确定性量化（UQ）是机器学习应用中应对这些挑战的关键因素。然而，迄今为止，针对LLM的不确定性量化研究在技术和评估方法上都是分散的。本研究通过引入新型基准测试来解决这一问题，该基准测试实现了先进的不确定性量化基准测试集合，并为各种文本生成任务上新型不确定性量化技术的可控和一致评估提供了环境。此外，该基准测试还支持对信心归一化方法的评估，以衡量其提供可解释分数能力的高低。借助我们的基准测试，我们对UQ和归一化技术在十一个任务上进行了大规模实证研究，并确定了最有效的途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的快速发展引发了对于处理其幻觉和低质量输出的关注。</li>
<li>不确定性量化（UQ）在应对LLM挑战中扮演重要角色。</li>
<li>目前针对LLM的不确定性量化研究在技术和评估方法上较为分散。</li>
<li>引入的新型基准测试集合实现了先进的不确定性量化基准测试。</li>
<li>该基准测试为各种文本生成任务上的新型不确定性量化技术提供了可控和一致的评估环境。</li>
<li>基准测试支持对信心归一化方法的评估，衡量其提供可解释分数的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.15627">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8e564a5f891e3d7e0893b95401140a1e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc18d24a40af11f55f58b1cf47d12e93.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="NV-Embed-Improved-Techniques-for-Training-LLMs-as-Generalist-Embedding-Models"><a href="#NV-Embed-Improved-Techniques-for-Training-LLMs-as-Generalist-Embedding-Models" class="headerlink" title="NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding   Models"></a>NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding   Models</h2><p><strong>Authors:Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping</strong></p>
<p>Decoder-only large language model (LLM)-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce the NV-Embed model, incorporating architectural designs, training procedures, and curated datasets to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility. For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last <EOS> token embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For training algorithm, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. For training data, we utilize the hard-negative mining, synthetic data generation and existing public available datasets to boost the performance of embedding model. By combining these techniques, our NV-Embed-v1 and NV-Embed-v2 models obtained the No.1 position on the Massive Text Embedding Benchmark (MTEB) (as of May 24, 2024 and August 30, 2024, respectively) across 56 embedding tasks, demonstrating the sustained effectiveness of the proposed methods over time. Additionally, it achieved the highest scores in the Long Doc section and the second-highest scores in the QA section of the AIR Benchmark, which covers a range of out-of-domain information retrieval topics beyond those in MTEB. </p>
<blockquote>
<p>基于解码器的大型语言模型（LLM）嵌入模型开始在通用文本嵌入任务中表现优于BERT或T5嵌入模型，包括基于密集向量的检索。在这项工作中，我们引入了NV-Embed模型，它结合了架构设计、训练程序和精选数据集，旨在显著增强LLM作为通用嵌入模型的表现，同时保持其简洁性和可重复性。在模型架构方面，我们提出了一种潜在注意力层来获得池化嵌入，与基于LLM的平均池化或使用最后一个<EOS>令牌嵌入相比，它始终提高了检索和下游任务的准确性。为了增强表示学习，我们在对比训练期间移除了LLM的因果注意力掩码。在训练算法方面，我们引入了一种两阶段的对比指令微调方法。它首先在检索数据集上应用带有指令的对比训练，利用批处理内的负样本和精选的硬负样本。在第二阶段，它将各种非检索任务融入指令微调中，这不仅提高了非检索任务准确性，还提高了检索性能。对于训练数据，我们利用硬负样本挖掘、合成数据生成和现有公共可用数据集来提升嵌入模型的性能。通过结合这些技术，我们的NV-Embed-v1和NV-Embed-v2模型在56个嵌入任务上获得了大规模文本嵌入基准测试（MTEB）的第一名（截至2024年5月24日和2024年8月30日），证明了所提出方法随时间推移的持续有效性。此外，它在AIR基准测试的Long Doc部分获得最高分，在QA部分获得第二名，涵盖了MTEB以外的广泛跨域信息检索主题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.17428v2">PDF</a> We open-source the model at:   <a target="_blank" rel="noopener" href="https://huggingface.co/nvidia/NV-Embed-v2">https://huggingface.co/nvidia/NV-Embed-v2</a></p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的解码器嵌入模型在通用文本嵌入任务中开始表现出优于BERT或T5的嵌入模型性能，包括基于密集向量的检索任务。本研究介绍了NV-Embed模型，通过设计架构、训练程序和定制数据集来显著提高LLM作为通用嵌入模型的性能，同时保持其简单性和可复现性。NV-Embed模型通过使用潜在意图层进行池化嵌入来改善检索和下游任务准确性，并去除LLM中的因果注意力掩码进行对比训练。此外，还介绍了两阶段对比指令微调方法，并利用硬负样本挖掘、合成数据生成等现有公开数据集提升嵌入模型的性能。NV-Embed模型在大量文本嵌入基准测试（MTEB）中获得第一名，在长期和短期评价中都展示了方法的有效性，并在AIR基准测试中取得了优异成绩。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM的解码器嵌入模型在通用文本嵌入任务中表现出优越性能。</li>
<li>NV-Embed模型通过特定架构、训练程序和定制数据集显著提高了LLM的嵌入性能。</li>
<li>潜在意图层用于池化嵌入改善了检索和下游任务准确性。</li>
<li>去除LLM的因果注意力掩码进行对比训练增强了表示学习。</li>
<li>两阶段对比指令微调方法提高了非检索任务准确性和检索性能。</li>
<li>利用硬负样本挖掘、合成数据生成等策略提升了嵌入模型的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.17428">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0e0d00d194d61af581720adde76700fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f6bed9c73cb84d797cc460c1b9e090a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5d3e81a4d996b2f5400d0d6a1d7cdbb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Attention-Mechanisms-Don’t-Learn-Additive-Models-Rethinking-Feature-Importance-for-Transformers"><a href="#Attention-Mechanisms-Don’t-Learn-Additive-Models-Rethinking-Feature-Importance-for-Transformers" class="headerlink" title="Attention Mechanisms Don’t Learn Additive Models: Rethinking Feature   Importance for Transformers"></a>Attention Mechanisms Don’t Learn Additive Models: Rethinking Feature   Importance for Transformers</h2><p><strong>Authors:Tobias Leemann, Alina Fastowski, Felix Pfeiffer, Gjergji Kasneci</strong></p>
<p>We address the critical challenge of applying feature attribution methods to the transformer architecture, which dominates current applications in natural language processing and beyond. Traditional attribution methods to explainable AI (XAI) explicitly or implicitly rely on linear or additive surrogate models to quantify the impact of input features on a model’s output. In this work, we formally prove an alarming incompatibility: transformers are structurally incapable of representing linear or additive surrogate models used for feature attribution, undermining the grounding of these conventional explanation methodologies. To address this discrepancy, we introduce the Softmax-Linked Additive Log Odds Model (SLALOM), a novel surrogate model specifically designed to align with the transformer framework. SLALOM demonstrates the capacity to deliver a range of insightful explanations with both synthetic and real-world datasets. We highlight SLALOM’s unique efficiency-quality curve by showing that SLALOM can produce explanations with substantially higher fidelity than competing surrogate models or provide explanations of comparable quality at a fraction of their computational costs. We release code for SLALOM as an open-source project online at <a target="_blank" rel="noopener" href="https://github.com/tleemann/slalom_explanations">https://github.com/tleemann/slalom_explanations</a>. </p>
<blockquote>
<p>我们面临将特征归因方法应用于主导当前自然语言处理及其他领域应用的变压器架构的关键挑战。传统的归因方法明确地或隐含地依赖于线性或附加替代模型来量化输入特征对模型输出的影响，以实现可解释人工智能（XAI）。在这项工作中，我们正式证明了一个令人警觉的矛盾：从结构上来说，变压器无法代表用于特征归因的线性或附加替代模型，这破坏了这些传统解释方法的基础。为了解决这个问题，我们引入了Softmax-Linked Additive Log Odds Model（SLALOM），这是一种专门设计来与变压器框架对齐的新型替代模型。SLALOM展示了在合成和真实世界数据集上提供一系列深刻解释的能力。我们通过展示SLALOM能够产生比竞争替代模型更高保真度的解释，或者提供同等质量的解释但计算成本较低，来突出SLALOM独特的效率-质量曲线。我们在<a target="_blank" rel="noopener" href="https://github.com/tleemann/slalom_explanations%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86SLALOM%E7%9A%84%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/tleemann/slalom_explanations上发布了SLALOM的开源项目代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.13536v2">PDF</a> TMLR Camera-Ready version</p>
<p><strong>Summary</strong></p>
<p>本文解决了将特征归因方法应用于自然语言处理及其他领域广泛应用的transformer架构的关键挑战。传统的人工智能解释（XAI）归因方法直接或间接依赖于线性或附加代理模型来衡量输入特征对模型输出的影响。然而，本文正式证明了一个令人担忧的不兼容性：transformer的结构无法代表用于特征归因的线性或附加代理模型，这使得这些传统解释方法的根基动摇。为解决这一问题，我们提出了名为SLALOM的新型代理模型，该模型专门设计以与transformer框架对齐。SLALOM能够在合成和真实数据集上提供一系列深入的解释。我们强调SLALOM的高效性与其质量之间的独特关系曲线，表明SLALOM可以在提供更高保真度的解释的同时，相较于其他代理模型大幅降低计算成本。我们已将SLALOM的代码作为开源项目在线发布在[链接地址]（请根据实际情况替换）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统特征归因方法依赖于线性或附加代理模型，但在transformer架构中存在不兼容性问题。</li>
<li>引入新型代理模型SLALOM，专为transformer架构设计，以解决传统解释方法的不兼容问题。</li>
<li>SLALOM在合成和真实数据集上展现出强大的解释能力。</li>
<li>SLALOM与现有代理模型相比具有较高的保真度和计算效率优势。 </li>
<li>通过公开代码实现，便于研究者和开发者使用SLALOM进行进一步研究和应用。 </li>
<li>SLALOM对于理解和解释transformer架构的工作原理具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.13536">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-73c677aae868613292b4aebaf3a79434.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-acd85c6b62daabeff15410f32047f968.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bcc361a70633a5f73b8583ed046eef6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AlgoFormer-An-Efficient-Transformer-Framework-with-Algorithmic-Structures"><a href="#AlgoFormer-An-Efficient-Transformer-Framework-with-Algorithmic-Structures" class="headerlink" title="AlgoFormer: An Efficient Transformer Framework with Algorithmic   Structures"></a>AlgoFormer: An Efficient Transformer Framework with Algorithmic   Structures</h2><p><strong>Authors:Yihang Gao, Chuanyang Zheng, Enze Xie, Han Shi, Tianyang Hu, Yu Li, Michael K. Ng, Zhenguo Li, Zhaoqiang Liu</strong></p>
<p>Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms. To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer, we design a novel transformer framework, dubbed Algorithm Transformer (abbreviated as AlgoFormer). We provide an insight that efficient transformer architectures can be designed by leveraging prior knowledge of tasks and the underlying structure of potential algorithms. Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can perform efficiently in algorithm representation in some specific tasks. In particular, inspired by the structure of human-designed learning algorithms, our transformer framework consists of a pre-transformer that is responsible for task preprocessing, a looped transformer for iterative optimization algorithms, and a post-transformer for producing the desired results after post-processing. We provide theoretical evidence of the expressive power of the AlgoFormer in solving some challenging problems, mirroring human-designed algorithms. Furthermore, some theoretical and empirical results are presented to show that the designed transformer has the potential to perform algorithm representation and learning. Experimental results demonstrate the empirical superiority of the proposed transformer in that it outperforms the standard transformer and vanilla looped transformer in some specific tasks. An extensive experiment on real language tasks (e.g., neural machine translation of German and English, and text classification) further validates the expressiveness and effectiveness of AlgoFormer. </p>
<blockquote>
<p>除了自然语言处理外，变压器（transformer）在解决更广泛的应用方面表现出卓越的性能，包括科学计算和计算机视觉。之前的研究工作试图从表现力和能力角度解释这一点，即标准变压器能够执行某些算法。为了赋予变压器算法能力，并受到最近提出的循环变压器的启发，我们设计了一种新型变压器框架，称为算法变压器（Algorithm Transformer，简称AlgoFormer）。我们提供了一种见解，即通过利用任务的先验知识和潜在算法的基本结构，可以有效地设计变压器架构。与标准变压器和基本的循环变压器相比，所提出的AlgoFormer在某些特定任务的算法表示方面能够高效执行。特别是，受到人类设计的学习算法的结构的启发，我们的变压器框架包括一个负责任务预处理的预变压器、一个用于迭代优化算法的循环变压器、以及一个用于后处理的后变压器，以产生所需的结果。我们提供了AlgoFormer在解决一些具有挑战性的问题时的表现力的理论证据，反映人类设计的算法。此外，我们还提供了一些理论和实证结果，以展示所设计的变压器在算法表示和学习方面的潜力。实验结果表明，所提出的变压器在某些特定任务上优于标准变压器和基本的循环变压器。在真实语言任务（例如德语和英语的神经机器翻译和文本分类）上的大量实验进一步验证了AlgoFormer的表达力和有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.13572v2">PDF</a> Published at Transactions on Machine Learning Research (TMLR). The   paper provides insight that the Transformer architectures can mimic the   algorithm structures in (in-context) algorithm learning and representation.   The incorporated algorithmic structure in Algoformer shows its potential in   (deep learning for) scientific computing, besides the real language tasks</p>
<p><strong>Summary</strong></p>
<p>本文介绍了除自然语言处理外，变压器在更广泛的应用中的卓越性能，包括科学计算和计算机视觉。为赋能变压器具有算法能力，并受循环变压器的启发，设计了一种新型变压器框架——Algorithm Transformer（简称AlgoFormer）。通过利用任务先验知识和潜在算法的底层结构，可以设计有效的变压器架构。与标准变压器和循环变压器相比，所提出的AlgoFormer在某些特定任务中的算法表示效率更高。该框架包括负责任务预处理的预变压器、负责迭代优化算法的循环变压器以及负责后处理的后变压器。理论证据表明AlgoFormer在解决某些挑战性问题时具有表现力，可模拟人类设计的算法。实验结果表明，该变压器在特定任务上的表现优于标准和循环变压器。在真实语言任务上的广泛实验进一步验证了AlgoFormer的表达力和有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>变压器在自然语言处理以外的领域，如科学计算和计算机视觉，也有卓越性能。</li>
<li>提出了一种新型变压器框架Algorithm Transformer（AlgoFormer），融合了预变压器、循环变压器和后变压器的设计。</li>
<li>AlgoFormer利用任务的先验知识和潜在算法的底层结构来提高算法表示的效率。</li>
<li>AlgoFormer在某些特定任务上的表现优于标准变压器和循环变压器。</li>
<li>AlgoFormer具有解决某些挑战性问题的理论依据，可模拟人类设计的算法。</li>
<li>实验结果证明了AlgoFormer的有效性和优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.13572">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d409c02cbc5d0682413f4b3ea8fb9f16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-316649059c7148ae44d0220a2695effb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-14/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-14/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-fbe9ac1ef0f46cbfd28ac30122713098.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-01-14  The ultraviolet luminosity function of star-forming galaxies between   redshifts of 0.4 and 0.6
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bf4c14965d29e3ebccb6c77db3899446.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-01-14  FaceMe Robust Blind Face Restoration with Personal Identification
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
