<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  DA-VPT Semantic-Guided Visual Prompt Tuning for Vision Transformers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b0fd69bb1aeb0c4049a13ed826889a8c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-31
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    5.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    24 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-31-æ›´æ–°"><a href="#2025-05-31-æ›´æ–°" class="headerlink" title="2025-05-31 æ›´æ–°"></a>2025-05-31 æ›´æ–°</h1><h2 id="DA-VPT-Semantic-Guided-Visual-Prompt-Tuning-for-Vision-Transformers"><a href="#DA-VPT-Semantic-Guided-Visual-Prompt-Tuning-for-Vision-Transformers" class="headerlink" title="DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers"></a>DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers</h2><p><strong>Authors:Li Ren, Chen Chen, Liqiang Wang, Kien Hua</strong></p>
<p>Visual Prompt Tuning (VPT) has become a promising solution for Parameter-Efficient Fine-Tuning (PEFT) approach for Vision Transformer (ViT) models by partially fine-tuning learnable tokens while keeping most model parameters frozen. Recent research has explored modifying the connection structures of the prompts. However, the fundamental correlation and distribution between the prompts and image tokens remain unexplored. In this paper, we leverage metric learning techniques to investigate how the distribution of prompts affects fine-tuning performance. Specifically, we propose a novel framework, Distribution Aware Visual Prompt Tuning (DA-VPT), to guide the distributions of the prompts by learning the distance metric from their class-related semantic data. Our method demonstrates that the prompts can serve as an effective bridge to share semantic information between image patches and the class token. We extensively evaluated our approach on popular benchmarks in both recognition and segmentation tasks. The results demonstrate that our approach enables more effective and efficient fine-tuning of ViT models by leveraging semantic information to guide the learning of the prompts, leading to improved performance on various downstream vision tasks. </p>
<blockquote>
<p>è§†è§‰æç¤ºå¾®è°ƒï¼ˆVPTï¼‰å·²æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡éƒ¨åˆ†å¾®è°ƒå¯å­¦ä¹ ä»¤ç‰Œçš„åŒæ—¶ä¿æŒå¤§éƒ¨åˆ†æ¨¡å‹å‚æ•°å†»ç»“ï¼Œä¸ºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¨¡å‹å®ç°äº†å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚æœ€è¿‘çš„ç ”ç©¶æ¢ç´¢äº†ä¿®æ”¹æç¤ºçš„è¿æ¥ç»“æ„ã€‚ç„¶è€Œï¼Œæç¤ºä¸å›¾åƒä»¤ç‰Œä¹‹é—´çš„åŸºæœ¬å…³è”å’Œåˆ†å¸ƒä»æœªè¢«æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨åº¦é‡å­¦ä¹ æŠ€æœ¯æ¥ç ”ç©¶æç¤ºåˆ†å¸ƒå¯¹å¾®è°ƒæ€§èƒ½çš„å½±å“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶â€”â€”åˆ†å¸ƒæ„ŸçŸ¥è§†è§‰æç¤ºå¾®è°ƒï¼ˆDA-VPTï¼‰ï¼Œé€šè¿‡å­¦ä¹ ä¸å…¶ç±»åˆ«ç›¸å…³è¯­ä¹‰æ•°æ®çš„è·ç¦»åº¦é‡æ¥æŒ‡å¯¼æç¤ºçš„åˆ†å¸ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•è¡¨æ˜ï¼Œæç¤ºå¯ä»¥ä½œä¸ºå›¾åƒè¡¥ä¸å’Œç±»åˆ«ä»¤ç‰Œä¹‹é—´å…±äº«è¯­ä¹‰ä¿¡æ¯çš„æœ‰æ•ˆæ¡¥æ¢ã€‚æˆ‘ä»¬åœ¨è¯†åˆ«å’Œåˆ†å‰²ä»»åŠ¡çš„æµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šå…¨é¢è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œé€šè¿‡åˆ©ç”¨è¯­ä¹‰ä¿¡æ¯æ¥æŒ‡å¯¼å­¦ä¹ æç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¯¹ViTæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨å„ç§ä¸‹æ¸¸è§†è§‰ä»»åŠ¡ä¸Šå®ç°æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23694v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰æç¤ºå¾®è°ƒï¼ˆVPTï¼‰å·²æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å¯¹å­¦ä¹ æ ‡è®°è¿›è¡Œéƒ¨åˆ†å¾®è°ƒæ¥å®ç°è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ã€‚æœ¬æ–‡åˆ©ç”¨åº¦é‡å­¦ä¹ æŠ€æœ¯ï¼Œç ”ç©¶æç¤ºåˆ†å¸ƒå¯¹å¾®è°ƒæ€§èƒ½çš„å½±å“ã€‚æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”åˆ†å¸ƒæ„ŸçŸ¥è§†è§‰æç¤ºå¾®è°ƒï¼ˆDA-VPTï¼‰ï¼Œé€šè¿‡å­¦ä¹ æ¥è‡ªç±»ç›¸å…³è¯­ä¹‰æ•°æ®çš„è·ç¦»åº¦é‡æ¥æŒ‡å¯¼æç¤ºåˆ†å¸ƒã€‚æç¤ºå¯ä»¥æœ‰æ•ˆåœ°ä½œä¸ºå›¾åƒè¡¥ä¸å’Œç±»æ ‡è®°ä¹‹é—´å…±äº«è¯­ä¹‰ä¿¡æ¯çš„æ¡¥æ¢ã€‚åœ¨è¯†åˆ«å’Œåˆ†å‰²ä»»åŠ¡çš„æµè¡ŒåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æç¤ºå¾®è°ƒï¼ˆVPTï¼‰æ˜¯ä¸€ç§é’ˆå¯¹Vision Transformerï¼ˆViTï¼‰æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œé€šè¿‡éƒ¨åˆ†å¾®è°ƒå­¦ä¹ æ ‡è®°æ¥å®ç°ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å·²æ¢ç´¢äº†æç¤ºè¿æ¥ç»“æ„çš„ä¿®æ”¹ï¼Œä½†æç¤ºä¸å›¾åƒæ ‡è®°ä¹‹é—´çš„åŸºæœ¬å…³è”å’Œåˆ†å¸ƒä»æœªè¢«æ¢ç´¢ã€‚</li>
<li>æœ¬æ–‡åˆ©ç”¨åº¦é‡å­¦ä¹ æŠ€æœ¯ï¼Œæå‡ºäº†åˆ†å¸ƒæ„ŸçŸ¥è§†è§‰æç¤ºå¾®è°ƒï¼ˆDA-VPTï¼‰æ¡†æ¶ï¼Œä»¥æŒ‡å¯¼ä¸ç±»ç›¸å…³è¯­ä¹‰æ•°æ®çš„è·ç¦»åº¦é‡ç›¸å…³çš„æç¤ºåˆ†å¸ƒã€‚</li>
<li>æç¤ºå¯ä»¥æœ‰æ•ˆåœ°ä½œä¸ºå›¾åƒè¡¥ä¸å’Œç±»æ ‡è®°ä¹‹é—´å…±äº«è¯­ä¹‰ä¿¡æ¯çš„æ¡¥æ¢ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åˆ©ç”¨è¯­ä¹‰ä¿¡æ¯æ¥æŒ‡å¯¼æç¤ºå­¦ä¹ ï¼Œå®ç°äº†å¯¹ViTæ¨¡å‹æ›´æœ‰æ•ˆçš„å¾®è°ƒã€‚</li>
<li>åœ¨å¤šä¸ªä¸‹æ¸¸è§†è§‰ä»»åŠ¡ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºVisual Transformeræ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23694">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e0c514216e145190f833c300f081468b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d78005a9ec6e9626078f4f7a84e1fc3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f76188fb0c289370a49cf10d45afd1d0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Deep-Modeling-and-Optimization-of-Medical-Image-Classification"><a href="#Deep-Modeling-and-Optimization-of-Medical-Image-Classification" class="headerlink" title="Deep Modeling and Optimization of Medical Image Classification"></a>Deep Modeling and Optimization of Medical Image Classification</h2><p><strong>Authors:Yihang Wu, Muhammad Owais, Reem Kateb, Ahmad Chaddad</strong></p>
<p>Deep models, such as convolutional neural networks (CNNs) and vision transformer (ViT), demonstrate remarkable performance in image classification. However, those deep models require large data to fine-tune, which is impractical in the medical domain due to the data privacy issue. Furthermore, despite the feasible performance of contrastive language image pre-training (CLIP) in the natural domain, the potential of CLIP has not been fully investigated in the medical field. To face these challenges, we considered three scenarios: 1) we introduce a novel CLIP variant using four CNNs and eight ViTs as image encoders for the classification of brain cancer and skin cancer, 2) we combine 12 deep models with two federated learning techniques to protect data privacy, and 3) we involve traditional machine learning (ML) methods to improve the generalization ability of those deep models in unseen domain data. The experimental results indicate that maxvit shows the highest averaged (AVG) test metrics (AVG &#x3D; 87.03%) in HAM10000 dataset with multimodal learning, while convnext_l demonstrates remarkable test with an F1-score of 83.98% compared to swin_b with 81.33% in FL model. Furthermore, the use of support vector machine (SVM) can improve the overall test metrics with AVG of $\sim 2%$ for swin transformer series in ISIC2018. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/AIPMLab/SkinCancerSimulation">https://github.com/AIPMLab/SkinCancerSimulation</a>. </p>
<blockquote>
<p>æ·±åº¦æ¨¡å‹ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼Œåœ¨å›¾åƒåˆ†ç±»æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ·±åº¦æ¨¡å‹éœ€è¦å¤§é‡æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œè¿™åœ¨åŒ»å­¦é¢†åŸŸç”±äºæ•°æ®éšç§é—®é¢˜å¹¶ä¸å®ç”¨ã€‚æ­¤å¤–ï¼Œå°½ç®¡å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨è‡ªç„¶é¢†åŸŸè¡¨ç°å‡ºå¯è¡Œçš„æ€§èƒ½ï¼Œä½†å…¶åœ¨åŒ»å­¦é¢†åŸŸçš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä»¥ä¸‹ä¸‰ç§æƒ…æ™¯ï¼š1ï¼‰æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„CLIPå˜ä½“ï¼Œä½¿ç”¨å››ç§CNNå’Œå…«ç§ViTä½œä¸ºå›¾åƒç¼–ç å™¨ï¼Œç”¨äºè„‘ç™Œå’Œçš®è‚¤ç™Œçš„åˆ†ç±»ï¼›2ï¼‰æˆ‘ä»¬å°†12ç§æ·±åº¦æ¨¡å‹ä¸ä¸¤ç§è”é‚¦å­¦ä¹ æŠ€æœ¯ç›¸ç»“åˆï¼Œä»¥ä¿æŠ¤æ•°æ®éšç§ï¼›3ï¼‰æˆ‘ä»¬å¼•å…¥ä¼ ç»Ÿæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ–¹æ³•æ¥æé«˜è¿™äº›æ·±åº¦æ¨¡å‹åœ¨æœªè§é¢†åŸŸæ•°æ®ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨HAM10000æ•°æ®é›†ä¸Šï¼Œmaxvitçš„å¹³å‡æµ‹è¯•æŒ‡æ ‡æœ€é«˜ï¼ˆAVG&#x3D;87.03%ï¼‰ï¼Œé‡‡ç”¨å¤šæ¨¡æ€å­¦ä¹ æ—¶è¡¨ç°å°¤ä¸ºçªå‡ºï¼›convnext_låœ¨è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ¨¡å‹ä¸­ç›¸å¯¹äºswin_bçš„F1åˆ†æ•°ä¸º83.98%ï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„æµ‹è¯•æ•ˆæœã€‚æ­¤å¤–ï¼Œæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰çš„ä½¿ç”¨å¯ä»¥æé«˜swin transformerç³»åˆ—çš„æ€»ä½“æµ‹è¯•æŒ‡æ ‡ï¼ŒISIC2018æ•°æ®é›†ä¸Šçš„å¹³å‡æå‡çº¦2%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AIPMLab/SkinCancerSimulation">https://github.com/AIPMLab/SkinCancerSimulation</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23040v1">PDF</a> Accepted in ISBI2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†ç±»çš„æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº†åŸºäºCLIPå˜ä½“çš„æ–°å‹æ¨¡å‹ï¼Œç»“åˆæ·±åº¦æ¨¡å‹ä¸è”é‚¦å­¦ä¹ æŠ€æœ¯ï¼Œå¹¶å¼•å…¥ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•æ¥æå‡æ¨¡å‹çš„æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è„‘ç™Œä¸çš®è‚¤ç™Œåˆ†ç±»ä¸Šå–å¾—æ˜¾è‘—æˆæ•ˆï¼Œä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åŸºäºCLIPå˜ä½“çš„æ–°å‹æ¨¡å‹ã€‚</li>
<li>ç»“åˆæ·±åº¦æ¨¡å‹ï¼ˆå¦‚CNNå’ŒViTï¼‰ä¸è”é‚¦å­¦ä¹ æŠ€æœ¯ï¼Œä»¥ä¿æŠ¤æ•°æ®éšç§ã€‚</li>
<li>å¼•å…¥ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨è„‘ç™Œå’Œçš®è‚¤ç™Œåˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶å–å¾—æ˜¾è‘—æˆæ•ˆã€‚</li>
<li>Maxvitåœ¨HAM10000æ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³ï¼Œå¹³å‡æµ‹è¯•æŒ‡æ ‡è¾¾åˆ°87.03%ã€‚</li>
<li>Convnext_låœ¨è”é‚¦å­¦ä¹ æ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²ï¼ŒF1åˆ†æ•°è¾¾åˆ°83.98%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ee3beb683cab7a16697744cf819036c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cff92564b66b185f00c2c7030292faf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3873872521239955ae7f8ba54bd03d41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d0f96b686882f8fe6fc199583be283c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Frequency-Adaptive-Discrete-Cosine-ViT-ResNet-Architecture-for-Sparse-Data-Vision"><a href="#Frequency-Adaptive-Discrete-Cosine-ViT-ResNet-Architecture-for-Sparse-Data-Vision" class="headerlink" title="Frequency-Adaptive Discrete Cosine-ViT-ResNet Architecture for   Sparse-Data Vision"></a>Frequency-Adaptive Discrete Cosine-ViT-ResNet Architecture for   Sparse-Data Vision</h2><p><strong>Authors:Ziyue Kang, Weichuan Zhang</strong></p>
<p>A major challenge in rare animal image classification is the scarcity of data, as many species usually have only a small number of labeled samples.   To address this challenge, we designed a hybrid deep-learning framework comprising a novel adaptive DCT preprocessing module, ViT-B16 and ResNet50 backbones, and a Bayesian linear classification head. To our knowledge, we are the first to introduce an adaptive frequency-domain selection mechanism that learns optimal low-, mid-, and high-frequency boundaries suited to the subsequent backbones.   Our network first captures image frequency-domain cues via this adaptive DCT partitioning. The adaptively filtered frequency features are then fed into ViT-B16 to model global contextual relationships, while ResNet50 concurrently extracts local, multi-scale spatial representations from the original image. A cross-level fusion strategy seamlessly integrates these frequency- and spatial-domain embeddings, and the fused features are passed through a Bayesian linear classifier to output the final category predictions. On our self-built 50-class wildlife dataset, this approach outperforms conventional CNN and fixed-band DCT pipelines, achieving state-of-the-art accuracy under extreme sample scarcity. </p>
<blockquote>
<p>ç¨€æœ‰åŠ¨ç‰©å›¾åƒåˆ†ç±»é¢ä¸´çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯æ•°æ®ç¨€ç¼ºï¼Œå› ä¸ºè®¸å¤šç‰©ç§é€šå¸¸åªæœ‰å°‘é‡æ ‡è®°æ ·æœ¬ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ··åˆæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬æ–°å‹è‡ªé€‚åº”DCTé¢„å¤„ç†æ¨¡å—ã€ViT-B16å’ŒResNet50ä¸»å¹²ç½‘ï¼Œä»¥åŠè´å¶æ–¯çº¿æ€§åˆ†ç±»å¤´ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡å¼•å…¥è‡ªé€‚åº”é¢‘åŸŸé€‰æ‹©æœºåˆ¶ï¼Œå­¦ä¹ é€‚åˆåç»­ä¸»å¹²çš„æœ€ä½³ä½ã€ä¸­ã€é«˜é¢‘è¾¹ç•Œã€‚æˆ‘ä»¬çš„ç½‘ç»œé¦–å…ˆé€šè¿‡è‡ªé€‚åº”DCTåˆ†åŒºæ•è·å›¾åƒé¢‘åŸŸçº¿ç´¢ã€‚ç„¶åï¼Œè‡ªé€‚åº”æ»¤æ³¢çš„é¢‘åŸŸç‰¹å¾è¾“å…¥ViT-B16ï¼Œä»¥å»ºæ¨¡å…¨å±€ä¸Šä¸‹æ–‡å…³ç³»ï¼Œè€ŒResNet50åˆ™åŒæ—¶ä»åŸå§‹å›¾åƒä¸­æå–å±€éƒ¨å¤šå°ºåº¦ç©ºé—´è¡¨ç¤ºã€‚è·¨çº§èåˆç­–ç•¥æ— ç¼é›†æˆäº†è¿™äº›é¢‘åŸŸå’Œç©ºé—´åŸŸåµŒå…¥ï¼Œèåˆçš„ç‰¹å¾é€šè¿‡è´å¶æ–¯çº¿æ€§åˆ†ç±»å™¨è¾“å‡ºæœ€ç»ˆçš„ç±»åˆ«é¢„æµ‹ã€‚åœ¨æˆ‘ä»¬è‡ªå»ºçš„50ç±»é‡ç”ŸåŠ¨ç‰©æ•°æ®é›†ä¸Šï¼Œè¿™ç§æ–¹æ³•ä¼˜äºä¼ ç»Ÿçš„CNNå’Œå›ºå®šé¢‘æ®µDCTç®¡é“ï¼Œåœ¨æç«¯æ ·æœ¬ç¨€ç¼ºçš„æƒ…å†µä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22701v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ··åˆæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè§£å†³ç¨€æœ‰åŠ¨ç‰©å›¾åƒåˆ†ç±»ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…å«è‡ªé€‚åº”DCTé¢„å¤„ç†æ¨¡å—ã€ViT-B16å’ŒResNet50éª¨å¹²ç½‘ï¼Œä»¥åŠè´å¶æ–¯çº¿æ€§åˆ†ç±»å¤´ã€‚é¦–æ¬¡å¼•å…¥è‡ªé€‚åº”é¢‘ç‡åŸŸé€‰æ‹©æœºåˆ¶ï¼Œå­¦ä¹ é€‚åˆåç»­éª¨å¹²ç½‘çš„æœ€ä½³ä½é¢‘ã€ä¸­é¢‘å’Œé«˜é¢‘è¾¹ç•Œã€‚è¯¥ç½‘ç»œé¦–å…ˆé€šè¿‡è‡ªé€‚åº”DCTåˆ†åŒºæ•è·å›¾åƒé¢‘ç‡åŸŸçº¿ç´¢ï¼Œç„¶åå°†è¿‡æ»¤åçš„é¢‘ç‡ç‰¹å¾è¾“å…¥ViT-B16ä»¥å»ºæ¨¡å…¨å±€ä¸Šä¸‹æ–‡å…³ç³»ï¼ŒåŒæ—¶ResNet50ä»åŸå§‹å›¾åƒä¸­æå–å±€éƒ¨å¤šå°ºåº¦ç©ºé—´è¡¨ç¤ºã€‚é€šè¿‡è·¨çº§åˆ«èåˆç­–ç•¥æ— ç¼é›†æˆè¿™äº›é¢‘ç‡åŸŸå’Œç©ºé—´åŸŸåµŒå…¥ï¼Œèåˆçš„ç‰¹å¾é€šè¿‡è´å¶æ–¯çº¿æ€§åˆ†ç±»å™¨è¾“å‡ºæœ€ç»ˆç±»åˆ«é¢„æµ‹ã€‚åœ¨è‡ªå»ºçš„50ç±»é‡ç”ŸåŠ¨ç‰©æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ä¼˜äºä¼ ç»Ÿçš„CNNå’Œå›ºå®šé¢‘æ®µDCTç®¡é“ï¼Œåœ¨æç«¯æ ·æœ¬ç¨€ç¼ºçš„æƒ…å†µä¸‹è¾¾åˆ°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºçš„æ··åˆæ·±åº¦å­¦ä¹ æ¡†æ¶æ—¨åœ¨è§£å†³ç¨€æœ‰åŠ¨ç‰©å›¾åƒåˆ†ç±»ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>æ¡†æ¶åŒ…å«è‡ªé€‚åº”DCTé¢„å¤„ç†æ¨¡å—ï¼Œèƒ½å¤Ÿå­¦ä¹ æœ€ä¼˜çš„é¢‘ç‡è¾¹ç•Œã€‚</li>
<li>å¼•å…¥ViT-B16å’ŒResNet50éª¨å¹²ç½‘æ¥åˆ†åˆ«å»ºæ¨¡å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ã€‚</li>
<li>ä½¿ç”¨è´å¶æ–¯çº¿æ€§åˆ†ç±»å¤´è¿›è¡Œç±»åˆ«é¢„æµ‹ã€‚</li>
<li>è‡ªé€‚åº”é¢‘ç‡åŸŸé€‰æ‹©æœºåˆ¶èƒ½å¤Ÿæé«˜åœ¨æ ·æœ¬ç¨€ç¼ºæƒ…å†µä¸‹çš„åˆ†ç±»å‡†ç¡®æ€§ã€‚</li>
<li>è·¨çº§åˆ«èåˆç­–ç•¥é›†æˆé¢‘ç‡åŸŸå’Œç©ºé—´åŸŸåµŒå…¥ï¼Œæå‡æ€§èƒ½ã€‚</li>
<li>åœ¨è‡ªå»ºçš„50ç±»é‡ç”ŸåŠ¨ç‰©æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22701">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-693a609cada822e24d05ad3096f95784.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5782f3dc282dede5f08f2bdcdd190f0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72531997d4de62d2b46095d42202650d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="From-Head-to-Tail-Towards-Balanced-Representation-in-Large-Vision-Language-Models-through-Adaptive-Data-Calibration"><a href="#From-Head-to-Tail-Towards-Balanced-Representation-in-Large-Vision-Language-Models-through-Adaptive-Data-Calibration" class="headerlink" title="From Head to Tail: Towards Balanced Representation in Large   Vision-Language Models through Adaptive Data Calibration"></a>From Head to Tail: Towards Balanced Representation in Large   Vision-Language Models through Adaptive Data Calibration</h2><p><strong>Authors:Mingyang Song, Xiaoye Qu, Jiawei Zhou, Yu Cheng</strong></p>
<p>Large Vision-Language Models (LVLMs) have achieved significant progress in combining visual comprehension with language generation. Despite this success, the training data of LVLMs still suffers from Long-Tail (LT) problems, where the data distribution is highly imbalanced. Previous works have mainly focused on traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as recognition and classification. Nevertheless, the exploration of LVLM (e.g. LLaVA) and more general tasks (e.g. Visual Question Answering and Visual Reasoning) remains under-explored. In this paper, we first conduct an in-depth analysis of the LT issues in LVLMs and identify two core causes: the overrepresentation of head concepts and the underrepresentation of tail concepts. Based on the above observation, we propose an $\textbf{A}$daptive $\textbf{D}$ata $\textbf{R}$efinement Framework ($\textbf{ADR}$), which consists of two stages: $\textbf{D}$ata $\textbf{R}$ebalancing ($\textbf{DR}$) and $\textbf{D}$ata $\textbf{S}$ynthesis ($\textbf{DS}$). In the DR stage, we adaptively rebalance the redundant data based on entity distributions, while in the DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and scarce images to supplement underrepresented portions. Through comprehensive evaluations across eleven benchmarks, our proposed ADR effectively mitigates the long-tail problem in the training data, improving the average performance of LLaVA 1.5 relatively by 4.36%, without increasing the training data volume. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨ç»“åˆè§†è§‰ç†è§£ä¸è¯­è¨€ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå°½ç®¡å–å¾—äº†æˆåŠŸï¼ŒLVLMçš„è®­ç»ƒæ•°æ®ä»ç„¶é¢ä¸´é•¿å°¾ï¼ˆLTï¼‰é—®é¢˜ï¼Œå³æ•°æ®åˆ†å¸ƒæåº¦ä¸å¹³è¡¡ã€‚ä¹‹å‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨ä¼ ç»Ÿçš„VLMæ¶æ„ï¼Œä¾‹å¦‚CLIPæˆ–ViTï¼Œä»¥åŠç‰¹å®šçš„ä»»åŠ¡ï¼Œå¦‚è¯†åˆ«å’Œåˆ†ç±»ã€‚ç„¶è€Œï¼Œå¯¹äºLVLMï¼ˆä¾‹å¦‚LLaVAï¼‰å’Œæ›´ä¸€èˆ¬çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚è§†è§‰é—®ç­”å’Œè§†è§‰æ¨ç†ï¼‰çš„æ¢ç´¢ä»ç„¶ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹LVLMä¸­çš„LTé—®é¢˜è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¹¶ç¡®å®šäº†ä¸¤ä¸ªæ ¸å¿ƒåŸå› ï¼šå¤´éƒ¨æ¦‚å¿µçš„è¿‡åº¦è¡¨ç¤ºå’Œå°¾éƒ¨æ¦‚å¿µçš„è¡¨ç¤ºä¸è¶³ã€‚åŸºäºä¸Šè¿°è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”æ•°æ®ç²¾ç‚¼æ¡†æ¶ï¼ˆADRï¼‰ï¼Œè¯¥æ¡†æ¶ç”±ä¸¤ä¸ªé˜¶æ®µç»„æˆï¼šæ•°æ®å†å¹³è¡¡ï¼ˆDRï¼‰å’Œæ•°æ®åˆæˆï¼ˆDSï¼‰ã€‚åœ¨DRé˜¶æ®µï¼Œæˆ‘ä»¬æ ¹æ®å®ä½“åˆ†å¸ƒè‡ªé€‚åº”åœ°é‡æ–°å¹³è¡¡å†—ä½™æ•°æ®ï¼Œè€Œåœ¨DSé˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰å’Œç¨€ç¼ºå›¾åƒæ¥è¡¥å……è¡¨ç¤ºä¸è¶³çš„éƒ¨åˆ†ã€‚é€šè¿‡è·¨è¶Šåä¸€ä¸ªåŸºå‡†ç‚¹çš„å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºçš„ADRæœ‰æ•ˆåœ°ç¼“è§£äº†è®­ç»ƒæ•°æ®ä¸­çš„é•¿å°¾é—®é¢˜ï¼Œç›¸å¯¹æé«˜äº†LLaVA 1.5çš„å¹³å‡æ€§èƒ½4.36%ï¼Œä¸”æ²¡æœ‰å¢åŠ è®­ç»ƒæ•°æ®é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12821v4">PDF</a> Accepted by CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://vlmlt.github.io/">https://vlmlt.github.io/</a></p>
<p><strong>Summary</strong><br>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ç»“åˆäº†è§†è§‰ç†è§£å’Œè¯­è¨€ç”ŸæˆæŠ€æœ¯ï¼Œå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå…¶è®­ç»ƒæ•°æ®å­˜åœ¨é•¿å°¾ï¼ˆLTï¼‰é—®é¢˜ï¼Œæ•°æ®åˆ†å¸ƒé«˜åº¦ä¸å¹³è¡¡ã€‚æœ¬æ–‡æ·±å…¥åˆ†æäº†LVLMä¸­çš„LTé—®é¢˜ï¼Œå¹¶ç¡®å®šäº†ä¸¤ä¸ªæ ¸å¿ƒåŸå› ï¼šå¤´éƒ¨æ¦‚å¿µçš„è¿‡åº¦è¡¨ç¤ºå’Œå°¾éƒ¨æ¦‚å¿µçš„è¡¨ç¤ºä¸è¶³ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†è‡ªé€‚åº”æ•°æ®ä¼˜åŒ–æ¡†æ¶ï¼ˆADRï¼‰ï¼ŒåŒ…æ‹¬æ•°æ®å†å¹³è¡¡ï¼ˆDRï¼‰å’Œæ•°æ®åˆæˆï¼ˆDSï¼‰ä¸¤ä¸ªé˜¶æ®µã€‚é€šè¿‡å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼ŒADRæœ‰æ•ˆåœ°ç¼“è§£äº†è®­ç»ƒæ•°æ®ä¸­çš„é•¿å°¾é—®é¢˜ï¼Œæé«˜äº†LLaVA 1.5çš„å¹³å‡æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰ç†è§£å’Œè¯­è¨€ç”Ÿæˆæ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ã€‚</li>
<li>LVLMsçš„è®­ç»ƒæ•°æ®å­˜åœ¨é•¿å°¾ï¼ˆLTï¼‰é—®é¢˜ï¼Œå³æ•°æ®åˆ†å¸ƒä¸å¹³è¡¡ã€‚</li>
<li>è®ºæ–‡æ·±å…¥åˆ†æäº†LTé—®é¢˜çš„ä¸¤ä¸ªæ ¸å¿ƒåŸå› ï¼šå¤´éƒ¨æ¦‚å¿µçš„è¿‡åº¦è¡¨ç¤ºå’Œå°¾éƒ¨æ¦‚å¿µçš„è¡¨ç¤ºä¸è¶³ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†è‡ªé€‚åº”æ•°æ®ä¼˜åŒ–æ¡†æ¶ï¼ˆADRï¼‰ï¼ŒåŒ…æ‹¬æ•°æ®å†å¹³è¡¡ï¼ˆDRï¼‰å’Œæ•°æ®åˆæˆï¼ˆDSï¼‰ä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>DRé˜¶æ®µé€šè¿‡è‡ªé€‚åº”åœ°å†å¹³è¡¡æ•°æ®æ¥è§£å†³å†—ä½™æ•°æ®é—®é¢˜ã€‚</li>
<li>DSé˜¶æ®µåˆ©ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰å’Œç¨€ç¼ºå›¾åƒæ¥è¡¥å……è¡¨ç¤ºä¸è¶³çš„éƒ¨åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4852890b00f95bf949498f981658774b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-521c9b4ba306272ccf4ecfd77d0f551b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b73bf18f8ffd9cdbf8a8688234a1e2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2e4ba0692f50c329b5560452c3ae0f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc3d4dc0ac0399291646b6977bc5dc3b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Audio-Visual-Segmentation-Through-Text-Embeddings"><a href="#Audio-Visual-Segmentation-Through-Text-Embeddings" class="headerlink" title="Audio Visual Segmentation Through Text Embeddings"></a>Audio Visual Segmentation Through Text Embeddings</h2><p><strong>Authors:Kyungbok Lee, You Zhang, Zhiyao Duan</strong></p>
<p>The goal of Audio-Visual Segmentation (AVS) is to localize and segment the sounding source objects from video frames. Research on AVS suffers from data scarcity due to the high cost of fine-grained manual annotations. Recent works attempt to overcome the challenge of limited data by leveraging the vision foundation model, Segment Anything Model (SAM), prompting it with audio to enhance its ability to segment sounding source objects. While this approach alleviates the modelâ€™s burden on understanding visual modality by utilizing knowledge of pre-trained SAM, it does not address the fundamental challenge of learning audio-visual correspondence with limited data. To address this limitation, we propose \textbf{AV2T-SAM}, a novel framework that bridges audio features with the text embedding space of pre-trained text-prompted SAM. Our method leverages multimodal correspondence learned from rich text-image paired datasets to enhance audio-visual alignment. Furthermore, we introduce a novel feature, $\mathbf{\textit{\textbf{f}}<em>{CLIP} \odot \textit{\textbf{f}}</em>{CLAP}}$, which emphasizes shared semantics of audio and visual modalities while filtering irrelevant noise. Our approach outperforms existing methods on the AVSBench dataset by effectively utilizing pre-trained segmentation models and cross-modal semantic alignment. The source code is released at <a target="_blank" rel="noopener" href="https://github.com/bok-bok/AV2T-SAM">https://github.com/bok-bok/AV2T-SAM</a>. </p>
<blockquote>
<p>éŸ³é¢‘è§†è§‰åˆ†å‰²ï¼ˆAVSï¼‰çš„ç›®æ ‡æ˜¯å®šä½è§†é¢‘å¸§ä¸­çš„å‘å£°æºå¯¹è±¡å¹¶å°†å…¶åˆ†å‰²å‡ºæ¥ã€‚ç”±äºç²¾ç»†çš„æ‰‹åŠ¨æ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼ŒAVSç ”ç©¶é¢ä¸´æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œè¯•å›¾é€šè¿‡åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹â€”â€”åˆ†æ®µä»»ä½•äº‹ç‰©æ¨¡å‹ï¼ˆSAMï¼‰æ¥å…‹æœæ•°æ®é‡æœ‰é™çš„æŒ‘æˆ˜ï¼Œå¹¶ç”¨éŸ³é¢‘æ¥æç¤ºå®ƒä»¥å¢å¼ºå…¶åˆ†å‰²å‘å£°æºå¯¹è±¡çš„èƒ½åŠ›ã€‚è™½ç„¶è¿™ç§æ–¹æ³•é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„SAMçš„çŸ¥è¯†å‡è½»äº†æ¨¡å‹å¯¹è§†è§‰æ¨¡æ€ç†è§£çš„å‹åŠ›ï¼Œä½†å®ƒå¹¶æ²¡æœ‰è§£å†³æ•°æ®æœ‰é™æ—¶å­¦ä¹ è§†å¬å¯¹åº”å…³ç³»çš„æ ¹æœ¬æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶AV2T-SAMï¼Œå®ƒå°†éŸ³é¢‘ç‰¹å¾ä¸é¢„è®­ç»ƒçš„æ–‡æœ¬æç¤ºSAMçš„æ–‡æœ¬åµŒå…¥ç©ºé—´ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä»ä¸°å¯Œçš„æ–‡æœ¬å›¾åƒé…å¯¹æ•°æ®é›†ä¸­å­¦ä¹ çš„å¤šæ¨¡æ€å¯¹åº”å…³ç³»æ¥å¢å¼ºè§†å¬å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„ç‰¹å¾$\mathbf{\textit{\textbf{f}}<em>{CLIP} \odot \textit{\textbf{f}}</em>{CLAP}}$ï¼Œå®ƒå¼ºè°ƒäº†éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€çš„å…±äº«è¯­ä¹‰ï¼ŒåŒæ—¶è¿‡æ»¤æ‰æ— å…³å™ªå£°ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åœ¨AVSBenchæ•°æ®é›†ä¸Šæœ‰æ•ˆåœ°åˆ©ç”¨é¢„è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹å’Œè·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ï¼Œå®ç°äº†å¯¹ç°æœ‰æ–¹æ³•çš„è¶…è¶Šã€‚æºä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/bok-bok/AV2T-SAM%E3%80%82">https://github.com/bok-bok/AV2T-SAMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16359v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†éŸ³é¢‘è§†è§‰åˆ†å‰²ï¼ˆAVSï¼‰çš„ç›®æ ‡æ˜¯ä»è§†é¢‘å¸§ä¸­å®šä½å¹¶åˆ†å‰²å£°éŸ³æºå¯¹è±¡ã€‚ç ”ç©¶AVSå› ç²¾ç»†ç²’åº¦æ‰‹åŠ¨æ³¨é‡Šçš„é«˜æˆæœ¬è€Œé¢ä¸´æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚æœ€è¿‘çš„å·¥ä½œå°è¯•é€šè¿‡åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹SAMå’ŒéŸ³é¢‘æç¤ºæ¥å¢å¼ºå…¶åˆ†å‰²å£°éŸ³æºå¯¹è±¡çš„èƒ½åŠ›æ¥å…‹æœæ•°æ®æœ‰é™çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¹¶æ²¡æœ‰è§£å†³åœ¨æœ‰é™æ•°æ®ä¸‹å­¦ä¹ éŸ³é¢‘è§†è§‰å¯¹åº”å…³ç³»çš„æ ¹æœ¬æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†AV2T-SAMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†éŸ³é¢‘ç‰¹å¾ä¸é¢„è®­ç»ƒæ–‡æœ¬æç¤ºSAMçš„æ–‡æœ¬åµŒå…¥ç©ºé—´ç›¸ç»“åˆï¼Œåˆ©ç”¨ä¸°å¯Œçš„æ–‡æœ¬å›¾åƒé…å¯¹æ•°æ®é›†å­¦ä¹ å¤šæ¨¡æ€å¯¹åº”å…³ç³»ä»¥å¢å¼ºéŸ³é¢‘è§†è§‰å¯¹é½ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§æ–°ç‰¹å¾fCLIPâŠ—fCLAPï¼Œå¼ºè°ƒéŸ³é¢‘å’Œè§†è§‰æ¨¡æ€çš„å…±äº«è¯­ä¹‰ï¼ŒåŒæ—¶è¿‡æ»¤æ‰æ— å…³å™ªå£°ã€‚è¯¥æ–¹æ³•åœ¨AVSBenchæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œé€šè¿‡æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒåˆ†å‰²æ¨¡å‹å’Œè·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘è§†è§‰åˆ†å‰²ï¼ˆAVSï¼‰çš„ç›®æ ‡æ˜¯ä»è§†é¢‘å¸§ä¸­å®šä½å¹¶åˆ†å‰²å£°éŸ³æºå¯¹è±¡ã€‚</li>
<li>æ•°æ®ç¨€ç¼ºæ˜¯AVSç ”ç©¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå› ä¸ºç²¾ç»†ç²’åº¦çš„æ‰‹åŠ¨æ³¨é‡Šæˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æœ€è¿‘çš„ç ”ç©¶å°è¯•é€šè¿‡åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹SAMå’ŒéŸ³é¢‘æç¤ºæ¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>æå‡ºçš„AV2T-SAMæ¡†æ¶ç»“åˆéŸ³é¢‘ç‰¹å¾ä¸é¢„è®­ç»ƒæ–‡æœ¬æç¤ºSAMçš„æ–‡æœ¬åµŒå…¥ç©ºé—´ï¼Œå¢å¼ºéŸ³é¢‘è§†è§‰å¯¹é½ã€‚</li>
<li>AV2T-SAMåˆ©ç”¨ä¸°å¯Œçš„æ–‡æœ¬å›¾åƒé…å¯¹æ•°æ®é›†å­¦ä¹ å¤šæ¨¡æ€å¯¹åº”å…³ç³»ã€‚</li>
<li>å¼•å…¥æ–°ç‰¹å¾fCLIPâŠ—fCLAPï¼Œå¼ºè°ƒéŸ³é¢‘å’Œè§†è§‰æ¨¡æ€çš„å…±äº«è¯­ä¹‰ï¼Œè¿‡æ»¤æ— å…³å™ªå£°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16359">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5c59b4fc6f77f4da6683b413d4f62727.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc06a03c14eb53ea28e0db8866b0cb8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5505295f90d44e3af8ac8d240ed3a1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-144ab32d46ccbbf47366393223650730.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="QMamba-On-First-Exploration-of-Vision-Mamba-for-Image-Quality-Assessment"><a href="#QMamba-On-First-Exploration-of-Vision-Mamba-for-Image-Quality-Assessment" class="headerlink" title="QMamba: On First Exploration of Vision Mamba for Image Quality   Assessment"></a>QMamba: On First Exploration of Vision Mamba for Image Quality   Assessment</h2><p><strong>Authors:Fengbin Guan, Xin Li, Zihao Yu, Yiting Lu, Zhibo Chen</strong></p>
<p>In this work, we take the first exploration of the recently popular foundation model, i.e., State Space Model&#x2F;Mamba, in image quality assessment (IQA), aiming at observing and excavating the perception potential in vision Mamba. A series of works on Mamba has shown its significant potential in various fields, e.g., segmentation and classification. However, the perception capability of Mamba remains under-explored. Consequently, we propose QMamba by revisiting and adapting the Mamba model for three crucial IQA tasks, i.e., task-specific, universal, and transferable IQA, which reveals its clear advantages over existing foundational models, e.g., Swin Transformer, ViT, and CNNs, in terms of perception and computational cost. To improve the transferability of QMamba, we propose the StylePrompt tuning paradigm, where lightweight mean and variance prompts are injected to assist task-adaptive transfer learning of pre-trained QMamba for different downstream IQA tasks. Compared with existing prompt tuning strategies, our StylePrompt enables better perceptual transfer with lower computational cost. Extensive experiments on multiple synthetic, authentic IQA datasets, and cross IQA datasets demonstrate the effectiveness of our proposed QMamba. The code will be available at: <a target="_blank" rel="noopener" href="https://github.com/bingo-G/QMamba.git">https://github.com/bingo-G/QMamba.git</a> </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æ¢ç´¢äº†æœ€è¿‘æµè¡Œçš„åŸºç¡€æ¨¡å‹ï¼Œå³çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆState Space Modelï¼‰&#x2F;Mambaåœ¨å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§‚å¯ŸæŒ–æ˜è§†è§‰Mambaä¸­çš„æ„ŸçŸ¥æ½œåŠ›ã€‚ä¸€ç³»åˆ—å…³äºMambaçš„å·¥ä½œå·²ç»æ˜¾ç¤ºå‡ºå®ƒåœ¨å„ä¸ªé¢†åŸŸçš„é‡è¦æ½œåŠ›ï¼Œä¾‹å¦‚åˆ†å‰²å’Œåˆ†ç±»ã€‚ç„¶è€Œï¼ŒMambaçš„æ„ŸçŸ¥èƒ½åŠ›ä»ç„¶æ²¡æœ‰å¾—åˆ°å……åˆ†çš„æ¢ç´¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹Mambaæ¨¡å‹è¿›è¡Œå›é¡¾å’Œé€‚åº”ï¼Œæå‡ºäº†QMambaï¼Œç”¨äºä¸‰é¡¹å…³é”®çš„IQAä»»åŠ¡ï¼Œå³ç‰¹å®šä»»åŠ¡IQAã€é€šç”¨IQAå’Œå¯è¿ç§»IQAã€‚åœ¨æ„ŸçŸ¥å’Œè®¡ç®—æˆæœ¬æ–¹é¢ï¼ŒQMambaæ˜¾ç¤ºå‡ºä¼˜äºç°æœ‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚Swin Transformerã€ViTå’ŒCNNï¼‰çš„æ˜æ˜¾ä¼˜åŠ¿ã€‚ä¸ºäº†æé«˜QMambaçš„å¯è¿ç§»æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†StylePromptå¾®è°ƒèŒƒå¼ï¼Œé€šè¿‡æ³¨å…¥è½»é‡çº§å‡å€¼å’Œæ–¹å·®æç¤ºæ¥è¾…åŠ©é’ˆå¯¹ä¸åŒä¸‹æ¸¸IQAä»»åŠ¡çš„é¢„è®­ç»ƒQMambaçš„ä»»åŠ¡é€‚åº”æ€§è¿ç§»å­¦ä¹ ã€‚ä¸ç°æœ‰çš„æç¤ºè°ƒæ•´ç­–ç•¥ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„StylePromptèƒ½å¤Ÿä»¥è¾ƒä½çš„è®¡ç®—æˆæœ¬å®ç°æ›´å¥½çš„æ„ŸçŸ¥è¿ç§»ã€‚åœ¨å¤šä¸ªåˆæˆã€çœŸå®çš„IQAæ•°æ®é›†å’Œè·¨IQAæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æ‰€æå‡ºçš„QMambaçš„æœ‰æ•ˆæ€§ã€‚ä»£ç å°†åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€ï¼š<a target="_blank" rel="noopener" href="https://github.com/bingo-G/QMamba.git">https://github.com/bingo-G/QMamba.git</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.09546v2">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡é¦–æ¬¡æ¢ç´¢äº†è¿‘æœŸæµè¡Œçš„åŸºç¡€æ¨¡å‹â€”â€”çŠ¶æ€ç©ºé—´æ¨¡å‹&#x2F;æ›¼å·´ï¼ˆState Space Model&#x2F;Mambaï¼‰åœ¨å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰é¢†åŸŸçš„åº”ç”¨ï¼Œæ—¨åœ¨æŒ–æ˜æ›¼å·´åœ¨è§†è§‰é¢†åŸŸçš„æ„ŸçŸ¥æ½œåŠ›ã€‚é’ˆå¯¹ä¸‰ç§å…³é”®çš„IQAä»»åŠ¡ï¼Œä½œè€…æå‡ºäº†æ”¹è¿›åçš„QMambaæ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶åœ¨æ„ŸçŸ¥å’Œè®¡ç®—æˆæœ¬æ–¹é¢çš„ä¼˜åŠ¿ã€‚ä¸ºæé«˜QMambaçš„è¿ç§»èƒ½åŠ›ï¼Œä½œè€…è¿˜æå‡ºäº†StylePromptå¾®è°ƒèŒƒå¼ï¼Œé€šè¿‡æ³¨å…¥è½»é‡çº§å‡å€¼å’Œæ–¹å·®æç¤ºæ¥è¾…åŠ©é¢„è®­ç»ƒçš„QMambaå¯¹ä¸åŒä¸‹æ¸¸IQAä»»åŠ¡çš„é€‚åº”æ€§è½¬ç§»å­¦ä¹ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« é¦–æ¬¡å°†State Space Model&#x2F;Mambaæ¨¡å‹åº”ç”¨äºå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰ï¼Œæ˜¾ç¤ºå…¶åœ¨è¯¥é¢†åŸŸçš„æ½œåŠ›ã€‚</li>
<li>QMambaæ¨¡å‹åœ¨ä»»åŠ¡ç‰¹å®šã€é€šç”¨å’Œå¯è¿ç§»çš„IQAä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºä¼˜äºå…¶ä»–åŸºç¡€æ¨¡å‹çš„æ„ŸçŸ¥å’Œè®¡ç®—æˆæœ¬ä¼˜åŠ¿ã€‚</li>
<li>ä¸ºæé«˜QMambaçš„è¿ç§»èƒ½åŠ›ï¼Œæå‡ºäº†StylePromptå¾®è°ƒèŒƒå¼ã€‚</li>
<li>StylePrompté€šè¿‡æ³¨å…¥è½»é‡çº§å‡å€¼å’Œæ–¹å·®æç¤ºï¼Œè¾…åŠ©é¢„è®­ç»ƒQMambaå¯¹ä¸åŒä¸‹æ¸¸IQAä»»åŠ¡çš„é€‚åº”æ€§è½¬ç§»å­¦ä¹ ã€‚</li>
<li>StylePromptç›¸å¯¹äºç°æœ‰çš„æç¤ºè°ƒæ•´ç­–ç•¥ï¼Œåœ¨è¾ƒä½çš„è®¡ç®—æˆæœ¬ä¸‹å®ç°äº†æ›´å¥½çš„æ„ŸçŸ¥è¿ç§»ã€‚</li>
<li>åœ¨å¤šä¸ªåˆæˆã€çœŸå®IQAæ•°æ®é›†ä»¥åŠè·¨IQAæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†QMambaæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.09546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1a0a04c51ee256aae6451b7dfa3ffe9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0fd69bb1aeb0c4049a13ed826889a8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-099316b69c8213697558e95e8f405176.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e42a25493d24198b1297b845c04bbb7c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6c30e410c34aa56433ba34851a2ccdad.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  Adaptive Spatial Augmentation for Semi-supervised Semantic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fa67526f971f8e26a4f1184758a55d40.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  VAU-R1 Advancing Video Anomaly Understanding via Reinforcement   Fine-Tuning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
