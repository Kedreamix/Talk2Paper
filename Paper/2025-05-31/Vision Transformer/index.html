<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-05-31  DA-VPT Semantic-Guided Visual Prompt Tuning for Vision Transformers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b0fd69bb1aeb0c4049a13ed826889a8c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-31
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    24 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-31-更新"><a href="#2025-05-31-更新" class="headerlink" title="2025-05-31 更新"></a>2025-05-31 更新</h1><h2 id="DA-VPT-Semantic-Guided-Visual-Prompt-Tuning-for-Vision-Transformers"><a href="#DA-VPT-Semantic-Guided-Visual-Prompt-Tuning-for-Vision-Transformers" class="headerlink" title="DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers"></a>DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers</h2><p><strong>Authors:Li Ren, Chen Chen, Liqiang Wang, Kien Hua</strong></p>
<p>Visual Prompt Tuning (VPT) has become a promising solution for Parameter-Efficient Fine-Tuning (PEFT) approach for Vision Transformer (ViT) models by partially fine-tuning learnable tokens while keeping most model parameters frozen. Recent research has explored modifying the connection structures of the prompts. However, the fundamental correlation and distribution between the prompts and image tokens remain unexplored. In this paper, we leverage metric learning techniques to investigate how the distribution of prompts affects fine-tuning performance. Specifically, we propose a novel framework, Distribution Aware Visual Prompt Tuning (DA-VPT), to guide the distributions of the prompts by learning the distance metric from their class-related semantic data. Our method demonstrates that the prompts can serve as an effective bridge to share semantic information between image patches and the class token. We extensively evaluated our approach on popular benchmarks in both recognition and segmentation tasks. The results demonstrate that our approach enables more effective and efficient fine-tuning of ViT models by leveraging semantic information to guide the learning of the prompts, leading to improved performance on various downstream vision tasks. </p>
<blockquote>
<p>视觉提示微调（VPT）已成为一种有前景的解决方案，通过部分微调可学习令牌的同时保持大部分模型参数冻结，为视觉转换器（ViT）模型实现了参数高效的微调（PEFT）方法。最近的研究探索了修改提示的连接结构。然而，提示与图像令牌之间的基本关联和分布仍未被探索。在本文中，我们利用度量学习技术来研究提示分布对微调性能的影响。具体来说，我们提出了一种新型框架——分布感知视觉提示微调（DA-VPT），通过学习与其类别相关语义数据的距离度量来指导提示的分布。我们的方法表明，提示可以作为图像补丁和类别令牌之间共享语义信息的有效桥梁。我们在识别和分割任务的流行基准测试上全面评估了我们的方法。结果表明，通过利用语义信息来指导学习提示，我们的方法能够更有效地对ViT模型进行微调，并在各种下游视觉任务上实现性能提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23694v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>视觉提示微调（VPT）已成为一种有前景的解决方案，通过对学习标记进行部分微调来实现视觉变压器（ViT）模型的参数高效微调（PEFT）。本文利用度量学习技术，研究提示分布对微调性能的影响。提出了一种新的框架——分布感知视觉提示微调（DA-VPT），通过学习来自类相关语义数据的距离度量来指导提示分布。提示可以有效地作为图像补丁和类标记之间共享语义信息的桥梁。在识别和分割任务的流行基准测试中，该方法展示了其有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉提示微调（VPT）是一种针对Vision Transformer（ViT）模型的参数高效微调（PEFT）方法，通过部分微调学习标记来实现。</li>
<li>现有研究已探索了提示连接结构的修改，但提示与图像标记之间的基本关联和分布仍未被探索。</li>
<li>本文利用度量学习技术，提出了分布感知视觉提示微调（DA-VPT）框架，以指导与类相关语义数据的距离度量相关的提示分布。</li>
<li>提示可以有效地作为图像补丁和类标记之间共享语义信息的桥梁。</li>
<li>该方法通过在微调过程中利用语义信息来指导提示学习，实现了对ViT模型更有效的微调。</li>
<li>在多个下游视觉任务上的测试表明，该方法可以提高性能。</li>
<li>该研究为Visual Transformer模型的参数高效微调提供了新的视角和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23694">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e0c514216e145190f833c300f081468b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d78005a9ec6e9626078f4f7a84e1fc3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f76188fb0c289370a49cf10d45afd1d0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Deep-Modeling-and-Optimization-of-Medical-Image-Classification"><a href="#Deep-Modeling-and-Optimization-of-Medical-Image-Classification" class="headerlink" title="Deep Modeling and Optimization of Medical Image Classification"></a>Deep Modeling and Optimization of Medical Image Classification</h2><p><strong>Authors:Yihang Wu, Muhammad Owais, Reem Kateb, Ahmad Chaddad</strong></p>
<p>Deep models, such as convolutional neural networks (CNNs) and vision transformer (ViT), demonstrate remarkable performance in image classification. However, those deep models require large data to fine-tune, which is impractical in the medical domain due to the data privacy issue. Furthermore, despite the feasible performance of contrastive language image pre-training (CLIP) in the natural domain, the potential of CLIP has not been fully investigated in the medical field. To face these challenges, we considered three scenarios: 1) we introduce a novel CLIP variant using four CNNs and eight ViTs as image encoders for the classification of brain cancer and skin cancer, 2) we combine 12 deep models with two federated learning techniques to protect data privacy, and 3) we involve traditional machine learning (ML) methods to improve the generalization ability of those deep models in unseen domain data. The experimental results indicate that maxvit shows the highest averaged (AVG) test metrics (AVG &#x3D; 87.03%) in HAM10000 dataset with multimodal learning, while convnext_l demonstrates remarkable test with an F1-score of 83.98% compared to swin_b with 81.33% in FL model. Furthermore, the use of support vector machine (SVM) can improve the overall test metrics with AVG of $\sim 2%$ for swin transformer series in ISIC2018. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/AIPMLab/SkinCancerSimulation">https://github.com/AIPMLab/SkinCancerSimulation</a>. </p>
<blockquote>
<p>深度模型，如卷积神经网络（CNN）和视觉转换器（ViT），在图像分类方面表现出卓越的性能。然而，这些深度模型需要大量数据进行微调，这在医学领域由于数据隐私问题并不实用。此外，尽管对比语言图像预训练（CLIP）在自然领域表现出可行的性能，但其在医学领域的潜力尚未得到充分研究。为了应对这些挑战，我们考虑了以下三种情景：1）我们引入了一种新型的CLIP变体，使用四种CNN和八种ViT作为图像编码器，用于脑癌和皮肤癌的分类；2）我们将12种深度模型与两种联邦学习技术相结合，以保护数据隐私；3）我们引入传统机器学习（ML）方法来提高这些深度模型在未见领域数据中的泛化能力。实验结果表明，在HAM10000数据集上，maxvit的平均测试指标最高（AVG&#x3D;87.03%），采用多模态学习时表现尤为突出；convnext_l在联邦学习（FL）模型中相对于swin_b的F1分数为83.98%，表现出优异的测试效果。此外，支持向量机（SVM）的使用可以提高swin transformer系列的总体测试指标，ISIC2018数据集上的平均提升约2%。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/AIPMLab/SkinCancerSimulation">https://github.com/AIPMLab/SkinCancerSimulation</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23040v1">PDF</a> Accepted in ISBI2025</p>
<p><strong>Summary</strong><br>     针对医学图像分类的挑战，研究提出了基于CLIP变体的新型模型，结合深度模型与联邦学习技术，并引入传统机器学习方法来提升模型的性能与泛化能力。在脑癌与皮肤癌分类上取得显著成效，代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究针对医学图像分类中的挑战，提出了基于CLIP变体的新型模型。</li>
<li>结合深度模型（如CNN和ViT）与联邦学习技术，以保护数据隐私。</li>
<li>引入传统机器学习方法，旨在提高模型的泛化能力。</li>
<li>在脑癌和皮肤癌分类任务上进行了实验，并取得显著成效。</li>
<li>Maxvit在HAM10000数据集上表现最佳，平均测试指标达到87.03%。</li>
<li>Convnext_l在联邦学习模型中表现出色，F1分数达到83.98%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23040">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2ee3beb683cab7a16697744cf819036c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cff92564b66b185f00c2c7030292faf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3873872521239955ae7f8ba54bd03d41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d0f96b686882f8fe6fc199583be283c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Frequency-Adaptive-Discrete-Cosine-ViT-ResNet-Architecture-for-Sparse-Data-Vision"><a href="#Frequency-Adaptive-Discrete-Cosine-ViT-ResNet-Architecture-for-Sparse-Data-Vision" class="headerlink" title="Frequency-Adaptive Discrete Cosine-ViT-ResNet Architecture for   Sparse-Data Vision"></a>Frequency-Adaptive Discrete Cosine-ViT-ResNet Architecture for   Sparse-Data Vision</h2><p><strong>Authors:Ziyue Kang, Weichuan Zhang</strong></p>
<p>A major challenge in rare animal image classification is the scarcity of data, as many species usually have only a small number of labeled samples.   To address this challenge, we designed a hybrid deep-learning framework comprising a novel adaptive DCT preprocessing module, ViT-B16 and ResNet50 backbones, and a Bayesian linear classification head. To our knowledge, we are the first to introduce an adaptive frequency-domain selection mechanism that learns optimal low-, mid-, and high-frequency boundaries suited to the subsequent backbones.   Our network first captures image frequency-domain cues via this adaptive DCT partitioning. The adaptively filtered frequency features are then fed into ViT-B16 to model global contextual relationships, while ResNet50 concurrently extracts local, multi-scale spatial representations from the original image. A cross-level fusion strategy seamlessly integrates these frequency- and spatial-domain embeddings, and the fused features are passed through a Bayesian linear classifier to output the final category predictions. On our self-built 50-class wildlife dataset, this approach outperforms conventional CNN and fixed-band DCT pipelines, achieving state-of-the-art accuracy under extreme sample scarcity. </p>
<blockquote>
<p>稀有动物图像分类面临的一个主要挑战是数据稀缺，因为许多物种通常只有少量标记样本。为了应对这一挑战，我们设计了一种混合深度学习框架，包括新型自适应DCT预处理模块、ViT-B16和ResNet50主干网，以及贝叶斯线性分类头。据我们所知，我们是首次引入自适应频域选择机制，学习适合后续主干的最佳低、中、高频边界。我们的网络首先通过自适应DCT分区捕获图像频域线索。然后，自适应滤波的频域特征输入ViT-B16，以建模全局上下文关系，而ResNet50则同时从原始图像中提取局部多尺度空间表示。跨级融合策略无缝集成了这些频域和空间域嵌入，融合的特征通过贝叶斯线性分类器输出最终的类别预测。在我们自建的50类野生动物数据集上，这种方法优于传统的CNN和固定频段DCT管道，在极端样本稀缺的情况下达到了最先进的准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22701v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种混合深度学习框架，用于解决稀有动物图像分类中的数据稀缺问题。该框架包含自适应DCT预处理模块、ViT-B16和ResNet50骨干网，以及贝叶斯线性分类头。首次引入自适应频率域选择机制，学习适合后续骨干网的最佳低频、中频和高频边界。该网络首先通过自适应DCT分区捕获图像频率域线索，然后将过滤后的频率特征输入ViT-B16以建模全局上下文关系，同时ResNet50从原始图像中提取局部多尺度空间表示。通过跨级别融合策略无缝集成这些频率域和空间域嵌入，融合的特征通过贝叶斯线性分类器输出最终类别预测。在自建的50类野生动物数据集上，该方法优于传统的CNN和固定频段DCT管道，在极端样本稀缺的情况下达到最先进的准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出的混合深度学习框架旨在解决稀有动物图像分类中的数据稀缺问题。</li>
<li>框架包含自适应DCT预处理模块，能够学习最优的频率边界。</li>
<li>引入ViT-B16和ResNet50骨干网来分别建模全局和局部特征。</li>
<li>使用贝叶斯线性分类头进行类别预测。</li>
<li>自适应频率域选择机制能够提高在样本稀缺情况下的分类准确性。</li>
<li>跨级别融合策略集成频率域和空间域嵌入，提升性能。</li>
<li>在自建的50类野生动物数据集上，该方法达到最先进的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22701">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-693a609cada822e24d05ad3096f95784.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5782f3dc282dede5f08f2bdcdd190f0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72531997d4de62d2b46095d42202650d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="From-Head-to-Tail-Towards-Balanced-Representation-in-Large-Vision-Language-Models-through-Adaptive-Data-Calibration"><a href="#From-Head-to-Tail-Towards-Balanced-Representation-in-Large-Vision-Language-Models-through-Adaptive-Data-Calibration" class="headerlink" title="From Head to Tail: Towards Balanced Representation in Large   Vision-Language Models through Adaptive Data Calibration"></a>From Head to Tail: Towards Balanced Representation in Large   Vision-Language Models through Adaptive Data Calibration</h2><p><strong>Authors:Mingyang Song, Xiaoye Qu, Jiawei Zhou, Yu Cheng</strong></p>
<p>Large Vision-Language Models (LVLMs) have achieved significant progress in combining visual comprehension with language generation. Despite this success, the training data of LVLMs still suffers from Long-Tail (LT) problems, where the data distribution is highly imbalanced. Previous works have mainly focused on traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as recognition and classification. Nevertheless, the exploration of LVLM (e.g. LLaVA) and more general tasks (e.g. Visual Question Answering and Visual Reasoning) remains under-explored. In this paper, we first conduct an in-depth analysis of the LT issues in LVLMs and identify two core causes: the overrepresentation of head concepts and the underrepresentation of tail concepts. Based on the above observation, we propose an $\textbf{A}$daptive $\textbf{D}$ata $\textbf{R}$efinement Framework ($\textbf{ADR}$), which consists of two stages: $\textbf{D}$ata $\textbf{R}$ebalancing ($\textbf{DR}$) and $\textbf{D}$ata $\textbf{S}$ynthesis ($\textbf{DS}$). In the DR stage, we adaptively rebalance the redundant data based on entity distributions, while in the DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and scarce images to supplement underrepresented portions. Through comprehensive evaluations across eleven benchmarks, our proposed ADR effectively mitigates the long-tail problem in the training data, improving the average performance of LLaVA 1.5 relatively by 4.36%, without increasing the training data volume. </p>
<blockquote>
<p>大型视觉语言模型（LVLMs）在结合视觉理解与语言生成方面取得了显著进展。然而，尽管取得了成功，LVLM的训练数据仍然面临长尾（LT）问题，即数据分布极度不平衡。之前的研究主要关注传统的VLM架构，例如CLIP或ViT，以及特定的任务，如识别和分类。然而，对于LVLM（例如LLaVA）和更一般的任务（例如视觉问答和视觉推理）的探索仍然不足。在本文中，我们首先对LVLM中的LT问题进行了深入分析，并确定了两个核心原因：头部概念的过度表示和尾部概念的表示不足。基于上述观察，我们提出了一个自适应数据精炼框架（ADR），该框架由两个阶段组成：数据再平衡（DR）和数据合成（DS）。在DR阶段，我们根据实体分布自适应地重新平衡冗余数据，而在DS阶段，我们利用去噪扩散概率模型（DDPMs）和稀缺图像来补充表示不足的部分。通过跨越十一个基准点的全面评估，我们提出的ADR有效地缓解了训练数据中的长尾问题，相对提高了LLaVA 1.5的平均性能4.36%，且没有增加训练数据量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12821v4">PDF</a> Accepted by CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://vlmlt.github.io/">https://vlmlt.github.io/</a></p>
<p><strong>Summary</strong><br>大型视觉语言模型（LVLMs）结合了视觉理解和语言生成技术，取得了显著进展。然而，其训练数据存在长尾（LT）问题，数据分布高度不平衡。本文深入分析了LVLM中的LT问题，并确定了两个核心原因：头部概念的过度表示和尾部概念的表示不足。针对这些问题，提出了自适应数据优化框架（ADR），包括数据再平衡（DR）和数据合成（DS）两个阶段。通过广泛的基准测试，ADR有效地缓解了训练数据中的长尾问题，提高了LLaVA 1.5的平均性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型视觉语言模型（LVLMs）在视觉理解和语言生成方面取得了重要进展。</li>
<li>LVLMs的训练数据存在长尾（LT）问题，即数据分布不平衡。</li>
<li>论文深入分析了LT问题的两个核心原因：头部概念的过度表示和尾部概念的表示不足。</li>
<li>为了解决这些问题，论文提出了自适应数据优化框架（ADR），包括数据再平衡（DR）和数据合成（DS）两个阶段。</li>
<li>DR阶段通过自适应地再平衡数据来解决冗余数据问题。</li>
<li>DS阶段利用去噪扩散概率模型（DDPMs）和稀缺图像来补充表示不足的部分。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12821">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4852890b00f95bf949498f981658774b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-521c9b4ba306272ccf4ecfd77d0f551b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b73bf18f8ffd9cdbf8a8688234a1e2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2e4ba0692f50c329b5560452c3ae0f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc3d4dc0ac0399291646b6977bc5dc3b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Audio-Visual-Segmentation-Through-Text-Embeddings"><a href="#Audio-Visual-Segmentation-Through-Text-Embeddings" class="headerlink" title="Audio Visual Segmentation Through Text Embeddings"></a>Audio Visual Segmentation Through Text Embeddings</h2><p><strong>Authors:Kyungbok Lee, You Zhang, Zhiyao Duan</strong></p>
<p>The goal of Audio-Visual Segmentation (AVS) is to localize and segment the sounding source objects from video frames. Research on AVS suffers from data scarcity due to the high cost of fine-grained manual annotations. Recent works attempt to overcome the challenge of limited data by leveraging the vision foundation model, Segment Anything Model (SAM), prompting it with audio to enhance its ability to segment sounding source objects. While this approach alleviates the model’s burden on understanding visual modality by utilizing knowledge of pre-trained SAM, it does not address the fundamental challenge of learning audio-visual correspondence with limited data. To address this limitation, we propose \textbf{AV2T-SAM}, a novel framework that bridges audio features with the text embedding space of pre-trained text-prompted SAM. Our method leverages multimodal correspondence learned from rich text-image paired datasets to enhance audio-visual alignment. Furthermore, we introduce a novel feature, $\mathbf{\textit{\textbf{f}}<em>{CLIP} \odot \textit{\textbf{f}}</em>{CLAP}}$, which emphasizes shared semantics of audio and visual modalities while filtering irrelevant noise. Our approach outperforms existing methods on the AVSBench dataset by effectively utilizing pre-trained segmentation models and cross-modal semantic alignment. The source code is released at <a target="_blank" rel="noopener" href="https://github.com/bok-bok/AV2T-SAM">https://github.com/bok-bok/AV2T-SAM</a>. </p>
<blockquote>
<p>音频视觉分割（AVS）的目标是定位视频帧中的发声源对象并将其分割出来。由于精细的手动标注成本高昂，AVS研究面临数据稀缺的问题。近期的研究工作试图通过利用视觉基础模型——分段任何事物模型（SAM）来克服数据量有限的挑战，并用音频来提示它以增强其分割发声源对象的能力。虽然这种方法通过利用预训练的SAM的知识减轻了模型对视觉模态理解的压力，但它并没有解决数据有限时学习视听对应关系的根本挑战。为了克服这一局限性，我们提出了一种新的框架AV2T-SAM，它将音频特征与预训练的文本提示SAM的文本嵌入空间相结合。我们的方法利用从丰富的文本图像配对数据集中学习的多模态对应关系来增强视听对齐。此外，我们还引入了一种新的特征$\mathbf{\textit{\textbf{f}}<em>{CLIP} \odot \textit{\textbf{f}}</em>{CLAP}}$，它强调了音频和视觉模态的共享语义，同时过滤掉无关噪声。我们的方法通过在AVSBench数据集上有效地利用预训练的分割模型和跨模态语义对齐，实现了对现有方法的超越。源代码已发布在<a target="_blank" rel="noopener" href="https://github.com/bok-bok/AV2T-SAM%E3%80%82">https://github.com/bok-bok/AV2T-SAM。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16359v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了音频视觉分割（AVS）的目标是从视频帧中定位并分割声音源对象。研究AVS因精细粒度手动注释的高成本而面临数据稀缺的问题。最近的工作尝试通过利用视觉基础模型SAM和音频提示来增强其分割声音源对象的能力来克服数据有限的挑战。然而，这种方法并没有解决在有限数据下学习音频视觉对应关系的根本挑战。为解决此问题，本文提出了AV2T-SAM框架，该框架将音频特征与预训练文本提示SAM的文本嵌入空间相结合，利用丰富的文本图像配对数据集学习多模态对应关系以增强音频视觉对齐。此外，还引入了一种新特征fCLIP⊗fCLAP，强调音频和视觉模态的共享语义，同时过滤掉无关噪声。该方法在AVSBench数据集上优于现有方法，通过有效利用预训练分割模型和跨模态语义对齐。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频视觉分割（AVS）的目标是从视频帧中定位并分割声音源对象。</li>
<li>数据稀缺是AVS研究面临的挑战，因为精细粒度的手动注释成本高昂。</li>
<li>最近的研究尝试通过利用视觉基础模型SAM和音频提示来解决数据稀缺问题。</li>
<li>提出的AV2T-SAM框架结合音频特征与预训练文本提示SAM的文本嵌入空间，增强音频视觉对齐。</li>
<li>AV2T-SAM利用丰富的文本图像配对数据集学习多模态对应关系。</li>
<li>引入新特征fCLIP⊗fCLAP，强调音频和视觉模态的共享语义，过滤无关噪声。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16359">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5c59b4fc6f77f4da6683b413d4f62727.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc06a03c14eb53ea28e0db8866b0cb8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5505295f90d44e3af8ac8d240ed3a1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-144ab32d46ccbbf47366393223650730.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="QMamba-On-First-Exploration-of-Vision-Mamba-for-Image-Quality-Assessment"><a href="#QMamba-On-First-Exploration-of-Vision-Mamba-for-Image-Quality-Assessment" class="headerlink" title="QMamba: On First Exploration of Vision Mamba for Image Quality   Assessment"></a>QMamba: On First Exploration of Vision Mamba for Image Quality   Assessment</h2><p><strong>Authors:Fengbin Guan, Xin Li, Zihao Yu, Yiting Lu, Zhibo Chen</strong></p>
<p>In this work, we take the first exploration of the recently popular foundation model, i.e., State Space Model&#x2F;Mamba, in image quality assessment (IQA), aiming at observing and excavating the perception potential in vision Mamba. A series of works on Mamba has shown its significant potential in various fields, e.g., segmentation and classification. However, the perception capability of Mamba remains under-explored. Consequently, we propose QMamba by revisiting and adapting the Mamba model for three crucial IQA tasks, i.e., task-specific, universal, and transferable IQA, which reveals its clear advantages over existing foundational models, e.g., Swin Transformer, ViT, and CNNs, in terms of perception and computational cost. To improve the transferability of QMamba, we propose the StylePrompt tuning paradigm, where lightweight mean and variance prompts are injected to assist task-adaptive transfer learning of pre-trained QMamba for different downstream IQA tasks. Compared with existing prompt tuning strategies, our StylePrompt enables better perceptual transfer with lower computational cost. Extensive experiments on multiple synthetic, authentic IQA datasets, and cross IQA datasets demonstrate the effectiveness of our proposed QMamba. The code will be available at: <a target="_blank" rel="noopener" href="https://github.com/bingo-G/QMamba.git">https://github.com/bingo-G/QMamba.git</a> </p>
<blockquote>
<p>在这项工作中，我们首次探索了最近流行的基础模型，即状态空间模型（State Space Model）&#x2F;Mamba在图像质量评估（IQA）中的应用，旨在观察挖掘视觉Mamba中的感知潜力。一系列关于Mamba的工作已经显示出它在各个领域的重要潜力，例如分割和分类。然而，Mamba的感知能力仍然没有得到充分的探索。因此，我们通过对Mamba模型进行回顾和适应，提出了QMamba，用于三项关键的IQA任务，即特定任务IQA、通用IQA和可迁移IQA。在感知和计算成本方面，QMamba显示出优于现有基础模型（如Swin Transformer、ViT和CNN）的明显优势。为了提高QMamba的可迁移性，我们提出了StylePrompt微调范式，通过注入轻量级均值和方差提示来辅助针对不同下游IQA任务的预训练QMamba的任务适应性迁移学习。与现有的提示调整策略相比，我们的StylePrompt能够以较低的计算成本实现更好的感知迁移。在多个合成、真实的IQA数据集和跨IQA数据集上的大量实验证明了所提出的QMamba的有效性。代码将在以下网址公开：<a target="_blank" rel="noopener" href="https://github.com/bingo-G/QMamba.git">https://github.com/bingo-G/QMamba.git</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.09546v2">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong><br>     本文首次探索了近期流行的基础模型——状态空间模型&#x2F;曼巴（State Space Model&#x2F;Mamba）在图像质量评估（IQA）领域的应用，旨在挖掘曼巴在视觉领域的感知潜力。针对三种关键的IQA任务，作者提出了改进后的QMamba模型，并在多个数据集上验证了其在感知和计算成本方面的优势。为提高QMamba的迁移能力，作者还提出了StylePrompt微调范式，通过注入轻量级均值和方差提示来辅助预训练的QMamba对不同下游IQA任务的适应性转移学习。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章首次将State Space Model&#x2F;Mamba模型应用于图像质量评估（IQA），显示其在该领域的潜力。</li>
<li>QMamba模型在任务特定、通用和可迁移的IQA任务上均表现出优于其他基础模型的感知和计算成本优势。</li>
<li>为提高QMamba的迁移能力，提出了StylePrompt微调范式。</li>
<li>StylePrompt通过注入轻量级均值和方差提示，辅助预训练QMamba对不同下游IQA任务的适应性转移学习。</li>
<li>StylePrompt相对于现有的提示调整策略，在较低的计算成本下实现了更好的感知迁移。</li>
<li>在多个合成、真实IQA数据集以及跨IQA数据集上的大量实验验证了QMamba模型的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.09546">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1a0a04c51ee256aae6451b7dfa3ffe9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0fd69bb1aeb0c4049a13ed826889a8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-099316b69c8213697558e95e8f405176.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e42a25493d24198b1297b845c04bbb7c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6c30e410c34aa56433ba34851a2ccdad.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-05-31  Adaptive Spatial Augmentation for Semi-supervised Semantic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fa67526f971f8e26a4f1184758a55d40.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-05-31  VAU-R1 Advancing Video Anomaly Understanding via Reinforcement   Fine-Tuning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
