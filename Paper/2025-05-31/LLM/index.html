<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  Argus Vision-Centric Reasoning with Grounded Chain-of-Thought">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-75a094d68bb8bbf4ce38724b838d3791.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-31
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-31-æ›´æ–°"><a href="#2025-05-31-æ›´æ–°" class="headerlink" title="2025-05-31 æ›´æ–°"></a>2025-05-31 æ›´æ–°</h1><h2 id="Argus-Vision-Centric-Reasoning-with-Grounded-Chain-of-Thought"><a href="#Argus-Vision-Centric-Reasoning-with-Grounded-Chain-of-Thought" class="headerlink" title="Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought"></a>Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought</h2><p><strong>Authors:Yunze Man, De-An Huang, Guilin Liu, Shiwei Sheng, Shilong Liu, Liang-Yan Gui, Jan Kautz, Yu-Xiong Wang, Zhiding Yu</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language tasks, yet they often struggle with vision-centric scenarios where precise visual focus is needed for accurate reasoning. In this paper, we introduce Argus to address these limitations with a new visual attention grounding mechanism. Our approach employs object-centric grounding as visual chain-of-thought signals, enabling more effective goal-conditioned visual attention during multimodal reasoning tasks. Evaluations on diverse benchmarks demonstrate that Argus excels in both multimodal reasoning tasks and referring object grounding tasks. Extensive analysis further validates various design choices of Argus, and reveals the effectiveness of explicit language-guided visual region-of-interest engagement in MLLMs, highlighting the importance of advancing multimodal intelligence from a visual-centric perspective. Project page: <a target="_blank" rel="noopener" href="https://yunzeman.github.io/argus/">https://yunzeman.github.io/argus/</a> </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œç„¶è€Œï¼Œå®ƒä»¬åœ¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­å¾€å¾€é¢ä¸´å›°éš¾ï¼Œè¿™äº›åœºæ™¯éœ€è¦ç²¾ç¡®çš„è§†è§‰ç„¦ç‚¹æ¥è¿›è¡Œå‡†ç¡®æ¨ç†ã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†Argusï¼Œé€šè¿‡ä¸€ç§æ–°çš„è§†è§‰æ³¨æ„åŠ›å®šä½æœºåˆ¶æ¥è§£å†³é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¯¹è±¡ä¸ºä¸­å¿ƒçš„å®šä½ä½œä¸ºè§†è§‰æ€ç»´é“¾ä¿¡å·ï¼Œä½¿å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æœŸé—´çš„ä»¥ç›®æ ‡ä¸ºå¯¼å‘çš„è§†è§‰æ³¨æ„åŠ›æ›´åŠ æœ‰æ•ˆã€‚åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒArgusåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡å’ŒæŒ‡ä»£å¯¹è±¡å®šä½ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ã€‚è¿›ä¸€æ­¥çš„åˆ†æéªŒè¯äº†Argusçš„å„ç§è®¾è®¡é€‰æ‹©çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ­ç¤ºäº†MLLMä¸­æ˜ç¡®çš„è¯­è¨€å¼•å¯¼çš„è§†è§‰æ„Ÿå…´è¶£åŒºåŸŸå‚ä¸çš„é‡è¦æ€§ï¼Œå¼ºè°ƒäº†ä»è§†è§‰ä¸­å¿ƒè§†è§’å‘å±•å¤šæ¨¡æ€æ™ºèƒ½çš„é‡è¦æ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://yunzeman.github.io/argus/">https://yunzeman.github.io/argus/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23766v1">PDF</a> CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://yunzeman.github.io/argus/">https://yunzeman.github.io/argus/</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦ç²¾ç¡®è§†è§‰æ¨ç†çš„ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºArgusï¼Œé€šè¿‡æ–°çš„è§†è§‰æ³¨æ„åŠ›å®šä½æœºåˆ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚Argusé‡‡ç”¨å¯¹è±¡ä¸ºä¸­å¿ƒçš„å®šä½ä½œä¸ºè§†è§‰æ€ç»´ä¿¡å·ï¼Œåœ¨å¤šåª’ä½“æ¨ç†ä»»åŠ¡ä¸­æ›´æœ‰æ•ˆåœ°å®ç°ç›®æ ‡å¯¼å‘çš„è§†è§‰æ³¨æ„åŠ›ã€‚åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒArgusåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡å’ŒæŒ‡ä»£å¯¹è±¡å®šä½ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Argusé€šè¿‡å¼•å…¥æ–°çš„è§†è§‰æ³¨æ„åŠ›å®šä½æœºåˆ¶ï¼Œè§£å†³äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>Argusé‡‡ç”¨å¯¹è±¡ä¸ºä¸­å¿ƒçš„å®šä½ä½œä¸ºè§†è§‰æ€ç»´ä¿¡å·ï¼Œæå‡å¤šåª’ä½“æ¨ç†ä»»åŠ¡ä¸­çš„ç›®æ ‡å¯¼å‘è§†è§‰æ³¨æ„åŠ›ã€‚</li>
<li>Argusåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</li>
<li>å…¨é¢çš„åˆ†æéªŒè¯äº†Arguså„ç§è®¾è®¡é€‰æ‹©çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>Arguså¼ºè°ƒäº†åœ¨å¤šæ¨¡æ€æ™ºèƒ½ä¸­æ˜ç¡®è¯­è¨€å¼•å¯¼çš„è§†è§‰åŒºåŸŸå…´è¶£ç‚¹å‚ä¸çš„é‡è¦æ€§ã€‚</li>
<li>Argusçš„å¼•å…¥ä¸ºæ¨è¿›å¤šæ¨¡æ€æ™ºèƒ½ä»ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„è§’åº¦æä¾›äº†æœ‰æ•ˆæ‰‹æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23766">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-667545b533b1d20697a3a865b3a4586e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a38cdd774687d05f12d73cdc57b37a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-833e6201faef1d4566b263957c19faf4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-586f1f4cca013267d9b6b043bb6148e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b6cf2ce053f58bf6d7f2ef9c4f5f9b5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MMSI-Bench-A-Benchmark-for-Multi-Image-Spatial-Intelligence"><a href="#MMSI-Bench-A-Benchmark-for-Multi-Image-Spatial-Intelligence" class="headerlink" title="MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence"></a>MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence</h2><p><strong>Authors:Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, Dahua Lin, Tai Wang, Jiangmiao Pang</strong></p>
<p>Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAIâ€™s o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Project page: <a target="_blank" rel="noopener" href="https://runsenxu.com/projects/MMSI_Bench">https://runsenxu.com/projects/MMSI_Bench</a> . </p>
<blockquote>
<p>ç©ºé—´æ™ºèƒ½å¯¹äºåœ¨å¤æ‚çš„ç‰©ç†ä¸–ç•Œä¸­è¿è¡Œçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä»…æ¢è®¨å•å›¾åƒå…³ç³»ï¼Œå› æ­¤æ— æ³•è¯„ä¼°ç°å®ä¸–ç•Œéƒ¨ç½²æ‰€éœ€çš„å¤šå›¾åƒç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†MMSI-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºå¤šå›¾åƒç©ºé—´æ™ºèƒ½çš„VQAåŸºå‡†æµ‹è¯•ã€‚å…­ä½3Dè§†è§‰ç ”ç©¶äººå‘˜èŠ±è´¹äº†è¶…è¿‡300å°æ—¶çš„æ—¶é—´ï¼Œä»è¶…è¿‡12ä¸‡å¼ å›¾åƒä¸­ç²¾å¿ƒåˆ›ä½œäº†1000ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ã€æ— æ­§ä¹‰çš„å¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½é…å¤‡äº†ç²¾å¿ƒè®¾è®¡çš„å¹²æ‰°é¡¹å’Œé€æ­¥æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶å¯¹34ä¸ªå¼€æºå’Œä¸“æœ‰MLLMè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè§‚å¯Ÿåˆ°è¾ƒå¤§çš„å·®è·ï¼šæœ€å¼ºçš„å¼€æºæ¨¡å‹å‡†ç¡®ç‡çº¦ä¸º30%ï¼ŒOpenAIçš„o3æ¨ç†æ¨¡å‹è¾¾åˆ°40%ï¼Œè€Œäººç±»å¾—åˆ†ä¸º97%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†MMSI-Benchçš„æŒ‘æˆ˜æ€§ï¼Œä»¥åŠæœªæ¥ç ”ç©¶çš„å·¨å¤§æ½œåŠ›ã€‚åˆ©ç”¨æ³¨é‡Šçš„æ¨ç†è¿‡ç¨‹ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªè‡ªåŠ¨é”™è¯¯åˆ†æç®¡é“ï¼Œè¯Šæ–­äº†å››ç§ä¸»è¦çš„å¤±è´¥æ¨¡å¼ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰æ¥åœ°é”™è¯¯ï¼Œï¼ˆ2ï¼‰é‡å åŒ¹é…å’Œåœºæ™¯é‡å»ºé”™è¯¯ï¼Œï¼ˆ3ï¼‰æƒ…å†µè½¬æ¢æ¨ç†é”™è¯¯ï¼Œå’Œï¼ˆ4ï¼‰ç©ºé—´é€»è¾‘é”™è¯¯ï¼Œä¸ºæ¨è¿›å¤šå›¾åƒç©ºé—´æ™ºèƒ½æä¾›äº†å®è´µçš„è§è§£ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://runsenxu.com/projects/MMSI_Bench%E3%80%82">https://runsenxu.com/projects/MMSI_Benchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23764v1">PDF</a> 34 pages. A comprehensive, fully human-curated, multi-image-based   spatial intelligence benchmark with reasoning annotation for MLLMs. Project   page: <a target="_blank" rel="noopener" href="https://runsenxu.com/projects/MMSI_Bench">https://runsenxu.com/projects/MMSI_Bench</a></p>
<p><strong>Summary</strong><br>     ç©ºé—´æ™ºèƒ½å¯¹äºåœ¨å¤æ‚ç‰©ç†ä¸–ç•Œä¸­è¿è¡Œçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä»…æ¢ç´¢å•å›¾åƒå…³ç³»ï¼Œæ— æ³•è¯„ä¼°å¤šå›¾åƒç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä¸ç°å®ä¸–ç•Œåº”ç”¨éœ€æ±‚ä¸ç¬¦ã€‚æœ¬ç ”ç©¶å¼•å…¥MMSI-Benchï¼Œä¸“é—¨é¢å‘å¤šå›¾åƒç©ºé—´æ™ºèƒ½çš„è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶å›¢é˜Ÿè€—æ—¶è¶…è¿‡300å°æ—¶ï¼Œç²¾å¿ƒåˆ›ä½œäº†è¶…è¿‡1ä¸‡é“é€‰æ‹©é¢˜ï¼Œæ¶µç›–è¶…è¿‡12ä¸‡å¼ å›¾åƒï¼Œæ¯ä¸ªé—®é¢˜å‡é…æœ‰ç²¾å¿ƒè®¾è®¡å¹²æ‰°é¡¹å’Œé€æ­¥æ¨ç†è¿‡ç¨‹ã€‚è¯„ä¼°äº†å¤šç§å¼€æºå’Œä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‘ç°è¡¨ç°å‚å·®ä¸é½ï¼Œè¡¨ç°æœ€å¥½çš„å¼€æºæ¨¡å‹å‡†ç¡®ç‡çº¦30%ï¼ŒOpenAIçš„o3æ¨ç†æ¨¡å‹è¾¾åˆ°40%ï¼Œè€Œäººç±»å‡†ç¡®ç‡é«˜è¾¾97%ã€‚æ­¤ç ”ç©¶è¿˜æä¾›äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–é”™è¯¯åˆ†æç®¡é“ï¼Œæ€»ç»“äº†å››å¤§å¤±è´¥æ¨¡å¼ã€‚è¯¦æƒ…è¯·è®¿é—®é¡¹ç›®ä¸»é¡µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç©ºé—´æ™ºèƒ½åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå°¤å…¶åœ¨å¤æ‚ç‰©ç†ä¸–ç•Œçš„åº”ç”¨èƒŒæ™¯ä¸‹ã€‚</li>
<li>å½“å‰åŸºå‡†æµ‹è¯•æ— æ³•å……åˆ†è¯„ä¼°å¤šå›¾åƒç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MMSI-Benchå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œæä¾›äº†ä¸“é—¨é¢å‘å¤šå›¾åƒç©ºé—´æ™ºèƒ½çš„è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†å¤§é‡çš„é€‰æ‹©é¢˜æ¥è¯„ä¼°æ¨¡å‹è¡¨ç°ï¼Œæ¶µç›–äº†ç²¾å¿ƒè®¾è®¡çš„å¹²æ‰°é¡¹å’Œé€æ­¥æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>æ¨¡å‹è¯„ä¼°ç»“æœæ˜¾ç¤ºå­˜åœ¨å·¨å¤§å·®å¼‚ï¼Œè¡¨ç°å‡ºå¤šå›¾åƒç©ºé—´æ™ºèƒ½çš„å¤æ‚æ€§åŠæå‡ç©ºé—´ã€‚</li>
<li>OpenAIçš„o3æ¨ç†æ¨¡å‹åœ¨è¯„ä¼°ä¸­è¡¨ç°ç›¸å¯¹è¾ƒå¥½ï¼Œä½†ä»æœ‰æå‡ç©ºé—´ã€‚</li>
<li>äººç±»åœ¨å¤šå›¾åƒç©ºé—´æ™ºèƒ½ä»»åŠ¡ä¸­çš„è¡¨ç°è¿œè¶…è¿‡ç°æœ‰æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23764">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6468bba9c9a74234195402e6a3a637a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e62a9bf63f3fbb36ea45d4510f20b17a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b8811fa1077ab1534085b10384187e30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d223572e0b9fc6c2b43aca469397231.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-03bb89879c3526ae4e503dcc54409571.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DeepTheorem-Advancing-LLM-Reasoning-for-Theorem-Proving-Through-Natural-Language-and-Reinforcement-Learning"><a href="#DeepTheorem-Advancing-LLM-Reasoning-for-Theorem-Proving-Through-Natural-Language-and-Reinforcement-Learning" class="headerlink" title="DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural   Language and Reinforcement Learning"></a>DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural   Language and Reinforcement Learning</h2><p><strong>Authors:Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhengwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu</strong></p>
<p>Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMsâ€™ strength derived from informal, natural language knowledge acquired during pre-training. In this work, we propose DeepTheorem, a comprehensive informal theorem-proving framework exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes a large-scale benchmark dataset consisting of 121K high-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants. We devise a novel reinforcement learning strategy (RL-Zero) explicitly tailored to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference. Additionally, we propose comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheoremâ€™s potential to fundamentally advance automated informal theorem proving and mathematical exploration. </p>
<blockquote>
<p>å®šç†è¯æ˜æ˜¯è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤æ‚æ¨ç†èƒ½åŠ›çš„ä¸»è¦æµ‹è¯•åºŠã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„è‡ªåŠ¨åŒ–å®šç†è¯æ˜ï¼ˆATPï¼‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºå½¢å¼åŒ–è¯æ˜ç³»ç»Ÿï¼Œè€Œè¿™äº›ç³»ç»Ÿå¹¶ä¸èƒ½å¾ˆå¥½åœ°ä¸LLMåœ¨é¢„è®­ç»ƒæœŸé—´è·å¾—çš„ä¸æ­£å¼çš„ã€è‡ªç„¶è¯­è¨€çŸ¥è¯†ç›¸ç»“åˆçš„ä¼˜åŠ¿ç›¸å¥‘åˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DeepTheoremï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„éæ­£å¼å®šç†è¯æ˜æ¡†æ¶ï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€å¢å¼ºLLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚DeepTheoremåŒ…æ‹¬ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«12.1ä¸‡ä¸ªé«˜è´¨é‡çš„IMOçº§éæ­£å¼å®šç†å’Œè¯æ˜ï¼Œæ¶µç›–å¤šæ ·åŒ–çš„æ•°å­¦é¢†åŸŸï¼Œå¯¹æ­£ç¡®æ€§ã€éš¾åº¦å’Œä¸»é¢˜ç±»åˆ«è¿›è¡Œäº†ä¸¥æ ¼çš„æ³¨é‡Šï¼Œå¹¶é…æœ‰ç³»ç»Ÿæ„å»ºçš„å¯éªŒè¯å®šç†å˜ä½“ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼ˆRL-Zeroï¼‰ï¼Œä¸“é—¨é’ˆå¯¹éæ­£å¼å®šç†è¯æ˜ï¼Œåˆ©ç”¨å¯éªŒè¯çš„å®šç†å˜ä½“æ¥æ¿€åŠ±ç¨³å¥çš„æ•°å­¦æ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨é¢çš„ç»“æœå’Œè¿‡ç¨‹è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥æ£€æŸ¥è¯æ˜çš„æ­£ç¡®æ€§å’Œæ¨ç†æ­¥éª¤çš„è´¨é‡ã€‚å¹¿æ³›çš„å®éªŒåˆ†æè¡¨æ˜ï¼Œä¸ç°æœ‰æ•°æ®é›†å’Œç›‘ç£å¾®è°ƒåè®®ç›¸æ¯”ï¼ŒDeepTheoremåœ¨å®šç†è¯æ˜æ€§èƒ½ä¸Šæ˜¾è‘—æé«˜LLMçš„å®šç†è¯æ˜èƒ½åŠ›ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œæ¨ç†è´¨é‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒDeepTheoremåœ¨è‡ªåŠ¨åŒ–éæ­£å¼å®šç†è¯æ˜å’Œæ•°å­¦æ¢ç´¢æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23754v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢å±•ç°å‡ºäº†æ½œåŠ›ï¼Œä½†åœ¨å®šç†è¯æ˜ä¸Šä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¼ ç»Ÿè‡ªåŠ¨å®šç†è¯æ˜æ–¹æ³•ä¸»è¦ä¾èµ–å½¢å¼åŒ–è¯æ˜ç³»ç»Ÿï¼Œè¿™ä¸LLMçš„ä¼˜åŠ¿å¹¶ä¸åŒ¹é…ã€‚æœ¬ç ”ç©¶æå‡ºäº†DeepTheoremæ¡†æ¶ï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€å¢å¼ºLLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«é«˜è´¨é‡çš„éæ­£å¼å®šç†å’Œè¯æ˜ï¼Œä»¥åŠé’ˆå¯¹éæ­£å¼å®šç†è¯æ˜å®šåˆ¶çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼ŒDeepTheoremæ˜¾è‘—æé«˜äº†LLMçš„å®šç†è¯æ˜æ€§èƒ½ï¼Œæœ‰æœ›æ¨åŠ¨éæ­£å¼å®šç†è¯æ˜çš„è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®šç†è¯æ˜ä¸Šä»æœ‰æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–å½¢å¼åŒ–è¯æ˜ç³»ç»Ÿï¼Œä¸LLMçš„ä¼˜åŠ¿ä¸åŒ¹é…ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†DeepTheoremæ¡†æ¶ï¼Œç»“åˆè‡ªç„¶è¯­è¨€å¢å¼ºLLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>DeepTheoremåŒ…å«å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–å¤šæ ·åŒ–æ•°å­¦é¢†åŸŸçš„éæ­£å¼å®šç†å’Œè¯æ˜ã€‚</li>
<li>æå‡ºäº†é’ˆå¯¹éæ­£å¼å®šç†è¯æ˜çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥RL-Zeroã€‚</li>
<li>DeepTheoremæ˜¾è‘—æé«˜LLMçš„å®šç†è¯æ˜æ€§èƒ½ï¼ŒåŒ…æ‹¬å‡†ç¡®ç‡å’Œæ¨ç†è´¨é‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23754">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25d0b7941d89428cbe84173cdf915108.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ef8cedc1008827cc3fdfae761cbb2fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-778733e3d530d7b931dd6a58ef31fb5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-193b7aad8ae90f97f6236d2b38a0296c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b8ca2ed393c9ca7355ed52805512929.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b653163b92d44fc8e1e05bc878d27a6a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Distortion-of-AI-Alignment-Does-Preference-Optimization-Optimize-for-Preferences"><a href="#Distortion-of-AI-Alignment-Does-Preference-Optimization-Optimize-for-Preferences" class="headerlink" title="Distortion of AI Alignment: Does Preference Optimization Optimize for   Preferences?"></a>Distortion of AI Alignment: Does Preference Optimization Optimize for   Preferences?</h2><p><strong>Authors:Paul GÃ¶lz, Nika Haghtalab, Kunhe Yang</strong></p>
<p>After pre-training, large language models are aligned with human preferences based on pairwise comparisons. State-of-the-art alignment methods (such as PPO-based RLHF and DPO) are built on the assumption of aligning with a single preference model, despite being deployed in settings where users have diverse preferences. As a result, it is not even clear that these alignment methods produce models that satisfy users on average â€“ a minimal requirement for pluralistic alignment. Drawing on social choice theory and modeling usersâ€™ comparisons through individual Bradley-Terry (BT) models, we introduce an alignment methodâ€™s distortion: the worst-case ratio between the optimal achievable average utility, and the average utility of the learned policy.   The notion of distortion helps draw sharp distinctions between alignment methods: Nash Learning from Human Feedback achieves the minimax optimal distortion of $(\frac{1}{2} + o(1)) \cdot \beta$ (for the BT temperature $\beta$), robustly across utility distributions, distributions of comparison pairs, and permissible KL divergences from the reference policy. RLHF and DPO, by contrast, suffer $\geq (1 - o(1)) \cdot \beta$ distortion already without a KL constraint, and $e^{\Omega(\beta)}$ or even unbounded distortion in the full setting, depending on how comparison pairs are sampled. </p>
<blockquote>
<p>é¢„è®­ç»ƒåï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åŸºäºé…å¯¹æ¯”è¾ƒä¸äººç±»åå¥½å¯¹é½ã€‚æœ€å…ˆè¿›çš„å¯¹é½æ–¹æ³•ï¼ˆå¦‚åŸºäºPPOçš„RLHFå’ŒDPOï¼‰å»ºç«‹åœ¨ä¸å•ä¸€åå¥½æ¨¡å‹å¯¹é½çš„å‡è®¾ä¹‹ä¸Šï¼Œå°½ç®¡å®ƒä»¬éƒ¨ç½²åœ¨ç”¨æˆ·åå¥½å¤šæ ·çš„ç¯å¢ƒä¸­ã€‚å› æ­¤ï¼Œè¿™äº›å¯¹é½æ–¹æ³•äº§ç”Ÿçš„æ¨¡å‹æ˜¯å¦æ»¡è¶³ç”¨æˆ·çš„å¹³å‡éœ€æ±‚å°šä¸æ¸…æ¥šâ€”â€”è¿™æ˜¯å¤šå…ƒå¯¹é½çš„æœ€ä½è¦æ±‚ã€‚æˆ‘ä»¬å€Ÿé‰´ç¤¾ä¼šé€‰æ‹©ç†è®ºå’Œé€šè¿‡ä¸ªä½“Bradley-Terryï¼ˆBTï¼‰æ¨¡å‹å¯¹ç”¨æˆ·æ¯”è¾ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¼•å…¥äº†ä¸€ç§å¯¹é½æ–¹æ³•çš„å¤±çœŸï¼šæœ€ä¼˜å¯è¾¾å¹³å‡æ•ˆç”¨ä¸æ‰€å­¦ç­–ç•¥çš„å¹³å‡æ•ˆç”¨ä¹‹é—´çš„æœ€åæƒ…å†µæ¯”ç‡ã€‚å¤±çœŸçš„æ¦‚å¿µæœ‰åŠ©äºåœ¨å¯¹é½æ–¹æ³•ä¹‹é—´åˆ’å‡ºé²œæ˜çš„ç•Œé™ï¼šä»äººç±»åé¦ˆä¸­å­¦ä¹ çº³ä»€ç­–ç•¥å®ç°äº†æœ€å°æœ€å¤§æœ€ä¼˜å¤±çœŸï¼ˆå¯¹äºBTæ¸©åº¦Î²çš„ï¼‰ï¼ˆ$\frac{1}{2} + o(1)$ï¼‰å€ï¼Œåœ¨å„ç§æ•ˆç”¨åˆ†å¸ƒã€å¯¹æ¯”å¯¹åˆ†å¸ƒå’Œå‚ç…§ç­–ç•¥çš„KLæ•£åº¦é™åˆ¶ä¸‹éƒ½èƒ½ç¨³å¥è¡¨ç°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRLHFå’ŒDPOåœ¨ä¸ä½¿ç”¨KLçº¦æŸçš„æƒ…å†µä¸‹å·²ç»é­å—äº†è‡³å°‘ï¼ˆ$1 - o(1)$ï¼‰å€çš„å¤±çœŸï¼Œè€Œåœ¨å¯¹æ¯”å¯¹é‡‡æ ·å®Œæ•´çš„æƒ…å†µä¸‹ï¼Œå¤±çœŸä¸º$e^{\Omega(\beta)}$ç”šè‡³æ›´å¤§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23749v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ç»è¿‡é¢„è®­ç»ƒåï¼ŒåŸºäºæˆå¯¹æ¯”è¾ƒä¸äººç±»åå¥½å¯¹é½ã€‚ç°æœ‰çš„å¯¹é½æ–¹æ³•ï¼ˆå¦‚åŸºäºPPOçš„RLHFå’ŒDPOï¼‰å‡è®¾å¯¹é½å•ä¸€åå¥½æ¨¡å‹ï¼Œä½†åœ¨ç”¨æˆ·åå¥½å¤šæ ·çš„ç¯å¢ƒä¸­éƒ¨ç½²æ—¶å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥å¯¹é½æ–¹æ³•çš„å¤±çœŸæ¦‚å¿µï¼Œå³æœ€ä¼˜å¯å®ç°çš„å¹³å‡æ•ˆç”¨ä¸å­¦ä¹ æ”¿ç­–çš„å¹³å‡æ•ˆç”¨ä¹‹é—´çš„æœ€åæƒ…å†µæ¯”ç‡ã€‚Nashä»äººç±»åé¦ˆå­¦ä¹ ä¸­å®ç°å¯¹é½çš„æ–¹æ³•è·å¾—æœ€æœ€ä¼˜åŒ–çš„å¤±çœŸï¼Œè€ŒRLHFå’ŒDPOåœ¨æ²¡æœ‰KLçº¦æŸçš„æƒ…å†µä¸‹å°±å·²ç»å­˜åœ¨è¾ƒå¤§çš„å¤±çœŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ç»è¿‡é¢„è®­ç»ƒåï¼Œå¯ä»¥é€šè¿‡æˆå¯¹æ¯”è¾ƒä¸äººç±»åå¥½å¯¹é½ã€‚</li>
<li>ç°æœ‰å¯¹é½æ–¹æ³•ï¼ˆå¦‚RLHFå’ŒDPOï¼‰å‡è®¾å¯¹é½å•ä¸€åå¥½æ¨¡å‹ï¼Œä½†åœ¨å®é™…ä¸­é¢ä¸´ç”¨æˆ·åå¥½å¤šæ ·æ€§é—®é¢˜ã€‚</li>
<li>å¯¹é½æ–¹æ³•çš„å¤±çœŸæ¦‚å¿µç”¨äºè¡¡é‡æœ€ä¼˜å¯å®ç°çš„å¹³å‡æ•ˆç”¨ä¸å­¦ä¹ æ”¿ç­–çš„å¹³å‡æ•ˆç”¨ä¹‹é—´çš„å·®è·ã€‚</li>
<li>Nashä»äººç±»åé¦ˆå­¦ä¹ ä¸­å®ç°å¯¹é½çš„æ–¹æ³•å…·æœ‰æœ€å°çš„å¤±çœŸã€‚</li>
<li>RLHFå’ŒDPOåœ¨æ²¡æœ‰KLçº¦æŸçš„æƒ…å†µä¸‹å­˜åœ¨è¾ƒå¤§çš„å¤±çœŸé—®é¢˜ã€‚</li>
<li>å¯¹æ¯”æ ·æœ¬çš„é‡‡é›†æ–¹å¼ä¼šå½±å“å¯¹é½æ–¹æ³•çš„å¤±çœŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3d42263ef79aefc6702a3ff4ff1ae3be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d65ed3b94bb7de4cad425f22d52387a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac65f343919e4ccdaffbcb78d5906898.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Spatial-MLLM-Boosting-MLLM-Capabilities-in-Visual-based-Spatial-Intelligence"><a href="#Spatial-MLLM-Boosting-MLLM-Capabilities-in-Visual-based-Spatial-Intelligence" class="headerlink" title="Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial   Intelligence"></a>Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial   Intelligence</h2><p><strong>Authors:Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan</strong></p>
<p>Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: <a target="_blank" rel="noopener" href="https://diankun-wu.github.io/Spatial-MLLM/">https://diankun-wu.github.io/Spatial-MLLM/</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è¿›æ­¥åœ¨2Dè§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œæé«˜å®ƒä»¬çš„ç©ºé—´æ™ºèƒ½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„3D MLLMæ€»æ˜¯ä¾èµ–äºé¢å¤–çš„3Dæˆ–2.5Dæ•°æ®æ¥èå…¥ç©ºé—´æ„ŸçŸ¥ï¼Œè¿™åœ¨åªæœ‰2Dè¾“å…¥çš„æƒ…å¢ƒä¸­ï¼ˆä¾‹å¦‚å›¾åƒæˆ–è§†é¢‘ï¼‰é™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Spatial-MLLMï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºçº¯ç²¹çš„2Dè§‚å¯Ÿè¿›è¡Œè§†è§‰ç©ºé—´æ¨ç†çš„æ–°å‹æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–äºCLIPçš„è§†è§‰ç¼–ç å™¨ä¼˜åŒ–çš„è¯­ä¹‰ç†è§£çš„è§†é¢‘MLLMä¸åŒï¼Œæˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯é‡Šæ”¾æ¥è‡ªå‰é¦ˆè§†è§‰å‡ ä½•åŸºç¡€æ¨¡å‹çš„å¼ºå¤§ç»“æ„å…ˆéªŒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒç¼–ç å™¨æ¶æ„ï¼šä¸€ä¸ªé¢„è®­ç»ƒçš„2Dè§†è§‰ç¼–ç å™¨æ¥æå–è¯­ä¹‰ç‰¹å¾ï¼Œä»¥åŠä¸€ä¸ªä»è§†è§‰å‡ ä½•æ¨¡å‹ä¸»å¹²åˆå§‹åŒ–çš„ç©ºé—´ç¼–ç å™¨æ¥æå–3Dç»“æ„ç‰¹å¾ã€‚ç„¶åï¼Œä¸€ä¸ªè¿æ¥å™¨å°†è¿™ä¸¤ç§ç‰¹å¾é›†æˆåˆ°ç»Ÿä¸€çš„è§†è§‰ä»¤ç‰Œä¸­ï¼Œä»¥å¢å¼ºç©ºé—´ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¨ç†æ—¶æå‡ºäº†ä¸€ç§ç©ºé—´æ„ŸçŸ¥å¸§é‡‡æ ·ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€‰æ‹©äº†è§†é¢‘åºåˆ—ä¸­ç©ºé—´ä¿¡æ¯ä¸°å¯Œçš„å¸§ï¼Œç¡®ä¿å³ä½¿åœ¨æœ‰é™çš„ä»¤ç‰Œé•¿åº¦ä¸‹ï¼Œæ¨¡å‹ä¹Ÿèƒ½å…³æ³¨å¯¹ç©ºé—´æ¨ç†è‡³å…³é‡è¦çš„å¸§ã€‚é™¤äº†æ¶æ„æ”¹è¿›å¤–ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†Spatial-MLLM-120kæ•°æ®é›†ï¼Œå¹¶åœ¨å…¶ä¸Šä½¿ç”¨æœ‰ç›‘ç£çš„å¾®è°ƒç­–ç•¥å’ŒGRPOè®­ç»ƒäº†æ¨¡å‹ã€‚åœ¨å¤šç§çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç©ºé—´MLLMåœ¨åŸºäºè§†è§‰çš„ç©ºé—´ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://diankun-wu.github.io/Spatial-MLLM/%E3%80%82">https://diankun-wu.github.io/Spatial-MLLM/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23747v1">PDF</a> 21 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSpatial-MLLMçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºåŸºäºçº¯ç²¹çš„äºŒç»´è§‚æµ‹è¿›è¡Œè§†è§‰ç©ºé—´æ¨ç†ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒç¼–ç å™¨æ¶æ„ï¼Œç»“åˆé¢„è®­ç»ƒçš„äºŒç»´è§†è§‰ç¼–ç å™¨å’Œç©ºé—´ç¼–ç å™¨ï¼Œä»¥æå–è¯­ä¹‰ç‰¹å¾å’Œä¸‰ç»´ç»“æ„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§ç©ºé—´æ„ŸçŸ¥å¸§é‡‡æ ·ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹åœ¨æœ‰é™ä»¤ç‰Œé•¿åº¦ä¸‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚é€šè¿‡Spatial-MLLM-120kæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œè¯æ˜äº†è¯¥æ¨¡å‹åœ¨è§†è§‰åŸºç¡€ç©ºé—´ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Spatial-MLLMæ¡†æ¶æˆåŠŸåœ°å°†äºŒç»´è§†è§‰ä»»åŠ¡ä¸ç©ºé—´æ¨ç†ç›¸ç»“åˆã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨åŒç¼–ç å™¨æ¶æ„ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒçš„äºŒç»´è§†è§‰ç¼–ç å™¨å’Œç©ºé—´ç¼–ç å™¨ï¼Œä»¥æå–ä¸åŒç‰¹å¾ã€‚</li>
<li>æå‡ºäº†ç©ºé—´æ„ŸçŸ¥å¸§é‡‡æ ·ç­–ç•¥ï¼Œç”¨äºæé«˜æ¨¡å‹åœ¨æœ‰é™ä»¤ç‰Œé•¿åº¦ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>Spatial-MLLM-120kæ•°æ®é›†ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œæ¶µç›–ä¸°å¯Œçš„ç©ºé—´ä¿¡æ¯ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜äº†è¯¥æ¨¡å‹åœ¨å¤šç§çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23747">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7ef78a0ce94d8677f63f6b11fee1f066.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7850f79b76ebf8f7d299f92ab2d7ee10.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Bounded-Rationality-for-LLMs-Satisficing-Alignment-at-Inference-Time"><a href="#Bounded-Rationality-for-LLMs-Satisficing-Alignment-at-Inference-Time" class="headerlink" title="Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time"></a>Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time</h2><p><strong>Authors:Mohamad Chehade, Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Dinesh Manocha, Hao Zhu, Amrit Singh Bedi</strong></p>
<p>Aligning large language models with humans is challenging due to the inherently multifaceted nature of preference feedback. While existing approaches typically frame this as a multi-objective optimization problem, they often overlook how humans actually make decisions. Research on bounded rationality suggests that human decision making follows satisficing strategies-optimizing primary objectives while ensuring others meet acceptable thresholds. To bridge this gap and operationalize the notion of satisficing alignment, we propose SITAlign: an inference time framework that addresses the multifaceted nature of alignment by maximizing a primary objective while satisfying threshold-based constraints on secondary criteria. We provide theoretical insights by deriving sub-optimality bounds of our satisficing based inference alignment approach. We empirically validate SITAlignâ€™s performance through extensive experimentation on multiple benchmarks. For instance, on the PKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while ensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art multi objective decoding strategy by a margin of 22.3% in terms of GPT-4 win-tie rate for helpfulness reward while adhering to the threshold on harmlessness. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»å¯¹é½æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºåå¥½åé¦ˆçš„å›ºæœ‰å¤šå…ƒæ€§è´¨ã€‚å°½ç®¡ç°æœ‰çš„æ–¹æ³•é€šå¸¸å°†å…¶æ„å»ºä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†äººç±»å®é™…å¦‚ä½•åšå‡ºå†³ç­–ã€‚å…³äºæœ‰é™ç†æ€§çš„ç ”ç©¶è¡¨æ˜ï¼Œäººç±»çš„å†³ç­–åˆ¶å®šéµå¾ªæ»¡æ„ç­–ç•¥ï¼Œå³ä¼˜åŒ–ä¸»è¦ç›®æ ‡ï¼ŒåŒæ—¶ç¡®ä¿å…¶ä»–ç›®æ ‡è¾¾åˆ°å¯æ¥å—çš„é˜ˆå€¼ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·å¹¶è½å®æ»¡æ„å¯¹é½çš„æ¦‚å¿µï¼Œæˆ‘ä»¬æå‡ºäº†SITAlignï¼šä¸€ä¸ªæ¨ç†æ—¶é—´æ¡†æ¶ï¼Œé€šè¿‡æœ€å¤§åŒ–ä¸»è¦ç›®æ ‡å¹¶æ»¡è¶³åŸºäºé˜ˆå€¼çš„æ¬¡è¦æ ‡å‡†çº¦æŸï¼Œæ¥è§£å†³å¯¹é½çš„å¤šå…ƒæ€§è´¨é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡æ¨å¯¼åŸºäºæ»¡æ„çš„æ¨ç†å¯¹é½æ–¹æ³•çš„æ¬¡ä¼˜æ€§ç•Œé™ï¼Œæä¾›äº†ç†è®ºè§è§£ã€‚æˆ‘ä»¬é€šè¿‡å¤šä¸ªåŸºå‡†æµ‹è¯•è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå®è¯éªŒè¯äº†SITAlignçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨PKU-SafeRLHFæ•°æ®é›†ä¸Šï¼Œä¸»è¦ç›®æ ‡æ˜¯æœ€å¤§åŒ–æœ‰ç›Šæ€§ï¼ŒåŒæ—¶ç¡®ä¿æ— å®³æ€§çš„é˜ˆå€¼ï¼ŒSITAlignåœ¨æœ‰ç›Šæ€§å¥–åŠ±æ–¹é¢ä»¥22.3%çš„ä¼˜åŠ¿è¶…è¶Šäº†æœ€æ–°çš„å¤šç›®æ ‡è§£ç ç­–ç•¥ï¼ŒåŒæ—¶éµå®ˆæ— å®³æ€§çš„é˜ˆå€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23729v1">PDF</a> Accepted at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»å¯¹é½çš„æŒ‘æˆ˜æ€§ï¼Œç”±äºåå¥½åé¦ˆçš„å¤šå…ƒæ€§è´¨ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥äººç±»å®é™…å†³ç­–è¿‡ç¨‹ã€‚ç ”ç©¶æŒ‡å‡ºäººç±»å†³ç­–éµå¾ªæ»¡æ„ç­–ç•¥ï¼Œä¼˜åŒ–ä¸»è¦ç›®æ ‡åŒæ—¶ç¡®ä¿å…¶ä»–ç›®æ ‡è¾¾åˆ°å¯æ¥å—é˜ˆå€¼ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºSATISFICINGå¯¹é½æ¡†æ¶ï¼ˆç®€ç§°SATAlignï¼‰ï¼Œåœ¨æ¨ç†é˜¶æ®µæœ€å¤§åŒ–ä¸»è¦ç›®æ ‡çš„åŒæ—¶æ»¡è¶³åŸºäºé˜ˆå€¼çš„æ¬¡è¦æ ‡å‡†çº¦æŸï¼Œä»¥ç¼©å°äººç±»ä¸è¯­è¨€æ¨¡å‹å†³ç­–ä¹‹é—´çš„å·®è·ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯ï¼ŒSATAlignè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨PKU-SafeRLHFæ•°æ®é›†ä¸Šï¼ŒSATAlignåœ¨æœ€å¤§åŒ–æœ‰ç”¨æ€§çš„åŒæ—¶ç¡®ä¿æ— å®³æ€§è¾¾åˆ°é˜ˆå€¼ï¼Œç›¸è¾ƒäºå½“å‰çš„å¤šç›®æ ‡è§£ç ç­–ç•¥ï¼Œæå‡äº†22.3%çš„GPT-4å‹å¥½æ€§å¥–åŠ±win-tieç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»å¯¹é½å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºäººç±»åé¦ˆå…·æœ‰å¤šå…ƒæ€§è´¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥äººç±»å®é™…å†³ç­–è¿‡ç¨‹ï¼Œè€Œäººç±»å†³ç­–éµå¾ªæ»¡æ„ç­–ç•¥ã€‚</li>
<li>SATAlignæ¡†æ¶æ—¨åœ¨åœ¨æ¨ç†é˜¶æ®µæœ€å¤§åŒ–ä¸»è¦ç›®æ ‡ï¼ŒåŒæ—¶æ»¡è¶³æ¬¡è¦ç›®æ ‡çš„é˜ˆå€¼çº¦æŸã€‚</li>
<li>SATAlignç»“åˆäº†ç†è®ºåˆ†æå’Œå®è¯ç ”ç©¶ï¼Œå±•ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>SATAligné€šè¿‡ä¼˜åŒ–æœ‰ç”¨æ€§å¹¶ç¡®ä¿æ— å®³æ€§è¾¾åˆ°é˜ˆå€¼ï¼Œæå‡äº†è¯­è¨€æ¨¡å‹çš„å¯¹äººç±»å‹å¥½çš„ç¨‹åº¦ã€‚</li>
<li>SATAlignåœ¨PKU-SafeRLHFæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå½“å‰çš„å¤šç›®æ ‡è§£ç ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53b47d6e56e5cd10f2223b6105ba2d66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cf47f8fd25cacba7ae40684f5f02465.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MuLoCo-Muon-is-a-practical-inner-optimizer-for-DiLoCo"><a href="#MuLoCo-Muon-is-a-practical-inner-optimizer-for-DiLoCo" class="headerlink" title="MuLoCo: Muon is a practical inner optimizer for DiLoCo"></a>MuLoCo: Muon is a practical inner optimizer for DiLoCo</h2><p><strong>Authors:Benjamin ThÃ©rien, Xiaolong Huang, Irina Rish, Eugene Belilovsky</strong></p>
<p>DiLoCo is a powerful framework for training large language models (LLMs) under networking constraints with advantages for increasing parallelism and accelerator utilization in data center settings. Despite significantly reducing communication frequency, however, DiLoCoâ€™s communication steps still involve all-reducing a complete copy of the modelâ€™s parameters. While existing works have explored ways to reduce communication in DiLoCo, the role of error feedback accumulators and the effect of the inner-optimizer on compressibility remain under-explored. In this work, we investigate the effectiveness of standard compression methods including Top-k sparsification and quantization for reducing the communication overhead of DiLoCo when paired with two local optimizers (AdamW and Muon). Our experiments pre-training decoder-only transformer language models (LMs) reveal that leveraging Muon as the inner optimizer for DiLoCo along with an error-feedback accumulator allows to aggressively compress the communicated delta to 2-bits with next to no performance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo) significantly outperforms DiLoCo while communicating 8X less and having identical memory complexity. </p>
<blockquote>
<p>DiLoCoæ˜¯åœ¨ç½‘ç»œçº¦æŸä¸‹è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§æ¡†æ¶ï¼Œå…¶åœ¨æ•°æ®ä¸­å¿ƒç¯å¢ƒä¸­å¢åŠ äº†å¹¶è¡Œæ€§å’ŒåŠ é€Ÿå™¨åˆ©ç”¨ç‡çš„ä¼˜ç‚¹ã€‚å°½ç®¡DiLoCoæ˜¾è‘—å‡å°‘äº†é€šä¿¡é¢‘ç‡ï¼Œä½†å…¶é€šä¿¡æ­¥éª¤ä»ç„¶æ¶‰åŠæ‰€æœ‰æ¨¡å‹å‚æ•°çš„å®Œæ•´å¤åˆ¶ã€‚è™½ç„¶ç°æœ‰å·¥ä½œå·²ç»æ¢ç´¢äº†å‡å°‘DiLoCoé€šä¿¡çš„æ–¹æ³•ï¼Œä½†è¯¯å·®åé¦ˆç´¯åŠ å™¨çš„ä½œç”¨ä»¥åŠå†…éƒ¨ä¼˜åŒ–å™¨å¯¹å‹ç¼©æ•ˆæœçš„å½±å“ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†æ ‡å‡†å‹ç¼©æ–¹æ³•ï¼ˆåŒ…æ‹¬Top-kç¨€ç–åŒ–å’Œé‡åŒ–ï¼‰åœ¨ç»“åˆä¸¤ç§æœ¬åœ°ä¼˜åŒ–å™¨ï¼ˆAdamWå’ŒMuonï¼‰æ—¶ï¼Œå‡å°‘DiLoCoé€šä¿¡å¼€é”€çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„å®éªŒé€šè¿‡å¯¹ä»…è§£ç å™¨å˜å‹å™¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰è¿›è¡Œé¢„è®­ç»ƒå‘ç°ï¼Œåˆ©ç”¨Muonä½œä¸ºDiLoCoçš„å†…éƒ¨ä¼˜åŒ–å™¨å¹¶ç»“åˆè¯¯å·®åé¦ˆç´¯åŠ å™¨ï¼Œå¯ä»¥ç§¯æå‹ç¼©é€šä¿¡çš„å¢é‡è‡³2ä½ï¼Œå‡ ä¹ä¸ä¼šå¯¹æ€§èƒ½äº§ç”Ÿé™çº§ã€‚å…³é”®çš„æ˜¯ï¼ŒMuLoCoï¼ˆMuonå†…éƒ¨ä¼˜åŒ–å™¨çš„DiLoCoï¼‰åœ¨é€šä¿¡é‡å‡å°‘8å€ã€å†…å­˜å¤æ‚åº¦ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—ä¼˜äºDiLoCoã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23725v1">PDF</a> </p>
<p><strong>Summary</strong><br>DiLoCoæ¡†æ¶åœ¨æ•°æ®ä¸­å¿ƒç¯å¢ƒä¸‹è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰ä¼˜åŠ¿ï¼Œèƒ½æé«˜å¹¶è¡Œæ€§å’ŒåŠ é€Ÿå™¨åˆ©ç”¨ç‡ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ç»“åˆä¸¤ç§æœ¬åœ°ä¼˜åŒ–å™¨ï¼ˆAdamWå’ŒMuonï¼‰æ—¶ï¼Œä½¿ç”¨Top-kç¨€ç–åŒ–å’Œé‡åŒ–ç­‰æ ‡å‡†å‹ç¼©æ–¹æ³•å‡å°‘DiLoCoé€šä¿¡å¼€é”€çš„æœ‰æ•ˆæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨Muonä½œä¸ºDiLoCoçš„å†…éƒ¨ä¼˜åŒ–å™¨å¹¶ç»“åˆè¯¯å·®åé¦ˆç´¯åŠ å™¨ï¼Œå¯ä»¥å¤§å¹…åº¦å‹ç¼©é€šä¿¡é‡ï¼ŒåŒæ—¶å‡ ä¹ä¸å½±å“æ€§èƒ½ã€‚MuLoCoï¼ˆä½¿ç”¨Muonå†…éƒ¨ä¼˜åŒ–å™¨çš„DiLoCoï¼‰åœ¨é€šä¿¡é‡å‡å°‘8å€ã€å†…å­˜å¤æ‚åº¦ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—ä¼˜äºDiLoCoã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiLoCoæ¡†æ¶é€‚ç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨æ•°æ®ä¸­å¿ƒç¯å¢ƒä¸­å¯æé«˜å¹¶è¡Œæ€§å’ŒåŠ é€Ÿå™¨åˆ©ç”¨ç‡ã€‚</li>
<li>æœ¬ç ”ç©¶æ¢è®¨äº†æ ‡å‡†å‹ç¼©æ–¹æ³•ï¼ˆå¦‚Top-kç¨€ç–åŒ–å’Œé‡åŒ–ï¼‰åœ¨å‡å°‘DiLoCoé€šä¿¡å¼€é”€æ–¹é¢çš„æ•ˆæœã€‚</li>
<li>ç»“åˆMuonä½œä¸ºDiLoCoçš„å†…éƒ¨ä¼˜åŒ–å™¨ï¼Œå¯å¤§å¹…åº¦å‹ç¼©é€šä¿¡é‡ã€‚</li>
<li>è¯¯å·®åé¦ˆç´¯åŠ å™¨çš„ä½¿ç”¨åœ¨å‹ç¼©é€šä¿¡ä¸­èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚</li>
<li>MuLoCoæ˜¾è‘—ä¼˜äºDiLoCoï¼Œåœ¨å‡å°‘é€šä¿¡å¼€é”€çš„åŒæ—¶ä¿æŒç›¸åŒçš„å†…å­˜å¤æ‚åº¦ã€‚<br>6.MuLoCoèƒ½å¤Ÿåœ¨å¤§å¹…åº¦å‹ç¼©é€šä¿¡çš„æƒ…å†µä¸‹ï¼Œå‡ ä¹ä¸å½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23725">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3d25d6044671917bd2f783dd5d53a9e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ba40e53638c8267a41165df06c79a76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddfa9153ee47a5b95b6fdd8925138689.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cb210323cc1c35dd3cabad425aa0778.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ML-Agent-Reinforcing-LLM-Agents-for-Autonomous-Machine-Learning-Engineering"><a href="#ML-Agent-Reinforcing-LLM-Agents-for-Autonomous-Machine-Learning-Engineering" class="headerlink" title="ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning   Engineering"></a>ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning   Engineering</h2><p><strong>Authors:Zexi Liu, Jingyi Chai, Xinyu Zhu, Shuo Tang, Rui Ye, Bo Zhang, Lei Bai, Siheng Chen</strong></p>
<p>The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched fine-tuning, which enables LLM agents to generate diverse actions for enhanced RL exploration; (2) step-wise RL, which enables training on a single action step, accelerating experience collection and improving training efficiency; (3) an agentic ML-specific reward module, which unifies varied ML feedback signals into consistent rewards for RL optimization. Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our 7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it achieves continuous performance improvements and demonstrates exceptional cross-task generalization capabilities. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººçš„å‡ºç°ï¼Œæå¤§åœ°æ¨åŠ¨äº†è‡ªä¸»æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å·¥ç¨‹çš„å‘å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºæ‰‹åŠ¨æç¤ºå·¥ç¨‹ï¼Œæ— æ³•æ ¹æ®ä¸°å¯Œçš„å®éªŒç»éªŒè¿›è¡Œé€‚åº”å’Œä¼˜åŒ–ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬é¦–æ¬¡æ¢ç´¢äº†åŸºäºå­¦ä¹ çš„äººå·¥æ™ºèƒ½MLèŒƒå¼ï¼Œè¯¥èŒƒå¼ä¸­çš„LLMä»£ç†é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨MLä»»åŠ¡ä¸Šè¿›è¡Œäº¤äº’å¼å®éªŒè¿›è¡Œå­¦ä¹ ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„äººå·¥æ™ºèƒ½MLè®­ç»ƒæ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰ä¸°å¯Œæ¢ç´¢çš„å¾®è°ƒï¼Œä½¿LLMä»£ç†èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–çš„è¡ŒåŠ¨ï¼Œä»¥å¢å¼ºRLçš„æ¢ç´¢ï¼›ï¼ˆ2ï¼‰åˆ†æ­¥RLï¼Œä½¿è®­ç»ƒå¯ä»¥åœ¨å•ä¸ªè¡ŒåŠ¨æ­¥éª¤ä¸Šè¿›è¡Œï¼ŒåŠ å¿«ç»éªŒæ”¶é›†ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ï¼›ï¼ˆ3ï¼‰é’ˆå¯¹äººå·¥æ™ºèƒ½MLçš„å¥–åŠ±æ¨¡å—ï¼Œå®ƒå°†å„ç§MLåé¦ˆä¿¡å·ç»Ÿä¸€ä¸ºä¸€è‡´çš„å¥–åŠ±ï¼Œç”¨äºRLä¼˜åŒ–ã€‚åˆ©ç”¨è¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ç”±7Bå¤§å°çš„Qwen-2.5 LLMé©±åŠ¨çš„ML-Agentè¿›è¡Œè‡ªä¸»MLã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡ä»…åœ¨9ä¸ªMLä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬7Bå¤§å°çš„ML-Agentè¡¨ç°å‡ºä¼˜äº671Bå¤§å°çš„DeepSeek-R1ä»£ç†çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒå®ç°äº†æŒç»­çš„æ€§èƒ½æ”¹è¿›ï¼Œå¹¶è¡¨ç°å‡ºå“è¶Šçš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23723v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°æå¤§åœ°æ¨åŠ¨äº†è‡ªä¸»æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å·¥ç¨‹çš„å‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•è¿‡äºä¾èµ–æ‰‹åŠ¨æç¤ºå·¥ç¨‹ï¼Œæ— æ³•æ ¹æ®ä¸°å¯Œçš„å®éªŒç»éªŒè¿›è¡Œé€‚åº”å’Œä¼˜åŒ–ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡é¦–æ¬¡æ¢ç´¢äº†åŸºäºå­¦ä¹ å‹çš„æ™ºèƒ½ä½“æœºå™¨å­¦ä¹ èŒƒå¼ï¼ŒLLMæ™ºèƒ½ä½“é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸Šè¿›è¡Œäº¤äº’å¼å®éªŒå­¦ä¹ ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°å‹æ™ºèƒ½ä½“æœºå™¨å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰ä¸°å¯Œæ¢ç´¢çš„å¾®è°ƒï¼Œä½¿LLMæ™ºèƒ½ä½“ç”Ÿæˆå¤šæ ·åŒ–è¡ŒåŠ¨ä»¥å¢å¼ºRLæ¢ç´¢ï¼›ï¼ˆ2ï¼‰åˆ†æ­¥å¼ºåŒ–å­¦ä¹ ï¼Œå®ç°å•ä¸€è¡ŒåŠ¨æ­¥éª¤çš„è®­ç»ƒï¼Œæé«˜ç»éªŒæ”¶é›†å’Œè®­ç»ƒæ•ˆç‡ï¼›ï¼ˆ3ï¼‰æ™ºèƒ½ä½“æœºå™¨å­¦ä¹ ç‰¹å®šå¥–åŠ±æ¨¡å—ï¼Œå°†å„ç§æœºå™¨å­¦ä¹ åé¦ˆä¿¡å·ç»Ÿä¸€ä¸ºä¸€è‡´çš„å¥–åŠ±ç”¨äºRLä¼˜åŒ–ã€‚åˆ©ç”¨è¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ML-Agentï¼Œç”±7Bè§„æ¨¡çš„Qwen-2.5 LLMé©±åŠ¨è¿›è¡Œè‡ªä¸»æœºå™¨å­¦ä¹ ã€‚å°½ç®¡ä»…åœ¨9ä¸ªæœºå™¨å­¦ä¹ ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬çš„7Bè§„æ¨¡ML-Agentè¡¨ç°å‡ºè¶…è¶Š671Bè§„æ¨¡DeepSeek-R1ä»£ç†çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒå®ç°äº†æŒç»­çš„æ€§èƒ½æ”¹è¿›ï¼Œå¹¶æ˜¾ç¤ºå‡ºå“è¶Šçš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªä¸»æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å·¥ç¨‹ä¸­èµ·åˆ°é‡è¦ä½œç”¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–æ‰‹åŠ¨æç¤ºå·¥ç¨‹ï¼Œç¼ºä¹å¯¹ä¸åŒç»éªŒçš„é€‚åº”å’Œä¼˜åŒ–ã€‚</li>
<li>é¦–æ¬¡æ¢ç´¢åŸºäºå­¦ä¹ å‹çš„æ™ºèƒ½ä½“æœºå™¨å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡äº¤äº’å¼å®éªŒå­¦ä¹ ã€‚</li>
<li>æå‡ºæ–°å‹æ™ºèƒ½ä½“æœºå™¨å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…å«ä¸°å¯Œæ¢ç´¢çš„å¾®è°ƒã€åˆ†æ­¥å¼ºåŒ–å­¦ä¹ å’Œç‰¹å®šå¥–åŠ±æ¨¡å—ä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>åˆ©ç”¨è¯¥æ¡†æ¶è®­ç»ƒçš„ML-Agentè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šå…¶ä»–ä»£ç†ã€‚</li>
<li>ML-Agentå…·æœ‰æŒç»­çš„æ€§èƒ½æ”¹è¿›å’Œè·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23723">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a96a244009b2e51e23cfa9a05933420b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-883b9fe7aa3daabadc8d44a19f179a1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa28eed8778e1c2f55d0a63d16889441.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Label-Guided-In-Context-Learning-for-Named-Entity-Recognition"><a href="#Label-Guided-In-Context-Learning-for-Named-Entity-Recognition" class="headerlink" title="Label-Guided In-Context Learning for Named Entity Recognition"></a>Label-Guided In-Context Learning for Named Entity Recognition</h2><p><strong>Authors:Fan Bai, Hamid Hassanzadeh, Ardavan Saeedi, Mark Dredze</strong></p>
<p>In-context learning (ICL) enables large language models (LLMs) to perform new tasks using only a few demonstrations. In Named Entity Recognition (NER), demonstrations are typically selected based on semantic similarity to the test instance, ignoring training labels and resulting in suboptimal performance. We introduce DEER, a new method that leverages training labels through token-level statistics to improve ICL performance. DEER first enhances example selection with a label-guided, token-based retriever that prioritizes tokens most informative for entity recognition. It then prompts the LLM to revisit error-prone tokens, which are also identified using label statistics, and make targeted corrections. Evaluated on five NER datasets using four different LLMs, DEER consistently outperforms existing ICL methods and approaches the performance of supervised fine-tuning. Further analysis shows its effectiveness on both seen and unseen entities and its robustness in low-resource settings. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»…é€šè¿‡å°‘æ•°å‡ ä¸ªæ¼”ç¤ºå°±èƒ½æ‰§è¡Œæ–°ä»»åŠ¡ã€‚åœ¨å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä¸­ï¼Œæ¼”ç¤ºé€šå¸¸åŸºäºä¸æµ‹è¯•å®ä¾‹çš„è¯­ä¹‰ç›¸ä¼¼æ€§è¿›è¡Œé€‰æ‹©ï¼Œå¿½ç•¥è®­ç»ƒæ ‡ç­¾ï¼Œä»è€Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚æˆ‘ä»¬å¼•å…¥äº†DEERï¼Œä¸€ç§æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä»¤ç‰Œçº§ç»Ÿè®¡åˆ©ç”¨è®­ç»ƒæ ‡ç­¾æ¥æé«˜ICLçš„æ€§èƒ½ã€‚DEERé¦–å…ˆé€šè¿‡æ ‡ç­¾å¼•å¯¼ã€åŸºäºä»¤ç‰Œçš„æ£€ç´¢å™¨å¢å¼ºç¤ºä¾‹é€‰æ‹©ï¼Œè¯¥æ£€ç´¢å™¨ä¼˜å…ˆå¤„ç†å¯¹å®ä½“è¯†åˆ«æœ€æœ‰ç”¨çš„ä»¤ç‰Œã€‚ç„¶åï¼Œå®ƒæç¤ºLLMé‡æ–°è®¿é—®æ˜“å‡ºé”™ä»¤ç‰Œï¼Œè¿™äº›ä»¤ç‰Œä¹Ÿæ˜¯é€šè¿‡æ ‡ç­¾ç»Ÿè®¡ç¡®å®šçš„ï¼Œå¹¶è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„æ›´æ­£ã€‚åœ¨äº”ä¸ªNERæ•°æ®é›†ä¸Šä½¿ç”¨å››ç§ä¸åŒçš„LLMè¿›è¡Œè¯„ä¼°ï¼ŒDEERå§‹ç»ˆä¼˜äºç°æœ‰çš„ICLæ–¹æ³•ï¼Œå¹¶æ¥è¿‘ç›‘ç£å¾®è°ƒçš„æ€§èƒ½ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œå®ƒåœ¨å·²çŸ¥å’ŒæœªçŸ¥å®ä½“ä¸Šéƒ½å¾ˆæœ‰æ•ˆï¼Œå¹¶ä¸”åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­å…·æœ‰ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23722v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»…é€šè¿‡å°‘æ•°ç¤ºèŒƒå³å¯æ‰§è¡Œæ–°ä»»åŠ¡ã€‚åœ¨å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä¸­ï¼Œç¤ºèŒƒé€šå¸¸æ ¹æ®ä¸æµ‹è¯•å®ä¾‹çš„è¯­ä¹‰ç›¸ä¼¼æ€§æ¥é€‰æ‹©ï¼Œå¿½ç•¥äº†è®­ç»ƒæ ‡ç­¾ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚æˆ‘ä»¬æ¨å‡ºDEERæ–°æ–¹æ³•ï¼Œåˆ©ç”¨è®­ç»ƒæ ‡ç­¾é€šè¿‡æ ‡è®°çº§åˆ«çš„ç»Ÿè®¡æ•°æ®æ¥æé«˜ICLæ€§èƒ½ã€‚DEERé¦–å…ˆé€šè¿‡æ ‡ç­¾å¼•å¯¼çš„åŸºäºæ ‡è®°çš„æ£€ç´¢å™¨å¢å¼ºç¤ºä¾‹é€‰æ‹©ï¼Œä¼˜å…ˆè¯†åˆ«å¯¹å®ä½“è¯†åˆ«æœ€æœ‰ç”¨çš„æ ‡è®°ã€‚ç„¶åï¼Œå®ƒæç¤ºLLMé‡æ–°æ£€æŸ¥é”™è¯¯å€¾å‘çš„æ ‡è®°ï¼ˆä¹Ÿé€šè¿‡æ ‡ç­¾ç»Ÿè®¡è¿›è¡Œè¯†åˆ«ï¼‰ï¼Œå¹¶è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„æ›´æ­£ã€‚åœ¨äº”ä¸ªNERæ•°æ®é›†ä¸Šä½¿ç”¨å››ç§ä¸åŒçš„LLMè¿›è¡Œè¯„ä¼°ï¼ŒDEERå§‹ç»ˆè¡¨ç°ä¼˜äºç°æœ‰çš„ICLæ–¹æ³•ï¼Œå¹¶æ¥è¿‘ç›‘ç£å¾®è°ƒæ€§èƒ½ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œå®ƒåœ¨å·²çŸ¥å’ŒæœªçŸ¥å®ä½“ä¸Šå‡æœ‰æ•ˆï¼Œä¸”åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­è¡¨ç°ç¨³å¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICLå…è®¸LLMä»…é€šè¿‡å°‘é‡ç¤ºèŒƒæ‰§è¡Œæ–°ä»»åŠ¡ã€‚</li>
<li>NERä¸­ç¤ºèŒƒé€‰æ‹©é€šå¸¸åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œä½†å¿½ç•¥äº†è®­ç»ƒæ ‡ç­¾ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>DEERé€šè¿‡åˆ©ç”¨è®­ç»ƒæ ‡ç­¾å’Œæ ‡è®°çº§åˆ«çš„ç»Ÿè®¡æ•°æ®æ¥æé«˜ICLæ€§èƒ½ã€‚</li>
<li>DEERé¦–å…ˆé€šè¿‡æ ‡ç­¾å¼•å¯¼çš„åŸºäºæ ‡è®°çš„æ£€ç´¢å™¨ä¼˜åŒ–ç¤ºä¾‹é€‰æ‹©ã€‚</li>
<li>DEERæç¤ºLLMé’ˆå¯¹é”™è¯¯å€¾å‘çš„æ ‡è®°è¿›è¡Œæ›´æ­£ï¼Œæé«˜è¯†åˆ«å‡†ç¡®æ€§ã€‚</li>
<li>DEERåœ¨å¤šä¸ªæ•°æ®é›†å’Œå¤šç§LLMä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰ICLæ–¹æ³•ï¼Œå¹¶æ¥è¿‘ç›‘ç£å¾®è°ƒæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23722">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94b222e25db098139d506d458d810a37.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8093ed3b3a527a5e7c6711537b9c8a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db5782a42bed57b6fc325721c4adae03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07d06e24c31cd5a1d2a2175c4579f085.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="TiRex-Zero-Shot-Forecasting-Across-Long-and-Short-Horizons-with-Enhanced-In-Context-Learning"><a href="#TiRex-Zero-Shot-Forecasting-Across-Long-and-Short-Horizons-with-Enhanced-In-Context-Learning" class="headerlink" title="TiRex: Zero-Shot Forecasting Across Long and Short Horizons with   Enhanced In-Context Learning"></a>TiRex: Zero-Shot Forecasting Across Long and Short Horizons with   Enhanced In-Context Learning</h2><p><strong>Authors:Andreas Auer, Patrick Podest, Daniel Klotz, Sebastian BÃ¶ck, GÃ¼nter Klambauer, Sepp Hochreiter</strong></p>
<p>In-context learning, the ability of large language models to perform tasks using only examples provided in the prompt, has recently been adapted for time series forecasting. This paradigm enables zero-shot prediction, where past values serve as context for forecasting future values, making powerful forecasting tools accessible to non-experts and increasing the performance when training data are scarce. Most existing zero-shot forecasting approaches rely on transformer architectures, which, despite their success in language, often fall short of expectations in time series forecasting, where recurrent models like LSTMs frequently have the edge. Conversely, while LSTMs are well-suited for time series modeling due to their state-tracking capabilities, they lack strong in-context learning abilities. We introduce TiRex that closes this gap by leveraging xLSTM, an enhanced LSTM with competitive in-context learning skills. Unlike transformers, state-space models, or parallelizable RNNs such as RWKV, TiRex retains state-tracking, a critical property for long-horizon forecasting. To further facilitate its state-tracking ability, we propose a training-time masking strategy called CPM. TiRex sets a new state of the art in zero-shot time series forecasting on the HuggingFace benchmarks GiftEval and Chronos-ZS, outperforming significantly larger models including TabPFN-TS (Prior Labs), Chronos Bolt (Amazon), TimesFM (Google), and Moirai (Salesforce) across both short- and long-term forecasts. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ â€”â€”å¤§å‹è¯­è¨€æ¨¡å‹ä»…ä½¿ç”¨æç¤ºä¸­æä¾›çš„ç¤ºä¾‹æ¥æ‰§è¡Œä»»åŠ¡çš„èƒ½åŠ›â€”â€”æœ€è¿‘å·²è¢«åº”ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹ã€‚è¿™ç§èŒƒå¼å®ç°äº†é›¶æ ·æœ¬é¢„æµ‹ï¼Œå…¶ä¸­è¿‡å»å€¼ä½œä¸ºé¢„æµ‹æœªæ¥å€¼çš„ä¸Šä¸‹æ–‡ï¼Œä½¿éä¸“å®¶ä¹Ÿèƒ½ä½¿ç”¨å¼ºå¤§çš„é¢„æµ‹å·¥å…·ï¼Œå¹¶åœ¨è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹æé«˜æ€§èƒ½ã€‚å¤§å¤šæ•°ç°æœ‰çš„é›¶æ ·æœ¬é¢„æµ‹æ–¹æ³•éƒ½ä¾èµ–äºå˜å‹å™¨æ¶æ„ï¼Œå°½ç®¡å®ƒä»¬åœ¨è¯­è¨€é¢†åŸŸå–å¾—äº†æˆåŠŸï¼Œä½†åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­å¾€å¾€ä¸èƒ½è¾¾åˆ°é¢„æœŸæ•ˆæœï¼Œå¾ªç¯æ¨¡å‹å¦‚LSTMç»å¸¸å…·æœ‰ä¼˜åŠ¿ã€‚ç›¸åï¼Œè™½ç„¶LSTMç”±äºå…¶çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›è€Œéå¸¸é€‚åˆæ—¶é—´åºåˆ—å»ºæ¨¡ï¼Œä½†å®ƒä»¬ç¼ºä¹å¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†TiRexï¼Œå®ƒé€šè¿‡åˆ©ç”¨å¢å¼ºçš„LSTMâ€”â€”xLSTMæ¥å¼¥è¡¥è¿™ä¸€å·®è·ï¼ŒxLSTMå…·æœ‰ç«äº‰åŠ›çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚ä¸å˜å‹å™¨ã€çŠ¶æ€ç©ºé—´æ¨¡å‹æˆ–å¯å¹¶è¡ŒåŒ–çš„RNNï¼ˆå¦‚RWKVï¼‰ä¸åŒï¼ŒTiRexä¿ç•™äº†çŠ¶æ€è·Ÿè¸ªåŠŸèƒ½ï¼Œè¿™æ˜¯é•¿æœŸé¢„æµ‹çš„ä¸€ä¸ªå…³é”®å±æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¿ƒè¿›å…¶çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§°ä¸ºCPMçš„è®­ç»ƒæ—¶æ©ç ç­–ç•¥ã€‚TiRexåœ¨HuggingFaceçš„GiftEvalå’ŒChronos-ZSåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é›¶æ ·æœ¬æ—¶é—´åºåˆ—é¢„æµ‹çš„æœ€æ–°æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºåŒ…æ‹¬TabPFN-TSï¼ˆPrior Labsï¼‰ã€Chronos Boltï¼ˆäºšé©¬é€Šï¼‰ã€TimesFMï¼ˆè°·æ­Œï¼‰å’ŒMoiraiï¼ˆSalesforceï¼‰ç­‰å¤§å‹æ¨¡å‹ï¼Œåœ¨çŸ­æœŸå’Œé•¿æœŸé¢„æµ‹æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23719v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›å·²åº”ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸï¼Œå®ç°äº†é›¶æ ·æœ¬é¢„æµ‹ã€‚ç°æœ‰æ–¹æ³•å¤šä¾èµ–Transformeræ¶æ„ï¼Œä½†åœ¨æ—¶é—´åºåˆ—é¢„æµ‹æ–¹é¢è¡¨ç°ä¸å°½å¦‚äººæ„ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ç»“åˆLSTMä¼˜åŠ¿çš„TiRexæ¨¡å‹ï¼Œå…·æœ‰å¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›åŠçŠ¶æ€è·Ÿè¸ªèƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥çŠ¶æ€ç©ºé—´æ¨¡å‹è®­ç»ƒæ—¶çš„æ©ç ç­–ç•¥ï¼ŒTiRexåœ¨é›¶æ ·æœ¬æ—¶é—´åºåˆ—é¢„æµ‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨HuggingFaceçš„GiftEvalå’ŒChronos-ZSåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›å·²åº”ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹ã€‚</li>
<li>ç°æœ‰é›¶æ ·æœ¬é¢„æµ‹æ–¹æ³•å¤šä¾èµ–Transformeræ¶æ„ï¼Œä½†åœ¨æ—¶é—´åºåˆ—é¢„æµ‹æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>LSTMæ¨¡å‹åœ¨æ—¶é—´åºåˆ—å»ºæ¨¡ä¸­å…·æœ‰çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›ä¼˜åŠ¿ï¼Œä½†ç¼ºä¹å¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>TiRexç»“åˆäº†LSTMå’ŒTransformerçš„ä¼˜åŠ¿ï¼Œå®ç°äº†å¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ å’ŒçŠ¶æ€è·Ÿè¸ªèƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†æ–°çš„è®­ç»ƒç­–ç•¥â€”â€”æ©ç ç­–ç•¥ï¼ˆCPMï¼‰ä»¥ä¿ƒè¿›çŠ¶æ€è·Ÿè¸ªã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23719">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c099b2844c004f3f03864197dcb82bfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-143734145391e87ade593f784599bd2a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ba4851e0b6b56e6e7c1642258247756e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99bfa8ed4b25002e08f9f54b1c6dbec8.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Donâ€™t-Take-the-Premise-for-Granted-Evaluating-the-Premise-Critique-Ability-of-Large-Language-Models"><a href="#Donâ€™t-Take-the-Premise-for-Granted-Evaluating-the-Premise-Critique-Ability-of-Large-Language-Models" class="headerlink" title="Donâ€™t Take the Premise for Granted: Evaluating the Premise Critique   Ability of Large Language Models"></a>Donâ€™t Take the Premise for Granted: Evaluating the Premise Critique   Ability of Large Language Models</h2><p><strong>Authors:Jinzhe Li, Gengxu Li, Yi Chang, Yuan Wu</strong></p>
<p>Large language models (LLMs) have witnessed rapid advancements, demonstrating remarkable capabilities. However, a notable vulnerability persists: LLMs often uncritically accept flawed or contradictory premises, leading to inefficient reasoning and unreliable outputs. This emphasizes the significance of possessing the \textbf{Premise Critique Ability} for LLMs, defined as the capacity to proactively identify and articulate errors in input premises. Most existing studies assess LLMsâ€™ reasoning ability in ideal settings, largely ignoring their vulnerabilities when faced with flawed premises. Thus, we introduce the \textbf{Premise Critique Bench (PCBench)}, designed by incorporating four error types across three difficulty levels, paired with multi-faceted evaluation metrics. We conducted systematic evaluations of 15 representative LLMs. Our findings reveal: (1) Most models rely heavily on explicit prompts to detect errors, with limited autonomous critique; (2) Premise critique ability depends on question difficulty and error type, with direct contradictions being easier to detect than complex or procedural errors; (3) Reasoning ability does not consistently correlate with the premise critique ability; (4) Flawed premises trigger overthinking in reasoning models, markedly lengthening responses due to repeated attempts at resolving conflicts. These insights underscore the urgent need to enhance LLMsâ€™ proactive evaluation of input validity, positioning premise critique as a foundational capability for developing reliable, human-centric systems. The code is available at <a target="_blank" rel="noopener" href="https://github.com/MLGroupJLU/Premise_Critique">https://github.com/MLGroupJLU/Premise_Critique</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œå¹¶å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸€ä¸ªæ˜æ˜¾çš„æ¼æ´æŒç»­å­˜åœ¨ï¼šLLMç»å¸¸æ— æ‰¹åˆ¤åœ°æ¥å—æœ‰ç¼ºé™·æˆ–çŸ›ç›¾çš„å‡è®¾ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹å’Œè¾“å‡ºä¸å¯é ã€‚è¿™å¼ºè°ƒäº†å¤§å‹è¯­è¨€æ¨¡å‹æ‹¥æœ‰â€œå‰ææ‰¹åˆ¤èƒ½åŠ›â€çš„é‡è¦æ€§ï¼Œè¿™è¢«å®šä¹‰ä¸ºèƒ½å¤Ÿä¸»åŠ¨è¯†åˆ«å’Œè¡¨è¾¾è¾“å…¥å‡è®¾ä¸­çš„é”™è¯¯ã€‚å¤§å¤šæ•°ç°æœ‰ç ”ç©¶åœ¨ç†æƒ³ç¯å¢ƒä¸­è¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šå¿½ç•¥äº†å®ƒä»¬åœ¨é¢å¯¹æœ‰ç¼ºé™·çš„å‰ææ—¶çš„è„†å¼±æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œå‰ææ‰¹åˆ¤åŸºå‡†æµ‹è¯•ï¼ˆPCBenchï¼‰â€ï¼Œå®ƒåŒ…å«ä¸‰ç§éš¾åº¦æ°´å¹³çš„å››ç§é”™è¯¯ç±»å‹ï¼Œå¹¶é…æœ‰å¤šæ–¹é¢çš„è¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬å¯¹15ä¸ªä»£è¡¨æ€§çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼šï¼ˆ1ï¼‰å¤§å¤šæ•°æ¨¡å‹ä¸¥é‡ä¾èµ–äºæ˜ç¡®çš„æç¤ºæ¥æ£€æµ‹é”™è¯¯ï¼Œè‡ªä¸»æ‰¹åˆ¤èƒ½åŠ›æœ‰é™ï¼›ï¼ˆ2ï¼‰å‰ææ‰¹åˆ¤èƒ½åŠ›å–å†³äºé—®é¢˜çš„éš¾åº¦å’Œé”™è¯¯çš„ç±»å‹ï¼Œç›´æ¥çŸ›ç›¾æ¯”å¤æ‚æˆ–ç¨‹åºæ€§é”™è¯¯æ›´å®¹æ˜“æ£€æµ‹ï¼›ï¼ˆ3ï¼‰æ¨ç†èƒ½åŠ›ä¸å‰ææ‰¹åˆ¤èƒ½åŠ›å¹¶ä¸æ€»æ˜¯ä¸€è‡´ï¼›ï¼ˆ4ï¼‰æœ‰ç¼ºé™·çš„å‰æä¼šå¼•å‘æ¨ç†æ¨¡å‹çš„è¿‡åº¦æ€è€ƒï¼Œç”±äºåå¤å°è¯•è§£å†³å†²çªï¼Œå“åº”æ˜æ˜¾å˜é•¿ã€‚è¿™äº›è§è§£çªæ˜¾äº†æé«˜LLMå¯¹è¾“å…¥æœ‰æ•ˆæ€§çš„ä¸»åŠ¨è¯„ä¼°èƒ½åŠ›çš„ç´§è¿«éœ€æ±‚ï¼Œå°†å‰ææ‰¹åˆ¤å®šä½ä¸ºå¼€å‘å¯é ã€ä»¥äººä¸ºæœ¬ç³»ç»Ÿçš„åŸºæœ¬èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MLGroupJLU/Premise_Critique%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MLGroupJLU/Premise_Critiqueæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23715v1">PDF</a> 31 pages,13 figures,15 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨æ˜¾è‘—å¼±ç‚¹ï¼šå®¹æ˜“æ¥å—é”™è¯¯æˆ–çŸ›ç›¾çš„é¢„è®¾ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹å’Œè¾“å‡ºä¸å¯é ã€‚å› æ­¤ï¼Œæ‹¥æœ‰å‰ææ‰¹åˆ¤èƒ½åŠ›ï¼ˆå¯¹è¾“å…¥é¢„è®¾è¿›è¡Œä¸»åŠ¨è¯†åˆ«å’Œè¡¨è¾¾é”™è¯¯çš„èƒ½åŠ›ï¼‰å¯¹LLMè‡³å…³é‡è¦ã€‚ç°æœ‰çš„ç ”ç©¶ä¸»è¦åœ¨ç†æƒ³ç¯å¢ƒä¸­è¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¿½è§†äº†å®ƒä»¬åœ¨é¢å¯¹é”™è¯¯å‰ææ—¶çš„è„†å¼±æ€§ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†å‰ææ‰¹åˆ¤åŸºå‡†æµ‹è¯•ï¼ˆPCBenchï¼‰ï¼Œæ¶µç›–å››ç§é”™è¯¯ç±»å‹å’Œä¸‰ä¸ªéš¾åº¦çº§åˆ«ï¼Œå¹¶é…å¤‡å¤šé¢è¯„ä¼°æŒ‡æ ‡ã€‚è¯„ä¼°å‘ç°ï¼šå¤§å¤šæ•°æ¨¡å‹ä¾èµ–æ˜æ˜¾çš„æç¤ºæ¥æ£€æµ‹é”™è¯¯ï¼Œè‡ªä¸»æ‰¹åˆ¤èƒ½åŠ›æœ‰é™ï¼›å‰ææ‰¹åˆ¤èƒ½åŠ›å–å†³äºé—®é¢˜çš„éš¾åº¦å’Œé”™è¯¯ç±»å‹ï¼›æ¨ç†èƒ½åŠ›ä¸å‰ææ‰¹åˆ¤èƒ½åŠ›å¹¶ä¸ä¸€è‡´ï¼›é”™è¯¯çš„å‰æä¼šå¯¼è‡´æ¨ç†æ¨¡å‹è¿‡åº¦æ€è€ƒï¼Œæ˜æ˜¾å»¶é•¿å›åº”æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå­˜åœ¨æ¥å—é”™è¯¯æˆ–çŸ›ç›¾é¢„è®¾çš„é—®é¢˜ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡å’Œè¾“å‡ºå¯é æ€§ä¸‹é™ã€‚</li>
<li>å‰ææ‰¹åˆ¤èƒ½åŠ›å¯¹LLMè‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬ä¸»åŠ¨è¯†åˆ«å’Œè¡¨è¾¾è¾“å…¥é¢„è®¾ä¸­çš„é”™è¯¯ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨ç†æƒ³ç¯å¢ƒä¸‹çš„LLMæ¨ç†èƒ½åŠ›è¯„ä¼°ï¼Œå¿½è§†äº†å…¶åœ¨é¢å¯¹é”™è¯¯å‰ææ—¶çš„è„†å¼±æ€§ã€‚</li>
<li>å¼•å…¥çš„å‰ææ‰¹åˆ¤åŸºå‡†æµ‹è¯•ï¼ˆPCBenchï¼‰æ—¨åœ¨å…¨é¢è¯„ä¼°LLMåœ¨é¢å¯¹ä¸åŒéš¾åº¦å’Œé”™è¯¯ç±»å‹çš„å‰ææ—¶çš„è¡¨ç°ã€‚</li>
<li>å¤§å¤šæ•°LLMä¾èµ–æ˜æ˜¾æç¤ºæ¥æ£€æµ‹é”™è¯¯ï¼Œè‡ªä¸»æ‰¹åˆ¤èƒ½åŠ›æœ‰é™ï¼Œä¸”å‰ææ‰¹åˆ¤èƒ½åŠ›å–å†³äºé—®é¢˜çš„éš¾åº¦å’Œé”™è¯¯ç±»å‹ã€‚</li>
<li>LLMçš„æ¨ç†èƒ½åŠ›ä¸å‰ææ‰¹åˆ¤èƒ½åŠ›å¹¶ä¸ä¸€è‡´ï¼Œéœ€è¦åŠ å¼ºå¯¹å…¶è¾“å…¥æœ‰æ•ˆæ€§çš„ä¸»åŠ¨è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23715">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-085fc3de8c6544d60d046c8bdd931c9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8684c350a65310c25da8fca1950435aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b118a87af8a4d7fa8d3d901068c13b7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SocialMaze-A-Benchmark-for-Evaluating-Social-Reasoning-in-Large-Language-Models"><a href="#SocialMaze-A-Benchmark-for-Evaluating-Social-Reasoning-in-Large-Language-Models" class="headerlink" title="SocialMaze: A Benchmark for Evaluating Social Reasoning in Large   Language Models"></a>SocialMaze: A Benchmark for Evaluating Social Reasoning in Large   Language Models</h2><p><strong>Authors:Zixiang Xu, Yanbo Wang, Yue Huang, Jiayi Ye, Haomin Zhuang, Zirui Song, Lang Gao, Chenxi Wang, Zhaorun Chen, Yujun Zhou, Sixian Li, Wang Pan, Yue Zhao, Jieyu Zhao, Xiangliang Zhang, Xiuying Chen</strong></p>
<p>Large language models (LLMs) are increasingly applied to socially grounded tasks, such as online community moderation, media content analysis, and social reasoning games. Success in these contexts depends on a modelâ€™s social reasoning ability - the capacity to interpret social contexts, infer othersâ€™ mental states, and assess the truthfulness of presented information. However, there is currently no systematic evaluation framework that comprehensively assesses the social reasoning capabilities of LLMs. Existing efforts often oversimplify real-world scenarios and consist of tasks that are too basic to challenge advanced models. To address this gap, we introduce SocialMaze, a new benchmark specifically designed to evaluate social reasoning. SocialMaze systematically incorporates three core challenges: deep reasoning, dynamic interaction, and information uncertainty. It provides six diverse tasks across three key settings: social reasoning games, daily-life interactions, and digital community platforms. Both automated and human validation are used to ensure data quality. Our evaluation reveals several key insights: models vary substantially in their ability to handle dynamic interactions and integrate temporally evolving information; models with strong chain-of-thought reasoning perform better on tasks requiring deeper inference beyond surface-level cues; and model reasoning degrades significantly under uncertainty. Furthermore, we show that targeted fine-tuning on curated reasoning examples can greatly improve model performance in complex social scenarios. The dataset is publicly available at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/MBZUAI/SocialMaze">https://huggingface.co/datasets/MBZUAI/SocialMaze</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«åº”ç”¨äºç¤¾ä¼šåŸºç¡€ä»»åŠ¡ï¼Œå¦‚åœ¨çº¿ç¤¾åŒºç®¡ç†ã€åª’ä½“å†…å®¹åˆ†æå’Œç¤¾äº¤æ¨ç†æ¸¸æˆã€‚åœ¨è¿™äº›èƒŒæ™¯ä¸‹çš„æˆåŠŸå–å†³äºæ¨¡å‹çš„ç¤¾äº¤æ¨ç†èƒ½åŠ›ï¼Œå³è§£é‡Šç¤¾ä¼šèƒŒæ™¯ã€æ¨æ–­ä»–äººå¿ƒç†çŠ¶æ€ä»¥åŠè¯„ä¼°æ‰€å‘ˆç°ä¿¡æ¯çš„çœŸå®æ€§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›®å‰å°šæ²¡æœ‰ç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶å…¨é¢è¯„ä¼°LLMçš„ç¤¾ä¼šæ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„åŠªåŠ›å¾€å¾€ç®€åŒ–äº†ç°å®ä¸–ç•Œåœºæ™¯ï¼ŒåŒ…å«çš„ä»»åŠ¡è¿‡äºåŸºæœ¬ï¼Œæ— æ³•æŒ‘æˆ˜é«˜çº§æ¨¡å‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†SocialMazeï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°ç¤¾äº¤æ¨ç†èƒ½åŠ›çš„æ–°åŸºå‡†ã€‚SocialMazeç³»ç»Ÿåœ°ç»“åˆäº†ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šæ·±åº¦æ¨ç†ã€åŠ¨æ€äº¤äº’å’Œä¿¡æ¯ä¸ç¡®å®šæ€§ã€‚å®ƒæä¾›äº†å…­ä¸ªå¤šæ ·åŒ–çš„ä»»åŠ¡ï¼Œæ¶µç›–ä¸‰ä¸ªå…³é”®åœºæ™¯ï¼šç¤¾äº¤æ¨ç†æ¸¸æˆã€æ—¥å¸¸äº’åŠ¨å’Œæ•°å­—ç¤¾åŒºå¹³å°ã€‚é‡‡ç”¨è‡ªåŠ¨åŒ–å’Œäººå·¥éªŒè¯ç›¸ç»“åˆçš„æ–¹å¼ç¡®ä¿æ•°æ®è´¨é‡ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ­ç¤ºäº†å‡ ä¸ªå…³é”®è§è§£ï¼šæ¨¡å‹åœ¨å¤„ç†åŠ¨æ€äº¤äº’å’Œæ•´åˆéšæ—¶é—´æ¼”å˜çš„ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›å·®å¼‚å¾ˆå¤§ï¼›å…·æœ‰å¼ºå¤§æ€ç»´é“¾æ¨ç†çš„æ¨¡å‹åœ¨éœ€è¦è¶…è¶Šè¡¨é¢çº¿ç´¢è¿›è¡Œæ›´æ·±å…¥æ¨ç†çš„ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼›åœ¨ä¸ç¡®å®šæ€§æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹æ¨ç†èƒ½åŠ›ä¼šæ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œå¯¹ç²¾é€‰çš„æ¨ç†ç¤ºä¾‹è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒå¯ä»¥æå¤§åœ°æé«˜æ¨¡å‹åœ¨å¤æ‚ç¤¾äº¤åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/MBZUAI/SocialMaze%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://huggingface.co/datasets/MBZUAI/SocialMazeå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23713v1">PDF</a> Code available at <a target="_blank" rel="noopener" href="https://github.com/xzx34/SocialMaze">https://github.com/xzx34/SocialMaze</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¤¾ä¼šæ€§ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå¦‚åœ¨çº¿ç¤¾åŒºç®¡ç†ã€åª’ä½“å†…å®¹åˆ†æå’Œç¤¾äº¤æ¨ç†æ¸¸æˆç­‰ã€‚ç„¶è€Œï¼Œç›®å‰å°šç¼ºä¹ä¸€ä¸ªå…¨é¢è¯„ä¼°LLMç¤¾ä¼šæ¨ç†èƒ½åŠ›çš„ç³»ç»Ÿè¯„ä»·æ¡†æ¶ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†SocialMazeè¿™ä¸€æ–°åŸºå‡†æµ‹è¯•ï¼Œå…¶æ¶µç›–äº†æ·±åº¦æ¨ç†ã€åŠ¨æ€äº¤äº’å’Œä¿¡æ¯ä¸ç¡®å®šæ€§ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå¹¶æä¾›å…­ä¸ªå¤šæ ·åŒ–çš„ä»»åŠ¡ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨å¤„ç†åŠ¨æ€äº¤äº’å’Œæ•´åˆæ—¶é—´æ¼”å˜ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›å·®å¼‚æ˜¾è‘—ï¼Œå¼ºåŒ–é“¾å¼æ€ç»´æ¨ç†çš„æ¨¡å‹åœ¨éœ€è¦æ·±å…¥æ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³ï¼Œä¸”æ¨¡å‹åœ¨ä¸ç¡®å®šæ€§ä¸‹çš„æ¨ç†èƒ½åŠ›æ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼Œå¯¹ç²¾é€‰æ¨ç†ç¤ºä¾‹çš„é’ˆå¯¹æ€§å¾®è°ƒå¯å¤§å¹…æé«˜æ¨¡å‹åœ¨å¤æ‚ç¤¾äº¤åœºæ™¯ä¸­çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè¢«å¹¿æ³›åº”ç”¨äºç¤¾ä¼šæ€§ä»»åŠ¡ï¼Œå¦‚åœ¨çº¿ç¤¾åŒºç®¡ç†å’Œç¤¾äº¤æ¨ç†æ¸¸æˆã€‚</li>
<li>ç›®å‰ç¼ºä¹è¯„ä¼°LLMç¤¾ä¼šæ¨ç†èƒ½åŠ›çš„ç³»ç»Ÿè¯„ä»·æ¡†æ¶ã€‚</li>
<li>SocialMazeåŸºå‡†æµ‹è¯•æ¶µç›–æ·±åº¦æ¨ç†ã€åŠ¨æ€äº¤äº’å’Œä¿¡æ¯ä¸ç¡®å®šæ€§ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>æ¨¡å‹åœ¨å¤„ç†åŠ¨æ€äº¤äº’å’Œæ•´åˆæ—¶é—´æ¼”å˜ä¿¡æ¯æ–¹é¢è¡¨ç°å·®å¼‚å¤§ã€‚</li>
<li>å¼ºè°ƒé“¾å¼æ€ç»´æ¨ç†çš„æ¨¡å‹åœ¨éœ€è¦æ·±å…¥æ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ã€‚</li>
<li>æ¨¡å‹åœ¨ä¸ç¡®å®šæ€§ä¸‹çš„æ¨ç†èƒ½åŠ›æ˜¾è‘—ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4a22364b161539081407c0668bd389a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8272f3d28eeb9a1f58e7d9dd244a016.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b260d5fcbdffe6de50e845ae2863f77.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Data-to-Dashboard-Multi-Agent-LLM-Framework-for-Insightful-Visualization-in-Enterprise-Analytics"><a href="#Data-to-Dashboard-Multi-Agent-LLM-Framework-for-Insightful-Visualization-in-Enterprise-Analytics" class="headerlink" title="Data-to-Dashboard: Multi-Agent LLM Framework for Insightful   Visualization in Enterprise Analytics"></a>Data-to-Dashboard: Multi-Agent LLM Framework for Insightful   Visualization in Enterprise Analytics</h2><p><strong>Authors:Ran Zhang, Mohannad Elhamod</strong></p>
<p>The rapid advancement of LLMs has led to the creation of diverse agentic systems in data analysis, utilizing LLMsâ€™ capabilities to improve insight generation and visualization. In this paper, we present an agentic system that automates the data-to-dashboard pipeline through modular LLM agents capable of domain detection, concept extraction, multi-perspective analysis generation, and iterative self-reflection. Unlike existing chart QA systems, our framework simulates the analytical reasoning process of business analysts by retrieving domain-relevant knowledge and adapting to diverse datasets without relying on closed ontologies or question templates.   We evaluate our system on three datasets across different domains. Benchmarked against GPT-4o with a single-prompt baseline, our approach shows improved insightfulness, domain relevance, and analytical depth, as measured by tailored evaluation metrics and qualitative human assessment.   This work contributes a novel modular pipeline to bridge the path from raw data to visualization, and opens new opportunities for human-in-the-loop validation by domain experts in business analytics. All code can be found here: <a target="_blank" rel="noopener" href="https://github.com/77luvC/D2D_Data2Dashboard">https://github.com/77luvC/D2D_Data2Dashboard</a> </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œåˆ©ç”¨LLMçš„èƒ½åŠ›ï¼Œåœ¨æ•°æ®åˆ†æä¸­åˆ›å»ºäº†å¤šæ ·åŒ–çš„æ™ºèƒ½ç³»ç»Ÿï¼Œç”¨äºæ”¹è¿›è§è§£ç”Ÿæˆå’Œå¯è§†åŒ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªæ™ºèƒ½ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡æ¨¡å—åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“è‡ªåŠ¨åŒ–æ•°æ®åˆ°ä»ªè¡¨æ¿çš„ç®¡é“ï¼Œè¿™äº›æ™ºèƒ½ä½“èƒ½å¤Ÿè¿›è¡Œé¢†åŸŸæ£€æµ‹ã€æ¦‚å¿µæå–ã€å¤šè§†è§’åˆ†æç”Ÿæˆå’Œè¿­ä»£è‡ªæˆ‘åæ€ã€‚ä¸ç°æœ‰çš„å›¾è¡¨é—®ç­”ç³»ç»Ÿä¸åŒï¼Œæˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡æ£€ç´¢ä¸é¢†åŸŸç›¸å…³çš„çŸ¥è¯†å¹¶é€‚åº”å„ç§æ•°æ®é›†ï¼Œè€Œæ— éœ€ä¾èµ–å°é—­çš„æœ¬ä½“æˆ–é—®é¢˜æ¨¡æ¿ï¼Œä»è€Œæ¨¡æ‹Ÿå•†ä¸šåˆ†æå¸ˆçš„åˆ†ææ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨ä¸åŒé¢†åŸŸçš„ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„ç³»ç»Ÿã€‚ä¸GPT-4oçš„å•æç¤ºåŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šåˆ¶çš„è¯„ä¼°æŒ‡æ ‡å’Œå®šæ€§çš„äººç±»è¯„ä¼°ä¸­æ˜¾ç¤ºå‡ºæ›´é«˜çš„æ´å¯ŸåŠ›ã€é¢†åŸŸç›¸å…³æ€§å’Œåˆ†ææ·±åº¦ã€‚è¿™é¡¹å·¥ä½œä¸ºä»åŸå§‹æ•°æ®åˆ°å¯è§†åŒ–çš„è·¯å¾„å»ºç«‹äº†ä¸€ä¸ªæ–°çš„æ¨¡å—åŒ–ç®¡é“ï¼Œå¹¶ä¸ºå•†ä¸šåˆ†æä¸­çš„é¢†åŸŸä¸“å®¶æä¾›äº†äººç±»å‚ä¸å¾ªç¯éªŒè¯çš„æ–°æœºä¼šã€‚æ‰€æœ‰ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/77luvC/D2D_Data2Dashboard">https://github.com/77luvC/D2D_Data2Dashboard</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23695v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œåˆ©ç”¨LLMçš„èƒ½åŠ›ï¼Œæ•°æ®åˆ†æå’Œå¯è§†åŒ–è¿‡ç¨‹ä¸­åˆ›å»ºäº†ä¸€ç§å¤šå…ƒåŒ–çš„æ™ºèƒ½ç³»ç»Ÿã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è‡ªåŠ¨åŒ–æ•°æ®åˆ°ä»ªè¡¨æ¿ç®¡é“çš„æ™ºèƒ½ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡æ¨¡å—åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“è¿›è¡Œå·¥ä½œï¼Œå¯å®Œæˆé¢†åŸŸæ£€æµ‹ã€æ¦‚å¿µæå–ã€å¤šç»´åº¦åˆ†æç”Ÿæˆå’Œè¿­ä»£è‡ªæˆ‘åæ€ã€‚ä¸åŒäºç°æœ‰çš„å›¾è¡¨é—®ç­”ç³»ç»Ÿï¼Œæˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡æ£€ç´¢ä¸é¢†åŸŸç›¸å…³çš„çŸ¥è¯†å¹¶é€‚åº”å„ç§æ•°æ®é›†ï¼Œæ— éœ€ä¾èµ–å°é—­çš„è¯­ä¹‰æˆ–é—®é¢˜æ¨¡æ¿å³å¯æ¨¡æ‹Ÿå•†ä¸šåˆ†æå¸ˆçš„åˆ†ææ¨ç†è¿‡ç¨‹ã€‚ç»å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ä¸‰ä¸ªä¸åŒé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ´å¯ŸåŠ›ã€é¢†åŸŸç›¸å…³æ€§å’Œåˆ†ææ·±åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²ç”¨äºåˆ›å»ºè‡ªåŠ¨åŒ–æ•°æ®åˆ°ä»ªè¡¨æ¿çš„æ™ºèƒ½ç³»ç»Ÿã€‚</li>
<li>è¯¥ç³»ç»ŸåŒ…å«æ¨¡å—åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ï¼Œèƒ½å®Œæˆé¢†åŸŸæ£€æµ‹ã€æ¦‚å¿µæå–å’Œåˆ†æç”Ÿæˆç­‰ä»»åŠ¡ã€‚</li>
<li>ä¸ç°æœ‰ç³»ç»Ÿä¸åŒï¼Œè¯¥ç³»ç»Ÿæ¨¡æ‹Ÿå•†ä¸šåˆ†æå¸ˆçš„åˆ†ææ¨ç†è¿‡ç¨‹ï¼Œå¹¶é€‚åº”å„ç§æ•°æ®é›†ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡æ£€ç´¢ä¸é¢†åŸŸç›¸å…³çš„çŸ¥è¯†æé«˜æ´å¯ŸåŠ›å’Œé¢†åŸŸç›¸å…³æ€§ã€‚</li>
<li>åœ¨ä¸‰ä¸ªä¸åŒé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿçš„æ€§èƒ½ä¼˜äºGPT-4oçš„å•æç¤ºåŸºçº¿ã€‚</li>
<li>ç³»ç»Ÿå±•ç°å‡ºå¼ºå¤§çš„æ´å¯ŸåŠ›ã€é¢†åŸŸç›¸å…³æ€§å’Œåˆ†ææ·±åº¦ï¼Œè¿™é€šè¿‡å®šåˆ¶çš„è¯„ä»·æŒ‡æ ‡å’Œäººç±»è¯„ä¼°å¾—åˆ°è¯å®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23695">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fc4362b4c80b4adffce23a0453866755.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75b421090235d30a8e38838d52063185.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0efcb0391c1e6edce334c54dcd1702e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ed9c59e9ed85f4cc3a79be138748799.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VF-Eval-Evaluating-Multimodal-LLMs-for-Generating-Feedback-on-AIGC-Videos"><a href="#VF-Eval-Evaluating-Multimodal-LLMs-for-Generating-Feedback-on-AIGC-Videos" class="headerlink" title="VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC   Videos"></a>VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC   Videos</h2><p><strong>Authors:Tingyu Song, Tongyan Hu, Guo Gan, Yilun Zhao</strong></p>
<p>MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks. This highlights the challenging nature of our benchmark. Additionally, to investigate the practical applications of VF-Eval in improving video generation, we conduct an experiment, RePrompt, demonstrating that aligning MLLMs more closely with human feedback can benefit video generation. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘é—®ç­”ä¸­å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°å¤§å¤šé›†ä¸­åœ¨è‡ªç„¶è§†é¢‘ä¸Šï¼Œå¿½è§†äº†åˆæˆè§†é¢‘ï¼Œå¦‚äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å†…å®¹ï¼ˆAIGCï¼‰ã€‚åŒæ—¶ï¼Œè§†é¢‘ç”Ÿæˆä¸­çš„ä¸€äº›å·¥ä½œä¾èµ–äºMLLMsæ¥è¯„ä¼°ç”Ÿæˆè§†é¢‘çš„è´¨é‡ï¼Œä½†MLLMsåœ¨è§£é‡ŠAIGCè§†é¢‘æ–¹é¢çš„èƒ½åŠ›ä»è¢«å¤§å¤§å¿½è§†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•VF-Evalï¼Œå®ƒå¼•å…¥äº†å››ä¸ªä»»åŠ¡ï¼šè¿è´¯æ€§éªŒè¯ã€é”™è¯¯æ„è¯†ã€é”™è¯¯ç±»å‹æ£€æµ‹å’Œæ¨ç†è¯„ä¼°ï¼Œä»¥å…¨é¢è¯„ä¼°MLLMsåœ¨AIGCè§†é¢‘ä¸Šçš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨VF-Evalä¸Šè¯„ä¼°äº†13ä¸ªå‰æ²¿çš„MLLMsï¼Œå‘ç°å³ä½¿æ˜¯è¡¨ç°æœ€ä½³çš„GPT-4.1æ¨¡å‹ï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šä¹Ÿæ— æ³•æŒç»­å®ç°è‰¯å¥½çš„æ€§èƒ½ã€‚è¿™å‡¸æ˜¾äº†æˆ‘ä»¬åŸºå‡†æµ‹è¯•çš„æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç ”ç©¶VF-Evalåœ¨æ”¹è¿›è§†é¢‘ç”Ÿæˆæ–¹é¢çš„å®é™…åº”ç”¨ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹åä¸ºRePromptçš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä½¿MLLMsä¸äººç±»åé¦ˆæ›´åŠ å¥‘åˆï¼Œå¯ä»¥å¯¹è§†é¢‘ç”Ÿæˆäº§ç”Ÿç›Šå¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23693v1">PDF</a> ACL 2025 Main</p>
<p><strong>Summary</strong></p>
<p>MLLMsåœ¨è§†é¢‘é—®ç­”æ–¹é¢å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†ç°æœ‰è¯„ä¼°ä¸»è¦é›†ä¸­äºè‡ªç„¶è§†é¢‘ï¼Œå¿½è§†äº†åˆæˆè§†é¢‘ï¼Œå¦‚AIç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰ã€‚é’ˆå¯¹æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•VF-Evalï¼ŒåŒ…å«å››ä¸ªä»»åŠ¡ï¼Œç”¨äºå…¨é¢è¯„ä¼°MLLMsåœ¨AIGCè§†é¢‘ä¸Šçš„èƒ½åŠ›ã€‚å¯¹13ä¸ªå‰æ²¿MLLMsçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€ä½³çš„GPT-4.1æ¨¡å‹ï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¹Ÿä¸ç¨³å®šï¼Œå‡¸æ˜¾äº†æ­¤åŸºå‡†çš„æŒ‘æˆ˜æ€§ã€‚åŒæ—¶ï¼Œä¸ºäº†ç ”ç©¶VF-Evalåœ¨æ”¹è¿›è§†é¢‘ç”Ÿæˆæ–¹é¢çš„å®é™…åº”ç”¨ï¼Œè¿›è¡Œäº†ä¸€é¡¹å®éªŒRePromptï¼Œè¡¨æ˜å°†MLLMsä¸äººç±»åé¦ˆæ›´ç´§å¯†åœ°ç»“åˆèµ·æ¥æœ‰ç›Šäºè§†é¢‘ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨è§†é¢‘é—®ç­”é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†ç°æœ‰è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨è‡ªç„¶è§†é¢‘ä¸Šï¼Œå¿½è§†äº†åˆæˆè§†é¢‘å¦‚AIç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºVF-Evalçš„æ–°åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°MLLMsåœ¨å¤„ç†AIGCè§†é¢‘ä¸Šçš„èƒ½åŠ›ã€‚</li>
<li>VF-EvalåŒ…å«å››ä¸ªä»»åŠ¡ï¼šè¿è´¯æ€§éªŒè¯ã€é”™è¯¯æ„è¯†ã€é”™è¯¯ç±»å‹æ£€æµ‹å’Œæ¨ç†èƒ½åŠ›è¯„ä¼°ã€‚</li>
<li>å¯¹13ä¸ªå‰æ²¿çš„MLLMsçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿åœ¨æœ€ä½³æ¨¡å‹GPT-4.1ä¸­ï¼Œæ‰€æœ‰ä»»åŠ¡çš„æ€§èƒ½ä¹Ÿä¸ç¨³å®šï¼Œå‡¸æ˜¾äº†åŸºå‡†æµ‹è¯•çš„æŒ‘æˆ˜æ€§ã€‚</li>
<li>å®éªŒRePromptè¡¨æ˜ï¼Œå°†MLLMsä¸äººç±»åé¦ˆæ›´ç´§å¯†åœ°ç»“åˆå¯ä»¥æ”¹è¿›è§†é¢‘ç”Ÿæˆã€‚</li>
<li>MLLMsåœ¨ç†è§£å’Œè§£é‡ŠAIGCè§†é¢‘æ–¹é¢è¿˜æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f51de5a894bc3599d2c4c8b2966b45df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e3082f4ec0a9f3ecd4cdc251f8d933a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-220db291ab3d828772dce7a7b3eb833f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-036311576f24023e000c693c9e9d69ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b15b88f389edd275e6d4245a0ae19d74.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ToolHaystack-Stress-Testing-Tool-Augmented-Language-Models-in-Realistic-Long-Term-Interactions"><a href="#ToolHaystack-Stress-Testing-Tool-Augmented-Language-Models-in-Realistic-Long-Term-Interactions" class="headerlink" title="ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic   Long-Term Interactions"></a>ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic   Long-Term Interactions</h2><p><strong>Authors:Beong-woo Kwak, Minju Kim, Dongha Lim, Hyungjoo Chae, Dongjin Kang, Sunghwan Kim, Dongil Yang, Jinyoung Yeo</strong></p>
<p>Large language models (LLMs) have demonstrated strong capabilities in using external tools to address user inquiries. However, most existing evaluations assume tool use in short contexts, offering limited insight into model behavior during realistic long-term interactions. To fill this gap, we introduce ToolHaystack, a benchmark for testing the tool use capabilities in long-term interactions. Each test instance in ToolHaystack includes multiple tasks execution contexts and realistic noise within a continuous conversation, enabling assessment of how well models maintain context and handle various disruptions. By applying this benchmark to 14 state-of-the-art LLMs, we find that while current models perform well in standard multi-turn settings, they often significantly struggle in ToolHaystack, highlighting critical gaps in their long-term robustness not revealed by previous tool benchmarks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åˆ©ç”¨å¤–éƒ¨å·¥å…·åº”å¯¹ç”¨æˆ·æŸ¥è¯¢æ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„è¯„ä¼°éƒ½æ˜¯å‡è®¾åœ¨çŸ­è¯­å¢ƒä¸­ä½¿ç”¨å·¥å…·ï¼Œå¯¹äºæ¨¡å‹åœ¨ç°å®é•¿æœŸäº’åŠ¨ä¸­çš„è¡Œä¸ºè¡¨ç°åªèƒ½æä¾›æœ‰é™æ´å¯Ÿã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ToolHaystackï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•é•¿æœŸäº’åŠ¨ä¸­å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ToolHaystackçš„æ¯ä¸ªæµ‹è¯•å®ä¾‹éƒ½åŒ…å«å¤šä¸ªä»»åŠ¡æ‰§è¡Œä¸Šä¸‹æ–‡å’Œè¿ç»­å¯¹è¯ä¸­çš„ç°å®å™ªéŸ³ï¼Œèƒ½å¤Ÿè¯„ä¼°æ¨¡å‹åœ¨ç»´æŒè¯­å¢ƒå’Œå¤„ç†å„ç§å¹²æ‰°æ–¹é¢çš„è¡¨ç°ã€‚é€šè¿‡å¯¹14ä¸ªæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨æ­¤åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°è™½ç„¶å½“å‰æ¨¡å‹åœ¨æ ‡å‡†å¤šè½®å¯¹è¯ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ToolHaystackä¸­ç»å¸¸é‡åˆ°ä¸¥é‡å›°éš¾ï¼Œçªå‡ºäº†ä»–ä»¬åœ¨é•¿æœŸç¨³å¥æ€§æ–¹é¢çš„å…³é”®å·®è·ï¼Œè¿™ä¸€ç‚¹ä¹‹å‰ä½¿ç”¨å·¥å…·åŸºå‡†æµ‹è¯•å¹¶æœªå‘ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23662v1">PDF</a> Our code and data are available at   <a target="_blank" rel="noopener" href="https://github.com/bwookwak/ToolHaystack">https://github.com/bwookwak/ToolHaystack</a></p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä½¿ç”¨å¤–éƒ¨å·¥å…·å›åº”ç”¨æˆ·æŸ¥è¯¢æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰è¯„ä¼°ä»…åœ¨çŸ­æœŸè¯­å¢ƒä¸­å‡è®¾å·¥å…·çš„ä½¿ç”¨ï¼Œå¯¹äºæ¨¡å‹åœ¨çœŸå®é•¿æœŸäº’åŠ¨ä¸­çš„è¡Œä¸ºæä¾›æœ‰é™çš„è§è§£ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºToolHaystackåŸºå‡†æµ‹è¯•ï¼Œç”¨äºæµ‹è¯•é•¿æœŸäº’åŠ¨ä¸­çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚ToolHaystackçš„æ¯ä¸ªæµ‹è¯•å®ä¾‹åŒ…å«å¤šä¸ªä»»åŠ¡æ‰§è¡Œè¯­å¢ƒå’Œç°å®ä¸­çš„å™ªéŸ³åœ¨è¿ç»­çš„å¯¹è¯ä¸­ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨ç»´æŒè¯­å¢ƒå’Œåº”å¯¹å„ç§å¹²æ‰°æ–¹é¢çš„è¡¨ç°ã€‚é€šè¿‡å¯¹14ä¸ªæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨æ­¤åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°è™½ç„¶å½“å‰æ¨¡å‹åœ¨æ ‡å‡†å¤šå›åˆè®¾ç½®ä¸­çš„è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ToolHaystackä¸­å¸¸å¸¸é‡åˆ°ä¸¥é‡å›°éš¾ï¼Œçªæ˜¾å‡ºå…¶åœ¨é•¿æœŸç¨³å¥æ€§æ–¹é¢çš„å…³é”®å·®è·ï¼Œè¿™ä¸€ç‚¹æ˜¯ä»¥å‰å·¥å…·åŸºå‡†æµ‹è¯•æ‰€æœªæ­ç¤ºçš„ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LLMså·²å±•ç°å‡ºåˆ©ç”¨å¤–éƒ¨å·¥å…·å›åº”ç”¨æˆ·æŸ¥è¯¢çš„å¼ºå¤§èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰è¯„ä¼°ä¸»è¦å…³æ³¨çŸ­æœŸè¯­å¢ƒä¸‹çš„å·¥å…·ä½¿ç”¨ï¼Œå¯¹æ¨¡å‹åœ¨çœŸå®é•¿æœŸäº’åŠ¨ä¸­çš„è¡Œä¸ºäº†è§£æœ‰é™ã€‚</li>
<li>ToolHaystackæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œèƒ½è¯„ä¼°æ¨¡å‹åœ¨é•¿æœŸäº’åŠ¨ä¸­ä½¿ç”¨å·¥å…·çš„èƒ½åŠ›ã€‚</li>
<li>ToolHaystackçš„æµ‹è¯•å®ä¾‹åŒ…å«å¤šä¸ªä»»åŠ¡æ‰§è¡Œè¯­å¢ƒå’Œç°å®ä¸­çš„å™ªéŸ³ï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹ã€‚</li>
<li>åœ¨ToolHaystackåŸºå‡†æµ‹è¯•ä¸­ï¼Œå½“å‰LLMsè¡¨ç°ä¸ä½³ï¼Œè¯´æ˜ä»–ä»¬åœ¨é•¿æœŸç¨³å¥æ€§æ–¹é¢å­˜åœ¨å…³é”®å·®è·ã€‚</li>
<li>è¿™ç§å·®è·åœ¨ä¹‹å‰çš„å·¥å…·è¯„ä¼°ä¸­å¹¶æœªè¢«æ­ç¤ºï¼Œå¼ºè°ƒäº†é•¿æœŸäº’åŠ¨è¯„ä¼°çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23662">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc683b1d0946c9ce3d41d7bdcb8008ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3a8ec38e8d9fa2f0dc844553d41131e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a1deff0e480d8cdf04ff7c4cd9e67ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ce4762279555781048674c794cc1911.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f50835adff9ef7d1a3badf16a275042.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="OpenUni-A-Simple-Baseline-for-Unified-Multimodal-Understanding-and-Generation"><a href="#OpenUni-A-Simple-Baseline-for-Unified-Multimodal-Understanding-and-Generation" class="headerlink" title="OpenUni: A Simple Baseline for Unified Multimodal Understanding and   Generation"></a>OpenUni: A Simple Baseline for Unified Multimodal Understanding and   Generation</h2><p><strong>Authors:Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy</strong></p>
<p>In this report, we present OpenUni, a simple, lightweight, and fully open-source baseline for unifying multimodal understanding and generation. Inspired by prevailing practices in unified model learning, we adopt an efficient training strategy that minimizes the training complexity and overhead by bridging the off-the-shelf multimodal large language models (LLMs) and diffusion models through a set of learnable queries and a light-weight transformer-based connector. With a minimalist choice of architecture, we demonstrate that OpenUni can: 1) generate high-quality and instruction-aligned images, and 2) achieve exceptional performance on standard benchmarks such as GenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To support open research and community advancement, we release all model weights, training code, and our curated training datasets (including 23M image-text pairs) at <a target="_blank" rel="noopener" href="https://github.com/wusize/OpenUni">https://github.com/wusize/OpenUni</a>. </p>
<blockquote>
<p>åœ¨è¿™ä»½æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OpenUniï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•ã€è½»é‡çº§ã€å®Œå…¨å¼€æºçš„åŸºçº¿ï¼Œç”¨äºç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚æˆ‘ä»¬å€Ÿé‰´äº†ç»Ÿä¸€æ¨¡å‹å­¦ä¹ ä¸­çš„æµè¡Œå®è·µï¼Œé‡‡ç”¨äº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡ä¸€ç»„å¯å­¦ä¹ çš„æŸ¥è¯¢å’Œä¸€ä¸ªè½»é‡çº§çš„åŸºäºå˜å‹å™¨çš„è¿æ¥å™¨ï¼Œå°†ç°æˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ‰©æ•£æ¨¡å‹è”ç³»èµ·æ¥ï¼Œä»è€Œé™ä½äº†è®­ç»ƒå¤æ‚æ€§å’Œé¢å¤–å¼€é”€ã€‚åœ¨æ¶æ„ä¸Šé€‰æ‹©æç®€ä¸»ä¹‰ï¼Œæˆ‘ä»¬è¯æ˜OpenUniå¯ä»¥ï¼š1ï¼‰ç”Ÿæˆé«˜è´¨é‡ä¸”ç¬¦åˆæŒ‡ä»¤çš„å›¾åƒï¼›2ï¼‰åœ¨GenEvalã€DPG-Benchå’ŒWISEç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°å“è¶Šæ€§èƒ½ï¼Œæ¿€æ´»çš„å‚æ•°åªæœ‰1.1Bå’Œ3.1Bã€‚ä¸ºäº†æ”¯æŒå¼€æ”¾ç ”ç©¶å’Œç¤¾åŒºå‘å±•ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/wusize/OpenUni%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%89%80%E6%9C%89%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E3%80%81%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%E5%92%8C%E6%88%91%E4%BB%AC%E7%B2%BE%E9%80%89%E7%9A%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%88%E5%8C%85%E6%8B%AC23M%E5%9B%BE%E5%83%8F%E6%96%87%E6%9C%AC%E5%AF%B9%EF%BC%89%E3%80%82">https://github.com/wusize/OpenUniä¸Šå‘å¸ƒäº†æ‰€æœ‰æ¨¡å‹æƒé‡ã€è®­ç»ƒä»£ç å’Œæˆ‘ä»¬ç²¾é€‰çš„è®­ç»ƒæ•°æ®é›†ï¼ˆåŒ…æ‹¬23Må›¾åƒæ–‡æœ¬å¯¹ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23661v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼€æºé¡¹ç›®OpenUniæ—¨åœ¨ç®€åŒ–å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„è¿‡ç¨‹ã€‚å®ƒé‡‡ç”¨é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡è½»é‡çº§è½¬æ¢å™¨è¿æ¥å™¨è¿æ¥ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡ä¸”ç¬¦åˆæŒ‡ä»¤çš„å›¾åƒï¼Œå¹¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚è¯¥é¡¹ç›®å·²å‘å¸ƒæ‰€æœ‰æ¨¡å‹æƒé‡ã€è®­ç»ƒä»£ç å’Œè®­ç»ƒæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenUniæ˜¯ä¸€ä¸ªç®€å•ã€è½»é‡çº§ã€å®Œå…¨å¼€æºçš„å¤šæ¨¡æ€ç†è§£å’Œç”ŸæˆåŸºå‡†ã€‚</li>
<li>å®ƒé‡‡ç”¨é«˜æ•ˆè®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡è½»é‡çº§è½¬æ¢å™¨è¿æ¥å™¨è¿æ¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>OpenUnièƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ä¸”ç¬¦åˆæŒ‡ä»¤çš„å›¾åƒã€‚</li>
<li>åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚GenEvalã€DPG-Benchå’ŒWISEï¼‰ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>OpenUniä½¿ç”¨çš„æ¨¡å‹å‚æ•°ä»…æœ‰1.1Bå’Œ3.1Bã€‚</li>
<li>é¡¹ç›®å·²å‘å¸ƒæ‰€æœ‰æ¨¡å‹æƒé‡ã€è®­ç»ƒä»£ç å’Œè®­ç»ƒæ•°æ®é›†ï¼ˆåŒ…æ‹¬23Må›¾åƒæ–‡æœ¬å¯¹ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7cd20762b5297aaaa5db3a7bd8a103aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2bbcb0e9d34e13c915cfb04cef56811.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4660fa82c63c1a45604f8604520060af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45d31729f8a2776c5c424dbbd0e69f52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-215343cec7a0cfdac617ff0769b3ee6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cc9a75d3eff5ca34e4fd3e764d149f1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="D-AR-Diffusion-via-Autoregressive-Models"><a href="#D-AR-Diffusion-via-Autoregressive-Models" class="headerlink" title="D-AR: Diffusion via Autoregressive Models"></a>D-AR: Diffusion via Autoregressive Models</h2><p><strong>Authors:Ziteng Gao, Mike Zheng Shou</strong></p>
<p>This paper presents Diffusion via Autoregressive models (D-AR), a new paradigm recasting the image diffusion process as a vanilla autoregressive procedure in the standard next-token-prediction fashion. We start by designing the tokenizer that converts images into sequences of discrete tokens, where tokens in different positions can be decoded into different diffusion denoising steps in the pixel space. Thanks to the diffusion properties, these tokens naturally follow a coarse-to-fine order, which directly lends itself to autoregressive modeling. Therefore, we apply standard next-token prediction on these tokens, without modifying any underlying designs (either causal masks or training&#x2F;inference strategies), and such sequential autoregressive token generation directly mirrors the diffusion procedure in image space. That is, once the autoregressive model generates an increment of tokens, we can directly decode these tokens into the corresponding diffusion denoising step in the streaming manner. Our pipeline naturally reveals several intriguing properties, for example, it supports consistent previews when generating only a subset of tokens and enables zero-shot layout-controlled synthesis. On the standard ImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone with 256 discrete tokens. We hope our work can inspire future research on unified autoregressive architectures of visual synthesis, especially with large language models. Code and models will be available at <a target="_blank" rel="noopener" href="https://github.com/showlab/D-AR">https://github.com/showlab/D-AR</a> </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†é€šè¿‡è‡ªå›å½’æ¨¡å‹å®ç°çš„æ‰©æ•£ï¼ˆD-ARï¼‰è¿™ä¸€æ–°èŒƒå¼ï¼Œå®ƒå°†å›¾åƒæ‰©æ•£è¿‡ç¨‹é‡æ–°æ„å»ºä¸ºä¸€ä¸ªæ ‡å‡†çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æ–¹å¼çš„æ™®é€šè‡ªå›å½’ç¨‹åºã€‚æˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªåˆ†è¯å™¨ï¼Œå°†å›¾åƒè½¬æ¢ä¸ºç¦»æ•£ä»¤ç‰Œåºåˆ—ï¼Œå…¶ä¸­ä¸åŒä½ç½®çš„ä»¤ç‰Œå¯ä»¥è§£ç ä¸ºåƒç´ ç©ºé—´ä¸­çš„ä¸åŒæ‰©æ•£å»å™ªæ­¥éª¤ã€‚å¾—ç›Šäºæ‰©æ•£å±æ€§ï¼Œè¿™äº›ä»¤ç‰Œè‡ªç„¶åœ°éµå¾ªä»ç²—ç³™åˆ°ç²¾ç»†çš„é¡ºåºï¼Œè¿™ç›´æ¥é€‚ç”¨äºè‡ªå›å½’å»ºæ¨¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹è¿™äº›ä»¤ç‰Œåº”ç”¨æ ‡å‡†çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ï¼Œè€Œæ— éœ€ä¿®æ”¹ä»»ä½•åŸºæœ¬è®¾è®¡ï¼ˆæ— è®ºæ˜¯å› æœæ©ç è¿˜æ˜¯è®­ç»ƒ&#x2F;æ¨ç†ç­–ç•¥ï¼‰ï¼Œå¹¶ä¸”è¿™ç§é¡ºåºè‡ªå›å½’ä»¤ç‰Œç”Ÿæˆç›´æ¥åæ˜ äº†å›¾åƒç©ºé—´ä¸­çš„æ‰©æ•£è¿‡ç¨‹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€æ—¦è‡ªå›å½’æ¨¡å‹ç”Ÿæˆäº†ä»¤ç‰Œçš„å¢é‡ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç›´æ¥å°†è¿™äº›ä»¤ç‰Œè§£ç ä¸ºç›¸åº”çš„æ‰©æ•£å»å™ªæ­¥éª¤ã€‚æˆ‘ä»¬çš„ç®¡é“è‡ªç„¶åœ°æ­ç¤ºäº†å‡ ä¸ªæœ‰è¶£çš„å±æ€§ï¼Œä¾‹å¦‚ï¼Œåœ¨ä»…ç”Ÿæˆä¸€éƒ¨åˆ†ä»¤ç‰Œæ—¶æ”¯æŒä¸€è‡´çš„é¢„è§ˆï¼Œå¹¶å®ç°äº†é›¶æ ·æœ¬å¸ƒå±€æ§åˆ¶åˆæˆã€‚åœ¨æ ‡å‡†çš„ImageNetåŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨åŒ…å«775Mä¸ªå‚æ•°çš„Llamaä¸»å¹²ç½‘ç»œå’Œ256ä¸ªç¦»æ•£ä»¤ç‰Œï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†2.09çš„FIDã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¤Ÿæ¿€å‘æœªæ¥å¯¹è§†è§‰åˆæˆç»Ÿä¸€è‡ªå›å½’æ¶æ„çš„ç ”ç©¶ï¼Œå°¤å…¶æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ–¹é¢ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/showlab/D-AR%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/showlab/D-ARä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23660v1">PDF</a> Technical report</p>
<p><strong>Summary</strong><br>     æ‰©æ•£è‡ªå›å½’æ¨¡å‹ï¼ˆD-ARï¼‰ä»¥å›¾åƒæ‰©æ•£è¿‡ç¨‹ä½œä¸ºåŸºæœ¬çš„è‡ªå›å½’ç¨‹åºé‡æ–°å®šä¹‰äº†æ–°çš„èŒƒå¼ï¼Œå°†å›¾åƒè½¬åŒ–ä¸ºç¦»æ•£ç¬¦å·åºåˆ—çš„å½¢å¼ã€‚å…¶ä»¥é€æ­¥å»å™ªæ–¹å¼ï¼Œå¯ç›´æ¥è§£ç ç”Ÿæˆåƒç´ ç©ºé—´çš„ä¸åŒä½ç½®ç¬¦å·ã€‚ç”±äºç¬¦å·çš„è‡ªç„¶é¡ºåºï¼ˆç”±ç²—åˆ°ç»†ï¼‰ï¼Œå…¶é€‚ç”¨äºè‡ªå›å½’å»ºæ¨¡ã€‚æ ‡å‡†è‡ªå›å½’é¢„æµ‹æ³•å¯ç”¨äºå›¾åƒåˆæˆé¢†åŸŸçš„è‡ªå›å½’ç®¡é“é¢„æµ‹é˜¶æ®µã€‚å¯¹äºå·²ç»ç”Ÿæˆçš„éƒ¨åˆ†ç¬¦å·é›†åˆè€Œè¨€ï¼Œå…¶å…·æœ‰ä¸€è‡´é¢„è§ˆçš„èƒ½åŠ›ï¼›å¹¶èƒ½è¿›è¡Œé›¶æ ·æœ¬å¸ƒå±€æ§åˆ¶çš„åˆæˆæ“ä½œã€‚åœ¨ImageNetæ ‡å‡†æµ‹è¯•ä¸­ï¼Œé‡‡ç”¨å…·æœ‰256ä¸ªç¦»æ•£ç¬¦å·çš„7.7äº¿å‚æ•°Llamaæ¨¡å‹ï¼Œå…¶FIDå¾—åˆ†è¾¾åˆ°2.09ã€‚æˆ‘ä»¬çš„å·¥ä½œå°†å¯å‘æœªæ¥çš„ç ”ç©¶ä¸“æ³¨äºæ„å»ºè§†è§‰åˆæˆçš„ç»Ÿä¸€è‡ªå›å½’æ¶æ„ï¼Œå°¤å…¶æ˜¯ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆçš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>D-ARæ¨¡å‹å°†å›¾åƒæ‰©æ•£è¿‡ç¨‹é‡æ–°å®šä¹‰ä¸ºè‡ªå›å½’ç¨‹åºï¼Œä»¥é€æ­¥å»å™ªæ–¹å¼è¿›è¡Œåƒç´ ç©ºé—´è§£ç ã€‚</li>
<li>åˆ©ç”¨è‡ªå›å½’å»ºæ¨¡æ–¹æ³•å¤„ç†å›¾åƒçš„ç¦»æ•£ç¬¦å·åºåˆ—ï¼Œé¡ºåº”ç¬¦å·çš„è‡ªç„¶é¡ºåºï¼ˆç”±ç²—åˆ°ç»†ï¼‰ã€‚</li>
<li>åˆ©ç”¨æ ‡å‡†è‡ªå›å½’é¢„æµ‹æ³•è¿›è¡Œå›¾åƒåˆæˆé¢†åŸŸçš„è‡ªå›å½’ç®¡é“é¢„æµ‹é˜¶æ®µã€‚</li>
<li>æ¨¡å‹å…·æœ‰ä¸€è‡´é¢„è§ˆèƒ½åŠ›ï¼Œå¹¶èƒ½è¿›è¡Œé›¶æ ·æœ¬å¸ƒå±€æ§åˆ¶çš„åˆæˆæ“ä½œã€‚</li>
<li>åœ¨ImageNetæµ‹è¯•ä¸­ï¼Œé‡‡ç”¨ç‰¹å®šæ¨¡å‹é…ç½®ï¼ŒFIDå¾—åˆ†è¾¾åˆ°è¾ƒä½æ°´å¹³ã€‚</li>
<li>æ¨¡å‹å±•ç¤ºäº†è‰¯å¥½çš„æ€§èƒ½ï¼Œå¯¹äºå­é›†ç¬¦å·ç”Ÿæˆæœ‰è¾ƒå¥½çš„ä¸€è‡´æ€§é¢„è§ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ecbdc25b97dc6b3e0bf1c4feb7686431.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d872045f883913329c666a233e5f9a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb81ded21820b07fba09f04957dd5180.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ed72320338fcb986c0dcf747fca8145.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Understanding-Refusal-in-Language-Models-with-Sparse-Autoencoders"><a href="#Understanding-Refusal-in-Language-Models-with-Sparse-Autoencoders" class="headerlink" title="Understanding Refusal in Language Models with Sparse Autoencoders"></a>Understanding Refusal in Language Models with Sparse Autoencoders</h2><p><strong>Authors:Wei Jie Yeo, Nirmalendu Prakash, Clement Neo, Roy Ka-Wei Lee, Erik Cambria, Ranjan Satapathy</strong></p>
<p>Refusal is a key safety behavior in aligned language models, yet the internal mechanisms driving refusals remain opaque. In this work, we conduct a mechanistic study of refusal in instruction-tuned LLMs using sparse autoencoders to identify latent features that causally mediate refusal behaviors. We apply our method to two open-source chat models and intervene on refusal-related features to assess their influence on generation, validating their behavioral impact across multiple harmful datasets. This enables a fine-grained inspection of how refusal manifests at the activation level and addresses key research questions such as investigating upstream-downstream latent relationship and understanding the mechanisms of adversarial jailbreaking techniques. We also establish the usefulness of refusal features in enhancing generalization for linear probes to out-of-distribution adversarial samples in classification tasks. We open source our code in <a target="_blank" rel="noopener" href="https://github.com/wj210/refusal_sae">https://github.com/wj210/refusal_sae</a>. </p>
<blockquote>
<p>æ‹’ç»æ˜¯è¯­è¨€æ¨¡å‹å¯¹é½ä¸­çš„å…³é”®å®‰å…¨è¡Œä¸ºï¼Œç„¶è€Œé©±åŠ¨æ‹’ç»çš„å†…éƒ¨æœºåˆ¶ä»ç„¶ä¸æ˜ç¡®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨å¯¹æŒ‡ä»¤è°ƒæ•´çš„LLMä¸­çš„æ‹’ç»è¡Œä¸ºè¿›è¡Œæœºåˆ¶æ€§ç ”ç©¶ï¼Œä»¥è¯†åˆ«å› æœä¸­ä»‹æ‹’ç»è¡Œä¸ºçš„æ½œåœ¨ç‰¹å¾ã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•åº”ç”¨äºä¸¤ä¸ªå¼€æºèŠå¤©æ¨¡å‹ï¼Œå¹¶å¯¹ä¸æ‹’ç»ç›¸å…³çš„ç‰¹å¾è¿›è¡Œå¹²é¢„ï¼Œä»¥è¯„ä¼°å®ƒä»¬å¯¹ç”Ÿæˆçš„å½±å“ï¼Œåœ¨å¤šä¸ªæœ‰å®³æ•°æ®é›†ä¸ŠéªŒè¯å…¶è¡Œä¸ºå½±å“ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿç²¾ç»†åœ°æ£€æŸ¥æ‹’ç»å¦‚ä½•åœ¨æ¿€æ´»å±‚é¢è¡¨ç°ï¼Œå¹¶è§£å†³å…³é”®çš„ç ”ç©¶é—®é¢˜ï¼Œä¾‹å¦‚è°ƒæŸ¥ä¸Šæ¸¸å’Œä¸‹æ¸¸çš„æ½œåœ¨å…³ç³»ä»¥åŠäº†è§£å¯¹æŠ—æ€§è¶Šç‹±æŠ€æœ¯çš„æœºåˆ¶ã€‚æˆ‘ä»¬è¿˜ç¡®å®šäº†æ‹’ç»ç‰¹å¾åœ¨å¢å¼ºåˆ†ç±»ä»»åŠ¡ä¸­å¯¹è¶…å‡ºåˆ†å¸ƒçš„å¯¹æŠ—æ ·æœ¬çš„é€šç”¨æ€§æ–¹é¢çš„ä½œç”¨ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/wj210/refusal_sae%E3%80%82">https://github.com/wj210/refusal_saeã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23556v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‹’ç»æ˜¯æŒ‡ä»¤å¯¹é½è¯­è¨€æ¨¡å‹ä¸­çš„å…³é”®å®‰å…¨è¡Œä¸ºï¼Œä½†å…¶å†…éƒ¨æœºåˆ¶ä»ä¸æ˜ç¡®ã€‚æœ¬ç ”ç©¶åˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨å¯¹æ‹’ç»è¡Œä¸ºåœ¨æŒ‡ä»¤å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æœºåˆ¶è¿›è¡Œç ”ç©¶ï¼Œä»¥è¯†åˆ«å› æœæ‹’ç»è¡Œä¸ºçš„æ½œåœ¨ç‰¹å¾ã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•åº”ç”¨äºä¸¤ä¸ªå¼€æºèŠå¤©æ¨¡å‹ï¼Œå¹¶å¯¹æ‹’ç»ç›¸å…³ç‰¹å¾è¿›è¡Œå¹²é¢„ï¼Œä»¥è¯„ä¼°å®ƒä»¬å¯¹ç”Ÿæˆçš„å½±å“ï¼Œåœ¨å¤šä¸ªæœ‰å®³æ•°æ®é›†ä¸ŠéªŒè¯å…¶è¡Œä¸ºå½±å“ã€‚è¿™ä½¿æˆ‘ä»¬å¯ä»¥ç»†è‡´åœ°æ£€æŸ¥æ‹’ç»å¦‚ä½•åœ¨æ¿€æ´»å±‚é¢è¡¨ç°å‡ºæ¥ï¼Œå¹¶è§£å†³å…³é”®ç ”ç©¶é—®é¢˜ï¼Œä¾‹å¦‚è°ƒæŸ¥ä¸Šæ¸¸å’Œä¸‹æ¸¸æ½œåœ¨å…³ç³»ä»¥åŠäº†è§£å¯¹æŠ—æ€§è¶Šç‹±æŠ€æœ¯çš„æœºåˆ¶ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†æ‹’ç»ç‰¹å¾åœ¨å¢å¼ºåˆ†ç±»ä»»åŠ¡ä¸­å¯¹è¶…å‡ºåˆ†å¸ƒçš„å¯¹æŠ—æ ·æœ¬çš„æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„å®ç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/wj210/refusal_sae%E3%80%82">https://github.com/wj210/refusal_saeã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‹’ç»æ˜¯æŒ‡ä»¤å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é‡è¦å®‰å…¨è¡Œä¸ºã€‚</li>
<li>åˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ç ”ç©¶æ‹’ç»è¡Œä¸ºçš„å†…éƒ¨æœºåˆ¶ã€‚</li>
<li>æ–¹æ³•åº”ç”¨äºä¸¤ä¸ªå¼€æºèŠå¤©æ¨¡å‹ï¼Œé€šè¿‡å¹²é¢„æ‹’ç»ç›¸å…³ç‰¹å¾è¯„ä¼°å…¶å¯¹ç”Ÿæˆçš„å½±å“ã€‚</li>
<li>åœ¨å¤šä¸ªæœ‰å®³æ•°æ®é›†ä¸ŠéªŒè¯äº†æ‹’ç»è¡Œä¸ºçš„å½±å“ã€‚</li>
<li>æ‹’ç»æœºåˆ¶çš„ç ”ç©¶æœ‰åŠ©äºç»†è‡´åœ°äº†è§£å…¶åœ¨æ¿€æ´»å±‚é¢çš„è¡¨ç°ã€‚</li>
<li>ç ”ç©¶è§£å†³äº†å…³äºä¸Šæ¸¸å’Œä¸‹æ¸¸æ½œåœ¨å…³ç³»ä»¥åŠå¯¹æŠ—æ€§è¶Šç‹±æŠ€æœ¯çš„å…³é”®ç ”ç©¶é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23556">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9eb88d1b19ce706dfd63f84cf66d7403.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5cfb5f55d2be5980b8bf6b09e1d6a0c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-056930b089209b2e6fb044b547c6416d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d93a8a30d7c0b97731f83953d1197da0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Distill-CLIP-DCLIP-Enhancing-Image-Text-Retrieval-via-Cross-Modal-Transformer-Distillation"><a href="#Distill-CLIP-DCLIP-Enhancing-Image-Text-Retrieval-via-Cross-Modal-Transformer-Distillation" class="headerlink" title="Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal   Transformer Distillation"></a>Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal   Transformer Distillation</h2><p><strong>Authors:Daniel Csizmadia, Andrei Codreanu, Victor Sim, Vighnesh Prabhu, Michael Lu, Kevin Zhu, Sean Oâ€™Brien, Vasu Sharma</strong></p>
<p>We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that enhances multimodal image-text retrieval while preserving the original modelâ€™s strong zero-shot classification capabilities. CLIP models are typically constrained by fixed image resolutions and limited context, which can hinder their effectiveness in retrieval tasks that require fine-grained cross-modal understanding. DCLIP addresses these challenges through a meta teacher-student distillation framework, where a cross-modal transformer teacher is fine-tuned to produce enriched embeddings via bidirectional cross-attention between YOLO-extracted image regions and corresponding textual spans. These semantically and spatially aligned global representations guide the training of a lightweight student model using a hybrid loss that combines contrastive learning and cosine similarity objectives. Despite being trained on only ~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a fraction of CLIPâ€™s original dataset-DCLIP significantly improves image-text retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIPâ€™s zero-shot classification performance. These results demonstrate that DCLIP effectively mitigates the trade-off between task specialization and generalization, offering a resource-efficient, domain-adaptive, and detail-sensitive solution for advanced vision-language tasks. Code available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DCLIP-B772/README.md">https://anonymous.4open.science/r/DCLIP-B772/README.md</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Distill CLIPï¼ˆDCLIPï¼‰ï¼Œå®ƒæ˜¯CLIPæ¨¡å‹çš„ä¸€ç§å¾®è°ƒå˜ä½“ï¼Œå¢å¼ºäº†å¤šæ¨¡æ€å›¾åƒæ–‡æœ¬æ£€ç´¢åŠŸèƒ½ï¼ŒåŒæ—¶ä¿ç•™äº†åŸå§‹æ¨¡å‹å¼ºå¤§çš„é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›ã€‚CLIPæ¨¡å‹é€šå¸¸å—åˆ°å›ºå®šå›¾åƒåˆ†è¾¨ç‡å’Œæœ‰é™ä¸Šä¸‹æ–‡çš„é™åˆ¶ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢å®ƒä»¬åœ¨éœ€è¦ç²¾ç»†è·¨æ¨¡æ€ç†è§£çš„ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚DCLIPé€šè¿‡å…ƒæ•™å¸ˆå­¦ç”Ÿè’¸é¦æ¡†æ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå…¶ä¸­è·¨æ¨¡æ€å˜å‹å™¨æ•™å¸ˆç»è¿‡å¾®è°ƒï¼Œé€šè¿‡YOLOæå–çš„å›¾åƒåŒºåŸŸå’Œç›¸åº”æ–‡æœ¬è·¨åº¦ä¹‹é—´çš„åŒå‘äº¤å‰æ³¨æ„åŠ›äº§ç”Ÿä¸°å¯Œçš„åµŒå…¥ã€‚è¿™äº›è¯­ä¹‰å’Œç©ºé—´å¯¹é½çš„å…¨å±€è¡¨ç¤ºé€šè¿‡ä½¿ç”¨ç»“åˆå¯¹æ¯”å­¦ä¹ å’Œä½™å¼¦ç›¸ä¼¼æ€§ç›®æ ‡çš„æ··åˆæŸå¤±æ¥æŒ‡å¯¼è½»é‡çº§å­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒã€‚å°½ç®¡ä»…åœ¨MSCOCOã€Flickr30kå’ŒConceptual Captionsç­‰æ•°æ®é›†ä¸­ç²¾å¿ƒæŒ‘é€‰çš„çº¦67,500ä¸ªæ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»…å CLIPåŸå§‹æ•°æ®é›†çš„ä¸€éƒ¨åˆ†ï¼Œä½†DCLIPæ˜¾è‘—æé«˜äº†å›¾åƒæ–‡æœ¬æ£€ç´¢æŒ‡æ ‡ï¼ˆRecall@Kï¼ŒMAPï¼‰ï¼ŒåŒæ—¶ä¿ç•™äº†CLIPçº¦94%çš„é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDCLIPæœ‰æ•ˆåœ°ç¼“è§£äº†ä»»åŠ¡ç‰¹åŒ–ä¸é€šç”¨åŒ–ä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºé«˜çº§è§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†èµ„æºé«˜æ•ˆã€é¢†åŸŸè‡ªé€‚åº”å’Œç»†èŠ‚æ•æ„Ÿçš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DCLIP-B772/README.md%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/DCLIP-B772/README.mdæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21549v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Distill CLIPï¼ˆDCLIPï¼‰æ˜¯CLIPæ¨¡å‹çš„ç²¾ç»†è°ƒæ•´å˜ä½“ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å›¾åƒæ–‡æœ¬æ£€ç´¢åŠŸèƒ½ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹æ¨¡å‹çš„å¼ºå¤§é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›ã€‚DCLIPé€šè¿‡å…ƒæ•™å¸ˆ-å­¦ç”Ÿè’¸é¦æ¡†æ¶è§£å†³CLIPæ¨¡å‹åœ¨å›¾åƒåˆ†è¾¨ç‡å’Œä¸Šä¸‹æ–‡æ–¹é¢çš„å±€é™æ€§ï¼Œé€šè¿‡è·¨æ¨¡æ€å˜å‹å™¨æ•™å¸ˆæ¨¡å‹äº§ç”Ÿä¸°å¯Œçš„åµŒå…¥ï¼Œé€šè¿‡YOLOæå–çš„å›¾åƒåŒºåŸŸå’Œç›¸åº”æ–‡æœ¬è·¨åº¦ä¹‹é—´çš„åŒå‘äº¤å‰æ³¨æ„åŠ›è¿›è¡ŒåŒå‘äº¤å‰æ³¨æ„åŠ›å¤„ç†ã€‚è¿™äº›è¯­ä¹‰å’Œç©ºé—´å¯¹é½çš„å…¨å±€è¡¨ç¤ºé€šè¿‡ä½¿ç”¨ç»“åˆå¯¹æ¯”å­¦ä¹ å’Œä½™å¼¦ç›¸ä¼¼æ€§ç›®æ ‡çš„æ··åˆæŸå¤±æ¥è®­ç»ƒè½»é‡çº§å­¦ç”Ÿæ¨¡å‹ã€‚å°½ç®¡ä»…åœ¨MSCOCOã€Flickr30kå’ŒConceptual Captionsç­‰æ•°æ®é›†çš„éƒ¨åˆ†æ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†DCLIPæ˜¾è‘—æé«˜äº†å›¾åƒæ–‡æœ¬æ£€ç´¢æŒ‡æ ‡ï¼ˆRecall@Kã€MAPï¼‰ï¼ŒåŒæ—¶ä¿ç•™CLIPçš„é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½çš„çº¦94%ã€‚ç»“æœè¯æ˜ï¼ŒDCLIPæœ‰æ•ˆç¼“è§£äº†ä»»åŠ¡ä¸“ä¸šåŒ–å’Œé€šç”¨åŒ–ä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºé«˜çº§è§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†èµ„æºé«˜æ•ˆã€é¢†åŸŸè‡ªé€‚åº”å’Œç»†èŠ‚æ•æ„Ÿçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DCLIPæ˜¯CLIPæ¨¡å‹çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œä¸“æ³¨äºå¢å¼ºå¤šæ¨¡æ€å›¾åƒæ–‡æœ¬æ£€ç´¢åŠŸèƒ½ã€‚</li>
<li>DCLIPé€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦æ¡†æ¶è§£å†³CLIPæ¨¡å‹çš„å±€é™æ€§ã€‚</li>
<li>æ•™å¸ˆæ¨¡å‹é€šè¿‡åŒå‘äº¤å‰æ³¨æ„åŠ›å¤„ç†å›¾åƒåŒºåŸŸå’Œæ–‡æœ¬è·¨åº¦ï¼Œäº§ç”Ÿä¸°å¯Œçš„åµŒå…¥ã€‚</li>
<li>DCLIPä½¿ç”¨æ··åˆæŸå¤±æ¥è®­ç»ƒè½»é‡çº§å­¦ç”Ÿæ¨¡å‹ï¼Œç»“åˆå¯¹æ¯”å­¦ä¹ å’Œä½™å¼¦ç›¸ä¼¼æ€§ç›®æ ‡ã€‚</li>
<li>DCLIPåœ¨æœ‰é™çš„æ ·æœ¬é›†ä¸Šè®­ç»ƒï¼Œä½†æ˜¾è‘—æé«˜äº†å›¾åƒæ–‡æœ¬æ£€ç´¢æ€§èƒ½ã€‚</li>
<li>DCLIPä¿ç•™äº†CLIPçš„é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½çš„çº¦94%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3448a93f5b547131cfd1cb209164d7c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64fc51ed96f60f5dfbe42773329d3a1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3c03365170299397c44c4faec81cd22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2a39fdf816a19d9a53b77d2690e9b54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46f8ddd452943fc59fedaf051f639e47.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Multilingual-Question-Answering-in-Low-Resource-Settings-A-Dzongkha-English-Benchmark-for-Foundation-Models"><a href="#Multilingual-Question-Answering-in-Low-Resource-Settings-A-Dzongkha-English-Benchmark-for-Foundation-Models" class="headerlink" title="Multilingual Question Answering in Low-Resource Settings: A   Dzongkha-English Benchmark for Foundation Models"></a>Multilingual Question Answering in Low-Resource Settings: A   Dzongkha-English Benchmark for Foundation Models</h2><p><strong>Authors:Md. Tanzib Hosain, Rajan Das Gupta, Md. Kishor Morol</strong></p>
<p>In this work, we provide DZEN, a dataset of parallel Dzongkha and English test questions for Bhutanese middle and high school students. The over 5K questions in our collection span a variety of scientific topics and include factual, application, and reasoning-based questions. We use our parallel dataset to test a number of Large Language Models (LLMs) and find a significant performance difference between the models in English and Dzongkha. We also look at different prompting strategies and discover that Chain-of-Thought (CoT) prompting works well for reasoning questions but less well for factual ones. We also find that adding English translations enhances the precision of Dzongkha question responses. Our results point to exciting avenues for further study to improve LLM performance in Dzongkha and, more generally, in low-resource languages. We release the dataset at: <a target="_blank" rel="noopener" href="https://github.com/kraritt/llm_dzongkha_evaluation">https://github.com/kraritt/llm_dzongkha_evaluation</a>. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æä¾›äº†DZENæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ç»„ç”¨äºä¸ä¸¹ä¸­å­¦å’Œé«˜ä¸­ç”Ÿçš„è—è¯­å’Œè‹±è¯­å¹³è¡Œæµ‹è¯•é—®é¢˜ã€‚æˆ‘ä»¬çš„æ”¶è—ä¸­è¶…è¿‡5000ä¸ªé—®é¢˜æ¶µç›–äº†å„ç§ç§‘å­¦ä¸»é¢˜ï¼ŒåŒ…æ‹¬äº‹å®ã€åº”ç”¨å’ŒåŸºäºæ¨ç†çš„é—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨å¹³è¡Œæ•°æ®é›†æµ‹è¯•äº†å¤§é‡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¹¶å‘ç°è‹±è¯­å’Œè—è¯­æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†ä¸åŒçš„æç¤ºç­–ç•¥ï¼Œå‘ç°é“¾å¼æ€ç»´æç¤ºå¯¹äºæ¨ç†é—®é¢˜æ•ˆæœå¾ˆå¥½ï¼Œä½†å¯¹äº‹å®é—®é¢˜æ•ˆæœè¾ƒå·®ã€‚æˆ‘ä»¬è¿˜å‘ç°æ·»åŠ è‹±è¯­ç¿»è¯‘å¯ä»¥æé«˜è—è¯­é—®é¢˜å›ç­”çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ç»“æœæŒ‡å‡ºäº†è¿›ä¸€æ­¥ç ”ç©¶ä»¥æ”¹å–„è—è¯­ä¹ƒè‡³ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­LLMæ€§èƒ½çš„æ¿€åŠ¨äººå¿ƒçš„é€”å¾„ã€‚æˆ‘ä»¬åœ¨ä»¥ä¸‹åœ°å€å‘å¸ƒæ•°æ®é›†ï¼š<a target="_blank" rel="noopener" href="https://github.com/kraritt/llm_dzongkha_evaluation">https://github.com/kraritt/llm_dzongkha_evaluation</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18638v2">PDF</a> 24 pages, 20 figures</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æä¾›äº†åä¸ºDZENçš„å¹³è¡Œæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«è¶…è¿‡äº”åƒé“é¢å‘ä¸ä¸¹ä¸­å­¦å’Œé«˜å¹´çº§å­¦ç”Ÿçš„Dzongkhaè¯­å’Œè‹±è¯­çš„æµ‹è¯•é—®é¢˜ã€‚è¿™äº›é—®é¢˜æ¶µç›–äº†å¤šä¸ªç§‘å­¦é¢†åŸŸï¼ŒåŒ…æ‹¬äº‹å®ã€åº”ç”¨å’Œæ¨ç†é—®é¢˜ã€‚ç ”ç©¶ä½¿ç”¨æ­¤æ•°æ®é›†æµ‹è¯•äº†å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå‘ç°ä¸åŒè¯­è¨€æ¨¡å‹ä¸­è‹±è¯­å’ŒDzongkhaè¯­çš„è¡¨ç°å·®å¼‚æ˜¾è‘—ã€‚ç ”ç©¶è¿˜æ¢è®¨äº†ä¸åŒçš„æç¤ºç­–ç•¥ï¼Œå‘ç°é“¾å¼æ€ç»´æç¤ºå¯¹æ¨ç†é—®é¢˜æœ‰æ•ˆä½†å¯¹äº‹å®é—®é¢˜æ•ˆæœè¾ƒå·®ã€‚æ­¤å¤–ï¼Œæ·»åŠ è‹±è¯­ç¿»è¯‘èƒ½æé«˜Dzongkhaé—®é¢˜çš„å›ç­”å‡†ç¡®æ€§ã€‚ç ”ç©¶æŒ‡å‡ºè¿›ä¸€æ­¥æ”¹å–„LLMåœ¨Dzongkhaå’Œä½èµ„æºè¯­è¨€è¡¨ç°çš„ç ”ç©¶æ–¹å‘ã€‚æ•°æ®é›†å·²åœ¨ç›¸å…³é“¾æ¥å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æä¾›äº†ä¸€ä¸ªåŒ…å«è¶…è¿‡äº”åƒé“æµ‹è¯•é—®é¢˜çš„å¹³è¡Œæ•°æ®é›†DZENï¼Œé¢å‘ä¸ä¸¹ä¸­å­¦å’Œé«˜å¹´çº§å­¦ç”Ÿï¼Œæ¶µç›–å¤šä¸ªç§‘å­¦é¢†åŸŸã€‚</li>
<li>é€šè¿‡æ•°æ®é›†æµ‹è¯•äº†å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå‘ç°è‹±è¯­å’ŒDzongkhaè¯­è¡¨ç°å­˜åœ¨å·®å¼‚ã€‚</li>
<li>ç ”ç©¶äº†ä¸åŒçš„æç¤ºç­–ç•¥ï¼Œå‘ç°é“¾å¼æ€ç»´æç¤ºå¯¹æ¨ç†é—®é¢˜æ•ˆæœå¥½ä½†å¯¹äº‹å®é—®é¢˜æ•ˆæœå·®ã€‚</li>
<li>æ·»åŠ è‹±è¯­ç¿»è¯‘èƒ½å¤Ÿæé«˜DzongKwaé—®é¢˜çš„å›ç­”å‡†ç¡®æ€§ã€‚</li>
<li>æ­¤ç ”ç©¶å±•ç¤ºäº†æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€å¦‚DzongKwaä¸­è¡¨ç°çš„ç ”ç©¶æ–¹å‘ã€‚</li>
<li>æ•°æ®é›†å·²åœ¨æŒ‡å®šé“¾æ¥å‘å¸ƒï¼Œä¾¿äºå…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18638">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b976f4c0e67c99ef87929cb4ed54b12f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab9abe748378dc9fbff0800fcb108bcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8812aa74f74ad2c1b01c140168dc820c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fc46e30ef6981405e5b9124c49a72ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c496c5465c761f2f77bc84c5ad42a41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75a094d68bb8bbf4ce38724b838d3791.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-49be2282d3d40772916925bc569017cb.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  ML-Agent Reinforcing LLM Agents for Autonomous Machine Learning   Engineering
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-16d9a6c967fccfdd4560736f3589596c.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  Argus Vision-Centric Reasoning with Grounded Chain-of-Thought
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23539.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
