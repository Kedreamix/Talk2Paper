<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  DarkDiff Advancing Low-Light Raw Enhancement by Retasking Diffusion   Models for Camera ISP">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-0d2a90d028abcb96a0153659c4c1beca.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-31
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    66 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-31-æ›´æ–°"><a href="#2025-05-31-æ›´æ–°" class="headerlink" title="2025-05-31 æ›´æ–°"></a>2025-05-31 æ›´æ–°</h1><h2 id="DarkDiff-Advancing-Low-Light-Raw-Enhancement-by-Retasking-Diffusion-Models-for-Camera-ISP"><a href="#DarkDiff-Advancing-Low-Light-Raw-Enhancement-by-Retasking-Diffusion-Models-for-Camera-ISP" class="headerlink" title="DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion   Models for Camera ISP"></a>DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion   Models for Camera ISP</h2><p><strong>Authors:Amber Yijia Zheng, Yu Zhang, Jun Hu, Raymond A. Yeh, Chen Chen</strong></p>
<p>High-quality photography in extreme low-light conditions is challenging but impactful for digital cameras. With advanced computing hardware, traditional camera image signal processor (ISP) algorithms are gradually being replaced by efficient deep networks that enhance noisy raw images more intelligently. However, existing regression-based models often minimize pixel errors and result in oversmoothing of low-light photos or deep shadows. Recent work has attempted to address this limitation by training a diffusion model from scratch, yet those models still struggle to recover sharp image details and accurate colors. We introduce a novel framework to enhance low-light raw images by retasking pre-trained generative diffusion models with the camera ISP. Extensive experiments demonstrate that our method outperforms the state-of-the-art in perceptual quality across three challenging low-light raw image benchmarks. </p>
<blockquote>
<p>åœ¨æç«¯ä½å…‰æ¡ä»¶ä¸‹æ‹æ‘„é«˜è´¨é‡ç…§ç‰‡å¯¹äºæ•°å­—ç›¸æœºæ¥è¯´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä½†æ„ä¹‰é‡å¤§ã€‚å€ŸåŠ©å…ˆè¿›çš„è®¡ç®—ç¡¬ä»¶ï¼Œä¼ ç»Ÿçš„ç›¸æœºå›¾åƒä¿¡å·å¤„ç†å™¨ï¼ˆISPï¼‰ç®—æ³•æ­£é€æ¸è¢«é«˜æ•ˆçš„æ·±åº¦ç½‘ç»œæ‰€å–ä»£ï¼Œè¿™äº›ç½‘ç»œèƒ½å¤Ÿæ›´æ™ºèƒ½åœ°å¢å¼ºå™ªå£°åŸå§‹å›¾åƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºå›å½’çš„æ¨¡å‹é€šå¸¸æœ€å°åŒ–åƒç´ è¯¯å·®ï¼Œå¯¼è‡´ä½å…‰ç…§ç‰‡æˆ–æ·±é˜´å½±è¿‡åº¦å¹³æ»‘ã€‚è¿‘æœŸçš„å·¥ä½œè¯•å›¾é€šè¿‡ä»å¤´å¼€å§‹è®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥è§£å†³è¿™ä¸€å±€é™ï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶éš¾ä»¥æ¢å¤æ¸…æ™°çš„å›¾åƒç»†èŠ‚å’Œå‡†ç¡®çš„é¢œè‰²ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œé€šè¿‡ç”¨ç›¸æœºISPé‡æ–°åˆ†é…é¢„è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹æ¥æé«˜ä½å…‰åŸå§‹å›¾åƒçš„è´¨é‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä½å…‰åŸå§‹å›¾åƒåŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨æ„ŸçŸ¥è´¨é‡ä¸Šè¶…è¿‡äº†æœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23743v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ ç½‘ç»œé€æ­¥å–ä»£ä¼ ç»Ÿç›¸æœºå›¾åƒä¿¡å·å¤„ç†å™¨ï¼ˆISPï¼‰ç®—æ³•ï¼Œæé«˜ä½å…‰ç¯å¢ƒä¸‹çš„é«˜è´¨é‡æ‘„å½±æ•ˆæœã€‚ç°æœ‰å›å½’æ¨¡å‹æ˜“å¯¼è‡´ä½å…‰ç…§ç‰‡æˆ–æ·±é˜´å½±è¿‡åº¦å¹³æ»‘ï¼Œå¤±å»ç»†èŠ‚ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ç»“åˆç›¸æœºISPæŠ€æœ¯æå‡ä½å…‰ç¯å¢ƒä¸‹çš„åŸå§‹å›¾åƒè´¨é‡ï¼Œè¶…è¶Šç°æœ‰æŠ€æœ¯æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ ç½‘ç»œåœ¨ä½å…‰ç¯å¢ƒä¸‹æé«˜æ‘„å½±è´¨é‡å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>ä¼ ç»ŸISPç®—æ³•æ­£åœ¨è¢«é«˜æ•ˆæ·±åº¦ç½‘ç»œå–ä»£ã€‚</li>
<li>ç°æœ‰å›å½’æ¨¡å‹åœ¨å¤„ç†ä½å…‰ç…§ç‰‡æ—¶æ˜“å¯¼è‡´è¿‡åº¦å¹³æ»‘ï¼Œä¸¢å¤±ç»†èŠ‚ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†ä½å…‰ç¯å¢ƒä¸‹çš„åŸå§‹å›¾åƒæ—¶è¡¨ç°è‰¯å¥½ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œç»“åˆäº†é¢„è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹å’Œç›¸æœºISPæŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸‰ç»„å…·æœ‰æŒ‘æˆ˜æ€§çš„ä½å…‰åŸå§‹å›¾åƒåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è¶…è¶Šç°æœ‰æŠ€æœ¯çš„æ„ŸçŸ¥è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23743">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fca65bf8bdebd958636791c4cb2458f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-591c5d83e2f016cc0ca43f7ef15982b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a5cbe3980a6cdee1791e9aac7b36ecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e9a505926508791fdbeb163df2476cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95d90ba4860f588d7363050e7e71bca6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LayerPeeler-Autoregressive-Peeling-for-Layer-wise-Image-Vectorization"><a href="#LayerPeeler-Autoregressive-Peeling-for-Layer-wise-Image-Vectorization" class="headerlink" title="LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization"></a>LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization</h2><p><strong>Authors:Ronghuan Wu, Wanchao Su, Jing Liao</strong></p>
<p>Image vectorization is a powerful technique that converts raster images into vector graphics, enabling enhanced flexibility and interactivity. However, popular image vectorization tools struggle with occluded regions, producing incomplete or fragmented shapes that hinder editability. While recent advancements have explored rule-based and data-driven layer-wise image vectorization, these methods face limitations in vectorization quality and flexibility. In this paper, we introduce LayerPeeler, a novel layer-wise image vectorization approach that addresses these challenges through a progressive simplification paradigm. The key to LayerPeelerâ€™s success lies in its autoregressive peeling strategy: by identifying and removing the topmost non-occluded layers while recovering underlying content, we generate vector graphics with complete paths and coherent layer structures. Our method leverages vision-language models to construct a layer graph that captures occlusion relationships among elements, enabling precise detection and description for non-occluded layers. These descriptive captions are used as editing instructions for a finetuned image diffusion model to remove the identified layers. To ensure accurate removal, we employ localized attention control that precisely guides the model to target regions while faithfully preserving the surrounding content. To support this, we contribute a large-scale dataset specifically designed for layer peeling tasks. Extensive quantitative and qualitative experiments demonstrate that LayerPeeler significantly outperforms existing techniques, producing vectorization results with superior path semantics, geometric regularity, and visual fidelity. </p>
<blockquote>
<p>å›¾åƒçŸ¢é‡åŒ–æ˜¯ä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œå¯å°†ä½å›¾å›¾åƒè½¬æ¢ä¸ºçŸ¢é‡å›¾å½¢ï¼Œä»è€Œæé«˜çµæ´»æ€§å’Œäº¤äº’æ€§ã€‚ç„¶è€Œï¼Œæµè¡Œçš„å›¾åƒçŸ¢é‡åŒ–å·¥å…·åœ¨å¤„ç†é®æŒ¡åŒºåŸŸæ—¶é‡åˆ°å›°éš¾ï¼Œäº§ç”Ÿä¸å®Œæ•´æˆ–ç¢ç‰‡åŒ–çš„å½¢çŠ¶ï¼Œå¦¨ç¢äº†ç¼–è¾‘èƒ½åŠ›ã€‚è™½ç„¶æœ€è¿‘çš„è¿›å±•æ¢ç´¢äº†åŸºäºè§„åˆ™å’ŒåŸºäºæ•°æ®åˆ†å±‚å›¾åƒçŸ¢é‡åŒ–æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åœ¨çŸ¢é‡åŒ–è´¨é‡å’Œçµæ´»æ€§æ–¹é¢ä»é¢ä¸´å±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LayerPeelerï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åˆ†å±‚å›¾åƒçŸ¢é‡åŒ–æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ¸è¿›ç®€åŒ–èŒƒå¼æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚LayerPeeleræˆåŠŸçš„å…³é”®åœ¨äºå…¶è‡ªå›å½’å‰¥ç¦»ç­–ç•¥ï¼šé€šè¿‡è¯†åˆ«å’Œç§»é™¤æœ€é¡¶éƒ¨çš„éé®æŒ¡å±‚å¹¶æ¢å¤åº•å±‚å†…å®¹ï¼Œæˆ‘ä»¬ç”Ÿæˆå…·æœ‰å®Œæ•´è·¯å¾„å’Œè¿è´¯å±‚ç»“æ„çš„çŸ¢é‡å›¾å½¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ„å»ºå±‚å›¾ï¼Œæ•è·å…ƒç´ ä¹‹é—´çš„é®æŒ¡å…³ç³»ï¼Œä¸ºéé®æŒ¡å±‚æä¾›ç²¾ç¡®çš„æ£€æµ‹å’Œæè¿°ã€‚è¿™äº›æè¿°æ€§å­—å¹•ç”¨ä½œå¾®è°ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„ç¼–è¾‘æŒ‡ä»¤ï¼Œä»¥ç§»é™¤å·²è¯†åˆ«çš„å›¾å±‚ã€‚ä¸ºäº†ç¡®ä¿å‡†ç¡®çš„ç§»é™¤æ“ä½œï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å±€éƒ¨æ³¨æ„åŠ›æ§åˆ¶ï¼Œç²¾ç¡®å¼•å¯¼æ¨¡å‹å®šä½åŒºåŸŸï¼ŒåŒæ—¶å¿ å®ä¿ç•™å‘¨å›´å†…å®¹ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬è´¡çŒ®äº†ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºå›¾å±‚å‰¥ç¦»ä»»åŠ¡çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚å¤§é‡çš„å®šé‡å’Œå®šæ€§å®éªŒè¡¨æ˜ï¼ŒLayerPeeleråœ¨è·¯å¾„è¯­ä¹‰ã€å‡ ä½•è§„åˆ™å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œäº§ç”Ÿäº†å‡ºè‰²çš„çŸ¢é‡åŒ–ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23740v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://layerpeeler.github.io/">https://layerpeeler.github.io/</a></p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹å›¾åƒçŸ¢é‡åŒ–æ–¹æ³•LayerPeelerï¼Œé‡‡ç”¨åˆ†å±‚å‰¥ç¦»ç­–ç•¥å°†å›¾åƒçŸ¢é‡åŒ–ä¸ºçŸ¢é‡å›¾å½¢ï¼Œèƒ½å¤„ç†é®æŒ¡åŒºåŸŸçš„æŒ‘æˆ˜ï¼Œç”Ÿæˆå®Œæ•´è·¯å¾„å’Œè¿è´¯å±‚ç»“æ„çš„çŸ¢é‡å›¾å½¢ã€‚åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ„å»ºå±‚å›¾ï¼Œæ•æ‰å…ƒç´ é—´çš„é®æŒ¡å…³ç³»ï¼Œå¹¶ä½¿ç”¨æè¿°æ€§æ ‡é¢˜ä½œä¸ºç¼–è¾‘æŒ‡ä»¤è¿›è¡Œå¾®è°ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„å±‚å‰¥ç¦»ã€‚é‡‡ç”¨å±€éƒ¨æ³¨æ„åŠ›æ§åˆ¶ç²¾ç¡®æŒ‡å¯¼æ¨¡å‹è¿›è¡Œç›®æ ‡åŒºåŸŸå‰¥ç¦»ï¼ŒåŒæ—¶å¿ å®ä¿ç•™å‘¨å›´å†…å®¹ã€‚LayerPeeleråœ¨çŸ¢é‡åŒ–è·¯å¾„è¯­ä¹‰ã€å‡ ä½•è§„åˆ™æ€§å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LayerPeeleræ˜¯ä¸€ç§æ–°å‹çš„å›¾åƒçŸ¢é‡åŒ–æ–¹æ³•ï¼Œé‡‡ç”¨åˆ†å±‚å‰¥ç¦»ç­–ç•¥è§£å†³é®æŒ¡åŒºåŸŸçš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡è¯†åˆ«å¹¶ç§»é™¤æœ€é¡¶å±‚çš„éé®æŒ¡å±‚ï¼ŒåŒæ—¶æ¢å¤åº•å±‚å†…å®¹ï¼Œç”Ÿæˆå®Œæ•´è·¯å¾„å’Œè¿è´¯å±‚ç»“æ„çš„çŸ¢é‡å›¾å½¢ã€‚</li>
<li>åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ„å»ºå±‚å›¾ï¼Œå‡†ç¡®æ•æ‰å…ƒç´ é—´çš„é®æŒ¡å…³ç³»ã€‚</li>
<li>ä½¿ç”¨æè¿°æ€§æ ‡é¢˜ä½œä¸ºç¼–è¾‘æŒ‡ä»¤å¯¹å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå®ç°ç²¾å‡†å±‚å‰¥ç¦»ã€‚</li>
<li>é‡‡ç”¨å±€éƒ¨æ³¨æ„åŠ›æ§åˆ¶ï¼Œç²¾ç¡®æŒ‡å¯¼æ¨¡å‹è¿›è¡Œç›®æ ‡åŒºåŸŸå‰¥ç¦»ï¼ŒåŒæ—¶ä¿ç•™å‘¨å›´å†…å®¹ã€‚</li>
<li>LayerPeeleråœ¨çŸ¢é‡åŒ–è·¯å¾„è¯­ä¹‰ã€å‡ ä½•è§„åˆ™æ€§å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2e374d8e1ce82f1a173e8d8356efdb7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d394156d39908f13442b8dd6adc75455.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-abdb7bf2fa4c5cb2e6ef496bb78edbb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10e692535414ab1c351113d6110af58f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffea8d12d199aa128168c57352a70889.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OpenUni-A-Simple-Baseline-for-Unified-Multimodal-Understanding-and-Generation"><a href="#OpenUni-A-Simple-Baseline-for-Unified-Multimodal-Understanding-and-Generation" class="headerlink" title="OpenUni: A Simple Baseline for Unified Multimodal Understanding and   Generation"></a>OpenUni: A Simple Baseline for Unified Multimodal Understanding and   Generation</h2><p><strong>Authors:Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy</strong></p>
<p>In this report, we present OpenUni, a simple, lightweight, and fully open-source baseline for unifying multimodal understanding and generation. Inspired by prevailing practices in unified model learning, we adopt an efficient training strategy that minimizes the training complexity and overhead by bridging the off-the-shelf multimodal large language models (LLMs) and diffusion models through a set of learnable queries and a light-weight transformer-based connector. With a minimalist choice of architecture, we demonstrate that OpenUni can: 1) generate high-quality and instruction-aligned images, and 2) achieve exceptional performance on standard benchmarks such as GenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To support open research and community advancement, we release all model weights, training code, and our curated training datasets (including 23M image-text pairs) at <a target="_blank" rel="noopener" href="https://github.com/wusize/OpenUni">https://github.com/wusize/OpenUni</a>. </p>
<blockquote>
<p>åœ¨è¿™ä»½æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OpenUniï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•ã€è½»é‡çº§ã€å®Œå…¨å¼€æºçš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆç»Ÿä¸€åŸºå‡†ã€‚æˆ‘ä»¬å—åˆ°ç»Ÿä¸€æ¨¡å‹å­¦ä¹ æµè¡Œå®è·µçš„å¯å‘ï¼Œé‡‡ç”¨äº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡ä¸€ç»„å¯å­¦ä¹ çš„æŸ¥è¯¢å’Œä¸€ä¸ªåŸºäºè½»é‡çº§å˜å‹å™¨çš„è¿æ¥å™¨ï¼Œå°†ç°æˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹è”ç³»èµ·æ¥ï¼Œä»è€Œæœ€å°åŒ–è®­ç»ƒå¤æ‚æ€§å’Œå¼€é”€ã€‚é€šè¿‡é€‰æ‹©æœ€ç®€å•çš„æ¶æ„ï¼Œæˆ‘ä»¬è¯æ˜OpenUniå¯ä»¥ï¼š1ï¼‰ç”Ÿæˆé«˜è´¨é‡ä¸”ç¬¦åˆæŒ‡ä»¤çš„å›¾åƒï¼›2ï¼‰åœ¨GenEvalã€DPG-Benchå’ŒWISEç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šå®ç°å“è¶Šæ€§èƒ½ï¼Œæ¿€æ´»çš„å‚æ•°åªæœ‰1.1Bå’Œ3.1Bã€‚ä¸ºäº†æ”¯æŒå¼€æ”¾ç ”ç©¶å’Œç¤¾åŒºå‘å±•ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/wusize/OpenUni">https://github.com/wusize/OpenUni</a>ä¸Šå‘å¸ƒäº†æ‰€æœ‰æ¨¡å‹æƒé‡ã€è®­ç»ƒä»£ç å’Œæˆ‘ä»¬ç²¾é€‰çš„è®­ç»ƒæ•°æ®é›†ï¼ˆåŒ…æ‹¬2300ä¸‡å¼ å›¾åƒæ–‡æœ¬å¯¹ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23661v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>OpenUniæ˜¯ä¸€ä¸ªç®€å•ã€è½»é‡çº§ã€å®Œå…¨å¼€æºçš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆç»Ÿä¸€åŸºå‡†ã€‚å®ƒé€šè¿‡é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œé‡‡ç”¨ç°æˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ä¸€ç»„å¯å­¦ä¹ çš„æŸ¥è¯¢å’Œè½»é‡çº§åŸºäºå˜å‹å™¨çš„è¿æ¥å™¨ï¼Œä»¥å‡å°è®­ç»ƒå¤æ‚æ€§å’Œå¼€é”€ã€‚OpenUnièƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€æŒ‡ä»¤å¯¹é½çš„å›¾åƒï¼Œå¹¶åœ¨GenEvalã€DPG-Benchå’ŒWISEç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°å“è¶Šæ€§èƒ½ï¼Œä»…éœ€1.1Bå’Œ3.1Bæ¿€æ´»å‚æ•°ã€‚æ‰€æœ‰æ¨¡å‹æƒé‡ã€è®­ç»ƒä»£ç å’Œç²¾é€‰çš„è®­ç»ƒæ•°æ®é›†ï¼ˆåŒ…æ‹¬23Må›¾åƒæ–‡æœ¬å¯¹ï¼‰å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenUniæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„ç»Ÿä¸€åŸºå‡†ã€‚æ—¨åœ¨å°†å„ç§æ¨¡æ€çš„æ•°æ®ï¼ˆå¦‚æ–‡æœ¬å’Œå›¾åƒï¼‰æ•´åˆåœ¨ä¸€èµ·è¿›è¡Œç†è§£å’Œç”Ÿæˆã€‚</li>
<li>OpenUniå…·æœ‰ç®€å•ã€è½»é‡çº§å’Œå¼€æºçš„ç‰¹ç‚¹ï¼Œä¾¿äºç¤¾åŒºç ”ç©¶å’Œå‘å±•ã€‚</li>
<li>é€šè¿‡é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼ŒOpenUnièƒ½æ¡¥æ¥ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>OpenUniä½¿ç”¨å¯å­¦ä¹ çš„æŸ¥è¯¢å’ŒåŸºäºè½»é‡çº§å˜å‹å™¨çš„è¿æ¥å™¨æ¥å®ç°è¿™ä¸€æ¡¥æ¥ï¼Œä»¥é™ä½è®­ç»ƒå¤æ‚æ€§å’Œå¼€é”€ã€‚</li>
<li>OpenUnièƒ½ç”Ÿæˆé«˜è´¨é‡ã€ä¸æŒ‡ä»¤å¯¹é½çš„å›¾åƒï¼Œæ˜¾ç¤ºå‡ºå…¶å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¦‚GenEvalã€DPG-Benchå’ŒWISEï¼ŒOpenUniè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7cd20762b5297aaaa5db3a7bd8a103aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2bbcb0e9d34e13c915cfb04cef56811.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4660fa82c63c1a45604f8604520060af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45d31729f8a2776c5c424dbbd0e69f52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-215343cec7a0cfdac617ff0769b3ee6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cc9a75d3eff5ca34e4fd3e764d149f1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LAFR-Efficient-Diffusion-based-Blind-Face-Restoration-via-Latent-Codebook-Alignment-Adapter"><a href="#LAFR-Efficient-Diffusion-based-Blind-Face-Restoration-via-Latent-Codebook-Alignment-Adapter" class="headerlink" title="LAFR: Efficient Diffusion-based Blind Face Restoration via Latent   Codebook Alignment Adapter"></a>LAFR: Efficient Diffusion-based Blind Face Restoration via Latent   Codebook Alignment Adapter</h2><p><strong>Authors:Runyi Li, Bin Chen, Jian Zhang, Radu Timofte</strong></p>
<p>Blind face restoration from low-quality (LQ) images is a challenging task that requires not only high-fidelity image reconstruction but also the preservation of facial identity. While diffusion models like Stable Diffusion have shown promise in generating high-quality (HQ) images, their VAE modules are typically trained only on HQ data, resulting in semantic misalignment when encoding LQ inputs. This mismatch significantly weakens the effectiveness of LQ conditions during the denoising process. Existing approaches often tackle this issue by retraining the VAE encoder, which is computationally expensive and memory-intensive. To address this limitation efficiently, we propose LAFR (Latent Alignment for Face Restoration), a novel codebook-based latent space adapter that aligns the latent distribution of LQ images with that of HQ counterparts, enabling semantically consistent diffusion sampling without altering the original VAE. To further enhance identity preservation, we introduce a multi-level restoration loss that combines constraints from identity embeddings and facial structural priors. Additionally, by leveraging the inherent structural regularity of facial images, we show that lightweight finetuning of diffusion prior on just 0.9% of FFHQ dataset is sufficient to achieve results comparable to state-of-the-art methods, reduce training time by 70%. Extensive experiments on both synthetic and real-world face restoration benchmarks demonstrate the effectiveness and efficiency of LAFR, achieving high-quality, identity-preserving face reconstruction from severely degraded inputs. </p>
<blockquote>
<p>ä»ä½è´¨é‡ï¼ˆLQï¼‰å›¾åƒä¸­æ¢å¤ç›²è„¸æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä¸ä»…è¦æ±‚é«˜è´¨é‡å›¾åƒé‡å»ºï¼Œè¿˜è¦æ±‚ä¿æŒé¢éƒ¨èº«ä»½ã€‚è™½ç„¶åƒStable Diffusionè¿™æ ·çš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡ï¼ˆHQï¼‰å›¾åƒæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬çš„VAEæ¨¡å—é€šå¸¸ä»…åœ¨HQæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´åœ¨ç¼–ç LQè¾“å…¥æ—¶å‡ºç°è¯­ä¹‰ä¸å¯¹é½ã€‚è¿™ç§ä¸åŒ¹é…ä¼šæ˜¾è‘—å‰Šå¼±å»å™ªè¿‡ç¨‹ä¸­LQæ¡ä»¶çš„æœ‰æ•ˆæ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡é‡æ–°è®­ç»ƒVAEç¼–ç å™¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿™åœ¨è®¡ç®—ä¸Šæ—¢æ˜‚è´µåˆå ç”¨å¤§é‡å†…å­˜ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†LAFRï¼ˆç”¨äºé¢éƒ¨æ¢å¤çš„æ½œåœ¨å¯¹é½ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä»£ç æœ¬çš„æ–°å‹æ½œåœ¨ç©ºé—´é€‚é…å™¨ï¼Œå®ƒå°†LQå›¾åƒçš„æ½œåœ¨åˆ†å¸ƒä¸HQå›¾åƒçš„æ½œåœ¨åˆ†å¸ƒå¯¹é½ï¼Œä»è€Œåœ¨ä¸æ”¹å˜åŸå§‹VAEçš„æƒ…å†µä¸‹å®ç°è¯­ä¹‰ä¸€è‡´çš„æ‰©æ•£é‡‡æ ·ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºèº«ä»½ä¿ç•™ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šçº§æ¢å¤æŸå¤±ï¼Œå®ƒç»“åˆäº†èº«ä»½åµŒå…¥å’Œé¢éƒ¨ç»“æ„å…ˆéªŒçš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨é¢éƒ¨å›¾åƒçš„å†…åœ¨ç»“æ„è§„å¾‹ï¼Œæˆ‘ä»¬è¯æ˜åœ¨FFHQæ•°æ®é›†ä»…0.9%çš„æ•°æ®ä¸Šè¿›è¡Œæ‰©æ•£å…ˆéªŒçš„å¾®è°ƒå°±è¶³ä»¥è¾¾åˆ°ä¸æœ€æ–°æ–¹æ³•ç›¸å½“çš„ç»“æœï¼Œå¹¶å°†è®­ç»ƒæ—¶é—´å‡å°‘70%ã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œé¢éƒ¨æ¢å¤åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†LAFRçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œå®ç°äº†ä»ä¸¥é‡é€€åŒ–çš„è¾“å…¥ä¸­è¿›è¡Œé«˜è´¨é‡ã€ä¿æŒèº«ä»½çš„è„¸éƒ¨é‡å»ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23462v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>é’ˆå¯¹ä½è´¨é‡å›¾åƒä¸­çš„ç›²è„¸æ¢å¤ä»»åŠ¡ï¼Œå­˜åœ¨é«˜ä¿çœŸé‡å»ºä¸é¢éƒ¨èº«ä»½ä¿ç•™çš„åŒé‡æŒ‘æˆ˜ã€‚è™½ç„¶Stable Diffusionç­‰æ‰©æ•£æ¨¡å‹åœ¨é«˜è´¨é‡å›¾åƒç”Ÿæˆä¸­è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†å…¶VAEæ¨¡å—ä»…åœ¨é«˜è´¨é‡æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´åœ¨ç¼–ç ä½è´¨é‡è¾“å…¥æ—¶å‡ºç°è¯­ä¹‰ä¸åŒ¹é…é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºLAFRï¼ˆé¢éƒ¨æ¢å¤çš„æ½œåœ¨å¯¹é½ï¼‰æ–¹æ³•ï¼Œé€šè¿‡åŸºäºä»£ç æœ¬çš„æ½œåœ¨ç©ºé—´é€‚é…å™¨ï¼Œå¯¹é½ä½è´¨é‡å›¾åƒä¸é«˜è´¨é‡å›¾åƒçš„æ½œåœ¨åˆ†å¸ƒï¼Œå®ç°è¯­ä¹‰ä¸€è‡´çš„æ‰©æ•£é‡‡æ ·ï¼Œä¸”æ— éœ€æ›´æ”¹åŸå§‹VAEã€‚ä¸ºè¿›ä¸€æ­¥æé«˜èº«ä»½ä¿ç•™æ•ˆæœï¼Œå¼•å…¥å¤šçº§æ¢å¤æŸå¤±ï¼Œç»“åˆèº«ä»½åµŒå…¥å’Œé¢éƒ¨ç»“æ„å…ˆéªŒçš„é™åˆ¶ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å°‘é‡æ•°æ®é›†ä¸Šå¾®è°ƒæ‰©æ•£å…ˆéªŒå³å¯å®ç°ä¸å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ•ˆæœï¼Œå¹¶å‡å°‘70%çš„è®­ç»ƒæ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¦‚Stable Diffusionåœ¨ä½è´¨é‡å›¾åƒç›²è„¸æ¢å¤ä¸Šåº”ç”¨é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹çš„VAEæ¨¡å—ä»…åœ¨é«˜è´¨é‡æ•°æ®ä¸Šè®­ç»ƒï¼Œå¯¼è‡´ä½è´¨é‡è¾“å…¥æ—¶çš„è¯­ä¹‰ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>LAFRæ–¹æ³•é€šè¿‡åŸºäºä»£ç æœ¬çš„æ½œåœ¨ç©ºé—´é€‚é…å™¨å¯¹é½ä½è´¨é‡ä¸é«˜è´¨é‡å›¾åƒçš„æ½œåœ¨åˆ†å¸ƒã€‚</li>
<li>LAFRå®ç°äº†è¯­ä¹‰ä¸€è‡´çš„æ‰©æ•£é‡‡æ ·ï¼Œä¸”æ— éœ€æ›´æ”¹åŸå§‹VAEè®¾è®¡ã€‚</li>
<li>å¼•å…¥å¤šçº§æ¢å¤æŸå¤±ä»¥æé«˜èº«ä»½ä¿ç•™æ•ˆæœï¼Œç»“åˆèº«ä»½åµŒå…¥å’Œé¢éƒ¨ç»“æ„å…ˆéªŒã€‚</li>
<li>åˆ©ç”¨é¢éƒ¨å›¾åƒçš„ç»“æ„è§„å¾‹æ€§ï¼Œå°‘é‡æ•°æ®é›†ä¸Šçš„æ‰©æ•£å…ˆéªŒå¾®è°ƒå³å¯å®ç°é«˜æ•ˆã€é«˜è´¨é‡çš„é¢éƒ¨é‡å»ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23462">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c95342d5ce246371a50724168ec94137.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa300c0ed08360fc9ae9a8557c55db84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0f7135a2f3b2f948f73a55e750d199f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7d760b621fe10b5a655784987fc1eba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e42b5a939afb8f6f7931d5f75dbd7afd.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CryoCCD-Conditional-Cycle-consistent-Diffusion-with-Biophysical-Modeling-for-Cryo-EM-Synthesis"><a href="#CryoCCD-Conditional-Cycle-consistent-Diffusion-with-Biophysical-Modeling-for-Cryo-EM-Synthesis" class="headerlink" title="CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical   Modeling for Cryo-EM Synthesis"></a>CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical   Modeling for Cryo-EM Synthesis</h2><p><strong>Authors:Runmin Jiang, Genpei Zhang, Yuntian Yang, Siqi Wu, Yuheng Zhang, Wanyue Feng, Yizhou Zhao, Xi Xiao, Xiao Wang, Tianyang Wang, Xingjian Li, Min Xu</strong></p>
<p>Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of macromolecules, but developing robust models for downstream analysis is hindered by the scarcity of high-quality annotated data. While synthetic data generation has emerged as a potential solution, existing methods often fail to capture both the structural diversity of biological specimens and the complex, spatially varying noise inherent in cryo-EM imaging. To overcome these limitations, we propose CryoCCD, a synthesis framework that integrates biophysical modeling with generative techniques. Specifically, CryoCCD produces multi-scale cryo-EM micrographs that reflect realistic biophysical variability through compositional heterogeneity, cellular context, and physics-informed imaging. To generate realistic noise, we employ a conditional diffusion model, enhanced by cycle consistency to preserve structural fidelity and mask-aware contrastive learning to capture spatially adaptive noise patterns. Extensive experiments show that CryoCCD generates structurally accurate micrographs and enhances performance in downstream tasks, outperforming state-of-the-art baselines in both particle picking and reconstruction. </p>
<blockquote>
<p>å†·å†»ç”µå­æ˜¾å¾®é•œï¼ˆcryo-EMï¼‰èƒ½å¤Ÿä¸ºå®è§‚åˆ†å­æä¾›æ¥è¿‘åŸå­åˆ†è¾¨ç‡çš„æˆåƒï¼Œä½†æ˜¯é«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºé˜»ç¢äº†ä¸‹æ¸¸åˆ†æçš„ç¨³å¥æ¨¡å‹çš„å‘å±•ã€‚è™½ç„¶åˆæˆæ•°æ®ç”Ÿæˆå·²ç»å‡ºç°ä½œä¸ºæ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æ•æ‰åˆ°ç”Ÿç‰©æ ·æœ¬çš„ç»“æ„å¤šæ ·æ€§å’Œå†·å†»ç”µå­æ˜¾å¾®é•œæˆåƒä¸­å›ºæœ‰çš„å¤æ‚ä¸”ç©ºé—´å˜åŒ–çš„å™ªå£°ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†CryoCCDï¼Œè¿™æ˜¯ä¸€ä¸ªå°†ç”Ÿç‰©ç‰©ç†å»ºæ¨¡ä¸ç”ŸæˆæŠ€æœ¯ç›¸ç»“åˆçš„åˆæˆæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒCryoCCDäº§ç”Ÿå¤šå°ºåº¦çš„å†·å†»ç”µå­æ˜¾å¾®é•œæ˜¾å¾®å›¾ï¼Œé€šè¿‡ç»„æˆå¼‚è´¨æ€§ã€ç»†èƒèƒŒæ™¯å’Œç‰©ç†æˆåƒåæ˜ çœŸå®çš„ç”Ÿç‰©ç‰©ç†å˜åŒ–ã€‚ä¸ºäº†äº§ç”ŸçœŸå®çš„å™ªå£°ï¼Œæˆ‘ä»¬é‡‡ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¾ªç¯ä¸€è‡´æ€§å¢å¼ºæ¥ä¿æŒç»“æ„å¿ å®åº¦ï¼Œå¹¶é€šè¿‡æ©è†œæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ æ¥æ•æ‰ç©ºé—´è‡ªé€‚åº”å™ªå£°æ¨¡å¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCryoCCDç”Ÿæˆçš„æ˜¾å¾®å›¾ç»“æ„å‡†ç¡®ï¼Œæé«˜äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œåœ¨ç²’å­æŒ‘é€‰å’Œé‡å»ºæ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23444v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå†·å†»ç”µå­æ˜¾å¾®é•œï¼ˆcryo-EMï¼‰çš„è¿‘åŸå­åˆ†è¾¨ç‡æˆåƒï¼Œåœ¨ä¸‹æ¸¸åˆ†æä¸­ç¼ºä¹é«˜è´¨é‡æ³¨é‡Šæ•°æ®é˜»ç¢å…¶å‘å±•çš„èƒŒæ™¯ä¸‹ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªæ–°çš„åˆæˆæ•°æ®ç”Ÿæˆæ¡†æ¶CryoCCDã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç”Ÿç‰©ç‰©ç†å»ºæ¨¡å’Œç”ŸæˆæŠ€æœ¯ï¼Œæ—¨åœ¨å…‹æœç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚é€šè¿‡æ„å»ºå¤šå°ºåº¦cryo-EMå¾®å›¾ï¼ŒCryoCCDèƒ½å¤Ÿåæ˜ çœŸå®çš„ç”Ÿç‰©ç‰©ç†å˜å¼‚æ€§å’Œå¤æ‚çš„ç©ºé—´å˜åŒ–å™ªå£°ã€‚é‡‡ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”ŸæˆçœŸå®å™ªå£°ï¼Œé€šè¿‡å¾ªç¯ä¸€è‡´æ€§å¢å¼ºç»“æ„ä¿çœŸåº¦ï¼Œå¹¶é€šè¿‡æ©è†œæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ æ•æ‰ç©ºé—´è‡ªé€‚åº”å™ªå£°æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒCryoCCDç”Ÿæˆçš„å¾®å›¾ç»“æ„å‡†ç¡®ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨ç²’å­æŒ‘é€‰å’Œé‡å»ºæ–¹é¢å‡ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å†·å†»ç”µå­æ˜¾å¾®é•œï¼ˆcryo-EMï¼‰æˆåƒå› ç¼ºä¹é«˜è´¨é‡æ³¨é‡Šæ•°æ®è€Œåœ¨ä¸‹æ¸¸åˆ†æä¸­å—é™ã€‚</li>
<li>ç°æœ‰çš„æ•°æ®ç”Ÿæˆæ–¹æ³•éš¾ä»¥æ•æ‰ç”Ÿç‰©æ ·æœ¬çš„ç»“æ„å¤šæ ·æ€§å’Œå¤æ‚çš„ç©ºé—´å˜åŒ–å™ªå£°ã€‚</li>
<li>æå‡ºæ–°çš„åˆæˆæ•°æ®ç”Ÿæˆæ¡†æ¶CryoCCDï¼Œç»“åˆç”Ÿç‰©ç‰©ç†å»ºæ¨¡å’Œç”ŸæˆæŠ€æœ¯ã€‚</li>
<li>CryoCCDèƒ½å¤Ÿç”Ÿæˆå¤šå°ºåº¦cryo-EMå¾®å›¾ï¼Œåæ˜ çœŸå®çš„ç”Ÿç‰©ç‰©ç†å˜å¼‚æ€§å’Œç©ºé—´å˜åŒ–å™ªå£°ã€‚</li>
<li>é‡‡ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”ŸæˆçœŸå®å™ªå£°ï¼Œé€šè¿‡å¾ªç¯ä¸€è‡´æ€§å’Œæ©è†œæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ æé«˜æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜CryoCCDç”Ÿæˆçš„å¾®å›¾ç»“æ„å‡†ç¡®ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf2202c72597ebe5476c2f43d610fa8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74332f918f3356e693e9226dded8535c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-838fdc5db52adab5246cb64cf7868c88.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TRACE-Trajectory-Constrained-Concept-Erasure-in-Diffusion-Models"><a href="#TRACE-Trajectory-Constrained-Concept-Erasure-in-Diffusion-Models" class="headerlink" title="TRACE: Trajectory-Constrained Concept Erasure in Diffusion Models"></a>TRACE: Trajectory-Constrained Concept Erasure in Diffusion Models</h2><p><strong>Authors:Finn Carter</strong></p>
<p>Text-to-image diffusion models have shown unprecedented generative capability, but their ability to produce undesirable concepts (e.g.<del>pornographic content, sensitive identities, copyrighted styles) poses serious concerns for privacy, fairness, and safety. {Concept erasure} aims to remove or suppress specific concept information in a generative model. In this paper, we introduce \textbf{TRACE (Trajectory-Constrained Attentional Concept Erasure)}, a novel method to erase targeted concepts from diffusion models while preserving overall generative quality. Our approach combines a rigorous theoretical framework, establishing formal conditions under which a concept can be provably suppressed in the diffusion process, with an effective fine-tuning procedure compatible with both conventional latent diffusion (Stable Diffusion) and emerging rectified flow models (e.g.</del>FLUX). We first derive a closed-form update to the modelâ€™s cross-attention layers that removes hidden representations of the target concept. We then introduce a trajectory-aware finetuning objective that steers the denoising process away from the concept only in the late sampling stages, thus maintaining the modelâ€™s fidelity on unrelated content. Empirically, we evaluate TRACE on multiple benchmarks used in prior concept erasure studies (object classes, celebrity faces, artistic styles, and explicit content from the I2P dataset). TRACE achieves state-of-the-art performance, outperforming recent methods such as ANT, EraseAnything, and MACE in terms of removal efficacy and output quality. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å±•ç°å‡ºäº†å‰æ‰€æœªæœ‰çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å®ƒä»¬äº§ç”Ÿä¸æƒ³è¦æ¦‚å¿µçš„èƒ½åŠ›ï¼ˆä¾‹å¦‚è‰²æƒ…å†…å®¹ã€æ•æ„Ÿèº«ä»½ã€ç‰ˆæƒé£æ ¼ï¼‰å¯¹éšç§ã€å…¬å¹³å’Œå®‰å…¨æå‡ºäº†ä¸¥é‡æ‹…å¿§ã€‚{æ¦‚å¿µæ“¦é™¤}æ—¨åœ¨ä»ç”Ÿæˆæ¨¡å‹ä¸­åˆ é™¤æˆ–æŠ‘åˆ¶ç‰¹å®šæ¦‚å¿µä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†\textbf{TRACEï¼ˆè½¨è¿¹çº¦æŸæ³¨æ„åŠ›æ¦‚å¿µæ“¦é™¤ï¼‰}ï¼Œè¿™æ˜¯ä¸€ç§ä»æ‰©æ•£æ¨¡å‹ä¸­åˆ é™¤ç›®æ ‡æ¦‚å¿µçš„åŒæ—¶ä¿æŒæ•´ä½“ç”Ÿæˆè´¨é‡çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä¸€ä¸ªä¸¥è°¨çš„ç†è®ºæ¡†æ¶ï¼Œå»ºç«‹äº†åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å¯ä»¥æŠ‘åˆ¶æ¦‚å¿µçš„æ­£å¼æ¡ä»¶ï¼Œä»¥åŠä¸€ä¸ªä¸å¸¸è§„æ½œåœ¨æ‰©æ•£ï¼ˆStable Diffusionï¼‰å’Œæ–°å…´æ•´æµæµæ¨¡å‹ï¼ˆä¾‹å¦‚FLUXï¼‰å…¼å®¹çš„æœ‰æ•ˆå¾®è°ƒç¨‹åºã€‚æˆ‘ä»¬é¦–å…ˆæ¨å¯¼å‡ºæ¨¡å‹äº¤å‰æ³¨æ„åŠ›å±‚çš„å°é—­å½¢å¼æ›´æ–°ï¼Œä»¥æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µçš„éšè—è¡¨ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè½¨è¿¹æ„ŸçŸ¥å¾®è°ƒç›®æ ‡ï¼Œè¯¥ç›®æ ‡ä»…åœ¨åæœŸé‡‡æ ·é˜¶æ®µå¼•å¯¼å»å™ªè¿‡ç¨‹è¿œç¦»æ¦‚å¿µï¼Œä»è€Œä¿æŒæ¨¡å‹åœ¨å¤„ç†ä¸ç›¸å…³å†…å®¹æ—¶çš„ä¿çœŸåº¦ã€‚ä»å®è¯è§’åº¦çœ‹ï¼Œæˆ‘ä»¬åœ¨å…ˆå‰æ¦‚å¿µæ“¦é™¤ç ”ç©¶ä½¿ç”¨çš„å¤šä¸ªåŸºå‡†æµ‹è¯•ï¼ˆå¦‚å¯¹è±¡ç±»åˆ«ã€åäººé¢å­”ã€è‰ºæœ¯é£æ ¼å’ŒI2Pæ•°æ®é›†ä¸­çš„æ˜ç¡®å†…å®¹ï¼‰ä¸Šå¯¹TRACEè¿›è¡Œäº†è¯„ä¼°ã€‚TRACEè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œåœ¨å»é™¤æ•ˆæœå’Œè¾“å‡ºè´¨é‡æ–¹é¢è¶…è¶Šäº†è¿‘æœŸçš„æ–¹æ³•ï¼Œå¦‚ANTã€EraseAnythingå’ŒMACEã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23312v1">PDF</a> In peer review</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å‰æ‰€æœªæœ‰ï¼Œä½†å…¶ç”Ÿæˆä¸å¸Œæœ›å‡ºç°æ¦‚å¿µï¼ˆå¦‚è‰²æƒ…å†…å®¹ã€æ•æ„Ÿèº«ä»½ã€ç‰ˆæƒé£æ ¼ï¼‰çš„èƒ½åŠ›å¯¹éšç§ã€å…¬å¹³æ€§å’Œå®‰å…¨æ„æˆä¸¥é‡å¨èƒã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTRACEï¼ˆè½¨è¿¹çº¦æŸæ³¨æ„åŠ›æ¦‚å¿µæ¶ˆé™¤ï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä»æ‰©æ•£æ¨¡å‹ä¸­åˆ é™¤ç›®æ ‡æ¦‚å¿µï¼ŒåŒæ—¶ä¿æŒæ•´ä½“ç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ä¸¥æ ¼çš„ç†è®ºæ¡†æ¶å’Œæœ‰æ•ˆçš„å¾®è°ƒç¨‹åºï¼Œå¯ä¸ä¼ ç»Ÿçš„æ½œåœ¨æ‰©æ•£ï¼ˆStable Diffusionï¼‰å’Œæ–°å…´çš„æ ¡æ­£æµæ¨¡å‹ï¼ˆå¦‚FLUXï¼‰å…¼å®¹ã€‚å®éªŒè¡¨æ˜ï¼ŒTRACEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¾¾åˆ°æˆ–è¶…è¶Šäº†æœ€æ–°çš„æ–¹æ³•ï¼Œå¦‚ANTã€EraseAnythingå’ŒMACEã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å­˜åœ¨ç”Ÿæˆä¸å¸Œæœ›å‡ºç°æ¦‚å¿µçš„é—®é¢˜ã€‚</li>
<li>æ¦‚å¿µæ¶ˆé™¤æŠ€æœ¯å¯¹äºä¿æŠ¤éšç§ã€å…¬å¹³æ€§å’Œå®‰å…¨è‡³å…³é‡è¦ã€‚</li>
<li>TRACEæ˜¯ä¸€ç§æ–°å‹çš„æ¦‚å¿µæ¶ˆé™¤æ–¹æ³•ï¼Œæ—¨åœ¨ä»æ‰©æ•£æ¨¡å‹ä¸­åˆ é™¤ç‰¹å®šæ¦‚å¿µï¼ŒåŒæ—¶ä¿æŒæ•´ä½“ç”Ÿæˆè´¨é‡ã€‚</li>
<li>TRACEç»“åˆäº†ä¸¥æ ¼çš„ç†è®ºæ¡†æ¶å’Œæœ‰æ•ˆçš„å¾®è°ƒç¨‹åºï¼Œå¯ä¸å¤šç§æ‰©æ•£æ¨¡å‹å…¼å®¹ã€‚</li>
<li>TRACEé€šè¿‡ä¿®æ”¹æ¨¡å‹çš„è·¨æ³¨æ„å±‚æ¥æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µçš„éšè—è¡¨ç¤ºã€‚</li>
<li>TRACEå¼•å…¥äº†è½¨è¿¹æ„ŸçŸ¥å¾®è°ƒç›®æ ‡ï¼Œä»…åœ¨åæœŸé‡‡æ ·é˜¶æ®µé¿å…æ¦‚å¿µï¼Œä»è€Œä¿æŒæ¨¡å‹å¯¹æ— å…³å†…å®¹çš„ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4828790ec6a5c687a4734a793bc18a0a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RSFAKE-1M-A-Large-Scale-Dataset-for-Detecting-Diffusion-Generated-Remote-Sensing-Forgeries"><a href="#RSFAKE-1M-A-Large-Scale-Dataset-for-Detecting-Diffusion-Generated-Remote-Sensing-Forgeries" class="headerlink" title="RSFAKE-1M: A Large-Scale Dataset for Detecting Diffusion-Generated   Remote Sensing Forgeries"></a>RSFAKE-1M: A Large-Scale Dataset for Detecting Diffusion-Generated   Remote Sensing Forgeries</h2><p><strong>Authors:Zhihong Tan, Jiayi Wang, Huiying Shi, Binyuan Huang, Hongchen Wei, Zhenzhong Chen</strong></p>
<p>Detecting forged remote sensing images is becoming increasingly critical, as such imagery plays a vital role in environmental monitoring, urban planning, and national security. While diffusion models have emerged as the dominant paradigm for image generation, their impact on remote sensing forgery detection remains underexplored. Existing benchmarks primarily target GAN-based forgeries or focus on natural images, limiting progress in this critical domain. To address this gap, we introduce RSFAKE-1M, a large-scale dataset of 500K forged and 500K real remote sensing images. The fake images are generated by ten diffusion models fine-tuned on remote sensing data, covering six generation conditions such as text prompts, structural guidance, and inpainting. This paper presents the construction of RSFAKE-1M along with a comprehensive experimental evaluation using both existing detectors and unified baselines. The results reveal that diffusion-based remote sensing forgeries remain challenging for current methods, and that models trained on RSFAKE-1M exhibit notably improved generalization and robustness. Our findings underscore the importance of RSFAKE-1M as a foundation for developing and evaluating next-generation forgery detection approaches in the remote sensing domain. The dataset and other supplementary materials are available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/TZHSW/RSFAKE/">https://huggingface.co/datasets/TZHSW/RSFAKE/</a>. </p>
<blockquote>
<p>é¥æ„Ÿå›¾åƒä¼ªé€ æ£€æµ‹åœ¨ç¯å¢ƒç›‘æ§ã€åŸå¸‚è§„åˆ’å’Œå›½å®¶å®‰å…¨ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå› æ­¤å˜å¾—è¶Šæ¥è¶Šå…³é”®ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹å·²æˆä¸ºå›¾åƒç”Ÿæˆçš„ä¸»å¯¼èŒƒå¼ï¼Œä½†å®ƒä»¬å¯¹é¥æ„Ÿä¼ªé€ æ£€æµ‹çš„å½±å“ä»è¢«æ¢ç´¢ä¸è¶³ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é¢å‘åŸºäºGANçš„ä¼ªé€ æˆ–è‡ªç„¶å›¾åƒï¼Œé™åˆ¶äº†è¿™ä¸€å…³é”®é¢†åŸŸçš„è¿›å±•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RSFAKE-1Mï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«50ä¸‡å¼ ä¼ªé€ å’Œ50ä¸‡å¼ çœŸå®é¥æ„Ÿå›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚ä¼ªé€ å›¾åƒæ˜¯ç”±åä¸ªåœ¨é¥æ„Ÿæ•°æ®ä¸Šå¾®è°ƒè¿‡çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„ï¼Œæ¶µç›–äº†æ–‡æœ¬æç¤ºã€ç»“æ„å¼•å¯¼å’Œå›¾åƒè¡¥å…¨ç­‰å…­ç§ç”Ÿæˆæ¡ä»¶ã€‚æœ¬æ–‡ä»‹ç»äº†RSFAKE-1Mçš„æ„å»ºï¼Œä»¥åŠä½¿ç”¨ç°æœ‰æ£€æµ‹å™¨å’Œç»Ÿä¸€åŸºçº¿è¿›è¡Œçš„å…¨é¢å®éªŒè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ‰©æ•£çš„é¥æ„Ÿä¼ªé€ å¯¹å½“å‰æ–¹æ³•ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè€Œåœ¨RSFAKE-1Mä¸Šè®­ç»ƒçš„æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—çš„æé«˜æ¨å¹¿èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†RSFAKE-1Måœ¨é¥æ„Ÿé¢†åŸŸå¼€å‘ä¸‹ä¸€ä»£ä¼ªé€ æ£€æµ‹æ–¹æ³•çš„è¯„ä¼°åŸºç¡€æ–¹é¢çš„é‡è¦æ€§ã€‚æ•°æ®é›†å’Œå…¶ä»–è¾…åŠ©ææ–™å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/TZHSW/RSFAKE/%E6%89%BE%E5%88%B0%E3%80%82">https://huggingface.co/datasets/TZHSW/RSFAKE/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23283v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†RSFAKE-1Mæ•°æ®é›†çš„å»ºè®¾ï¼Œè¯¥æ•°æ®é›†åŒ…å«50ä¸‡å¼ ä¼ªé€ çš„å’ŒçœŸå®çš„é¥æ„Ÿå›¾åƒã€‚è¯¥æ•°æ®é›†çš„ä¼ªé€ å›¾åƒç”±ç»è¿‡é¥æ„Ÿæ•°æ®å¾®è°ƒåçš„åä¸ªæ‰©æ•£æ¨¡å‹ç”Ÿæˆï¼Œæ¶µç›–æ–‡æœ¬æç¤ºã€ç»“æ„æŒ‡å¯¼ç­‰å…­ç§ç”Ÿæˆæ¡ä»¶ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒåŸºäºæ‰©æ•£çš„é¥æ„Ÿä¼ªé€ å›¾åƒå¯¹å½“å‰æ–¹æ³•ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè€Œåœ¨RSFAKE-1Mä¸Šè®­ç»ƒçš„æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—çš„æé«˜æ¨å¹¿å’Œç¨³å¥æ€§ã€‚è¯¥æ•°æ®é›†å¯¹å¼€å‘ä¸‹ä¸€ä»£é¥æ„Ÿé¢†åŸŸä¼ªé€ æ£€æµ‹æŠ€æœ¯è‡³å…³é‡è¦ã€‚è¯¥æ•°æ®é›†å¯é€šè¿‡[<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/TZHSW/RSFAKE/]%E8%8E%B7%E5%8F%96%E3%80%82">https://huggingface.co/datasets/TZHSW/RSFAKE/]è·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RSFAKE-1Mæ•°æ®é›†åŒ…å«å¤§é‡çš„ä¼ªé€ å’ŒçœŸå®é¥æ„Ÿå›¾åƒï¼Œä¸ºé¥æ„Ÿä¼ªé€ æ£€æµ‹ç ”ç©¶æä¾›äº†ä¸°å¯Œçš„æ•°æ®èµ„æºã€‚</li>
<li>æ•°æ®é›†ä¸­çš„ä¼ªé€ å›¾åƒç”±å¤šç§æ‰©æ•£æ¨¡å‹ç”Ÿæˆï¼Œæ¶µç›–ä¸åŒçš„ç”Ÿæˆæ¡ä»¶ï¼Œå¢åŠ äº†ç ”ç©¶çš„å¤æ‚æ€§ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨æ£€æµ‹åŸºäºæ‰©æ•£çš„é¥æ„Ÿä¼ªé€ å›¾åƒæ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>åœ¨RSFAKE-1Mä¸Šè®­ç»ƒçš„æ¨¡å‹è¡¨ç°å‡ºæ›´å¥½çš„æ¨å¹¿å’Œç¨³å¥æ€§ã€‚</li>
<li>RSFAKE-1Mæ•°æ®é›†å¯¹å¼€å‘ä¸‹ä¸€ä»£é¥æ„Ÿé¢†åŸŸä¼ªé€ æ£€æµ‹æŠ€æœ¯å…·æœ‰å…³é”®ä½œç”¨ã€‚</li>
<li>è¯¥æ•°æ®é›†ä¿ƒè¿›äº†é¥æ„Ÿå›¾åƒä¼ªé€ æ£€æµ‹é¢†åŸŸçš„ç ”ç©¶è¿›å±•å’Œå®é™…åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23283">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fcf9ee016cc783e357f9fe38af0ef372.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfb9ce74855223095a0d882f89f7ff25.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c8e51e35a264877f46aba459f5c70e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55fcd9754e53e39916f464b05e214791.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80bc005d2c97351d00017fc91b89f6a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-386c0be5edd59589036da21bc90e72f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51ee256db155cf5e8d2352a2d71ca106.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Image-Aesthetic-Reasoning-A-New-Benchmark-for-Medical-Image-Screening-with-MLLMs"><a href="#Image-Aesthetic-Reasoning-A-New-Benchmark-for-Medical-Image-Screening-with-MLLMs" class="headerlink" title="Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening   with MLLMs"></a>Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening   with MLLMs</h2><p><strong>Authors:Zheng Sun, Yi Wei, Long Yu</strong></p>
<p>Multimodal Large Language Models (MLLMs) are of great application across many domains, such as multimodal understanding and generation. With the development of diffusion models (DM) and unified MLLMs, the performance of image generation has been significantly improved, however, the study of image screening is rare and its performance with MLLMs is unsatisfactory due to the lack of data and the week image aesthetic reasoning ability in MLLMs. In this work, we propose a complete solution to address these problems in terms of data and methodology. For data, we collect a comprehensive medical image screening dataset with 1500+ samples, each sample consists of a medical image, four generated images, and a multiple-choice answer. The dataset evaluates the aesthetic reasoning ability under four aspects: \textit{(1) Appearance Deformation, (2) Principles of Physical Lighting and Shadow, (3) Placement Layout, (4) Extension Rationality}. For methodology, we utilize long chains of thought (CoT) and Group Relative Policy Optimization with Dynamic Proportional Accuracy reward, called DPA-GRPO, to enhance the image aesthetic reasoning ability of MLLMs. Our experimental results reveal that even state-of-the-art closed-source MLLMs, such as GPT-4o and Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic reasoning. In contrast, by leveraging the reinforcement learning approach, we are able to surpass the score of both large-scale models and leading closed-source models using a much smaller model. We hope our attempt on medical image screening will serve as a regular configuration in image aesthetic reasoning in the future. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¼—å¤šé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œå¦‚å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚éšç€æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å’Œç»Ÿä¸€MLLMsçš„å‘å±•ï¼Œå›¾åƒç”Ÿæˆçš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚ç„¶è€Œï¼Œå›¾åƒç­›é€‰çš„ç ”ç©¶å¾ˆå°‘ï¼Œå…¶ä¸MLLMsçš„æ€§èƒ½ä¹Ÿä¸å°½å¦‚äººæ„ï¼Œè¿™æ˜¯ç”±äºæ•°æ®ç¼ºä¹ä»¥åŠMLLMsä¸­å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›çš„è–„å¼±ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹è¿™äº›é—®é¢˜åœ¨æ•°æ®å’Œæ–¹æ³•ä¸Šæå‡ºäº†å…¨é¢çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨æ•°æ®æ–¹é¢ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªå…¨é¢çš„åŒ»å­¦å›¾åƒç­›é€‰æ•°æ®é›†ï¼ŒåŒ…å«1500å¤šä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬ç”±ä¸€å¼ åŒ»å­¦å›¾åƒã€å››å¼ ç”Ÿæˆçš„å›¾åƒå’Œå¤šä¸ªé€‰æ‹©é¢˜ç»„æˆã€‚è¯¥æ•°æ®é›†ä»ä»¥ä¸‹å››ä¸ªæ–¹é¢è¯„ä¼°ç¾å­¦æ¨ç†èƒ½åŠ›ï¼šï¼ˆ1ï¼‰å¤–è§‚å˜å½¢ï¼›ï¼ˆ2ï¼‰ç‰©ç†ç…§æ˜å’Œé˜´å½±åŸåˆ™ï¼›ï¼ˆ3ï¼‰æ”¾ç½®å¸ƒå±€ï¼›ï¼ˆ4ï¼‰æ‰©å±•åˆç†æ€§ã€‚åœ¨æ–¹æ³•ä¸Šï¼Œæˆ‘ä»¬åˆ©ç”¨é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰å’ŒåŠ¨æ€æ¯”ä¾‹ç²¾åº¦å¥–åŠ±çš„é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆç§°ä¸ºDPA-GRPOï¼‰ï¼Œä»¥æé«˜MLLMsçš„å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„é—­æºMLLMsï¼Œå¦‚GPT-4oå’ŒQwen-VL-Maxï¼Œåœ¨å›¾åƒç¾å­¦æ¨ç†æ–¹é¢çš„è¡¨ç°ä¹Ÿä¸éšæœºçŒœæµ‹æ— å¼‚ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé€šè¿‡åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨æ¨¡å‹è§„æ¨¡è¾ƒå°çš„æƒ…å†µä¸‹è¶…è¶Šè¿™ä¸¤ä¸ªå¤§è§„æ¨¡æ¨¡å‹çš„å¾—åˆ†ï¼Œå¹¶é¢†å…ˆå…¶ä»–é—­æºæ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„åŒ»å­¦å›¾åƒç­›é€‰å°è¯•æœªæ¥èƒ½åœ¨å›¾åƒç¾å­¦æ¨ç†ä¸­æˆä¸ºä¸€ç§å¸¸è§„é…ç½®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23265v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼Œå¦‚å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å’Œç»Ÿä¸€MLLMsçš„å‘å±•å·²æ˜¾è‘—æ”¹å–„å›¾åƒç”Ÿæˆæ€§èƒ½ï¼Œä½†å›¾åƒç­›é€‰ç ”ç©¶ç¨€å°‘ï¼Œå…¶ä¸MLLMsçš„æ€§èƒ½ä¹Ÿä¸å°½äººæ„ï¼Œä¸»è¦ç”±äºæ•°æ®ç¼ºä¹ä»¥åŠMLLMsä¸­å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›è–„å¼±ã€‚æœ¬ç ”ç©¶æå‡ºé’ˆå¯¹è¿™äº›é—®é¢˜åœ¨æ•°æ®å’Œæ–¹æ³•ä¸Šçš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚åœ¨æ•°æ®æ–¹é¢ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªç»¼åˆåŒ»ç–—å›¾åƒç­›é€‰æ•°æ®é›†ï¼ŒåŒ…å«1500å¤šä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…æ‹¬åŒ»ç–—å›¾åƒã€å››ä¸ªç”Ÿæˆå›¾åƒå’Œå¤šä¸ªé€‰æ‹©é¢˜ã€‚è¯¥æ•°æ®é›†è¯„ä¼°äº†ç¾å­¦æ¨ç†èƒ½åŠ›çš„å››ä¸ªæ–¹é¢ï¼šå¤–è§‚å˜å½¢ã€ç‰©ç†ç…§æ˜å’Œé˜´å½±åŸåˆ™ã€æ”¾ç½®å¸ƒå±€ã€æ‰©å±•åˆç†æ€§ã€‚åœ¨æ–¹æ³•ä¸Šï¼Œæˆ‘ä»¬åˆ©ç”¨é•¿é“¾æ€ç»´å’Œå¸¦æœ‰åŠ¨æ€æ¯”ä¾‹ç²¾åº¦å¥–åŠ±çš„é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆDPA-GRPOï¼‰ï¼Œæé«˜MLLMsçš„å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„é—­æºMLLMsï¼Œå¦‚GPT-4oå’ŒQwen-VL-Maxï¼Œåœ¨å›¾åƒç¾å­¦æ¨ç†æ–¹é¢çš„è¡¨ç°ä¹Ÿå¦‚åŒéšæœºçŒœæµ‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé€šè¿‡åˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œå³ä½¿ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½å¤Ÿè¶…è¶Šå¤§å‹æ¨¡å‹çš„å¾—åˆ†ï¼Œè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„åŒ»ç–—å›¾åƒç­›é€‰å°è¯•èƒ½ä¸ºæœªæ¥çš„å›¾åƒç¾å­¦æ¨ç†æä¾›å¸¸è§„é…ç½®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å›¾åƒç­›é€‰æ–¹é¢çš„åº”ç”¨ä¸å°½äººæ„ã€‚</li>
<li>æ•°æ®ç¼ºä¹å’ŒMLLMsä¸­å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›è–„å¼±æ˜¯ä¸»è¦åŸå› ã€‚</li>
<li>æå‡ºä¸€ä¸ªç»¼åˆåŒ»ç–—å›¾åƒç­›é€‰æ•°æ®é›†ï¼ŒåŒ…å«å¤šæ–¹é¢çš„ç¾å­¦æ¨ç†èƒ½åŠ›è¯„ä¼°ã€‚</li>
<li>é‡‡ç”¨é•¿é“¾æ€ç»´å’Œå¸¦æœ‰åŠ¨æ€æ¯”ä¾‹ç²¾åº¦å¥–åŠ±çš„é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆDPA-GRPOï¼‰æ–¹æ³•æé«˜MLLMsçš„å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æœ€å…ˆè¿›çš„é—­æºMLLMsåœ¨å›¾åƒç¾å­¦æ¨ç†æ–¹é¢çš„è¡¨ç°æœ‰é™ã€‚</li>
<li>åˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•èƒ½å¤Ÿåœ¨è¾ƒå°çš„æ¨¡å‹ä¸Šè¶…è¶Šå¤§å‹æ¨¡å‹çš„å¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8d19a4c72639245ea283e992fd20ca2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94f800422ed87ab8d8f6aeab3fe7c84d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b6950cabc4284cd495fc92aaacd877a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GeoMan-Temporally-Consistent-Human-Geometry-Estimation-using-Image-to-Video-Diffusion"><a href="#GeoMan-Temporally-Consistent-Human-Geometry-Estimation-using-Image-to-Video-Diffusion" class="headerlink" title="GeoMan: Temporally Consistent Human Geometry Estimation using   Image-to-Video Diffusion"></a>GeoMan: Temporally Consistent Human Geometry Estimation using   Image-to-Video Diffusion</h2><p><strong>Authors:Gwanghyun Kim, Xueting Li, Ye Yuan, Koki Nagano, Tianye Li, Jan Kautz, Se Young Chun, Umar Iqbal</strong></p>
<p>Estimating accurate and temporally consistent 3D human geometry from videos is a challenging problem in computer vision. Existing methods, primarily optimized for single images, often suffer from temporal inconsistencies and fail to capture fine-grained dynamic details. To address these limitations, we present GeoMan, a novel architecture designed to produce accurate and temporally consistent depth and normal estimations from monocular human videos. GeoMan addresses two key challenges: the scarcity of high-quality 4D training data and the need for metric depth estimation to accurately model human size. To overcome the first challenge, GeoMan employs an image-based model to estimate depth and normals for the first frame of a video, which then conditions a video diffusion model, reframing video geometry estimation task as an image-to-video generation problem. This design offloads the heavy lifting of geometric estimation to the image model and simplifies the video modelâ€™s role to focus on intricate details while using priors learned from large-scale video datasets. Consequently, GeoMan improves temporal consistency and generalizability while requiring minimal 4D training data. To address the challenge of accurate human size estimation, we introduce a root-relative depth representation that retains critical human-scale details and is easier to be estimated from monocular inputs, overcoming the limitations of traditional affine-invariant and metric depth representations. GeoMan achieves state-of-the-art performance in both qualitative and quantitative evaluations, demonstrating its effectiveness in overcoming longstanding challenges in 3D human geometry estimation from videos. </p>
<blockquote>
<p>ä»è§†é¢‘ä¸­ä¼°è®¡å‡†ç¡®ä¸”æ—¶é—´ä¸Šä¸€è‡´çš„3Däººä½“å‡ ä½•æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é’ˆå¯¹å•å¼ å›¾åƒè¿›è¡Œä¼˜åŒ–ï¼Œå¸¸å¸¸å­˜åœ¨æ—¶é—´ä¸Šçš„ä¸ä¸€è‡´æ€§ï¼Œå¹¶ä¸”æ— æ³•æ•æ‰ç²¾ç»†çš„åŠ¨æ€ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†GeoManï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨ä»å•ç›®äººä½“è§†é¢‘ä¸­äº§ç”Ÿå‡†ç¡®ä¸”æ—¶é—´ä¸Šä¸€è‡´çš„æ·±åº¦å’Œæ³•çº¿ä¼°è®¡ã€‚GeoManè§£å†³äº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šé«˜è´¨é‡4Dè®­ç»ƒæ•°æ®çš„ç¨€ç¼ºæ€§å’Œå¯¹åº¦é‡æ·±åº¦ä¼°è®¡ä»¥å‡†ç¡®å»ºæ¨¡äººä½“å°ºå¯¸çš„éœ€æ±‚ã€‚ä¸ºäº†å…‹æœç¬¬ä¸€ä¸ªæŒ‘æˆ˜ï¼ŒGeoMané‡‡ç”¨åŸºäºå›¾åƒçš„æ¨¡å‹æ¥ä¼°è®¡è§†é¢‘çš„ç¬¬ä¸€å¸§çš„æ·±åº¦å’Œæ³•çº¿ï¼Œç„¶åå°†æ¡ä»¶åº”ç”¨äºè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå°†è§†é¢‘å‡ ä½•ä¼°è®¡ä»»åŠ¡é‡æ–°å®šä½ä¸ºå›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆé—®é¢˜ã€‚è¿™ç§è®¾è®¡å°†å‡ ä½•ä¼°è®¡çš„é‡æ‹…è½¬ç§»ç»™äº†å›¾åƒæ¨¡å‹ï¼Œç®€åŒ–äº†è§†é¢‘æ¨¡å‹çš„è§’è‰²ï¼Œä½¿å…¶ä¸“æ³¨äºç»†èŠ‚ï¼ŒåŒæ—¶ä½¿ç”¨ä»å¤§è§„æ¨¡è§†é¢‘æ•°æ®é›†ä¸­å­¦ä¹ çš„å…ˆéªŒçŸ¥è¯†ã€‚å› æ­¤ï¼ŒGeoManæé«˜äº†æ—¶é—´ä¸€è‡´æ€§å’Œé€šç”¨æ€§ï¼ŒåŒæ—¶åªéœ€è¦æœ€å°‘çš„4Dè®­ç»ƒæ•°æ®ã€‚ä¸ºäº†è§£å†³å‡†ç¡®ä¼°è®¡äººä½“å°ºå¯¸çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ ¹ç›¸å¯¹æ·±åº¦è¡¨ç¤ºæ³•ï¼Œè¯¥æ–¹æ³•ä¿ç•™äº†å…³é”®çš„äººä½“å°ºåº¦ç»†èŠ‚ï¼Œå¹¶ä¸”æ›´å®¹æ˜“ä»å•ç›®è¾“å…¥ä¸­è¿›è¡Œä¼°è®¡ï¼Œå…‹æœäº†ä¼ ç»Ÿçš„ä»¿å°„ä¸å˜å’Œåº¦é‡æ·±åº¦è¡¨ç¤ºæ³•çš„å±€é™æ€§ã€‚GeoManåœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨å…‹æœä»è§†é¢‘ä¸­ä¼°è®¡3Däººä½“å‡ ä½•çš„é•¿æœŸæŒ‘æˆ˜ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23085v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/dair/geoman">https://research.nvidia.com/labs/dair/geoman</a></p>
<p><strong>Summary</strong><br>åœ¨è§†é¢‘ä¸­çš„äººä½“ä¸‰ç»´å‡ ä½•å‡†ç¡®ä¸”æ—¶é—´ä¸€è‡´çš„ä¼°è®¡æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªéš¾é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é’ˆå¯¹å•å¼ å›¾åƒè¿›è¡Œä¼˜åŒ–ï¼Œå¸¸å¸¸å­˜åœ¨æ—¶é—´ä¸ä¸€è‡´æ€§ä¸”æ— æ³•æ•æ‰ç²¾ç»†çš„åŠ¨æ€ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GeoManæ¨¡å‹ï¼Œèƒ½å¤Ÿä»å•ç›®äººä½“è§†é¢‘ä¸­å‡†ç¡®ä¸”æ—¶é—´ä¸€è‡´åœ°ä¼°è®¡æ·±åº¦å’Œæ³•çº¿ã€‚GeoManè§£å†³äº†é«˜è´¨é‡å››ç»´è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºæ€§å’Œç²¾ç¡®å»ºæ¨¡äººä½“å°ºå¯¸æ‰€éœ€çš„åº¦é‡æ·±åº¦ä¼°è®¡è¿™ä¸¤ä¸ªå…³é”®é—®é¢˜ã€‚é€šè¿‡åŸºäºå›¾åƒçš„æ¨¡å‹ä¼°è®¡è§†é¢‘ç¬¬ä¸€å¸§çš„æ·±åº¦å’Œæ³•çº¿ï¼Œå¹¶ä»¥æ­¤ä¸ºæ¡ä»¶æ„å»ºè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå°†è§†é¢‘å‡ ä½•ä¼°è®¡ä»»åŠ¡é‡æ„ä¸ºå›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆé—®é¢˜ã€‚è¿™ç§è®¾è®¡ç®€åŒ–äº†è§†é¢‘æ¨¡å‹çš„ä»»åŠ¡ï¼Œä½¿å…¶ä¸“æ³¨äºç»†èŠ‚ï¼Œå¹¶åˆ©ç”¨ä»å¤§è§„æ¨¡è§†é¢‘æ•°æ®é›†ä¸­å­¦ä¹ çš„å…ˆéªŒçŸ¥è¯†ã€‚å› æ­¤ï¼ŒGeoManæé«˜äº†æ—¶é—´ä¸€è‡´æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶éœ€è¦å¾ˆå°‘çš„å››ç»´è®­ç»ƒæ•°æ®ã€‚å¼•å…¥æ ¹ç›¸å¯¹æ·±åº¦è¡¨ç¤ºæ³•æ¥è§£å†³å‡†ç¡®ä¼°è®¡äººä½“å°ºå¯¸çš„æŒ‘æˆ˜ï¼Œä¿ç•™äº†å…³é”®çš„äººä½“å°ºåº¦ç»†èŠ‚ï¼Œå¹¶æ›´å®¹æ˜“ä»å•ç›®è¾“å…¥è¿›è¡Œä¼°è®¡ï¼Œå…‹æœäº†ä¼ ç»Ÿçš„ä»¿å°„ä¸å˜å’Œåº¦é‡æ·±åº¦è¡¨ç¤ºæ–¹æ³•çš„å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GeoManæ¨¡å‹èƒ½å¤Ÿå‡†ç¡®ä¸”æ—¶é—´ä¸€è‡´åœ°ä»è§†é¢‘ä¸­ä¼°è®¡äººä½“ä¸‰ç»´å‡ ä½•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é¢å¯¹ä¸¤ä¸ªæŒ‘æˆ˜ï¼šå››ç»´è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºæ€§å’Œäººä½“å°ºå¯¸çš„ç²¾ç¡®å»ºæ¨¡ã€‚</li>
<li>GeoMané€šè¿‡å°†è§†é¢‘å‡ ä½•ä¼°è®¡ä»»åŠ¡è½¬åŒ–ä¸ºå›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆé—®é¢˜æ¥è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡åŸºäºå›¾åƒçš„æ¨¡å‹ä¼°è®¡è§†é¢‘ç¬¬ä¸€å¸§çš„æ·±åº¦å’Œæ³•çº¿ï¼Œå¹¶ä»¥æ­¤ä¸ºæ¡ä»¶æ„å»ºè§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>æ ¹ç›¸å¯¹æ·±åº¦è¡¨ç¤ºæ³•ç”¨äºå‡†ç¡®ä¼°è®¡äººä½“å°ºå¯¸ï¼ŒåŒæ—¶ä¿ç•™å…³é”®çš„äººä½“å°ºåº¦ç»†èŠ‚ã€‚</li>
<li>è¯¥æ¨¡å‹å®ç°äº†åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-275fb7597c36973038759182ff49fa24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79e2148cf068aa5c5163e7aaa410cf1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a655d877bf5ef963092d0c3c3baab5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd5fe9054641381efd5ae8f823074559.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dbf06b9bd134fc67e4deddd0876ffd86.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="EquiReg-Equivariance-Regularized-Diffusion-for-Inverse-Problems"><a href="#EquiReg-Equivariance-Regularized-Diffusion-for-Inverse-Problems" class="headerlink" title="EquiReg: Equivariance Regularized Diffusion for Inverse Problems"></a>EquiReg: Equivariance Regularized Diffusion for Inverse Problems</h2><p><strong>Authors:Bahareh Tolooshams, Aditi Chandrashekar, Rayhan Zirvi, Abbas Mammadov, Jiachen Yao, Chuwei Wang, Anima Anandkumar</strong></p>
<p>Diffusion models represent the state-of-the-art for solving inverse problems such as image restoration tasks. In the Bayesian framework, diffusion-based inverse solvers incorporate a likelihood term to guide the prior sampling process, generating data consistent with the posterior distribution. However, due to the intractability of the likelihood term, many current methods rely on isotropic Gaussian approximations, which lead to deviations from the data manifold and result in inconsistent, unstable reconstructions. We propose Equivariance Regularized (EquiReg) diffusion, a general framework for regularizing posterior sampling in diffusion-based inverse problem solvers. EquiReg enhances reconstructions by reweighting diffusion trajectories and penalizing those that deviate from the data manifold. We define a new distribution-dependent equivariance error, empirically identify functions that exhibit low error for on-manifold samples and higher error for off-manifold samples, and leverage these functions to regularize the diffusion sampling process. When applied to a variety of solvers, EquiReg outperforms state-of-the-art diffusion models in both linear and nonlinear image restoration tasks, as well as in reconstructing partial differential equations. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ä»£è¡¨äº†è§£å†³å›¾åƒæ¢å¤ç­‰é€†å‘é—®é¢˜çš„æœ€æ–°æŠ€æœ¯å‰æ²¿ã€‚åœ¨è´å¶æ–¯æ¡†æ¶ä¸‹ï¼ŒåŸºäºæ‰©æ•£çš„é€†å‘æ±‚è§£å™¨èå…¥ä¸€ä¸ªå¯èƒ½æ€§æœ¯è¯­æ¥å¼•å¯¼å…ˆéªŒé‡‡æ ·è¿‡ç¨‹ï¼Œç”Ÿæˆä¸åéªŒåˆ†å¸ƒä¸€è‡´çš„æ•°æ®ã€‚ç„¶è€Œï¼Œç”±äºå¯èƒ½æ€§æœ¯è¯­çš„ä¸å¯é¢„æµ‹æ€§ï¼Œè®¸å¤šå½“å‰çš„æ–¹æ³•ä¾èµ–äºå„å‘åŒæ€§é«˜æ–¯è¿‘ä¼¼ï¼Œè¿™ä¼šå¯¼è‡´åç¦»æ•°æ®æµå½¢å¹¶äº§ç”Ÿä¸ä¸€è‡´ã€ä¸ç¨³å®šçš„é‡å»ºç»“æœã€‚æˆ‘ä»¬æå‡ºäº†ç­‰ä»·æ­£åˆ™åŒ–ï¼ˆEquiRegï¼‰æ‰©æ•£ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåŸºäºæ‰©æ•£çš„é€†å‘é—®é¢˜æ±‚è§£å™¨ä¸­çš„åéªŒé‡‡æ ·çš„é€šç”¨æ¡†æ¶ã€‚EquiRegé€šè¿‡é‡æ–°åŠ æƒæ‰©æ•£è½¨è¿¹å¹¶æƒ©ç½šé‚£äº›åç¦»æ•°æ®æµå½¢çš„è½¨è¿¹æ¥å¢å¼ºé‡å»ºæ•ˆæœã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæ–°çš„åˆ†å¸ƒç›¸å…³çš„ç­‰ä»·è¯¯å·®ï¼Œå®è¯ç¡®å®šäº†å¯¹äºæµå½¢å†…æ ·æœ¬æ˜¾ç¤ºä½è¯¯å·®è€Œå¯¹äºæµå½¢å¤–æ ·æœ¬æ˜¾ç¤ºæ›´é«˜è¯¯å·®çš„å‡½æ•°ï¼Œå¹¶åˆ©ç”¨è¿™äº›å‡½æ•°æ¥è§„èŒƒæ‰©æ•£é‡‡æ ·è¿‡ç¨‹ã€‚å½“åº”ç”¨äºå„ç§æ±‚è§£å™¨æ—¶ï¼ŒEquiRegåœ¨çº¿æ€§å’Œéçº¿æ€§å›¾åƒæ¢å¤ä»»åŠ¡ä»¥åŠåå¾®åˆ†æ–¹ç¨‹é‡å»ºä¸­éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22973v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹æ˜¯è§£å†³å›¾åƒæ¢å¤ç­‰é€†é—®é¢˜çš„æœ€æ–°æŠ€æœ¯ã€‚åœ¨è´å¶æ–¯æ¡†æ¶ä¸‹ï¼ŒåŸºäºæ‰©æ•£çš„é€†æ±‚è§£å™¨é€šè¿‡ç»“åˆå¯èƒ½æ€§æœ¯è¯­æ¥å¼•å¯¼å…ˆéªŒé‡‡æ ·è¿‡ç¨‹ï¼Œç”Ÿæˆä¸åéªŒåˆ†å¸ƒä¸€è‡´çš„æ•°æ®ã€‚ç„¶è€Œï¼Œç”±äºå¯èƒ½æ€§æœ¯è¯­çš„ä¸å¯è®¡ç®—æ€§ï¼Œè®¸å¤šå½“å‰çš„æ–¹æ³•ä¾èµ–äºå„å‘åŒæ€§çš„é«˜æ–¯è¿‘ä¼¼ï¼Œè¿™ä¼šå¯¼è‡´åç¦»æ•°æ®æµå½¢å¹¶äº§ç”Ÿä¸ä¸€è‡´ã€ä¸ç¨³å®šçš„é‡å»ºã€‚æˆ‘ä»¬æå‡ºäº†ç­‰è·æ­£åˆ™åŒ–ï¼ˆEquiRegï¼‰æ‰©æ•£ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ­£åˆ™åŒ–åŸºäºæ‰©æ•£çš„é€†é—®é¢˜æ±‚è§£å™¨ä¸­çš„åéªŒé‡‡æ ·çš„é€šç”¨æ¡†æ¶ã€‚EquiRegé€šè¿‡é‡æ–°åŠ æƒæ‰©æ•£è½¨è¿¹å¹¶æƒ©ç½šåç¦»æ•°æ®æµå½¢çš„è½¨è¿¹æ¥å¢å¼ºé‡å»ºæ•ˆæœã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæ–°çš„åˆ†å¸ƒç›¸å…³çš„ç­‰è·è¯¯å·®ï¼Œå®è¯ç¡®å®šäº†å¯¹äºæµå½¢å†…æ ·æœ¬å…·æœ‰ä½è¯¯å·®è€Œå¯¹äºæµå½¢å¤–æ ·æœ¬å…·æœ‰è¾ƒé«˜è¯¯å·®çš„å‡½æ•°ï¼Œå¹¶åˆ©ç”¨è¿™äº›å‡½æ•°æ¥æ­£åˆ™åŒ–æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ã€‚åœ¨å¤šç§æ±‚è§£å™¨ä¸Šåº”ç”¨æ—¶ï¼ŒEquiRegåœ¨çº¿æ€§å’Œéçº¿æ€§å›¾åƒæ¢å¤ä»»åŠ¡ä»¥åŠåå¾®åˆ†æ–¹ç¨‹é‡å»ºä¸­å‡è¡¨ç°å‡ºä¼˜äºæœ€æ–°æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹æ˜¯è§£å†³é€†é—®é¢˜çš„æœ€æ–°æŠ€æœ¯ï¼Œå°¤å…¶åœ¨å›¾åƒæ¢å¤é¢†åŸŸæœ‰å‡ºè‰²è¡¨ç°ã€‚</li>
<li>åœ¨è´å¶æ–¯æ¡†æ¶ä¸‹ï¼Œæ‰©æ•£æ¨¡å‹ç»“åˆå¯èƒ½æ€§æœ¯è¯­æ¥å¼•å¯¼å…ˆéªŒé‡‡æ ·ã€‚</li>
<li>å½“å‰æ–¹æ³•å› å¯èƒ½æ€§æœ¯è¯­çš„ä¸å¯è®¡ç®—æ€§è€Œä¾èµ–å„å‘åŒæ€§çš„é«˜æ–¯è¿‘ä¼¼ï¼Œè¿™å¯èƒ½å¯¼è‡´æ•°æ®æµå½¢åç¦»å¹¶äº§ç”Ÿä¸ç¨³å®šçš„é‡å»ºç»“æœã€‚</li>
<li>æå‡ºçš„EquiRegæ‰©æ•£æ¡†æ¶ç”¨äºæ­£åˆ™åŒ–åŸºäºæ‰©æ•£çš„é€†é—®é¢˜æ±‚è§£å™¨ä¸­çš„åéªŒé‡‡æ ·ã€‚</li>
<li>EquiRegé€šè¿‡é‡æ–°åŠ æƒæ‰©æ•£è½¨è¿¹å¹¶æƒ©ç½šåç¦»æ•°æ®æµå½¢çš„è½¨è¿¹æ¥å¢å¼ºé‡å»ºæ•ˆæœã€‚</li>
<li>EquiRegå®šä¹‰äº†ä¸€ä¸ªæ–°çš„åˆ†å¸ƒç›¸å…³çš„ç­‰è·è¯¯å·®æ¥è¡¡é‡æ ·æœ¬çš„è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-71268d1041b7bb3c01878fef096171aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-462f48d63ae5842c6e9fdd93fa0ee8e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-045221756c3a37d3ebc360b96bb6d77e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Plug-and-Play-Posterior-Sampling-for-Blind-Inverse-Problems"><a href="#Plug-and-Play-Posterior-Sampling-for-Blind-Inverse-Problems" class="headerlink" title="Plug-and-Play Posterior Sampling for Blind Inverse Problems"></a>Plug-and-Play Posterior Sampling for Blind Inverse Problems</h2><p><strong>Authors:Anqi Li, Weijie Gan, Ulugbek S. Kamilov</strong></p>
<p>We introduce Blind Plug-and-Play Diffusion Models (Blind-PnPDM) as a novel framework for solving blind inverse problems where both the target image and the measurement operator are unknown. Unlike conventional methods that rely on explicit priors or separate parameter estimation, our approach performs posterior sampling by recasting the problem into an alternating Gaussian denoising scheme. We leverage two diffusion models as learned priors: one to capture the distribution of the target image and another to characterize the parameters of the measurement operator. This PnP integration of diffusion models ensures flexibility and ease of adaptation. Our experiments on blind image deblurring show that Blind-PnPDM outperforms state-of-the-art methods in terms of both quantitative metrics and visual fidelity. Our results highlight the effectiveness of treating blind inverse problems as a sequence of denoising subproblems while harnessing the expressive power of diffusion-based priors. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥ç›²æ’æ’­æ‰©æ•£æ¨¡å‹ï¼ˆBlind-PnPDMï¼‰ä½œä¸ºä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œç”¨äºè§£å†³ç›²åé—®é¢˜ï¼Œå…¶ä¸­ç›®æ ‡å›¾åƒå’Œæµ‹é‡ç®—å­éƒ½æ˜¯æœªçŸ¥çš„ã€‚ä¸åŒäºä¾èµ–æ˜¾å¼å…ˆéªŒæˆ–å•ç‹¬å‚æ•°ä¼°è®¡çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æŠŠé—®é¢˜è½¬åŒ–ä¸ºäº¤æ›¿é«˜æ–¯å»å™ªæ–¹æ¡ˆæ¥æ‰§è¡ŒåéªŒé‡‡æ ·ã€‚æˆ‘ä»¬åˆ©ç”¨ä¸¤ä¸ªæ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼šä¸€ä¸ªç”¨äºæ•æ‰ç›®æ ‡å›¾åƒçš„åˆ†å¸ƒï¼Œå¦ä¸€ä¸ªç”¨äºæè¿°æµ‹é‡ç®—å­çš„å‚æ•°ã€‚è¿™ç§PnPæ‰©æ•£æ¨¡å‹çš„é›†æˆç¡®ä¿äº†çµæ´»æ€§å’Œæ˜“äºé€‚åº”ã€‚æˆ‘ä»¬åœ¨ç›²å›¾åƒå»æ¨¡ç³Šå®éªŒä¸Šè¡¨æ˜ï¼ŒBlind-PnPDMåœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†å°†ç›²åé—®é¢˜è§†ä¸ºä¸€ç³»åˆ—å»å™ªå­é—®é¢˜åºåˆ—çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶åˆ©ç”¨åŸºäºæ‰©æ•£çš„å…ˆéªŒçŸ¥è¯†çš„è¡¨ç°åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22923v1">PDF</a> arXiv admin note: text overlap with arXiv:2305.12672</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç›²æ’æ¥æ‰©æ•£æ¨¡å‹ï¼ˆBlind-PnPDMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è§£å†³ç›²é€†é—®é¢˜çš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ç›®æ ‡å›¾åƒå’Œæµ‹é‡ç®—å­å‡æœªçŸ¥çš„æƒ…å†µä¸‹è¿ä½œã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•é€šè¿‡é‡æ–°æ„å»ºé—®é¢˜ä¸ºäº¤æ›¿é«˜æ–¯å»å™ªæ–¹æ¡ˆè¿›è¡ŒåéªŒé‡‡æ ·ï¼Œè€Œæ— éœ€ä¾èµ–æ˜¾å¼å…ˆéªŒæˆ–å•ç‹¬å‚æ•°ä¼°è®¡ã€‚å®éªŒè¯æ˜ï¼Œåœ¨ç›²å›¾åƒå»æ¨¡ç³Šä»»åŠ¡ä¸Šï¼ŒBlind-PnPDMåœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Blind-PnPDMæ˜¯ä¸€ç§è§£å†³ç›²é€†é—®é¢˜çš„æ–°å‹æ¡†æ¶ï¼Œé€‚ç”¨äºç›®æ ‡å›¾åƒå’Œæµ‹é‡ç®—å­å‡æœªçŸ¥çš„æƒ…å†µã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡äº¤æ›¿é«˜æ–¯å»å™ªæ–¹æ¡ˆè¿›è¡ŒåéªŒé‡‡æ ·ï¼Œæ— éœ€æ˜¾å¼å…ˆéªŒæˆ–å•ç‹¬å‚æ•°ä¼°è®¡ã€‚</li>
<li>Blind-PnPDMåˆ©ç”¨ä¸¤ä¸ªæ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼šä¸€ä¸ªç”¨äºæ•æ‰ç›®æ ‡å›¾åƒåˆ†å¸ƒï¼Œå¦ä¸€ä¸ªç”¨äºæè¿°æµ‹é‡ç®—å­çš„å‚æ•°ã€‚</li>
<li>Blind-PnPDMåœ¨ç›²å›¾åƒå»æ¨¡ç³Šä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰çµæ´»æ€§å’Œé€‚åº”æ€§ï¼Œæ˜“äºé€‚åº”ä¸åŒçš„ä»»åŠ¡ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†å°†ç›²é€†é—®é¢˜è§†ä¸ºä¸€ç³»åˆ—å»å™ªå­é—®é¢˜çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼Œå±•ç°äº†å…¶å¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22923">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4ace22b73954869c7f00088e80b42a9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1202074e1a4b7c51d99a9abdf113e332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97f6ce60fcd7e8ae7337ad220dc2a481.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d58515b2280c99163d4a3e4a4866af0f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Re-ttention-Ultra-Sparse-Visual-Generation-via-Attention-Statistical-Reshape"><a href="#Re-ttention-Ultra-Sparse-Visual-Generation-via-Attention-Statistical-Reshape" class="headerlink" title="Re-ttention: Ultra Sparse Visual Generation via Attention Statistical   Reshape"></a>Re-ttention: Ultra Sparse Visual Generation via Attention Statistical   Reshape</h2><p><strong>Authors:Ruichen Chen, Keith G. Mills, Liyao Jiang, Chao Gao, Di Niu</strong></p>
<p>Diffusion Transformers (DiT) have become the de-facto model for generating high-quality visual content like videos and images. A huge bottleneck is the attention mechanism where complexity scales quadratically with resolution and video length. One logical way to lessen this burden is sparse attention, where only a subset of tokens or patches are included in the calculation. However, existing techniques fail to preserve visual quality at extremely high sparsity levels and might even incur non-negligible compute overheads. % To address this concern, we propose Re-ttention, which implements very high sparse attention for visual generation models by leveraging the temporal redundancy of Diffusion Models to overcome the probabilistic normalization shift within the attention mechanism. Specifically, Re-ttention reshapes attention scores based on the prior softmax distribution history in order to preserve the visual quality of the full quadratic attention at very high sparsity levels. % Experimental results on T2V&#x2F;T2I models such as CogVideoX and the PixArt DiTs demonstrate that Re-ttention requires as few as 3.1% of the tokens during inference, outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and MInference. Further, we measure latency to show that our method can attain over 45% end-to-end % and over 92% self-attention latency reduction on an H100 GPU at negligible overhead cost.   Code available online here: \href{<a target="_blank" rel="noopener" href="https://github.com/cccrrrccc/Re-ttention%7D%7Bhttps://github.com/cccrrrccc/Re-ttention%7D">https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}</a> </p>
<blockquote>
<p>æ‰©æ•£Transformerï¼ˆDiTï¼‰å·²æˆä¸ºç”Ÿæˆé«˜è´¨é‡è§†è§‰å†…å®¹ï¼ˆå¦‚è§†é¢‘å’Œå›¾åƒï¼‰çš„é»˜è®¤æ¨¡å‹ã€‚ä¸€ä¸ªå·¨å¤§çš„ç“¶é¢ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶ä¸­å¤æ‚æ€§éšåˆ†è¾¨ç‡å’Œè§†é¢‘é•¿åº¦çš„å¢åŠ è€ŒäºŒæ¬¡æ–¹å¢é•¿ã€‚å‡è½»è¿™ä¸€è´Ÿæ‹…çš„ä¸€ç§é€»è¾‘æ–¹æ³•æ˜¯ç¨€ç–æ³¨æ„åŠ›ï¼Œå…¶ä¸­ä»…åŒ…æ‹¬ä¸€éƒ¨åˆ†ä»¤ç‰Œæˆ–è¡¥ä¸è¿›è¡Œè®¡ç®—ã€‚ç„¶è€Œï¼Œç°æœ‰æŠ€æœ¯åœ¨æé«˜çš„ç¨€ç–æ€§æ°´å¹³ä¸‹æ— æ³•ä¿æŒè§†è§‰è´¨é‡ï¼Œå¹¶å¯èƒ½äº§ç”Ÿä¸å¯å¿½ç•¥çš„è®¡ç®—å¼€é”€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ‹…å¿§ï¼Œæˆ‘ä»¬æå‡ºäº†Re-ttentionï¼Œå®ƒé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ—¶åºå†—ä½™æ¥å®ç°è§†è§‰ç”Ÿæˆæ¨¡å‹çš„æé«˜ç¨€ç–æ³¨æ„åŠ›ï¼Œä»¥å…‹æœæ³¨æ„åŠ›æœºåˆ¶å†…çš„æ¦‚ç‡å½’ä¸€åŒ–ç§»ä½ã€‚å…·ä½“æ¥è¯´ï¼ŒRe-ttentionæ ¹æ®å…ˆå‰çš„softmaxåˆ†å¸ƒå†å²é‡æ–°è°ƒæ•´æ³¨æ„åŠ›åˆ†æ•°ï¼Œä»¥åœ¨æé«˜çš„ç¨€ç–æ€§æ°´å¹³ä¸‹ä¿æŒå…¨äºŒæ¬¡æ³¨æ„åŠ›çš„è§†è§‰è´¨é‡ã€‚åœ¨T2V&#x2F;T2Iæ¨¡å‹ï¼ˆå¦‚CogVideoXå’ŒPixArt DiTsï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRe-ttentionåœ¨æ¨ç†è¿‡ç¨‹ä¸­åªéœ€è¦3.1%çš„ä»¤ç‰Œï¼Œè¶…è¶Šäº†FastDiTAttnã€Sparse VideoGenå’ŒMInferenceç­‰å½“ä»£æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æµ‹é‡å»¶è¿Ÿæ¥å±•ç¤ºæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨H100 GPUä¸Šå®ç°è¶…è¿‡45%çš„ç«¯åˆ°ç«¯å»¶è¿Ÿå‡å°‘å’Œè¶…è¿‡92%çš„è‡ªæ³¨æ„åŠ›å»¶è¿Ÿå‡å°‘ï¼ŒåŒæ—¶å‡ ä¹ä¸å¢åŠ å¼€é”€æˆæœ¬ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/cccrrrccc/Re-ttention">https://github.com/cccrrrccc/Re-ttention</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22918v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£Transformerï¼ˆDiTï¼‰å·²æˆä¸ºç”Ÿæˆé«˜è´¨é‡è§†è§‰å†…å®¹ï¼ˆå¦‚è§†é¢‘å’Œå›¾åƒï¼‰çš„é»˜è®¤æ¨¡å‹ã€‚å½“å‰é¢ä¸´çš„ä¸€ä¸ªå·¨å¤§ç“¶é¢ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶å¤æ‚æ€§éšåˆ†è¾¨ç‡å’Œè§†é¢‘é•¿åº¦çš„å¢åŠ è€Œå‘ˆäºŒæ¬¡æ–¹å¢é•¿ã€‚ä¸ºå‡è½»è¿™ä¸€è´Ÿæ‹…ï¼Œäººä»¬é‡‡ç”¨ç¨€ç–æ³¨æ„åŠ›æ³•ï¼Œä»…é€‰æ‹©ä¸€éƒ¨åˆ†ä»¤ç‰Œæˆ–è¡¥ä¸è¿›è¡Œè®¡ç®—ã€‚ç„¶è€Œï¼Œç°æœ‰æŠ€æœ¯åœ¨æé«˜çš„ç¨€ç–æ°´å¹³ä¸Šæ— æ³•ä¿æŒè§†è§‰è´¨é‡ï¼Œå¹¶å¯èƒ½äº§ç”Ÿä¸å¯å¿½ç•¥çš„è®¡ç®—å¼€é”€ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºRe-ttentionï¼Œé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ—¶åºå†—ä½™æ€§ï¼Œå®ç°å¯¹è§†è§‰ç”Ÿæˆæ¨¡å‹çš„æé«˜ç¨€ç–æ³¨æ„åŠ›ã€‚Re-ttentionæ ¹æ®å…ˆå‰çš„softmaxåˆ†å¸ƒå†å²é‡å¡‘æ³¨æ„åŠ›åˆ†æ•°ï¼Œä»¥åœ¨æé«˜çš„ç¨€ç–æ°´å¹³ä¸Šä¿æŒå…¨äºŒæ¬¡æ³¨æ„åŠ›è§†è§‰è´¨é‡ã€‚åœ¨T2V&#x2F;T2Iæ¨¡å‹ï¼ˆå¦‚CogVideoXå’ŒPixArt DiTsï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRe-ttentionåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»…éœ€3.1%çš„ä»¤ç‰Œï¼Œè¶…è¶Šäº†FastDiTAttnã€Sparse VideoGenå’ŒMInferenceç­‰å½“ä»£æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æµ‹é‡å»¶è¿Ÿæ—¶é—´ï¼Œè¯æ˜è¯¥æ–¹æ³•å¯åœ¨H100 GPUä¸Šå®ç°è¶…è¿‡45%çš„ç«¯åˆ°ç«¯å’Œè¶…è¿‡92%çš„è‡ªæ³¨æ„åŠ›å»¶è¿Ÿå‡å°‘ï¼ŒåŒæ—¶å‡ ä¹ä¸å¢åŠ å¼€é”€ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/cccrrrccc/Re-ttention%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/cccrrrccc/Re-ttentionè®¿é—®ã€‚</a></p>
<p><strong>è¦ç‚¹é€Ÿé€’</strong></p>
<ol>
<li>æ‰©æ•£Transformerï¼ˆDiTï¼‰å·²æˆä¸ºé«˜è´¨é‡è§†è§‰å†…å®¹ç”Ÿæˆçš„æ ‡å‡†æ¨¡å‹ï¼Œé¢ä¸´æ³¨æ„åŠ›æœºåˆ¶å¤æ‚æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ç¨€ç–æ³¨æ„åŠ›æŠ€æœ¯åœ¨é«˜ç¨€ç–æ€§æ—¶è§†è§‰è´¨é‡ä¸‹é™å¹¶å¯èƒ½äº§ç”Ÿé¢å¤–è®¡ç®—å¼€é”€ã€‚</li>
<li>Re-ttentioné€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ—¶åºå†—ä½™æ€§ï¼Œå®ç°é«˜ç¨€ç–æ³¨æ„åŠ›ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>Re-ttentioné€šè¿‡é‡å¡‘æ³¨æ„åŠ›åˆ†æ•°ï¼Œåœ¨æé«˜ç¨€ç–æ°´å¹³ä¸Šä¿æŒè§†è§‰è´¨é‡ã€‚</li>
<li>åœ¨T2V&#x2F;T2Iæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRe-ttentionä»…éœ€å°‘é‡ä»¤ç‰Œå³å¯è¶…è¶Šå…¶ä»–æ–¹æ³•ã€‚</li>
<li>Re-ttentionèƒ½æœ‰æ•ˆå‡å°‘å»¶è¿Ÿï¼Œæé«˜æ¨¡å‹æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22918">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a75cd7f47adc14567e99e44316bdc5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30216dafe9bcff6b84a9b4339723b5f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7905eafd41f579c842d54bda2d8a4f4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d71d2f80fd6570fad70cdaf1f531b0ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9478fd57d613165c33ec1defc3bccfb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Diffusion-Classifiers-Understand-Compositionality-but-Conditions-Apply"><a href="#Diffusion-Classifiers-Understand-Compositionality-but-Conditions-Apply" class="headerlink" title="Diffusion Classifiers Understand Compositionality, but Conditions Apply"></a>Diffusion Classifiers Understand Compositionality, but Conditions Apply</h2><p><strong>Authors:Yujin Jeong, Arnas Uselis, Seong Joon Oh, Anna Rohrbach</strong></p>
<p>Understanding visual scenes is fundamental to human intelligence. While discriminative models have significantly advanced computer vision, they often struggle with compositional understanding. In contrast, recent generative text-to-image diffusion models excel at synthesizing complex scenes, suggesting inherent compositional capabilities. Building on this, zero-shot diffusion classifiers have been proposed to repurpose diffusion models for discriminative tasks. While prior work offered promising results in discriminative compositional scenarios, these results remain preliminary due to a small number of benchmarks and a relatively shallow analysis of conditions under which the models succeed. To address this, we present a comprehensive study of the discriminative capabilities of diffusion classifiers on a wide range of compositional tasks. Specifically, our study covers three diffusion models (SD 1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks. Further, we shed light on the role that target dataset domains play in respective performance; to isolate the domain effects, we introduce a new diagnostic benchmark Self-Bench comprised of images created by diffusion models themselves. Finally, we explore the importance of timestep weighting and uncover a relationship between domain gap and timestep sensitivity, particularly for SD3-m. To sum up, diffusion classifiers understand compositionality, but conditions apply! Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/eugene6923/Diffusion-Classifiers-Compositionality">https://github.com/eugene6923/Diffusion-Classifiers-Compositionality</a>. </p>
<blockquote>
<p>ç†è§£è§†è§‰åœºæ™¯å¯¹äººç±»æ™ºèƒ½è‡³å…³é‡è¦ã€‚è™½ç„¶åˆ¤åˆ«æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç»„åˆç†è§£æ–¹é¢å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ€è¿‘çš„æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹åœ¨åˆæˆå¤æ‚åœºæ™¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¿™è¡¨æ˜å…¶å…·æœ‰å†…åœ¨çš„ç»„æˆèƒ½åŠ›ã€‚åŸºäºæ­¤ï¼Œé›¶æ ·æœ¬æ‰©æ•£åˆ†ç±»å™¨è¢«æå‡ºç”¨äºå°†æ‰©æ•£æ¨¡å‹é‡æ–°ç”¨äºåˆ¤åˆ«ä»»åŠ¡ã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œåœ¨åˆ¤åˆ«ç»„åˆåœºæ™¯æ–¹é¢æä¾›äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†ç”±äºåŸºå‡†æµ‹è¯•çš„æ•°é‡æœ‰é™ä»¥åŠå¯¹æ¨¡å‹æˆåŠŸçš„æ¡ä»¶åˆ†æç›¸å¯¹è‚¤æµ…ï¼Œè¿™äº›ç»“æœä»æ˜¯åˆæ­¥çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹æ‰©æ•£åˆ†ç±»å™¨åœ¨å¹¿æ³›ç»„åˆä»»åŠ¡ä¸Šçš„åˆ¤åˆ«èƒ½åŠ›è¿›è¡Œäº†ç»¼åˆç ”ç©¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ¶µç›–äº†ä¸‰ä¸ªæ‰©æ•£æ¨¡å‹ï¼ˆSD 1.5ã€2.0ï¼Œä»¥åŠé¦–æ¬¡å°è¯•çš„3-mï¼‰ï¼Œæ¶‰åŠ10ä¸ªæ•°æ®é›†å’Œè¶…è¿‡30ä¸ªä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é˜æ˜äº†ç›®æ ‡æ•°æ®é›†é¢†åŸŸåœ¨å„è‡ªæ€§èƒ½ä¸­çš„ä½œç”¨ï¼›ä¸ºäº†éš”ç¦»é¢†åŸŸæ•ˆåº”ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€ä¸ªæ–°çš„è¯Šæ–­åŸºå‡†Self-Benchï¼Œç”±æ‰©æ•£æ¨¡å‹æœ¬èº«åˆ›å»ºçš„å›¾åƒç»„æˆã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†æ—¶é—´æ­¥é•¿æƒé‡çš„é‡è¦æ€§ï¼Œå¹¶æ­ç¤ºäº†é¢†åŸŸå·®è·ä¸æ—¶é—´æ­¥é•¿æ•æ„Ÿæ€§ä¹‹é—´çš„å…³ç³»ï¼Œç‰¹åˆ«æ˜¯å¯¹äºSD3-mã€‚æ€»è€Œè¨€ä¹‹ï¼Œæ‰©æ•£åˆ†ç±»å™¨ç†è§£ç»„åˆæ€§ï¼Œä½†æœ‰æ¡ä»¶é™åˆ¶ï¼ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/eugene6923/Diffusion-Classifiers-Compositionality%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/eugene6923/Diffusion-Classifiers-Compositionalityè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17955v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†è§‰åœºæ™¯çš„ç†è§£å¯¹äººç±»æ™ºèƒ½è‡³å…³é‡è¦ã€‚åˆ¤åˆ«æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç»„åˆç†è§£æ–¹é¢ä»æœ‰å›°éš¾ã€‚ç›¸åï¼Œæœ€è¿‘çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹åœ¨åˆæˆå¤æ‚åœºæ™¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå†…åœ¨çš„ç»„æˆèƒ½åŠ›ã€‚åŸºäºæ­¤ï¼Œé›¶æ ·æœ¬æ‰©æ•£åˆ†ç±»å™¨è¢«æå‡ºç”¨äºå°†æ‰©æ•£æ¨¡å‹ç”¨äºåˆ¤åˆ«ä»»åŠ¡ã€‚è™½ç„¶å…ˆå‰çš„ç ”ç©¶åœ¨åˆ¤åˆ«ç»„åˆåœºæ™¯æ–¹é¢æä¾›äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†ç”±äºåŸºå‡†æµ‹è¯•æ•°é‡è¾ƒå°‘ä»¥åŠå¯¹æ¨¡å‹æˆåŠŸæ¡ä»¶çš„åˆ†æç›¸å¯¹è‚¤æµ…ï¼Œè¿™äº›ç»“æœä»ç„¶åˆæ­¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹æ‰©æ•£åˆ†ç±»å™¨åœ¨å¤šç§ç»„åˆä»»åŠ¡ä¸Šçš„åˆ¤åˆ«èƒ½åŠ›è¿›è¡Œäº†ç»¼åˆç ”ç©¶ã€‚ç ”ç©¶æ¶µç›–äº†ä¸‰ä¸ªæ‰©æ•£æ¨¡å‹ï¼ˆSD 1.5ã€2.0å’Œé¦–æ¬¡æ¨å‡ºçš„3-mï¼‰ï¼Œæ¶‰åŠ10ä¸ªæ•°æ®é›†å’Œè¶…è¿‡30ä¸ªä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é˜æ˜äº†ç›®æ ‡æ•°æ®é›†é¢†åŸŸåœ¨å„è‡ªæ€§èƒ½ä¸­çš„ä½œç”¨ï¼›ä¸ºäº†éš”ç¦»é¢†åŸŸæ•ˆåº”ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€ä¸ªæ–°çš„è¯Šæ–­åŸºå‡†Self-Benchï¼Œç”±æ‰©æ•£æ¨¡å‹æœ¬èº«åˆ›å»ºçš„å›¾åƒç»„æˆã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†æ—¶é—´æ­¥é•¿æƒé‡çš„é‡è¦æ€§ï¼Œå¹¶æ­ç¤ºäº†é¢†åŸŸå·®è·ä¸æ—¶é—´æ­¥é•¿æ•æ„Ÿæ€§ä¹‹é—´çš„å…³ç³»ï¼Œç‰¹åˆ«æ˜¯å¯¹äºSD3-mã€‚æ€»ä¹‹ï¼Œæ‰©æ•£åˆ†ç±»å™¨ç†è§£ç»„åˆæ€§ï¼Œä½†æ¡ä»¶åº”ç”¨ï¼ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/eugene6923/Diffusion-Classifiers-Compositionality">é“¾æ¥</a>ä¸­æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨åˆæˆå¤æ‚åœºæ™¯æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œè¡¨æ˜å…¶å†…åœ¨çš„ç»„æˆèƒ½åŠ›ã€‚</li>
<li>é›¶æ ·æœ¬æ‰©æ•£åˆ†ç±»å™¨è¢«æˆåŠŸåº”ç”¨äºåˆ¤åˆ«ä»»åŠ¡ï¼Œæ˜¾ç¤ºå‡ºæ‰©æ•£æ¨¡å‹çš„æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶æ¶µç›–äº†å¤šä¸ªæ‰©æ•£æ¨¡å‹å’Œå¤§é‡æ•°æ®é›†ï¼Œæä¾›äº†å¯¹æ‰©æ•£åˆ†ç±»å™¨åˆ¤åˆ«èƒ½åŠ›çš„å…¨é¢åˆ†æã€‚</li>
<li>æ­ç¤ºäº†ç›®æ ‡æ•°æ®é›†é¢†åŸŸå¯¹æ¨¡å‹æ€§èƒ½çš„é‡è¦å½±å“ã€‚</li>
<li>å¼•å…¥æ–°çš„è¯Šæ–­åŸºå‡†Self-Benchï¼Œç”¨äºè¯„ä¼°ç”±æ‰©æ•£æ¨¡å‹åˆ›å»ºçš„å›¾åƒã€‚</li>
<li>æ¢è®¨äº†æ—¶é—´æ­¥é•¿æƒé‡çš„é‡è¦æ€§ï¼Œå¹¶å‘ç°å…¶ä¸é¢†åŸŸå·®è·ä¹‹é—´çš„å…³è”ã€‚</li>
<li>æ‰©æ•£åˆ†ç±»å™¨è™½ç„¶ç†è§£ç»„åˆæ€§ï¼Œä½†åº”ç”¨æ—¶éœ€è€ƒè™‘æ¡ä»¶å› ç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f450b936d9be2850bb44675f6ccac592.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0080c0b9b17fb3606b3e80ba1a7bbefa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a64b45fc99c8261784ae83a6ed212b0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f0b3628beb67d5ec1e22f3c85742a57.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="OSCAR-One-Step-Diffusion-Codec-for-Image-Compression-Across-Multiple-Bit-rates"><a href="#OSCAR-One-Step-Diffusion-Codec-for-Image-Compression-Across-Multiple-Bit-rates" class="headerlink" title="OSCAR: One-Step Diffusion Codec for Image Compression Across Multiple   Bit-rates"></a>OSCAR: One-Step Diffusion Codec for Image Compression Across Multiple   Bit-rates</h2><p><strong>Authors:Jinpei Guo, Yifei Ji, Zheng Chen, Kai Liu, Min Liu, Wang Rao, Wenbo Li, Yong Guo, Yulun Zhang</strong></p>
<p>Pretrained latent diffusion models have shown strong potential for lossy image compression, owing to their powerful generative priors. Most existing diffusion-based methods reconstruct images by iteratively denoising from random noise, guided by compressed latent representations. While these approaches have achieved high reconstruction quality, their multi-step sampling process incurs substantial computational overhead. Moreover, they typically require training separate models for different compression bit-rates, leading to significant training and storage costs. To address these challenges, we propose a one-step diffusion codec across multiple bit-rates. termed OSCAR. Specifically, our method views compressed latents as noisy variants of the original latents, where the level of distortion depends on the bit-rate. This perspective allows them to be modeled as intermediate states along a diffusion trajectory. By establishing a mapping from the compression bit-rate to a pseudo diffusion timestep, we condition a single generative model to support reconstructions at multiple bit-rates. Meanwhile, we argue that the compressed latents retain rich structural information, thereby making one-step denoising feasible. Thus, OSCAR replaces iterative sampling with a single denoising pass, significantly improving inference efficiency. Extensive experiments demonstrate that OSCAR achieves superior performance in both quantitative and visual quality metrics. The code and models will be released at <a target="_blank" rel="noopener" href="https://github.com/jp-guo/OSCAR">https://github.com/jp-guo/OSCAR</a>. </p>
<blockquote>
<p>é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨æœ‰æŸå›¾åƒå‹ç¼©æ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ½œåŠ›ï¼Œè¿™å¾—ç›Šäºå…¶å¼ºå¤§çš„ç”Ÿæˆå…ˆéªŒã€‚å¤§å¤šæ•°ç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•é€šè¿‡è¿­ä»£å»å™ªä»éšæœºå™ªå£°ä¸­é‡å»ºå›¾åƒï¼Œç”±å‹ç¼©çš„æ½œåœ¨è¡¨ç¤ºæ‰€å¼•å¯¼ã€‚è™½ç„¶è¿™äº›æ–¹æ³•è¾¾åˆ°äº†è¾ƒé«˜çš„é‡å»ºè´¨é‡ï¼Œä½†å®ƒä»¬çš„å¤šæ­¥é‡‡æ ·è¿‡ç¨‹äº§ç”Ÿäº†å¤§é‡çš„è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦é’ˆå¯¹ä¸åŒçš„å‹ç¼©æ¯”ç‰¹ç‡è®­ç»ƒå•ç‹¬çš„æ¨¡å¼ï¼Œä»è€Œäº§ç”Ÿäº†æ˜¾è‘—çš„è®­ç»ƒå’Œå­˜å‚¨æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è·¨å¤šä¸ªæ¯”ç‰¹ç‡çš„æ‰©æ•£ç¼–è§£ç å™¨ï¼Œç§°ä¸ºOSCARã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†å‹ç¼©æ½œåœ¨å˜é‡è§†ä¸ºåŸå§‹æ½œåœ¨å˜é‡çš„å™ªå£°ç‰ˆæœ¬ï¼Œå…¶ä¸­å¤±çœŸç¨‹åº¦å–å†³äºæ¯”ç‰¹ç‡ã€‚è¿™ä¸ªè§†è§’å…è®¸å®ƒä»¬è¢«å»ºæ¨¡ä¸ºæ‰©æ•£è½¨è¿¹ä¸­çš„ä¸­é—´çŠ¶æ€ã€‚é€šè¿‡å»ºç«‹ä»å‹ç¼©æ¯”ç‰¹ç‡åˆ°ä¼ªæ‰©æ•£æ—¶é—´æ­¥é•¿çš„æ˜ å°„ï¼Œæˆ‘ä»¬å°†å•ä¸ªç”Ÿæˆæ¨¡å¼åº”ç”¨äºå¤šä¸ªæ¯”ç‰¹ç‡çš„é‡å»ºã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¤ä¸ºå‹ç¼©çš„æ½œåœ¨å˜é‡ä¿ç•™äº†ä¸°å¯Œçš„ç»“æ„ä¿¡æ¯ï¼Œä»è€Œä½¿å¾—ä¸€æ­¥å»å™ªå˜å¾—å¯è¡Œã€‚å› æ­¤ï¼ŒOSCARç”¨ä¸€æ¬¡å»å™ªè¿‡ç¨‹å–ä»£äº†è¿­ä»£é‡‡æ ·ï¼Œå¤§å¤§æé«˜äº†æ¨ç†æ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOSCARåœ¨å®šé‡å’Œè§†è§‰è´¨é‡æŒ‡æ ‡ä¸Šå‡å–å¾—äº†ä¼˜å¼‚çš„è¡¨ç°ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/jp-guo/OSCAR%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/jp-guo/OSCARä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16091v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒæœ‰æŸå‹ç¼©æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚ç°æœ‰æ‰©æ•£æ–¹æ³•é€šè¿‡è¿­ä»£å»å™ªä»éšæœºå™ªå£°é‡å»ºå›¾åƒï¼Œå—æ½œåœ¨è¡¨ç¤ºå¼•å¯¼ã€‚è™½ç„¶è¿™äº›æ–¹æ³•é‡å»ºè´¨é‡é«˜ï¼Œä½†å¤šæ­¥é‡‡æ ·è¿‡ç¨‹è®¡ç®—å¼€é”€å¤§ï¼Œä¸”å¯¹ä¸åŒå‹ç¼©æ¯”ç‰¹ç‡éœ€è®­ç»ƒä¸åŒæ¨¡å‹ï¼Œå¯¼è‡´è®­ç»ƒå’Œå­˜å‚¨æˆæœ¬é«˜ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºè·¨å¤šæ¯”ç‰¹ç‡çš„æ‰©æ•£ç¼–è§£ç å™¨OSCARã€‚å®ƒå°†å‹ç¼©æ½œåœ¨è§†ä½œåŸå§‹æ½œåœ¨çš„å™ªå£°ç‰ˆæœ¬ï¼Œå¤±çœŸç¨‹åº¦å–å†³äºæ¯”ç‰¹ç‡ã€‚é€šè¿‡æ„å»ºä»å‹ç¼©æ¯”ç‰¹ç‡åˆ°ä¼ªæ‰©æ•£æ—¶æ­¥çš„æ˜ å°„ï¼Œæˆ‘ä»¬ç”¨å•ä¸€ç”Ÿæˆæ¨¡å‹æ”¯æŒå¤šç§æ¯”ç‰¹ç‡çš„é‡å»ºã€‚æˆ‘ä»¬è®¤ä¸ºå‹ç¼©æ½œåœ¨ä¿ç•™äº†ä¸°å¯Œçš„ç»“æ„ä¿¡æ¯ï¼Œä½¿å¾—ä¸€æ­¥å»å™ªå¯è¡Œã€‚å› æ­¤ï¼ŒOSCARç”¨å•é€šå»å™ªæ›¿æ¢è¿­ä»£é‡‡æ ·ï¼Œæ˜¾è‘—æé«˜æ¨ç†æ•ˆç‡ã€‚å®éªŒè¡¨æ˜OSCARåœ¨å®šé‡å’Œè§†è§‰è´¨é‡æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/jp-guo/OSCAR%E5%B9%BF%E5%B3%BB%E3%80%82">https://github.com/jp-guo/OSCARå‘å¸ƒã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒæœ‰æŸå‹ç¼©æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹é€šå¸¸é‡‡ç”¨å¤šæ­¥é‡‡æ ·è¿›è¡Œå›¾åƒé‡å»ºï¼Œè®¡ç®—å¼€é”€å¤§ã€‚</li>
<li>OSCARæå‡ºä¸€ç§è·¨å¤šæ¯”ç‰¹ç‡çš„æ‰©æ•£ç¼–è§£ç å™¨æ–¹æ³•ï¼Œç”¨å•ä¸€ç”Ÿæˆæ¨¡å‹æ”¯æŒå¤šç§æ¯”ç‰¹ç‡çš„å›¾åƒé‡å»ºã€‚</li>
<li>OSCARå°†å‹ç¼©æ½œåœ¨è§†ä½œåŸå§‹æ½œåœ¨çš„å™ªå£°ç‰ˆæœ¬ï¼Œå¹¶å»ºç«‹ä»å‹ç¼©æ¯”ç‰¹ç‡åˆ°ä¼ªæ‰©æ•£æ—¶æ­¥çš„æ˜ å°„ã€‚</li>
<li>OSCARé€šè¿‡ä¸€æ­¥å»å™ªè¿‡ç¨‹æ›¿ä»£è¿­ä»£é‡‡æ ·ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚</li>
<li>å®éªŒè¡¨æ˜OSCARåœ¨å›¾åƒé‡å»ºçš„å®šé‡å’Œè§†è§‰è´¨é‡æŒ‡æ ‡ä¸Šå®ç°ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5430aa2fa5614cf8e6e311b143436be3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30fced3344ddec95fcdb6f16cc547286.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3a6a289238a5a219cc6292763810bc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-003e472d813eadd16c17e71dd2e022d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5aebc4ce4acc0e0bb70f1e5729ef3800.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Non-rigid-Motion-Correction-for-MRI-Reconstruction-via-Coarse-To-Fine-Diffusion-Models"><a href="#Non-rigid-Motion-Correction-for-MRI-Reconstruction-via-Coarse-To-Fine-Diffusion-Models" class="headerlink" title="Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine   Diffusion Models"></a>Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine   Diffusion Models</h2><p><strong>Authors:Frederic Wang, Jonathan I. Tamir</strong></p>
<p>Magnetic Resonance Imaging (MRI) is highly susceptible to motion artifacts due to the extended acquisition times required for k-space sampling. These artifacts can compromise diagnostic utility, particularly for dynamic imaging. We propose a novel alternating minimization framework that leverages a bespoke diffusion model to jointly reconstruct and correct non-rigid motion-corrupted k-space data. The diffusion model uses a coarse-to-fine denoising strategy to capture large overall motion and reconstruct the lower frequencies of the image first, providing a better inductive bias for motion estimation than that of standard diffusion models. We demonstrate the performance of our approach on both real-world cine cardiac MRI datasets and complex simulated rigid and non-rigid deformations, even when each motion state is undersampled by a factor of 64x. Additionally, our method is agnostic to sampling patterns, anatomical variations, and MRI scanning protocols, as long as some low frequency components are sampled during each motion state. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ç”±äºkç©ºé—´é‡‡æ ·æ‰€éœ€çš„é•¿é‡‡é›†æ—¶é—´ï¼Œå¾ˆå®¹æ˜“å—åˆ°è¿åŠ¨ä¼ªå½±çš„å½±å“ã€‚è¿™äº›ä¼ªå½±å¯èƒ½ä¼šé™ä½è¯Šæ–­æ•ˆç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€æˆåƒä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„äº¤æ›¿æœ€å°åŒ–æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä¸“é—¨çš„æ‰©æ•£æ¨¡å‹æ¥è”åˆé‡å»ºå’Œçº æ­£éåˆšæ€§è¿åŠ¨å—æŸçš„kç©ºé—´æ•°æ®ã€‚è¯¥æ‰©æ•£æ¨¡å‹é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„é™å™ªç­–ç•¥ï¼Œå…ˆæ•æ‰æ•´ä½“å¤§è¿åŠ¨å¹¶é‡å»ºå›¾åƒçš„ä½é¢‘éƒ¨åˆ†ï¼Œä¸ºè¿åŠ¨ä¼°è®¡æä¾›æ›´å¥½çš„å½’çº³åç½®ï¼Œä¼˜äºæ ‡å‡†æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œçš„å¿ƒè„ç”µå½±MRIæ•°æ®é›†å’Œå¤æ‚çš„æ¨¡æ‹Ÿåˆšæ€§å’Œéåˆšæ€§å˜å½¢ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•æ€§èƒ½ï¼Œå³ä½¿åœ¨æ¯ç§è¿åŠ¨çŠ¶æ€ä¸‹é‡‡æ ·ç‡é™ä½64å€çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯¹äºé‡‡æ ·æ¨¡å¼ã€è§£å‰–å˜å¼‚å’ŒMRIæ‰«æåè®®å…·æœ‰ä¸å¯çŸ¥æ€§ï¼Œåªè¦åœ¨æ¯ä¸ªè¿åŠ¨çŠ¶æ€ä¸‹é‡‡æ ·ä¸€äº›ä½é¢‘æˆåˆ†å³å¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15057v2">PDF</a> ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„äº¤æ›¿æœ€å°åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨ä¸“é—¨çš„æ‰©æ•£æ¨¡å‹è”åˆé‡å»ºå’Œçº æ­£éåˆšæ€§è¿åŠ¨è…èš€çš„k-spaceæ•°æ®ã€‚è¯¥æ‰©æ•£æ¨¡å‹é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„é™å™ªç­–ç•¥ï¼Œå…ˆæ•æ‰æ•´ä½“å¤§è¿åŠ¨å¹¶é‡å»ºå›¾åƒçš„ä½é¢‘éƒ¨åˆ†ï¼Œä¸ºè¿åŠ¨ä¼°è®¡æä¾›æ›´å¥½çš„å½’çº³åç½®ã€‚æ­¤æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œçš„ç”µå½±å¿ƒè„MRIæ•°æ®é›†å’Œå¤æ‚çš„æ¨¡æ‹Ÿåˆšæ€§å’Œéåˆšæ€§å˜å½¢ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå³ä½¿åœ¨æ¯ç§è¿åŠ¨çŠ¶æ€ä¸‹é‡‡æ ·ç‡ä½è‡³64å€æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¯¥æ–¹æ³•å¯¹é‡‡æ ·æ¨¡å¼ã€è§£å‰–å˜å¼‚å’ŒMRIæ‰«æåè®®å…·æœ‰é€šç”¨æ€§ï¼Œåªè¦åœ¨æ¯ä¸ªè¿åŠ¨çŠ¶æ€ä¸‹é‡‡æ ·ä¸€äº›ä½é¢‘æˆåˆ†å³å¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºçš„äº¤æ›¿æœ€å°åŒ–æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹è”åˆé‡å»ºå’Œçº æ­£éåˆšæ€§è¿åŠ¨è…èš€çš„k-spaceæ•°æ®ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„é™å™ªç­–ç•¥ï¼Œå…ˆæ•æ‰æ•´ä½“å¤§è¿åŠ¨ã€‚</li>
<li>å…ˆé‡å»ºå›¾åƒçš„ä½é¢‘éƒ¨åˆ†ï¼Œä»¥æé«˜è¿åŠ¨ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œçš„ç”µå½±å¿ƒè„MRIæ•°æ®é›†å’Œæ¨¡æ‹Ÿçš„åˆšæ€§å’Œéåˆšæ€§å˜å½¢ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å³ä½¿åœ¨é‡‡æ ·ç‡ä½è‡³64å€çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ä»è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>æ–¹æ³•å¯¹é‡‡æ ·æ¨¡å¼ã€è§£å‰–å˜å¼‚å’ŒMRIæ‰«æåè®®å…·æœ‰é€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-645581bebb039dd369e68b3ea2ddeed4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40c450a3001a4b039f9561669912de35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7a89deb44f02ea190b093675187992e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1879d5afbf5f04752a5b3d36a1b7585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1b84582bd7f57ab6c58ed143a97ad8e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-57052dc9379f13a30206187b3a442480.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4118c0aa3cf8ed7d5566417769df99db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d2a90d028abcb96a0153659c4c1beca.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CraftsMan3D-High-fidelity-Mesh-Generation-with-3D-Native-Generation-and-Interactive-Geometry-Refiner"><a href="#CraftsMan3D-High-fidelity-Mesh-Generation-with-3D-Native-Generation-and-Interactive-Geometry-Refiner" class="headerlink" title="CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and   Interactive Geometry Refiner"></a>CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and   Interactive Geometry Refiner</h2><p><strong>Authors:Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, Xiaoxiao Long</strong></p>
<p>We present a novel generative 3D modeling system, coined CraftsMan, which can generate high-fidelity 3D geometries with highly varied shapes, regular mesh topologies, and detailed surfaces, and, notably, allows for refining the geometry in an interactive manner. Despite the significant advancements in 3D generation, existing methods still struggle with lengthy optimization processes, irregular mesh topologies, noisy surfaces, and difficulties in accommodating user edits, consequently impeding their widespread adoption and implementation in 3D modeling software. Our work is inspired by the craftsman, who usually roughs out the holistic figure of the work first and elaborates the surface details subsequently. Specifically, we employ a 3D native diffusion model, which operates on latent space learned from latent set-based 3D representations, to generate coarse geometries with regular mesh topology in seconds. In particular, this process takes as input a text prompt or a reference image and leverages a powerful multi-view (MV) diffusion model to generate multiple views of the coarse geometry, which are fed into our MV-conditioned 3D diffusion model for generating the 3D geometry, significantly improving robustness and generalizability. Following that, a normal-based geometry refiner is used to significantly enhance the surface details. This refinement can be performed automatically, or interactively with user-supplied edits. Extensive experiments demonstrate that our method achieves high efficacy in producing superior-quality 3D assets compared to existing methods. HomePage: <a target="_blank" rel="noopener" href="https://craftsman3d.github.io/">https://craftsman3d.github.io/</a>, Code: <a target="_blank" rel="noopener" href="https://github.com/wyysf-98/CraftsMan">https://github.com/wyysf-98/CraftsMan</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸‰ç»´ç”Ÿæˆå»ºæ¨¡ç³»ç»Ÿï¼Œåä¸ºCraftsManã€‚è¯¥ç³»ç»Ÿå¯ä»¥ç”Ÿæˆé«˜åº¦é€¼çœŸçš„ä¸‰ç»´å‡ ä½•æ¨¡å‹ï¼Œå…·æœ‰å¤šæ ·åŒ–çš„å½¢çŠ¶ã€è§„åˆ™çš„ç½‘æ ¼æ‹“æ‰‘å’Œç²¾ç»†çš„è¡¨é¢ç»†èŠ‚ï¼Œå¹¶ä¸”èƒ½ä»¥äº¤äº’æ–¹å¼å®Œå–„å‡ ä½•æ¨¡å‹ã€‚å°½ç®¡ä¸‰ç»´ç”ŸæˆæŠ€æœ¯å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä»ç„¶é¢ä¸´ä¼˜åŒ–æµç¨‹å†—é•¿ã€ç½‘æ ¼æ‹“æ‰‘ä¸è§„åˆ™ã€è¡¨é¢å™ªå£°å¤§ä»¥åŠéš¾ä»¥å®¹çº³ç”¨æˆ·ç¼–è¾‘ç­‰é—®é¢˜ï¼Œä»è€Œé˜»ç¢äº†å®ƒä»¬åœ¨ä¸‰ç»´å»ºæ¨¡è½¯ä»¶ä¸­çš„å¹¿æ³›é‡‡ç”¨å’Œå®æ–½ã€‚æˆ‘ä»¬çš„å·¥ä½œå—åˆ°å·¥åŒ çš„å¯å‘ï¼Œä»–ä»¬é€šå¸¸å…ˆå¤§è‡´å‹¾å‹’å‡ºä½œå“çš„æ•´ä½“è½®å»“ï¼Œç„¶åç»†åŒ–è¡¨é¢ç»†èŠ‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä¸‰ç»´æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ä¸Šè¿è¡Œï¼Œè¯¥ç©ºé—´æ˜¯ä»åŸºäºé›†åˆçš„æ½œåœ¨ä¸‰ç»´è¡¨ç¤ºä¸­å­¦ä¹ çš„ï¼Œä»¥åœ¨å‡ ç§’å†…ç”Ÿæˆå…·æœ‰è§„åˆ™ç½‘æ ¼æ‹“æ‰‘çš„ç²—ç•¥å‡ ä½•å½¢çŠ¶ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¿™ä¸ªè¿‡ç¨‹ä»¥æ–‡æœ¬æç¤ºæˆ–å‚è€ƒå›¾åƒä¸ºè¾“å…¥ï¼Œå¹¶åˆ©ç”¨å¼ºå¤§çš„å¤šè§†å›¾ï¼ˆMVï¼‰æ‰©æ•£æ¨¡å‹ç”Ÿæˆç²—ç•¥å‡ ä½•å½¢çŠ¶çš„å¤šè§†å›¾ï¼Œç„¶åå°†å…¶è¾“å…¥æˆ‘ä»¬çš„MVæ¡ä»¶ä¸‰ç»´æ‰©æ•£æ¨¡å‹ä»¥ç”Ÿæˆä¸‰ç»´å‡ ä½•å½¢çŠ¶ï¼Œè¿™å¤§å¤§æé«˜äº†ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚ä¹‹åï¼Œä½¿ç”¨åŸºäºæ³•çº¿çš„å‡ ä½•ç»†åŒ–å™¨æ¥æ˜¾è‘—å¢å¼ºè¡¨é¢ç»†èŠ‚ã€‚è¿™ç§ç»†åŒ–å¯ä»¥è‡ªåŠ¨è¿›è¡Œï¼Œä¹Ÿå¯ä»¥ä¸ç”¨æˆ·æä¾›çš„ç¼–è¾‘è¿›è¡Œäº¤äº’ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´èµ„äº§æ–¹é¢ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰å¾ˆé«˜çš„æœ‰æ•ˆæ€§ã€‚ä¸»é¡µï¼š<a target="_blank" rel="noopener" href="https://craftsman3d.github.io/%EF%BC%8C%E4%BB%A3%E7%A0%81%EF%BC%9Ahttps://github.com/wyysf-98/CraftsMan">https://craftsman3d.github.io/ï¼Œä»£ç ï¼šhttps://github.com/wyysf-98/CraftsMan</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14979v3">PDF</a> HomePage: <a target="_blank" rel="noopener" href="https://craftsman3d.github.io/">https://craftsman3d.github.io/</a>, Code:   <a target="_blank" rel="noopener" href="https://github.com/wyysf-98/CraftsMan3D">https://github.com/wyysf-98/CraftsMan3D</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥é¡¹ç›®æå‡ºäº†ä¸€ç§åä¸ºCraftsMançš„æ–°å‹ä¸‰ç»´å»ºæ¨¡ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´æ¨¡å‹ï¼Œå…·æœ‰å¤šæ ·åŒ–çš„å½¢çŠ¶ã€è§„åˆ™åŒ–çš„ç½‘æ ¼æ‹“æ‰‘å’Œç²¾ç»†çš„è¡¨é¢ç»†èŠ‚ï¼Œå¹¶ä¸”å…è®¸ç”¨æˆ·è¿›è¡Œäº¤äº’å¼å‡ ä½•ç²¾ä¿®ã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œè¯¥é¡¹ç›®é€šè¿‡ä½¿ç”¨ä¸€ç§åœ¨æ½œåœ¨ç©ºé—´ä¸Šè¿è¡Œçš„3Dæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå…·æœ‰è§„åˆ™ç½‘æ ¼æ‹“æ‰‘çš„ç²—ç³™å‡ ä½•ç»“æ„ï¼Œå¹¶åœ¨ä¹‹åé€šè¿‡åŸºäºæ­£å¸¸çš„å‡ ä½•ç»†åŒ–å™¨æå‡è¡¨é¢ç»†èŠ‚ã€‚è¿™ä¸ä»…ç¼©çŸ­äº†ä¼˜åŒ–æµç¨‹ï¼Œè¿˜èƒ½å¤„ç†ç”¨æˆ·ç¼–è¾‘ï¼Œä»è€Œä¿ƒè¿›äº†å…¶åœ¨3Då»ºæ¨¡è½¯ä»¶ä¸­çš„å¹¿æ³›åº”ç”¨å’Œå®æ–½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CraftsManæ˜¯ä¸€ä¸ªæ–°å‹çš„ä¸‰ç»´å»ºæ¨¡ç³»ç»Ÿï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´æ¨¡å‹ï¼ŒåŒ…æ‹¬å¤šæ ·åŒ–çš„å½¢çŠ¶ã€è§„åˆ™åŒ–çš„ç½‘æ ¼æ‹“æ‰‘å’Œç²¾ç»†çš„è¡¨é¢ã€‚</li>
<li>è¯¥ç³»ç»Ÿé‡‡ç”¨äº†ä¸€ç§3Dæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ½œåœ¨ç©ºé—´ä¸Šè¿è¡Œï¼Œä»è€Œå¿«é€Ÿç”Ÿæˆå…·æœ‰è§„åˆ™ç½‘æ ¼æ‹“æ‰‘çš„ç²—ç³™å‡ ä½•ç»“æ„ã€‚</li>
<li>CraftsManå…è®¸ç”¨æˆ·ä»¥äº¤äº’å¼çš„æ–¹å¼è¿›è¡Œå‡ ä½•ç²¾ä¿®ï¼Œæå‡äº†ç³»ç»Ÿçš„å®ç”¨æ€§å’Œç”¨æˆ·ä½“éªŒã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡å¤šè§†è§’ï¼ˆMVï¼‰æ‰©æ•£æ¨¡å‹æé«˜äº†é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥æ›´å¥½åœ°å¤„ç†ä¸åŒçš„è¾“å…¥å’Œç”Ÿæˆæ›´å‡†ç¡®çš„3Då‡ ä½•ã€‚</li>
<li>CraftsManç³»ç»Ÿé€šè¿‡åŸºäºæ­£å¸¸çš„å‡ ä½•ç»†åŒ–å™¨ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹è¡¨é¢çš„ç»†èŠ‚è´¨é‡ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒCraftsManåœ¨ç”Ÿæˆä¼˜è´¨ä¸‰ç»´èµ„äº§æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ•ˆèƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-34fcc19d951424fd8920f10b07a59f06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5bf449708d03a86e4c9efb4e4cd813d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35016a66d5990272953fc5bd05fc344f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98a80358eb5cf508aebb5b903dc26a92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82ac853d4d86cea304537373b6745573.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fa300c0ed08360fc9ae9a8557c55db84.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  DeepChest Dynamic Gradient-Free Task Weighting for Effective Multi-Task   Learning in Chest X-ray Classification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f7aa04e65a0b26ef7500d36cc8243c3a.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  PhysicsNeRF Physics-Guided 3D Reconstruction from Sparse Views
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26551.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
