<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-05-31  DarkDiff Advancing Low-Light Raw Enhancement by Retasking Diffusion   Models for Camera ISP">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-0d2a90d028abcb96a0153659c4c1beca.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-31
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    66 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-31-更新"><a href="#2025-05-31-更新" class="headerlink" title="2025-05-31 更新"></a>2025-05-31 更新</h1><h2 id="DarkDiff-Advancing-Low-Light-Raw-Enhancement-by-Retasking-Diffusion-Models-for-Camera-ISP"><a href="#DarkDiff-Advancing-Low-Light-Raw-Enhancement-by-Retasking-Diffusion-Models-for-Camera-ISP" class="headerlink" title="DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion   Models for Camera ISP"></a>DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion   Models for Camera ISP</h2><p><strong>Authors:Amber Yijia Zheng, Yu Zhang, Jun Hu, Raymond A. Yeh, Chen Chen</strong></p>
<p>High-quality photography in extreme low-light conditions is challenging but impactful for digital cameras. With advanced computing hardware, traditional camera image signal processor (ISP) algorithms are gradually being replaced by efficient deep networks that enhance noisy raw images more intelligently. However, existing regression-based models often minimize pixel errors and result in oversmoothing of low-light photos or deep shadows. Recent work has attempted to address this limitation by training a diffusion model from scratch, yet those models still struggle to recover sharp image details and accurate colors. We introduce a novel framework to enhance low-light raw images by retasking pre-trained generative diffusion models with the camera ISP. Extensive experiments demonstrate that our method outperforms the state-of-the-art in perceptual quality across three challenging low-light raw image benchmarks. </p>
<blockquote>
<p>在极端低光条件下拍摄高质量照片对于数字相机来说是一个挑战，但意义重大。借助先进的计算硬件，传统的相机图像信号处理器（ISP）算法正逐渐被高效的深度网络所取代，这些网络能够更智能地增强噪声原始图像。然而，现有的基于回归的模型通常最小化像素误差，导致低光照片或深阴影过度平滑。近期的工作试图通过从头开始训练扩散模型来解决这一局限，但这些模型仍然难以恢复清晰的图像细节和准确的颜色。我们引入了一种新的框架，通过用相机ISP重新分配预训练的生成扩散模型来提高低光原始图像的质量。大量实验表明，我们的方法在三个具有挑战性的低光原始图像基准测试中，在感知质量上超过了最新技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23743v1">PDF</a> </p>
<p><strong>Summary</strong><br>     深度学习网络逐步取代传统相机图像信号处理器（ISP）算法，提高低光环境下的高质量摄影效果。现有回归模型易导致低光照片或深阴影过度平滑，失去细节。本研究提出一种新型框架，利用预训练的生成扩散模型结合相机ISP技术提升低光环境下的原始图像质量，超越现有技术标准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习网络在低光环境下提高摄影质量具有优势。</li>
<li>传统ISP算法正在被高效深度网络取代。</li>
<li>现有回归模型在处理低光照片时易导致过度平滑，丢失细节。</li>
<li>扩散模型在处理低光环境下的原始图像时表现良好。</li>
<li>研究引入了一种新型框架，结合了预训练的生成扩散模型和相机ISP技术。</li>
<li>该方法在三组具有挑战性的低光原始图像基准测试中实现了超越现有技术的感知质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23743">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0fca65bf8bdebd958636791c4cb2458f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-591c5d83e2f016cc0ca43f7ef15982b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a5cbe3980a6cdee1791e9aac7b36ecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e9a505926508791fdbeb163df2476cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95d90ba4860f588d7363050e7e71bca6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LayerPeeler-Autoregressive-Peeling-for-Layer-wise-Image-Vectorization"><a href="#LayerPeeler-Autoregressive-Peeling-for-Layer-wise-Image-Vectorization" class="headerlink" title="LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization"></a>LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization</h2><p><strong>Authors:Ronghuan Wu, Wanchao Su, Jing Liao</strong></p>
<p>Image vectorization is a powerful technique that converts raster images into vector graphics, enabling enhanced flexibility and interactivity. However, popular image vectorization tools struggle with occluded regions, producing incomplete or fragmented shapes that hinder editability. While recent advancements have explored rule-based and data-driven layer-wise image vectorization, these methods face limitations in vectorization quality and flexibility. In this paper, we introduce LayerPeeler, a novel layer-wise image vectorization approach that addresses these challenges through a progressive simplification paradigm. The key to LayerPeeler’s success lies in its autoregressive peeling strategy: by identifying and removing the topmost non-occluded layers while recovering underlying content, we generate vector graphics with complete paths and coherent layer structures. Our method leverages vision-language models to construct a layer graph that captures occlusion relationships among elements, enabling precise detection and description for non-occluded layers. These descriptive captions are used as editing instructions for a finetuned image diffusion model to remove the identified layers. To ensure accurate removal, we employ localized attention control that precisely guides the model to target regions while faithfully preserving the surrounding content. To support this, we contribute a large-scale dataset specifically designed for layer peeling tasks. Extensive quantitative and qualitative experiments demonstrate that LayerPeeler significantly outperforms existing techniques, producing vectorization results with superior path semantics, geometric regularity, and visual fidelity. </p>
<blockquote>
<p>图像矢量化是一种强大的技术，可将位图图像转换为矢量图形，从而提高灵活性和交互性。然而，流行的图像矢量化工具在处理遮挡区域时遇到困难，产生不完整或碎片化的形状，妨碍了编辑能力。虽然最近的进展探索了基于规则和基于数据分层图像矢量化方法，这些方法在矢量化质量和灵活性方面仍面临局限性。在本文中，我们介绍了LayerPeeler，这是一种新型的分层图像矢量化方法，它通过渐进简化范式来解决这些挑战。LayerPeeler成功的关键在于其自回归剥离策略：通过识别和移除最顶部的非遮挡层并恢复底层内容，我们生成具有完整路径和连贯层结构的矢量图形。我们的方法利用视觉语言模型构建层图，捕获元素之间的遮挡关系，为非遮挡层提供精确的检测和描述。这些描述性字幕用作微调图像扩散模型的编辑指令，以移除已识别的图层。为了确保准确的移除操作，我们采用了局部注意力控制，精确引导模型定位区域，同时忠实保留周围内容。为了支持这一点，我们贡献了一个专门设计用于图层剥离任务的大规模数据集。大量的定量和定性实验表明，LayerPeeler在路径语义、几何规则和视觉保真度方面显著优于现有技术，产生了出色的矢量化结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23740v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://layerpeeler.github.io/">https://layerpeeler.github.io/</a></p>
<p><strong>Summary</strong>：本文介绍了一种新型图像矢量化方法LayerPeeler，采用分层剥离策略将图像矢量化为矢量图形，能处理遮挡区域的挑战，生成完整路径和连贯层结构的矢量图形。利用视觉语言模型构建层图，捕捉元素间的遮挡关系，并使用描述性标题作为编辑指令进行微调图像扩散模型的层剥离。采用局部注意力控制精确指导模型进行目标区域剥离，同时忠实保留周围内容。LayerPeeler在矢量化路径语义、几何规则性和视觉保真度方面显著优于现有技术。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LayerPeeler是一种新型的图像矢量化方法，采用分层剥离策略解决遮挡区域的挑战。</li>
<li>通过识别并移除最顶层的非遮挡层，同时恢复底层内容，生成完整路径和连贯层结构的矢量图形。</li>
<li>利用视觉语言模型构建层图，准确捕捉元素间的遮挡关系。</li>
<li>使用描述性标题作为编辑指令对图像扩散模型进行微调，实现精准层剥离。</li>
<li>采用局部注意力控制，精确指导模型进行目标区域剥离，同时保留周围内容。</li>
<li>LayerPeeler在矢量化路径语义、几何规则性和视觉保真度方面表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23740">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2e374d8e1ce82f1a173e8d8356efdb7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d394156d39908f13442b8dd6adc75455.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-abdb7bf2fa4c5cb2e6ef496bb78edbb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10e692535414ab1c351113d6110af58f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffea8d12d199aa128168c57352a70889.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OpenUni-A-Simple-Baseline-for-Unified-Multimodal-Understanding-and-Generation"><a href="#OpenUni-A-Simple-Baseline-for-Unified-Multimodal-Understanding-and-Generation" class="headerlink" title="OpenUni: A Simple Baseline for Unified Multimodal Understanding and   Generation"></a>OpenUni: A Simple Baseline for Unified Multimodal Understanding and   Generation</h2><p><strong>Authors:Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy</strong></p>
<p>In this report, we present OpenUni, a simple, lightweight, and fully open-source baseline for unifying multimodal understanding and generation. Inspired by prevailing practices in unified model learning, we adopt an efficient training strategy that minimizes the training complexity and overhead by bridging the off-the-shelf multimodal large language models (LLMs) and diffusion models through a set of learnable queries and a light-weight transformer-based connector. With a minimalist choice of architecture, we demonstrate that OpenUni can: 1) generate high-quality and instruction-aligned images, and 2) achieve exceptional performance on standard benchmarks such as GenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To support open research and community advancement, we release all model weights, training code, and our curated training datasets (including 23M image-text pairs) at <a target="_blank" rel="noopener" href="https://github.com/wusize/OpenUni">https://github.com/wusize/OpenUni</a>. </p>
<blockquote>
<p>在这份报告中，我们介绍了OpenUni，这是一个简单、轻量级、完全开源的多模态理解和生成统一基准。我们受到统一模型学习流行实践的启发，采用了一种高效的训练策略，通过一组可学习的查询和一个基于轻量级变压器的连接器，将现成的多模态大型语言模型（LLMs）和扩散模型联系起来，从而最小化训练复杂性和开销。通过选择最简单的架构，我们证明OpenUni可以：1）生成高质量且符合指令的图像；2）在GenEval、DPG-Bench和WISE等标准基准测试上实现卓越性能，激活的参数只有1.1B和3.1B。为了支持开放研究和社区发展，我们在<a target="_blank" rel="noopener" href="https://github.com/wusize/OpenUni">https://github.com/wusize/OpenUni</a>上发布了所有模型权重、训练代码和我们精选的训练数据集（包括2300万张图像文本对）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23661v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>OpenUni是一个简单、轻量级、完全开源的多模态理解和生成统一基准。它通过高效的训练策略，采用现成的多模态大型语言模型和扩散模型，通过一组可学习的查询和轻量级基于变压器的连接器，以减小训练复杂性和开销。OpenUni能够生成高质量、指令对齐的图像，并在GenEval、DPG-Bench和WISE等标准基准测试中实现卓越性能，仅需1.1B和3.1B激活参数。所有模型权重、训练代码和精选的训练数据集（包括23M图像文本对）已在GitHub上发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenUni是一个多模态理解和生成的统一基准。旨在将各种模态的数据（如文本和图像）整合在一起进行理解和生成。</li>
<li>OpenUni具有简单、轻量级和开源的特点，便于社区研究和发展。</li>
<li>通过高效的训练策略，OpenUni能桥接现有的多模态大型语言模型和扩散模型。</li>
<li>OpenUni使用可学习的查询和基于轻量级变压器的连接器来实现这一桥接，以降低训练复杂性和开销。</li>
<li>OpenUni能生成高质量、与指令对齐的图像，显示出其强大的生成能力。</li>
<li>在标准基准测试中，如GenEval、DPG-Bench和WISE，OpenUni表现出卓越的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23661">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7cd20762b5297aaaa5db3a7bd8a103aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2bbcb0e9d34e13c915cfb04cef56811.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4660fa82c63c1a45604f8604520060af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45d31729f8a2776c5c424dbbd0e69f52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-215343cec7a0cfdac617ff0769b3ee6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cc9a75d3eff5ca34e4fd3e764d149f1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LAFR-Efficient-Diffusion-based-Blind-Face-Restoration-via-Latent-Codebook-Alignment-Adapter"><a href="#LAFR-Efficient-Diffusion-based-Blind-Face-Restoration-via-Latent-Codebook-Alignment-Adapter" class="headerlink" title="LAFR: Efficient Diffusion-based Blind Face Restoration via Latent   Codebook Alignment Adapter"></a>LAFR: Efficient Diffusion-based Blind Face Restoration via Latent   Codebook Alignment Adapter</h2><p><strong>Authors:Runyi Li, Bin Chen, Jian Zhang, Radu Timofte</strong></p>
<p>Blind face restoration from low-quality (LQ) images is a challenging task that requires not only high-fidelity image reconstruction but also the preservation of facial identity. While diffusion models like Stable Diffusion have shown promise in generating high-quality (HQ) images, their VAE modules are typically trained only on HQ data, resulting in semantic misalignment when encoding LQ inputs. This mismatch significantly weakens the effectiveness of LQ conditions during the denoising process. Existing approaches often tackle this issue by retraining the VAE encoder, which is computationally expensive and memory-intensive. To address this limitation efficiently, we propose LAFR (Latent Alignment for Face Restoration), a novel codebook-based latent space adapter that aligns the latent distribution of LQ images with that of HQ counterparts, enabling semantically consistent diffusion sampling without altering the original VAE. To further enhance identity preservation, we introduce a multi-level restoration loss that combines constraints from identity embeddings and facial structural priors. Additionally, by leveraging the inherent structural regularity of facial images, we show that lightweight finetuning of diffusion prior on just 0.9% of FFHQ dataset is sufficient to achieve results comparable to state-of-the-art methods, reduce training time by 70%. Extensive experiments on both synthetic and real-world face restoration benchmarks demonstrate the effectiveness and efficiency of LAFR, achieving high-quality, identity-preserving face reconstruction from severely degraded inputs. </p>
<blockquote>
<p>从低质量（LQ）图像中恢复盲脸是一项具有挑战性的任务，不仅要求高质量图像重建，还要求保持面部身份。虽然像Stable Diffusion这样的扩散模型在生成高质量（HQ）图像方面显示出潜力，但它们的VAE模块通常仅在HQ数据上进行训练，导致在编码LQ输入时出现语义不对齐。这种不匹配会显著削弱去噪过程中LQ条件的有效性。现有方法通常通过重新训练VAE编码器来解决这个问题，这在计算上既昂贵又占用大量内存。为了有效地解决这一限制，我们提出了LAFR（用于面部恢复的潜在对齐），这是一种基于代码本的新型潜在空间适配器，它将LQ图像的潜在分布与HQ图像的潜在分布对齐，从而在不改变原始VAE的情况下实现语义一致的扩散采样。为了进一步增强身份保留，我们引入了一种多级恢复损失，它结合了身份嵌入和面部结构先验的限制。此外，通过利用面部图像的内在结构规律，我们证明在FFHQ数据集仅0.9%的数据上进行扩散先验的微调就足以达到与最新方法相当的结果，并将训练时间减少70%。在合成和真实世界面部恢复基准测试上的大量实验证明了LAFR的有效性和效率，实现了从严重退化的输入中进行高质量、保持身份的脸部重建。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23462v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>针对低质量图像中的盲脸恢复任务，存在高保真重建与面部身份保留的双重挑战。虽然Stable Diffusion等扩散模型在高质量图像生成中表现出潜力，但其VAE模块仅在高质量数据上进行训练，导致在编码低质量输入时出现语义不匹配问题。为解决这一问题，本文提出LAFR（面部恢复的潜在对齐）方法，通过基于代码本的潜在空间适配器，对齐低质量图像与高质量图像的潜在分布，实现语义一致的扩散采样，且无需更改原始VAE。为进一步提高身份保留效果，引入多级恢复损失，结合身份嵌入和面部结构先验的限制。实验表明，在少量数据集上微调扩散先验即可实现与先进方法相当的效果，并减少70%的训练时间。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>扩散模型如Stable Diffusion在低质量图像盲脸恢复上应用面临挑战。</li>
<li>现有扩散模型的VAE模块仅在高质量数据上训练，导致低质量输入时的语义不匹配问题。</li>
<li>LAFR方法通过基于代码本的潜在空间适配器对齐低质量与高质量图像的潜在分布。</li>
<li>LAFR实现了语义一致的扩散采样，且无需更改原始VAE设计。</li>
<li>引入多级恢复损失以提高身份保留效果，结合身份嵌入和面部结构先验。</li>
<li>利用面部图像的结构规律性，少量数据集上的扩散先验微调即可实现高效、高质量的面部重建。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23462">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c95342d5ce246371a50724168ec94137.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa300c0ed08360fc9ae9a8557c55db84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0f7135a2f3b2f948f73a55e750d199f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7d760b621fe10b5a655784987fc1eba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e42b5a939afb8f6f7931d5f75dbd7afd.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CryoCCD-Conditional-Cycle-consistent-Diffusion-with-Biophysical-Modeling-for-Cryo-EM-Synthesis"><a href="#CryoCCD-Conditional-Cycle-consistent-Diffusion-with-Biophysical-Modeling-for-Cryo-EM-Synthesis" class="headerlink" title="CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical   Modeling for Cryo-EM Synthesis"></a>CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical   Modeling for Cryo-EM Synthesis</h2><p><strong>Authors:Runmin Jiang, Genpei Zhang, Yuntian Yang, Siqi Wu, Yuheng Zhang, Wanyue Feng, Yizhou Zhao, Xi Xiao, Xiao Wang, Tianyang Wang, Xingjian Li, Min Xu</strong></p>
<p>Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of macromolecules, but developing robust models for downstream analysis is hindered by the scarcity of high-quality annotated data. While synthetic data generation has emerged as a potential solution, existing methods often fail to capture both the structural diversity of biological specimens and the complex, spatially varying noise inherent in cryo-EM imaging. To overcome these limitations, we propose CryoCCD, a synthesis framework that integrates biophysical modeling with generative techniques. Specifically, CryoCCD produces multi-scale cryo-EM micrographs that reflect realistic biophysical variability through compositional heterogeneity, cellular context, and physics-informed imaging. To generate realistic noise, we employ a conditional diffusion model, enhanced by cycle consistency to preserve structural fidelity and mask-aware contrastive learning to capture spatially adaptive noise patterns. Extensive experiments show that CryoCCD generates structurally accurate micrographs and enhances performance in downstream tasks, outperforming state-of-the-art baselines in both particle picking and reconstruction. </p>
<blockquote>
<p>冷冻电子显微镜（cryo-EM）能够为宏观分子提供接近原子分辨率的成像，但是高质量标注数据的稀缺阻碍了下游分析的稳健模型的发展。虽然合成数据生成已经出现作为潜在的解决方案，但现有方法往往无法捕捉到生物样本的结构多样性和冷冻电子显微镜成像中固有的复杂且空间变化的噪声。为了克服这些局限性，我们提出了CryoCCD，这是一个将生物物理建模与生成技术相结合的合成框架。具体来说，CryoCCD产生多尺度的冷冻电子显微镜显微图，通过组成异质性、细胞背景和物理成像反映真实的生物物理变化。为了产生真实的噪声，我们采用条件扩散模型，通过循环一致性增强来保持结构忠实度，并通过掩膜感知对比学习来捕捉空间自适应噪声模式。大量实验表明，CryoCCD生成的显微图结构准确，提高了下游任务的性能，在粒子挑选和重建方面都优于最先进的基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23444v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于冷冻电子显微镜（cryo-EM）的近原子分辨率成像，在下游分析中缺乏高质量注释数据阻碍其发展的背景下，文章提出了一个新的合成数据生成框架CryoCCD。该框架结合了生物物理建模和生成技术，旨在克服现有方法的局限性。通过构建多尺度cryo-EM微图，CryoCCD能够反映真实的生物物理变异性和复杂的空间变化噪声。采用条件扩散模型生成真实噪声，通过循环一致性增强结构保真度，并通过掩膜感知对比学习捕捉空间自适应噪声模式。实验表明，CryoCCD生成的微图结构准确，在下游任务中表现优异，在粒子挑选和重建方面均优于现有基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>冷冻电子显微镜（cryo-EM）成像因缺乏高质量注释数据而在下游分析中受限。</li>
<li>现有的数据生成方法难以捕捉生物样本的结构多样性和复杂的空间变化噪声。</li>
<li>提出新的合成数据生成框架CryoCCD，结合生物物理建模和生成技术。</li>
<li>CryoCCD能够生成多尺度cryo-EM微图，反映真实的生物物理变异性和空间变化噪声。</li>
<li>采用条件扩散模型生成真实噪声，通过循环一致性和掩膜感知对比学习提高性能。</li>
<li>实验证明CryoCCD生成的微图结构准确，在下游任务中表现优异。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23444">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cf2202c72597ebe5476c2f43d610fa8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74332f918f3356e693e9226dded8535c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-838fdc5db52adab5246cb64cf7868c88.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TRACE-Trajectory-Constrained-Concept-Erasure-in-Diffusion-Models"><a href="#TRACE-Trajectory-Constrained-Concept-Erasure-in-Diffusion-Models" class="headerlink" title="TRACE: Trajectory-Constrained Concept Erasure in Diffusion Models"></a>TRACE: Trajectory-Constrained Concept Erasure in Diffusion Models</h2><p><strong>Authors:Finn Carter</strong></p>
<p>Text-to-image diffusion models have shown unprecedented generative capability, but their ability to produce undesirable concepts (e.g.<del>pornographic content, sensitive identities, copyrighted styles) poses serious concerns for privacy, fairness, and safety. {Concept erasure} aims to remove or suppress specific concept information in a generative model. In this paper, we introduce \textbf{TRACE (Trajectory-Constrained Attentional Concept Erasure)}, a novel method to erase targeted concepts from diffusion models while preserving overall generative quality. Our approach combines a rigorous theoretical framework, establishing formal conditions under which a concept can be provably suppressed in the diffusion process, with an effective fine-tuning procedure compatible with both conventional latent diffusion (Stable Diffusion) and emerging rectified flow models (e.g.</del>FLUX). We first derive a closed-form update to the model’s cross-attention layers that removes hidden representations of the target concept. We then introduce a trajectory-aware finetuning objective that steers the denoising process away from the concept only in the late sampling stages, thus maintaining the model’s fidelity on unrelated content. Empirically, we evaluate TRACE on multiple benchmarks used in prior concept erasure studies (object classes, celebrity faces, artistic styles, and explicit content from the I2P dataset). TRACE achieves state-of-the-art performance, outperforming recent methods such as ANT, EraseAnything, and MACE in terms of removal efficacy and output quality. </p>
<blockquote>
<p>文本到图像的扩散模型展现出了前所未有的生成能力，但它们产生不想要概念的能力（例如色情内容、敏感身份、版权风格）对隐私、公平和安全提出了严重担忧。{概念擦除}旨在从生成模型中删除或抑制特定概念信息。在本文中，我们介绍了\textbf{TRACE（轨迹约束注意力概念擦除）}，这是一种从扩散模型中删除目标概念的同时保持整体生成质量的新方法。我们的方法结合了一个严谨的理论框架，建立了在扩散过程中可以抑制概念的正式条件，以及一个与常规潜在扩散（Stable Diffusion）和新兴整流流模型（例如FLUX）兼容的有效微调程序。我们首先推导出模型交叉注意力层的封闭形式更新，以消除目标概念的隐藏表示。然后，我们引入了一个轨迹感知微调目标，该目标仅在后期采样阶段引导去噪过程远离概念，从而保持模型在处理不相关内容时的保真度。从实证角度看，我们在先前概念擦除研究使用的多个基准测试（如对象类别、名人面孔、艺术风格和I2P数据集中的明确内容）上对TRACE进行了评估。TRACE达到了最新性能水平，在去除效果和输出质量方面超越了近期的方法，如ANT、EraseAnything和MACE。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23312v1">PDF</a> In peer review</p>
<p><strong>Summary</strong></p>
<p>文本到图像扩散模型的生成能力前所未有，但其生成不希望出现概念（如色情内容、敏感身份、版权风格）的能力对隐私、公平性和安全构成严重威胁。本文介绍了一种名为TRACE（轨迹约束注意力概念消除）的新方法，旨在从扩散模型中删除目标概念，同时保持整体生成质量。该方法结合了严格的理论框架和有效的微调程序，可与传统的潜在扩散（Stable Diffusion）和新兴的校正流模型（如FLUX）兼容。实验表明，TRACE在多个基准测试上的表现达到或超越了最新的方法，如ANT、EraseAnything和MACE。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像扩散模型具有强大的生成能力，但存在生成不希望出现概念的问题。</li>
<li>概念消除技术对于保护隐私、公平性和安全至关重要。</li>
<li>TRACE是一种新型的概念消除方法，旨在从扩散模型中删除特定概念，同时保持整体生成质量。</li>
<li>TRACE结合了严格的理论框架和有效的微调程序，可与多种扩散模型兼容。</li>
<li>TRACE通过修改模型的跨注意层来消除目标概念的隐藏表示。</li>
<li>TRACE引入了轨迹感知微调目标，仅在后期采样阶段避免概念，从而保持模型对无关内容的保真度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23312">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4828790ec6a5c687a4734a793bc18a0a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RSFAKE-1M-A-Large-Scale-Dataset-for-Detecting-Diffusion-Generated-Remote-Sensing-Forgeries"><a href="#RSFAKE-1M-A-Large-Scale-Dataset-for-Detecting-Diffusion-Generated-Remote-Sensing-Forgeries" class="headerlink" title="RSFAKE-1M: A Large-Scale Dataset for Detecting Diffusion-Generated   Remote Sensing Forgeries"></a>RSFAKE-1M: A Large-Scale Dataset for Detecting Diffusion-Generated   Remote Sensing Forgeries</h2><p><strong>Authors:Zhihong Tan, Jiayi Wang, Huiying Shi, Binyuan Huang, Hongchen Wei, Zhenzhong Chen</strong></p>
<p>Detecting forged remote sensing images is becoming increasingly critical, as such imagery plays a vital role in environmental monitoring, urban planning, and national security. While diffusion models have emerged as the dominant paradigm for image generation, their impact on remote sensing forgery detection remains underexplored. Existing benchmarks primarily target GAN-based forgeries or focus on natural images, limiting progress in this critical domain. To address this gap, we introduce RSFAKE-1M, a large-scale dataset of 500K forged and 500K real remote sensing images. The fake images are generated by ten diffusion models fine-tuned on remote sensing data, covering six generation conditions such as text prompts, structural guidance, and inpainting. This paper presents the construction of RSFAKE-1M along with a comprehensive experimental evaluation using both existing detectors and unified baselines. The results reveal that diffusion-based remote sensing forgeries remain challenging for current methods, and that models trained on RSFAKE-1M exhibit notably improved generalization and robustness. Our findings underscore the importance of RSFAKE-1M as a foundation for developing and evaluating next-generation forgery detection approaches in the remote sensing domain. The dataset and other supplementary materials are available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/TZHSW/RSFAKE/">https://huggingface.co/datasets/TZHSW/RSFAKE/</a>. </p>
<blockquote>
<p>遥感图像伪造检测在环境监控、城市规划和国家安全中发挥着至关重要的作用，因此变得越来越关键。尽管扩散模型已成为图像生成的主导范式，但它们对遥感伪造检测的影响仍被探索不足。现有的基准测试主要面向基于GAN的伪造或自然图像，限制了这一关键领域的进展。为了弥补这一空白，我们推出了RSFAKE-1M，这是一个包含50万张伪造和50万张真实遥感图像的大规模数据集。伪造图像是由十个在遥感数据上微调过的扩散模型生成的，涵盖了文本提示、结构引导和图像补全等六种生成条件。本文介绍了RSFAKE-1M的构建，以及使用现有检测器和统一基线进行的全面实验评估。结果表明，基于扩散的遥感伪造对当前方法仍然具有挑战性，而在RSFAKE-1M上训练的模型表现出显著的提高推广能力和稳健性。我们的研究结果强调了RSFAKE-1M在遥感领域开发下一代伪造检测方法的评估基础方面的重要性。数据集和其他辅助材料可在<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/TZHSW/RSFAKE/%E6%89%BE%E5%88%B0%E3%80%82">https://huggingface.co/datasets/TZHSW/RSFAKE/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23283v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了RSFAKE-1M数据集的建设，该数据集包含50万张伪造的和真实的遥感图像。该数据集的伪造图像由经过遥感数据微调后的十个扩散模型生成，涵盖文本提示、结构指导等六种生成条件。实验评估表明，基于扩散的遥感伪造图像对当前方法仍然具有挑战性，而在RSFAKE-1M上训练的模型表现出显著的提高推广和稳健性。该数据集对开发下一代遥感领域伪造检测技术至关重要。该数据集可通过[<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/TZHSW/RSFAKE/]%E8%8E%B7%E5%8F%96%E3%80%82">https://huggingface.co/datasets/TZHSW/RSFAKE/]获取。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RSFAKE-1M数据集包含大量的伪造和真实遥感图像，为遥感伪造检测研究提供了丰富的数据资源。</li>
<li>数据集中的伪造图像由多种扩散模型生成，涵盖不同的生成条件，增加了研究的复杂性。</li>
<li>当前方法在检测基于扩散的遥感伪造图像时仍面临挑战。</li>
<li>在RSFAKE-1M上训练的模型表现出更好的推广和稳健性。</li>
<li>RSFAKE-1M数据集对开发下一代遥感领域伪造检测技术具有关键作用。</li>
<li>该数据集促进了遥感图像伪造检测领域的研究进展和实际应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23283">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fcf9ee016cc783e357f9fe38af0ef372.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfb9ce74855223095a0d882f89f7ff25.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c8e51e35a264877f46aba459f5c70e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55fcd9754e53e39916f464b05e214791.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80bc005d2c97351d00017fc91b89f6a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-386c0be5edd59589036da21bc90e72f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51ee256db155cf5e8d2352a2d71ca106.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Image-Aesthetic-Reasoning-A-New-Benchmark-for-Medical-Image-Screening-with-MLLMs"><a href="#Image-Aesthetic-Reasoning-A-New-Benchmark-for-Medical-Image-Screening-with-MLLMs" class="headerlink" title="Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening   with MLLMs"></a>Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening   with MLLMs</h2><p><strong>Authors:Zheng Sun, Yi Wei, Long Yu</strong></p>
<p>Multimodal Large Language Models (MLLMs) are of great application across many domains, such as multimodal understanding and generation. With the development of diffusion models (DM) and unified MLLMs, the performance of image generation has been significantly improved, however, the study of image screening is rare and its performance with MLLMs is unsatisfactory due to the lack of data and the week image aesthetic reasoning ability in MLLMs. In this work, we propose a complete solution to address these problems in terms of data and methodology. For data, we collect a comprehensive medical image screening dataset with 1500+ samples, each sample consists of a medical image, four generated images, and a multiple-choice answer. The dataset evaluates the aesthetic reasoning ability under four aspects: \textit{(1) Appearance Deformation, (2) Principles of Physical Lighting and Shadow, (3) Placement Layout, (4) Extension Rationality}. For methodology, we utilize long chains of thought (CoT) and Group Relative Policy Optimization with Dynamic Proportional Accuracy reward, called DPA-GRPO, to enhance the image aesthetic reasoning ability of MLLMs. Our experimental results reveal that even state-of-the-art closed-source MLLMs, such as GPT-4o and Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic reasoning. In contrast, by leveraging the reinforcement learning approach, we are able to surpass the score of both large-scale models and leading closed-source models using a much smaller model. We hope our attempt on medical image screening will serve as a regular configuration in image aesthetic reasoning in the future. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）在众多领域具有广泛的应用，如多模态理解和生成。随着扩散模型（DM）和统一MLLMs的发展，图像生成的性能得到了显著提高。然而，图像筛选的研究很少，其与MLLMs的性能也不尽如人意，这是由于数据缺乏以及MLLMs中图像美学推理能力的薄弱。在这项工作中，我们针对这些问题在数据和方法上提出了全面的解决方案。在数据方面，我们收集了一个全面的医学图像筛选数据集，包含1500多个样本，每个样本由一张医学图像、四张生成的图像和多个选择题组成。该数据集从以下四个方面评估美学推理能力：（1）外观变形；（2）物理照明和阴影原则；（3）放置布局；（4）扩展合理性。在方法上，我们利用长链思维（CoT）和动态比例精度奖励的集团相对策略优化（称为DPA-GRPO），以提高MLLMs的图像美学推理能力。我们的实验结果表明，即使是最先进的闭源MLLMs，如GPT-4o和Qwen-VL-Max，在图像美学推理方面的表现也与随机猜测无异。相比之下，通过利用强化学习方法，我们能够在模型规模较小的情况下超越这两个大规模模型的得分，并领先其他闭源模型。我们希望我们的医学图像筛选尝试未来能在图像美学推理中成为一种常规配置。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23265v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>多模态大型语言模型（MLLMs）在多个领域有广泛应用，如多模态理解和生成。扩散模型（DM）和统一MLLMs的发展已显著改善图像生成性能，但图像筛选研究稀少，其与MLLMs的性能也不尽人意，主要由于数据缺乏以及MLLMs中图像美学推理能力薄弱。本研究提出针对这些问题在数据和方法上的完整解决方案。在数据方面，我们收集了一个综合医疗图像筛选数据集，包含1500多个样本，每个样本包括医疗图像、四个生成图像和多个选择题。该数据集评估了美学推理能力的四个方面：外观变形、物理照明和阴影原则、放置布局、扩展合理性。在方法上，我们利用长链思维和带有动态比例精度奖励的集团相对策略优化（DPA-GRPO），提高MLLMs的图像美学推理能力。实验结果表明，即使是最先进的闭源MLLMs，如GPT-4o和Qwen-VL-Max，在图像美学推理方面的表现也如同随机猜测。相比之下，通过利用强化学习的方法，即使使用较小的模型，我们也能够超越大型模型的得分，达到领先水平。我们希望我们的医疗图像筛选尝试能为未来的图像美学推理提供常规配置。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在图像生成方面表现出色，但在图像筛选方面的应用不尽人意。</li>
<li>数据缺乏和MLLMs中图像美学推理能力薄弱是主要原因。</li>
<li>提出一个综合医疗图像筛选数据集，包含多方面的美学推理能力评估。</li>
<li>采用长链思维和带有动态比例精度奖励的集团相对策略优化（DPA-GRPO）方法提高MLLMs的图像美学推理能力。</li>
<li>最先进的闭源MLLMs在图像美学推理方面的表现有限。</li>
<li>利用强化学习的方法能够在较小的模型上超越大型模型的得分。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23265">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8d19a4c72639245ea283e992fd20ca2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94f800422ed87ab8d8f6aeab3fe7c84d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b6950cabc4284cd495fc92aaacd877a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GeoMan-Temporally-Consistent-Human-Geometry-Estimation-using-Image-to-Video-Diffusion"><a href="#GeoMan-Temporally-Consistent-Human-Geometry-Estimation-using-Image-to-Video-Diffusion" class="headerlink" title="GeoMan: Temporally Consistent Human Geometry Estimation using   Image-to-Video Diffusion"></a>GeoMan: Temporally Consistent Human Geometry Estimation using   Image-to-Video Diffusion</h2><p><strong>Authors:Gwanghyun Kim, Xueting Li, Ye Yuan, Koki Nagano, Tianye Li, Jan Kautz, Se Young Chun, Umar Iqbal</strong></p>
<p>Estimating accurate and temporally consistent 3D human geometry from videos is a challenging problem in computer vision. Existing methods, primarily optimized for single images, often suffer from temporal inconsistencies and fail to capture fine-grained dynamic details. To address these limitations, we present GeoMan, a novel architecture designed to produce accurate and temporally consistent depth and normal estimations from monocular human videos. GeoMan addresses two key challenges: the scarcity of high-quality 4D training data and the need for metric depth estimation to accurately model human size. To overcome the first challenge, GeoMan employs an image-based model to estimate depth and normals for the first frame of a video, which then conditions a video diffusion model, reframing video geometry estimation task as an image-to-video generation problem. This design offloads the heavy lifting of geometric estimation to the image model and simplifies the video model’s role to focus on intricate details while using priors learned from large-scale video datasets. Consequently, GeoMan improves temporal consistency and generalizability while requiring minimal 4D training data. To address the challenge of accurate human size estimation, we introduce a root-relative depth representation that retains critical human-scale details and is easier to be estimated from monocular inputs, overcoming the limitations of traditional affine-invariant and metric depth representations. GeoMan achieves state-of-the-art performance in both qualitative and quantitative evaluations, demonstrating its effectiveness in overcoming longstanding challenges in 3D human geometry estimation from videos. </p>
<blockquote>
<p>从视频中估计准确且时间上一致的3D人体几何是计算机视觉中的一个具有挑战性的问题。现有方法主要针对单张图像进行优化，常常存在时间上的不一致性，并且无法捕捉精细的动态细节。为了解决这些局限性，我们提出了GeoMan，这是一种新型架构，旨在从单目人体视频中产生准确且时间上一致的深度和法线估计。GeoMan解决了两个关键挑战：高质量4D训练数据的稀缺性和对度量深度估计以准确建模人体尺寸的需求。为了克服第一个挑战，GeoMan采用基于图像的模型来估计视频的第一帧的深度和法线，然后将条件应用于视频扩散模型，将视频几何估计任务重新定位为图像到视频的生成问题。这种设计将几何估计的重担转移给了图像模型，简化了视频模型的角色，使其专注于细节，同时使用从大规模视频数据集中学习的先验知识。因此，GeoMan提高了时间一致性和通用性，同时只需要最少的4D训练数据。为了解决准确估计人体尺寸的挑战，我们引入了一种根相对深度表示法，该方法保留了关键的人体尺度细节，并且更容易从单目输入中进行估计，克服了传统的仿射不变和度量深度表示法的局限性。GeoMan在定性和定量评估中都达到了最先进的性能，证明了其在克服从视频中估计3D人体几何的长期挑战中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23085v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/dair/geoman">https://research.nvidia.com/labs/dair/geoman</a></p>
<p><strong>Summary</strong><br>在视频中的人体三维几何准确且时间一致的估计是计算机视觉中的一个难题。现有方法主要针对单张图像进行优化，常常存在时间不一致性且无法捕捉精细的动态细节。为了解决这个问题，我们提出了GeoMan模型，能够从单目人体视频中准确且时间一致地估计深度和法线。GeoMan解决了高质量四维训练数据的稀缺性和精确建模人体尺寸所需的度量深度估计这两个关键问题。通过基于图像的模型估计视频第一帧的深度和法线，并以此为条件构建视频扩散模型，将视频几何估计任务重构为图像到视频的生成问题。这种设计简化了视频模型的任务，使其专注于细节，并利用从大规模视频数据集中学习的先验知识。因此，GeoMan提高了时间一致性和泛化能力，同时需要很少的四维训练数据。引入根相对深度表示法来解决准确估计人体尺寸的挑战，保留了关键的人体尺度细节，并更容易从单目输入进行估计，克服了传统的仿射不变和度量深度表示方法的局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GeoMan模型能够准确且时间一致地从视频中估计人体三维几何。</li>
<li>现有方法主要面对两个挑战：四维训练数据的稀缺性和人体尺寸的精确建模。</li>
<li>GeoMan通过将视频几何估计任务转化为图像到视频的生成问题来解决这两个挑战。</li>
<li>通过基于图像的模型估计视频第一帧的深度和法线，并以此为条件构建视频扩散模型。</li>
<li>根相对深度表示法用于准确估计人体尺寸，同时保留关键的人体尺度细节。</li>
<li>该模型实现了在定性和定量评估中的最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23085">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-275fb7597c36973038759182ff49fa24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79e2148cf068aa5c5163e7aaa410cf1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a655d877bf5ef963092d0c3c3baab5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd5fe9054641381efd5ae8f823074559.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dbf06b9bd134fc67e4deddd0876ffd86.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="EquiReg-Equivariance-Regularized-Diffusion-for-Inverse-Problems"><a href="#EquiReg-Equivariance-Regularized-Diffusion-for-Inverse-Problems" class="headerlink" title="EquiReg: Equivariance Regularized Diffusion for Inverse Problems"></a>EquiReg: Equivariance Regularized Diffusion for Inverse Problems</h2><p><strong>Authors:Bahareh Tolooshams, Aditi Chandrashekar, Rayhan Zirvi, Abbas Mammadov, Jiachen Yao, Chuwei Wang, Anima Anandkumar</strong></p>
<p>Diffusion models represent the state-of-the-art for solving inverse problems such as image restoration tasks. In the Bayesian framework, diffusion-based inverse solvers incorporate a likelihood term to guide the prior sampling process, generating data consistent with the posterior distribution. However, due to the intractability of the likelihood term, many current methods rely on isotropic Gaussian approximations, which lead to deviations from the data manifold and result in inconsistent, unstable reconstructions. We propose Equivariance Regularized (EquiReg) diffusion, a general framework for regularizing posterior sampling in diffusion-based inverse problem solvers. EquiReg enhances reconstructions by reweighting diffusion trajectories and penalizing those that deviate from the data manifold. We define a new distribution-dependent equivariance error, empirically identify functions that exhibit low error for on-manifold samples and higher error for off-manifold samples, and leverage these functions to regularize the diffusion sampling process. When applied to a variety of solvers, EquiReg outperforms state-of-the-art diffusion models in both linear and nonlinear image restoration tasks, as well as in reconstructing partial differential equations. </p>
<blockquote>
<p>扩散模型代表了解决图像恢复等逆向问题的最新技术前沿。在贝叶斯框架下，基于扩散的逆向求解器融入一个可能性术语来引导先验采样过程，生成与后验分布一致的数据。然而，由于可能性术语的不可预测性，许多当前的方法依赖于各向同性高斯近似，这会导致偏离数据流形并产生不一致、不稳定的重建结果。我们提出了等价正则化（EquiReg）扩散，这是一个用于基于扩散的逆向问题求解器中的后验采样的通用框架。EquiReg通过重新加权扩散轨迹并惩罚那些偏离数据流形的轨迹来增强重建效果。我们定义了一个新的分布相关的等价误差，实证确定了对于流形内样本显示低误差而对于流形外样本显示更高误差的函数，并利用这些函数来规范扩散采样过程。当应用于各种求解器时，EquiReg在线性和非线性图像恢复任务以及偏微分方程重建中都优于最先进的扩散模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22973v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散模型是解决图像恢复等逆问题的最新技术。在贝叶斯框架下，基于扩散的逆求解器通过结合可能性术语来引导先验采样过程，生成与后验分布一致的数据。然而，由于可能性术语的不可计算性，许多当前的方法依赖于各向同性的高斯近似，这会导致偏离数据流形并产生不一致、不稳定的重建。我们提出了等距正则化（EquiReg）扩散，这是一个用于正则化基于扩散的逆问题求解器中的后验采样的通用框架。EquiReg通过重新加权扩散轨迹并惩罚偏离数据流形的轨迹来增强重建效果。我们定义了一个新的分布相关的等距误差，实证确定了对于流形内样本具有低误差而对于流形外样本具有较高误差的函数，并利用这些函数来正则化扩散采样过程。在多种求解器上应用时，EquiReg在线性和非线性图像恢复任务以及偏微分方程重建中均表现出优于最新扩散模型的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型是解决逆问题的最新技术，尤其在图像恢复领域有出色表现。</li>
<li>在贝叶斯框架下，扩散模型结合可能性术语来引导先验采样。</li>
<li>当前方法因可能性术语的不可计算性而依赖各向同性的高斯近似，这可能导致数据流形偏离并产生不稳定的重建结果。</li>
<li>提出的EquiReg扩散框架用于正则化基于扩散的逆问题求解器中的后验采样。</li>
<li>EquiReg通过重新加权扩散轨迹并惩罚偏离数据流形的轨迹来增强重建效果。</li>
<li>EquiReg定义了一个新的分布相关的等距误差来衡量样本的质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22973">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-71268d1041b7bb3c01878fef096171aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-462f48d63ae5842c6e9fdd93fa0ee8e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-045221756c3a37d3ebc360b96bb6d77e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Plug-and-Play-Posterior-Sampling-for-Blind-Inverse-Problems"><a href="#Plug-and-Play-Posterior-Sampling-for-Blind-Inverse-Problems" class="headerlink" title="Plug-and-Play Posterior Sampling for Blind Inverse Problems"></a>Plug-and-Play Posterior Sampling for Blind Inverse Problems</h2><p><strong>Authors:Anqi Li, Weijie Gan, Ulugbek S. Kamilov</strong></p>
<p>We introduce Blind Plug-and-Play Diffusion Models (Blind-PnPDM) as a novel framework for solving blind inverse problems where both the target image and the measurement operator are unknown. Unlike conventional methods that rely on explicit priors or separate parameter estimation, our approach performs posterior sampling by recasting the problem into an alternating Gaussian denoising scheme. We leverage two diffusion models as learned priors: one to capture the distribution of the target image and another to characterize the parameters of the measurement operator. This PnP integration of diffusion models ensures flexibility and ease of adaptation. Our experiments on blind image deblurring show that Blind-PnPDM outperforms state-of-the-art methods in terms of both quantitative metrics and visual fidelity. Our results highlight the effectiveness of treating blind inverse problems as a sequence of denoising subproblems while harnessing the expressive power of diffusion-based priors. </p>
<blockquote>
<p>我们引入盲插播扩散模型（Blind-PnPDM）作为一个新的框架，用于解决盲反问题，其中目标图像和测量算子都是未知的。不同于依赖显式先验或单独参数估计的传统方法，我们的方法通过把问题转化为交替高斯去噪方案来执行后验采样。我们利用两个扩散模型作为先验知识：一个用于捕捉目标图像的分布，另一个用于描述测量算子的参数。这种PnP扩散模型的集成确保了灵活性和易于适应。我们在盲图像去模糊实验上表明，Blind-PnPDM在定量指标和视觉保真度方面都优于最先进的方法。我们的结果突出了将盲反问题视为一系列去噪子问题序列的有效性，同时利用基于扩散的先验知识的表现力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22923v1">PDF</a> arXiv admin note: text overlap with arXiv:2305.12672</p>
<p><strong>Summary</strong></p>
<p>本文介绍了盲插接扩散模型（Blind-PnPDM），这是一种解决盲逆问题的新型框架，该框架在目标图像和测量算子均未知的情况下运作。与传统方法不同，该方法通过重新构建问题为交替高斯去噪方案进行后验采样，而无需依赖显式先验或单独参数估计。实验证明，在盲图像去模糊任务上，Blind-PnPDM在定量指标和视觉保真度方面均优于现有最先进的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Blind-PnPDM是一种解决盲逆问题的新型框架，适用于目标图像和测量算子均未知的情况。</li>
<li>该方法通过交替高斯去噪方案进行后验采样，无需显式先验或单独参数估计。</li>
<li>Blind-PnPDM利用两个扩散模型作为先验知识：一个用于捕捉目标图像分布，另一个用于描述测量算子的参数。</li>
<li>Blind-PnPDM在盲图像去模糊任务上表现出优异性能，优于现有最先进的方法。</li>
<li>该方法具有灵活性和适应性，易于适应不同的任务。</li>
<li>实验结果证明了将盲逆问题视为一系列去噪子问题的有效性。</li>
<li>扩散模型作为先验知识，展现了其强大的表达能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22923">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4ace22b73954869c7f00088e80b42a9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1202074e1a4b7c51d99a9abdf113e332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97f6ce60fcd7e8ae7337ad220dc2a481.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d58515b2280c99163d4a3e4a4866af0f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Re-ttention-Ultra-Sparse-Visual-Generation-via-Attention-Statistical-Reshape"><a href="#Re-ttention-Ultra-Sparse-Visual-Generation-via-Attention-Statistical-Reshape" class="headerlink" title="Re-ttention: Ultra Sparse Visual Generation via Attention Statistical   Reshape"></a>Re-ttention: Ultra Sparse Visual Generation via Attention Statistical   Reshape</h2><p><strong>Authors:Ruichen Chen, Keith G. Mills, Liyao Jiang, Chao Gao, Di Niu</strong></p>
<p>Diffusion Transformers (DiT) have become the de-facto model for generating high-quality visual content like videos and images. A huge bottleneck is the attention mechanism where complexity scales quadratically with resolution and video length. One logical way to lessen this burden is sparse attention, where only a subset of tokens or patches are included in the calculation. However, existing techniques fail to preserve visual quality at extremely high sparsity levels and might even incur non-negligible compute overheads. % To address this concern, we propose Re-ttention, which implements very high sparse attention for visual generation models by leveraging the temporal redundancy of Diffusion Models to overcome the probabilistic normalization shift within the attention mechanism. Specifically, Re-ttention reshapes attention scores based on the prior softmax distribution history in order to preserve the visual quality of the full quadratic attention at very high sparsity levels. % Experimental results on T2V&#x2F;T2I models such as CogVideoX and the PixArt DiTs demonstrate that Re-ttention requires as few as 3.1% of the tokens during inference, outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and MInference. Further, we measure latency to show that our method can attain over 45% end-to-end % and over 92% self-attention latency reduction on an H100 GPU at negligible overhead cost.   Code available online here: \href{<a target="_blank" rel="noopener" href="https://github.com/cccrrrccc/Re-ttention%7D%7Bhttps://github.com/cccrrrccc/Re-ttention%7D">https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}</a> </p>
<blockquote>
<p>扩散Transformer（DiT）已成为生成高质量视觉内容（如视频和图像）的默认模型。一个巨大的瓶颈是注意力机制，其中复杂性随分辨率和视频长度的增加而二次方增长。减轻这一负担的一种逻辑方法是稀疏注意力，其中仅包括一部分令牌或补丁进行计算。然而，现有技术在极高的稀疏性水平下无法保持视觉质量，并可能产生不可忽略的计算开销。为了解决这一担忧，我们提出了Re-ttention，它通过利用扩散模型的时序冗余来实现视觉生成模型的极高稀疏注意力，以克服注意力机制内的概率归一化移位。具体来说，Re-ttention根据先前的softmax分布历史重新调整注意力分数，以在极高的稀疏性水平下保持全二次注意力的视觉质量。在T2V&#x2F;T2I模型（如CogVideoX和PixArt DiTs）上的实验结果表明，Re-ttention在推理过程中只需要3.1%的令牌，超越了FastDiTAttn、Sparse VideoGen和MInference等当代方法。此外，我们通过测量延迟来展示我们的方法可以在H100 GPU上实现超过45%的端到端延迟减少和超过92%的自注意力延迟减少，同时几乎不增加开销成本。代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/cccrrrccc/Re-ttention">https://github.com/cccrrrccc/Re-ttention</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22918v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>扩散Transformer（DiT）已成为生成高质量视觉内容（如视频和图像）的默认模型。当前面临的一个巨大瓶颈是注意力机制，其复杂性随分辨率和视频长度的增加而呈二次方增长。为减轻这一负担，人们采用稀疏注意力法，仅选择一部分令牌或补丁进行计算。然而，现有技术在极高的稀疏水平上无法保持视觉质量，并可能产生不可忽略的计算开销。为解决这一问题，我们提出Re-ttention，通过利用扩散模型的时序冗余性，实现对视觉生成模型的极高稀疏注意力。Re-ttention根据先前的softmax分布历史重塑注意力分数，以在极高的稀疏水平上保持全二次注意力视觉质量。在T2V&#x2F;T2I模型（如CogVideoX和PixArt DiTs）上的实验结果表明，Re-ttention在推理过程中仅需3.1%的令牌，超越了FastDiTAttn、Sparse VideoGen和MInference等当代方法。此外，我们测量延迟时间，证明该方法可在H100 GPU上实现超过45%的端到端和超过92%的自注意力延迟减少，同时几乎不增加开销。相关代码可通过<a target="_blank" rel="noopener" href="https://github.com/cccrrrccc/Re-ttention%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/cccrrrccc/Re-ttention访问。</a></p>
<p><strong>要点速递</strong></p>
<ol>
<li>扩散Transformer（DiT）已成为高质量视觉内容生成的标准模型，面临注意力机制复杂性的挑战。</li>
<li>现有稀疏注意力技术在高稀疏性时视觉质量下降并可能产生额外计算开销。</li>
<li>Re-ttention通过利用扩散模型的时序冗余性，实现高稀疏注意力，旨在解决上述问题。</li>
<li>Re-ttention通过重塑注意力分数，在极高稀疏水平上保持视觉质量。</li>
<li>在T2V&#x2F;T2I模型上的实验表明，Re-ttention仅需少量令牌即可超越其他方法。</li>
<li>Re-ttention能有效减少延迟，提高模型效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22918">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3a75cd7f47adc14567e99e44316bdc5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30216dafe9bcff6b84a9b4339723b5f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7905eafd41f579c842d54bda2d8a4f4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d71d2f80fd6570fad70cdaf1f531b0ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9478fd57d613165c33ec1defc3bccfb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Diffusion-Classifiers-Understand-Compositionality-but-Conditions-Apply"><a href="#Diffusion-Classifiers-Understand-Compositionality-but-Conditions-Apply" class="headerlink" title="Diffusion Classifiers Understand Compositionality, but Conditions Apply"></a>Diffusion Classifiers Understand Compositionality, but Conditions Apply</h2><p><strong>Authors:Yujin Jeong, Arnas Uselis, Seong Joon Oh, Anna Rohrbach</strong></p>
<p>Understanding visual scenes is fundamental to human intelligence. While discriminative models have significantly advanced computer vision, they often struggle with compositional understanding. In contrast, recent generative text-to-image diffusion models excel at synthesizing complex scenes, suggesting inherent compositional capabilities. Building on this, zero-shot diffusion classifiers have been proposed to repurpose diffusion models for discriminative tasks. While prior work offered promising results in discriminative compositional scenarios, these results remain preliminary due to a small number of benchmarks and a relatively shallow analysis of conditions under which the models succeed. To address this, we present a comprehensive study of the discriminative capabilities of diffusion classifiers on a wide range of compositional tasks. Specifically, our study covers three diffusion models (SD 1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks. Further, we shed light on the role that target dataset domains play in respective performance; to isolate the domain effects, we introduce a new diagnostic benchmark Self-Bench comprised of images created by diffusion models themselves. Finally, we explore the importance of timestep weighting and uncover a relationship between domain gap and timestep sensitivity, particularly for SD3-m. To sum up, diffusion classifiers understand compositionality, but conditions apply! Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/eugene6923/Diffusion-Classifiers-Compositionality">https://github.com/eugene6923/Diffusion-Classifiers-Compositionality</a>. </p>
<blockquote>
<p>理解视觉场景对人类智能至关重要。虽然判别模型在计算机视觉领域取得了显著进展，但在组合理解方面常常遇到困难。相比之下，最近的文本到图像的生成扩散模型在合成复杂场景方面表现出色，这表明其具有内在的组成能力。基于此，零样本扩散分类器被提出用于将扩散模型重新用于判别任务。虽然先前的工作在判别组合场景方面提供了有前景的结果，但由于基准测试的数量有限以及对模型成功的条件分析相对肤浅，这些结果仍是初步的。为了解决这一问题，我们对扩散分类器在广泛组合任务上的判别能力进行了综合研究。具体来说，我们的研究涵盖了三个扩散模型（SD 1.5、2.0，以及首次尝试的3-m），涉及10个数据集和超过30个任务。此外，我们阐明了目标数据集领域在各自性能中的作用；为了隔离领域效应，我们推出了一个新的诊断基准Self-Bench，由扩散模型本身创建的图像组成。最后，我们探讨了时间步长权重的重要性，并揭示了领域差距与时间步长敏感性之间的关系，特别是对于SD3-m。总而言之，扩散分类器理解组合性，但有条件限制！相关代码和数据集可通过<a target="_blank" rel="noopener" href="https://github.com/eugene6923/Diffusion-Classifiers-Compositionality%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/eugene6923/Diffusion-Classifiers-Compositionality获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17955v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>视觉场景的理解对人类智能至关重要。判别模型在计算机视觉领域取得了显著进展，但在组合理解方面仍有困难。相反，最近的文本到图像生成扩散模型在合成复杂场景方面表现出色，显示出内在的组成能力。基于此，零样本扩散分类器被提出用于将扩散模型用于判别任务。虽然先前的研究在判别组合场景方面提供了有前景的结果，但由于基准测试数量较少以及对模型成功条件的分析相对肤浅，这些结果仍然初步。为了解决这一问题，我们对扩散分类器在多种组合任务上的判别能力进行了综合研究。研究涵盖了三个扩散模型（SD 1.5、2.0和首次推出的3-m），涉及10个数据集和超过30个任务。此外，我们阐明了目标数据集领域在各自性能中的作用；为了隔离领域效应，我们推出了一个新的诊断基准Self-Bench，由扩散模型本身创建的图像组成。最后，我们探讨了时间步长权重的重要性，并揭示了领域差距与时间步长敏感性之间的关系，特别是对于SD3-m。总之，扩散分类器理解组合性，但条件应用！相关代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/eugene6923/Diffusion-Classifiers-Compositionality">链接</a>中找到。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型在合成复杂场景方面表现出强大的能力，表明其内在的组成能力。</li>
<li>零样本扩散分类器被成功应用于判别任务，显示出扩散模型的潜力。</li>
<li>研究涵盖了多个扩散模型和大量数据集，提供了对扩散分类器判别能力的全面分析。</li>
<li>揭示了目标数据集领域对模型性能的重要影响。</li>
<li>引入新的诊断基准Self-Bench，用于评估由扩散模型创建的图像。</li>
<li>探讨了时间步长权重的重要性，并发现其与领域差距之间的关联。</li>
<li>扩散分类器虽然理解组合性，但应用时需考虑条件因素。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17955">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f450b936d9be2850bb44675f6ccac592.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0080c0b9b17fb3606b3e80ba1a7bbefa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a64b45fc99c8261784ae83a6ed212b0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f0b3628beb67d5ec1e22f3c85742a57.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="OSCAR-One-Step-Diffusion-Codec-for-Image-Compression-Across-Multiple-Bit-rates"><a href="#OSCAR-One-Step-Diffusion-Codec-for-Image-Compression-Across-Multiple-Bit-rates" class="headerlink" title="OSCAR: One-Step Diffusion Codec for Image Compression Across Multiple   Bit-rates"></a>OSCAR: One-Step Diffusion Codec for Image Compression Across Multiple   Bit-rates</h2><p><strong>Authors:Jinpei Guo, Yifei Ji, Zheng Chen, Kai Liu, Min Liu, Wang Rao, Wenbo Li, Yong Guo, Yulun Zhang</strong></p>
<p>Pretrained latent diffusion models have shown strong potential for lossy image compression, owing to their powerful generative priors. Most existing diffusion-based methods reconstruct images by iteratively denoising from random noise, guided by compressed latent representations. While these approaches have achieved high reconstruction quality, their multi-step sampling process incurs substantial computational overhead. Moreover, they typically require training separate models for different compression bit-rates, leading to significant training and storage costs. To address these challenges, we propose a one-step diffusion codec across multiple bit-rates. termed OSCAR. Specifically, our method views compressed latents as noisy variants of the original latents, where the level of distortion depends on the bit-rate. This perspective allows them to be modeled as intermediate states along a diffusion trajectory. By establishing a mapping from the compression bit-rate to a pseudo diffusion timestep, we condition a single generative model to support reconstructions at multiple bit-rates. Meanwhile, we argue that the compressed latents retain rich structural information, thereby making one-step denoising feasible. Thus, OSCAR replaces iterative sampling with a single denoising pass, significantly improving inference efficiency. Extensive experiments demonstrate that OSCAR achieves superior performance in both quantitative and visual quality metrics. The code and models will be released at <a target="_blank" rel="noopener" href="https://github.com/jp-guo/OSCAR">https://github.com/jp-guo/OSCAR</a>. </p>
<blockquote>
<p>预训练的潜在扩散模型在有损图像压缩方面表现出了强大的潜力，这得益于其强大的生成先验。大多数现有的基于扩散的方法通过迭代去噪从随机噪声中重建图像，由压缩的潜在表示所引导。虽然这些方法达到了较高的重建质量，但它们的多步采样过程产生了大量的计算开销。此外，它们通常需要针对不同的压缩比特率训练单独的模式，从而产生了显著的训练和存储成本。为了解决这些挑战，我们提出了一种跨多个比特率的扩散编解码器，称为OSCAR。具体来说，我们的方法将压缩潜在变量视为原始潜在变量的噪声版本，其中失真程度取决于比特率。这个视角允许它们被建模为扩散轨迹中的中间状态。通过建立从压缩比特率到伪扩散时间步长的映射，我们将单个生成模式应用于多个比特率的重建。同时，我们认为压缩的潜在变量保留了丰富的结构信息，从而使得一步去噪变得可行。因此，OSCAR用一次去噪过程取代了迭代采样，大大提高了推理效率。大量实验表明，OSCAR在定量和视觉质量指标上均取得了优异的表现。代码和模型将在<a target="_blank" rel="noopener" href="https://github.com/jp-guo/OSCAR%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/jp-guo/OSCAR上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16091v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>预训练潜在扩散模型在图像有损压缩方面展现出强大的潜力。现有扩散方法通过迭代去噪从随机噪声重建图像，受潜在表示引导。虽然这些方法重建质量高，但多步采样过程计算开销大，且对不同压缩比特率需训练不同模型，导致训练和存储成本高。针对这些挑战，我们提出跨多比特率的扩散编解码器OSCAR。它将压缩潜在视作原始潜在的噪声版本，失真程度取决于比特率。通过构建从压缩比特率到伪扩散时步的映射，我们用单一生成模型支持多种比特率的重建。我们认为压缩潜在保留了丰富的结构信息，使得一步去噪可行。因此，OSCAR用单通去噪替换迭代采样，显著提高推理效率。实验表明OSCAR在定量和视觉质量指标上表现优越。代码和模型将在<a target="_blank" rel="noopener" href="https://github.com/jp-guo/OSCAR%E5%B9%BF%E5%B3%BB%E3%80%82">https://github.com/jp-guo/OSCAR发布。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练潜在扩散模型在图像有损压缩方面表现出强大的潜力。</li>
<li>现有扩散模型通常采用多步采样进行图像重建，计算开销大。</li>
<li>OSCAR提出一种跨多比特率的扩散编解码器方法，用单一生成模型支持多种比特率的图像重建。</li>
<li>OSCAR将压缩潜在视作原始潜在的噪声版本，并建立从压缩比特率到伪扩散时步的映射。</li>
<li>OSCAR通过一步去噪过程替代迭代采样，提高推理效率。</li>
<li>实验表明OSCAR在图像重建的定量和视觉质量指标上实现优越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16091">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5430aa2fa5614cf8e6e311b143436be3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30fced3344ddec95fcdb6f16cc547286.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3a6a289238a5a219cc6292763810bc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-003e472d813eadd16c17e71dd2e022d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5aebc4ce4acc0e0bb70f1e5729ef3800.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Non-rigid-Motion-Correction-for-MRI-Reconstruction-via-Coarse-To-Fine-Diffusion-Models"><a href="#Non-rigid-Motion-Correction-for-MRI-Reconstruction-via-Coarse-To-Fine-Diffusion-Models" class="headerlink" title="Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine   Diffusion Models"></a>Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine   Diffusion Models</h2><p><strong>Authors:Frederic Wang, Jonathan I. Tamir</strong></p>
<p>Magnetic Resonance Imaging (MRI) is highly susceptible to motion artifacts due to the extended acquisition times required for k-space sampling. These artifacts can compromise diagnostic utility, particularly for dynamic imaging. We propose a novel alternating minimization framework that leverages a bespoke diffusion model to jointly reconstruct and correct non-rigid motion-corrupted k-space data. The diffusion model uses a coarse-to-fine denoising strategy to capture large overall motion and reconstruct the lower frequencies of the image first, providing a better inductive bias for motion estimation than that of standard diffusion models. We demonstrate the performance of our approach on both real-world cine cardiac MRI datasets and complex simulated rigid and non-rigid deformations, even when each motion state is undersampled by a factor of 64x. Additionally, our method is agnostic to sampling patterns, anatomical variations, and MRI scanning protocols, as long as some low frequency components are sampled during each motion state. </p>
<blockquote>
<p>磁共振成像（MRI）由于k空间采样所需的长采集时间，很容易受到运动伪影的影响。这些伪影可能会降低诊断效用，特别是在动态成像中。我们提出了一种新型的交替最小化框架，它利用专门的扩散模型来联合重建和纠正非刚性运动受损的k空间数据。该扩散模型采用由粗到细的降噪策略，先捕捉整体大运动并重建图像的低频部分，为运动估计提供更好的归纳偏置，优于标准扩散模型。我们在现实世界的心脏电影MRI数据集和复杂的模拟刚性和非刚性变形上展示了我们的方法性能，即使在每种运动状态下采样率降低64倍的情况下也是如此。此外，我们的方法对于采样模式、解剖变异和MRI扫描协议具有不可知性，只要在每个运动状态下采样一些低频成分即可。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15057v2">PDF</a> ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的交替最小化框架，利用专门的扩散模型联合重建和纠正非刚性运动腐蚀的k-space数据。该扩散模型采用由粗到细的降噪策略，先捕捉整体大运动并重建图像的低频部分，为运动估计提供更好的归纳偏置。此方法在真实世界的电影心脏MRI数据集和复杂的模拟刚性和非刚性变形上均表现出优异的性能，即使在每种运动状态下采样率低至64倍时也是如此。该方法对采样模式、解剖变异和MRI扫描协议具有通用性，只要在每个运动状态下采样一些低频成分即可。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出的交替最小化框架利用扩散模型联合重建和纠正非刚性运动腐蚀的k-space数据。</li>
<li>扩散模型采用由粗到细的降噪策略，先捕捉整体大运动。</li>
<li>先重建图像的低频部分，以提高运动估计的准确性。</li>
<li>方法在真实世界的电影心脏MRI数据集和模拟的刚性和非刚性变形上表现优异。</li>
<li>即使在采样率低至64倍的情况下，该方法仍表现出良好的性能。</li>
<li>方法对采样模式、解剖变异和MRI扫描协议具有通用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15057">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-645581bebb039dd369e68b3ea2ddeed4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40c450a3001a4b039f9561669912de35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7a89deb44f02ea190b093675187992e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1879d5afbf5f04752a5b3d36a1b7585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1b84582bd7f57ab6c58ed143a97ad8e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-57052dc9379f13a30206187b3a442480.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4118c0aa3cf8ed7d5566417769df99db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d2a90d028abcb96a0153659c4c1beca.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CraftsMan3D-High-fidelity-Mesh-Generation-with-3D-Native-Generation-and-Interactive-Geometry-Refiner"><a href="#CraftsMan3D-High-fidelity-Mesh-Generation-with-3D-Native-Generation-and-Interactive-Geometry-Refiner" class="headerlink" title="CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and   Interactive Geometry Refiner"></a>CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and   Interactive Geometry Refiner</h2><p><strong>Authors:Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, Xiaoxiao Long</strong></p>
<p>We present a novel generative 3D modeling system, coined CraftsMan, which can generate high-fidelity 3D geometries with highly varied shapes, regular mesh topologies, and detailed surfaces, and, notably, allows for refining the geometry in an interactive manner. Despite the significant advancements in 3D generation, existing methods still struggle with lengthy optimization processes, irregular mesh topologies, noisy surfaces, and difficulties in accommodating user edits, consequently impeding their widespread adoption and implementation in 3D modeling software. Our work is inspired by the craftsman, who usually roughs out the holistic figure of the work first and elaborates the surface details subsequently. Specifically, we employ a 3D native diffusion model, which operates on latent space learned from latent set-based 3D representations, to generate coarse geometries with regular mesh topology in seconds. In particular, this process takes as input a text prompt or a reference image and leverages a powerful multi-view (MV) diffusion model to generate multiple views of the coarse geometry, which are fed into our MV-conditioned 3D diffusion model for generating the 3D geometry, significantly improving robustness and generalizability. Following that, a normal-based geometry refiner is used to significantly enhance the surface details. This refinement can be performed automatically, or interactively with user-supplied edits. Extensive experiments demonstrate that our method achieves high efficacy in producing superior-quality 3D assets compared to existing methods. HomePage: <a target="_blank" rel="noopener" href="https://craftsman3d.github.io/">https://craftsman3d.github.io/</a>, Code: <a target="_blank" rel="noopener" href="https://github.com/wyysf-98/CraftsMan">https://github.com/wyysf-98/CraftsMan</a> </p>
<blockquote>
<p>我们提出了一种新型的三维生成建模系统，名为CraftsMan。该系统可以生成高度逼真的三维几何模型，具有多样化的形状、规则的网格拓扑和精细的表面细节，并且能以交互方式完善几何模型。尽管三维生成技术取得了重大进展，但现有方法仍然面临优化流程冗长、网格拓扑不规则、表面噪声大以及难以容纳用户编辑等问题，从而阻碍了它们在三维建模软件中的广泛采用和实施。我们的工作受到工匠的启发，他们通常先大致勾勒出作品的整体轮廓，然后细化表面细节。具体来说，我们采用了一种三维扩散模型，该模型在潜在空间上运行，该空间是从基于集合的潜在三维表示中学习的，以在几秒内生成具有规则网格拓扑的粗略几何形状。特别是，这个过程以文本提示或参考图像为输入，并利用强大的多视图（MV）扩散模型生成粗略几何形状的多视图，然后将其输入我们的MV条件三维扩散模型以生成三维几何形状，这大大提高了稳健性和通用性。之后，使用基于法线的几何细化器来显著增强表面细节。这种细化可以自动进行，也可以与用户提供的编辑进行交互。大量实验表明，我们的方法在生成高质量的三维资产方面与现有方法相比具有很高的有效性。主页：<a target="_blank" rel="noopener" href="https://craftsman3d.github.io/%EF%BC%8C%E4%BB%A3%E7%A0%81%EF%BC%9Ahttps://github.com/wyysf-98/CraftsMan">https://craftsman3d.github.io/，代码：https://github.com/wyysf-98/CraftsMan</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14979v3">PDF</a> HomePage: <a target="_blank" rel="noopener" href="https://craftsman3d.github.io/">https://craftsman3d.github.io/</a>, Code:   <a target="_blank" rel="noopener" href="https://github.com/wyysf-98/CraftsMan3D">https://github.com/wyysf-98/CraftsMan3D</a></p>
<p><strong>Summary</strong></p>
<p>该项目提出了一种名为CraftsMan的新型三维建模系统，该系统能够生成高质量的三维模型，具有多样化的形状、规则化的网格拓扑和精细的表面细节，并且允许用户进行交互式几何精修。相较于现有方法，该项目通过使用一种在潜在空间上运行的3D扩散模型来生成具有规则网格拓扑的粗糙几何结构，并在之后通过基于正常的几何细化器提升表面细节。这不仅缩短了优化流程，还能处理用户编辑，从而促进了其在3D建模软件中的广泛应用和实施。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CraftsMan是一个新型的三维建模系统，可以生成高质量的三维模型，包括多样化的形状、规则化的网格拓扑和精细的表面。</li>
<li>该系统采用了一种3D扩散模型，能够在潜在空间上运行，从而快速生成具有规则网格拓扑的粗糙几何结构。</li>
<li>CraftsMan允许用户以交互式的方式进行几何精修，提升了系统的实用性和用户体验。</li>
<li>该系统通过多视角（MV）扩散模型提高了鲁棒性和泛化能力，可以更好地处理不同的输入和生成更准确的3D几何。</li>
<li>CraftsMan系统通过基于正常的几何细化器，能够显著提升模型表面的细节质量。</li>
<li>与现有方法相比，CraftsMan在生成优质三维资产方面表现出更高的效能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14979">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-34fcc19d951424fd8920f10b07a59f06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5bf449708d03a86e4c9efb4e4cd813d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35016a66d5990272953fc5bd05fc344f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98a80358eb5cf508aebb5b903dc26a92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82ac853d4d86cea304537373b6745573.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fa300c0ed08360fc9ae9a8557c55db84.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-31  DeepChest Dynamic Gradient-Free Task Weighting for Effective Multi-Task   Learning in Chest X-ray Classification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f7aa04e65a0b26ef7500d36cc8243c3a.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-05-31  PhysicsNeRF Physics-Guided 3D Reconstruction from Sparse Views
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26551.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
