<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  Argus Vision-Centric Reasoning with Grounded Chain-of-Thought">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-16d9a6c967fccfdd4560736f3589596c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-31
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-31-æ›´æ–°"><a href="#2025-05-31-æ›´æ–°" class="headerlink" title="2025-05-31 æ›´æ–°"></a>2025-05-31 æ›´æ–°</h1><h2 id="Argus-Vision-Centric-Reasoning-with-Grounded-Chain-of-Thought"><a href="#Argus-Vision-Centric-Reasoning-with-Grounded-Chain-of-Thought" class="headerlink" title="Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought"></a>Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought</h2><p><strong>Authors:Yunze Man, De-An Huang, Guilin Liu, Shiwei Sheng, Shilong Liu, Liang-Yan Gui, Jan Kautz, Yu-Xiong Wang, Zhiding Yu</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language tasks, yet they often struggle with vision-centric scenarios where precise visual focus is needed for accurate reasoning. In this paper, we introduce Argus to address these limitations with a new visual attention grounding mechanism. Our approach employs object-centric grounding as visual chain-of-thought signals, enabling more effective goal-conditioned visual attention during multimodal reasoning tasks. Evaluations on diverse benchmarks demonstrate that Argus excels in both multimodal reasoning tasks and referring object grounding tasks. Extensive analysis further validates various design choices of Argus, and reveals the effectiveness of explicit language-guided visual region-of-interest engagement in MLLMs, highlighting the importance of advancing multimodal intelligence from a visual-centric perspective. Project page: <a target="_blank" rel="noopener" href="https://yunzeman.github.io/argus/">https://yunzeman.github.io/argus/</a> </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­å¾€å¾€ä¼šå‡ºç°å›°éš¾ï¼Œè¿™äº›åœºæ™¯éœ€è¦ç²¾ç¡®çš„è§†è§‰ç„¦ç‚¹æ¥è¿›è¡Œåˆç†çš„æ¨ç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥Argusæ¥è§£å†³è¿™äº›å±€é™æ€§ï¼Œå®ƒé‡‡ç”¨äº†ä¸€ç§æ–°çš„è§†è§‰æ³¨æ„åŠ›å®šä½æœºåˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å®šä½ä½œä¸ºè§†è§‰æ€ç»´é“¾ä¿¡å·ï¼Œä»è€Œåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æœŸé—´å®ç°æ›´æœ‰æ•ˆçš„ç›®æ ‡æ¡ä»¶è§†è§‰æ³¨æ„åŠ›ã€‚åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒArgusåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡å’ŒæŒ‡å‘å¯¹è±¡å®šä½ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ã€‚è¿›ä¸€æ­¥çš„åˆ†æéªŒè¯äº†Argusçš„å„ç§è®¾è®¡é€‰æ‹©çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ­ç¤ºäº†MLLMä¸­æ˜ç¡®çš„è¯­è¨€å¼•å¯¼è§†è§‰æ„Ÿå…´è¶£åŒºåŸŸå‚ä¸çš„é‡è¦æ€§ï¼Œå¼ºè°ƒäº†ä»è§†è§‰ä¸­å¿ƒè§†è§’å‘å±•å¤šæ¨¡æ€æ™ºèƒ½çš„é‡è¦æ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://yunzeman.github.io/argus/">https://yunzeman.github.io/argus/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23766v1">PDF</a> CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://yunzeman.github.io/argus/">https://yunzeman.github.io/argus/</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦ç²¾ç¡®è§†è§‰æ¨ç†çš„ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºArgusï¼Œé€šè¿‡æ–°çš„è§†è§‰æ³¨æ„åŠ›å®šä½æœºåˆ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚Argusé‡‡ç”¨å¯¹è±¡ä¸ºä¸­å¿ƒçš„å®šä½ä½œä¸ºè§†è§‰æ€ç»´ä¿¡å·ï¼Œåœ¨å¤šåª’ä½“æ¨ç†ä»»åŠ¡ä¸­æ›´æœ‰æ•ˆåœ°å®ç°ç›®æ ‡å¯¼å‘çš„è§†è§‰æ³¨æ„åŠ›ã€‚åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸­çš„è¯„ä¼°è¡¨æ˜ï¼ŒArgusåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡å’ŒæŒ‡å‘å¯¹è±¡å®šä½ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ã€‚è¿›ä¸€æ­¥çš„åˆ†æéªŒè¯äº†Argusçš„å„ç§è®¾è®¡é€‰æ‹©çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ­ç¤ºäº†æ˜ç¡®çš„è¯­è¨€å¼•å¯¼åœ¨MLLMsä¸­çš„è§†è§‰æ„Ÿå…´è¶£åŒºåŸŸå‚ä¸çš„é‡è¦æ€§ï¼Œå¼ºè°ƒäº†ä»è§†è§‰ä¸­å¿ƒè§†è§’å‘å±•å¤šæ¨¡æ€æ™ºèƒ½çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Argusé€šè¿‡å¼•å…¥æ–°çš„è§†è§‰æ³¨æ„åŠ›å®šä½æœºåˆ¶ï¼Œè§£å†³äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­çš„å±€é™æ€§ã€‚</li>
<li>Argusé‡‡ç”¨å¯¹è±¡ä¸ºä¸­å¿ƒçš„å®šä½æ–¹å¼ï¼Œä½œä¸ºè§†è§‰æ€ç»´ä¿¡å·ï¼Œä»¥æ›´æœ‰æ•ˆåœ°è¿›è¡Œç›®æ ‡å¯¼å‘çš„è§†è§‰æ³¨æ„åŠ›ã€‚</li>
<li>åœ¨ä¸åŒçš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒArgusåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡å’ŒæŒ‡å‘å¯¹è±¡å®šä½ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¯¹Argusçš„æ·±å…¥åˆ†æï¼ŒéªŒè¯äº†å…¶è®¾è®¡é€‰æ‹©çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ˜ç¡®äº†è¯­è¨€å¼•å¯¼åœ¨å¤šåª’ä½“æ¨ç†ä¸­çš„è§†è§‰æ„Ÿå…´è¶£åŒºåŸŸå‚ä¸çš„é‡è¦æ€§ã€‚</li>
<li>Arguså¼ºè°ƒäº†ä»è§†è§‰ä¸­å¿ƒè§†è§’å‘å±•å¤šæ¨¡æ€æ™ºèƒ½çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23766">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-667545b533b1d20697a3a865b3a4586e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a38cdd774687d05f12d73cdc57b37a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-833e6201faef1d4566b263957c19faf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-586f1f4cca013267d9b6b043bb6148e1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b6cf2ce053f58bf6d7f2ef9c4f5f9b5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DeepTheorem-Advancing-LLM-Reasoning-for-Theorem-Proving-Through-Natural-Language-and-Reinforcement-Learning"><a href="#DeepTheorem-Advancing-LLM-Reasoning-for-Theorem-Proving-Through-Natural-Language-and-Reinforcement-Learning" class="headerlink" title="DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural   Language and Reinforcement Learning"></a>DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural   Language and Reinforcement Learning</h2><p><strong>Authors:Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhengwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu</strong></p>
<p>Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMsâ€™ strength derived from informal, natural language knowledge acquired during pre-training. In this work, we propose DeepTheorem, a comprehensive informal theorem-proving framework exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes a large-scale benchmark dataset consisting of 121K high-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants. We devise a novel reinforcement learning strategy (RL-Zero) explicitly tailored to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference. Additionally, we propose comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheoremâ€™s potential to fundamentally advance automated informal theorem proving and mathematical exploration. </p>
<blockquote>
<p>å®šç†è¯æ˜æ˜¯è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤æ‚æ¨ç†èƒ½åŠ›çš„ä¸»è¦æµ‹è¯•åºŠã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„è‡ªåŠ¨åŒ–å®šç†è¯æ˜ï¼ˆATPï¼‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºå½¢å¼åŒ–è¯æ˜ç³»ç»Ÿï¼Œè€ŒLLMçš„ä¼˜åŠ¿æ¥è‡ªäºé¢„è®­ç»ƒæœŸé—´è·å–çš„éæ­£å¼è‡ªç„¶è¯­è¨€çŸ¥è¯†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DeepTheoremï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„éæ­£å¼å®šç†è¯æ˜æ¡†æ¶ï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€å¢å¼ºLLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚DeepTheoremåŒ…æ‹¬ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«12.1ä¸‡ä¸ªé«˜è´¨é‡IMOçº§åˆ«çš„éæ­£å¼å®šç†å’Œè¯æ˜ï¼Œæ¶µç›–å¤šä¸ªæ•°å­¦é¢†åŸŸï¼Œå¯¹æ­£ç¡®æ€§ã€éš¾åº¦å’Œä¸»é¢˜ç±»åˆ«è¿›è¡Œäº†ä¸¥æ ¼æ³¨é‡Šï¼Œå¹¶é…æœ‰ç³»ç»Ÿæ„å»ºçš„å¯éªŒè¯å®šç†å˜ä½“ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼ˆRL-Zeroï¼‰ï¼Œä¸“é—¨é’ˆå¯¹éæ­£å¼å®šç†è¯æ˜ï¼Œåˆ©ç”¨å¯éªŒè¯çš„å®šç†å˜ä½“æ¥æ¿€åŠ±ç¨³å¥çš„æ•°å­¦æ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†å…¨é¢çš„ç»“æœå’Œè¿‡ç¨‹è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥æ£€æŸ¥è¯æ˜çš„æ­£ç¡®æ€§å’Œæ¨ç†æ­¥éª¤çš„è´¨é‡ã€‚å¹¿æ³›çš„å®éªŒåˆ†æè¡¨æ˜ï¼Œä¸ç°æœ‰æ•°æ®é›†å’Œç›‘ç£å¾®è°ƒåè®®ç›¸æ¯”ï¼ŒDeepTheoremåœ¨å®šç†è¯æ˜æ€§èƒ½ä¸Šæ˜¾è‘—æé«˜ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œæ¨ç†è´¨é‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒDeepTheoremåœ¨è‡ªåŠ¨åŒ–éæ­£å¼å®šç†è¯æ˜å’Œæ•°å­¦æ¢ç´¢æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23754v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDeepTheoremçš„ç»¼åˆéæ­£å¼å®šç†è¯æ˜æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è‡ªç„¶è¯­è¨€å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚DeepTheoremåŒ…æ‹¬å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«12.1ä¸‡æ¡é«˜è´¨é‡çš„IMOçº§åˆ«éæ­£å¼å®šç†å’Œè¯æ˜ï¼Œè¿™äº›è¯æ˜ç»è¿‡ä¸¥æ ¼æ ‡æ³¨ï¼ŒåŒ…æ‹¬æ­£ç¡®æ€§ã€éš¾åº¦å’Œä¸»é¢˜ç±»åˆ«ç­‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§é’ˆå¯¹éæ­£å¼å®šç†è¯æ˜é‡èº«å®šåˆ¶çš„æ–°å‹å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼ˆRL-Zeroï¼‰ï¼Œå¹¶åˆ©ç”¨éªŒè¯å®šç†å˜ä½“æ¿€åŠ±ç¨³å¥çš„æ•°å­¦æ¨ç†ã€‚å®éªŒåˆ†æè¡¨æ˜ï¼ŒDeepTheoremæ˜¾è‘—æé«˜äº†LLMå®šç†è¯æ˜çš„æ€§èƒ½ï¼Œä¸ç°æœ‰æ•°æ®é›†å’Œç›‘ç£å¾®è°ƒåè®®ç›¸æ¯”ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œæ¨ç†è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepTheoremæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¤æ‚æ¨ç†èƒ½åŠ›çš„éæ­£å¼å®šç†è¯æ˜æ¡†æ¶ã€‚</li>
<li>å®ƒåŒ…æ‹¬ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«ç»è¿‡ä¸¥æ ¼æ ‡æ³¨çš„éæ­£å¼å®šç†å’Œè¯æ˜ã€‚</li>
<li>DeepTheoremåˆ©ç”¨è‡ªç„¶è¯­è¨€å¢å¼ºLLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ ç­–ç•¥RL-Zeroï¼Œä¸“é—¨é’ˆå¯¹éæ­£å¼å®šç†è¯æ˜ã€‚</li>
<li>éªŒè¯å®šç†å˜ä½“è¢«ç”¨äºæ¿€åŠ±ç¨³å¥çš„æ•°å­¦æ¨ç†ã€‚</li>
<li>DeepTheoremæ˜¾è‘—æé«˜äº†LLMåœ¨å®šç†è¯æ˜æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•å’Œåè®®ç›¸æ¯”ï¼ŒDeepTheoremè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œæ¨ç†è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23754">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-25d0b7941d89428cbe84173cdf915108.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ef8cedc1008827cc3fdfae761cbb2fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-778733e3d530d7b931dd6a58ef31fb5b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-193b7aad8ae90f97f6236d2b38a0296c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b8ca2ed393c9ca7355ed52805512929.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b653163b92d44fc8e1e05bc878d27a6a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Spatial-MLLM-Boosting-MLLM-Capabilities-in-Visual-based-Spatial-Intelligence"><a href="#Spatial-MLLM-Boosting-MLLM-Capabilities-in-Visual-based-Spatial-Intelligence" class="headerlink" title="Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial   Intelligence"></a>Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial   Intelligence</h2><p><strong>Authors:Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan</strong></p>
<p>Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: <a target="_blank" rel="noopener" href="https://diankun-wu.github.io/Spatial-MLLM/">https://diankun-wu.github.io/Spatial-MLLM/</a>. </p>
<blockquote>
<p>æœ€è¿‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•åœ¨äºŒç»´è§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°æœ‰äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œæé«˜å®ƒä»¬çš„ç©ºé—´æ™ºèƒ½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„ä¸‰ç»´MLLMsæ€»æ˜¯ä¾èµ–äºé¢å¤–çš„ä¸‰ç»´æˆ–2.5ç»´æ•°æ®æ¥èå…¥ç©ºé—´æ„ŸçŸ¥ï¼Œè¿™åœ¨åªæœ‰äºŒç»´è¾“å…¥çš„æƒ…å¢ƒä¸­ï¼ˆä¾‹å¦‚å›¾åƒæˆ–è§†é¢‘ï¼‰é™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Spatial-MLLMï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºçº¯äºŒç»´è§‚å¯Ÿçš„è§†è§‰ç©ºé—´æ¨ç†çš„æ–°å‹æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–äºCLIPçš„è§†è§‰ç¼–ç å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ä»å‰é¦ˆè§†è§‰å‡ ä½•åŸºç¡€æ¨¡å‹ä¸­é‡Šæ”¾å¼ºå¤§çš„ç»“æ„å…ˆéªŒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒç¼–ç å™¨æ¶æ„ï¼šä¸€ä¸ªé¢„è®­ç»ƒçš„äºŒç»´è§†è§‰ç¼–ç å™¨æ¥æå–è¯­ä¹‰ç‰¹å¾ï¼Œä¸€ä¸ªä»è§†è§‰å‡ ä½•æ¨¡å‹çš„éª¨å¹²ç½‘åˆå§‹åŒ–çš„ç©ºé—´ç¼–ç å™¨æ¥æå–ä¸‰ç»´ç»“æ„ç‰¹å¾ã€‚ç„¶åï¼Œä¸€ä¸ªè¿æ¥å™¨å°†è¿™ä¸¤ç§ç‰¹å¾é›†æˆåˆ°ç»Ÿä¸€çš„è§†è§‰ä»¤ç‰Œä¸­ï¼Œä»¥å¢å¼ºç©ºé—´ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¨ç†æ—¶æå‡ºäº†ä¸€ç§ç©ºé—´æ„ŸçŸ¥å¸§é‡‡æ ·ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€‰æ‹©äº†è§†é¢‘åºåˆ—ä¸­ç©ºé—´ä¿¡æ¯ä¸°å¯Œçš„å¸§ï¼Œç¡®ä¿å³ä½¿åœ¨æœ‰é™çš„ä»¤ç‰Œé•¿åº¦ä¸‹ï¼Œæ¨¡å‹ä¹Ÿèƒ½å…³æ³¨å¯¹ç©ºé—´æ¨ç†è‡³å…³é‡è¦çš„å¸§ã€‚é™¤äº†æ¶æ„æ”¹è¿›å¤–ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†Spatial-MLLM-120kæ•°æ®é›†ï¼Œå¹¶åœ¨å…¶ä¸Šä½¿ç”¨æœ‰ç›‘ç£å¾®è°ƒï¼ˆGRPOï¼‰å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚åœ¨å¤šç§çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Spatial-MLLMåœ¨ç©ºé—´ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://diankun-wu.github.io/Spatial-MLLM/%E3%80%82">https://diankun-wu.github.io/Spatial-MLLM/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23747v1">PDF</a> 21 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSpatial-MLLMçš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»çº¯ç²¹çš„2Dè§‚å¯Ÿä¸­è¿›è¡Œè§†è§‰ç©ºé—´æ¨ç†ã€‚å®ƒé‡‡ç”¨åŒç¼–ç å™¨æ¶æ„ï¼Œç»“åˆé¢„è®­ç»ƒçš„2Dè§†è§‰ç¼–ç å™¨å’ŒåŸºäºè§†è§‰å‡ ä½•æ¨¡å‹çš„ç©ºé—´ç¼–ç å™¨ï¼Œä»¥æå–è¯­ä¹‰ç‰¹å¾å’Œ3Dç»“æ„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§ç©ºé—´æ„ŸçŸ¥å¸§é‡‡æ ·ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§åŸºäºè§†è§‰çš„ç©ºé—´ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Spatial-MLLMæ¡†æ¶å®ç°äº†ä»çº¯ç²¹çš„2Dè§‚å¯Ÿä¸­è¿›è¡Œè§†è§‰ç©ºé—´æ¨ç†çš„èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨åŒç¼–ç å™¨æ¶æ„ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒçš„2Dè§†è§‰ç¼–ç å™¨å’ŒåŸºäºè§†è§‰å‡ ä½•æ¨¡å‹çš„ç©ºé—´ç¼–ç å™¨ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§ç©ºé—´æ„ŸçŸ¥å¸§é‡‡æ ·ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œåœ¨å¤šç§åŸºäºè§†è§‰çš„ç©ºé—´ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>æ„å»ºäº†Spatial-MLLM-120kæ•°æ®é›†ç”¨äºæ¨¡å‹è®­ç»ƒã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨ç›‘ç£ç²¾ç»†è°ƒæ•´å’ŒGRPOæ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23747">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ef78a0ce94d8677f63f6b11fee1f066.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7850f79b76ebf8f7d299f92ab2d7ee10.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ML-Agent-Reinforcing-LLM-Agents-for-Autonomous-Machine-Learning-Engineering"><a href="#ML-Agent-Reinforcing-LLM-Agents-for-Autonomous-Machine-Learning-Engineering" class="headerlink" title="ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning   Engineering"></a>ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning   Engineering</h2><p><strong>Authors:Zexi Liu, Jingyi Chai, Xinyu Zhu, Shuo Tang, Rui Ye, Bo Zhang, Lei Bai, Siheng Chen</strong></p>
<p>The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched fine-tuning, which enables LLM agents to generate diverse actions for enhanced RL exploration; (2) step-wise RL, which enables training on a single action step, accelerating experience collection and improving training efficiency; (3) an agentic ML-specific reward module, which unifies varied ML feedback signals into consistent rewards for RL optimization. Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our 7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it achieves continuous performance improvements and demonstrates exceptional cross-task generalization capabilities. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººçš„å‡ºç°ï¼Œæå¤§åœ°æ¨åŠ¨äº†è‡ªä¸»æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å·¥ç¨‹çš„å‘å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºæ‰‹åŠ¨æç¤ºå·¥ç¨‹ï¼Œæ— æ³•æ ¹æ®ä¸°å¯Œçš„å®éªŒç»éªŒè¿›è¡Œé€‚åº”å’Œä¼˜åŒ–ã€‚é’ˆå¯¹è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–æ¬¡æ¢ç´¢äº†åŸºäºå­¦ä¹ ä»£ç†çš„æœºå™¨å­¦ä¹ èŒƒå¼ï¼Œè¯¥èŒƒå¼ä¸­LLMä»£ç†é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸Šè¿›è¡Œäº¤äº’å¼å®éªŒè¿›è¡Œå­¦ä¹ ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ä»£ç†æœºå™¨å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰ä¸°å¯Œæ¢ç´¢çš„å¾®è°ƒï¼Œä½¿LLMä»£ç†èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–çš„åŠ¨ä½œï¼Œå¢å¼ºRLæ¢ç´¢ï¼›ï¼ˆ2ï¼‰åˆ†æ­¥RLï¼Œä½¿å•ä¸ªåŠ¨ä½œæ­¥éª¤ä¸Šçš„è®­ç»ƒæˆä¸ºå¯èƒ½ï¼ŒåŠ å¿«ç»éªŒæ”¶é›†ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ï¼›ï¼ˆ3ï¼‰é’ˆå¯¹ä»£ç†MLçš„å¥–åŠ±æ¨¡å—ï¼Œå®ƒå°†å„ç§MLåé¦ˆä¿¡å·ç»Ÿä¸€ä¸ºä¸€è‡´çš„å¥–åŠ±ï¼Œç”¨äºRLä¼˜åŒ–ã€‚åˆ©ç”¨è¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ç”±7Bçº§Qwen-2.5 LLMé©±åŠ¨çš„ML-Agentè¿›è¡Œè‡ªä¸»æœºå™¨å­¦ä¹ ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡ä»…åœ¨9ä¸ªMLä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬7Bçº§çš„ML-Agentè¡¨ç°å‡ºè¶…è¿‡671Bçº§DeepSeek-R1ä»£ç†çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒå®ç°äº†æŒç»­çš„æ€§èƒ½æ”¹è¿›ï¼Œå¹¶æ˜¾ç¤ºå‡ºå“è¶Šçš„ä»»åŠ¡é—´æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23723v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªä¸»æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å·¥ç¨‹ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œä½†ç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–æ‰‹åŠ¨æç¤ºå·¥ç¨‹ï¼Œæ— æ³•æ ¹æ®å¤šæ ·åŒ–çš„å®éªŒç»éªŒè¿›è¡Œé€‚åº”å’Œä¼˜åŒ–ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ¢ç´¢äº†åŸºäºå­¦ä¹ å‹çš„æ™ºèƒ½ä½“MLèŒƒå¼ï¼Œæå‡ºä¸€ä¸ªæ–°å‹çš„æ™ºèƒ½ä½“MLè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å®ç°LLMæ™ºèƒ½ä½“åœ¨MLä»»åŠ¡ä¸Šçš„äº¤äº’å®éªŒå­¦ä¹ ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šä¸°å¯Œæ¢ç´¢çš„å¾®è°ƒã€åˆ†æ­¥RLå’Œæ™ºèƒ½ä½“MLç‰¹å®šçš„å¥–åŠ±æ¨¡å—ã€‚åŸºäºè¯¥æ¡†æ¶è®­ç»ƒçš„ML-Agentï¼Œåœ¨ä»…9ä¸ªMLä»»åŠ¡ä¸Šï¼Œè¶…è¶Šäº†è§„æ¨¡è¾¾671Bçš„DeepSeek-R1ä»£ç†ï¼Œå±•ç°å‡ºä¼˜å¼‚çš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªä¸»æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å·¥ç¨‹ä¸­å®ç°æ˜¾è‘—å‘å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–æ‰‹åŠ¨æç¤ºå·¥ç¨‹ï¼Œæ— æ³•é€‚åº”å¤šæ ·åŒ–çš„å®éªŒç»éªŒã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡æ¢ç´¢åŸºäºå­¦ä¹ å‹çš„æ™ºèƒ½ä½“MLèŒƒå¼ã€‚</li>
<li>æå‡ºæ–°å‹æ™ºèƒ½ä½“MLè®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸°å¯Œæ¢ç´¢çš„å¾®è°ƒã€åˆ†æ­¥å¼ºåŒ–å­¦ä¹ å’Œæ™ºèƒ½ä½“MLç‰¹å®šå¥–åŠ±æ¨¡å—ã€‚</li>
<li>åŸºäºè¯¥æ¡†æ¶è®­ç»ƒçš„ML-Agentè¡¨ç°è¶…è¶Šè§„æ¨¡è¾¾671Bçš„DeepSeek-R1ä»£ç†ã€‚</li>
<li>ML-Agentåœ¨ä»…9ä¸ªMLä»»åŠ¡ä¸Šå±•ç°å‡ºå“è¶Šçš„è¿ç»­æ€§èƒ½æå‡å’Œè·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23723">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a96a244009b2e51e23cfa9a05933420b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-883b9fe7aa3daabadc8d44a19f179a1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa28eed8778e1c2f55d0a63d16889441.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Donâ€™t-Take-the-Premise-for-Granted-Evaluating-the-Premise-Critique-Ability-of-Large-Language-Models"><a href="#Donâ€™t-Take-the-Premise-for-Granted-Evaluating-the-Premise-Critique-Ability-of-Large-Language-Models" class="headerlink" title="Donâ€™t Take the Premise for Granted: Evaluating the Premise Critique   Ability of Large Language Models"></a>Donâ€™t Take the Premise for Granted: Evaluating the Premise Critique   Ability of Large Language Models</h2><p><strong>Authors:Jinzhe Li, Gengxu Li, Yi Chang, Yuan Wu</strong></p>
<p>Large language models (LLMs) have witnessed rapid advancements, demonstrating remarkable capabilities. However, a notable vulnerability persists: LLMs often uncritically accept flawed or contradictory premises, leading to inefficient reasoning and unreliable outputs. This emphasizes the significance of possessing the \textbf{Premise Critique Ability} for LLMs, defined as the capacity to proactively identify and articulate errors in input premises. Most existing studies assess LLMsâ€™ reasoning ability in ideal settings, largely ignoring their vulnerabilities when faced with flawed premises. Thus, we introduce the \textbf{Premise Critique Bench (PCBench)}, designed by incorporating four error types across three difficulty levels, paired with multi-faceted evaluation metrics. We conducted systematic evaluations of 15 representative LLMs. Our findings reveal: (1) Most models rely heavily on explicit prompts to detect errors, with limited autonomous critique; (2) Premise critique ability depends on question difficulty and error type, with direct contradictions being easier to detect than complex or procedural errors; (3) Reasoning ability does not consistently correlate with the premise critique ability; (4) Flawed premises trigger overthinking in reasoning models, markedly lengthening responses due to repeated attempts at resolving conflicts. These insights underscore the urgent need to enhance LLMsâ€™ proactive evaluation of input validity, positioning premise critique as a foundational capability for developing reliable, human-centric systems. The code is available at <a target="_blank" rel="noopener" href="https://github.com/MLGroupJLU/Premise_Critique">https://github.com/MLGroupJLU/Premise_Critique</a>. </p>
<blockquote>
<p>éšç€è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆæ•ˆï¼Œå±•ç°å‡ºäº†æƒŠäººçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸€ä¸ªæ˜¾è‘—çš„æ¼æ´ä»ç„¶å­˜åœ¨ï¼šLLMsç»å¸¸æ— æ‰¹åˆ¤åœ°æ¥å—æœ‰ç¼ºé™·æˆ–ç›¸äº’çŸ›ç›¾çš„å‡è®¾ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹å’Œè¾“å‡ºä¸å¯é ã€‚è¿™å¼ºè°ƒäº†å¯¹LLMsæ‹¥æœ‰â€œå‰ææ‰¹åˆ¤èƒ½åŠ›â€çš„é‡è¦æ€§ï¼Œè¿™è¢«å®šä¹‰ä¸ºèƒ½å¤Ÿä¸»åŠ¨è¯†åˆ«å’Œè¡¨è¾¾è¾“å…¥å‡è®¾ä¸­çš„é”™è¯¯çš„èƒ½åŠ›ã€‚å¤§å¤šæ•°ç°æœ‰ç ”ç©¶åœ¨ç†æƒ³ç¯å¢ƒä¸­è¯„ä¼°LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†å®ƒä»¬åœ¨é¢å¯¹æœ‰ç¼ºé™·çš„å‰ææ—¶çš„è„†å¼±æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œå‰ææ‰¹åˆ¤åŸºå‡†æµ‹è¯•ï¼ˆPCBenchï¼‰â€ï¼Œè¯¥æµ‹è¯•é€šè¿‡èå…¥ä¸‰ç§éš¾åº¦çº§åˆ«ä¸­çš„å››ç§é”™è¯¯ç±»å‹ï¼Œå¹¶é…ä»¥å¤šæ–¹é¢çš„è¯„ä¼°æŒ‡æ ‡è¿›è¡Œè®¾è®¡ã€‚æˆ‘ä»¬å¯¹15ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„LLMsè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼šï¼ˆ1ï¼‰å¤§å¤šæ•°æ¨¡å‹åœ¨æ£€æµ‹é”™è¯¯æ—¶ä¸¥é‡ä¾èµ–äºæ˜ç¡®çš„æç¤ºï¼Œè‡ªä¸»æ‰¹åˆ¤èƒ½åŠ›æœ‰é™ï¼›ï¼ˆ2ï¼‰å‰ææ‰¹åˆ¤èƒ½åŠ›å–å†³äºé—®é¢˜çš„éš¾åº¦å’Œé”™è¯¯çš„ç±»å‹ï¼Œç›´æ¥çš„çŸ›ç›¾æ¯”å¤æ‚æˆ–ç¨‹åºæ€§çš„é”™è¯¯æ›´å®¹æ˜“æ£€æµ‹ï¼›ï¼ˆ3ï¼‰æ¨ç†èƒ½åŠ›ä¸å‰ææ‰¹åˆ¤èƒ½åŠ›å¹¶ä¸æ€»æ˜¯ç›¸å…³ï¼›ï¼ˆ4ï¼‰æœ‰ç¼ºé™·çš„å‰æä¼šå¼•å‘æ¨ç†æ¨¡å‹çš„è¿‡åº¦æ€è€ƒï¼Œç”±äºåå¤å°è¯•è§£å†³å†²çªï¼Œå“åº”æ˜æ˜¾å˜é•¿ã€‚è¿™äº›è§è§£çªæ˜¾äº†æé«˜LLMså¯¹è¾“å…¥æœ‰æ•ˆæ€§çš„ä¸»åŠ¨è¯„ä¼°èƒ½åŠ›çš„ç´§è¿«æ€§ï¼Œå¹¶å°†å‰ææ‰¹åˆ¤èƒ½åŠ›å®šä½ä¸ºå¼€å‘å¯é ã€ä»¥äººç±»ä¸ºä¸­å¿ƒçš„ç³»ç»Ÿçš„åŸºç¡€èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MLGroupJLU/Premise_Critique%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MLGroupJLU/Premise_Critiqueè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23715v1">PDF</a> 31 pages,13 figures,15 tables</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å´å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼šç¼ºä¹æ‰¹åˆ¤æ€§åœ°æ¥å—å­˜åœ¨é”™è¯¯çš„æˆ–ç›¸äº’çŸ›ç›¾çš„å‡è®¾ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹å’Œè¾“å‡ºä¸å¯é ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†å‰ææ‰¹åˆ¤èƒ½åŠ›ï¼ˆPremise Critique Abilityï¼‰çš„æ¦‚å¿µï¼Œå¹¶è®¾è®¡äº†å‰ææ‰¹åˆ¤åŸºå‡†æµ‹è¯•ï¼ˆPCBenchï¼‰ï¼Œä»¥è¯„ä¼°LLMså¯¹è¾“å…¥å‡è®¾çš„æ‰¹åˆ¤æ€§è¯†åˆ«ä¸è¡¨è¿°èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°æ¨¡å‹ä¾èµ–æ˜æ˜¾çš„æç¤ºæ¥æ£€æµ‹é”™è¯¯ï¼Œè‡ªä¸»æ‰¹åˆ¤èƒ½åŠ›æœ‰é™ï¼›å‰ææ‰¹åˆ¤èƒ½åŠ›å–å†³äºé—®é¢˜çš„éš¾åº¦å’Œé”™è¯¯ç±»å‹ï¼›æ¨ç†èƒ½åŠ›ä¸å‰ææ‰¹åˆ¤èƒ½åŠ›å¹¶ä¸ä¸€è‡´ï¼›æœ‰ç¼ºé™·çš„å‰æä¼šå¼•å‘æ¨¡å‹è¿‡åº¦æ€è€ƒï¼Œæ˜¾è‘—å»¶é•¿å“åº”æ—¶é—´ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦æé«˜LLMså¯¹è¾“å…¥æœ‰æ•ˆæ€§çš„ä¸»åŠ¨è¯„ä¼°èƒ½åŠ›ï¼Œå°†å‰ææ‰¹åˆ¤ä½œä¸ºå¼€å‘å¯é ã€ä»¥äººä¸ºæœ¬çš„ç³»ç»Ÿçš„åŸºç¡€èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å­˜åœ¨æ¥å—é”™è¯¯æˆ–çŸ›ç›¾å‰æçš„é—®é¢˜ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡å’Œè¾“å‡ºå¯é æ€§ä¸‹é™ã€‚</li>
<li>å‰ææ‰¹åˆ¤èƒ½åŠ›ï¼ˆPCAï¼‰æ˜¯LLMsçš„é‡è¦èƒ½åŠ›ï¼ŒæŒ‡ä¸»åŠ¨è¯†åˆ«å’Œè¡¨è¿°è¾“å…¥å‡è®¾ä¸­çš„é”™è¯¯ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¤§å¤šåœ¨ç†æƒ³ç¯å¢ƒä¸‹è¯„ä¼°LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œå¿½ç•¥äº†å…¶åœ¨é¢å¯¹é”™è¯¯å‰ææ—¶çš„è„†å¼±æ€§ã€‚</li>
<li>æ–°æå‡ºçš„å‰ææ‰¹åˆ¤åŸºå‡†æµ‹è¯•ï¼ˆPCBenchï¼‰åŒ…æ‹¬å››ç§é”™è¯¯ç±»å‹å’Œä¸‰ä¸ªéš¾åº¦çº§åˆ«ï¼Œä»¥åŠå¤šæ–¹é¢çš„è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>å¤§å¤šæ•°LLMsä¾èµ–æ˜æ˜¾çš„æç¤ºæ¥æ£€æµ‹é”™è¯¯ï¼Œè‡ªä¸»æ‰¹åˆ¤èƒ½åŠ›æœ‰é™ã€‚</li>
<li>å‰ææ‰¹åˆ¤èƒ½åŠ›å—é—®é¢˜éš¾åº¦å’Œé”™è¯¯ç±»å‹å½±å“ï¼Œç›´æ¥çŸ›ç›¾æ¯”å¤æ‚æˆ–ç¨‹åºæ€§é”™è¯¯æ›´å®¹æ˜“æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23715">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-085fc3de8c6544d60d046c8bdd931c9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8684c350a65310c25da8fca1950435aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b118a87af8a4d7fa8d3d901068c13b7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SocialMaze-A-Benchmark-for-Evaluating-Social-Reasoning-in-Large-Language-Models"><a href="#SocialMaze-A-Benchmark-for-Evaluating-Social-Reasoning-in-Large-Language-Models" class="headerlink" title="SocialMaze: A Benchmark for Evaluating Social Reasoning in Large   Language Models"></a>SocialMaze: A Benchmark for Evaluating Social Reasoning in Large   Language Models</h2><p><strong>Authors:Zixiang Xu, Yanbo Wang, Yue Huang, Jiayi Ye, Haomin Zhuang, Zirui Song, Lang Gao, Chenxi Wang, Zhaorun Chen, Yujun Zhou, Sixian Li, Wang Pan, Yue Zhao, Jieyu Zhao, Xiangliang Zhang, Xiuying Chen</strong></p>
<p>Large language models (LLMs) are increasingly applied to socially grounded tasks, such as online community moderation, media content analysis, and social reasoning games. Success in these contexts depends on a modelâ€™s social reasoning ability - the capacity to interpret social contexts, infer othersâ€™ mental states, and assess the truthfulness of presented information. However, there is currently no systematic evaluation framework that comprehensively assesses the social reasoning capabilities of LLMs. Existing efforts often oversimplify real-world scenarios and consist of tasks that are too basic to challenge advanced models. To address this gap, we introduce SocialMaze, a new benchmark specifically designed to evaluate social reasoning. SocialMaze systematically incorporates three core challenges: deep reasoning, dynamic interaction, and information uncertainty. It provides six diverse tasks across three key settings: social reasoning games, daily-life interactions, and digital community platforms. Both automated and human validation are used to ensure data quality. Our evaluation reveals several key insights: models vary substantially in their ability to handle dynamic interactions and integrate temporally evolving information; models with strong chain-of-thought reasoning perform better on tasks requiring deeper inference beyond surface-level cues; and model reasoning degrades significantly under uncertainty. Furthermore, we show that targeted fine-tuning on curated reasoning examples can greatly improve model performance in complex social scenarios. The dataset is publicly available at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/MBZUAI/SocialMaze">https://huggingface.co/datasets/MBZUAI/SocialMaze</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«åº”ç”¨äºç¤¾ä¼šåŸºç¡€ä»»åŠ¡ï¼Œå¦‚åœ¨çº¿ç¤¾åŒºç®¡ç†ã€åª’ä½“å†…å®¹åˆ†æå’Œç¤¾äº¤æ¨ç†æ¸¸æˆã€‚åœ¨è¿™äº›ç¯å¢ƒä¸­çš„æˆåŠŸå–å†³äºæ¨¡å‹çš„ç¤¾äº¤æ¨ç†èƒ½åŠ›ï¼Œå³è§£é‡Šç¤¾ä¼šèƒŒæ™¯ã€æ¨æ–­ä»–äººå¿ƒç†çŠ¶æ€ä»¥åŠè¯„ä¼°å‘ˆç°ä¿¡æ¯çœŸå®æ€§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›®å‰å°šæ²¡æœ‰ç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶å…¨é¢è¯„ä¼°LLMçš„ç¤¾äº¤æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„åŠªåŠ›å¾€å¾€ç®€åŒ–äº†çœŸå®ä¸–ç•Œçš„åœºæ™¯ï¼Œä¸”ä»»åŠ¡è¿‡äºåŸºæœ¬ï¼Œæ— æ³•æŒ‘æˆ˜é«˜çº§æ¨¡å‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†SocialMazeï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°ç¤¾äº¤æ¨ç†çš„æ–°åŸºå‡†ã€‚SocialMazeç³»ç»Ÿåœ°ç»“åˆäº†ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šæ·±åº¦æ¨ç†ã€åŠ¨æ€äº¤äº’å’Œä¿¡æ¯ä¸ç¡®å®šæ€§ã€‚å®ƒæä¾›äº†å…­ä¸ªå¤šæ ·åŒ–çš„ä»»åŠ¡ï¼Œæ¶µç›–ä¸‰ä¸ªå…³é”®åœºæ™¯ï¼šç¤¾äº¤æ¨ç†æ¸¸æˆã€æ—¥å¸¸äº’åŠ¨å’Œæ•°å­—ç¤¾åŒºå¹³å°ã€‚é‡‡ç”¨è‡ªåŠ¨åŒ–å’Œäººå·¥éªŒè¯ç›¸ç»“åˆçš„æ–¹å¼ç¡®ä¿æ•°æ®è´¨é‡ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ­ç¤ºäº†å‡ ä¸ªå…³é”®è§è§£ï¼šæ¨¡å‹åœ¨å¤„ç†åŠ¨æ€äº¤äº’å’Œæ•´åˆéšæ—¶é—´æ¼”å˜çš„ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›å·®å¼‚å¾ˆå¤§ï¼›å…·æœ‰å¼ºå¤§æ€ç»´é“¾æ¨ç†çš„æ¨¡å‹åœ¨éœ€è¦è¶…è¶Šè¡¨é¢çº¿ç´¢è¿›è¡Œæ·±å…¥æ¨ç†çš„ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼›åœ¨ä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹ï¼Œæ¨¡å‹æ¨ç†èƒ½åŠ›ä¼šæ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œå¯¹ç²¾é€‰çš„æ¨ç†ç¤ºä¾‹è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒå¯ä»¥æå¤§åœ°æé«˜æ¨¡å‹åœ¨å¤æ‚ç¤¾äº¤åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/MBZUAI/SocialMaze%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://huggingface.co/datasets/MBZUAI/SocialMazeå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23713v1">PDF</a> Code available at <a target="_blank" rel="noopener" href="https://github.com/xzx34/SocialMaze">https://github.com/xzx34/SocialMaze</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¤¾ä¼šæ€§ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå¦‚åœ¨çº¿ç¤¾åŒºç®¡ç†ã€åª’ä½“å†…å®¹åˆ†æå’Œç¤¾äº¤æ¨ç†æ¸¸æˆç­‰ã€‚ç„¶è€Œï¼Œç›®å‰å°šç¼ºä¹ä¸€ä¸ªå…¨é¢è¯„ä¼°LLMç¤¾ä¼šæ¨ç†èƒ½åŠ›çš„ç³»ç»Ÿè¯„ä¼°æ¡†æ¶ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºSocialMazeï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è¯„ä¼°ç¤¾ä¼šæ¨ç†çš„æ–°åŸºå‡†ã€‚SocialMazeç³»ç»Ÿåœ°èåˆäº†ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šæ·±åº¦æ¨ç†ã€åŠ¨æ€äº¤äº’å’Œä¿¡æ¯ä¸ç¡®å®šæ€§ã€‚å®ƒæä¾›äº†å…­ä¸ªå¤šæ ·åŒ–çš„ä»»åŠ¡ï¼Œæ¶µç›–ä¸‰å¤§å…³é”®åœºæ™¯ï¼šç¤¾äº¤æ¨ç†æ¸¸æˆã€æ—¥å¸¸äº’åŠ¨å’Œæ•°å­—ç¤¾åŒºå¹³å°ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ­ç¤ºäº†å‡ å¤§å…³é”®è§è§£ï¼šæ¨¡å‹åœ¨å¤„ç†åŠ¨æ€äº¤äº’å’Œæ•´åˆæ—¶é—´æ¼”å˜ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›å·®å¼‚æ˜¾è‘—ï¼›å…·æœ‰å¼ºå¤§é“¾å¼æ¨ç†çš„æ¨¡å‹åœ¨éœ€è¦æ·±å…¥æ¨ç†çš„ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼›åœ¨ä¸ç¡®å®šæ€§ä¸‹ï¼Œæ¨¡å‹æ¨ç†èƒ½åŠ›æ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼Œå¯¹ç²¾é€‰æ¨ç†ç¤ºä¾‹çš„é’ˆå¯¹æ€§å¾®è°ƒå¯ä»¥å¤§å¤§æé«˜æ¨¡å‹åœ¨å¤æ‚ç¤¾äº¤åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¤¾ä¼šæ€§ä»»åŠ¡ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>ç›®å‰ç¼ºä¹è¯„ä¼°LLMç¤¾ä¼šæ¨ç†èƒ½åŠ›çš„ç³»ç»Ÿè¯„ä¼°æ¡†æ¶ã€‚</li>
<li>SocialMazeæ˜¯ä¸€ä¸ªä¸“é—¨è¯„ä¼°ç¤¾ä¼šæ¨ç†çš„æ–°åŸºå‡†ï¼ŒåŒ…å«æ·±åº¦æ¨ç†ã€åŠ¨æ€äº¤äº’å’Œä¿¡æ¯ä¸ç¡®å®šæ€§ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>SocialMazeæä¾›äº†å…­ä¸ªå¤šæ ·åŒ–çš„ä»»åŠ¡ï¼Œæ¶µç›–ç¤¾äº¤æ¨ç†æ¸¸æˆã€æ—¥å¸¸äº’åŠ¨å’Œæ•°å­—ç¤¾åŒºå¹³å°ç­‰åœºæ™¯ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨å¤„ç†åŠ¨æ€äº¤äº’å’Œæ•´åˆæ—¶é—´æ¼”å˜ä¿¡æ¯æ–¹é¢å­˜åœ¨å·®å¼‚ï¼Œä¸”åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°ä¸ç¨³å®šã€‚</li>
<li>å…·æœ‰å¼ºå¤§é“¾å¼æ¨ç†çš„æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a22364b161539081407c0668bd389a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8272f3d28eeb9a1f58e7d9dd244a016.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b260d5fcbdffe6de50e845ae2863f77.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Grounded-Reinforcement-Learning-for-Visual-Reasoning"><a href="#Grounded-Reinforcement-Learning-for-Visual-Reasoning" class="headerlink" title="Grounded Reinforcement Learning for Visual Reasoning"></a>Grounded Reinforcement Learning for Visual Reasoning</h2><p><strong>Authors:Gabriel Sarch, Snigdha Saha, Naitik Khandelwal, Ayush Jain, Michael J. Tarr, Aviral Kumar, Katerina Fragkiadaki</strong></p>
<p>While reinforcement learning (RL) over chains of thought has significantly advanced language models in tasks such as mathematics and coding, visual reasoning introduces added complexity by requiring models to direct visual attention, interpret perceptual inputs, and ground abstract reasoning in spatial evidence. We introduce ViGoRL (Visually Grounded Reinforcement Learning), a vision-language model trained with RL to explicitly anchor each reasoning step to specific visual coordinates. Inspired by human visual decision-making, ViGoRL learns to produce spatially grounded reasoning traces, guiding visual attention to task-relevant regions at each step. When fine-grained exploration is required, our novel multi-turn RL framework enables the model to dynamically zoom into predicted coordinates as reasoning unfolds. Across a diverse set of visual reasoning benchmarksâ€“including SAT-2 and BLINK for spatial reasoning, V<em>bench for visual search, and ScreenSpot and VisualWebArena for web-based groundingâ€“ViGoRL consistently outperforms both supervised fine-tuning and conventional RL baselines that lack explicit grounding mechanisms. Incorporating multi-turn RL with zoomed-in visual feedback significantly improves ViGoRLâ€™s performance on localizing small GUI elements and visual search, achieving 86.4% on V</em>Bench. Additionally, we find that grounding amplifies other visual behaviors such as region exploration, grounded subgoal setting, and visual verification. Finally, human evaluations show that the modelâ€™s visual references are not only spatially accurate but also helpful for understanding model reasoning steps. Our results show that visually grounded RL is a strong paradigm for imbuing models with general-purpose visual reasoning. </p>
<blockquote>
<p>è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ€ç»´é“¾ä¸Šå·²ç»åœ¨æ•°å­¦å’Œç¼–ç ç­‰ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œä½†è§†è§‰æ¨ç†å¼•å…¥äº†é¢å¤–çš„å¤æ‚æ€§ï¼Œè¦æ±‚æ¨¡å‹å¼•å¯¼è§†è§‰æ³¨æ„åŠ›ã€è§£é‡Šæ„ŸçŸ¥è¾“å…¥ä»¥åŠå°†æŠ½è±¡æ¨ç†åŸºäºç©ºé—´è¯æ®è¿›è¡Œè§£é‡Šã€‚æˆ‘ä»¬å¼•å…¥äº†ViGoRLï¼ˆè§†è§‰æ¥åœ°å¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ˜ç¡®å°†æ¯ä¸€æ­¥æ¨ç†é”šå®šåˆ°ç‰¹å®šçš„è§†è§‰åæ ‡ä¸Šã€‚å—äººç±»è§†è§‰å†³ç­–åˆ¶å®šçš„å¯å‘ï¼ŒViGoRLå­¦ä¼šäº†äº§ç”Ÿç©ºé—´åŸºç¡€çš„æ¨ç†è½¨è¿¹ï¼Œå¼•å¯¼è§†è§‰æ³¨æ„åŠ›å…³æ³¨æ¯ä¸ªæ­¥éª¤ä¸­ä¸ä»»åŠ¡ç›¸å…³çš„åŒºåŸŸã€‚å½“éœ€è¦è¿›è¡Œç²¾ç»†æ¢ç´¢æ—¶ï¼Œæˆ‘ä»¬æ–°é¢–çš„å¤šå›åˆå¼ºåŒ–å­¦ä¹ æ¡†æ¶ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†å±•å¼€æ—¶åŠ¨æ€åœ°èšç„¦é¢„æµ‹åæ ‡ã€‚åœ¨å¤šç§è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­â€”â€”åŒ…æ‹¬ç”¨äºç©ºé—´æ¨ç†çš„SAT-2å’ŒBLINKã€ç”¨äºè§†è§‰æœç´¢çš„V<em>benchä»¥åŠç”¨äºç½‘ç»œåŸºç¡€çš„ScreenSpotå’ŒVisualWebArenaâ€”â€”ViGoRLæŒç»­è¡¨ç°å‡ºä¼˜äºç›‘ç£å¾®è°ƒä»¥åŠç¼ºä¹æ˜ç¡®æ¥åœ°æœºåˆ¶çš„å¸¸è§„å¼ºåŒ–å­¦ä¹ åŸºå‡†çš„æ€§èƒ½ã€‚ç»“åˆå¤šå›åˆå¼ºåŒ–å­¦ä¹ ä¸ç²¾ç»†çš„è§†è§‰åé¦ˆæ˜¾è‘—æé«˜äº†ViGoRLåœ¨å®šä½å°å‹GUIå…ƒç´ å’Œè§†è§‰æœç´¢æ–¹é¢çš„æ€§èƒ½ï¼Œåœ¨V</em>Benchä¸Šè¾¾åˆ°86.4%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æ¥åœ°æ”¾å¤§äº†å…¶ä»–è§†è§‰è¡Œä¸ºï¼Œå¦‚åŒºåŸŸæ¢ç´¢ã€æ¥åœ°å­ç›®æ ‡è®¾å®šå’Œè§†è§‰éªŒè¯ã€‚æœ€åï¼Œäººç±»è¯„ä¼°è¡¨æ˜ï¼Œæ¨¡å‹çš„è§†è§‰å‚è€ƒä¸ä»…ç©ºé—´å‡†ç¡®ï¼Œè€Œä¸”æœ‰åŠ©äºç†è§£æ¨¡å‹çš„æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè§†è§‰æ¥åœ°çš„å¼ºåŒ–å­¦ä¹ æ˜¯èµ‹äºˆæ¨¡å‹é€šç”¨è§†è§‰æ¨ç†çš„å¼ºå¤§èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23678v1">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://visually-grounded-rl.github.io/">https://visually-grounded-rl.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ViGoRLï¼ˆè§†è§‰åŸºç¡€å¼ºåŒ–å­¦ä¹ ï¼‰è¿™ä¸€èåˆè§†è§‰ä¸è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºï¼Œé€šè¿‡æ˜ç¡®å°†æ¯ä¸€æ­¥æ¨ç†é”šå®šåˆ°ç‰¹å®šè§†è§‰åæ ‡ä¸Šï¼Œä½¿å¾—æ¨¡å‹èƒ½ç›´æ¥åœ¨è§†è§‰ä¸Šå…³æ³¨ä»»åŠ¡ç›¸å…³åŒºåŸŸã€‚åœ¨å¤šé¡¹è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒViGoRLè¡¨ç°å‡ºè¶…è¶Šç›‘ç£å¾®è°ƒåŠä¼ ç»Ÿç¼ºä¹æ˜ç¡®åŸºç¡€æœºåˆ¶å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ã€‚å¼•å…¥å¤šå›åˆå¼ºåŒ–å­¦ä¹ ä¸ç²¾ç»†è§†è§‰åé¦ˆåï¼ŒViGoRLåœ¨å®šä½å°GUIå…ƒç´ åŠè§†è§‰æœç´¢ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—æé«˜ã€‚æ€»çš„æ¥è¯´ï¼Œè§†è§‰åŸºç¡€çš„å¼ºåŒ–å­¦ä¹ å·²æˆä¸ºèµ‹äºˆæ¨¡å‹é€šç”¨è§†è§‰æ¨ç†èƒ½åŠ›çš„ä¸€ç§å¼ºå¤§èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ViGoRLç»“åˆäº†è§†è§‰ä¸è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå¤„ç†è§†è§‰æ¨ç†ä»»åŠ¡ã€‚</li>
<li>ViGoRLé€šè¿‡æ˜ç¡®å°†æ¨ç†æ­¥éª¤é”šå®šåˆ°è§†è§‰åæ ‡ä¸Šï¼Œå®ç°äº†æ¨¡å‹çš„è§†è§‰å…³æ³¨æœºåˆ¶ã€‚</li>
<li>åœ¨å¤šé¡¹è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒViGoRLæ€§èƒ½è¶…è¶Šäº†ç›‘ç£å¾®è°ƒåŠä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>å¤šå›åˆå¼ºåŒ–å­¦ä¹ ä¸ç²¾ç»†è§†è§‰åé¦ˆæé«˜äº†ViGoRLåœ¨å®šä½å°GUIå…ƒç´ åŠè§†è§‰æœç´¢ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>è§†è§‰åŸºç¡€å¼ºåŒ–å­¦ä¹ æœ‰åŠ©äºæé«˜æ¨¡å‹çš„é€šç”¨è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ViGoRLçš„æ¨¡å‹æ¨ç†æ­¥éª¤ä¸­çš„è§†è§‰å‚è€ƒä¸ä»…ç©ºé—´å‡†ç¡®ï¼Œè€Œä¸”æœ‰åŠ©äºç†è§£æ¨ç†è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a0cf2db09c45b857c11cf06d39d4f36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e95b4bf804dc65bde2f1a0decd8906d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fad4df0821d04250f7cde61df1453fa3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Comprehensive-Evaluation-of-Multi-Modal-Large-Language-Models-for-Endoscopy-Analysis"><a href="#A-Comprehensive-Evaluation-of-Multi-Modal-Large-Language-Models-for-Endoscopy-Analysis" class="headerlink" title="A Comprehensive Evaluation of Multi-Modal Large Language Models for   Endoscopy Analysis"></a>A Comprehensive Evaluation of Multi-Modal Large Language Models for   Endoscopy Analysis</h2><p><strong>Authors:Shengyuan Liu, Boyun Zheng, Wenting Chen, Zhihao Peng, Zhenfei Yin, Jing Shao, Jiancong Hu, Yixuan Yuan</strong></p>
<p>Endoscopic procedures are essential for diagnosing and treating internal diseases, and multi-modal large language models (MLLMs) are increasingly applied to assist in endoscopy analysis. However, current benchmarks are limited, as they typically cover specific endoscopic scenarios and a small set of clinical tasks, failing to capture the real-world diversity of endoscopic scenarios and the full range of skills needed in clinical workflows. To address these issues, we introduce EndoBench, the first comprehensive benchmark specifically designed to assess MLLMs across the full spectrum of endoscopic practice with multi-dimensional capacities. EndoBench encompasses 4 distinct endoscopic scenarios, 12 specialized clinical tasks with 12 secondary subtasks, and 5 levels of visual prompting granularities, resulting in 6,832 rigorously validated VQA pairs from 21 diverse datasets. Our multi-dimensional evaluation framework mirrors the clinical workflowâ€“spanning anatomical recognition, lesion analysis, spatial localization, and surgical operationsâ€“to holistically gauge the perceptual and diagnostic abilities of MLLMs in realistic scenarios. We benchmark 23 state-of-the-art models, including general-purpose, medical-specialized, and proprietary MLLMs, and establish human clinician performance as a reference standard. Our extensive experiments reveal: (1) proprietary MLLMs outperform open-source and medical-specialized models overall, but still trail human experts; (2) medical-domain supervised fine-tuning substantially boosts task-specific accuracy; and (3) model performance remains sensitive to prompt format and clinical task complexity. EndoBench establishes a new standard for evaluating and advancing MLLMs in endoscopy, highlighting both progress and persistent gaps between current models and expert clinical reasoning. We publicly release our benchmark and code. </p>
<blockquote>
<p>å†…çª¥é•œç¨‹åºå¯¹è¯Šæ–­å’Œæ²»ç–—å†…éƒ¨ç–¾ç—…è‡³å…³é‡è¦ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°åº”ç”¨äºè¾…åŠ©å†…çª¥é•œåˆ†æã€‚ç„¶è€Œï¼Œç›®å‰çš„åŸºå‡†æµ‹è¯•å­˜åœ¨å±€é™æ€§ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸æ¶µç›–ç‰¹å®šçš„å†…çª¥é•œåœºæ™¯å’Œå°‘é‡çš„ä¸´åºŠä»»åŠ¡ï¼Œæ— æ³•æ•æ‰å†…çª¥é•œåœºæ™¯çš„ç°å®ä¸–ç•Œå¤šæ ·æ€§å’Œä¸´åºŠå·¥ä½œæµç¨‹ä¸­æ‰€éœ€çš„å…¨éƒ¨æŠ€èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EndoBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMsåœ¨å†…çª¥é•œå®è·µä¸­çš„å…¨æ–¹ä½èƒ½åŠ›ã€‚EndoBenchåŒ…å«4ç§ç‹¬ç‰¹çš„å†…çª¥é•œåœºæ™¯ã€12é¡¹ä¸“ä¸šä¸´åºŠä»»åŠ¡å’Œ12é¡¹äºŒçº§å­ä»»åŠ¡ï¼Œä»¥åŠ5ä¸ªçº§åˆ«çš„è§†è§‰æç¤ºç²’åº¦ï¼Œä»è€Œäº§ç”Ÿ6832å¯¹ç»è¿‡ä¸¥æ ¼éªŒè¯çš„é—®ç­”å¯¹ï¼Œæ¥è‡ª21ä¸ªå¤šæ ·åŒ–çš„æ•°æ®é›†ã€‚æˆ‘ä»¬çš„å¤šç»´è¯„ä¼°æ¡†æ¶åæ˜ äº†ä¸´åºŠå·¥ä½œæµç¨‹â€”â€”æ¶µç›–è§£å‰–è¯†åˆ«ã€ç—…å˜åˆ†æã€ç©ºé—´å®šä½å’Œæ‰‹æœ¯æ“ä½œâ€”â€”ä»¥å…¨é¢è¯„ä¼°ç°å®åœºæ™¯ä¸­MLLMsçš„æ„ŸçŸ¥å’Œè¯Šæ–­èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹23ç§æœ€æ–°æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬é€šç”¨å‹ã€åŒ»ç–—ä¸“ä¸šå‹å’Œä¸“æœ‰MLLMsï¼Œå¹¶å°†äººç±»åŒ»ç”Ÿçš„æ€§èƒ½ä½œä¸ºå‚è€ƒæ ‡å‡†ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼šï¼ˆ1ï¼‰ä¸“æœ‰MLLMsæ€»ä½“ä¸Šä¼˜äºå¼€æºå’ŒåŒ»ç–—ä¸“ä¸šæ¨¡å‹ï¼Œä½†ä»è½åäºäººç±»ä¸“å®¶ï¼›ï¼ˆ2ï¼‰åŒ»ç–—é¢†åŸŸç›‘ç£å¾®è°ƒæ˜¾è‘—æé«˜ä»»åŠ¡ç‰¹å®šå‡†ç¡®æ€§ï¼›ï¼ˆ3)æ¨¡å‹æ€§èƒ½å¯¹æç¤ºæ ¼å¼å’Œä»»åŠ¡å¤æ‚æ€§ä»ç„¶æ•æ„Ÿã€‚EndoBenchä¸ºè¯„ä¼°å’Œæ”¹è¿›å†…çª¥é•œé¢†åŸŸçš„MLLMså»ºç«‹äº†æ–°æ ‡å‡†ï¼Œçªå‡ºäº†å½“å‰æ¨¡å‹ä¸ä¸“å®¶ä¸´åºŠæ¨ç†ä¹‹é—´çš„è¿›å±•å’ŒæŒç»­å·®è·ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23601v1">PDF</a> 36 pages, 18 figures</p>
<p><strong>Summary</strong><br>    å†…é•œç¨‹åºå¯¹äºè¯Šæ–­ä¸æ²»ç–—å†…éƒ¨ç–¾ç—…è‡³å…³é‡è¦ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°åº”ç”¨äºå†…é•œåˆ†æè¾…åŠ©ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ ‡å‡†å­˜åœ¨å±€é™æ€§ï¼Œé€šå¸¸åªæ¶µç›–ç‰¹å®šçš„å†…é•œåœºæ™¯å’Œå°‘é‡ä¸´åºŠä»»åŠ¡ï¼Œæ— æ³•æ•æ‰ç°å®ä¸–ç•Œä¸­å†…é•œåœºæ™¯çš„å¤šæ ·æ€§å’Œä¸´åºŠå·¥ä½œæµç¨‹ä¸­æ‰€éœ€çš„å…¨é¢æŠ€èƒ½ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæ¨å‡ºEndoBenchï¼Œé¦–ä¸ªå…¨é¢è¯„ä¼°MLLMsåœ¨å†…é•œå®è·µå…¨é¢†åŸŸçš„ç»¼åˆåŸºå‡†ã€‚EndoBenchåŒ…å«4ç§ç‹¬ç‰¹çš„å†…é•œåœºæ™¯ã€12é¡¹ä¸“ä¸šä¸´åºŠä»»åŠ¡å’Œäº”ä¸ªçº§åˆ«çš„è§†è§‰æç¤ºç²¾ç»†åº¦ï¼ŒåŒ…å«ä»21ä¸ªä¸åŒæ•°æ®é›†ä¸­ä¸¥æ ¼éªŒè¯çš„6832å¯¹é—®ç­”ã€‚å…¶å¤šç»´è¯„ä¼°æ¡†æ¶æ¶µç›–äº†è§£å‰–è¯†åˆ«ã€ç—…å˜åˆ†æã€ç©ºé—´å®šä½ä»¥åŠæ‰‹æœ¯æ“ä½œç­‰ä»¥å…¨é¢è¯„ä¼°MLLMsåœ¨ç°å®åœºæ™¯ä¸­çš„æ„ŸçŸ¥å’Œè¯Šæ–­èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹åŒ…æ‹¬é€šç”¨ã€åŒ»ç–—ä¸“ä¸šä»¥åŠä¸“æœ‰MLLMsç­‰23é¡¹å‰æ²¿æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶ä»¥äººç±»ä¸´åºŠåŒ»ç”Ÿçš„è¡¨ç°ä½œä¸ºå‚è€ƒæ ‡å‡†ã€‚å¤§é‡å®éªŒæ­ç¤ºï¼šä¸“æœ‰MLLMsæ€»ä½“ä¸Šä¼˜äºå¼€æºå’ŒåŒ»ç–—ä¸“ä¸šæ¨¡å‹ï¼Œä½†ä»è½åäºäººç±»ä¸“å®¶ï¼›åŒ»ç–—åŸŸç›‘ç£å¾®è°ƒæ˜¾è‘—æé«˜ç‰¹å®šä»»åŠ¡çš„å‡†ç¡®æ€§ï¼›æ¨¡å‹æ€§èƒ½å¯¹æç¤ºæ ¼å¼å’Œä»»åŠ¡å¤æ‚æ€§ä»ç„¶æ•æ„Ÿã€‚EndoBenchä¸ºè¯„ä¼°å’Œæ”¹è¿›å†…é•œé¢†åŸŸçš„MLLMså»ºç«‹äº†æ–°æ ‡å‡†ï¼Œçªæ˜¾äº†å½“å‰æ¨¡å‹ä¸ä¸“å®¶ä¸´åºŠæ¨ç†ä¹‹é—´çš„è¿›æ­¥å’ŒæŒç»­å·®è·ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒæ­¤åŸºå‡†å’Œä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†…é•œç¨‹åºåœ¨ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—ä¸­èµ·å…³é”®ä½œç”¨ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å†…é•œåˆ†æä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ã€‚</li>
<li>å½“å‰è¯„ä¼°æ ‡å‡†å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å…¨é¢åæ˜ MLLMsåœ¨å†…é•œå®è·µä¸­çš„èƒ½åŠ›ã€‚</li>
<li>EndoBenchæ˜¯é¦–ä¸ªå…¨é¢è¯„ä¼°MLLMsåœ¨å†…é•œé¢†åŸŸçš„ç»¼åˆåŸºå‡†ï¼ŒåŒ…å«å¤šç§åœºæ™¯ã€ä»»åŠ¡å’Œè§†è§‰æç¤ºç²¾ç»†åº¦ã€‚</li>
<li>EndoBenchå»ºç«‹äº†è¯„ä¼°MLLMsçš„æ–°æ ‡å‡†ï¼Œæ¶µç›–è§£å‰–è¯†åˆ«ã€ç—…å˜åˆ†æç­‰å¤šä¸ªæ–¹é¢ã€‚</li>
<li>ä¸“æœ‰MLLMsæ€§èƒ½ä¼˜äºä¸€äº›å…¶ä»–æ¨¡å‹ï¼Œä½†ä»éœ€è¿›ä¸€æ­¥æé«˜ä»¥åŒ¹é…äººç±»ä¸“å®¶æ°´å¹³ã€‚</li>
<li>åŒ»ç–—åŸŸç›‘ç£å¾®è°ƒèƒ½æœ‰æ•ˆæé«˜ä»»åŠ¡ç‰¹å®šå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-94a01d1f05c1bf524253dbb30cbbd151.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a80377ec86b242f1b96510acd99685e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-566a07a2b6a11fbd52a308ec9f714b28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35899044d5252919c5250f02b9a2ca27.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MAPLE-A-Mobile-Assistant-with-Persistent-Finite-State-Machines-for-Recovery-Reasoning"><a href="#MAPLE-A-Mobile-Assistant-with-Persistent-Finite-State-Machines-for-Recovery-Reasoning" class="headerlink" title="MAPLE: A Mobile Assistant with Persistent Finite State Machines for   Recovery Reasoning"></a>MAPLE: A Mobile Assistant with Persistent Finite State Machines for   Recovery Reasoning</h2><p><strong>Authors:Linqiang Guo, Wei Liu, Yi Wen Heng,  Tse-Hsun,  Chen, Yang Wang</strong></p>
<p>Mobile GUI agents aim to autonomously complete user-instructed tasks across mobile apps. Recent advances in Multimodal Large Language Models (MLLMs) enable these agents to interpret UI screens, identify actionable elements, and perform interactions such as tapping or typing. However, existing agents remain reactive: they reason only over the current screen and lack a structured model of app navigation flow, limiting their ability to understand context, detect unexpected outcomes, and recover from errors. We present MAPLE, a state-aware multi-agent framework that abstracts app interactions as a Finite State Machine (FSM). We computationally model each UI screen as a discrete state and user actions as transitions, allowing the FSM to provide a structured representation of the app execution. MAPLE consists of specialized agents responsible for four phases of task execution: planning, execution, verification, error recovery, and knowledge retention. These agents collaborate to dynamically construct FSMs in real time based on perception data extracted from the UI screen, allowing the GUI agents to track navigation progress and flow, validate action outcomes through pre- and post-conditions of the states, and recover from errors by rolling back to previously stable states. Our evaluation results on two challenging cross-app benchmarks, Mobile-Eval-E and SPA-Bench, show that MAPLE outperforms the state-of-the-art baseline, improving task success rate by up to 12%, recovery success by 13.8%, and action accuracy by 6.5%. Our results highlight the importance of structured state modeling in guiding mobile GUI agents during task execution. Moreover, our FSM representation can be integrated into future GUI agent architectures as a lightweight, model-agnostic memory layer to support structured planning, execution verification, and error recovery. </p>
<blockquote>
<p>ç§»åŠ¨GUIä»£ç†æ—¨åœ¨è‡ªä¸»å®Œæˆè·¨ç§»åŠ¨åº”ç”¨ç”¨æˆ·æŒ‡å®šçš„ä»»åŠ¡ã€‚æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥ä½¿è¿™äº›ä»£ç†èƒ½å¤Ÿè§£é‡ŠUIå±å¹•ï¼Œè¯†åˆ«å¯æ“ä½œå…ƒç´ ï¼Œå¹¶æ‰§è¡Œå¦‚ç‚¹å‡»æˆ–é”®å…¥ç­‰äº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰ä»£ç†ä»ç„¶æ˜¯ååº”å¼çš„ï¼šå®ƒä»¬åªé’ˆå¯¹å½“å‰å±å¹•è¿›è¡Œæ¨ç†ï¼Œç¼ºä¹åº”ç”¨å¯¼èˆªæµçš„ç»“æ„åŒ–æ¨¡å‹ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬ç†è§£ä¸Šä¸‹æ–‡ã€æ£€æµ‹æ„å¤–ç»“æœå¹¶ä»é”™è¯¯ä¸­æ¢å¤çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†MAPLEï¼Œä¸€ä¸ªçŠ¶æ€æ„ŸçŸ¥çš„å¤šä»£ç†æ¡†æ¶ï¼Œå®ƒå°†åº”ç”¨äº¤äº’æŠ½è±¡ä¸ºæœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰ã€‚æˆ‘ä»¬å°†æ¯ä¸ªUIå±å¹•è®¡ç®—å»ºæ¨¡ä¸ºç¦»æ•£çŠ¶æ€ï¼Œç”¨æˆ·æ“ä½œä½œä¸ºè½¬æ¢ï¼Œå…è®¸FSMæä¾›åº”ç”¨æ‰§è¡Œçš„ç»“æ„åŒ–è¡¨ç¤ºã€‚MAPLEç”±è´Ÿè´£ä»»åŠ¡æ‰§è¡Œå››ä¸ªé˜¶æ®µçš„ä¸“èŒä»£ç†ç»„æˆï¼šè§„åˆ’ã€æ‰§è¡Œã€éªŒè¯ã€é”™è¯¯æ¢å¤å’ŒçŸ¥è¯†ä¿ç•™ã€‚è¿™äº›ä»£ç†åä½œï¼Œæ ¹æ®ä»UIå±å¹•æå–çš„æ„ŸçŸ¥æ•°æ®å®æ—¶åŠ¨æ€æ„å»ºFSMï¼Œä½¿GUIä»£ç†èƒ½å¤Ÿè·Ÿè¸ªå¯¼èˆªè¿›åº¦å’Œæµç¨‹ï¼Œé€šè¿‡çŠ¶æ€çš„å‰ç½®å’Œåç½®æ¡ä»¶éªŒè¯æ“ä½œç»“æœï¼Œå¹¶é€šè¿‡å›æ»šåˆ°å…ˆå‰ç¨³å®šçŠ¶æ€æ¥ä»é”™è¯¯ä¸­æ¢å¤ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è·¨åº”ç”¨åŸºå‡†æµ‹è¯•Mobile-Eval-Eå’ŒSPA-Benchä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒMAPLEä¼˜äºæœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œä»»åŠ¡æˆåŠŸç‡æé«˜12%ï¼Œæ¢å¤æˆåŠŸç‡æé«˜13.8%ï¼Œè¡ŒåŠ¨å‡†ç¡®æ€§æé«˜6.5%ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†ç»“æ„åŒ–çŠ¶æ€å»ºæ¨¡åœ¨æŒ‡å¯¼ç§»åŠ¨GUIä»£ç†æ‰§è¡Œä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„FSMè¡¨ç¤ºå¯ä»¥ä½œä¸ºæœªæ¥GUIä»£ç†æ¶æ„ä¸­çš„è½»é‡çº§ã€æ¨¡å‹æ— å…³çš„å†…å­˜å±‚è¿›è¡Œé›†æˆï¼Œä»¥æ”¯æŒç»“æ„åŒ–è§„åˆ’ã€æ‰§è¡ŒéªŒè¯å’Œé”™è¯¯æ¢å¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23596v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç§»åŠ¨GUIä»£ç†æ—¨åœ¨è‡ªä¸»å®Œæˆè·¨ç§»åŠ¨åº”ç”¨çš„ä»»åŠ¡ã€‚æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥ä½¿å¾—è¿™äº›ä»£ç†èƒ½å¤Ÿè§£é‡ŠUIå±å¹•ã€è¯†åˆ«å¯æ“ä½œå…ƒç´ å¹¶è¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰ä»£ç†ä»ç„¶ååº”æ€§å¼ºï¼Œä»…å¯¹å½“å‰å±å¹•è¿›è¡Œæ¨ç†ï¼Œç¼ºä¹åº”ç”¨å¯¼èˆªæµçš„ç»“æ„åŒ–æ¨¡å‹ï¼Œé™åˆ¶äº†å®ƒä»¬ç†è§£ä¸Šä¸‹æ–‡ã€æ£€æµ‹æ„å¤–ç»“æœå’Œä»é”™è¯¯ä¸­æ¢å¤çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MAPLEï¼Œä¸€ç§çŠ¶æ€æ„ŸçŸ¥çš„å¤šä»£ç†æ¡†æ¶ï¼Œå®ƒå°†åº”ç”¨äº¤äº’æŠ½è±¡ä¸ºæœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰ã€‚æ¯ä¸ªUIå±å¹•è¢«è®¡ç®—å»ºæ¨¡ä¸ºä¸€ä¸ªç¦»æ•£çŠ¶æ€ï¼Œç”¨æˆ·æ“ä½œè¢«è§†ä¸ºè½¬æ¢ï¼Œå…è®¸FSMæä¾›åº”ç”¨æ‰§è¡Œçš„ç»“æ„åŒ–è¡¨ç¤ºã€‚MAPLEç”±ä¸“é—¨è´Ÿè´£ä»»åŠ¡æ‰§è¡Œå››ä¸ªé˜¶æ®µçš„ä»£ç†ç»„æˆï¼šè§„åˆ’ã€æ‰§è¡Œã€éªŒè¯ã€é”™è¯¯æ¢å¤å’ŒçŸ¥è¯†ä¿ç•™ã€‚è¿™äº›ä»£ç†ååŒå·¥ä½œï¼Œæ ¹æ®ä»UIå±å¹•ä¸­æå–çš„æ„ŸçŸ¥æ•°æ®å®æ—¶æ„å»ºFSMï¼Œä½¿GUIä»£ç†èƒ½å¤Ÿè·Ÿè¸ªå¯¼èˆªè¿›åº¦å’Œæµç¨‹ï¼Œé€šè¿‡çŠ¶æ€çš„å‰ç½®å’Œåç½®æ¡ä»¶éªŒè¯æ“ä½œç»“æœï¼Œå¹¶é€šè¿‡å›æ»šåˆ°å…ˆå‰ç¨³å®šçŠ¶æ€æ¥ä»é”™è¯¯ä¸­æ¢å¤ã€‚åœ¨ä¸¤é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„è·¨åº”ç”¨åŸºå‡†æµ‹è¯•Mobile-Eval-Eå’ŒSPA-Benchä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒMAPLEä¼˜äºæœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œä»»åŠ¡æˆåŠŸç‡æé«˜12%ï¼Œæ¢å¤æˆåŠŸç‡æé«˜13.8%ï¼Œè¡ŒåŠ¨å‡†ç¡®ç‡æé«˜6.5%ã€‚ç»“æœå¼ºè°ƒäº†ç»“æ„åŒ–çŠ¶æ€å»ºæ¨¡åœ¨æŒ‡å¯¼ç§»åŠ¨GUIä»£ç†ä»»åŠ¡æ‰§è¡Œä¸­çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„FSMè¡¨ç¤ºå¯ä»¥é›†æˆåˆ°æœªæ¥çš„GUIä»£ç†æ¶æ„ä¸­ï¼Œä½œä¸ºè½»é‡çº§ã€æ¨¡å‹æ— å…³çš„è®°å¿†å±‚ï¼Œæ”¯æŒç»“æ„åŒ–è§„åˆ’ã€æ‰§è¡ŒéªŒè¯å’Œé”™è¯¯æ¢å¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§»åŠ¨GUIä»£ç†è‡´åŠ›äºè‡ªä¸»å®Œæˆè·¨ç§»åŠ¨åº”ç”¨çš„ä»»åŠ¡ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½¿ä»£ç†å…·å¤‡è§£é‡ŠUIå±å¹•ã€è¯†åˆ«å…ƒç´ å’Œäº¤äº’çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰ä»£ç†ç¼ºä¹åº”ç”¨å¯¼èˆªæµçš„ç»“æ„åŒ–æ¨¡å‹ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸Šä¸‹æ–‡ç†è§£ã€æ„å¤–æ£€æµ‹åŠé”™è¯¯æ¢å¤æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>MAPLEåˆ©ç”¨æœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰ä¸ºåº”ç”¨äº¤äº’æä¾›ç»“æ„åŒ–è¡¨ç¤ºã€‚</li>
<li>MAPLEåŒ…å«å¤šä¸ªä¸“é—¨ä»£ç†ï¼Œè´Ÿè´£è§„åˆ’ã€æ‰§è¡Œã€éªŒè¯ã€é”™è¯¯æ¢å¤å’ŒçŸ¥è¯†ä¿ç•™ã€‚</li>
<li>MAPLEé€šè¿‡å®æ—¶æ„å»ºFSMæ¥æé«˜ä»»åŠ¡æ‰§è¡Œçš„ç»“æ„æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9027258f904bf4857d3572d3de35dc60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96fdc67e15657107ea50458ec12dc727.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Jigsaw-R1-A-Study-of-Rule-based-Visual-Reinforcement-Learning-with-Jigsaw-Puzzles"><a href="#Jigsaw-R1-A-Study-of-Rule-based-Visual-Reinforcement-Learning-with-Jigsaw-Puzzles" class="headerlink" title="Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with   Jigsaw Puzzles"></a>Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with   Jigsaw Puzzles</h2><p><strong>Authors:Zifu Wang, Junyi Zhu, Bo Tang, Zhiyu Li, Feiyu Xiong, Jiaqian Yu, Matthew B. Blaschko</strong></p>
<p>The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL using jigsaw puzzles as a structured experimental framework, revealing several key findings. \textit{Firstly,} we find that MLLMs, initially performing near to random guessing on simple puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. \textit{Secondly,} training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. \textit{Thirdly,} MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. \textit{Fourthly,} we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. \textit{Finally,} our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: \href{<a target="_blank" rel="noopener" href="https://github.com/zifuwanggg/Jigsaw-R1%7D%7Bhttps://github.com/zifuwanggg/Jigsaw-R1%7D">https://github.com/zifuwanggg/Jigsaw-R1}{https://github.com/zifuwanggg/Jigsaw-R1}</a>. </p>
<blockquote>
<p>å°†åŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸ºæ„ŸçŸ¥å¯†é›†å‹ä»»åŠ¡å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜å’Œå¯èƒ½çš„åå·®ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬é¢†åŸŸä»¥å¤–çš„å‘ç°ã€‚æœ¬æ–‡é‡‡ç”¨æ‹¼å›¾ä½œä¸ºç»“æ„åŒ–å®éªŒæ¡†æ¶ï¼Œå¯¹åŸºäºè§„åˆ™çš„è§†è§‰å¼ºåŒ–å­¦ä¹ è¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ï¼Œæ­ç¤ºäº†å‡ ä¸ªå…³é”®å‘ç°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å‘ç°MLLMsåœ¨ç®€å•çš„æ‹¼å›¾ä»»åŠ¡ä¸Šæœ€åˆçš„è¡¨ç°æ¥è¿‘éšæœºçŒœæµ‹ï¼Œä½†é€šè¿‡å¾®è°ƒå¯ä»¥è¾¾åˆ°è¿‘ä¹å®Œç¾çš„å‡†ç¡®ç‡å’Œæ³›åŒ–åˆ°å¤æ‚çš„æœªè§è¿‡çš„é…ç½®ã€‚å…¶æ¬¡ï¼Œåœ¨æ‹¼å›¾ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–è§†è§‰ä»»åŠ¡ï¼Œå…¶æœ‰æ•ˆæ€§å–å†³äºç‰¹å®šçš„ä»»åŠ¡é…ç½®ã€‚ç¬¬ä¸‰ï¼ŒMLLMså¯ä»¥åœ¨æ²¡æœ‰æ˜¾å¼æ¨ç†çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ³›åŒ–ï¼Œå°½ç®¡å¼€æºæ¨¡å‹é€šå¸¸å€¾å‘äºç›´æ¥å›ç­”é—®é¢˜ã€‚å› æ­¤ï¼Œå³ä½¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œé€æ­¥æ¨ç†ï¼Œä»–ä»¬ä¹Ÿå¯èƒ½å¿½ç•¥å¾—å‡ºæœ€ç»ˆç­”æ¡ˆçš„æ€è€ƒè¿‡ç¨‹ã€‚ç¬¬å››ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å¤æ‚çš„æ¨ç†æ¨¡å¼ä¼¼ä¹æ˜¯é¢„å…ˆå­˜åœ¨çš„è€Œä¸æ˜¯çªç„¶å‡ºç°çš„ï¼Œå®ƒä»¬çš„é¢‘ç‡éšç€è®­ç»ƒå’Œä»»åŠ¡éš¾åº¦çš„å¢åŠ è€Œå¢åŠ ã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜å¼ºåŒ–å­¦ä¹ ç›¸è¾ƒäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œåˆå§‹çš„SFTå†·å¯åŠ¨é˜¶æ®µå¯èƒ½ä¼šé˜»ç¢åç»­çš„RLä¼˜åŒ–ã€‚å°½ç®¡è¿™äº›è§‚å¯Ÿæ˜¯åŸºäºæ‹¼å›¾ä»»åŠ¡çš„ï¼Œå¹¶å¯èƒ½åœ¨å…¶ä»–è§†è§‰ä»»åŠ¡ä¸­æœ‰æ‰€ä¸åŒï¼Œä½†æœ¬ç ”ç©¶ä¸ºé›†ä½“ç†è§£åŸºäºè§„åˆ™çš„è§†è§‰å¼ºåŒ–å­¦ä¹ åŠå…¶åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ½œåŠ›è¿™ä¸€æ›´å¤§çš„è°œé¢˜è´¡çŒ®äº†ä¸€å—æœ‰ä»·å€¼çš„æ‹¼å›¾ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/zifuwanggg/Jigsaw-R1%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/zifuwanggg/Jigsaw-R1è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23590v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜å’Œæ–‡æœ¬åŸŸç ”ç©¶çš„æ½œåœ¨åå·®ï¼Œç‰¹åˆ«æ˜¯åœ¨æ„ŸçŸ¥ä»»åŠ¡æ–¹é¢ã€‚æœ¬æ–‡é€šè¿‡æ‹¼å›¾å®éªŒæ¡†æ¶è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼Œæ­ç¤ºäº†å…³é”®å‘ç°ï¼šé¦–å…ˆï¼Œæœ€åˆå‡ ä¹éšæœºçŒœæµ‹çš„MLLMåœ¨å¾®è°ƒåå‡ ä¹è¾¾åˆ°å®Œç¾å‡†ç¡®åº¦å¹¶æ¨å¹¿åˆ°å¤æ‚æœªè§é…ç½®ï¼›å…¶æ¬¡ï¼Œæ‹¼å›¾è®­ç»ƒå¯ä»¥è¯±å¯¼åˆ°å…¶ä»–è§†è§‰ä»»åŠ¡çš„æ³›åŒ–ï¼Œä½†å…¶æœ‰æ•ˆæ€§å–å†³äºç‰¹å®šä»»åŠ¡é…ç½®ï¼›å†æ¬¡ï¼ŒMLLMå¯ç‹¬ç«‹è¿›è¡Œå­¦ä¹ å’Œæ³›åŒ–æ¨ç†æˆ–å€ŸåŠ©å¼€æºæ¨¡å‹ç›´æ¥å›ç­”é—®é¢˜ï¼›ç¬¬å››ï¼Œå¤æ‚æ¨ç†æ¨¡å¼ä¼¼ä¹ä¸ç”Ÿä¿±æ¥è€Œéçªç„¶å‡ºç°ï¼Œé¢‘ç‡éšè®­ç»ƒå’Œä»»åŠ¡éš¾åº¦å¢åŠ è€Œå¢åŠ ï¼›æœ€åï¼Œç›¸å¯¹äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼ŒRLè¡¨ç°å‡ºæ›´å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œåˆå§‹SFTå†·å¯åŠ¨é˜¶æ®µå¯èƒ½ä¼šé˜»ç¢åç»­RLä¼˜åŒ–ã€‚å°½ç®¡è¿™äº›è§‚å¯Ÿä»…é™äºæ‹¼å›¾ä»»åŠ¡ï¼Œä½†æœ¬ç ”ç©¶ä¸ºç†è§£åŸºäºè§„åˆ™çš„è§†è§‰å¼ºåŒ–å­¦ä¹ åŠå…¶åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ½œåŠ›æä¾›äº†é‡è¦çº¿ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>MLLMsé€šè¿‡å¾®è°ƒå¯ä»¥åœ¨æ‹¼å›¾ä»»åŠ¡ä¸Šå®ç°é«˜å‡†ç¡®åº¦å¹¶æ³›åŒ–åˆ°å¤æ‚é…ç½®ã€‚</li>
<li>æ‹¼å›¾è®­ç»ƒæœ‰åŠ©äºæ³›åŒ–åˆ°å…¶ä»–è§†è§‰ä»»åŠ¡ï¼Œä»»åŠ¡é…ç½®å½±å“å…¶æ•ˆæœã€‚</li>
<li>MLLMå¯åœ¨æ— æ˜ç¡®æ¨ç†çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ³›åŒ–ï¼Œä½†å¼€æºæ¨¡å‹å€¾å‘äºç›´æ¥å›ç­”ã€‚</li>
<li>å¤æ‚æ¨ç†æ¨¡å¼ä¼¼ä¹å›ºæœ‰å­˜åœ¨ï¼Œé¢‘ç‡éšè®­ç»ƒå’Œä»»åŠ¡éš¾åº¦å¢åŠ ã€‚</li>
<li>RLç›¸è¾ƒäºç›‘ç£å¾®è°ƒå±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åˆå§‹ç›‘ç£å¾®è°ƒå†·å¯åŠ¨å¯èƒ½å½±å“åç»­RLä¼˜åŒ–ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹ç†è§£åŸºäºè§„åˆ™çš„è§†è§‰å¼ºåŒ–å­¦ä¹ åŠå…¶åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ½œåŠ›æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23590">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-80e07546a8dd6f4c6ffd4263f22051a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d15d4912786a6ce57e2b5f5c3060f5b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="On-Policy-RL-with-Optimal-Reward-Baseline"><a href="#On-Policy-RL-with-Optimal-Reward-Baseline" class="headerlink" title="On-Policy RL with Optimal Reward Baseline"></a>On-Policy RL with Optimal Reward Baseline</h2><p><strong>Authors:Yaru Hao, Li Dong, Xun Wu, Shaohan Huang, Zewen Chi, Furu Wei</strong></p>
<p>Reinforcement learning algorithms are fundamental to align large language models with human preferences and to enhance their reasoning capabilities. However, current reinforcement learning algorithms often suffer from training instability due to loose on-policy constraints and computational inefficiency due to auxiliary models. In this work, we propose On-Policy RL with Optimal reward baseline (OPO), a novel and simplified reinforcement learning algorithm designed to address these challenges. OPO emphasizes the importance of exact on-policy training, which empirically stabilizes the training process and enhances exploration. Moreover, OPO introduces the optimal reward baseline that theoretically minimizes gradient variance. We evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its superior performance and training stability without additional models or regularization terms. Furthermore, OPO achieves lower policy shifts and higher output entropy, encouraging more diverse and less repetitive responses. These results highlight OPO as a promising direction for stable and effective reinforcement learning in large language model alignment and reasoning tasks. The implementation is provided at <a target="_blank" rel="noopener" href="https://github.com/microsoft/LMOps/tree/main/opo">https://github.com/microsoft/LMOps/tree/main/opo</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½å¹¶æé«˜å…¶æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å¾€å¾€å­˜åœ¨ç”±äºæ”¿ç­–çº¦æŸå®½æ¾è€Œå¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®šé—®é¢˜å’Œç”±äºè¾…åŠ©æ¨¡å‹å¯¼è‡´çš„è®¡ç®—æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæœ€ä¼˜å¥–åŠ±åŸºçº¿çš„On-Policy RLï¼ˆOPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜çš„æ–°å‹ç®€åŒ–å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚OPOå¼ºè°ƒç²¾ç¡®çš„ç­–ç•¥å†…è®­ç»ƒçš„é‡è¦æ€§ï¼Œè¿™ä»ç»éªŒä¸Šç¨³å®šäº†è®­ç»ƒè¿‡ç¨‹å¹¶å¢å¼ºäº†æ¢ç´¢ã€‚æ­¤å¤–ï¼ŒOPOå¼•å…¥äº†æœ€ä¼˜å¥–åŠ±åŸºçº¿ï¼Œä»ç†è®ºä¸Šæœ€å°åŒ–äº†æ¢¯åº¦æ–¹å·®ã€‚æˆ‘ä»¬åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå¯¹OPOè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œå®ƒåœ¨æ²¡æœ‰é™„åŠ æ¨¡å‹æˆ–æ­£åˆ™åŒ–æœ¯è¯­çš„æƒ…å†µä¸‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ã€‚æ­¤å¤–ï¼ŒOPOå®ç°äº†æ›´ä½çš„ç­–ç•¥è½¬å˜å’Œæ›´é«˜çš„è¾“å‡ºç†µï¼Œé¼“åŠ±äº§ç”Ÿæ›´å¤šæ ·åŒ–ä¸”æ›´å°‘é‡å¤æ€§çš„å“åº”ã€‚è¿™äº›ç»“æœçªæ˜¾äº†OPOåœ¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½å’Œæ¨ç†ä»»åŠ¡ä¸­å®ç°ç¨³å®šæœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ çš„æœ‰å‰é€”çš„æ–¹å‘ã€‚ç›¸å…³å®ç°å¯è§äº<a target="_blank" rel="noopener" href="https://github.com/microsoft/LMOps/tree/main/opo%E3%80%82">https://github.com/microsoft/LMOps/tree/main/opoã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23585v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½åŠæå‡æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰å¼ºåŒ–å­¦ä¹ ç®—æ³•å¸¸é¢ä¸´è®­ç»ƒä¸ç¨³å®šå’Œè®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹ç®€åŒ–å¼ºåŒ–å­¦ä¹ ç®—æ³•â€”â€”åŸºäºæœ€ä¼˜å¥–åŠ±åŸºçº¿çš„åœ¨ç­–ç•¥å¼ºåŒ–å­¦ä¹ ï¼ˆOPOï¼‰ï¼Œä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚OPOå¼ºè°ƒç²¾ç¡®çš„åœ¨ç­–ç•¥è®­ç»ƒï¼Œè¿™æœ‰åŠ©äºç¨³å®šè®­ç»ƒè¿‡ç¨‹å¹¶æå‡æ¢ç´¢èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒOPOå¼•å…¥æœ€ä¼˜å¥–åŠ±åŸºçº¿ï¼Œç†è®ºä¸Šå¯æœ€å°åŒ–æ¢¯åº¦æ–¹å·®ã€‚æˆ‘ä»¬åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå¯¹OPOè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œå…¶åœ¨æ— é¢å¤–æ¨¡å‹æˆ–æ­£åˆ™åŒ–æœ¯è¯­çš„æƒ…å†µä¸‹ï¼Œå…·æœ‰ä¼˜è¶Šæ€§èƒ½å’Œç¨³å®šçš„è®­ç»ƒè¡¨ç°ã€‚æ­¤å¤–ï¼ŒOPOå®ç°äº†æ›´ä½çš„ç­–ç•¥è½¬ç§»å’Œæ›´é«˜çš„è¾“å‡ºç†µï¼Œé¼“åŠ±ç”Ÿæˆæ›´å¤šæ ·åŒ–ã€æ›´å°‘é‡å¤æ€§çš„å›åº”ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†OPOåœ¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½å’Œæ¨ç†ä»»åŠ¡ä¸­å…·å¤‡ç¨³å®šä¸”æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸äººç±»åå¥½å¯¹é½åŠæå‡æ¨ç†èƒ½åŠ›éå¸¸é‡è¦ã€‚</li>
<li>å½“å‰å¼ºåŒ–å­¦ä¹ ç®—æ³•å­˜åœ¨è®­ç»ƒä¸ç¨³å®šå’Œè®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</li>
<li>OPOç®—æ³•å¼ºè°ƒç²¾ç¡®çš„åœ¨ç­–ç•¥è®­ç»ƒï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒè¿‡ç¨‹å¹¶æå‡æ¢ç´¢èƒ½åŠ›ã€‚</li>
<li>OPOå¼•å…¥æœ€ä¼˜å¥–åŠ±åŸºçº¿ï¼Œç†è®ºä¸Šå¯æœ€å°åŒ–æ¢¯åº¦æ–¹å·®ã€‚</li>
<li>åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒOPOè¡¨ç°ä¼˜è¶Šï¼Œå…·æœ‰ç¨³å®šçš„è®­ç»ƒæ€§èƒ½ã€‚</li>
<li>OPOå®ç°æ›´ä½çš„ç­–ç•¥è½¬ç§»å’Œæ›´é«˜çš„è¾“å‡ºç†µï¼Œé¼“åŠ±ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„å›åº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-779d6334453fde2b57fd93456aee563b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16d9a6c967fccfdd4560736f3589596c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-562329c2a6698292da686dcc4c3f11a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-527d18431366a97b0cf1a313f2a82f90.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="BioReason-Incentivizing-Multimodal-Biological-Reasoning-within-a-DNA-LLM-Model"><a href="#BioReason-Incentivizing-Multimodal-Biological-Reasoning-within-a-DNA-LLM-Model" class="headerlink" title="BioReason: Incentivizing Multimodal Biological Reasoning within a   DNA-LLM Model"></a>BioReason: Incentivizing Multimodal Biological Reasoning within a   DNA-LLM Model</h2><p><strong>Authors:Adibvafa Fallahpour, Andrew Magnuson, Purav Gupta, Shihao Ma, Jack Naimer, Arnav Shah, Haonan Duan, Omar Ibrahim, Hani Goodarzi, Chris J. Maddison, Bo Wang</strong></p>
<p>Unlocking deep, interpretable biological reasoning from complex genomic data is a major AI challenge hindering scientific discovery. Current DNA foundation models, despite strong sequence representation, struggle with multi-step reasoning and lack inherent transparent, biologically intuitive explanations. We introduce BioReason, a pioneering architecture that, for the first time, deeply integrates a DNA foundation model with a Large Language Model (LLM). This novel connection enables the LLM to directly process and reason with genomic information as a fundamental input, fostering a new form of multimodal biological understanding. BioReasonâ€™s sophisticated multi-step reasoning is developed through supervised fine-tuning and targeted reinforcement learning, guiding the system to generate logical, biologically coherent deductions. On biological reasoning benchmarks including KEGG-based disease pathway prediction - where accuracy improves from 88% to 97% - and variant effect prediction, BioReason demonstrates an average 15% performance gain over strong single-modality baselines. BioReason reasons over unseen biological entities and articulates decision-making through interpretable, step-by-step biological traces, offering a transformative approach for AI in biology that enables deeper mechanistic insights and accelerates testable hypothesis generation from genomic data. Data, code, and checkpoints are publicly available at <a target="_blank" rel="noopener" href="https://github.com/bowang-lab/BioReason">https://github.com/bowang-lab/BioReason</a> </p>
<blockquote>
<p>ä»å¤æ‚çš„åŸºå› ç»„æ•°æ®ä¸­è§£é”æ·±å±‚ã€å¯è§£é‡Šçš„ç”Ÿç‰©æ¨ç†æ˜¯é˜»ç¢ç§‘å­¦å‘ç°çš„ä¸€é¡¹é‡å¤§äººå·¥æ™ºèƒ½æŒ‘æˆ˜ã€‚å°½ç®¡å½“å‰çš„DNAåŸºç¡€æ¨¡å‹å…·æœ‰å¼ºå¤§çš„åºåˆ—è¡¨ç¤ºèƒ½åŠ›ï¼Œä½†åœ¨å¤šæ­¥æ¨ç†æ–¹é¢ä»é¢ä¸´å›°éš¾ï¼Œå¹¶ä¸”ç¼ºä¹å†…åœ¨é€æ˜ã€ç”Ÿç‰©å­¦ä¸Šç›´è§‚çš„è§£é‡Šã€‚æˆ‘ä»¬å¼•å…¥äº†BioReasonï¼Œè¿™æ˜¯ä¸€ç§å¼€åˆ›æ€§çš„æ¶æ„ï¼Œé¦–æ¬¡å°†DNAåŸºç¡€æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ·±åº¦é›†æˆã€‚è¿™ç§æ–°å‹è¿æ¥ä½¿LLMèƒ½å¤Ÿç›´æ¥å¤„ç†å’Œæ¨ç†åŸºå› ç»„ä¿¡æ¯ä½œä¸ºåŸºæœ¬è¾“å…¥ï¼Œä¿ƒè¿›äº†ä¸€ç§æ–°çš„å¤šæ¨¡å¼ç”Ÿç‰©ç†è§£ã€‚BioReasonçš„é«˜çº§å¤šæ­¥æ¨ç†æ˜¯é€šè¿‡æœ‰ç›‘ç£çš„å¾®è°ƒä»¥åŠæœ‰é’ˆå¯¹æ€§çš„å¼ºåŒ–å­¦ä¹ æ¥å‘å±•çš„ï¼Œå¼•å¯¼ç³»ç»Ÿäº§ç”Ÿé€»è¾‘ä¸Šè¿è´¯ã€ç”Ÿç‰©å­¦ä¸Šè¿è´¯çš„æ¨è®ºã€‚åœ¨åŒ…æ‹¬åŸºäºKEGGçš„ç–¾ç—…é€”å¾„é¢„æµ‹ç­‰ç”Ÿç‰©æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼Œå‡†ç¡®ç‡ä»88%æé«˜åˆ°97%ï¼Œè€Œåœ¨å˜ä½“æ•ˆåº”é¢„æµ‹æ–¹é¢ï¼ŒBioReasonç›¸è¾ƒäºå¼ºå¤§çš„å•æ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œè¡¨ç°å‡ºå¹³å‡15%çš„æ€§èƒ½æå‡ã€‚BioReasonèƒ½å¤Ÿå¯¹æœªè§è¿‡çš„ç”Ÿç‰©å®ä½“è¿›è¡Œæ¨ç†ï¼Œå¹¶é€šè¿‡å¯è§£é‡Šçš„ã€é€æ­¥çš„ç”Ÿç‰©è½¨è¿¹è¡¨è¿°å†³ç­–è¿‡ç¨‹ï¼Œè¿™ä¸ºç”Ÿç‰©å­¦ä¸­çš„äººå·¥æ™ºèƒ½æä¾›äº†ä¸€ç§å˜é©æ€§çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæä¾›æ›´æ·±å…¥çš„æœºåˆ¶æ´å¯Ÿï¼Œå¹¶ä»åŸºå› ç»„æ•°æ®ä¸­åŠ é€Ÿå¯æµ‹è¯•å‡è®¾çš„äº§ç”Ÿã€‚æ•°æ®ã€ä»£ç å’Œæ£€æŸ¥ç‚¹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bowang-lab/BioReason">https://github.com/bowang-lab/BioReason</a>å…¬å¼€è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23579v1">PDF</a> 16 pages, 3 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªåä¸ºBioReasonçš„åˆ›æ–°æ¶æ„ï¼Œå®ƒå°†DNAåŸºç¡€æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ·±åº¦é›†æˆã€‚BioReasonèƒ½å¤Ÿç›´æ¥å¤„ç†åŸºå› ç»„ä¿¡æ¯å¹¶è¿›è¡Œå¤šæ­¥æ¨ç†ï¼Œä»è€Œå®ç°å¤šæ¨¡æ€ç”Ÿç‰©ç†è§£çš„æ–°å½¢å¼ã€‚åœ¨ç”Ÿç‰©æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒBioReasonè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡æ˜¾è‘—æé«˜ï¼Œå¹¶èƒ½å¤Ÿé€šè¿‡å¯è§£é‡Šçš„æ­¥éª¤æä¾›ç”Ÿç‰©å­¦ä¾æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BioReasonæ˜¯é¦–ä¸ªå°†DNAåŸºç¡€æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ·±åº¦é›†æˆçš„æ¶æ„ã€‚</li>
<li>BioReasonèƒ½å¤Ÿç›´æ¥å¤„ç†åŸºå› ç»„ä¿¡æ¯å¹¶è¿›è¡Œå¤šæ­¥æ¨ç†ã€‚</li>
<li>BioReasonå®ç°äº†å¤šæ¨¡æ€ç”Ÿç‰©ç†è§£çš„æ–°å½¢å¼ï¼Œä¿ƒè¿›å¯¹åŸºå› ç»„æ•°æ®çš„æ›´æ·±å±‚æ¬¡ç†è§£ã€‚</li>
<li>BioReasonåœ¨ç”Ÿç‰©æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚</li>
<li>BioReasonèƒ½å¤Ÿé€šè¿‡å¯è§£é‡Šçš„æ­¥éª¤æä¾›ç”Ÿç‰©å­¦ä¾æ®ï¼Œæœ‰åŠ©äºç”Ÿæˆå¯æµ‹è¯•çš„ç”Ÿç‰©å‡è¯´ã€‚</li>
<li>BioReasonç³»ç»Ÿå¯ç”Ÿæˆé€»è¾‘ä¸Šè¿è´¯ã€ç”Ÿç‰©å­¦ä¸Šåˆç†çš„æ¨è®ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1fcab2a9d41f713e9fa1a7385ebab295.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2443695f1e803134a14619be2fc393c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e09c1a7f3ef0adecb05224361e20787.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Segment-Policy-Optimization-Effective-Segment-Level-Credit-Assignment-in-RL-for-Large-Language-Models"><a href="#Segment-Policy-Optimization-Effective-Segment-Level-Credit-Assignment-in-RL-for-Large-Language-Models" class="headerlink" title="Segment Policy Optimization: Effective Segment-Level Credit Assignment   in RL for Large Language Models"></a>Segment Policy Optimization: Effective Segment-Level Credit Assignment   in RL for Large Language Models</h2><p><strong>Authors:Yiran Guo, Lijie Xu, Jie Liu, Dan Ye, Shuang Qiu</strong></p>
<p>Enhancing the reasoning capabilities of large language models effectively using reinforcement learning (RL) remains a crucial challenge. Existing approaches primarily adopt two contrasting advantage estimation granularities: Token-level methods (e.g., PPO) aim to provide the fine-grained advantage signals but suffer from inaccurate estimation due to difficulties in training an accurate critic model. On the other extreme, trajectory-level methods (e.g., GRPO) solely rely on a coarse-grained advantage signal from the final reward, leading to imprecise credit assignment. To address these limitations, we propose Segment Policy Optimization (SPO), a novel RL framework that leverages segment-level advantage estimation at an intermediate granularity, achieving a better balance by offering more precise credit assignment than trajectory-level methods and requiring fewer estimation points than token-level methods, enabling accurate advantage estimation based on Monte Carlo (MC) without a critic model. SPO features three components with novel strategies: (1) flexible segment partition; (2) accurate segment advantage estimation; and (3) policy optimization using segment advantages, including a novel probability-mask strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain for short chain-of-thought (CoT), featuring novel cutpoint-based partition and chain-based advantage estimation, achieving $6$-$12$ percentage point improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT, featuring novel tree-based advantage estimation, which significantly reduces the cost of MC estimation, achieving $7$-$11$ percentage point improvements over GRPO on MATH500 under 2K and 4K context evaluation. We make our code publicly available at <a target="_blank" rel="noopener" href="https://github.com/AIFrameResearch/SPO">https://github.com/AIFrameResearch/SPO</a>. </p>
<blockquote>
<p>ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æœ‰æ•ˆåœ°å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é‡‡ç”¨ä¸¤ç§ç›¸åçš„ä¼˜åŠ¿ä¼°è®¡ç²’åº¦ï¼šä»¤ç‰Œçº§æ–¹æ³•ï¼ˆä¾‹å¦‚PPOï¼‰æ—¨åœ¨æä¾›ç²¾ç»†ç²’åº¦çš„ä¼˜åŠ¿ä¿¡å·ï¼Œä½†ç”±äºéš¾ä»¥è®­ç»ƒå‡†ç¡®çš„è¯„è®ºå®¶æ¨¡å‹ï¼Œå¯¼è‡´ä¼°è®¡ä¸å‡†ç¡®ã€‚å¦ä¸€æ–¹é¢ï¼Œè½¨è¿¹çº§æ–¹æ³•ï¼ˆä¾‹å¦‚GRPOï¼‰ä»…ä¾èµ–äºæ¥è‡ªæœ€ç»ˆå¥–åŠ±çš„ç²—ç•¥ä¼˜åŠ¿ä¿¡å·ï¼Œå¯¼è‡´ä¿¡ç”¨åˆ†é…ä¸ç²¾ç¡®ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†æ®µç­–ç•¥ä¼˜åŒ–ï¼ˆSPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„RLæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä¸­é—´ç²’åº¦çš„åˆ†æ®µçº§ä¼˜åŠ¿ä¼°è®¡ï¼Œé€šè¿‡æä¾›æ›´ç²¾ç¡®çš„ä¿¡ç”¨åˆ†é…æ¯”è½¨è¿¹çº§æ–¹æ³•ï¼Œå¹¶ä¸”éœ€è¦æ¯”ä»¤ç‰Œçº§æ–¹æ³•æ›´å°‘çš„ä¼°è®¡ç‚¹ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰è¯„è®ºå®¶æ¨¡å‹çš„æƒ…å†µä¸‹åŸºäºè’™ç‰¹å¡æ´›ï¼ˆMCï¼‰è¿›è¡Œå‡†ç¡®çš„ä¼˜åŠ¿ä¼°è®¡ã€‚SPOæœ‰ä¸‰ä¸ªå…·æœ‰æ–°ç­–ç•¥çš„ç‰¹ç‚¹ï¼šï¼ˆ1ï¼‰çµæ´»çš„åˆ†æ®µåˆ†åŒºï¼›ï¼ˆ2ï¼‰ç²¾ç¡®çš„åˆ†æ®µä¼˜åŠ¿ä¼°è®¡ï¼›ï¼ˆ3ï¼‰ä½¿ç”¨åˆ†æ®µä¼˜åŠ¿çš„ç­–ç•¥ä¼˜åŒ–ï¼ŒåŒ…æ‹¬ä¸€ç§æ–°çš„æ¦‚ç‡æ©ç ç­–ç•¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä¸ºä¸¤ä¸ªç‰¹å®šåœºæ™¯å®ä¾‹åŒ–äº†SPOï¼šï¼ˆ1ï¼‰SPO-chainç”¨äºçŸ­é“¾æ€ç»´ï¼ˆCoTï¼‰ï¼Œå…·æœ‰åŸºäºåˆ‡å‰²ç‚¹çš„åˆ†åŒºå’ŒåŸºäºé“¾çš„ä¼˜åŠ¿ä¼°è®¡ï¼Œåœ¨GSM8Kä¸Šç›¸å¯¹äºPPOå’ŒGRPOå®ç°äº†6%-12%çš„å‡†ç¡®ç‡æå‡ã€‚ï¼ˆ2ï¼‰SPO-treeç”¨äºé•¿CoTï¼Œå…·æœ‰åŸºäºæ ‘çš„ä¼˜åŠ¿ä¼°è®¡ï¼Œè¿™æ˜¾è‘—å‡å°‘äº†MCä¼°è®¡çš„æˆæœ¬ï¼Œåœ¨MATH500çš„2Kå’Œ4Kä¸Šä¸‹æ–‡è¯„ä¼°ä¸‹ï¼Œç›¸å¯¹äºGRPOå®ç°äº†7%-11%çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/AIFrameResearch/SPO%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/AIFrameResearch/SPOå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23564v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¼ºåŒ–ä¾ç„¶æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡ä¸¤ç§ä¸åŒçš„ä¼˜åŠ¿ä¼°è®¡ç²’åº¦è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå¦‚tokençº§æ–¹æ³•å’Œè½¨è¿¹çº§æ–¹æ³•ã€‚é’ˆå¯¹è¿™äº›æ–¹æ³•çš„ä¸å‡†ç¡®ä¿¡ç”¨åˆ†é…é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶â€”â€”åˆ†æ®µç­–ç•¥ä¼˜åŒ–ï¼ˆSPOï¼‰ã€‚SPOé€šè¿‡ä¸­é—´ç²’åº¦çš„åˆ†æ®µçº§ä¼˜åŠ¿ä¼°è®¡å®ç°æ›´ç²¾ç¡®çš„ä¿¡ç”¨åˆ†é…ï¼Œå¹¶å¼•å…¥äº†çµæ´»çš„æ®µåˆ’åˆ†ã€ç²¾ç¡®çš„åˆ†æ®µä¼˜åŠ¿ä¼°è®¡å’ŒåŸºäºåˆ†æ®µä¼˜åŠ¿çš„ç­–ç•¥ä¼˜åŒ–ç­‰ä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é’ˆå¯¹çŸ­é“¾å’Œé•¿é“¾ä¸¤ç§æƒ…å†µåˆ†åˆ«è¿›è¡Œäº†å®ä¾‹åŒ–ç ”ç©¶ï¼Œå–å¾—äº†æ˜¾è‘—æˆæœã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å¼ºåŒ–æ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>å½“å‰çš„ä¼˜åŠ¿ä¼°è®¡æ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼štokençº§å’Œè½¨è¿¹çº§ï¼Œå®ƒä»¬å­˜åœ¨ä¸å‡†ç¡®çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„åˆ†æ®µç­–ç•¥ä¼˜åŒ–ï¼ˆSPOï¼‰æ¡†æ¶ç»“åˆäº†è¿™ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„ä¿¡ç”¨åˆ†é…ã€‚</li>
<li>SPOå…·æœ‰çµæ´»çš„æ®µåˆ’åˆ†ã€ç²¾ç¡®çš„åˆ†æ®µä¼˜åŠ¿ä¼°è®¡å’ŒåŸºäºåˆ†æ®µä¼˜åŠ¿çš„ç­–ç•¥ä¼˜åŒ–ç­‰å…³é”®ç»„ä»¶ã€‚</li>
<li>å¯¹äºçŸ­é“¾å’Œé•¿é“¾æƒ…å†µï¼Œåˆ†åˆ«è¿›è¡Œäº†å®ä¾‹åŒ–ç ”ç©¶ï¼Œå–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23564">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e587baf9198eaf6e435f1c963f95c727.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-294453246eef56fee2bdde19119923af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ad3b509e93f96b6082435bb75763695.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Qwen-Look-Again-Guiding-Vision-Language-Reasoning-Models-to-Re-attention-Visual-Information"><a href="#Qwen-Look-Again-Guiding-Vision-Language-Reasoning-Models-to-Re-attention-Visual-Information" class="headerlink" title="Qwen Look Again: Guiding Vision-Language Reasoning Models to   Re-attention Visual Information"></a>Qwen Look Again: Guiding Vision-Language Reasoning Models to   Re-attention Visual Information</h2><p><strong>Authors:Xu Chu, Xinrong Chen, Guanyu Wang, Zhijie Tan, Kui Huang, Wenyu Lv, Tong Mo, Weiping Li</strong></p>
<p>Inference time scaling drives extended reasoning to enhance the performance of Vision-Language Models (VLMs), thus forming powerful Vision-Language Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens, causing visual information to receive less attention and may trigger hallucinations. Although introducing text-only reflection processes shows promise in language models, we demonstrate that it is insufficient to suppress hallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain (Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a vision-text reflection process that guides the model to re-attention visual information during reasoning. We first propose a reinforcement learning method Balanced Reflective Policy Optimization (BRPO), which guides the model to decide when to generate vision-text reflection on its own and balance the number and length of reflections. Then, we formally prove that VLRMs lose attention to visual tokens as reasoning progresses, and demonstrate that supplementing visual information during reflection enhances visual attention. Therefore, during training and inference, Visual Token COPY and Visual Token ROUTE are introduced to force the model to re-attention visual information at the visual level, addressing the limitations of text-only reflection. Experiments on multiple visual QA datasets and hallucination metrics indicate that Qwen-LA achieves leading accuracy performance while reducing hallucinations. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/Liar406/Look_Again">https://github.com/Liar406/Look_Again</a>. </p>
<blockquote>
<p>æ¨ç†æ—¶é—´ç¼©æ”¾é©±åŠ¨æ‰©å±•æ¨ç†ï¼Œä»¥æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ€§èƒ½ï¼Œä»è€Œå½¢æˆå¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨ç†æ¨¡å‹ï¼ˆVLRMï¼‰ã€‚ç„¶è€Œï¼Œé•¿æ—¶é—´çš„æ¨ç†ä¼šç¨€é‡Šè§†è§‰æ ‡è®°ï¼Œå¯¼è‡´è§†è§‰ä¿¡æ¯å—åˆ°çš„å…³æ³¨å‡å°‘ï¼Œå¹¶å¯èƒ½å¼•å‘å¹»è§‰ã€‚è™½ç„¶åœ¨è¯­è¨€æ¨¡å‹ä¸­å¼•å…¥çº¯æ–‡æœ¬åæ€è¿‡ç¨‹æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†æˆ‘ä»¬è¯æ˜åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æŠ‘åˆ¶å¹»è§‰ä»…å‡­è¿™ä¸€ç‚¹æ˜¯ä¸å¤Ÿçš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Qwen-LookAgainï¼ˆQwen-LAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹VLRMè®¾è®¡ï¼Œæ—¨åœ¨é€šè¿‡èå…¥è§†è§‰æ–‡æœ¬åæ€è¿‡ç¨‹æ¥å‡å°‘å¹»è§‰çš„å‘ç”Ÿï¼Œè¯¥è¿‡ç¨‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­æŒ‡å¯¼æ¨¡å‹é‡æ–°å…³æ³¨è§†è§‰ä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•â€”â€”å¹³è¡¡åå°„ç­–ç•¥ä¼˜åŒ–ï¼ˆBRPOï¼‰ï¼Œè¯¥æ–¹æ³•å¼•å¯¼æ¨¡å‹è‡ªè¡Œå†³å®šä½•æ—¶è¿›è¡Œè§†è§‰æ–‡æœ¬åå°„å¹¶å¹³è¡¡åå°„çš„æ•°é‡å’Œé•¿åº¦ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»å½¢å¼ä¸Šè¯æ˜äº†éšç€æ¨ç†çš„è¿›å±•ï¼ŒVLRMå¯¹è§†è§‰æ ‡è®°çš„å…³æ³¨åº¦ä¼šé€æ¸é™ä½ï¼Œå¹¶è¯æ˜äº†åœ¨åæ€è¿‡ç¨‹ä¸­è¡¥å……è§†è§‰ä¿¡æ¯å¯ä»¥å¢å¼ºè§†è§‰å…³æ³¨åº¦ã€‚å› æ­¤ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥äº†è§†è§‰æ ‡è®°å¤åˆ¶å’Œè§†è§‰æ ‡è®°è·¯ç”±ï¼Œä»¥å¼ºåˆ¶æ¨¡å‹åœ¨è§†è§‰å±‚é¢ä¸Šé‡æ–°å…³æ³¨è§†è§‰ä¿¡æ¯ï¼Œè§£å†³äº†çº¯æ–‡æœ¬åæ€çš„å±€é™æ€§ã€‚åœ¨å¤šä¸ªè§†è§‰é—®ç­”æ•°æ®é›†å’Œå¹»è§‰æŒ‡æ ‡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒQwen-LAåœ¨ä¿æŒé¢†å…ˆå‡†ç¡®æ€§çš„åŒæ—¶å‡å°‘äº†å¹»è§‰ã€‚æˆ‘ä»¬çš„ä»£ç ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/Liar406/Look_Again%E3%80%82">https://github.com/Liar406/Look_Againã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23558v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å¦‚ä½•é€šè¿‡å¼•å…¥è§†è§‰æ–‡æœ¬åå°„è¿‡ç¨‹æ¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é•¿æ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿçš„è§†è§‰ä¿¡æ¯ä¸¢å¤±å’Œå¹»è§‰é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„è§†è§‰è¯­è¨€æ¨ç†æ¨¡å‹ï¼ˆVLRMï¼‰â€”â€”Qwen-LookAgainï¼ˆQwen-LAï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥å¹³è¡¡åå°„ç­–ç•¥ä¼˜åŒ–ï¼ˆBRPOï¼‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»å†³å®šä½•æ—¶è¿›è¡Œè§†è§‰æ–‡æœ¬åå°„ï¼Œå¹¶å¹³è¡¡åå°„çš„æ¬¡æ•°å’Œé•¿åº¦ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†è§†è§‰ç¬¦å·å¤åˆ¶å’Œè§†è§‰ç¬¦å·è·¯ç”±æœºåˆ¶æ¥å¼ºåˆ¶æ¨¡å‹åœ¨è§†è§‰å±‚é¢é‡æ–°å…³æ³¨è§†è§‰ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQwen-LAåœ¨å¤šä¸ªè§†è§‰é—®ç­”æ•°æ®é›†ä¸Šå–å¾—äº†é¢†å…ˆçš„å‡†ç¡®æ€§è¡¨ç°ï¼Œå¹¶æœ‰æ•ˆå‡å°‘äº†å¹»è§‰çš„äº§ç”Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ—¶é—´ç¼©æ”¾å¯æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ€§èƒ½ï¼Œå½¢æˆå¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨ç†æ¨¡å‹ï¼ˆVLRMsï¼‰ã€‚</li>
<li>é•¿æ¨ç†è¿‡ç¨‹ä¼šå¯¼è‡´è§†è§‰ä»¤ç‰Œç¨€é‡Šï¼Œä½¿è§†è§‰ä¿¡æ¯è·å¾—è¾ƒå°‘çš„å…³æ³¨ï¼Œå¹¶å¯èƒ½å¼•å‘å¹»è§‰ã€‚</li>
<li>ä»…å¼•å…¥æ–‡æœ¬åå°„è¿‡ç¨‹åœ¨æŠ‘åˆ¶è¯­è¨€æ¨¡å‹çš„å¹»è§‰æ–¹é¢æ•ˆæœæœ‰é™ï¼Œä¸è¶³ä»¥è§£å†³VLMsä¸­çš„å¹»è§‰é—®é¢˜ã€‚</li>
<li>Qwen-LookAgainï¼ˆQwen-LAï¼‰æ˜¯ä¸€ç§æ–°å‹çš„VLRMï¼Œé€šè¿‡å¼•å…¥è§†è§‰æ–‡æœ¬åå°„è¿‡ç¨‹æ¥å‡è½»å¹»è§‰ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å¯¼æ¨¡å‹é‡æ–°å…³æ³¨è§†è§‰ä¿¡æ¯ã€‚</li>
<li>Qwen-LAä½¿ç”¨å¹³è¡¡åå°„ç­–ç•¥ä¼˜åŒ–ï¼ˆBRPOï¼‰æ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»å†³å®šä½•æ—¶è¿›è¡Œè§†è§‰æ–‡æœ¬åå°„ï¼Œå¹¶å¹³è¡¡åå°„æ¬¡æ•°å’Œé•¿åº¦ã€‚</li>
<li>åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†è§†è§‰ç¬¦å·å¤åˆ¶å’Œè§†è§‰ç¬¦å·è·¯ç”±æœºåˆ¶æ¥å¼ºåˆ¶æ¨¡å‹é‡æ–°å…³æ³¨è§†è§‰ä¿¡æ¯ï¼Œè§£å†³äº†ä»…ä½¿ç”¨æ–‡æœ¬åå°„çš„å±€é™æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒQwen-LAåœ¨å¤šä¸ªè§†è§‰é—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡é«˜ä¸”èƒ½æœ‰æ•ˆå‡å°‘å¹»è§‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b6c790ee9294b4d44b0417983724b4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c738e1b1af5d75fb148d4cf266c4e7d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-91789ea98e732662bf88fea2cc4026ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f0489204046aed9fd9fb9b86bed0d83.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Probability-Consistent-Preference-Optimization-for-Enhanced-LLM-Reasoning"><a href="#Probability-Consistent-Preference-Optimization-for-Enhanced-LLM-Reasoning" class="headerlink" title="Probability-Consistent Preference Optimization for Enhanced LLM   Reasoning"></a>Probability-Consistent Preference Optimization for Enhanced LLM   Reasoning</h2><p><strong>Authors:Yunqiao Yang, Houxing Ren, Zimu Lu, Ke Wang, Weikang Shi, Aojun Zhou, Junting Pan, Mingjie Zhan, Hongsheng Li</strong></p>
<p>Recent advances in preference optimization have demonstrated significant potential for improving mathematical reasoning capabilities in large language models (LLMs). While current approaches leverage high-quality pairwise preference data through outcome-based criteria like answer correctness or consistency, they fundamentally neglect the internal logical coherence of responses. To overcome this, we propose Probability-Consistent Preference Optimization (PCPO), a novel framework that establishes dual quantitative metrics for preference selection: (1) surface-level answer correctness and (2) intrinsic token-level probability consistency across responses. Extensive experiments show that our PCPO consistently outperforms existing outcome-only criterion approaches across a diverse range of LLMs and benchmarks. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/YunqiaoYang/PCPO">https://github.com/YunqiaoYang/PCPO</a>. </p>
<blockquote>
<p>è¿‘æœŸåå¥½ä¼˜åŒ–æ–¹é¢çš„è¿›å±•æ˜¾ç¤ºå‡ºåœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚è™½ç„¶å½“å‰çš„æ–¹æ³•é€šè¿‡ç­”æ¡ˆçš„æ­£ç¡®æ€§æˆ–ä¸€è‡´æ€§ç­‰ç»“æœæ ‡å‡†æ¥åˆ©ç”¨é«˜è´¨é‡çš„æˆå¯¹åå¥½æ•°æ®ï¼Œä½†å®ƒä»¬ä»æ ¹æœ¬ä¸Šå¿½è§†äº†å“åº”çš„å†…éƒ¨é€»è¾‘è¿è´¯æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¦‚ç‡ä¸€è‡´åå¥½ä¼˜åŒ–ï¼ˆPCPOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºåå¥½é€‰æ‹©å»ºç«‹åŒé‡å®šé‡æŒ‡æ ‡çš„æ–°æ¡†æ¶ï¼ŒåŒ…æ‹¬ï¼š(1)è¡¨é¢çº§çš„ç­”æ¡ˆæ­£ç¡®æ€§å’Œ(2)å“åº”é—´å†…åœ¨æ ‡è®°çº§æ¦‚ç‡çš„ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„PCPOåœ¨å¤šç§LLMå’ŒåŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆä¼˜äºä»…ä½¿ç”¨ç»“æœæ ‡å‡†çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/YunqiaoYang/PCPO%E3%80%82">https://github.com/YunqiaoYang/PCPOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23540v1">PDF</a> 14 pages, to be published in ACL 2025 findings</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸåå¥½ä¼˜åŒ–æŠ€æœ¯çš„è¿›å±•åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•é€šè¿‡ç­”æ¡ˆçš„æ­£ç¡®æ€§æˆ–ä¸€è‡´æ€§ç­‰ç»“æœå¯¼å‘çš„å‡†åˆ™æ¥åˆ©ç”¨é«˜è´¨é‡çš„é…å¯¹åå¥½æ•°æ®ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†å“åº”çš„å†…åœ¨é€»è¾‘è¿è´¯æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºæ¦‚ç‡ä¸€è‡´åå¥½ä¼˜åŒ–ï¼ˆPCPOï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œå»ºç«‹åŒé‡å®šé‡æŒ‡æ ‡è¿›è¡Œåå¥½é€‰æ‹©ï¼šï¼ˆ1ï¼‰è¡¨é¢å±‚æ¬¡çš„ç­”æ¡ˆæ­£ç¡®æ€§å’Œï¼ˆ2ï¼‰å“åº”ä¹‹é—´å†…åœ¨æ ‡è®°å±‚çº§çš„æ¦‚ç‡ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPCPOåœ¨å¤šç§LLMå’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡ä¼˜äºä»…ä¾èµ–ç»“æœå¯¼å‘æ ‡å‡†çš„ç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/YunqiaoYang/PCPO%E3%80%82">https://github.com/YunqiaoYang/PCPOã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸåå¥½ä¼˜åŒ–æŠ€æœ¯åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç»“æœå¯¼å‘çš„å‡†åˆ™ï¼Œå¦‚ç­”æ¡ˆçš„æ­£ç¡®æ€§æˆ–ä¸€è‡´æ€§ï¼Œä½†å¿½ç•¥äº†å“åº”çš„å†…åœ¨é€»è¾‘è¿è´¯æ€§ã€‚</li>
<li>PCPOæ¡†æ¶é€šè¿‡å¼•å…¥åŒé‡å®šé‡æŒ‡æ ‡è¿›è¡Œåå¥½é€‰æ‹©ï¼ŒåŒ…æ‹¬è¡¨é¢å±‚æ¬¡çš„ç­”æ¡ˆæ­£ç¡®æ€§å’Œå“åº”é—´å†…åœ¨æ ‡è®°å±‚çº§çš„æ¦‚ç‡ä¸€è‡´æ€§ã€‚</li>
<li>PCPOåœ¨å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡ä¼˜äºä»…ä¾èµ–ç»“æœå¯¼å‘æ ‡å‡†çš„ç°æœ‰æ–¹æ³•ã€‚</li>
<li>PCPOæ¡†æ¶æœ‰åŠ©äºæ›´å…¨é¢åœ°è¯„ä¼°è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ¨ç†èƒ½åŠ›æ–¹é¢ã€‚</li>
<li>å…¬å¼€å¯ç”¨çš„PCPOä»£ç ä¸ºå…¶ä»–ç ”ç©¶è€…æä¾›äº†ä½¿ç”¨å’Œè¿›ä¸€æ­¥å¼€å‘çš„ä¾¿åˆ©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-965b1300cb38bd2f574b8f80e5d367ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1efa13dde66d0722bf1626bb9d712a8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7c3df8813ae59f96b863197465ada5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f8e976026d85e91416523caaa87df92.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="VAU-R1-Advancing-Video-Anomaly-Understanding-via-Reinforcement-Fine-Tuning"><a href="#VAU-R1-Advancing-Video-Anomaly-Understanding-via-Reinforcement-Fine-Tuning" class="headerlink" title="VAU-R1: Advancing Video Anomaly Understanding via Reinforcement   Fine-Tuning"></a>VAU-R1: Advancing Video Anomaly Understanding via Reinforcement   Fine-Tuning</h2><p><strong>Authors:Liyun Zhu, Qixiang Chen, Xi Shen, Xiaodong Cun</strong></p>
<p>Video Anomaly Understanding (VAU) is essential for applications such as smart cities, security surveillance, and disaster alert systems, yet remains challenging due to its demand for fine-grained spatio-temporal perception and robust reasoning under ambiguity. Despite advances in anomaly detection, existing methods often lack interpretability and struggle to capture the causal and contextual aspects of abnormal events. This limitation is further compounded by the absence of comprehensive benchmarks for evaluating reasoning ability in anomaly scenarios. To address both challenges, we introduce VAU-R1, a data-efficient framework built upon Multimodal Large Language Models (MLLMs), which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT). Besides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored for video anomaly reasoning, featuring multiple-choice QA, detailed rationales, temporal annotations, and descriptive captions. Empirical results show that VAU-R1 significantly improves question answering accuracy, temporal grounding, and reasoning coherence across diverse contexts. Together, our method and benchmark establish a strong foundation for interpretable and reasoning-aware video anomaly understanding. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/GVCLab/VAU-R1">https://github.com/GVCLab/VAU-R1</a>. </p>
<blockquote>
<p>è§†é¢‘å¼‚å¸¸ç†è§£ï¼ˆVAUï¼‰åœ¨æ™ºæ…§åŸå¸‚ã€å®‰å…¨ç›‘æ§å’Œç¾éš¾é¢„è­¦ç³»ç»Ÿç­‰é¢†åŸŸæœ‰ç€è‡³å…³é‡è¦çš„åº”ç”¨ï¼Œä½†ç”±äºå…¶éœ€è¦ç²¾ç»†çš„æ—¶ç©ºæ„ŸçŸ¥å’Œåœ¨æ¨¡ç³Šæƒ…å†µä¸‹çš„ç¨³å¥æ¨ç†ï¼Œä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚å°½ç®¡å¼‚å¸¸æ£€æµ‹å·²ç»å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹è§£é‡Šæ€§ï¼Œä¸”éš¾ä»¥æ•æ‰å¼‚å¸¸äº‹ä»¶çš„å› æœå’Œä¸Šä¸‹æ–‡æ–¹é¢ã€‚ç”±äºç¼ºä¹ç”¨äºè¯„ä¼°å¼‚å¸¸åœºæ™¯ä¸­æ¨ç†èƒ½åŠ›çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œè¿™ä¸€å±€é™æ€§è¿›ä¸€æ­¥åŠ å‰§ã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VAU-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é«˜æ•ˆæ•°æ®æ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰å¢å¼ºå¼‚å¸¸æ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†VAU-Benchï¼Œè¿™æ˜¯ä¸“ä¸ºè§†é¢‘å¼‚å¸¸æ¨ç†è®¾è®¡çš„é¦–ä¸ªâ€œæ€ç»´é“¾â€åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å¤šé¡¹é€‰æ‹©é¢˜ã€è¯¦ç»†ç†ç”±ã€æ—¶é—´æ³¨é‡Šå’Œæè¿°æ€§å­—å¹•ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼ŒVAU-R1åœ¨å¤šç§ä¸Šä¸‹æ–‡ä¸­æ˜¾è‘—æé«˜äº†é—®ç­”å‡†ç¡®æ€§ã€æ—¶é—´å®šä½å’Œæ¨ç†è¿è´¯æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•å…±åŒä¸ºå¯è§£é‡Šæ€§å’Œæ¨ç†æ„è¯†çš„è§†é¢‘å¼‚å¸¸ç†è§£å¥ å®šäº†åšå®åŸºç¡€ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/GVCLab/VAU-R1%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/GVCLab/VAU-R1æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23504v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨è§†é¢‘å¼‚å¸¸ç†è§£ï¼ˆVAUï¼‰é¢†åŸŸä¸­ï¼Œæ™ºèƒ½åŸå¸‚ã€å®‰å…¨ç›‘æ§å’Œç¾å®³é¢„è­¦ç³»ç»Ÿç­‰åº”ç”¨ä¸­ï¼Œç²¾ç»†æ—¶ç©ºæ„ŸçŸ¥å’Œæ¨¡ç³ŠèƒŒæ™¯ä¸‹çš„æ¨ç†éœ€æ±‚å¸¦æ¥äº†è¯¸å¤šæŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œéš¾ä»¥æ•æ‰å¼‚å¸¸äº‹ä»¶çš„å› æœå’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„VAU-R1æ¡†æ¶ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰å¢å¼ºäº†å¼‚å¸¸æ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸“é—¨é’ˆå¯¹è§†é¢‘å¼‚å¸¸æ¨ç†çš„VAU-BenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å¤šé¡¹é€‰æ‹©é¢˜ã€è¯¦ç»†ç†ç”±ã€æ—¶é—´æ³¨é‡Šå’Œæè¿°å­—å¹•ç­‰ç‰¹ç‚¹ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒVAU-R1åœ¨å¤šç§ç¯å¢ƒä¸‹çš„é—®ç­”å‡†ç¡®æ€§ã€æ—¶é—´å®šä½å’Œæ¨ç†è¿è´¯æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•ä¸ºå¯è§£é‡Šæ€§å’Œæ¨ç†æ„è¯†çš„è§†é¢‘å¼‚å¸¸ç†è§£å¥ å®šäº†åšå®åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘å¼‚å¸¸ç†è§£ï¼ˆVAUï¼‰åœ¨æ™ºèƒ½åŸå¸‚ã€å®‰å…¨ç›‘æ§å’Œç¾å®³é¢„è­¦ç³»ç»Ÿä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œä½†å­˜åœ¨ç²¾ç»†æ—¶ç©ºæ„ŸçŸ¥å’Œæ¨¡ç³ŠèƒŒæ™¯ä¸‹çš„æ¨ç†éœ€æ±‚ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œéš¾ä»¥æ•æ‰å¼‚å¸¸äº‹ä»¶çš„å› æœå’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥çš„VAU-R1æ¡†æ¶åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œé€šè¿‡å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰å¢å¼ºå¼‚å¸¸æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†VAU-BenchåŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºè§†é¢‘å¼‚å¸¸æ¨ç†ï¼ŒåŒ…æ‹¬å¤šé¡¹é€‰æ‹©é¢˜ã€è¯¦ç»†ç†ç”±ã€æ—¶é—´æ³¨é‡Šå’Œæè¿°å­—å¹•ç­‰ç‰¹ç‚¹ã€‚</li>
<li>VAU-R1åœ¨é—®ç­”å‡†ç¡®æ€§ã€æ—¶é—´å®šä½å’Œæ¨ç†è¿è´¯æ€§æ–¹é¢æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>æˆ‘ä»¬çš„æ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•ä¸ºè§†é¢‘å¼‚å¸¸ç†è§£çš„å¯è§£é‡Šæ€§å’Œæ¨ç†æ„è¯†å¥ å®šäº†åšå®åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a84bd388f25041eed070e35dad07ea5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51e8de699a716ad683e5815df3d3642e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f24a93824be48a85596800277b435e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97244b90fc2274a7560dae2483fe7b4b.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="R2I-Bench-Benchmarking-Reasoning-Driven-Text-to-Image-Generation"><a href="#R2I-Bench-Benchmarking-Reasoning-Driven-Text-to-Image-Generation" class="headerlink" title="R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation"></a>R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation</h2><p><strong>Authors:Kaijie Chen, Zihao Lin, Zhiyang Xu, Ying Shen, Yuguang Yao, Joy Rimchala, Jiaxin Zhang, Lifu Huang</strong></p>
<p>Reasoning is a fundamental capability often required in real-world text-to-image (T2I) generation, e.g., generating <code>a bitten apple that has been left in the air for more than a week</code> necessitates understanding temporal decay and commonsense concepts. While recent T2I models have made impressive progress in producing photorealistic images, their reasoning capability remains underdeveloped and insufficiently evaluated. To bridge this gap, we introduce R2I-Bench, a comprehensive benchmark specifically designed to rigorously assess reasoning-driven T2I generation. R2I-Bench comprises meticulously curated data instances, spanning core reasoning categories, including commonsense, mathematical, logical, compositional, numerical, causal, and concept mixing. To facilitate fine-grained evaluation, we design R2IScore, a QA-style metric based on instance-specific, reasoning-oriented evaluation questions that assess three critical dimensions: text-image alignment, reasoning accuracy, and image quality. Extensive experiments with 16 representative T2I models, including a strong pipeline-based framework that decouples reasoning and generation using the state-of-the-art language and image generation models, demonstrate consistently limited reasoning performance, highlighting the need for more robust, reasoning-aware architectures in the next generation of T2I systems. Project Page: <a target="_blank" rel="noopener" href="https://r2i-bench.github.io/">https://r2i-bench.github.io</a> </p>
<blockquote>
<p>æ¨ç†æ˜¯ç°å®ä¸–ç•Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆä¸­ç»å¸¸éœ€è¦çš„ä¸€é¡¹åŸºæœ¬èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œç”Ÿæˆâ€œä¸€ä¸ªè¢«ç•™åœ¨ç©ºæ°”ä¸­è¶…è¿‡ä¸€å‘¨çš„è‹¹æœè¢«å’¬äº†ä¸€å£â€çš„å›¾åƒï¼Œéœ€è¦ç†è§£æ—¶é—´çš„æµé€å’Œå¸¸è¯†æ¦‚å¿µã€‚è™½ç„¶æœ€è¿‘çš„T2Iæ¨¡å‹åœ¨ç”Ÿæˆè¶…é€¼çœŸçš„å›¾åƒæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ï¼Œä½†å…¶æ¨ç†èƒ½åŠ›ä»ç„¶å‘å±•ä¸è¶³ä¸”è¯„ä¼°ä¸è¶³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†R2I-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œç”¨äºä¸¥æ ¼è¯„ä¼°ä»¥æ¨ç†é©±åŠ¨çš„T2Iç”Ÿæˆã€‚R2I-BenchåŒ…å«ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®å®ä¾‹ï¼Œæ¶µç›–æ ¸å¿ƒæ¨ç†ç±»åˆ«ï¼ŒåŒ…æ‹¬å¸¸è¯†ã€æ•°å­¦ã€é€»è¾‘ã€ç»„åˆã€æ•°å€¼ã€å› æœå’Œæ¦‚å¿µæ··åˆã€‚ä¸ºäº†è¿›è¡Œç²¾ç»†çš„è¯„ä¼°ï¼Œæˆ‘ä»¬è®¾è®¡äº†R2IScoreï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºé—®ç­”é£æ ¼çš„æŒ‡æ ‡ï¼Œå®ƒåŸºäºç‰¹å®šå®ä¾‹çš„ã€ä»¥æ¨ç†ä¸ºå¯¼å‘çš„è¯„ä¼°é—®é¢˜ï¼Œè¯„ä¼°ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šæ–‡æœ¬ä¸å›¾åƒçš„å¯¹é½ã€æ¨ç†çš„å‡†ç¡®æ€§ä»¥åŠå›¾åƒè´¨é‡ã€‚ä¸16ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„T2Iæ¨¡å‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå¼ºå¤§çš„åŸºäºç®¡é“æ¡†æ¶çš„æ¨¡å‹åœ¨å†…ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨æœ€å…ˆè¿›çš„è¯­è¨€å’Œå›¾åƒç”Ÿæˆæ¨¡å‹æ¥è§£è€¦æ¨ç†å’Œç”Ÿæˆï¼Œå…¶æ¨ç†æ€§èƒ½ä¸€ç›´è¡¨ç°æœ‰é™ï¼Œè¿™çªæ˜¾äº†ä¸‹ä¸€ä»£T2Iç³»ç»Ÿéœ€è¦æ›´å¼ºå¤§ã€å…·æœ‰æ¨ç†æ„è¯†çš„æ¶æ„ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://r2i-bench.github.io./">https://r2i-bench.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23493v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://r2i-bench.github.io/">https://r2i-bench.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†T2Iç”Ÿæˆä¸­æ‰€éœ€çš„ä¸€ç§åŸºæœ¬èƒ½åŠ›â€”â€”æ¨ç†ã€‚è™½ç„¶æœ€è¿‘çš„T2Iæ¨¡å‹åœ¨ç”Ÿæˆè¶…ç°å®å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æ¨ç†èƒ½åŠ›ä»å­˜åœ¨ä¸è¶³ä¸”ç¼ºä¹è¯„ä¼°ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡å¼•å…¥äº†R2I-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºä¸¥æ ¼è¯„ä¼°æ¨ç†é©±åŠ¨çš„T2Iç”Ÿæˆçš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ã€‚R2I-BenchåŒ…æ‹¬ç²¾å¿ƒç­–åˆ’çš„æ•°æ®å®ä¾‹ï¼Œæ¶µç›–æ ¸å¿ƒæ¨ç†ç±»åˆ«ï¼Œå¦‚å¸¸è¯†ã€æ•°å­¦ã€é€»è¾‘ã€ç»„åˆã€æ•°å€¼ã€å› æœå’Œæ¦‚å¿µæ··åˆã€‚ä¸ºäº†è¿›è¡Œç²¾ç»†è¯„ä¼°ï¼Œæœ¬æ–‡è¿˜è®¾è®¡äº†R2IScoreï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºé—®ç­”é£æ ¼çš„æŒ‡æ ‡ï¼Œé€šè¿‡ç‰¹å®šå®ä¾‹çš„æ¨ç†è¯„ä¼°é—®é¢˜æ¥è¯„ä¼°ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šæ–‡æœ¬ä¸å›¾åƒçš„å¯¹é½ã€æ¨ç†çš„å‡†ç¡®æ€§ä»¥åŠå›¾åƒè´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰T2Iæ¨¡å‹çš„æ¨ç†æ€§èƒ½å­˜åœ¨æ™®éé™åˆ¶ï¼Œè¿™çªæ˜¾äº†ä¸‹ä¸€ä»£T2Iç³»ç»Ÿéœ€è¦æ›´å¼ºå¤§ã€æ³¨é‡æ¨ç†çš„æ¶æ„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†èƒ½åŠ›æ˜¯çœŸå®ä¸–ç•Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ˆT2Iï¼‰ä¸­çš„åŸºæœ¬éœ€æ±‚ã€‚</li>
<li>æœ€è¿‘çš„T2Iæ¨¡å‹è™½ç„¶èƒ½ç”Ÿæˆè¶…ç°å®å›¾åƒï¼Œä½†æ¨ç†èƒ½åŠ›ä»æœ‰ä¸è¶³ã€‚</li>
<li>R2I-Benchæ˜¯ä¸€ä¸ªæ–°æ¨å‡ºçš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ä¸¥æ ¼è¯„ä¼°æ¨ç†é©±åŠ¨çš„T2Iç”Ÿæˆã€‚</li>
<li>R2I-BenchåŒ…å«æ¶µç›–å¤šç§æ ¸å¿ƒæ¨ç†ç±»åˆ«çš„æ•°æ®å®ä¾‹ã€‚</li>
<li>è®¾è®¡äº†R2IScoreè¿™ä¸€åŸºäºé—®ç­”é£æ ¼çš„æŒ‡æ ‡ï¼Œç”¨äºç²¾ç»†è¯„ä¼°T2Iæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰T2Iæ¨¡å‹åœ¨æ¨ç†æ€§èƒ½æ–¹é¢å­˜åœ¨æ™®éé™åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6d4d24f74cfb181fb49abc6c2110850.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4c28f8af41a8ae5d0970a2330573581.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dec7d218892b5122050107322c6b47e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93a03d141088472463a4657e6fe874ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca88409b603e49041086076d39db251f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="UniRL-Self-Improving-Unified-Multimodal-Models-via-Supervised-and-Reinforcement-Learning"><a href="#UniRL-Self-Improving-Unified-Multimodal-Models-via-Supervised-and-Reinforcement-Learning" class="headerlink" title="UniRL: Self-Improving Unified Multimodal Models via Supervised and   Reinforcement Learning"></a>UniRL: Self-Improving Unified Multimodal Models via Supervised and   Reinforcement Learning</h2><p><strong>Authors:Weijia Mao, Zhenheng Yang, Mike Zheng Shou</strong></p>
<p>Unified multimodal large language models such as Show-o and Janus have achieved strong performance across both generation and understanding tasks. However, these models typically rely on large-scale datasets and require substantial computation during the pretraining stage. In addition, several post-training methods have been proposed, but they often depend on external data or are limited to task-specific customization. In this work, we introduce UniRL, a self-improving post-training approach. Our approach enables the model to generate images from prompts and use them as training data in each iteration, without relying on any external image data. Moreover, it enables the two tasks to enhance each other: the generated images are used for understanding, and the understanding results are used to supervise generation. We explore supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to optimize the models. UniRL offers three key advantages: (1) it requires no external image data, as all training samples are generated by the model itself during training; (2) it not only improves individual task performance, but also reduces the imbalance between generation and understanding; and (3) it requires only several additional training steps during the post-training stage. We evaluate UniRL on top of Show-o and Janus, achieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and models will be released in <a target="_blank" rel="noopener" href="https://github.com/showlab/UniRL">https://github.com/showlab/UniRL</a>. </p>
<blockquote>
<p>ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚Show-oå’ŒJanusï¼Œåœ¨ç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¸­éƒ½å–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶åœ¨é¢„è®­ç»ƒé˜¶æ®µéœ€è¦å¤§é‡çš„è®¡ç®—ã€‚æ­¤å¤–ï¼Œå·²ç»æå‡ºäº†å‡ ç§åè®­ç»ƒæ–¹æ³•ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºå¤–éƒ¨æ•°æ®æˆ–ä»…é™äºç‰¹å®šä»»åŠ¡çš„å®šåˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniRLï¼Œä¸€ç§è‡ªæˆ‘æ”¹è¿›çš„åè®­ç»ƒæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æç¤ºç”Ÿæˆå›¾åƒï¼Œå¹¶å°†å…¶ç”¨ä½œæ¯æ¬¡è¿­ä»£çš„è®­ç»ƒæ•°æ®ï¼Œè€Œæ— éœ€ä¾èµ–ä»»ä½•å¤–éƒ¨å›¾åƒæ•°æ®ã€‚æ­¤å¤–ï¼Œå®ƒä½¿è¿™ä¸¤ä¸ªä»»åŠ¡èƒ½å¤Ÿç›¸äº’å¢å¼ºï¼šç”Ÿæˆçš„å›¾åƒç”¨äºç†è§£ï¼Œç†è§£çš„ç»“æœç”¨äºç›‘ç£ç”Ÿæˆã€‚æˆ‘ä»¬æ¢ç´¢äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥ä¼˜åŒ–æ¨¡å‹ã€‚UniRLæä¾›äº†ä¸‰ä¸ªå…³é”®ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰å®ƒä¸éœ€è¦å¤–éƒ¨å›¾åƒæ•°æ®ï¼Œå› ä¸ºæ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬éƒ½æ˜¯ç”±æ¨¡å‹æœ¬èº«åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„ï¼›ï¼ˆ2ï¼‰å®ƒä¸ä»…æé«˜äº†å•ä¸ªä»»åŠ¡æ€§èƒ½ï¼Œè€Œä¸”å‡å°‘äº†ç”Ÿæˆå’Œç†è§£ä¹‹é—´çš„ä¸å¹³è¡¡ï¼›ï¼ˆ3ï¼‰å®ƒä»…åœ¨åè®­ç»ƒé˜¶æ®µéœ€è¦é¢å¤–çš„å‡ ä¸ªè®­ç»ƒæ­¥éª¤ã€‚æˆ‘ä»¬åœ¨Show-oå’ŒJanusçš„é¡¶éƒ¨è¯„ä¼°äº†UniRLï¼ŒShow-oçš„GenEvalå¾—åˆ†ä¸º0.77ï¼ŒJanusçš„GenEvalå¾—åˆ†ä¸º0.65ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/showlab/UniRL%E5%B8%AE%E5%B9%BF%E5%AE%9E%E9%AA%8C%E7%9A%84%E5%BC%8F%E5%AE%BE%E6%BB%9D/%E7%BB%9C%E4%B8%AD+%E9%A6%AC%E9%A3%BE+%E5%BC%BA+%E5%A4%A7+%E7%AF%AB%E8+%BFA+%EB%B6%+BF+%E7%+BB%+BD+%E5%+AE%+9A+%E5%+AF%+BC+%E4%+BF%+AEADathwillnotesheroutinguntiltrainingbeginstextformatusiayina...%E5%B9%B4%E4%BB%A5%E4%B8%8A%E5%B7%A5%E4%BD%9C%E7%BB%8F%E9%AA%8C%EF%BC%89+%E5%AF%BB%E6%89%BE%E6%96%B0%E5%8A%9F%E8%83%BD%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BA%A4%E5%8F%89%E4%B8%8E%E8%BE%85%E5%8A%A9%E5%9B%A2%E9%98%9F%E5%90%88%E4%BD%9C%E5%8F%91%E7%8E%B0%E5%90%88%E4%BD%9C%E6%9C%BA%E5%88%B6%E6%9C%80%E5%90%8E%E4%BB%96%E8%BF%98%E9%81%87%E5%88%B0%E4%BA%86%E9%BA%BB%E7%83%A6%E7%9A%84%E5%BA%97%E4%B8%BB%E5%8E%BB%E8%A7%A3%E5%86%B3%E5%90%84%E7%A7%8D%E5%8E%9F%E5%9B%A0%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%96%E4%BB%AC%E5%BA%94%E8%AF%A5%E6%B3%A8%E6%84%8F%E7%9A%84%E4%B8%BB%E8%A6%81%E6%80%9D%E8%B7%AF%E8%BF%99%E4%B8%80%E5%B7%A5%E4%BD%9C%E6%96%B9%E5%BC%8F%E8%BF%98%E5%8C%85%E6%8B%AC%E4%BA%86%E6%96%B0%E7%9A%84%E5%91%BD%E4%BB%A4%E8%A1%A8%E8%BF%B0%E4%BB%A5%E6%8F%90%E5%8D%87%E8%AE%A1%E7%AE%97%E6%95%88%E8%83%BD%E9%87%87%E7%94%A8%E4%BA%86%E5%8F%8C%E5%90%91%E7%BB%84%E5%90%88%E7%9A%84%E5%BD%A2%E5%BC%8F%E4%BE%8B%E5%A6%82%E5%8F%AF%E8%A7%86%E5%8C%96%E7%95%8C%E9%9D%A2%E7%9A%84%E8%AE%A1%E7%AE%97%E7%A8%8B%E5%BA%8F%E5%8C%85%E8%BE%93%E5%85%A5%E5%8F%AF%E5%85%BC%E5%AE%B9%E7%9A%84%E5%90%84%E7%A7%8D%E5%BD%A2%E5%BC%8F%E7%9A%84%E7%AC%A6%E5%8F%B7%E8%BE%93%E5%87%BA%E5%B8%A6%E6%9C%89%E8%AF%AD%E8%A8%80%E7%BB%93%E6%9E%84%E7%9A%84%E8%BE%85%E5%8A%A9%E6%8C%87%E5%AF%BC%E4%BD%BF%E7%94%A8%E7%BB%9F%E8%AE%A1%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B%E7%94%9F%E6%88%90%E5%8F%AF%E8%83%BD%E7%9A%84%E7%AD%94%E6%A1%88%E6%88%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%AD%89%E7%AD%89%E6%AD%A4%E5%A4%96%E6%A8%A1%E5%9E%8B%E8%BF%98%E8%83%BD%E5%A4%9F%E8%BF%9B%E8%A1%8C%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3%E5%92%8C%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E4%BB%8E%E8%80%8C%E6%8F%90%E4%BE%9B%E6%9B%B4%E5%85%B7%E4%BA%BA%E6%80%A7%E5%8C%96%E7%9A%84%E6%9C%8D%E5%8A%A1%E4%BB%A5%E6%94%AF%E6%8C%81%E5%AE%A2%E6%88%B7%E6%9C%8D%E5%8A%A1%E5%92%8C%E4%B8%9A%E5%8A%A1%E8%BF%90%E8%90%A5%E5%9C%A8%E4%B8%9A%E5%8A%A1%E8%BF%90%E8%90%A5%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BF%9D%E6%8C%81%E5%B7%A5%E4%BD%9C%E6%88%90%E6%9E%9C%E7%9A%84%E9%AB%98%E8%B4%A8%E9%87%8F%E5%B9%B6%E4%B8%94%E5%A7%8B%E7%BB%88%E4%BF%9D%E6%8C%81%E5%9C%A8%E8%A1%8C%E4%B8%9A%E4%B8%AD%E5%85%B7%E6%9C%89%E7%AB%9E%E4%BA%89%E5%8A%9B%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%88%90%E6%9E%9C%E8%BF%98%E9%9C%80%E8%A6%81%E9%80%9A%E8%BF%87%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E7%BB%8F%E9%AA%8C%E8%BF%9B%E8%A1%8C%E6%8E%A2%E7%B4%A2%E5%92%8C%E6%80%BB%E7%BB%93%E6%AF%94%E5%A6%82%E5%AF%BB%E6%89%BE%E6%BD%9C%E5%9C%A8%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%88%E6%9D%A5%E6%8F%90%E5%8D%87%E5%AE%A2%E6%88%B7%E6%9C%8D%E5%8A%A1%E5%92%8C%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E7%9A%84%E8%B4%A8%E9%87%8F%E7%AD%89%E7%AD%89%E8%BF%99%E4%BA%9B%E9%83%BD%E9%9C%80%E8%A6%81%E9%80%9A%E8%BF%87%E4%B8%8D%E6%96%AD%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%AE%9E%E8%B7%B5%E6%9D%A5%E4%B8%8D%E6%96%AD%E6%8F%90%E5%8D%87%E8%87%AA%E5%B7%B1%E7%9A%84%E4%B8%93%E4%B8%9A%E8%83%BD%E5%8A%9B%E4%BB%A5%E9%80%82%E5%BA%94%E4%B8%8D%E6%96%AD%E5%8F%98%E5%8C%96%E7%9A%84%E5%B8%82%E5%9C%BA%E9%9C%80%E6%B1%82%E5%92%8C%E5%B8%82%E5%9C%BA%E7%8E%AF%E5%A2%83">https://github.com/showlab/UniRLå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23380v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†UniRLï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è‡ªæ”¹è¿›çš„åè®­ç»ƒç­–ç•¥ï¼Œç”¨äºç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å¦‚Show-oå’ŒJanusã€‚è¯¥ç­–ç•¥è®©æ¨¡å‹èƒ½å¤Ÿä»æç¤ºç”Ÿæˆå›¾åƒï¼Œå¹¶å°†å…¶ç”¨ä½œè®­ç»ƒæ•°æ®ï¼Œæ— éœ€ä¾èµ–ä»»ä½•å¤–éƒ¨å›¾åƒæ•°æ®ã€‚åŒæ—¶ï¼Œå®ƒè¿˜ä¿ƒè¿›äº†ç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¹‹é—´çš„ç›¸äº’ä¿ƒè¿›ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥ä¼˜åŒ–æ¨¡å‹ã€‚UniRLå…·æœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼šæ— éœ€å¤–éƒ¨å›¾åƒæ•°æ®ã€ä¸ä»…æé«˜å•ä¸€ä»»åŠ¡æ€§èƒ½è¿˜å‡å°‘ç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¹‹é—´çš„ä¸å¹³è¡¡ï¼Œä»¥åŠåœ¨åè®­ç»ƒé˜¶æ®µä»…éœ€è¦é¢å¤–çš„å‡ ä¸ªè®­ç»ƒæ­¥éª¤ã€‚å¯¹Show-oå’ŒJanusçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒUniRLå–å¾—äº†è‰¯å¥½çš„æˆç»©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniRLæ˜¯ä¸€ç§è‡ªæ”¹è¿›çš„åè®­ç»ƒç­–ç•¥ï¼Œé€‚ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>UniRLå…è®¸æ¨¡å‹ä»æç¤ºç”Ÿæˆå›¾åƒï¼Œå¹¶ç”¨ä½œè®­ç»ƒæ•°æ®ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨å›¾åƒæ•°æ®ã€‚</li>
<li>UniRLä¿ƒè¿›äº†ç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¹‹é—´çš„ç›¸äº’ä¿ƒè¿›ã€‚</li>
<li>UniRLé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¼˜åŒ–æ¨¡å‹ã€‚</li>
<li>UniRLçš„ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬ï¼šæ— éœ€å¤–éƒ¨æ•°æ®ã€æé«˜ä»»åŠ¡æ€§èƒ½ã€å‡å°‘ä»»åŠ¡é—´ä¸å¹³è¡¡ï¼Œä»¥åŠåè®­ç»ƒé˜¶æ®µçš„è®­ç»ƒæ­¥éª¤è¾ƒå°‘ã€‚</li>
<li>åœ¨Show-oå’ŒJanusæ¨¡å‹ä¸Šåº”ç”¨UniRLç­–ç•¥å–å¾—äº†è‰¯å¥½çš„è¯„ä¼°ç»“æœã€‚</li>
<li>UniRLçš„ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/showlab/UniRL%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/showlab/UniRLå‘å¸ƒã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23380">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-47530cda690c3a15577dae3bf3418bd5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-449386843ae97d5a170d519b0c9909ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d761fd910ef743b4f385231803e1169.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-291918f92cc4ee1f382e111f20df5a8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1907137e019697244c50417f01104123.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Threading-the-Needle-Reweaving-Chain-of-Thought-Reasoning-to-Explain-Human-Label-Variation"><a href="#Threading-the-Needle-Reweaving-Chain-of-Thought-Reasoning-to-Explain-Human-Label-Variation" class="headerlink" title="Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain   Human Label Variation"></a>Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain   Human Label Variation</h2><p><strong>Authors:Beiduo Chen, Yang Janet Liu, Anna Korhonen, Barbara Plank</strong></p>
<p>The recent rise of reasoning-tuned Large Language Models (LLMs)â€“which generate chains of thought (CoTs) before giving the final answerâ€“has attracted significant attention and offers new opportunities for gaining insights into human label variation, which refers to plausible differences in how multiple annotators label the same data instance. Prior work has shown that LLM-generated explanations can help align model predictions with human label distributions, but typically adopt a reverse paradigm: producing explanations based on given answers. In contrast, CoTs provide a forward reasoning path that may implicitly embed rationales for each answer option, before generating the answers. We thus propose a novel LLM-based pipeline enriched with linguistically-grounded discourse segmenters to extract supporting and opposing statements for each answer option from CoTs with improved accuracy. We also propose a rank-based HLV evaluation framework that prioritizes the ranking of answers over exact scores, which instead favor direct comparison of label distributions. Our method outperforms a direct generation method as well as baselines on three datasets, and shows better alignment of ranking methods with humans, highlighting the effectiveness of our approach. </p>
<blockquote>
<p>æœ€è¿‘å‡ºç°çš„ç»è¿‡æ¨ç†è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç»™å‡ºæœ€ç»ˆç­”æ¡ˆä¹‹å‰ä¼šç”Ÿæˆä¸€ç³»åˆ—æ€ç»´é“¾ï¼ˆCoTsï¼‰ï¼Œè¿™å¼•èµ·äº†äººä»¬çš„æå¤§å…³æ³¨ï¼Œå¹¶ä¸ºæ´å¯Ÿäººç±»æ ‡ç­¾å˜åŒ–æä¾›äº†æ–°çš„æœºä¼šã€‚äººç±»æ ‡ç­¾å˜åŒ–æ˜¯æŒ‡å¤šä¸ªæ³¨é‡Šè€…å¯¹åŒä¸€æ•°æ®å®ä¾‹è¿›è¡Œæ ‡æ³¨æ—¶å¯èƒ½å­˜åœ¨çš„åˆç†å·®å¼‚ã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMç”Ÿæˆçš„è§£é‡Šæœ‰åŠ©äºä½¿æ¨¡å‹é¢„æµ‹ä¸äººç±»æ ‡ç­¾åˆ†å¸ƒå¯¹é½ï¼Œä½†é€šå¸¸é‡‡ç”¨é€†å‘æ¨¡å¼ï¼šåŸºäºç»™å®šç­”æ¡ˆç”Ÿæˆè§£é‡Šã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒCoTsæä¾›äº†ä¸€ç§æ­£å‘æ¨ç†è·¯å¾„ï¼Œåœ¨ç”Ÿæˆç­”æ¡ˆä¹‹å‰å¯èƒ½éšå«åœ°åµŒå…¥æ¯ä¸ªç­”æ¡ˆé€‰é¡¹çš„åˆç†æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºLLMçš„æ–°å‹ç®¡é“ï¼Œè¯¥ç®¡é“é…å¤‡äº†è¯­è¨€åŸºç¡€çš„å¯¹è¯åˆ†æ®µå™¨ï¼Œä»¥æ›´å‡†ç¡®åœ°ä»CoTsä¸­æå–æ¯ä¸ªç­”æ¡ˆé€‰é¡¹çš„æ”¯æŒå’Œåå¯¹é™ˆè¿°ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†åŸºäºæ’åçš„HLVè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¼˜å…ˆäºç­”æ¡ˆçš„æ’åè€Œéç¡®åˆ‡åˆ†æ•°ï¼Œä»è€Œæœ‰åˆ©äºæ ‡ç­¾åˆ†å¸ƒçš„ç›´æ¥æ¯”è¾ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç›´æ¥ç”Ÿæˆæ–¹æ³•å’ŒåŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”æ˜¾ç¤ºçš„æ’åæ–¹æ³•ä¸äººç±»æ›´åŠ å¯¹é½ï¼Œè¿™å‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23368v1">PDF</a> 22 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç”Ÿæˆæ€è€ƒé“¾ï¼ˆCoTsï¼‰å†ç»™å‡ºæœ€ç»ˆç­”æ¡ˆçš„æ¨ç†è°ƒæ•´æ–¹å¼ï¼Œè¿‘æœŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œå¹¶ä¸ºç†è§£äººç±»æ ‡ç­¾å˜åŒ–æä¾›äº†æ–°çš„æœºä¼šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºLLMçš„ç®¡é“ï¼Œé€šè¿‡èå…¥è¯­è¨€åŸºç¡€çš„å¯¹è¯åˆ†æ®µå™¨ï¼Œä»¥æ”¹å–„ä»æ€è€ƒé“¾ä¸­æå–å¯¹ç­”æ¡ˆé€‰é¡¹çš„æ”¯æŒå’Œåå¯¹é™ˆè¿°çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†åŸºäºæ’åçš„HLVè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¼˜å…ˆå¯¹ç­”æ¡ˆè¿›è¡Œæ’åï¼Œè€Œéä»…å…³æ³¨ç²¾ç¡®åˆ†æ•°ï¼Œä»è€Œæ›´ç›´æ¥åœ°æ¯”è¾ƒæ ‡ç­¾åˆ†å¸ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç›´æ¥ç”Ÿæˆæ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶æ˜¾ç¤ºå‡ºä¸äººç±»æ’åæ–¹æ³•çš„æ›´å¥½å¯¹é½ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç”Ÿæˆæ€è€ƒé“¾ï¼ˆCoTsï¼‰ç»™å‡ºç­”æ¡ˆï¼Œè¿™æœ‰åŠ©äºç†è§£äººç±»æ ‡ç­¾å˜åŒ–ã€‚</li>
<li>LLMç”Ÿæˆçš„è§£é‡Šå¯ä»¥å¸®åŠ©æ¨¡å‹é¢„æµ‹ä¸äººç±»æ ‡ç­¾åˆ†å¸ƒå¯¹é½ã€‚</li>
<li>ä¸åŸºäºç»™å®šç­”æ¡ˆäº§ç”Ÿè§£é‡Šçš„åå‘æ¨¡å¼ä¸åŒï¼ŒCoTsæä¾›å‰å‘æ¨ç†è·¯å¾„ï¼Œå¯èƒ½éšå«æ¯ä¸ªç­”æ¡ˆé€‰é¡¹çš„ç†ç”±ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºLLMçš„ç®¡é“ï¼Œé€šè¿‡è¯­è¨€åŸºç¡€çš„å¯¹è¯åˆ†æ®µå™¨æ”¹å–„ä»CoTsä¸­æå–æ”¯æŒæˆ–åå¯¹é™ˆè¿°çš„å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºäº†åŸºäºæ’åçš„HLVè¯„ä¼°æ¡†æ¶ï¼Œä¼˜å…ˆå¯¹ç­”æ¡ˆè¿›è¡Œæ’åï¼Œè€Œéå…³æ³¨ç²¾ç¡®åˆ†æ•°ï¼Œä¾¿äºæ¯”è¾ƒæ ‡ç­¾åˆ†å¸ƒã€‚</li>
<li>æå‡ºçš„æ–¹æ³•åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç›´æ¥ç”Ÿæˆæ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-17b366dff02c765719dc4b7a371725ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7929fbd50307d1b410bb3c601057a503.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c86a019cf607b1cfe434de7dba698dd2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b054c93da0ac7b17ff7a878a45294b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-517923a174598b7c7e9e7290e01c4015.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2ec6cb41f74106fc0b0f9dede94ced1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Discriminative-Policy-Optimization-for-Token-Level-Reward-Models"><a href="#Discriminative-Policy-Optimization-for-Token-Level-Reward-Models" class="headerlink" title="Discriminative Policy Optimization for Token-Level Reward Models"></a>Discriminative Policy Optimization for Token-Level Reward Models</h2><p><strong>Authors:Hongzhan Chen, Tao Yang, Shiping Gao, Ruijun Chen, Xiaojun Quan, Hongtao Tian, Ting Yao</strong></p>
<p>Process reward models (PRMs) provide more nuanced supervision compared to outcome reward models (ORMs) for optimizing policy models, positioning them as a promising approach to enhancing the capabilities of LLMs in complex reasoning tasks. Recent efforts have advanced PRMs from step-level to token-level granularity by integrating reward modeling into the training of generative models, with reward scores derived from token generation probabilities. However, the conflict between generative language modeling and reward modeling may introduce instability and lead to inaccurate credit assignments. To address this challenge, we revisit token-level reward assignment by decoupling reward modeling from language generation and derive a token-level reward model through the optimization of a discriminative policy, termed the Q-function Reward Model (Q-RM). We theoretically demonstrate that Q-RM explicitly learns token-level Q-functions from preference data without relying on fine-grained annotations. In our experiments, Q-RM consistently outperforms all baseline methods across various benchmarks. For example, when integrated into PPO&#x2F;REINFORCE algorithms, Q-RM enhances the average Pass@1 score by 5.85&#x2F;4.70 points on mathematical reasoning tasks compared to the ORM baseline, and by 4.56&#x2F;5.73 points compared to the token-level PRM counterpart. Moreover, reinforcement learning with Q-RM significantly enhances training efficiency, achieving convergence 12 times faster than ORM on GSM8K and 11 times faster than step-level PRM on MATH. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/homzer/Q-RM">https://github.com/homzer/Q-RM</a>. </p>
<blockquote>
<p>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ä¸ºä¼˜åŒ–ç­–ç•¥æ¨¡å‹æä¾›äº†ä¸ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMsï¼‰ç›¸æ¯”æ›´ä¸ºå¾®å¦™çš„ç›‘ç£ï¼Œå°†å…¶å®šä½ä¸ºæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­èƒ½åŠ›çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚è¿‘æœŸçš„ç ”ç©¶å·²ç»æ¨è¿›äº†ä»æ­¥éª¤çº§åˆ«åˆ°ä»¤ç‰Œçº§åˆ«çš„PRMsçš„ç²’åº¦ï¼Œé€šè¿‡å°†å¥–åŠ±å»ºæ¨¡é›†æˆåˆ°ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œå¥–åŠ±åˆ†æ•°æ¥æºäºä»¤ç‰Œçš„ç”Ÿæˆæ¦‚ç‡ã€‚ç„¶è€Œï¼Œç”Ÿæˆè¯­è¨€å»ºæ¨¡å’Œå¥–åŠ±å»ºæ¨¡ä¹‹é—´çš„å†²çªå¯èƒ½ä¼šå¼•å…¥ä¸ç¨³å®šå¹¶å¯¼è‡´ä¸å‡†ç¡®çš„ä¿¡ç”¨åˆ†é…ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é‡æ–°è®¿é—®ä»¤ç‰Œçº§åˆ«çš„å¥–åŠ±åˆ†é…ï¼Œé€šè¿‡å°†å¥–åŠ±å»ºæ¨¡ä¸è¯­è¨€ç”Ÿæˆè§£è€¦ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–åˆ¤åˆ«ç­–ç•¥æ¥æ¨å¯¼ä»¤ç‰Œçº§åˆ«çš„å¥–åŠ±æ¨¡å‹ï¼Œç§°ä¸ºQå‡½æ•°å¥–åŠ±æ¨¡å‹ï¼ˆQ-RMï¼‰ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†Q-RMå¯ä»¥æ˜ç¡®åœ°ä»åå¥½æ•°æ®ä¸­å­¦ä¹ ä»¤ç‰Œçº§åˆ«çš„Qå‡½æ•°ï¼Œè€Œæ— éœ€ä¾èµ–ç²¾ç»†çš„æ³¨é‡Šã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼ŒQ-RMåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œå½“é›†æˆåˆ°PPO&#x2F;REINFORCEç®—æ³•ä¸­æ—¶ï¼Œä¸ORMåŸºçº¿ç›¸æ¯”ï¼ŒQ-RMåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„å¹³å‡Pass@1å¾—åˆ†æé«˜äº†5.85&#x2F;4.70ç‚¹ï¼›ä¸ä»¤ç‰Œçº§åˆ«çš„PRMç›¸æ¯”æé«˜äº†4.56&#x2F;5.73ç‚¹ã€‚æ­¤å¤–ï¼Œä½¿ç”¨Q-RMçš„å¼ºåŒ–å­¦ä¹ æ˜¾ç€æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œåœ¨GSM8Kä¸Šæ¯”ORMå¿«12å€ï¼Œåœ¨MATHä¸Šæ¯”æ­¥éª¤çº§åˆ«çš„PRMå¿«11å€ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/homzer/Q-RM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/homzer/Q-RMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23363v1">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨ä¼˜åŒ–ç­–ç•¥æ¨¡å‹æ–¹é¢çš„ä¼˜åŠ¿ï¼Œç›¸å¯¹äºç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMï¼‰ï¼ŒPRMæä¾›äº†æ›´å¾®å¦™çš„ç›‘ç£ï¼Œæœ‰åŠ©äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å°†å¥–åŠ±å»ºæ¨¡é›†æˆåˆ°ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œå®ç°äº†ä»æ­¥éª¤çº§åˆ«åˆ°ä»¤ç‰Œçº§åˆ«çš„PRMç²’åº¦æå‡ã€‚ç„¶è€Œï¼Œç”Ÿæˆè¯­è¨€æ¨¡å‹å’Œå¥–åŠ±å»ºæ¨¡ä¹‹é—´çš„å†²çªå¯èƒ½ä¼šå¼•å…¥ä¸ç¨³å®šæ€§å’Œä¸å‡†ç¡®çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶äººå‘˜é‡æ–°å®¡è§†äº†ä»¤ç‰Œçº§åˆ«çš„å¥–åŠ±åˆ†é…ï¼Œé€šè¿‡è§£è€¦å¥–åŠ±å»ºæ¨¡ä¸è¯­è¨€ç”Ÿæˆï¼Œæå‡ºäº†åŸºäºQå‡½æ•°çš„å¥–åŠ±æ¨¡å‹ï¼ˆQ-RMï¼‰ã€‚ç†è®ºä¸Šï¼ŒQ-RMèƒ½å¤Ÿæ˜¾å¼åœ°ä»åå¥½æ•°æ®ä¸­å­¦ä¹ ä»¤ç‰Œçº§åˆ«çš„Qå‡½æ•°ï¼Œè€Œæ— éœ€ä¾èµ–ç²¾ç»†çš„æ³¨é‡Šã€‚å®éªŒè¡¨æ˜ï¼ŒQ-RMåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œå½“é›†æˆåˆ°PPO&#x2F;REINFORCEç®—æ³•ä¸­æ—¶ï¼ŒQ-RMåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„å¹³å‡Pass@1åˆ†æ•°ç›¸è¾ƒäºORMåŸºå‡†æµ‹è¯•æé«˜äº†5.85&#x2F;4.70ä¸ªç™¾åˆ†ç‚¹ï¼Œç›¸è¾ƒäºä»¤ç‰Œçº§åˆ«çš„PRMæé«˜äº†4.56&#x2F;5.73ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œä½¿ç”¨Q-RMçš„å¼ºåŒ–å­¦ä¹ æ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œåœ¨GSM8Kä¸Šç›¸è¾ƒäºORMè¾¾åˆ°12å€çš„æ”¶æ•›é€Ÿåº¦æå‡ï¼Œåœ¨MATHä¸Šç›¸è¾ƒäºæ­¥éª¤çº§åˆ«çš„PRMè¾¾åˆ°11å€çš„æ”¶æ•›é€Ÿåº¦æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ä¸ºä¼˜åŒ–ç­–ç•¥æ¨¡å‹æä¾›äº†æ›´å¾®å¦™çš„ç›‘ç£ï¼Œæœ‰æœ›æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚</li>
<li>æœ€è¿‘çš„ç ”ç©¶å·²å°†PRMçš„ç²’åº¦ä»æ­¥éª¤çº§åˆ«æå‡åˆ°ä»¤ç‰Œçº§åˆ«ï¼Œé€šè¿‡æ•´åˆå¥–åŠ±å»ºæ¨¡åˆ°ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒä¸­ã€‚</li>
<li>ç”Ÿæˆè¯­è¨€æ¨¡å‹å’Œå¥–åŠ±å»ºæ¨¡ä¹‹é—´çš„å†²çªå¯èƒ½ä¼šå¼•å…¥ä¸ç¨³å®šæ€§å’Œä¸å‡†ç¡®çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚</li>
<li>Q-RMï¼ˆåŸºäºQå‡½æ•°çš„å¥–åŠ±æ¨¡å‹ï¼‰é€šè¿‡è§£è€¦å¥–åŠ±å»ºæ¨¡ä¸è¯­è¨€ç”Ÿæˆï¼Œä»¥åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>Q-RMèƒ½ä»åå¥½æ•°æ®ä¸­æ˜¾å¼åœ°å­¦ä¹ ä»¤ç‰Œçº§åˆ«çš„Qå‡½æ•°ï¼Œæ— éœ€ç²¾ç»†çš„æ³¨é‡Šã€‚</li>
<li>å®éªŒæ˜¾ç¤ºQ-RMåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½æ˜¾è‘—æé«˜æ•°å­¦æ¨ç†ä»»åŠ¡çš„å¹³å‡Pass@1åˆ†æ•°ã€‚</li>
<li>ä½¿ç”¨Q-RMçš„å¼ºåŒ–å­¦ä¹ æ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œç›¸è¾ƒäºå…¶ä»–æ–¹æ³•è¾¾åˆ°æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-939dc61204c8060c11d6513de20b33a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56233ac66fed2f0e59d118d907a10a9d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d44a69a8a75b0bc73c545bef714fc40.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-75a094d68bb8bbf4ce38724b838d3791.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  Argus Vision-Centric Reasoning with Grounded Chain-of-Thought
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-773d3f8e9ec076acb0ead358c33eda51.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  SoPo Text-to-Motion Generation Using Semi-Online Preference   Optimization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25879.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
