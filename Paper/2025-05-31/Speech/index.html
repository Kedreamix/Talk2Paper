<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  Boosting Domain Incremental Learning Selecting the Optimal Parameters   is All You Need">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-4b190aeb9d149262962deef8143a11cd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-31
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    49 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-31-æ›´æ–°"><a href="#2025-05-31-æ›´æ–°" class="headerlink" title="2025-05-31 æ›´æ–°"></a>2025-05-31 æ›´æ–°</h1><h2 id="Boosting-Domain-Incremental-Learning-Selecting-the-Optimal-Parameters-is-All-You-Need"><a href="#Boosting-Domain-Incremental-Learning-Selecting-the-Optimal-Parameters-is-All-You-Need" class="headerlink" title="Boosting Domain Incremental Learning: Selecting the Optimal Parameters   is All You Need"></a>Boosting Domain Incremental Learning: Selecting the Optimal Parameters   is All You Need</h2><p><strong>Authors:Qiang Wang, Xiang Song, Yuhang He, Jizhou Han, Chenhao Ding, Xinyuan Gao, Yihong Gong</strong></p>
<p>Deep neural networks (DNNs) often underperform in real-world, dynamic settings where data distributions change over time. Domain Incremental Learning (DIL) offers a solution by enabling continual model adaptation, with Parameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce knowledge conflicts. However, existing PIDIL methods struggle with parameter selection accuracy, especially as the number of domains and corresponding classes grows. To address this, we propose SOYO, a lightweight framework that improves domain selection in PIDIL. SOYO introduces a Gaussian Mixture Compressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior domain data efficiently, while a Multi-level Domain Feature Fusion Network (MDFN) enhances domain feature extraction. Our framework supports multiple Parameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks such as image classification, object detection, and speech enhancement. Experimental results on six benchmarks demonstrate SOYOâ€™s consistent superiority over existing baselines, showcasing its robustness and adaptability in complex, evolving environments. The codes will be released in <a target="_blank" rel="noopener" href="https://github.com/qwangcv/SOYO">https://github.com/qwangcv/SOYO</a>. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰åœ¨ç°å®ä¸–ç•Œä¸­çš„åŠ¨æ€ç¯å¢ƒä¸­å¾€å¾€è¡¨ç°ä¸ä½³ï¼Œåœ¨è¿™äº›ç¯å¢ƒä¸­ï¼Œæ•°æ®åˆ†å¸ƒä¼šéšæ—¶é—´å‘ç”Ÿå˜åŒ–ã€‚é¢†åŸŸå¢é‡å­¦ä¹ ï¼ˆDILï¼‰é€šè¿‡å®ç°æŒç»­æ¨¡å‹è‡ªé€‚åº”æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œå‚æ•°éš”ç¦»å‹DILï¼ˆPIDILï¼‰ä½œä¸ºä¸€ç§å‡å°‘çŸ¥è¯†å†²çªçš„æœ‰å‰é€”çš„èŒƒå¼è€Œå‡ºç°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„PIDILæ–¹æ³•åœ¨å‚æ•°é€‰æ‹©å‡†ç¡®æ€§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯éšç€é¢†åŸŸå’Œå¯¹åº”ç±»åˆ«çš„æ•°é‡å¢é•¿æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SOYOï¼Œè¿™æ˜¯ä¸€ä¸ªæ”¹è¿›çš„PIDILé¢†åŸŸé€‰æ‹©çš„è½»é‡çº§æ¡†æ¶ã€‚SOYOå¼•å…¥äº†é«˜æ–¯æ··åˆå‹ç¼©æœºï¼ˆGMCï¼‰å’Œé¢†åŸŸç‰¹å¾é‡é‡‡æ ·å™¨ï¼ˆDFRï¼‰ä»¥æœ‰æ•ˆåœ°å­˜å‚¨å’Œå¹³è¡¡å…ˆå‰çš„é¢†åŸŸæ•°æ®ï¼ŒåŒæ—¶å¤šçº§é¢†åŸŸç‰¹å¾èåˆç½‘ç»œï¼ˆMDFNï¼‰å¢å¼ºäº†é¢†åŸŸç‰¹å¾çš„æå–ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ”¯æŒå¤šç§å‚æ•°æœ‰æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œå¹¶åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­éŸ³å¢å¼ºç­‰ä»»åŠ¡ä¸­å¾—åˆ°äº†éªŒè¯ã€‚åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰åŸºå‡†ç›¸æ¯”ï¼ŒSOYOå…·æœ‰æŒç»­çš„ä¼˜è¶Šæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚ã€ä¸æ–­å‘å±•çš„ç¯å¢ƒä¸­çš„ç¨³å¥æ€§å’Œé€‚åº”æ€§ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/qwangcv/SOYO">https://github.com/qwangcv/SOYO</a>å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23744v1">PDF</a> Accepted at CVPR 2025</p>
<p><strong>Summary</strong><br>     é¢å¯¹éšæ—¶é—´å˜åŒ–çš„åŠ¨æ€ç¯å¢ƒï¼Œæ·±åº¦ç¥ç»ç½‘ç»œåœ¨çœŸå®ä¸–ç•Œä¸­çš„è¡¨ç°å¸¸å¸¸ä¸å°½äººæ„ã€‚åŸŸå¢é‡å­¦ä¹ ï¼ˆDILï¼‰é€šè¿‡æŒç»­æ¨¡å‹è‡ªé€‚åº”æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œè€Œå‚æ•°éš”ç¦»å‹åŸŸå¢é‡å­¦ä¹ ï¼ˆPIDILï¼‰æˆä¸ºå‡å°‘çŸ¥è¯†å†²çªçš„æœ‰å‰é€”çš„èŒƒå¼ã€‚ç„¶è€Œï¼Œéšç€åŸŸå’Œç›¸åº”ç±»åˆ«çš„æ•°é‡å¢é•¿ï¼Œç°æœ‰çš„PIDILæ–¹æ³•åœ¨å‚æ•°é€‰æ‹©å‡†ç¡®æ€§æ–¹é¢é‡åˆ°äº†æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SOYOæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ”¹è¿›äº†PIDILä¸­çš„åŸŸé€‰æ‹©ã€‚SOYOå¼•å…¥äº†é«˜æ–¯æ··åˆå‹ç¼©æœºï¼ˆGMCï¼‰å’ŒåŸŸç‰¹å¾é‡é‡‡æ ·å™¨ï¼ˆDFRï¼‰ä»¥æœ‰æ•ˆå­˜å‚¨å’Œå¹³è¡¡å…ˆå‰çš„åŸŸæ•°æ®ï¼ŒåŒæ—¶å¤šå±‚æ¬¡åŸŸç‰¹å¾èåˆç½‘ç»œï¼ˆMDFNï¼‰å¢å¼ºäº†åŸŸç‰¹å¾çš„æå–ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šç§å‚æ•°æœ‰æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œå¹¶åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­éŸ³å¢å¼ºç­‰ä»»åŠ¡ä¸­è¿›è¡Œäº†éªŒè¯ã€‚åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSOYOç›¸è¾ƒäºç°æœ‰åŸºçº¿å…·æœ‰æŒç»­çš„ä¼˜è¶Šæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚ã€ä¸æ–­å˜åŒ–ç¯å¢ƒä¸­çš„ç¨³å¥æ€§å’Œé€‚åº”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨åŠ¨æ€ç¯å¢ƒä¸­é¢ä¸´æ€§èƒ½æŒ‘æˆ˜ã€‚</li>
<li>åŸŸå¢é‡å­¦ä¹ ï¼ˆDILï¼‰é€šè¿‡æŒç»­æ¨¡å‹è‡ªé€‚åº”ä¸ºè§£å†³æ­¤æŒ‘æˆ˜æä¾›äº†æ–¹æ¡ˆã€‚</li>
<li>å‚æ•°éš”ç¦»å‹åŸŸå¢é‡å­¦ä¹ ï¼ˆPIDILï¼‰æ˜¯å‡å°‘çŸ¥è¯†å†²çªçš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>SOYOæ¡†æ¶æ”¹è¿›äº†PIDILä¸­çš„åŸŸé€‰æ‹©é—®é¢˜ã€‚</li>
<li>SOYOå¼•å…¥äº†é«˜æ–¯æ··åˆå‹ç¼©æœºï¼ˆGMCï¼‰ã€åŸŸç‰¹å¾é‡é‡‡æ ·å™¨ï¼ˆDFRï¼‰å’Œå¤šå±‚æ¬¡åŸŸç‰¹å¾èåˆç½‘ç»œï¼ˆMDFNï¼‰ã€‚</li>
<li>SOYOæ¡†æ¶æ”¯æŒå¤šç§å‚æ•°æœ‰æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚</li>
<li>åœ¨å¤šä¸ªä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸Šï¼ŒSOYOç›¸è¾ƒäºç°æœ‰æ–¹æ³•è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23744">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4be836680807bcbbdcee6e0e0c6d521e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a3afa408cd8bd9044a5d09e77a9252e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58830f0db2e21cda60f2a5341dc884f9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Prompting-Whisper-for-Improved-Verbatim-Transcription-and-End-to-end-Miscue-Detection"><a href="#Prompting-Whisper-for-Improved-Verbatim-Transcription-and-End-to-end-Miscue-Detection" class="headerlink" title="Prompting Whisper for Improved Verbatim Transcription and End-to-end   Miscue Detection"></a>Prompting Whisper for Improved Verbatim Transcription and End-to-end   Miscue Detection</h2><p><strong>Authors:Griffin Dietz Smith, Dianna Yee, Jennifer King Chen, Leah Findlater</strong></p>
<p>Identifying mistakes (i.e., miscues) made while reading aloud is commonly approached post-hoc by comparing automatic speech recognition (ASR) transcriptions to the target reading text. However, post-hoc methods perform poorly when ASR inaccurately transcribes verbatim speech. To improve on current methods for reading error annotation, we propose a novel end-to-end architecture that incorporates the target reading text via prompting and is trained for both improved verbatim transcription and direct miscue detection. Our contributions include: first, demonstrating that incorporating reading text through prompting benefits verbatim transcription performance over fine-tuning, and second, showing that it is feasible to augment speech recognition tasks for end-to-end miscue detection. We conducted two case studies â€“ childrenâ€™s read-aloud and adult atypical speech â€“ and found that our proposed strategies improve verbatim transcription and miscue detection compared to current state-of-the-art. </p>
<blockquote>
<p>è¯†åˆ«æœ—è¯»æ—¶å‡ºç°çš„é”™è¯¯ï¼ˆå³å¤±è¯¯ï¼‰é€šå¸¸æ˜¯é€šè¿‡äº‹åæ¯”è¾ƒè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•ä¸ç›®æ ‡é˜…è¯»æ–‡æœ¬è¿›è¡Œçš„ã€‚ç„¶è€Œï¼Œå½“ASRä¸å‡†ç¡®è½¬å½•åŸå£°è¯­éŸ³æ—¶ï¼Œäº‹åæ–¹æ³•çš„æ€§èƒ½è¾ƒå·®ã€‚ä¸ºäº†æ”¹è¿›å½“å‰çš„é˜…è¯»é”™è¯¯æ³¨é‡Šæ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯æ¶æ„ï¼Œè¯¥æ¶æ„é€šè¿‡æç¤ºèå…¥ç›®æ ‡é˜…è¯»æ–‡æœ¬ï¼Œå¹¶ç»è¿‡è®­ç»ƒï¼Œæ—¢èƒ½æé«˜é€å­—è½¬å½•è´¨é‡ï¼Œåˆèƒ½ç›´æ¥æ£€æµ‹å¤±è¯¯ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ï¼šé¦–å…ˆï¼Œè¯æ˜é€šè¿‡æç¤ºèå…¥é˜…è¯»æ–‡æœ¬æœ‰åŠ©äºæé«˜é€å­—è½¬å½•æ€§èƒ½ä¼˜äºå¾®è°ƒï¼›å…¶æ¬¡ï¼Œè¡¨æ˜å¯¹ç«¯åˆ°ç«¯å¤±è¯¯æ£€æµ‹å¢å¼ºè¯­éŸ³è¯†åˆ«ä»»åŠ¡æ˜¯å¯è¡Œçš„ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸¤é¡¹æ¡ˆä¾‹ç ”ç©¶â€”â€”å„¿ç«¥æœ—è¯»å’Œæˆäººéå…¸å‹è¯­éŸ³ï¼Œå‘ç°ä¸å½“å‰æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„ç­–ç•¥æ”¹è¿›äº†é€å­—è½¬å½•å’Œå¤±è¯¯æ£€æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23627v1">PDF</a> Interspeech 2025</p>
<p><strong>æ€»ç»“</strong><br>    æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç«¯åˆ°ç«¯æ¶æ„ï¼Œè¯¥æ¶æ„é€šè¿‡æç¤ºç›®æ ‡é˜…è¯»æ–‡æœ¬ï¼Œæ”¹è¿›äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„è½¬å½•ï¼Œæé«˜äº†ç›´æ¥è¯¯è¯»æ£€æµ‹èƒ½åŠ›ã€‚è¯¥ç ”ç©¶è´¡çŒ®åœ¨äºè¯æ˜é€šè¿‡æç¤ºé˜…è¯»æ–‡æœ¬å¯ä»¥æ”¹å–„è½¬å½•æ€§èƒ½ï¼Œå¹¶å±•ç¤ºå¢å¼ºè¯­éŸ³è¯†åˆ«ä»»åŠ¡ä»¥å®ç°ç«¯åˆ°ç«¯è¯¯è¯»æ£€æµ‹çš„å¯è¡Œæ€§ã€‚åœ¨å„¿ç«¥æœ—è¯»å’Œæˆäººéå…¸å‹è¯­éŸ³çš„ä¸¤ä¸ªæ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œè¯¥ç­–ç•¥æ”¹è¿›äº†è½¬å½•å’Œè¯¯è¯»æ£€æµ‹æ•ˆæœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•ä¸ç›® æ ‡é˜…è¯»æ–‡æœ¬å¯¹æ¯”æ¥è¯†åˆ«æœ—è¯»æ—¶çš„é”™è¯¯ï¼Œä½†å½“ASRè½¬å½•ä¸ç²¾ç¡®æ—¶è¡¨ç°ä¸ä½³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç«¯åˆ°ç«¯æ¶æ„ï¼Œè¯¥æ¶æ„é€šè¿‡æç¤ºç›®æ ‡é˜…è¯»æ–‡æœ¬æ”¹è¿›äº†è½¬å½•æ•ˆæœã€‚</li>
<li>è¯æ˜äº†é€šè¿‡æç¤ºé˜…è¯»æ–‡æœ¬å¯ä»¥æ”¹å–„è½¬å½•æ€§èƒ½ï¼Œç›¸è¾ƒäºå¾®è°ƒæ–¹å¼å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>å±•ç¤ºäº†å¯¹è¯­éŸ³è¯†åˆ«ä»»åŠ¡è¿›è¡Œå¢å¼ºï¼Œä»¥å®ç°ç«¯åˆ°ç«¯çš„è¯¯è¯»æ£€æµ‹æ˜¯å¯è¡Œçš„ã€‚</li>
<li>åœ¨å„¿ç«¥æœ—è¯»å’Œæˆäººéå…¸å‹è¯­éŸ³çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œæ–°ç­–ç•¥æé«˜äº†è½¬å½•å’Œè¯¯è¯»æ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›æŠ€æœ¯æœ‰æ‰€æ”¹è¿›ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºæé«˜è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„æ€§èƒ½å’Œå‡†ç¡®æ€§å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-875035034919165f5a46f68d704fac34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2019ecbd2f06d04902ba2a6cf4fd42b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa4ca47e6186dab43f8f2c0409fa652b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6258f7ef2340b5d728a90dcd37589848.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46203d7b5aef82bd21b9d1981d1cc089.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Wav2Sem-Plug-and-Play-Audio-Semantic-Decoupling-for-3D-Speech-Driven-Facial-Animation"><a href="#Wav2Sem-Plug-and-Play-Audio-Semantic-Decoupling-for-3D-Speech-Driven-Facial-Animation" class="headerlink" title="Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven   Facial Animation"></a>Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven   Facial Animation</h2><p><strong>Authors:Hao Li, Ju Dai, Xin Zhao, Feng Zhou, Junjun Pan, Lei Li</strong></p>
<p>In 3D speech-driven facial animation generation, existing methods commonly employ pre-trained self-supervised audio models as encoders. However, due to the prevalence of phonetically similar syllables with distinct lip shapes in language, these near-homophone syllables tend to exhibit significant coupling in self-supervised audio feature spaces, leading to the averaging effect in subsequent lip motion generation. To address this issue, this paper proposes a plug-and-play semantic decorrelation module-Wav2Sem. This module extracts semantic features corresponding to the entire audio sequence, leveraging the added semantic information to decorrelate audio encodings within the feature space, thereby achieving more expressive audio features. Extensive experiments across multiple Speech-driven models indicate that the Wav2Sem module effectively decouples audio features, significantly alleviating the averaging effect of phonetically similar syllables in lip shape generation, thereby enhancing the precision and naturalness of facial animations. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/wslh852/Wav2Sem.git">https://github.com/wslh852/Wav2Sem.git</a>. </p>
<blockquote>
<p>åœ¨3Dè¯­éŸ³é©±åŠ¨é¢éƒ¨åŠ¨ç”»ç”Ÿæˆä¸­ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨é¢„è®­ç»ƒçš„è‡ªæˆ‘ç›‘ç£éŸ³é¢‘æ¨¡å‹ä½œä¸ºç¼–ç å™¨ã€‚ç„¶è€Œï¼Œç”±äºè¯­è¨€ä¸­éŸ³ä½ç›¸ä¼¼ä½†å”‡å½¢ä¸åŒçš„éŸ³èŠ‚æ™®éå­˜åœ¨ï¼Œè¿™äº›è¿‘ä¼¼åŒéŸ³èŠ‚çš„éŸ³èŠ‚åœ¨è‡ªæˆ‘ç›‘ç£çš„éŸ³é¢‘ç‰¹å¾ç©ºé—´ä¸­å¾€å¾€è¡¨ç°å‡ºæ˜¾è‘—çš„è€¦åˆï¼Œå¯¼è‡´åç»­å”‡åŠ¨ç”Ÿæˆä¸­çš„å¹³å‡æ•ˆåº”ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„è¯­ä¹‰å»ç›¸å…³æ¨¡å—â€”â€”Wav2Semã€‚è¯¥æ¨¡å—æå–ä¸æ•´ä¸ªéŸ³é¢‘åºåˆ—å¯¹åº”çš„è¯­ä¹‰ç‰¹å¾ï¼Œåˆ©ç”¨æ·»åŠ çš„è¯­ä¹‰ä¿¡æ¯å»ç›¸å…³ç‰¹å¾ç©ºé—´ä¸­çš„éŸ³é¢‘ç¼–ç ï¼Œä»è€Œå®ç°æ›´å…·è¡¨ç°åŠ›çš„éŸ³é¢‘ç‰¹å¾ã€‚åœ¨å¤šä¸ªè¯­éŸ³é©±åŠ¨æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒWav2Semæ¨¡å—æœ‰æ•ˆåœ°è§£è€¦äº†éŸ³é¢‘ç‰¹å¾ï¼Œæ˜¾è‘—å‡è½»äº†éŸ³ä½ç›¸ä¼¼éŸ³èŠ‚åœ¨å”‡å½¢ç”Ÿæˆä¸­çš„å¹³å‡æ•ˆåº”ï¼Œä»è€Œæé«˜äº†é¢éƒ¨åŠ¨ç”»çš„ç²¾åº¦å’Œè‡ªç„¶åº¦ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wslh852/Wav2Sem.git%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wslh852/Wav2Sem.gitä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23290v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong><br>è¯­éŸ³é©±åŠ¨çš„ä¸‰ç»´é¢éƒ¨åŠ¨ç”»ç”Ÿæˆä¸­ï¼Œç°æœ‰æ–¹æ³•å¸¸ä½¿ç”¨é¢„è®­ç»ƒçš„è‡ªæˆ‘ç›‘ç£éŸ³é¢‘æ¨¡å‹ä½œä¸ºç¼–ç å™¨ã€‚ç„¶è€Œï¼Œç”±äºè¯­è¨€ä¸­éŸ³ä½ç›¸ä¼¼ä½†å”‡å½¢ä¸åŒçš„éŸ³èŠ‚æ™®éå­˜åœ¨ï¼Œè¿™äº›è¿‘ä¼¼çš„éŸ³èŠ‚åœ¨è‡ªæˆ‘ç›‘ç£çš„éŸ³é¢‘ç‰¹å¾ç©ºé—´ä¸­å¾€å¾€è¡¨ç°å‡ºæ˜¾è‘—çš„è€¦åˆç°è±¡ï¼Œå¯¼è‡´åç»­çš„å”‡éƒ¨è¿åŠ¨ç”Ÿæˆå‡ºç°å¹³å‡æ•ˆåº”ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å³æ’å³ç”¨çš„è¯­ä¹‰å»ç›¸å…³æ¨¡å—â€”â€”Wav2Semã€‚è¯¥æ¨¡å—æå–æ•´ä¸ªéŸ³é¢‘åºåˆ—çš„è¯­ä¹‰ç‰¹å¾ï¼Œåˆ©ç”¨é¢å¤–çš„è¯­ä¹‰ä¿¡æ¯å»ç›¸å…³éŸ³é¢‘ç¼–ç ç‰¹å¾ç©ºé—´ä¸­çš„ç¼–ç ï¼Œä»è€Œå¾—åˆ°æ›´å¯Œæœ‰è¡¨ç°åŠ›çš„éŸ³é¢‘ç‰¹å¾ã€‚è·¨å¤šä¸ªè¯­éŸ³é©±åŠ¨æ¨¡å‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒWav2Semæ¨¡å—æœ‰æ•ˆåœ°è§£é™¤äº†éŸ³é¢‘ç‰¹å¾çš„è€¦åˆï¼Œæ˜¾è‘—å‡è½»äº†éŸ³ä½ç›¸ä¼¼éŸ³èŠ‚åœ¨å”‡å½¢ç”Ÿæˆä¸­çš„å¹³å‡æ•ˆåº”ï¼Œä»è€Œæé«˜äº†é¢éƒ¨åŠ¨ç”»çš„ç²¾ç¡®æ€§å’Œè‡ªç„¶æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è¯­éŸ³é©±åŠ¨é¢éƒ¨åŠ¨ç”»æ–¹æ³•ä¸»è¦ä½¿ç”¨é¢„è®­ç»ƒè‡ªæˆ‘ç›‘ç£éŸ³é¢‘æ¨¡å‹ä½œä¸ºç¼–ç å™¨ã€‚</li>
<li>è¿‘éŸ³èŠ‚çš„æ™®éå­˜åœ¨å¯¼è‡´éŸ³é¢‘ç‰¹å¾ç©ºé—´ä¸­çš„è€¦åˆç°è±¡ã€‚</li>
<li>è€¦åˆç°è±¡ä¼šå¼•å‘å”‡éƒ¨è¿åŠ¨ç”Ÿæˆçš„å¹³å‡æ•ˆåº”ã€‚</li>
<li>Wav2Semæ¨¡å—æ—¨åœ¨è§£å†³æ­¤é—®é¢˜ï¼Œé€šè¿‡æå–æ•´ä¸ªéŸ³é¢‘åºåˆ—çš„è¯­ä¹‰ç‰¹å¾æ¥è£…é¥°ç›¸å…³éŸ³é¢‘ç¼–ç ã€‚</li>
<li>Wav2Semåˆ©ç”¨é¢å¤–è¯­ä¹‰ä¿¡æ¯å»ç›¸å…³éŸ³é¢‘ç¼–ç ç‰¹å¾ç©ºé—´ä¸­çš„ç¼–ç ã€‚</li>
<li>Wav2Semæ¨¡å—æœ‰æ•ˆè§£é™¤éŸ³é¢‘ç‰¹å¾çš„è€¦åˆï¼Œå‡è½»å¹³å‡æ•ˆåº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4903ff89b1d15af0ea5529bb293a49a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71ffbc5c3f62a5886efa1e7a420a6b25.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f6df0ead30368881a33dd1422fa9d276.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Towards-LLM-Empowered-Fine-Grained-Speech-Descriptors-for-Explainable-Emotion-Recognition"><a href="#Towards-LLM-Empowered-Fine-Grained-Speech-Descriptors-for-Explainable-Emotion-Recognition" class="headerlink" title="Towards LLM-Empowered Fine-Grained Speech Descriptors for Explainable   Emotion Recognition"></a>Towards LLM-Empowered Fine-Grained Speech Descriptors for Explainable   Emotion Recognition</h2><p><strong>Authors:Youjun Chen, Xurong Xie, Haoning Xu, Mengzhe Geng, Guinan Li, Chengxi Deng, Huimeng Wang, Shujie Hu, Xunying Liu</strong></p>
<p>This paper presents a novel end-to-end LLM-empowered explainable speech emotion recognition (SER) approach. Fine-grained speech emotion descriptor (SED) features, e.g., pitch, tone and emphasis, are disentangled from HuBERT SSL representations via alternating LLM fine-tuning to joint SER-SED prediction and ASR tasks. VAE compressed HuBERT features derived via Information Bottleneck (IB) are used to adjust feature granularity. Experiments on the IEMOCAP and MELD benchmarks demonstrate that our approach consistently outperforms comparable LLaMA-based SER baselines, including those using either (a) alternating multi-task fine-tuning alone or (b) feature disentanglement only. Statistically significant increase of SER unweighted accuracy by up to 4.0% and 3.7% absolute (5.4% and 6.6% relative) are obtained. More importantly, emotion descriptors offer further explainability for SER. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯çš„LLMèµ‹èƒ½å¯è§£é‡Šè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ–¹æ³•ã€‚é€šè¿‡äº¤æ›¿çš„LLMå¾®è°ƒï¼Œä»HuBERT SSLè¡¨ç¤ºä¸­åˆ†ç¦»å‡ºç²¾ç»†çš„è¯­éŸ³æƒ…æ„Ÿæè¿°ç¬¦ï¼ˆSEDï¼‰ç‰¹å¾ï¼Œå¦‚éŸ³è°ƒã€éŸ³è°ƒå’Œå¼ºè°ƒï¼Œä»¥è”åˆè¿›è¡ŒSER-SEDé¢„æµ‹å’ŒASRä»»åŠ¡ã€‚ä½¿ç”¨åŸºäºä¿¡æ¯ç“¶é¢ˆï¼ˆIBï¼‰çš„VAEå‹ç¼©HuBERTç‰¹å¾æ¥è°ƒæ•´ç‰¹å¾ç²’åº¦ã€‚åœ¨IEMOCAPå’ŒMELDåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºç±»ä¼¼çš„LLaMAåŸºç¡€çš„SERåŸºçº¿ï¼ŒåŒ…æ‹¬é‚£äº›ï¼ˆaï¼‰ä»…ä½¿ç”¨äº¤æ›¿å¤šä»»åŠ¡å¾®è°ƒæˆ–ï¼ˆbï¼‰ä»…ä½¿ç”¨ç‰¹å¾åˆ†ç¦»çš„æ–¹æ³•ã€‚åœ¨æœªç»åŠ æƒçš„SERå‡†ç¡®ç‡ä¸Šè·å¾—äº†é«˜è¾¾4.0%å’Œ3.7%çš„ç»å¯¹å¢é•¿ï¼ˆåˆ†åˆ«ä¸º5.4%å’Œ6.6%çš„ç›¸å¯¹å¢é•¿ï¼‰ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæƒ…æ„Ÿæè¿°ç¬¦ä¸ºSERæä¾›äº†è¿›ä¸€æ­¥çš„è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23236v1">PDF</a> Accepted by INTERSPEECH2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯çš„LLMèµ‹èƒ½çš„å¯è§£é‡Šè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡äº¤æ›¿çš„LLMå¾®è°ƒï¼Œä»HuBERT SSLè¡¨å¾ä¸­åˆ†ç¦»å‡ºç²¾ç»†çš„è¯­éŸ³æƒ…æ„Ÿæè¿°ç¬¦ï¼ˆSEDï¼‰ç‰¹å¾ï¼Œå¦‚éŸ³è°ƒã€éŸ³è°ƒå’Œå¼ºè°ƒç­‰ï¼Œç„¶åè¿›è¡Œè”åˆSER-SEDé¢„æµ‹å’ŒASRä»»åŠ¡ã€‚é€šè¿‡ä¿¡æ¯ç“¶é¢ˆï¼ˆIBï¼‰ä½¿ç”¨VAEå‹ç¼©çš„HuBERTç‰¹å¾æ¥è°ƒæ•´ç‰¹å¾ç²’åº¦ã€‚åœ¨IEMOCAPå’ŒMELDåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºç±»ä¼¼çš„LLaMAåŸºç¡€çš„SERåŸºçº¿ï¼ŒåŒ…æ‹¬é‚£äº›åªä½¿ç”¨ï¼ˆaï¼‰äº¤æ›¿å¤šä»»åŠ¡å¾®è°ƒæˆ–ï¼ˆbï¼‰ç‰¹å¾åˆ†ç¦»çš„æ–¹æ³•ã€‚ç»Ÿè®¡ä¸Šï¼ŒSERçš„æ— æƒé‡å‡†ç¡®ç‡ç»å¯¹æé«˜äº†4.0%å’Œ3.7%ï¼ˆç›¸å¯¹æé«˜äº†5.4%å’Œ6.6%ï¼‰ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæƒ…æ„Ÿæè¿°ç¬¦ä¸ºSERæä¾›äº†è¿›ä¸€æ­¥çš„è§£é‡Šæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ç«¯åˆ°ç«¯çš„LLMèµ‹èƒ½è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ–°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡äº¤æ›¿LLMå¾®è°ƒï¼Œå®ç°äº†ä»HuBERT SSLè¡¨å¾ä¸­åˆ†ç¦»å‡ºè¯­éŸ³æƒ…æ„Ÿæè¿°ç¬¦ç‰¹å¾ã€‚</li>
<li>ç»“åˆSEDç‰¹å¾å’Œè”åˆSER-SEDé¢„æµ‹åŠASRä»»åŠ¡ï¼Œæé«˜äº†æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨VAEå‹ç¼©çš„HuBERTç‰¹å¾ï¼Œé€šè¿‡ä¿¡æ¯ç“¶é¢ˆè°ƒæ•´ç‰¹å¾ç²’åº¦ã€‚</li>
<li>åœ¨IEMOCAPå’ŒMELDåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸å¯¹æé«˜SERçš„æ— æƒé‡å‡†ç¡®ç‡è¾¾5.4%å’Œ6.6%ã€‚</li>
<li>æƒ…æ„Ÿæè¿°ç¬¦ä¸ºSERæä¾›äº†é¢å¤–çš„è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ae15c9b1d481c250062b9a600b9dfd4d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-329845d937f85656e24ede96fddd0760.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6674ac72e8487b01960bca52ba8eefcd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1f23becb478cef846b3b17e5d8981b2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Interspeech-2025-URGENT-Speech-Enhancement-Challenge"><a href="#Interspeech-2025-URGENT-Speech-Enhancement-Challenge" class="headerlink" title="Interspeech 2025 URGENT Speech Enhancement Challenge"></a>Interspeech 2025 URGENT Speech Enhancement Challenge</h2><p><strong>Authors:Kohei Saijo, Wangyou Zhang, Samuele Cornell, Robin Scheibler, Chenda Li, Zhaoheng Ni, Anurag Kumar, Marvin Sach, Yihui Fu, Wei Wang, Tim Fingscheidt, Shinji Watanabe</strong></p>
<p>There has been a growing effort to develop universal speech enhancement (SE) to handle inputs with various speech distortions and recording conditions. The URGENT Challenge series aims to foster such universal SE by embracing a broad range of distortion types, increasing data diversity, and incorporating extensive evaluation metrics. This work introduces the Interspeech 2025 URGENT Challenge, the second edition of the series, to explore several aspects that have received limited attention so far: language dependency, universality for more distortion types, data scalability, and the effectiveness of using noisy training data. We received 32 submissions, where the best system uses a discriminative model, while most other competitive ones are hybrid methods. Analysis reveals some key findings: (i) some generative or hybrid approaches are preferred in subjective evaluations over the top discriminative model, and (ii) purely generative SE models can exhibit language dependency. </p>
<blockquote>
<p>åœ¨å¼€å‘èƒ½å¤Ÿå¤„ç†å„ç§è¯­éŸ³å¤±çœŸå’Œå½•éŸ³æ¡ä»¶çš„é€šç”¨è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹é¢ï¼Œå·²ç»ä»˜å‡ºäº†è¶Šæ¥è¶Šå¤šçš„åŠªåŠ›ã€‚URGENT Challengeç³»åˆ—æ—¨åœ¨é€šè¿‡æ¥çº³å¹¿æ³›çš„å¤±çœŸç±»å‹ã€å¢åŠ æ•°æ®å¤šæ ·æ€§å’Œèå…¥å¹¿æ³›çš„è¯„ä¼°æŒ‡æ ‡æ¥ä¿ƒè¿›è¿™ç§é€šç”¨SEçš„å‘å±•ã€‚è¿™é¡¹å·¥ä½œä»‹ç»äº†Interspeech 2025 URGENT Challengeï¼Œå³è¯¥ç³»åˆ—çš„ç¬¬äºŒç‰ˆï¼Œæ—¨åœ¨æ¢ç´¢è¿„ä»Šä¸ºæ­¢å—åˆ°æœ‰é™å…³æ³¨çš„å‡ ä¸ªæ–¹é¢ï¼šè¯­è¨€ä¾èµ–æ€§ã€æ›´å¤šå¤±çœŸç±»å‹çš„æ™®éæ€§ã€æ•°æ®å¯æ‰©å±•æ€§ä»¥åŠä½¿ç”¨å¸¦å™ªå£°è®­ç»ƒæ•°æ®çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æ”¶åˆ°äº†32ä»½æäº¤ï¼Œå…¶ä¸­æœ€ä½³ç³»ç»Ÿä½¿ç”¨çš„æ˜¯åˆ¤åˆ«æ¨¡å‹ï¼Œè€Œå…¶ä»–å¤§å¤šæ•°æœ‰ç«äº‰åŠ›çš„ç³»ç»Ÿéƒ½æ˜¯æ··åˆæ–¹æ³•ã€‚åˆ†ææ­ç¤ºäº†ä¸€äº›å…³é”®å‘ç°ï¼šï¼ˆiï¼‰åœ¨æŸäº›ä¸»è§‚è¯„ä»·ä¸­ï¼Œä¸€äº›ç”Ÿæˆå¼æˆ–æ··åˆæ–¹æ³•ä¼˜äºé¡¶çº§åˆ¤åˆ«æ¨¡å‹ï¼›ï¼ˆiiï¼‰çº¯ç²¹çš„ç”Ÿæˆå¼SEæ¨¡å‹ä¼šè¡¨ç°å‡ºè¯­è¨€ä¾èµ–æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23212v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ®µå†…å®¹ä»‹ç»äº†ä¸ºäº†åº”å¯¹å„ç§è¯­éŸ³å¤±çœŸå’Œå½•åˆ¶æ¡ä»¶è€Œå¼€å‘çš„é€šç”¨è¯­éŸ³å¢å¼ºæŠ€æœ¯ï¼ˆSEï¼‰ã€‚ç‰¹åˆ«æ˜¯ä»‹ç»äº†Interspeech 2025 URGENT Challengeè¿™ä¸€æŒ‘æˆ˜ç³»åˆ—çš„ç¬¬äºŒè½®ï¼Œè¯¥æŒ‘æˆ˜æ—¨åœ¨æ¢è®¨è¯­è¨€ä¾èµ–æ€§ã€æ›´å¤šå¤±çœŸç±»å‹çš„æ™®éæ€§ã€æ•°æ®å¯ä¼¸ç¼©æ€§å’Œä½¿ç”¨å™ªå£°è®­ç»ƒæ•°æ®çš„æœ‰æ•ˆæ€§ç­‰é—®é¢˜ã€‚åŒæ—¶æåˆ°äº†æ”¶åˆ°çš„ä¸€äº›é‡è¦å‘ç°å’Œå…³é”®å‘ç°ï¼šä¸»è§‚è¯„ä»·ä¸­ä¸€äº›ç”Ÿæˆæ€§æˆ–æ··åˆæ–¹æ³•ä¼˜äºé¡¶çº§åˆ¤åˆ«æ¨¡å‹ï¼Œè€Œçº¯ç²¹çš„ç”Ÿæˆæ€§SEæ¨¡å‹å¯èƒ½ä¼šè¡¨ç°å‡ºè¯­è¨€ä¾èµ–æ€§ã€‚ç›®å‰ï¼Œæœ€ä½³ç³»ç»Ÿä½¿ç”¨çš„æ˜¯åˆ¤åˆ«æ¨¡å‹ï¼Œè€Œå…¶ä»–æœ‰ç«äº‰åŠ›çš„ç³»ç»Ÿåˆ™å¤šä¸ºæ··åˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é€šç”¨è¯­éŸ³å¢å¼ºæŠ€æœ¯ï¼ˆSEï¼‰æ­£åœ¨ä¸æ–­å‘å±•ï¼Œä»¥åº”å¯¹å„ç§è¯­éŸ³å¤±çœŸå’Œå½•åˆ¶æ¡ä»¶ã€‚</li>
<li>Interspeech 2025 URGENT Challengeæ—¨åœ¨æ¢ç´¢è¯­è¨€ä¾èµ–æ€§ã€æ›´å¤šå¤±çœŸç±»å‹çš„æ™®éæ€§ç­‰é—®é¢˜ã€‚</li>
<li>è¯¥æŒ‘æˆ˜æ”¶åˆ°äº†32ä»½æäº¤ï¼Œå…¶ä¸­æœ€ä½³ç³»ç»Ÿä½¿ç”¨çš„æ˜¯åˆ¤åˆ«æ¨¡å‹ã€‚</li>
<li>åˆ†æå‘ç°ä¸€äº›ç”Ÿæˆæ€§æˆ–æ··åˆæ–¹æ³•åœ¨ä¸»è§‚è¯„ä»·ä¸­ä¼˜äºé¡¶çº§åˆ¤åˆ«æ¨¡å‹ã€‚</li>
<li>çº¯ç²¹çš„ç”Ÿæˆæ€§SEæ¨¡å‹å¯èƒ½ä¼šè¡¨ç°å‡ºè¯­è¨€ä¾èµ–æ€§ã€‚</li>
<li>æ•°æ®å¤šæ ·æ€§å’Œå¹¿æ³›è¯„ä¼°æŒ‡æ ‡å¯¹äºé€šç”¨SEçš„å‘å±•è‡³å…³é‡è¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23212">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2123504f6e669952e7c0d2301296b8a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e614e74ac01ce210994345e326b1fa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-040c92ede9916fac7c809c4c0ce69a17.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Towards-Robust-Overlapping-Speech-Detection-A-Speaker-Aware-Progressive-Approach-Using-WavLM"><a href="#Towards-Robust-Overlapping-Speech-Detection-A-Speaker-Aware-Progressive-Approach-Using-WavLM" class="headerlink" title="Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive   Approach Using WavLM"></a>Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive   Approach Using WavLM</h2><p><strong>Authors:Zhaokai Sun, Li Zhang, Qing Wang, Pan Zhou, Lei Xie</strong></p>
<p>Overlapping Speech Detection (OSD) aims to identify regions where multiple speakers overlap in a conversation, a critical challenge in multi-party speech processing. This work proposes a speaker-aware progressive OSD model that leverages a progressive training strategy to enhance the correlation between subtasks such as voice activity detection (VAD) and overlap detection. To improve acoustic representation, we explore the effectiveness of state-of-the-art self-supervised learning (SSL) models, including WavLM and wav2vec 2.0, while incorporating a speaker attention module to enrich features with frame-level speaker information. Experimental results show that the proposed method achieves state-of-the-art performance, with an F1 score of 82.76% on the AMI test set, demonstrating its robustness and effectiveness in OSD. </p>
<blockquote>
<p>é‡å è¯­éŸ³æ£€æµ‹ï¼ˆOSDï¼‰æ—¨åœ¨è¯†åˆ«å¯¹è¯ä¸­å¤šä¸ªå‘è¨€äººé‡å çš„åŒºåŸŸï¼Œè¿™æ˜¯å¤šæ–¹è¯­éŸ³å¤„ç†ä¸­çš„ä¸€é¡¹å…³é”®æŒ‘æˆ˜ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºå‘è¨€äººæ„ŸçŸ¥çš„æ¸è¿›å¼OSDæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œä»¥å¢å¼ºå­ä»»åŠ¡ï¼ˆå¦‚è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆVADï¼‰å’Œé‡å æ£€æµ‹ï¼‰ä¹‹é—´çš„ç›¸å…³æ€§ã€‚ä¸ºäº†æé«˜å£°éŸ³è¡¨ç¤ºèƒ½åŠ›ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æœ€å…ˆè¿›çš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬WavLMå’Œwav2vec 2.0ï¼ŒåŒæ—¶ç»“åˆå‘è¨€äººæ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥ä¸°å¯Œå¸§çº§å‘è¨€äººä¿¡æ¯çš„ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼Œåœ¨AMIæµ‹è¯•é›†ä¸Šçš„F1åˆ†æ•°ä¸º82.76%ï¼Œè¯æ˜äº†å…¶åœ¨OSDä¸­çš„ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23207v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯´è¯è€…æ„ŸçŸ¥çš„æ¸è¿›å¼é‡å è¯­éŸ³æ£€æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œå¢å¼ºè¯­éŸ³æ´»åŠ¨æ£€æµ‹å’Œé‡å æ£€æµ‹ç­‰å­ä»»åŠ¡ä¹‹é—´çš„ç›¸å…³æ€§ã€‚ä¸ºæé«˜å£°å­¦è¡¨ç°ï¼Œæœ¬æ–‡æ¢ç´¢äº†æœ€å‰æ²¿çš„è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œå¦‚WavLMå’Œwav2vec 2.0ï¼ŒåŒæ—¶ç»“åˆè¯´è¯è€…æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥ä¸°å¯Œå¸§çº§åˆ«çš„è¯´è¯è€…ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨AMIæµ‹è¯•é›†ä¸Šå–å¾—äº†F1åˆ†æ•°ä¸º82.76%çš„å“è¶Šæ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨é‡å è¯­éŸ³æ£€æµ‹ä¸­çš„ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯´è¯è€…æ„ŸçŸ¥çš„æ¸è¿›å¼é‡å è¯­éŸ³æ£€æµ‹æ¨¡å‹è¢«æå‡ºã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨æ¸è¿›å¼è®­ç»ƒç­–ç•¥ä»¥å¢å¼ºè¯­éŸ³æ´»åŠ¨æ£€æµ‹å’Œé‡å æ£€æµ‹ç­‰å­ä»»åŠ¡çš„ç›¸å…³æ€§ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨æœ€å‰æ²¿çš„è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚WavLMå’Œwav2vec 2.0ï¼‰ï¼Œæé«˜äº†å£°å­¦è¡¨ç°ã€‚</li>
<li>ç»“åˆè¯´è¯è€…æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥ä¸°å¯Œå¸§çº§åˆ«çš„è¯´è¯è€…ä¿¡æ¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ¨¡å‹åœ¨AMIæµ‹è¯•é›†ä¸Šå–å¾—äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>æ¨¡å‹å…·æœ‰ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ï¼Œå¯ç”¨äºå¤šæ–¹çš„è¯­éŸ³å¤„ç†ä¸­çš„é‡å è¯­éŸ³æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23207">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cfb4188a4f4778ca4948f85a759bf86c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28f431198b37f1b0489c327744f6269d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50da43822c3606272564bdcf76a3de00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f6c60a75f054933826fb800fdffe45b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c44c9dd077e029bb370986fd6725db76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f934d9f7470c36a114974aa85d2622b1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ZIPA-A-family-of-efficient-models-for-multilingual-phone-recognition"><a href="#ZIPA-A-family-of-efficient-models-for-multilingual-phone-recognition" class="headerlink" title="ZIPA: A family of efficient models for multilingual phone recognition"></a>ZIPA: A family of efficient models for multilingual phone recognition</h2><p><strong>Authors:Jian Zhu, Farhan Samir, Eleanor Chodroff, David R. Mortensen</strong></p>
<p>We present ZIPA, a family of efficient speech models that advances the state-of-the-art performance of crosslinguistic phone recognition. We first curated IPAPack++, a large-scale multilingual speech corpus with 17,132 hours of normalized phone transcriptions and a novel evaluation set capturing unseen languages and sociophonetic variation. With the large-scale training data, ZIPA, including transducer (ZIPA-T) and CTC-based (ZIPA-CR) variants, leverage the efficient Zipformer backbones and outperform existing phone recognition systems with much fewer parameters. Further scaling via noisy student training on 11,000 hours of pseudo-labeled multilingual data yields further improvement. While ZIPA achieves strong performance on benchmarks, error analysis reveals persistent limitations in modeling sociophonetic diversity, underscoring challenges for future research. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ZIPAï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„è¯­éŸ³æ¨¡å‹å®¶æ—ï¼Œå®ƒæé«˜äº†è·¨è¯­è¨€è¯­éŸ³è¯†åˆ«çš„æœ€æ–°æ€§èƒ½ã€‚æˆ‘ä»¬é¦–å…ˆæ•´ç†äº†IPAPack++ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè¯­è¨€è¯­éŸ³è¯­æ–™åº“ï¼ŒåŒ…å«17,132å°æ—¶çš„æ ‡å‡†åŒ–è¯­éŸ³è½¬å½•å’Œæ•æ‰æœªè§è¯­è¨€å’Œè¯­éŸ³ç¤¾ä¼šå·®å¼‚çš„æ–°å‹è¯„ä¼°é›†ã€‚å€ŸåŠ©å¤§è§„æ¨¡è®­ç»ƒæ•°æ®ï¼ŒZIPAåŒ…æ‹¬è½¬æ¢å™¨ï¼ˆZIPA-Tï¼‰å’ŒåŸºäºCTCçš„å˜ä½“ï¼ˆZIPA-CRï¼‰ï¼Œåˆ©ç”¨é«˜æ•ˆçš„Zipformerä¸»å¹²ï¼Œä»¥æ›´å°‘çš„å‚æ•°è¶…è¶Šç°æœ‰è¯­éŸ³è¯†åˆ«ç³»ç»Ÿã€‚é€šè¿‡å¯¹11,000å°æ—¶ä¼ªæ ‡è®°çš„å¤šè¯­è¨€æ•°æ®è¿›è¡Œå™ªå£°å­¦ç”Ÿè®­ç»ƒï¼Œå¯ä»¥å®ç°è¿›ä¸€æ­¥çš„æ‰©å±•å’Œæ”¹è¿›ã€‚è™½ç„¶ZIPAåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†è¯¯å·®åˆ†ææ˜¾ç¤ºï¼Œåœ¨æ¨¡æ‹Ÿç¤¾ä¼šè¯­éŸ³å¤šæ ·æ€§æ–¹é¢ä»å­˜åœ¨æŒç»­çš„å±€é™æ€§ï¼Œè¿™çªå‡ºäº†æœªæ¥ç ”ç©¶çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23170v1">PDF</a> ACL 2025 Main</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ZIPAè¿™ä¸€é«˜æ•ˆè¯­éŸ³æ¨¡å‹å®¶æ—ï¼Œå®ƒæé«˜äº†è·¨è¯­è¨€è¯­éŸ³è¯†åˆ«çš„æœ€æ–°æ€§èƒ½ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆæ¨å‡ºäº†IPAPack++ï¼Œä¸€ä¸ªå¤§å‹å¤šè¯­è¨€è¯­éŸ³è¯­æ–™åº“ï¼ŒåŒ…å«17,132å°æ—¶çš„æ ‡å‡†åŒ–è¯­éŸ³è½¬å½•å’Œæ•æ‰æœªè§è¯­è¨€å’Œç¤¾éŸ³å˜å¼‚çš„æ–°å‹è¯„ä¼°é›†ã€‚åˆ©ç”¨å¤§è§„æ¨¡è®­ç»ƒæ•°æ®ï¼ŒZIPAçš„è½¬æ¢å™¨ï¼ˆZIPA-Tï¼‰å’ŒCTCï¼ˆZIPA-CRï¼‰å˜ä½“å€ŸåŠ©é«˜æ•ˆçš„Zipformerä¸»å¹²ï¼Œä»¥æ›´å°‘çš„å‚æ•°è¡¨ç°è¶…è¿‡äº†ç°æœ‰è¯­éŸ³è¯†åˆ«ç³»ç»Ÿã€‚é€šè¿‡å¯¹è¶…è¿‡ä¸€ä¸‡å°æ—¶çš„ä¼ªæ ‡ç­¾å¤šè¯­è¨€æ•°æ®è¿›è¡Œå™ªå£°å­¦ç”Ÿè®­ç»ƒåè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚å°½ç®¡ZIPAåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†è¯¯å·®åˆ†æè¡¨æ˜å…¶åœ¨æ¨¡æ‹Ÿç¤¾ä¼šè¯­éŸ³å¤šæ ·æ€§æ–¹é¢ä»æœ‰å±€é™ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å¸¦æ¥æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZIPAæ˜¯å…ˆè¿›çš„è¯­éŸ³æ¨¡å‹å®¶æ—ï¼Œæé«˜äº†è·¨è¯­è¨€è¯­éŸ³è¯†åˆ«çš„æ€§èƒ½ã€‚</li>
<li>IPAPack++æ˜¯ä¸€ä¸ªå¤§å‹å¤šè¯­è¨€è¯­éŸ³è¯­æ–™åº“ï¼ŒåŒ…å«æ ‡å‡†åŒ–è¯­éŸ³è½¬å½•å’Œæ–°å‹è¯„ä¼°é›†ï¼Œç”¨äºæ•æ‰æœªè§è¯­è¨€å’Œç¤¾éŸ³å˜å¼‚ã€‚</li>
<li>ZIPAåŒ…æ‹¬è½¬æ¢å™¨ï¼ˆZIPA-Tï¼‰å’ŒCTCï¼ˆZIPA-CRï¼‰å˜ä½“ï¼Œåˆ©ç”¨Zipformerä¸»å¹²å®ç°é«˜æ•ˆæ€§èƒ½ã€‚</li>
<li>ZIPAåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œä½¿ç”¨æ›´å°‘çš„å‚æ•°ã€‚</li>
<li>é€šè¿‡å™ªå£°å­¦ç”Ÿè®­ç»ƒå’Œè¶…è¿‡ä¸€ä¸‡å°æ—¶çš„ä¼ªæ ‡ç­¾å¤šè¯­è¨€æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥æ€§èƒ½æå‡ã€‚</li>
<li>ZIPAåœ¨å»ºæ¨¡ç¤¾ä¼šè¯­éŸ³å¤šæ ·æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23170">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4bb93751e048231f35e3f3a430d84a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64f3f222c252111b166fdb875dcef29e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aec38b7de908900eeea06752a8f22ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d33352056d0ee4836674e81c6d8f72bd.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AISHELL-5-The-First-Open-Source-In-Car-Multi-Channel-Multi-Speaker-Speech-Dataset-for-Automatic-Speech-Diarization-and-Recognition"><a href="#AISHELL-5-The-First-Open-Source-In-Car-Multi-Channel-Multi-Speaker-Speech-Dataset-for-Automatic-Speech-Diarization-and-Recognition" class="headerlink" title="AISHELL-5: The First Open-Source In-Car Multi-Channel Multi-Speaker   Speech Dataset for Automatic Speech Diarization and Recognition"></a>AISHELL-5: The First Open-Source In-Car Multi-Channel Multi-Speaker   Speech Dataset for Automatic Speech Diarization and Recognition</h2><p><strong>Authors:Yuhang Dai, He Wang, Xingchen Li, Zihan Zhang, Shuiyuan Wang, Lei Xie, Xin Xu, Hongxiao Guo, Shaoji Zhang, Hui Bu, Wei Chen</strong></p>
<p>This paper delineates AISHELL-5, the first open-source in-car multi-channel multi-speaker Mandarin automatic speech recognition (ASR) dataset. AISHLL-5 includes two parts: (1) over 100 hours of multi-channel speech data recorded in an electric vehicle across more than 60 real driving scenarios. This audio data consists of four far-field speech signals captured by microphones located on each car door, as well as near-field signals obtained from high-fidelity headset microphones worn by each speaker. (2) a collection of 40 hours of real-world environmental noise recordings, which supports the in-car speech data simulation. Moreover, we also provide an open-access, reproducible baseline system based on this dataset. This system features a speech frontend model that employs speech source separation to extract each speakerâ€™s clean speech from the far-field signals, along with a speech recognition module that accurately transcribes the content of each individual speaker. Experimental results demonstrate the challenges faced by various mainstream ASR models when evaluated on the AISHELL-5. We firmly believe the AISHELL-5 dataset will significantly advance the research on ASR systems under complex driving scenarios by establishing the first publicly available in-car ASR benchmark. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†AISHELL-5ï¼Œè¿™æ˜¯é¦–ä¸ªå¼€æºçš„è½¦å†…å¤šé€šé“å¤šè¯´è¯è€…æ±‰è¯­è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ•°æ®é›†ã€‚AISHELL-5åŒ…æ‹¬ä¸¤éƒ¨åˆ†ï¼šï¼ˆ1ï¼‰åœ¨è¶…è¿‡60ä¸ªçœŸå®é©¾é©¶åœºæ™¯ä¸­ï¼Œåœ¨ç”µåŠ¨æ±½è½¦ä¸­å½•åˆ¶çš„è¶…è¿‡100å°æ—¶çš„å¤šé€šé“è¯­éŸ³æ•°æ®ã€‚è¯¥éŸ³é¢‘æ•°æ®åŒ…æ‹¬ç”±æ¯æ‰‡é—¨ä¸Šçš„éº¦å…‹é£æ•æ‰åˆ°çš„å››ä¸ªè¿œåœºè¯­éŸ³ä¿¡å·ï¼Œä»¥åŠæ¯ä¸ªå‘è¨€è€…ä½©æˆ´çš„é«˜ä¿çœŸå¤´æˆ´å¼éº¦å…‹é£è·å¾—çš„è¿‘åœºä¿¡å·ã€‚ï¼ˆ2ï¼‰40å°æ—¶çœŸå®ä¸–ç•Œç¯å¢ƒå™ªå£°å½•éŸ³é›†ï¼Œæ”¯æŒè½¦å†…è¯­éŸ³æ•°æ®çš„æ¨¡æ‹Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†åŸºäºæ­¤æ•°æ®é›†çš„å¼€æ”¾è®¿é—®ã€å¯é‡å¤ä½¿ç”¨çš„åŸºçº¿ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿå…·æœ‰è¯­éŸ³å‰ç«¯æ¨¡å‹ï¼Œé‡‡ç”¨è¯­éŸ³æºåˆ†ç¦»æŠ€æœ¯ä»è¿œåœºä¿¡å·ä¸­æå–æ¯ä¸ªå‘è¨€è€…çš„æ¸…æ´è¯­éŸ³ï¼Œä»¥åŠèƒ½å¤Ÿå‡†ç¡®è½¬å½•æ¯ä¸ªå•ç‹¬å‘è¨€è€…å†…å®¹çš„è¯­éŸ³è¯†åˆ«æ¨¡å—ã€‚å®éªŒç»“æœè¯æ˜äº†å„ç§ä¸»æµASRæ¨¡å‹åœ¨AISHELL-5ä¸Šæ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬åšä¿¡ï¼ŒAISHELL-5æ•°æ®é›†å°†é€šè¿‡å»ºç«‹é¦–ä¸ªå…¬å¼€å¯ç”¨çš„è½¦å†…ASRåŸºå‡†ï¼Œæå¤§åœ°æ¨åŠ¨å¤æ‚é©¾é©¶åœºæ™¯ä¸‹çš„ASRç³»ç»Ÿç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23036v1">PDF</a> 5 pages, 1 figures, 3 tables, accepted by InterSpeech 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†AISHELL-5æ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªå¼€æºçš„è½¦å†…å¤šé€šé“å¤šè¯´è¯è€…æ™®é€šè¯è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ•°æ®é›†ã€‚å®ƒåŒ…å«ä¸¤éƒ¨åˆ†ï¼šä¸€æ˜¯è¶…è¿‡100å°æ—¶çš„åœ¨ç”µåŠ¨æ±½è½¦ä¸­å½•åˆ¶çš„å¤šé€šé“è¯­éŸ³æ•°æ®ï¼Œæ¶‰åŠ60å¤šä¸ªçœŸå®é©¾é©¶åœºæ™¯ï¼›äºŒæ˜¯40å°æ—¶çš„çœŸå®ä¸–ç•Œç¯å¢ƒå™ªå£°å½•éŸ³ï¼Œæ”¯æŒè½¦å†…è¯­éŸ³æ•°æ®çš„æ¨¡æ‹Ÿã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€ä¸ªåŸºäºè¯¥æ•°æ®é›†çš„å¼€æ”¾å¯è®¿é—®çš„åŸºå‡†ç³»ç»Ÿï¼ŒåŒ…æ‹¬è¯­éŸ³å‰ç«¯æ¨¡å‹å’Œè¯­éŸ³è¯†åˆ«æ¨¡å—ã€‚è¯¥æ•°æ®é›†ä¸ºå¤æ‚é©¾é©¶åœºæ™¯ä¸‹çš„ASRç³»ç»Ÿç ”ç©¶å¸¦æ¥äº†é‡è¦æ¨è¿›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>AISHELL-5æ˜¯é¦–ä¸ªå¼€æºçš„è½¦å†…å¤šé€šé“å¤šè¯´è¯è€…æ™®é€šè¯è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«è¶…è¿‡100å°æ—¶çš„çœŸå®é©¾é©¶åœºæ™¯è¯­éŸ³æ•°æ®å’Œ40å°æ—¶çš„ç¯å¢ƒå™ªå£°å½•éŸ³ã€‚</li>
<li>æ•°æ®é›†æ”¯æŒè½¦å†…è¯­éŸ³æ•°æ®çš„æ¨¡æ‹Ÿã€‚</li>
<li>æä¾›äº†åŸºäºAISHELL-5æ•°æ®é›†çš„å¼€æ”¾å¯è®¿é—®çš„åŸºå‡†ç³»ç»Ÿã€‚</li>
<li>åŸºå‡†ç³»ç»ŸåŒ…æ‹¬è¯­éŸ³å‰ç«¯æ¨¡å‹å’Œè¯­éŸ³è¯†åˆ«æ¨¡å—ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œä¸»æµASRæ¨¡å‹åœ¨AISHELL-5ä¸Šé¢ä¸´æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23036">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dee01ff72dcbffa9b43a710a6e55abde.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-926631c1927da49e14fd191f06773bf0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a4417da738b3a80343a982db585dd6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ffda7902a8bdc39c76419b1fbaa0928.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f477ae897edf7459ecec2b1512621226.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="EmotionTalk-An-Interactive-Chinese-Multimodal-Emotion-Dataset-With-Rich-Annotations"><a href="#EmotionTalk-An-Interactive-Chinese-Multimodal-Emotion-Dataset-With-Rich-Annotations" class="headerlink" title="EmotionTalk: An Interactive Chinese Multimodal Emotion Dataset With Rich   Annotations"></a>EmotionTalk: An Interactive Chinese Multimodal Emotion Dataset With Rich   Annotations</h2><p><strong>Authors:Haoqin Sun, Xuechen Wang, Jinghua Zhao, Shiwan Zhao, Jiaming Zhou, Hui Wang, Jiabei He, Aobo Kong, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin</strong></p>
<p>In recent years, emotion recognition plays a critical role in applications such as human-computer interaction, mental health monitoring, and sentiment analysis. While datasets for emotion analysis in languages such as English have proliferated, there remains a pressing need for high-quality, comprehensive datasets tailored to the unique linguistic, cultural, and multimodal characteristics of Chinese. In this work, we propose \textbf{EmotionTalk}, an interactive Chinese multimodal emotion dataset with rich annotations. This dataset provides multimodal information from 19 actors participating in dyadic conversational settings, incorporating acoustic, visual, and textual modalities. It includes 23.6 hours of speech (19,250 utterances), annotations for 7 utterance-level emotion categories (happy, surprise, sad, disgust, anger, fear, and neutral), 5-dimensional sentiment labels (negative, weakly negative, neutral, weakly positive, and positive) and 4-dimensional speech captions (speaker, speaking style, emotion and overall). The dataset is well-suited for research on unimodal and multimodal emotion recognition, missing modality challenges, and speech captioning tasks. To our knowledge, it represents the first high-quality and versatile Chinese dialogue multimodal emotion dataset, which is a valuable contribution to research on cross-cultural emotion analysis and recognition. Additionally, we conduct experiments on EmotionTalk to demonstrate the effectiveness and quality of the dataset. It will be open-source and freely available for all academic purposes. The dataset and codes will be made available at: <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/EmotionTalk">https://github.com/NKU-HLT/EmotionTalk</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæƒ…æ„Ÿè¯†åˆ«åœ¨äººæœºäº¤äº’ã€å¿ƒç†å¥åº·ç›‘æµ‹å’Œæƒ…æ„Ÿåˆ†æç­‰é¢†åŸŸçš„åº”ç”¨ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è™½ç„¶è‹±è¯­æƒ…æ„Ÿåˆ†æçš„æ•°æ®é›†å·²ç»å¤§é‡æ¶Œç°ï¼Œä½†ä»è¿«åˆ‡éœ€è¦é’ˆå¯¹ä¸­æ–‡ç‹¬ç‰¹è¯­è¨€ã€æ–‡åŒ–å’Œå¤šæ¨¡æ€ç‰¹å¾çš„é«˜è´¨é‡ã€ç»¼åˆæ•°æ®é›†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œEmotionTalkâ€è¿™ä¸€äº¤äº’å¼ä¸­æ–‡å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®é›†ï¼ŒåŒ…å«ä¸°å¯Œçš„æ³¨é‡Šã€‚è¯¥æ•°æ®é›†æä¾›äº†æ¥è‡ª19åæ¼”å‘˜åœ¨äºŒå…ƒå¯¹è¯ç¯å¢ƒä¸­çš„è·¨æ¨¡æ€ä¿¡æ¯ï¼Œèåˆäº†å£°éŸ³ã€è§†è§‰å’Œæ–‡æœ¬æ¨¡å¼ã€‚å®ƒåŒ…å«23.6å°æ—¶çš„è¯­éŸ³ï¼ˆ19,250å¥è¯ï¼‰ï¼Œæ³¨é‡Šäº†7ä¸ªè¯è¯­çº§æƒ…æ„Ÿç±»åˆ«ï¼ˆå¿«ä¹ã€æƒŠè®¶ã€æ‚²ä¼¤ã€åŒæ¶ã€æ„¤æ€’ã€ææƒ§å’Œä¸­æ€§ï¼‰ï¼Œ5ç»´æƒ…æ„Ÿæ ‡ç­¾ï¼ˆè´Ÿé¢ã€è½»å¾®è´Ÿé¢ã€ä¸­æ€§ã€è½»å¾®æ­£é¢å’Œæ­£é¢ï¼‰ï¼Œä»¥åŠ4ç»´è¯­éŸ³å­—å¹•ï¼ˆè¯´è¯è€…ã€è¯´è¯é£æ ¼ã€æƒ…æ„Ÿå’Œæ€»ä½“è¯„ä»·ï¼‰ã€‚è¯¥æ•°æ®é›†é€‚ç”¨äºå•æ¨¡æ€å’Œå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ç ”ç©¶ã€ç¼ºå¤±æ¨¡æ€æŒ‘æˆ˜å’Œè¯­éŸ³å­—å¹•ä»»åŠ¡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªé«˜è´¨é‡ä¸”é€šç”¨çš„ä¸­æ–‡å¯¹è¯å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®é›†ï¼Œå¯¹è·¨æ–‡åŒ–æƒ…æ„Ÿåˆ†æå’Œè¯†åˆ«ç ”ç©¶åšå‡ºäº†å®è´µçš„è´¡çŒ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨EmotionTalkä¸Šè¿›è¡Œäº†å®éªŒï¼Œä»¥è¯æ˜æ•°æ®é›†çš„æœ‰æ•ˆæ€§å’Œè´¨é‡ã€‚è¯¥æ•°æ®é›†å°†å¼€æºå¹¶å…è´¹æä¾›æ‰€æœ‰å­¦æœ¯ç”¨é€”ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨ä»¥ä¸‹ç½‘å€æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/EmotionTalk">https://github.com/NKU-HLT/EmotionTalk</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23018v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºEmotionTalkçš„ä¸­æ–‡å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ª19åå‚ä¸è€…åœ¨å¯¹è¯ç¯å¢ƒä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬å£°éŸ³ã€è§†é¢‘å’Œæ–‡å­—ã€‚æ•°æ®é›†åŒ…å«ä¸°å¯Œçš„æ ‡æ³¨ä¿¡æ¯ï¼Œå¦‚æƒ…æ„Ÿç±»åˆ«ã€æƒ…æ„Ÿç»´åº¦å’Œè¯­éŸ³å­—å¹•ç­‰ã€‚è¯¥æ•°æ®é›†é€‚ç”¨äºå•æ¨¡æ€å’Œå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ç ”ç©¶ï¼Œç¼ºå¤±æ¨¡æ€æŒ‘æˆ˜å’Œè¯­éŸ³å­—å¹•ä»»åŠ¡ç­‰ã€‚è¿™æ˜¯é¦–ä¸ªé«˜è´¨é‡ã€å¤šåŠŸèƒ½çš„ä¸­æ–‡å¯¹è¯å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®é›†ï¼Œå¯¹è·¨æ–‡åŒ–æƒ…æ„Ÿåˆ†æå’Œè¯†åˆ«ç ”ç©¶å…·æœ‰å®è´µè´¡çŒ®ã€‚æ•°æ®é›†å°†å¼€æºå¹¶å…è´¹æä¾›ç»™æ‰€æœ‰å­¦æœ¯ç”¨é€”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EmotionTalkæ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸­æ–‡çš„äº¤äº’å¼å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®é›†ï¼ŒåŒ…å«å£°éŸ³ã€è§†é¢‘å’Œæ–‡å­—ç­‰å¤šæ¨¡æ€ä¿¡æ¯ã€‚</li>
<li>æ•°æ®é›†åŒ…å«æ¥è‡ª19åå‚ä¸è€…åœ¨å¯¹è¯ç¯å¢ƒä¸­çš„ä¸°å¯Œæ ‡æ³¨ä¿¡æ¯ã€‚</li>
<li>æ•°æ®é›†é€‚ç”¨äºå•æ¨¡æ€å’Œå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ç ”ç©¶ï¼Œå°¤å…¶é€‚åˆå¤„ç†ç¼ºå¤±æ¨¡æ€çš„æŒ‘æˆ˜ã€‚</li>
<li>æ•°æ®é›†åŒ…å«æƒ…æ„Ÿç±»åˆ«ã€æƒ…æ„Ÿç»´åº¦å’Œè¯­éŸ³å­—å¹•ç­‰å¤šä¸ªç»´åº¦çš„æ ‡æ³¨ä¿¡æ¯ã€‚</li>
<li>EmotionTalkæ˜¯é¦–ä¸ªé«˜è´¨é‡ã€å¤šåŠŸèƒ½çš„ä¸­æ–‡å¯¹è¯å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®é›†ï¼Œå¯¹è·¨æ–‡åŒ–æƒ…æ„Ÿåˆ†æå…·æœ‰å®è´µè´¡çŒ®ã€‚</li>
<li>æ•°æ®é›†å°†å¼€æºå¹¶å…è´¹æä¾›ç»™æ‰€æœ‰å­¦æœ¯ç”¨é€”ï¼Œæ–¹ä¾¿ç ”ç©¶è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23018">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3a4400a961f2324183687dea9ea5f054.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da46e6fee0874d1347830a4333e9940c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ad51a139fd96f0a97245e3c95822724.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="VietASR-Achieving-Industry-level-Vietnamese-ASR-with-50-hour-labeled-data-and-Large-Scale-Speech-Pretraining"><a href="#VietASR-Achieving-Industry-level-Vietnamese-ASR-with-50-hour-labeled-data-and-Large-Scale-Speech-Pretraining" class="headerlink" title="VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled   data and Large-Scale Speech Pretraining"></a>VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled   data and Large-Scale Speech Pretraining</h2><p><strong>Authors:Jianheng Zhuo, Yifan Yang, Yiwen Shao, Yong Xu, Dong Yu, Kai Yu, Xie Chen</strong></p>
<p>Automatic speech recognition (ASR) has made remarkable progress but heavily relies on large-scale labeled data, which is scarce for low-resource languages like Vietnamese. While existing systems such as Whisper, USM, and MMS achieve promising performance, their efficacy remains inadequate in terms of training costs, latency, and accessibility. To address these issues, we propose VietASR, a novel ASR training pipeline that leverages vast amounts of unlabeled data and a small set of labeled data. Through multi-iteration ASR-biased self-supervised learning on a large-scale unlabeled dataset, VietASR offers a cost-effective and practical solution for enhancing ASR performance. Experiments demonstrate that pre-training on 70,000-hour unlabeled data and fine-tuning on merely 50-hour labeled data yield a lightweight but powerful ASR model. It outperforms Whisper Large-v3 and commercial ASR systems on real-world data. Our code and models will be open-sourced to facilitate research in low-resource ASR. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ä¸¥é‡ä¾èµ–äºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®ï¼Œå¯¹äºåƒè¶Šå—è¯­è¿™æ ·çš„ä½èµ„æºè¯­è¨€ï¼Œè¿™äº›æ•°æ®æ˜¯ç¨€ç¼ºçš„ã€‚è™½ç„¶ç°æœ‰çš„ç³»ç»Ÿå¦‚Whisperã€USMå’ŒMMSè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è®­ç»ƒæˆæœ¬ã€å»¶è¿Ÿå’Œå¯è®¿é—®æ€§æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VietASRï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ASRè®­ç»ƒç®¡é“ï¼Œèƒ½å¤Ÿåˆ©ç”¨å¤§é‡çš„æ— æ ‡ç­¾æ•°æ®å’Œä¸€å°éƒ¨åˆ†æœ‰æ ‡ç­¾æ•°æ®ã€‚VietASRé€šè¿‡åœ¨å¤§è§„æ¨¡æ— æ ‡ç­¾æ•°æ®é›†ä¸Šè¿›è¡Œå¤šæ¬¡è¿­ä»£çš„ASRåå‘è‡ªç›‘ç£å­¦ä¹ ï¼Œä¸ºå¢å¼ºASRæ€§èƒ½æä¾›äº†ç»æµé«˜æ•ˆä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨7ä¸‡å°æ—¶çš„æ— æ ‡ç­¾æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»…åœ¨50å°æ—¶çš„æœ‰æ ‡ç­¾æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥ç”Ÿæˆä¸€ä¸ªè½»ä¾¿ä½†å¼ºå¤§çš„ASRæ¨¡å‹ã€‚å®ƒåœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„è¡¨ç°ä¼˜äºWhisper Large-v3å’Œå•†ä¸šASRç³»ç»Ÿã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å°†å¼€æºï¼Œä»¥ä¿ƒè¿›ä½èµ„æºASRçš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21527v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹è¶Šå—è¯­è¿™ç±»ä½èµ„æºè¯­è¨€å­˜åœ¨çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é—®é¢˜ï¼Œæå‡ºçš„ä¸€ç§æ–°å‹çš„ASRè®­ç»ƒç®¡é“VietASRã€‚VietASRåˆ©ç”¨å¤§é‡çš„æ— æ ‡ç­¾æ•°æ®å’Œå°‘é‡çš„æœ‰æ ‡ç­¾æ•°æ®ï¼Œé€šè¿‡å¤šè½®ASRåå‘çš„è‡ªç›‘ç£å­¦ä¹ ï¼Œä¸ºè§£å†³é«˜æˆæœ¬ã€å»¶è¿Ÿå’Œå¯è®¿é—®æ€§é—®é¢˜æä¾›äº†ç»æµå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨7ä¸‡å°æ—¶çš„æ— æ ‡ç­¾æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»…åœ¨50å°æ—¶çš„æœ‰æ ‡ç­¾æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå³å¯è·å¾—ä¸€ä¸ªè½»ä¾¿ä½†åŠŸèƒ½å¼ºå¤§çš„ASRæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨çœŸå®æ•°æ®ä¸Šçš„è¡¨ç°ä¼˜äºWhisper Large-v3å’Œå•†ç”¨ASRç³»ç»Ÿã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å°†å¼€æºï¼Œä»¥ä¿ƒè¿›ä½èµ„æºASRçš„ç ”ç©¶ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨ä½èµ„æºè¯­è¨€å¦‚è¶Šå—è¯­ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦å¤§è§„æ¨¡æœ‰æ ‡ç­¾æ•°æ®ã€‚</li>
<li>ç°æœ‰ç³»ç»Ÿå¦‚Whisperã€USMå’ŒMMSåœ¨è®­ç»ƒæˆæœ¬ã€å»¶è¿Ÿå’Œå¯è®¿é—®æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚<br>3.VietASRæ˜¯ä¸€ç§æ–°å‹çš„ASRè®­ç»ƒç®¡é“ï¼Œåˆ©ç”¨å¤§é‡çš„æ— æ ‡ç­¾æ•°æ®å’Œå°‘é‡çš„æœ‰æ ‡ç­¾æ•°æ®ã€‚</li>
<li>é€šè¿‡å¤šè½®ASRåå‘çš„è‡ªç›‘ç£å­¦ä¹ ï¼ŒVietASRæé«˜äº†ASRçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒVietASRåœ¨çœŸå®æ•°æ®ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–ç³»ç»Ÿã€‚<br>6.VietASRçš„ä»£ç å’Œæ¨¡å‹å°†å¼€æºï¼Œä»¥ä¿ƒè¿›ä½èµ„æºASRçš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76e47008f752518f39abc20fff4588dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82125c81a62731a4ec96c902a099f9af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2418633da7da69f31a5791d1366b9c54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c12f86026958972d49bc8c8800dad3a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Exploring-Spatiotemporal-Emotional-Synchrony-in-Dyadic-Interactions-The-Role-of-Speech-Conditions-in-Facial-and-Vocal-Affective-Alignment"><a href="#Exploring-Spatiotemporal-Emotional-Synchrony-in-Dyadic-Interactions-The-Role-of-Speech-Conditions-in-Facial-and-Vocal-Affective-Alignment" class="headerlink" title="Exploring Spatiotemporal Emotional Synchrony in Dyadic Interactions: The   Role of Speech Conditions in Facial and Vocal Affective Alignment"></a>Exploring Spatiotemporal Emotional Synchrony in Dyadic Interactions: The   Role of Speech Conditions in Facial and Vocal Affective Alignment</h2><p><strong>Authors:Von Ralph Dane Marquez Herbuela, Yukie Nagai</strong></p>
<p>Understanding how humans express and synchronize emotions across multiple communication channels particularly facial expressions and speech has significant implications for emotion recognition systems and human computer interaction. Motivated by the notion that non-overlapping speech promotes clearer emotional coordination, while overlapping speech disrupts synchrony, this study examines how these conversational dynamics shape the spatial and temporal alignment of arousal and valence across facial and vocal modalities. Using dyadic interactions from the IEMOCAP dataset, we extracted continuous emotion estimates via EmoNet (facial video) and a Wav2Vec2-based model (speech audio). Segments were categorized based on speech overlap, and emotional alignment was assessed using Pearson correlation, lag adjusted analysis, and Dynamic Time Warping (DTW). Across analyses, non overlapping speech was associated with more stable and predictable emotional synchrony than overlapping speech. While zero-lag correlations were low and not statistically different, non overlapping speech showed reduced variability, especially for arousal. Lag adjusted correlations and best-lag distributions revealed clearer, more consistent temporal alignment in these segments. In contrast, overlapping speech exhibited higher variability and flatter lag profiles, though DTW indicated unexpectedly tighter alignment suggesting distinct coordination strategies. Notably, directionality patterns showed that facial expressions more often preceded speech during turn-taking, while speech led during simultaneous vocalizations. These findings underscore the importance of conversational structure in regulating emotional communication and provide new insight into the spatial and temporal dynamics of multimodal affective alignment in real world interaction. </p>
<blockquote>
<p>ç†è§£äººç±»å¦‚ä½•åœ¨å¤šä¸ªæ²Ÿé€šæ¸ é“ï¼ˆå°¤å…¶æ˜¯é¢éƒ¨è¡¨æƒ…å’Œè¨€è¯­ï¼‰ä¸Šè¡¨è¾¾å’ŒåŒæ­¥æƒ…ç»ªï¼Œå¯¹äºæƒ…ç»ªè¯†åˆ«ç³»ç»Ÿå’Œäººæœºäº¤äº’æœ‰ç€é‡å¤§å¯ç¤ºã€‚æœ¬ç ”ç©¶å—åˆ°éé‡å æ€§è¨€è¯­èƒ½å¤Ÿä¿ƒè¿›æ›´æ¸…æ™°æƒ…æ„Ÿåè°ƒçš„è§‚å¿µçš„å¯å‘ï¼ŒåŒæ—¶é‡å æ€§è¨€è¯­ä¼šç ´ååŒæ­¥æ€§ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†è¿™äº›å¯¹è¯åŠ¨æ€å¦‚ä½•å½±å“é¢éƒ¨å’Œå£°éŸ³æ¨¡æ€çš„å…´å¥‹å’Œä»·å€¼çš„ç©ºé—´å’Œæ—¶é—´å¯¹é½ã€‚æˆ‘ä»¬ä½¿ç”¨äº†IEMOCAPæ•°æ®é›†ä¸­çš„äºŒå…ƒäº’åŠ¨ï¼Œé€šè¿‡EmoNetï¼ˆé¢éƒ¨è§†é¢‘ï¼‰å’ŒåŸºäºWav2Vec2çš„æ¨¡å‹ï¼ˆè¯­éŸ³éŸ³é¢‘ï¼‰æå–äº†è¿ç»­çš„æƒ…ç»ªä¼°è®¡å€¼ã€‚æ ¹æ®è¯­éŸ³é‡å æƒ…å†µå¯¹ç‰‡æ®µè¿›è¡Œåˆ†ç±»ï¼Œå¹¶ä½¿ç”¨Pearsonç›¸å…³æ€§ã€æ»åè°ƒæ•´åˆ†æå’ŒåŠ¨æ€æ—¶é—´å¼¯æ›²ï¼ˆDTWï¼‰æ¥è¯„ä¼°æƒ…æ„Ÿå¯¹é½æƒ…å†µã€‚åœ¨å„ç§åˆ†æä¸­ï¼Œéé‡å æ€§è¨€è¯­åœ¨æƒ…ç»ªåŒæ­¥æ–¹é¢è¡¨ç°å¾—æ›´ç¨³å®šå’Œå¯é¢„æµ‹ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œé‡å æ€§è¨€è¯­åˆ™è¡¨ç°å‡ºæ›´é«˜çš„å¯å˜æ€§å’Œè¾ƒå¹³å¦çš„æ»ååˆ†å¸ƒã€‚ç„¶è€Œï¼ŒDTWæ„å¤–åœ°æ˜¾ç¤ºå‡ºæ›´ç´§å¯†çš„å¯¹é½ï¼Œè¡¨æ˜å­˜åœ¨ä¸åŒçš„åè°ƒç­–ç•¥ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ–¹å‘æ€§æ¨¡å¼æ˜¾ç¤ºï¼Œåœ¨è½®æµå‘è¨€æ—¶ï¼Œé¢éƒ¨è¡¨æƒ…å¾€å¾€å…ˆäºè¨€è¯­ï¼Œè€Œåœ¨åŒæ—¶å‘å£°æ—¶ï¼Œåˆ™æ˜¯è¨€è¯­é¢†å…ˆã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¯¹è¯ç»“æ„åœ¨è°ƒèŠ‚æƒ…æ„Ÿæ²Ÿé€šä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºç°å®äº’åŠ¨ä¸­å¤šæ¨¡å¼æƒ…æ„Ÿå¯¹é½çš„ç©ºé—´å’Œæ—¶é—´åŠ¨æ€æä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13455v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†äººç±»åœ¨å¤šé€šé“æ²Ÿé€šä¸­å¦‚ä½•è¡¨è¾¾å’ŒåŒæ­¥æƒ…ç»ªï¼Œç‰¹åˆ«æ˜¯åœ¨é¢éƒ¨è¡¨æƒ…å’Œè¨€è¯­æ–¹é¢ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œéé‡å çš„è¯­éŸ³æœ‰åŠ©äºæ›´æ¸…æ™°çš„æƒ…æ„Ÿåè°ƒï¼Œè€Œé‡å çš„è¯­éŸ³åˆ™ä¼šç ´ååŒæ­¥æ€§ã€‚é€šè¿‡å¯¹IEMOCAPæ•°æ®é›†çš„åŒè¯­äº’åŠ¨è¿›è¡Œç ”ç©¶ï¼Œå‘ç°éé‡å è¯­éŸ³åœ¨æƒ…æ„ŸåŒæ­¥æ–¹é¢è¡¨ç°æ›´ç¨³å®šä¸”å¯é¢„æµ‹ã€‚é¢éƒ¨è¡¨æƒ…æ›´å¸¸ä¸»å¯¼è¨€è¯­è½¬æ¢æ—¶çš„æƒ…ç»ªè¡¨è¾¾ï¼Œè€ŒåŒæ—¶å‘å£°æ—¶è¯­éŸ³åˆ™æ›´ä¸ºä¸»å¯¼ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¯¹è¯ç»“æ„åœ¨æƒ…æ„Ÿæ²Ÿé€šä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºçœŸå®äº’åŠ¨ä¸­çš„å¤šæ¨¡æ€æƒ…æ„Ÿå¯¹é½çš„æ—¶ç©ºåŠ¨æ€æä¾›äº†æ–°çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»åœ¨å¤šé€šé“æ²Ÿé€šä¸­è¡¨è¾¾å’ŒåŒæ­¥æƒ…ç»ªçš„ç ”ç©¶å¯¹æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿå’Œäººæœºäº¤äº’æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>éé‡å çš„è¯­éŸ³æœ‰åŠ©äºæ›´æ¸…æ™°çš„æƒ…æ„Ÿåè°ƒï¼Œè€Œé‡å çš„è¯­éŸ³å¯èƒ½å¯¼è‡´æƒ…æ„ŸåŒæ­¥çš„æ··ä¹±ã€‚</li>
<li>é€šè¿‡IEMOCAPæ•°æ®é›†çš„ç ”ç©¶å‘ç°ï¼Œéé‡å è¯­éŸ³åœ¨æƒ…æ„ŸåŒæ­¥æ–¹é¢è¡¨ç°æ›´ç¨³å®šä¸”å¯é¢„æµ‹ã€‚</li>
<li>é¢éƒ¨è¡¨æƒ…åœ¨è¨€è¯­è½¬æ¢æ—¶æ›´å¸¸ä¸»å¯¼æƒ…ç»ªè¡¨è¾¾ï¼Œè€ŒåŒæ—¶å‘å£°æ—¶è¯­éŸ³ä¸ºä¸»å¯¼ã€‚</li>
<li>é›¶æ—¶é—´æ»åç›¸å…³æ€§ä½ï¼Œè€Œéé‡å è¯­éŸ³é™ä½äº†å˜å¼‚æ€§ï¼Œç‰¹åˆ«æ˜¯è§‰é†’åº¦æ–¹é¢ã€‚</li>
<li>ç ”ç©¶æ–¹å‘è¡¨æ˜å¯¹è¯ç»“æ„åœ¨æƒ…æ„Ÿæ²Ÿé€šä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13455">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6534b1cb12ba1ab5ef5d9f8b4327623.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e7bcee293262680c3989cd14efa2f22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6c4ba807c9419e5219bd6c8226d0943.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3ce6ba2d83fb87f738742a6918012c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45c7d33c09dcdbf10fb4f4c1fde958cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87d24130e63350bedd63d3b99af963e6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Nexus-An-Omni-Perceptive-And-Interactive-Model-for-Language-Audio-And-Vision"><a href="#Nexus-An-Omni-Perceptive-And-Interactive-Model-for-Language-Audio-And-Vision" class="headerlink" title="Nexus: An Omni-Perceptive And -Interactive Model for Language, Audio,   And Vision"></a>Nexus: An Omni-Perceptive And -Interactive Model for Language, Audio,   And Vision</h2><p><strong>Authors:Che Liu, Yingji Zhang, Dong Zhang, Weijie Zhang, Chenggong Gong, Haohan Li, Yu Lu, Shilin Zhou, Yue Lu, Ziliang Gan, Ziao Wang, Junwei Liao, Haipang Wu, Ji Liu, AndrÃ© Freitas, Qifan Wang, Zenglin Xu, Rongjuncheng Zhang, Yong Dai</strong></p>
<p>This work proposes an industry-level omni-modal large language model (LLM) pipeline that integrates auditory, visual, and linguistic modalities to overcome challenges such as limited tri-modal datasets, high computational costs, and complex feature alignments. Our pipeline consists of three main components: First, a modular framework enabling flexible configuration of various encoder-LLM-decoder architectures. Second, a lightweight training strategy that pre-trains audio-language alignment on the state-of-the-art vision-language model Qwen2.5-VL, thus avoiding the costly pre-training of vision-specific modalities. Third, an audio synthesis pipeline that generates high-quality audio-text data from diverse real-world scenarios, supporting applications such as Automatic Speech Recognition and Speech-to-Speech chat. To this end, we introduce an industry-level omni-modal LLM, Nexus. Extensive experiments validate the efficacy of our pipeline, yielding the following key findings:(1) In the visual understanding task, Nexus exhibits superior performance compared with its backbone model - Qwen2.5-VL-7B, validating the efficiency of our training strategy. (2) Within the English Spoken Question-Answering task, the model achieves better accuracy than the same-period competitor (i.e, MiniCPM-o2.6-7B) in the LLaMA Q. benchmark. (3) In our real-world ASR testset, Nexus achieves outstanding performance, indicating its robustness in real scenarios. (4) In the Speech-to-Text Translation task, our model outperforms Qwen2-Audio-Instruct-7B. (5) In the Text-to-Speech task, based on pretrained vocoder (e.g., Fishspeech1.4 or CosyVoice2.0), Nexus is comparable to its backbone vocoder on Seed-TTS benchmark. (6) An in-depth analysis of tri-modal alignment reveals that incorporating the audio modality enhances representational alignment between vision and language. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è·¨è¡Œä¸šçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç®¡é“ï¼Œè¯¥ç®¡é“èåˆäº†å¬è§‰ã€è§†è§‰å’Œè¯­è¨€å­¦æ¨¡æ€ï¼Œä»¥å…‹æœå¦‚æœ‰é™çš„ä¸‰æ¨¡æ€æ•°æ®é›†ã€é«˜è®¡ç®—æˆæœ¬å’Œå¤æ‚ç‰¹å¾å¯¹é½ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç®¡é“ä¸»è¦ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼šé¦–å…ˆï¼Œä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œèƒ½å¤Ÿçµæ´»é…ç½®å„ç§ç¼–ç å™¨-LLM-è§£ç å™¨æ¶æ„ã€‚å…¶æ¬¡ï¼Œä¸€ç§è½»é‡çº§çš„è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡å¯¹æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹Qwen2.5-VLè¿›è¡ŒéŸ³é¢‘è¯­è¨€å¯¹é½çš„é¢„è®­ç»ƒï¼Œä»è€Œé¿å…äº†é’ˆå¯¹ç‰¹å®šè§†è§‰æ¨¡æ€çš„æ˜‚è´µé¢„è®­ç»ƒã€‚æœ€åï¼Œä¸€ä¸ªéŸ³é¢‘åˆæˆç®¡é“ï¼Œä»å¤šæ ·åŒ–çš„ç°å®åœºæ™¯ä¸­ç”Ÿæˆé«˜è´¨é‡éŸ³é¢‘æ–‡æœ¬æ•°æ®ï¼Œæ”¯æŒå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³åˆ°è¯­éŸ³èŠå¤©ç­‰åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè·¨è¡Œä¸šçš„å¤šæ¨¡æ€LLMâ€”â€”Nexusã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬ç®¡é“çš„æœ‰æ•ˆæ€§ï¼Œå¹¶äº§ç”Ÿäº†ä»¥ä¸‹å…³é”®å‘ç°ï¼š(1)åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸­ï¼ŒNexusç›¸è¾ƒäºå…¶éª¨å¹²æ¨¡å‹Qwen2.5-VL-7Bå±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„è®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚(2)åœ¨è‹±è¯­å£è¯­é—®ç­”ä»»åŠ¡ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨LLaMA Q. benchmarkä¸Šçš„å‡†ç¡®ç‡é«˜äºåŒæœŸç«äº‰å¯¹æ‰‹ï¼ˆå³MiniCPM-o2.6-7Bï¼‰ã€‚(3)åœ¨æˆ‘ä»¬çš„ç°å®ASRæµ‹è¯•é›†ä¸­ï¼ŒNexusè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¡¨æ˜å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚(4)åœ¨è¯­éŸ³åˆ°æ–‡æœ¬ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°ä¼˜äºQwen2-Audio-Instruct-7Bã€‚(5)åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ä»»åŠ¡ä¸­ï¼ŒåŸºäºé¢„è®­ç»ƒçš„vocoderï¼ˆä¾‹å¦‚Fishspeech1.4æˆ–CosyVoice2.0ï¼‰ï¼ŒNexusåœ¨Seed-TTS benchmarkä¸Šçš„è¡¨ç°ä¸å…¶éª¨å¹²vocoderç›¸å½“ã€‚(6)å¯¹ä¸‰æ¨¡æ€å¯¹é½çš„æ·±å…¥åˆ†æè¡¨æ˜ï¼ŒåŠ å…¥éŸ³é¢‘æ¨¡æ€å¢å¼ºäº†è§†è§‰å’Œè¯­è¨€ä¹‹é—´çš„ä»£è¡¨æ€§å¯¹é½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01879v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è·¨è¡Œä¸šçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç®¡é“ï¼Œè¯¥ç®¡é“èåˆäº†å¬è§‰ã€è§†è§‰å’Œè¯­è¨€å­¦ä¸‰å¤§æ¨¡æ€ï¼Œä»¥å…‹æœå¦‚æœ‰é™çš„ä¸‰æ¨¡æ€æ•°æ®é›†ã€é«˜æ˜‚çš„è®¡ç®—æˆæœ¬å’Œå¤æ‚çš„ç‰¹å¾å¯¹é½ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ã€ä¸€ç§è½»é‡çº§è®­ç»ƒç­–ç•¥å’Œä¸€ä¸ªéŸ³é¢‘åˆæˆç®¡é“ã€‚æ¨¡å—åŒ–æ¡†æ¶å¯çµæ´»é…ç½®å„ç§ç¼–ç å™¨-LLM-è§£ç å™¨æ¶æ„ï¼›è½»é‡çº§è®­ç»ƒç­–ç•¥é€šè¿‡é¢„è®­ç»ƒéŸ³é¢‘è¯­è¨€å¯¹é½æ¥é¿å…æ˜‚è´µçš„è§†è§‰ç‰¹å®šæ¨¡æ€çš„é¢„è®­ç»ƒï¼›éŸ³é¢‘åˆæˆç®¡é“å¯ç”Ÿæˆé«˜è´¨é‡éŸ³é¢‘æ–‡æœ¬æ•°æ®ï¼Œæ”¯æŒå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³èŠå¤©ç­‰åº”ç”¨ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç®¡é“åœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬è§†è§‰ç†è§£ä»»åŠ¡ã€è‹±è¯­å£è¯­é—®ç­”ä»»åŠ¡ç­‰ã€‚è¯¥æ¨¡å‹çš„éŸ³é¢‘æ¨¡æ€èå…¥è¿›ä¸€æ­¥å¢å¼ºäº†è§†è§‰ä¸è¯­è¨€çš„ä»£è¡¨æ€§å¯¹é½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·¥ä½œæå‡ºäº†ä¸€ç§èåˆå¬è§‰ã€è§†è§‰å’Œè¯­è¨€ä¸‰å¤§æ¨¡æ€çš„è·¨è¡Œä¸šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç®¡é“ã€‚</li>
<li>æ¨¡å—åŒ–æ¡†æ¶æä¾›çµæ´»çš„æ¶æ„é…ç½®ã€‚</li>
<li>è½»é‡çº§è®­ç»ƒç­–ç•¥é€šè¿‡é¢„è®­ç»ƒéŸ³é¢‘è¯­è¨€å¯¹é½æ¥é™ä½æˆæœ¬å¹¶æé«˜æ•ˆç‡ã€‚</li>
<li>éŸ³é¢‘åˆæˆç®¡é“æ”¯æŒè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡éŸ³é¢‘æ–‡æœ¬æ•°æ®ï¼Œé€‚ç”¨äºå¤šç§åº”ç”¨åœºæ™¯ã€‚</li>
<li>åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸­ï¼ŒNexusæ¨¡å‹è¡¨ç°å‡ºæ¯”å…¶éª¨å¹²æ¨¡å‹æ›´ä¼˜çš„æ€§èƒ½ã€‚</li>
<li>åœ¨è‹±è¯­å£è¯­é—®ç­”ä»»åŠ¡ä¸­ï¼ŒNexusæ¨¡å‹åœ¨LLaMA Q. benchmarkä¸Šå®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01879">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e930c62cce7fe631c175d10d960d53d7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e47567a66fd0c62723052f4b05ead6ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb98e337962b4c98b584b1d2718b13e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d20fe4d48d94062be0b57cdbfcf57f5e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="FlexDuo-A-Pluggable-System-for-Enabling-Full-Duplex-Capabilities-in-Speech-Dialogue-Systems"><a href="#FlexDuo-A-Pluggable-System-for-Enabling-Full-Duplex-Capabilities-in-Speech-Dialogue-Systems" class="headerlink" title="FlexDuo: A Pluggable System for Enabling Full-Duplex Capabilities in   Speech Dialogue Systems"></a>FlexDuo: A Pluggable System for Enabling Full-Duplex Capabilities in   Speech Dialogue Systems</h2><p><strong>Authors:Borui Liao, Yulong Xu, Jiao Ou, Kaiyuan Yang, Weihua Jian, Pengfei Wan, Di Zhang</strong></p>
<p>Full-Duplex Speech Dialogue Systems (Full-Duplex SDS) have significantly enhanced the naturalness of human-machine interaction by enabling real-time bidirectional communication. However, existing approaches face challenges such as difficulties in independent module optimization and contextual noise interference due to highly coupled architectural designs and oversimplified binary state modeling. This paper proposes FlexDuo, a flexible full-duplex control module that decouples duplex control from spoken dialogue systems through a plug-and-play architectural design. Furthermore, inspired by human information-filtering mechanisms in conversations, we introduce an explicit Idle state. On one hand, the Idle state filters redundant noise and irrelevant audio to enhance dialogue quality. On the other hand, it establishes a semantic integrity-based buffering mechanism, reducing the risk of mutual interruptions while ensuring accurate response transitions. Experimental results on the Fisher corpus demonstrate that FlexDuo reduces the false interruption rate by 24.9% and improves response accuracy by 7.6% compared to integrated full-duplex dialogue system baselines. It also outperforms voice activity detection (VAD) controlled baseline systems in both Chinese and English dialogue quality. The proposed modular architecture and state-based dialogue model provide a novel technical pathway for building flexible and efficient duplex dialogue systems. </p>
<blockquote>
<p>å…¨åŒå·¥è¯­éŸ³å¯¹è¯ç³»ç»Ÿï¼ˆFull-Duplex SDSï¼‰é€šè¿‡å®ç°å®æ—¶åŒå‘é€šä¿¡ï¼Œæ˜¾è‘—å¢å¼ºäº†äººæœºäº¤äº’çš„è‡ªç„¶æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚å› æ¶æ„é«˜åº¦è€¦åˆå’Œè¿‡äºç®€åŒ–çš„äºŒå…ƒçŠ¶æ€å»ºæ¨¡è€Œå¯¼è‡´çš„ç‹¬ç«‹æ¨¡å—ä¼˜åŒ–å›°éš¾å’Œä¸Šä¸‹æ–‡å™ªå£°å¹²æ‰°ç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†FlexDuoï¼Œè¿™æ˜¯ä¸€ç§çµæ´»çš„å…¨åŒå·¥æ§åˆ¶æ¨¡å—ï¼Œå®ƒé€šè¿‡å³æ’å³ç”¨çš„æ¶æ„è®¾è®¡ï¼Œå°†åŒå·¥æ§åˆ¶ä»è¯­éŸ³å¯¹è¯ç³»ç»Ÿä¸­è§£è€¦å‡ºæ¥ã€‚æ­¤å¤–ï¼Œå—äººç±»å¯¹è¯ä¸­çš„ä¿¡æ¯è¿‡æ»¤æœºåˆ¶çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ˜ç¡®çš„ç©ºé—²çŠ¶æ€ã€‚ä¸€æ–¹é¢ï¼Œç©ºé—²çŠ¶æ€å¯ä»¥è¿‡æ»¤æ‰å†—ä½™çš„å™ªå£°å’Œæ— å…³çš„éŸ³é¢‘ï¼Œä»¥æé«˜å¯¹è¯è´¨é‡ã€‚å¦ä¸€æ–¹é¢ï¼Œå®ƒå»ºç«‹äº†ä¸€ç§åŸºäºè¯­ä¹‰å®Œæ•´æ€§çš„ç¼“å†²æœºåˆ¶ï¼Œé™ä½äº†ç›¸äº’å¹²æ‰°çš„é£é™©ï¼ŒåŒæ—¶ç¡®ä¿äº†å‡†ç¡®çš„å“åº”è½¬æ¢ã€‚åœ¨Fisherè¯­æ–™åº“ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸é›†æˆå…¨åŒå·¥å¯¹è¯ç³»ç»ŸåŸºçº¿ç›¸æ¯”ï¼ŒFlexDuoå°†é”™è¯¯ä¸­æ–­ç‡é™ä½äº†24.9%ï¼Œå“åº”å‡†ç¡®ç‡æé«˜äº†7.6%ã€‚ä¸è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆVADï¼‰æ§åˆ¶çš„åŸºçº¿ç³»ç»Ÿç›¸æ¯”ï¼Œå®ƒåœ¨ä¸­æ–‡å’Œè‹±æ–‡å¯¹è¯è´¨é‡æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚æ‰€æå‡ºçš„æ¨¡å—æ¶æ„å’ŒåŸºäºçŠ¶æ€çš„å¯¹è¯æ¨¡å‹ä¸ºæ„å»ºçµæ´»é«˜æ•ˆçš„å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿæä¾›äº†æ–°é¢–çš„æŠ€æœ¯é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13472v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å…¨åŒå·¥è¯­éŸ³å¯¹è¯ç³»ç»Ÿï¼ˆFull-Duplex SDSï¼‰é€šè¿‡å®ç°å®æ—¶åŒå‘é€šä¿¡ï¼Œæ˜¾è‘—æé«˜äº†äººæœºäº¤äº’çš„è‡ªç„¶æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ç‹¬ç«‹æ¨¡å—ä¼˜åŒ–å›°éš¾å’Œä¸Šä¸‹æ–‡å™ªå£°å¹²æ‰°ç­‰æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºé«˜åº¦è€¦åˆçš„æ¶æ„è®¾è®¡ä»¥åŠç®€åŒ–çš„äºŒå…ƒçŠ¶æ€å»ºæ¨¡ã€‚æœ¬æ–‡æå‡ºFlexDuoï¼Œä¸€ç§çµæ´»çš„å…¨åŒå·¥æ§åˆ¶æ¨¡å—ï¼Œé€šè¿‡å³æ’å³ç”¨æ¶æ„å®ç°ä¸å£è¯­å¯¹è¯ç³»ç»Ÿçš„è§£è€¦ã€‚æ­¤å¤–ï¼Œå—äººç±»å¯¹è¯ä¸­çš„ä¿¡æ¯è¿‡æ»¤æœºåˆ¶çš„å¯å‘ï¼Œå¼•å…¥äº†æ˜ç¡®çš„ç©ºé—²çŠ¶æ€ã€‚ä¸€æ–¹é¢ï¼Œç©ºé—²çŠ¶æ€è¿‡æ»¤å†—ä½™å™ªå£°å’Œæ— å…³éŸ³é¢‘ï¼Œæé«˜å¯¹è¯è´¨é‡ã€‚å¦ä¸€æ–¹é¢ï¼Œå®ƒå»ºç«‹åŸºäºè¯­ä¹‰å®Œæ•´æ€§çš„ç¼“å†²æœºåˆ¶ï¼Œé™ä½ç›¸äº’å¹²æ‰°çš„é£é™©ï¼ŒåŒæ—¶ç¡®ä¿å‡†ç¡®çš„å“åº”è½¬æ¢ã€‚åœ¨Fisherè¯­æ–™åº“ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFlexDuoä¸é›†æˆå…¨åŒå·¥å¯¹è¯ç³»ç»ŸåŸºçº¿ç›¸æ¯”ï¼Œè¯¯ä¸­æ–­ç‡é™ä½24.9%ï¼Œå“åº”å‡†ç¡®ç‡æé«˜7.6%ã€‚å®ƒè¿˜ä¼˜äºåŸºäºè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆVADï¼‰æ§åˆ¶çš„åŸºçº¿ç³»ç»Ÿåœ¨ä¸­æ–‡å’Œè‹±è¯­å¯¹è¯è´¨é‡æ–¹é¢çš„è¡¨ç°ã€‚æ‰€æå‡ºçš„æ¨¡å—åŒ–æ¶æ„å’ŒåŸºäºçŠ¶æ€çš„å¯¹è¯æ¨¡å‹ä¸ºæ„å»ºçµæ´»é«˜æ•ˆçš„å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿæä¾›äº†æ–°çš„æŠ€æœ¯é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨åŒå·¥è¯­éŸ³å¯¹è¯ç³»ç»Ÿå¢å¼ºäº†äººæœºäº¤äº’çš„è‡ªç„¶æ€§ï¼Œé€šè¿‡å®æ—¶åŒå‘é€šä¿¡å®ç°ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´ç‹¬ç«‹æ¨¡å—ä¼˜åŒ–å’Œä¸Šä¸‹æ–‡å™ªå£°å¹²æ‰°çš„æŒ‘æˆ˜ã€‚</li>
<li>FlexDuoé€šè¿‡å³æ’å³ç”¨æ¶æ„å®ç°ä¸å£è¯­å¯¹è¯ç³»ç»Ÿçš„è§£è€¦ï¼Œæå‡ºçµæ´»çš„å…¨åŒå·¥æ§åˆ¶æ¨¡å—ã€‚</li>
<li>å¼•å…¥æ˜ç¡®çš„ç©ºé—²çŠ¶æ€ï¼Œæé«˜å¯¹è¯è´¨é‡ï¼Œé™ä½è¯¯ä¸­æ–­é£é™©ã€‚</li>
<li>FlexDuoåœ¨Fisherè¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºåŸºçº¿ç³»ç»Ÿï¼Œè¯¯ä¸­æ–­ç‡é™ä½24.9%ï¼Œå“åº”å‡†ç¡®ç‡æé«˜7.6%ã€‚</li>
<li>FlexDuoåœ¨ä¸­æ–‡å’Œè‹±è¯­å¯¹è¯è´¨é‡æ–¹é¢ä¼˜äºåŸºäºè¯­éŸ³æ´»åŠ¨æ£€æµ‹æ§åˆ¶çš„åŸºçº¿ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13472">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a4a6eb6c4fe51204f386c94b27e061f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c60b4cf22ad8d888f909813ebe61ef41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b190aeb9d149262962deef8143a11cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58198fff3332edf6dec487c59fa126ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ef5c012ed5b10114d3273f246de710f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-02080de4c1a7688aab5bdddfd4b0c2b1.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  Beyond Face Swapping A Diffusion-Based Digital Human Benchmark for   Multimodal Deepfake Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-bc3f5ec4ede5eea62f56e6883d034cb7.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound   Tumor Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27083.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
