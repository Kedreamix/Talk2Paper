<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-05-31  FMG-Det Foundation Model Guided Robust Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6c6fb40fb9641761d614d821c391e191.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-31
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-02
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-31-更新"><a href="#2025-05-31-更新" class="headerlink" title="2025-05-31 更新"></a>2025-05-31 更新</h1><h2 id="FMG-Det-Foundation-Model-Guided-Robust-Object-Detection"><a href="#FMG-Det-Foundation-Model-Guided-Robust-Object-Detection" class="headerlink" title="FMG-Det: Foundation Model Guided Robust Object Detection"></a>FMG-Det: Foundation Model Guided Robust Object Detection</h2><p><strong>Authors:Darryl Hannan, Timothy Doster, Henry Kvinge, Adam Attarian, Yijing Watkins</strong></p>
<p>Collecting high quality data for object detection tasks is challenging due to the inherent subjectivity in labeling the boundaries of an object. This makes it difficult to not only collect consistent annotations across a dataset but also to validate them, as no two annotators are likely to label the same object using the exact same coordinates. These challenges are further compounded when object boundaries are partially visible or blurred, which can be the case in many domains. Training on noisy annotations significantly degrades detector performance, rendering them unusable, particularly in few-shot settings, where just a few corrupted annotations can impact model performance. In this work, we propose FMG-Det, a simple, efficient methodology for training models with noisy annotations. More specifically, we propose combining a multiple instance learning (MIL) framework with a pre-processing pipeline that leverages powerful foundation models to correct labels prior to training. This pre-processing pipeline, along with slight modifications to the detector head, results in state-of-the-art performance across a number of datasets, for both standard and few-shot scenarios, while being much simpler and more efficient than other approaches. </p>
<blockquote>
<p>收集高质量的对象检测任务数据是一个挑战，因为标注对象边界存在固有的主观性。这导致不仅在数据集上收集一致的注释变得困难，而且验证它们也变得困难，因为两个注释者不太可能使用完全相同的坐标来标注同一对象。当对象边界部分可见或模糊时，这些挑战会进一步加剧，这在许多领域都可能是这种情况。在有噪声的注释上进行训练会显著降低检测器的性能，使其无法使用，特别是在小样本设置中，几个损坏的注释就会影响模型性能。在本工作中，我们提出了FMG-Det，这是一种用于在有噪声的注释上训练模型简单高效的方法。更具体地说，我们提议将多实例学习（MIL）框架与预处理管道相结合，该管道利用强大的基础模型在训练之前校正标签。该预处理管道与检测器头部的轻微修改相结合，在多个数据集上实现了最先进的性能，无论是标准场景还是小样本场景，同时比其他方法更简单、更高效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23726v1">PDF</a> 10 pages, ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种简单高效的方法FMG-Det，用于在带有噪声标注的数据集上进行模型训练。该方法结合了多重实例学习（MIL）框架和一个预处理管道，利用强大的基础模型在训练前进行标签校正。此预处理管道与检测器头部的轻微修改相结合，可在多个数据集上实现标准与少样本场景的最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据收集是目标检测任务中的一大挑战，标注对象边界具有主观性，导致跨数据集难以获得一致的注释并对其进行验证。</li>
<li>当对象边界部分可见或模糊时，挑战进一步加大，这在许多领域都可能是常态。</li>
<li>噪声标注会对检测器性能造成严重影响，特别是在少样本场景下，几个被损坏的标注就会影响模型性能。</li>
<li>提出了一种新的方法FMG-Det，结合多重实例学习（MIL）框架和预处理管道来解决噪声标注问题。</li>
<li>预处理管道利用强大的基础模型在训练前进行标签校正，可以提高模型对各种数据集的处理能力。</li>
<li>FMG-Det在标准与少样本场景下的多个数据集上实现了最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23726">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-715bdd16bfc11b694a125733c819d50c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe342e3836a41af2a1bb1d144991d80b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30ab005fb4752a106260f87b3e5403dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c402c331f586503387de94acfb99428.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89971eccca49237ee148a68263449922.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Speech-Deepfake-Detection-Adaptation-with-Gaussian-Processes"><a href="#Few-Shot-Speech-Deepfake-Detection-Adaptation-with-Gaussian-Processes" class="headerlink" title="Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes"></a>Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes</h2><p><strong>Authors:Neta Glazer, David Chernin, Idan Achituve, Sharon Gannot, Ethan Fetaya</strong></p>
<p>Recent advancements in Text-to-Speech (TTS) models, particularly in voice cloning, have intensified the demand for adaptable and efficient deepfake detection methods. As TTS systems continue to evolve, detection models must be able to efficiently adapt to previously unseen generation models with minimal data. This paper introduces ADD-GP, a few-shot adaptive framework based on a Gaussian Process (GP) classifier for Audio Deepfake Detection (ADD). We show how the combination of a powerful deep embedding model with the Gaussian processes flexibility can achieve strong performance and adaptability. Additionally, we show this approach can also be used for personalized detection, with greater robustness to new TTS models and one-shot adaptability. To support our evaluation, a benchmark dataset is constructed for this task using new state-of-the-art voice cloning models. </p>
<blockquote>
<p>近期文本转语音（TTS）模型的进步，尤其是在语音克隆方面，进一步加剧了对自适应和高效深度伪造检测方法的需求。随着TTS系统的不断发展，检测模型必须能够以最小的数据量高效地适应以前未见过的生成模型。本文介绍了ADD-GP，这是一种基于高斯过程（GP）分类器的音频深度伪造检测（ADD）的少量自适应框架。我们展示了强大的深度嵌入模型与高斯过程的灵活性相结合如何实现强大的性能和适应性。此外，我们还展示了这种方法也可用于个性化检测，对新TTS模型的鲁棒性更强，并具有一次适应性。为了支持我们的评估，我们使用最新的语音克隆模型构建了用于此任务的标准数据集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23619v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期文本转语音（TTS）模型的进步，特别是语音克隆领域的发展，使得对适应性高且高效的深度伪造检测方法的呼声上升。文章提出了ADD-GP模型，该模型结合了高斯过程分类器和深度嵌入模型的特点，以实现高效而适应多变的音频深度伪造检测（ADD）。研究表明，ADD-GP框架具有良好的性能表现及对新模型的适应性，并支持个性化检测，尤其是对新出现的TTS模型的健壮性尤为突出，并能进行单例适应性。此外，该研究通过采用最新语音克隆模型构建基准数据集，对提出的方案进行了全面评估。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>TTS模型的最新进展推动了深度伪造检测方法的进化需求。</li>
<li>ADD-GP模型结合了高斯过程分类器和深度嵌入模型的优势。</li>
<li>ADD-GP框架表现出强大的性能和适应性，尤其是对新出现的TTS模型的健壮性突出。</li>
<li>该方法支持个性化检测并具有单例适应性。</li>
<li>采用最新语音克隆模型构建的基准数据集用于评估该方法的性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23619">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9c5f0ee5a90d319251cbd329d021d84d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c888462649dbd9a9addbc91e785591d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84d3b51910bf7d5a63c9a9661966703f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Material-Recognition-from-Local-Appearance"><a href="#Hierarchical-Material-Recognition-from-Local-Appearance" class="headerlink" title="Hierarchical Material Recognition from Local Appearance"></a>Hierarchical Material Recognition from Local Appearance</h2><p><strong>Authors:Matthew Beveridge, Shree K. Nayar</strong></p>
<p>We introduce a taxonomy of materials for hierarchical recognition from local appearance. Our taxonomy is motivated by vision applications and is arranged according to the physical traits of materials. We contribute a diverse, in-the-wild dataset with images and depth maps of the taxonomy classes. Utilizing the taxonomy and dataset, we present a method for hierarchical material recognition based on graph attention networks. Our model leverages the taxonomic proximity between classes and achieves state-of-the-art performance. We demonstrate the model’s potential to generalize to adverse, real-world imaging conditions, and that novel views rendered using the depth maps can enhance this capability. Finally, we show the model’s capacity to rapidly learn new materials in a few-shot learning setting. </p>
<blockquote>
<p>我们介绍了一种用于从局部外观进行层次识别的材料分类。我们的分类学受视觉应用的启发，根据材料的物理特征进行排列。我们贡献了一个包含分类图像和深度图的多样化野外数据集。利用分类和数据集，我们提出了一种基于图注意力网络的层次材料识别方法。我们的模型利用类之间的分类邻近性，实现了最先进的性能。我们证明了该模型在恶劣的、真实世界的成像条件下具有推广潜力，以及使用深度图呈现的新视角可以增强这一能力。最后，我们展示了该模型在少量学习环境中快速学习新材料的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22911v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于局部外观的用于层次识别的材料分类体系。该分类体系以视觉应用为动机，根据材料的物理特征进行排列。文章贡献了一个包含分类类别图像和深度图的野外数据集，并提出了一种基于图注意力网络的方法来进行层次材料识别。该方法利用类别之间的分类接近性，实现了最先进的性能。文章展示了该模型在恶劣现实成像条件下的泛化潜力，以及使用深度图渲染的新视角增强此能力的可能性。最后，展示了该模型在少量学习场景中的快速学习新材料的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种基于视觉应用的材料分类体系，该体系根据材料的物理特征进行分类。</li>
<li>提供了一个包含图像和深度图的野外数据集，用于层次材料识别研究。</li>
<li>提出了一种基于图注意力网络的层次材料识别方法，该方法利用类别之间的分类接近性。</li>
<li>该模型在恶劣现实成像条件下具有良好的泛化能力。</li>
<li>使用深度图渲染的新视角可以增强模型的识别能力。</li>
<li>模型具有快速学习新材料的能力，在少量学习场景中表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22911">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b4379d00842132fb989f4296bb98cc47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e54ed452c901b8dabf804d9cf39e9a12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75af70afb0c744287fdb407193bfcd29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e6dac87a7c92374ba073039029d16d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c6fb40fb9641761d614d821c391e191.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Sparseformer-a-Transferable-Transformer-with-Multi-granularity-Token-Sparsification-for-Medical-Time-Series-Classification"><a href="#Sparseformer-a-Transferable-Transformer-with-Multi-granularity-Token-Sparsification-for-Medical-Time-Series-Classification" class="headerlink" title="Sparseformer: a Transferable Transformer with Multi-granularity Token   Sparsification for Medical Time Series Classification"></a>Sparseformer: a Transferable Transformer with Multi-granularity Token   Sparsification for Medical Time Series Classification</h2><p><strong>Authors:Jiexia Ye, Weiqi Zhang, Ziyue Li, Jia Li, Fugee Tsung</strong></p>
<p>Medical time series (MedTS) classification is crucial for improved diagnosis in healthcare, and yet it is challenging due to the varying granularity of patterns, intricate inter-channel correlation, information redundancy, and label scarcity. While existing transformer-based models have shown promise in time series analysis, they mainly focus on forecasting and fail to fully exploit the distinctive characteristics of MedTS data. In this paper, we introduce Sparseformer, a transformer specifically designed for MedTS classification. We propose a sparse token-based dual-attention mechanism that enables global modeling and token compression, allowing dynamic focus on the most informative tokens while distilling redundant features. This mechanism is then applied to the multi-granularity, cross-channel encoding of medical signals, capturing intra- and inter-granularity correlations and inter-channel connections. The sparsification design allows our model to handle heterogeneous inputs of varying lengths and channels directly. Further, we introduce an adaptive label encoder to address label space misalignment across datasets, equipping our model with cross-dataset transferability to alleviate the medical label scarcity issue. Our model outperforms 12 baselines across seven medical datasets under supervised learning. In the few-shot learning experiments, our model also achieves superior average results. In addition, the in-domain and cross-domain experiments among three diagnostic scenarios demonstrate our model’s zero-shot learning capability. Collectively, these findings underscore the robustness and transferability of our model in various medical applications. </p>
<blockquote>
<p>医疗时间序列（MedTS）分类对于改进医疗保健中的诊断至关重要，然而由于模式的不同粒度、复杂的跨通道相关性、信息冗余和标签稀缺，它仍然是一个挑战。尽管现有的基于变压器的模型在时间序列分析中显示出希望，但它们主要侧重于预测，未能充分利用MedTS数据的特征。在本文中，我们介绍了Sparseformer，这是一个专为MedTS分类设计的变压器。我们提出了一种基于稀疏标记的双重注意力机制，能够实现全局建模和标记压缩，允许动态关注最具信息的标记，同时提炼冗余特征。然后，该机制应用于医疗信号的多粒度、跨通道编码，捕获粒度和跨粒度关联以及跨通道连接。稀疏化设计允许我们的模型直接处理不同长度和通道的异构输入。此外，我们引入了一种自适应标签编码器来解决数据集之间标签空间的不对齐问题，为我们的模型配备跨数据集的可转移性，以缓解医疗标签稀缺问题。我们的模型在七个医疗数据集的监督学习下超越了12个基准测试。在少量学习实验中，我们的模型也取得了优越的平均结果。此外，三个诊断场景中的域内和跨域实验证明了我们的模型的零样本学习能力。总的来说，这些发现凸显了我们的模型在各种医疗应用中的稳健性和可转移性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15578v2">PDF</a> 3 figures, 16 pages, 5 tables</p>
<p><strong>Summary</strong></p>
<p>本文提出了一个专为医疗时间序列分类设计的Sparseformer模型。该模型采用稀疏标记的基于双注意力机制，实现全局建模和标记压缩，能够动态关注最具信息量的标记并过滤冗余特征。该模型应用于医疗信号的跨粒度、跨通道编码，捕捉内部和外部粒度的关联和通道间的连接。其稀疏设计可处理不同长度和通道的异质输入。此外，为解决标签空间跨数据集的对齐问题，模型引入了自适应标签编码器，赋予了其跨数据集迁移能力，以缓解医疗标签稀缺的问题。Sparseformer模型在七个医疗数据集上的监督学习表现优于十二个基线模型。在少量的学习中，我们的模型也取得了优异的平均结果。同时，跨三种诊断场景的实验表明，该模型还具备零样本学习能力。这些结果突显了模型在各种医疗应用中的稳健性和迁移能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sparseformer是一个专为医疗时间序列分类设计的变压器模型。</li>
<li>模型采用稀疏标记的基于双注意力机制，实现全局建模和标记压缩。</li>
<li>该模型能够处理不同长度和通道的异质输入。</li>
<li>引入自适应标签编码器以解决跨数据集的标签空间对齐问题。</li>
<li>Sparseformer在多个医疗数据集上的监督学习表现优于其他模型。</li>
<li>在少量学习场景中，Sparseformer也表现出优异的性能。</li>
<li>模型具备零样本学习能力，突显了其稳健性和迁移能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15578">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-667e036121a176284c82f5d70d6a102b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-012925f43e5abf60ba4981e8dae2e871.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Do-we-still-need-Human-Annotators-Prompting-Large-Language-Models-for-Aspect-Sentiment-Quad-Prediction"><a href="#Do-we-still-need-Human-Annotators-Prompting-Large-Language-Models-for-Aspect-Sentiment-Quad-Prediction" class="headerlink" title="Do we still need Human Annotators? Prompting Large Language Models for   Aspect Sentiment Quad Prediction"></a>Do we still need Human Annotators? Prompting Large Language Models for   Aspect Sentiment Quad Prediction</h2><p><strong>Authors:Nils Constantin Hellwig, Jakob Fehle, Udo Kruschwitz, Christian Wolff</strong></p>
<p>Aspect sentiment quad prediction (ASQP) facilitates a detailed understanding of opinions expressed in a text by identifying the opinion term, aspect term, aspect category and sentiment polarity for each opinion. However, annotating a full set of training examples to fine-tune models for ASQP is a resource-intensive process. In this study, we explore the capabilities of large language models (LLMs) for zero- and few-shot learning on the ASQP task across five diverse datasets. We report F1 scores almost up to par with those obtained with state-of-the-art fine-tuned models and exceeding previously reported zero- and few-shot performance. In the 20-shot setting on the Rest16 restaurant domain dataset, LLMs achieved an F1 score of 51.54, compared to 60.39 by the best-performing fine-tuned method MVP. Additionally, we report the performance of LLMs in target aspect sentiment detection (TASD), where the F1 scores were close to fine-tuned models, achieving 68.93 on Rest16 in the 30-shot setting, compared to 72.76 with MVP. While human annotators remain essential for achieving optimal performance, LLMs can reduce the need for extensive manual annotation in ASQP tasks. </p>
<blockquote>
<p>面向方面的情感四元预测（ASQP）通过识别每个意见的观点词、方面词、方面类别和情感极性，从而促进对文本中所表达意见的深度理解。然而，为ASQP任务标注全套训练样本以微调模型是一个资源密集型的流程。在本研究中，我们探索大型语言模型（LLM）在五个不同数据集上执行ASQP任务的零样本学习和少样本学习能力。我们报告的F1分数几乎与最先进经过微调模型的F1分数持平，并且超过了先前报告的零样本和少样本性能。在Rest16餐厅域数据集的20次实验中，LLMs取得了F1分数为51.54的分数，而表现最佳的经过训练的MVP方法取得的分数为60.39。此外，我们报告了目标方面情感检测（TASD）中LLMs的性能表现。在Rest16数据集上的多项实验中，其F1分数接近经过训练的模型，在30次实验中取得了68.93的分数，而MVP取得了的分数为72.76。虽然人类注释器对于实现最佳性能至关重要，但LLMs可以减少ASQP任务中对大量手动标注的需求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13044v3">PDF</a> </p>
<p><strong>Summary</strong>:</p>
<p>大型语言模型（LLMs）在零样本和少样本学习方面展现出强大的能力，用于执行面向方面的情感四重预测（ASQP）任务。本研究展示了LLMs在五个不同数据集上的性能，并报告了在近乎小样本数据下的成绩已接近前沿微调模型。虽然LLMs在某些方面表现不如最佳微调模型，但它们显著减少了人工标注的需求。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>大型语言模型（LLMs）具备零样本和少样本学习的能力，可以在面向方面的情感四重预测（ASQP）任务上展现强大的性能。</li>
<li>在五个不同的数据集上进行了实验，并报告了近乎小样本数据下的性能表现。</li>
<li>LLMs的F1得分与最前沿的微调模型相当，并且在某些场景下超越了以往的零样本和少样本性能记录。</li>
<li>在Rest16餐厅领域数据集上进行的20次小样本实验中，LLMs的F1得分为51.54，而最佳微调模型MVP的得分为60.39。</li>
<li>LLMs在目标方面情感检测（TASD）任务中的性能也接近微调模型，在Rest16数据集上的30次小样本实验中，其F1得分为68.93，而MVP的得分为72.76。</li>
<li>虽然人工标注对于实现最佳性能仍然至关重要，但LLMs显著减少了面向方面的情感预测任务中对大量人工标注的需求。</li>
<li>此研究证实了大型语言模型在解决自然语言处理任务时的潜在应用，尤其是资源有限的情况下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13044">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-839e79d9c16dd00f4d4a96f762c17a03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f45479c983dba00127f197503deb6840.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d0ee2ac98174f58ecfcb13ac3233c21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c827ad25f82f2878b3abc79a21649a99.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CVOCSemRPL-Class-Variance-Optimized-Clustering-Semantic-Information-Injection-and-Restricted-Pseudo-Labeling-based-Improved-Semi-Supervised-Few-Shot-Learning"><a href="#CVOCSemRPL-Class-Variance-Optimized-Clustering-Semantic-Information-Injection-and-Restricted-Pseudo-Labeling-based-Improved-Semi-Supervised-Few-Shot-Learning" class="headerlink" title="CVOCSemRPL: Class-Variance Optimized Clustering, Semantic Information   Injection and Restricted Pseudo Labeling based Improved Semi-Supervised   Few-Shot Learning"></a>CVOCSemRPL: Class-Variance Optimized Clustering, Semantic Information   Injection and Restricted Pseudo Labeling based Improved Semi-Supervised   Few-Shot Learning</h2><p><strong>Authors:Souvik Maji, Rhythm Baghel, Pratik Mazumder</strong></p>
<p>Few-shot learning has been extensively explored to address problems where the amount of labeled samples is very limited for some classes. In the semi-supervised few-shot learning setting, substantial quantities of unlabeled samples are available. Such unlabeled samples are generally cheaper to obtain and can be used to improve the few-shot learning performance of the model. Some of the recent methods for this setting rely on clustering to generate pseudo-labels for the unlabeled samples. Since the effectiveness of clustering heavily influences the labeling of the unlabeled samples, it can significantly affect the few-shot learning performance. In this paper, we focus on improving the representation learned by the model in order to improve the clustering and, consequently, the model performance. We propose an approach for semi-supervised few-shot learning that performs a class-variance optimized clustering coupled with a cluster separation tuner in order to improve the effectiveness of clustering the labeled and unlabeled samples in this setting. It also optimizes the clustering-based pseudo-labeling process using a restricted pseudo-labeling approach and performs semantic information injection in order to improve the semi-supervised few-shot learning performance of the model. We experimentally demonstrate that our proposed approach significantly outperforms recent state-of-the-art methods on the benchmark datasets. </p>
<blockquote>
<p>小样本学习已经被广泛探索，以解决某些类别的标注样本数量非常有限的问题。在半监督小样本学习环境中，存在大量的未标注样本。这些未标注样本通常更容易获取，并可用于提高模型的少样本学习性能。最近的一些方法依赖于聚类来为未标注样本生成伪标签。由于聚类的有效性对未标注样本的标注有重要影响，因此它可能会显著影响小样本学习的性能。在本文中，我们专注于改进模型所学的表示，以提高聚类能力和模型性能。我们提出了一种半监督小样本学习方法，采用类方差优化聚类与聚类分离调节器相结合，以提高该环境中带标签和无标签样本的聚类效果。它还通过限制伪标签方法和执行语义信息注入来优化基于聚类的伪标签过程，以提高模型的半监督小样本学习性能。实验证明，我们提出的方法在基准数据集上显著优于最新的先进方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.14401v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了半监督小样本学习问题，其中涉及对模型表示的改进以提高聚类效果。文章提出了一种半监督小样本学习方法，通过类方差优化聚类和集群分离调节器改进了聚类效果。同时，通过限制伪标签方法和语义信息注入优化了基于聚类的伪标签过程。实验证明，该方法在基准数据集上显著优于现有最新方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章集中在改进模型表示以改善半监督小样本学习中的聚类效果。</li>
<li>提出了一种新的半监督小样本学习方法，包括类方差优化聚类和集群分离调节器。</li>
<li>限制伪标签方法和语义信息注入技术用于优化基于聚类的伪标签过程。</li>
<li>方法在基准数据集上显著优于现有最新方法。</li>
<li>未标注样本在半监督小样本学习中扮演重要角色，可以利用这些样本提升模型的性能。</li>
<li>聚类方法的优化直接影响模型对小样本学习的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.14401">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f8ea77c3826e1d1b2a731d759d490973.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7ce8092401c029b65b68962884922da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54baa5af340fa404f693103841d5a7c9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Re-ranking-Using-Large-Language-Models-for-Mitigating-Exposure-to-Harmful-Content-on-Social-Media-Platforms"><a href="#Re-ranking-Using-Large-Language-Models-for-Mitigating-Exposure-to-Harmful-Content-on-Social-Media-Platforms" class="headerlink" title="Re-ranking Using Large Language Models for Mitigating Exposure to   Harmful Content on Social Media Platforms"></a>Re-ranking Using Large Language Models for Mitigating Exposure to   Harmful Content on Social Media Platforms</h2><p><strong>Authors:Rajvardhan Oak, Muhammad Haroon, Claire Jo, Magdalena Wojcieszak, Anshuman Chhabra</strong></p>
<p>Social media platforms utilize Machine Learning (ML) and Artificial Intelligence (AI) powered recommendation algorithms to maximize user engagement, which can result in inadvertent exposure to harmful content. Current moderation efforts, reliant on classifiers trained with extensive human-annotated data, struggle with scalability and adapting to new forms of harm. To address these challenges, we propose a novel re-ranking approach using Large Language Models (LLMs) in zero-shot and few-shot settings. Our method dynamically assesses and re-ranks content sequences, effectively mitigating harmful content exposure without requiring extensive labeled data. Alongside traditional ranking metrics, we also introduce two new metrics to evaluate the effectiveness of re-ranking in reducing exposure to harmful content. Through experiments on three datasets, three models and across three configurations, we demonstrate that our LLM-based approach significantly outperforms existing proprietary moderation approaches, offering a scalable and adaptable solution for harm mitigation. </p>
<blockquote>
<p>社交媒体平台利用机器学习和人工智能驱动的推荐算法来最大化用户参与度，这可能导致无意中接触到有害内容。当前的审核工作依赖于使用大量人工注释数据训练的分类器，面临着可扩展性和适应新形式的挑战。为了解决这些挑战，我们提出了一种在零样本和少样本环境下使用大型语言模型进行再排序的新方法。我们的方法动态地评估和重新排序内容序列，有效地减少接触有害内容的可能性，无需依赖大量的标记数据。除了传统的排名指标外，我们还引入了两种新指标来评估重新排序在减少接触有害内容方面的有效性。通过对三个数据集、三个模型以及三种配置的测试实验表明，我们基于大型语言模型的再排序方法显著优于现有的专有审核方法，为危害缓解提供了可扩展和可适应的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13977v3">PDF</a> Accepted to ACL 2025 Main Conference</p>
<p><strong>Summary</strong>：社交媒体平台利用机器学习和人工智能驱动的推荐算法来最大化用户参与度，这可能导致无意中接触到有害内容。为解决当前依赖大量人工标注数据训练的分类器在可扩展性和适应新形式危害方面的挑战，我们提出了一种利用大型语言模型（LLM）进行零样本和少样本设置的新型重新排序方法。该方法能够动态评估和重新排序内容序列，有效地减少有害内容的暴露，且无需大量标注数据。除了传统的排名指标外，我们还引入了两个新指标来评估重新排序在减少有害内容暴露方面的有效性。通过在三套数据集上进行的实验显示，基于LLM的方法显著优于现有的专有调节方法，为危害缓解提供了可扩展和可适应的解决方案。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>社交媒体平台利用机器学习和人工智能推荐算法最大化用户参与度，可能无意中暴露用户于有害内容。</li>
<li>当前的内容调节方法面临可扩展性和适应新危害形式的挑战。</li>
<li>提出一种基于大型语言模型（LLM）的重新排序方法，能在零样本和少样本设置下动态评估和重新排序内容序列。</li>
<li>该方法无需大量标注数据，即可有效减少有害内容的暴露。</li>
<li>除了传统排名指标外，还引入两个新指标来评估重新排序的效果。</li>
<li>实验表明，基于LLM的方法在减少有害内容暴露方面显著优于现有专有调节方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13977">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-096f58dd2249d0896c8e6c3991db5030.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60fedf0358b98bcc751591f4e1ecafba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-091996f8dd1accff063cd47e57f55673.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e79737b4fd9d9f7b0199330cd48b515b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="mOSCAR-A-Large-scale-Multilingual-and-Multimodal-Document-level-Corpus"><a href="#mOSCAR-A-Large-scale-Multilingual-and-Multimodal-Document-level-Corpus" class="headerlink" title="mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus"></a>mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus</h2><p><strong>Authors:Matthieu Futeral, Armel Zebaze, Pedro Ortiz Suarez, Julien Abadji, Rémi Lacroix, Cordelia Schmid, Rachel Bawden, Benoît Sagot</strong></p>
<p>Multimodal Large Language Models (mLLMs) are trained on a large amount of text-image data. While most mLLMs are trained on caption-like data only, Alayrac et al. (2022) showed that additionally training them on interleaved sequences of text and images can lead to the emergence of in-context learning capabilities. However, the dataset they used, M3W, is not public and is only in English. There have been attempts to reproduce their results but the released datasets are English-only. In contrast, current multilingual and multimodal datasets are either composed of caption-like only or medium-scale or fully private data. This limits mLLM research for the 7,000 other languages spoken in the world. We therefore introduce mOSCAR, to the best of our knowledge the first large-scale multilingual and multimodal document corpus crawled from the web. It covers 163 languages, 303M documents, 200B tokens and 1.15B images. We carefully conduct a set of filtering and evaluation steps to make sure mOSCAR is sufficiently safe, diverse and of good quality. We additionally train two types of multilingual model to prove the benefits of mOSCAR: (1) a model trained on a subset of mOSCAR and captioning data and (2) a model trained on captioning data only. The model additionally trained on mOSCAR shows a strong boost in few-shot learning performance across various multilingual image-text tasks and benchmarks, confirming previous findings for English-only mLLMs. The dataset is released under the Creative Commons CC BY 4.0 license and can be accessed here: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/oscar-corpus/mOSCAR">https://huggingface.co/datasets/oscar-corpus/mOSCAR</a> </p>
<blockquote>
<p>多模态大型语言模型（mLLMs）是在大量的文本图像数据上训练的。虽然大多数mLLM仅在类似标题的数据上进行训练，但Alayrac等人（2022）的研究表明，通过在文本和图像的交错序列上进行额外训练，可以产生上下文学习能力。然而，他们使用的数据集M3W并非公开且仅适用于英语。尽管有人试图复制他们的结果，但发布的数据集都是英语版本的。相比之下，当前的多语种和多模态数据集要么仅包含类似标题的数据，要么是中规模或完全私有数据。这限制了全球其他7,000多种语言的多模态语言模型（mLLM）研究。因此，我们推出了mOSCAR，据我们所知，它是首个从网络爬虫中获取的大规模多语种多模态文档语料库。它涵盖163种语言、3.03亿份文档、20亿个令牌和1.15亿张图像。我们精心进行了一系列过滤和评估步骤，以确保mOSCAR足够安全、多样且质量良好。此外，我们还训练了两种多语种模型来证明mOSCAR的效益：（1）一种在mOSCAR子集和描述数据上训练的模型，（2）仅用于描述数据训练的模型。经过mOSCAR额外训练的模型在各种多语种图像文本任务和基准测试上的少样本学习能力得到了极大的提升，这证实了之前针对英语mLLM的发现。该数据集是在创意共享CC BY 4.0许可证下发布的，可从以下链接访问：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/oscar-corpus/mOSCAR">https://huggingface.co/datasets/oscar-corpus/mOSCAR</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.08707v2">PDF</a> ACL 2025 (Findings)</p>
<p><strong>Summary</strong><br>     多模态大型语言模型（mLLM）训练时融合了海量的文本和图像数据。相较于仅使用描述性数据的训练方式，Alayrac等人展示了通过交替序列的文本和图像进行训练能够促使模型具备上下文学习能力。然而，其使用的数据集M3W并未公开且仅限于英语。尽管有尝试复制其成果，但发布的数据集同样仅限于英语。相较之下，现有的多语言多模态数据集要么是描述性的，要么是中等规模或封闭性的，这限制了全球其他七千种语言的mLLM研究。为此，推出mOSCAR这一据称首个大规模的多语言多模态文档语料库，涵盖163种语言、3.03亿文档、20亿词汇和1.1亿图像。经过严格的筛选和评估流程，确保语料库的优质和安全多样性。通过两种类型的多语种模型验证了mOSCAR的效益：在mOSCAR子集和描述数据上训练的模型以及在仅描述数据上训练的模型。额外在mOSCAR上训练的模型在多语种图像文本任务和基准测试中显示出强大的少样本学习能力提升，证实了英语mLLM的先前发现。该数据集以创意共享CC BY 4.0许可证发布，可在此访问：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/oscar-corpus/mOSCAR">huggingface.co&#x2F;datasets&#x2F;oscar-corpus&#x2F;mOSCAR</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>mOSCAR是首个大规模的多语言多模态文档语料库，涵盖多种语言。</li>
<li>mOSCAR包含来自网络的文本和图像数据，文档数量庞大且覆盖面广。</li>
<li>mOSCAR数据经过筛选和评估以确保质量、多样性和安全性。</li>
<li>通过多语种模型的训练验证了mOSCAR的效益，少样本学习能力有所提升。</li>
<li>mOSCAR数据集具有创意共享CC BY 4.0许可证，便于访问和使用。</li>
<li>mOSCAR的发布填补了多语言多模态数据集领域的空白，促进了全球其他语言的mLLM研究发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.08707">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6647e5320b8dd98467975775d648ecb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2eefdeab08719210bcbe76bc5d05bc68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b20d104d37f634bac6c1a9af4ebfe6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00aae31414b95a34930255e88fc19be6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e50da50a636af0e129f81aeb6ea33d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adf5cd4428b1b27e8052c84f816990f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e037b51bff2221027547761f6b4adffe.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-0f7d5ba4831bfe386f88647faff46233.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-05-31  Adaptive Spatial Augmentation for Semi-supervised Semantic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-49be2282d3d40772916925bc569017cb.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-05-31  ML-Agent Reinforcing LLM Agents for Autonomous Machine Learning   Engineering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19778.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
