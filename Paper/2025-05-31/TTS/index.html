<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-05-31  Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-cc771af2bdc4ffade361c5cfb5d7e578.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-31
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    25 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-31-更新"><a href="#2025-05-31-更新" class="headerlink" title="2025-05-31 更新"></a>2025-05-31 更新</h1><h2 id="Few-Shot-Speech-Deepfake-Detection-Adaptation-with-Gaussian-Processes"><a href="#Few-Shot-Speech-Deepfake-Detection-Adaptation-with-Gaussian-Processes" class="headerlink" title="Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes"></a>Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes</h2><p><strong>Authors:Neta Glazer, David Chernin, Idan Achituve, Sharon Gannot, Ethan Fetaya</strong></p>
<p>Recent advancements in Text-to-Speech (TTS) models, particularly in voice cloning, have intensified the demand for adaptable and efficient deepfake detection methods. As TTS systems continue to evolve, detection models must be able to efficiently adapt to previously unseen generation models with minimal data. This paper introduces ADD-GP, a few-shot adaptive framework based on a Gaussian Process (GP) classifier for Audio Deepfake Detection (ADD). We show how the combination of a powerful deep embedding model with the Gaussian processes flexibility can achieve strong performance and adaptability. Additionally, we show this approach can also be used for personalized detection, with greater robustness to new TTS models and one-shot adaptability. To support our evaluation, a benchmark dataset is constructed for this task using new state-of-the-art voice cloning models. </p>
<blockquote>
<p>近期文本转语音（TTS）模型的进步，特别是在语音克隆方面，加剧了对自适应和高效的深度伪造检测方法的需求。随着TTS系统的不断发展，检测模型必须能够以最小的数据量高效地适应以前未见过的生成模型。本文介绍了ADD-GP，这是一种基于高斯过程（GP）分类器的音频深度伪造检测（ADD）的少量自适应框架。我们展示了强大的深度嵌入模型与高斯过程的灵活性如何结合，以实现强大的性能和适应性。此外，我们还证明该方法也可用于个性化检测，对新TTS模型具有更强的鲁棒性，并具备一次适应性。为了支持我们的评估，我们使用了最新的语音克隆模型构建了一个用于此任务的标准数据集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23619v1">PDF</a> </p>
<p><strong>总结</strong><br>随着文本转语音（TTS）模型的进步，特别是语音克隆技术的发展，对灵活且高效的声音深度伪造检测方法的需求愈发强烈。本文介绍了一个基于高斯过程（GP）分类器的ADD-GP少数自适应框架，用于音频深度伪造检测（ADD）。结合强大的深度嵌入模型与高斯过程的灵活性，该框架可实现出色的性能和适应性。此外，该框架还可用于个性化检测，对新TTS模型具有更强的鲁棒性，并具有一次适应性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>TTS模型的最新进展要求检测模型具有快速适应新生成模型的能力。</li>
<li>ADD-GP框架结合了深度嵌入模型与高斯过程分类器，用于音频深度伪造检测。</li>
<li>该框架可实现强大的性能和适应性，尤其是针对新出现的TTS模型。</li>
<li>可以通过个性化检测增强框架的鲁棒性。</li>
<li>为支持评估，使用最新的语音克隆模型构建了针对此任务的标准数据集。</li>
<li>该框架具有一次适应性，即能够仅通过少量数据快速适应新的TTS模型。</li>
<li>这种自适应能力对于应对不断演变的TTS技术具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23619">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9c5f0ee5a90d319251cbd329d021d84d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c888462649dbd9a9addbc91e785591d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84d3b51910bf7d5a63c9a9661966703f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="EmergentTTS-Eval-Evaluating-TTS-Models-on-Complex-Prosodic-Expressiveness-and-Linguistic-Challenges-Using-Model-as-a-Judge"><a href="#EmergentTTS-Eval-Evaluating-TTS-Models-on-Complex-Prosodic-Expressiveness-and-Linguistic-Challenges-Using-Model-as-a-Judge" class="headerlink" title="EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,   Expressiveness, and Linguistic Challenges Using Model-as-a-Judge"></a>EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,   Expressiveness, and Linguistic Challenges Using Model-as-a-Judge</h2><p><strong>Authors:Ruskin Raj Manku, Yuzhi Tang, Xingjian Shi, Mu Li, Alex Smola</strong></p>
<p>Text-to-Speech (TTS) benchmarks often fail to capture how well models handle nuanced and semantically complex text. Building on $\textit{EmergentTTS}$, we introduce $\textit{EmergentTTS-Eval}$, a comprehensive benchmark covering six challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic complexity, complex pronunciation (e.g. URLs, formulas), and questions. Crucially, our framework automates both test-case generation and evaluation, making the benchmark easily extensible. Starting from a small set of human-written seed prompts, we iteratively extend them using LLMs to target specific structural, phonetic and prosodic challenges, resulting in 1,645 diverse test cases. Moreover, we employ a model-as-a-judge approach, using a Large Audio Language Model (LALM) to assess the speech across multiple dimensions such as expressed emotion, prosodic, intonational, and pronunciation accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems, such as 11Labs, Deepgram, and OpenAI’s 4o-mini-TTS, on EmergentTTS-Eval, demonstrating its ability to reveal fine-grained performance differences. Results show that the model-as-a-judge approach offers robust TTS assessment and a high correlation with human preferences. We open source the evaluation $\href{<a target="_blank" rel="noopener" href="https://github.com/boson-ai/EmergentTTS-Eval-public%7D%7Bcode%7D$">https://github.com/boson-ai/EmergentTTS-Eval-public}{code}$</a> and the $\href{<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/bosonai/EmergentTTS-Eval%7D%7Bdataset%7D$">https://huggingface.co/datasets/bosonai/EmergentTTS-Eval}{dataset}$</a>. </p>
<blockquote>
<p>文本转语音（TTS）基准测试通常无法捕捉模型处理微妙和语义复杂文本的能力。基于$\textit{EmergentTTS}$，我们推出了$\textit{EmergentTTS-Eval}$，这是一个涵盖六种具有挑战性TTS场景的全面基准测试：情感、副语言、外来词、句法复杂性、复杂发音（例如：URLs、公式）以及问题。我们的框架可以自动进行测试用例生成和评估，使得基准测试易于扩展。我们从一组人类写入的种子提示开始，通过迭代使用大型语言模型（LLMs）来针对特定的结构、语音和韵律挑战，从而生成了1645个多样化的测试用例。此外，我们采用模型作为法官的方法，使用大型音频语言模型（LALM）来评估语音的多个维度，如表达的情感、韵律、语调以及发音准确性等。我们在EmergentTTS-Eval上评估了先进的开源和专有TTS系统，如11Labs、Deepgram和OpenAI的4o-mini-TTS，展示了其揭示细微性能差异的能力。结果表明，模型作为法官的方法为TTS评估提供了稳健性，与人类偏好高度相关。我们公开了评估代码和数据集，分别位于$\href{<a target="_blank" rel="noopener" href="https://github.com/boson-ai/EmergentTTS-Eval-public%7D%7B%E6%AD%A4%E5%A4%84%7D$%E5%92%8C$/href%7Bhttps://huggingface.co/datasets/bosonai/EmergentTTS-Eval%7D%7B%E6%AD%A4%E5%A4%84%7D$%E3%80%82">https://github.com/boson-ai/EmergentTTS-Eval-public}{此处}$和$\href{https://huggingface.co/datasets/bosonai/EmergentTTS-Eval}{此处}$。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23009v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>新兴TTS评测框架研究旨在克服现有TTS基准测试无法准确评估模型处理细微复杂文本的问题。该研究引入了EmergentTTS-Eval评测框架，涵盖情感、副语言、外来词、句法复杂性、复杂发音（如网址、公式）和问题等六个挑战场景。该框架实现了测试案例生成和评估的自动化，增强了可扩展性。通过使用大型音频语言模型评估语音的各种维度（如表达情感、语调等），其性能在评估顶尖开源和专有TTS系统时得到了验证。此研究为评估TTS性能提供了新的视角。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>EmergentTTS-Eval是一个全面的TTS评测框架，解决了现有基准测试无法充分评估模型处理复杂文本的问题。</li>
<li>该框架涵盖了六个挑战性的TTS场景，包括情感、副语言等。</li>
<li>通过自动化测试案例生成和评估，提高了基准测试的效率和可扩展性。</li>
<li>利用大型音频语言模型（LALM）作为评委，从多个维度评估语音质量。</li>
<li>在顶尖开源和专有TTS系统上的实验验证了EmergentTTS-Eval的有效性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23009">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a72312e0a309768fd73d1e93aa176ccd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2460c9f26012128f1ddc2998ace58c68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8c91686bae7c54e61d4fce56ef8e1d4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LLM-Synth4KWS-Scalable-Automatic-Generation-and-Synthesis-of-Confusable-Data-for-Custom-Keyword-Spotting"><a href="#LLM-Synth4KWS-Scalable-Automatic-Generation-and-Synthesis-of-Confusable-Data-for-Custom-Keyword-Spotting" class="headerlink" title="LLM-Synth4KWS: Scalable Automatic Generation and Synthesis of Confusable   Data for Custom Keyword Spotting"></a>LLM-Synth4KWS: Scalable Automatic Generation and Synthesis of Confusable   Data for Custom Keyword Spotting</h2><p><strong>Authors:Pai Zhu, Quan Wang, Dhruuv Agarwal, Kurt Partridge</strong></p>
<p>Custom keyword spotting (KWS) allows detecting user-defined spoken keywords from streaming audio. This is achieved by comparing the embeddings from voice enrollments and input audio. State-of-the-art custom KWS models are typically trained contrastively using utterances whose keywords are randomly sampled from training dataset. These KWS models often struggle with confusing keywords, such as “blue” versus “glue”. This paper introduces an effective way to augment the training with confusable utterances where keywords are generated and grouped from large language models (LLMs), and speech signals are synthesized with diverse speaking styles from text-to-speech (TTS) engines. To better measure user experience on confusable KWS, we define a new northstar metric using the average area under DET curve from confusable groups (c-AUC). Featuring high scalability and zero labor cost, the proposed method improves AUC by 3.7% and c-AUC by 11.3% on the Speech Commands testing set. </p>
<blockquote>
<p>自定义关键词识别（KWS）允许从流式音频中检测用户定义的语音关键词。这是通过比较语音注册和输入音频的嵌入来实现的。目前最先进的自定义KWS模型通常使用随机采样自训练数据集的语音来进行对比训练。这些KWS模型经常在与易混淆关键词对抗时遇到困境，例如“蓝色”与“胶水”。本文介绍了一种通过使用大型语言模型（LLM）生成并组合关键词，并用文本到语音（TTS）引擎合成具有不同说话风格的语音信号来增强训练的有效方法。为了更好地衡量用户对易混淆KWS的体验，我们使用来自混淆组的DET曲线平均面积（c-AUC）定义了一个新的北极星指标。该方法具有高度的可扩展性和零人工成本的特点，在Speech Commands测试集上提高了AUC 3.7%和c-AUC 11.3%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22995v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了通过合成语音信号和大型语言模型（LLM）生成混淆关键词来增强对比式训练关键词识别模型的方法。通过引入新的评估指标c-AUC，该方法提高了模型的性能，提高了AUC值3.7%，同时改善了模型在混淆关键词上的表现，提升了c-AUC值达11.3%。此方法具有高可扩展性和零人工成本的优势。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>介绍了通过对比语音嵌入进行自定义关键词识别（KWS）的方法。</li>
<li>训练先进的KWS模型时，通常使用随机采样的关键词训练集进行对比训练。</li>
<li>现有模型面临混淆关键词的挑战，如”蓝色”与”胶水”。</li>
<li>提出了一种通过大型语言模型（LLM）生成并分组关键词的合成语音信号的训练增强方法。</li>
<li>使用文本到语音（TTS）引擎合成不同说话风格的语音信号。</li>
<li>为了更好地衡量用户对混淆KWS的体验，引入了使用混淆组平均检测曲线下的面积（c-AUC）作为新的评估指标。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22995">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-48ae05648e3fe3dceb7e121236488093.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc771af2bdc4ffade361c5cfb5d7e578.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d6815e70782be97b700c8e029b6ade4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86edc6acb54381c3609c0f1b94f6bf67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d710fdfce3494309b8f788a96526217.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be30fc842692a37e4963254acac68256.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="BinauralFlow-A-Causal-and-Streamable-Approach-for-High-Quality-Binaural-Speech-Synthesis-with-Flow-Matching-Models"><a href="#BinauralFlow-A-Causal-and-Streamable-Approach-for-High-Quality-Binaural-Speech-Synthesis-with-Flow-Matching-Models" class="headerlink" title="BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural   Speech Synthesis with Flow Matching Models"></a>BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural   Speech Synthesis with Flow Matching Models</h2><p><strong>Authors:Susan Liang, Dejan Markovic, Israel D. Gebru, Steven Krenn, Todd Keebler, Jacob Sandakly, Frank Yu, Samuel Hassel, Chenliang Xu, Alexander Richard</strong></p>
<p>Binaural rendering aims to synthesize binaural audio that mimics natural hearing based on a mono audio and the locations of the speaker and listener. Although many methods have been proposed to solve this problem, they struggle with rendering quality and streamable inference. Synthesizing high-quality binaural audio that is indistinguishable from real-world recordings requires precise modeling of binaural cues, room reverb, and ambient sounds. Additionally, real-world applications demand streaming inference. To address these challenges, we propose a flow matching based streaming binaural speech synthesis framework called BinauralFlow. We consider binaural rendering to be a generation problem rather than a regression problem and design a conditional flow matching model to render high-quality audio. Moreover, we design a causal U-Net architecture that estimates the current audio frame solely based on past information to tailor generative models for streaming inference. Finally, we introduce a continuous inference pipeline incorporating streaming STFT&#x2F;ISTFT operations, a buffer bank, a midpoint solver, and an early skip schedule to improve rendering continuity and speed. Quantitative and qualitative evaluations demonstrate the superiority of our method over SOTA approaches. A perceptual study further reveals that our model is nearly indistinguishable from real-world recordings, with a $42%$ confusion rate. </p>
<blockquote>
<p>双耳渲染旨在根据单声道音频和说话人及听众的位置合成模仿自然听觉的双耳音频。尽管已经提出了许多方法来解决这一问题，但它们在处理渲染质量和流式推断方面遇到了困难。合成高质量的双耳音频，要求与真实世界录音无法区分，需要对双耳线索、房间回音和环境声音进行精确建模。此外，实际应用要求流式推理。为了应对这些挑战，我们提出了一种基于流匹配的流式双耳语音合成框架，称为BinauralFlow。我们认为双耳渲染是一个生成问题，而不是一个回归问题，并设计了一个条件流匹配模型来渲染高质量音频。此外，我们设计了一种因果U-Net架构，该架构仅根据过去的信息来估计当前音频帧，以针对流式推断定制生成模型。最后，我们引入了一个连续的推理管道，结合了流式STFT&#x2F;ISTFT操作、缓冲区银行、中点解算器和早期跳过调度，以提高渲染的连续性和速度。定量和定性评估表明，我们的方法优于最新方法。感知研究进一步表明，我们的模型与真实世界录音几乎无法区分，混淆率为42%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22865v1">PDF</a> ICML 2025, 18 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于流的匹配流技术的双耳音频合成框架BinauralFlow。该方法将双耳渲染视为生成问题而非回归问题，通过设计条件流匹配模型来合成高质量音频。同时，采用因果U-Net架构实现流式推理，并引入连续推理管道提高渲染的连续性和速度。实验证明，该方法在音质和流式推理方面均优于现有方法，与真实录音的混淆率仅为42%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BinauralFlow是一个基于流的匹配流技术的双耳音频合成框架，旨在合成高质量的双耳音频。</li>
<li>该方法将双耳渲染视为生成问题，通过设计条件流匹配模型来合成高质量音频。</li>
<li>采用了因果U-Net架构来实现流式推理，适应实际应用的需求。</li>
<li>引入了连续推理管道，包括流式STFT&#x2F;ISTFT操作、缓冲区银行、中点解算器和早期跳过调度，以提高渲染的连续性和速度。</li>
<li>定量和定性评估表明，BinauralFlow在音质和流式推理方面均优于现有方法。</li>
<li>感知研究表明，该模型的音频与真实录音的混淆率仅为42%，难以区分。</li>
<li>BinauralFlow对于双耳音频合成领域的进步具有重要意义，有望在虚拟现实、增强现实等领域得到广泛应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22865">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-074cb3d13270b1e5e6900782debac0dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5be1b000686f050cf5e412ead331bb8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-246429498d2f202c576c7707358dc6d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8581dda8e2570b8a076e7b97c1517f29.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Nexus-An-Omni-Perceptive-And-Interactive-Model-for-Language-Audio-And-Vision"><a href="#Nexus-An-Omni-Perceptive-And-Interactive-Model-for-Language-Audio-And-Vision" class="headerlink" title="Nexus: An Omni-Perceptive And -Interactive Model for Language, Audio,   And Vision"></a>Nexus: An Omni-Perceptive And -Interactive Model for Language, Audio,   And Vision</h2><p><strong>Authors:Che Liu, Yingji Zhang, Dong Zhang, Weijie Zhang, Chenggong Gong, Haohan Li, Yu Lu, Shilin Zhou, Yue Lu, Ziliang Gan, Ziao Wang, Junwei Liao, Haipang Wu, Ji Liu, André Freitas, Qifan Wang, Zenglin Xu, Rongjuncheng Zhang, Yong Dai</strong></p>
<p>This work proposes an industry-level omni-modal large language model (LLM) pipeline that integrates auditory, visual, and linguistic modalities to overcome challenges such as limited tri-modal datasets, high computational costs, and complex feature alignments. Our pipeline consists of three main components: First, a modular framework enabling flexible configuration of various encoder-LLM-decoder architectures. Second, a lightweight training strategy that pre-trains audio-language alignment on the state-of-the-art vision-language model Qwen2.5-VL, thus avoiding the costly pre-training of vision-specific modalities. Third, an audio synthesis pipeline that generates high-quality audio-text data from diverse real-world scenarios, supporting applications such as Automatic Speech Recognition and Speech-to-Speech chat. To this end, we introduce an industry-level omni-modal LLM, Nexus. Extensive experiments validate the efficacy of our pipeline, yielding the following key findings:(1) In the visual understanding task, Nexus exhibits superior performance compared with its backbone model - Qwen2.5-VL-7B, validating the efficiency of our training strategy. (2) Within the English Spoken Question-Answering task, the model achieves better accuracy than the same-period competitor (i.e, MiniCPM-o2.6-7B) in the LLaMA Q. benchmark. (3) In our real-world ASR testset, Nexus achieves outstanding performance, indicating its robustness in real scenarios. (4) In the Speech-to-Text Translation task, our model outperforms Qwen2-Audio-Instruct-7B. (5) In the Text-to-Speech task, based on pretrained vocoder (e.g., Fishspeech1.4 or CosyVoice2.0), Nexus is comparable to its backbone vocoder on Seed-TTS benchmark. (6) An in-depth analysis of tri-modal alignment reveals that incorporating the audio modality enhances representational alignment between vision and language. </p>
<blockquote>
<p>本文提出了一种行业级的跨模态大型语言模型（LLM）管道，该管道融合了听觉、视觉和语言模式，以克服如有限的三模态数据集、高计算成本和复杂的特征对齐等挑战。我们的管道主要由三个部分组成：首先，一个模块化框架，能够灵活配置各种编码器-LLM-解码器架构。其次，一种轻量级的训练策略，通过在最先进的视觉语言模型Qwen2.5-VL上进行音频语言对齐的预训练，从而避免了针对特定视觉模式的昂贵预训练。最后，一个音频合成管道，用于从各种真实场景生成高质量音频文本数据，支持如自动语音识别和语音到语音聊天等应用。为此，我们引入了一种行业级的跨模态LLM，Nexus。大量实验验证了我们的管道的有效性，得出以下关键发现：（1）在视觉理解任务中，Nexus相较于其骨干模型Qwen2.5-VL-7B表现出卓越的性能，验证了我们的训练策略的有效性。（2）在英语口语问答任务中，该模型在LLaMA Q. benchmark上达到了比同期竞争对手（即MiniCPM-o2.6-7B）更高的准确性。（3）在我们的真实世界ASR测试集上，Nexus表现出卓越的性能，表明其在真实场景中的稳健性。（4）在语音到文本翻译任务中，我们的模型优于Qwen2-Audio-Instruct-7B。（5）在文本到语音任务中，基于预训练的vocoder（例如Fishspeech1.4或CosyVoice2.0），Nexus在Seed-TTS benchmark上的表现与其骨干vocoder相当。（6）对三模态对齐的深入分析表明，融入音频模式增强了视觉和语言之间的代表性对齐。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01879v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种跨行业的多模态大型语言模型（LLM）管道，该管道融合了听觉、视觉和语言学三大模态，以应对如有限的三模态数据集、高计算成本和复杂特征对齐等挑战。该管道包括三个主要组件：一个支持各种编码器-LLM-解码器架构灵活配置的模块化框架；一种基于前沿视觉语言模型Qwen2.5-VL的轻量级训练策略，避免了昂贵的视觉特定模态预训练；以及一个从各种现实场景生成高质量音频文本数据的声音合成管道。引进的产业级多模态LLM Nexus经过广泛实验验证，在多项任务中表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一个多模态的大型语言模型（LLM）管道，集成听觉、视觉和语言学三大模态。</li>
<li>管道包含模块化框架、轻量级训练策略和音频合成管道三个主要组件。</li>
<li>Nexus模型在视觉理解任务中表现出卓越性能，相较于其基础模型Qwen2.5-VL-7B有显著提升。</li>
<li>在英语口语问答任务中，Nexus模型达到同期竞品之上的准确性。</li>
<li>在实际场景的自动语音识别（ASR）测试集中，Nexus表现出出色性能，体现了其在真实场景中的稳健性。</li>
<li>在语音到文本翻译任务中，Nexus模型超越了Qwen2-Audio-Instruct-7B。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01879">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e930c62cce7fe631c175d10d960d53d7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e47567a66fd0c62723052f4b05ead6ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb98e337962b4c98b584b1d2718b13e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d20fe4d48d94062be0b57cdbfcf57f5e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Visatronic-A-Multimodal-Decoder-Only-Model-for-Speech-Synthesis"><a href="#Visatronic-A-Multimodal-Decoder-Only-Model-for-Speech-Synthesis" class="headerlink" title="Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis"></a>Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis</h2><p><strong>Authors:Akshita Gupta, Tatiana Likhomanenko, Karren Dai Yang, Richard He Bai, Zakaria Aldeneh, Navdeep Jaitly</strong></p>
<p>The rapid progress of foundation models and large language models (LLMs) has fueled significantly improvement in the capabilities of machine learning systems that benefit from mutlimodal input data. However, existing multimodal models are predominantly built on top of pre-trained LLMs, which can limit accurate modeling of temporal dependencies across other modalities and thus limit the model’s ability to jointly process and leverage multimodal inputs. To specifically investigate the alignment of text, video, and speech modalities in LLM-style (decoder-only) models, we consider a simplified multimodal generation task, Video-Text to Speech (VTTS): speech generation conditioned on both its corresponding text and video of talking people. The ultimate goal is to generate speech that not only follows the text but also aligns temporally with the video and is consistent with the facial expressions. In this paper, we first introduce Visatronic, a unified multimodal decoder-only transformer model that adopts an LLM-style architecture to embed visual, textual, and speech inputs into a shared subspace, treating all modalities as temporally aligned token streams. Next, we carefully explore different token mixing strategies to understand the best way to propagate information from the steps where video and text conditioning is input to the steps where the audio is generated. We extensively evaluate Visatronic on the challenging VoxCeleb2 dataset and demonstrate zero-shot generalization to LRS3, where Visatronic, trained on VoxCeleb2, achieves a 4.5% WER, outperforming prior SOTA methods trained only on LRS3, which report a 21.4% WER. Additionally, we propose a new objective metric, TimeSync, specifically designed to measure phoneme-level temporal alignment between generated and reference speech, further ensuring synchronization quality. Demo: <a target="_blank" rel="noopener" href="https://apple.github.io/visatronic-demo/">https://apple.github.io/visatronic-demo/</a> </p>
<blockquote>
<p>随着基础模型和大语言模型（LLM）的快速发展，受益于多模态输入数据的机器学习系统的能力得到了显著提升。然而，现有的多模态模型主要是建立在预训练LLM之上，这可能会限制跨其他模态的时间依赖性的准确建模，从而限制模型联合处理并利用多模态输入的能力。为了专门研究LLM风格（仅解码器）模型中文字、视频和语音模态的对齐问题，我们考虑了一个简化的多模态生成任务，即视频文本转语音（VTTS）：语音生成既取决于相应的文本，又取决于人们讲话的视频。最终目标是生成不仅遵循文本而且与视频在时间上有对齐的语音，并与面部表情保持一致。在本文中，我们首先介绍了Visatronic，这是一个统一的多模态仅解码器transformer模型，它采用LLM风格架构将视觉、文本和语音输入嵌入到共享子空间中，将所有模态视为时间对齐的令牌流。接下来，我们仔细探索了不同的令牌混合策略，以了解从视频和文本调节步骤向生成音频步骤传播信息的最佳方式。我们在具有挑战性的VoxCeleb2数据集上对Visatronic进行了广泛评估，并展示了零样本泛化到LRS3的能力。在VoxCeleb2上训练的Visatronic在LRS3上实现了4.5%的WER（词错误率），优于仅针对LRS3进行训练的先前最佳方法（报告了21.4%的WER）。此外，我们还提出了一种新的客观指标TimeSync，专门用于测量生成语音和参考语音之间的音素级时间对齐，进一步确保同步质量。演示地址：<a target="_blank" rel="noopener" href="https://apple.github.io/visatronic-demo/">https://apple.github.io/visatronic-demo/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17690v2">PDF</a> </p>
<p><strong>Summary</strong><br>     随着基础模型和大语言模型（LLM）的快速发展，多模态输入数据对机器学习系统的能力提升显著。然而，现有的多模态模型主要依赖于预训练LLM，限制了跨其他模态的时间依赖性的准确建模以及多模态输入的联合处理。为此，我们研究文本、视频和语音模态的对齐问题，在LLM风格（仅解码器）模型中考虑简化的多模态生成任务——视频文本转语音（VTTS）。目标是生成不仅遵循文本还与时间视频对齐且符合面部表情的语音。本文介绍Visatronic模型，采用LLM风格架构嵌入视觉、文本和语音输入到共享子空间，将所有模态视为时间对齐的令牌流。通过探索不同的令牌混合策略，我们在VoxCeleb2数据集上评估Visatronic，并展示零样本泛化至LRS3数据集的能力。Visatronic在VoxCeleb2上训练的模型达到4.5%的WER，优于仅在LRS3上训练的先前最佳方法（报告为21.4%的WER）。我们还提出新的客观指标TimeSync，专门用于测量生成语音和参考语音之间的音素级时间对齐，以确保同步质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态模型的快速发展受益于基础模型和大语言模型（LLM）的进步。</li>
<li>当前多模态模型主要基于预训练LLM，存在对跨模态时间依赖性建模的限制。</li>
<li>研究聚焦于文本、视频和语音模态的对齐问题，引入视频文本转语音（VTTS）任务作为研究焦点。</li>
<li>Visatronic模型采用LLM风格架构处理多模态输入，实现视觉、文本和语音的嵌入。</li>
<li>通过不同的令牌混合策略探索，优化了信息从视频和文本输入步骤到音频生成步骤的传播。</li>
<li>在VoxCeleb2数据集上的评估显示Visatronic模型性能优越，并实现了零样本泛化至LRS3数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17690">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-45a6c6dfa6b81fd005b779083e2a519a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbe22e9665ec8b0309dfaa8a708b8550.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02c056bd02f973ea4e3070664b7e8576.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a90e2243467d0f786c7463ce9e7a5d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acef3515d5a164a7d5b20ebe94e1f514.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4f4fb89cfe947b46d5aa0b772b4f3ea.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-19ffc29965317a33915b16d94846495e.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-05-31  MCTSr-Zero Self-Reflective Psychological Counseling Dialogues   Generation via Principles and Adaptive Exploration
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fa300c0ed08360fc9ae9a8557c55db84.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-31  DeepChest Dynamic Gradient-Free Task Weighting for Effective Multi-Task   Learning in Chest X-ray Classification
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28292.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
