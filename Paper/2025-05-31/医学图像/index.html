<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  DeepChest Dynamic Gradient-Free Task Weighting for Effective Multi-Task   Learning in Chest X-ray Classification">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-fa300c0ed08360fc9ae9a8557c55db84.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-31
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-31-æ›´æ–°"><a href="#2025-05-31-æ›´æ–°" class="headerlink" title="2025-05-31 æ›´æ–°"></a>2025-05-31 æ›´æ–°</h1><h2 id="DeepChest-Dynamic-Gradient-Free-Task-Weighting-for-Effective-Multi-Task-Learning-in-Chest-X-ray-Classification"><a href="#DeepChest-Dynamic-Gradient-Free-Task-Weighting-for-Effective-Multi-Task-Learning-in-Chest-X-ray-Classification" class="headerlink" title="DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task   Learning in Chest X-ray Classification"></a>DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task   Learning in Chest X-ray Classification</h2><p><strong>Authors:Youssef Mohamed, Noran Mohamed, Khaled Abouhashad, Feilong Tang, Sara Atito, Shoaib Jameel, Imran Razzak, Ahmed B. Zaky</strong></p>
<p>While Multi-Task Learning (MTL) offers inherent advantages in complex domains such as medical imaging by enabling shared representation learning, effectively balancing task contributions remains a significant challenge. This paper addresses this critical issue by introducing DeepChest, a novel, computationally efficient and effective dynamic task-weighting framework specifically designed for multi-label chest X-ray (CXR) classification. Unlike existing heuristic or gradient-based methods that often incur substantial overhead, DeepChest leverages a performance-driven weighting mechanism based on effective analysis of task-specific loss trends. Given a network architecture (e.g., ResNet18), our model-agnostic approach adaptively adjusts task importance without requiring gradient access, thereby significantly reducing memory usage and achieving a threefold increase in training speed. It can be easily applied to improve various state-of-the-art methods. Extensive experiments on a large-scale CXR dataset demonstrate that DeepChest not only outperforms state-of-the-art MTL methods by 7% in overall accuracy but also yields substantial reductions in individual task losses, indicating improved generalization and effective mitigation of negative transfer. The efficiency and performance gains of DeepChest pave the way for more practical and robust deployment of deep learning in critical medical diagnostic applications. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/youssefkhalil320/DeepChest-MTL">https://github.com/youssefkhalil320/DeepChest-MTL</a> </p>
<blockquote>
<p>å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰åœ¨åŒ»å­¦æˆåƒç­‰å¤æ‚é¢†åŸŸå…·æœ‰å†…åœ¨ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿå®ç°å¯¹å…±äº«è¡¨ç¤ºçš„å­¦ä¹ ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆå¹³è¡¡ä»»åŠ¡è´¡çŒ®ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥DeepChestæ¥è§£å†³è¿™ä¸€å…³é”®é—®é¢˜ï¼ŒDeepChestæ˜¯ä¸€ç§æ–°å‹åŠ¨æ€ä»»åŠ¡åŠ æƒæ¡†æ¶ï¼Œä¸“ä¸ºå¤šæ ‡ç­¾èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰åˆ†ç±»è€Œè®¾è®¡ï¼Œè®¡ç®—æ•ˆç‡é«˜ä¸”æœ‰æ•ˆã€‚ä¸ç°æœ‰çš„å¯å‘å¼æˆ–åŸºäºæ¢¯åº¦çš„æ–¹æ³•ç»å¸¸äº§ç”Ÿå·¨å¤§å¼€é”€ä¸åŒï¼ŒDeepCheståˆ©ç”¨ä¸€ç§ä»¥æ€§èƒ½é©±åŠ¨çš„åŠ æƒæœºåˆ¶ï¼ŒåŸºäºä»»åŠ¡ç‰¹å®šæŸå¤±è¶‹åŠ¿çš„æœ‰æ•ˆåˆ†æã€‚å¯¹äºç»™å®šçš„ç½‘ç»œæ¶æ„ï¼ˆä¾‹å¦‚ResNet18ï¼‰ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ— å…³æ–¹æ³•è‡ªé€‚åº”åœ°è°ƒæ•´ä»»åŠ¡é‡è¦æ€§ï¼Œæ— éœ€æ¢¯åº¦è®¿é—®ï¼Œä»è€Œå¤§å¤§é™ä½äº†å†…å­˜ä½¿ç”¨å¹¶å®ç°äº†ä¸‰å€çš„è®­ç»ƒé€Ÿåº¦æå‡ã€‚å®ƒå¯ä»¥è½»æ¾åº”ç”¨äºæ”¹è¿›å„ç§æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚åœ¨å¤§è§„æ¨¡CXRæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDeepChestä¸ä»…ä»¥7%çš„æ•´ä½“å‡†ç¡®ç‡è¶…è¶Šäº†æœ€å…ˆè¿›çš„å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ï¼Œè€Œä¸”åœ¨ä¸ªåˆ«ä»»åŠ¡æŸå¤±ä¸Šä¹Ÿå®ç°äº†æ˜¾è‘—å‡å°‘ï¼Œè¿™è¡¨æ˜äº†æ›´å¥½çš„æ³›åŒ–å’Œæœ‰æ•ˆçš„è´Ÿè¿ç§»ç¼“è§£ã€‚DeepChestçš„æ•ˆç‡å’Œæ€§èƒ½æå‡ä¸ºæ·±åº¦å­¦ä¹ åœ¨å®é™…åŒ»ç–—è¯Šæ–­åº”ç”¨ä¸­çš„æ›´å®ç”¨å’Œç¨³å¥éƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/youssefkhalil320/DeepChest-MTL%E3%80%82">https://github.com/youssefkhalil320/DeepChest-MTLã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23595v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤šä»»åŠ¡å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒç­‰é¢†åŸŸå­˜åœ¨ä»»åŠ¡è´¡çŒ®å¹³è¡¡çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºDeepChestæ¡†æ¶ï¼Œé‡‡ç”¨åŠ¨æ€ä»»åŠ¡åŠ æƒæœºåˆ¶ï¼Œé’ˆå¯¹å¤šæ ‡ç­¾èƒ¸ç‰‡åˆ†ç±»é—®é¢˜å®ç°æœ‰æ•ˆå¹³è¡¡ã€‚è¯¥æ¡†æ¶è®¡ç®—æ•ˆç‡é«˜ï¼Œå¯è‡ªé€‚åº”è°ƒæ•´ä»»åŠ¡é‡è¦æ€§ï¼Œæ— éœ€æ¢¯åº¦è®¿é—®ï¼Œæ˜¾è‘—æé«˜è®­ç»ƒé€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨ç‡ã€‚åœ¨å¤§å‹èƒ¸ç‰‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDeepCheståœ¨æ€»ä½“å‡†ç¡®åº¦ä¸Šä¼˜äºæœ€æ–°å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•7%ï¼Œå¹¶æœ‰æ•ˆå‡å°‘å•ä¸ªä»»åŠ¡æŸå¤±ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›å’Œå‡å°‘äº†è´Ÿè¿ç§»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šä»»åŠ¡å­¦ä¹ åœ¨åŒ»å­¦æˆåƒä¸­å­˜åœ¨å¹³è¡¡ä»»åŠ¡è´¡çŒ®çš„æŒ‘æˆ˜ã€‚</li>
<li>DeepChestæ¡†æ¶é€šè¿‡åŠ¨æ€ä»»åŠ¡åŠ æƒæœºåˆ¶è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>DeepChesté‡‡ç”¨æ€§èƒ½é©±åŠ¨çš„ä»»åŠ¡æƒé‡è°ƒæ•´æ–¹æ³•ï¼Œåˆ†æç‰¹å®šä»»åŠ¡çš„æŸå¤±è¶‹åŠ¿ã€‚</li>
<li>è¯¥æ¡†æ¶æ¨¡å‹é€šç”¨æ€§å¼ºï¼Œå¯åœ¨ä¸åŒçš„ç½‘ç»œæ¶æ„ï¼ˆå¦‚ResNet18ï¼‰ä¸­è‡ªé€‚åº”è°ƒæ•´ä»»åŠ¡é‡è¦æ€§ã€‚</li>
<li>DeepChestæé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œæ˜¾è‘—å‡å°‘äº†å†…å­˜ä½¿ç”¨ï¼Œå¹¶å®ç°äº†ä¸‰å€çš„è®­ç»ƒé€Ÿåº¦æå‡ã€‚</li>
<li>åœ¨å¤§å‹èƒ¸ç‰‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDeepCheståœ¨æ€»ä½“å‡†ç¡®åº¦ä¸Šä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23595">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-32cf1e651738caba971d4efd31acf416.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85ab6b9ad1a05a72145d120b0dc414c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf060b22245496133d59e8aa6c9d8682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d29c3c050721be233e55c2aa66ad3464.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd771eb45f95d9b87ca84f700be441ea.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multilook-Coherent-Imaging-Theoretical-Guarantees-and-Algorithms"><a href="#Multilook-Coherent-Imaging-Theoretical-Guarantees-and-Algorithms" class="headerlink" title="Multilook Coherent Imaging: Theoretical Guarantees and Algorithms"></a>Multilook Coherent Imaging: Theoretical Guarantees and Algorithms</h2><p><strong>Authors:Xi Chen, Soham Jana, Christopher A. Metzler, Arian Maleki, Shirin Jalali</strong></p>
<p>Multilook coherent imaging is a widely used technique in applications such as digital holography, ultrasound imaging, and synthetic aperture radar. A central challenge in these systems is the presence of multiplicative noise, commonly known as speckle, which degrades image quality. Despite the widespread use of coherent imaging systems, their theoretical foundations remain relatively underexplored. In this paper, we study both the theoretical and algorithmic aspects of likelihood-based approaches for multilook coherent imaging, providing a rigorous framework for analysis and method development. Our theoretical contributions include establishing the first theoretical upper bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under the deep image prior hypothesis. Our results capture the dependence of MSE on the number of parameters in the deep image prior, the number of looks, the signal dimension, and the number of measurements per look. On the algorithmic side, we employ projected gradient descent (PGD) as an efficient method for computing the maximum likelihood solution. Furthermore, we introduce two key ideas to enhance the practical performance of PGD. First, we incorporate the Newton-Schulz algorithm to compute matrix inverses within the PGD iterations, significantly reducing computational complexity. Second, we develop a bagging strategy to mitigate projection errors introduced during PGD updates. We demonstrate that combining these techniques with PGD yields state-of-the-art performance. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Computational-Imaging-RU/Bagged-DIP-Speckle">https://github.com/Computational-Imaging-RU/Bagged-DIP-Speckle</a>. </p>
<blockquote>
<p>å¤šè§†è§’ç›¸å¹²æˆåƒæŠ€æœ¯å¹¿æ³›åº”ç”¨äºæ•°å­—å…¨æ¯ã€è¶…å£°æˆåƒå’Œåˆæˆå­”å¾„é›·è¾¾ç­‰é¢†åŸŸã€‚è¿™äº›ç³»ç»Ÿé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯å­˜åœ¨ä¹˜æ³•å™ªå£°ï¼Œä¹Ÿç§°ä¸ºæ–‘ç‚¹å™ªå£°ï¼Œä¼šé™ä½å›¾åƒè´¨é‡ã€‚å°½ç®¡ç›¸å¹²æˆåƒç³»ç»Ÿå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶ç†è®ºåŸºç¡€ä»ç›¸å¯¹æœªè¢«å……åˆ†ç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŸºäºä¼¼ç„¶çš„å¤šè§†è§’ç›¸å¹²æˆåƒçš„ç†è®ºå’Œç®—æ³•æ–¹é¢ï¼Œä¸ºåˆ†æå’Œæ–¹æ³•çš„å¼€å‘æä¾›äº†ä¸¥æ ¼æ¡†æ¶ã€‚æˆ‘ä»¬çš„ç†è®ºè´¡çŒ®åŒ…æ‹¬åœ¨æ·±åº¦å›¾åƒå…ˆéªŒå‡è®¾ä¸‹å»ºç«‹æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰çš„ç¬¬ä¸€ä¸ªç†è®ºä¸Šé™ã€‚æˆ‘ä»¬çš„ç»“æœæ•æ‰åˆ°äº†MSEå¯¹æ·±åº¦å›¾åƒå…ˆéªŒå‚æ•°æ•°é‡ã€è§‚å¯Ÿæ¬¡æ•°ã€ä¿¡å·ç»´åº¦å’Œæ¯æ¬¡è§‚å¯Ÿçš„æµ‹é‡æ•°é‡ç­‰çš„ä¾èµ–å…³ç³»ã€‚åœ¨ç®—æ³•æ–¹é¢ï¼Œæˆ‘ä»¬é‡‡ç”¨æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼ˆPGDï¼‰ä½œä¸ºè®¡ç®—æœ€å¤§ä¼¼ç„¶è§£çš„æœ‰æ•ˆæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§å¢å¼ºPGDå®é™…æ€§èƒ½çš„å…³é”®æ€æƒ³ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç»“åˆäº†ç‰›é¡¿-èˆ’å°”èŒ¨ç®—æ³•åœ¨PGDè¿­ä»£è¿‡ç¨‹ä¸­è®¡ç®—çŸ©é˜µé€†ï¼Œå¤§å¤§é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§baggingç­–ç•¥æ¥å‡è½»PGDæ›´æ–°è¿‡ç¨‹ä¸­å¼•å…¥çš„æŠ•å½±è¯¯å·®ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå°†è¿™äº›æŠ€æœ¯ä¸PGDç›¸ç»“åˆï¼Œå¯è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Computational-Imaging-RU/Bagged-DIP-Speckle%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Computational-Imaging-RU/Bagged-DIP-Speckleæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23594v1">PDF</a> 29 pages, 4 figures, 3 tables. arXiv admin note: substantial text   overlap with arXiv:2402.15635</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºä¼¼ç„¶çš„æ–¹æ³•åœ¨å¤šé‡ç›¸å¹²æˆåƒä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬ç†è®ºå’Œç®—æ³•æ–¹é¢ã€‚ç†è®ºè´¡çŒ®åŒ…æ‹¬å»ºç«‹äº†æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰çš„ç†è®ºä¸Šç•Œï¼Œåˆ†æäº†æ·±åº¦å›¾åƒå…ˆéªŒå‡è®¾ã€è§‚å¯Ÿæ¬¡æ•°ã€ä¿¡å·ç»´åº¦å’Œæ¯æ¬¡è§‚å¯Ÿçš„æµ‹é‡æ¬¡æ•°å¯¹MSEçš„å½±å“ã€‚ç®—æ³•æ–¹é¢ï¼Œé‡‡ç”¨æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼ˆPGDï¼‰è®¡ç®—æœ€å¤§ä¼¼ç„¶è§£ï¼Œå¹¶å¼•å…¥ä¸¤ä¸ªå…³é”®æ€æƒ³æé«˜PGDçš„å®é™…æ€§èƒ½ï¼šä¸€æ˜¯ç»“åˆç‰›é¡¿-èˆ’å°”èŒ¨ç®—æ³•é™ä½è®¡ç®—å¤æ‚åº¦ï¼ŒäºŒæ˜¯å¼€å‘baggingç­–ç•¥å‡è½»PGDæ›´æ–°ä¸­çš„æŠ•å½±è¯¯å·®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šé‡ç›¸å¹²æˆåƒä¸­é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ä¹˜æ³•å™ªå£°ï¼ˆå³æ–‘ç‚¹ï¼‰å¯¼è‡´çš„å›¾åƒè´¨é‡ä¸‹é™ã€‚</li>
<li>è®ºæ–‡ç ”ç©¶äº†åŸºäºä¼¼ç„¶çš„æ–¹æ³•åœ¨å¤šé‡ç›¸å¹²æˆåƒä¸­çš„ç†è®ºå’Œç®—æ³•æ–¹é¢ã€‚</li>
<li>å»ºç«‹äº†æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰çš„ç†è®ºä¸Šç•Œã€‚</li>
<li>åˆ†æäº†æ·±åº¦å›¾åƒå…ˆéªŒå‡è®¾ã€è§‚å¯Ÿæ¬¡æ•°ã€ä¿¡å·ç»´åº¦å’Œæ¯æ¬¡è§‚å¯Ÿçš„æµ‹é‡æ¬¡æ•°å¯¹MSEçš„å½±å“ã€‚</li>
<li>é‡‡ç”¨æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼ˆPGDï¼‰è®¡ç®—æœ€å¤§ä¼¼ç„¶è§£ï¼Œè¯¥æ–¹æ³•æ˜¯ä¸€ç§é«˜æ•ˆçš„æ–¹æ³•ã€‚</li>
<li>å¼•å…¥ç‰›é¡¿-èˆ’å°”èŒ¨ç®—æ³•å’Œbaggingç­–ç•¥æ¥æé«˜PGDçš„å®é™…æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23594">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f788c5a7ca4b826da2affcb17de4a58c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PCA-for-Enhanced-Cross-Dataset-Generalizability-in-Breast-Ultrasound-Tumor-Segmentation"><a href="#PCA-for-Enhanced-Cross-Dataset-Generalizability-in-Breast-Ultrasound-Tumor-Segmentation" class="headerlink" title="PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound   Tumor Segmentation"></a>PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound   Tumor Segmentation</h2><p><strong>Authors:Christian Schmidt, Heinrich Martin Overhoff</strong></p>
<p>In medical image segmentation, limited external validity remains a critical obstacle when models are deployed across unseen datasets, an issue particularly pronounced in the ultrasound image domain. Existing solutions-such as domain adaptation and GAN-based style transfer-while promising, often fall short in the medical domain where datasets are typically small and diverse. This paper presents a novel application of principal component analysis (PCA) to address this limitation. PCA preprocessing reduces noise and emphasizes essential features by retaining approximately 90% of the dataset variance. We evaluate our approach across six diverse breast tumor ultrasound datasets comprising 3,983 B-mode images and corresponding expert tumor segmentation masks. For each dataset, a corresponding dimensionality reduced PCA-dataset is created and U-Net-based segmentation models are trained on each of the twelve datasets. Each model trained on an original dataset was inferenced on the remaining five out-of-domain original datasets (baseline results), while each model trained on a PCA dataset was inferenced on five out-of-domain PCA datasets. Our experimental results indicate that using PCA reconstructed datasets, instead of original images, improves the modelâ€™s recall and Dice scores, particularly for model-dataset pairs where baseline performance was lowest, achieving statistically significant gains in recall (0.57 $\pm$ 0.07 vs. 0.70 $\pm$ 0.05, $p &#x3D; 0.0004$) and Dice scores (0.50 $\pm$ 0.06 vs. 0.58 $\pm$ 0.06, $p &#x3D; 0.03$). Our method reduced the decline in recall values due to external validation by $33%$. These findings underscore the potential of PCA reconstruction as a safeguard to mitigate declines in segmentation performance, especially in challenging cases, with implications for enhancing external validity in real-world medical applications. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸï¼Œå½“æ¨¡å‹éƒ¨ç½²åœ¨æœªè§è¿‡çš„æ•°æ®é›†ä¸Šæ—¶ï¼Œæœ‰é™çš„å¤–éƒ¨æœ‰æ•ˆæ€§ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®çš„éšœç¢ï¼Œè¿™ä¸€é—®é¢˜åœ¨è¶…å£°å›¾åƒé¢†åŸŸå°¤å…¶çªå‡ºã€‚è™½ç„¶ç°æœ‰çš„è§£å†³æ–¹æ¡ˆï¼ˆå¦‚åŸŸé€‚åº”å’ŒåŸºäºGANçš„é£æ ¼è½¬ç§»ï¼‰å‰æ™¯å¹¿é˜”ï¼Œä½†åœ¨åŒ»å­¦é¢†åŸŸé€šå¸¸é¢ä¸´æ•°æ®é›†å°ä¸”å¤šæ ·çš„é—®é¢˜ã€‚æœ¬æ–‡é¦–æ¬¡å°†ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰åº”ç”¨äºè§£å†³è¿™ä¸€é™åˆ¶ã€‚PCAé¢„å¤„ç†é€šè¿‡ä¿ç•™å¤§çº¦90%çš„æ•°æ®é›†æ–¹å·®æ¥å‡å°‘å™ªå£°å¹¶å¼ºè°ƒé‡è¦ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨åŒ…å«3983å¼ Bæ¨¡å¼å›¾åƒå’Œç›¸åº”çš„ä¸“å®¶è‚¿ç˜¤åˆ†å‰²æ©ç çš„å…­ä¸ªä¸åŒä¹³è…ºç™Œè¶…å£°æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å¯¹äºæ¯ä¸ªæ•°æ®é›†ï¼Œéƒ½ä¼šåˆ›å»ºä¸€ä¸ªç›¸åº”çš„é™ç»´PCAæ•°æ®é›†ï¼Œå¹¶åœ¨æ¯ä¸ªåäºŒä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒåŸºäºU-Netçš„åˆ†å‰²æ¨¡å‹ã€‚åœ¨æ¯ä¸ªåŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹å¯¹å‰©ä½™çš„äº”ä¸ªå¤–éƒ¨åŸå§‹æ•°æ®é›†è¿›è¡Œæ¨ç†ï¼ˆåŸºçº¿ç»“æœï¼‰ï¼Œè€Œåœ¨PCAæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹å¯¹äº”ä¸ªå¤–éƒ¨PCAæ•°æ®é›†è¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨PCAé‡å»ºçš„æ•°æ®é›†è€Œä¸æ˜¯åŸå§‹å›¾åƒï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„å¬å›ç‡å’ŒDiceå¾—åˆ†ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåŸºçº¿æ€§èƒ½æœ€ä½çš„æ¨¡å‹-æ•°æ®é›†å¯¹ï¼Œå¬å›ç‡ï¼ˆ0.57Â±0.07 vs. 0.70Â±0.05ï¼Œp&#x3D;0.0004ï¼‰å’ŒDiceå¾—åˆ†ï¼ˆ0.50Â±0.06 vs. 0.58Â±0.06ï¼Œp&#x3D;0.03ï¼‰éƒ½æœ‰ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ç”±äºå¤–éƒ¨éªŒè¯è€Œå¯¼è‡´çš„å¬å›å€¼ä¸‹é™å‡å°‘äº†33%ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†PCAé‡å»ºä½œä¸ºç¼“è§£åˆ†å‰²æ€§èƒ½ä¸‹é™çš„å®‰å…¨ä¿éšœçš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æƒ…å†µä¸‹ï¼Œè¿™å¯¹å¢å¼ºç°å®ä¸–ç•ŒåŒ»å­¦åº”ç”¨çš„å¤–éƒ¨æœ‰æ•ˆæ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23587v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ¨¡å‹åœ¨æœªè§æ•°æ®é›†ä¸Šéƒ¨ç½²æ—¶å­˜åœ¨çš„å¤–éƒ¨æœ‰æ•ˆæ€§æœ‰é™çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯è¶…å£°å›¾åƒé¢†åŸŸçš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰çš„æ–°åº”ç”¨ã€‚PCAé¢„å¤„ç†é€šè¿‡ä¿ç•™å¤§çº¦90%çš„æ•°æ®é›†æ–¹å·®ï¼Œå‡å°‘å™ªå£°å¹¶å¼ºè°ƒé‡è¦ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨PCAé‡å»ºçš„æ•°æ®é›†ç›¸è¾ƒäºåŸå§‹å›¾åƒï¼Œèƒ½æé«˜æ¨¡å‹çš„å¬å›ç‡å’ŒDiceå¾—åˆ†ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºçº¿æ€§èƒ½æœ€ä½çš„æ¨¡å‹-æ•°æ®é›†å¯¹ä¸Šï¼Œå®ç°äº†ç»Ÿè®¡å­¦ä¸Šçš„å¬å›ç‡æ˜¾è‘—å¢é•¿ã€‚è¯¥æ–¹æ³•å‡è½»äº†ç”±äºå¤–éƒ¨éªŒè¯å¯¼è‡´çš„å¬å›å€¼ä¸‹é™33%ï¼Œä¸ºæé«˜åŒ»å­¦åº”ç”¨ä¸­çš„å¤–éƒ¨æœ‰æ•ˆæ€§æä¾›äº†æ½œåœ¨çš„å®‰å…¨ä¿éšœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ¨¡å‹åœ¨æœªè§æ•°æ®é›†ä¸Šçš„å¤–éƒ¨æœ‰æ•ˆæ€§æœ‰é™æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è¶…å£°å›¾åƒé¢†åŸŸã€‚</li>
<li>ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰è¢«åº”ç”¨äºåŒ»å­¦å›¾åƒé¢„å¤„ç†ï¼Œä»¥å‡å°‘å™ªå£°å¹¶å¼ºè°ƒé‡è¦ç‰¹å¾ï¼Œä¿ç•™å¤§çº¦90%çš„æ•°æ®é›†æ–¹å·®ã€‚</li>
<li>é€šè¿‡å®éªŒè¯„ä¼°äº†PCAåœ¨å…­ä¸ªä¸åŒçš„ä¹³è…ºç™Œè¶…å£°æ•°æ®é›†ä¸Šçš„æ•ˆæœã€‚</li>
<li>ä½¿ç”¨PCAé‡å»ºçš„æ•°æ®é›†è®­ç»ƒæ¨¡å‹ï¼Œç›¸è¾ƒäºåŸå§‹å›¾åƒï¼Œèƒ½æé«˜æ¨¡å‹çš„å¬å›ç‡å’ŒDiceå¾—åˆ†ã€‚</li>
<li>åœ¨åŸºçº¿æ€§èƒ½è¾ƒä½çš„æ¨¡å‹-æ•°æ®é›†å¯¹ä¸Šï¼ŒPCAçš„æ•ˆæœæ›´ä¸ºæ˜¾è‘—ï¼Œå®ç°äº†ç»Ÿè®¡å­¦ä¸Šçš„å¬å›ç‡å¢é•¿ã€‚</li>
<li>PCAæ–¹æ³•å‡è½»äº†ç”±äºå¤–éƒ¨éªŒè¯å¯¼è‡´çš„å¬å›å€¼ä¸‹é™33%ã€‚</li>
<li>PCAçš„åº”ç”¨ä¸ºæé«˜åŒ»å­¦åº”ç”¨ä¸­çš„å¤–éƒ¨æœ‰æ•ˆæ€§æä¾›äº†æ½œåœ¨çš„å®‰å…¨ä¿éšœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bc3f5ec4ede5eea62f56e6883d034cb7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e9180fa1bb449cc73388871f532dd44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea681739f311029988d8325c82c94e80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64156cde2d548441648fbcf925267fa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e2c4fa6f8fc81d81d9fb511617b7f3f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Comparative-assessment-of-fairness-definitions-and-bias-mitigation-strategies-in-machine-learning-based-diagnosis-of-Alzheimerâ€™s-disease-from-MR-images"><a href="#Comparative-assessment-of-fairness-definitions-and-bias-mitigation-strategies-in-machine-learning-based-diagnosis-of-Alzheimerâ€™s-disease-from-MR-images" class="headerlink" title="Comparative assessment of fairness definitions and bias mitigation   strategies in machine learning-based diagnosis of Alzheimerâ€™s disease from MR   images"></a>Comparative assessment of fairness definitions and bias mitigation   strategies in machine learning-based diagnosis of Alzheimerâ€™s disease from MR   images</h2><p><strong>Authors:Maria Eleftheria Vlontzou, Maria Athanasiou, Christos Davatzikos, Konstantina S. Nikita</strong></p>
<p>The present study performs a comprehensive fairness analysis of machine learning (ML) models for the diagnosis of Mild Cognitive Impairment (MCI) and Alzheimerâ€™s disease (AD) from MRI-derived neuroimaging features. Biases associated with age, race, and gender in a multi-cohort dataset, as well as the influence of proxy features encoding these sensitive attributes, are investigated. The reliability of various fairness definitions and metrics in the identification of such biases is also assessed. Based on the most appropriate fairness measures, a comparative analysis of widely used pre-processing, in-processing, and post-processing bias mitigation strategies is performed. Moreover, a novel composite measure is introduced to quantify the trade-off between fairness and performance by considering the F1-score and the equalized odds ratio, making it appropriate for medical diagnostic applications. The obtained results reveal the existence of biases related to age and race, while no significant gender bias is observed. The deployed mitigation strategies yield varying improvements in terms of fairness across the different sensitive attributes and studied subproblems. For race and gender, Reject Option Classification improves equalized odds by 46% and 57%, respectively, and achieves harmonic mean scores of 0.75 and 0.80 in the MCI versus AD subproblem, whereas for age, in the same subproblem, adversarial debiasing yields the highest equalized odds improvement of 40% with a harmonic mean score of 0.69. Insights are provided into how variations in AD neuropathology and risk factors, associated with demographic characteristics, influence model fairness. </p>
<blockquote>
<p>æœ¬ç ”ç©¶å¯¹æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹åœ¨è¯Šæ–­è½»åº¦è®¤çŸ¥éšœç¢ï¼ˆMCIï¼‰å’Œé˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰ä¸­çš„å…¬å¹³æ€§è¿›è¡Œäº†å…¨é¢åˆ†æï¼Œåˆ†æåŸºäºMRIè¡ç”Ÿçš„ç¥ç»æˆåƒç‰¹å¾ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¤šé˜Ÿåˆ—æ•°æ®é›†ä¸­ä¸å¹´é¾„ã€ç§æ—å’Œæ€§åˆ«ç›¸å…³çš„åè§ï¼Œä»¥åŠè¿™äº›æ•æ„Ÿå±æ€§çš„ä»£ç†ç‰¹å¾ç¼–ç çš„å½±å“ã€‚è¿˜è¯„ä¼°äº†ä¸åŒå…¬å¹³å®šä¹‰å’ŒæŒ‡æ ‡åœ¨è¯†åˆ«æ­¤ç±»åè§æ–¹é¢çš„å¯é æ€§ã€‚åŸºäºæœ€åˆé€‚çš„å…¬å¹³è¡¡é‡æ ‡å‡†ï¼Œå¯¹å¹¿æ³›ä½¿ç”¨çš„é¢„å¤„ç†ã€è¿‡ç¨‹å¤„ç†å’Œåå¤„ç†åè§ç¼“è§£ç­–ç•¥è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹ç»„åˆåº¦é‡æ–¹æ³•ï¼Œé€šè¿‡è€ƒè™‘F1åˆ†æ•°å’Œå¹³ç­‰æœºä¼šæ¯”ç‡æ¥é‡åŒ–å…¬å¹³æ€§å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡ï¼Œè¿™å¯¹äºåŒ»ç–—è¯Šæ–­åº”ç”¨æ˜¯é€‚å½“çš„ã€‚è·å¾—çš„ç»“æœè¡¨æ˜å­˜åœ¨ä¸å¹´é¾„å’Œç§æ—ç›¸å…³çš„åè§ï¼Œè€Œæ€§åˆ«åè§å¹¶ä¸æ˜¾è‘—ã€‚æ‰€å®æ–½çš„ç¼“è§£ç­–ç•¥åœ¨é’ˆå¯¹ä¸åŒæ•æ„Ÿå±æ€§å’Œæ‰€ç ”ç©¶å­é—®é¢˜çš„å…¬å¹³æ€§æ–¹é¢äº§ç”Ÿäº†ä¸åŒç¨‹åº¦çš„æ”¹è¿›ã€‚å¯¹äºç§æ—å’Œæ€§åˆ«è€Œè¨€ï¼Œæ‹’ç»é€‰é¡¹åˆ†ç±»å°†å¹³ç­‰æœºä¼šæé«˜äº†46%å’Œ57%ï¼Œå¹¶åœ¨MCIä¸ADçš„å­é—®é¢˜ä¸­å®ç°äº†0.75å’Œ0.80çš„è°ƒå’Œå¹³å‡æ•°å¾—åˆ†ï¼›è€Œå¯¹äºå¹´é¾„ï¼Œåœ¨åŒä¸€å­é—®é¢˜ä¸­ï¼Œå¯¹æŠ—å»åç½®ä½¿å¹³ç­‰æœºä¼šæé«˜äº†40%ï¼Œè°ƒå’Œå¹³å‡æ•°å¾—åˆ†ä¸º0.69ã€‚æœ¬ç ”ç©¶æä¾›äº†å…³äºä¸äººå£ç»Ÿè®¡å­¦ç‰¹å¾ç›¸å…³çš„ADç¥ç»ç—…ç†å­¦å’Œé£é™©å› ç´ çš„å˜åŠ¨å¦‚ä½•å½±å“æ¨¡å‹å…¬å¹³æ€§çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23528v1">PDF</a> (C) 2025 IEEE Paper accepted at IEEE Engineering in Medicine and   Biology Society Conference, 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…¨é¢åˆ†æäº†ç”¨äºè½»åº¦è®¤çŸ¥éšœç¢ï¼ˆMCIï¼‰å’Œé˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰è¯Šæ–­çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„å…¬å¹³æ€§ã€‚ç ”ç©¶è°ƒæŸ¥äº†å¹´é¾„ã€ç§æ—å’Œæ€§åˆ«ç­‰å¤šé˜Ÿåˆ—æ•°æ®é›†ä¸­çš„åè§ï¼Œä»¥åŠè¿™äº›æ•æ„Ÿå±æ€§ç¼–ç çš„ä»£ç†ç‰¹å¾çš„å½±å“ã€‚åŒæ—¶è¯„ä¼°äº†ä¸åŒå…¬å¹³æ€§å®šä¹‰å’ŒæŒ‡æ ‡åœ¨è¯†åˆ«è¿™äº›åè§æ–¹é¢çš„å¯é æ€§ã€‚æ ¹æ®æœ€åˆé€‚çš„å…¬å¹³æªæ–½ï¼Œå¯¹é¢„å¤„ç†ã€ä¸­å¤„ç†å’Œåå¤„ç†åè§ç¼“è§£ç­–ç•¥è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚æ­¤å¤–ï¼Œä»‹ç»äº†ä¸€ç§æ–°çš„å¤åˆåº¦é‡æ ‡å‡†ï¼Œè¯¥æ ‡å‡†èƒ½å¹³è¡¡å…¬å¹³æ€§å’Œæ€§èƒ½ï¼Œè€ƒè™‘äº†F1åˆ†æ•°å’Œå‡ç­‰æœºä¼šæ¯”ç‡ï¼Œé€‚ç”¨äºåŒ»ç–—è¯Šæ–­åº”ç”¨ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºå­˜åœ¨ä¸å¹´é¾„å’Œç§æ—ç›¸å…³çš„åè§ï¼Œè€Œæ€§åˆ«åè§ä¸æ˜¾è‘—ã€‚éƒ¨ç½²çš„ç¼“è§£ç­–ç•¥åœ¨ä¸åŒæ•æ„Ÿå±æ€§å’Œç ”ç©¶å­é—®é¢˜ä¸Šå…¬å¹³æ€§çš„æ”¹è¿›ç¨‹åº¦ä¸åŒã€‚å¯¹äºç§æ—å’Œæ€§åˆ«ï¼Œæ‹’ç»é€‰é¡¹åˆ†ç±»æé«˜äº†å‡ç­‰æœºä¼šæ¯”ç‡åˆ†åˆ«è¾¾46%å’Œ57%ï¼Œåœ¨MCIä¸ADå­é—®é¢˜ä¸­è¾¾åˆ°å’Œè°å‡å€¼0.75å’Œ0.80ï¼›è€Œå¯¹äºå¹´é¾„ï¼Œåœ¨åŒä¸€å­é—®é¢˜ä¸­ï¼Œå¯¹æŠ—æ€§å»åç½®æé«˜äº†å¹³ç­‰æœºä¼šæ”¹å–„ç‡è¾¾40%ï¼Œå’Œè°å‡å€¼ä¸º0.69ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†ä¸äººå£ç‰¹å¾ç›¸å…³çš„ADç¥ç»ç—…ç†åŠé£é™©å› ç´ çš„å·®å¼‚å¦‚ä½•å½±å“æ¨¡å‹å…¬å¹³æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¯¹æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨MCIå’ŒADè¯Šæ–­ä¸­çš„å…¬å¹³æ€§è¿›è¡Œäº†å…¨é¢åˆ†æã€‚</li>
<li>è€ƒå¯Ÿäº†å¹´é¾„ã€ç§æ—å’Œæ€§åˆ«ç­‰å¤šé˜Ÿåˆ—æ•°æ®é›†ä¸­çš„åè§ã€‚</li>
<li>è¯„ä¼°äº†ä¸åŒå…¬å¹³æ€§å®šä¹‰å’ŒæŒ‡æ ‡çš„å¯é æ€§ã€‚</li>
<li>é€šè¿‡åˆé€‚çš„å…¬å¹³æªæ–½ï¼Œå¯¹æ¯”åˆ†æäº†ä¸åŒçš„åè§ç¼“è§£ç­–ç•¥ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„å¤åˆåº¦é‡æ ‡å‡†ï¼Œç”¨äºå¹³è¡¡å…¬å¹³æ€§å’Œæ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å‘ç°å­˜åœ¨ä¸å¹´é¾„å’Œç§æ—ç›¸å…³çš„åè§ï¼Œè€Œæ€§åˆ«åè§ä¸æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23528">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6471c74a144b998437897fc50df33458.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c7691f0a14cf26c19d85b6e613b2457.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31c12c250bca4a1103ad39de2b5e6558.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a578dc35bfee846da3080eb48dc0bfd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-659bd31422e14a35a5b6f2eec0cb176a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1def417dc53fed2bf45b9c813f99a273.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LAFR-Efficient-Diffusion-based-Blind-Face-Restoration-via-Latent-Codebook-Alignment-Adapter"><a href="#LAFR-Efficient-Diffusion-based-Blind-Face-Restoration-via-Latent-Codebook-Alignment-Adapter" class="headerlink" title="LAFR: Efficient Diffusion-based Blind Face Restoration via Latent   Codebook Alignment Adapter"></a>LAFR: Efficient Diffusion-based Blind Face Restoration via Latent   Codebook Alignment Adapter</h2><p><strong>Authors:Runyi Li, Bin Chen, Jian Zhang, Radu Timofte</strong></p>
<p>Blind face restoration from low-quality (LQ) images is a challenging task that requires not only high-fidelity image reconstruction but also the preservation of facial identity. While diffusion models like Stable Diffusion have shown promise in generating high-quality (HQ) images, their VAE modules are typically trained only on HQ data, resulting in semantic misalignment when encoding LQ inputs. This mismatch significantly weakens the effectiveness of LQ conditions during the denoising process. Existing approaches often tackle this issue by retraining the VAE encoder, which is computationally expensive and memory-intensive. To address this limitation efficiently, we propose LAFR (Latent Alignment for Face Restoration), a novel codebook-based latent space adapter that aligns the latent distribution of LQ images with that of HQ counterparts, enabling semantically consistent diffusion sampling without altering the original VAE. To further enhance identity preservation, we introduce a multi-level restoration loss that combines constraints from identity embeddings and facial structural priors. Additionally, by leveraging the inherent structural regularity of facial images, we show that lightweight finetuning of diffusion prior on just 0.9% of FFHQ dataset is sufficient to achieve results comparable to state-of-the-art methods, reduce training time by 70%. Extensive experiments on both synthetic and real-world face restoration benchmarks demonstrate the effectiveness and efficiency of LAFR, achieving high-quality, identity-preserving face reconstruction from severely degraded inputs. </p>
<blockquote>
<p>ä»ä½è´¨é‡ï¼ˆLQï¼‰å›¾åƒä¸­æ¢å¤ç›²è„¸æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä¸ä»…éœ€è¦é«˜ä¿çœŸåº¦çš„å›¾åƒé‡å»ºï¼Œè¿˜éœ€è¦ä¿æŒé¢éƒ¨èº«ä»½ã€‚è™½ç„¶åƒStable Diffusionè¿™æ ·çš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡ï¼ˆHQï¼‰å›¾åƒæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬çš„VAEæ¨¡å—é€šå¸¸ä»…åœ¨HQæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´åœ¨ç¼–ç LQè¾“å…¥æ—¶å‡ºç°è¯­ä¹‰ä¸å¯¹é½ã€‚è¿™ç§ä¸åŒ¹é…ä¼šæ˜¾è‘—å‰Šå¼±å»å™ªè¿‡ç¨‹ä¸­LQæ¡ä»¶çš„æœ‰æ•ˆæ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡é‡æ–°è®­ç»ƒVAEç¼–ç å™¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿™åœ¨è®¡ç®—ä¸Šæ—¢æ˜‚è´µåˆå¯†é›†ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†LAFRï¼ˆé¢éƒ¨æ¢å¤çš„æ½œåœ¨å¯¹é½ï¼Œä¸€ç§åŸºäºä»£ç æœ¬çš„æ–°å‹æ½œåœ¨ç©ºé—´é€‚é…å™¨ï¼‰ï¼Œå®ƒå¯ä»¥å¯¹é½LQå›¾åƒçš„æ½œåœ¨åˆ†å¸ƒä¸HQå›¾åƒçš„æ½œåœ¨åˆ†å¸ƒï¼Œåœ¨ä¸æ”¹å˜åŸå§‹VAEçš„æƒ…å†µä¸‹å®ç°è¯­ä¹‰ä¸€è‡´çš„æ‰©æ•£é‡‡æ ·ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºèº«ä»½ä¿æŒèƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šçº§æ¢å¤æŸå¤±ï¼Œå®ƒç»“åˆäº†èº«ä»½åµŒå…¥å’Œé¢éƒ¨ç»“æ„å…ˆéªŒçš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨é¢éƒ¨å›¾åƒå›ºæœ‰çš„ç»“æ„è§„å¾‹ï¼Œæˆ‘ä»¬è¯æ˜åœ¨ä»…ä½¿ç”¨FFHQæ•°æ®é›†çš„0.9%è¿›è¡Œæ‰©æ•£å…ˆéªŒçš„è½»é‡çº§å¾®è°ƒè¶³ä»¥å®ç°ä¸æœ€æ–°æ–¹æ³•ç›¸å½“çš„ç»“æœï¼Œå¹¶å°†è®­ç»ƒæ—¶é—´å‡å°‘70%ã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œé¢éƒ¨æ¢å¤åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†LAFRçš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ï¼Œå®ç°äº†ä»ä¸¥é‡é€€åŒ–çš„è¾“å…¥ä¸­è¿›è¡Œé«˜è´¨é‡ã€ä¿æŒèº«ä»½çš„é¢éƒ¨é‡å»ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23462v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºLAFRçš„æ–¹æ³•ï¼Œç”¨äºä»ä½è´¨é‡å›¾åƒä¸­æ¢å¤ç›²è„¸ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»£ç æœ¬åŸºäºçš„æ½œåœ¨ç©ºé—´é€‚é…å™¨ï¼Œå®ç°å¯¹æ½œåœ¨ç©ºé—´å†…ä½è´¨é‡å›¾åƒå’Œé«˜è´¨é‡å›¾åƒä¹‹é—´çš„å¯¹é½ã€‚æ­¤å¤–ï¼Œå¼•å…¥å¤šå±‚æ¬¡æ¢å¤æŸå¤±ä»¥åŠ å¼ºèº«ä»½ä¿ç•™ï¼Œå¹¶é€šè¿‡åˆ©ç”¨é¢éƒ¨å›¾åƒçš„ç»“æ„è§„å¾‹æ€§ï¼Œå®ç°äº†é«˜æ•ˆçš„è®­ç»ƒå¹¶ä¿è¯äº†æ¢å¤ç»“æœçš„è‰¯å¥½è´¨é‡ã€‚LAFRèƒ½æœ‰æ•ˆåˆæˆå‡ºé«˜ä¿çœŸã€ä¿ç•™èº«ä»½çš„ä½è´¨é‡è¾“å…¥é¢éƒ¨é‡å»ºç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LAFRæ–¹æ³•ä½¿ç”¨ä»£ç æœ¬åŸºç¡€çš„æ½œåœ¨ç©ºé—´é€‚é…å™¨ï¼Œå®ç°äº†ä½è´¨é‡ï¼ˆLQï¼‰å’Œé«˜è´¨é‡ï¼ˆHQï¼‰å›¾åƒä¹‹é—´çš„æ½œåœ¨åˆ†å¸ƒå¯¹é½ã€‚</li>
<li>é€šè¿‡å¼•å…¥å¤šå±‚æ¬¡æ¢å¤æŸå¤±ï¼ŒåŠ å¼ºäº†èº«ä»½ä¿ç•™ï¼Œç¡®ä¿åœ¨å»é™¤å™ªå£°è¿‡ç¨‹ä¸­ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>åˆ©ç”¨é¢éƒ¨å›¾åƒçš„ç»“æ„è§„å¾‹æ€§ï¼Œå®ç°äº†é«˜æ•ˆè®­ç»ƒå¹¶æé«˜äº†æ¢å¤ç»“æœçš„ä¿çœŸåº¦ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹çš„è½»é‡çº§å¾®è°ƒæ‰©æ•£å…ˆéªŒæ–¹æ³•ï¼Œä»…ä½¿ç”¨FFHQæ•°æ®é›†çš„0.9%å³å¯è¾¾åˆ°ä¸æœ€æ–°æ–¹æ³•ç›¸å½“çš„ç»“æœã€‚</li>
<li>æ–¹æ³•èƒ½æœ‰æ•ˆç¼©çŸ­è®­ç»ƒæ—¶é—´ï¼Œè¾¾åˆ°70%çš„å‡å°‘ã€‚</li>
<li>åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œçš„é¢éƒ¨æ¢å¤åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†LAFRçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23462">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c95342d5ce246371a50724168ec94137.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa300c0ed08360fc9ae9a8557c55db84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0f7135a2f3b2f948f73a55e750d199f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7d760b621fe10b5a655784987fc1eba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e42b5a939afb8f6f7931d5f75dbd7afd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Comparison-of-water-models-for-structure-prediction"><a href="#Comparison-of-water-models-for-structure-prediction" class="headerlink" title="Comparison of water models for structure prediction"></a>Comparison of water models for structure prediction</h2><p><strong>Authors:BÃ¡lint SoczÃ³, IldikÃ³ Pethes</strong></p>
<p>Describing the interactions of water molecules is one of the most common, yet critical, tasks in molecular dynamics simulations. Because of its unique properties, hundreds of attempts have been made to construct an ideal interaction potential model for water. In various studies, the models have been evaluated based on their ability to reproduce different properties of water. This work focuses on the atomic-scale structure in the liquid phase of water. Forty-four classical water potential models are compared to identify those that can accurately describe the structure in alignment with experimental results. In addition to some older models that are still popular today, new or re-parametrized classical models using effective pair-additive potentials that have appeared in recent years are examined. Molecular dynamics simulations were performed over a wide range of temperatures and the resulting trajectories were used to calculate the partial radial distribution functions. The total scattering structure factors were compared with data from neutron and X-ray diffraction experiments. Our analysis indicates that models with more than four interaction sites, as well as flexible or polarizable models with higher computational requirements, do not provide a significant advantage in accurately describing the structure. On the other hand, recent three-site models have made considerable progress in this area, although the best agreement with experimental data over the entire temperature range was achieved with four-site, TIP4P-type models. </p>
<blockquote>
<p>æè¿°æ°´åˆ†å­é—´çš„ç›¸äº’ä½œç”¨æ˜¯åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿä¸­æœ€å¸¸è§ä½†ä¹Ÿæ˜¯è‡³å…³é‡è¦çš„ä»»åŠ¡ä¹‹ä¸€ã€‚ç”±äºå…¶ç‹¬ç‰¹çš„ç‰¹æ€§ï¼Œäººä»¬å·²ç»æ„å»ºäº†æ•°ç™¾ä¸ªç†æƒ³ç›¸äº’ä½œç”¨åŠ¿æ¨¡å‹æ¥æ¨¡æ‹Ÿæ°´ã€‚åœ¨å„ç§ç ”ç©¶ä¸­ï¼Œè¿™äº›æ¨¡å‹çš„è¯„ä»·åŸºç¡€æ˜¯å®ƒä»¬é‡ç°æ°´çš„ä¸åŒç‰¹æ€§çš„èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œä¸“æ³¨äºæ¶²æ€æ°´åŸå­å°ºåº¦çš„ç»“æ„ã€‚å¯¹æ¯”äº†44ç§ç»å…¸çš„æ°´åŠ¿æ¨¡å‹ï¼Œä»¥è¯†åˆ«é‚£äº›èƒ½å¤Ÿå‡†ç¡®æè¿°ç»“æ„ä¸å®éªŒç»“æœç›¸ç¬¦åˆçš„æ¨¡å‹ã€‚é™¤äº†ä»Šå¤©ä»ç„¶å¾ˆæµè¡Œçš„ä¸€äº›æ—§æ¨¡å‹å¤–ï¼Œè¿˜æ£€æŸ¥äº†è¿‘å¹´æ¥å‡ºç°çš„ä½¿ç”¨æœ‰æ•ˆå¯¹åŠ åŠ¿çš„æ–°æˆ–é‡æ–°å‚æ•°åŒ–çš„ç»å…¸æ¨¡å‹ã€‚åœ¨è¾ƒå®½çš„æ¸©åº¦èŒƒå›´å†…è¿›è¡Œäº†åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿï¼Œå¹¶ä½¿ç”¨å¾—åˆ°çš„è½¨è¿¹è®¡ç®—äº†éƒ¨åˆ†å¾„å‘åˆ†å¸ƒå‡½æ•°ã€‚æ€»æ•£å°„ç»“æ„å› å­ä¸ä¸­å­è¡å°„å’ŒXå°„çº¿è¡å°„å®éªŒæ•°æ®è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå…·æœ‰è¶…è¿‡å››ä¸ªç›¸äº’ä½œç”¨ä½ç‚¹çš„æ¨¡å‹ï¼Œä»¥åŠå…·æœ‰æ›´é«˜è®¡ç®—è¦æ±‚çš„çµæ´»æˆ–æåŒ–æ¨¡å‹ï¼Œåœ¨å‡†ç¡®æè¿°ç»“æ„æ–¹é¢å¹¶æ²¡æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚å¦ä¸€æ–¹é¢ï¼Œæœ€è¿‘çš„ä¸‰ä¸ªä½ç‚¹çš„æ¨¡å‹åœ¨è¿™ä¸€é¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œå°½ç®¡åœ¨æ•´ä¸ªæ¸©åº¦èŒƒå›´å†…ä¸å®éªŒæ•°æ®å»åˆåº¦æœ€å¥½çš„æ˜¯å››ä½ç‚¹TIP4Pç±»å‹çš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23446v1">PDF</a> 97 pages together with supplementary; submitted to Journal of   Molecular Liqudis</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¯”è¾ƒäº†44ç§ç»å…¸æ°´åŠ¿æ¨¡å‹ï¼Œä»¥è¯†åˆ«èƒ½å¤Ÿå‡†ç¡®æè¿°æ¶²æ€æ°´åŸå­å°ºåº¦ç»“æ„å¹¶ä¸å®éªŒç»“æœä¸€è‡´çš„æ¨¡å‹ã€‚é€šè¿‡åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿï¼Œå‘ç°è¶…è¿‡å››ä¸ªç›¸äº’ä½œç”¨ä½ç‚¹çš„æ¨¡å‹ä»¥åŠè®¡ç®—è¦æ±‚è¾ƒé«˜çš„çµæ´»æˆ–æåŒ–æ¨¡å‹å¹¶æ²¡æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚ç›¸åï¼Œæœ€æ–°çš„ä¸‰ä½ç‚¹æ¨¡å‹åœ¨è¿™æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä¸æ•´ä¸ªæ¸©åº¦èŒƒå›´å†…çš„å®éªŒç»“æœæœ€ä½³å»åˆçš„æ˜¯å››ä½ç‚¹TIP4På‹æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æ¯”è¾ƒäº†å¤šç§æ°´åŠ¿æ¨¡å‹ä»¥æè¿°æ¶²æ€æ°´çš„åŸå­å°ºåº¦ç»“æ„ã€‚</li>
<li>é€šè¿‡åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿå¯¹æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>å‘ç°è¶…è¿‡å››ä¸ªç›¸äº’ä½œç”¨ä½ç‚¹çš„æ¨¡å‹å¹¶æœªæä¾›æ˜¾è‘—çš„æè¿°ç»“æ„å‡†ç¡®æ€§ä¼˜åŠ¿ã€‚</li>
<li>çµæ´»æˆ–æåŒ–æ¨¡å‹åœ¨é«˜è®¡ç®—è¦æ±‚ä¸‹å¹¶æœªæ˜¾ç¤ºå‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚</li>
<li>ä¸‰ä½ç‚¹æ¨¡å‹åœ¨æè¿°æ¶²æ€æ°´ç»“æ„æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>å››ä½ç‚¹TIP4På‹æ¨¡å‹åœ¨æ•´ä¸ªæ¸©åº¦èŒƒå›´å†…ä¸å®éªŒç»“æœæœ€ä½³å»åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab21e3226fcdff8261613400e5061964.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Self-supervised-feature-learning-for-cardiac-Cine-MR-image-reconstruction"><a href="#Self-supervised-feature-learning-for-cardiac-Cine-MR-image-reconstruction" class="headerlink" title="Self-supervised feature learning for cardiac Cine MR image   reconstruction"></a>Self-supervised feature learning for cardiac Cine MR image   reconstruction</h2><p><strong>Authors:Siying Xu, Marcel FrÃ¼h, Kerstin Hammernik, Andreas Lingg, Jens KÃ¼bler, Patrick Krumm, Daniel Rueckert, Sergios Gatidis, Thomas KÃ¼stner</strong></p>
<p>We propose a self-supervised feature learning assisted reconstruction (SSFL-Recon) framework for MRI reconstruction to address the limitation of existing supervised learning methods. Although recent deep learning-based methods have shown promising performance in MRI reconstruction, most require fully-sampled images for supervised learning, which is challenging in practice considering long acquisition times under respiratory or organ motion. Moreover, nearly all fully-sampled datasets are obtained from conventional reconstruction of mildly accelerated datasets, thus potentially biasing the achievable performance. The numerous undersampled datasets with different accelerations in clinical practice, hence, remain underutilized. To address these issues, we first train a self-supervised feature extractor on undersampled images to learn sampling-insensitive features. The pre-learned features are subsequently embedded in the self-supervised reconstruction network to assist in removing artifacts. Experiments were conducted retrospectively on an in-house 2D cardiac Cine dataset, including 91 cardiovascular patients and 38 healthy subjects. The results demonstrate that the proposed SSFL-Recon framework outperforms existing self-supervised MRI reconstruction methods and even exhibits comparable or better performance to supervised learning up to $16\times$ retrospective undersampling. The feature learning strategy can effectively extract global representations, which have proven beneficial in removing artifacts and increasing generalization ability during reconstruction. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªç›‘ç£ç‰¹å¾å­¦ä¹ è¾…åŠ©é‡å»ºï¼ˆSSFL-Reconï¼‰æ¡†æ¶ï¼Œç”¨äºMRIé‡å»ºï¼Œä»¥è§£å†³ç°æœ‰ç›‘ç£å­¦ä¹ æ–¹æ³•çš„å±€é™æ€§ã€‚å°½ç®¡æœ€è¿‘åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åœ¨MRIé‡å»ºä¸­æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å¤§å¤šæ•°éœ€è¦å®Œå…¨é‡‡æ ·çš„å›¾åƒè¿›è¡Œæœ‰ç›‘ç£å­¦ä¹ ï¼Œè¿™åœ¨è€ƒè™‘å‘¼å¸æˆ–å™¨å®˜è¿åŠ¨çš„é•¿æ—¶é—´é‡‡é›†æ—¶ï¼Œå®è·µä¸­å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œå‡ ä¹æ‰€æœ‰å®Œå…¨é‡‡æ ·çš„æ•°æ®é›†éƒ½æ˜¯ä»è½»åº¦åŠ é€Ÿæ•°æ®é›†çš„å¸¸è§„é‡å»ºä¸­è·å¾—çš„ï¼Œè¿™å¯èƒ½æ½œåœ¨åœ°å½±å“å¯è¾¾åˆ°çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œä¸´åºŠå®è·µä¸­è®¸å¤šä¸åŒåŠ é€Ÿç¨‹åº¦çš„æ¬ é‡‡æ ·æ•°æ®é›†ä»æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒä¸€ä¸ªè‡ªç›‘ç£ç‰¹å¾æå–å™¨ï¼Œç”¨äºä»æ¬ é‡‡æ ·å›¾åƒä¸­å­¦ä¹ é‡‡æ ·æ— å…³ç‰¹å¾ã€‚é¢„å­¦ä¹ åˆ°çš„ç‰¹å¾éšååµŒå…¥è‡ªç›‘ç£é‡å»ºç½‘ç»œä¸­ï¼Œä»¥å¸®åŠ©å»é™¤ä¼ªå½±ã€‚å®éªŒå¯¹å†…éƒ¨2Då¿ƒè„ç”µå½±æ•°æ®é›†è¿›è¡Œäº†å›é¡¾æ€§ç ”ç©¶ï¼ŒåŒ…æ‹¬91åå¿ƒè¡€ç®¡æ‚£è€…å’Œ38åå¥åº·å—è¯•è€…ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SSFL-Reconæ¡†æ¶ä¼˜äºç°æœ‰çš„è‡ªç›‘ç£MRIé‡å»ºæ–¹æ³•ï¼Œç”šè‡³åœ¨$16\times$å›é¡¾æ€§æ¬ é‡‡æ ·çš„æƒ…å†µä¸‹ï¼Œå…¶æ€§èƒ½ä¸æœ‰ç›‘ç£å­¦ä¹ ç›¸å½“ç”šè‡³æ›´å¥½ã€‚ç‰¹å¾å­¦ä¹ ç­–ç•¥å¯ä»¥æœ‰æ•ˆåœ°æå–å…¨å±€è¡¨ç¤ºï¼Œè¿™åœ¨å»é™¤ä¼ªå½±å’Œæé«˜é‡å»ºè¿‡ç¨‹ä¸­çš„æ³›åŒ–èƒ½åŠ›æ–¹é¢è¯æ˜äº†å…¶å¥½å¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23408v1">PDF</a> Accepted to IEEE Transactions on Medical Imaging (TMI), 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£ç‰¹å¾å­¦ä¹ çš„MRIé‡å»ºæ¡†æ¶ï¼ˆSSFL-Reconï¼‰ï¼Œä»¥è§£å†³ç°æœ‰ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨MRIé‡å»ºä¸­çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨è‡ªç›‘ç£ç¯å¢ƒä¸‹å­¦ä¹ é‡‡æ ·æ— å…³çš„ç‰¹å¾ï¼Œè¿›è€Œè¾…åŠ©å»é™¤é‡å»ºè¿‡ç¨‹ä¸­çš„ä¼ªå½±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å›é¡¾æ€§æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå³ä½¿åœ¨é«˜è¾¾16å€çš„å›é¡¾æ€§æ¬ é‡‡æ ·æƒ…å†µä¸‹ï¼Œå…¶æ€§èƒ½ä¹Ÿä¸ç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†è‡ªç›‘ç£ç‰¹å¾å­¦ä¹ è¾…åŠ©çš„MRIé‡å»ºæ¡†æ¶ï¼ˆSSFL-Reconï¼‰ã€‚</li>
<li>è§£å†³äº†ç°æœ‰ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨MRIé‡å»ºä¸­çš„å±€é™æ€§ã€‚</li>
<li>é€šè¿‡è‡ªç›‘ç£ç‰¹å¾æå–å™¨è®­ç»ƒï¼Œå­¦ä¹ é‡‡æ ·æ— å…³çš„ç‰¹å¾ã€‚</li>
<li>é¢„å­¦ä¹ ç‰¹å¾åµŒå…¥åˆ°è‡ªç›‘ç£é‡å»ºç½‘ç»œä¸­ï¼Œä»¥è¾…åŠ©å»é™¤ä¼ªå½±ã€‚</li>
<li>å®éªŒåœ¨å†…éƒ¨2Då¿ƒè„ç”µå½±æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œæ¶µç›–äº†å¿ƒè¡€ç®¡æ‚£è€…å’Œå¥åº·å—è¯•è€…ã€‚</li>
<li>SSFL-Reconæ¡†æ¶æ€§èƒ½ä¼˜äºç°æœ‰è‡ªç›‘ç£MRIé‡å»ºæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23408">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25765b0def350ca4ead673de9a0f16c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-655af3d94300c7a668f2928cf3596f81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eaff06279e45969f6a22284e6b6a953c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36b0a97ebd8a8b66e29e4aa696b90481.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Point-or-Line-Using-Line-based-Representation-for-Panoptic-Symbol-Spotting-in-CAD-Drawings"><a href="#Point-or-Line-Using-Line-based-Representation-for-Panoptic-Symbol-Spotting-in-CAD-Drawings" class="headerlink" title="Point or Line? Using Line-based Representation for Panoptic Symbol   Spotting in CAD Drawings"></a>Point or Line? Using Line-based Representation for Panoptic Symbol   Spotting in CAD Drawings</h2><p><strong>Authors:Xingguang Wei, Haomin Wang, Shenglong Ye, Ruifeng Luo, Yanting Zhang, Lixin Gu, Jifeng Dai, Yu Qiao, Wenhai Wang, Hongjie Zhang</strong></p>
<p>We study the task of panoptic symbol spotting, which involves identifying both individual instances of countable things and the semantic regions of uncountable stuff in computer-aided design (CAD) drawings composed of vector graphical primitives. Existing methods typically rely on image rasterization, graph construction, or point-based representation, but these approaches often suffer from high computational costs, limited generality, and loss of geometric structural information. In this paper, we propose VecFormer, a novel method that addresses these challenges through line-based representation of primitives. This design preserves the geometric continuity of the original primitive, enabling more accurate shape representation while maintaining a computation-friendly structure, making it well-suited for vector graphic understanding tasks. To further enhance prediction reliability, we introduce a Branch Fusion Refinement module that effectively integrates instance and semantic predictions, resolving their inconsistencies for more coherent panoptic outputs. Extensive experiments demonstrate that our method establishes a new state-of-the-art, achieving 91.1 PQ, with Stuff-PQ improved by 9.6 and 21.2 points over the second-best results under settings with and without prior information, respectively, highlighting the strong potential of line-based representation as a foundation for vector graphic understanding. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†å…¨æ™¯ç¬¦å·è¯†åˆ«ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ¶‰åŠè¯†åˆ«ç”±çŸ¢é‡å›¾å½¢åŸºæœ¬å…ƒç´ ç»„æˆçš„è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç»˜å›¾ä¸­çš„å¯æ•°äº‹ç‰©çš„å„ä¸ªå®ä¾‹ä»¥åŠä¸å¯æ•°å†…å®¹çš„è¯­ä¹‰åŒºåŸŸã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå›¾åƒæ …æ ¼åŒ–ã€å›¾æ„å»ºæˆ–åŸºäºç‚¹çš„è¡¨ç¤ºï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€é€šç”¨æ€§å·®ä»¥åŠå‡ ä½•ç»“æ„ä¿¡æ¯ä¸¢å¤±ç­‰é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†VecFormerè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡åŸºäºçº¿æ¡çš„åŸºæœ¬å…ƒç´ è¡¨ç¤ºæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚è¿™ç§è®¾è®¡ä¿ç•™äº†åŸå§‹åŸºæœ¬å…ƒç´ çš„å‡ ä½•è¿ç»­æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒè®¡ç®—å‹å¥½çš„ç»“æ„çš„åŒæ—¶å®ç°æ›´å‡†ç¡®çš„å½¢çŠ¶è¡¨ç¤ºï¼Œä½¿å…¶éå¸¸é€‚åˆçŸ¢é‡å›¾å½¢ç†è§£ä»»åŠ¡ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜é¢„æµ‹å¯é æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†æ”¯èåˆç»†åŒ–æ¨¡å—ï¼Œè¯¥æ¨¡å—æœ‰æ•ˆåœ°èåˆäº†å®ä¾‹å’Œè¯­ä¹‰é¢„æµ‹ï¼Œè§£å†³äº†å®ƒä»¬çš„ä¸ä¸€è‡´æ€§ï¼Œä»è€Œè·å¾—äº†æ›´è¿è´¯çš„å…¨æ™¯è¾“å‡ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æ–°çš„å…ˆè¿›æ°´å¹³ï¼Œå®ç°äº†91.1çš„PQå€¼ã€‚åœ¨å¸¦æœ‰å’Œä¸å¸¦æœ‰å…ˆéªŒä¿¡æ¯çš„è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Stuff-PQä¸Šåˆ†åˆ«æé«˜äº†9.6å’Œ21.2ä¸ªç™¾åˆ†ç‚¹ï¼Œè¿™çªæ˜¾äº†åŸºäºçº¿æ¡è¡¨ç¤ºä½œä¸ºçŸ¢é‡å›¾å½¢ç†è§£åŸºç¡€çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23395v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è®¡ç®—æœºè§†è§‰ä¸­çš„å…¨æ™¯ç¬¦å·è¯†åˆ«ä»»åŠ¡ï¼Œå³åœ¨è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰çŸ¢é‡å›¾å½¢ä¸­è¯†åˆ«å¯æ•°å¯¹è±¡çš„ä¸ªä½“å®ä¾‹å’Œä¸å¯æ•°åŒºåŸŸçš„è¯­ä¹‰ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å­˜åœ¨çš„è®¡ç®—æˆæœ¬é«˜ã€é€šç”¨æ€§å·®ä»¥åŠå‡ ä½•ç»“æ„ä¿¡æ¯ä¸¢å¤±ç­‰é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVecFormerçš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºçº¿æ¡çš„å›¾å½¢è¡¨ç¤ºï¼Œæœ‰æ•ˆä¿ç•™äº†åŸå§‹å›¾å½¢çš„å‡ ä½•è¿ç»­æ€§ï¼Œæé«˜äº†å½¢çŠ¶è¡¨ç¤ºçš„å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œå°¤å…¶é€‚ç”¨äºçŸ¢é‡å›¾å½¢ç†è§£ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ä¸ªåä¸ºBranch Fusion Refinementçš„æ¨¡å—ï¼Œç”¨äºæé«˜é¢„æµ‹ç»“æœçš„å¯é æ€§ï¼Œé€šè¿‡æ•´åˆå®ä¾‹å’Œè¯­ä¹‰é¢„æµ‹ç»“æœï¼Œè§£å†³äº†å®ƒä»¬ä¹‹é—´ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œç”Ÿæˆæ›´è¿è´¯çš„å…¨æ™¯è¾“å‡ºç»“æœã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æ–°çš„ç ”ç©¶æ°´å¹³ï¼Œåœ¨æœ‰æˆ–æ— å…ˆéªŒä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œç›¸è¾ƒäºç¬¬äºŒåç»“æœåˆ†åˆ«æé«˜äº†9.6å’Œ21.2ç‚¹çš„Stuff-PQå€¼ï¼Œæ˜¾ç¤ºå‡ºåŸºäºçº¿æ¡è¡¨ç¤ºçš„çŸ¢é‡å›¾å½¢ç†è§£çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VecFormeræ–¹æ³•è§£å†³äº†è®¡ç®—æœºè§†è§‰ä¸­å…¨æ™¯ç¬¦å·è¯†åˆ«ä»»åŠ¡çš„é—®é¢˜ï¼Œé€‚ç”¨äºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰çŸ¢é‡å›¾å½¢çš„ç†è§£ã€‚</li>
<li>VecFormeré‡‡ç”¨åŸºäºçº¿æ¡çš„å›¾å½¢è¡¨ç¤ºï¼Œä¿ç•™äº†åŸå§‹å›¾å½¢çš„å‡ ä½•è¿ç»­æ€§ï¼Œæé«˜äº†å½¢çŠ¶è¡¨ç¤ºçš„å‡†ç¡®æ€§ã€‚</li>
<li>VecFormeræ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œé€‚ç”¨äºçŸ¢é‡å›¾å½¢ç†è§£ä»»åŠ¡ã€‚</li>
<li>Branch Fusion Refinementæ¨¡å—æé«˜äº†é¢„æµ‹ç»“æœçš„å¯é æ€§ï¼Œè§£å†³äº†å®ä¾‹å’Œè¯­ä¹‰é¢„æµ‹ä¹‹é—´çš„ä¸ä¸€è‡´é—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºVecFormeræ–¹æ³•åœ¨å…¨æ™¯ç¬¦å·è¯†åˆ«ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æ–°çš„ç ”ç©¶æ°´å¹³ã€‚</li>
<li>VecFormeræ–¹æ³•åœ¨æœ‰æˆ–æ— å…ˆéªŒä¿¡æ¯çš„æƒ…å†µä¸‹å‡è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ed89f68a6a27c6eef4324b1a16d8a29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e103ae8a0c8b56686a26f3c399667775.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b284b97584f8e009b8fd34cacef6124a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Federated-Unsupervised-Semantic-Segmentation"><a href="#Federated-Unsupervised-Semantic-Segmentation" class="headerlink" title="Federated Unsupervised Semantic Segmentation"></a>Federated Unsupervised Semantic Segmentation</h2><p><strong>Authors:Evangelos Charalampakis, Vasileios Mygdalis, Ioannis Pitas</strong></p>
<p>This work explores the application of Federated Learning (FL) in Unsupervised Semantic image Segmentation (USS). Recent USS methods extract pixel-level features using frozen visual foundation models and refine them through self-supervised objectives that encourage semantic grouping. These features are then grouped to semantic clusters to produce segmentation masks. Extending these ideas to federated settings requires feature representation and cluster centroid alignment across distributed clients â€“ an inherently difficult task under heterogeneous data distributions in the absence of supervision. To address this, we propose FUSS Federated Unsupervised image Semantic Segmentation) which is, to our knowledge, the first framework to enable fully decentralized, label-free semantic segmentation training. FUSS introduces novel federation strategies that promote global consistency in feature and prototype space, jointly optimizing local segmentation heads and shared semantic centroids. Experiments on both benchmark and real-world datasets, including binary and multi-class segmentation tasks, show that FUSS consistently outperforms local-only client trainings as well as extensions of classical FL algorithms under varying client data distributions. To support reproducibility, full code will be released upon manuscript acceptance. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰åœ¨æ— ç›‘ç£è¯­ä¹‰å›¾åƒåˆ†å‰²ï¼ˆUSSï¼‰ä¸­çš„åº”ç”¨ã€‚æœ€è¿‘çš„USSæ–¹æ³•ä½¿ç”¨å›ºå®šçš„è§†è§‰åŸºç¡€æ¨¡å‹æå–åƒç´ çº§ç‰¹å¾ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘ç›‘ç£ç›®æ ‡å¯¹å…¶è¿›è¡Œç»†åŒ–ï¼Œè¿™äº›ç›®æ ‡é¼“åŠ±è¯­ä¹‰åˆ†ç»„ã€‚ç„¶åï¼Œè¿™äº›ç‰¹å¾è¢«åˆ†ç»„åˆ°è¯­ä¹‰é›†ç¾¤ä¸­ï¼Œä»¥äº§ç”Ÿåˆ†å‰²æ©è†œã€‚å°†è¿™äº›æ€æƒ³æ‰©å±•åˆ°è”é‚¦ç¯å¢ƒéœ€è¦è·¨åˆ†å¸ƒå¼å®¢æˆ·ç«¯è¿›è¡Œç‰¹å¾è¡¨ç¤ºå’Œèšç±»ä¸­å¿ƒå¯¹é½â€”â€”åœ¨ç¼ºä¹ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œåœ¨å¼‚è´¨æ•°æ®åˆ†å¸ƒä¸­è¿™æ˜¯ä¸€é¡¹å†…åœ¨å›°éš¾çš„ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FUSSï¼ˆè”é‚¦æ— ç›‘ç£å›¾åƒè¯­ä¹‰åˆ†å‰²ï¼‰ï¼ˆFederated Unsupervised Semantic Segmentationï¼‰ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿå®ç°å®Œå…¨å»ä¸­å¿ƒåŒ–ã€æ— æ ‡ç­¾è¯­ä¹‰åˆ†å‰²è®­ç»ƒçš„æ¡†æ¶ã€‚FUSSå¼•å…¥äº†æ–°å‹è”é‚¦ç­–ç•¥ï¼Œä¿ƒè¿›ç‰¹å¾å’ŒåŸå‹ç©ºé—´ä¸­çš„å…¨å±€ä¸€è‡´æ€§ï¼Œè”åˆä¼˜åŒ–æœ¬åœ°åˆ†å‰²å¤´å’Œå…±äº«è¯­ä¹‰ä¸­å¿ƒã€‚åœ¨åŸºå‡†æ•°æ®é›†å’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒï¼ŒåŒ…æ‹¬äºŒè¿›åˆ¶å’Œå¤šç±»åˆ†å‰²ä»»åŠ¡ï¼Œè¡¨æ˜FUSSåœ¨å„ç§å®¢æˆ·ç«¯æ•°æ®åˆ†å¸ƒä¸‹ï¼Œå§‹ç»ˆä¼˜äºä»…æœ¬åœ°çš„å®¢æˆ·ç«¯è®­ç»ƒå’Œç»å…¸FLç®—æ³•çš„æ‰©å±•ã€‚ä¸ºäº†æ”¯æŒå¤ç°ï¼Œè®ºæ–‡æ¥å—åå°†å…¬å¸ƒå®Œæ•´ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23292v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬å·¥ä½œæ¢è®¨äº†è”é‚¦å­¦ä¹ åœ¨æ— äººç›‘ç£è¯­ä¹‰å›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨ã€‚æ–‡ç« ä»‹ç»äº†FUSSï¼ˆè”é‚¦æ— äººç›‘ç£å›¾åƒè¯­ä¹‰åˆ†å‰²ï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½åœ¨å®Œå…¨åˆ†æ•£ã€æ— éœ€æ ‡ç­¾çš„è¯­ä¹‰åˆ†å‰²è®­ç»ƒä¸­å®ç°ç‰¹å¾è¡¨ç¤ºå’Œé›†ç¾¤è´¨å¿ƒå¯¹é½ã€‚FUSSå¼•å…¥æ–°å‹è”é‚¦ç­–ç•¥ï¼Œä¿ƒè¿›ç‰¹å¾ç©ºé—´å’ŒåŸå‹ç©ºé—´ä¸­çš„å…¨å±€ä¸€è‡´æ€§ï¼Œè”åˆä¼˜åŒ–æœ¬åœ°åˆ†å‰²å¤´å’Œå…±äº«è¯­ä¹‰è´¨å¿ƒã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å„ç§å®¢æˆ·ç«¯æ•°æ®åˆ†å¸ƒä¸‹ï¼ŒFUSSåœ¨åŸºå‡†æ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„äºŒè¿›åˆ¶å’Œå¤šç±»åˆ«åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬å·¥ä½œå°†è”é‚¦å­¦ä¹ åº”ç”¨äºæ— äººç›‘ç£è¯­ä¹‰å›¾åƒåˆ†å‰²é¢†åŸŸã€‚</li>
<li>æå‡ºäº†FUSSæ¡†æ¶ï¼Œå®ç°å®Œå…¨åˆ†æ•£ã€æ— éœ€æ ‡ç­¾çš„è¯­ä¹‰åˆ†å‰²è®­ç»ƒã€‚</li>
<li>FUSSé€šè¿‡å¼•å…¥æ–°å‹è”é‚¦ç­–ç•¥ï¼Œä¿ƒè¿›ç‰¹å¾ç©ºé—´å’ŒåŸå‹ç©ºé—´ä¸­çš„å…¨å±€ä¸€è‡´æ€§ã€‚</li>
<li>FUSSèƒ½ä¼˜åŒ–æœ¬åœ°åˆ†å‰²å¤´å’Œå…±äº«è¯­ä¹‰è´¨å¿ƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§å®¢æˆ·ç«¯æ•°æ®åˆ†å¸ƒä¸‹ï¼ŒFUSSåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæœ¬åœ°è®­ç»ƒä»¥åŠç»å…¸è”é‚¦å­¦ä¹ ç®—æ³•çš„æ‰©å±•ã€‚</li>
<li>FUSSæ¡†æ¶æœ‰æœ›è§£å†³åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è¿›è¡Œæ— äººç›‘ç£è¯­ä¹‰å›¾åƒåˆ†å‰²çš„éš¾é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23292">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fdb14e977697b12bdb137178a57c4d74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74364f4340ff6df7fdaf446711253d31.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GenCAD-Self-Repairing-Feasibility-Enhancement-for-3D-CAD-Generation"><a href="#GenCAD-Self-Repairing-Feasibility-Enhancement-for-3D-CAD-Generation" class="headerlink" title="GenCAD-Self-Repairing: Feasibility Enhancement for 3D CAD Generation"></a>GenCAD-Self-Repairing: Feasibility Enhancement for 3D CAD Generation</h2><p><strong>Authors:Chikaha Tsuji, Enrique Flores Medina, Harshit Gupta, Md Ferdous Alam</strong></p>
<p>With the advancement of generative AI, research on its application to 3D model generation has gained traction, particularly in automating the creation of Computer-Aided Design (CAD) files from images. GenCAD is a notable model in this domain, leveraging an autoregressive transformer-based architecture with a contrastive learning framework to generate CAD programs.   However, a major limitation of GenCAD is its inability to consistently produce feasible boundary representations (B-reps), with approximately 10% of generated designs being infeasible. To address this, we propose GenCAD-Self-Repairing, a framework that enhances the feasibility of generative CAD models through diffusion guidance and a self-repairing pipeline. This framework integrates a guided diffusion denoising process in the latent space and a regression-based correction mechanism to refine infeasible CAD command sequences while preserving geometric accuracy. Our approach successfully converted two-thirds of infeasible designs in the baseline method into feasible ones, significantly improving the feasibility rate while simultaneously maintaining a reasonable level of geometric accuracy between the point clouds of ground truth models and generated models.   By significantly improving the feasibility rate of generating CAD models, our approach helps expand the availability of high-quality training data and enhances the applicability of AI-driven CAD generation in manufacturing, architecture, and product design. </p>
<blockquote>
<p>éšç€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è¿›æ­¥ï¼Œå…¶åœ¨3Dæ¨¡å‹ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨ç ”ç©¶é€æ¸å—åˆ°å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»å›¾åƒè‡ªåŠ¨ç”Ÿæˆè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ–‡ä»¶æ–¹é¢ã€‚GenCADæ˜¯è¿™ä¸ªé¢†åŸŸçš„ä¸€ä¸ªæ˜¾è‘—æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨åŸºäºè‡ªå›å½’å˜å‹å™¨æ¶æ„å’Œå¯¹æ¯”å­¦ä¹ æ¡†æ¶æ¥ç”ŸæˆCADç¨‹åºã€‚ç„¶è€Œï¼ŒGenCADçš„ä¸€ä¸ªä¸»è¦å±€é™æ€§æ˜¯ï¼Œå®ƒæ— æ³•å§‹ç»ˆäº§ç”Ÿå¯è¡Œçš„è¾¹ç•Œè¡¨ç¤ºï¼ˆB-repsï¼‰ï¼Œå¤§çº¦10%çš„ç”Ÿæˆè®¾è®¡ä¸å¯è¡Œã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GenCAD-Self-Repairingæ¡†æ¶ï¼Œå®ƒé€šè¿‡æ‰©æ•£å¼•å¯¼å’Œè‡ªæˆ‘ä¿®å¤ç®¡é“å¢å¼ºç”ŸæˆCADæ¨¡å‹çš„å¯è¡Œæ€§ã€‚è¯¥æ¡†æ¶é›†æˆäº†æ½œåœ¨ç©ºé—´ä¸­çš„å¼•å¯¼æ‰©æ•£å»å™ªè¿‡ç¨‹ï¼Œä»¥åŠåŸºäºå›å½’çš„æ ¡æ­£æœºåˆ¶ï¼Œä»¥ç»†åŒ–ä¸å¯è¡Œçš„CADå‘½ä»¤åºåˆ—ï¼ŒåŒæ—¶ä¿ç•™å‡ ä½•ç²¾åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°å°†åŸºçº¿æ–¹æ³•ä¸­ä¸‰åˆ†ä¹‹äºŒçš„ä¸å¯è¡Œè®¾è®¡è½¬åŒ–ä¸ºå¯è¡Œè®¾è®¡ï¼Œåœ¨æ˜¾è‘—æé«˜å¯è¡Œæ€§ç‡çš„åŒæ—¶ï¼Œè¿˜ä¿æŒäº†çœŸå®æ¨¡å‹ä¸ç”Ÿæˆæ¨¡å‹ç‚¹äº‘ä¹‹é—´çš„å‡ ä½•ç²¾åº¦åˆç†æ°´å¹³ã€‚é€šè¿‡æ˜¾è‘—æé«˜ç”ŸæˆCADæ¨¡å‹çš„å¯è¡Œæ€§ç‡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰åŠ©äºä¸°å¯Œé«˜è´¨é‡è®­ç»ƒæ•°æ®çš„å¯ç”¨æ€§ï¼Œå¹¶å¢å¼ºAIé©±åŠ¨çš„CADç”Ÿæˆåœ¨åˆ¶é€ ã€å»ºç­‘å’Œäº§å“è®¾è®¡é¢†åŸŸçš„é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23287v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è¿›æ­¥ï¼Œå…¶åœ¨3Dæ¨¡å‹ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨é€æ¸å—åˆ°å…³æ³¨ï¼Œå°¤å…¶åœ¨ä»å›¾åƒè‡ªåŠ¨ç”Ÿæˆè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ–‡ä»¶æ–¹é¢ã€‚å°½ç®¡GenCADæ¨¡å‹åœ¨è¿™ä¸€é¢†åŸŸè¡¨ç°çªå‡ºï¼Œä½†å…¶æ— æ³•æŒç»­äº§ç”Ÿå¯è¡Œçš„è¾¹ç•Œè¡¨ç¤ºï¼ˆB-repsï¼‰ï¼Œçº¦æœ‰10%çš„è®¾è®¡ä¸å¯è¡Œã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GenCAD-Self-Repairingæ¡†æ¶ï¼Œé€šè¿‡æ‰©æ•£å¼•å¯¼å’Œè‡ªæˆ‘ä¿®å¤ç®¡é“å¢å¼ºç”ŸæˆCADæ¨¡å‹çš„å¯è¡Œæ€§ã€‚è¯¥æ¡†æ¶åœ¨æ½œåœ¨ç©ºé—´ä¸­é›†æˆäº†å¼•å¯¼æ‰©æ•£å»å™ªè¿‡ç¨‹ï¼Œå¹¶é€šè¿‡åŸºäºå›å½’çš„æ ¡æ­£æœºåˆ¶æ¥ä¼˜åŒ–ä¸å¯è¡Œçš„CADå‘½ä»¤åºåˆ—ï¼ŒåŒæ—¶ä¿ç•™å‡ ä½•ç²¾åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°å°†åŸºçº¿æ–¹æ³•ä¸­ä¸‰åˆ†ä¹‹äºŒçš„ä¸å¯è¡Œè®¾è®¡è½¬åŒ–ä¸ºå¯è¡Œè®¾è®¡ï¼Œåœ¨æ˜¾è‘—æé«˜å¯è¡Œæ€§ç‡çš„åŒæ—¶ï¼Œä¿æŒäº†åœ°é¢çœŸå®æ¨¡å‹å’Œç”Ÿæˆæ¨¡å‹ç‚¹äº‘ä¹‹é—´çš„åˆç†å‡ ä½•ç²¾åº¦ã€‚è¿™æœ‰åŠ©äºæ‰©å¤§é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„å¯ç”¨æ€§ï¼Œå¢å¼ºAIé©±åŠ¨çš„CADç”Ÿæˆåœ¨åˆ¶é€ ã€å»ºç­‘å’Œäº§å“è®¾è®¡ä¸­çš„é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨3Dæ¨¡å‹ç”Ÿæˆé¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨ä»å›¾åƒç”ŸæˆCADæ–‡ä»¶æ–¹é¢ï¼Œå·²å¼•èµ·å…³æ³¨ã€‚</li>
<li>GenCADæ˜¯æ­¤é¢†åŸŸçš„ä¸€ä¸ªæ˜¾è‘—æ¨¡å‹ï¼Œä½†å­˜åœ¨çº¦10%çš„è®¾è®¡ä¸å¯è¡Œçš„é—®é¢˜ã€‚</li>
<li>GenCAD-Self-Repairingæ¡†æ¶é€šè¿‡æ‰©æ•£å¼•å¯¼å’Œè‡ªæˆ‘ä¿®å¤ç®¡é“æé«˜CADæ¨¡å‹çš„å¯è¡Œæ€§ã€‚</li>
<li>æ¡†æ¶é€šè¿‡å¼•å¯¼æ‰©æ•£å»å™ªè¿‡ç¨‹å’ŒåŸºäºå›å½’çš„æ ¡æ­£æœºåˆ¶æ¥ä¼˜åŒ–ä¸å¯è¡Œçš„è®¾è®¡ã€‚</li>
<li>è¯¥æ–¹æ³•æˆåŠŸåœ°å°†å¤§å¤šæ•°ä¸å¯è¡Œè®¾è®¡è½¬åŒ–ä¸ºå¯è¡Œè®¾è®¡ã€‚</li>
<li>æ˜¾è‘—æé«˜äº†è®¾è®¡çš„å¯è¡Œæ€§ç‡ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„å‡ ä½•ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23287">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c63d84da584668eaa2212252cb2ae506.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cc08f26316c50e38a3ce77244d1e35f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ce5bc53105f3463aaa2beefe6d02998.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f392c0a996e0fee1b5c3a9982cf07a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe6ea6d4426fa16d67327ddb7afad4de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20c4fd935546f8a0fcec51774c014243.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c37ce38fae6f6f991e6638842b4d5e79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2998355960e36aab8152198f3a47f92e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d75e90a5edc865b8d34a0318d5509762.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90e154a5b7eea9a8279d2b56a6970604.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Image-Aesthetic-Reasoning-A-New-Benchmark-for-Medical-Image-Screening-with-MLLMs"><a href="#Image-Aesthetic-Reasoning-A-New-Benchmark-for-Medical-Image-Screening-with-MLLMs" class="headerlink" title="Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening   with MLLMs"></a>Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening   with MLLMs</h2><p><strong>Authors:Zheng Sun, Yi Wei, Long Yu</strong></p>
<p>Multimodal Large Language Models (MLLMs) are of great application across many domains, such as multimodal understanding and generation. With the development of diffusion models (DM) and unified MLLMs, the performance of image generation has been significantly improved, however, the study of image screening is rare and its performance with MLLMs is unsatisfactory due to the lack of data and the week image aesthetic reasoning ability in MLLMs. In this work, we propose a complete solution to address these problems in terms of data and methodology. For data, we collect a comprehensive medical image screening dataset with 1500+ samples, each sample consists of a medical image, four generated images, and a multiple-choice answer. The dataset evaluates the aesthetic reasoning ability under four aspects: \textit{(1) Appearance Deformation, (2) Principles of Physical Lighting and Shadow, (3) Placement Layout, (4) Extension Rationality}. For methodology, we utilize long chains of thought (CoT) and Group Relative Policy Optimization with Dynamic Proportional Accuracy reward, called DPA-GRPO, to enhance the image aesthetic reasoning ability of MLLMs. Our experimental results reveal that even state-of-the-art closed-source MLLMs, such as GPT-4o and Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic reasoning. In contrast, by leveraging the reinforcement learning approach, we are able to surpass the score of both large-scale models and leading closed-source models using a much smaller model. We hope our attempt on medical image screening will serve as a regular configuration in image aesthetic reasoning in the future. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸéƒ½æœ‰å¾ˆå¥½çš„åº”ç”¨ï¼Œå¦‚å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚éšç€æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å’Œç»Ÿä¸€MLLMsçš„å‘å±•ï¼Œå›¾åƒç”Ÿæˆæ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚ç„¶è€Œï¼Œå…³äºå›¾åƒç­›é€‰çš„ç ”ç©¶å¾ˆå°‘è§ï¼Œå¹¶ä¸”ç”±äºæ•°æ®ç¼ºä¹å’ŒMLLMsä¸­å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›è¾ƒå¼±ï¼Œå…¶æ€§èƒ½å¹¶ä¸ä»¤äººæ»¡æ„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹æ•°æ®å’Œæ–¹æ³•çš„è¿™äº›é—®é¢˜æå‡ºäº†ä¸€ä¸ªå®Œæ•´çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨æ•°æ®æ–¹é¢ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªå…¨é¢çš„åŒ»å­¦å›¾åƒç­›é€‰æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡1500ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…æ‹¬ä¸€å¼ åŒ»å­¦å›¾åƒã€å››å¼ ç”Ÿæˆçš„å›¾åƒå’Œä¸€ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ã€‚è¯¥æ•°æ®é›†ä»ä»¥ä¸‹å››ä¸ªæ–¹é¢è¯„ä¼°ç¾å­¦æ¨ç†èƒ½åŠ›ï¼šï¼ˆ1ï¼‰å¤–è§‚å˜å½¢ã€ï¼ˆ2ï¼‰ç‰©ç†ç…§æ˜å’Œé˜´å½±åŸç†ã€ï¼ˆ3ï¼‰å¸ƒå±€æ”¾ç½®ã€ï¼ˆ4ï¼‰æ‰©å±•åˆç†æ€§ã€‚åœ¨æ–¹æ³•ä¸Šï¼Œæˆ‘ä»¬åˆ©ç”¨é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰å’ŒåŠ¨æ€æ¯”ä¾‹ç²¾åº¦å¥–åŠ±ä¸‹çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆDPA-GRPOï¼‰ï¼Œæé«˜MLLMsçš„å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„é—­æºMLLMsï¼Œå¦‚GPT-4oå’ŒQwen-VL-Maxï¼Œåœ¨å›¾åƒç¾å­¦æ¨ç†æ–¹é¢çš„è¡¨ç°ä¹Ÿå¦‚åŒéšæœºçŒœæµ‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé€šè¿‡åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨è¾ƒå°çš„æ¨¡å‹åœ¨å¾—åˆ†ä¸Šè¶…è¶Šå¤§è§„æ¨¡æ¨¡å‹å’Œé¢†å…ˆçš„é—­æºæ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„åŒ»å­¦å›¾åƒç­›é€‰å°è¯•èƒ½ä¸ºæœªæ¥çš„å›¾åƒç¾å­¦æ¨ç†æä¾›å¸¸è§„é…ç½®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23265v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„æ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å’Œç»Ÿä¸€MLLMsçš„å‘å±•æ¨åŠ¨ä¸‹ã€‚ç„¶è€Œï¼Œå…³äºå›¾åƒç­›é€‰çš„ç ”ç©¶è¾ƒå°‘ï¼Œä¸”MLLMsçš„æ€§èƒ½å¹¶ä¸ä»¤äººæ»¡æ„ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å®Œæ•´çš„æ•°æ®å’Œæ–¹æ³•è§£å†³æ–¹æ¡ˆã€‚åœ¨æ•°æ®æ–¹é¢ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªåŒ…å«1500å¤šä¸ªæ ·æœ¬çš„ç»¼åˆåŒ»å­¦å›¾åƒç­›é€‰æ•°æ®é›†ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…æ‹¬åŒ»å­¦å›¾åƒã€å››ä¸ªç”Ÿæˆçš„å›¾åƒå’Œå¤šä¸ªé€‰æ‹©é¢˜ã€‚åœ¨æ–¹æ³•è®ºæ–¹é¢ï¼Œæˆ‘ä»¬åˆ©ç”¨é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰å’ŒåŠ¨æ€æ¯”ä¾‹ç²¾åº¦å¥–åŠ±çš„é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆDPA-GRPOï¼‰ï¼Œä»¥å¢å¼ºMLLMsçš„å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨è¾ƒå°çš„æ¨¡å‹å¯ä»¥è¶…è¶Šå¤§è§„æ¨¡æ¨¡å‹å’Œé¢†å…ˆé—­æºæ¨¡å‹çš„å¾—åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å¤šé¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒç”Ÿæˆæ–¹é¢ï¼ŒDMå’Œç»Ÿä¸€MLLMsçš„å‘å±•æé«˜äº†æ€§èƒ½ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸­å›¾åƒç­›é€‰çš„ç ”ç©¶è¾ƒå°‘ï¼ŒMLLMsåœ¨è¯¥é¢†åŸŸçš„æ€§èƒ½æœ‰å¾…æé«˜ã€‚</li>
<li>ç¼ºä¹æ•°æ®å’ŒMLLMsçš„å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›è¾ƒå¼±æ˜¯å½±å“æ€§èƒ½çš„ä¸»è¦åŸå› ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŒ…å«åŒ»å­¦å›¾åƒã€ç”Ÿæˆå›¾åƒå’Œé€‰æ‹©é¢˜çš„ç»¼åˆåŒ»å­¦å›¾åƒç­›é€‰æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰å’ŒDPA-GRPOæ–¹æ³•å¢å¼ºMLLMsçš„å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºå¼ºåŒ–å­¦ä¹ æ–¹æ³•å¯ä»¥æé«˜æ¨¡å‹åœ¨å›¾åƒç¾å­¦æ¨ç†æ–¹é¢çš„æ€§èƒ½ï¼Œè¶…è¶Šå¤§è§„æ¨¡å’Œé—­æºæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d19a4c72639245ea283e992fd20ca2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94f800422ed87ab8d8f6aeab3fe7c84d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b6950cabc4284cd495fc92aaacd877a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Interpreting-Chest-X-rays-Like-a-Radiologist-A-Benchmark-with-Clinical-Reasoning"><a href="#Interpreting-Chest-X-rays-Like-a-Radiologist-A-Benchmark-with-Clinical-Reasoning" class="headerlink" title="Interpreting Chest X-rays Like a Radiologist: A Benchmark with Clinical   Reasoning"></a>Interpreting Chest X-rays Like a Radiologist: A Benchmark with Clinical   Reasoning</h2><p><strong>Authors:Jinquan Guan, Qi Chen, Lizhou Liang, Yuhang Liu, Vu Minh Hieu Phan, Minh-Son To, Jian Chen, Yutong Xie</strong></p>
<p>Artificial intelligence (AI)-based chest X-ray (CXR) interpretation assistants have demonstrated significant progress and are increasingly being applied in clinical settings. However, contemporary medical AI models often adhere to a simplistic input-to-output paradigm, directly processing an image and an instruction to generate a result, where the instructions may be integral to the modelâ€™s architecture. This approach overlooks the modeling of the inherent diagnostic reasoning in chest X-ray interpretation. Such reasoning is typically sequential, where each interpretive stage considers the images, the current task, and the contextual information from previous stages. This oversight leads to several shortcomings, including misalignment with clinical scenarios, contextless reasoning, and untraceable errors. To fill this gap, we construct CXRTrek, a new multi-stage visual question answering (VQA) dataset for CXR interpretation. The dataset is designed to explicitly simulate the diagnostic reasoning process employed by radiologists in real-world clinical settings for the first time. CXRTrek covers 8 sequential diagnostic stages, comprising 428,966 samples and over 11 million question-answer (Q&amp;A) pairs, with an average of 26.29 Q&amp;A pairs per sample. Building on the CXRTrek dataset, we propose a new vision-language large model (VLLM), CXRTrekNet, specifically designed to incorporate the clinical reasoning flow into the VLLM framework. CXRTrekNet effectively models the dependencies between diagnostic stages and captures reasoning patterns within the radiological context. Trained on our dataset, the model consistently outperforms existing medical VLLMs on the CXRTrek benchmarks and demonstrates superior generalization across multiple tasks on five diverse external datasets. The dataset and model can be found in our repository (<a target="_blank" rel="noopener" href="https://github.com/guanjinquan/CXRTrek">https://github.com/guanjinquan/CXRTrek</a>). </p>
<blockquote>
<p>åŸºäºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰è§£è¯»åŠ©æ‰‹å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¹¶è¶Šæ¥è¶Šå¤šåœ°åº”ç”¨äºä¸´åºŠç¯å¢ƒã€‚ç„¶è€Œï¼Œå½“ä»£çš„åŒ»ç–—äººå·¥æ™ºèƒ½æ¨¡å‹é€šå¸¸éµå¾ªä¸€ä¸ªç®€åŒ–çš„è¾“å…¥åˆ°è¾“å‡ºçš„æ¨¡å¼ï¼Œç›´æ¥å¤„ç†å›¾åƒå’ŒæŒ‡ä»¤ä»¥ç”Ÿæˆç»“æœï¼Œå…¶ä¸­æŒ‡ä»¤å¯èƒ½æ˜¯æ¨¡å‹æ¶æ„çš„ç»„æˆéƒ¨åˆ†ã€‚è¿™ç§æ–¹æ³•å¿½ç•¥äº†èƒ¸éƒ¨Xå°„çº¿è§£è¯»ä¸­å†…åœ¨è¯Šæ–­æ¨ç†çš„å»ºæ¨¡ã€‚è¿™ç§æ¨ç†é€šå¸¸æ˜¯åºè´¯çš„ï¼Œæ¯ä¸ªè§£è¯»é˜¶æ®µéƒ½ä¼šè€ƒè™‘å›¾åƒã€å½“å‰ä»»åŠ¡å’Œæ¥è‡ªå…ˆå‰é˜¶æ®µçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ç§ç–å¿½å¯¼è‡´äº†å‡ ä¸ªç¼ºç‚¹ï¼ŒåŒ…æ‹¬ä¸ä¸´åºŠåœºæ™¯çš„ä¸åŒ¹é…ã€ç¼ºä¹ä¸Šä¸‹æ–‡æ¨ç†å’Œä¸å¯è¿½è¸ªçš„é”™è¯¯ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ„å»ºäº†CXRTrekï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤šé˜¶æ®µè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ•°æ®é›†ï¼Œç”¨äºCXRè§£è¯»ã€‚è¯¥æ•°æ®é›†é¦–æ¬¡æ˜ç¡®æ¨¡æ‹Ÿäº†æ”¾å°„ç§‘åŒ»ç”Ÿåœ¨ç°å®ä¸–ç•Œä¸´åºŠç¯å¢ƒä¸­çš„è¯Šæ–­æ¨ç†è¿‡ç¨‹ã€‚CXRTrekæ¶µç›–8ä¸ªè¿ç»­çš„è¯Šæ–­é˜¶æ®µï¼ŒåŒ…å«428966ä¸ªæ ·æœ¬å’Œè¶…è¿‡1100ä¸‡ä¸ªé—®ç­”å¯¹ï¼Œæ¯ä¸ªæ ·æœ¬å¹³å‡æœ‰26.29ä¸ªé—®ç­”å¯¹ã€‚åŸºäºCXRTrekæ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€å¤§å‹æ¨¡å‹ï¼ˆVLLMï¼‰ï¼Œå³CXRTrekNetï¼Œä¸“é—¨è®¾è®¡ä»¥å°†ä¸´åºŠæ¨ç†æµç¨‹çº³å…¥VLLMæ¡†æ¶ã€‚CXRTrekNetæœ‰æ•ˆåœ°å»ºæ¨¡äº†è¯Šæ–­é˜¶æ®µä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå¹¶æ•è·äº†æ”¾å°„å­¦ä¸Šä¸‹æ–‡ä¸­çš„æ¨ç†æ¨¡å¼ã€‚åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨CXRTrekåŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰çš„åŒ»ç–—VLLMï¼Œå¹¶åœ¨äº”ä¸ªä¸åŒçš„å¤–éƒ¨æ•°æ®é›†ä¸Šå®ç°äº†å¤šé¡¹ä»»åŠ¡çš„å“è¶Šæ³›åŒ–èƒ½åŠ›ã€‚æ•°æ®é›†å’Œæ¨¡å‹å¯åœ¨æˆ‘ä»¬çš„å­˜å‚¨åº“ä¸­æ‰¾åˆ°ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/guanjinquan/CXRTrek%EF%BC%89%E3%80%82">https://github.com/guanjinquan/CXRTrekï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23143v1">PDF</a> 10 pages (main text), 18 pages (appendix)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºäººå·¥æ™ºèƒ½çš„èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰è§£è¯»åŠ©æ‰‹åœ¨ä¸´åºŠåº”ç”¨ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå½“ä»£åŒ»ç–—AIæ¨¡å‹é€šå¸¸éµå¾ªç®€å•çš„è¾“å…¥-è¾“å‡ºæ¨¡å¼ï¼Œå¿½ç•¥äº†èƒ¸éƒ¨Xå…‰è§£è¯»ä¸­çš„å†…åœ¨è¯Šæ–­æ¨ç†è¿‡ç¨‹ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ä¸è¶³ï¼Œæˆ‘ä»¬æ„å»ºäº†CXRTrekå¤šé˜¶æ®µè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ•°æ®é›†ï¼Œæ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿåœ¨çœŸå®ä¸´åºŠç¯å¢ƒä¸­çš„è¯Šæ–­æ¨ç†è¿‡ç¨‹ã€‚CXRTrekæ¶µç›–8ä¸ªè¿ç»­è¯Šæ–­é˜¶æ®µï¼ŒåŒ…å«428966ä¸ªæ ·æœ¬å’Œè¶…è¿‡1100ä¸‡é—®é¢˜ç­”æ¡ˆå¯¹ã€‚åŸºäºCXRTrekæ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨æ–°çš„è§†è§‰è¯­è¨€å¤§å‹æ¨¡å‹ï¼ˆVLLMï¼‰â€”â€”CXRTrekNetï¼Œæœ‰æ•ˆèå…¥ä¸´åºŠæ¨ç†æµç¨‹è‡³VLLMæ¡†æ¶ä¸­ã€‚è¯¥æ¨¡å‹åœ¨CXRTrekåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨äº”ä¸ªå¤–éƒ¨æ•°æ®é›†ä¸Šå±•ç°å‡ºå“è¶Šçš„å¤šä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨èƒ¸éƒ¨Xå°„çº¿è§£è¯»ä¸­åº”ç”¨å¹¿æ³›ï¼Œä½†ç°æœ‰æ¨¡å‹å¿½ç•¥è¯Šæ–­æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>CXRTrekæ•°æ®é›†æ¨¡æ‹ŸçœŸå®ä¸´åºŠç¯å¢ƒä¸­æ”¾å°„ç§‘åŒ»ç”Ÿçš„è¯Šæ–­æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>CXRTrekæ•°æ®é›†åŒ…å«8ä¸ªè¿ç»­è¯Šæ–­é˜¶æ®µï¼Œè¦†ç›–å¤§é‡æ ·æœ¬å’Œé—®é¢˜ç­”æ¡ˆå¯¹ã€‚</li>
<li>CXRTrekNetæ˜¯å…¨æ–°çš„è§†è§‰è¯­è¨€å¤§å‹æ¨¡å‹ï¼Œèå…¥ä¸´åºŠæ¨ç†æµç¨‹ã€‚</li>
<li>CXRTrekNetåœ¨CXRTrekåŸºå‡†æµ‹è¯•åŠå¤–éƒ¨æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ¨¡å‹æœ‰åŠ©äºæ”¹è¿›AIåœ¨åŒ»ç–—å½±åƒè§£è¯»ä¸­çš„æ€§èƒ½ï¼Œä½¿å…¶æ›´è´´è¿‘å®é™…ä¸´åºŠéœ€æ±‚ã€‚</li>
<li>æ•°æ®é›†å’Œæ¨¡å‹å·²å…¬å¼€ï¼Œå¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c380330b0de2efe3320086307920616e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90868e943ea46b6502bbd14e3ee78d21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df72ad2f6999f267d8a4d947132da3db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9864a704e37aa5cbfdf9a330e57fac36.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Zero-P-to-3-Zero-Shot-Partial-View-Images-to-3D-Object"><a href="#Zero-P-to-3-Zero-Shot-Partial-View-Images-to-3D-Object" class="headerlink" title="Zero-P-to-3: Zero-Shot Partial-View Images to 3D Object"></a>Zero-P-to-3: Zero-Shot Partial-View Images to 3D Object</h2><p><strong>Authors:Yuxuan Lin, Ruihang Chu, Zhenyu Chen, Xiao Tang, Lei Ke, Haoling Li, Yingji Zhong, Zhihao Li, Shiyong Liu, Xiaofei Wu, Jianzhuang Liu, Yujiu Yang</strong></p>
<p>Generative 3D reconstruction shows strong potential in incomplete observations. While sparse-view and single-image reconstruction are well-researched, partial observation remains underexplored. In this context, dense views are accessible only from a specific angular range, with other perspectives remaining inaccessible. This task presents two main challenges: (i) limited View Range: observations confined to a narrow angular scope prevent effective traditional interpolation techniques that require evenly distributed perspectives. (ii) inconsistent Generation: views created for invisible regions often lack coherence with both visible regions and each other, compromising reconstruction consistency. To address these challenges, we propose \method, a novel training-free approach that integrates the local dense observations and multi-source priors for reconstruction. Our method introduces a fusion-based strategy to effectively align these priors in DDIM sampling, thereby generating multi-view consistent images to supervise invisible views. We further design an iterative refinement strategy, which uses the geometric structures of the object to enhance reconstruction quality. Extensive experiments on multiple datasets show the superiority of our method over SOTAs, especially in invisible regions. </p>
<blockquote>
<p>ç”Ÿæˆå¼ä¸‰ç»´é‡å»ºåœ¨ä¸å®Œå…¨è§‚å¯Ÿä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚è™½ç„¶ç¨€ç–è§†è§’å’Œå•å›¾åƒé‡å»ºå·²ç»å¾—åˆ°äº†å……åˆ†çš„ç ”ç©¶ï¼Œä½†éƒ¨åˆ†è§‚å¯Ÿä»ç„¶è¢«æ¢ç´¢ä¸è¶³ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåªæœ‰ä»ç‰¹å®šçš„è§’åº¦èŒƒå›´æ‰èƒ½è·å–å¯†é›†è§†å›¾ï¼Œè€Œå…¶ä»–è§’åº¦åˆ™æ— æ³•è·å–ã€‚æ­¤ä»»åŠ¡é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š(i)æœ‰é™çš„è§†è§’èŒƒå›´ï¼šè§‚å¯Ÿä»…é™äºç‹­çª„çš„è§’åº¦èŒƒå›´ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆåˆ©ç”¨çš„ä¼ ç»Ÿæ’å€¼æŠ€æœ¯éœ€è¦å‡åŒ€åˆ†å¸ƒçš„è§’åº¦ã€‚(ii)ä¸ä¸€è‡´çš„ç”Ÿæˆï¼šä¸ºä¸å¯è§åŒºåŸŸåˆ›å»ºçš„è§†å›¾é€šå¸¸ä¸å¯è§åŒºåŸŸå’Œå…¶ä»–è§†å›¾ç¼ºä¹è¿è´¯æ€§ï¼Œä»è€ŒæŸå®³é‡å»ºçš„ä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†\methodï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒæ–¹æ³•ï¼Œå®ƒæ•´åˆäº†å±€éƒ¨å¯†é›†è§‚å¯Ÿå’Œå¤šç§æºå…ˆéªŒè¿›è¡Œé‡å»ºã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§åŸºäºèåˆçš„ç­–ç•¥ï¼Œæœ‰æ•ˆåœ°å¯¹é½è¿™äº›å…ˆéªŒåœ¨DDIMé‡‡æ ·ä¸­ï¼Œä»è€Œç”Ÿæˆå¤šè§†è§’ä¸€è‡´çš„å›¾åƒæ¥ç›‘ç£ä¸å¯è§è§†å›¾ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§è¿­ä»£ç»†åŒ–ç­–ç•¥ï¼Œåˆ©ç”¨å¯¹è±¡çš„å‡ ä½•ç»“æ„æ¥æé«˜é‡å»ºè´¨é‡ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸å¯è§åŒºåŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23054v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    ç”Ÿæˆå¼ä¸‰ç»´é‡å»ºåœ¨ä¸å…¨è§‚æµ‹ä¸‹å±•ç°å·¨å¤§æ½œåŠ›ã€‚å°½ç®¡ç¨€ç–è§†è§’å’Œå•å›¾åƒé‡å»ºå·²å¾—åˆ°æ·±å…¥ç ”ç©¶ï¼Œä½†éƒ¨åˆ†è§‚æµ‹ä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚å¯†é›†è§†è§’ä»…ä»ç‰¹å®šè§’åº¦èŒƒå›´å¯è·å–ï¼Œå…¶ä»–è§’åº¦åˆ™æ— æ³•è·å–ã€‚æ­¤ä»»åŠ¡é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯è§†è§’èŒƒå›´æœ‰é™ï¼Œè§‚æµ‹ä»…é™äºç‹­çª„è§’åº¦ï¼Œæ— æ³•æœ‰æ•ˆåº”ç”¨éœ€å‡åŒ€åˆ†å¸ƒè§†è§’çš„ä¼ ç»Ÿæ’å€¼æŠ€æœ¯ï¼›äºŒæ˜¯ä¸ä¸€è‡´çš„ç”Ÿæˆï¼Œä¸ºä¸å¯è§åŒºåŸŸåˆ›å»ºçš„è§†è§’å¾€å¾€ä¸å¯è§åŒºåŸŸåŠå…¶ä»–è§†è§’ç¼ºä¹è¿è´¯æ€§ï¼Œå½±å“é‡å»ºçš„ä¸€è‡´æ€§ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°å‹æ— è®­ç»ƒæ–¹æ³•ï¼Œé›†æˆå±€éƒ¨å¯†é›†è§‚æµ‹å’Œå¤šæºå…ˆéªŒä¿¡æ¯è¿›è¡Œé‡å»ºã€‚\methodé€šè¿‡èåˆç­–ç•¥æœ‰æ•ˆå¯¹é½è¿™äº›å…ˆéªŒä¿¡æ¯ï¼Œåœ¨DDIMé‡‡æ ·ä¸­ç”Ÿæˆå¤šè§†è§’ä¸€è‡´å›¾åƒä»¥ç›‘ç£ä¸å¯è§è§†è§’ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§è¿­ä»£ä¼˜åŒ–ç­–ç•¥ï¼Œåˆ©ç”¨ç‰©ä½“å‡ ä½•ç»“æ„æé«˜é‡å»ºè´¨é‡ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå°¤å…¶åœ¨ä¸å¯è§åŒºåŸŸã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ç”Ÿæˆå¼ä¸‰ç»´é‡å»ºåœ¨å¤„ç†ä¸å®Œå…¨è§‚æµ‹æ•°æ®æ—¶è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬è§†è§’èŒƒå›´æœ‰é™å’Œç”Ÿæˆçš„å›¾åƒè¿è´¯æ€§ä¸è¶³ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹æ— è®­ç»ƒæ–¹æ³•ï¼Œé›†æˆå±€éƒ¨å¯†é›†è§‚æµ‹å’Œå¤šæºå…ˆéªŒä¿¡æ¯ï¼Œä»¥æ”¹å–„é‡å»ºè´¨é‡ã€‚</li>
<li>é€šè¿‡èåˆç­–ç•¥æœ‰æ•ˆå¯¹é½å…ˆéªŒä¿¡æ¯ï¼Œç”Ÿæˆå¤šè§†è§’ä¸€è‡´å›¾åƒã€‚</li>
<li>è®¾è®¡è¿­ä»£ä¼˜åŒ–ç­–ç•¥ï¼Œåˆ©ç”¨ç‰©ä½“å‡ ä½•ç»“æ„æé«˜é‡å»ºè´¨é‡ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¸å¯è§åŒºåŸŸçš„é‡å»ºæ•ˆæœå°¤ä½³ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºæœªæ¥å¤„ç†æ›´ä¸ºå¤æ‚çš„ä¸‰ç»´é‡å»ºä»»åŠ¡æä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b87168fa1468bbfa8d1590bf306f2a50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-26b28b2604b61543901053082c869f64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a3ec1022e7d9d07df5232d453e73a82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b65a1cce9b8748cd0de8190729f23d9b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Deep-Modeling-and-Optimization-of-Medical-Image-Classification"><a href="#Deep-Modeling-and-Optimization-of-Medical-Image-Classification" class="headerlink" title="Deep Modeling and Optimization of Medical Image Classification"></a>Deep Modeling and Optimization of Medical Image Classification</h2><p><strong>Authors:Yihang Wu, Muhammad Owais, Reem Kateb, Ahmad Chaddad</strong></p>
<p>Deep models, such as convolutional neural networks (CNNs) and vision transformer (ViT), demonstrate remarkable performance in image classification. However, those deep models require large data to fine-tune, which is impractical in the medical domain due to the data privacy issue. Furthermore, despite the feasible performance of contrastive language image pre-training (CLIP) in the natural domain, the potential of CLIP has not been fully investigated in the medical field. To face these challenges, we considered three scenarios: 1) we introduce a novel CLIP variant using four CNNs and eight ViTs as image encoders for the classification of brain cancer and skin cancer, 2) we combine 12 deep models with two federated learning techniques to protect data privacy, and 3) we involve traditional machine learning (ML) methods to improve the generalization ability of those deep models in unseen domain data. The experimental results indicate that maxvit shows the highest averaged (AVG) test metrics (AVG &#x3D; 87.03%) in HAM10000 dataset with multimodal learning, while convnext_l demonstrates remarkable test with an F1-score of 83.98% compared to swin_b with 81.33% in FL model. Furthermore, the use of support vector machine (SVM) can improve the overall test metrics with AVG of $\sim 2%$ for swin transformer series in ISIC2018. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/AIPMLab/SkinCancerSimulation">https://github.com/AIPMLab/SkinCancerSimulation</a>. </p>
<blockquote>
<p>æ·±åº¦æ¨¡å‹ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼Œåœ¨å›¾åƒåˆ†ç±»æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ·±åº¦æ¨¡å‹éœ€è¦å¤§é‡æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œè¿™åœ¨åŒ»å­¦é¢†åŸŸç”±äºæ•°æ®éšç§é—®é¢˜è€Œä¸åˆ‡å®é™…ã€‚æ­¤å¤–ï¼Œå°½ç®¡å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨è‡ªç„¶é¢†åŸŸè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä½†å…¶åœ¨åŒ»å­¦é¢†åŸŸçš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä»¥ä¸‹ä¸‰ç§æƒ…æ™¯ï¼š1ï¼‰æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„CLIPå˜ä½“ï¼Œä½¿ç”¨å››ç§CNNå’Œå…«ç§ViTä½œä¸ºå›¾åƒç¼–ç å™¨ï¼Œç”¨äºè„‘ç™Œå’Œçš®è‚¤ç™Œçš„åˆ†ç±»ï¼›2ï¼‰æˆ‘ä»¬å°†12ä¸ªæ·±åº¦æ¨¡å‹ä¸ä¸¤ç§è”é‚¦å­¦ä¹ æŠ€æœ¯ç›¸ç»“åˆï¼Œä»¥ä¿æŠ¤æ•°æ®éšç§ï¼›3ï¼‰æˆ‘ä»¬å¼•å…¥ä¼ ç»Ÿæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ–¹æ³•æ¥æé«˜è¿™äº›æ·±åº¦æ¨¡å‹åœ¨æœªè§é¢†åŸŸæ•°æ®ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å…·æœ‰å¤šæ¨¡å¼å­¦ä¹ çš„HAM10000æ•°æ®é›†ä¸­ï¼ŒMaxVitçš„å¹³å‡æµ‹è¯•æŒ‡æ ‡ï¼ˆAVGï¼‰æœ€é«˜ï¼ˆAVG&#x3D;87.03%ï¼‰ï¼Œè€ŒConvNext_låœ¨è”é‚¦å­¦ä¹ æ¨¡å‹ä¸­ç›¸è¾ƒäºSwin_bå–å¾—äº†ä»¤äººç©ç›®çš„æˆç»©ï¼Œå…¶F1åˆ†æ•°ä¸º83.98%ã€‚æ­¤å¤–ï¼Œæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰çš„ä½¿ç”¨å¯ä»¥æ”¹è¿›ISIC2018ä¸­Swin Transformerç³»åˆ—çš„æ€»ä½“æµ‹è¯•æŒ‡æ ‡ï¼Œå¹³å‡æé«˜äº†çº¦2%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AIPMLab/SkinCancerSimulation%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/AIPMLab/SkinCancerSimulationä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23040v1">PDF</a> Accepted in ISBI2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»é¢†åŸŸï¼Œæ·±åº¦æ¨¡å‹å¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„åº”ç”¨ä¸æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹æ•°æ®éšç§å’Œæœªè§åŸŸæ•°æ®çš„é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä½¿ç”¨æ–°å‹CLIPå˜ä½“ã€ç»“åˆå¤šç§æ·±åº¦æ¨¡å‹ä¸è”é‚¦å­¦ä¹ æŠ€æœ¯ã€ä»¥åŠå¼•å…¥ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•æ¥æ”¹å–„æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æŸäº›åœºæ™¯ä¸‹ï¼Œæ–°å‹æ¨¡å‹è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦æ¨¡å‹å¦‚CNNå’ŒViTåœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­å…·æœ‰å“è¶Šæ€§èƒ½ï¼Œä½†éœ€è¦å¤§é‡æ•°æ®è¿›è¡Œå¾®è°ƒã€‚</li>
<li>åŒ»å­¦é¢†åŸŸçš„æ•°æ®éšç§é—®é¢˜æ˜¯å®ç°æ·±åº¦æ¨¡å‹å¾®è°ƒçš„ä¸åˆ‡å®é™…ä¹‹å¤„ã€‚</li>
<li>CLIPåœ¨è‡ªç„¶é¢†åŸŸè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨æ½œåŠ›å°šæœªå……åˆ†ç ”ç©¶ã€‚</li>
<li>æ–°å‹CLIPå˜ä½“ä½¿ç”¨CNNå’ŒViTä½œä¸ºå›¾åƒç¼–ç å™¨ï¼Œç”¨äºè„‘ç™Œå’Œçš®è‚¤ç™Œåˆ†ç±»ã€‚</li>
<li>ç»“åˆå¤šç§æ·±åº¦æ¨¡å‹ä¸è”é‚¦å­¦ä¹ æŠ€æœ¯ä»¥ä¿æŠ¤æ•°æ®éšç§ã€‚</li>
<li>å¼•å…¥ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œæé«˜æ¨¡å‹åœ¨æœªè§åŸŸæ•°æ®ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æŸäº›æ¨¡å‹å’Œæ•°æ®é›†ä¸‹ï¼Œæ–°å‹æ–¹æ³•è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚maxvitåœ¨HAM10000æ•°æ®é›†ä¸Šçš„å¹³å‡æµ‹è¯•æŒ‡æ ‡è¾¾åˆ°87.03%ï¼Œconvnext_låœ¨FLæ¨¡å‹ä¸­çš„F1åˆ†æ•°è¾¾åˆ°83.98%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ee3beb683cab7a16697744cf819036c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cff92564b66b185f00c2c7030292faf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3873872521239955ae7f8ba54bd03d41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d0f96b686882f8fe6fc199583be283c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="cadrille-Multi-modal-CAD-Reconstruction-with-Online-Reinforcement-Learning"><a href="#cadrille-Multi-modal-CAD-Reconstruction-with-Online-Reinforcement-Learning" class="headerlink" title="cadrille: Multi-modal CAD Reconstruction with Online Reinforcement   Learning"></a>cadrille: Multi-modal CAD Reconstruction with Online Reinforcement   Learning</h2><p><strong>Authors:Maksim Kolodiazhnyi, Denis Tarasov, Dmitrii Zhemchuzhnikov, Alexander Nikulin, Ilya Zisman, Anna Vorontsova, Anton Konushin, Vladislav Kurenkov, Danila Rukhovich</strong></p>
<p>Computer-Aided Design (CAD) plays a central role in engineering and manufacturing, making it possible to create precise and editable 3D models. Using a variety of sensor or user-provided data as inputs for CAD reconstruction can democratize access to design applications. However, existing methods typically focus on a single input modality, such as point clouds, images, or text, which limits their generalizability and robustness. Leveraging recent advances in vision-language models (VLM), we propose a multi-modal CAD reconstruction model that simultaneously processes all three input modalities. Inspired by large language model (LLM) training paradigms, we adopt a two-stage pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated data, followed by reinforcement learning (RL) fine-tuning using online feedback, obtained programatically. Furthermore, we are the first to explore RL fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such as Group Relative Preference Optimization (GRPO) outperform offline alternatives. In the DeepCAD benchmark, our SFT model outperforms existing single-modal approaches in all three input modalities simultaneously. More importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three challenging datasets, including a real-world one. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å·¥ç¨‹å’Œåˆ¶é€ ä¸šä¸­æ‰®æ¼”ç€æ ¸å¿ƒè§’è‰²ï¼Œèƒ½å¤Ÿåˆ›å»ºç²¾ç¡®ä¸”å¯ç¼–è¾‘çš„3Dæ¨¡å‹ã€‚ä½¿ç”¨å„ç§ä¼ æ„Ÿå™¨æˆ–ç”¨æˆ·æä¾›çš„æ•°æ®ä½œä¸ºCADé‡å»ºçš„è¾“å…¥ï¼Œå¯ä»¥ä½¿è®¾è®¡åº”ç”¨ç¨‹åºçš„è®¿é—®æ›´åŠ æ°‘ä¸»åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¸“æ³¨äºå•ä¸€çš„è¾“å…¥æ¨¡å¼ï¼Œå¦‚ç‚¹äº‘ã€å›¾åƒæˆ–æ–‡æœ¬ï¼Œè¿™é™åˆ¶äº†å…¶é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§å¤šæ¨¡æ€CADé‡å»ºæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥åŒæ—¶å¤„ç†ä¸‰ç§è¾“å…¥æ¨¡å¼ã€‚å—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒèŒƒå¼çš„å¯å‘ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆåœ¨å¤§é‡ç¨‹åºç”Ÿæˆçš„æ•°æ®ä¸Šè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œç„¶åé‡‡ç”¨åœ¨çº¿åé¦ˆè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒï¼Œè¿™äº›åé¦ˆæ˜¯ç¨‹åºè·å–çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªæ¢ç´¢CADä»»åŠ¡çš„LLMçš„RLå¾®è°ƒï¼Œè¯æ˜åœ¨çº¿RLç®—æ³•ï¼ˆå¦‚é›†å›¢ç›¸å¯¹åå¥½ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼‰èƒœè¿‡ç¦»çº¿æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨DeepCADåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„SFTæ¨¡å‹åœ¨ä¸‰ç§è¾“å…¥æ¨¡å¼ä¸ŠåŒæ—¶è¶…è¶Šäº†ç°æœ‰çš„å•æ¨¡æ€æ–¹æ³•ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œç»è¿‡RLå¾®è°ƒåï¼Œcadrilleåœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šåˆ›ä¸‹äº†æœ€æ–°ä¸–ç•Œçºªå½•ï¼ŒåŒ…æ‹¬ä¸€ä¸ªçœŸå®ä¸–ç•Œçš„æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22914v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å·¥ç¨‹å’Œåˆ¶é€ ä¸šä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œç ”ç©¶æå‡ºä¸€ç§å¤šæ¨¡æ€CADé‡å»ºæ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†ç‚¹äº‘ã€å›¾åƒå’Œæ–‡æœ¬ä¸‰ç§è¾“å…¥æ¨¡æ€ã€‚é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¸¤é˜¶æ®µç®¡é“ï¼Œé€šè¿‡ç¨‹åºç”Ÿæˆå¤§æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨é›†å›¢ç›¸å¯¹åå¥½ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­‰åœ¨çº¿RLç®—æ³•è¿›è¡Œå¾®è°ƒã€‚è¯¥æ¨¡å‹åœ¨DeepCADåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å·¥ç¨‹å’Œåˆ¶é€ ä¸šä¸­æ‰®æ¼”æ ¸å¿ƒè§’è‰²ï¼Œèƒ½å¤Ÿåˆ›å»ºç²¾ç¡®ä¸”å¯ç¼–è¾‘çš„3Dæ¨¡å‹ã€‚</li>
<li>ç°æœ‰CADé‡å»ºæ–¹æ³•ä¸»è¦å…³æ³¨å•ä¸€è¾“å…¥æ¨¡æ€ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€CADé‡å»ºæ¨¡å‹ï¼Œèƒ½åŒæ—¶å¤„ç†ç‚¹äº‘ã€å›¾åƒå’Œæ–‡æœ¬ä¸‰ç§è¾“å…¥æ¨¡æ€ã€‚</li>
<li>é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼Œå…¶ä¸­SFTåœ¨å¤§å‹ç¨‹åºç”Ÿæˆæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œéšåé€šè¿‡åœ¨çº¿åé¦ˆè¿›è¡ŒRLå¾®è°ƒã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡æ¢ç´¢äº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹CADä»»åŠ¡çš„LLMè¿›è¡Œå¾®è°ƒã€‚</li>
<li>åœ¨çº¿RLç®—æ³•å¦‚é›†å›¢ç›¸å¯¹åå¥½ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨CADä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22914">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6ed245f01a08dbaf350fa40c20d291d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2c948de385ec04a2e23b34153e9e315.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-547dc86ba4c2c54237a277985093fcc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26eaafa4b55ab3e8c2172b7fc164265f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Anomalies-by-Synthesis-Anomaly-Detection-using-Generative-Diffusion-Models-for-Off-Road-Navigation"><a href="#Anomalies-by-Synthesis-Anomaly-Detection-using-Generative-Diffusion-Models-for-Off-Road-Navigation" class="headerlink" title="Anomalies by Synthesis: Anomaly Detection using Generative Diffusion   Models for Off-Road Navigation"></a>Anomalies by Synthesis: Anomaly Detection using Generative Diffusion   Models for Off-Road Navigation</h2><p><strong>Authors:Siddharth Ancha, Sunshine Jiang, Travis Manderson, Laura Brandt, Yilun Du, Philip R. Osteen, Nicholas Roy</strong></p>
<p>In order to navigate safely and reliably in off-road and unstructured environments, robots must detect anomalies that are out-of-distribution (OOD) with respect to the training data. We present an analysis-by-synthesis approach for pixel-wise anomaly detection without making any assumptions about the nature of OOD data. Given an input image, we use a generative diffusion model to synthesize an edited image that removes anomalies while keeping the remaining image unchanged. Then, we formulate anomaly detection as analyzing which image segments were modified by the diffusion model. We propose a novel inference approach for guided diffusion by analyzing the ideal guidance gradient and deriving a principled approximation that bootstraps the diffusion model to predict guidance gradients. Our editing technique is purely test-time that can be integrated into existing workflows without the need for retraining or fine-tuning. Finally, we use a combination of vision-language foundation models to compare pixels in a learned feature space and detect semantically meaningful edits, enabling accurate anomaly detection for off-road navigation. Project website: <a target="_blank" rel="noopener" href="https://siddancha.github.io/anomalies-by-diffusion-synthesis/">https://siddancha.github.io/anomalies-by-diffusion-synthesis/</a> </p>
<blockquote>
<p>ä¸ºäº†åœ¨è¶Šé‡å’Œéç»“æ„åŒ–ç¯å¢ƒä¸­å®‰å…¨ã€å¯é åœ°å¯¼èˆªï¼Œæœºå™¨äººå¿…é¡»æ£€æµ‹ä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒå¤–çš„å¼‚å¸¸å€¼ï¼ˆOODï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåˆæˆçš„åƒç´ çº§å¼‚å¸¸æ£€æµ‹åˆ†ææ–¹æ³•ï¼Œæ— éœ€å¯¹OODæ•°æ®çš„æ€§è´¨åšå‡ºä»»ä½•å‡è®¾ã€‚ç»™å®šè¾“å…¥å›¾åƒï¼Œæˆ‘ä»¬ä½¿ç”¨ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹åˆæˆç¼–è¾‘åçš„å›¾åƒï¼Œè¯¥å›¾åƒç§»é™¤äº†å¼‚å¸¸å€¼ï¼ŒåŒæ—¶ä¿æŒå…¶ä½™éƒ¨åˆ†ä¸å˜ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å¼‚å¸¸æ£€æµ‹è¡¨è¿°ä¸ºåˆ†æå“ªäº›å›¾åƒæ®µè¢«æ‰©æ•£æ¨¡å‹ä¿®æ”¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼•å¯¼æ‰©æ•£æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡åˆ†æç†æƒ³å¼•å¯¼æ¢¯åº¦å¹¶æ¨å¯¼å‡ºä¸€ç§åŸåˆ™è¿‘ä¼¼å€¼ï¼Œä»è€Œæ¨åŠ¨æ‰©æ•£æ¨¡å‹é¢„æµ‹å¼•å¯¼æ¢¯åº¦ã€‚æˆ‘ä»¬çš„ç¼–è¾‘æŠ€æœ¯æ˜¯çº¯æµ‹è¯•æ—¶é—´çš„ï¼Œå¯ä»¥é›†æˆåˆ°ç°æœ‰å·¥ä½œæµç¨‹ä¸­ï¼Œæ— éœ€è¿›è¡Œå†è®­ç»ƒæˆ–å¾®è°ƒã€‚æœ€åï¼Œæˆ‘ä»¬ç»“åˆè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œåœ¨å­¦ä¹ çš„ç‰¹å¾ç©ºé—´ä¸­æ¯”è¾ƒåƒç´ å¹¶æ£€æµ‹è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„ç¼–è¾‘ï¼Œä¸ºå®ç°è¶Šé‡å¯¼èˆªä¸­çš„ç²¾ç¡®å¼‚å¸¸æ£€æµ‹æä¾›äº†å¯èƒ½ã€‚é¡¹ç›®ç½‘ç«™åœ°å€ä¸ºï¼š[<a target="_blank" rel="noopener" href="https://siddancha.github.io/anomalies-by-diffusion-synthesis/]">https://siddancha.github.io/anomalies-by-diffusion-synthesis/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22805v1">PDF</a> Presented at ICRA 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹çš„åˆ†æåˆæˆæ–¹æ³•ï¼Œå®ç°æ— éœ€äº†è§£å¼‚å¸¸æ•°æ®æ€§è´¨çš„åƒç´ çº§å¼‚å¸¸æ£€æµ‹ã€‚é€šè¿‡åˆæˆç¼–è¾‘å›¾åƒï¼Œç§»é™¤å¼‚å¸¸åŒæ—¶ä¿ç•™åŸå›¾åƒï¼Œå°†å¼‚å¸¸æ£€æµ‹è½¬åŒ–ä¸ºåˆ†æå“ªäº›å›¾åƒåŒºåŸŸè¢«æ‰©æ•£æ¨¡å‹ä¿®æ”¹ã€‚æå‡ºæ–°çš„å¼•å¯¼æ‰©æ•£æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡ç†æƒ³å¼•å¯¼æ¢¯åº¦åˆ†æå’Œæ¨å¯¼åŸåˆ™è¿‘ä¼¼å€¼ï¼Œä½¿æ‰©æ•£æ¨¡å‹é¢„æµ‹å¼•å¯¼æ¢¯åº¦ã€‚ç¼–è¾‘æŠ€æœ¯åœ¨æµ‹è¯•é˜¶æ®µçº¯ç²¹ï¼Œå¯æ— ç¼é›†æˆç°æœ‰å·¥ä½œæµç¨‹ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒã€‚ç»“åˆè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œåœ¨å­¦ä¹ çš„ç‰¹å¾ç©ºé—´ä¸­æ¯”è¾ƒåƒç´ ï¼Œæ£€æµ‹è¯­ä¹‰ç¼–è¾‘ï¼Œä¸ºè¶Šé‡å¯¼èˆªæä¾›å‡†ç¡®çš„å¼‚å¸¸æ£€æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººéœ€è¦åœ¨éé“è·¯å’Œéç»“æ„åŒ–ç¯å¢ƒä¸­å®‰å…¨å¯é åœ°å¯¼èˆªï¼Œå¿…é¡»æ£€æµ‹ä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒå¤–çš„å¼‚å¸¸ã€‚</li>
<li>æå‡ºä¸€ç§åˆ†æåˆæˆæ–¹æ³•ï¼Œç”¨äºåƒç´ çº§çš„å¼‚å¸¸æ£€æµ‹ï¼Œæ— éœ€å‡è®¾å¼‚å¸¸æ•°æ®çš„æ€§è´¨ã€‚</li>
<li>åˆ©ç”¨ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹åˆæˆç¼–è¾‘å›¾åƒï¼Œç§»é™¤å¼‚å¸¸åŒæ—¶ä¿ç•™åŸå›¾åƒã€‚</li>
<li>å°†å¼‚å¸¸æ£€æµ‹è½¬åŒ–ä¸ºåˆ†æå›¾åƒä¸­å“ªäº›åŒºåŸŸè¢«æ‰©æ•£æ¨¡å‹ä¿®æ”¹ã€‚</li>
<li>æå‡ºæ–°çš„å¼•å¯¼æ‰©æ•£æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡ç†æƒ³å¼•å¯¼æ¢¯åº¦åˆ†æå’ŒåŸåˆ™è¿‘ä¼¼å€¼æ¥é¢„æµ‹æŒ‡å¯¼æ¢¯åº¦ã€‚</li>
<li>ç¼–è¾‘æŠ€æœ¯åœ¨æµ‹è¯•é˜¶æ®µçº¯ç²¹ï¼Œå¯è½»æ¾é›†æˆç°æœ‰å·¥ä½œæµç¨‹ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒã€‚</li>
<li>ç»“åˆè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹æ£€æµ‹è¯­ä¹‰ç¼–è¾‘ï¼Œä¸ºè¶Šé‡å¯¼èˆªæä¾›å‡†ç¡®çš„å¼‚å¸¸æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22805">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3815d050a0173b2b3bdcbce9ef28e686.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-699e8fc0f45ab257492e0cbd73e85273.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bcdd80d0ae9ff3a3f4836140d1e3cd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54bd9850d28d5bbeba799a8ecc842b93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ed78b3a7edffde8b23ce5681df1392f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MIAS-SAM-Medical-Image-Anomaly-Segmentation-without-thresholding"><a href="#MIAS-SAM-Medical-Image-Anomaly-Segmentation-without-thresholding" class="headerlink" title="MIAS-SAM: Medical Image Anomaly Segmentation without thresholding"></a>MIAS-SAM: Medical Image Anomaly Segmentation without thresholding</h2><p><strong>Authors:Marco Colussi, Dragan Ahmetovic, Sergio Mascetti</strong></p>
<p>This paper presents MIAS-SAM, a novel approach for the segmentation of anomalous regions in medical images. MIAS-SAM uses a patch-based memory bank to store relevant image features, which are extracted from normal data using the SAM encoder. At inference time, the embedding patches extracted from the SAM encoder are compared with those in the memory bank to obtain the anomaly map. Finally, MIAS-SAM computes the center of gravity of the anomaly map to prompt the SAM decoder, obtaining an accurate segmentation from the previously extracted features. Differently from prior works, MIAS-SAM does not require to define a threshold value to obtain the segmentation from the anomaly map. Experimental results conducted on three publicly available datasets, each with a different imaging modality (Brain MRI, Liver CT, and Retina OCT) show accurate anomaly segmentation capabilities measured using DICE score. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/warpcut/MIAS-SAM">https://github.com/warpcut/MIAS-SAM</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒå¼‚å¸¸åŒºåŸŸåˆ†å‰²çš„æ–°æ–¹æ³•MIAS-SAMã€‚MIAS-SAMä½¿ç”¨åŸºäºè¡¥ä¸çš„å†…å­˜åº“æ¥å­˜å‚¨ä»æ­£å¸¸æ•°æ®ä½¿ç”¨SAMç¼–ç å™¨æå–çš„ç›¸å…³å›¾åƒç‰¹å¾ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œå°†SAMç¼–ç å™¨æå–çš„åµŒå…¥è¡¥ä¸ä¸å†…å­˜åº“ä¸­çš„è¡¥ä¸è¿›è¡Œæ¯”è¾ƒï¼Œä»¥è·å¾—å¼‚å¸¸æ˜ å°„ã€‚æœ€åï¼ŒMIAS-SAMè®¡ç®—å¼‚å¸¸æ˜ å°„çš„é‡å¿ƒä»¥æç¤ºSAMè§£ç å™¨ï¼Œå¹¶ä»å…ˆå‰æå–çš„ç‰¹å¾ä¸­è·å¾—å‡†ç¡®çš„åˆ†å‰²ã€‚ä¸ä¹‹å‰çš„å·¥ä½œä¸åŒï¼ŒMIAS-SAMä¸éœ€è¦å®šä¹‰é˜ˆå€¼å³å¯ä»å¼‚å¸¸æ˜ å°„ä¸­è·å¾—åˆ†å‰²ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒï¼Œæ¯ä¸ªæ•°æ®é›†éƒ½æœ‰ä¸åŒçš„æˆåƒæ¨¡å¼ï¼ˆå¤§è„‘MRIã€è‚è„CTå’Œè§†ç½‘è†œOCTï¼‰ï¼Œä½¿ç”¨DICEå¾—åˆ†è¡¡é‡ï¼Œæ˜¾ç¤ºå‡ºå‡†ç¡®çš„å¼‚å¸¸åˆ†å‰²èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/warpcut/MIAS-SAM">https://github.com/warpcut/MIAS-SAM</a>å¤„è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22762v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒå¼‚å¸¸åŒºåŸŸåˆ†å‰²æ–°æ–¹æ³•MIAS-SAMä»‹ç»ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åŸºäºè¡¥ä¸çš„å†…å­˜åº“å­˜å‚¨ä»æ­£å¸¸æ•°æ®ä¸­æå–çš„ç‰¹å¾ï¼Œé€šè¿‡SAMç¼–ç å™¨ç”ŸæˆåµŒå…¥è¡¥ä¸å¹¶ä¸å†…å­˜åº“ä¸­çš„è¡¥ä¸è¿›è¡Œæ¯”è¾ƒï¼Œç”Ÿæˆå¼‚å¸¸åœ°å›¾ã€‚è®¡ç®—å¼‚å¸¸åœ°å›¾çš„é‡å¿ƒï¼Œå¼•å¯¼SAMè§£ç å™¨è·å¾—å‡†ç¡®åˆ†å‰²ã€‚æ— éœ€è®¾å®šé˜ˆå€¼ï¼Œå¯åœ¨å¤šç§åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šå®ç°å‡†ç¡®åˆ†å‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MIAS-SAMæ˜¯ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒå¼‚å¸¸åŒºåŸŸåˆ†å‰²çš„æ–°æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•åˆ©ç”¨åŸºäºè¡¥ä¸çš„å†…å­˜åº“å­˜å‚¨æ­£å¸¸å›¾åƒçš„ç‰¹å¾ã€‚</li>
<li>é€šè¿‡SAMç¼–ç å™¨ç”ŸæˆåµŒå…¥è¡¥ä¸ï¼Œå¹¶ä¸å†…å­˜åº“ä¸­çš„è¡¥ä¸è¿›è¡Œæ¯”è¾ƒï¼Œç”Ÿæˆå¼‚å¸¸åœ°å›¾ã€‚</li>
<li>è®¡ç®—å¼‚å¸¸åœ°å›¾çš„é‡å¿ƒï¼Œä»¥å¼•å¯¼SAMè§£ç å™¨è¿›è¡Œå‡†ç¡®åˆ†å‰²ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€è®¾å®šé˜ˆå€¼æ¥è·å–åˆ†å‰²ç»“æœã€‚</li>
<li>åœ¨ä¸‰ç§ä¸åŒæˆåƒæ¨¡æ€çš„å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼ŒåŒ…æ‹¬Brain MRIã€Liver CTå’ŒRetina OCTã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22762">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81b75a04b63b25de15e7d668102b965f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cb757a9ec509c5431d1a2f0e1db24ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b260ed64bb281b29a6653e0db69d79bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa9379edb6fb1afdd4be2063ab72b9c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cafafeb777aa2ff1cca2877ca15c4c95.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="PolyPose-Localizing-Deformable-Anatomy-in-3D-from-Sparse-2D-X-ray-Images-using-Polyrigid-Transforms"><a href="#PolyPose-Localizing-Deformable-Anatomy-in-3D-from-Sparse-2D-X-ray-Images-using-Polyrigid-Transforms" class="headerlink" title="PolyPose: Localizing Deformable Anatomy in 3D from Sparse 2D X-ray   Images using Polyrigid Transforms"></a>PolyPose: Localizing Deformable Anatomy in 3D from Sparse 2D X-ray   Images using Polyrigid Transforms</h2><p><strong>Authors:Vivek Gopalakrishnan, Neel Dey, Polina Golland</strong></p>
<p>Determining the 3D pose of a patient from a limited set of 2D X-ray images is a critical task in interventional settings. While preoperative volumetric imaging (e.g., CT and MRI) provides precise 3D localization and visualization of anatomical targets, these modalities cannot be acquired during procedures, where fast 2D imaging (X-ray) is used instead. To integrate volumetric guidance into intraoperative procedures, we present PolyPose, a simple and robust method for deformable 2D&#x2F;3D registration. PolyPose parameterizes complex 3D deformation fields as a composition of rigid transforms, leveraging the biological constraint that individual bones do not bend in typical motion. Unlike existing methods that either assume no inter-joint movement or fail outright in this under-determined setting, our polyrigid formulation enforces anatomically plausible priors that respect the piecewise rigid nature of human movement. This approach eliminates the need for expensive deformation regularizers that require patient- and procedure-specific hyperparameter optimization. Across extensive experiments on diverse datasets from orthopedic surgery and radiotherapy, we show that this strong inductive bias enables PolyPose to successfully align the patientâ€™s preoperative volume to as few as two X-ray images, thereby providing crucial 3D guidance in challenging sparse-view and limited-angle settings where current registration methods fail. </p>
<blockquote>
<p>åœ¨ä»‹å…¥ç¯å¢ƒä¸­ï¼Œä»æœ‰é™çš„2D Xå°„çº¿å›¾åƒé›†ä¸­ç¡®å®šæ‚£è€…çš„3Då§¿æ€æ˜¯ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚è™½ç„¶æœ¯å‰ä½“ç§¯æˆåƒï¼ˆä¾‹å¦‚CTå’ŒMRIï¼‰å¯ä»¥æä¾›ç²¾ç¡®çš„3Då®šä½å’Œè§£å‰–ç›®æ ‡çš„å¯è§†åŒ–ï¼Œä½†è¿™äº›æ¨¡å¼æ— æ³•åœ¨æ‰‹æœ¯è¿‡ç¨‹ä¸­è·å–ï¼Œæ­¤æ—¶ä½¿ç”¨çš„æ˜¯å¿«é€Ÿçš„2Dæˆåƒï¼ˆXå°„çº¿ï¼‰ã€‚ä¸ºäº†å°†ä½“ç§¯å¼•å¯¼æ•´åˆåˆ°æœ¯ä¸­ç¨‹åºä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PolyPoseï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œç¨³å¥çš„å¯å˜å½¢2D&#x2F;3Dæ³¨å†Œæ–¹æ³•ã€‚PolyPoseå°†å¤æ‚çš„3Då˜å½¢åœºå‚æ•°åŒ–ä¸ºåˆšæ€§å˜æ¢çš„ç»„åˆï¼Œåˆ©ç”¨ä¸ªä½“éª¨éª¼åœ¨å…¸å‹è¿åŠ¨ä¸­çš„ä¸å¼¯æ›²çš„ç”Ÿç‰©çº¦æŸã€‚ä¸å‡è®¾æ— å…³èŠ‚é—´è¿åŠ¨æˆ–åœ¨è¿™ä¸ªä¸ç¡®å®šç¯å¢ƒä¸­å½»åº•å¤±è´¥çš„å…¶ä»–ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„å¤šåˆšæ€§å…¬å¼å®æ–½äº†ç¬¦åˆäººä½“è¿åŠ¨åˆ†æ®µåˆšæ€§çš„è§£å‰–åˆç†å…ˆéªŒã€‚è¿™ç§æ–¹æ³•æ¶ˆé™¤äº†æ˜‚è´µçš„å˜å½¢è°ƒèŠ‚å™¨ï¼Œè¿™äº›è°ƒèŠ‚å™¨éœ€è¦è¿›è¡Œæ‚£è€…å’Œæ‰‹æœ¯ç‰¹å®šçš„è¶…å‚æ•°ä¼˜åŒ–ã€‚åœ¨éª¨ç§‘æ‰‹æœ¯å’Œæ”¾å°„æ²»ç–—çš„ä¸åŒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¿™ç§å¼ºçƒˆçš„å½’çº³åè§ä½¿PolyPoseèƒ½å¤ŸæˆåŠŸåœ°å°†æ‚£è€…çš„æœ¯å‰ä½“ç§¯ä¸è‡³å°‘ä¸¤å¼ Xå°„çº¿å›¾åƒå¯¹é½ï¼Œä»è€Œåœ¨å½“å‰çš„æ³¨å†Œæ–¹æ³•å¤±è´¥çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¨€ç–è§†å›¾å’Œæœ‰é™è§’åº¦è®¾ç½®ä¸­æä¾›å…³é”®çš„3DæŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19256v2">PDF</a> Code available at <a target="_blank" rel="noopener" href="https://github.com/eigenvivek/polypose">https://github.com/eigenvivek/polypose</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPolyPoseçš„ç®€æ˜“ç¨³å¥æ–¹æ³•ï¼Œç”¨äºå°†ä½“ç§¯å¼•å¯¼é›†æˆåˆ°æœ¯ä¸­ç¨‹åºä¸­ï¼Œå®ç°ä»æœ‰é™çš„2D Xå…‰å›¾åƒä¸­ç¡®å®šæ‚£è€…çš„3Då§¿æ€ã€‚è¯¥æ–¹æ³•å°†å¤æ‚çš„3Då˜å½¢åœºå‚æ•°åŒ–ä¸ºåˆšæ€§å˜æ¢çš„ç»„åˆï¼Œå¹¶åˆ©ç”¨äººä½“éª¨éª¼åœ¨å…¸å‹è¿åŠ¨ä¸­çš„ç”Ÿç‰©çº¦æŸï¼ˆå³éª¨éª¼ä¸ä¼šå¼¯æ›²ï¼‰ã€‚è¯¥æ–¹æ³•æ— éœ€æ˜‚è´µçš„å˜å½¢æ­£åˆ™åŒ–å™¨ï¼Œåè€…éœ€è¦é’ˆå¯¹æ‚£è€…å’Œç¨‹åºè¿›è¡Œç‰¹å®šçš„è¶…å‚æ•°ä¼˜åŒ–ã€‚åœ¨éª¨ç§‘æ‰‹æœ¯å’Œæ”¾å°„æ²»ç–—çš„å„ç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPolyPoseèƒ½å¤Ÿåœ¨ä»…ä½¿ç”¨å°‘æ•°å‡ å¼ Xå…‰å›¾åƒçš„æƒ…å†µä¸‹æˆåŠŸå¯¹é½æ‚£è€…çš„æœ¯å‰ä½“ç§¯ï¼Œä¸ºå½“å‰æ³¨å†Œæ–¹æ³•æ— æ³•åº”å¯¹çš„æŒ‘æˆ˜æ€§ç¨€ç–è§†å›¾å’Œæœ‰é™è§’åº¦è®¾ç½®æä¾›äº†å…³é”®çš„3DæŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PolyPoseæ˜¯ä¸€ç§ç”¨äºæœ¯ä¸­ç¡®å®šæ‚£è€…ä¸‰ç»´å§¿æ€çš„æ–¹æ³•ï¼Œå¯ä»æœ‰é™çš„äºŒç»´Xå…‰å›¾åƒä¸­å®ç°ã€‚</li>
<li>PolyPoseé‡‡ç”¨åˆšæ€§å˜æ¢çš„ç»„åˆæ¥å‚æ•°åŒ–å¤æ‚çš„ä¸‰ç»´å˜å½¢åœºã€‚</li>
<li>åˆ©ç”¨äººä½“éª¨éª¼çš„ç”Ÿç‰©çº¦æŸï¼ˆéª¨éª¼åœ¨å…¸å‹è¿åŠ¨ä¸­ä¸ä¼šå¼¯æ›²ï¼‰ï¼Œå¢å¼ºäº†æ–¹æ³•çš„ç¨³å¥æ€§ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒPolyPoseå¯ä»¥åœ¨æ— å…³èŠ‚é—´è¿åŠ¨å‡è®¾çš„åœºæ™¯ä¸­åº”ç”¨ï¼Œå¹¶ä¸”èƒ½åœ¨ç¼ºä¹è¶³å¤Ÿçš„å›¾åƒä¿¡æ¯çš„æƒ…å†µä¸‹ä»ç„¶æœ‰æ•ˆã€‚</li>
<li>PolyPoseæ–¹æ³•å…·æœ‰å¼ºå¤§çš„å½’çº³åè§ï¼Œèƒ½å¤ŸæˆåŠŸå¯¹é½æ‚£è€…çš„æœ¯å‰ä½“ç§¯ï¼Œå³ä½¿åªä½¿ç”¨å°‘æ•°å‡ å¼ Xå…‰å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨éª¨ç§‘æ‰‹æœ¯å’Œæ”¾å°„æ²»ç–—çš„å¤šç§æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒéªŒè¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9dd93adf44946277a3f77aa48453c06c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3eb00f213d94e43bb89137527ce19c7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f20b9c04100ba392d5957f8df4f12c2.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Non-rigid-Motion-Correction-for-MRI-Reconstruction-via-Coarse-To-Fine-Diffusion-Models"><a href="#Non-rigid-Motion-Correction-for-MRI-Reconstruction-via-Coarse-To-Fine-Diffusion-Models" class="headerlink" title="Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine   Diffusion Models"></a>Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine   Diffusion Models</h2><p><strong>Authors:Frederic Wang, Jonathan I. Tamir</strong></p>
<p>Magnetic Resonance Imaging (MRI) is highly susceptible to motion artifacts due to the extended acquisition times required for k-space sampling. These artifacts can compromise diagnostic utility, particularly for dynamic imaging. We propose a novel alternating minimization framework that leverages a bespoke diffusion model to jointly reconstruct and correct non-rigid motion-corrupted k-space data. The diffusion model uses a coarse-to-fine denoising strategy to capture large overall motion and reconstruct the lower frequencies of the image first, providing a better inductive bias for motion estimation than that of standard diffusion models. We demonstrate the performance of our approach on both real-world cine cardiac MRI datasets and complex simulated rigid and non-rigid deformations, even when each motion state is undersampled by a factor of 64x. Additionally, our method is agnostic to sampling patterns, anatomical variations, and MRI scanning protocols, as long as some low frequency components are sampled during each motion state. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ç”±äºkç©ºé—´é‡‡æ ·æ‰€éœ€çš„å»¶é•¿é‡‡é›†æ—¶é—´è€Œææ˜“å—åˆ°è¿åŠ¨ä¼ªå½±çš„å½±å“ã€‚è¿™äº›ä¼ªå½±å¯èƒ½æŸå®³è¯Šæ–­æ•ˆç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€æˆåƒä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„äº¤æ›¿æœ€å°åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸“ç”¨æ‰©æ•£æ¨¡å‹è”åˆé‡å»ºå’Œçº æ­£éåˆšæ€§è¿åŠ¨å—å¹²æ‰°çš„kç©ºé—´æ•°æ®ã€‚è¯¥æ‰©æ•£æ¨¡å‹é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„é™å™ªç­–ç•¥ï¼Œé¦–å…ˆæ•è·æ•´ä½“å¤§è¿åŠ¨å¹¶é‡å»ºå›¾åƒçš„ä½é¢‘éƒ¨åˆ†ï¼Œä¸ºè¿åŠ¨ä¼°è®¡æä¾›æ›´å¥½çš„å½’çº³åç½®ï¼Œä¼˜äºæ ‡å‡†æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œçš„ç”µå½±å¿ƒè„MRIæ•°æ®é›†å’Œå¤æ‚çš„æ¨¡æ‹Ÿåˆšæ€§å’Œéåˆšæ€§å˜å½¢ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•æ€§èƒ½ï¼Œå³ä½¿åœ¨æ¯ç§è¿åŠ¨çŠ¶æ€ä¸‹ä»¥64å€æ¬ é‡‡æ ·çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯¹äºé‡‡æ ·æ¨¡å¼ã€è§£å‰–å˜å¼‚å’ŒMRIæ‰«æåè®®å‡ä¿æŒä¸æ•æ„Ÿæ€§ï¼Œåªè¦åœ¨æ¯ä¸ªè¿åŠ¨çŠ¶æ€ä¸‹é‡‡æ ·ä¸€äº›ä½é¢‘æˆåˆ†å³å¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15057v2">PDF</a> ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹è§£å†³ç£å…±æŒ¯æˆåƒä¸­ç”±äºé•¿æ—¶é—´é‡‡é›†å¼•èµ·çš„éåˆšæ€§è¿åŠ¨å¹²æ‰°é—®é¢˜çš„æ–¹æ³•ã€‚é€šè¿‡ç²—åˆ°ç»†çš„é™å™ªç­–ç•¥ï¼Œå…ˆé‡å»ºå›¾åƒçš„ä½é¢‘éƒ¨åˆ†ï¼Œä¼°è®¡æ•´ä½“è¿åŠ¨ï¼Œæé«˜è¯Šæ–­æ•ˆæœã€‚åœ¨çœŸå®çš„å¿ƒè„MRIæ•°æ®é›†å’Œå¤æ‚çš„æ¨¡æ‹Ÿåˆšæ€§ä¸éåˆšæ€§å˜å½¢ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå³ä½¿åœ¨æ¯ä¸ªè¿åŠ¨çŠ¶æ€ä¸‹é‡‡æ ·é¢‘ç‡é™ä½64å€ï¼Œè¯¥æ–¹æ³•ä¾ç„¶æœ‰æ•ˆã€‚æ­¤æ–¹æ³•å¯¹é‡‡æ ·æ¨¡å¼ã€è§£å‰–å˜å¼‚å’ŒMRIæ‰«æåè®®å…·æœ‰é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ç”±äºé•¿æ—¶é—´çš„é‡‡é›†è¿‡ç¨‹å®¹æ˜“å—åˆ°è¿åŠ¨å¹²æ‰°çš„å½±å“ã€‚</li>
<li>éåˆšæ€§è¿åŠ¨å¹²æ‰°ä¼šä¸¥é‡å½±å“MRIçš„è¯Šæ–­æ•ˆæœã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„äº¤æ›¿æœ€å°åŒ–æ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨ç²—åˆ°ç»†çš„é™å™ªç­–ç•¥ï¼Œé¦–å…ˆé‡å»ºå›¾åƒçš„ä½é¢‘éƒ¨åˆ†ï¼Œç”¨äºä¼°è®¡æ•´ä½“è¿åŠ¨ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨çœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¯¹å¤šç§ç±»å‹çš„è¿åŠ¨å¹²æ‰°æœ‰æ•ˆã€‚</li>
<li>å³ä½¿åœ¨é‡‡æ ·é¢‘ç‡é™ä½çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ä»ç„¶å…·æœ‰é«˜æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-645581bebb039dd369e68b3ea2ddeed4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40c450a3001a4b039f9561669912de35.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7a89deb44f02ea190b093675187992e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1879d5afbf5f04752a5b3d36a1b7585.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1b84582bd7f57ab6c58ed143a97ad8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57052dc9379f13a30206187b3a442480.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4118c0aa3cf8ed7d5566417769df99db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d2a90d028abcb96a0153659c4c1beca.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="UniBiomed-A-Universal-Foundation-Model-for-Grounded-Biomedical-Image-Interpretation"><a href="#UniBiomed-A-Universal-Foundation-Model-for-Grounded-Biomedical-Image-Interpretation" class="headerlink" title="UniBiomed: A Universal Foundation Model for Grounded Biomedical Image   Interpretation"></a>UniBiomed: A Universal Foundation Model for Grounded Biomedical Image   Interpretation</h2><p><strong>Authors:Linshan Wu, Yuxiang Nie, Sunan He, Jiaxin Zhuang, Luyang Luo, Neeraj Mahboobani, Varut Vardhanabhuti, Ronald Cheong Kin Chan, Yifan Peng, Pranav Rajpurkar, Hao Chen</strong></p>
<p>The integration of AI-assisted biomedical image analysis into clinical practice demands AI-generated findings that are not only accurate but also interpretable to clinicians. However, existing biomedical AI models generally lack the ability to simultaneously generate diagnostic findings and localize corresponding biomedical objects. This limitation makes it challenging for clinicians to correlate AI-generated findings with visual evidence (e.g., tiny lesions) in images and interpret the results of AI models. To address this challenge, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation, which is capable of generating accurate diagnostic findings and simultaneously segmenting the corresponding biomedical targets. UniBiomed is based on a novel integration of Multi-modal Large Language Model and Segment Anything Model, which can effectively unify diverse biomedical tasks in universal training for advancing grounded interpretation. To develop UniBiomed, we curate a large-scale dataset comprising over 27 million triplets of images, region annotations, and text descriptions across ten biomedical imaging modalities. Extensive validation on 70 internal and 14 external datasets demonstrated the state-of-the-art performance of UniBiomed in diverse biomedical tasks, including image segmentation, disease recognition, region-aware diagnosis, vision question answering, and report generation. In summary, UniBiomed is a powerful and versatile biomedical foundation model, unlocking the untapped grounded interpretation capability for optimizing AI-assisted biomedical image analysis. </p>
<blockquote>
<p>å°†äººå·¥æ™ºèƒ½è¾…åŠ©ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ææ•´åˆåˆ°ä¸´åºŠå®è·µä¸­çš„è¦æ±‚ï¼Œæ˜¯äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å‘ç°ä¸ä»…å¿…é¡»å‡†ç¡®ï¼Œè€Œä¸”å¿…é¡»èƒ½å¤Ÿè®©ä¸´åºŠåŒ»ç”Ÿè¿›è¡Œè§£è¯»ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç”Ÿç‰©åŒ»å­¦äººå·¥æ™ºèƒ½æ¨¡å‹é€šå¸¸ç¼ºä¹åŒæ—¶ç”Ÿæˆè¯Šæ–­ç»“æœå¹¶å®šä½ç›¸åº”ç”Ÿç‰©åŒ»å­¦å¯¹è±¡çš„èƒ½åŠ›ã€‚è¿™ä¸€å±€é™æ€§ä½¿å¾—ä¸´åºŠåŒ»ç”Ÿéš¾ä»¥å°†äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å‘ç°ä¸å›¾åƒä¸­çš„è§†è§‰è¯æ®ï¼ˆä¾‹å¦‚å¾®å°ç—…å˜ï¼‰ç›¸å…³è”ï¼Œå¹¶è§£è¯»äººå·¥æ™ºèƒ½æ¨¡å‹çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21336v2">PDF</a> The first universal foundation model for grounded biomedical image   interpretation</p>
<p><strong>Summary</strong></p>
<p>åŸºäºäººå·¥æ™ºèƒ½çš„ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æåœ¨ä¸´åºŠå®è·µä¸­çš„é›†æˆè¦æ±‚AIç”Ÿæˆçš„å‘ç°ä¸ä»…å‡†ç¡®ï¼Œè€Œä¸”ä¸´åºŠåŒ»ç”Ÿå¯è§£é‡Šã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç”Ÿç‰©åŒ»å­¦AIæ¨¡å‹é€šå¸¸ç¼ºä¹åŒæ—¶ç”Ÿæˆè¯Šæ–­ç»“æœå’Œå®šä½ç›¸åº”ç”Ÿç‰©åŒ»å­¦å¯¹è±¡çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniBiomedï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºåŸºäºåŸºç¡€ç”Ÿç‰©åŒ»å­¦å›¾åƒè§£é‡Šçš„é€šç”¨åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆå‡†ç¡®çš„è¯Šæ–­ç»“æœå¹¶åŒæ—¶åˆ†å‰²ç›¸åº”çš„ç”Ÿç‰©åŒ»å­¦ç›®æ ‡ã€‚å®ƒé€šè¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œåˆ†æ®µä»»ä½•æ¨¡å‹çš„å…¨æ–°æ•´åˆï¼Œæœ‰æ•ˆåœ°ç»Ÿä¸€äº†å¤šæ ·åŒ–çš„ç”Ÿç‰©åŒ»å­¦ä»»åŠ¡ï¼Œä»¥æ¨åŠ¨åŸºäºç°å®çš„è§£é‡Šã€‚ä¸ºå¼€å‘UniBiomedï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡2700ä¸‡å¼ å›¾åƒã€åŒºåŸŸæ³¨é‡Šå’Œæ–‡æœ¬æè¿°çš„ä¸‰å…ƒç»„ï¼Œè·¨è¶Šåç§ç”Ÿç‰©åŒ»å­¦æˆåƒæ¨¡å¼ã€‚åœ¨70ä¸ªå†…éƒ¨å’Œ14ä¸ªå¤–éƒ¨æ•°æ®é›†ä¸Šçš„å¹¿æ³›éªŒè¯è¡¨æ˜ï¼ŒUniBiomedåœ¨å¤šç§ç”Ÿç‰©åŒ»å­¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†å‰²ã€ç–¾ç—…è¯†åˆ«ã€åŒºåŸŸæ„ŸçŸ¥è¯Šæ–­ã€è§†è§‰é—®ç­”å’ŒæŠ¥å‘Šç”Ÿæˆã€‚æ€»çš„æ¥è¯´ï¼ŒUniBiomedæ˜¯ä¸€ä¸ªå¼ºå¤§ä¸”é€šç”¨çš„ç”Ÿç‰©åŒ»å­¦åŸºç¡€æ¨¡å‹ï¼Œè§£é”äº†åŸºäºç°å®è§£é‡Šçš„æ½œåœ¨èƒ½åŠ›ï¼Œä»¥ä¼˜åŒ–äººå·¥æ™ºèƒ½è¾…åŠ©ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é›†æˆéœ€è¦æ—¢å‡†ç¡®åˆä¸´åºŠåŒ»ç”Ÿå¯è§£é‡Šçš„å‘ç°ã€‚</li>
<li>ç°æœ‰ç”Ÿç‰©åŒ»å­¦AIæ¨¡å‹é€šå¸¸ä¸èƒ½åŒæ—¶ç”Ÿæˆè¯Šæ–­ç»“æœå’Œå®šä½ç›¸åº”çš„ç”Ÿç‰©åŒ»å­¦å¯¹è±¡ï¼Œç»™ä¸´åºŠåŒ»ç”Ÿå¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>UniBiomedæ˜¯é¦–ä¸ªç”¨äºåŸºäºåŸºç¡€ç”Ÿç‰©åŒ»å­¦å›¾åƒè§£é‡Šçš„é€šç”¨æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶ç”Ÿæˆè¯Šæ–­ç»“æœå¹¶åˆ†å‰²ç”Ÿç‰©åŒ»å­¦ç›®æ ‡ã€‚</li>
<li>UniBiomedé€šè¿‡æ•´åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œåˆ†æ®µä»»ä½•æ¨¡å‹ï¼Œæœ‰æ•ˆç»Ÿä¸€å¤šæ ·åŒ–çš„ç”Ÿç‰©åŒ»å­¦ä»»åŠ¡ã€‚</li>
<li>UniBiomedçš„å¼€å‘åŸºäºä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«å›¾åƒã€åŒºåŸŸæ³¨é‡Šå’Œæ–‡æœ¬æè¿°çš„ä¸‰å…ƒç»„ã€‚</li>
<li>UniBiomedåœ¨å¤šç§ç”Ÿç‰©åŒ»å­¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†å‰²ã€ç–¾ç—…è¯†åˆ«ã€åŒºåŸŸæ„ŸçŸ¥è¯Šæ–­ã€è§†è§‰é—®ç­”å’ŒæŠ¥å‘Šç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21336">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c70c472b79263507f04fe3f49d87fa50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-92c98ba83c01a31ea9e9c57592c961b1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-31/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cc771af2bdc4ffade361c5cfb5d7e578.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0d2a90d028abcb96a0153659c4c1beca.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  DarkDiff Advancing Low-Light Raw Enhancement by Retasking Diffusion   Models for Camera ISP
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
