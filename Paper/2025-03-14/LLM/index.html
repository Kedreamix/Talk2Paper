<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-03-14  SimLingo Vision-Only Closed-Loop Autonomous Driving with   Language-Action Alignment">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-2443f5f69ced566e7b2344260a6fae41.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    82 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-14-更新"><a href="#2025-03-14-更新" class="headerlink" title="2025-03-14 更新"></a>2025-03-14 更新</h1><h2 id="SimLingo-Vision-Only-Closed-Loop-Autonomous-Driving-with-Language-Action-Alignment"><a href="#SimLingo-Vision-Only-Closed-Loop-Autonomous-Driving-with-Language-Action-Alignment" class="headerlink" title="SimLingo: Vision-Only Closed-Loop Autonomous Driving with   Language-Action Alignment"></a>SimLingo: Vision-Only Closed-Loop Autonomous Driving with   Language-Action Alignment</h2><p><strong>Authors:Katrin Renz, Long Chen, Elahe Arani, Oleg Sinavski</strong></p>
<p>Integrating large language models (LLMs) into autonomous driving has attracted significant attention with the hope of improving generalization and explainability. However, existing methods often focus on either driving or vision-language understanding but achieving both high driving performance and extensive language understanding remains challenging. In addition, the dominant approach to tackle vision-language understanding is using visual question answering. However, for autonomous driving, this is only useful if it is aligned with the action space. Otherwise, the model’s answers could be inconsistent with its behavior. Therefore, we propose a model that can handle three different tasks: (1) closed-loop driving, (2) vision-language understanding, and (3) language-action alignment. Our model SimLingo is based on a vision language model (VLM) and works using only camera, excluding expensive sensors like LiDAR. SimLingo obtains state-of-the-art performance on the widely used CARLA simulator on the Bench2Drive benchmark and is the winning entry at the CARLA challenge 2024. Additionally, we achieve strong results in a wide variety of language-related tasks while maintaining high driving performance. </p>
<blockquote>
<p>将大型语言模型（LLM）集成到自动驾驶中，旨在提高通用性和可解释性，已经引起了人们的广泛关注。然而，现有方法往往集中在驾驶或视觉语言理解上，而实现高驾驶性能和广泛的语言理解仍然具有挑战性。此外，解决视觉语言理解的主要方法是使用视觉问答。然而，对于自动驾驶来说，只有将其与动作空间对齐时才有用。否则，模型的答案可能与其行为不一致。因此，我们提出了一种能够处理三种不同任务模型：即（1）闭环驾驶、（2）视觉语言理解和（3）语言动作对齐。我们的模型SimLingo基于视觉语言模型（VLM），仅使用相机工作，不包括昂贵的传感器，如激光雷达。SimLingo在广泛使用的CARLA模拟器上的Bench2Drive基准测试中获得了最先进的性能，并在CARLA挑战2024中获得了第一名。此外，我们在多种语言相关任务中取得了强劲的结果，同时保持了高驾驶性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09594v1">PDF</a> CVPR 2025. 1st Place @ CARLA Challenge 2024. Challenge tech report   (preliminary version of SimLingo): arXiv:2406.10165</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在自动驾驶中的应用已经引起了广泛关注，有助于提高通用性和可解释性。然而，现有方法常常集中在驾驶或视觉语言理解方面，实现高驾驶性能和广泛的语言理解仍然具有挑战性。为此，本文提出了一种可以处理三种不同任务（闭环驾驶、视觉语言理解和语言行为对齐）的模型SimLingo。该模型基于视觉语言模型（VLM），仅使用相机工作，不使用昂贵的传感器如激光雷达。SimLingo在广泛使用的CARLA模拟器上的Bench2Drive基准测试中取得了最新性能，并在CARLA挑战赛中获得了第一名。同时，该模型在各种语言相关任务中也取得了显著成果，同时保持了较高的驾驶性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在自动驾驶中的集成旨在提高通用性和可解释性。</li>
<li>当前方法在实现高驾驶性能和广泛的语言理解方面存在挑战。</li>
<li>SimLingo模型能够处理闭环驾驶、视觉语言理解和语言行为对齐三种任务。</li>
<li>SimLingo基于视觉语言模型（VLM），仅使用相机，不使用昂贵的传感器。</li>
<li>SimLingo在CARLA模拟器的Bench2Drive基准测试中取得了最新性能。</li>
<li>SimLingo在CARLA挑战赛中获得了第一名。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09594">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c460299ce81847049cf133ea2da88a3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa3e14340d4485e856439b0827fdb56d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcf9aa9a45b3bb2a5d09a332c06c7d32.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BIMBA-Selective-Scan-Compression-for-Long-Range-Video-Question-Answering"><a href="#BIMBA-Selective-Scan-Compression-for-Long-Range-Video-Question-Answering" class="headerlink" title="BIMBA: Selective-Scan Compression for Long-Range Video Question   Answering"></a>BIMBA: Selective-Scan Compression for Long-Range Video Question   Answering</h2><p><strong>Authors:Md Mohaiminul Islam, Tushar Nagarajan, Huiyu Wang, Gedas Bertasius, Lorenzo Torresani</strong></p>
<p>Video Question Answering (VQA) in long videos poses the key challenge of extracting relevant information and modeling long-range dependencies from many redundant frames. The self-attention mechanism provides a general solution for sequence modeling, but it has a prohibitive cost when applied to a massive number of spatiotemporal tokens in long videos. Most prior methods rely on compression strategies to lower the computational cost, such as reducing the input length via sparse frame sampling or compressing the output sequence passed to the large language model (LLM) via space-time pooling. However, these naive approaches over-represent redundant information and often miss salient events or fast-occurring space-time patterns. In this work, we introduce BIMBA, an efficient state-space model to handle long-form videos. Our model leverages the selective scan algorithm to learn to effectively select critical information from high-dimensional video and transform it into a reduced token sequence for efficient LLM processing. Extensive experiments demonstrate that BIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks, including PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and Video-MME. Code, and models are publicly available at <a target="_blank" rel="noopener" href="https://sites.google.com/view/bimba-mllm">https://sites.google.com/view/bimba-mllm</a>. </p>
<blockquote>
<p>视频问答（VQA）在长视频中面临的关键挑战是从大量冗余帧中提取相关信息并对长距离依赖进行建模。自注意力机制为序列建模提供了一般解决方案，但当应用于长视频中大量时空令牌时，其成本是巨大的。大多数先前的方法依赖于压缩策略来降低计算成本，例如通过稀疏帧采样减少输入长度，或通过时空池化压缩传递给大型语言模型（LLM）的输出序列。然而，这些简单的方法过度代表了冗余信息，并且经常错过重要事件或快速发生的时空模式。在这项工作中，我们引入了BIMBA，一种用于处理长格式视频的高效状态空间模型。我们的模型利用选择性扫描算法来学习从高维视频中选择关键信息并将其转换为减少的令牌序列以进行高效的LLM处理。大量实验表明，BIMBA在多个长格式VQA基准测试中实现了最先进的准确性，包括PerceptionTest、NExT-QA、EgoSchema、VNBench、LongVideoBench和视频MME。代码和模型可在<a target="_blank" rel="noopener" href="https://sites.google.com/view/bimba-mllm%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://sites.google.com/view/bimba-mllm公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09590v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了针对长视频中的视频问题回答（VQA）挑战，提出了一种新的高效状态空间模型BIMBA。该模型利用选择性扫描算法从高维视频中选择关键信息，并将其转换为减少的令牌序列，以便高效的大型语言模型处理。实验表明，BIMBA在多个长格式VQA基准测试中实现了最先进的准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VQA在长视频上面临提取相关信息和建模长范围依赖性的关键挑战。</li>
<li>自注意力机制为序列建模提供了通用解决方案，但应用于长视频的众多时空令牌时计算成本高昂。</li>
<li>之前的策略如稀疏帧采样和时空池化以降低计算成本，但可能过度代表冗余信息并错过重要事件或快速发生的时空模式。</li>
<li>BIMBA模型是一个高效的状态空间模型，用于处理长格式视频。</li>
<li>BIMBA利用选择性扫描算法从高维视频中选择关键信息。</li>
<li>BIMBA在多个长格式VQA基准测试中实现了最先进的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09590">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-acaf27607f9b7fdc041e00437b14dcb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79ae60c7dc8957471ea7d235fbdff4c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27de4f09c104aa2e7b1dfbda68b5e095.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b92ad4ecc0aaabba4ce8ea9eb170980.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Search-R1-Training-LLMs-to-Reason-and-Leverage-Search-Engines-with-Reinforcement-Learning"><a href="#Search-R1-Training-LLMs-to-Reason-and-Leverage-Search-Engines-with-Reinforcement-Learning" class="headerlink" title="Search-R1: Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning"></a>Search-R1: Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning</h2><p><strong>Authors:Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, Jiawei Han</strong></p>
<p>Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Retrieval augmentation and tool-use training approaches where a search engine is treated as a tool lack complex multi-turn retrieval flexibility or require large-scale supervised data. Prompting advanced LLMs with reasoning capabilities during inference to use search engines is not optimal, since the LLM does not learn how to optimally interact with the search engine. This paper introduces Search-R1, an extension of the DeepSeek-R1 model where the LLM learns – solely through reinforcement learning (RL) – to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM rollouts with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/PeterGriffinJin/Search-R1">https://github.com/PeterGriffinJin/Search-R1</a>. </p>
<blockquote>
<p>在大规模语言模型（LLM）中进行有效的推理和文本生成，获取外部知识和最新信息是关键。尽管检索增强和工具使用训练的方法将搜索引擎视为工具，但它们在复杂的多轮检索灵活性方面有所不足或需要大量监督数据。在推理过程中提示具有推理能力的先进LLM使用搜索引擎并不理想，因为LLM并没有学习如何最优地与搜索引擎进行交互。本文介绍了Search-R1，它是DeepSeek-R1模型的扩展，其中LLM通过强化学习（RL）自主学习，在逐步推理过程中自主生成（多个）搜索查询并进行实时检索。Search-R1通过多轮搜索交互优化LLM的滚动操作，利用检索令牌屏蔽进行稳定的RL训练和一个简单的基于结果奖励函数。在七个问答数据集上的实验表明，Search-R1相较于最先进基线提高了26%（Qwen2.5-7B）、21%（Qwen2.5-3B）和10%（LLaMA3.2-3B）的性能。本文还深入探讨了强化学习优化方法、LLM选择和检索增强推理中的响应长度动态。代码和模型检查点可访问于：<a target="_blank" rel="noopener" href="https://github.com/PeterGriffinJin/Search-R1%E3%80%82">https://github.com/PeterGriffinJin/Search-R1。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09516v1">PDF</a> 16 pages</p>
<p><strong>Summary</strong></p>
<p>本论文提出Search-R1模型，该模型是大规模语言模型（LLM）的一种扩展，通过强化学习（RL）自主学习在实时检索中进行多轮搜索查询。Search-R1优化了LLM的推理过程，通过多轮搜索交互、检索到的令牌掩码用于稳定的RL训练以及简单的结果导向奖励函数来实现。实验结果显示，Search-R1在七个问答数据集上的性能较现有技术提高了26%（Qwen2.5-7B）、21%（Qwen2.5-3B）和10%（LLaMA3.2-3B）。同时，本论文还对RL优化方法、LLM选择和响应长度动态等进行了实证分析。代码和模型检查点可在指定网址下载。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Search-R1模型是LLM的一种扩展，采用强化学习（RL）自主学习进行多轮搜索查询。</li>
<li>该模型通过实时检索、多轮搜索交互等方式优化LLM推理过程。</li>
<li>Search-R1通过检索到的令牌掩码实现稳定的RL训练。</li>
<li>该模型采用简单的结果导向奖励函数。</li>
<li>实验结果显示，Search-R1在多个问答数据集上的性能显著提升。</li>
<li>本论文提供了关于RL优化方法、LLM选择和响应长度动态的实证分析。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09516">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c71d0293ecd0ef9e3d7bef31402bf39b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7677d1b5771eb2d8fc9f3c532c475d8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ReMA-Learning-to-Meta-think-for-LLMs-with-Multi-Agent-Reinforcement-Learning"><a href="#ReMA-Learning-to-Meta-think-for-LLMs-with-Multi-Agent-Reinforcement-Learning" class="headerlink" title="ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement   Learning"></a>ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement   Learning</h2><p><strong>Authors:Ziyu Wan, Yunxiang Li, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, Ying Wen</strong></p>
<p>Recent research on Reasoning of Large Language Models (LLMs) has sought to further enhance their performance by integrating meta-thinking – enabling models to monitor, evaluate, and control their reasoning processes for more adaptive and effective problem-solving. However, current single-agent work lacks a specialized design for acquiring meta-thinking, resulting in low efficacy. To address this challenge, we introduce Reinforced Meta-thinking Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think about thinking. ReMA decouples the reasoning process into two hierarchical agents: a high-level meta-thinking agent responsible for generating strategic oversight and plans, and a low-level reasoning agent for detailed executions. Through iterative reinforcement learning with aligned objectives, these agents explore and learn collaboration, leading to improved generalization and robustness. Experimental results demonstrate that ReMA outperforms single-agent RL baselines on complex reasoning tasks, including competitive-level mathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation studies further illustrate the evolving dynamics of each distinct agent, providing valuable insights into how the meta-thinking reasoning process enhances the reasoning capabilities of LLMs. </p>
<blockquote>
<p>近期关于大语言模型（LLM）推理的研究，旨在通过融入元思维来进一步提升其性能。元思维能够让模型监控、评估和控制系统自身的推理过程，从而实现更加适应性和高效的问题解决。然而，现有的单智能体研究缺乏获取元思维的专门设计，导致效果不佳。为解决这一挑战，我们提出了强化元思维智能体（ReMA）这一新型框架，利用多智能体强化学习（MARL）来激发元思维行为，鼓励LLM进行反思性思考。ReMA将推理过程解耦为两个层次化的智能体：一个高层次的元思维智能体，负责生成战略性监督和计划；一个低层次的推理智能体，负责详细执行。通过目标一致的迭代强化学习，这些智能体探索和学习协作，实现了更好的泛化能力和稳健性。实验结果表明，ReMA在复杂的推理任务上超越了单智能体RL基准测试，包括竞争级别的数学基准测试和LLM作为法官的基准测试。全面的消融研究进一步说明了每个独特智能体的动态演化过程，为元思维推理过程如何增强LLM的推理能力提供了宝贵的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09501v1">PDF</a> </p>
<p><strong>Summary</strong><br>大型语言模型（LLM）引入元思维以增强其性能，但现有单一模型设计在获取元思维方面存在局限性。为此，我们提出强化元思维代理（ReMA）框架，利用多代理强化学习（MARL）激发元思维行为。ReMA将推理过程分为两个层次代理：高级元思维代理负责战略监督与计划，低级推理代理负责详细执行。通过目标对齐的迭代强化学习，这些代理能够探索与协作，从而提高泛化与稳健性。实验结果显示，ReMA在复杂推理任务上优于单一代理RL基线，包括竞争性数学基准测试和LLM作为法官的基准测试。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM引入元思维以增强性能，面临单一模型设计获取元思维的局限性。</li>
<li>提出ReMA框架，利用MARL激发元思维行为。</li>
<li>ReMA将推理过程分为高级元思维代理和低级推理代理。</li>
<li>通过目标对齐的迭代强化学习，代理能够探索与协作。</li>
<li>ReMA在复杂推理任务上表现优越，包括数学和LLM判断任务。</li>
<li>消融研究展示了不同代理在元思维推理过程中的动态变化。</li>
<li>元思维增强LLM的推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09501">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-31bc4292f3764e1db00880af7f69f7be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5087306fb21b19db6c96420a875b08bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f204a1cf33c2518ea1cd5e1c60c1b75.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CASTLE-Benchmarking-Dataset-for-Static-Code-Analyzers-and-LLMs-towards-CWE-Detection"><a href="#CASTLE-Benchmarking-Dataset-for-Static-Code-Analyzers-and-LLMs-towards-CWE-Detection" class="headerlink" title="CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards   CWE Detection"></a>CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards   CWE Detection</h2><p><strong>Authors:Richard A. Dubniczky, Krisztofer Zoltán Horvát, Tamás Bisztray, Mohamed Amine Ferrag, Lucas C. Cordeiro, Norbert Tihanyi</strong></p>
<p>Identifying vulnerabilities in source code is crucial, especially in critical software components. Existing methods such as static analysis, dynamic analysis, formal verification, and recently Large Language Models are widely used to detect security flaws. This paper introduces CASTLE (CWE Automated Security Testing and Low-Level Evaluation), a benchmarking framework for evaluating the vulnerability detection capabilities of different methods. We assess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using a hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs. We propose the CASTLE Score, a novel evaluation metric to ensure fair comparison. Our results reveal key differences: ESBMC (a formal verification tool) minimizes false positives but struggles with vulnerabilities beyond model checking, such as weak cryptography or SQL injection. Static analyzers suffer from high false positives, increasing manual validation efforts for developers. LLMs perform exceptionally well in the CASTLE dataset when identifying vulnerabilities in small code snippets. However, their accuracy declines, and hallucinations increase as the code size grows. These results suggest that LLMs could play a pivotal role in future security solutions, particularly within code completion frameworks, where they can provide real-time guidance to prevent vulnerabilities. The dataset is accessible at <a target="_blank" rel="noopener" href="https://github.com/CASTLE-Benchmark">https://github.com/CASTLE-Benchmark</a>. </p>
<blockquote>
<p>识别源代码中的漏洞至关重要，特别是在关键软件组件中。现有的方法，如静态分析、动态分析、形式化验证和最近出现的大型语言模型，被广泛用于检测安全漏洞。本文介绍了CASTLE（CWE自动化安全测试和低级评估），这是一个用于评估不同方法漏洞检测能力的基准测试框架。我们使用包含25个常见CWE的250个手工制作的小型基准程序数据集，评估了13种静态分析工具、1LLM和2种形式化验证工具。我们提出了CASTLE分数，一种新的评估指标，以确保公平比较。我们的结果表明了关键差异：ESBMC（一种形式化验证工具）虽然能最小化误报，但在模型检查之外的漏洞，如弱加密或SQL注入等方面存在困难。静态分析器存在较高的误报率，增加了开发者的手动验证工作。大型语言模型在CASTLE数据集中表现优异，特别是在识别小代码片段中的漏洞方面。然而，随着代码量的增长，其准确性会下降，幻觉现象也会增多。这些结果表明，大型语言模型在未来的安全解决方案中可能会发挥关键作用，特别是在代码补全框架中，它们可以提供实时指导以防止漏洞。数据集可在<a target="_blank" rel="noopener" href="https://github.com/CASTLE-Benchmark%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/CASTLE-Benchmark上获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09433v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了一个名为CASTLE的基准测试框架，用于评估源代码漏洞检测工具的性能。通过对多种静态分析工具、大型语言模型和形式验证工具的综合评估，发现不同工具在检测不同种类的公共弱点方面具有显著区别。大型语言模型在处理小型代码片段时表现良好，但随着代码量的增长，其准确性下降，出现更多的误判。本文结果对未来安全解决方案中的大型语言模型角色有重要影响。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>CASTLE框架用于评估源代码漏洞检测工具的性能。</li>
<li>评估了多种静态分析工具、大型语言模型和形式验证工具。</li>
<li>发现形式验证工具ESBMC在减少误报方面表现良好，但在检测某些特定漏洞方面存在困难。</li>
<li>静态分析工具虽然会出现高误报率，增加了开发者的手动验证工作负担。</li>
<li>大型语言模型在处理小型代码片段时表现出良好的识别漏洞能力，但随着代码规模的增大，其准确性下降，产生更多的幻觉结果。</li>
<li>大型语言模型在未来安全解决方案中可能扮演重要角色，特别是在代码补全框架中提供实时指导以防止漏洞。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09433">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-958dd88f56e53ddec3429fadbf3322c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6779c78f5ce29093a05384c452e1864b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bda835a092b21ac43efd822e29ba2626.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Got-Compute-but-No-Data-Lessons-From-Post-training-a-Finnish-LLM"><a href="#Got-Compute-but-No-Data-Lessons-From-Post-training-a-Finnish-LLM" class="headerlink" title="Got Compute, but No Data: Lessons From Post-training a Finnish LLM"></a>Got Compute, but No Data: Lessons From Post-training a Finnish LLM</h2><p><strong>Authors:Elaine Zosa, Ville Komulainen, Sampo Pyysalo</strong></p>
<p>As LLMs gain more popularity as chatbots and general assistants, methods have been developed to enable LLMs to follow instructions and align with human preferences. These methods have found success in the field, but their effectiveness has not been demonstrated outside of high-resource languages. In this work, we discuss our experiences in post-training an LLM for instruction-following for English and Finnish. We use a multilingual LLM to translate instruction and preference datasets from English to Finnish. We perform instruction tuning and preference optimization in English and Finnish and evaluate the instruction-following capabilities of the model in both languages. Our results show that with a few hundred Finnish instruction samples we can obtain competitive performance in Finnish instruction-following. We also found that although preference optimization in English offers some cross-lingual benefits, we obtain our best results by using preference data from both languages. We release our model, datasets, and recipes under open licenses at <a target="_blank" rel="noopener" href="https://huggingface.co/LumiOpen/Poro-34B-chat-OpenAssistant">https://huggingface.co/LumiOpen/Poro-34B-chat-OpenAssistant</a> </p>
<blockquote>
<p>随着大型语言模型（LLM）在聊天机器人和通用助理中越来越受欢迎，已经开发了一些方法使LLM能够遵循指令并与人类偏好保持一致。这些方法在该领域已经取得了成功，但它们在非高资源语言环境中的有效性尚未得到证明。在这项工作中，我们分享了我们在为英语和芬兰语进行指令遵循训练大型语言模型后的经验。我们使用一种多语言的大型语言模型，将指令和偏好数据集从英语翻译成芬兰语。我们在英语和芬兰语中进行指令调整和偏好优化，并评估了该模型在这两种语言中的指令遵循能力。我们的结果表明，只需几百个芬兰语指令样本，我们就可以在芬兰语的指令遵循方面取得具有竞争力的性能。我们还发现，尽管英语中的偏好优化提供了一些跨语言的好处，但我们通过使用两种语言的偏好数据获得了最佳结果。我们在<a target="_blank" rel="noopener" href="https://huggingface.co/LumiOpen/Poro-34B-chat-OpenAssistant%E4%B8%8B%E4%BB%A5%E5%BC%80%E6%94%BE%E8%AE%B8%E5%8F%AF%E8%AF%81%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84%E6%A8%A1%E5%9E%8B%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E9%85%8D%E6%96%B9%E3%80%82">https://huggingface.co/LumiOpen/Poro-34B-chat-OpenAssistant下以开放许可证发布我们的模型、数据集和配方。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09407v1">PDF</a> 7 pages</p>
<p><strong>Summary</strong></p>
<p>本摘要介绍LLM在自然语言处理中的应用及其所面临的挑战，特别是如何适应人类偏好。通过对英文和芬兰语的训练数据和模型的评估结果进行了讨论，表明LLM能够遵循指令并具有跨文化能力。最佳实践是使用两种语言的偏好数据，并且仅通过数百个芬兰指令样本即可获得具有竞争力的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在聊天机器人和通用助理方面日益普及，开发人员为此开发出能使LLM遵循指令和适应人类偏好的方法。这些方法已经在高资源语言领域表现出其效果，但在其他语言领域尚未得到验证。</li>
<li>通过使用多语言LLM将指令和偏好数据集从英语翻译成芬兰语，实现了对英语的训练和优化后，对模型在芬兰语中的指令遵循能力进行了评估。结果显示少量芬兰语指令样本就能获得具有竞争力的性能。</li>
<li>虽然英语偏好优化提供了一些跨语言优势，但使用两种语言的偏好数据才能获得最佳结果。这表明LLM的性能可以通过适应多种语言的偏好数据来进一步提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09407">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5fb950bcae7e9ad0453b6d56242d2545.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf21e6eb2f8c60977cef3694ab742f58.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56cd64160ef969a430f18641e44a84ad.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Towards-Next-Generation-Recommender-Systems-A-Benchmark-for-Personalized-Recommendation-Assistant-with-LLMs"><a href="#Towards-Next-Generation-Recommender-Systems-A-Benchmark-for-Personalized-Recommendation-Assistant-with-LLMs" class="headerlink" title="Towards Next-Generation Recommender Systems: A Benchmark for   Personalized Recommendation Assistant with LLMs"></a>Towards Next-Generation Recommender Systems: A Benchmark for   Personalized Recommendation Assistant with LLMs</h2><p><strong>Authors:Jiani Huang, Shijie Wang, Liang-bo Ning, Wenqi Fan, Shuaiqiang Wang, Dawei Yin, Qing Li</strong></p>
<p>Recommender systems (RecSys) are widely used across various modern digital platforms and have garnered significant attention. Traditional recommender systems usually focus only on fixed and simple recommendation scenarios, making it difficult to generalize to new and unseen recommendation tasks in an interactive paradigm. Recently, the advancement of large language models (LLMs) has revolutionized the foundational architecture of RecSys, driving their evolution into more intelligent and interactive personalized recommendation assistants. However, most existing studies rely on fixed task-specific prompt templates to generate recommendations and evaluate the performance of personalized assistants, which limits the comprehensive assessments of their capabilities. This is because commonly used datasets lack high-quality textual user queries that reflect real-world recommendation scenarios, making them unsuitable for evaluating LLM-based personalized recommendation assistants. To address this gap, we introduce RecBench+, a new dataset benchmark designed to access LLMs’ ability to handle intricate user recommendation needs in the era of LLMs. RecBench+ encompasses a diverse set of queries that span both hard conditions and soft preferences, with varying difficulty levels. We evaluated commonly used LLMs on RecBench+ and uncovered below findings: 1) LLMs demonstrate preliminary abilities to act as recommendation assistants, 2) LLMs are better at handling queries with explicitly stated conditions, while facing challenges with queries that require reasoning or contain misleading information. Our dataset has been released at <a target="_blank" rel="noopener" href="https://github.com/jiani-huang/RecBench.git">https://github.com/jiani-huang/RecBench.git</a>. </p>
<blockquote>
<p>推荐系统（RecSys）在现代各种数字平台上得到广泛应用，并引起了广泛关注。传统的推荐系统通常只关注固定和简单的推荐场景，难以推广到交互式范式中的新和未见过的推荐任务。最近，大型语言模型（LLM）的进展彻底改变了RecSys的基础架构，推动其进化为更智能、更交互的个性化推荐助手。然而，大多数现有研究依赖于固定的任务特定提示模板来生成推荐并评估个性化助手的性能，这限制了对其能力的全面评估。这是因为常用的数据集缺乏反映真实世界推荐场景的高质量文本用户查询，因此不适合评估基于LLM的个性化推荐助手。为了弥补这一差距，我们推出了RecBench +，这是一个新的数据集基准，旨在评估LLM时代LLM处理复杂的用户推荐需求的能力。RecBench+包含涵盖硬条件和软偏好以及不同难度级别的各种查询。我们在RecBench+上对常用的LLM进行了评估，并发现了以下发现：1）LLM初步具备作为推荐助手的能力；2）LLM在处理明确陈述的条件查询时表现较好，而在需要推理或包含误导信息的查询时面临挑战。我们的数据集已在<a target="_blank" rel="noopener" href="https://github.com/jiani-huang/RecBench.git">https://github.com/jiani-huang/RecBench.git</a>发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09382v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文介绍了推荐系统（RecSys）在现代数字平台中的广泛应用，并指出传统推荐系统在新兴的互动型推荐任务上存在着局限性。随着大型语言模型（LLMs）的发展，推荐系统正朝着智能化和个性化方向发展。然而，现有的研究大多依赖于固定的任务特定提示模板来生成推荐并评估个性化助理的性能，这限制了对其能力的全面评估。为此，文章引入了一个新的数据集基准测试RecBench+，旨在评估LLM在复杂用户推荐需求方面的能力。RecBench+包含了不同难度层次的查询，既涵盖硬条件也涵盖软偏好。对常用的LLMs在RecBench+上的评估发现，LLMs有初步作为推荐助理的能力，在处理明确条件的查询时表现较好，但在需要推理或含有误导信息的查询时面临挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>推荐系统（RecSys）在现代数字平台中广泛应用，但传统系统在处理新兴互动型推荐任务时存在局限性。</li>
<li>大型语言模型（LLMs）的发展推动了推荐系统的智能化和个性化发展。</li>
<li>现有研究依赖固定任务特定提示模板来评估个性化助理的性能，这限制了全面评估。</li>
<li>引入新的数据集基准测试RecBench+，旨在评估LLM在复杂用户推荐需求上的能力。</li>
<li>RecBench+包含了涵盖硬条件和软偏好的不同难度层次的查询。</li>
<li>LLMs有初步作为推荐助理的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09382">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-06e649bfa90a2cd81bb0718c34a72f8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c26603d658c58a30948ff46a487f0e9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae8bcc24b3c5a371c039c96884fc8314.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bd6d55970ab07aade5c43081264cf9a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ade6c6a5f5bd8217e675584a55fd8cf6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RetSTA-An-LLM-Based-Approach-for-Standardizing-Clinical-Fundus-Image-Reports"><a href="#RetSTA-An-LLM-Based-Approach-for-Standardizing-Clinical-Fundus-Image-Reports" class="headerlink" title="RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image   Reports"></a>RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image   Reports</h2><p><strong>Authors:Jiushen Cai, Weihang Zhang, Hanruo Liu, Ningli Wang, Huiqi Li</strong></p>
<p>Standardization of clinical reports is crucial for improving the quality of healthcare and facilitating data integration. The lack of unified standards, including format, terminology, and style, is a great challenge in clinical fundus diagnostic reports, which increases the difficulty for large language models (LLMs) to understand the data. To address this, we construct a bilingual standard terminology, containing fundus clinical terms and commonly used descriptions in clinical diagnosis. Then, we establish two models, RetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented dataset simulating clinical scenarios, demonstrates powerful standardization behaviors. However, it encounters a challenge of limitation to cover a wider range of diseases. To further enhance standardization performance, we build RetSTA-7B, which integrates a substantial amount of standardized data generated by RetSTA-7B-Zero along with corresponding English data, covering diverse complex clinical scenarios and achieving report-level standardization for the first time. Experimental results demonstrate that RetSTA-7B outperforms other compared LLMs in bilingual standardization task, which validates its superior performance and generalizability. The checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/AB-Story/RetSTA-7B">https://github.com/AB-Story/RetSTA-7B</a>. </p>
<blockquote>
<p>临床报告的标准化对于提高医疗保健质量并促进数据整合至关重要。在临床眼底诊断报告中，由于缺乏统一的格式、术语和风格等标准，存在巨大的挑战。这给大型语言模型（LLM）理解数据增加了难度。为了解决这个问题，我们构建了一个双语标准术语集，包含眼底临床术语和临床诊断中常用的描述。接着，我们建立了两个模型，即RetSTA-7B-Zero和RetSTA-7B。RetSTA-7B-Zero在模拟临床场景的增强数据集上进行微调，表现出强大的标准化行为。然而，它在覆盖更广泛的疾病方面遇到了挑战。为了进一步提高标准化性能，我们构建了RetSTA-7B，它集成了由RetSTA-7B-Zero生成的标准化数据以及相应的英语数据，覆盖多种复杂的临床场景，首次实现了报告级别的标准化。实验结果表明，RetSTA-7B在双语标准化任务上优于其他比较的大型语言模型，验证了其卓越的性能和通用性。相关关键点代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/AB-Story/RetSTA-7B">https://github.com/AB-Story/RetSTA-7B</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09358v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>临床报告标准化对于提高医疗保健质量和促进数据整合至关重要。缺乏统一的标准，包括格式、术语和风格，是临床眼底诊断报告中的一个巨大挑战，这也增加了大型语言模型（LLM）理解数据的难度。为解决这一问题，我们构建了一个包含眼底临床术语和临床诊断中常用描述的双语标准术语。随后，我们建立了两个模型：RetSTA-7B-Zero和RetSTA-7B。RetSTA-7B-Zero在模拟临床场景的增强数据集上进行微调，表现出强大的标准化行为，但覆盖的疾病范围有限。为进一步增强标准化性能，我们构建了RetSTA-7B，集成了由RetSTA-7B-Zero生成的标准化数据及其对应的英文数据，覆盖多种复杂的临床场景，首次实现了报告级别的标准化。实验结果表明，RetSTA-7B在双语标准化任务上的表现优于其他对比的LLM，验证了其卓越的性能和泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>临床报告标准化对医疗保健质量的提高和数据整合的促进具有关键作用。</li>
<li>缺乏统一的标准，如格式、术语和风格，是临床眼底诊断报告的主要挑战之一。<br>3.双语标准术语的创建是解决这一挑战的重要步骤，包含了眼底临床术语和临床诊断中的常用描述。</li>
<li>RetSTA-7B-Zero模型展示了强大的标准化能力，但在疾病覆盖范围上有所限制。</li>
<li>RetSTA-7B模型通过集成大量标准化数据，扩展了疾病覆盖范围，并首次实现了报告级别的标准化。</li>
<li>实验结果证明RetSTA-7B在双语标准化任务上的表现优于其他LLM。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09358">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e9744f6858ab21d27f911948971731ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48db061a723beb80741db43f0ad90e31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fd4414cb2f9ca8bf51d665a33b799a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3dd7b61f1976e06fb3720f2530c7aaeb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Exo2Ego-Exocentric-Knowledge-Guided-MLLM-for-Egocentric-Video-Understanding"><a href="#Exo2Ego-Exocentric-Knowledge-Guided-MLLM-for-Egocentric-Video-Understanding" class="headerlink" title="Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video   Understanding"></a>Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video   Understanding</h2><p><strong>Authors:Haoyu Zhang, Qiaohui Chu, Meng Liu, Yunxiao Wang, Bin Wen, Fan Yang, Tingting Gao, Di Zhang, Yaowei Wang, Liqiang Nie</strong></p>
<p>AI personal assistants, deployed through robots or wearables, require embodied understanding to collaborate effectively with humans. Current Multimodal Large Language Models (MLLMs) primarily focus on third-person (exocentric) vision, overlooking the unique aspects of first-person (egocentric) videos. Additionally, high acquisition costs limit data size, impairing MLLM performance. To address these challenges, we propose learning the mapping between exocentric and egocentric domains, leveraging the extensive exocentric knowledge within existing MLLMs to enhance egocentric video understanding. To this end, we introduce Ego-ExoClip, a pre-training dataset comprising 1.1M synchronized ego-exo clip-text pairs derived from Ego-Exo4D. Our approach features a progressive training pipeline with three stages: Teacher Self-Preparation, Teacher-Student Guidance, and Student Self-Practice. Additionally, we propose an instruction-tuning data EgoIT from multiple sources to strengthen the model’s instruction-following capabilities, along with the EgoBench benchmark comprising eight different tasks for thorough evaluation. Extensive experiments across diverse egocentric tasks reveal that existing MLLMs perform inadequately in egocentric video understanding, while our model significantly outperforms these leading models. </p>
<blockquote>
<p>通过机器人或可穿戴设备部署的人工智能个人助理，需要实体理解才能与人类有效协作。当前的多模态大型语言模型（MLLM）主要关注第三人称（外视）视角，忽略了第一人称（内视）视频的独特方面。此外，高昂的采集成本限制了数据量，影响了MLLM的性能。为了应对这些挑战，我们提出了学习外视和内视领域之间的映射关系，利用现有MLLM中的大量外视知识来增强对内视视频的理解。为此，我们引入了Ego-ExoClip预训练数据集，包含从Ego-Exo4D派生的110万同步的ego-exo剪辑文本对。我们的方法采用分阶段训练管道，包括三个阶段：教师自我准备、教师学生指导和学生自我实践。此外，我们从多个来源提出了用于加强模型指令跟随能力的指令调整数据EgoIT，以及包含八个不同任务的EgoBench基准测试集进行全面评估。在多种第一人称任务的广泛实验表明，现有的MLLM在内视视频理解方面表现不佳，而我们的模型显著优于这些领先模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09143v1">PDF</a> Project: <a target="_blank" rel="noopener" href="https://egovisiongroup.github.io/Exo2Ego.github.io/">https://egovisiongroup.github.io/Exo2Ego.github.io/</a></p>
<p><strong>Summary</strong><br>     机器人或可穿戴设备上的AI个人助理需要身体感知来与人类有效协作。当前的多模态大型语言模型（MLLM）主要关注第三人称视角（外部视角）的视野，忽略了第一人称视角（内部视角）的独特性。此外，高昂的采集成本限制了数据量，影响了MLLM的性能。为解决这些挑战，本文提出了学习外部视角和内部视角领域之间的映射关系的方法，利用现有的MLLM中的外部视角知识增强内部视角视频的理解能力。为此，引入了Ego-ExoClip预训练数据集，包含从Ego-Exo4D派生的110万同步自我外部剪辑文本对。我们的方法采用分阶段训练管道，包括教师自我准备、教师学生指导和学生自我实践三个阶段。此外，还提出了来自多个来源的指令调整数据EgoIT，以加强模型的指令执行能力，以及包含八个不同任务的EgoBench基准测试用于全面评估。大量实验表明，现有MLLM在内部视角视频理解方面表现不足，而我们的模型显著优于这些领先模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI个人助理需要身体感知来与人类协作。</li>
<li>当前MLLM主要关注第三人称视角（外部视角），忽略第一人称视角（内部视角）。</li>
<li>数据采集成本高限制了MLLM的性能提升。</li>
<li>提出学习外部视角和内部视角领域映射的方法。</li>
<li>引入Ego-ExoClip预训练数据集用于增强内部视角视频理解能力。</li>
<li>采用分阶段训练管道来提升模型性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09143">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e78efcd71dd3c20e4361915764922d23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b45e083b7d2925bed0ae315ac739666.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-258d6194034dd56c81793faef1b14ce3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-188e58234f29947ef48d8b4b7baa6afd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32f843066099c1f6d7714658dc8a1b73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c641c62c8bdb0d0ad13ebbfc960a574.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Teaching-LLMs-How-to-Learn-with-Contextual-Fine-Tuning"><a href="#Teaching-LLMs-How-to-Learn-with-Contextual-Fine-Tuning" class="headerlink" title="Teaching LLMs How to Learn with Contextual Fine-Tuning"></a>Teaching LLMs How to Learn with Contextual Fine-Tuning</h2><p><strong>Authors:Younwoo Choi, Muhammad Adil Asif, Ziwen Han, John Willes, Rahul G. Krishnan</strong></p>
<p>Prompting Large Language Models (LLMs), or providing context on the expected model of operation, is an effective way to steer the outputs of such models to satisfy human desiderata after they have been trained. But in rapidly evolving domains, there is often need to fine-tune LLMs to improve either the kind of knowledge in their memory or their abilities to perform open ended reasoning in new domains. When human’s learn new concepts, we often do so by linking the new material that we are studying to concepts we have already learned before. To that end, we ask, “can prompting help us teach LLMs how to learn”. In this work, we study a novel generalization of instruction tuning, called contextual fine-tuning, to fine-tune LLMs. Our method leverages instructional prompts designed to mimic human cognitive strategies in learning and problem-solving to guide the learning process during training, aiming to improve the model’s interpretation and understanding of domain-specific knowledge. We empirically demonstrate that this simple yet effective modification improves the ability of LLMs to be fine-tuned rapidly on new datasets both within the medical and financial domains. </p>
<blockquote>
<p>提示大型语言模型（LLM）或提供预期的模型操作上下文，是引导此类模型的输出以满足人类需求的有效方法，这些模型在训练后。但在快速发展的领域中，通常需要微调LLM，以改善其内存中的知识类型或在新的领域进行开放式推理的能力。当人类学习新概念时，我们常常通过把我们正在研究的新材料与之前已经学过的概念联系起来来学习。为此，我们提出的问题是：“提示能否帮助我们教会LLM如何学习”。在这项工作中，我们研究了一种指令微调的新泛化方法，称为上下文微调，以微调LLM。我们的方法利用指令提示来模仿人类在学习和解决问题中的认知策略，以指导训练过程中的学习过程，旨在提高模型对特定领域知识的解释和理解能力。我们从实证上证明，这种简单而有效的改进提高了LLM在新数据集上的快速微调能力，无论是在医疗领域还是金融领域。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09032v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong><br>     通过对大型语言模型（LLMs）进行提示或提供操作预期模型上下文，可以有效引导模型训练后的输出以满足人类的需求。然而，在快速发展的领域中，往往需要微调LLM，以提高其记忆中的知识或在新领域进行开放式推理的能力。本文研究了一种名为“上下文微调”的新型指令微调方法，通过设计模仿人类学习和问题解决认知策略的指令提示来指导学习过程，旨在提高模型对特定领域知识的理解和解释能力。实验证明，这种简单而有效的方法能够改进LLM在新数据集上的快速微调能力，尤其是在医疗和金融领域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>通过提示大型语言模型（LLMs）可有效引导模型输出以满足人类需求。</li>
<li>在快速发展的领域中，需要微调LLM以适应新知识和新领域。</li>
<li>上下文微调是一种新型的指令微调方法，通过设计模仿人类学习和问题解决认知策略的指令提示来指导学习过程。</li>
<li>上下文微调方法旨在提高模型对特定领域知识的理解和解释能力。</li>
<li>上下文微调能够改进LLM在新数据集上的快速微调能力。</li>
<li>这种方法在医疗和金融领域取得了实证效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09032">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0432d32251c9933395c3001794c693da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfe698d678a658ff6c1fc5568dfebab1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="OpenRAG-Optimizing-RAG-End-to-End-via-In-Context-Retrieval-Learning"><a href="#OpenRAG-Optimizing-RAG-End-to-End-via-In-Context-Retrieval-Learning" class="headerlink" title="OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning"></a>OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning</h2><p><strong>Authors:Jiawei Zhou, Lei Chen</strong></p>
<p>In this paper, we analyze and empirically show that the learned relevance for conventional information retrieval (IR) scenarios may be inconsistent in retrieval-augmented generation (RAG) scenarios. To bridge this gap, we introduce OpenRAG, a RAG framework that is optimized end-to-end by tuning the retriever to capture in-context relevance, enabling adaptation to the diverse and evolving needs. Extensive experiments across a wide range of tasks demonstrate that OpenRAG, by tuning a retriever end-to-end, leads to a consistent improvement of 4.0% over the original retriever, consistently outperforming existing state-of-the-art retrievers by 2.1%. Additionally, our results indicate that for some tasks, an end-to-end tuned 0.2B retriever can achieve improvements that surpass those of RAG-oriented or instruction-tuned 8B large language models (LLMs), highlighting the cost-effectiveness of our approach in enhancing RAG systems. </p>
<blockquote>
<p>本文分析和实证表明，传统信息检索（IR）场景中学习的相关性可能在检索增强生成（RAG）场景中不一致。为了弥合这一差距，我们引入了OpenRAG，这是一个通过调整检索器来捕获上下文相关性的端到端优化的RAG框架，能够适应多样且不断发展的需求。在多个任务上的广泛实验表明，通过端到端调整检索器，OpenRAG在原始检索器的基础上实现了4.0%的持续改进，并且始终优于现有的最先进的检索器，领先了2.1%。此外，我们的结果表明，对于某些任务，端到端调整的0.2B检索器的改进甚至超过了面向RAG或指令调整的8B大型语言模型（LLM），这突显了我们方法在增强RAG系统中的成本效益。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08398v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出并分析传统的信息检索场景在检索增强生成（RAG）场景中可能存在的不一致性。为了弥补这一差距，引入了OpenRAG框架，通过端到端优化检索器来捕捉上下文相关性，适应多样化和不断发展的需求。实验证明，OpenRAG相较于原始检索器有4.0%的一致性提升，相较于现有的先进检索器有2.1%的提升。此外，对于某些任务，端到端优化的较小规模检索器的改进甚至超过了面向RAG或指令优化的超大规模语言模型（LLM），突显了增强RAG系统成本效益的优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统的信息检索场景在检索增强生成（RAG）场景中可能存在不一致性。</li>
<li>OpenRAG是一个优化的RAG框架，旨在通过端到端优化检索器来解决上述问题。</li>
<li>OpenRAG框架通过捕捉上下文相关性，适应了多样化和不断发展的需求。</li>
<li>实验结果显示，OpenRAG相较于原始检索器有显著提升。</li>
<li>OpenRAG相较于现有先进检索器也有显著优势。</li>
<li>对于某些任务，优化后的较小规模检索器的改进甚至超过了大型语言模型（LLM）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08398">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bb1dbdeb1d58056449ebd2365b699873.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67e8cfb6c5d7010ac7947608c38d6c7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6919a1c27f050afa0f6440140af692dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-966b4673ab7573d37754621ce0d3acf0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GPT-PPG-A-GPT-based-Foundation-Model-for-Photoplethysmography-Signals"><a href="#GPT-PPG-A-GPT-based-Foundation-Model-for-Photoplethysmography-Signals" class="headerlink" title="GPT-PPG: A GPT-based Foundation Model for Photoplethysmography Signals"></a>GPT-PPG: A GPT-based Foundation Model for Photoplethysmography Signals</h2><p><strong>Authors:Zhaoliang Chen, Cheng Ding, Saurabh Kataria, Runze Yan, Minxiao Wang, Randall Lee, Xiao Hu</strong></p>
<p>This study introduces a novel application of a Generative Pre-trained Transformer (GPT) model tailored for photoplethysmography (PPG) signals, serving as a foundation model for various downstream tasks. Adapting the standard GPT architecture to suit the continuous characteristics of PPG signals, our approach demonstrates promising results. Our models are pre-trained on our extensive dataset that contains more than 200 million 30s PPG samples. We explored different supervised fine-tuning techniques to adapt our model to downstream tasks, resulting in performance comparable to or surpassing current state-of-the-art (SOTA) methods in tasks like atrial fibrillation detection. A standout feature of our GPT model is its inherent capability to perform generative tasks such as signal denoising effectively, without the need for further fine-tuning. This success is attributed to the generative nature of the GPT framework. </p>
<blockquote>
<p>本研究介绍了一种针对光体积描记法（PPG）信号的新型应用，即生成式预训练转换器（GPT）模型，该模型为各种下游任务提供了基础模型。通过调整标准GPT架构以适应PPG信号的连续特性，我们的方法显示出有前景的结果。我们的模型在包含超过2000万份30秒PPG样本的庞大数据集上进行预训练。我们探索了不同的监督微调技术，以使我们的模型适应下游任务，从而在诸如心房颤动检测等任务的性能上达到或超越了当前最新技术水平。我们的GPT模型的一个突出特点是，它具有执行生成任务（如信号去噪）的固有能力，并且不需要进一步的微调。这一成功归因于GPT框架的生成性质。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08015v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究介绍了一种针对光体积脉搏波信号（PPG）的新型生成预训练转换器（GPT）模型的应用，该模型可作为各种下游任务的基础模型。通过调整标准GPT架构以适应PPG信号的连续特性，该研究展现出令人鼓舞的结果。该模型在包含超过20亿个30秒PPG样本的大规模数据集上进行预训练。研究还探索了不同的监督微调技术，使模型能够适应下游任务，在房颤检测等任务中的性能达到或超越了当前最先进的水平。GPT模型的一个突出特点是其固有的生成任务能力，如信号去噪，无需进一步微调。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究人员成功将生成预训练转换器（GPT）模型应用于光体积脉搏波信号（PPG）。</li>
<li>GPT模型经过微调以适应下游任务，如房颤检测，性能卓越。</li>
<li>模型在大型数据集上进行预训练，包含超过20亿个PPG样本。</li>
<li>GPT模型的生成性质使其能够执行生成任务，如信号去噪。</li>
<li>此研究为多种下游任务提供了基础模型。</li>
<li>模型连续适应性强，能够适应PPG信号的连续特性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08015">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2443f5f69ced566e7b2344260a6fae41.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Fully-Autonomous-Programming-using-Iterative-Multi-Agent-Debugging-with-Large-Language-Models"><a href="#Fully-Autonomous-Programming-using-Iterative-Multi-Agent-Debugging-with-Large-Language-Models" class="headerlink" title="Fully Autonomous Programming using Iterative Multi-Agent Debugging with   Large Language Models"></a>Fully Autonomous Programming using Iterative Multi-Agent Debugging with   Large Language Models</h2><p><strong>Authors:Anastasiia Grishina, Vadim Liventsev, Aki Härmä, Leon Moonen</strong></p>
<p>Program synthesis with Large Language Models (LLMs) suffers from a “near-miss syndrome”: the generated code closely resembles a correct solution but fails unit tests due to minor errors. We address this with a multi-agent framework called Synthesize, Execute, Instruct, Debug, and Repair (SEIDR). Effectively applying SEIDR to instruction-tuned LLMs requires determining (a) optimal prompts for LLMs, (b) what ranking algorithm selects the best programs in debugging rounds, and (c) balancing the repair of unsuccessful programs with the generation of new ones. We empirically explore these trade-offs by comparing replace-focused, repair-focused, and hybrid debug strategies. We also evaluate lexicase and tournament selection to rank candidates in each generation. On Program Synthesis Benchmark 2 (PSB2), our framework outperforms both conventional use of OpenAI Codex without a repair phase and traditional genetic programming approaches. SEIDR outperforms the use of an LLM alone, solving 18 problems in C++ and 20 in Python on PSB2 at least once across experiments. To assess generalizability, we employ GPT-3.5 and Llama 3 on the PSB2 and HumanEval-X benchmarks. Although SEIDR with these models does not surpass current state-of-the-art methods on the Python benchmarks, the results on HumanEval-C++ are promising. SEIDR with Llama 3-8B achieves an average pass@100 of 84.2%. Across all SEIDR runs, 163 of 164 problems are solved at least once with GPT-3.5 in HumanEval-C++, and 162 of 164 with the smaller Llama 3-8B. We conclude that SEIDR effectively overcomes the near-miss syndrome in program synthesis with LLMs. </p>
<blockquote>
<p>使用大型语言模型（LLM）进行程序合成时，存在一种“近似综合征”：生成的代码与正确解决方案非常相似，但由于微小错误而无法通过单元测试。我们通过一个名为“合成、执行、指令、调试和修复（SEIDR）”的多智能体框架来解决这个问题。将SEIDR有效应用于指令调整LLM，需要确定（a）针对LLM的最佳提示，（b）在调试轮中选择最佳程序的排名算法，以及（c）平衡不成功程序的修复和新程序的生成。我们通过比较以替换为重点、以修复为重点以及混合调试策略，实证地探索了这些权衡。我们还评估了词典选择法和锦标赛选拔法以在每一代中挑选候选者。在程序合成基准测试第二版（PSB2）上，我们的框架优于常规使用OpenAI Codex而不进行修复阶段以及传统的遗传编程方法。SEIDR在单独使用LLM时表现出色，在PSB2的C++和Python中分别解决了至少一次实验的18个问题和20个问题。为了评估通用性，我们在PSB2和HumanEval-X基准测试上使用了GPT-3.5和Llama 3。尽管使用这些模型的SEIDR在Python基准测试上并未超越当前的最先进方法，但在HumanEval-C++上的结果具有希望。使用Llama 3-8B的SEIDR达到平均通过率为84.2%。在所有SEIDR运行中，GPT-3.5在HumanEval-C++中至少解决了一次以上其中的所有问题中的至少解决过有答案的前置题（pass@100）有十六分之三（即至少解决了十六题中的十六题），较小的Llama 3-8B解决了一六二题中的十六题以上。我们得出结论，SEIDR有效地克服了使用LLM进行程序合成时的近失误问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07693v1">PDF</a> Accepted for publication in ACM Trans. Evol. Learn. Optim., February   2025. arXiv admin note: text overlap with arXiv:2304.10423</p>
<p><strong>摘要</strong></p>
<p>基于大型语言模型（LLM）的程序合成受到“近似综合征”的影响，即生成的代码虽然与正确解决方案相似，但由于细微错误而无法通过单元测试。为解决这一问题，我们提出了一个名为SEIDR（合成、执行、指令、调试和修复）的多智能体框架。有效应用SEIDR到指令调整LLM需要确定（a）LLM的最佳提示，（b）在调试轮次中选择最佳程序的排名算法，以及（c）平衡不成功程序的修复与新生成的程序之间的平衡。我们通过比较注重替换、注重修复和混合调试策略来实证探索这些权衡。我们还评估了词典选择和锦标赛选择来排列每一代的候选人。在程序合成基准测试2（PSB2）上，我们的框架优于常规使用OpenAI Codex而不进行修复阶段以及传统的遗传编程方法。SEIDR优于单独使用LLM，在PSB2上解决C++的18个问题和Python的20个问题。为了评估通用性，我们在PSB2和HumanEval-X基准测试上使用了GPT-3.5和Llama 3。虽然SEIDR与这些模型的结合在Python基准测试上并未超越当前最先进的方法，但在HumanEval-C++上的结果很有希望。使用Llama 3-8B的SEIDR达到平均通过率为84.2%。在所有SEIDR运行中，GPT-3.5在HumanEval-C++中至少解决了一次的163个问题和较小的Llama 3-8B解决的至少一次的162个问题。我们得出结论，SEIDR有效地克服了基于LLM的程序合成的近似综合征问题。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>SEIDR框架解决了基于大型语言模型（LLM）的程序合成中的“近似综合征”问题，即通过生成的代码虽近似正确解但无法通过单元测试的问题。</li>
<li>SEIDR应用的关键要素包括确定最佳提示、选择排名算法的调试轮次中最佳程序的策略，以及平衡修复与生成新程序之间的策略。</li>
<li>通过比较不同的调试策略（如注重替换、注重修复和混合策略），实证探索了这些权衡。</li>
<li>SEIDR在程序合成基准测试上的表现优于常规方法和遗传编程方法。</li>
<li>SEIDR在不同的大型语言模型（如GPT-3.5和Llama 3）上的表现显示出良好的通用性，尤其在HumanEval-C++上的结果有潜力。</li>
<li>SEIDR使用Llama 3-8B在HumanEval-C++的基准测试中达到的平均通过率为84.2%，显示出其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07693">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-85cce17d57fddf1b38507efc2310aed6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6231532fff3ab088e70108844ee4b47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-931da4bdc3eea788ca701cea0bd2eba6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73cf226bab88f258c76865a38dd20540.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Implicit-Reasoning-in-Transformers-is-Reasoning-through-Shortcuts"><a href="#Implicit-Reasoning-in-Transformers-is-Reasoning-through-Shortcuts" class="headerlink" title="Implicit Reasoning in Transformers is Reasoning through Shortcuts"></a>Implicit Reasoning in Transformers is Reasoning through Shortcuts</h2><p><strong>Authors:Tianhe Lin, Jian Xie, Siyu Yuan, Deqing Yang</strong></p>
<p>Test-time compute is emerging as a new paradigm for enhancing language models’ complex multi-step reasoning capabilities, as demonstrated by the success of OpenAI’s o1 and o3, as well as DeepSeek’s R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on unfixed-pattern data tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization. </p>
<blockquote>
<p>测试时的计算正成为一种新兴的模式，用于增强语言模型的复杂多步推理能力，OpenAI的o1和o3以及DeepSeek的R1的成功演示了这一点。与测试时计算中的显式推理相比，隐式推理的推理效率更高，生成的标记更少。然而，为什么先进的推理能力无法在隐式推理风格中产生呢？在这项工作中，我们从零开始训练GPT-2，使其适应精选的多步数学推理数据集，并进行分析实验，以研究语言模型在多步任务中进行隐式推理的表现。我们的研究发现：1）语言模型可以通过隐式推理进行逐步推理，并在域内和域外测试中实现高准确性。但这种能力仅在训练固定模式数据时出现。2）相反，从训练非固定模式数据中出现的隐式推理能力往往倾向于过度适应特定模式，而无法进一步推广。值得注意的是，这一局限性也被观察到存在于最先进的自然语言模型中。这些发现表明，语言模型通过捷径学习获得隐式推理能力，能够在具有相似模式的任务上表现出强大的性能，但却缺乏泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07604v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了测试时计算（test-time compute）这一新兴范式在增强语言模型复杂多步推理能力方面的应用。通过对GPT-2进行训练和实验分析，研究发现在固定模式数据上训练的语言模型可以通过隐式推理实现逐步推理，并在域内和域外测试中获得高准确性。然而，在非标定模式数据上训练的模型往往会出现过度拟合特定模式的情况，缺乏进一步的泛化能力。这表明语言模型的隐式推理能力是通过捷径学习获得的，能够在具有相似模式的任务上表现出色，但缺乏泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>测试时计算是一种新兴范式，用于增强语言模型的复杂多步推理能力。</li>
<li>隐式推理相比显式推理更推理高效，需要生成的标记更少。</li>
<li>语言模型在固定模式数据上训练后，可以通过隐式推理进行逐步推理，并在测试中获得高准确性。</li>
<li>在非标定模式数据上训练的模型会出现过度拟合特定模式的情况，缺乏泛化能力。</li>
<li>隐式推理能力是通过捷径学习获得的，能够在具有相似模式的任务上表现出色。</li>
<li>在不同领域（如数学）的隐式推理研究中发现了类似的语言模型限制和局限。这种泛化能力不足的问题可能影响语言模型在其他复杂任务上的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07604">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7321e2424716906cd8925eff5706f8ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f18e45a5bc348f7a33db54aec4d1a127.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2da07d60a628e76629c6cb199ab2ac39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36ef7753475442622205a6d532211666.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da4ae7ec347e475a6ab350797ce3cbef.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Filter-Images-First-Generate-Instructions-Later-Pre-Instruction-Data-Selection-for-Visual-Instruction-Tuning"><a href="#Filter-Images-First-Generate-Instructions-Later-Pre-Instruction-Data-Selection-for-Visual-Instruction-Tuning" class="headerlink" title="Filter Images First, Generate Instructions Later: Pre-Instruction Data   Selection for Visual Instruction Tuning"></a>Filter Images First, Generate Instructions Later: Pre-Instruction Data   Selection for Visual Instruction Tuning</h2><p><strong>Authors:Bardia Safaei, Faizan Siddiqui, Jiacong Xu, Vishal M. Patel, Shao-Yuan Lo</strong></p>
<p>Visual instruction tuning (VIT) for large vision-language models (LVLMs) requires training on expansive datasets of image-instruction pairs, which can be costly. Recent efforts in VIT data selection aim to select a small subset of high-quality image-instruction pairs, reducing VIT runtime while maintaining performance comparable to full-scale training. However, a major challenge often overlooked is that generating instructions from unlabeled images for VIT is highly expensive. Most existing VIT datasets rely heavily on human annotations or paid services like the GPT API, which limits users with constrained resources from creating VIT datasets for custom applications. To address this, we introduce Pre-Instruction Data Selection (PreSel), a more practical data selection paradigm that directly selects the most beneficial unlabeled images and generates instructions only for the selected images. PreSel first estimates the relative importance of each vision task within VIT datasets to derive task-wise sampling budgets. It then clusters image features within each task, selecting the most representative images with the budget. This approach reduces computational overhead for both instruction generation during VIT data formation and LVLM fine-tuning. By generating instructions for only 15% of the images, PreSel achieves performance comparable to full-data VIT on the LLaVA-1.5 and Vision-Flan datasets. The link to our project page: <a target="_blank" rel="noopener" href="https://bardisafa.github.io/PreSel">https://bardisafa.github.io/PreSel</a> </p>
<blockquote>
<p>视觉指令调优（VIT）对于大型视觉语言模型（LVLM）需要在大量的图像指令对数据集上进行训练，这可能会很昂贵。近期关于VIT数据选择的努力旨在选择一小部分高质量的图像指令对，在减少VIT运行时间的同时保持与全规模训练相当的性能。然而，经常被忽视的一个主要挑战是，从未标记的图像中生成指令的成本非常高。大多数现有的VIT数据集严重依赖于人工注释或如GPT API之类的付费服务，这限制了资源有限的用户创建用于自定义应用程序的VIT数据集。为了解决这一问题，我们引入了预指令数据选择（PreSel），这是一种更实用的数据选择模式，它直接选择最有益的无标签图像，只为所选图像生成指令。PreSel首先估计VIT数据集中每个视觉任务的相对重要性，以得出任务级采样预算。然后，它在每个任务内对图像特征进行聚类，选择最具代表性的图像以符合预算。这种方法减少了VIT数据形成和LVLM微调过程中的指令生成计算开销。只为15%的图像生成指令，PreSel在LLaVA-1.5和Vision-Flan数据集上的性能与全数据VIT相当。我们的项目页面链接：<a target="_blank" rel="noopener" href="https://bardisafa.github.io/PreSel">https://bardisafa.github.io/PreSel</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07591v1">PDF</a> Accepted at Computer Vision and Pattern Recognition Conference (CVPR)   2025</p>
<p><strong>摘要</strong></p>
<p>视觉指令微调（VIT）需要大量图像-指令对数据集对大型视觉语言模型（LVLMs）进行训练，成本较高。最新研究致力于选择高质量的图像-指令对数据子集，以在保持性能的同时减少VIT的运行时间。然而，经常被忽视的一个挑战是从无标签图像中生成指令的成本高昂。大多数现有的VIT数据集严重依赖于人工注释或有偿服务（如GPT API），这限制了资源受限的用户创建自定义应用的VIT数据集。为解决此问题，我们引入了预指令数据选择（PreSel）这一更实际的数据选择范式，该范式直接选择最有益的无标签图像，只为所选图像生成指令。PreSel首先估计VIT数据集中每个视觉任务的重要性，以导出任务特定的采样预算。然后它在每个任务中对图像特征进行聚类，选择最具代表性的图像来使用预算。这种方法减少了在形成VIT数据和微调LVLM期间的指令生成计算开销。只为图像的15%生成指令，PreSel在LLaVA-1.5和Vision-Flan数据集上的性能与全数据VIT相当。项目链接：<a target="_blank" rel="noopener" href="https://bardisafa.github.io/PreSel%E3%80%82">https://bardisafa.github.io/PreSel。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>VIT需要大量的图像-指令对数据集进行训练，成本高昂。</li>
<li>最新研究致力于选择高质量的图像-指令对数据子集，以减少运行时间并保持性能。</li>
<li>生成指令的成本是从无标签图像中生成指令的一个挑战。</li>
<li>大多数现有方法高度依赖于人工注释或有偿服务，限制了资源受限用户的自定义应用。</li>
<li>PreSel直接选择最有益的无标签图像，并为所选图像生成指令，降低计算开销。</li>
<li>PreSel通过估计每个视觉任务的重要性来导出任务特定的采样预算，选择最具代表性的图像。</li>
<li>仅对少量图像生成指令，PreSel的性能与全数据VIT相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07591">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fa170626bf00ee708fa1e6a942043708.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93ec5db65b826b27d696e3a99ffda4de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e265abccdd703087bfb59e5a7564ce27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f68ee10db0da8de33097a4f4f36d45f7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="GRITHopper-Decomposition-Free-Multi-Hop-Dense-Retrieval"><a href="#GRITHopper-Decomposition-Free-Multi-Hop-Dense-Retrieval" class="headerlink" title="GRITHopper: Decomposition-Free Multi-Hop Dense Retrieval"></a>GRITHopper: Decomposition-Free Multi-Hop Dense Retrieval</h2><p><strong>Authors:Justus-Jonas Erker, Nils Reimers, Iryna Gurevych</strong></p>
<p>Decomposition-based multi-hop retrieval methods rely on many autoregressive steps to break down complex queries, which breaks end-to-end differentiability and is computationally expensive. Decomposition-free methods tackle this, but current decomposition-free approaches struggle with longer multi-hop problems and generalization to out-of-distribution data. To address these challenges, we introduce GRITHopper-7B, a novel multi-hop dense retrieval model that achieves state-of-the-art performance on both in-distribution and out-of-distribution benchmarks. GRITHopper combines generative and representational instruction tuning by integrating causal language modeling with dense retrieval training. Through controlled studies, we find that incorporating additional context after the retrieval process, referred to as post-retrieval language modeling, enhances dense retrieval performance. By including elements such as final answers during training, the model learns to better contextualize and retrieve relevant information. GRITHopper-7B offers a robust, scalable, and generalizable solution for multi-hop dense retrieval, and we release it to the community for future research and applications requiring multi-hop reasoning and retrieval capabilities. </p>
<blockquote>
<p>基于分解的多跳检索方法依赖于许多自回归步骤来分解复杂查询，这破坏了端到端的可区分性，并且计算成本高昂。无分解的方法解决了这个问题，但当前的无分解方法在处理较长的多跳问题和泛化到分布外数据时遇到困难。为了解决这些挑战，我们引入了GRITHopper-7B，这是一种新型的多跳密集检索模型，在分布内和分布外的基准测试中均实现了最新性能。GRITHopper通过结合因果语言建模和密集检索训练，实现了生成和代表性指令调整。通过对照研究，我们发现，在检索过程后融入额外的上下文，即所谓的后检索语言建模，可以增强密集检索的性能。通过在训练中加入最终答案等元素，模型学会了更好地上下文化和检索相关信息。GRITHopper-7B为多跳密集检索提供了稳健、可扩展和通用的解决方案，我们将其发布给社区，以供未来研究和需要多跳推理和检索能力的应用使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07519v1">PDF</a> Under Review at ACL Rolling Review (ARR)</p>
<p><strong>Summary</strong></p>
<p>基于分解的多跳检索方法依赖多个自回归步骤来分解复杂查询，这破坏了端到端的可区分性，并且计算成本高昂。无分解方法解决了这一问题，但当前的无分解方法对于较长的多跳问题和泛化到离群数据方面存在挑战。为解决这些挑战，我们推出了GRITHopper-7B，一种新型多跳密集检索模型，在内外分布基准测试上均实现了卓越性能。GRITHopper通过整合因果语言建模与密集检索训练，实现了生成性和代表性指令调整。通过对照研究，我们发现，在检索过程后增加额外的上下文，即所谓的后检索语言建模，可以增强密集检索的性能。通过在训练中包含最终答案等元素，模型学会了更好地上下文化和检索相关信息。GRITHopper-7B为需要多跳推理和检索能力的未来研究与应用提供了稳健、可扩展和通用的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基于分解的多跳检索方法存在端到端不可区分性和计算成本高昂的问题。</li>
<li>无分解方法解决了这一问题，但在处理较长多跳问题和泛化到离群数据方面存在挑战。</li>
<li>GRITHopper-7B是一个新型多跳密集检索模型，具有强大的性能和广泛的应用前景。</li>
<li>GRITHopper结合了生成性和代表性指令调整，通过整合因果语言建模与密集检索训练实现高性能。</li>
<li>后检索语言建模能增强密集检索性能。</li>
<li>在训练中加入最终答案等元素使模型能更精准地上下文化和检索相关信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07519">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b3c078aa23bc59c7f06f491a7f07de8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e8b1055296fbc479baf67d437260c02.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5fc485531a1ecfaa51ecf8caab999f1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CalliReader-Contextualizing-Chinese-Calligraphy-via-an-Embedding-Aligned-Vision-Language-Model"><a href="#CalliReader-Contextualizing-Chinese-Calligraphy-via-an-Embedding-Aligned-Vision-Language-Model" class="headerlink" title="CalliReader: Contextualizing Chinese Calligraphy via an   Embedding-Aligned Vision-Language Model"></a>CalliReader: Contextualizing Chinese Calligraphy via an   Embedding-Aligned Vision-Language Model</h2><p><strong>Authors:Yuxuan Luo, Jiaqi Tang, Chenyi Huang, Feiyang Hao, Zhouhui Lian</strong></p>
<p>Chinese calligraphy, a UNESCO Heritage, remains computationally challenging due to visual ambiguity and cultural complexity. Existing AI systems fail to contextualize their intricate scripts, because of limited annotated data and poor visual-semantic alignment. We propose CalliReader, a vision-language model (VLM) that solves the Chinese Calligraphy Contextualization (CC$^2$) problem through three innovations: (1) character-wise slicing for precise character extraction and sorting, (2) CalliAlign for visual-text token compression and alignment, (3) embedding instruction tuning (e-IT) for improving alignment and addressing data scarcity. We also build CalliBench, the first benchmark for full-page calligraphic contextualization, addressing three critical issues in previous OCR and VQA approaches: fragmented context, shallow reasoning, and hallucination. Extensive experiments including user studies have been conducted to verify our CalliReader’s \textbf{superiority to other state-of-the-art methods and even human professionals in page-level calligraphy recognition and interpretation}, achieving higher accuracy while reducing hallucination. Comparisons with reasoning models highlight the importance of accurate recognition as a prerequisite for reliable comprehension. Quantitative analyses validate CalliReader’s efficiency; evaluations on document and real-world benchmarks confirm its robust generalization ability. </p>
<blockquote>
<p>中国书法作为联合国教科文组织（UNESCO）遗产，仍然具有计算上的挑战性，原因在于视觉模糊和文化复杂性。现有的AI系统无法对其复杂的脚本进行语境化理解，这主要是因为标注数据有限以及视觉语义对齐不佳。我们提出了CalliReader，这是一种视觉语言模型（VLM），它通过三项创新解决了中文书法的语境化问题（CC$^2$）：（1）字符级切片，用于精确字符提取和排序；（2）CalliAlign用于视觉文本符号压缩和对齐；（3）嵌入指令调整（e-IT）用于改善对齐并解决数据稀缺问题。我们还建立了CalliBench，这是第一个全页书法语境化的基准测试，解决了之前OCR和VQA方法中的三个关键问题：上下文碎片化、推理浅显和幻觉。进行了大量实验和用户研究，验证了CalliReader在页级书法识别和解释方面优于其他最先进的方法，甚至优于专业书法家，在提高准确性的同时减少了幻觉。与推理模型的比较突显了准确识别作为可靠理解先决条件的重要性。定量分析验证了CalliReader的效率；对文档和真实世界的基准测试评估证明了其稳健的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06472v2">PDF</a> 11 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了中文书法作为联合国教科文组织遗产所面临的计算挑战，包括视觉模糊和文化复杂性。现有的AI系统因缺乏注释数据和视觉语义对齐不良而无法适应复杂的脚本上下文。提出了一种新型的视觉语言模型CalliReader，它通过三个创新解决了中文书法上下文化问题：字符级切片技术用于精确字符提取和排序，CalliAlign用于视觉文本令牌压缩和对齐，嵌入指令调整e-IT用于改进对齐并解决数据稀缺问题。此外建立了首个全页书法上下文基准测试CalliBench，解决了以往OCR和VQA方法中的三个关键问题：上下文碎片化、推理浅显和幻觉现象。通过广泛的实验和用户研究验证了CalliReader在页级书法识别和解释方面优于其他先进方法和专业人士，在保持高准确性的同时降低了幻觉现象。与推理模型的比较突显了准确识别作为可靠理解先决条件的重要性。定量分析验证了CalliReader的效率，对文档和真实世界的基准测试评估证明了其稳健的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>中文书法作为联合国教科文组织遗产，面临视觉模糊和文化复杂性的计算挑战。</li>
<li>现有AI系统在处理中文书法时存在困难，主要是因为缺乏注释数据和视觉语义对齐不良。</li>
<li>CalliReader是一种新型的视觉语言模型，通过三个创新解决了中文书法上下文化问题：字符级切片技术、CalliAlign和嵌入指令调整e-IT。</li>
<li>CalliBench作为首个全页书法上下文基准测试，解决了以往OCR和VQA方法中的关键问题和缺陷。</li>
<li>CalliReader在页级书法识别和解释方面优于其他先进方法和专业人士，且具有较高的准确性和效率。</li>
<li>与推理模型的比较突显了准确识别对于可靠理解的先决条件的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06472">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-41bac42630b6d93f95593872be4cd34b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a79a389fe6dcfd8a8c3047079226a228.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32033de385c9a5cc7a307938287adf80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b3e86df68e0bce0258df634e69fbae7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-54a95fc09bdc8be6d33a50a811d85885.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd6d5ae91fd1ea30c3930888162d3b3d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MastermindEval-A-Simple-But-Scalable-Reasoning-Benchmark"><a href="#MastermindEval-A-Simple-But-Scalable-Reasoning-Benchmark" class="headerlink" title="MastermindEval: A Simple But Scalable Reasoning Benchmark"></a>MastermindEval: A Simple But Scalable Reasoning Benchmark</h2><p><strong>Authors:Jonas Golde, Patrick Haller, Fabio Barth, Alan Akbik</strong></p>
<p>Recent advancements in large language models (LLMs) have led to remarkable performance across a wide range of language understanding and mathematical tasks. As a result, increasing attention has been given to assessing the true reasoning capabilities of LLMs, driving research into commonsense, numerical, logical, and qualitative reasoning. However, with the rapid progress of reasoning-focused models such as OpenAI’s o1 and DeepSeek’s R1, there has been a growing demand for reasoning benchmarks that can keep pace with ongoing model developments. In this paper, we introduce MastermindEval, a simple, scalable, and interpretable deductive reasoning benchmark inspired by the board game Mastermind. Our benchmark supports two evaluation paradigms: (1) agentic evaluation, in which the model autonomously plays the game, and (2) deductive reasoning evaluation, in which the model is given a pre-played game state with only one possible valid code to infer. In our experimental results we (1) find that even easy Mastermind instances are difficult for current models and (2) demonstrate that the benchmark is scalable to possibly more advanced models in the future Furthermore, we investigate possible reasons why models cannot deduce the final solution and find that current models are limited in deducing the concealed code as the number of statement to combine information from is increasing. </p>
<blockquote>
<p>近期大型语言模型（LLM）的最新进展在各种语言理解和数学任务中取得了显著的成绩。因此，人们越来越关注评估LLM的真正推理能力，推动了常识、数值、逻辑和定性推理的研究。然而，随着以OpenAI的o1和DeepSeek的R1等为代表的推理重点模型的快速发展，对能与当前模型发展同步的推理基准测试的需求也在增长。在本文中，我们介绍了MastermindEval，这是一个受棋盘游戏《猜密码》启发的简单、可扩展和可解释的演绎推理基准测试。我们的基准测试支持两种评估范式：（1）代理评估，即模型自主玩游戏；（2）演绎推理评估，即给模型一个预先玩过的游戏状态，只有一个可能的正确密码需要推断。在我们的实验结果中，我们（1）发现即使是简单的《猜密码》实例对当前的模型来说也是困难的，（2）证明该基准测试在未来可以扩展到更先进的模型。此外，我们还调查了模型无法推断最终解决方案的可能原因，并发现随着需要从陈述中结合信息数量的增加，当前模型在推断隐藏密码方面存在局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05891v3">PDF</a> 9 pages, 2 figures, 4 tables. In: ICLR 2025 Workshop on Reasoning and   Planning for Large Language Models</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的最新进展在各种语言理解和数学任务上取得了显著的成绩，引发了人们对评估其真正推理能力的关注，推动了常识、数值、逻辑和定性推理的研究。为此，人们迫切需要与模型发展同步的推理基准测试。本文介绍了MastermindEval，一个简单、可扩展、可解释的推理基准测试，其灵感来自猜谜游戏Mastermind。该基准测试支持两种评估范式：一是模型自主玩游戏，二是推理评估，即模型根据给定的游戏状态进行推理。实验结果表明，即使是简单的猜谜实例对当前的模型来说也是困难的，并且该基准测试可以扩展到未来的更高级模型。当前模型的局限性在于，随着需要组合的信息数量的增加，它们无法推断出最终的答案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM的最新进展在各种语言理解和数学任务上表现出色，引发了对评估其真正推理能力的关注。</li>
<li>迫切需要与模型发展同步的推理基准测试。</li>
<li>介绍了MastermindEval，一个简单、可扩展、可解释的推理基准测试，灵感来自猜谜游戏Mastermind。</li>
<li>MastermindEval支持两种评估范式：模型自主玩游戏和推理评估。</li>
<li>实验表明，即使是简单的猜谜实例对当前的模型来说也是困难的。</li>
<li>MastermindEval可以扩展到未来的更高级模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05891">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ae2a4bea4df439f87dcf060498129c24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b23084d546f81e31370fec9616adbf70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9417ec18fef4236b19fbdf03aa03f636.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-981cc37b1f842857a47af413369acb78.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="The-Lazy-Student’s-Dream-ChatGPT-Passing-an-Engineering-Course-on-Its-Own"><a href="#The-Lazy-Student’s-Dream-ChatGPT-Passing-an-Engineering-Course-on-Its-Own" class="headerlink" title="The Lazy Student’s Dream: ChatGPT Passing an Engineering Course on Its   Own"></a>The Lazy Student’s Dream: ChatGPT Passing an Engineering Course on Its   Own</h2><p><strong>Authors:Gokul Puthumanaillam, Melkior Ornik</strong></p>
<p>This paper presents a comprehensive investigation into the capability of Large Language Models (LLMs) to successfully complete a semester-long undergraduate control systems course. Through evaluation of 115 course deliverables, we assess LLM performance using ChatGPT under a “minimal effort” protocol that simulates realistic student usage patterns. The investigation employs a rigorous testing methodology across multiple assessment formats, from auto-graded multiple choice questions to complex Python programming tasks and long-form analytical writing. Our analysis provides quantitative insights into AI’s strengths and limitations in handling mathematical formulations, coding challenges, and theoretical concepts in control systems engineering. The LLM achieved a B-grade performance (82.24%), approaching but not exceeding the class average (84.99%), with strongest results in structured assignments and greatest limitations in open-ended projects. The findings inform discussions about course design adaptation in response to AI advancement, moving beyond simple prohibition towards thoughtful integration of these tools in engineering education. Additional materials including syllabus, examination papers, design projects, and example responses can be found at the project website: <a target="_blank" rel="noopener" href="https://gradegpt.github.io/">https://gradegpt.github.io</a>. </p>
<blockquote>
<p>本文全面探讨了大型语言模型（LLM）成功完成一学期本科控制系统课程的能力。通过对115份课程交付成果的评价，我们采用ChatGPT评估LLM性能，遵循模拟现实学生使用模式的“最小努力”协议。调查采用严格的测试方法，涵盖多种评估形式，从自动评分的多项选择题到复杂的Python编程任务和长篇分析写作。我们的分析提供了关于AI在处理控制系统工程中的数学公式、编程挑战和理论概念的优点和局限性的定量见解。LLM取得了B级表现（82.24%），接近但未达到班级平均水平（84.99%），结构化作业成绩最佳，开放式项目成绩局限性最大。这些发现引发了关于适应AI发展的课程设计适应的讨论，超越简单的禁令，朝着在工程教育中深思熟虑地整合这些工具的方向发展。附加材料包括教学大纲、试卷、设计项目和示例答案，可在项目网站找到：<a target="_blank" rel="noopener" href="https://gradegpt.github.io./">https://gradegpt.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05760v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型（LLM）在完成学期长的控制工程系本科生课程方面的能力。通过对使用ChatGPT在模拟真实学生使用模式的“最小努力”协议下完成的115份课程成果进行评价，该研究采用严格的测试方法，涵盖多种评估形式，从自动批改的选择题到复杂的Python编程任务和长格式分析写作。分析提供了关于人工智能在处理控制系工程中的数学公式、编程挑战和理论概念的优点和局限性的定量见解。LLM的表现达到了B级水平（82.24%），接近但未超过班级平均水平（84.99%），在结构化作业中的表现最佳，而在开放式项目中的表现最为受限。这些发现引发了关于适应人工智能发展的课程设计的讨论，推动人们超越简单的禁令，开始思考如何整合这些工具在工程教育中发挥作用。更多材料可以在项目网站上找到：<a target="_blank" rel="noopener" href="https://gradegpt.github.io/">https://gradegpt.github.io</a>。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是关于该文本的关键见解：</p>
<ul>
<li>大型语言模型（LLM）在完成控制工程系本科生课程方面的能力得到了全面研究。</li>
<li>通过使用ChatGPT进行模拟真实学生使用模式的测试，对LLM的表现进行了评价。</li>
<li>研究采用了多种评估形式的严格测试方法，包括自动批改的选择题、Python编程任务和长格式分析写作。</li>
<li>LLM在处理控制系工程中的数学公式、编程挑战和理论概念方面表现出优点和局限性。</li>
<li>LLM的成绩达到了B级水平，接近班级平均水平，但并未超过。</li>
<li>LLM在结构化作业中的表现最佳，而在开放式项目中的表现受限。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05760">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cf74afe4c6d0703c62ae52d3b0e924ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67bdee915f3f1854ce7502388fdf0d0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d89ff0a65bdd64b9f97d6185862a6f43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3b91291511b7b3b642c59b7d5279932.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e8ab861defd4399182b8ef0f4a9ae66.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="PaCA-Partial-Connection-Adaptation-for-Efficient-Fine-Tuning"><a href="#PaCA-Partial-Connection-Adaptation-for-Efficient-Fine-Tuning" class="headerlink" title="PaCA: Partial Connection Adaptation for Efficient Fine-Tuning"></a>PaCA: Partial Connection Adaptation for Efficient Fine-Tuning</h2><p><strong>Authors:Sunghyeon Woo, Sol Namkung, Sunwoo Lee, Inho Jeong, Beomseok Kim, Dongsuk Jeon</strong></p>
<p>Prior parameter-efficient fine-tuning (PEFT) algorithms reduce memory usage and computational costs of fine-tuning large neural network models by training only a few additional adapter parameters, rather than the entire model. However, the reduction in computational costs due to PEFT does not necessarily translate to a reduction in training time; although the computational costs of the adapter layers are much smaller than the pretrained layers, it is well known that those two types of layers are processed sequentially on GPUs, resulting in significant latency overhead. LoRA and its variants merge low-rank adapter matrices with pretrained weights during inference to avoid latency overhead, but during training, the pretrained weights remain frozen while the adapter matrices are continuously updated, preventing such merging. To mitigate this issue, we propose Partial Connection Adaptation (PaCA), which fine-tunes randomly selected partial connections within the pretrained weights instead of introducing adapter layers in the model. PaCA not only enhances training speed by eliminating the time overhead due to the sequential processing of the adapter and pretrained layers but also reduces activation memory since only partial activations, rather than full activations, need to be stored for gradient computation. Compared to LoRA, PaCA reduces training time by 22% and total memory usage by 16%, while maintaining comparable accuracy across various fine-tuning scenarios, such as fine-tuning on the MMLU dataset and instruction tuning on the Oasst1 dataset. PaCA can also be combined with quantization, enabling the fine-tuning of large models such as LLaMA3.1-70B. In addition, PaCA enables training with 23% longer sequence and improves throughput by 16% on both NVIDIA A100 GPU and INTEL Gaudi2 HPU compared to LoRA. The code is available at <a target="_blank" rel="noopener" href="https://github.com/WooSunghyeon/paca">https://github.com/WooSunghyeon/paca</a>. </p>
<blockquote>
<p>之前的参数高效微调（PEFT）算法通过仅训练少量额外的适配器参数，而不是对整个模型进行训练，从而减少了微调大型神经网络模型的内存使用和计算成本。然而，由于PEFT导致的计算成本降低并不一定转化为训练时间的减少。尽管适配器层的计算成本远小于预训练层，众所周知，这两种类型的层在GPU上是顺序处理的，这会导致显著的延迟开销。LoRA及其变体在推理过程中将低阶适配器矩阵与预训练权重合并，以避免延迟开销，但在训练过程中，预训练权重保持冻结状态，而适配器矩阵不断更新，这阻止了合并。为了缓解这个问题，我们提出了部分连接适配（PaCA），它在预训练权重内微调随机选择的部分连接，而不是在模型中引入适配器层。PaCA不仅通过消除由于适配器层和预训练层的顺序处理而产生的时间开销，提高了训练速度，而且减少了激活内存，因为只需要存储部分激活值，而不是完整的激活值来进行梯度计算。与LoRA相比，PaCA减少了22%的训练时间和16%的总内存使用量，同时在各种微调场景（如使用MMLU数据集进行微调和使用Oasst1数据集进行指令调整）中保持相当的准确性。PaCA还可以与量化相结合，实现对大型模型（如LLaMA3.1-70B）的微调。此外，PaCA使用NVIDIA A100 GPU和INTEL Gaudi2 HPU时，支持23%更长的序列训练，并提高了16%的吞吐量相比LoRA。代码可在<a target="_blank" rel="noopener" href="https://github.com/WooSunghyeon/paca%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/WooSunghyeon/paca找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01905v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为Partial Connection Adaptation（PaCA）的新方法，用于改进现有的参数高效微调（PEFT）算法。该方法通过微调预训练权重中的部分连接，避免了引入额外的适配器层，从而提高训练速度和减少内存使用。相较于传统的LoRA方法，PaCA能够减少训练时间和总内存使用，同时保持相当的精度。此外，PaCA还可以与量化结合，支持大规模模型的微调，并能提高序列长度处理能力和吞吐量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PaCA是一种改进的PEFT方法，通过微调预训练权重中的部分连接，避免引入额外的适配器层。</li>
<li>PaCA提高了训练速度，减少了因适配器层和预训练层顺序处理导致的延迟开销。</li>
<li>PaCA降低了激活内存的使用，因为只需存储部分激活，而不是完整的激活，以供梯度计算。</li>
<li>PaCA相较于LoRA，能够减少训练时间22%和总内存使用16%，同时保持相似的精度。</li>
<li>PaCA支持在多种微调场景（如MMLU数据集上的微调、Oasst1数据集上的指令微调）中使用。</li>
<li>PaCA可与量化结合，使大规模模型的微调成为可能，如LLaMA3.1-70B。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01905">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cdb4aa9dcf132368d1c021d6ad68e811.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ca70077b7eea935ee161e2b872ea259.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c31f5d31b8b8c07a3cf173277a1fe769.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-578ca7e29e65ec372bdbe8b9980966ce.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-add2cbcb953bd382c69d1628dc25ea89.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-03-14  ReMA Learning to Meta-think for LLMs with Multi-Agent Reinforcement   Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d435ed7ca1c36725b8fded997977b9fe.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-03-14  Search-R1 Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">16190.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
