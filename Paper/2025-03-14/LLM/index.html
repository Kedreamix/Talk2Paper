<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  SimLingo Vision-Only Closed-Loop Autonomous Driving with   Language-Action Alignment">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-2443f5f69ced566e7b2344260a6fae41.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-14-æ›´æ–°"><a href="#2025-03-14-æ›´æ–°" class="headerlink" title="2025-03-14 æ›´æ–°"></a>2025-03-14 æ›´æ–°</h1><h2 id="SimLingo-Vision-Only-Closed-Loop-Autonomous-Driving-with-Language-Action-Alignment"><a href="#SimLingo-Vision-Only-Closed-Loop-Autonomous-Driving-with-Language-Action-Alignment" class="headerlink" title="SimLingo: Vision-Only Closed-Loop Autonomous Driving with   Language-Action Alignment"></a>SimLingo: Vision-Only Closed-Loop Autonomous Driving with   Language-Action Alignment</h2><p><strong>Authors:Katrin Renz, Long Chen, Elahe Arani, Oleg Sinavski</strong></p>
<p>Integrating large language models (LLMs) into autonomous driving has attracted significant attention with the hope of improving generalization and explainability. However, existing methods often focus on either driving or vision-language understanding but achieving both high driving performance and extensive language understanding remains challenging. In addition, the dominant approach to tackle vision-language understanding is using visual question answering. However, for autonomous driving, this is only useful if it is aligned with the action space. Otherwise, the modelâ€™s answers could be inconsistent with its behavior. Therefore, we propose a model that can handle three different tasks: (1) closed-loop driving, (2) vision-language understanding, and (3) language-action alignment. Our model SimLingo is based on a vision language model (VLM) and works using only camera, excluding expensive sensors like LiDAR. SimLingo obtains state-of-the-art performance on the widely used CARLA simulator on the Bench2Drive benchmark and is the winning entry at the CARLA challenge 2024. Additionally, we achieve strong results in a wide variety of language-related tasks while maintaining high driving performance. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆåˆ°è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œæ—¨åœ¨æé«˜é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§ï¼Œå·²ç»å¼•èµ·äº†äººä»¬çš„å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€é›†ä¸­åœ¨é©¾é©¶æˆ–è§†è§‰è¯­è¨€ç†è§£ä¸Šï¼Œè€Œå®ç°é«˜é©¾é©¶æ€§èƒ½å’Œå¹¿æ³›çš„è¯­è¨€ç†è§£ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œè§£å†³è§†è§‰è¯­è¨€ç†è§£çš„ä¸»è¦æ–¹æ³•æ˜¯ä½¿ç”¨è§†è§‰é—®ç­”ã€‚ç„¶è€Œï¼Œå¯¹äºè‡ªåŠ¨é©¾é©¶æ¥è¯´ï¼Œåªæœ‰å°†å…¶ä¸åŠ¨ä½œç©ºé—´å¯¹é½æ—¶æ‰æœ‰ç”¨ã€‚å¦åˆ™ï¼Œæ¨¡å‹çš„ç­”æ¡ˆå¯èƒ½ä¸å…¶è¡Œä¸ºä¸ä¸€è‡´ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§èƒ½å¤Ÿå¤„ç†ä¸‰ç§ä¸åŒä»»åŠ¡æ¨¡å‹ï¼šå³ï¼ˆ1ï¼‰é—­ç¯é©¾é©¶ã€ï¼ˆ2ï¼‰è§†è§‰è¯­è¨€ç†è§£å’Œï¼ˆ3ï¼‰è¯­è¨€åŠ¨ä½œå¯¹é½ã€‚æˆ‘ä»¬çš„æ¨¡å‹SimLingoåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œä»…ä½¿ç”¨ç›¸æœºå·¥ä½œï¼Œä¸åŒ…æ‹¬æ˜‚è´µçš„ä¼ æ„Ÿå™¨ï¼Œå¦‚æ¿€å…‰é›·è¾¾ã€‚SimLingoåœ¨å¹¿æ³›ä½¿ç”¨çš„CARLAæ¨¡æ‹Ÿå™¨ä¸Šçš„Bench2DriveåŸºå‡†æµ‹è¯•ä¸­è·å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨CARLAæŒ‘æˆ˜2024ä¸­è·å¾—äº†ç¬¬ä¸€åã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨å¤šç§è¯­è¨€ç›¸å…³ä»»åŠ¡ä¸­å–å¾—äº†å¼ºåŠ²çš„ç»“æœï¼ŒåŒæ—¶ä¿æŒäº†é«˜é©¾é©¶æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09594v1">PDF</a> CVPR 2025. 1st Place @ CARLA Challenge 2024. Challenge tech report   (preliminary version of SimLingo): arXiv:2406.10165</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨å·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œæœ‰åŠ©äºæé«˜é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸é›†ä¸­åœ¨é©¾é©¶æˆ–è§†è§‰è¯­è¨€ç†è§£æ–¹é¢ï¼Œå®ç°é«˜é©¾é©¶æ€§èƒ½å’Œå¹¿æ³›çš„è¯­è¨€ç†è§£ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¯ä»¥å¤„ç†ä¸‰ç§ä¸åŒä»»åŠ¡ï¼ˆé—­ç¯é©¾é©¶ã€è§†è§‰è¯­è¨€ç†è§£å’Œè¯­è¨€è¡Œä¸ºå¯¹é½ï¼‰çš„æ¨¡å‹SimLingoã€‚è¯¥æ¨¡å‹åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œä»…ä½¿ç”¨ç›¸æœºå·¥ä½œï¼Œä¸ä½¿ç”¨æ˜‚è´µçš„ä¼ æ„Ÿå™¨å¦‚æ¿€å…‰é›·è¾¾ã€‚SimLingoåœ¨å¹¿æ³›ä½¿ç”¨çš„CARLAæ¨¡æ‹Ÿå™¨ä¸Šçš„Bench2DriveåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æ€§èƒ½ï¼Œå¹¶åœ¨CARLAæŒ‘æˆ˜èµ›ä¸­è·å¾—äº†ç¬¬ä¸€åã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹åœ¨å„ç§è¯­è¨€ç›¸å…³ä»»åŠ¡ä¸­ä¹Ÿå–å¾—äº†æ˜¾è‘—æˆæœï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„é©¾é©¶æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„é›†æˆæ—¨åœ¨æé«˜é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨å®ç°é«˜é©¾é©¶æ€§èƒ½å’Œå¹¿æ³›çš„è¯­è¨€ç†è§£æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>SimLingoæ¨¡å‹èƒ½å¤Ÿå¤„ç†é—­ç¯é©¾é©¶ã€è§†è§‰è¯­è¨€ç†è§£å’Œè¯­è¨€è¡Œä¸ºå¯¹é½ä¸‰ç§ä»»åŠ¡ã€‚</li>
<li>SimLingoåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œä»…ä½¿ç”¨ç›¸æœºï¼Œä¸ä½¿ç”¨æ˜‚è´µçš„ä¼ æ„Ÿå™¨ã€‚</li>
<li>SimLingoåœ¨CARLAæ¨¡æ‹Ÿå™¨çš„Bench2DriveåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æ€§èƒ½ã€‚</li>
<li>SimLingoåœ¨CARLAæŒ‘æˆ˜èµ›ä¸­è·å¾—äº†ç¬¬ä¸€åã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09594">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c460299ce81847049cf133ea2da88a3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa3e14340d4485e856439b0827fdb56d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcf9aa9a45b3bb2a5d09a332c06c7d32.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BIMBA-Selective-Scan-Compression-for-Long-Range-Video-Question-Answering"><a href="#BIMBA-Selective-Scan-Compression-for-Long-Range-Video-Question-Answering" class="headerlink" title="BIMBA: Selective-Scan Compression for Long-Range Video Question   Answering"></a>BIMBA: Selective-Scan Compression for Long-Range Video Question   Answering</h2><p><strong>Authors:Md Mohaiminul Islam, Tushar Nagarajan, Huiyu Wang, Gedas Bertasius, Lorenzo Torresani</strong></p>
<p>Video Question Answering (VQA) in long videos poses the key challenge of extracting relevant information and modeling long-range dependencies from many redundant frames. The self-attention mechanism provides a general solution for sequence modeling, but it has a prohibitive cost when applied to a massive number of spatiotemporal tokens in long videos. Most prior methods rely on compression strategies to lower the computational cost, such as reducing the input length via sparse frame sampling or compressing the output sequence passed to the large language model (LLM) via space-time pooling. However, these naive approaches over-represent redundant information and often miss salient events or fast-occurring space-time patterns. In this work, we introduce BIMBA, an efficient state-space model to handle long-form videos. Our model leverages the selective scan algorithm to learn to effectively select critical information from high-dimensional video and transform it into a reduced token sequence for efficient LLM processing. Extensive experiments demonstrate that BIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks, including PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and Video-MME. Code, and models are publicly available at <a target="_blank" rel="noopener" href="https://sites.google.com/view/bimba-mllm">https://sites.google.com/view/bimba-mllm</a>. </p>
<blockquote>
<p>è§†é¢‘é—®ç­”ï¼ˆVQAï¼‰åœ¨é•¿è§†é¢‘ä¸­é¢ä¸´çš„å…³é”®æŒ‘æˆ˜æ˜¯ä»å¤§é‡å†—ä½™å¸§ä¸­æå–ç›¸å…³ä¿¡æ¯å¹¶å¯¹é•¿è·ç¦»ä¾èµ–è¿›è¡Œå»ºæ¨¡ã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸ºåºåˆ—å»ºæ¨¡æä¾›äº†ä¸€èˆ¬è§£å†³æ–¹æ¡ˆï¼Œä½†å½“åº”ç”¨äºé•¿è§†é¢‘ä¸­å¤§é‡æ—¶ç©ºä»¤ç‰Œæ—¶ï¼Œå…¶æˆæœ¬æ˜¯å·¨å¤§çš„ã€‚å¤§å¤šæ•°å…ˆå‰çš„æ–¹æ³•ä¾èµ–äºå‹ç¼©ç­–ç•¥æ¥é™ä½è®¡ç®—æˆæœ¬ï¼Œä¾‹å¦‚é€šè¿‡ç¨€ç–å¸§é‡‡æ ·å‡å°‘è¾“å…¥é•¿åº¦ï¼Œæˆ–é€šè¿‡æ—¶ç©ºæ± åŒ–å‹ç¼©ä¼ é€’ç»™å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºåºåˆ—ã€‚ç„¶è€Œï¼Œè¿™äº›ç®€å•çš„æ–¹æ³•è¿‡åº¦ä»£è¡¨äº†å†—ä½™ä¿¡æ¯ï¼Œå¹¶ä¸”ç»å¸¸é”™è¿‡é‡è¦äº‹ä»¶æˆ–å¿«é€Ÿå‘ç”Ÿçš„æ—¶ç©ºæ¨¡å¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†BIMBAï¼Œä¸€ç§ç”¨äºå¤„ç†é•¿æ ¼å¼è§†é¢‘çš„é«˜æ•ˆçŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨é€‰æ‹©æ€§æ‰«æç®—æ³•æ¥å­¦ä¹ ä»é«˜ç»´è§†é¢‘ä¸­é€‰æ‹©å…³é”®ä¿¡æ¯å¹¶å°†å…¶è½¬æ¢ä¸ºå‡å°‘çš„ä»¤ç‰Œåºåˆ—ä»¥è¿›è¡Œé«˜æ•ˆçš„LLMå¤„ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBIMBAåœ¨å¤šä¸ªé•¿æ ¼å¼VQAåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼ŒåŒ…æ‹¬PerceptionTestã€NExT-QAã€EgoSchemaã€VNBenchã€LongVideoBenchå’Œè§†é¢‘MMEã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://sites.google.com/view/bimba-mllm%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://sites.google.com/view/bimba-mllmå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09590v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹é•¿è§†é¢‘ä¸­çš„è§†é¢‘é—®é¢˜å›ç­”ï¼ˆVQAï¼‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„é«˜æ•ˆçŠ¶æ€ç©ºé—´æ¨¡å‹BIMBAã€‚è¯¥æ¨¡å‹åˆ©ç”¨é€‰æ‹©æ€§æ‰«æç®—æ³•ä»é«˜ç»´è§†é¢‘ä¸­é€‰æ‹©å…³é”®ä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå‡å°‘çš„ä»¤ç‰Œåºåˆ—ï¼Œä»¥ä¾¿é«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒBIMBAåœ¨å¤šä¸ªé•¿æ ¼å¼VQAåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VQAåœ¨é•¿è§†é¢‘ä¸Šé¢ä¸´æå–ç›¸å…³ä¿¡æ¯å’Œå»ºæ¨¡é•¿èŒƒå›´ä¾èµ–æ€§çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸ºåºåˆ—å»ºæ¨¡æä¾›äº†é€šç”¨è§£å†³æ–¹æ¡ˆï¼Œä½†åº”ç”¨äºé•¿è§†é¢‘çš„ä¼—å¤šæ—¶ç©ºä»¤ç‰Œæ—¶è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>ä¹‹å‰çš„ç­–ç•¥å¦‚ç¨€ç–å¸§é‡‡æ ·å’Œæ—¶ç©ºæ± åŒ–ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œä½†å¯èƒ½è¿‡åº¦ä»£è¡¨å†—ä½™ä¿¡æ¯å¹¶é”™è¿‡é‡è¦äº‹ä»¶æˆ–å¿«é€Ÿå‘ç”Ÿçš„æ—¶ç©ºæ¨¡å¼ã€‚</li>
<li>BIMBAæ¨¡å‹æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œç”¨äºå¤„ç†é•¿æ ¼å¼è§†é¢‘ã€‚</li>
<li>BIMBAåˆ©ç”¨é€‰æ‹©æ€§æ‰«æç®—æ³•ä»é«˜ç»´è§†é¢‘ä¸­é€‰æ‹©å…³é”®ä¿¡æ¯ã€‚</li>
<li>BIMBAåœ¨å¤šä¸ªé•¿æ ¼å¼VQAåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09590">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-acaf27607f9b7fdc041e00437b14dcb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79ae60c7dc8957471ea7d235fbdff4c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27de4f09c104aa2e7b1dfbda68b5e095.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b92ad4ecc0aaabba4ce8ea9eb170980.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Search-R1-Training-LLMs-to-Reason-and-Leverage-Search-Engines-with-Reinforcement-Learning"><a href="#Search-R1-Training-LLMs-to-Reason-and-Leverage-Search-Engines-with-Reinforcement-Learning" class="headerlink" title="Search-R1: Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning"></a>Search-R1: Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning</h2><p><strong>Authors:Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, Jiawei Han</strong></p>
<p>Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Retrieval augmentation and tool-use training approaches where a search engine is treated as a tool lack complex multi-turn retrieval flexibility or require large-scale supervised data. Prompting advanced LLMs with reasoning capabilities during inference to use search engines is not optimal, since the LLM does not learn how to optimally interact with the search engine. This paper introduces Search-R1, an extension of the DeepSeek-R1 model where the LLM learns â€“ solely through reinforcement learning (RL) â€“ to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM rollouts with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/PeterGriffinJin/Search-R1">https://github.com/PeterGriffinJin/Search-R1</a>. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†å’Œæ–‡æœ¬ç”Ÿæˆï¼Œè·å–å¤–éƒ¨çŸ¥è¯†å’Œæœ€æ–°ä¿¡æ¯æ˜¯å…³é”®ã€‚å°½ç®¡æ£€ç´¢å¢å¼ºå’Œå·¥å…·ä½¿ç”¨è®­ç»ƒçš„æ–¹æ³•å°†æœç´¢å¼•æ“è§†ä¸ºå·¥å…·ï¼Œä½†å®ƒä»¬åœ¨å¤æ‚çš„å¤šè½®æ£€ç´¢çµæ´»æ€§æ–¹é¢æœ‰æ‰€ä¸è¶³æˆ–éœ€è¦å¤§é‡ç›‘ç£æ•°æ®ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­æç¤ºå…·æœ‰æ¨ç†èƒ½åŠ›çš„å…ˆè¿›LLMä½¿ç”¨æœç´¢å¼•æ“å¹¶ä¸ç†æƒ³ï¼Œå› ä¸ºLLMå¹¶æ²¡æœ‰å­¦ä¹ å¦‚ä½•æœ€ä¼˜åœ°ä¸æœç´¢å¼•æ“è¿›è¡Œäº¤äº’ã€‚æœ¬æ–‡ä»‹ç»äº†Search-R1ï¼Œå®ƒæ˜¯DeepSeek-R1æ¨¡å‹çš„æ‰©å±•ï¼Œå…¶ä¸­LLMé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è‡ªä¸»å­¦ä¹ ï¼Œåœ¨é€æ­¥æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»ç”Ÿæˆï¼ˆå¤šä¸ªï¼‰æœç´¢æŸ¥è¯¢å¹¶è¿›è¡Œå®æ—¶æ£€ç´¢ã€‚Search-R1é€šè¿‡å¤šè½®æœç´¢äº¤äº’ä¼˜åŒ–LLMçš„æ»šåŠ¨æ“ä½œï¼Œåˆ©ç”¨æ£€ç´¢ä»¤ç‰Œå±è”½è¿›è¡Œç¨³å®šçš„RLè®­ç»ƒå’Œä¸€ä¸ªç®€å•çš„åŸºäºç»“æœå¥–åŠ±å‡½æ•°ã€‚åœ¨ä¸ƒä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSearch-R1ç›¸è¾ƒäºæœ€å…ˆè¿›åŸºçº¿æé«˜äº†26%ï¼ˆQwen2.5-7Bï¼‰ã€21%ï¼ˆQwen2.5-3Bï¼‰å’Œ10%ï¼ˆLLaMA3.2-3Bï¼‰çš„æ€§èƒ½ã€‚æœ¬æ–‡è¿˜æ·±å…¥æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ–¹æ³•ã€LLMé€‰æ‹©å’Œæ£€ç´¢å¢å¼ºæ¨ç†ä¸­çš„å“åº”é•¿åº¦åŠ¨æ€ã€‚ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/PeterGriffinJin/Search-R1%E3%80%82">https://github.com/PeterGriffinJin/Search-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09516v1">PDF</a> 16 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºSearch-R1æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ˜¯å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€ç§æ‰©å±•ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è‡ªä¸»å­¦ä¹ åœ¨å®æ—¶æ£€ç´¢ä¸­è¿›è¡Œå¤šè½®æœç´¢æŸ¥è¯¢ã€‚Search-R1ä¼˜åŒ–äº†LLMçš„æ¨ç†è¿‡ç¨‹ï¼Œé€šè¿‡å¤šè½®æœç´¢äº¤äº’ã€æ£€ç´¢åˆ°çš„ä»¤ç‰Œæ©ç ç”¨äºç¨³å®šçš„RLè®­ç»ƒä»¥åŠç®€å•çš„ç»“æœå¯¼å‘å¥–åŠ±å‡½æ•°æ¥å®ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSearch-R1åœ¨ä¸ƒä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¾ƒç°æœ‰æŠ€æœ¯æé«˜äº†26%ï¼ˆQwen2.5-7Bï¼‰ã€21%ï¼ˆQwen2.5-3Bï¼‰å’Œ10%ï¼ˆLLaMA3.2-3Bï¼‰ã€‚åŒæ—¶ï¼Œæœ¬è®ºæ–‡è¿˜å¯¹RLä¼˜åŒ–æ–¹æ³•ã€LLMé€‰æ‹©å’Œå“åº”é•¿åº¦åŠ¨æ€ç­‰è¿›è¡Œäº†å®è¯åˆ†æã€‚ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å¯åœ¨æŒ‡å®šç½‘å€ä¸‹è½½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Search-R1æ¨¡å‹æ˜¯LLMçš„ä¸€ç§æ‰©å±•ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è‡ªä¸»å­¦ä¹ è¿›è¡Œå¤šè½®æœç´¢æŸ¥è¯¢ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å®æ—¶æ£€ç´¢ã€å¤šè½®æœç´¢äº¤äº’ç­‰æ–¹å¼ä¼˜åŒ–LLMæ¨ç†è¿‡ç¨‹ã€‚</li>
<li>Search-R1é€šè¿‡æ£€ç´¢åˆ°çš„ä»¤ç‰Œæ©ç å®ç°ç¨³å®šçš„RLè®­ç»ƒã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨ç®€å•çš„ç»“æœå¯¼å‘å¥–åŠ±å‡½æ•°ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSearch-R1åœ¨å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
<li>æœ¬è®ºæ–‡æä¾›äº†å…³äºRLä¼˜åŒ–æ–¹æ³•ã€LLMé€‰æ‹©å’Œå“åº”é•¿åº¦åŠ¨æ€çš„å®è¯åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09516">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c71d0293ecd0ef9e3d7bef31402bf39b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7677d1b5771eb2d8fc9f3c532c475d8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ReMA-Learning-to-Meta-think-for-LLMs-with-Multi-Agent-Reinforcement-Learning"><a href="#ReMA-Learning-to-Meta-think-for-LLMs-with-Multi-Agent-Reinforcement-Learning" class="headerlink" title="ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement   Learning"></a>ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement   Learning</h2><p><strong>Authors:Ziyu Wan, Yunxiang Li, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, Ying Wen</strong></p>
<p>Recent research on Reasoning of Large Language Models (LLMs) has sought to further enhance their performance by integrating meta-thinking â€“ enabling models to monitor, evaluate, and control their reasoning processes for more adaptive and effective problem-solving. However, current single-agent work lacks a specialized design for acquiring meta-thinking, resulting in low efficacy. To address this challenge, we introduce Reinforced Meta-thinking Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think about thinking. ReMA decouples the reasoning process into two hierarchical agents: a high-level meta-thinking agent responsible for generating strategic oversight and plans, and a low-level reasoning agent for detailed executions. Through iterative reinforcement learning with aligned objectives, these agents explore and learn collaboration, leading to improved generalization and robustness. Experimental results demonstrate that ReMA outperforms single-agent RL baselines on complex reasoning tasks, including competitive-level mathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation studies further illustrate the evolving dynamics of each distinct agent, providing valuable insights into how the meta-thinking reasoning process enhances the reasoning capabilities of LLMs. </p>
<blockquote>
<p>è¿‘æœŸå…³äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„ç ”ç©¶ï¼Œæ—¨åœ¨é€šè¿‡èå…¥å…ƒæ€ç»´æ¥è¿›ä¸€æ­¥æå‡å…¶æ€§èƒ½ã€‚å…ƒæ€ç»´èƒ½å¤Ÿè®©æ¨¡å‹ç›‘æ§ã€è¯„ä¼°å’Œæ§åˆ¶ç³»ç»Ÿè‡ªèº«çš„æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œå®ç°æ›´åŠ é€‚åº”æ€§å’Œé«˜æ•ˆçš„é—®é¢˜è§£å†³ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å•æ™ºèƒ½ä½“ç ”ç©¶ç¼ºä¹è·å–å…ƒæ€ç»´çš„ä¸“é—¨è®¾è®¡ï¼Œå¯¼è‡´æ•ˆæœä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¼ºåŒ–å…ƒæ€ç»´æ™ºèƒ½ä½“ï¼ˆReMAï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ¥æ¿€å‘å…ƒæ€ç»´è¡Œä¸ºï¼Œé¼“åŠ±LLMè¿›è¡Œåæ€æ€§æ€è€ƒã€‚ReMAå°†æ¨ç†è¿‡ç¨‹è§£è€¦ä¸ºä¸¤ä¸ªå±‚æ¬¡åŒ–çš„æ™ºèƒ½ä½“ï¼šä¸€ä¸ªé«˜å±‚æ¬¡çš„å…ƒæ€ç»´æ™ºèƒ½ä½“ï¼Œè´Ÿè´£ç”Ÿæˆæˆ˜ç•¥æ€§ç›‘ç£å’Œè®¡åˆ’ï¼›ä¸€ä¸ªä½å±‚æ¬¡çš„æ¨ç†æ™ºèƒ½ä½“ï¼Œè´Ÿè´£è¯¦ç»†æ‰§è¡Œã€‚é€šè¿‡ç›®æ ‡ä¸€è‡´çš„è¿­ä»£å¼ºåŒ–å­¦ä¹ ï¼Œè¿™äº›æ™ºèƒ½ä½“æ¢ç´¢å’Œå­¦ä¹ åä½œï¼Œå®ç°äº†æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReMAåœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†å•æ™ºèƒ½ä½“RLåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ç«äº‰çº§åˆ«çš„æ•°å­¦åŸºå‡†æµ‹è¯•å’ŒLLMä½œä¸ºæ³•å®˜çš„åŸºå‡†æµ‹è¯•ã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯´æ˜äº†æ¯ä¸ªç‹¬ç‰¹æ™ºèƒ½ä½“çš„åŠ¨æ€æ¼”åŒ–è¿‡ç¨‹ï¼Œä¸ºå…ƒæ€ç»´æ¨ç†è¿‡ç¨‹å¦‚ä½•å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09501v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼•å…¥å…ƒæ€ç»´ä»¥å¢å¼ºå…¶æ€§èƒ½ï¼Œä½†ç°æœ‰å•ä¸€æ¨¡å‹è®¾è®¡åœ¨è·å–å…ƒæ€ç»´æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºå¼ºåŒ–å…ƒæ€ç»´ä»£ç†ï¼ˆReMAï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨å¤šä»£ç†å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ¿€å‘å…ƒæ€ç»´è¡Œä¸ºã€‚ReMAå°†æ¨ç†è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªå±‚æ¬¡ä»£ç†ï¼šé«˜çº§å…ƒæ€ç»´ä»£ç†è´Ÿè´£æˆ˜ç•¥ç›‘ç£ä¸è®¡åˆ’ï¼Œä½çº§æ¨ç†ä»£ç†è´Ÿè´£è¯¦ç»†æ‰§è¡Œã€‚é€šè¿‡ç›®æ ‡å¯¹é½çš„è¿­ä»£å¼ºåŒ–å­¦ä¹ ï¼Œè¿™äº›ä»£ç†èƒ½å¤Ÿæ¢ç´¢ä¸åä½œï¼Œä»è€Œæé«˜æ³›åŒ–ä¸ç¨³å¥æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReMAåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šä¼˜äºå•ä¸€ä»£ç†RLåŸºçº¿ï¼ŒåŒ…æ‹¬ç«äº‰æ€§æ•°å­¦åŸºå‡†æµ‹è¯•å’ŒLLMä½œä¸ºæ³•å®˜çš„åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå¼•å…¥å…ƒæ€ç»´ä»¥å¢å¼ºæ€§èƒ½ï¼Œé¢ä¸´å•ä¸€æ¨¡å‹è®¾è®¡è·å–å…ƒæ€ç»´çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºReMAæ¡†æ¶ï¼Œåˆ©ç”¨MARLæ¿€å‘å…ƒæ€ç»´è¡Œä¸ºã€‚</li>
<li>ReMAå°†æ¨ç†è¿‡ç¨‹åˆ†ä¸ºé«˜çº§å…ƒæ€ç»´ä»£ç†å’Œä½çº§æ¨ç†ä»£ç†ã€‚</li>
<li>é€šè¿‡ç›®æ ‡å¯¹é½çš„è¿­ä»£å¼ºåŒ–å­¦ä¹ ï¼Œä»£ç†èƒ½å¤Ÿæ¢ç´¢ä¸åä½œã€‚</li>
<li>ReMAåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬æ•°å­¦å’ŒLLMåˆ¤æ–­ä»»åŠ¡ã€‚</li>
<li>æ¶ˆèç ”ç©¶å±•ç¤ºäº†ä¸åŒä»£ç†åœ¨å…ƒæ€ç»´æ¨ç†è¿‡ç¨‹ä¸­çš„åŠ¨æ€å˜åŒ–ã€‚</li>
<li>å…ƒæ€ç»´å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-31bc4292f3764e1db00880af7f69f7be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5087306fb21b19db6c96420a875b08bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f204a1cf33c2518ea1cd5e1c60c1b75.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CASTLE-Benchmarking-Dataset-for-Static-Code-Analyzers-and-LLMs-towards-CWE-Detection"><a href="#CASTLE-Benchmarking-Dataset-for-Static-Code-Analyzers-and-LLMs-towards-CWE-Detection" class="headerlink" title="CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards   CWE Detection"></a>CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards   CWE Detection</h2><p><strong>Authors:Richard A. Dubniczky, Krisztofer ZoltÃ¡n HorvÃ¡t, TamÃ¡s Bisztray, Mohamed Amine Ferrag, Lucas C. Cordeiro, Norbert Tihanyi</strong></p>
<p>Identifying vulnerabilities in source code is crucial, especially in critical software components. Existing methods such as static analysis, dynamic analysis, formal verification, and recently Large Language Models are widely used to detect security flaws. This paper introduces CASTLE (CWE Automated Security Testing and Low-Level Evaluation), a benchmarking framework for evaluating the vulnerability detection capabilities of different methods. We assess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using a hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs. We propose the CASTLE Score, a novel evaluation metric to ensure fair comparison. Our results reveal key differences: ESBMC (a formal verification tool) minimizes false positives but struggles with vulnerabilities beyond model checking, such as weak cryptography or SQL injection. Static analyzers suffer from high false positives, increasing manual validation efforts for developers. LLMs perform exceptionally well in the CASTLE dataset when identifying vulnerabilities in small code snippets. However, their accuracy declines, and hallucinations increase as the code size grows. These results suggest that LLMs could play a pivotal role in future security solutions, particularly within code completion frameworks, where they can provide real-time guidance to prevent vulnerabilities. The dataset is accessible at <a target="_blank" rel="noopener" href="https://github.com/CASTLE-Benchmark">https://github.com/CASTLE-Benchmark</a>. </p>
<blockquote>
<p>è¯†åˆ«æºä»£ç ä¸­çš„æ¼æ´è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å…³é”®è½¯ä»¶ç»„ä»¶ä¸­ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚é™æ€åˆ†æã€åŠ¨æ€åˆ†æã€å½¢å¼åŒ–éªŒè¯å’Œæœ€è¿‘å‡ºç°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¢«å¹¿æ³›ç”¨äºæ£€æµ‹å®‰å…¨æ¼æ´ã€‚æœ¬æ–‡ä»‹ç»äº†CASTLEï¼ˆCWEè‡ªåŠ¨åŒ–å®‰å…¨æµ‹è¯•å’Œä½çº§è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°ä¸åŒæ–¹æ³•æ¼æ´æ£€æµ‹èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«25ä¸ªå¸¸è§CWEçš„250ä¸ªæ‰‹å·¥åˆ¶ä½œçš„å°å‹åŸºå‡†ç¨‹åºæ•°æ®é›†ï¼Œè¯„ä¼°äº†13ç§é™æ€åˆ†æå·¥å…·ã€1LLMå’Œ2ç§å½¢å¼åŒ–éªŒè¯å·¥å…·ã€‚æˆ‘ä»¬æå‡ºäº†CASTLEåˆ†æ•°ï¼Œä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥ç¡®ä¿å…¬å¹³æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜äº†å…³é”®å·®å¼‚ï¼šESBMCï¼ˆä¸€ç§å½¢å¼åŒ–éªŒè¯å·¥å…·ï¼‰è™½ç„¶èƒ½æœ€å°åŒ–è¯¯æŠ¥ï¼Œä½†åœ¨æ¨¡å‹æ£€æŸ¥ä¹‹å¤–çš„æ¼æ´ï¼Œå¦‚å¼±åŠ å¯†æˆ–SQLæ³¨å…¥ç­‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚é™æ€åˆ†æå™¨å­˜åœ¨è¾ƒé«˜çš„è¯¯æŠ¥ç‡ï¼Œå¢åŠ äº†å¼€å‘è€…çš„æ‰‹åŠ¨éªŒè¯å·¥ä½œã€‚å¤§å‹è¯­è¨€æ¨¡å‹åœ¨CASTLEæ•°æ®é›†ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«å°ä»£ç ç‰‡æ®µä¸­çš„æ¼æ´æ–¹é¢ã€‚ç„¶è€Œï¼Œéšç€ä»£ç é‡çš„å¢é•¿ï¼Œå…¶å‡†ç¡®æ€§ä¼šä¸‹é™ï¼Œå¹»è§‰ç°è±¡ä¹Ÿä¼šå¢å¤šã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœªæ¥çš„å®‰å…¨è§£å†³æ–¹æ¡ˆä¸­å¯èƒ½ä¼šå‘æŒ¥å…³é”®ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£ç è¡¥å…¨æ¡†æ¶ä¸­ï¼Œå®ƒä»¬å¯ä»¥æä¾›å®æ—¶æŒ‡å¯¼ä»¥é˜²æ­¢æ¼æ´ã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CASTLE-Benchmark%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/CASTLE-Benchmarkä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09433v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCASTLEçš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æºä»£ç æ¼æ´æ£€æµ‹å·¥å…·çš„æ€§èƒ½ã€‚é€šè¿‡å¯¹å¤šç§é™æ€åˆ†æå·¥å…·ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œå½¢å¼éªŒè¯å·¥å…·çš„ç»¼åˆè¯„ä¼°ï¼Œå‘ç°ä¸åŒå·¥å…·åœ¨æ£€æµ‹ä¸åŒç§ç±»çš„å…¬å…±å¼±ç‚¹æ–¹é¢å…·æœ‰æ˜¾è‘—åŒºåˆ«ã€‚å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å°å‹ä»£ç ç‰‡æ®µæ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†éšç€ä»£ç é‡çš„å¢é•¿ï¼Œå…¶å‡†ç¡®æ€§ä¸‹é™ï¼Œå‡ºç°æ›´å¤šçš„è¯¯åˆ¤ã€‚æœ¬æ–‡ç»“æœå¯¹æœªæ¥å®‰å…¨è§£å†³æ–¹æ¡ˆä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹è§’è‰²æœ‰é‡è¦å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>CASTLEæ¡†æ¶ç”¨äºè¯„ä¼°æºä»£ç æ¼æ´æ£€æµ‹å·¥å…·çš„æ€§èƒ½ã€‚</li>
<li>è¯„ä¼°äº†å¤šç§é™æ€åˆ†æå·¥å…·ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œå½¢å¼éªŒè¯å·¥å…·ã€‚</li>
<li>å‘ç°å½¢å¼éªŒè¯å·¥å…·ESBMCåœ¨å‡å°‘è¯¯æŠ¥æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ£€æµ‹æŸäº›ç‰¹å®šæ¼æ´æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>é™æ€åˆ†æå·¥å…·è™½ç„¶ä¼šå‡ºç°é«˜è¯¯æŠ¥ç‡ï¼Œå¢åŠ äº†å¼€å‘è€…çš„æ‰‹åŠ¨éªŒè¯å·¥ä½œè´Ÿæ‹…ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å°å‹ä»£ç ç‰‡æ®µæ—¶è¡¨ç°å‡ºè‰¯å¥½çš„è¯†åˆ«æ¼æ´èƒ½åŠ›ï¼Œä½†éšç€ä»£ç è§„æ¨¡çš„å¢å¤§ï¼Œå…¶å‡†ç¡®æ€§ä¸‹é™ï¼Œäº§ç”Ÿæ›´å¤šçš„å¹»è§‰ç»“æœã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœªæ¥å®‰å…¨è§£å†³æ–¹æ¡ˆä¸­å¯èƒ½æ‰®æ¼”é‡è¦è§’è‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£ç è¡¥å…¨æ¡†æ¶ä¸­æä¾›å®æ—¶æŒ‡å¯¼ä»¥é˜²æ­¢æ¼æ´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09433">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-958dd88f56e53ddec3429fadbf3322c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6779c78f5ce29093a05384c452e1864b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bda835a092b21ac43efd822e29ba2626.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Got-Compute-but-No-Data-Lessons-From-Post-training-a-Finnish-LLM"><a href="#Got-Compute-but-No-Data-Lessons-From-Post-training-a-Finnish-LLM" class="headerlink" title="Got Compute, but No Data: Lessons From Post-training a Finnish LLM"></a>Got Compute, but No Data: Lessons From Post-training a Finnish LLM</h2><p><strong>Authors:Elaine Zosa, Ville Komulainen, Sampo Pyysalo</strong></p>
<p>As LLMs gain more popularity as chatbots and general assistants, methods have been developed to enable LLMs to follow instructions and align with human preferences. These methods have found success in the field, but their effectiveness has not been demonstrated outside of high-resource languages. In this work, we discuss our experiences in post-training an LLM for instruction-following for English and Finnish. We use a multilingual LLM to translate instruction and preference datasets from English to Finnish. We perform instruction tuning and preference optimization in English and Finnish and evaluate the instruction-following capabilities of the model in both languages. Our results show that with a few hundred Finnish instruction samples we can obtain competitive performance in Finnish instruction-following. We also found that although preference optimization in English offers some cross-lingual benefits, we obtain our best results by using preference data from both languages. We release our model, datasets, and recipes under open licenses at <a target="_blank" rel="noopener" href="https://huggingface.co/LumiOpen/Poro-34B-chat-OpenAssistant">https://huggingface.co/LumiOpen/Poro-34B-chat-OpenAssistant</a> </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨èŠå¤©æœºå™¨äººå’Œé€šç”¨åŠ©ç†ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå·²ç»å¼€å‘äº†ä¸€äº›æ–¹æ³•ä½¿LLMèƒ½å¤Ÿéµå¾ªæŒ‡ä»¤å¹¶ä¸äººç±»åå¥½ä¿æŒä¸€è‡´ã€‚è¿™äº›æ–¹æ³•åœ¨è¯¥é¢†åŸŸå·²ç»å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬åœ¨éé«˜èµ„æºè¯­è¨€ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å°šæœªå¾—åˆ°è¯æ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ†äº«äº†æˆ‘ä»¬åœ¨ä¸ºè‹±è¯­å’ŒèŠ¬å…°è¯­è¿›è¡ŒæŒ‡ä»¤éµå¾ªè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹åçš„ç»éªŒã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ç§å¤šè¯­è¨€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°†æŒ‡ä»¤å’Œåå¥½æ•°æ®é›†ä»è‹±è¯­ç¿»è¯‘æˆèŠ¬å…°è¯­ã€‚æˆ‘ä»¬åœ¨è‹±è¯­å’ŒèŠ¬å…°è¯­ä¸­è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´å’Œåå¥½ä¼˜åŒ–ï¼Œå¹¶è¯„ä¼°äº†è¯¥æ¨¡å‹åœ¨è¿™ä¸¤ç§è¯­è¨€ä¸­çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåªéœ€å‡ ç™¾ä¸ªèŠ¬å…°è¯­æŒ‡ä»¤æ ·æœ¬ï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨èŠ¬å…°è¯­çš„æŒ‡ä»¤éµå¾ªæ–¹é¢å–å¾—å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œå°½ç®¡è‹±è¯­ä¸­çš„åå¥½ä¼˜åŒ–æä¾›äº†ä¸€äº›è·¨è¯­è¨€çš„å¥½å¤„ï¼Œä½†æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ä¸¤ç§è¯­è¨€çš„åå¥½æ•°æ®è·å¾—äº†æœ€ä½³ç»“æœã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/LumiOpen/Poro-34B-chat-OpenAssistant%E4%B8%8B%E4%BB%A5%E5%BC%80%E6%94%BE%E8%AE%B8%E5%8F%AF%E8%AF%81%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84%E6%A8%A1%E5%9E%8B%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E9%85%8D%E6%96%B9%E3%80%82">https://huggingface.co/LumiOpen/Poro-34B-chat-OpenAssistantä¸‹ä»¥å¼€æ”¾è®¸å¯è¯å‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®é›†å’Œé…æ–¹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09407v1">PDF</a> 7 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»‹ç»LLMåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨åŠå…¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•é€‚åº”äººç±»åå¥½ã€‚é€šè¿‡å¯¹è‹±æ–‡å’ŒèŠ¬å…°è¯­çš„è®­ç»ƒæ•°æ®å’Œæ¨¡å‹çš„è¯„ä¼°ç»“æœè¿›è¡Œäº†è®¨è®ºï¼Œè¡¨æ˜LLMèƒ½å¤Ÿéµå¾ªæŒ‡ä»¤å¹¶å…·æœ‰è·¨æ–‡åŒ–èƒ½åŠ›ã€‚æœ€ä½³å®è·µæ˜¯ä½¿ç”¨ä¸¤ç§è¯­è¨€çš„åå¥½æ•°æ®ï¼Œå¹¶ä¸”ä»…é€šè¿‡æ•°ç™¾ä¸ªèŠ¬å…°æŒ‡ä»¤æ ·æœ¬å³å¯è·å¾—å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨èŠå¤©æœºå™¨äººå’Œé€šç”¨åŠ©ç†æ–¹é¢æ—¥ç›Šæ™®åŠï¼Œå¼€å‘äººå‘˜ä¸ºæ­¤å¼€å‘å‡ºèƒ½ä½¿LLMéµå¾ªæŒ‡ä»¤å’Œé€‚åº”äººç±»åå¥½çš„æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å·²ç»åœ¨é«˜èµ„æºè¯­è¨€é¢†åŸŸè¡¨ç°å‡ºå…¶æ•ˆæœï¼Œä½†åœ¨å…¶ä»–è¯­è¨€é¢†åŸŸå°šæœªå¾—åˆ°éªŒè¯ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨å¤šè¯­è¨€LLMå°†æŒ‡ä»¤å’Œåå¥½æ•°æ®é›†ä»è‹±è¯­ç¿»è¯‘æˆèŠ¬å…°è¯­ï¼Œå®ç°äº†å¯¹è‹±è¯­çš„è®­ç»ƒå’Œä¼˜åŒ–åï¼Œå¯¹æ¨¡å‹åœ¨èŠ¬å…°è¯­ä¸­çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºå°‘é‡èŠ¬å…°è¯­æŒ‡ä»¤æ ·æœ¬å°±èƒ½è·å¾—å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</li>
<li>è™½ç„¶è‹±è¯­åå¥½ä¼˜åŒ–æä¾›äº†ä¸€äº›è·¨è¯­è¨€ä¼˜åŠ¿ï¼Œä½†ä½¿ç”¨ä¸¤ç§è¯­è¨€çš„åå¥½æ•°æ®æ‰èƒ½è·å¾—æœ€ä½³ç»“æœã€‚è¿™è¡¨æ˜LLMçš„æ€§èƒ½å¯ä»¥é€šè¿‡é€‚åº”å¤šç§è¯­è¨€çš„åå¥½æ•°æ®æ¥è¿›ä¸€æ­¥æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09407">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5fb950bcae7e9ad0453b6d56242d2545.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf21e6eb2f8c60977cef3694ab742f58.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56cd64160ef969a430f18641e44a84ad.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Towards-Next-Generation-Recommender-Systems-A-Benchmark-for-Personalized-Recommendation-Assistant-with-LLMs"><a href="#Towards-Next-Generation-Recommender-Systems-A-Benchmark-for-Personalized-Recommendation-Assistant-with-LLMs" class="headerlink" title="Towards Next-Generation Recommender Systems: A Benchmark for   Personalized Recommendation Assistant with LLMs"></a>Towards Next-Generation Recommender Systems: A Benchmark for   Personalized Recommendation Assistant with LLMs</h2><p><strong>Authors:Jiani Huang, Shijie Wang, Liang-bo Ning, Wenqi Fan, Shuaiqiang Wang, Dawei Yin, Qing Li</strong></p>
<p>Recommender systems (RecSys) are widely used across various modern digital platforms and have garnered significant attention. Traditional recommender systems usually focus only on fixed and simple recommendation scenarios, making it difficult to generalize to new and unseen recommendation tasks in an interactive paradigm. Recently, the advancement of large language models (LLMs) has revolutionized the foundational architecture of RecSys, driving their evolution into more intelligent and interactive personalized recommendation assistants. However, most existing studies rely on fixed task-specific prompt templates to generate recommendations and evaluate the performance of personalized assistants, which limits the comprehensive assessments of their capabilities. This is because commonly used datasets lack high-quality textual user queries that reflect real-world recommendation scenarios, making them unsuitable for evaluating LLM-based personalized recommendation assistants. To address this gap, we introduce RecBench+, a new dataset benchmark designed to access LLMsâ€™ ability to handle intricate user recommendation needs in the era of LLMs. RecBench+ encompasses a diverse set of queries that span both hard conditions and soft preferences, with varying difficulty levels. We evaluated commonly used LLMs on RecBench+ and uncovered below findings: 1) LLMs demonstrate preliminary abilities to act as recommendation assistants, 2) LLMs are better at handling queries with explicitly stated conditions, while facing challenges with queries that require reasoning or contain misleading information. Our dataset has been released at <a target="_blank" rel="noopener" href="https://github.com/jiani-huang/RecBench.git">https://github.com/jiani-huang/RecBench.git</a>. </p>
<blockquote>
<p>æ¨èç³»ç»Ÿï¼ˆRecSysï¼‰åœ¨ç°ä»£å„ç§æ•°å­—å¹³å°ä¸Šå¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œå¹¶å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ä¼ ç»Ÿçš„æ¨èç³»ç»Ÿé€šå¸¸åªå…³æ³¨å›ºå®šå’Œç®€å•çš„æ¨èåœºæ™¯ï¼Œéš¾ä»¥æ¨å¹¿åˆ°äº¤äº’å¼èŒƒå¼ä¸­çš„æ–°å’Œæœªè§è¿‡çš„æ¨èä»»åŠ¡ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•å½»åº•æ”¹å˜äº†RecSysçš„åŸºç¡€æ¶æ„ï¼Œæ¨åŠ¨å…¶è¿›åŒ–ä¸ºæ›´æ™ºèƒ½ã€æ›´äº¤äº’çš„ä¸ªæ€§åŒ–æ¨èåŠ©æ‰‹ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ç ”ç©¶ä¾èµ–äºå›ºå®šçš„ä»»åŠ¡ç‰¹å®šæç¤ºæ¨¡æ¿æ¥ç”Ÿæˆæ¨èå¹¶è¯„ä¼°ä¸ªæ€§åŒ–åŠ©æ‰‹çš„æ€§èƒ½ï¼Œè¿™é™åˆ¶äº†å¯¹å…¶èƒ½åŠ›çš„å…¨é¢è¯„ä¼°ã€‚è¿™æ˜¯å› ä¸ºå¸¸ç”¨çš„æ•°æ®é›†ç¼ºä¹åæ˜ çœŸå®ä¸–ç•Œæ¨èåœºæ™¯çš„é«˜è´¨é‡æ–‡æœ¬ç”¨æˆ·æŸ¥è¯¢ï¼Œå› æ­¤ä¸é€‚åˆè¯„ä¼°åŸºäºLLMçš„ä¸ªæ€§åŒ–æ¨èåŠ©æ‰‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RecBench +ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ•°æ®é›†åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°LLMæ—¶ä»£LLMå¤„ç†å¤æ‚çš„ç”¨æˆ·æ¨èéœ€æ±‚çš„èƒ½åŠ›ã€‚RecBench+åŒ…å«æ¶µç›–ç¡¬æ¡ä»¶å’Œè½¯åå¥½ä»¥åŠä¸åŒéš¾åº¦çº§åˆ«çš„å„ç§æŸ¥è¯¢ã€‚æˆ‘ä»¬åœ¨RecBench+ä¸Šå¯¹å¸¸ç”¨çš„LLMè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å‘ç°äº†ä»¥ä¸‹å‘ç°ï¼š1ï¼‰LLMåˆæ­¥å…·å¤‡ä½œä¸ºæ¨èåŠ©æ‰‹çš„èƒ½åŠ›ï¼›2ï¼‰LLMåœ¨å¤„ç†æ˜ç¡®é™ˆè¿°çš„æ¡ä»¶æŸ¥è¯¢æ—¶è¡¨ç°è¾ƒå¥½ï¼Œè€Œåœ¨éœ€è¦æ¨ç†æˆ–åŒ…å«è¯¯å¯¼ä¿¡æ¯çš„æŸ¥è¯¢æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/jiani-huang/RecBench.git">https://github.com/jiani-huang/RecBench.git</a>å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09382v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†æ¨èç³»ç»Ÿï¼ˆRecSysï¼‰åœ¨ç°ä»£æ•°å­—å¹³å°ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºä¼ ç»Ÿæ¨èç³»ç»Ÿåœ¨æ–°å…´çš„äº’åŠ¨å‹æ¨èä»»åŠ¡ä¸Šå­˜åœ¨ç€å±€é™æ€§ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ï¼Œæ¨èç³»ç»Ÿæ­£æœç€æ™ºèƒ½åŒ–å’Œä¸ªæ€§åŒ–æ–¹å‘å‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶å¤§å¤šä¾èµ–äºå›ºå®šçš„ä»»åŠ¡ç‰¹å®šæç¤ºæ¨¡æ¿æ¥ç”Ÿæˆæ¨èå¹¶è¯„ä¼°ä¸ªæ€§åŒ–åŠ©ç†çš„æ€§èƒ½ï¼Œè¿™é™åˆ¶äº†å¯¹å…¶èƒ½åŠ›çš„å…¨é¢è¯„ä¼°ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†åŸºå‡†æµ‹è¯•RecBench+ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨å¤æ‚ç”¨æˆ·æ¨èéœ€æ±‚æ–¹é¢çš„èƒ½åŠ›ã€‚RecBench+åŒ…å«äº†ä¸åŒéš¾åº¦å±‚æ¬¡çš„æŸ¥è¯¢ï¼Œæ—¢æ¶µç›–ç¡¬æ¡ä»¶ä¹Ÿæ¶µç›–è½¯åå¥½ã€‚å¯¹å¸¸ç”¨çš„LLMsåœ¨RecBench+ä¸Šçš„è¯„ä¼°å‘ç°ï¼ŒLLMsæœ‰åˆæ­¥ä½œä¸ºæ¨èåŠ©ç†çš„èƒ½åŠ›ï¼Œåœ¨å¤„ç†æ˜ç¡®æ¡ä»¶çš„æŸ¥è¯¢æ—¶è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨éœ€è¦æ¨ç†æˆ–å«æœ‰è¯¯å¯¼ä¿¡æ¯çš„æŸ¥è¯¢æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨èç³»ç»Ÿï¼ˆRecSysï¼‰åœ¨ç°ä»£æ•°å­—å¹³å°ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†ä¼ ç»Ÿç³»ç»Ÿåœ¨å¤„ç†æ–°å…´äº’åŠ¨å‹æ¨èä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•æ¨åŠ¨äº†æ¨èç³»ç»Ÿçš„æ™ºèƒ½åŒ–å’Œä¸ªæ€§åŒ–å‘å±•ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¾èµ–å›ºå®šä»»åŠ¡ç‰¹å®šæç¤ºæ¨¡æ¿æ¥è¯„ä¼°ä¸ªæ€§åŒ–åŠ©ç†çš„æ€§èƒ½ï¼Œè¿™é™åˆ¶äº†å…¨é¢è¯„ä¼°ã€‚</li>
<li>å¼•å…¥æ–°çš„æ•°æ®é›†åŸºå‡†æµ‹è¯•RecBench+ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨å¤æ‚ç”¨æˆ·æ¨èéœ€æ±‚ä¸Šçš„èƒ½åŠ›ã€‚</li>
<li>RecBench+åŒ…å«äº†æ¶µç›–ç¡¬æ¡ä»¶å’Œè½¯åå¥½çš„ä¸åŒéš¾åº¦å±‚æ¬¡çš„æŸ¥è¯¢ã€‚</li>
<li>LLMsæœ‰åˆæ­¥ä½œä¸ºæ¨èåŠ©ç†çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09382">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-06e649bfa90a2cd81bb0718c34a72f8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c26603d658c58a30948ff46a487f0e9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae8bcc24b3c5a371c039c96884fc8314.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bd6d55970ab07aade5c43081264cf9a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ade6c6a5f5bd8217e675584a55fd8cf6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RetSTA-An-LLM-Based-Approach-for-Standardizing-Clinical-Fundus-Image-Reports"><a href="#RetSTA-An-LLM-Based-Approach-for-Standardizing-Clinical-Fundus-Image-Reports" class="headerlink" title="RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image   Reports"></a>RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image   Reports</h2><p><strong>Authors:Jiushen Cai, Weihang Zhang, Hanruo Liu, Ningli Wang, Huiqi Li</strong></p>
<p>Standardization of clinical reports is crucial for improving the quality of healthcare and facilitating data integration. The lack of unified standards, including format, terminology, and style, is a great challenge in clinical fundus diagnostic reports, which increases the difficulty for large language models (LLMs) to understand the data. To address this, we construct a bilingual standard terminology, containing fundus clinical terms and commonly used descriptions in clinical diagnosis. Then, we establish two models, RetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented dataset simulating clinical scenarios, demonstrates powerful standardization behaviors. However, it encounters a challenge of limitation to cover a wider range of diseases. To further enhance standardization performance, we build RetSTA-7B, which integrates a substantial amount of standardized data generated by RetSTA-7B-Zero along with corresponding English data, covering diverse complex clinical scenarios and achieving report-level standardization for the first time. Experimental results demonstrate that RetSTA-7B outperforms other compared LLMs in bilingual standardization task, which validates its superior performance and generalizability. The checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/AB-Story/RetSTA-7B">https://github.com/AB-Story/RetSTA-7B</a>. </p>
<blockquote>
<p>ä¸´åºŠæŠ¥å‘Šçš„æ ‡å‡†åŒ–å¯¹äºæé«˜åŒ»ç–—ä¿å¥è´¨é‡å¹¶ä¿ƒè¿›æ•°æ®æ•´åˆè‡³å…³é‡è¦ã€‚åœ¨ä¸´åºŠçœ¼åº•è¯Šæ–­æŠ¥å‘Šä¸­ï¼Œç”±äºç¼ºä¹ç»Ÿä¸€çš„æ ¼å¼ã€æœ¯è¯­å’Œé£æ ¼ç­‰æ ‡å‡†ï¼Œå­˜åœ¨å·¨å¤§çš„æŒ‘æˆ˜ã€‚è¿™ç»™å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç†è§£æ•°æ®å¢åŠ äº†éš¾åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒè¯­æ ‡å‡†æœ¯è¯­é›†ï¼ŒåŒ…å«çœ¼åº•ä¸´åºŠæœ¯è¯­å’Œä¸´åºŠè¯Šæ–­ä¸­å¸¸ç”¨çš„æè¿°ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸¤ä¸ªæ¨¡å‹ï¼Œå³RetSTA-7B-Zeroå’ŒRetSTA-7Bã€‚RetSTA-7B-Zeroåœ¨æ¨¡æ‹Ÿä¸´åºŠåœºæ™¯çš„å¢å¼ºæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æ ‡å‡†åŒ–è¡Œä¸ºã€‚ç„¶è€Œï¼Œå®ƒåœ¨è¦†ç›–æ›´å¹¿æ³›çš„ç–¾ç—…æ–¹é¢é‡åˆ°äº†æŒ‘æˆ˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ ‡å‡†åŒ–æ€§èƒ½ï¼Œæˆ‘ä»¬æ„å»ºäº†RetSTA-7Bï¼Œå®ƒé›†æˆäº†ç”±RetSTA-7B-Zeroç”Ÿæˆçš„æ ‡å‡†åŒ–æ•°æ®ä»¥åŠç›¸åº”çš„è‹±è¯­æ•°æ®ï¼Œè¦†ç›–å¤šç§å¤æ‚çš„ä¸´åºŠåœºæ™¯ï¼Œé¦–æ¬¡å®ç°äº†æŠ¥å‘Šçº§åˆ«çš„æ ‡å‡†åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRetSTA-7Båœ¨åŒè¯­æ ‡å‡†åŒ–ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–æ¯”è¾ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶å“è¶Šçš„æ€§èƒ½å’Œé€šç”¨æ€§ã€‚ç›¸å…³å…³é”®ç‚¹ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/AB-Story/RetSTA-7B">https://github.com/AB-Story/RetSTA-7B</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09358v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸´åºŠæŠ¥å‘Šæ ‡å‡†åŒ–å¯¹äºæé«˜åŒ»ç–—ä¿å¥è´¨é‡å’Œä¿ƒè¿›æ•°æ®æ•´åˆè‡³å…³é‡è¦ã€‚ç¼ºä¹ç»Ÿä¸€çš„æ ‡å‡†ï¼ŒåŒ…æ‹¬æ ¼å¼ã€æœ¯è¯­å’Œé£æ ¼ï¼Œæ˜¯ä¸´åºŠçœ¼åº•è¯Šæ–­æŠ¥å‘Šä¸­çš„ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ï¼Œè¿™ä¹Ÿå¢åŠ äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç†è§£æ•°æ®çš„éš¾åº¦ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«çœ¼åº•ä¸´åºŠæœ¯è¯­å’Œä¸´åºŠè¯Šæ–­ä¸­å¸¸ç”¨æè¿°çš„åŒè¯­æ ‡å‡†æœ¯è¯­ã€‚éšåï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸¤ä¸ªæ¨¡å‹ï¼šRetSTA-7B-Zeroå’ŒRetSTA-7Bã€‚RetSTA-7B-Zeroåœ¨æ¨¡æ‹Ÿä¸´åºŠåœºæ™¯çš„å¢å¼ºæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æ ‡å‡†åŒ–è¡Œä¸ºï¼Œä½†è¦†ç›–çš„ç–¾ç—…èŒƒå›´æœ‰é™ã€‚ä¸ºè¿›ä¸€æ­¥å¢å¼ºæ ‡å‡†åŒ–æ€§èƒ½ï¼Œæˆ‘ä»¬æ„å»ºäº†RetSTA-7Bï¼Œé›†æˆäº†ç”±RetSTA-7B-Zeroç”Ÿæˆçš„æ ‡å‡†åŒ–æ•°æ®åŠå…¶å¯¹åº”çš„è‹±æ–‡æ•°æ®ï¼Œè¦†ç›–å¤šç§å¤æ‚çš„ä¸´åºŠåœºæ™¯ï¼Œé¦–æ¬¡å®ç°äº†æŠ¥å‘Šçº§åˆ«çš„æ ‡å‡†åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRetSTA-7Båœ¨åŒè¯­æ ‡å‡†åŒ–ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å¯¹æ¯”çš„LLMï¼ŒéªŒè¯äº†å…¶å“è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸´åºŠæŠ¥å‘Šæ ‡å‡†åŒ–å¯¹åŒ»ç–—ä¿å¥è´¨é‡çš„æé«˜å’Œæ•°æ®æ•´åˆçš„ä¿ƒè¿›å…·æœ‰å…³é”®ä½œç”¨ã€‚</li>
<li>ç¼ºä¹ç»Ÿä¸€çš„æ ‡å‡†ï¼Œå¦‚æ ¼å¼ã€æœ¯è¯­å’Œé£æ ¼ï¼Œæ˜¯ä¸´åºŠçœ¼åº•è¯Šæ–­æŠ¥å‘Šçš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚<br>3.åŒè¯­æ ‡å‡†æœ¯è¯­çš„åˆ›å»ºæ˜¯è§£å†³è¿™ä¸€æŒ‘æˆ˜çš„é‡è¦æ­¥éª¤ï¼ŒåŒ…å«äº†çœ¼åº•ä¸´åºŠæœ¯è¯­å’Œä¸´åºŠè¯Šæ–­ä¸­çš„å¸¸ç”¨æè¿°ã€‚</li>
<li>RetSTA-7B-Zeroæ¨¡å‹å±•ç¤ºäº†å¼ºå¤§çš„æ ‡å‡†åŒ–èƒ½åŠ›ï¼Œä½†åœ¨ç–¾ç—…è¦†ç›–èŒƒå›´ä¸Šæœ‰æ‰€é™åˆ¶ã€‚</li>
<li>RetSTA-7Bæ¨¡å‹é€šè¿‡é›†æˆå¤§é‡æ ‡å‡†åŒ–æ•°æ®ï¼Œæ‰©å±•äº†ç–¾ç—…è¦†ç›–èŒƒå›´ï¼Œå¹¶é¦–æ¬¡å®ç°äº†æŠ¥å‘Šçº§åˆ«çš„æ ‡å‡†åŒ–ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜RetSTA-7Båœ¨åŒè¯­æ ‡å‡†åŒ–ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–LLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e9744f6858ab21d27f911948971731ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48db061a723beb80741db43f0ad90e31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fd4414cb2f9ca8bf51d665a33b799a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3dd7b61f1976e06fb3720f2530c7aaeb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Exo2Ego-Exocentric-Knowledge-Guided-MLLM-for-Egocentric-Video-Understanding"><a href="#Exo2Ego-Exocentric-Knowledge-Guided-MLLM-for-Egocentric-Video-Understanding" class="headerlink" title="Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video   Understanding"></a>Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video   Understanding</h2><p><strong>Authors:Haoyu Zhang, Qiaohui Chu, Meng Liu, Yunxiao Wang, Bin Wen, Fan Yang, Tingting Gao, Di Zhang, Yaowei Wang, Liqiang Nie</strong></p>
<p>AI personal assistants, deployed through robots or wearables, require embodied understanding to collaborate effectively with humans. Current Multimodal Large Language Models (MLLMs) primarily focus on third-person (exocentric) vision, overlooking the unique aspects of first-person (egocentric) videos. Additionally, high acquisition costs limit data size, impairing MLLM performance. To address these challenges, we propose learning the mapping between exocentric and egocentric domains, leveraging the extensive exocentric knowledge within existing MLLMs to enhance egocentric video understanding. To this end, we introduce Ego-ExoClip, a pre-training dataset comprising 1.1M synchronized ego-exo clip-text pairs derived from Ego-Exo4D. Our approach features a progressive training pipeline with three stages: Teacher Self-Preparation, Teacher-Student Guidance, and Student Self-Practice. Additionally, we propose an instruction-tuning data EgoIT from multiple sources to strengthen the modelâ€™s instruction-following capabilities, along with the EgoBench benchmark comprising eight different tasks for thorough evaluation. Extensive experiments across diverse egocentric tasks reveal that existing MLLMs perform inadequately in egocentric video understanding, while our model significantly outperforms these leading models. </p>
<blockquote>
<p>é€šè¿‡æœºå™¨äººæˆ–å¯ç©¿æˆ´è®¾å¤‡éƒ¨ç½²çš„äººå·¥æ™ºèƒ½ä¸ªäººåŠ©ç†ï¼Œéœ€è¦å®ä½“ç†è§£æ‰èƒ½ä¸äººç±»æœ‰æ•ˆåä½œã€‚å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸»è¦å…³æ³¨ç¬¬ä¸‰äººç§°ï¼ˆå¤–è§†ï¼‰è§†è§’ï¼Œå¿½ç•¥äº†ç¬¬ä¸€äººç§°ï¼ˆå†…è§†ï¼‰è§†é¢‘çš„ç‹¬ç‰¹æ–¹é¢ã€‚æ­¤å¤–ï¼Œé«˜æ˜‚çš„é‡‡é›†æˆæœ¬é™åˆ¶äº†æ•°æ®é‡ï¼Œå½±å“äº†MLLMçš„æ€§èƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å­¦ä¹ å¤–è§†å’Œå†…è§†é¢†åŸŸä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œåˆ©ç”¨ç°æœ‰MLLMä¸­çš„å¤§é‡å¤–è§†çŸ¥è¯†æ¥å¢å¼ºå¯¹å†…è§†è§†é¢‘çš„ç†è§£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†Ego-ExoClipé¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«ä»Ego-Exo4Dæ´¾ç”Ÿçš„110ä¸‡åŒæ­¥çš„ego-exoå‰ªè¾‘æ–‡æœ¬å¯¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç®¡é“ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šæ•™å¸ˆè‡ªæˆ‘å‡†å¤‡ã€æ•™å¸ˆå­¦ç”ŸæŒ‡å¯¼å’Œå­¦ç”Ÿè‡ªæˆ‘å®è·µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»å¤šä¸ªæ¥æºæå‡ºäº†ç”¨äºåŠ å¼ºæ¨¡å‹æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®EgoITï¼Œä»¥åŠåŒ…å«å…«ä¸ªä¸åŒä»»åŠ¡çš„EgoBenchåŸºå‡†æµ‹è¯•é›†è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚åœ¨å¤šç§ç¬¬ä¸€äººç§°ä»»åŠ¡çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„MLLMåœ¨å†…è§†è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œè€Œæˆ‘ä»¬çš„æ¨¡å‹æ˜¾è‘—ä¼˜äºè¿™äº›é¢†å…ˆæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09143v1">PDF</a> Project: <a target="_blank" rel="noopener" href="https://egovisiongroup.github.io/Exo2Ego.github.io/">https://egovisiongroup.github.io/Exo2Ego.github.io/</a></p>
<p><strong>Summary</strong><br>     æœºå™¨äººæˆ–å¯ç©¿æˆ´è®¾å¤‡ä¸Šçš„AIä¸ªäººåŠ©ç†éœ€è¦èº«ä½“æ„ŸçŸ¥æ¥ä¸äººç±»æœ‰æ•ˆåä½œã€‚å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸»è¦å…³æ³¨ç¬¬ä¸‰äººç§°è§†è§’ï¼ˆå¤–éƒ¨è§†è§’ï¼‰çš„è§†é‡ï¼Œå¿½ç•¥äº†ç¬¬ä¸€äººç§°è§†è§’ï¼ˆå†…éƒ¨è§†è§’ï¼‰çš„ç‹¬ç‰¹æ€§ã€‚æ­¤å¤–ï¼Œé«˜æ˜‚çš„é‡‡é›†æˆæœ¬é™åˆ¶äº†æ•°æ®é‡ï¼Œå½±å“äº†MLLMçš„æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†å­¦ä¹ å¤–éƒ¨è§†è§’å’Œå†…éƒ¨è§†è§’é¢†åŸŸä¹‹é—´çš„æ˜ å°„å…³ç³»çš„æ–¹æ³•ï¼Œåˆ©ç”¨ç°æœ‰çš„MLLMä¸­çš„å¤–éƒ¨è§†è§’çŸ¥è¯†å¢å¼ºå†…éƒ¨è§†è§’è§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†Ego-ExoClipé¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«ä»Ego-Exo4Dæ´¾ç”Ÿçš„110ä¸‡åŒæ­¥è‡ªæˆ‘å¤–éƒ¨å‰ªè¾‘æ–‡æœ¬å¯¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç®¡é“ï¼ŒåŒ…æ‹¬æ•™å¸ˆè‡ªæˆ‘å‡†å¤‡ã€æ•™å¸ˆå­¦ç”ŸæŒ‡å¯¼å’Œå­¦ç”Ÿè‡ªæˆ‘å®è·µä¸‰ä¸ªé˜¶æ®µã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†æ¥è‡ªå¤šä¸ªæ¥æºçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®EgoITï¼Œä»¥åŠ å¼ºæ¨¡å‹çš„æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ï¼Œä»¥åŠåŒ…å«å…«ä¸ªä¸åŒä»»åŠ¡çš„EgoBenchåŸºå‡†æµ‹è¯•ç”¨äºå…¨é¢è¯„ä¼°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œç°æœ‰MLLMåœ¨å†…éƒ¨è§†è§’è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°ä¸è¶³ï¼Œè€Œæˆ‘ä»¬çš„æ¨¡å‹æ˜¾è‘—ä¼˜äºè¿™äº›é¢†å…ˆæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIä¸ªäººåŠ©ç†éœ€è¦èº«ä½“æ„ŸçŸ¥æ¥ä¸äººç±»åä½œã€‚</li>
<li>å½“å‰MLLMä¸»è¦å…³æ³¨ç¬¬ä¸‰äººç§°è§†è§’ï¼ˆå¤–éƒ¨è§†è§’ï¼‰ï¼Œå¿½ç•¥ç¬¬ä¸€äººç§°è§†è§’ï¼ˆå†…éƒ¨è§†è§’ï¼‰ã€‚</li>
<li>æ•°æ®é‡‡é›†æˆæœ¬é«˜é™åˆ¶äº†MLLMçš„æ€§èƒ½æå‡ã€‚</li>
<li>æå‡ºå­¦ä¹ å¤–éƒ¨è§†è§’å’Œå†…éƒ¨è§†è§’é¢†åŸŸæ˜ å°„çš„æ–¹æ³•ã€‚</li>
<li>å¼•å…¥Ego-ExoClipé¢„è®­ç»ƒæ•°æ®é›†ç”¨äºå¢å¼ºå†…éƒ¨è§†è§’è§†é¢‘ç†è§£èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç®¡é“æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e78efcd71dd3c20e4361915764922d23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b45e083b7d2925bed0ae315ac739666.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-258d6194034dd56c81793faef1b14ce3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-188e58234f29947ef48d8b4b7baa6afd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32f843066099c1f6d7714658dc8a1b73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c641c62c8bdb0d0ad13ebbfc960a574.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Teaching-LLMs-How-to-Learn-with-Contextual-Fine-Tuning"><a href="#Teaching-LLMs-How-to-Learn-with-Contextual-Fine-Tuning" class="headerlink" title="Teaching LLMs How to Learn with Contextual Fine-Tuning"></a>Teaching LLMs How to Learn with Contextual Fine-Tuning</h2><p><strong>Authors:Younwoo Choi, Muhammad Adil Asif, Ziwen Han, John Willes, Rahul G. Krishnan</strong></p>
<p>Prompting Large Language Models (LLMs), or providing context on the expected model of operation, is an effective way to steer the outputs of such models to satisfy human desiderata after they have been trained. But in rapidly evolving domains, there is often need to fine-tune LLMs to improve either the kind of knowledge in their memory or their abilities to perform open ended reasoning in new domains. When humanâ€™s learn new concepts, we often do so by linking the new material that we are studying to concepts we have already learned before. To that end, we ask, â€œcan prompting help us teach LLMs how to learnâ€. In this work, we study a novel generalization of instruction tuning, called contextual fine-tuning, to fine-tune LLMs. Our method leverages instructional prompts designed to mimic human cognitive strategies in learning and problem-solving to guide the learning process during training, aiming to improve the modelâ€™s interpretation and understanding of domain-specific knowledge. We empirically demonstrate that this simple yet effective modification improves the ability of LLMs to be fine-tuned rapidly on new datasets both within the medical and financial domains. </p>
<blockquote>
<p>æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆ–æä¾›é¢„æœŸçš„æ¨¡å‹æ“ä½œä¸Šä¸‹æ–‡ï¼Œæ˜¯å¼•å¯¼æ­¤ç±»æ¨¡å‹çš„è¾“å‡ºä»¥æ»¡è¶³äººç±»éœ€æ±‚çš„æœ‰æ•ˆæ–¹æ³•ï¼Œè¿™äº›æ¨¡å‹åœ¨è®­ç»ƒåã€‚ä½†åœ¨å¿«é€Ÿå‘å±•çš„é¢†åŸŸä¸­ï¼Œé€šå¸¸éœ€è¦å¾®è°ƒLLMï¼Œä»¥æ”¹å–„å…¶å†…å­˜ä¸­çš„çŸ¥è¯†ç±»å‹æˆ–åœ¨æ–°çš„é¢†åŸŸè¿›è¡Œå¼€æ”¾å¼æ¨ç†çš„èƒ½åŠ›ã€‚å½“äººç±»å­¦ä¹ æ–°æ¦‚å¿µæ—¶ï¼Œæˆ‘ä»¬å¸¸å¸¸é€šè¿‡æŠŠæˆ‘ä»¬æ­£åœ¨ç ”ç©¶çš„æ–°ææ–™ä¸ä¹‹å‰å·²ç»å­¦è¿‡çš„æ¦‚å¿µè”ç³»èµ·æ¥æ¥å­¦ä¹ ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºçš„é—®é¢˜æ˜¯ï¼šâ€œæç¤ºèƒ½å¦å¸®åŠ©æˆ‘ä»¬æ•™ä¼šLLMå¦‚ä½•å­¦ä¹ â€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§æŒ‡ä»¤å¾®è°ƒçš„æ–°æ³›åŒ–æ–¹æ³•ï¼Œç§°ä¸ºä¸Šä¸‹æ–‡å¾®è°ƒï¼Œä»¥å¾®è°ƒLLMã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æŒ‡ä»¤æç¤ºæ¥æ¨¡ä»¿äººç±»åœ¨å­¦ä¹ å’Œè§£å†³é—®é¢˜ä¸­çš„è®¤çŸ¥ç­–ç•¥ï¼Œä»¥æŒ‡å¯¼è®­ç»ƒè¿‡ç¨‹ä¸­çš„å­¦ä¹ è¿‡ç¨‹ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹å¯¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„è§£é‡Šå’Œç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬ä»å®è¯ä¸Šè¯æ˜ï¼Œè¿™ç§ç®€å•è€Œæœ‰æ•ˆçš„æ”¹è¿›æé«˜äº†LLMåœ¨æ–°æ•°æ®é›†ä¸Šçš„å¿«é€Ÿå¾®è°ƒèƒ½åŠ›ï¼Œæ— è®ºæ˜¯åœ¨åŒ»ç–—é¢†åŸŸè¿˜æ˜¯é‡‘èé¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09032v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong><br>     é€šè¿‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæç¤ºæˆ–æä¾›æ“ä½œé¢„æœŸæ¨¡å‹ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥æœ‰æ•ˆå¼•å¯¼æ¨¡å‹è®­ç»ƒåçš„è¾“å‡ºä»¥æ»¡è¶³äººç±»çš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œåœ¨å¿«é€Ÿå‘å±•çš„é¢†åŸŸä¸­ï¼Œå¾€å¾€éœ€è¦å¾®è°ƒLLMï¼Œä»¥æé«˜å…¶è®°å¿†ä¸­çš„çŸ¥è¯†æˆ–åœ¨æ–°é¢†åŸŸè¿›è¡Œå¼€æ”¾å¼æ¨ç†çš„èƒ½åŠ›ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§åä¸ºâ€œä¸Šä¸‹æ–‡å¾®è°ƒâ€çš„æ–°å‹æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡è®¾è®¡æ¨¡ä»¿äººç±»å­¦ä¹ å’Œé—®é¢˜è§£å†³è®¤çŸ¥ç­–ç•¥çš„æŒ‡ä»¤æç¤ºæ¥æŒ‡å¯¼å­¦ä¹ è¿‡ç¨‹ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹å¯¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„ç†è§£å’Œè§£é‡Šèƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•èƒ½å¤Ÿæ”¹è¿›LLMåœ¨æ–°æ•°æ®é›†ä¸Šçš„å¿«é€Ÿå¾®è°ƒèƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨åŒ»ç–—å’Œé‡‘èé¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šè¿‡æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯æœ‰æ•ˆå¼•å¯¼æ¨¡å‹è¾“å‡ºä»¥æ»¡è¶³äººç±»éœ€æ±‚ã€‚</li>
<li>åœ¨å¿«é€Ÿå‘å±•çš„é¢†åŸŸä¸­ï¼Œéœ€è¦å¾®è°ƒLLMä»¥é€‚åº”æ–°çŸ¥è¯†å’Œæ–°é¢†åŸŸã€‚</li>
<li>ä¸Šä¸‹æ–‡å¾®è°ƒæ˜¯ä¸€ç§æ–°å‹çš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡è®¾è®¡æ¨¡ä»¿äººç±»å­¦ä¹ å’Œé—®é¢˜è§£å†³è®¤çŸ¥ç­–ç•¥çš„æŒ‡ä»¤æç¤ºæ¥æŒ‡å¯¼å­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>ä¸Šä¸‹æ–‡å¾®è°ƒæ–¹æ³•æ—¨åœ¨æé«˜æ¨¡å‹å¯¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„ç†è§£å’Œè§£é‡Šèƒ½åŠ›ã€‚</li>
<li>ä¸Šä¸‹æ–‡å¾®è°ƒèƒ½å¤Ÿæ”¹è¿›LLMåœ¨æ–°æ•°æ®é›†ä¸Šçš„å¿«é€Ÿå¾®è°ƒèƒ½åŠ›ã€‚</li>
<li>è¿™ç§æ–¹æ³•åœ¨åŒ»ç–—å’Œé‡‘èé¢†åŸŸå–å¾—äº†å®è¯æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0432d32251c9933395c3001794c693da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfe698d678a658ff6c1fc5568dfebab1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="OpenRAG-Optimizing-RAG-End-to-End-via-In-Context-Retrieval-Learning"><a href="#OpenRAG-Optimizing-RAG-End-to-End-via-In-Context-Retrieval-Learning" class="headerlink" title="OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning"></a>OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning</h2><p><strong>Authors:Jiawei Zhou, Lei Chen</strong></p>
<p>In this paper, we analyze and empirically show that the learned relevance for conventional information retrieval (IR) scenarios may be inconsistent in retrieval-augmented generation (RAG) scenarios. To bridge this gap, we introduce OpenRAG, a RAG framework that is optimized end-to-end by tuning the retriever to capture in-context relevance, enabling adaptation to the diverse and evolving needs. Extensive experiments across a wide range of tasks demonstrate that OpenRAG, by tuning a retriever end-to-end, leads to a consistent improvement of 4.0% over the original retriever, consistently outperforming existing state-of-the-art retrievers by 2.1%. Additionally, our results indicate that for some tasks, an end-to-end tuned 0.2B retriever can achieve improvements that surpass those of RAG-oriented or instruction-tuned 8B large language models (LLMs), highlighting the cost-effectiveness of our approach in enhancing RAG systems. </p>
<blockquote>
<p>æœ¬æ–‡åˆ†æå’Œå®è¯è¡¨æ˜ï¼Œä¼ ç»Ÿä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰åœºæ™¯ä¸­å­¦ä¹ çš„ç›¸å…³æ€§å¯èƒ½åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœºæ™¯ä¸­ä¸ä¸€è‡´ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†OpenRAGï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è°ƒæ•´æ£€ç´¢å™¨æ¥æ•è·ä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„ç«¯åˆ°ç«¯ä¼˜åŒ–çš„RAGæ¡†æ¶ï¼Œèƒ½å¤Ÿé€‚åº”å¤šæ ·ä¸”ä¸æ–­å‘å±•çš„éœ€æ±‚ã€‚åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œé€šè¿‡ç«¯åˆ°ç«¯è°ƒæ•´æ£€ç´¢å™¨ï¼ŒOpenRAGåœ¨åŸå§‹æ£€ç´¢å™¨çš„åŸºç¡€ä¸Šå®ç°äº†4.0%çš„æŒç»­æ”¹è¿›ï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„æ£€ç´¢å™¨ï¼Œé¢†å…ˆäº†2.1%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¯¹äºæŸäº›ä»»åŠ¡ï¼Œç«¯åˆ°ç«¯è°ƒæ•´çš„0.2Bæ£€ç´¢å™¨çš„æ”¹è¿›ç”šè‡³è¶…è¿‡äº†é¢å‘RAGæˆ–æŒ‡ä»¤è°ƒæ•´çš„8Bå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¿™çªæ˜¾äº†æˆ‘ä»¬æ–¹æ³•åœ¨å¢å¼ºRAGç³»ç»Ÿä¸­çš„æˆæœ¬æ•ˆç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08398v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºå¹¶åˆ†æä¼ ç»Ÿçš„ä¿¡æ¯æ£€ç´¢åœºæ™¯åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœºæ™¯ä¸­å¯èƒ½å­˜åœ¨çš„ä¸ä¸€è‡´æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œå¼•å…¥äº†OpenRAGæ¡†æ¶ï¼Œé€šè¿‡ç«¯åˆ°ç«¯ä¼˜åŒ–æ£€ç´¢å™¨æ¥æ•æ‰ä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼Œé€‚åº”å¤šæ ·åŒ–å’Œä¸æ–­å‘å±•çš„éœ€æ±‚ã€‚å®éªŒè¯æ˜ï¼ŒOpenRAGç›¸è¾ƒäºåŸå§‹æ£€ç´¢å™¨æœ‰4.0%çš„ä¸€è‡´æ€§æå‡ï¼Œç›¸è¾ƒäºç°æœ‰çš„å…ˆè¿›æ£€ç´¢å™¨æœ‰2.1%çš„æå‡ã€‚æ­¤å¤–ï¼Œå¯¹äºæŸäº›ä»»åŠ¡ï¼Œç«¯åˆ°ç«¯ä¼˜åŒ–çš„è¾ƒå°è§„æ¨¡æ£€ç´¢å™¨çš„æ”¹è¿›ç”šè‡³è¶…è¿‡äº†é¢å‘RAGæˆ–æŒ‡ä»¤ä¼˜åŒ–çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œçªæ˜¾äº†å¢å¼ºRAGç³»ç»Ÿæˆæœ¬æ•ˆç›Šçš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿçš„ä¿¡æ¯æ£€ç´¢åœºæ™¯åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœºæ™¯ä¸­å¯èƒ½å­˜åœ¨ä¸ä¸€è‡´æ€§ã€‚</li>
<li>OpenRAGæ˜¯ä¸€ä¸ªä¼˜åŒ–çš„RAGæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç«¯åˆ°ç«¯ä¼˜åŒ–æ£€ç´¢å™¨æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>OpenRAGæ¡†æ¶é€šè¿‡æ•æ‰ä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼Œé€‚åº”äº†å¤šæ ·åŒ–å’Œä¸æ–­å‘å±•çš„éœ€æ±‚ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOpenRAGç›¸è¾ƒäºåŸå§‹æ£€ç´¢å™¨æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>OpenRAGç›¸è¾ƒäºç°æœ‰å…ˆè¿›æ£€ç´¢å™¨ä¹Ÿæœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>å¯¹äºæŸäº›ä»»åŠ¡ï¼Œä¼˜åŒ–åçš„è¾ƒå°è§„æ¨¡æ£€ç´¢å™¨çš„æ”¹è¿›ç”šè‡³è¶…è¿‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08398">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bb1dbdeb1d58056449ebd2365b699873.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67e8cfb6c5d7010ac7947608c38d6c7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6919a1c27f050afa0f6440140af692dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-966b4673ab7573d37754621ce0d3acf0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GPT-PPG-A-GPT-based-Foundation-Model-for-Photoplethysmography-Signals"><a href="#GPT-PPG-A-GPT-based-Foundation-Model-for-Photoplethysmography-Signals" class="headerlink" title="GPT-PPG: A GPT-based Foundation Model for Photoplethysmography Signals"></a>GPT-PPG: A GPT-based Foundation Model for Photoplethysmography Signals</h2><p><strong>Authors:Zhaoliang Chen, Cheng Ding, Saurabh Kataria, Runze Yan, Minxiao Wang, Randall Lee, Xiao Hu</strong></p>
<p>This study introduces a novel application of a Generative Pre-trained Transformer (GPT) model tailored for photoplethysmography (PPG) signals, serving as a foundation model for various downstream tasks. Adapting the standard GPT architecture to suit the continuous characteristics of PPG signals, our approach demonstrates promising results. Our models are pre-trained on our extensive dataset that contains more than 200 million 30s PPG samples. We explored different supervised fine-tuning techniques to adapt our model to downstream tasks, resulting in performance comparable to or surpassing current state-of-the-art (SOTA) methods in tasks like atrial fibrillation detection. A standout feature of our GPT model is its inherent capability to perform generative tasks such as signal denoising effectively, without the need for further fine-tuning. This success is attributed to the generative nature of the GPT framework. </p>
<blockquote>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å…‰ä½“ç§¯æè®°æ³•ï¼ˆPPGï¼‰ä¿¡å·çš„æ–°å‹åº”ç”¨ï¼Œå³ç”Ÿæˆå¼é¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆGPTï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸ºå„ç§ä¸‹æ¸¸ä»»åŠ¡æä¾›äº†åŸºç¡€æ¨¡å‹ã€‚é€šè¿‡è°ƒæ•´æ ‡å‡†GPTæ¶æ„ä»¥é€‚åº”PPGä¿¡å·çš„è¿ç»­ç‰¹æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„ç»“æœã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨åŒ…å«è¶…è¿‡2000ä¸‡ä»½30ç§’PPGæ ·æœ¬çš„åºå¤§æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸åŒçš„ç›‘ç£å¾®è°ƒæŠ€æœ¯ï¼Œä»¥ä½¿æˆ‘ä»¬çš„æ¨¡å‹é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ï¼Œä»è€Œåœ¨è¯¸å¦‚å¿ƒæˆ¿é¢¤åŠ¨æ£€æµ‹ç­‰ä»»åŠ¡çš„æ€§èƒ½ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šäº†å½“å‰æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æˆ‘ä»¬çš„GPTæ¨¡å‹çš„ä¸€ä¸ªçªå‡ºç‰¹ç‚¹æ˜¯ï¼Œå®ƒå…·æœ‰æ‰§è¡Œç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚ä¿¡å·å»å™ªï¼‰çš„å›ºæœ‰èƒ½åŠ›ï¼Œå¹¶ä¸”ä¸éœ€è¦è¿›ä¸€æ­¥çš„å¾®è°ƒã€‚è¿™ä¸€æˆåŠŸå½’å› äºGPTæ¡†æ¶çš„ç”Ÿæˆæ€§è´¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08015v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å…‰ä½“ç§¯è„‰ææ³¢ä¿¡å·ï¼ˆPPGï¼‰çš„æ–°å‹ç”Ÿæˆé¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆGPTï¼‰æ¨¡å‹çš„åº”ç”¨ï¼Œè¯¥æ¨¡å‹å¯ä½œä¸ºå„ç§ä¸‹æ¸¸ä»»åŠ¡çš„åŸºç¡€æ¨¡å‹ã€‚é€šè¿‡è°ƒæ•´æ ‡å‡†GPTæ¶æ„ä»¥é€‚åº”PPGä¿¡å·çš„è¿ç»­ç‰¹æ€§ï¼Œè¯¥ç ”ç©¶å±•ç°å‡ºä»¤äººé¼“èˆçš„ç»“æœã€‚è¯¥æ¨¡å‹åœ¨åŒ…å«è¶…è¿‡20äº¿ä¸ª30ç§’PPGæ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚ç ”ç©¶è¿˜æ¢ç´¢äº†ä¸åŒçš„ç›‘ç£å¾®è°ƒæŠ€æœ¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”ä¸‹æ¸¸ä»»åŠ¡ï¼Œåœ¨æˆ¿é¢¤æ£€æµ‹ç­‰ä»»åŠ¡ä¸­çš„æ€§èƒ½è¾¾åˆ°æˆ–è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ°´å¹³ã€‚GPTæ¨¡å‹çš„ä¸€ä¸ªçªå‡ºç‰¹ç‚¹æ˜¯å…¶å›ºæœ‰çš„ç”Ÿæˆä»»åŠ¡èƒ½åŠ›ï¼Œå¦‚ä¿¡å·å»å™ªï¼Œæ— éœ€è¿›ä¸€æ­¥å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äººå‘˜æˆåŠŸå°†ç”Ÿæˆé¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆGPTï¼‰æ¨¡å‹åº”ç”¨äºå…‰ä½“ç§¯è„‰ææ³¢ä¿¡å·ï¼ˆPPGï¼‰ã€‚</li>
<li>GPTæ¨¡å‹ç»è¿‡å¾®è°ƒä»¥é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚æˆ¿é¢¤æ£€æµ‹ï¼Œæ€§èƒ½å“è¶Šã€‚</li>
<li>æ¨¡å‹åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ŒåŒ…å«è¶…è¿‡20äº¿ä¸ªPPGæ ·æœ¬ã€‚</li>
<li>GPTæ¨¡å‹çš„ç”Ÿæˆæ€§è´¨ä½¿å…¶èƒ½å¤Ÿæ‰§è¡Œç”Ÿæˆä»»åŠ¡ï¼Œå¦‚ä¿¡å·å»å™ªã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºå¤šç§ä¸‹æ¸¸ä»»åŠ¡æä¾›äº†åŸºç¡€æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹è¿ç»­é€‚åº”æ€§å¼ºï¼Œèƒ½å¤Ÿé€‚åº”PPGä¿¡å·çš„è¿ç»­ç‰¹æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08015">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2443f5f69ced566e7b2344260a6fae41.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Fully-Autonomous-Programming-using-Iterative-Multi-Agent-Debugging-with-Large-Language-Models"><a href="#Fully-Autonomous-Programming-using-Iterative-Multi-Agent-Debugging-with-Large-Language-Models" class="headerlink" title="Fully Autonomous Programming using Iterative Multi-Agent Debugging with   Large Language Models"></a>Fully Autonomous Programming using Iterative Multi-Agent Debugging with   Large Language Models</h2><p><strong>Authors:Anastasiia Grishina, Vadim Liventsev, Aki HÃ¤rmÃ¤, Leon Moonen</strong></p>
<p>Program synthesis with Large Language Models (LLMs) suffers from a â€œnear-miss syndromeâ€: the generated code closely resembles a correct solution but fails unit tests due to minor errors. We address this with a multi-agent framework called Synthesize, Execute, Instruct, Debug, and Repair (SEIDR). Effectively applying SEIDR to instruction-tuned LLMs requires determining (a) optimal prompts for LLMs, (b) what ranking algorithm selects the best programs in debugging rounds, and (c) balancing the repair of unsuccessful programs with the generation of new ones. We empirically explore these trade-offs by comparing replace-focused, repair-focused, and hybrid debug strategies. We also evaluate lexicase and tournament selection to rank candidates in each generation. On Program Synthesis Benchmark 2 (PSB2), our framework outperforms both conventional use of OpenAI Codex without a repair phase and traditional genetic programming approaches. SEIDR outperforms the use of an LLM alone, solving 18 problems in C++ and 20 in Python on PSB2 at least once across experiments. To assess generalizability, we employ GPT-3.5 and Llama 3 on the PSB2 and HumanEval-X benchmarks. Although SEIDR with these models does not surpass current state-of-the-art methods on the Python benchmarks, the results on HumanEval-C++ are promising. SEIDR with Llama 3-8B achieves an average pass@100 of 84.2%. Across all SEIDR runs, 163 of 164 problems are solved at least once with GPT-3.5 in HumanEval-C++, and 162 of 164 with the smaller Llama 3-8B. We conclude that SEIDR effectively overcomes the near-miss syndrome in program synthesis with LLMs. </p>
<blockquote>
<p>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç¨‹åºåˆæˆæ—¶ï¼Œå­˜åœ¨ä¸€ç§â€œè¿‘ä¼¼ç»¼åˆå¾â€ï¼šç”Ÿæˆçš„ä»£ç ä¸æ­£ç¡®è§£å†³æ–¹æ¡ˆéå¸¸ç›¸ä¼¼ï¼Œä½†ç”±äºå¾®å°é”™è¯¯è€Œæ— æ³•é€šè¿‡å•å…ƒæµ‹è¯•ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªåä¸ºâ€œåˆæˆã€æ‰§è¡Œã€æŒ‡ä»¤ã€è°ƒè¯•å’Œä¿®å¤ï¼ˆSEIDRï¼‰â€çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å°†SEIDRæœ‰æ•ˆåº”ç”¨äºæŒ‡ä»¤è°ƒæ•´LLMï¼Œéœ€è¦ç¡®å®šï¼ˆaï¼‰é’ˆå¯¹LLMçš„æœ€ä½³æç¤ºï¼Œï¼ˆbï¼‰åœ¨è°ƒè¯•è½®ä¸­é€‰æ‹©æœ€ä½³ç¨‹åºçš„æ’åç®—æ³•ï¼Œä»¥åŠï¼ˆcï¼‰å¹³è¡¡ä¸æˆåŠŸç¨‹åºçš„ä¿®å¤å’Œæ–°ç¨‹åºçš„ç”Ÿæˆã€‚æˆ‘ä»¬é€šè¿‡æ¯”è¾ƒä»¥æ›¿æ¢ä¸ºé‡ç‚¹ã€ä»¥ä¿®å¤ä¸ºé‡ç‚¹ä»¥åŠæ··åˆè°ƒè¯•ç­–ç•¥ï¼Œå®è¯åœ°æ¢ç´¢äº†è¿™äº›æƒè¡¡ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†è¯å…¸é€‰æ‹©æ³•å’Œé”¦æ ‡èµ›é€‰æ‹”æ³•ä»¥åœ¨æ¯ä¸€ä»£ä¸­æŒ‘é€‰å€™é€‰è€…ã€‚åœ¨ç¨‹åºåˆæˆåŸºå‡†æµ‹è¯•ç¬¬äºŒç‰ˆï¼ˆPSB2ï¼‰ä¸Šï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¼˜äºå¸¸è§„ä½¿ç”¨OpenAI Codexè€Œä¸è¿›è¡Œä¿®å¤é˜¶æ®µä»¥åŠä¼ ç»Ÿçš„é—ä¼ ç¼–ç¨‹æ–¹æ³•ã€‚SEIDRåœ¨å•ç‹¬ä½¿ç”¨LLMæ—¶è¡¨ç°å‡ºè‰²ï¼Œåœ¨PSB2çš„C++å’ŒPythonä¸­åˆ†åˆ«è§£å†³äº†è‡³å°‘ä¸€æ¬¡å®éªŒçš„18ä¸ªé—®é¢˜å’Œ20ä¸ªé—®é¢˜ã€‚ä¸ºäº†è¯„ä¼°é€šç”¨æ€§ï¼Œæˆ‘ä»¬åœ¨PSB2å’ŒHumanEval-XåŸºå‡†æµ‹è¯•ä¸Šä½¿ç”¨äº†GPT-3.5å’ŒLlama 3ã€‚å°½ç®¡ä½¿ç”¨è¿™äº›æ¨¡å‹çš„SEIDRåœ¨PythonåŸºå‡†æµ‹è¯•ä¸Šå¹¶æœªè¶…è¶Šå½“å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œä½†åœ¨HumanEval-C++ä¸Šçš„ç»“æœå…·æœ‰å¸Œæœ›ã€‚ä½¿ç”¨Llama 3-8Bçš„SEIDRè¾¾åˆ°å¹³å‡é€šè¿‡ç‡ä¸º84.2%ã€‚åœ¨æ‰€æœ‰SEIDRè¿è¡Œä¸­ï¼ŒGPT-3.5åœ¨HumanEval-C++ä¸­è‡³å°‘è§£å†³äº†ä¸€æ¬¡ä»¥ä¸Šå…¶ä¸­çš„æ‰€æœ‰é—®é¢˜ä¸­çš„è‡³å°‘è§£å†³è¿‡æœ‰ç­”æ¡ˆçš„å‰ç½®é¢˜ï¼ˆpass@100ï¼‰æœ‰åå…­åˆ†ä¹‹ä¸‰ï¼ˆå³è‡³å°‘è§£å†³äº†åå…­é¢˜ä¸­çš„åå…­é¢˜ï¼‰ï¼Œè¾ƒå°çš„Llama 3-8Bè§£å†³äº†ä¸€å…­äºŒé¢˜ä¸­çš„åå…­é¢˜ä»¥ä¸Šã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼ŒSEIDRæœ‰æ•ˆåœ°å…‹æœäº†ä½¿ç”¨LLMè¿›è¡Œç¨‹åºåˆæˆæ—¶çš„è¿‘å¤±è¯¯é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07693v1">PDF</a> Accepted for publication in ACM Trans. Evol. Learn. Optim., February   2025. arXiv admin note: text overlap with arXiv:2304.10423</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¨‹åºåˆæˆå—åˆ°â€œè¿‘ä¼¼ç»¼åˆå¾â€çš„å½±å“ï¼Œå³ç”Ÿæˆçš„ä»£ç è™½ç„¶ä¸æ­£ç¡®è§£å†³æ–¹æ¡ˆç›¸ä¼¼ï¼Œä½†ç”±äºç»†å¾®é”™è¯¯è€Œæ— æ³•é€šè¿‡å•å…ƒæµ‹è¯•ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºSEIDRï¼ˆåˆæˆã€æ‰§è¡Œã€æŒ‡ä»¤ã€è°ƒè¯•å’Œä¿®å¤ï¼‰çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚æœ‰æ•ˆåº”ç”¨SEIDRåˆ°æŒ‡ä»¤è°ƒæ•´LLMéœ€è¦ç¡®å®šï¼ˆaï¼‰LLMçš„æœ€ä½³æç¤ºï¼Œï¼ˆbï¼‰åœ¨è°ƒè¯•è½®æ¬¡ä¸­é€‰æ‹©æœ€ä½³ç¨‹åºçš„æ’åç®—æ³•ï¼Œä»¥åŠï¼ˆcï¼‰å¹³è¡¡ä¸æˆåŠŸç¨‹åºçš„ä¿®å¤ä¸æ–°ç”Ÿæˆçš„ç¨‹åºä¹‹é—´çš„å¹³è¡¡ã€‚æˆ‘ä»¬é€šè¿‡æ¯”è¾ƒæ³¨é‡æ›¿æ¢ã€æ³¨é‡ä¿®å¤å’Œæ··åˆè°ƒè¯•ç­–ç•¥æ¥å®è¯æ¢ç´¢è¿™äº›æƒè¡¡ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†è¯å…¸é€‰æ‹©å’Œé”¦æ ‡èµ›é€‰æ‹©æ¥æ’åˆ—æ¯ä¸€ä»£çš„å€™é€‰äººã€‚åœ¨ç¨‹åºåˆæˆåŸºå‡†æµ‹è¯•2ï¼ˆPSB2ï¼‰ä¸Šï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¼˜äºå¸¸è§„ä½¿ç”¨OpenAI Codexè€Œä¸è¿›è¡Œä¿®å¤é˜¶æ®µä»¥åŠä¼ ç»Ÿçš„é—ä¼ ç¼–ç¨‹æ–¹æ³•ã€‚SEIDRä¼˜äºå•ç‹¬ä½¿ç”¨LLMï¼Œåœ¨PSB2ä¸Šè§£å†³C++çš„18ä¸ªé—®é¢˜å’ŒPythonçš„20ä¸ªé—®é¢˜ã€‚ä¸ºäº†è¯„ä¼°é€šç”¨æ€§ï¼Œæˆ‘ä»¬åœ¨PSB2å’ŒHumanEval-XåŸºå‡†æµ‹è¯•ä¸Šä½¿ç”¨äº†GPT-3.5å’ŒLlama 3ã€‚è™½ç„¶SEIDRä¸è¿™äº›æ¨¡å‹çš„ç»“åˆåœ¨PythonåŸºå‡†æµ‹è¯•ä¸Šå¹¶æœªè¶…è¶Šå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä½†åœ¨HumanEval-C++ä¸Šçš„ç»“æœå¾ˆæœ‰å¸Œæœ›ã€‚ä½¿ç”¨Llama 3-8Bçš„SEIDRè¾¾åˆ°å¹³å‡é€šè¿‡ç‡ä¸º84.2%ã€‚åœ¨æ‰€æœ‰SEIDRè¿è¡Œä¸­ï¼ŒGPT-3.5åœ¨HumanEval-C++ä¸­è‡³å°‘è§£å†³äº†ä¸€æ¬¡çš„163ä¸ªé—®é¢˜å’Œè¾ƒå°çš„Llama 3-8Bè§£å†³çš„è‡³å°‘ä¸€æ¬¡çš„162ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼ŒSEIDRæœ‰æ•ˆåœ°å…‹æœäº†åŸºäºLLMçš„ç¨‹åºåˆæˆçš„è¿‘ä¼¼ç»¼åˆå¾é—®é¢˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SEIDRæ¡†æ¶è§£å†³äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¨‹åºåˆæˆä¸­çš„â€œè¿‘ä¼¼ç»¼åˆå¾â€é—®é¢˜ï¼Œå³é€šè¿‡ç”Ÿæˆçš„ä»£ç è™½è¿‘ä¼¼æ­£ç¡®è§£ä½†æ— æ³•é€šè¿‡å•å…ƒæµ‹è¯•çš„é—®é¢˜ã€‚</li>
<li>SEIDRåº”ç”¨çš„å…³é”®è¦ç´ åŒ…æ‹¬ç¡®å®šæœ€ä½³æç¤ºã€é€‰æ‹©æ’åç®—æ³•çš„è°ƒè¯•è½®æ¬¡ä¸­æœ€ä½³ç¨‹åºçš„ç­–ç•¥ï¼Œä»¥åŠå¹³è¡¡ä¿®å¤ä¸ç”Ÿæˆæ–°ç¨‹åºä¹‹é—´çš„ç­–ç•¥ã€‚</li>
<li>é€šè¿‡æ¯”è¾ƒä¸åŒçš„è°ƒè¯•ç­–ç•¥ï¼ˆå¦‚æ³¨é‡æ›¿æ¢ã€æ³¨é‡ä¿®å¤å’Œæ··åˆç­–ç•¥ï¼‰ï¼Œå®è¯æ¢ç´¢äº†è¿™äº›æƒè¡¡ã€‚</li>
<li>SEIDRåœ¨ç¨‹åºåˆæˆåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå¸¸è§„æ–¹æ³•å’Œé—ä¼ ç¼–ç¨‹æ–¹æ³•ã€‚</li>
<li>SEIDRåœ¨ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-3.5å’ŒLlama 3ï¼‰ä¸Šçš„è¡¨ç°æ˜¾ç¤ºå‡ºè‰¯å¥½çš„é€šç”¨æ€§ï¼Œå°¤å…¶åœ¨HumanEval-C++ä¸Šçš„ç»“æœæœ‰æ½œåŠ›ã€‚</li>
<li>SEIDRä½¿ç”¨Llama 3-8Båœ¨HumanEval-C++çš„åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°çš„å¹³å‡é€šè¿‡ç‡ä¸º84.2%ï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-85cce17d57fddf1b38507efc2310aed6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6231532fff3ab088e70108844ee4b47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-931da4bdc3eea788ca701cea0bd2eba6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73cf226bab88f258c76865a38dd20540.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Implicit-Reasoning-in-Transformers-is-Reasoning-through-Shortcuts"><a href="#Implicit-Reasoning-in-Transformers-is-Reasoning-through-Shortcuts" class="headerlink" title="Implicit Reasoning in Transformers is Reasoning through Shortcuts"></a>Implicit Reasoning in Transformers is Reasoning through Shortcuts</h2><p><strong>Authors:Tianhe Lin, Jian Xie, Siyu Yuan, Deqing Yang</strong></p>
<p>Test-time compute is emerging as a new paradigm for enhancing language modelsâ€™ complex multi-step reasoning capabilities, as demonstrated by the success of OpenAIâ€™s o1 and o3, as well as DeepSeekâ€™s R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on unfixed-pattern data tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization. </p>
<blockquote>
<p>æµ‹è¯•æ—¶çš„è®¡ç®—æ­£æˆä¸ºä¸€ç§æ–°å…´çš„æ¨¡å¼ï¼Œç”¨äºå¢å¼ºè¯­è¨€æ¨¡å‹çš„å¤æ‚å¤šæ­¥æ¨ç†èƒ½åŠ›ï¼ŒOpenAIçš„o1å’Œo3ä»¥åŠDeepSeekçš„R1çš„æˆåŠŸæ¼”ç¤ºäº†è¿™ä¸€ç‚¹ã€‚ä¸æµ‹è¯•æ—¶è®¡ç®—ä¸­çš„æ˜¾å¼æ¨ç†ç›¸æ¯”ï¼Œéšå¼æ¨ç†çš„æ¨ç†æ•ˆç‡æ›´é«˜ï¼Œç”Ÿæˆçš„æ ‡è®°æ›´å°‘ã€‚ç„¶è€Œï¼Œä¸ºä»€ä¹ˆå…ˆè¿›çš„æ¨ç†èƒ½åŠ›æ— æ³•åœ¨éšå¼æ¨ç†é£æ ¼ä¸­äº§ç”Ÿå‘¢ï¼Ÿåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»é›¶å¼€å§‹è®­ç»ƒGPT-2ï¼Œä½¿å…¶é€‚åº”ç²¾é€‰çš„å¤šæ­¥æ•°å­¦æ¨ç†æ•°æ®é›†ï¼Œå¹¶è¿›è¡Œåˆ†æå®éªŒï¼Œä»¥ç ”ç©¶è¯­è¨€æ¨¡å‹åœ¨å¤šæ­¥ä»»åŠ¡ä¸­è¿›è¡Œéšå¼æ¨ç†çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼š1ï¼‰è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡éšå¼æ¨ç†è¿›è¡Œé€æ­¥æ¨ç†ï¼Œå¹¶åœ¨åŸŸå†…å’ŒåŸŸå¤–æµ‹è¯•ä¸­å®ç°é«˜å‡†ç¡®æ€§ã€‚ä½†è¿™ç§èƒ½åŠ›ä»…åœ¨è®­ç»ƒå›ºå®šæ¨¡å¼æ•°æ®æ—¶å‡ºç°ã€‚2ï¼‰ç›¸åï¼Œä»è®­ç»ƒéå›ºå®šæ¨¡å¼æ•°æ®ä¸­å‡ºç°çš„éšå¼æ¨ç†èƒ½åŠ›å¾€å¾€å€¾å‘äºè¿‡åº¦é€‚åº”ç‰¹å®šæ¨¡å¼ï¼Œè€Œæ— æ³•è¿›ä¸€æ­¥æ¨å¹¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸€å±€é™æ€§ä¹Ÿè¢«è§‚å¯Ÿåˆ°å­˜åœ¨äºæœ€å…ˆè¿›çš„è‡ªç„¶è¯­è¨€æ¨¡å‹ä¸­ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œè¯­è¨€æ¨¡å‹é€šè¿‡æ·å¾„å­¦ä¹ è·å¾—éšå¼æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å…·æœ‰ç›¸ä¼¼æ¨¡å¼çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å´ç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07604v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æµ‹è¯•æ—¶è®¡ç®—ï¼ˆtest-time computeï¼‰è¿™ä¸€æ–°å…´èŒƒå¼åœ¨å¢å¼ºè¯­è¨€æ¨¡å‹å¤æ‚å¤šæ­¥æ¨ç†èƒ½åŠ›æ–¹é¢çš„åº”ç”¨ã€‚é€šè¿‡å¯¹GPT-2è¿›è¡Œè®­ç»ƒå’Œå®éªŒåˆ†æï¼Œç ”ç©¶å‘ç°åœ¨å›ºå®šæ¨¡å¼æ•°æ®ä¸Šè®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡éšå¼æ¨ç†å®ç°é€æ­¥æ¨ç†ï¼Œå¹¶åœ¨åŸŸå†…å’ŒåŸŸå¤–æµ‹è¯•ä¸­è·å¾—é«˜å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œåœ¨éæ ‡å®šæ¨¡å¼æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹å¾€å¾€ä¼šå‡ºç°è¿‡åº¦æ‹Ÿåˆç‰¹å®šæ¨¡å¼çš„æƒ…å†µï¼Œç¼ºä¹è¿›ä¸€æ­¥çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™è¡¨æ˜è¯­è¨€æ¨¡å‹çš„éšå¼æ¨ç†èƒ½åŠ›æ˜¯é€šè¿‡æ·å¾„å­¦ä¹ è·å¾—çš„ï¼Œèƒ½å¤Ÿåœ¨å…·æœ‰ç›¸ä¼¼æ¨¡å¼çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶è®¡ç®—æ˜¯ä¸€ç§æ–°å…´èŒƒå¼ï¼Œç”¨äºå¢å¼ºè¯­è¨€æ¨¡å‹çš„å¤æ‚å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚</li>
<li>éšå¼æ¨ç†ç›¸æ¯”æ˜¾å¼æ¨ç†æ›´æ¨ç†é«˜æ•ˆï¼Œéœ€è¦ç”Ÿæˆçš„æ ‡è®°æ›´å°‘ã€‚</li>
<li>è¯­è¨€æ¨¡å‹åœ¨å›ºå®šæ¨¡å¼æ•°æ®ä¸Šè®­ç»ƒåï¼Œå¯ä»¥é€šè¿‡éšå¼æ¨ç†è¿›è¡Œé€æ­¥æ¨ç†ï¼Œå¹¶åœ¨æµ‹è¯•ä¸­è·å¾—é«˜å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨éæ ‡å®šæ¨¡å¼æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ä¼šå‡ºç°è¿‡åº¦æ‹Ÿåˆç‰¹å®šæ¨¡å¼çš„æƒ…å†µï¼Œç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>éšå¼æ¨ç†èƒ½åŠ›æ˜¯é€šè¿‡æ·å¾„å­¦ä¹ è·å¾—çš„ï¼Œèƒ½å¤Ÿåœ¨å…·æœ‰ç›¸ä¼¼æ¨¡å¼çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚</li>
<li>åœ¨ä¸åŒé¢†åŸŸï¼ˆå¦‚æ•°å­¦ï¼‰çš„éšå¼æ¨ç†ç ”ç©¶ä¸­å‘ç°äº†ç±»ä¼¼çš„è¯­è¨€æ¨¡å‹é™åˆ¶å’Œå±€é™ã€‚è¿™ç§æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜å¯èƒ½å½±å“è¯­è¨€æ¨¡å‹åœ¨å…¶ä»–å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7321e2424716906cd8925eff5706f8ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f18e45a5bc348f7a33db54aec4d1a127.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2da07d60a628e76629c6cb199ab2ac39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36ef7753475442622205a6d532211666.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da4ae7ec347e475a6ab350797ce3cbef.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Filter-Images-First-Generate-Instructions-Later-Pre-Instruction-Data-Selection-for-Visual-Instruction-Tuning"><a href="#Filter-Images-First-Generate-Instructions-Later-Pre-Instruction-Data-Selection-for-Visual-Instruction-Tuning" class="headerlink" title="Filter Images First, Generate Instructions Later: Pre-Instruction Data   Selection for Visual Instruction Tuning"></a>Filter Images First, Generate Instructions Later: Pre-Instruction Data   Selection for Visual Instruction Tuning</h2><p><strong>Authors:Bardia Safaei, Faizan Siddiqui, Jiacong Xu, Vishal M. Patel, Shao-Yuan Lo</strong></p>
<p>Visual instruction tuning (VIT) for large vision-language models (LVLMs) requires training on expansive datasets of image-instruction pairs, which can be costly. Recent efforts in VIT data selection aim to select a small subset of high-quality image-instruction pairs, reducing VIT runtime while maintaining performance comparable to full-scale training. However, a major challenge often overlooked is that generating instructions from unlabeled images for VIT is highly expensive. Most existing VIT datasets rely heavily on human annotations or paid services like the GPT API, which limits users with constrained resources from creating VIT datasets for custom applications. To address this, we introduce Pre-Instruction Data Selection (PreSel), a more practical data selection paradigm that directly selects the most beneficial unlabeled images and generates instructions only for the selected images. PreSel first estimates the relative importance of each vision task within VIT datasets to derive task-wise sampling budgets. It then clusters image features within each task, selecting the most representative images with the budget. This approach reduces computational overhead for both instruction generation during VIT data formation and LVLM fine-tuning. By generating instructions for only 15% of the images, PreSel achieves performance comparable to full-data VIT on the LLaVA-1.5 and Vision-Flan datasets. The link to our project page: <a target="_blank" rel="noopener" href="https://bardisafa.github.io/PreSel">https://bardisafa.github.io/PreSel</a> </p>
<blockquote>
<p>è§†è§‰æŒ‡ä»¤è°ƒä¼˜ï¼ˆVITï¼‰å¯¹äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰éœ€è¦åœ¨å¤§é‡çš„å›¾åƒæŒ‡ä»¤å¯¹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™å¯èƒ½ä¼šå¾ˆæ˜‚è´µã€‚è¿‘æœŸå…³äºVITæ•°æ®é€‰æ‹©çš„åŠªåŠ›æ—¨åœ¨é€‰æ‹©ä¸€å°éƒ¨åˆ†é«˜è´¨é‡çš„å›¾åƒæŒ‡ä»¤å¯¹ï¼Œåœ¨å‡å°‘VITè¿è¡Œæ—¶é—´çš„åŒæ—¶ä¿æŒä¸å…¨è§„æ¨¡è®­ç»ƒç›¸å½“çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç»å¸¸è¢«å¿½è§†çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯ï¼Œä»æœªæ ‡è®°çš„å›¾åƒä¸­ç”ŸæˆæŒ‡ä»¤çš„æˆæœ¬éå¸¸é«˜ã€‚å¤§å¤šæ•°ç°æœ‰çš„VITæ•°æ®é›†ä¸¥é‡ä¾èµ–äºäººå·¥æ³¨é‡Šæˆ–å¦‚GPT APIä¹‹ç±»çš„ä»˜è´¹æœåŠ¡ï¼Œè¿™é™åˆ¶äº†èµ„æºæœ‰é™çš„ç”¨æˆ·åˆ›å»ºç”¨äºè‡ªå®šä¹‰åº”ç”¨ç¨‹åºçš„VITæ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢„æŒ‡ä»¤æ•°æ®é€‰æ‹©ï¼ˆPreSelï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ›´å®ç”¨çš„æ•°æ®é€‰æ‹©æ¨¡å¼ï¼Œå®ƒç›´æ¥é€‰æ‹©æœ€æœ‰ç›Šçš„æ— æ ‡ç­¾å›¾åƒï¼Œåªä¸ºæ‰€é€‰å›¾åƒç”ŸæˆæŒ‡ä»¤ã€‚PreSelé¦–å…ˆä¼°è®¡VITæ•°æ®é›†ä¸­æ¯ä¸ªè§†è§‰ä»»åŠ¡çš„ç›¸å¯¹é‡è¦æ€§ï¼Œä»¥å¾—å‡ºä»»åŠ¡çº§é‡‡æ ·é¢„ç®—ã€‚ç„¶åï¼Œå®ƒåœ¨æ¯ä¸ªä»»åŠ¡å†…å¯¹å›¾åƒç‰¹å¾è¿›è¡Œèšç±»ï¼Œé€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„å›¾åƒä»¥ç¬¦åˆé¢„ç®—ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†VITæ•°æ®å½¢æˆå’ŒLVLMå¾®è°ƒè¿‡ç¨‹ä¸­çš„æŒ‡ä»¤ç”Ÿæˆè®¡ç®—å¼€é”€ã€‚åªä¸º15%çš„å›¾åƒç”ŸæˆæŒ‡ä»¤ï¼ŒPreSelåœ¨LLaVA-1.5å’ŒVision-Flanæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸å…¨æ•°æ®VITç›¸å½“ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://bardisafa.github.io/PreSel">https://bardisafa.github.io/PreSel</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07591v1">PDF</a> Accepted at Computer Vision and Pattern Recognition Conference (CVPR)   2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†è§‰æŒ‡ä»¤å¾®è°ƒï¼ˆVITï¼‰éœ€è¦å¤§é‡å›¾åƒ-æŒ‡ä»¤å¯¹æ•°æ®é›†å¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è¿›è¡Œè®­ç»ƒï¼Œæˆæœ¬è¾ƒé«˜ã€‚æœ€æ–°ç ”ç©¶è‡´åŠ›äºé€‰æ‹©é«˜è´¨é‡çš„å›¾åƒ-æŒ‡ä»¤å¯¹æ•°æ®å­é›†ï¼Œä»¥åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å‡å°‘VITçš„è¿è¡Œæ—¶é—´ã€‚ç„¶è€Œï¼Œç»å¸¸è¢«å¿½è§†çš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ä»æ— æ ‡ç­¾å›¾åƒä¸­ç”ŸæˆæŒ‡ä»¤çš„æˆæœ¬é«˜æ˜‚ã€‚å¤§å¤šæ•°ç°æœ‰çš„VITæ•°æ®é›†ä¸¥é‡ä¾èµ–äºäººå·¥æ³¨é‡Šæˆ–æœ‰å¿æœåŠ¡ï¼ˆå¦‚GPT APIï¼‰ï¼Œè¿™é™åˆ¶äº†èµ„æºå—é™çš„ç”¨æˆ·åˆ›å»ºè‡ªå®šä¹‰åº”ç”¨çš„VITæ•°æ®é›†ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢„æŒ‡ä»¤æ•°æ®é€‰æ‹©ï¼ˆPreSelï¼‰è¿™ä¸€æ›´å®é™…çš„æ•°æ®é€‰æ‹©èŒƒå¼ï¼Œè¯¥èŒƒå¼ç›´æ¥é€‰æ‹©æœ€æœ‰ç›Šçš„æ— æ ‡ç­¾å›¾åƒï¼Œåªä¸ºæ‰€é€‰å›¾åƒç”ŸæˆæŒ‡ä»¤ã€‚PreSelé¦–å…ˆä¼°è®¡VITæ•°æ®é›†ä¸­æ¯ä¸ªè§†è§‰ä»»åŠ¡çš„é‡è¦æ€§ï¼Œä»¥å¯¼å‡ºä»»åŠ¡ç‰¹å®šçš„é‡‡æ ·é¢„ç®—ã€‚ç„¶åå®ƒåœ¨æ¯ä¸ªä»»åŠ¡ä¸­å¯¹å›¾åƒç‰¹å¾è¿›è¡Œèšç±»ï¼Œé€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„å›¾åƒæ¥ä½¿ç”¨é¢„ç®—ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†åœ¨å½¢æˆVITæ•°æ®å’Œå¾®è°ƒLVLMæœŸé—´çš„æŒ‡ä»¤ç”Ÿæˆè®¡ç®—å¼€é”€ã€‚åªä¸ºå›¾åƒçš„15%ç”ŸæˆæŒ‡ä»¤ï¼ŒPreSelåœ¨LLaVA-1.5å’ŒVision-Flanæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸å…¨æ•°æ®VITç›¸å½“ã€‚é¡¹ç›®é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://bardisafa.github.io/PreSel%E3%80%82">https://bardisafa.github.io/PreSelã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VITéœ€è¦å¤§é‡çš„å›¾åƒ-æŒ‡ä»¤å¯¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æœ€æ–°ç ”ç©¶è‡´åŠ›äºé€‰æ‹©é«˜è´¨é‡çš„å›¾åƒ-æŒ‡ä»¤å¯¹æ•°æ®å­é›†ï¼Œä»¥å‡å°‘è¿è¡Œæ—¶é—´å¹¶ä¿æŒæ€§èƒ½ã€‚</li>
<li>ç”ŸæˆæŒ‡ä»¤çš„æˆæœ¬æ˜¯ä»æ— æ ‡ç­¾å›¾åƒä¸­ç”ŸæˆæŒ‡ä»¤çš„ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>å¤§å¤šæ•°ç°æœ‰æ–¹æ³•é«˜åº¦ä¾èµ–äºäººå·¥æ³¨é‡Šæˆ–æœ‰å¿æœåŠ¡ï¼Œé™åˆ¶äº†èµ„æºå—é™ç”¨æˆ·çš„è‡ªå®šä¹‰åº”ç”¨ã€‚</li>
<li>PreSelç›´æ¥é€‰æ‹©æœ€æœ‰ç›Šçš„æ— æ ‡ç­¾å›¾åƒï¼Œå¹¶ä¸ºæ‰€é€‰å›¾åƒç”ŸæˆæŒ‡ä»¤ï¼Œé™ä½è®¡ç®—å¼€é”€ã€‚</li>
<li>PreSelé€šè¿‡ä¼°è®¡æ¯ä¸ªè§†è§‰ä»»åŠ¡çš„é‡è¦æ€§æ¥å¯¼å‡ºä»»åŠ¡ç‰¹å®šçš„é‡‡æ ·é¢„ç®—ï¼Œé€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„å›¾åƒã€‚</li>
<li>ä»…å¯¹å°‘é‡å›¾åƒç”ŸæˆæŒ‡ä»¤ï¼ŒPreSelçš„æ€§èƒ½ä¸å…¨æ•°æ®VITç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa170626bf00ee708fa1e6a942043708.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93ec5db65b826b27d696e3a99ffda4de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e265abccdd703087bfb59e5a7564ce27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f68ee10db0da8de33097a4f4f36d45f7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="GRITHopper-Decomposition-Free-Multi-Hop-Dense-Retrieval"><a href="#GRITHopper-Decomposition-Free-Multi-Hop-Dense-Retrieval" class="headerlink" title="GRITHopper: Decomposition-Free Multi-Hop Dense Retrieval"></a>GRITHopper: Decomposition-Free Multi-Hop Dense Retrieval</h2><p><strong>Authors:Justus-Jonas Erker, Nils Reimers, Iryna Gurevych</strong></p>
<p>Decomposition-based multi-hop retrieval methods rely on many autoregressive steps to break down complex queries, which breaks end-to-end differentiability and is computationally expensive. Decomposition-free methods tackle this, but current decomposition-free approaches struggle with longer multi-hop problems and generalization to out-of-distribution data. To address these challenges, we introduce GRITHopper-7B, a novel multi-hop dense retrieval model that achieves state-of-the-art performance on both in-distribution and out-of-distribution benchmarks. GRITHopper combines generative and representational instruction tuning by integrating causal language modeling with dense retrieval training. Through controlled studies, we find that incorporating additional context after the retrieval process, referred to as post-retrieval language modeling, enhances dense retrieval performance. By including elements such as final answers during training, the model learns to better contextualize and retrieve relevant information. GRITHopper-7B offers a robust, scalable, and generalizable solution for multi-hop dense retrieval, and we release it to the community for future research and applications requiring multi-hop reasoning and retrieval capabilities. </p>
<blockquote>
<p>åŸºäºåˆ†è§£çš„å¤šè·³æ£€ç´¢æ–¹æ³•ä¾èµ–äºè®¸å¤šè‡ªå›å½’æ­¥éª¤æ¥åˆ†è§£å¤æ‚æŸ¥è¯¢ï¼Œè¿™ç ´åäº†ç«¯åˆ°ç«¯çš„å¯åŒºåˆ†æ€§ï¼Œå¹¶ä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æ— åˆ†è§£çš„æ–¹æ³•è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œä½†å½“å‰çš„æ— åˆ†è§£æ–¹æ³•åœ¨å¤„ç†è¾ƒé•¿çš„å¤šè·³é—®é¢˜å’Œæ³›åŒ–åˆ°åˆ†å¸ƒå¤–æ•°æ®æ—¶é‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GRITHopper-7Bï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šè·³å¯†é›†æ£€ç´¢æ¨¡å‹ï¼Œåœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–çš„åŸºå‡†æµ‹è¯•ä¸­å‡å®ç°äº†æœ€æ–°æ€§èƒ½ã€‚GRITHopperé€šè¿‡ç»“åˆå› æœè¯­è¨€å»ºæ¨¡å’Œå¯†é›†æ£€ç´¢è®­ç»ƒï¼Œå®ç°äº†ç”Ÿæˆå’Œä»£è¡¨æ€§æŒ‡ä»¤è°ƒæ•´ã€‚é€šè¿‡å¯¹ç…§ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°ï¼Œåœ¨æ£€ç´¢è¿‡ç¨‹åèå…¥é¢å¤–çš„ä¸Šä¸‹æ–‡ï¼Œå³æ‰€è°“çš„åæ£€ç´¢è¯­è¨€å»ºæ¨¡ï¼Œå¯ä»¥å¢å¼ºå¯†é›†æ£€ç´¢çš„æ€§èƒ½ã€‚é€šè¿‡åœ¨è®­ç»ƒä¸­åŠ å…¥æœ€ç»ˆç­”æ¡ˆç­‰å…ƒç´ ï¼Œæ¨¡å‹å­¦ä¼šäº†æ›´å¥½åœ°ä¸Šä¸‹æ–‡åŒ–å’Œæ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚GRITHopper-7Bä¸ºå¤šè·³å¯†é›†æ£€ç´¢æä¾›äº†ç¨³å¥ã€å¯æ‰©å±•å’Œé€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬å°†å…¶å‘å¸ƒç»™ç¤¾åŒºï¼Œä»¥ä¾›æœªæ¥ç ”ç©¶å’Œéœ€è¦å¤šè·³æ¨ç†å’Œæ£€ç´¢èƒ½åŠ›çš„åº”ç”¨ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07519v1">PDF</a> Under Review at ACL Rolling Review (ARR)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåˆ†è§£çš„å¤šè·³æ£€ç´¢æ–¹æ³•ä¾èµ–å¤šä¸ªè‡ªå›å½’æ­¥éª¤æ¥åˆ†è§£å¤æ‚æŸ¥è¯¢ï¼Œè¿™ç ´åäº†ç«¯åˆ°ç«¯çš„å¯åŒºåˆ†æ€§ï¼Œå¹¶ä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æ— åˆ†è§£æ–¹æ³•è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œä½†å½“å‰çš„æ— åˆ†è§£æ–¹æ³•å¯¹äºè¾ƒé•¿çš„å¤šè·³é—®é¢˜å’Œæ³›åŒ–åˆ°ç¦»ç¾¤æ•°æ®æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GRITHopper-7Bï¼Œä¸€ç§æ–°å‹å¤šè·³å¯†é›†æ£€ç´¢æ¨¡å‹ï¼Œåœ¨å†…å¤–åˆ†å¸ƒåŸºå‡†æµ‹è¯•ä¸Šå‡å®ç°äº†å“è¶Šæ€§èƒ½ã€‚GRITHopperé€šè¿‡æ•´åˆå› æœè¯­è¨€å»ºæ¨¡ä¸å¯†é›†æ£€ç´¢è®­ç»ƒï¼Œå®ç°äº†ç”Ÿæˆæ€§å’Œä»£è¡¨æ€§æŒ‡ä»¤è°ƒæ•´ã€‚é€šè¿‡å¯¹ç…§ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°ï¼Œåœ¨æ£€ç´¢è¿‡ç¨‹åå¢åŠ é¢å¤–çš„ä¸Šä¸‹æ–‡ï¼Œå³æ‰€è°“çš„åæ£€ç´¢è¯­è¨€å»ºæ¨¡ï¼Œå¯ä»¥å¢å¼ºå¯†é›†æ£€ç´¢çš„æ€§èƒ½ã€‚é€šè¿‡åœ¨è®­ç»ƒä¸­åŒ…å«æœ€ç»ˆç­”æ¡ˆç­‰å…ƒç´ ï¼Œæ¨¡å‹å­¦ä¼šäº†æ›´å¥½åœ°ä¸Šä¸‹æ–‡åŒ–å’Œæ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚GRITHopper-7Bä¸ºéœ€è¦å¤šè·³æ¨ç†å’Œæ£€ç´¢èƒ½åŠ›çš„æœªæ¥ç ”ç©¶ä¸åº”ç”¨æä¾›äº†ç¨³å¥ã€å¯æ‰©å±•å’Œé€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºåˆ†è§£çš„å¤šè·³æ£€ç´¢æ–¹æ³•å­˜åœ¨ç«¯åˆ°ç«¯ä¸å¯åŒºåˆ†æ€§å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚</li>
<li>æ— åˆ†è§£æ–¹æ³•è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œä½†åœ¨å¤„ç†è¾ƒé•¿å¤šè·³é—®é¢˜å’Œæ³›åŒ–åˆ°ç¦»ç¾¤æ•°æ®æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>GRITHopper-7Bæ˜¯ä¸€ä¸ªæ–°å‹å¤šè·³å¯†é›†æ£€ç´¢æ¨¡å‹ï¼Œå…·æœ‰å¼ºå¤§çš„æ€§èƒ½å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
<li>GRITHopperç»“åˆäº†ç”Ÿæˆæ€§å’Œä»£è¡¨æ€§æŒ‡ä»¤è°ƒæ•´ï¼Œé€šè¿‡æ•´åˆå› æœè¯­è¨€å»ºæ¨¡ä¸å¯†é›†æ£€ç´¢è®­ç»ƒå®ç°é«˜æ€§èƒ½ã€‚</li>
<li>åæ£€ç´¢è¯­è¨€å»ºæ¨¡èƒ½å¢å¼ºå¯†é›†æ£€ç´¢æ€§èƒ½ã€‚</li>
<li>åœ¨è®­ç»ƒä¸­åŠ å…¥æœ€ç»ˆç­”æ¡ˆç­‰å…ƒç´ ä½¿æ¨¡å‹èƒ½æ›´ç²¾å‡†åœ°ä¸Šä¸‹æ–‡åŒ–å’Œæ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07519">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b3c078aa23bc59c7f06f491a7f07de8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e8b1055296fbc479baf67d437260c02.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5fc485531a1ecfaa51ecf8caab999f1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CalliReader-Contextualizing-Chinese-Calligraphy-via-an-Embedding-Aligned-Vision-Language-Model"><a href="#CalliReader-Contextualizing-Chinese-Calligraphy-via-an-Embedding-Aligned-Vision-Language-Model" class="headerlink" title="CalliReader: Contextualizing Chinese Calligraphy via an   Embedding-Aligned Vision-Language Model"></a>CalliReader: Contextualizing Chinese Calligraphy via an   Embedding-Aligned Vision-Language Model</h2><p><strong>Authors:Yuxuan Luo, Jiaqi Tang, Chenyi Huang, Feiyang Hao, Zhouhui Lian</strong></p>
<p>Chinese calligraphy, a UNESCO Heritage, remains computationally challenging due to visual ambiguity and cultural complexity. Existing AI systems fail to contextualize their intricate scripts, because of limited annotated data and poor visual-semantic alignment. We propose CalliReader, a vision-language model (VLM) that solves the Chinese Calligraphy Contextualization (CC$^2$) problem through three innovations: (1) character-wise slicing for precise character extraction and sorting, (2) CalliAlign for visual-text token compression and alignment, (3) embedding instruction tuning (e-IT) for improving alignment and addressing data scarcity. We also build CalliBench, the first benchmark for full-page calligraphic contextualization, addressing three critical issues in previous OCR and VQA approaches: fragmented context, shallow reasoning, and hallucination. Extensive experiments including user studies have been conducted to verify our CalliReaderâ€™s \textbf{superiority to other state-of-the-art methods and even human professionals in page-level calligraphy recognition and interpretation}, achieving higher accuracy while reducing hallucination. Comparisons with reasoning models highlight the importance of accurate recognition as a prerequisite for reliable comprehension. Quantitative analyses validate CalliReaderâ€™s efficiency; evaluations on document and real-world benchmarks confirm its robust generalization ability. </p>
<blockquote>
<p>ä¸­å›½ä¹¦æ³•ä½œä¸ºè”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡ï¼ˆUNESCOï¼‰é—äº§ï¼Œä»ç„¶å…·æœ‰è®¡ç®—ä¸Šçš„æŒ‘æˆ˜æ€§ï¼ŒåŸå› åœ¨äºè§†è§‰æ¨¡ç³Šå’Œæ–‡åŒ–å¤æ‚æ€§ã€‚ç°æœ‰çš„AIç³»ç»Ÿæ— æ³•å¯¹å…¶å¤æ‚çš„è„šæœ¬è¿›è¡Œè¯­å¢ƒåŒ–ç†è§£ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæ ‡æ³¨æ•°æ®æœ‰é™ä»¥åŠè§†è§‰è¯­ä¹‰å¯¹é½ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†CalliReaderï¼Œè¿™æ˜¯ä¸€ç§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå®ƒé€šè¿‡ä¸‰é¡¹åˆ›æ–°è§£å†³äº†ä¸­æ–‡ä¹¦æ³•çš„è¯­å¢ƒåŒ–é—®é¢˜ï¼ˆCC$^2$ï¼‰ï¼šï¼ˆ1ï¼‰å­—ç¬¦çº§åˆ‡ç‰‡ï¼Œç”¨äºç²¾ç¡®å­—ç¬¦æå–å’Œæ’åºï¼›ï¼ˆ2ï¼‰CalliAlignç”¨äºè§†è§‰æ–‡æœ¬ç¬¦å·å‹ç¼©å’Œå¯¹é½ï¼›ï¼ˆ3ï¼‰åµŒå…¥æŒ‡ä»¤è°ƒæ•´ï¼ˆe-ITï¼‰ç”¨äºæ”¹å–„å¯¹é½å¹¶è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†CalliBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¡µä¹¦æ³•è¯­å¢ƒåŒ–çš„åŸºå‡†æµ‹è¯•ï¼Œè§£å†³äº†ä¹‹å‰OCRå’ŒVQAæ–¹æ³•ä¸­çš„ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–ã€æ¨ç†æµ…æ˜¾å’Œå¹»è§‰ã€‚è¿›è¡Œäº†å¤§é‡å®éªŒå’Œç”¨æˆ·ç ”ç©¶ï¼ŒéªŒè¯äº†CalliReaderåœ¨é¡µçº§ä¹¦æ³•è¯†åˆ«å’Œè§£é‡Šæ–¹é¢ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œç”šè‡³ä¼˜äºä¸“ä¸šä¹¦æ³•å®¶ï¼Œåœ¨æé«˜å‡†ç¡®æ€§çš„åŒæ—¶å‡å°‘äº†å¹»è§‰ã€‚ä¸æ¨ç†æ¨¡å‹çš„æ¯”è¾ƒçªæ˜¾äº†å‡†ç¡®è¯†åˆ«ä½œä¸ºå¯é ç†è§£å…ˆå†³æ¡ä»¶çš„é‡è¦æ€§ã€‚å®šé‡åˆ†æéªŒè¯äº†CalliReaderçš„æ•ˆç‡ï¼›å¯¹æ–‡æ¡£å’ŒçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•è¯„ä¼°è¯æ˜äº†å…¶ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06472v2">PDF</a> 11 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸­æ–‡ä¹¦æ³•ä½œä¸ºè”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡é—äº§æ‰€é¢ä¸´çš„è®¡ç®—æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è§†è§‰æ¨¡ç³Šå’Œæ–‡åŒ–å¤æ‚æ€§ã€‚ç°æœ‰çš„AIç³»ç»Ÿå› ç¼ºä¹æ³¨é‡Šæ•°æ®å’Œè§†è§‰è¯­ä¹‰å¯¹é½ä¸è‰¯è€Œæ— æ³•é€‚åº”å¤æ‚çš„è„šæœ¬ä¸Šä¸‹æ–‡ã€‚æå‡ºäº†ä¸€ç§æ–°å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹CalliReaderï¼Œå®ƒé€šè¿‡ä¸‰ä¸ªåˆ›æ–°è§£å†³äº†ä¸­æ–‡ä¹¦æ³•ä¸Šä¸‹æ–‡åŒ–é—®é¢˜ï¼šå­—ç¬¦çº§åˆ‡ç‰‡æŠ€æœ¯ç”¨äºç²¾ç¡®å­—ç¬¦æå–å’Œæ’åºï¼ŒCalliAlignç”¨äºè§†è§‰æ–‡æœ¬ä»¤ç‰Œå‹ç¼©å’Œå¯¹é½ï¼ŒåµŒå…¥æŒ‡ä»¤è°ƒæ•´e-ITç”¨äºæ”¹è¿›å¯¹é½å¹¶è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æ­¤å¤–å»ºç«‹äº†é¦–ä¸ªå…¨é¡µä¹¦æ³•ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•CalliBenchï¼Œè§£å†³äº†ä»¥å¾€OCRå’ŒVQAæ–¹æ³•ä¸­çš„ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–ã€æ¨ç†æµ…æ˜¾å’Œå¹»è§‰ç°è±¡ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œç”¨æˆ·ç ”ç©¶éªŒè¯äº†CalliReaderåœ¨é¡µçº§ä¹¦æ³•è¯†åˆ«å’Œè§£é‡Šæ–¹é¢ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•å’Œä¸“ä¸šäººå£«ï¼Œåœ¨ä¿æŒé«˜å‡†ç¡®æ€§çš„åŒæ—¶é™ä½äº†å¹»è§‰ç°è±¡ã€‚ä¸æ¨ç†æ¨¡å‹çš„æ¯”è¾ƒçªæ˜¾äº†å‡†ç¡®è¯†åˆ«ä½œä¸ºå¯é ç†è§£å…ˆå†³æ¡ä»¶çš„é‡è¦æ€§ã€‚å®šé‡åˆ†æéªŒè¯äº†CalliReaderçš„æ•ˆç‡ï¼Œå¯¹æ–‡æ¡£å’ŒçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•è¯„ä¼°è¯æ˜äº†å…¶ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸­æ–‡ä¹¦æ³•ä½œä¸ºè”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡é—äº§ï¼Œé¢ä¸´è§†è§‰æ¨¡ç³Šå’Œæ–‡åŒ–å¤æ‚æ€§çš„è®¡ç®—æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰AIç³»ç»Ÿåœ¨å¤„ç†ä¸­æ–‡ä¹¦æ³•æ—¶å­˜åœ¨å›°éš¾ï¼Œä¸»è¦æ˜¯å› ä¸ºç¼ºä¹æ³¨é‡Šæ•°æ®å’Œè§†è§‰è¯­ä¹‰å¯¹é½ä¸è‰¯ã€‚</li>
<li>CalliReaderæ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä¸‰ä¸ªåˆ›æ–°è§£å†³äº†ä¸­æ–‡ä¹¦æ³•ä¸Šä¸‹æ–‡åŒ–é—®é¢˜ï¼šå­—ç¬¦çº§åˆ‡ç‰‡æŠ€æœ¯ã€CalliAlignå’ŒåµŒå…¥æŒ‡ä»¤è°ƒæ•´e-ITã€‚</li>
<li>CalliBenchä½œä¸ºé¦–ä¸ªå…¨é¡µä¹¦æ³•ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ï¼Œè§£å†³äº†ä»¥å¾€OCRå’ŒVQAæ–¹æ³•ä¸­çš„å…³é”®é—®é¢˜å’Œç¼ºé™·ã€‚</li>
<li>CalliReaderåœ¨é¡µçº§ä¹¦æ³•è¯†åˆ«å’Œè§£é‡Šæ–¹é¢ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•å’Œä¸“ä¸šäººå£«ï¼Œä¸”å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>ä¸æ¨ç†æ¨¡å‹çš„æ¯”è¾ƒçªæ˜¾äº†å‡†ç¡®è¯†åˆ«å¯¹äºå¯é ç†è§£çš„å…ˆå†³æ¡ä»¶çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06472">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-41bac42630b6d93f95593872be4cd34b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a79a389fe6dcfd8a8c3047079226a228.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32033de385c9a5cc7a307938287adf80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b3e86df68e0bce0258df634e69fbae7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-54a95fc09bdc8be6d33a50a811d85885.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd6d5ae91fd1ea30c3930888162d3b3d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MastermindEval-A-Simple-But-Scalable-Reasoning-Benchmark"><a href="#MastermindEval-A-Simple-But-Scalable-Reasoning-Benchmark" class="headerlink" title="MastermindEval: A Simple But Scalable Reasoning Benchmark"></a>MastermindEval: A Simple But Scalable Reasoning Benchmark</h2><p><strong>Authors:Jonas Golde, Patrick Haller, Fabio Barth, Alan Akbik</strong></p>
<p>Recent advancements in large language models (LLMs) have led to remarkable performance across a wide range of language understanding and mathematical tasks. As a result, increasing attention has been given to assessing the true reasoning capabilities of LLMs, driving research into commonsense, numerical, logical, and qualitative reasoning. However, with the rapid progress of reasoning-focused models such as OpenAIâ€™s o1 and DeepSeekâ€™s R1, there has been a growing demand for reasoning benchmarks that can keep pace with ongoing model developments. In this paper, we introduce MastermindEval, a simple, scalable, and interpretable deductive reasoning benchmark inspired by the board game Mastermind. Our benchmark supports two evaluation paradigms: (1) agentic evaluation, in which the model autonomously plays the game, and (2) deductive reasoning evaluation, in which the model is given a pre-played game state with only one possible valid code to infer. In our experimental results we (1) find that even easy Mastermind instances are difficult for current models and (2) demonstrate that the benchmark is scalable to possibly more advanced models in the future Furthermore, we investigate possible reasons why models cannot deduce the final solution and find that current models are limited in deducing the concealed code as the number of statement to combine information from is increasing. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•åœ¨å„ç§è¯­è¨€ç†è§£å’Œæ•°å­¦ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆç»©ã€‚å› æ­¤ï¼Œäººä»¬è¶Šæ¥è¶Šå…³æ³¨è¯„ä¼°LLMçš„çœŸæ­£æ¨ç†èƒ½åŠ›ï¼Œæ¨åŠ¨äº†å¸¸è¯†ã€æ•°å€¼ã€é€»è¾‘å’Œå®šæ€§æ¨ç†çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œéšç€ä»¥OpenAIçš„o1å’ŒDeepSeekçš„R1ç­‰ä¸ºä»£è¡¨çš„æ¨ç†é‡ç‚¹æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹èƒ½ä¸å½“å‰æ¨¡å‹å‘å±•åŒæ­¥çš„æ¨ç†åŸºå‡†æµ‹è¯•çš„éœ€æ±‚ä¹Ÿåœ¨å¢é•¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MastermindEvalï¼Œè¿™æ˜¯ä¸€ä¸ªå—æ£‹ç›˜æ¸¸æˆã€ŠçŒœå¯†ç ã€‹å¯å‘çš„ç®€å•ã€å¯æ‰©å±•å’Œå¯è§£é‡Šçš„æ¼”ç»æ¨ç†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ”¯æŒä¸¤ç§è¯„ä¼°èŒƒå¼ï¼šï¼ˆ1ï¼‰ä»£ç†è¯„ä¼°ï¼Œå³æ¨¡å‹è‡ªä¸»ç©æ¸¸æˆï¼›ï¼ˆ2ï¼‰æ¼”ç»æ¨ç†è¯„ä¼°ï¼Œå³ç»™æ¨¡å‹ä¸€ä¸ªé¢„å…ˆç©è¿‡çš„æ¸¸æˆçŠ¶æ€ï¼Œåªæœ‰ä¸€ä¸ªå¯èƒ½çš„æ­£ç¡®å¯†ç éœ€è¦æ¨æ–­ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒç»“æœä¸­ï¼Œæˆ‘ä»¬ï¼ˆ1ï¼‰å‘ç°å³ä½¿æ˜¯ç®€å•çš„ã€ŠçŒœå¯†ç ã€‹å®ä¾‹å¯¹å½“å‰çš„æ¨¡å‹æ¥è¯´ä¹Ÿæ˜¯å›°éš¾çš„ï¼Œï¼ˆ2ï¼‰è¯æ˜è¯¥åŸºå‡†æµ‹è¯•åœ¨æœªæ¥å¯ä»¥æ‰©å±•åˆ°æ›´å…ˆè¿›çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è°ƒæŸ¥äº†æ¨¡å‹æ— æ³•æ¨æ–­æœ€ç»ˆè§£å†³æ–¹æ¡ˆçš„å¯èƒ½åŸå› ï¼Œå¹¶å‘ç°éšç€éœ€è¦ä»é™ˆè¿°ä¸­ç»“åˆä¿¡æ¯æ•°é‡çš„å¢åŠ ï¼Œå½“å‰æ¨¡å‹åœ¨æ¨æ–­éšè—å¯†ç æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05891v3">PDF</a> 9 pages, 2 figures, 4 tables. In: ICLR 2025 Workshop on Reasoning and   Planning for Large Language Models</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•åœ¨å„ç§è¯­è¨€ç†è§£å’Œæ•°å­¦ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆç»©ï¼Œå¼•å‘äº†äººä»¬å¯¹è¯„ä¼°å…¶çœŸæ­£æ¨ç†èƒ½åŠ›çš„å…³æ³¨ï¼Œæ¨åŠ¨äº†å¸¸è¯†ã€æ•°å€¼ã€é€»è¾‘å’Œå®šæ€§æ¨ç†çš„ç ”ç©¶ã€‚ä¸ºæ­¤ï¼Œäººä»¬è¿«åˆ‡éœ€è¦ä¸æ¨¡å‹å‘å±•åŒæ­¥çš„æ¨ç†åŸºå‡†æµ‹è¯•ã€‚æœ¬æ–‡ä»‹ç»äº†MastermindEvalï¼Œä¸€ä¸ªç®€å•ã€å¯æ‰©å±•ã€å¯è§£é‡Šçš„æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œå…¶çµæ„Ÿæ¥è‡ªçŒœè°œæ¸¸æˆMastermindã€‚è¯¥åŸºå‡†æµ‹è¯•æ”¯æŒä¸¤ç§è¯„ä¼°èŒƒå¼ï¼šä¸€æ˜¯æ¨¡å‹è‡ªä¸»ç©æ¸¸æˆï¼ŒäºŒæ˜¯æ¨ç†è¯„ä¼°ï¼Œå³æ¨¡å‹æ ¹æ®ç»™å®šçš„æ¸¸æˆçŠ¶æ€è¿›è¡Œæ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯ç®€å•çš„çŒœè°œå®ä¾‹å¯¹å½“å‰çš„æ¨¡å‹æ¥è¯´ä¹Ÿæ˜¯å›°éš¾çš„ï¼Œå¹¶ä¸”è¯¥åŸºå‡†æµ‹è¯•å¯ä»¥æ‰©å±•åˆ°æœªæ¥çš„æ›´é«˜çº§æ¨¡å‹ã€‚å½“å‰æ¨¡å‹çš„å±€é™æ€§åœ¨äºï¼Œéšç€éœ€è¦ç»„åˆçš„ä¿¡æ¯æ•°é‡çš„å¢åŠ ï¼Œå®ƒä»¬æ— æ³•æ¨æ–­å‡ºæœ€ç»ˆçš„ç­”æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„æœ€æ–°è¿›å±•åœ¨å„ç§è¯­è¨€ç†è§£å’Œæ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¼•å‘äº†å¯¹è¯„ä¼°å…¶çœŸæ­£æ¨ç†èƒ½åŠ›çš„å…³æ³¨ã€‚</li>
<li>è¿«åˆ‡éœ€è¦ä¸æ¨¡å‹å‘å±•åŒæ­¥çš„æ¨ç†åŸºå‡†æµ‹è¯•ã€‚</li>
<li>ä»‹ç»äº†MastermindEvalï¼Œä¸€ä¸ªç®€å•ã€å¯æ‰©å±•ã€å¯è§£é‡Šçš„æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œçµæ„Ÿæ¥è‡ªçŒœè°œæ¸¸æˆMastermindã€‚</li>
<li>MastermindEvalæ”¯æŒä¸¤ç§è¯„ä¼°èŒƒå¼ï¼šæ¨¡å‹è‡ªä¸»ç©æ¸¸æˆå’Œæ¨ç†è¯„ä¼°ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯ç®€å•çš„çŒœè°œå®ä¾‹å¯¹å½“å‰çš„æ¨¡å‹æ¥è¯´ä¹Ÿæ˜¯å›°éš¾çš„ã€‚</li>
<li>MastermindEvalå¯ä»¥æ‰©å±•åˆ°æœªæ¥çš„æ›´é«˜çº§æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae2a4bea4df439f87dcf060498129c24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b23084d546f81e31370fec9616adbf70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9417ec18fef4236b19fbdf03aa03f636.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-981cc37b1f842857a47af413369acb78.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="The-Lazy-Studentâ€™s-Dream-ChatGPT-Passing-an-Engineering-Course-on-Its-Own"><a href="#The-Lazy-Studentâ€™s-Dream-ChatGPT-Passing-an-Engineering-Course-on-Its-Own" class="headerlink" title="The Lazy Studentâ€™s Dream: ChatGPT Passing an Engineering Course on Its   Own"></a>The Lazy Studentâ€™s Dream: ChatGPT Passing an Engineering Course on Its   Own</h2><p><strong>Authors:Gokul Puthumanaillam, Melkior Ornik</strong></p>
<p>This paper presents a comprehensive investigation into the capability of Large Language Models (LLMs) to successfully complete a semester-long undergraduate control systems course. Through evaluation of 115 course deliverables, we assess LLM performance using ChatGPT under a â€œminimal effortâ€ protocol that simulates realistic student usage patterns. The investigation employs a rigorous testing methodology across multiple assessment formats, from auto-graded multiple choice questions to complex Python programming tasks and long-form analytical writing. Our analysis provides quantitative insights into AIâ€™s strengths and limitations in handling mathematical formulations, coding challenges, and theoretical concepts in control systems engineering. The LLM achieved a B-grade performance (82.24%), approaching but not exceeding the class average (84.99%), with strongest results in structured assignments and greatest limitations in open-ended projects. The findings inform discussions about course design adaptation in response to AI advancement, moving beyond simple prohibition towards thoughtful integration of these tools in engineering education. Additional materials including syllabus, examination papers, design projects, and example responses can be found at the project website: <a target="_blank" rel="noopener" href="https://gradegpt.github.io/">https://gradegpt.github.io</a>. </p>
<blockquote>
<p>æœ¬æ–‡å…¨é¢æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆåŠŸå®Œæˆä¸€å­¦æœŸæœ¬ç§‘æ§åˆ¶ç³»ç»Ÿè¯¾ç¨‹çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹115ä»½è¯¾ç¨‹äº¤ä»˜æˆæœçš„è¯„ä»·ï¼Œæˆ‘ä»¬é‡‡ç”¨ChatGPTè¯„ä¼°LLMæ€§èƒ½ï¼Œéµå¾ªæ¨¡æ‹Ÿç°å®å­¦ç”Ÿä½¿ç”¨æ¨¡å¼çš„â€œæœ€å°åŠªåŠ›â€åè®®ã€‚è°ƒæŸ¥é‡‡ç”¨ä¸¥æ ¼çš„æµ‹è¯•æ–¹æ³•ï¼Œæ¶µç›–å¤šç§è¯„ä¼°å½¢å¼ï¼Œä»è‡ªåŠ¨è¯„åˆ†çš„å¤šé¡¹é€‰æ‹©é¢˜åˆ°å¤æ‚çš„Pythonç¼–ç¨‹ä»»åŠ¡å’Œé•¿ç¯‡åˆ†æå†™ä½œã€‚æˆ‘ä»¬çš„åˆ†ææä¾›äº†å…³äºAIåœ¨å¤„ç†æ§åˆ¶ç³»ç»Ÿå·¥ç¨‹ä¸­çš„æ•°å­¦å…¬å¼ã€ç¼–ç¨‹æŒ‘æˆ˜å’Œç†è®ºæ¦‚å¿µçš„ä¼˜ç‚¹å’Œå±€é™æ€§çš„å®šé‡è§è§£ã€‚LLMå–å¾—äº†Bçº§è¡¨ç°ï¼ˆ82.24%ï¼‰ï¼Œæ¥è¿‘ä½†æœªè¾¾åˆ°ç­çº§å¹³å‡æ°´å¹³ï¼ˆ84.99%ï¼‰ï¼Œç»“æ„åŒ–ä½œä¸šæˆç»©æœ€ä½³ï¼Œå¼€æ”¾å¼é¡¹ç›®æˆç»©å±€é™æ€§æœ€å¤§ã€‚è¿™äº›å‘ç°å¼•å‘äº†å…³äºé€‚åº”AIå‘å±•çš„è¯¾ç¨‹è®¾è®¡é€‚åº”çš„è®¨è®ºï¼Œè¶…è¶Šç®€å•çš„ç¦ä»¤ï¼Œæœç€åœ¨å·¥ç¨‹æ•™è‚²ä¸­æ·±æ€ç†Ÿè™‘åœ°æ•´åˆè¿™äº›å·¥å…·çš„æ–¹å‘å‘å±•ã€‚é™„åŠ ææ–™åŒ…æ‹¬æ•™å­¦å¤§çº²ã€è¯•å·ã€è®¾è®¡é¡¹ç›®å’Œç¤ºä¾‹ç­”æ¡ˆï¼Œå¯åœ¨é¡¹ç›®ç½‘ç«™æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://gradegpt.github.io./">https://gradegpt.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05760v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®Œæˆå­¦æœŸé•¿çš„æ§åˆ¶å·¥ç¨‹ç³»æœ¬ç§‘ç”Ÿè¯¾ç¨‹æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹ä½¿ç”¨ChatGPTåœ¨æ¨¡æ‹ŸçœŸå®å­¦ç”Ÿä½¿ç”¨æ¨¡å¼çš„â€œæœ€å°åŠªåŠ›â€åè®®ä¸‹å®Œæˆçš„115ä»½è¯¾ç¨‹æˆæœè¿›è¡Œè¯„ä»·ï¼Œè¯¥ç ”ç©¶é‡‡ç”¨ä¸¥æ ¼çš„æµ‹è¯•æ–¹æ³•ï¼Œæ¶µç›–å¤šç§è¯„ä¼°å½¢å¼ï¼Œä»è‡ªåŠ¨æ‰¹æ”¹çš„é€‰æ‹©é¢˜åˆ°å¤æ‚çš„Pythonç¼–ç¨‹ä»»åŠ¡å’Œé•¿æ ¼å¼åˆ†æå†™ä½œã€‚åˆ†ææä¾›äº†å…³äºäººå·¥æ™ºèƒ½åœ¨å¤„ç†æ§åˆ¶ç³»å·¥ç¨‹ä¸­çš„æ•°å­¦å…¬å¼ã€ç¼–ç¨‹æŒ‘æˆ˜å’Œç†è®ºæ¦‚å¿µçš„ä¼˜ç‚¹å’Œå±€é™æ€§çš„å®šé‡è§è§£ã€‚LLMçš„è¡¨ç°è¾¾åˆ°äº†Bçº§æ°´å¹³ï¼ˆ82.24%ï¼‰ï¼Œæ¥è¿‘ä½†æœªè¶…è¿‡ç­çº§å¹³å‡æ°´å¹³ï¼ˆ84.99%ï¼‰ï¼Œåœ¨ç»“æ„åŒ–ä½œä¸šä¸­çš„è¡¨ç°æœ€ä½³ï¼Œè€Œåœ¨å¼€æ”¾å¼é¡¹ç›®ä¸­çš„è¡¨ç°æœ€ä¸ºå—é™ã€‚è¿™äº›å‘ç°å¼•å‘äº†å…³äºé€‚åº”äººå·¥æ™ºèƒ½å‘å±•çš„è¯¾ç¨‹è®¾è®¡çš„è®¨è®ºï¼Œæ¨åŠ¨äººä»¬è¶…è¶Šç®€å•çš„ç¦ä»¤ï¼Œå¼€å§‹æ€è€ƒå¦‚ä½•æ•´åˆè¿™äº›å·¥å…·åœ¨å·¥ç¨‹æ•™è‚²ä¸­å‘æŒ¥ä½œç”¨ã€‚æ›´å¤šææ–™å¯ä»¥åœ¨é¡¹ç›®ç½‘ç«™ä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://gradegpt.github.io/">https://gradegpt.github.io</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºè¯¥æ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®Œæˆæ§åˆ¶å·¥ç¨‹ç³»æœ¬ç§‘ç”Ÿè¯¾ç¨‹æ–¹é¢çš„èƒ½åŠ›å¾—åˆ°äº†å…¨é¢ç ”ç©¶ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ChatGPTè¿›è¡Œæ¨¡æ‹ŸçœŸå®å­¦ç”Ÿä½¿ç”¨æ¨¡å¼çš„æµ‹è¯•ï¼Œå¯¹LLMçš„è¡¨ç°è¿›è¡Œäº†è¯„ä»·ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†å¤šç§è¯„ä¼°å½¢å¼çš„ä¸¥æ ¼æµ‹è¯•æ–¹æ³•ï¼ŒåŒ…æ‹¬è‡ªåŠ¨æ‰¹æ”¹çš„é€‰æ‹©é¢˜ã€Pythonç¼–ç¨‹ä»»åŠ¡å’Œé•¿æ ¼å¼åˆ†æå†™ä½œã€‚</li>
<li>LLMåœ¨å¤„ç†æ§åˆ¶ç³»å·¥ç¨‹ä¸­çš„æ•°å­¦å…¬å¼ã€ç¼–ç¨‹æŒ‘æˆ˜å’Œç†è®ºæ¦‚å¿µæ–¹é¢è¡¨ç°å‡ºä¼˜ç‚¹å’Œå±€é™æ€§ã€‚</li>
<li>LLMçš„æˆç»©è¾¾åˆ°äº†Bçº§æ°´å¹³ï¼Œæ¥è¿‘ç­çº§å¹³å‡æ°´å¹³ï¼Œä½†å¹¶æœªè¶…è¿‡ã€‚</li>
<li>LLMåœ¨ç»“æ„åŒ–ä½œä¸šä¸­çš„è¡¨ç°æœ€ä½³ï¼Œè€Œåœ¨å¼€æ”¾å¼é¡¹ç›®ä¸­çš„è¡¨ç°å—é™ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf74afe4c6d0703c62ae52d3b0e924ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67bdee915f3f1854ce7502388fdf0d0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d89ff0a65bdd64b9f97d6185862a6f43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3b91291511b7b3b642c59b7d5279932.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e8ab861defd4399182b8ef0f4a9ae66.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="PaCA-Partial-Connection-Adaptation-for-Efficient-Fine-Tuning"><a href="#PaCA-Partial-Connection-Adaptation-for-Efficient-Fine-Tuning" class="headerlink" title="PaCA: Partial Connection Adaptation for Efficient Fine-Tuning"></a>PaCA: Partial Connection Adaptation for Efficient Fine-Tuning</h2><p><strong>Authors:Sunghyeon Woo, Sol Namkung, Sunwoo Lee, Inho Jeong, Beomseok Kim, Dongsuk Jeon</strong></p>
<p>Prior parameter-efficient fine-tuning (PEFT) algorithms reduce memory usage and computational costs of fine-tuning large neural network models by training only a few additional adapter parameters, rather than the entire model. However, the reduction in computational costs due to PEFT does not necessarily translate to a reduction in training time; although the computational costs of the adapter layers are much smaller than the pretrained layers, it is well known that those two types of layers are processed sequentially on GPUs, resulting in significant latency overhead. LoRA and its variants merge low-rank adapter matrices with pretrained weights during inference to avoid latency overhead, but during training, the pretrained weights remain frozen while the adapter matrices are continuously updated, preventing such merging. To mitigate this issue, we propose Partial Connection Adaptation (PaCA), which fine-tunes randomly selected partial connections within the pretrained weights instead of introducing adapter layers in the model. PaCA not only enhances training speed by eliminating the time overhead due to the sequential processing of the adapter and pretrained layers but also reduces activation memory since only partial activations, rather than full activations, need to be stored for gradient computation. Compared to LoRA, PaCA reduces training time by 22% and total memory usage by 16%, while maintaining comparable accuracy across various fine-tuning scenarios, such as fine-tuning on the MMLU dataset and instruction tuning on the Oasst1 dataset. PaCA can also be combined with quantization, enabling the fine-tuning of large models such as LLaMA3.1-70B. In addition, PaCA enables training with 23% longer sequence and improves throughput by 16% on both NVIDIA A100 GPU and INTEL Gaudi2 HPU compared to LoRA. The code is available at <a target="_blank" rel="noopener" href="https://github.com/WooSunghyeon/paca">https://github.com/WooSunghyeon/paca</a>. </p>
<blockquote>
<p>ä¹‹å‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ç®—æ³•é€šè¿‡ä»…è®­ç»ƒå°‘é‡é¢å¤–çš„é€‚é…å™¨å‚æ•°ï¼Œè€Œä¸æ˜¯å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä»è€Œå‡å°‘äº†å¾®è°ƒå¤§å‹ç¥ç»ç½‘ç»œæ¨¡å‹çš„å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œç”±äºPEFTå¯¼è‡´çš„è®¡ç®—æˆæœ¬é™ä½å¹¶ä¸ä¸€å®šè½¬åŒ–ä¸ºè®­ç»ƒæ—¶é—´çš„å‡å°‘ã€‚å°½ç®¡é€‚é…å™¨å±‚çš„è®¡ç®—æˆæœ¬è¿œå°äºé¢„è®­ç»ƒå±‚ï¼Œä¼—æ‰€å‘¨çŸ¥ï¼Œè¿™ä¸¤ç§ç±»å‹çš„å±‚åœ¨GPUä¸Šæ˜¯é¡ºåºå¤„ç†çš„ï¼Œè¿™ä¼šå¯¼è‡´æ˜¾è‘—çš„å»¶è¿Ÿå¼€é”€ã€‚LoRAåŠå…¶å˜ä½“åœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†ä½é˜¶é€‚é…å™¨çŸ©é˜µä¸é¢„è®­ç»ƒæƒé‡åˆå¹¶ï¼Œä»¥é¿å…å»¶è¿Ÿå¼€é”€ï¼Œä½†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé¢„è®­ç»ƒæƒé‡ä¿æŒå†»ç»“çŠ¶æ€ï¼Œè€Œé€‚é…å™¨çŸ©é˜µä¸æ–­æ›´æ–°ï¼Œè¿™é˜»æ­¢äº†åˆå¹¶ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†éƒ¨åˆ†è¿æ¥é€‚é…ï¼ˆPaCAï¼‰ï¼Œå®ƒåœ¨é¢„è®­ç»ƒæƒé‡å†…å¾®è°ƒéšæœºé€‰æ‹©çš„éƒ¨åˆ†è¿æ¥ï¼Œè€Œä¸æ˜¯åœ¨æ¨¡å‹ä¸­å¼•å…¥é€‚é…å™¨å±‚ã€‚PaCAä¸ä»…é€šè¿‡æ¶ˆé™¤ç”±äºé€‚é…å™¨å±‚å’Œé¢„è®­ç»ƒå±‚çš„é¡ºåºå¤„ç†è€Œäº§ç”Ÿçš„æ—¶é—´å¼€é”€ï¼Œæé«˜äº†è®­ç»ƒé€Ÿåº¦ï¼Œè€Œä¸”å‡å°‘äº†æ¿€æ´»å†…å­˜ï¼Œå› ä¸ºåªéœ€è¦å­˜å‚¨éƒ¨åˆ†æ¿€æ´»å€¼ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„æ¿€æ´»å€¼æ¥è¿›è¡Œæ¢¯åº¦è®¡ç®—ã€‚ä¸LoRAç›¸æ¯”ï¼ŒPaCAå‡å°‘äº†22%çš„è®­ç»ƒæ—¶é—´å’Œ16%çš„æ€»å†…å­˜ä½¿ç”¨é‡ï¼ŒåŒæ—¶åœ¨å„ç§å¾®è°ƒåœºæ™¯ï¼ˆå¦‚ä½¿ç”¨MMLUæ•°æ®é›†è¿›è¡Œå¾®è°ƒå’Œä½¿ç”¨Oasst1æ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼‰ä¸­ä¿æŒç›¸å½“çš„å‡†ç¡®æ€§ã€‚PaCAè¿˜å¯ä»¥ä¸é‡åŒ–ç›¸ç»“åˆï¼Œå®ç°å¯¹å¤§å‹æ¨¡å‹ï¼ˆå¦‚LLaMA3.1-70Bï¼‰çš„å¾®è°ƒã€‚æ­¤å¤–ï¼ŒPaCAä½¿ç”¨NVIDIA A100 GPUå’ŒINTEL Gaudi2 HPUæ—¶ï¼Œæ”¯æŒ23%æ›´é•¿çš„åºåˆ—è®­ç»ƒï¼Œå¹¶æé«˜äº†16%çš„ååé‡ç›¸æ¯”LoRAã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/WooSunghyeon/paca%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/WooSunghyeon/pacaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01905v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPartial Connection Adaptationï¼ˆPaCAï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›ç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ç®—æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæƒé‡ä¸­çš„éƒ¨åˆ†è¿æ¥ï¼Œé¿å…äº†å¼•å…¥é¢å¤–çš„é€‚é…å™¨å±‚ï¼Œä»è€Œæé«˜è®­ç»ƒé€Ÿåº¦å’Œå‡å°‘å†…å­˜ä½¿ç”¨ã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„LoRAæ–¹æ³•ï¼ŒPaCAèƒ½å¤Ÿå‡å°‘è®­ç»ƒæ—¶é—´å’Œæ€»å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒç›¸å½“çš„ç²¾åº¦ã€‚æ­¤å¤–ï¼ŒPaCAè¿˜å¯ä»¥ä¸é‡åŒ–ç»“åˆï¼Œæ”¯æŒå¤§è§„æ¨¡æ¨¡å‹çš„å¾®è°ƒï¼Œå¹¶èƒ½æé«˜åºåˆ—é•¿åº¦å¤„ç†èƒ½åŠ›å’Œååé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PaCAæ˜¯ä¸€ç§æ”¹è¿›çš„PEFTæ–¹æ³•ï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæƒé‡ä¸­çš„éƒ¨åˆ†è¿æ¥ï¼Œé¿å…å¼•å…¥é¢å¤–çš„é€‚é…å™¨å±‚ã€‚</li>
<li>PaCAæé«˜äº†è®­ç»ƒé€Ÿåº¦ï¼Œå‡å°‘äº†å› é€‚é…å™¨å±‚å’Œé¢„è®­ç»ƒå±‚é¡ºåºå¤„ç†å¯¼è‡´çš„å»¶è¿Ÿå¼€é”€ã€‚</li>
<li>PaCAé™ä½äº†æ¿€æ´»å†…å­˜çš„ä½¿ç”¨ï¼Œå› ä¸ºåªéœ€å­˜å‚¨éƒ¨åˆ†æ¿€æ´»ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„æ¿€æ´»ï¼Œä»¥ä¾›æ¢¯åº¦è®¡ç®—ã€‚</li>
<li>PaCAç›¸è¾ƒäºLoRAï¼Œèƒ½å¤Ÿå‡å°‘è®­ç»ƒæ—¶é—´22%å’Œæ€»å†…å­˜ä½¿ç”¨16%ï¼ŒåŒæ—¶ä¿æŒç›¸ä¼¼çš„ç²¾åº¦ã€‚</li>
<li>PaCAæ”¯æŒåœ¨å¤šç§å¾®è°ƒåœºæ™¯ï¼ˆå¦‚MMLUæ•°æ®é›†ä¸Šçš„å¾®è°ƒã€Oasst1æ•°æ®é›†ä¸Šçš„æŒ‡ä»¤å¾®è°ƒï¼‰ä¸­ä½¿ç”¨ã€‚</li>
<li>PaCAå¯ä¸é‡åŒ–ç»“åˆï¼Œä½¿å¤§è§„æ¨¡æ¨¡å‹çš„å¾®è°ƒæˆä¸ºå¯èƒ½ï¼Œå¦‚LLaMA3.1-70Bã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01905">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cdb4aa9dcf132368d1c021d6ad68e811.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ca70077b7eea935ee161e2b872ea259.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c31f5d31b8b8c07a3cf173277a1fe769.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-578ca7e29e65ec372bdbe8b9980966ce.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-add2cbcb953bd382c69d1628dc25ea89.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  ReMA Learning to Meta-think for LLMs with Multi-Agent Reinforcement   Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d435ed7ca1c36725b8fded997977b9fe.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Search-R1 Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16190.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
