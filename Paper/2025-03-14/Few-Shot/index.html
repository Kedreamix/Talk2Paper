<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Membership Inference Attacks fueled by Few-Short Learning to detect   privacy leakage tackling data integrity">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b5713d02847055507997eb5be3d3f6db.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    61 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-14-æ›´æ–°"><a href="#2025-03-14-æ›´æ–°" class="headerlink" title="2025-03-14 æ›´æ–°"></a>2025-03-14 æ›´æ–°</h1><h2 id="Membership-Inference-Attacks-fueled-by-Few-Short-Learning-to-detect-privacy-leakage-tackling-data-integrity"><a href="#Membership-Inference-Attacks-fueled-by-Few-Short-Learning-to-detect-privacy-leakage-tackling-data-integrity" class="headerlink" title="Membership Inference Attacks fueled by Few-Short Learning to detect   privacy leakage tackling data integrity"></a>Membership Inference Attacks fueled by Few-Short Learning to detect   privacy leakage tackling data integrity</h2><p><strong>Authors:Daniel JimÃ©nez-LÃ³pez, Nuria RodrÃ­guez-Barroso, M. Victoria LuzÃ³n, Francisco Herrera</strong></p>
<p>Deep learning models have an intrinsic privacy issue as they memorize parts of their training data, creating a privacy leakage. Membership Inference Attacks (MIA) exploit it to obtain confidential information about the data used for training, aiming to steal information. They can be repurposed as a measurement of data integrity by inferring whether it was used to train a machine learning model. While state-of-the-art attacks achieve a significant privacy leakage, their requirements are not feasible enough, hindering their role as practical tools to assess the magnitude of the privacy risk. Moreover, the most appropriate evaluation metric of MIA, the True Positive Rate at low False Positive Rate lacks interpretability. We claim that the incorporation of Few-Shot Learning techniques to the MIA field and a proper qualitative and quantitative privacy evaluation measure should deal with these issues. In this context, our proposal is twofold. We propose a Few-Shot learning based MIA, coined as the FeS-MIA model, which eases the evaluation of the privacy breach of a deep learning model by significantly reducing the number of resources required for the purpose. Furthermore, we propose an interpretable quantitative and qualitative measure of privacy, referred to as Log-MIA measure. Jointly, these proposals provide new tools to assess the privacy leakage and to ease the evaluation of the training data integrity of deep learning models, that is, to analyze the privacy breach of a deep learning model. Experiments carried out with MIA over image classification and language modeling tasks and its comparison to the state-of-the-art show that our proposals excel at reporting the privacy leakage of a deep learning model with little extra information. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹å­˜åœ¨å›ºæœ‰çš„éšç§é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬ä¼šè®°å¿†éƒ¨åˆ†è®­ç»ƒæ•°æ®ï¼Œä»è€Œé€ æˆéšç§æ³„éœ²ã€‚æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰ä¼šåˆ©ç”¨è¿™ä¸€ç‚¹æ¥è·å–æœ‰å…³ç”¨äºè®­ç»ƒçš„æ•°æ®çš„æœºå¯†ä¿¡æ¯ï¼Œæ—¨åœ¨çªƒå–ä¿¡æ¯ã€‚å®ƒä»¬å¯ä»¥é€šè¿‡æ¨æ–­æ•°æ®æ˜¯å¦ç”¨äºè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹æ¥é‡æ–°ç”¨ä½œæ•°æ®å®Œæ•´æ€§çš„åº¦é‡ã€‚è™½ç„¶æœ€å…ˆè¿›çš„æ”»å‡»ä¼šé€ æˆæ˜¾è‘—çš„éšç§æ³„éœ²ï¼Œä½†å®ƒä»¬çš„è¦æ±‚å¹¶ä¸å¯è¡Œï¼Œé˜»ç¢äº†å…¶ä½œä¸ºè¯„ä¼°éšç§é£é™©å¤§å°çš„å®ç”¨å·¥å…·çš„ä½œç”¨ã€‚æ­¤å¤–ï¼ŒMIAçš„æœ€é€‚å½“çš„è¯„ä¼°æŒ‡æ ‡â€”â€”åœ¨ä½è¯¯æŠ¥ç‡ä¸‹çš„çœŸæ­£ç‡ç¼ºä¹å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬ä¸»å¼ å°†å°æ ·å­¦ä¹ æŠ€æœ¯å¼•å…¥MIAé¢†åŸŸï¼Œå¹¶å¼•å…¥é€‚å½“çš„å®šæ€§å’Œå®šé‡éšç§è¯„ä¼°æªæ–½æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬çš„ææ¡ˆæ˜¯åŒå‘çš„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå°æ ·å­¦ä¹ çš„MIAï¼Œç§°ä¸ºFeS-MIAæ¨¡å‹ï¼Œé€šè¿‡æ˜¾è‘—å‡å°‘ç”¨äºè¯„ä¼°æ·±åº¦å­¦ä¹ æ¨¡å‹éšç§æ³„éœ²æ‰€éœ€çš„èµ„æºï¼Œä»è€Œç®€åŒ–äº†è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„é‡åŒ–å’Œå®šæ€§éšç§åº¦é‡æ ‡å‡†ï¼Œç§°ä¸ºLog-MIAåº¦é‡ã€‚å…±åŒåœ°ï¼Œè¿™äº›æè®®æä¾›äº†æ–°çš„å·¥å…·æ¥è¯„ä¼°éšç§æ³„éœ²å¹¶è½»æ¾è¯„ä¼°æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒæ•°æ®å®Œæ•´æ€§ï¼Œå³åˆ†ææ·±åº¦å­¦ä¹ æ¨¡å‹çš„éšç§æ³„éœ²æƒ…å†µã€‚é’ˆå¯¹å›¾åƒåˆ†ç±»å’Œè¯­è¨€å»ºæ¨¡ä»»åŠ¡è¿›è¡Œçš„MIAå®éªŒåŠå…¶ä¸æœ€æ–°æŠ€æœ¯çš„æ¯”è¾ƒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨æŠ¥å‘Šæ·±åº¦å­¦ä¹ æ¨¡å‹çš„éšç§æ³„éœ²æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”å‡ ä¹ä¸éœ€è¦é¢å¤–çš„ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09365v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ·±åº¦å­¦ä¹ çš„éšç§é—®é¢˜ï¼ŒæŒ‡å‡ºå…¶è®­ç»ƒæ•°æ®å­˜åœ¨éšç§æ³„éœ²çš„é£é™©ã€‚æ–‡ç« ä»‹ç»äº†æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰ä½œä¸ºè¡¡é‡æ•°æ®å®Œæ•´æ€§å’Œéšç§æ³„éœ²çš„å·¥å…·ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰æ”»å‡»æ–¹æ³•å­˜åœ¨çš„ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºå°æ ·æœ¬å­¦ä¹ çš„MIAæ–¹æ³•ï¼ˆFeS-MIAæ¨¡å‹ï¼‰ï¼Œå¹¶å¼•å…¥äº†å¯è§£é‡Šæ€§å¼ºã€å®šé‡å®šæ€§çš„éšç§åº¦é‡æ ‡å‡†ï¼ˆLog-MIAåº¦é‡ï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œæ–°æ–¹æ³•åœ¨æŠ¥å‘Šæ·±åº¦å­¦ä¹ çš„éšç§æ³„éœ²æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹å­˜åœ¨éšç§æ³„éœ²é—®é¢˜ï¼Œè®­ç»ƒæ•°æ®å¯èƒ½è¢«æ³„éœ²ã€‚</li>
<li>æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰å¯ç”¨äºè·å–è®­ç»ƒæ•°æ®çš„ç›¸å…³ä¿¡æ¯ï¼Œè¯„ä¼°æ•°æ®å®Œæ•´æ€§ã€‚</li>
<li>ç°æœ‰MIAæ–¹æ³•èµ„æºéœ€æ±‚å¤§ï¼Œä¸é€‚åˆä½œä¸ºå®ç”¨å·¥å…·è¯„ä¼°éšç§é£é™©ã€‚</li>
<li>å¼•å…¥åŸºäºå°æ ·æœ¬å­¦ä¹ çš„MIAæ–¹æ³•ï¼ˆFeS-MIAæ¨¡å‹ï¼‰ï¼Œé™ä½èµ„æºéœ€æ±‚ï¼Œç®€åŒ–éšç§æ³„éœ²è¯„ä¼°ã€‚</li>
<li>æå‡ºæ–°çš„éšç§åº¦é‡æ ‡å‡†ï¼ˆLog-MIAåº¦é‡ï¼‰ï¼Œå…·æœ‰å¯è§£é‡Šæ€§ã€å®šé‡å®šæ€§ç‰¹ç‚¹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œæ–°æ–¹æ³•åœ¨æŠ¥å‘Šæ·±åº¦å­¦ä¹ çš„éšç§æ³„éœ²æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09365">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40b8c981b0fca6e46bd473cf7967beab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-585cf3cd5841ce1cb3250cf91757d381.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="N2C2-Nearest-Neighbor-Enhanced-Confidence-Calibration-for-Cross-Lingual-In-Context-Learning"><a href="#N2C2-Nearest-Neighbor-Enhanced-Confidence-Calibration-for-Cross-Lingual-In-Context-Learning" class="headerlink" title="N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual   In-Context Learning"></a>N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual   In-Context Learning</h2><p><strong>Authors:Jie He, Simon Yu, Deyi Xiong, VÃ­ctor GutiÃ©rrez-Basulto, Jeff Z. Pan</strong></p>
<p>Recent advancements of in-context learning (ICL) show language models can significantly improve their performance when demonstrations are provided. However, little attention has been paid to model calibration and prediction confidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a thorough analysis of ICL for cross-lingual sentiment classification. Our findings suggest that ICL performs poorly in cross-lingual scenarios, exhibiting low accuracy and presenting high calibration errors. In response, we propose a novel approach, N2C2, which employs a -nearest neighbors augmented classifier for prediction confidence calibration. N2C2 narrows the prediction gap by leveraging a datastore of cached few-shot instances. Specifically, N2C2 integrates the predictions from the datastore and incorporates confidence-aware distribution, semantically consistent retrieval representation, and adaptive neighbor combination modules to effectively utilize the limited number of supporting instances. Evaluation on two multilingual sentiment classification datasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine tuning, prompt tuning and recent state-of-the-art methods in terms of accuracy and calibration errors. </p>
<blockquote>
<p>æœ€è¿‘å…³äºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„è¿›å±•è¡¨æ˜ï¼Œå½“æä¾›æ¼”ç¤ºæ—¶ï¼Œè¯­è¨€æ¨¡å‹çš„æ€§èƒ½å¯ä»¥å¾—åˆ°æ˜¾è‘—çš„æå‡ã€‚ç„¶è€Œï¼Œå¯¹äºè·¨è¯­è¨€åœºæ™¯ä¸‹ICLçš„æ¨¡å‹æ ¡å‡†å’Œé¢„æµ‹ç½®ä¿¡åº¦çš„å…³æ³¨åº¦å¾ˆä½ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹è·¨è¯­è¨€æƒ…æ„Ÿåˆ†ç±»çš„ICLè¿›è¡Œäº†æ·±å…¥çš„åˆ†æã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œåœ¨è·¨è¯­è¨€åœºæ™¯ä¸­ï¼ŒICLè¡¨ç°ä¸ä½³ï¼Œå‡†ç¡®ç‡è¾ƒä½ï¼Œå¹¶ä¸”å­˜åœ¨è¾ƒé«˜çš„æ ¡å‡†è¯¯å·®ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•N2C2ï¼Œå®ƒé‡‡ç”¨-æœ€è¿‘é‚»å¢å¼ºåˆ†ç±»å™¨è¿›è¡Œé¢„æµ‹ç½®ä¿¡åº¦æ ¡å‡†ã€‚N2C2é€šè¿‡åˆ©ç”¨å­˜å‚¨å°‘é‡å®ä¾‹çš„æ•°æ®å­˜å‚¨åº“æ¥ç¼©å°é¢„æµ‹å·®è·ã€‚å…·ä½“æ¥è¯´ï¼ŒN2C2ç»“åˆäº†æ•°æ®å­˜å‚¨ä¸­çš„é¢„æµ‹ï¼Œå¹¶èå…¥äº†ä¿¡å¿ƒæ„ŸçŸ¥åˆ†å¸ƒã€è¯­ä¹‰ä¸€è‡´æ£€ç´¢è¡¨ç¤ºå’Œè‡ªé€‚åº”é‚»å±…ç»„åˆæ¨¡å—ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨æœ‰é™çš„æ”¯æŒå®ä¾‹ã€‚åœ¨ä¸¤ä¸ªå¤šè¯­è¨€æƒ…æ„Ÿåˆ†ç±»æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒN2C2ä¼˜äºä¼ ç»Ÿçš„ICLã€‚åœ¨å‡†ç¡®æ€§å’Œæ ¡å‡†è¯¯å·®æ–¹é¢ï¼Œå®ƒè¶…è¶Šäº†å¾®è°ƒã€æç¤ºè°ƒæ•´å’Œæœ€è¿‘çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09218v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨è·¨è¯­è¨€åœºæ™¯ä¸­ï¼Œè¯­å¢ƒå­¦ä¹ ï¼ˆICLï¼‰çš„è¡¨ç°æœ‰å¾…æå‡ï¼Œå­˜åœ¨å‡†ç¡®æ€§ä½å’Œæ ¡å‡†è¯¯å·®é«˜çš„é—®é¢˜ã€‚é’ˆå¯¹æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºN2C2çš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨æœ€è¿‘é‚»å¢å¼ºåˆ†ç±»å™¨è¿›è¡Œé¢„æµ‹ä¿¡å¿ƒæ ¡å‡†ã€‚N2C2é€šè¿‡åˆ©ç”¨ç¼“å­˜çš„å°‘é‡å®ä¾‹æ•°æ®æ¥ç¼©å°é¢„æµ‹å·®è·ï¼Œå¹¶æœ‰æ•ˆç»“åˆä¿¡å¿ƒæ„ŸçŸ¥åˆ†å¸ƒã€è¯­ä¹‰ä¸€è‡´æ£€ç´¢è¡¨ç¤ºå’Œè‡ªé€‚åº”é‚»å±…ç»„åˆæ¨¡å—ã€‚åœ¨ä¸¤ç§å¤šè¯­è¨€æƒ…æ„Ÿåˆ†ç±»æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒN2C2åœ¨å‡†ç¡®æ€§å’Œæ ¡å‡†è¯¯å·®æ–¹é¢è¶…è¶Šäº†ä¼ ç»Ÿçš„ICLã€å¾®è°ƒã€æç¤ºè°ƒæ•´å’Œæœ€è¿‘çš„é«˜çº§æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­å¢ƒå­¦ä¹ ï¼ˆICLï¼‰åœ¨è·¨è¯­è¨€æƒ…æ„Ÿåˆ†ç±»ä¸­è¡¨ç°ä¸ä½³ï¼Œå­˜åœ¨å‡†ç¡®åº¦å’Œæ ¡å‡†é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•N2C2ï¼Œåˆ©ç”¨æœ€è¿‘é‚»å¢å¼ºåˆ†ç±»å™¨è¿›è¡Œé¢„æµ‹ä¿¡å¿ƒæ ¡å‡†ã€‚</li>
<li>N2C2é€šè¿‡ç¼“å­˜çš„å°‘é‡å®ä¾‹æ•°æ®æ¥ç¼©å°é¢„æµ‹å·®è·ã€‚</li>
<li>N2C2ç»“åˆäº†ä¿¡å¿ƒæ„ŸçŸ¥åˆ†å¸ƒã€è¯­ä¹‰ä¸€è‡´æ£€ç´¢è¡¨ç¤ºå’Œè‡ªé€‚åº”é‚»å±…ç»„åˆæ¨¡å—ã€‚</li>
<li>N2C2åœ¨å‡†ç¡®æ€§å’Œæ ¡å‡†è¯¯å·®æ–¹é¢è¶…è¶Šäº†ä¼ ç»Ÿçš„è¯­å¢ƒå­¦ä¹ æ–¹æ³•å’Œå¾®è°ƒã€æç¤ºè°ƒæ•´æŠ€æœ¯ã€‚</li>
<li>N2C2æ–¹æ³•åœ¨è·¨è¯­è¨€æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸­å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d524f2e89579c3ca032c6e6325192756.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70e3bddec422b8e17c72c281ed127da4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72da43c4e7b47f0700820a54857c817c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01b1be3a8e2a3536e8e1811929183ce8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MMRL-Multi-Modal-Representation-Learning-for-Vision-Language-Models"><a href="#MMRL-Multi-Modal-Representation-Learning-for-Vision-Language-Models" class="headerlink" title="MMRL: Multi-Modal Representation Learning for Vision-Language Models"></a>MMRL: Multi-Modal Representation Learning for Vision-Language Models</h2><p><strong>Authors:Yuncheng Guo, Xiaodong Gu</strong></p>
<p>Large-scale pre-trained Vision-Language Models (VLMs) have become essential for transfer learning across diverse tasks. However, adapting these models with limited few-shot data often leads to overfitting, diminishing their performance on new tasks. To tackle this issue, we propose a novel Multi-Modal Representation Learning (MMRL) framework that introduces a shared, learnable, and modality-agnostic representation space. MMRL projects the space tokens to text and image representation tokens, facilitating more effective multi-modal interactions. Unlike previous approaches that solely optimize class token features, MMRL integrates representation tokens at higher layers of the encodersâ€“where dataset-specific features are more prominentâ€“while preserving generalized knowledge in the lower layers. During training, both representation and class features are optimized, with trainable projection layer applied to the representation tokens, whereas the class token projection layer remains frozen to retain pre-trained knowledge. Furthermore, a regularization term is introduced to align the class features and text features with the zero-shot features from the frozen VLM, thereby safeguarding the modelâ€™s generalization capacity. For inference, a decoupling strategy is employed, wherein both representation and class features are utilized for base classes, while only the class features, which retain more generalized knowledge, are used for new tasks. Extensive experiments across 15 datasets demonstrate that MMRL outperforms state-of-the-art methods, achieving a balanced trade-off between task-specific adaptation and generalization. Code is available at <a target="_blank" rel="noopener" href="https://github.com/yunncheng/MMRL">https://github.com/yunncheng/MMRL</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹äºè·¨ä¸åŒä»»åŠ¡çš„è¿ç§»å­¦ä¹ å·²ç»å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä½¿ç”¨æœ‰é™çš„å°‘é‡æ•°æ®é€‚åº”è¿™äº›æ¨¡å‹å¾€å¾€ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œä»è€Œåœ¨æ–°çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼ˆMMRLï¼‰æ¡†æ¶ï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªå…±äº«ã€å¯å­¦ä¹ å’Œæ¨¡æ€æ— å…³çš„è¡¨ç¤ºç©ºé—´ã€‚MMRLå°†ç©ºé—´æ ‡è®°æŠ•å½±åˆ°æ–‡æœ¬å’Œå›¾åƒè¡¨ç¤ºæ ‡è®°ä¸Šï¼Œä¿ƒè¿›äº†æ›´æœ‰æ•ˆçš„å¤šæ¨¡æ€äº¤äº’ã€‚ä¸ä»¥å‰ä»…ä¼˜åŒ–ç±»åˆ«æ ‡è®°ç‰¹å¾çš„æ–¹æ³•ä¸åŒï¼ŒMMRLåœ¨ç¼–ç å™¨çš„æ›´é«˜å±‚é›†æˆäº†è¡¨ç¤ºæ ‡è®°ï¼Œè¿™äº›å±‚çº§çš„æ•°æ®é›†ç‰¹å®šç‰¹å¾æ›´ä¸ºçªå‡ºï¼ŒåŒæ—¶åœ¨ä¸‹å±‚ä¿æŒé€šç”¨çŸ¥è¯†ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¡¨ç¤ºå’Œç±»åˆ«ç‰¹å¾éƒ½å¾—åˆ°äº†ä¼˜åŒ–ï¼Œå¯¹è¡¨ç¤ºæ ‡è®°åº”ç”¨äº†å¯è®­ç»ƒçš„æŠ•å½±å±‚ï¼Œè€Œç±»åˆ«æ ‡è®°æŠ•å½±å±‚åˆ™ä¿æŒå†»ç»“ä»¥ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†æ­£åˆ™åŒ–é¡¹æ¥å¯¹é½ç±»åˆ«ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ä¸å†»ç»“VLMçš„é›¶å°„å‡»ç‰¹å¾ï¼Œä»è€Œä¿æŠ¤æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å¯¹äºæ¨æ–­ï¼Œé‡‡ç”¨äº†è§£è€¦ç­–ç•¥ï¼Œå…¶ä¸­è¡¨ç¤ºå’Œç±»åˆ«ç‰¹å¾éƒ½ç”¨äºåŸºç¡€ç±»ï¼Œè€Œä»…ä½¿ç”¨ä¿ç•™æ›´å¤šé€šç”¨çŸ¥è¯†çš„ç±»åˆ«ç‰¹å¾ç”¨äºæ–°ä»»åŠ¡ã€‚åœ¨15ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMMRLä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œå®ç°äº†ä»»åŠ¡ç‰¹å®šé€‚åº”æ€§å’Œæ³›åŒ–ä¹‹é—´çš„å¹³è¡¡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yunncheng/MMRL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yunncheng/MMRLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08497v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è¿ç§»å­¦ä¹ å„ç§ä»»åŠ¡ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚ç„¶è€Œï¼Œä½¿ç”¨æœ‰é™çš„å°‘é‡æ•°æ®è¿›è¡Œæ¨¡å‹é€‚åº”å¸¸å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå½±å“æ–°ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°å‹å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼ˆMMRLï¼‰æ¡†æ¶ï¼Œå¼•å…¥å…±äº«ã€å¯å­¦ä¹ å’Œæ¨¡æ€æ— å…³çš„è¡¨ç¤ºç©ºé—´ã€‚MMRLå°†ç©ºé—´æ ‡è®°æŠ•å½±åˆ°æ–‡æœ¬å’Œå›¾åƒè¡¨ç¤ºæ ‡è®°ï¼Œå®ç°æ›´æœ‰æ•ˆçš„å¤šæ¨¡æ€äº¤äº’ã€‚ä¸åŒäºä»…ä¼˜åŒ–ç±»åˆ«æ ‡è®°ç‰¹å¾çš„å…ˆå‰æ–¹æ³•ï¼ŒMMRLåœ¨ç¼–ç å™¨çš„é«˜å±‚é›†æˆè¡¨ç¤ºæ ‡è®°ï¼ŒåŒæ—¶ä¿ç•™ä½å±‚çš„é€šç”¨çŸ¥è¯†ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŒæ—¶ä¼˜åŒ–è¡¨ç¤ºå’Œç±»åˆ«ç‰¹å¾ï¼Œåº”ç”¨å¯è®­ç»ƒçš„æŠ•å½±å±‚äºè¡¨ç¤ºæ ‡è®°ï¼Œè€Œå†»ç»“ç±»åˆ«æ ‡è®°æŠ•å½±å±‚ä»¥ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œå¼•å…¥æ­£åˆ™åŒ–é¡¹ä»¥å¯¹é½ç±»åˆ«ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ï¼Œä¸å†»ç»“VLMçš„é›¶æ ·æœ¬ç‰¹å¾ï¼Œä»è€Œä¿éšœæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ¨ç†æ—¶ï¼Œé‡‡ç”¨è§£è€¦ç­–ç•¥ï¼ŒåŒæ—¶åˆ©ç”¨è¡¨ç¤ºå’Œç±»åˆ«ç‰¹å¾è¿›è¡ŒåŸºç¡€ç±»åˆ«çš„å¤„ç†ï¼Œè€Œä»…ä½¿ç”¨ä¿ç•™æ›´å¤šé€šç”¨çŸ¥è¯†çš„ç±»åˆ«ç‰¹å¾åº”å¯¹æ–°ä»»åŠ¡ã€‚è·¨15ä¸ªæ•°æ®é›†çš„å¹¿æ³›å®éªŒæ˜¾ç¤ºï¼ŒMMRLä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†ä»»åŠ¡ç‰¹å®šé€‚åº”å’Œæ³›åŒ–ä¹‹é—´çš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è¿ç§»å­¦ä¹ ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ä½¿ç”¨æœ‰é™çš„å°‘é‡æ•°æ®è¿›è¡Œæ¨¡å‹é€‚åº”å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>æ–°å‹å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼ˆMMRLï¼‰æ¡†æ¶å¼•å…¥å…±äº«ã€å¯å­¦ä¹ å’Œæ¨¡æ€æ— å…³çš„è¡¨ç¤ºç©ºé—´ã€‚</li>
<li>MMRLé€šè¿‡æŠ•å½±ç©ºé—´æ ‡è®°åˆ°æ–‡æœ¬å’Œå›¾åƒè¡¨ç¤ºæ ‡è®°ï¼Œå®ç°å¤šæ¨¡æ€äº¤äº’ã€‚</li>
<li>MMRLåœ¨ç¼–ç å™¨çš„é«˜å±‚é›†æˆè¡¨ç¤ºæ ‡è®°ï¼ŒåŒæ—¶ä¿ç•™ä½å±‚çš„é€šç”¨çŸ¥è¯†ã€‚</li>
<li>åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŒæ—¶ä¼˜åŒ–è¡¨ç¤ºå’Œç±»åˆ«ç‰¹å¾ï¼Œå¹¶åº”ç”¨æ­£åˆ™åŒ–é¡¹ä»¥å¯¹é½ç±»åˆ«ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08497">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-db8e9041f69f7afeb18815ef80b34c0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dae975ba432b158293f3f1a0ab1b109b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb187fc42024f5b87086306375cfba0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f7612300d9f368546758d7630a0a27b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Personalized-Code-Readability-Assessment-Are-We-There-Yet"><a href="#Personalized-Code-Readability-Assessment-Are-We-There-Yet" class="headerlink" title="Personalized Code Readability Assessment: Are We There Yet?"></a>Personalized Code Readability Assessment: Are We There Yet?</h2><p><strong>Authors:Antonio Vitale, Emanuela Guglielmi, Rocco Oliveto, Simone Scalabrino</strong></p>
<p>Unreadable code could be a breeding ground for errors. Thus, previous work defined approaches based on machine learning to automatically assess code readability that can warn developers when some code artifacts (e.g., classes) become unreadable. Given datasets of code snippets manually evaluated by several developers in terms of their perceived readability, such approaches (i) establish a snippet-level ground truth, and (ii) train a binary (readable&#x2F;unreadable) or a ternary (readable&#x2F;neutral&#x2F;unreadable) code readability classifier. Given this procedure, all existing approaches neglect the subjectiveness of code readability, i.e., the possible different developer-specific nuances in the code readability perception. In this paper, we aim to understand to what extent it is possible to assess code readability as subjectively perceived by developers through a personalized code readability assessment approach. This problem is significantly more challenging than the snippet-level classification problem: We assume that, in a realistic scenario, a given developer is keen to provide only a few code readability evaluations, thus less data is available. For this reason, we adopt an LLM with few-shot learning to achieve our goal. Our results, however, show that such an approach achieves worse results than a state-of-the-art feature-based model that is trained to work at the snippet-level. We tried to understand why this happens by looking more closely at the quality of the available code readability datasets and assessed the consistency of the inter-developer evaluations. We observed that up to a third of the evaluations are self-contradictory. Our negative results call for new and more reliable code readability datasets. </p>
<blockquote>
<p>éš¾ä»¥ç†è§£çš„ä»£ç å¯èƒ½ä¼šæˆä¸ºé”™è¯¯çš„æ»‹ç”Ÿåœ°ã€‚å› æ­¤ï¼Œå…ˆå‰çš„å·¥ä½œå®šä¹‰äº†åŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œä»¥è‡ªåŠ¨è¯„ä¼°ä»£ç å¯è¯»æ€§ï¼Œå½“æŸäº›ä»£ç äº§ç‰©ï¼ˆä¾‹å¦‚ç±»ï¼‰å˜å¾—éš¾ä»¥ç†è§£æ—¶ï¼Œå¯ä»¥è­¦å‘Šå¼€å‘äººå‘˜ã€‚ç»™å®šç”±å¤šä¸ªå¼€å‘äººå‘˜æ‰‹åŠ¨è¯„ä¼°çš„ä»£ç ç‰‡æ®µæ•°æ®é›†ï¼Œåœ¨æ„ŸçŸ¥å¯è¯»æ€§æ–¹é¢ï¼Œè¿™äº›æ–¹æ³•ï¼ˆiï¼‰å»ºç«‹ç‰‡æ®µçº§åˆ«çš„åŸºå‡†çœŸå®å€¼ï¼Œï¼ˆiiï¼‰è®­ç»ƒäºŒè¿›åˆ¶ï¼ˆå¯è¯»&#x2F;ä¸å¯è¯»ï¼‰æˆ–ä¸‰å…ƒï¼ˆå¯è¯»&#x2F;ä¸­æ€§&#x2F;ä¸å¯è¯»ï¼‰çš„ä»£ç å¯è¯»æ€§åˆ†ç±»å™¨ã€‚é‰´äºè¿™ä¸€ç¨‹åºï¼Œæ‰€æœ‰ç°æœ‰æ–¹æ³•éƒ½å¿½ç•¥äº†ä»£ç å¯è¯»æ€§çš„ä¸»è§‚æ€§ï¼Œå³å¼€å‘äººå‘˜å¯¹ä»£ç å¯è¯»æ€§æ„ŸçŸ¥ä¸­å¯èƒ½å­˜åœ¨çš„ä¸åŒç‰¹å®šç»†å¾®å·®åˆ«ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯äº†è§£åœ¨ä½•ç§ç¨‹åº¦ä¸Šå¯ä»¥é€šè¿‡ä¸ªæ€§åŒ–çš„ä»£ç å¯è¯»æ€§è¯„ä¼°æ–¹æ³•æ¥ä¸»è§‚åœ°è¯„ä¼°å¼€å‘äººå‘˜æ‰€æ„ŸçŸ¥çš„ä»£ç å¯è¯»æ€§ã€‚è¿™ä¸ªé—®é¢˜æ¯”ç‰‡æ®µçº§åˆ«çš„åˆ†ç±»é—®é¢˜æ›´å…·æŒ‘æˆ˜æ€§ï¼šæˆ‘ä»¬å‡è®¾åœ¨ç°å®åœºæ™¯ä¸­ï¼Œç»™å®šçš„å¼€å‘äººå‘˜çƒ­è¡·äºæä¾›æœ‰é™çš„ä»£ç å¯è¯»æ€§è¯„ä¼°ï¼Œå› æ­¤å¯ç”¨çš„æ•°æ®è¾ƒå°‘ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨å…·æœ‰å°æ ·æœ¬å­¦ä¹ çš„LLMæ¥å®ç°æˆ‘ä»¬çš„ç›®æ ‡ã€‚ç„¶è€Œï¼Œç»“æœå´æ˜¾ç¤ºï¼Œè¿™ç§æ–¹æ³•çš„ç»“æœä¸å¦‚åŸºäºç‰¹å¾çš„æœ€æ–°æ¨¡å‹ï¼ˆè¯¥æ¨¡å‹ç»è¿‡è®­ç»ƒä»¥åœ¨ç‰‡æ®µçº§åˆ«å·¥ä½œï¼‰ã€‚ä¸ºäº†äº†è§£ä¸ºä»€ä¹ˆä¼šå‘ç”Ÿè¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬æ›´ä»”ç»†åœ°ç ”ç©¶äº†å¯ç”¨çš„ä»£ç å¯è¯»æ€§æ•°æ®é›†çš„è´¨é‡å¹¶è¯„ä¼°äº†å¼€å‘äººå‘˜ä¹‹é—´è¯„ä»·çš„è¿è´¯æ€§ã€‚æˆ‘ä»¬å‘ç°é«˜è¾¾ä¸‰åˆ†ä¹‹ä¸€çš„è¯„ä¼°æ˜¯ç›¸äº’çŸ›ç›¾çš„ã€‚æˆ‘ä»¬çš„è´Ÿé¢ç»“æœå‘¼åéœ€è¦æ–°çš„ã€æ›´å¯é çš„ä»£ç å¯è¯»æ€§æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07870v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨ä»£ç å¯è¯»æ€§è¯„ä¼°çš„ä¸»è§‚æ€§é—®é¢˜ï¼Œå³ä¸åŒå¼€å‘è€…å¯¹ä»£ç å¯è¯»æ€§çš„æ„ŸçŸ¥å¯èƒ½å­˜åœ¨å·®å¼‚ã€‚ç ”ç©¶è€…æ—¨åœ¨é€šè¿‡ä¸ªæ€§åŒ–ä»£ç å¯è¯»æ€§è¯„ä¼°æ–¹æ³•æ¥ç†è§£å¦‚ä½•è¯„ä¼°å¼€å‘è€…ä¸»è§‚æ„ŸçŸ¥çš„ä»£ç å¯è¯»æ€§ã€‚å°½ç®¡é‡‡ç”¨äº†å…·æœ‰few-shotå­¦ä¹ åŠŸèƒ½çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½†è¯¥æ–¹æ³•çš„æ•ˆæœå´ä¸å¦‚åŸºäºç‰¹å¾çš„ç‰‡æ®µçº§åˆ«åˆ†ç±»æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰ä»£ç å¯è¯»æ€§æ•°æ®é›†å­˜åœ¨è´¨é‡é—®é¢˜ï¼Œå¼€å‘è€…ä¹‹é—´çš„è¯„ä»·ä¸€è‡´æ€§æœ‰å¾…æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ æ–¹æ³•å¯è‡ªåŠ¨è¯„ä¼°ä»£ç å¯è¯»æ€§ï¼Œè­¦å‘Šå¼€å‘è€…å½“ä»£ç ç‰‡æ®µå˜å¾—ä¸å¯è¯»æ—¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥äº†ä»£ç å¯è¯»æ€§è¯„ä¼°ä¸­çš„ä¸»è§‚æ€§ï¼Œå³ä¸åŒå¼€å‘è€…å¯¹ä»£ç å¯è¯»æ€§çš„æ„ŸçŸ¥å·®å¼‚ã€‚</li>
<li>ç ”ç©¶è€…å°è¯•é€šè¿‡ä¸ªæ€§åŒ–ä»£ç å¯è¯»æ€§è¯„ä¼°æ–¹æ³•æ¥ç†è§£è¿™ä¸€ä¸»è§‚æ€§ã€‚</li>
<li>é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œfew-shotå­¦ä¹ ä»¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œä½†æ•ˆæœä¸å¦‚åŸºäºç‰¹å¾çš„ç‰‡æ®µçº§åˆ«åˆ†ç±»æ¨¡å‹ã€‚</li>
<li>ç°å­˜çš„ä»£ç å¯è¯»æ€§æ•°æ®é›†å­˜åœ¨è´¨é‡é—®é¢˜ï¼Œä¾‹å¦‚éƒ¨åˆ†è¯„ä»·è‡ªç›¸çŸ›ç›¾ã€‚</li>
<li>ç ”ç©¶ç»“æœå¼ºè°ƒäº†éœ€è¦æ–°çš„ã€æ›´å¯é çš„ä»£ç å¯è¯»æ€§æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07870">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-88b64da22ac5285e6fbe3ba9af8dafaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2510c95e419144f9ac68399271737ad7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-255aea8838cd36cf99471e91591fa87c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bd617391296c86f232bfe71a9b622a3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PointVLA-Injecting-the-3D-World-into-Vision-Language-Action-Models"><a href="#PointVLA-Injecting-the-3D-World-into-Vision-Language-Action-Models" class="headerlink" title="PointVLA: Injecting the 3D World into Vision-Language-Action Models"></a>PointVLA: Injecting the 3D World into Vision-Language-Action Models</h2><p><strong>Authors:Chengmeng Li, Junjie Wen, Yan Peng, Yaxin Peng, Feifei Feng, Yichen Zhu</strong></p>
<p>Vision-Language-Action (VLA) models excel at robotic tasks by leveraging large-scale 2D vision-language pretraining, but their reliance on RGB images limits spatial reasoning critical for real-world interaction. Retraining these models with 3D data is computationally prohibitive, while discarding existing 2D datasets wastes valuable resources. To bridge this gap, we propose PointVLA, a framework that enhances pre-trained VLAs with point cloud inputs without requiring retraining. Our method freezes the vanilla action expert and injects 3D features via a lightweight modular block. To identify the most effective way of integrating point cloud representations, we conduct a skip-block analysis to pinpoint less useful blocks in the vanilla action expert, ensuring that 3D features are injected only into these blocksâ€“minimizing disruption to pre-trained representations.   Extensive experiments demonstrate that PointVLA outperforms state-of-the-art 2D imitation learning methods, such as OpenVLA, Diffusion Policy and DexVLA, across both simulated and real-world robotic tasks. Specifically, we highlight several key advantages of PointVLA enabled by point cloud integration: (1) Few-shot multi-tasking, where PointVLA successfully performs four different tasks using only 20 demonstrations each; (2) Real-vs-photo discrimination, where PointVLA distinguishes real objects from their images, leveraging 3D world knowledge to improve safety and reliability; (3) Height adaptability, Unlike conventional 2D imitation learning methods, PointVLA enables robots to adapt to objects at varying table height that unseen in train data. Furthermore, PointVLA achieves strong performance in long-horizon tasks, such as picking and packing objects from a moving conveyor belt, showcasing its ability to generalize across complex, dynamic environments. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡äºŒç»´è§†è§‰è¯­è¨€é¢„è®­ç»ƒåœ¨æœºå™¨äººä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¯¹RGBå›¾åƒçš„ä¾èµ–é™åˆ¶äº†ç©ºé—´æ¨ç†ï¼Œè¿™å¯¹äºç°å®ä¸–ç•Œäº¤äº’è‡³å…³é‡è¦ã€‚ä½¿ç”¨ä¸‰ç»´æ•°æ®å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå†è®­ç»ƒè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œè€Œæ”¾å¼ƒç°æœ‰çš„äºŒç»´æ•°æ®é›†åˆ™æµªè´¹å®è´µèµ„æºã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†PointVLAæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥åœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå¢å¼ºé¢„è®­ç»ƒçš„VLAæ¨¡å‹ä»¥å¤„ç†ç‚¹äº‘è¾“å…¥ã€‚æˆ‘ä»¬çš„æ–¹æ³•å†»ç»“äº†åŸå§‹åŠ¨ä½œä¸“å®¶æ¨¡å‹ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªè½»é‡çº§æ¨¡å—å—æ³¨å…¥ä¸‰ç»´ç‰¹å¾ã€‚ä¸ºäº†ç¡®å®šæ•´åˆç‚¹äº‘è¡¨ç¤ºçš„æœ€æœ‰æ•ˆæ–¹å¼ï¼Œæˆ‘ä»¬è¿›è¡Œäº†è·³è¿‡å—åˆ†æï¼Œä»¥å®šä½åŸå§‹åŠ¨ä½œä¸“å®¶æ¨¡å‹ä¸­ä¸å¤ªæœ‰ç”¨çš„å—ï¼Œç¡®ä¿ä¸‰ç»´ç‰¹å¾ä»…æ³¨å…¥è¿™äº›å—ä¸­ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘å¯¹é¢„è®­ç»ƒè¡¨ç¤ºçš„å¹²æ‰°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPointVLAåœ¨æ¨¡æ‹Ÿå’Œç°å®ä¸–ç•Œæœºå™¨äººä»»åŠ¡ä¸Šå‡ä¼˜äºæœ€æ–°çš„äºŒç»´æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œå¦‚OpenVLAã€Diffusion Policyå’ŒDexVLAã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çªå‡ºäº†é€šè¿‡ç‚¹äº‘é›†æˆå®ç°çš„PointVLAçš„å‡ ä¸ªå…³é”®ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰å°‘æ ·æœ¬å¤šä»»åŠ¡å¤„ç†èƒ½åŠ›ï¼Œå…¶ä¸­PointVLAä»…ä½¿ç”¨æ¯ä¸ªä»»åŠ¡20æ¬¡æ¼”ç¤ºå°±æˆåŠŸå®Œæˆäº†å››ä¸ªä¸åŒä»»åŠ¡ï¼›ï¼ˆ2ï¼‰çœŸå®ä¸ç…§ç‰‡åŒºåˆ†èƒ½åŠ›ï¼ŒPointVLAèƒ½å¤ŸåŒºåˆ†çœŸå®ç‰©ä½“å’Œå®ƒä»¬çš„å›¾åƒï¼Œåˆ©ç”¨ä¸‰ç»´ä¸–ç•ŒçŸ¥è¯†æé«˜å®‰å…¨æ€§å’Œå¯é æ€§ï¼›ï¼ˆ3ï¼‰é«˜åº¦é€‚åº”æ€§ï¼šä¸ä¼ ç»Ÿçš„äºŒç»´æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒPointVLAä½¿æœºå™¨äººèƒ½å¤Ÿé€‚åº”è®­ç»ƒä¸­æœªè§è¿‡çš„ä¸åŒé«˜åº¦çš„æ¡Œå­ä¸Šçš„ç‰©ä½“ã€‚ï¼ˆ4ï¼‰æ­¤å¤–ï¼ŒPointVLAåœ¨æ‰§è¡Œé•¿æœŸä»»åŠ¡æ—¶ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¦‚ä»ç§»åŠ¨ä¼ é€å¸¦ä¸ŠæŒ‘é€‰å’Œæ‰“åŒ…ç‰©ä½“ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07511v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡äºŒç»´è§†è§‰è¯­è¨€é¢„è®­ç»ƒçš„Vision-Language-Actionï¼ˆVLAï¼‰æ¨¡å‹åœ¨æœºå™¨äººä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å…¶ä¾èµ–RGBå›¾åƒé™åˆ¶äº†ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºçš„PointVLAæ¡†æ¶èƒ½å¤Ÿåœ¨ä¸éœ€è¦é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå¢å¼ºé¢„è®­ç»ƒçš„VLAæ¨¡å‹å¯¹ç‚¹äº‘è¾“å…¥çš„å¤„ç†èƒ½åŠ›ã€‚é€šè¿‡å†»ç»“åŸå§‹åŠ¨ä½œä¸“å®¶å¹¶æ³¨å…¥3Dç‰¹å¾ï¼ŒPointVLAå®ç°äº†ä¸ç‚¹äº‘è¡¨ç¤ºçš„æœ‰æ•ˆé›†æˆã€‚å®éªŒè¡¨æ˜ï¼ŒPointVLAåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººä»»åŠ¡ä¸Šå‡ä¼˜äºæœ€æ–°çš„äºŒç»´æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PointVLAæ¡†æ¶å¢å¼ºäº†é¢„è®­ç»ƒçš„Vision-Language-Actionï¼ˆVLAï¼‰æ¨¡å‹å¯¹ç‚¹äº‘è¾“å…¥çš„å¤„ç†èƒ½åŠ›ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</li>
<li>é€šè¿‡å†»ç»“åŸå§‹åŠ¨ä½œä¸“å®¶å¹¶æ³¨å…¥3Dç‰¹å¾ï¼Œå®ç°äº†ä¸ç‚¹äº‘è¡¨ç¤ºçš„æœ‰æ•ˆé›†æˆã€‚</li>
<li>PointVLAåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°çš„äºŒç»´æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>PointVLAæ”¯æŒfew-shotå¤šä»»åŠ¡å­¦ä¹ ï¼Œèƒ½å¤Ÿä½¿ç”¨å°‘é‡æ¼”ç¤ºå®Œæˆå¤šä¸ªä»»åŠ¡ã€‚</li>
<li>PointVLAèƒ½å¤ŸåŒºåˆ†çœŸå®ç‰©ä½“å’Œå›¾åƒï¼Œåˆ©ç”¨3Dä¸–ç•ŒçŸ¥è¯†æé«˜å®‰å…¨æ€§å’Œå¯é æ€§ã€‚</li>
<li>PointVLAå…·æœ‰é«˜åº¦çš„é€‚åº”æ€§ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒé«˜åº¦çš„ç‰©ä½“ï¼Œä¸åŒäºä¼ ç»Ÿçš„äºŒç»´æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cd4cb28ef57ca631b9fdcfad825769c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a70a08352f90368401bc96bcfbac63c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4e0fe57ed497f54a83bb6e53c020e32.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2324fc0a8ef047a41b0045c91892a781.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-519564003962391a5648efff7e754dfd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="X2CT-CLIP-Enable-Multi-Abnormality-Detection-in-Computed-Tomography-from-Chest-Radiography-via-Tri-Modal-Contrastive-Learning"><a href="#X2CT-CLIP-Enable-Multi-Abnormality-Detection-in-Computed-Tomography-from-Chest-Radiography-via-Tri-Modal-Contrastive-Learning" class="headerlink" title="X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography   from Chest Radiography via Tri-Modal Contrastive Learning"></a>X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography   from Chest Radiography via Tri-Modal Contrastive Learning</h2><p><strong>Authors:Jianzhong You, Yuan Gao, Sangwook Kim, Chris Mcintosh</strong></p>
<p>Computed tomography (CT) is a key imaging modality for diagnosis, yet its clinical utility is marred by high radiation exposure and long turnaround times, restricting its use for larger-scale screening. Although chest radiography (CXR) is more accessible and safer, existing CXR foundation models focus primarily on detecting diseases that are readily visible on the CXR. Recently, works have explored training disease classification models on simulated CXRs, but they remain limited to recognizing a single disease type from CT. CT foundation models have also emerged with significantly improved detection of pathologies in CT. However, the generalized application of CT-derived labels on CXR has remained illusive. In this study, we propose X2CT-CLIP, a tri-modal knowledge transfer learning framework that bridges the modality gap between CT and CXR while reducing the computational burden of model training. Our approach is the first work to enable multi-abnormality classification in CT, using CXR, by transferring knowledge from 3D CT volumes and associated radiology reports to a CXR encoder via a carefully designed tri-modal alignment mechanism in latent space. Extensive evaluations on three multi-label CT datasets demonstrate that our method outperforms state-of-the-art baselines in cross-modal retrieval, few-shot adaptation, and external validation. These results highlight the potential of CXR, enriched with knowledge derived from CT, as a viable efficient alternative for disease detection in resource-limited settings. </p>
<blockquote>
<p>è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ˜¯è¯Šæ–­çš„å…³é”®æˆåƒæ–¹å¼ï¼Œä½†å…¶ä¸´åºŠåº”ç”¨å—åˆ°é«˜è¾å°„æš´éœ²å’Œé•¿æ—¶é—´ç­‰å¾…ç»“æœçš„é™åˆ¶ï¼Œé˜»ç¢äº†å…¶åœ¨å¤§è§„æ¨¡ç­›æŸ¥ä¸­çš„ä½¿ç”¨ã€‚è™½ç„¶èƒ¸éƒ¨æ”¾å°„æ‘„å½±ï¼ˆCXRï¼‰æ›´å®¹æ˜“è·å–ä¸”æ›´å®‰å…¨ï¼Œä½†ç°æœ‰çš„CXRåŸºç¡€æ¨¡å‹ä¸»è¦å…³æ³¨äºæ£€æµ‹åœ¨CXRä¸Šå®¹æ˜“çœ‹åˆ°çš„ç–¾ç—…ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œè™½å·²æ¢ç´¢åœ¨æ¨¡æ‹Ÿçš„CXRä¸Šè¿›è¡Œç–¾ç—…åˆ†ç±»æ¨¡å‹è®­ç»ƒï¼Œä½†å®ƒä»¬ä»…é™äºä»CTå›¾åƒä¸­è¯†åˆ«å•ä¸€ç–¾ç—…ç±»å‹ã€‚CTåŸºç¡€æ¨¡å‹ä¹Ÿå·²å‡ºç°ï¼Œæ˜¾è‘—æé«˜äº†CTä¸­ç—…ç†çš„æ£€æµ‹èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†CTè¡ç”Ÿçš„æ ‡ç­¾åœ¨CXRä¸Šçš„é€šç”¨åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†X2CT-CLIPï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰æ¨¡æ€çŸ¥è¯†è¿ç§»å­¦ä¹ æ¡†æ¶ï¼Œç¼©å°äº†CTå’ŒCXRä¹‹é—´çš„æ¨¡æ€å·®è·ï¼ŒåŒæ—¶é™ä½äº†æ¨¡å‹è®­ç»ƒçš„è®¡ç®—è´Ÿæ‹…ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯é€šè¿‡ä»3D CTä½“ç§¯å’Œç›¸å…³æ”¾å°„å­¦æŠ¥å‘Šå‘CXRç¼–ç å™¨è½¬ç§»çŸ¥è¯†ï¼Œåˆ©ç”¨ç²¾å¿ƒè®¾è®¡çš„æ½œåœ¨ç©ºé—´ä¸‰æ¨¡æ€å¯¹é½æœºåˆ¶ï¼Œå®ç°äº†ä½¿ç”¨CXRåœ¨CTä¸­è¿›è¡Œå¤šå¼‚å¸¸åˆ†ç±»çš„é¦–é¡¹å·¥ä½œã€‚åœ¨ä¸‰ä¸ªå¤šæ ‡ç­¾CTæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è·¨æ¨¡æ€æ£€ç´¢ã€å°‘æ ·æœ¬é€‚åº”å’Œå¤–éƒ¨éªŒè¯æ–¹é¢å‡ä¼˜äºæœ€æ–°åŸºçº¿ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†ä»¥CTçŸ¥è¯†ä¸°å¯ŒCXRçš„æ½œåŠ›ï¼Œå¯ä½œä¸ºèµ„æºæœ‰é™ç¯å¢ƒä¸­ç–¾ç—…æ£€æµ‹çš„å¯è¡Œé«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02162v2">PDF</a> 11 pages, 1 figure, 5 tables</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºX2CT-CLIPçš„ä¸‰æ¨¡æ€çŸ¥è¯†è¿ç§»å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ç¼©å°CTå’ŒCXRä¹‹é—´çš„æ¨¡æ€å·®è·ï¼ŒåŒæ—¶é™ä½æ¨¡å‹è®­ç»ƒçš„è®¡ç®—è´Ÿæ‹…ã€‚è¯¥æ¡†æ¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ä¸‰æ¨¡æ€å¯¹é½æœºåˆ¶ï¼Œå°†æ¥è‡ªä¸‰ç»´CTä½“ç§¯å’Œç›¸å…³æ”¾å°„å­¦æŠ¥å‘Šçš„éšå«çŸ¥è¯†ä¼ é€’ç»™CXRç¼–ç å™¨ï¼Œä½¿å¾—ç”¨CXRè¿›è¡Œå¤šå¼‚å¸¸æ€§CTåˆ†ç±»æˆä¸ºå¯èƒ½ã€‚åœ¨ä¸‰ä¸ªå¤šæ ‡ç­¾CTæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è·¨æ¨¡æ€æ£€ç´¢ã€å°æ ·æœ¬é€‚åº”å’Œå¤–éƒ¨éªŒè¯æ–¹é¢çš„è¡¨ç°å‡ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚è¿™äº›ç»“æœçªæ˜¾äº†ä½¿ç”¨æ¥è‡ªCTçš„çŸ¥è¯†æ¥ä¸°å¯ŒCXRçš„æ½œåŠ›ï¼Œä½¿å…¶æˆä¸ºèµ„æºå—é™ç¯å¢ƒä¸­ç–¾ç—…æ£€æµ‹çš„æœ‰æ•ˆé«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>X2CT-CLIPæ¡†æ¶å®ç°äº†CTå’ŒCXRä¹‹é—´çš„çŸ¥è¯†è¿ç§»ï¼Œç¼©å°äº†ä¸¤ç§æˆåƒæ–¹å¼çš„æ¨¡æ€å·®è·ã€‚</li>
<li>é€šè¿‡ä¸‰æ¨¡æ€å¯¹é½æœºåˆ¶ï¼Œå°†æ¥è‡ªCTçš„ä½“ç§¯æ•°æ®å’Œæ”¾å°„å­¦æŠ¥å‘Šçš„çŸ¥è¯†èåˆåˆ°CXRç¼–ç å™¨ä¸­ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒä½¿ç”¨CXRè¿›è¡Œå¤šå¼‚å¸¸æ€§CTåˆ†ç±»ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„ç ”ç©¶é¢†åŸŸã€‚</li>
<li>åœ¨å¤šä¸ªå¤šæ ‡ç­¾CTæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒX2CT-CLIPåœ¨è·¨æ¨¡æ€æ£€ç´¢ã€å°æ ·æœ¬é€‚åº”å’Œå¤–éƒ¨éªŒè¯æ–¹é¢å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>X2CT-CLIPçš„ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ¥è‡ªCTçš„çŸ¥è¯†å¢å¼ºçš„CXRåœ¨èµ„æºå—é™ç¯å¢ƒä¸­è¿›è¡Œç–¾ç—…æ£€æµ‹æ˜¯ä¸€ç§å¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒX2CT-CLIPå…·æœ‰æ›´ä½çš„è®¡ç®—è´Ÿæ‹…å’Œæ›´é«˜çš„æ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02162">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e52ed559bd34ca0791e6fd0e6f0095f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca5a9fd9eb9e82ba594ec13d430ba23b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fb5dcf0b66a8a6fae22c18927754faf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d730010f2b7ee6dc2a7be69f95cffd32.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Rationalization-Models-for-Text-to-SQL"><a href="#Rationalization-Models-for-Text-to-SQL" class="headerlink" title="Rationalization Models for Text-to-SQL"></a>Rationalization Models for Text-to-SQL</h2><p><strong>Authors:Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Dharmashankar Subramanian</strong></p>
<p>We introduce a framework for generating Chain-of-Thought (CoT) rationales to enhance text-to-SQL model fine-tuning. These rationales consist of intermediate SQL statements and explanations, serving as incremental steps toward constructing the final SQL query. The process begins with manually annotating a small set of examples, which are then used to prompt a large language model in an iterative, dynamic few-shot knowledge distillation procedure from a teacher model. A rationalization model is subsequently trained on the validated decomposed queries, enabling extensive synthetic CoT annotations for text-to-SQL datasets. To evaluate the approach, we fine-tune small language models with and without these rationales on the BIRD dataset. Results indicate that step-by-step query generation improves execution accuracy, especially for moderately and highly complex queries, while also enhancing explainability. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç”ŸæˆChain-of-Thoughtï¼ˆCoTï¼‰ç†ç”±çš„æ¡†æ¶ï¼Œä»¥å¢å¼ºæ–‡æœ¬åˆ°SQLæ¨¡å‹çš„å¾®è°ƒã€‚è¿™äº›ç†ç”±åŒ…æ‹¬ä¸­é—´çš„SQLè¯­å¥å’Œè§£é‡Šï¼Œä½œä¸ºæ„å»ºæœ€ç»ˆSQLæŸ¥è¯¢çš„å¢é‡æ­¥éª¤ã€‚è¿‡ç¨‹å§‹äºæ‰‹åŠ¨æ ‡æ³¨ä¸€å°éƒ¨åˆ†ä¾‹å­ï¼Œç„¶åç”¨äºåœ¨è¿­ä»£ã€åŠ¨æ€çš„å°‘æ•°æ ·æœ¬çŸ¥è¯†è’¸é¦è¿‡ç¨‹ä¸­æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ä»æ•™å¸ˆæ¨¡å‹ä¸­å­¦ä¹ ã€‚éšååœ¨éªŒè¯è¿‡çš„åˆ†è§£æŸ¥è¯¢ä¸Šè®­ç»ƒåˆç†åŒ–æ¨¡å‹ï¼Œä¸ºæ–‡æœ¬åˆ°SQLæ•°æ®é›†æä¾›å¹¿æ³›çš„åˆæˆCoTæ³¨é‡Šã€‚ä¸ºäº†è¯„ä¼°è¯¥æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨BIRDæ•°æ®é›†ä¸Šå¯¹å¸¦æœ‰å’Œä¸å¸¦æœ‰è¿™äº›ç†ç”±çš„å°å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚ç»“æœè¡¨æ˜ï¼Œé€æ­¥ç”ŸæˆæŸ¥è¯¢æé«˜äº†æ‰§è¡Œå‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä¸­ç­‰å’Œé«˜åº¦å¤æ‚çš„æŸ¥è¯¢ï¼ŒåŒæ—¶å¢å¼ºäº†å¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06759v3">PDF</a> Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs</p>
<p><strong>Summary</strong></p>
<p>åŸºäºChain-of-Thoughtï¼ˆCoTï¼‰åŸç†ï¼Œæå‡ºä¸€ç§æå‡æ–‡æœ¬åˆ°SQLæ¨¡å‹å¾®è°ƒæ•ˆæœçš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ‰‹åŠ¨æ ‡æ³¨å°‘é‡ç¤ºä¾‹ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¿­ä»£ã€åŠ¨æ€çš„å°‘æ ·æœ¬çŸ¥è¯†è’¸é¦è¿‡ç¨‹ï¼Œä»æ•™å¸ˆæ¨¡å‹ä¸­è·å–ä¿¡æ¯ã€‚éšååœ¨éªŒè¯åçš„åˆ†è§£æŸ¥è¯¢ä¸Šè®­ç»ƒè§£é‡Šæ¨¡å‹ï¼Œä¸ºæ–‡æœ¬åˆ°SQLæ•°æ®é›†æä¾›ä¸°å¯Œçš„åˆæˆCoTæ³¨é‡Šã€‚åœ¨BIRDæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒå®éªŒè¡¨æ˜ï¼Œé€æ­¥æŸ¥è¯¢ç”Ÿæˆèƒ½æé«˜æ‰§è¡Œå‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­ç­‰å’Œé«˜åº¦å¤æ‚çš„æŸ¥è¯¢ä¸Šï¼ŒåŒæ—¶æé«˜äº†è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä½¿ç”¨Chain-of-Thoughtï¼ˆCoTï¼‰åŸç†å¢å¼ºæ–‡æœ¬åˆ°SQLæ¨¡å‹çš„å¾®è°ƒæ•ˆæœã€‚</li>
<li>é€šè¿‡æ‰‹åŠ¨æ ‡æ³¨å°‘é‡ç¤ºä¾‹æ¥å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒçŸ¥è¯†è’¸é¦ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨è¿­ä»£ã€åŠ¨æ€çš„å°‘æ ·æœ¬çŸ¥è¯†è’¸é¦è¿‡ç¨‹ä»æ•™å¸ˆæ¨¡å‹ä¸­è·å–ä¿¡æ¯ã€‚</li>
<li>è®­ç»ƒè§£é‡Šæ¨¡å‹äºéªŒè¯åçš„åˆ†è§£æŸ¥è¯¢ä¸Šï¼Œæä¾›ä¸°å¯Œçš„åˆæˆCoTæ³¨é‡Šã€‚</li>
<li>é€æ­¥æŸ¥è¯¢ç”Ÿæˆèƒ½æé«˜æ‰§è¡Œå‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æŸ¥è¯¢ä¸Šã€‚</li>
<li>æ­¤æ–¹æ³•ä¸ä»…æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¹Ÿå¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06759">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b5713d02847055507997eb5be3d3f6db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac7591afcabb4de48b07038449651318.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ffb3a4fac446037e8b054310b875c4fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-465989f087ba4c2ed99c1b84af82473f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-â€“-An-evaluation-on-urological-doctorsâ€™-notes"><a href="#Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-â€“-An-evaluation-on-urological-doctorsâ€™-notes" class="headerlink" title="Can open source large language models be used for tumor documentation in   Germany? â€“ An evaluation on urological doctorsâ€™ notes"></a>Can open source large language models be used for tumor documentation in   Germany? â€“ An evaluation on urological doctorsâ€™ notes</h2><p><strong>Authors:Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Torsten Panholzer</strong></p>
<p>Tumor documentation in Germany is largely done manually, requiring reading patient records and entering data into structured databases. Large language models (LLMs) could potentially enhance this process by improving efficiency and reliability. This evaluation tests eleven different open source LLMs with sizes ranging from 1-70 billion model parameters on three basic tasks of the tumor documentation process: identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis. For evaluating the LLMs on these tasks, a dataset of annotated text snippets based on anonymized doctorsâ€™ notes from urology was prepared. Different prompting strategies were used to investigate the effect of the number of examples in few-shot prompting and to explore the capabilities of the LLMs in general. The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks. Models with less extensive training data or having fewer than 7 billion parameters showed notably lower performance, while larger models did not display performance gains. Examples from a different medical domain than urology could also improve the outcome in few-shot prompting, which demonstrates the ability of LLMs to handle tasks needed for tumor documentation. Open source LLMs show a strong potential for automating tumor documentation. Models from 7-12 billion parameters could offer an optimal balance between performance and resource efficiency. With tailored fine-tuning and well-designed prompting, these models might become important tools for clinical documentation in the future. The code for the evaluation is available from <a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval">https://github.com/stefan-m-lenz/UroLlmEval</a>. We also release the dataset as a new valuable resource that addresses the shortage of authentic and easily accessible benchmarks in German-language medical NLP. </p>
<blockquote>
<p>åœ¨å¾·å›½ï¼Œè‚¿ç˜¤è®°å½•å·¥ä½œå¤§å¤šä»¥æ‰‹åŠ¨æ–¹å¼è¿›è¡Œï¼Œéœ€è¦é˜…è¯»æ‚£è€…ç—…å†å¹¶å°†æ•°æ®å½•å…¥ç»“æ„åŒ–æ•°æ®åº“ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ‰æ½œåŠ›é€šè¿‡æé«˜æ•ˆç‡å’Œå¯é æ€§æ¥å¢å¼ºè¿™ä¸€æµç¨‹ã€‚æœ¬æ¬¡è¯„ä¼°å¯¹ä¸‰ç§åŸºæœ¬ä»»åŠ¡ï¼ˆè¯†åˆ«è‚¿ç˜¤è¯Šæ–­ã€åˆ†é…ICD-10ä»£ç å’Œæå–é¦–æ¬¡è¯Šæ–­æ—¥æœŸï¼‰ä¸­ï¼Œä»1äº¿åˆ°70äº¿æ¨¡å‹å‚æ•°çš„11ä¸ªä¸åŒå¼€æºLLMè¿›è¡Œäº†æµ‹è¯•ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›LLMåœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‡†å¤‡äº†ä¸€ä¸ªåŸºäºæ³Œå°¿å­¦åŒ¿ååŒ»ç”Ÿç¬”è®°çš„æ ‡æ³¨æ–‡æœ¬ç‰‡æ®µæ•°æ®é›†ã€‚ä½¿ç”¨äº†ä¸åŒçš„æç¤ºç­–ç•¥æ¥ç ”ç©¶å°‘æ•°ç¤ºä¾‹æç¤ºä¸­ç¤ºä¾‹æ•°é‡çš„å½±å“ï¼Œå¹¶æ¢ç´¢LLMçš„ä¸€èˆ¬åŠŸèƒ½ã€‚Llama 3.1 8Bã€Mistral 7Bå’ŒMistral NeMo 12Bç­‰æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚è®­ç»ƒæ•°æ®è¾ƒå°‘çš„æ¨¡å‹æˆ–å‚æ•°å°‘äº7äº¿çš„æ¨¡å‹è¡¨ç°æ˜æ˜¾è¾ƒå·®ï¼Œè€Œè¾ƒå¤§çš„æ¨¡å‹å¹¶æ²¡æœ‰æ˜¾ç¤ºå‡ºæ€§èƒ½æå‡ã€‚æ¥è‡ªæ³Œå°¿å­¦ä»¥å¤–çš„åŒ»å­¦é¢†åŸŸçš„ä¾‹å­ä¹Ÿèƒ½æ”¹å–„å°‘æ•°ç¤ºä¾‹æç¤ºçš„ç»“æœï¼Œè¿™è¯æ˜äº†LLMå¤„ç†è‚¿ç˜¤è®°å½•æ‰€éœ€ä»»åŠ¡çš„èƒ½åŠ›ã€‚å¼€æºLLMåœ¨è‡ªåŠ¨åŒ–è‚¿ç˜¤è®°å½•æ–¹é¢æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚å…·æœ‰7-12äº¿å‚æ•°çš„æ¨¡å‹å¯èƒ½åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒå’Œç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œè¿™äº›æ¨¡å‹å¯èƒ½æˆä¸ºæœªæ¥ä¸´åºŠè®°å½•çš„é‡è¦å·¥å…·ã€‚è¯„ä¼°çš„ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval%E8%8E%B7%E5%8F%96%E3%80%82%E6%88%91%E4%BB%AC%E8%BF%98%E5%8F%91%E5%B8%83%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E4%BD%9C%E4%B8%BA%E8%A7%A3%E5%86%B3%E5%BE%B7%E5%9B%BD%E5%8C%BB%E5%AD%A6NLP%E4%B8%AD%E7%9C%9F%E5%AE%9E%E3%80%81%E6%98%93%E4%BA%8E%E8%AE%BF%E9%97%AE%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%B5%84%E6%BA%90%E7%9F%AD%E7%BC%BA%E7%9A%84%E6%96%B0%E6%9C%89%E4%BB%B7%E5%80%BC%E8%B5%84%E6%BA%90%E3%80%82">https://github.com/stefan-m-lenz/UroLlmEvalè·å–ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒè¯¥æ•°æ®é›†ï¼Œä½œä¸ºè§£å†³å¾·å›½åŒ»å­¦NLPä¸­çœŸå®ã€æ˜“äºè®¿é—®çš„åŸºå‡†æµ‹è¯•èµ„æºçŸ­ç¼ºçš„æ–°æœ‰ä»·å€¼èµ„æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12106v2">PDF</a> 48 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¾·å›½è‚¿ç˜¤æ–‡æ¡£è®°å½•è¿‡ç¨‹ä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œæœ¬ç ”ç©¶è¯„ä¼°äº†åä¸€ç§å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚è¿™äº›æ¨¡å‹åœ¨è‚¿ç˜¤è¯Šæ–­è¯†åˆ«ã€ICD-10ä»£ç åˆ†é…å’Œé¦–æ¬¡è¯Šæ–­æ—¥æœŸæå–ä¸‰ä¸ªåŸºæœ¬ä»»åŠ¡ä¸­çš„è¡¨ç°è¢«è¿›è¡Œäº†æµ‹è¯•ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹å¦‚Llama 3.1 8Bã€Mistral 7Bå’ŒMistral NeMo 12Båœ¨ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚å…·æœ‰è¾ƒå°‘è®­ç»ƒæ•°æ®æˆ–å‚æ•°å°‘äº7äº¿çš„æ¨¡å‹è¡¨ç°è¾ƒå·®ï¼Œè€Œæ›´å¤§çš„æ¨¡å‹å¹¶æœªæ˜¾ç¤ºå‡ºæ€§èƒ½æå‡ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œä½¿ç”¨ä¸åŒäºæ³Œå°¿å­¦é¢†åŸŸçš„åŒ»å­¦é¢†åŸŸæ•°æ®åœ¨å°‘æ ·æœ¬æç¤ºä¸‹ä¹Ÿèƒ½æ”¹å–„ç»“æœï¼Œè¿™è¯æ˜äº†LLMså¤„ç†è‚¿ç˜¤æ–‡æ¡£æ‰€éœ€ä»»åŠ¡çš„èƒ½åŠ›ã€‚å¼€æºLLMsåœ¨è‡ªåŠ¨åŒ–è‚¿ç˜¤æ–‡æ¡£æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œå‚æ•°åœ¨7-12äº¿ä¹‹é—´çš„æ¨¡å‹å¯èƒ½åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒä»¥åŠç²¾å¿ƒè®¾è®¡æç¤ºï¼Œè¿™äº›æ¨¡å‹æœ‰å¯èƒ½æˆä¸ºæœªæ¥ä¸´åºŠæ–‡æ¡£è®°å½•çš„é‡è¦å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ‰æ½œåŠ›æ”¹å–„å¾·å›½è‚¿ç˜¤æ–‡æ¡£çš„æ•ˆç‡ä¸å¯é æ€§ã€‚</li>
<li>åœ¨è‚¿ç˜¤è¯Šæ–­è¯†åˆ«ã€ICD-10ä»£ç åˆ†é…å’Œé¦–æ¬¡è¯Šæ–­æ—¥æœŸæå–ä¸‰ä¸ªä»»åŠ¡ä¸­ï¼ŒLlama 3.1 8Bã€Mistral 7Bå’ŒMistral NeMo 12Bç­‰æ¨¡å‹è¡¨ç°è‰¯å¥½ã€‚</li>
<li>æ¨¡å‹è¡¨ç°ä¸è®­ç»ƒæ•°æ®å’Œå‚æ•°æ•°é‡æœ‰å…³ï¼Œå‚æ•°å°‘äº7äº¿çš„æ¨¡å‹è¡¨ç°è¾ƒå·®ã€‚</li>
<li>ä¸åŒåŒ»å­¦é¢†åŸŸçš„æ•°æ®åœ¨å°‘æ ·æœ¬æç¤ºä¸‹èƒ½æé«˜æ¨¡å‹è¡¨ç°ï¼Œå±•ç¤ºLLMsçš„é€‚åº”æ€§å’Œæ½œåŠ›ã€‚</li>
<li>å‚æ•°åœ¨7-12äº¿ä¹‹é—´çš„æ¨¡å‹å¯èƒ½åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¸Šè¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚</li>
<li>é€šè¿‡å¾®è°ƒä¸ç²¾å¿ƒè®¾è®¡æç¤ºï¼ŒLLMsæœ‰æ½œåŠ›æˆä¸ºä¸´åºŠæ–‡æ¡£è®°å½•çš„é‡è¦å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-764cd5a8d839a45da36d211b0c673397.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5fbd2fce63b2cad7e3b967780aa11e83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b245a6522b0d161dce0f49b1bb1190c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="BiomedCoOp-Learning-to-Prompt-for-Biomedical-Vision-Language-Models"><a href="#BiomedCoOp-Learning-to-Prompt-for-Biomedical-Vision-Language-Models" class="headerlink" title="BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models"></a>BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models</h2><p><strong>Authors:Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao</strong></p>
<p>Recent advancements in vision-language models (VLMs), such as CLIP, have demonstrated substantial success in self-supervised representation learning for vision tasks. However, effectively adapting VLMs to downstream applications remains challenging, as their accuracy often depends on time-intensive and expertise-demanding prompt engineering, while full model fine-tuning is costly. This is particularly true for biomedical images, which, unlike natural images, typically suffer from limited annotated datasets, unintuitive image contrasts, and nuanced visual features. Recent prompt learning techniques, such as Context Optimization (CoOp) intend to tackle these issues, but still fall short in generalizability. Meanwhile, explorations in prompt learning for biomedical image analysis are still highly limited. In this work, we propose BiomedCoOp, a novel prompt learning framework that enables efficient adaptation of BiomedCLIP for accurate and highly generalizable few-shot biomedical image classification. Our approach achieves effective prompt context learning by leveraging semantic consistency with average prompt ensembles from Large Language Models (LLMs) and knowledge distillation with a statistics-based prompt selection strategy. We conducted comprehensive validation of our proposed framework on 11 medical datasets across 9 modalities and 10 organs against existing state-of-the-art methods, demonstrating significant improvements in both accuracy and generalizability. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/BiomedCoOp">https://github.com/HealthX-Lab/BiomedCoOp</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚CLIPï¼Œåœ¨è§†è§‰ä»»åŠ¡çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå°†VLMsæœ‰æ•ˆåœ°é€‚åº”åˆ°ä¸‹æ¸¸åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬çš„å‡†ç¡®æ€§é€šå¸¸ä¾èµ–äºè€—æ—¶ä¸”éœ€è¦ä¸“ä¸šçŸ¥è¯†çš„æç¤ºå·¥ç¨‹ï¼Œè€Œå®Œæ•´çš„æ¨¡å‹å¾®è°ƒæˆæœ¬é«˜æ˜‚ã€‚ç‰¹åˆ«æ˜¯å¯¹äºç”Ÿç‰©åŒ»å­¦å›¾åƒï¼Œä¸è‡ªç„¶å›¾åƒä¸åŒï¼Œå®ƒä»¬é€šå¸¸å—é™äºæ ‡æ³¨æ•°æ®é›†ã€å›¾åƒå¯¹æ¯”åº¦ä¸ç›´è§‚ä»¥åŠå¾®å¦™çš„è§†è§‰ç‰¹å¾ã€‚æœ€è¿‘çš„æç¤ºå­¦ä¹ æŠ€æœ¯ï¼Œå¦‚ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼ˆCoOpï¼‰ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†åœ¨é€šç”¨æ€§æ–¹é¢ä»æœ‰ä¸è¶³ã€‚åŒæ—¶ï¼Œé’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æçš„æç¤ºå­¦ä¹ æ¢ç´¢ä»ç„¶éå¸¸æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†BiomedCoOpï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°é€‚åº”BiomedCLIPï¼Œä»¥å®ç°å‡†ç¡®ä¸”é«˜åº¦é€šç”¨çš„ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»çš„å°‘é‡æ ·æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹³å‡æç¤ºé›†åˆçš„è¯­ä¹‰ä¸€è‡´æ€§å’ŒåŸºäºç»Ÿè®¡çš„æç¤ºé€‰æ‹©ç­–ç•¥çš„çŸ¥è¯†è’¸é¦ï¼Œå®ç°äº†æœ‰æ•ˆçš„æç¤ºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨9ç§æ¨¡æ€ã€æ¶‰åŠ10ä¸ªå™¨å®˜çš„11ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šå¯¹æ‰€æå‡ºçš„æ¡†æ¶è¿›è¡Œäº†å…¨é¢çš„éªŒè¯ï¼Œä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å‡†ç¡®æ€§å’Œé€šç”¨æ€§æ–¹é¢éƒ½å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ä»£ç å…¬å¼€å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/BiomedCoOp%E3%80%82">https://github.com/HealthX-Lab/BiomedCoOpã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15232v2">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡çš„æ–°å‹æç¤ºå­¦ä¹ æ¡†æ¶BiomedCoOpã€‚è¯¥æ¡†æ¶ç»“åˆäº†è¯­ä¹‰ä¸€è‡´æ€§ã€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹³å‡æç¤ºé›†åˆä»¥åŠåŸºäºç»Ÿè®¡çš„æç¤ºé€‰æ‹©ç­–ç•¥ï¼Œå®ç°äº†æœ‰æ•ˆæç¤ºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚åœ¨å¤šä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„éªŒè¯æ˜¾ç¤ºï¼ŒBiomedCoOpåœ¨å‡†ç¡®æ€§å’Œé€šç”¨æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸè¿›å±•çš„è§†è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚CLIPåœ¨è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„é€‚åº”ä»ç„¶å…·æœ‰æŒ‘æˆ˜ã€‚</li>
<li>å…¨æ¨¡å‹å¾®è°ƒæˆæœ¬é«˜æ˜‚ï¼Œè€Œæç¤ºå·¥ç¨‹åˆ™éœ€è¦æ—¶é—´å’Œä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>ç”Ÿç‰©åŒ»å­¦å›¾åƒé¢ä¸´æœ‰é™æ ‡æ³¨æ•°æ®é›†ã€å›¾åƒå¯¹æ¯”ä¸æ˜æ˜¾å’Œå¾®å¦™è§†è§‰ç‰¹å¾ç­‰é—®é¢˜ã€‚</li>
<li>Context Optimizationï¼ˆCoOpï¼‰ç­‰ç°æœ‰æç¤ºå­¦ä¹ æŠ€æœ¯è™½èƒ½è§£å†³éƒ¨åˆ†é—®é¢˜ï¼Œä½†åœ¨é€šç”¨æ€§æ–¹é¢ä»æœ‰ä¸è¶³ã€‚</li>
<li>æå‡ºçš„BiomedCoOpæ¡†æ¶ç»“åˆäº†è¯­ä¹‰ä¸€è‡´æ€§ã€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹³å‡æç¤ºé›†åˆå’ŒåŸºäºç»Ÿè®¡çš„æç¤ºé€‰æ‹©ç­–ç•¥ï¼Œå®ç°äº†é«˜æ•ˆçš„ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»æç¤ºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚</li>
<li>åœ¨å¤šä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„éªŒè¯æ˜¾ç¤ºï¼ŒBiomedCoOpåœ¨å‡†ç¡®æ€§å’Œé€šç”¨æ€§æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-38e06f0b12f79ae2df2bf415fffa84ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d742d08ca799cb2ef67626f8fec6be7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bec45fcdc9fbfcf81f31e3126e9fa5c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Self-Ensembling-Gaussian-Splatting-for-Few-Shot-Novel-View-Synthesis"><a href="#Self-Ensembling-Gaussian-Splatting-for-Few-Shot-Novel-View-Synthesis" class="headerlink" title="Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis"></a>Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis</h2><p><strong>Authors:Chen Zhao, Xuan Wang, Tong Zhang, Saqib Javed, Mathieu Salzmann</strong></p>
<p>3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in novel view synthesis (NVS). However, 3DGS tends to overfit when trained with sparse views, limiting its generalization to novel viewpoints. In this paper, we address this overfitting issue by introducing Self-Ensembling Gaussian Splatting (SE-GS). We achieve self-ensembling by incorporating an uncertainty-aware perturbation strategy during training. A $\mathbf{\Delta}$-model and a $\mathbf{\Sigma}$-model are jointly trained on the available images. The $\mathbf{\Delta}$-model is dynamically perturbed based on rendering uncertainty across training steps, generating diverse perturbed models with negligible computational overhead. Discrepancies between the $\mathbf{\Sigma}$-model and these perturbed models are minimized throughout training, forming a robust ensemble of 3DGS models. This ensemble, represented by the $\mathbf{\Sigma}$-model, is then used to generate novel-view images during inference. Experimental results on the LLFF, Mip-NeRF360, DTU, and MVImgNet datasets demonstrate that our approach enhances NVS quality under few-shot training conditions, outperforming existing state-of-the-art methods. The code is released at: <a target="_blank" rel="noopener" href="https://sailor-z.github.io/projects/SEGS.html">https://sailor-z.github.io/projects/SEGS.html</a>. </p>
<blockquote>
<p>ä¸‰ç»´é«˜æ–¯æ’å€¼ï¼ˆ3DGSï¼‰åœ¨æ–°å‹è§†å›¾åˆæˆï¼ˆNVSï¼‰ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆæœã€‚ç„¶è€Œï¼Œå½“ä½¿ç”¨ç¨€ç–è§†å›¾è¿›è¡Œè®­ç»ƒæ—¶ï¼Œ3DGSå®¹æ˜“è¿‡åº¦æ‹Ÿåˆï¼Œé™åˆ¶äº†å…¶åœ¨æ–°å‹è§‚ç‚¹ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥è‡ªé›†æˆé«˜æ–¯æ’å€¼ï¼ˆSE-GSï¼‰æ¥è§£å†³è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥ä¸€ç§æ„ŸçŸ¥ä¸ç¡®å®šæ€§çš„æ‰°åŠ¨ç­–ç•¥æ¥å®ç°è‡ªé›†æˆã€‚Î”æ¨¡å‹å’ŒÎ£æ¨¡å‹åœ¨å¯ç”¨å›¾åƒä¸Šè”åˆè®­ç»ƒã€‚Î”æ¨¡å‹æ ¹æ®è®­ç»ƒæ­¥éª¤ä¸­çš„æ¸²æŸ“ä¸ç¡®å®šæ€§è¿›è¡ŒåŠ¨æ€æ‰°åŠ¨ï¼Œç”Ÿæˆå¤šç§æ‰°åŠ¨æ¨¡å‹ï¼Œè®¡ç®—å¼€é”€å¾®ä¹å…¶å¾®ã€‚åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæœ€å°åŒ–Î£æ¨¡å‹ä¸è¿™äº›æ‰°åŠ¨æ¨¡å‹ä¹‹é—´çš„å·®å¼‚ï¼Œå½¢æˆç¨³å¥çš„3DGSæ¨¡å‹é›†åˆã€‚è¿™ä¸ªé›†åˆç”±Î£æ¨¡å‹è¡¨ç¤ºï¼Œç„¶åç”¨äºæ¨ç†è¿‡ç¨‹ä¸­çš„æ–°å‹è§†å›¾å›¾åƒç”Ÿæˆã€‚åœ¨LLFFã€Mip-NeRF360ã€DTUå’ŒMVImgNetæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æé«˜å°‘é‡è®­ç»ƒæƒ…å†µä¸‹çš„NVSè´¨é‡æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚ä»£ç å·²å‘å¸ƒåœ¨ï¼š[<a target="_blank" rel="noopener" href="https://sailor-z.github.io/projects/SEGS.html%E3%80%82]">https://sailor-z.github.io/projects/SEGS.htmlã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00144v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäº3Dé«˜æ–¯ç‚¹æ¸²æŸ“æŠ€æœ¯ï¼ˆGaussian Splattingï¼‰åœ¨åˆæˆæ–°è§†è§’å›¾åƒï¼ˆNVSï¼‰æ—¶è¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœï¼Œä½†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡åˆ°ç¨€ç–è§†è§’æ—¶çš„è¿‡æ‹Ÿåˆé—®é¢˜é™åˆ¶äº†å…¶åœ¨æ–°è§†è§’ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºè‡ªé›†æˆé«˜æ–¯ç‚¹æ¸²æŸ“æŠ€æœ¯ï¼ˆSelf-Ensembling Gaussian Splattingï¼Œç®€ç§°SE-GSï¼‰çš„è§£å†³æ–¹æ¡ˆæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ‰°åŠ¨ç­–ç•¥å®ç°è‡ªé›†æˆï¼ŒåŒæ—¶è®­ç»ƒä¸€ä¸ªÎ”æ¨¡å‹å’Œä¸€ä¸ªÎ£æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨æ¸²æŸ“ä¸ç¡®å®šæ€§å¯¹Î”æ¨¡å‹è¿›è¡ŒåŠ¨æ€æ‰°åŠ¨ï¼Œç”Ÿæˆå¤šä¸ªæ‰°åŠ¨æ¨¡å‹ä»¥æœ€å°åŒ–ä¸¤è€…ä¹‹é—´çš„å·®å¼‚ã€‚åœ¨LLFFã€Mip-NeRF360ã€DTUå’ŒMVImgNetæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒæ ·æœ¬è¾ƒå°‘çš„æƒ…å†µä¸‹æé«˜äº†NVSçš„è´¨é‡ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„ç®—æ³•ã€‚ä»£ç å·²å‘å¸ƒåœ¨ç½‘å€ï¼š<a target="_blank" rel="noopener" href="https://sailor-z.github.io/projects/SEGS.html">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼•å…¥è‡ªé›†æˆé«˜æ–¯ç‚¹æ¸²æŸ“æŠ€æœ¯è§£å†³äº†ç¨€ç–è§†è§’ä¸‹çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>é€šè¿‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ‰°åŠ¨ç­–ç•¥å®ç°è‡ªé›†æˆè®­ç»ƒã€‚</li>
<li>åŒæ—¶è®­ç»ƒÎ”æ¨¡å‹å’ŒÎ£æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨æ¸²æŸ“ä¸ç¡®å®šæ€§è¿›è¡ŒåŠ¨æ€æ‰°åŠ¨ã€‚</li>
<li>é€šè¿‡æœ€å°åŒ–ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ï¼Œæé«˜äº†æ–°è§†è§’å›¾åƒçš„åˆæˆè´¨é‡ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„ç®—æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.00144">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3254bfcfa9714ae7a1e529f2b3f884fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1e7fa4af9e4280dcb5de87a19497871.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f481c96c362e1caca155db837cdb972.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ce69bf882cc5d1744531b175d7dbdbe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a7e9e4c9c47a127c467d32726fc5c4e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f948a5edb48f80832743d4ba8d618228.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Probabilistic-Language-Image-Pre-Training"><a href="#Probabilistic-Language-Image-Pre-Training" class="headerlink" title="Probabilistic Language-Image Pre-Training"></a>Probabilistic Language-Image Pre-Training</h2><p><strong>Authors:Sanghyuk Chun, Wonjae Kim, Song Park, Sangdoo Yun</strong></p>
<p>Vision-language models (VLMs) embed aligned image-text pairs into a joint space but often rely on deterministic embeddings, assuming a one-to-one correspondence between images and texts. This oversimplifies real-world relationships, which are inherently many-to-many, with multiple captions describing a single image and vice versa. We introduce Probabilistic Language-Image Pre-training (ProLIP), the first probabilistic VLM pre-trained on a billion-scale image-text dataset using only probabilistic objectives, achieving a strong zero-shot capability (e.g., 74.6% ImageNet zero-shot accuracy with ViT-B&#x2F;16). ProLIP efficiently estimates uncertainty by an â€œuncertainty tokenâ€ without extra parameters. We also introduce a novel inclusion loss that enforces distributional inclusion relationships between image-text pairs and between original and masked inputs. Experiments demonstrate that, by leveraging uncertainty estimates, ProLIP benefits downstream tasks and aligns with intuitive notions of uncertainty, e.g., shorter texts being more uncertain and more general inputs including specific ones. Utilizing text uncertainties, we further improve ImageNet accuracy from 74.6% to 75.8% (under a few-shot setting), supporting the practical advantages of our probabilistic approach. The code is available at <a target="_blank" rel="noopener" href="https://github.com/naver-ai/prolip">https://github.com/naver-ai/prolip</a> </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å°†å›¾åƒæ–‡æœ¬å¯¹åµŒå…¥åˆ°è”åˆç©ºé—´ä¸­ï¼Œä½†é€šå¸¸ä¾èµ–äºç¡®å®šæ€§åµŒå…¥ï¼Œå‡è®¾å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´å­˜åœ¨ä¸€å¯¹ä¸€çš„å¯¹åº”å…³ç³»ã€‚è¿™ç®€åŒ–äº†çœŸå®ä¸–ç•Œä¸­çš„å…³ç³»ï¼ŒçœŸå®ä¸–ç•Œä¸­çš„å…³ç³»æ˜¯å›ºæœ‰çš„å¤šå¯¹å¤šå…³ç³»ï¼Œå¤šä¸ªå­—å¹•æè¿°å•ä¸ªå›¾åƒï¼Œåä¹‹äº¦ç„¶ã€‚æˆ‘ä»¬å¼•å…¥äº†æ¦‚ç‡è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆProLIPï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä½¿ç”¨æ¦‚ç‡ç›®æ ‡åœ¨ç™¾äº¿çº§å›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„æ¦‚ç‡VLMï¼Œå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ViT-B&#x2F;1lè¾¾åˆ°ImageNetçš„é›¶æ ·æœ¬å‡†ç¡®ç‡æ˜¯ç™¾åˆ†ä¹‹ä¸ƒåå››ç‚¹å…­ï¼‰ã€‚ProLIPé€šè¿‡ä¸€ç§ä¸ç¡®å®šæ€§ä»¤ç‰Œæœ‰æ•ˆåœ°ä¼°è®¡ä¸ç¡®å®šæ€§ï¼Œæ— éœ€é¢å¤–çš„å‚æ•°ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹åŒ…å®¹æŸå¤±ï¼Œè¯¥æŸå¤±å¯ä»¥å¼ºåˆ¶å›¾åƒæ–‡æœ¬å¯¹ä¹‹é—´ä»¥åŠåŸå§‹è¾“å…¥å’Œé®ç½©è¾“å…¥ä¹‹é—´çš„åˆ†å¸ƒåŒ…å®¹å…³ç³»ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡åˆ©ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ŒProLIPæœ‰åˆ©äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶ä¸”ä¸ç›´è§‚çš„ä¸ç¡®å®šæ€§æ¦‚å¿µç›¸ä¸€è‡´ï¼Œä¾‹å¦‚æ–‡æœ¬è¶ŠçŸ­åˆ™ä¸ç¡®å®šæ€§è¶Šé«˜ï¼Œæ¶µç›–å…·ä½“ä¿¡æ¯çš„è¾“å…¥é€šå¸¸ä¹Ÿæ›´åŠ é€šç”¨ã€‚é€šè¿‡åˆ©ç”¨æ–‡æœ¬çš„ä¸ç¡®å®šæ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æé«˜äº†ImageNetçš„å‡†ç¡®ç‡ï¼Œä»ç™¾åˆ†ä¹‹ä¸ƒåå››ç‚¹å…­æé«˜åˆ°ç™¾åˆ†ä¹‹ä¸ƒåäº”ç‚¹å…«ï¼ˆåœ¨å°æ ·æœ¬è®¾ç½®ä¸‹ï¼‰ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ¦‚ç‡æ–¹æ³•å…·æœ‰å®é™…ä¼˜åŠ¿ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/naver-ai/prolip%E3%80%82">https://github.com/naver-ai/prolipã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18857v3">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/naver-ai/prolip">https://github.com/naver-ai/prolip</a> HuggingFace Hub:   <a target="_blank" rel="noopener" href="https://huggingface.co/collections/SanghyukChun/prolip-6712595dfc87fd8597350291">https://huggingface.co/collections/SanghyukChun/prolip-6712595dfc87fd8597350291</a>   33 pages, 4.8 MB; LongProLIP paper: arXiv:2503.08048</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Probabilistic Language-Image Pre-trainingï¼ˆProLIPï¼‰æ¨¡å‹ï¼Œå®ƒæ˜¯é¦–ä¸ªåŸºäºæ¦‚ç‡çš„è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨ç™¾äº¿çº§å›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸Šé¢„è®­ç»ƒï¼Œä½¿ç”¨æ¦‚ç‡ç›®æ ‡å‡½æ•°ï¼Œå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¦‚74.6%çš„ImageNeté›¶æ ·æœ¬å‡†ç¡®ç‡ã€‚ProLIPé€šè¿‡â€œä¸ç¡®å®šæ€§ä»¤ç‰Œâ€æœ‰æ•ˆåœ°ä¼°è®¡ä¸ç¡®å®šæ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„åŒ…å«æŸå¤±ï¼Œç”¨äºæ‰§è¡Œå›¾åƒæ–‡æœ¬å¯¹ä¹‹é—´çš„åˆ†å¸ƒåŒ…å«å…³ç³»ã€‚é€šè¿‡åˆ©ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ŒProLIPæœ‰åŠ©äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶ä¸ä¸ç¡®å®šæ€§ç›´è§‚è®¤è¯†ç›¸ä¸€è‡´ã€‚åˆ©ç”¨æ–‡æœ¬ä¸ç¡®å®šæ€§ï¼Œè¿›ä¸€æ­¥å°†ImageNetå‡†ç¡®ç‡ä»74.6%æé«˜åˆ°75.8%ï¼ŒéªŒè¯äº†æ¦‚ç‡æ–¹æ³•çš„å®é™…ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ProLIPæ˜¯é¦–ä¸ªåœ¨ç™¾äº¿çº§å›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„åŸºäºæ¦‚ç‡çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨æ¦‚ç‡ç›®æ ‡å‡½æ•°ï¼Œå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¦‚ImageNetçš„74.6%é›¶æ ·æœ¬å‡†ç¡®ç‡ã€‚</li>
<li>ProLIPé€šè¿‡â€œä¸ç¡®å®šæ€§ä»¤ç‰Œâ€ä¼°è®¡ä¸ç¡®å®šæ€§ï¼Œæ— éœ€é¢å¤–å‚æ•°ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„åŒ…å«æŸå¤±ï¼Œç”¨äºå›¾åƒæ–‡æœ¬å¯¹å’ŒåŸå§‹ä¸é®æŒ¡è¾“å…¥ä¹‹é—´çš„åˆ†å¸ƒåŒ…å«å…³ç³»ã€‚</li>
<li>åˆ©ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ŒProLIPæœ‰åŠ©äºæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶ä¸ä¸ç¡®å®šæ€§çš„ç›´è§‚è®¤è¯†ç›¸ä¸€è‡´ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨æ–‡æœ¬ä¸ç¡®å®šæ€§ï¼Œè¿›ä¸€æ­¥æå‡äº†ImageNetçš„å‡†ç¡®ç‡ã€‚</li>
<li>ProLIPçš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.18857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c60fa82099b92cb0f49e0ce9914d0f1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5138f348f837858749275fdc2284b173.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-302a41c22cb6722ba1d22ab548259fcb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-683d735bf52c5e5373cc9576998bbe9b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Cross-Modal-Few-Shot-Learning-a-Generative-Transfer-Learning-Framework"><a href="#Cross-Modal-Few-Shot-Learning-a-Generative-Transfer-Learning-Framework" class="headerlink" title="Cross-Modal Few-Shot Learning: a Generative Transfer Learning Framework"></a>Cross-Modal Few-Shot Learning: a Generative Transfer Learning Framework</h2><p><strong>Authors:Zhengwei Yang, Yuke Li, Qiang Sun, Basura Fernando, Heng Huang, Zheng Wang</strong></p>
<p>Most existing studies on few-shot learning focus on unimodal settings, where models are trained to generalize to unseen data using a limited amount of labeled examples from a single modality. However, real-world data are inherently multi-modal, and such unimodal approaches limit the practical applications of few-shot learning. To bridge this gap, this paper introduces the Cross-modal Few-Shot Learning (CFSL) task, which aims to recognize instances across multiple modalities while relying on scarce labeled data. This task presents unique challenges compared to classical few-shot learning arising from the distinct visual attributes and structural disparities inherent to each modality. To tackle these challenges, we propose a Generative Transfer Learning (GTL) framework by simulating how humans abstract and generalize concepts. Specifically, the GTL jointly estimates the latent shared concept across modalities and the in-modality disturbance through a generative structure. Establishing the relationship between latent concepts and visual content among abundant unimodal data enables GTL to effectively transfer knowledge from unimodal to novel multimodal data, as humans did. Comprehensive experiments demonstrate that the GTL achieves state-of-the-art performance across seven multi-modal datasets across RGB-Sketch, RGB-Infrared, and RGB-Depth. </p>
<blockquote>
<p>å½“å‰å…³äºå°‘æ ·æœ¬å­¦ä¹ çš„ç ”ç©¶å¤§å¤šé›†ä¸­åœ¨å•æ¨¡æ€è®¾ç½®ä¸Šï¼Œå³æ¨¡å‹åœ¨æœ‰é™çš„æ ‡ç­¾æ ·æœ¬åŸºç¡€ä¸Šè®­ç»ƒï¼Œä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„æ•°æ®ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„æ•°æ®æœ¬è´¨ä¸Šæ˜¯å¤šæ¨¡æ€çš„ï¼Œè¿™ç§å•æ¨¡æ€çš„æ–¹æ³•é™åˆ¶äº†å°‘æ ·æœ¬å­¦ä¹ çš„å®é™…åº”ç”¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡å¼•å…¥äº†è·¨æ¨¡æ€å°‘æ ·æœ¬å­¦ä¹ ï¼ˆCFSLï¼‰ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ—¨åœ¨åˆ©ç”¨ç¨€ç¼ºçš„æ ‡ç­¾æ•°æ®è¯†åˆ«è·¨å¤šä¸ªæ¨¡æ€çš„å®ä¾‹ã€‚ä¸ç»å…¸å°‘æ ·æœ¬å­¦ä¹ ç›¸æ¯”ï¼Œæ­¤ä»»åŠ¡å‘ˆç°å‡ºç‹¬ç‰¹æŒ‘æˆ˜ï¼Œæºäºæ¯ä¸ªæ¨¡æ€å›ºæœ‰çš„ä¸åŒè§†è§‰å±æ€§å’Œç»“æ„å·®å¼‚ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”Ÿæˆè½¬ç§»å­¦ä¹ ï¼ˆGTLï¼‰æ¡†æ¶ï¼Œæ¨¡æ‹Ÿäººç±»å¦‚ä½•æŠ½è±¡å’Œæ¦‚æ‹¬æ¦‚å¿µã€‚å…·ä½“æ¥è¯´ï¼ŒGTLé€šè¿‡ä¸€ä¸ªç”Ÿæˆç»“æ„æ¥å…±åŒä¼°è®¡è·¨æ¨¡æ€çš„æ½œåœ¨å…±äº«æ¦‚å¿µå’Œæ¨¡æ€å†…çš„å¹²æ‰°ã€‚åœ¨å¤§é‡å•æ¨¡æ€æ•°æ®ä¸­å»ºç«‹æ½œåœ¨æ¦‚å¿µä¸è§†è§‰å†…å®¹ä¹‹é—´çš„å…³ç³»ï¼Œä½¿GTLèƒ½å¤Ÿåƒäººç±»ä¸€æ ·æœ‰æ•ˆåœ°å°†ä»å•æ¨¡æ€å­¦åˆ°çš„çŸ¥è¯†è½¬ç§»åˆ°æ–°çš„å¤šæ¨¡æ€æ•°æ®ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒGTLåœ¨RGB-Sketchã€RGB-çº¢å¤–å’ŒRGB-æ·±åº¦ç­‰ä¸ƒä¸ªå¤šæ¨¡æ€æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10663v2">PDF</a> 15 pages, 9 figures, 7 tables</p>
<p><strong>Summary</strong><br>å°‘é‡æ ‡æ³¨æ•°æ®çš„è·¨æ¨¡æ€è¯†åˆ«æ˜¯å½“å‰ç ”ç©¶çš„ç©ºç™½ã€‚è¯¥ç ”ç©¶å¼•å…¥äº†è·¨æ¨¡æ€å°æ ·æœ¬å­¦ä¹ ï¼ˆCFSLï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³åœ¨å¤šç§æ¨¡æ€ä¸‹è¯†åˆ«å®ä¾‹çš„é—®é¢˜ã€‚ç ”ç©¶æå‡ºäº†ç”Ÿæˆè¿ç§»å­¦ä¹ ï¼ˆGTLï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»æŠ½è±¡å’Œæ¦‚æ‹¬æ¦‚å¿µçš„æ–¹å¼ï¼Œä¼°è®¡è·¨æ¨¡æ€çš„æ½œåœ¨å…±äº«æ¦‚å¿µå’Œæ¨¡æ€å†…çš„å¹²æ‰°ã€‚è¯¥æ¡†æ¶åœ¨ä¸ƒä¸ªå¤šæ¨¡æ€æ•°æ®é›†ä¸Šçš„è¡¨ç°è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å…³äºå°æ ·æœ¬å­¦ä¹ çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•æ¨¡æ€è®¾ç½®ä¸Šï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚</li>
<li>çœŸå®ä¸–ç•Œçš„æ•°æ®æ˜¯å¤šæ¨¡æ€çš„ï¼Œå› æ­¤éœ€è¦è·¨æ¨¡æ€å°æ ·æœ¬å­¦ä¹ ï¼ˆCFSLï¼‰æ¥è¯†åˆ«å¤šç§æ¨¡æ€ä¸‹çš„å®ä¾‹ã€‚</li>
<li>CFSLä»»åŠ¡é¢ä¸´ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œæºäºæ¯ä¸ªæ¨¡æ€ç‹¬ç‰¹çš„è§†è§‰å±æ€§å’Œç»“æ„å·®å¼‚ã€‚</li>
<li>ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ç”Ÿæˆè¿ç§»å­¦ä¹ ï¼ˆGTLï¼‰æ¡†æ¶ï¼Œæ¨¡æ‹Ÿäººç±»æŠ½è±¡å’Œæ¦‚æ‹¬æ¦‚å¿µçš„æ–¹å¼ã€‚</li>
<li>GTLæ¡†æ¶é€šè¿‡ä¼°è®¡è·¨æ¨¡æ€çš„æ½œåœ¨å…±äº«æ¦‚å¿µå’Œæ¨¡æ€å†…çš„å¹²æ‰°æ¥å»ºç«‹å…³ç³»ã€‚</li>
<li>GTLæ¡†æ¶åœ¨å¤šä¸ªå¤šæ¨¡æ€æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10663">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fd94004deb1b73d04d7940f0a13180c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-151494d07f63c80425a396162c8d7e5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-825c84662e7bed80bcf9a41ea3cb3965.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ad089e89695e1be7e94c2cfdcc47479.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d47a7b2c4f70e3f0b0d412ac8ad85cc.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Understand-Time-Series-Anomalies"><a href="#Can-LLMs-Understand-Time-Series-Anomalies" class="headerlink" title="Can LLMs Understand Time Series Anomalies?"></a>Can LLMs Understand Time Series Anomalies?</h2><p><strong>Authors:Zihao Zhou, Rose Yu</strong></p>
<p>Large Language Models (LLMs) have gained popularity in time series forecasting, but their potential for anomaly detection remains largely unexplored. Our study investigates whether LLMs can understand and detect anomalies in time series data, focusing on zero-shot and few-shot scenarios. Inspired by conjectures about LLMsâ€™ behavior from time series forecasting research, we formulate key hypotheses about LLMsâ€™ capabilities in time series anomaly detection. We design and conduct principled experiments to test each of these hypotheses. Our investigation reveals several surprising findings about LLMs for time series: (1) LLMs understand time series better as images rather than as text, (2) LLMs do not demonstrate enhanced performance when prompted to engage in explicit reasoning about time series analysis. (3) Contrary to common beliefs, LLMsâ€™ understanding of time series does not stem from their repetition biases or arithmetic abilities. (4) LLMsâ€™ behaviors and performance in time series analysis vary significantly across different models. This study provides the first comprehensive analysis of contemporary LLM capabilities in time series anomaly detection. Our results suggest that while LLMs can understand trivial time series anomalies, we have no evidence that they can understand more subtle real-world anomalies. Many common conjectures based on their reasoning capabilities do not hold. All synthetic dataset generators, final prompts, and evaluation scripts have been made available in <a target="_blank" rel="noopener" href="https://github.com/rose-stl-lab/anomllm">https://github.com/rose-stl-lab/anomllm</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­å—åˆ°æ¬¢è¿ï¼Œä½†å®ƒä»¬åœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è°ƒæŸ¥LLMæ˜¯å¦èƒ½å¤Ÿç†è§£å’Œæ£€æµ‹æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„å¼‚å¸¸å€¼ï¼Œé‡ç‚¹å…³æ³¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ã€‚æˆ‘ä»¬ä»æ—¶é—´åºåˆ—é¢„æµ‹ç ”ç©¶ä¸­å…³äºLLMè¡Œä¸ºçš„æ¨æµ‹ä¸­è·å¾—çµæ„Ÿï¼Œé’ˆå¯¹LLMåœ¨æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„èƒ½åŠ›åˆ¶å®šå…³é”®å‡è®¾ã€‚æˆ‘ä»¬è®¾è®¡å¹¶è¿›è¡Œäº†æœ‰é’ˆå¯¹æ€§çš„å®éªŒæ¥æµ‹è¯•è¿™äº›å‡è®¾ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†LLMåœ¨æ—¶é—´åºåˆ—æ–¹é¢çš„å‡ ä¸ªä»¤äººæƒŠè®¶çš„å‘ç°ï¼šï¼ˆ1ï¼‰LLMæ›´å–„äºå°†æ—¶é—´åºåˆ—è§†ä¸ºå›¾åƒè€Œä¸æ˜¯æ–‡æœ¬æ¥ç†è§£ï¼›ï¼ˆ2ï¼‰å½“è¢«è¦æ±‚å‚ä¸å…³äºæ—¶é—´åºåˆ—åˆ†æçš„æ˜¾å¼æ¨ç†æ—¶ï¼ŒLLMå¹¶æ²¡æœ‰è¡¨ç°å‡ºå¢å¼ºçš„æ€§èƒ½ã€‚ï¼ˆ3ï¼‰ä¸æ™®éä¿¡å¿µç›¸åï¼ŒLLMå¯¹æ—¶é—´åºåˆ—çš„ç†è§£å¹¶éæºäºå…¶é‡å¤åè§æˆ–ç®—æœ¯èƒ½åŠ›ã€‚ï¼ˆ4ï¼‰ä¸åŒæ¨¡å‹åœ¨æ—¶é—´åºåˆ—åˆ†æä¸­çš„è¡Œä¸ºå’Œæ€§èƒ½å·®å¼‚å¾ˆå¤§ã€‚æœ¬ç ”ç©¶æä¾›äº†å¯¹å½“ä»£LLMåœ¨æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹èƒ½åŠ›æ–¹é¢çš„é¦–æ¬¡ç»¼åˆåˆ†æã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶LLMèƒ½å¤Ÿç†è§£ç®€å•çš„æ—¶é—´åºåˆ—å¼‚å¸¸ï¼Œä½†æ²¡æœ‰è¯æ®è¡¨æ˜å®ƒä»¬èƒ½å¤Ÿç†è§£æ›´å¾®å¦™çš„ç°å®ä¸–ç•Œå¼‚å¸¸ã€‚åŸºäºå…¶æ¨ç†èƒ½åŠ›çš„è®¸å¤šå¸¸è§æ¨æµ‹å¹¶ä¸æˆç«‹ã€‚æ‰€æœ‰åˆæˆæ•°æ®é›†ç”Ÿæˆå™¨ã€æœ€ç»ˆæç¤ºå’Œè¯„ä¼°è„šæœ¬å‡å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/rose-stl-lab/anomllm">https://github.com/rose-stl-lab/anomllm</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05440v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒå‘ç°LLMså¯¹æ—¶é—´åºåˆ—çš„ç†è§£æ›´åå‘äºå›¾åƒè€Œéæ–‡æœ¬ï¼Œä¸”æ²¡æœ‰å±•ç°å‡ºé€šè¿‡æ˜ç¡®æ¨ç†è¿›è¡Œæ—¶é—´åºåˆ—åˆ†æçš„å¢å¼ºæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒLLMsçš„ç†è§£å¹¶éæºäºå…¶é‡å¤åè§æˆ–ç®—æœ¯èƒ½åŠ›ï¼Œä¸”ä¸åŒæ¨¡å‹åœ¨æ—¶é—´åºåˆ—åˆ†æä¸­çš„è¡Œä¸ºå’Œæ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è™½ç„¶LLMsèƒ½å¤Ÿç†è§£ç®€å•çš„æ—¶é—´åºåˆ—å¼‚å¸¸ï¼Œä½†å¯¹äºæ›´å¾®å¦™çš„ç°å®ä¸–ç•Œå¼‚å¸¸å°šæ— è¯æ®è¡¨æ˜å…¶èƒ½å¤Ÿç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¯¹æ—¶é—´åºåˆ—çš„ç†è§£æ›´åå‘äºå›¾åƒå½¢å¼ã€‚</li>
<li>LLMsåœ¨æ˜ç¡®æ¨ç†æ–¹é¢å¹¶æœªå±•ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>LLMsç†è§£æ—¶é—´åºåˆ—å¹¶éåŸºäºé‡å¤åè§æˆ–ç®—æœ¯èƒ½åŠ›ã€‚</li>
<li>ä¸åŒLLMsåœ¨æ—¶é—´åºåˆ—åˆ†æä¸­çš„è¡Œä¸ºå’Œæ€§èƒ½å­˜åœ¨å·®å¼‚ã€‚</li>
<li>LLMsèƒ½å¤Ÿè¯†åˆ«ç®€å•çš„æ—¶é—´åºåˆ—å¼‚å¸¸ã€‚</li>
<li>å¯¹äºæ›´å¾®å¦™çš„ç°å®ä¸–ç•Œå¼‚å¸¸ï¼ŒLLMsçš„ç†è§£èƒ½åŠ›å°šæ— è¯æ®æ”¯æŒã€‚</li>
<li>ç›¸å…³æ•°æ®å’Œç ”ç©¶æˆæœå·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05440">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4409af2a116ae84d5d5215c024a1b769.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2e7428c22f7adbba05d6a1f2526d395.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="P3P-Pseudo-3D-Pre-training-for-Scaling-3D-Masked-Autoencoders"><a href="#P3P-Pseudo-3D-Pre-training-for-Scaling-3D-Masked-Autoencoders" class="headerlink" title="P3P: Pseudo-3D Pre-training for Scaling 3D Masked Autoencoders"></a>P3P: Pseudo-3D Pre-training for Scaling 3D Masked Autoencoders</h2><p><strong>Authors:Xuechao Chen, Ying Chen, Jialin Li, Qiang Nie, Hanqiu Deng, Yong Liu, Qixing Huang, Yang Li</strong></p>
<p>Pre-training in 3D is pivotal for advancing 3D perception tasks. However, the scarcity of clean 3D data poses significant challenges for scaling 3D pre-training efforts. Drawing inspiration from semi-supervised learning, which effectively combines limited labeled data with abundant unlabeled data, we introduce an innovative self-supervised pre-training framework. This framework leverages both authentic 3D data and pseudo-3D data generated from images using a robust depth estimation model. Another critical challenge is the efficiency of the pre-training process. Existing approaches, such as Point-BERT and Point-MAE, utilize k-nearest neighbors for 3D token embedding, resulting in quadratic time complexity. To address this, we propose a novel token embedding strategy with linear time complexity, coupled with a training-efficient 2D reconstruction target. Our method not only achieves state-of-the-art performance in 3D classification, detection, and few-shot learning but also ensures high efficiency in both pre-training and downstream fine-tuning processes. </p>
<blockquote>
<p>3Dé¢„è®­ç»ƒåœ¨æ¨è¿›3Dæ„ŸçŸ¥ä»»åŠ¡ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œå¹²å‡€3Dæ•°æ®çš„ç¨€ç¼ºç»™å¤§è§„æ¨¡3Dé¢„è®­ç»ƒå·¥ä½œå¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬ä»åŠç›‘ç£å­¦ä¹ ä¸­å¾—åˆ°å¯å‘ï¼ŒåŠç›‘ç£å­¦ä¹ æœ‰æ•ˆåœ°ç»“åˆäº†æœ‰é™çš„æœ‰æ ‡ç­¾æ•°æ®å’Œå¤§é‡çš„æ— æ ‡ç­¾æ•°æ®ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶ä¸ä»…åˆ©ç”¨çœŸå®çš„3Dæ•°æ®ï¼Œè¿˜åˆ©ç”¨ä»å›¾åƒä¸­ä½¿ç”¨ç¨³å¥çš„æ·±åº¦ä¼°è®¡æ¨¡å‹ç”Ÿæˆçš„ä¼ª3Dæ•°æ®ã€‚å¦ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯é¢„è®­ç»ƒè¿‡ç¨‹çš„æ•ˆç‡ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚Point-BERTå’ŒPoint-MAEï¼Œä½¿ç”¨k-æœ€è¿‘é‚»è¿›è¡Œ3Dä»¤ç‰ŒåµŒå…¥ï¼Œå¯¼è‡´æ—¶é—´å¤æ‚åº¦ä¸ºäºŒæ¬¡æ–¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰çº¿æ€§æ—¶é—´å¤æ‚åº¦çš„æ–°å‹ä»¤ç‰ŒåµŒå…¥ç­–ç•¥ï¼Œå¹¶ç»“åˆäº†é«˜æ•ˆçš„2Dé‡å»ºç›®æ ‡è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨3Dåˆ†ç±»ã€æ£€æµ‹å’Œå°‘æ ·æœ¬å­¦ä¹ ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œè€Œä¸”è¿˜ç¡®ä¿äº†é¢„è®­ç»ƒå’Œä¸‹æ¸¸å¾®è°ƒè¿‡ç¨‹çš„é«˜æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10007v2">PDF</a> Under review. Pre-print</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ä¸‰ç»´æ„ŸçŸ¥ä»»åŠ¡é¢„è®­ç»ƒçš„æŒ‘æˆ˜åŠåº”å¯¹ç­–ç•¥ã€‚åˆ©ç”¨åŠç›‘ç£å­¦ä¹ çš„çµæ„Ÿï¼Œé€šè¿‡ç»“åˆçœŸå®çš„ä¸‰ç»´æ•°æ®å’Œå›¾åƒç”Ÿæˆçš„ä¸‰ç»´æ•°æ®è¿›è¡Œè‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒã€‚ä¸ºè§£å†³é¢„è®­ç»ƒè¿‡ç¨‹æ•ˆç‡é—®é¢˜ï¼Œæå‡ºäº†çº¿æ€§æ—¶é—´å¤æ‚åº¦çš„å…¨æ–°tokenåµŒå…¥ç­–ç•¥åŠé«˜æ•ˆçš„äºŒç»´é‡å»ºç›®æ ‡ï¼Œæå‡äº†ä¸‰ç»´åˆ†ç±»ã€æ£€æµ‹å’Œå°‘æ ·æœ¬å­¦ä¹ çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†é¢„è®­ç»ƒåœ¨ä¸‰ç»´æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„é‡è¦æ€§ä»¥åŠç¼ºä¹æ¸…æ´ä¸‰ç»´æ•°æ®å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>å€ŸåŠ©åŠç›‘ç£å­¦ä¹ çš„ç†å¿µï¼Œå°†æœ‰é™çš„æ ‡æ³¨æ•°æ®ä¸å¤§é‡çš„æ— æ ‡æ³¨æ•°æ®ç»“åˆï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åˆ›æ–°æ€§åœ°åˆ©ç”¨çœŸå®ä¸‰ç»´æ•°æ®å’Œå›¾åƒç”Ÿæˆçš„ä¸‰ç»´æ•°æ®è¿›è¡Œè‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒã€‚</li>
<li>ç°æœ‰çš„é¢„è®­ç»ƒæ–¹æ³•å¦‚Point-BERTå’ŒPoint-MAEä½¿ç”¨kæœ€è¿‘é‚»è¿›è¡Œä¸‰ç»´tokenåµŒå…¥ï¼Œå¯¼è‡´äºŒæ¬¡æ—¶é—´å¤æ‚åº¦è¾ƒé«˜ã€‚</li>
<li>é’ˆå¯¹é«˜æ—¶é—´å¤æ‚åº¦é—®é¢˜ï¼Œæå‡ºäº†å…·æœ‰çº¿æ€§æ—¶é—´å¤æ‚åº¦çš„å…¨æ–°tokenåµŒå…¥ç­–ç•¥ã€‚</li>
<li>ç»“åˆé«˜æ•ˆçš„äºŒç»´é‡å»ºç›®æ ‡ï¼Œæå‡äº†é¢„è®­ç»ƒå’Œä¸‹æ¸¸å¾®è°ƒè¿‡ç¨‹çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f99ffe5a9e6db230decb0483b6be34fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-721f466d13daa84fb953a2d423221b3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9ea6215f920837c1ce89251fbcc0ea1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8879d6207d6265f3665c648ef133956b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CoMapGS-Covisibility-Map-based-Gaussian-Splatting-for-Sparse-Novel-View-Synthesis"><a href="#CoMapGS-Covisibility-Map-based-Gaussian-Splatting-for-Sparse-Novel-View-Synthesis" class="headerlink" title="CoMapGS: Covisibility Map-based Gaussian Splatting for Sparse Novel View   Synthesis"></a>CoMapGS: Covisibility Map-based Gaussian Splatting for Sparse Novel View   Synthesis</h2><p><strong>Authors:Youngkyoon Jang, Eduardo PÃ©rez-Pellitero</strong></p>
<p>We propose Covisibility Map-based Gaussian Splatting (CoMapGS), designed to recover underrepresented sparse regions in sparse novel view synthesis. CoMapGS addresses both high- and low-uncertainty regions by constructing covisibility maps, enhancing initial point clouds, and applying uncertainty-aware weighted supervision with a proximity classifier. Our contributions are threefold: (1) CoMapGS reframes novel view synthesis by leveraging covisibility maps as a core component to address region-specific uncertainty levels; (2) Enhanced initial point clouds for both low- and high-uncertainty regions compensate for sparse COLMAP-derived point clouds, improving reconstruction quality and benefiting few-shot 3DGS methods; (3) Adaptive supervision with covisibility-score-based weighting and proximity classification achieves consistent performance gains across scenes with various sparsity scores derived from covisibility maps. Experimental results demonstrate that CoMapGS outperforms state-of-the-art methods on datasets including Mip-NeRF 360 and LLFF. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†åŸºäºå¯è§æ€§æ˜ å°„çš„é«˜æ–¯ç‚¹æ‰©æ•£æŠ€æœ¯ï¼ˆç®€ç§°CoMapGSï¼‰ï¼Œè¯¥æŠ€æœ¯æ—¨åœ¨è§£å†³æ–°å‹ç¨€ç–è§†å›¾åˆæˆä¸­æœªè¢«å……åˆ†ä»£è¡¨çš„ç¨€ç–åŒºåŸŸæ¢å¤é—®é¢˜ã€‚CoMapGSé€šè¿‡æ„å»ºå¯è§æ€§æ˜ å°„ã€å¢å¼ºåˆå§‹ç‚¹äº‘ä»¥åŠé‡‡ç”¨åŸºäºæ¥è¿‘åº¦çš„åˆ†ç±»å™¨è¿›è¡Œä¸ç¡®å®šæ€§åŠ æƒç›‘ç£æ¥è§£å†³é«˜ä¸ç¡®å®šæ€§å’Œä½ä¸ç¡®å®šæ€§åŒºåŸŸé—®é¢˜ã€‚æˆ‘ä»¬çš„è´¡çŒ®ä¸»è¦æœ‰ä¸‰ç‚¹ï¼šï¼ˆ1ï¼‰CoMapGSé€šè¿‡åˆ©ç”¨å¯è§æ€§æ˜ å°„ä½œä¸ºæ ¸å¿ƒç»„ä»¶æ¥è§£å†³ç‰¹å®šåŒºåŸŸçš„ä¸ç¡®å®šæ€§æ°´å¹³é—®é¢˜ï¼Œä»è€Œé‡æ–°å®šä¹‰äº†æ–°å‹è§†å›¾åˆæˆï¼›ï¼ˆ2ï¼‰å¢å¼ºåˆå§‹ç‚¹äº‘å¯¹äºä½ä¸ç¡®å®šæ€§å’Œé«˜ä¸ç¡®å®šæ€§åŒºåŸŸï¼Œå¯ä»¥è¡¥å¿ç¨€ç–çš„COLMAPè¡ç”Ÿç‚¹äº‘ï¼Œä»è€Œæé«˜é‡å»ºè´¨é‡å¹¶æœ‰åˆ©äºå°‘æ ·æœ¬çš„3DGSæ–¹æ³•ï¼›ï¼ˆ3ï¼‰åŸºäºå¯è§æ€§è¯„åˆ†åŠ æƒçš„è‡ªé€‚åº”ç›‘ç£ä¸æ¥è¿‘åº¦åˆ†ç±»æŠ€æœ¯ï¼Œåœ¨ä¸åŒçš„åœºæ™¯ä¸­å‡å®ç°äº†æ€§èƒ½çš„æå‡ï¼Œè¿™äº›åœºæ™¯ç”±å¯è§æ€§æ˜ å°„å¾—å‡ºçš„ç¨€ç–åº¦å¾—åˆ†å„ä¸ç›¸åŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åŒ…æ‹¬Mip-NeRF 360å’ŒLLFFåœ¨å†…çš„æ•°æ®é›†ä¸Šï¼ŒCoMapGSä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.11057v3">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¯è§æ€§æ˜ å°„çš„åæ–¹å·®åˆ†å‰²ï¼ˆCoMapGSï¼‰æ—¨åœ¨æ¢å¤ç¨€ç–æ–°é¢–è§†å›¾åˆæˆä¸­çš„æ¬ ä»£è¡¨ç¨€ç–åŒºåŸŸã€‚CoMapGSé€šè¿‡æ„å»ºåå¯è§æ€§æ˜ å°„ã€å¢å¼ºåˆå§‹ç‚¹äº‘å’Œåº”ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥åŠ æƒç›‘ç£ä¸æ¥è¿‘åˆ†ç±»å™¨ï¼Œè§£å†³é«˜ä¸ç¡®å®šæ€§å’Œä½ä¸ç¡®å®šæ€§åŒºåŸŸçš„é—®é¢˜ã€‚å…¶è´¡çŒ®åŒ…æ‹¬ï¼šåˆ©ç”¨åå¯è§æ€§æ˜ å°„ä½œä¸ºæ ¸å¿ƒç»„ä»¶è§£å†³åŒºåŸŸç‰¹å®šä¸ç¡®å®šæ€§æ°´å¹³çš„é—®é¢˜ï¼›å¢å¼ºåˆå§‹ç‚¹äº‘ä»¥æé«˜é‡å»ºè´¨é‡å’Œæœ‰åˆ©äºå°‘æ•°æ‹æ‘„ç‚¹çš„ä¸‰ç»´å‡ ä½•æ‰«ææ–¹æ³•ï¼›è‡ªé€‚åº”ç›‘ç£é€šè¿‡åŸºäºåå¯è§æ€§å¾—åˆ†çš„åŠ æƒå’Œæ¥è¿‘åˆ†ç±»å®ç°è·¨åœºæ™¯çš„æŒç»­æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoMapGSåœ¨Mip-NeRF 360å’ŒLLFFæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CoMapGSè¢«è®¾è®¡ä¸ºæ¢å¤ç¨€ç–æ–°é¢–è§†å›¾åˆæˆä¸­çš„æ¬ ä»£è¡¨ç¨€ç–åŒºåŸŸã€‚</li>
<li>é€šè¿‡æ„å»ºåå¯è§æ€§æ˜ å°„ï¼Œè§£å†³é«˜ä¸ç¡®å®šæ€§å’Œä½ä¸ç¡®å®šæ€§åŒºåŸŸçš„é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨åå¯è§æ€§æ˜ å°„ä½œä¸ºæ ¸å¿ƒç»„ä»¶ï¼Œé’ˆå¯¹ç‰¹å®šåŒºåŸŸçš„ä¸ç¡®å®šæ€§è¿›è¡Œå¤„ç†ã€‚</li>
<li>å¢å¼ºåˆå§‹ç‚¹äº‘ä»¥æé«˜é‡å»ºè´¨é‡ï¼Œå¹¶ä¼˜åŒ–å°‘æ•°æ‹æ‘„ç‚¹çš„ä¸‰ç»´å‡ ä½•æ‰«ææ–¹æ³•ã€‚</li>
<li>é€šè¿‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥åŠ æƒç›‘ç£ä¸æ¥è¿‘åˆ†ç±»å™¨å®ç°è‡ªé€‚åº”ç›‘ç£ã€‚</li>
<li>åŸºäºåå¯è§æ€§å¾—åˆ†çš„åŠ æƒå’Œæ¥è¿‘åˆ†ç±»çš„æ–¹æ³•åœ¨æ‰€æœ‰åœºæ™¯ä¸­å®ç°æ€§èƒ½æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.11057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7de16bc229b040217a920853b43cbfe4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d567642e9ee155e19b2a3f7b426337c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55f7ddd5aa3c7a224c093bb40b7396ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2cc4c25d3a32cf797b0c3d095a83fe6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24098115e478797ad4abf4d2af80d418.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e9c42602c21b81f4a9b9c1e75bf0491a.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Electromyography-Informed Facial Expression Reconstruction for   Physiological-Based Synthesis and Analysis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9a114a08cd327cd33dbc03616a84b252.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Towards Zero-Shot Multimodal Machine Translation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18799.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
