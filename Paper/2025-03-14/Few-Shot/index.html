<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-03-14  Membership Inference Attacks fueled by Few-Short Learning to detect   privacy leakage tackling data integrity">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b5713d02847055507997eb5be3d3f6db.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    61 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-14-更新"><a href="#2025-03-14-更新" class="headerlink" title="2025-03-14 更新"></a>2025-03-14 更新</h1><h2 id="Membership-Inference-Attacks-fueled-by-Few-Short-Learning-to-detect-privacy-leakage-tackling-data-integrity"><a href="#Membership-Inference-Attacks-fueled-by-Few-Short-Learning-to-detect-privacy-leakage-tackling-data-integrity" class="headerlink" title="Membership Inference Attacks fueled by Few-Short Learning to detect   privacy leakage tackling data integrity"></a>Membership Inference Attacks fueled by Few-Short Learning to detect   privacy leakage tackling data integrity</h2><p><strong>Authors:Daniel Jiménez-López, Nuria Rodríguez-Barroso, M. Victoria Luzón, Francisco Herrera</strong></p>
<p>Deep learning models have an intrinsic privacy issue as they memorize parts of their training data, creating a privacy leakage. Membership Inference Attacks (MIA) exploit it to obtain confidential information about the data used for training, aiming to steal information. They can be repurposed as a measurement of data integrity by inferring whether it was used to train a machine learning model. While state-of-the-art attacks achieve a significant privacy leakage, their requirements are not feasible enough, hindering their role as practical tools to assess the magnitude of the privacy risk. Moreover, the most appropriate evaluation metric of MIA, the True Positive Rate at low False Positive Rate lacks interpretability. We claim that the incorporation of Few-Shot Learning techniques to the MIA field and a proper qualitative and quantitative privacy evaluation measure should deal with these issues. In this context, our proposal is twofold. We propose a Few-Shot learning based MIA, coined as the FeS-MIA model, which eases the evaluation of the privacy breach of a deep learning model by significantly reducing the number of resources required for the purpose. Furthermore, we propose an interpretable quantitative and qualitative measure of privacy, referred to as Log-MIA measure. Jointly, these proposals provide new tools to assess the privacy leakage and to ease the evaluation of the training data integrity of deep learning models, that is, to analyze the privacy breach of a deep learning model. Experiments carried out with MIA over image classification and language modeling tasks and its comparison to the state-of-the-art show that our proposals excel at reporting the privacy leakage of a deep learning model with little extra information. </p>
<blockquote>
<p>深度学习模型存在固有的隐私问题，因为它们会记忆部分训练数据，从而造成隐私泄露。成员推理攻击（MIA）会利用这一点来获取有关用于训练的数据的机密信息，旨在窃取信息。它们可以通过推断数据是否用于训练机器学习模型来重新用作数据完整性的度量。虽然最先进的攻击会造成显著的隐私泄露，但它们的要求并不可行，阻碍了其作为评估隐私风险大小的实用工具的作用。此外，MIA的最适当的评估指标——在低误报率下的真正率缺乏可解释性。我们主张将小样学习技术引入MIA领域，并引入适当的定性和定量隐私评估措施来解决这些问题。在此背景下，我们的提案是双向的。我们提出了一种基于小样学习的MIA，称为FeS-MIA模型，通过显著减少用于评估深度学习模型隐私泄露所需的资源，从而简化了评估。此外，我们提出了一种可解释的量化和定性隐私度量标准，称为Log-MIA度量。共同地，这些提议提供了新的工具来评估隐私泄露并轻松评估深度学习模型的训练数据完整性，即分析深度学习模型的隐私泄露情况。针对图像分类和语言建模任务进行的MIA实验及其与最新技术的比较表明，我们的方案在报告深度学习模型的隐私泄露方面表现出色，并且几乎不需要额外的信息。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09365v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了深度学习的隐私问题，指出其训练数据存在隐私泄露的风险。文章介绍了成员推理攻击（MIA）作为衡量数据完整性和隐私泄露的工具，并指出了现有攻击方法存在的不足。为了解决这些问题，本文提出了基于小样本学习的MIA方法（FeS-MIA模型），并引入了可解释性强、定量定性的隐私度量标准（Log-MIA度量）。实验表明，新方法在报告深度学习的隐私泄露方面表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习模型存在隐私泄露问题，训练数据可能被泄露。</li>
<li>成员推理攻击（MIA）可用于获取训练数据的相关信息，评估数据完整性。</li>
<li>现有MIA方法资源需求大，不适合作为实用工具评估隐私风险。</li>
<li>引入基于小样本学习的MIA方法（FeS-MIA模型），降低资源需求，简化隐私泄露评估。</li>
<li>提出新的隐私度量标准（Log-MIA度量），具有可解释性、定量定性特点。</li>
<li>实验表明，新方法在报告深度学习的隐私泄露方面表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09365">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-40b8c981b0fca6e46bd473cf7967beab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-585cf3cd5841ce1cb3250cf91757d381.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="N2C2-Nearest-Neighbor-Enhanced-Confidence-Calibration-for-Cross-Lingual-In-Context-Learning"><a href="#N2C2-Nearest-Neighbor-Enhanced-Confidence-Calibration-for-Cross-Lingual-In-Context-Learning" class="headerlink" title="N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual   In-Context Learning"></a>N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual   In-Context Learning</h2><p><strong>Authors:Jie He, Simon Yu, Deyi Xiong, Víctor Gutiérrez-Basulto, Jeff Z. Pan</strong></p>
<p>Recent advancements of in-context learning (ICL) show language models can significantly improve their performance when demonstrations are provided. However, little attention has been paid to model calibration and prediction confidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a thorough analysis of ICL for cross-lingual sentiment classification. Our findings suggest that ICL performs poorly in cross-lingual scenarios, exhibiting low accuracy and presenting high calibration errors. In response, we propose a novel approach, N2C2, which employs a -nearest neighbors augmented classifier for prediction confidence calibration. N2C2 narrows the prediction gap by leveraging a datastore of cached few-shot instances. Specifically, N2C2 integrates the predictions from the datastore and incorporates confidence-aware distribution, semantically consistent retrieval representation, and adaptive neighbor combination modules to effectively utilize the limited number of supporting instances. Evaluation on two multilingual sentiment classification datasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine tuning, prompt tuning and recent state-of-the-art methods in terms of accuracy and calibration errors. </p>
<blockquote>
<p>最近关于上下文学习（ICL）的进展表明，当提供演示时，语言模型的性能可以得到显著的提升。然而，对于跨语言场景下ICL的模型校准和预测置信度的关注度很低。为了填补这一空白，我们对跨语言情感分类的ICL进行了深入的分析。我们的研究发现，在跨语言场景中，ICL表现不佳，准确率较低，并且存在较高的校准误差。为了应对这一问题，我们提出了一种新方法N2C2，它采用-最近邻增强分类器进行预测置信度校准。N2C2通过利用存储少量实例的数据存储库来缩小预测差距。具体来说，N2C2结合了数据存储中的预测，并融入了信心感知分布、语义一致检索表示和自适应邻居组合模块，以有效利用有限的支持实例。在两个多语言情感分类数据集上的评估表明，N2C2优于传统的ICL。在准确性和校准误差方面，它超越了微调、提示调整和最近的最先进方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09218v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在跨语言场景中，语境学习（ICL）的表现有待提升，存在准确性低和校准误差高的问题。针对此，我们提出了一种名为N2C2的新方法，利用最近邻增强分类器进行预测信心校准。N2C2通过利用缓存的少量实例数据来缩小预测差距，并有效结合信心感知分布、语义一致检索表示和自适应邻居组合模块。在两种多语言情感分类数据集上的评估表明，N2C2在准确性和校准误差方面超越了传统的ICL、微调、提示调整和最近的高级方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语境学习（ICL）在跨语言情感分类中表现不佳，存在准确度和校准问题。</li>
<li>提出了一种新方法N2C2，利用最近邻增强分类器进行预测信心校准。</li>
<li>N2C2通过缓存的少量实例数据来缩小预测差距。</li>
<li>N2C2结合了信心感知分布、语义一致检索表示和自适应邻居组合模块。</li>
<li>N2C2在准确性和校准误差方面超越了传统的语境学习方法和微调、提示调整技术。</li>
<li>N2C2方法在跨语言情感分类任务中具有优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09218">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d524f2e89579c3ca032c6e6325192756.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70e3bddec422b8e17c72c281ed127da4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72da43c4e7b47f0700820a54857c817c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01b1be3a8e2a3536e8e1811929183ce8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MMRL-Multi-Modal-Representation-Learning-for-Vision-Language-Models"><a href="#MMRL-Multi-Modal-Representation-Learning-for-Vision-Language-Models" class="headerlink" title="MMRL: Multi-Modal Representation Learning for Vision-Language Models"></a>MMRL: Multi-Modal Representation Learning for Vision-Language Models</h2><p><strong>Authors:Yuncheng Guo, Xiaodong Gu</strong></p>
<p>Large-scale pre-trained Vision-Language Models (VLMs) have become essential for transfer learning across diverse tasks. However, adapting these models with limited few-shot data often leads to overfitting, diminishing their performance on new tasks. To tackle this issue, we propose a novel Multi-Modal Representation Learning (MMRL) framework that introduces a shared, learnable, and modality-agnostic representation space. MMRL projects the space tokens to text and image representation tokens, facilitating more effective multi-modal interactions. Unlike previous approaches that solely optimize class token features, MMRL integrates representation tokens at higher layers of the encoders–where dataset-specific features are more prominent–while preserving generalized knowledge in the lower layers. During training, both representation and class features are optimized, with trainable projection layer applied to the representation tokens, whereas the class token projection layer remains frozen to retain pre-trained knowledge. Furthermore, a regularization term is introduced to align the class features and text features with the zero-shot features from the frozen VLM, thereby safeguarding the model’s generalization capacity. For inference, a decoupling strategy is employed, wherein both representation and class features are utilized for base classes, while only the class features, which retain more generalized knowledge, are used for new tasks. Extensive experiments across 15 datasets demonstrate that MMRL outperforms state-of-the-art methods, achieving a balanced trade-off between task-specific adaptation and generalization. Code is available at <a target="_blank" rel="noopener" href="https://github.com/yunncheng/MMRL">https://github.com/yunncheng/MMRL</a>. </p>
<blockquote>
<p>大规模预训练的视觉语言模型（VLMs）对于跨不同任务的迁移学习已经变得至关重要。然而，使用有限的少量数据适应这些模型往往会导致过拟合，从而在新的任务上表现不佳。为了解决这个问题，我们提出了一种新颖的多模态表示学习（MMRL）框架，它引入了一个共享、可学习和模态无关的表示空间。MMRL将空间标记投影到文本和图像表示标记上，促进了更有效的多模态交互。与以前仅优化类别标记特征的方法不同，MMRL在编码器的更高层集成了表示标记，这些层级的数据集特定特征更为突出，同时在下层保持通用知识。在训练过程中，表示和类别特征都得到了优化，对表示标记应用了可训练的投影层，而类别标记投影层则保持冻结以保留预训练知识。此外，引入了正则化项来对齐类别特征和文本特征与冻结VLM的零射击特征，从而保护模型的泛化能力。对于推断，采用了解耦策略，其中表示和类别特征都用于基础类，而仅使用保留更多通用知识的类别特征用于新任务。在15个数据集上的大量实验表明，MMRL优于最新方法，实现了任务特定适应性和泛化之间的平衡。代码可在<a target="_blank" rel="noopener" href="https://github.com/yunncheng/MMRL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yunncheng/MMRL找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08497v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>大规模预训练视语言模型（VLMs）在迁移学习各种任务中扮演重要角色。然而，使用有限的少量数据进行模型适应常导致过拟合，影响新任务上的性能。为解决这一问题，我们提出一种新型多模态表示学习（MMRL）框架，引入共享、可学习和模态无关的表示空间。MMRL将空间标记投影到文本和图像表示标记，实现更有效的多模态交互。不同于仅优化类别标记特征的先前方法，MMRL在编码器的高层集成表示标记，同时保留低层的通用知识。训练过程中，同时优化表示和类别特征，应用可训练的投影层于表示标记，而冻结类别标记投影层以保留预训练知识。此外，引入正则化项以对齐类别特征和文本特征，与冻结VLM的零样本特征，从而保障模型的泛化能力。推理时，采用解耦策略，同时利用表示和类别特征进行基础类别的处理，而仅使用保留更多通用知识的类别特征应对新任务。跨15个数据集的广泛实验显示，MMRL优于现有方法，实现了任务特定适应和泛化之间的平衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模预训练视语言模型（VLMs）在迁移学习中的重要性。</li>
<li>使用有限的少量数据进行模型适应可能导致过拟合问题。</li>
<li>新型多模态表示学习（MMRL）框架引入共享、可学习和模态无关的表示空间。</li>
<li>MMRL通过投影空间标记到文本和图像表示标记，实现多模态交互。</li>
<li>MMRL在编码器的高层集成表示标记，同时保留低层的通用知识。</li>
<li>在训练过程中，同时优化表示和类别特征，并应用正则化项以对齐类别特征和文本特征。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08497">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-db8e9041f69f7afeb18815ef80b34c0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dae975ba432b158293f3f1a0ab1b109b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb187fc42024f5b87086306375cfba0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f7612300d9f368546758d7630a0a27b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Personalized-Code-Readability-Assessment-Are-We-There-Yet"><a href="#Personalized-Code-Readability-Assessment-Are-We-There-Yet" class="headerlink" title="Personalized Code Readability Assessment: Are We There Yet?"></a>Personalized Code Readability Assessment: Are We There Yet?</h2><p><strong>Authors:Antonio Vitale, Emanuela Guglielmi, Rocco Oliveto, Simone Scalabrino</strong></p>
<p>Unreadable code could be a breeding ground for errors. Thus, previous work defined approaches based on machine learning to automatically assess code readability that can warn developers when some code artifacts (e.g., classes) become unreadable. Given datasets of code snippets manually evaluated by several developers in terms of their perceived readability, such approaches (i) establish a snippet-level ground truth, and (ii) train a binary (readable&#x2F;unreadable) or a ternary (readable&#x2F;neutral&#x2F;unreadable) code readability classifier. Given this procedure, all existing approaches neglect the subjectiveness of code readability, i.e., the possible different developer-specific nuances in the code readability perception. In this paper, we aim to understand to what extent it is possible to assess code readability as subjectively perceived by developers through a personalized code readability assessment approach. This problem is significantly more challenging than the snippet-level classification problem: We assume that, in a realistic scenario, a given developer is keen to provide only a few code readability evaluations, thus less data is available. For this reason, we adopt an LLM with few-shot learning to achieve our goal. Our results, however, show that such an approach achieves worse results than a state-of-the-art feature-based model that is trained to work at the snippet-level. We tried to understand why this happens by looking more closely at the quality of the available code readability datasets and assessed the consistency of the inter-developer evaluations. We observed that up to a third of the evaluations are self-contradictory. Our negative results call for new and more reliable code readability datasets. </p>
<blockquote>
<p>难以理解的代码可能会成为错误的滋生地。因此，先前的工作定义了基于机器学习的方法，以自动评估代码可读性，当某些代码产物（例如类）变得难以理解时，可以警告开发人员。给定由多个开发人员手动评估的代码片段数据集，在感知可读性方面，这些方法（i）建立片段级别的基准真实值，（ii）训练二进制（可读&#x2F;不可读）或三元（可读&#x2F;中性&#x2F;不可读）的代码可读性分类器。鉴于这一程序，所有现有方法都忽略了代码可读性的主观性，即开发人员对代码可读性感知中可能存在的不同特定细微差别。在本文中，我们的目标是了解在何种程度上可以通过个性化的代码可读性评估方法来主观地评估开发人员所感知的代码可读性。这个问题比片段级别的分类问题更具挑战性：我们假设在现实场景中，给定的开发人员热衷于提供有限的代码可读性评估，因此可用的数据较少。因此，我们采用具有小样本学习的LLM来实现我们的目标。然而，结果却显示，这种方法的结果不如基于特征的最新模型（该模型经过训练以在片段级别工作）。为了了解为什么会发生这种情况，我们更仔细地研究了可用的代码可读性数据集的质量并评估了开发人员之间评价的连贯性。我们发现高达三分之一的评估是相互矛盾的。我们的负面结果呼吁需要新的、更可靠的代码可读性数据集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07870v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注代码可读性评估的主观性问题，即不同开发者对代码可读性的感知可能存在差异。研究者旨在通过个性化代码可读性评估方法来理解如何评估开发者主观感知的代码可读性。尽管采用了具有few-shot学习功能的大型语言模型，但该方法的效果却不如基于特征的片段级别分类模型。研究发现，现有代码可读性数据集存在质量问题，开发者之间的评价一致性有待提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器学习方法可自动评估代码可读性，警告开发者当代码片段变得不可读时。</li>
<li>现有方法忽略了代码可读性评估中的主观性，即不同开发者对代码可读性的感知差异。</li>
<li>研究者尝试通过个性化代码可读性评估方法来理解这一主观性。</li>
<li>采用大型语言模型进行few-shot学习以实现这一目标，但效果不如基于特征的片段级别分类模型。</li>
<li>现存的代码可读性数据集存在质量问题，例如部分评价自相矛盾。</li>
<li>研究结果强调了需要新的、更可靠的代码可读性数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07870">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-88b64da22ac5285e6fbe3ba9af8dafaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2510c95e419144f9ac68399271737ad7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-255aea8838cd36cf99471e91591fa87c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bd617391296c86f232bfe71a9b622a3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PointVLA-Injecting-the-3D-World-into-Vision-Language-Action-Models"><a href="#PointVLA-Injecting-the-3D-World-into-Vision-Language-Action-Models" class="headerlink" title="PointVLA: Injecting the 3D World into Vision-Language-Action Models"></a>PointVLA: Injecting the 3D World into Vision-Language-Action Models</h2><p><strong>Authors:Chengmeng Li, Junjie Wen, Yan Peng, Yaxin Peng, Feifei Feng, Yichen Zhu</strong></p>
<p>Vision-Language-Action (VLA) models excel at robotic tasks by leveraging large-scale 2D vision-language pretraining, but their reliance on RGB images limits spatial reasoning critical for real-world interaction. Retraining these models with 3D data is computationally prohibitive, while discarding existing 2D datasets wastes valuable resources. To bridge this gap, we propose PointVLA, a framework that enhances pre-trained VLAs with point cloud inputs without requiring retraining. Our method freezes the vanilla action expert and injects 3D features via a lightweight modular block. To identify the most effective way of integrating point cloud representations, we conduct a skip-block analysis to pinpoint less useful blocks in the vanilla action expert, ensuring that 3D features are injected only into these blocks–minimizing disruption to pre-trained representations.   Extensive experiments demonstrate that PointVLA outperforms state-of-the-art 2D imitation learning methods, such as OpenVLA, Diffusion Policy and DexVLA, across both simulated and real-world robotic tasks. Specifically, we highlight several key advantages of PointVLA enabled by point cloud integration: (1) Few-shot multi-tasking, where PointVLA successfully performs four different tasks using only 20 demonstrations each; (2) Real-vs-photo discrimination, where PointVLA distinguishes real objects from their images, leveraging 3D world knowledge to improve safety and reliability; (3) Height adaptability, Unlike conventional 2D imitation learning methods, PointVLA enables robots to adapt to objects at varying table height that unseen in train data. Furthermore, PointVLA achieves strong performance in long-horizon tasks, such as picking and packing objects from a moving conveyor belt, showcasing its ability to generalize across complex, dynamic environments. </p>
<blockquote>
<p>视觉语言动作（VLA）模型通过利用大规模二维视觉语言预训练在机器人任务方面表现出色，但它们对RGB图像的依赖限制了空间推理，这对于现实世界交互至关重要。使用三维数据对这些模型进行再训练计算成本高昂，而放弃现有的二维数据集则浪费宝贵资源。为了弥补这一差距，我们提出了PointVLA框架，该框架可以在无需重新训练的情况下，增强预训练的VLA模型以处理点云输入。我们的方法冻结了原始动作专家模型，并通过一个轻量级模块块注入三维特征。为了确定整合点云表示的最有效方式，我们进行了跳过块分析，以定位原始动作专家模型中不太有用的块，确保三维特征仅注入这些块中，最大限度地减少对预训练表示的干扰。大量实验表明，PointVLA在模拟和现实世界机器人任务上均优于最新的二维模仿学习方法，如OpenVLA、Diffusion Policy和DexVLA。特别是，我们突出了通过点云集成实现的PointVLA的几个关键优势：（1）少样本多任务处理能力，其中PointVLA仅使用每个任务20次演示就成功完成了四个不同任务；（2）真实与照片区分能力，PointVLA能够区分真实物体和它们的图像，利用三维世界知识提高安全性和可靠性；（3）高度适应性：与传统的二维模仿学习方法不同，PointVLA使机器人能够适应训练中未见过的不同高度的桌子上的物体。（4）此外，PointVLA在执行长期任务时也表现出强大的性能，如从移动传送带上挑选和打包物体，展示了其在复杂动态环境中的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07511v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大规模二维视觉语言预训练的Vision-Language-Action（VLA）模型在机器人任务上表现出色，但其依赖RGB图像限制了空间推理能力。本文提出的PointVLA框架能够在不需要重新训练的情况下，增强预训练的VLA模型对点云输入的处理能力。通过冻结原始动作专家并注入3D特征，PointVLA实现了与点云表示的有效集成。实验表明，PointVLA在模拟和真实机器人任务上均优于最新的二维模仿学习方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PointVLA框架增强了预训练的Vision-Language-Action（VLA）模型对点云输入的处理能力，无需重新训练。</li>
<li>通过冻结原始动作专家并注入3D特征，实现了与点云表示的有效集成。</li>
<li>PointVLA在模拟和真实机器人任务上的表现优于最新的二维模仿学习方法。</li>
<li>PointVLA支持few-shot多任务学习，能够使用少量演示完成多个任务。</li>
<li>PointVLA能够区分真实物体和图像，利用3D世界知识提高安全性和可靠性。</li>
<li>PointVLA具有高度的适应性，能够适应不同高度的物体，不同于传统的二维模仿学习方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07511">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cd4cb28ef57ca631b9fdcfad825769c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a70a08352f90368401bc96bcfbac63c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4e0fe57ed497f54a83bb6e53c020e32.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2324fc0a8ef047a41b0045c91892a781.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-519564003962391a5648efff7e754dfd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="X2CT-CLIP-Enable-Multi-Abnormality-Detection-in-Computed-Tomography-from-Chest-Radiography-via-Tri-Modal-Contrastive-Learning"><a href="#X2CT-CLIP-Enable-Multi-Abnormality-Detection-in-Computed-Tomography-from-Chest-Radiography-via-Tri-Modal-Contrastive-Learning" class="headerlink" title="X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography   from Chest Radiography via Tri-Modal Contrastive Learning"></a>X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography   from Chest Radiography via Tri-Modal Contrastive Learning</h2><p><strong>Authors:Jianzhong You, Yuan Gao, Sangwook Kim, Chris Mcintosh</strong></p>
<p>Computed tomography (CT) is a key imaging modality for diagnosis, yet its clinical utility is marred by high radiation exposure and long turnaround times, restricting its use for larger-scale screening. Although chest radiography (CXR) is more accessible and safer, existing CXR foundation models focus primarily on detecting diseases that are readily visible on the CXR. Recently, works have explored training disease classification models on simulated CXRs, but they remain limited to recognizing a single disease type from CT. CT foundation models have also emerged with significantly improved detection of pathologies in CT. However, the generalized application of CT-derived labels on CXR has remained illusive. In this study, we propose X2CT-CLIP, a tri-modal knowledge transfer learning framework that bridges the modality gap between CT and CXR while reducing the computational burden of model training. Our approach is the first work to enable multi-abnormality classification in CT, using CXR, by transferring knowledge from 3D CT volumes and associated radiology reports to a CXR encoder via a carefully designed tri-modal alignment mechanism in latent space. Extensive evaluations on three multi-label CT datasets demonstrate that our method outperforms state-of-the-art baselines in cross-modal retrieval, few-shot adaptation, and external validation. These results highlight the potential of CXR, enriched with knowledge derived from CT, as a viable efficient alternative for disease detection in resource-limited settings. </p>
<blockquote>
<p>计算机断层扫描（CT）是诊断的关键成像方式，但其临床应用受到高辐射暴露和长时间等待结果的限制，阻碍了其在大规模筛查中的使用。虽然胸部放射摄影（CXR）更容易获取且更安全，但现有的CXR基础模型主要关注于检测在CXR上容易看到的疾病。近期的研究工作虽已探索在模拟的CXR上进行疾病分类模型训练，但它们仅限于从CT图像中识别单一疾病类型。CT基础模型也已出现，显著提高了CT中病理的检测能力。然而，将CT衍生的标签在CXR上的通用应用仍然具有挑战性。本研究中，我们提出了X2CT-CLIP，这是一个三模态知识迁移学习框架，缩小了CT和CXR之间的模态差距，同时降低了模型训练的计算负担。我们的方法是通过从3D CT体积和相关放射学报告向CXR编码器转移知识，利用精心设计的潜在空间三模态对齐机制，实现了使用CXR在CT中进行多异常分类的首项工作。在三个多标签CT数据集上的广泛评估表明，我们的方法在跨模态检索、少样本适应和外部验证方面均优于最新基线。这些结果凸显了以CT知识丰富CXR的潜力，可作为资源有限环境中疾病检测的可行高效替代方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02162v2">PDF</a> 11 pages, 1 figure, 5 tables</p>
<p><strong>Summary</strong>：本研究提出了一个名为X2CT-CLIP的三模态知识迁移学习框架，旨在缩小CT和CXR之间的模态差距，同时降低模型训练的计算负担。该框架通过精心设计的三模态对齐机制，将来自三维CT体积和相关放射学报告的隐含知识传递给CXR编码器，使得用CXR进行多异常性CT分类成为可能。在三个多标签CT数据集上的评估表明，该方法在跨模态检索、小样本适应和外部验证方面的表现均优于现有技术基线。这些结果突显了使用来自CT的知识来丰富CXR的潜力，使其成为资源受限环境中疾病检测的有效高效替代方案。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>X2CT-CLIP框架实现了CT和CXR之间的知识迁移，缩小了两种成像方式的模态差距。</li>
<li>通过三模态对齐机制，将来自CT的体积数据和放射学报告的知识融合到CXR编码器中。</li>
<li>该框架支持使用CXR进行多异常性CT分类，这是一个全新的研究领域。</li>
<li>在多个多标签CT数据集上的评估显示，X2CT-CLIP在跨模态检索、小样本适应和外部验证方面均表现出卓越性能。</li>
<li>X2CT-CLIP的结果表明，使用来自CT的知识增强的CXR在资源受限环境中进行疾病检测是一种可行的替代方案。</li>
<li>与现有技术相比，X2CT-CLIP具有更低的计算负担和更高的检测准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02162">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e52ed559bd34ca0791e6fd0e6f0095f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca5a9fd9eb9e82ba594ec13d430ba23b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fb5dcf0b66a8a6fae22c18927754faf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d730010f2b7ee6dc2a7be69f95cffd32.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Rationalization-Models-for-Text-to-SQL"><a href="#Rationalization-Models-for-Text-to-SQL" class="headerlink" title="Rationalization Models for Text-to-SQL"></a>Rationalization Models for Text-to-SQL</h2><p><strong>Authors:Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Dharmashankar Subramanian</strong></p>
<p>We introduce a framework for generating Chain-of-Thought (CoT) rationales to enhance text-to-SQL model fine-tuning. These rationales consist of intermediate SQL statements and explanations, serving as incremental steps toward constructing the final SQL query. The process begins with manually annotating a small set of examples, which are then used to prompt a large language model in an iterative, dynamic few-shot knowledge distillation procedure from a teacher model. A rationalization model is subsequently trained on the validated decomposed queries, enabling extensive synthetic CoT annotations for text-to-SQL datasets. To evaluate the approach, we fine-tune small language models with and without these rationales on the BIRD dataset. Results indicate that step-by-step query generation improves execution accuracy, especially for moderately and highly complex queries, while also enhancing explainability. </p>
<blockquote>
<p>我们引入了一个生成Chain-of-Thought（CoT）理由的框架，以增强文本到SQL模型的微调。这些理由包括中间的SQL语句和解释，作为构建最终SQL查询的增量步骤。过程始于手动标注一小部分例子，然后用于在迭代、动态的少数样本知识蒸馏过程中提示大型语言模型从教师模型中学习。随后在验证过的分解查询上训练合理化模型，为文本到SQL数据集提供广泛的合成CoT注释。为了评估该方法，我们在BIRD数据集上对带有和不带有这些理由的小型语言模型进行了微调。结果表明，逐步生成查询提高了执行准确性，特别是对于中等和高度复杂的查询，同时增强了可解释性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06759v3">PDF</a> Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs</p>
<p><strong>Summary</strong></p>
<p>基于Chain-of-Thought（CoT）原理，提出一种提升文本到SQL模型微调效果的框架。该框架通过手动标注少量示例，利用大型语言模型进行迭代、动态的少样本知识蒸馏过程，从教师模型中获取信息。随后在验证后的分解查询上训练解释模型，为文本到SQL数据集提供丰富的合成CoT注释。在BIRD数据集上进行微调实验表明，逐步查询生成能提高执行准确性，特别是在中等和高度复杂的查询上，同时提高了解释性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出使用Chain-of-Thought（CoT）原理增强文本到SQL模型的微调效果。</li>
<li>通过手动标注少量示例来引导大型语言模型进行知识蒸馏。</li>
<li>框架采用迭代、动态的少样本知识蒸馏过程从教师模型中获取信息。</li>
<li>训练解释模型于验证后的分解查询上，提供丰富的合成CoT注释。</li>
<li>逐步查询生成能提高执行准确性，特别是在复杂查询上。</li>
<li>此方法不仅提高了模型的性能，同时也增强了模型的可解释性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06759">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b5713d02847055507997eb5be3d3f6db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac7591afcabb4de48b07038449651318.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ffb3a4fac446037e8b054310b875c4fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-465989f087ba4c2ed99c1b84af82473f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-–-An-evaluation-on-urological-doctors’-notes"><a href="#Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-–-An-evaluation-on-urological-doctors’-notes" class="headerlink" title="Can open source large language models be used for tumor documentation in   Germany? – An evaluation on urological doctors’ notes"></a>Can open source large language models be used for tumor documentation in   Germany? – An evaluation on urological doctors’ notes</h2><p><strong>Authors:Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Torsten Panholzer</strong></p>
<p>Tumor documentation in Germany is largely done manually, requiring reading patient records and entering data into structured databases. Large language models (LLMs) could potentially enhance this process by improving efficiency and reliability. This evaluation tests eleven different open source LLMs with sizes ranging from 1-70 billion model parameters on three basic tasks of the tumor documentation process: identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis. For evaluating the LLMs on these tasks, a dataset of annotated text snippets based on anonymized doctors’ notes from urology was prepared. Different prompting strategies were used to investigate the effect of the number of examples in few-shot prompting and to explore the capabilities of the LLMs in general. The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks. Models with less extensive training data or having fewer than 7 billion parameters showed notably lower performance, while larger models did not display performance gains. Examples from a different medical domain than urology could also improve the outcome in few-shot prompting, which demonstrates the ability of LLMs to handle tasks needed for tumor documentation. Open source LLMs show a strong potential for automating tumor documentation. Models from 7-12 billion parameters could offer an optimal balance between performance and resource efficiency. With tailored fine-tuning and well-designed prompting, these models might become important tools for clinical documentation in the future. The code for the evaluation is available from <a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval">https://github.com/stefan-m-lenz/UroLlmEval</a>. We also release the dataset as a new valuable resource that addresses the shortage of authentic and easily accessible benchmarks in German-language medical NLP. </p>
<blockquote>
<p>在德国，肿瘤记录工作大多以手动方式进行，需要阅读患者病历并将数据录入结构化数据库。大型语言模型（LLM）有潜力通过提高效率和可靠性来增强这一流程。本次评估对三种基本任务（识别肿瘤诊断、分配ICD-10代码和提取首次诊断日期）中，从1亿到70亿模型参数的11个不同开源LLM进行了测试。为了评估这些LLM在这些任务上的表现，准备了一个基于泌尿学匿名医生笔记的标注文本片段数据集。使用了不同的提示策略来研究少数示例提示中示例数量的影响，并探索LLM的一般功能。Llama 3.1 8B、Mistral 7B和Mistral NeMo 12B等模型在这些任务中表现良好。训练数据较少的模型或参数少于7亿的模型表现明显较差，而较大的模型并没有显示出性能提升。来自泌尿学以外的医学领域的例子也能改善少数示例提示的结果，这证明了LLM处理肿瘤记录所需任务的能力。开源LLM在自动化肿瘤记录方面显示出强大的潜力。具有7-12亿参数的模型可能在性能和资源效率之间达到最佳平衡。通过有针对性的微调和精心设计的提示，这些模型可能成为未来临床记录的重要工具。评估的代码可从<a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval%E8%8E%B7%E5%8F%96%E3%80%82%E6%88%91%E4%BB%AC%E8%BF%98%E5%8F%91%E5%B8%83%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E4%BD%9C%E4%B8%BA%E8%A7%A3%E5%86%B3%E5%BE%B7%E5%9B%BD%E5%8C%BB%E5%AD%A6NLP%E4%B8%AD%E7%9C%9F%E5%AE%9E%E3%80%81%E6%98%93%E4%BA%8E%E8%AE%BF%E9%97%AE%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%B5%84%E6%BA%90%E7%9F%AD%E7%BC%BA%E7%9A%84%E6%96%B0%E6%9C%89%E4%BB%B7%E5%80%BC%E8%B5%84%E6%BA%90%E3%80%82">https://github.com/stefan-m-lenz/UroLlmEval获取。我们还发布该数据集，作为解决德国医学NLP中真实、易于访问的基准测试资源短缺的新有价值资源。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12106v2">PDF</a> 48 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>基于德国肿瘤文档记录过程中存在的问题，本研究评估了十一种开源的大型语言模型（LLMs）。这些模型在肿瘤诊断识别、ICD-10代码分配和首次诊断日期提取三个基本任务中的表现被进行了测试。研究结果显示，模型如Llama 3.1 8B、Mistral 7B和Mistral NeMo 12B在任务中表现良好。具有较少训练数据或参数少于7亿的模型表现较差，而更大的模型并未显示出性能提升。研究还表明，使用不同于泌尿学领域的医学领域数据在少样本提示下也能改善结果，这证明了LLMs处理肿瘤文档所需任务的能力。开源LLMs在自动化肿瘤文档方面显示出巨大潜力，参数在7-12亿之间的模型可能在性能和资源效率之间达到最佳平衡。通过有针对性的微调以及精心设计提示，这些模型有可能成为未来临床文档记录的重要工具。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）有潜力改善德国肿瘤文档的效率与可靠性。</li>
<li>在肿瘤诊断识别、ICD-10代码分配和首次诊断日期提取三个任务中，Llama 3.1 8B、Mistral 7B和Mistral NeMo 12B等模型表现良好。</li>
<li>模型表现与训练数据和参数数量有关，参数少于7亿的模型表现较差。</li>
<li>不同医学领域的数据在少样本提示下能提高模型表现，展示LLMs的适应性和潜力。</li>
<li>参数在7-12亿之间的模型可能在性能和资源效率上达到最佳平衡。</li>
<li>通过微调与精心设计提示，LLMs有潜力成为临床文档记录的重要工具。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12106">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-764cd5a8d839a45da36d211b0c673397.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5fbd2fce63b2cad7e3b967780aa11e83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b245a6522b0d161dce0f49b1bb1190c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="BiomedCoOp-Learning-to-Prompt-for-Biomedical-Vision-Language-Models"><a href="#BiomedCoOp-Learning-to-Prompt-for-Biomedical-Vision-Language-Models" class="headerlink" title="BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models"></a>BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models</h2><p><strong>Authors:Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao</strong></p>
<p>Recent advancements in vision-language models (VLMs), such as CLIP, have demonstrated substantial success in self-supervised representation learning for vision tasks. However, effectively adapting VLMs to downstream applications remains challenging, as their accuracy often depends on time-intensive and expertise-demanding prompt engineering, while full model fine-tuning is costly. This is particularly true for biomedical images, which, unlike natural images, typically suffer from limited annotated datasets, unintuitive image contrasts, and nuanced visual features. Recent prompt learning techniques, such as Context Optimization (CoOp) intend to tackle these issues, but still fall short in generalizability. Meanwhile, explorations in prompt learning for biomedical image analysis are still highly limited. In this work, we propose BiomedCoOp, a novel prompt learning framework that enables efficient adaptation of BiomedCLIP for accurate and highly generalizable few-shot biomedical image classification. Our approach achieves effective prompt context learning by leveraging semantic consistency with average prompt ensembles from Large Language Models (LLMs) and knowledge distillation with a statistics-based prompt selection strategy. We conducted comprehensive validation of our proposed framework on 11 medical datasets across 9 modalities and 10 organs against existing state-of-the-art methods, demonstrating significant improvements in both accuracy and generalizability. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/BiomedCoOp">https://github.com/HealthX-Lab/BiomedCoOp</a>. </p>
<blockquote>
<p>近期，视觉语言模型（VLMs），如CLIP，在视觉任务的自监督表示学习方面取得了显著的成功。然而，将VLMs有效地适应到下游应用仍然具有挑战性，因为它们的准确性通常依赖于耗时且需要专业知识的提示工程，而完整的模型微调成本高昂。特别是对于生物医学图像，与自然图像不同，它们通常受限于标注数据集、图像对比度不直观以及微妙的视觉特征。最近的提示学习技术，如上下文优化（CoOp），旨在解决这些问题，但在通用性方面仍有不足。同时，针对生物医学图像分析的提示学习探索仍然非常有限。在这项工作中，我们提出了BiomedCoOp，这是一种新型的提示学习框架，能够高效地适应BiomedCLIP，以实现准确且高度通用的生物医学图像分类的少量样本。我们的方法通过利用大型语言模型（LLMs）的平均提示集合的语义一致性和基于统计的提示选择策略的知识蒸馏，实现了有效的提示上下文学习。我们在9种模态、涉及10个器官的11个医学数据集上对所提出的框架进行了全面的验证，与现有的最先进的方法相比，在准确性和通用性方面都实现了显著的改进。代码公开可用在<a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/BiomedCoOp%E3%80%82">https://github.com/HealthX-Lab/BiomedCoOp。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15232v2">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了针对生物医学图像分类任务的新型提示学习框架BiomedCoOp。该框架结合了语义一致性、大型语言模型的平均提示集合以及基于统计的提示选择策略，实现了有效提示上下文学习。在多个医学数据集上的验证显示，BiomedCoOp在准确性和通用性方面都有显著提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期进展的视语言模型（VLMs）如CLIP在自监督表示学习方面取得了巨大成功，但在下游应用中的适应仍然具有挑战。</li>
<li>全模型微调成本高昂，而提示工程则需要时间和专业知识。</li>
<li>生物医学图像面临有限标注数据集、图像对比不明显和微妙视觉特征等问题。</li>
<li>Context Optimization（CoOp）等现有提示学习技术虽能解决部分问题，但在通用性方面仍有不足。</li>
<li>提出的BiomedCoOp框架结合了语义一致性、大型语言模型的平均提示集合和基于统计的提示选择策略，实现了高效的生物医学图像分类提示上下文学习。</li>
<li>在多个医学数据集上的验证显示，BiomedCoOp在准确性和通用性方面均表现出显著改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15232">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-38e06f0b12f79ae2df2bf415fffa84ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d742d08ca799cb2ef67626f8fec6be7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bec45fcdc9fbfcf81f31e3126e9fa5c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Self-Ensembling-Gaussian-Splatting-for-Few-Shot-Novel-View-Synthesis"><a href="#Self-Ensembling-Gaussian-Splatting-for-Few-Shot-Novel-View-Synthesis" class="headerlink" title="Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis"></a>Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis</h2><p><strong>Authors:Chen Zhao, Xuan Wang, Tong Zhang, Saqib Javed, Mathieu Salzmann</strong></p>
<p>3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in novel view synthesis (NVS). However, 3DGS tends to overfit when trained with sparse views, limiting its generalization to novel viewpoints. In this paper, we address this overfitting issue by introducing Self-Ensembling Gaussian Splatting (SE-GS). We achieve self-ensembling by incorporating an uncertainty-aware perturbation strategy during training. A $\mathbf{\Delta}$-model and a $\mathbf{\Sigma}$-model are jointly trained on the available images. The $\mathbf{\Delta}$-model is dynamically perturbed based on rendering uncertainty across training steps, generating diverse perturbed models with negligible computational overhead. Discrepancies between the $\mathbf{\Sigma}$-model and these perturbed models are minimized throughout training, forming a robust ensemble of 3DGS models. This ensemble, represented by the $\mathbf{\Sigma}$-model, is then used to generate novel-view images during inference. Experimental results on the LLFF, Mip-NeRF360, DTU, and MVImgNet datasets demonstrate that our approach enhances NVS quality under few-shot training conditions, outperforming existing state-of-the-art methods. The code is released at: <a target="_blank" rel="noopener" href="https://sailor-z.github.io/projects/SEGS.html">https://sailor-z.github.io/projects/SEGS.html</a>. </p>
<blockquote>
<p>三维高斯插值（3DGS）在新型视图合成（NVS）中显示出显著的效果。然而，当使用稀疏视图进行训练时，3DGS容易过度拟合，限制了其在新型观点上的泛化能力。在本文中，我们通过引入自集成高斯插值（SE-GS）来解决过度拟合问题。我们通过训练过程中融入一种感知不确定性的扰动策略来实现自集成。Δ模型和Σ模型在可用图像上联合训练。Δ模型根据训练步骤中的渲染不确定性进行动态扰动，生成多种扰动模型，计算开销微乎其微。在整个训练过程中，最小化Σ模型与这些扰动模型之间的差异，形成稳健的3DGS模型集合。这个集合由Σ模型表示，然后用于推理过程中的新型视图图像生成。在LLFF、Mip-NeRF360、DTU和MVImgNet数据集上的实验结果证明，我们的方法在提高少量训练情况下的NVS质量方面优于现有的最先进方法。代码已发布在：[<a target="_blank" rel="noopener" href="https://sailor-z.github.io/projects/SEGS.html%E3%80%82]">https://sailor-z.github.io/projects/SEGS.html。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00144v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于3D高斯点渲染技术（Gaussian Splatting）在合成新视角图像（NVS）时表现出显著效果，但在训练过程中遇到稀疏视角时的过拟合问题限制了其在新视角上的泛化能力。本文提出一种名为自集成高斯点渲染技术（Self-Ensembling Gaussian Splatting，简称SE-GS）的解决方案来解决这一问题。通过训练过程中引入不确定性感知扰动策略实现自集成，同时训练一个Δ模型和一个Σ模型，并利用渲染不确定性对Δ模型进行动态扰动，生成多个扰动模型以最小化两者之间的差异。在LLFF、Mip-NeRF360、DTU和MVImgNet数据集上的实验结果表明，该方法在训练样本较少的情况下提高了NVS的质量，优于现有最先进的算法。代码已发布在网址：<a target="_blank" rel="noopener" href="https://sailor-z.github.io/projects/SEGS.html">链接地址</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>引入自集成高斯点渲染技术解决了稀疏视角下的过拟合问题。</li>
<li>通过不确定性感知扰动策略实现自集成训练。</li>
<li>同时训练Δ模型和Σ模型，并利用渲染不确定性进行动态扰动。</li>
<li>通过最小化两个模型之间的差异，提高了新视角图像的合成质量。</li>
<li>在多个数据集上的实验结果表明该方法优于现有最先进的算法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.00144">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3254bfcfa9714ae7a1e529f2b3f884fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1e7fa4af9e4280dcb5de87a19497871.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f481c96c362e1caca155db837cdb972.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ce69bf882cc5d1744531b175d7dbdbe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a7e9e4c9c47a127c467d32726fc5c4e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f948a5edb48f80832743d4ba8d618228.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Probabilistic-Language-Image-Pre-Training"><a href="#Probabilistic-Language-Image-Pre-Training" class="headerlink" title="Probabilistic Language-Image Pre-Training"></a>Probabilistic Language-Image Pre-Training</h2><p><strong>Authors:Sanghyuk Chun, Wonjae Kim, Song Park, Sangdoo Yun</strong></p>
<p>Vision-language models (VLMs) embed aligned image-text pairs into a joint space but often rely on deterministic embeddings, assuming a one-to-one correspondence between images and texts. This oversimplifies real-world relationships, which are inherently many-to-many, with multiple captions describing a single image and vice versa. We introduce Probabilistic Language-Image Pre-training (ProLIP), the first probabilistic VLM pre-trained on a billion-scale image-text dataset using only probabilistic objectives, achieving a strong zero-shot capability (e.g., 74.6% ImageNet zero-shot accuracy with ViT-B&#x2F;16). ProLIP efficiently estimates uncertainty by an “uncertainty token” without extra parameters. We also introduce a novel inclusion loss that enforces distributional inclusion relationships between image-text pairs and between original and masked inputs. Experiments demonstrate that, by leveraging uncertainty estimates, ProLIP benefits downstream tasks and aligns with intuitive notions of uncertainty, e.g., shorter texts being more uncertain and more general inputs including specific ones. Utilizing text uncertainties, we further improve ImageNet accuracy from 74.6% to 75.8% (under a few-shot setting), supporting the practical advantages of our probabilistic approach. The code is available at <a target="_blank" rel="noopener" href="https://github.com/naver-ai/prolip">https://github.com/naver-ai/prolip</a> </p>
<blockquote>
<p>视觉语言模型（VLMs）将图像文本对嵌入到联合空间中，但通常依赖于确定性嵌入，假设图像和文本之间存在一对一的对应关系。这简化了真实世界中的关系，真实世界中的关系是固有的多对多关系，多个字幕描述单个图像，反之亦然。我们引入了概率语言图像预训练（ProLIP），这是第一个使用概率目标在百亿级图像文本数据集上进行预训练的概率VLM，实现了强大的零样本能力（例如，使用ViT-B&#x2F;1l达到ImageNet的零样本准确率是百分之七十四点六）。ProLIP通过一种不确定性令牌有效地估计不确定性，无需额外的参数。我们还引入了一种新型包容损失，该损失可以强制图像文本对之间以及原始输入和遮罩输入之间的分布包容关系。实验表明，通过利用不确定性估计，ProLIP有利于下游任务，并且与直观的不确定性概念相一致，例如文本越短则不确定性越高，涵盖具体信息的输入通常也更加通用。通过利用文本的不确定性，我们进一步提高了ImageNet的准确率，从百分之七十四点六提高到百分之七十五点八（在小样本设置下），证明了我们的概率方法具有实际优势。代码可用在<a target="_blank" rel="noopener" href="https://github.com/naver-ai/prolip%E3%80%82">https://github.com/naver-ai/prolip。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18857v3">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/naver-ai/prolip">https://github.com/naver-ai/prolip</a> HuggingFace Hub:   <a target="_blank" rel="noopener" href="https://huggingface.co/collections/SanghyukChun/prolip-6712595dfc87fd8597350291">https://huggingface.co/collections/SanghyukChun/prolip-6712595dfc87fd8597350291</a>   33 pages, 4.8 MB; LongProLIP paper: arXiv:2503.08048</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Probabilistic Language-Image Pre-training（ProLIP）模型，它是首个基于概率的视觉语言预训练模型。该模型在百亿级图像文本数据集上预训练，使用概率目标函数，实现了强大的零样本能力，如74.6%的ImageNet零样本准确率。ProLIP通过“不确定性令牌”有效地估计不确定性，并引入了一种新的包含损失，用于执行图像文本对之间的分布包含关系。通过利用不确定性估计，ProLIP有助于下游任务，并与不确定性直观认识相一致。利用文本不确定性，进一步将ImageNet准确率从74.6%提高到75.8%，验证了概率方法的实际优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ProLIP是首个在百亿级图像文本数据集上预训练的基于概率的视觉语言模型。</li>
<li>该模型使用概率目标函数，实现了强大的零样本能力，如ImageNet的74.6%零样本准确率。</li>
<li>ProLIP通过“不确定性令牌”估计不确定性，无需额外参数。</li>
<li>引入了一种新的包含损失，用于图像文本对和原始与遮挡输入之间的分布包含关系。</li>
<li>利用不确定性估计，ProLIP有助于提高下游任务的性能，并与不确定性的直观认识相一致。</li>
<li>通过利用文本不确定性，进一步提升了ImageNet的准确率。</li>
<li>ProLIP的代码已公开可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.18857">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c60fa82099b92cb0f49e0ce9914d0f1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5138f348f837858749275fdc2284b173.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-302a41c22cb6722ba1d22ab548259fcb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-683d735bf52c5e5373cc9576998bbe9b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Cross-Modal-Few-Shot-Learning-a-Generative-Transfer-Learning-Framework"><a href="#Cross-Modal-Few-Shot-Learning-a-Generative-Transfer-Learning-Framework" class="headerlink" title="Cross-Modal Few-Shot Learning: a Generative Transfer Learning Framework"></a>Cross-Modal Few-Shot Learning: a Generative Transfer Learning Framework</h2><p><strong>Authors:Zhengwei Yang, Yuke Li, Qiang Sun, Basura Fernando, Heng Huang, Zheng Wang</strong></p>
<p>Most existing studies on few-shot learning focus on unimodal settings, where models are trained to generalize to unseen data using a limited amount of labeled examples from a single modality. However, real-world data are inherently multi-modal, and such unimodal approaches limit the practical applications of few-shot learning. To bridge this gap, this paper introduces the Cross-modal Few-Shot Learning (CFSL) task, which aims to recognize instances across multiple modalities while relying on scarce labeled data. This task presents unique challenges compared to classical few-shot learning arising from the distinct visual attributes and structural disparities inherent to each modality. To tackle these challenges, we propose a Generative Transfer Learning (GTL) framework by simulating how humans abstract and generalize concepts. Specifically, the GTL jointly estimates the latent shared concept across modalities and the in-modality disturbance through a generative structure. Establishing the relationship between latent concepts and visual content among abundant unimodal data enables GTL to effectively transfer knowledge from unimodal to novel multimodal data, as humans did. Comprehensive experiments demonstrate that the GTL achieves state-of-the-art performance across seven multi-modal datasets across RGB-Sketch, RGB-Infrared, and RGB-Depth. </p>
<blockquote>
<p>当前关于少样本学习的研究大多集中在单模态设置上，即模型在有限的标签样本基础上训练，以推广到未见过的数据。然而，现实世界的数据本质上是多模态的，这种单模态的方法限制了少样本学习的实际应用。为了弥补这一差距，本文引入了跨模态少样本学习（CFSL）任务，该任务旨在利用稀缺的标签数据识别跨多个模态的实例。与经典少样本学习相比，此任务呈现出独特挑战，源于每个模态固有的不同视觉属性和结构差异。为了应对这些挑战，我们提出了生成转移学习（GTL）框架，模拟人类如何抽象和概括概念。具体来说，GTL通过一个生成结构来共同估计跨模态的潜在共享概念和模态内的干扰。在大量单模态数据中建立潜在概念与视觉内容之间的关系，使GTL能够像人类一样有效地将从单模态学到的知识转移到新的多模态数据。综合实验表明，GTL在RGB-Sketch、RGB-红外和RGB-深度等七个多模态数据集上取得了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10663v2">PDF</a> 15 pages, 9 figures, 7 tables</p>
<p><strong>Summary</strong><br>少量标注数据的跨模态识别是当前研究的空白。该研究引入了跨模态小样本学习（CFSL）任务，旨在解决在多种模态下识别实例的问题。研究提出了生成迁移学习（GTL）框架，通过模拟人类抽象和概括概念的方式，估计跨模态的潜在共享概念和模态内的干扰。该框架在七个多模态数据集上的表现达到了领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前关于小样本学习的研究主要集中在单模态设置上，限制了其在实际应用中的使用。</li>
<li>真实世界的数据是多模态的，因此需要跨模态小样本学习（CFSL）来识别多种模态下的实例。</li>
<li>CFSL任务面临独特的挑战，源于每个模态独特的视觉属性和结构差异。</li>
<li>为了应对这些挑战，提出了生成迁移学习（GTL）框架，模拟人类抽象和概括概念的方式。</li>
<li>GTL框架通过估计跨模态的潜在共享概念和模态内的干扰来建立关系。</li>
<li>GTL框架在多个多模态数据集上的表现超过了现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10663">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fd94004deb1b73d04d7940f0a13180c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-151494d07f63c80425a396162c8d7e5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-825c84662e7bed80bcf9a41ea3cb3965.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ad089e89695e1be7e94c2cfdcc47479.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d47a7b2c4f70e3f0b0d412ac8ad85cc.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Understand-Time-Series-Anomalies"><a href="#Can-LLMs-Understand-Time-Series-Anomalies" class="headerlink" title="Can LLMs Understand Time Series Anomalies?"></a>Can LLMs Understand Time Series Anomalies?</h2><p><strong>Authors:Zihao Zhou, Rose Yu</strong></p>
<p>Large Language Models (LLMs) have gained popularity in time series forecasting, but their potential for anomaly detection remains largely unexplored. Our study investigates whether LLMs can understand and detect anomalies in time series data, focusing on zero-shot and few-shot scenarios. Inspired by conjectures about LLMs’ behavior from time series forecasting research, we formulate key hypotheses about LLMs’ capabilities in time series anomaly detection. We design and conduct principled experiments to test each of these hypotheses. Our investigation reveals several surprising findings about LLMs for time series: (1) LLMs understand time series better as images rather than as text, (2) LLMs do not demonstrate enhanced performance when prompted to engage in explicit reasoning about time series analysis. (3) Contrary to common beliefs, LLMs’ understanding of time series does not stem from their repetition biases or arithmetic abilities. (4) LLMs’ behaviors and performance in time series analysis vary significantly across different models. This study provides the first comprehensive analysis of contemporary LLM capabilities in time series anomaly detection. Our results suggest that while LLMs can understand trivial time series anomalies, we have no evidence that they can understand more subtle real-world anomalies. Many common conjectures based on their reasoning capabilities do not hold. All synthetic dataset generators, final prompts, and evaluation scripts have been made available in <a target="_blank" rel="noopener" href="https://github.com/rose-stl-lab/anomllm">https://github.com/rose-stl-lab/anomllm</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在时间序列预测中受到欢迎，但它们在异常检测方面的潜力尚未得到充分探索。本研究旨在调查LLM是否能够理解和检测时间序列数据中的异常值，重点关注零样本和少样本场景。我们从时间序列预测研究中关于LLM行为的推测中获得灵感，针对LLM在时间序列异常检测方面的能力制定关键假设。我们设计并进行了有针对性的实验来测试这些假设。我们的研究揭示了LLM在时间序列方面的几个令人惊讶的发现：（1）LLM更善于将时间序列视为图像而不是文本来理解；（2）当被要求参与关于时间序列分析的显式推理时，LLM并没有表现出增强的性能。（3）与普遍信念相反，LLM对时间序列的理解并非源于其重复偏见或算术能力。（4）不同模型在时间序列分析中的行为和性能差异很大。本研究提供了对当代LLM在时间序列异常检测能力方面的首次综合分析。我们的结果表明，虽然LLM能够理解简单的时间序列异常，但没有证据表明它们能够理解更微妙的现实世界异常。基于其推理能力的许多常见推测并不成立。所有合成数据集生成器、最终提示和评估脚本均已在<a target="_blank" rel="noopener" href="https://github.com/rose-stl-lab/anomllm">https://github.com/rose-stl-lab/anomllm</a>上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05440v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了大型语言模型（LLMs）在时间序列异常检测方面的能力。实验发现LLMs对时间序列的理解更偏向于图像而非文本，且没有展现出通过明确推理进行时间序列分析的增强性能。此外，LLMs的理解并非源于其重复偏见或算术能力，且不同模型在时间序列分析中的行为和性能存在显著差异。虽然LLMs能够理解简单的时间序列异常，但对于更微妙的现实世界异常尚无证据表明其能够理解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs对时间序列的理解更偏向于图像形式。</li>
<li>LLMs在明确推理方面并未展现出优势。</li>
<li>LLMs理解时间序列并非基于重复偏见或算术能力。</li>
<li>不同LLMs在时间序列分析中的行为和性能存在差异。</li>
<li>LLMs能够识别简单的时间序列异常。</li>
<li>对于更微妙的现实世界异常，LLMs的理解能力尚无证据支持。</li>
<li>相关数据和研究成果已公开在GitHub上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05440">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4409af2a116ae84d5d5215c024a1b769.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2e7428c22f7adbba05d6a1f2526d395.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="P3P-Pseudo-3D-Pre-training-for-Scaling-3D-Masked-Autoencoders"><a href="#P3P-Pseudo-3D-Pre-training-for-Scaling-3D-Masked-Autoencoders" class="headerlink" title="P3P: Pseudo-3D Pre-training for Scaling 3D Masked Autoencoders"></a>P3P: Pseudo-3D Pre-training for Scaling 3D Masked Autoencoders</h2><p><strong>Authors:Xuechao Chen, Ying Chen, Jialin Li, Qiang Nie, Hanqiu Deng, Yong Liu, Qixing Huang, Yang Li</strong></p>
<p>Pre-training in 3D is pivotal for advancing 3D perception tasks. However, the scarcity of clean 3D data poses significant challenges for scaling 3D pre-training efforts. Drawing inspiration from semi-supervised learning, which effectively combines limited labeled data with abundant unlabeled data, we introduce an innovative self-supervised pre-training framework. This framework leverages both authentic 3D data and pseudo-3D data generated from images using a robust depth estimation model. Another critical challenge is the efficiency of the pre-training process. Existing approaches, such as Point-BERT and Point-MAE, utilize k-nearest neighbors for 3D token embedding, resulting in quadratic time complexity. To address this, we propose a novel token embedding strategy with linear time complexity, coupled with a training-efficient 2D reconstruction target. Our method not only achieves state-of-the-art performance in 3D classification, detection, and few-shot learning but also ensures high efficiency in both pre-training and downstream fine-tuning processes. </p>
<blockquote>
<p>3D预训练在推进3D感知任务中起着至关重要的作用。然而，干净3D数据的稀缺给大规模3D预训练工作带来了巨大挑战。我们从半监督学习中得到启发，半监督学习有效地结合了有限的有标签数据和大量的无标签数据，因此我们引入了一种创新的自监督预训练框架。该框架不仅利用真实的3D数据，还利用从图像中使用稳健的深度估计模型生成的伪3D数据。另一个关键挑战是预训练过程的效率。现有的方法，如Point-BERT和Point-MAE，使用k-最近邻进行3D令牌嵌入，导致时间复杂度为二次方。为了解决这个问题，我们提出了一种具有线性时间复杂度的新型令牌嵌入策略，并结合了高效的2D重建目标进行训练。我们的方法不仅在3D分类、检测和少样本学习上达到了最新性能水平，而且还确保了预训练和下游微调过程的高效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10007v2">PDF</a> Under review. Pre-print</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对三维感知任务预训练的挑战及应对策略。利用半监督学习的灵感，通过结合真实的三维数据和图像生成的三维数据进行自我监督预训练。为解决预训练过程效率问题，提出了线性时间复杂度的全新token嵌入策略及高效的二维重建目标，提升了三维分类、检测和少样本学习的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了预训练在三维感知任务中的重要性以及缺乏清洁三维数据带来的挑战。</li>
<li>借助半监督学习的理念，将有限的标注数据与大量的无标注数据结合，提高了模型的性能。</li>
<li>创新性地利用真实三维数据和图像生成的三维数据进行自我监督预训练。</li>
<li>现有的预训练方法如Point-BERT和Point-MAE使用k最近邻进行三维token嵌入，导致二次时间复杂度较高。</li>
<li>针对高时间复杂度问题，提出了具有线性时间复杂度的全新token嵌入策略。</li>
<li>结合高效的二维重建目标，提升了预训练和下游微调过程的效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10007">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f99ffe5a9e6db230decb0483b6be34fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-721f466d13daa84fb953a2d423221b3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9ea6215f920837c1ce89251fbcc0ea1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8879d6207d6265f3665c648ef133956b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CoMapGS-Covisibility-Map-based-Gaussian-Splatting-for-Sparse-Novel-View-Synthesis"><a href="#CoMapGS-Covisibility-Map-based-Gaussian-Splatting-for-Sparse-Novel-View-Synthesis" class="headerlink" title="CoMapGS: Covisibility Map-based Gaussian Splatting for Sparse Novel View   Synthesis"></a>CoMapGS: Covisibility Map-based Gaussian Splatting for Sparse Novel View   Synthesis</h2><p><strong>Authors:Youngkyoon Jang, Eduardo Pérez-Pellitero</strong></p>
<p>We propose Covisibility Map-based Gaussian Splatting (CoMapGS), designed to recover underrepresented sparse regions in sparse novel view synthesis. CoMapGS addresses both high- and low-uncertainty regions by constructing covisibility maps, enhancing initial point clouds, and applying uncertainty-aware weighted supervision with a proximity classifier. Our contributions are threefold: (1) CoMapGS reframes novel view synthesis by leveraging covisibility maps as a core component to address region-specific uncertainty levels; (2) Enhanced initial point clouds for both low- and high-uncertainty regions compensate for sparse COLMAP-derived point clouds, improving reconstruction quality and benefiting few-shot 3DGS methods; (3) Adaptive supervision with covisibility-score-based weighting and proximity classification achieves consistent performance gains across scenes with various sparsity scores derived from covisibility maps. Experimental results demonstrate that CoMapGS outperforms state-of-the-art methods on datasets including Mip-NeRF 360 and LLFF. </p>
<blockquote>
<p>我们提出了基于可见性映射的高斯点扩散技术（简称CoMapGS），该技术旨在解决新型稀疏视图合成中未被充分代表的稀疏区域恢复问题。CoMapGS通过构建可见性映射、增强初始点云以及采用基于接近度的分类器进行不确定性加权监督来解决高不确定性和低不确定性区域问题。我们的贡献主要有三点：（1）CoMapGS通过利用可见性映射作为核心组件来解决特定区域的不确定性水平问题，从而重新定义了新型视图合成；（2）增强初始点云对于低不确定性和高不确定性区域，可以补偿稀疏的COLMAP衍生点云，从而提高重建质量并有利于少样本的3DGS方法；（3）基于可见性评分加权的自适应监督与接近度分类技术，在不同的场景中均实现了性能的提升，这些场景由可见性映射得出的稀疏度得分各不相同。实验结果表明，在包括Mip-NeRF 360和LLFF在内的数据集上，CoMapGS优于最新技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.11057v3">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>基于可见性映射的协方差分割（CoMapGS）旨在恢复稀疏新颖视图合成中的欠代表稀疏区域。CoMapGS通过构建协可见性映射、增强初始点云和应用不确定性感知加权监督与接近分类器，解决高不确定性和低不确定性区域的问题。其贡献包括：利用协可见性映射作为核心组件解决区域特定不确定性水平的问题；增强初始点云以提高重建质量和有利于少数拍摄点的三维几何扫描方法；自适应监督通过基于协可见性得分的加权和接近分类实现跨场景的持续性能提升。实验结果表明，CoMapGS在Mip-NeRF 360和LLFF数据集上的表现优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CoMapGS被设计为恢复稀疏新颖视图合成中的欠代表稀疏区域。</li>
<li>通过构建协可见性映射，解决高不确定性和低不确定性区域的问题。</li>
<li>利用协可见性映射作为核心组件，针对特定区域的不确定性进行处理。</li>
<li>增强初始点云以提高重建质量，并优化少数拍摄点的三维几何扫描方法。</li>
<li>通过不确定性感知加权监督与接近分类器实现自适应监督。</li>
<li>基于协可见性得分的加权和接近分类的方法在所有场景中实现性能提升。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.11057">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7de16bc229b040217a920853b43cbfe4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d567642e9ee155e19b2a3f7b426337c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55f7ddd5aa3c7a224c093bb40b7396ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2cc4c25d3a32cf797b0c3d095a83fe6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24098115e478797ad4abf4d2af80d418.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e9c42602c21b81f4a9b9c1e75bf0491a.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-03-14  Electromyography-Informed Facial Expression Reconstruction for   Physiological-Based Synthesis and Analysis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9a114a08cd327cd33dbc03616a84b252.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-03-14  Towards Zero-Shot Multimodal Machine Translation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18799.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
