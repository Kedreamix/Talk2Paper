<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Fair Federated Medical Image Classification Against Quality Shift via   Inter-Client Progressive State Matching">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-6089824ad79aeedf6a4b9311eecffd0e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-14-æ›´æ–°"><a href="#2025-03-14-æ›´æ–°" class="headerlink" title="2025-03-14 æ›´æ–°"></a>2025-03-14 æ›´æ–°</h1><h2 id="Fair-Federated-Medical-Image-Classification-Against-Quality-Shift-via-Inter-Client-Progressive-State-Matching"><a href="#Fair-Federated-Medical-Image-Classification-Against-Quality-Shift-via-Inter-Client-Progressive-State-Matching" class="headerlink" title="Fair Federated Medical Image Classification Against Quality Shift via   Inter-Client Progressive State Matching"></a>Fair Federated Medical Image Classification Against Quality Shift via   Inter-Client Progressive State Matching</h2><p><strong>Authors:Nannan Wu, Zhuo Kuang, Zengqiang Yan, Ping Wang, Li Yu</strong></p>
<p>Despite the potential of federated learning in medical applications, inconsistent imaging quality across institutions-stemming from lower-quality data from a minority of clients-biases federated models toward more common high-quality images. This raises significant fairness concerns. Existing fair federated learning methods have demonstrated some effectiveness in solving this problem by aligning a single 0th- or 1st-order state of convergence (e.g., training loss or sharpness). However, we argue in this work that fairness based on such a single state is still not an adequate surrogate for fairness during testing, as these single metrics fail to fully capture the convergence characteristics, making them suboptimal for guiding fair learning. To address this limitation, we develop a generalized framework. Specifically, we propose assessing convergence using multiple states, defined as sharpness or perturbed loss computed at varying search distances. Building on this comprehensive assessment, we propose promoting fairness for these states across clients to achieve our ultimate fairness objective. This is accomplished through the proposed method, FedISM+. In FedISM+, the search distance evolves over time, progressively focusing on different states. We then incorporate two components in local training and global aggregation to ensure cross-client fairness for each state. This gradually makes convergence equitable for all states, thereby improving fairness during testing. Our empirical evaluations, performed on the well-known RSNA ICH and ISIC 2019 datasets, demonstrate the superiority of FedISM+ over existing state-of-the-art methods for fair federated learning. The code is available at <a target="_blank" rel="noopener" href="https://github.com/wnn2000/FFL4MIA">https://github.com/wnn2000/FFL4MIA</a>. </p>
<blockquote>
<p>å°½ç®¡è”é‚¦å­¦ä¹ åœ¨åŒ»ç–—åº”ç”¨ä¸­æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†æœºæ„é—´æˆåƒè´¨é‡çš„ä¸ä¸€è‡´æ€§â€”â€”æºäºå°‘æ•°å®¢æˆ·è¾ƒä½è´¨é‡çš„æ•°æ®â€”â€”ä½¿è”é‚¦æ¨¡å‹åå‘äºæ›´å¸¸è§çš„é«˜è´¨é‡å›¾åƒã€‚è¿™å¼•å‘äº†å…³äºå…¬å¹³æ€§çš„é‡å¤§æ‹…å¿§ã€‚ç°æœ‰çš„å…¬å¹³è”é‚¦å­¦ä¹ æ–¹æ³•å·²ç»é€šè¿‡è°ƒæ•´æ”¶æ•›çš„å•ä¸€çŠ¶æ€ï¼ˆå¦‚è®­ç»ƒæŸå¤±æˆ–æ¸…æ™°åº¦ï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ˜¾ç¤ºå‡ºä¸€å®šçš„æ•ˆæœã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­è®¤ä¸ºï¼ŒåŸºäºå•ä¸€çŠ¶æ€çš„å…¬å¹³å¹¶ä¸æ˜¯æµ‹è¯•æœŸé—´å…¬å¹³æ€§çš„å……åˆ†æ›¿ä»£ã€‚è¿™äº›å•ä¸€æŒ‡æ ‡æœªèƒ½å……åˆ†æ•æ‰æ”¶æ•›ç‰¹æ€§ï¼Œå¯¹äºæŒ‡å¯¼å…¬å¹³å­¦ä¹ æ¥è¯´æ•ˆæœå¹¶ä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªé€šç”¨æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡å¤šä¸ªçŠ¶æ€æ¥è¯„ä¼°æ”¶æ•›æ€§ï¼Œå®šä¹‰ä¸ºåœ¨ä¸åŒæœç´¢è·ç¦»ä¸‹è®¡ç®—çš„æ¸…æ™°åº¦æˆ–æ‰°åŠ¨æŸå¤±ã€‚åŸºäºè¿™ä¸€å…¨é¢çš„è¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºäº†åœ¨å®¢æˆ·ç«¯å¯¹è¿™äº›çŠ¶æ€å®ç°å…¬å¹³çš„ä¿ƒè¿›ç­–ç•¥ï¼Œä»¥å®ç°æˆ‘ä»¬çš„æœ€ç»ˆå…¬å¹³ç›®æ ‡ã€‚è¿™æ˜¯é€šè¿‡æå‡ºçš„æ–¹æ³•FedISM+æ¥å®ç°çš„ã€‚åœ¨FedISM+ä¸­ï¼Œæœç´¢è·ç¦»éšæ—¶é—´å˜åŒ–ï¼Œé€æ¸å…³æ³¨ä¸åŒçš„çŠ¶æ€ã€‚ç„¶åæˆ‘ä»¬åœ¨æœ¬åœ°è®­ç»ƒå’Œå…¨å±€èšåˆä¸­èå…¥ä¸¤ä¸ªç»„ä»¶ï¼Œä»¥ç¡®ä¿æ¯ä¸ªçŠ¶æ€çš„è·¨å®¢æˆ·ç«¯å…¬å¹³æ€§ã€‚è¿™å°†é€æ­¥ä½¿æ‰€æœ‰çŠ¶æ€çš„æ”¶æ•›æ›´åŠ å…¬å¹³ï¼Œä»è€Œæé«˜æµ‹è¯•æœŸé—´çš„å…¬å¹³æ€§ã€‚æˆ‘ä»¬åœ¨è‘—åçš„RSNA ICHå’ŒISIC 2019æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒFedISM+ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„å…¬å¹³è”é‚¦å­¦ä¹ æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wnn2000/FFL4MIA%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/wnn2000/FFL4MIAè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09587v1">PDF</a> Preprint</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å°½ç®¡è”é‚¦å­¦ä¹ åœ¨åŒ»ç–—åº”ç”¨ä¸­æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†æœºæ„é—´æˆåƒè´¨é‡çš„ä¸ä¸€è‡´æ€§â€”â€”æºäºéƒ¨åˆ†å®¢æˆ·è¾ƒä½è´¨é‡çš„æ•°æ®â€”â€”ä½¿å¾—è”é‚¦æ¨¡å‹åå‘æ›´å¸¸è§çš„é«˜è´¨é‡å›¾åƒï¼Œä»è€Œå¼•å‘å…¬å¹³æ€§æ‹…å¿§ã€‚ç°æœ‰å…¬å¹³è”é‚¦å­¦ä¹ æ–¹æ³•é€šè¿‡ç»Ÿä¸€æ”¶æ•›çŠ¶æ€ï¼ˆå¦‚è®­ç»ƒæŸå¤±æˆ–æ¸…æ™°åº¦ï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†æœ¬æ–‡è®¤ä¸ºåŸºäºå•ä¸€çŠ¶æ€çš„å…¬å¹³æ€§è¯„ä»·ä¸è¶³ä»¥åœ¨æµ‹è¯•é˜¶æ®µä½“ç°å…¬å¹³æ€§ã€‚å› ä¸ºè¿™äº›å•ä¸€æŒ‡æ ‡æ— æ³•å®Œå…¨æ•æ‰æ”¶æ•›ç‰¹æ€§ï¼Œå› æ­¤å®ƒä»¬å¯¹äºæŒ‡å¯¼å…¬å¹³å­¦ä¹ çš„æ•ˆæœå¹¶ä¸ç†æƒ³ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œæå‡ºé€šè¿‡å¤šä¸ªçŠ¶æ€æ¥è¯„ä¼°æ”¶æ•›æ€§ï¼Œå®šä¹‰ä¸ºä¸åŒæœç´¢è·ç¦»è®¡ç®—çš„æ¸…æ™°åº¦æˆ–æ‰°åŠ¨æŸå¤±ã€‚åŸºäºè¿™ä¸€å…¨é¢çš„è¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºé’ˆå¯¹è¿™äº›çŠ¶æ€çš„å…¬å¹³æ€§ä¿ƒè¿›æ–¹æ³•ï¼Œä»¥å®ç°æˆ‘ä»¬çš„æœ€ç»ˆå…¬å¹³ç›®æ ‡ã€‚é€šè¿‡æ–°æ–¹æ³•FedISM+ï¼Œæœç´¢è·ç¦»éšæ—¶é—´æ¼”å˜ï¼Œé€æ¸å…³æ³¨ä¸åŒçš„çŠ¶æ€ã€‚ç„¶åæˆ‘ä»¬åœ¨æœ¬åœ°è®­ç»ƒå’Œå…¨å±€èšåˆä¸­èå…¥ä¸¤ä¸ªç»„ä»¶ï¼Œä»¥ç¡®ä¿æ¯ä¸ªçŠ¶æ€çš„è·¨å®¢æˆ·ç«¯å…¬å¹³æ€§ã€‚è¿™é€æ¸ä½¿æ‰€æœ‰çŠ¶æ€çš„æ”¶æ•›è¶‹äºå…¬å¹³ï¼Œä»è€Œæé«˜æµ‹è¯•é˜¶æ®µçš„å…¬å¹³æ€§ã€‚åœ¨è‘—åçš„RSNA ICHå’ŒISIC 2019æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒFedISM+ä¼˜äºç°æœ‰çš„å…ˆè¿›å…¬å¹³è”é‚¦å­¦ä¹ æ–¹æ³•ã€‚ç›¸å…³ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘ç«™æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/wnn2000/FFL4MIA">https://github.com/wnn2000/FFL4MIA</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ åœ¨åŒ»ç–—åº”ç”¨ä¸­é¢ä¸´å› æœºæ„é—´æˆåƒè´¨é‡ä¸ä¸€è‡´å¯¼è‡´çš„å…¬å¹³æ€§æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å…¬å¹³è”é‚¦å­¦ä¹ æ–¹æ³•ä¸»è¦é€šè¿‡å•ä¸€æ”¶æ•›çŠ¶æ€è¯„ä»·å…¬å¹³æ€§ï¼Œä½†åœ¨æµ‹è¯•é˜¶æ®µè¿™æ˜¾å¾—ä¸è¶³ã€‚</li>
<li>æå‡ºä¸€ç§é€šç”¨æ¡†æ¶ï¼Œé€šè¿‡å¤šä¸ªçŠ¶æ€è¯„ä¼°æ”¶æ•›æ€§ï¼Œä»¥æ›´å…¨é¢æ•æ‰æ”¶æ•›ç‰¹æ€§ã€‚</li>
<li>å¼•å…¥FedISM+æ–¹æ³•ï¼Œå…¶ä¸­æœç´¢è·ç¦»éšæ—¶é—´æ¼”å˜ï¼Œå¹¶åœ¨æœ¬åœ°è®­ç»ƒå’Œå…¨å±€èšåˆä¸­èå…¥ä¸¤ä¸ªç»„ä»¶æ¥ç¡®ä¿è·¨å®¢æˆ·ç«¯å…¬å¹³æ€§ã€‚</li>
<li>FedISM+æ–¹æ³•å¯ä»¥æ”¹å–„æµ‹è¯•é˜¶æ®µçš„å…¬å¹³æ€§ï¼Œå¹¶åœ¨RSNA ICHå’ŒISIC 2019æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>FedISM+çš„ä»£ç å…¬å¼€å¯ç”¨ï¼Œç½‘å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/wnn2000/FFL4MIA">https://github.com/wnn2000/FFL4MIA</a>ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå…¬å¹³è”é‚¦å­¦ä¹ æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œæœ‰æœ›æ”¹å–„åŒ»ç–—åº”ç”¨ä¸­æ¨¡å‹å…¬å¹³æ€§é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-63cab4bd91b9ae77f23faaa6ec414e47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e111bf5b767cce2d19694a80242bb7de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b32e1d029b1e176f4cc856e85c34d618.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9381a439136ec38dcd3e10ed84dd9cc2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="The-R2D2-Deep-Neural-Network-Series-for-Scalable-Non-Cartesian-Magnetic-Resonance-Imaging"><a href="#The-R2D2-Deep-Neural-Network-Series-for-Scalable-Non-Cartesian-Magnetic-Resonance-Imaging" class="headerlink" title="The R2D2 Deep Neural Network Series for Scalable Non-Cartesian Magnetic   Resonance Imaging"></a>The R2D2 Deep Neural Network Series for Scalable Non-Cartesian Magnetic   Resonance Imaging</h2><p><strong>Authors:Yiwei Chen, Amir Aghabiglou, Shijie Chen, Motahare Torki, Chao Tang, Ruud B. van Heeswijk, Yves Wiaux</strong></p>
<p>We introduce the R2D2 Deep Neural Network (DNN) series paradigm for fast and scalable image reconstruction from highly-accelerated non-Cartesian k-space acquisitions in Magnetic Resonance Imaging (MRI). While unrolled DNN architectures provide a robust image formation approach via data-consistency layers, embedding non-uniform fast Fourier transform operators in a DNN can become impractical to train at large scale, e.g in 2D MRI with a large number of coils, or for higher-dimensional imaging. Plug-and-play approaches that alternate a learned denoiser blind to the measurement setting with a data-consistency step are not affected by this limitation but their highly iterative nature implies slow reconstruction. To address this scalability challenge, we leverage the R2D2 paradigm that was recently introduced to enable ultra-fast reconstruction for large-scale Fourier imaging in radio astronomy. R2D2â€™s reconstruction is formed as a series of residual images iteratively estimated as outputs of DNN modules taking the previous iterationâ€™s data residual as input. The method can be interpreted as a learned version of the Matching Pursuit algorithm. A series of R2D2 DNN modules were sequentially trained in a supervised manner on the fastMRI dataset and validated for 2D multi-coil MRI in simulation and on real data, targeting highly under-sampled radial k-space sampling. Results suggest that a series with only few DNNs achieves superior reconstruction quality over its unrolled incarnation R2D2-Net (whose training is also much less scalable), and over the state-of-the-art diffusion-based â€œDecomposed Diffusion Samplerâ€ approach (also characterised by a slower reconstruction process). </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†R2D2æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ç³»åˆ—èŒƒå¼ï¼Œè¯¥èŒƒå¼å¯ç”¨äºä»ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­çš„é«˜åº¦åŠ é€Ÿçš„éç¬›å¡å°”kç©ºé—´é‡‡é›†å¿«é€Ÿä¸”å¯æ‰©å±•çš„å›¾åƒé‡å»ºã€‚è™½ç„¶å±•å¼€çš„DNNæ¶æ„é€šè¿‡æ•°æ®ä¸€è‡´æ€§å±‚æä¾›äº†ç¨³å¥çš„å›¾åƒå½¢æˆæ–¹æ³•ï¼Œä½†åœ¨å¤§è§„æ¨¡åµŒå…¥éå‡åŒ€å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ç®—å­æ—¶å¯èƒ½å˜å¾—ä¸åˆ‡å®é™…ï¼Œä¾‹å¦‚åœ¨å…·æœ‰å¤§é‡çº¿åœˆçš„2DMRIä¸­ï¼Œæˆ–åœ¨æ›´é«˜ç»´åº¦çš„æˆåƒä¸­ã€‚äº¤æ›¿ä½¿ç”¨å¯¹æµ‹é‡è®¾ç½®ç›²ç‚¹çš„å¯å­¦ä¹ å»å™ªå™¨å’Œæ•°æ®ä¸€è‡´æ€§æ­¥éª¤çš„å³æ’å³ç”¨æ–¹æ³•ä¸ä¼šå—åˆ°æ­¤é™åˆ¶ï¼Œä½†å®ƒä»¬çš„é«˜åº¦è¿­ä»£æ€§è´¨æ„å‘³ç€é‡å»ºé€Ÿåº¦è¾ƒæ…¢ã€‚ä¸ºäº†åº”å¯¹å¯æ‰©å±•æ€§æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æœ€è¿‘å¼•å…¥çš„R2D2èŒƒå¼ï¼Œè¯¥èŒƒå¼å¯å®ç°å°„ç”µå¤©æ–‡å­¦ä¸­å¤§è§„æ¨¡å‚…é‡Œå¶æˆåƒçš„è¶…å¿«é€Ÿé‡å»ºã€‚R2D2çš„é‡å»ºæ˜¯ç”±ä¸€ç³»åˆ—æ®‹å·®å›¾åƒç»„æˆï¼Œè¿™äº›æ®‹å·®å›¾åƒæ˜¯DNNæ¨¡å—çš„è¿­ä»£ä¼°è®¡è¾“å‡ºï¼Œä»¥ä¹‹å‰çš„è¿­ä»£æ•°æ®æ®‹å·®ä½œä¸ºè¾“å…¥ã€‚è¯¥æ–¹æ³•å¯è§£é‡Šä¸ºåŒ¹é…è¿½è¸ªç®—æ³•çš„å­¦ä¹ ç‰ˆæœ¬ã€‚ä¸€ç³»åˆ—R2D2 DNNæ¨¡å—åœ¨fastMRIæ•°æ®é›†ä¸Šä»¥ç›‘ç£æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®å¯¹2Då¤šçº¿åœˆMRIè¿›è¡Œäº†éªŒè¯ï¼Œé’ˆå¯¹é«˜åº¦æ¬ é‡‡æ ·çš„å¾„å‘kç©ºé—´é‡‡æ ·ã€‚ç»“æœè¡¨æ˜ï¼Œåªæœ‰å°‘æ•°DNNçš„ç³»åˆ—å®ç°äº†ä¼˜äºå…¶å±•å¼€å½¢å¼R2D2-Netï¼ˆå…¶è®­ç»ƒä¹Ÿä¸å¤ªå¯æ‰©å±•ï¼‰ä»¥åŠä¼˜äºæœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„â€œåˆ†è§£æ‰©æ•£é‡‡æ ·å™¨â€æ–¹æ³•çš„é‡å»ºè´¨é‡ï¼ˆå…¶é‡å»ºè¿‡ç¨‹ä¹Ÿè¾ƒæ…¢ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09559v1">PDF</a> 13 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºR2D2æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ç³»åˆ—çš„èŒƒå¼ï¼Œç”¨äºä»é«˜åº¦åŠ é€Ÿçš„éç¬›å¡å°”kç©ºé—´é‡‡é›†ä¸­è¿›è¡Œå¿«é€Ÿä¸”å¯ä¼¸ç¼©çš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å›¾åƒé‡å»ºã€‚è¯¥èŒƒå¼é€šè¿‡æ•°æ®ä¸€è‡´æ€§å±‚æä¾›ç¨³å¥çš„å›¾åƒå½¢æˆæ–¹æ³•ï¼Œè§£å†³äº†åœ¨å¤§å‹è§„æ¨¡ï¼Œå¦‚åœ¨å…·æœ‰å¤šä¸ªçº¿åœˆçš„2D MRIæˆ–æ›´é«˜ç»´åº¦çš„æˆåƒä¸­ï¼Œå°†éå‡åŒ€å¿«é€Ÿå‚…ç«‹å¶å˜æ¢ç®—å­åµŒå…¥DNNä¸­è¿›è¡Œè®­ç»ƒçš„ä¸åˆ‡å®é™…çš„é—®é¢˜ã€‚R2D2èŒƒå¼è¢«å¼•å…¥ä»¥è§£å†³è¿™ä¸€å¯æ‰©å±•æ€§æŒ‘æˆ˜ï¼Œå®ƒå¯å¿«é€Ÿé‡å»ºå¤§è§„æ¨¡å‚…ç«‹å¶æˆåƒï¼Œå¯è§£é‡Šä¸ºåŒ¹é…è¿½è¸ªç®—æ³•çš„å­¦æˆç‰ˆæœ¬ã€‚åœ¨fastMRIæ•°æ®é›†ä¸Šï¼Œä¸€ç³»åˆ—R2D2 DNNæ¨¡å—é€šè¿‡ç›‘ç£æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®ä¸Šå¯¹2Då¤šçº¿åœˆMRIè¿›è¡ŒéªŒè¯ï¼Œç›®æ ‡ä¸ºé«˜åº¦æ¬ é‡‡æ ·çš„å¾„å‘kç©ºé—´é‡‡æ ·ã€‚ç»“æœè¡¨æ˜ï¼Œä»…åŒ…å«å°‘æ•°DNNçš„ç³»åˆ—å®ç°äº†ä¼˜äºR2D2-Netï¼ˆå…¶è®­ç»ƒä¹Ÿä¸å¯ä¼¸ç¼©ï¼‰å’Œä¼˜äºå½“å‰æŠ€æœ¯æ°´å¹³çš„åˆ†è§£æ‰©æ•£é‡‡æ ·å™¨æ–¹æ³•çš„é‡å»ºè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥R2D2 Deep Neural Network (DNN)ç³»åˆ—èŒƒå¼ï¼Œç”¨äºå¿«é€Ÿä¸”å¯ä¼¸ç¼©åœ°ä»éç¬›å¡å°”kç©ºé—´é‡‡é›†ä¸­è¿›è¡Œç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å›¾åƒé‡å»ºã€‚</li>
<li>R2D2èŒƒå¼è§£å†³äº†åœ¨å¤§å‹è§„æ¨¡ä¸‹ï¼Œå°†éå‡åŒ€å¿«é€Ÿå‚…ç«‹å¶å˜æ¢ç®—å­åµŒå…¥DNNè¿›è¡Œè®­ç»ƒçš„ä¸åˆ‡å®é™…çš„é—®é¢˜ã€‚</li>
<li>R2D2èŒƒå¼å¯è§£é‡Šä¸ºåŒ¹é…è¿½è¸ªç®—æ³•çš„å­¦æˆç‰ˆæœ¬ã€‚</li>
<li>R2D2 DNNæ¨¡å—é€šè¿‡ç›‘ç£æ–¹å¼åœ¨fastMRIæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>R2D2ç³»åˆ—åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®çš„2Då¤šçº¿åœˆMRIéªŒè¯ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>ä¸R2D2-Netå’Œå½“å‰æŠ€æœ¯æ°´å¹³çš„åˆ†è§£æ‰©æ•£é‡‡æ ·å™¨æ–¹æ³•ç›¸æ¯”ï¼ŒR2D2ç³»åˆ—å®ç°æ›´é«˜çš„é‡å»ºè´¨é‡ã€‚</li>
<li>R2D2ç³»åˆ—å…·æœ‰æ›´å¿«çš„é‡å»ºè¿‡ç¨‹å’Œæ›´é«˜çš„å¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09559">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a03bdb9db0a044e9b05a236459fa3d64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a524737d211f3be291033c04129e9aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cbd605fb9510cbe541ee99174935e3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14662143e51ba085df24c074e745b545.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Evaluating-Visual-Explanations-of-Attention-Maps-for-Transformer-based-Medical-Imaging"><a href="#Evaluating-Visual-Explanations-of-Attention-Maps-for-Transformer-based-Medical-Imaging" class="headerlink" title="Evaluating Visual Explanations of Attention Maps for Transformer-based   Medical Imaging"></a>Evaluating Visual Explanations of Attention Maps for Transformer-based   Medical Imaging</h2><p><strong>Authors:Minjae Chung, Jong Bum Won, Ganghyun Kim, Yujin Kim, Utku Ozbulak</strong></p>
<p>Although Vision Transformers (ViTs) have recently demonstrated superior performance in medical imaging problems, they face explainability issues similar to previous architectures such as convolutional neural networks. Recent research efforts suggest that attention maps, which are part of decision-making process of ViTs can potentially address the explainability issue by identifying regions influencing predictions, especially in models pretrained with self-supervised learning. In this work, we compare the visual explanations of attention maps to other commonly used methods for medical imaging problems. To do so, we employ four distinct medical imaging datasets that involve the identification of (1) colonic polyps, (2) breast tumors, (3) esophageal inflammation, and (4) bone fractures and hardware implants. Through large-scale experiments on the aforementioned datasets using various supervised and self-supervised pretrained ViTs, we find that although attention maps show promise under certain conditions and generally surpass GradCAM in explainability, they are outperformed by transformer-specific interpretability methods. Our findings indicate that the efficacy of attention maps as a method of interpretability is context-dependent and may be limited as they do not consistently provide the comprehensive insights required for robust medical decision-making. </p>
<blockquote>
<p>å°½ç®¡è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰åœ¨åŒ»å­¦æˆåƒé—®é¢˜ä¸Šè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬é¢ä¸´ç€ä¸ä¹‹å‰çš„æ¶æ„ï¼ˆå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼‰ç±»ä¼¼çš„è§£é‡Šæ€§é—®é¢˜ã€‚æœ€è¿‘çš„ç ”ç©¶åŠªåŠ›è¡¨æ˜ï¼Œè§†è§‰è½¬æ¢å™¨çš„å†³ç­–è¿‡ç¨‹ä¸­çš„æ³¨æ„åŠ›å›¾æœ‰å¯èƒ½é€šè¿‡è¯†åˆ«å½±å“é¢„æµ‹çš„åŒºåŸŸæ¥è§£å†³è§£é‡Šæ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ è¿›è¡Œé¢„è®­ç»ƒçš„æ¨¡å‹ä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†æ³¨æ„åŠ›å›¾çš„è§†è§‰è§£é‡Šä¸å…¶ä»–å¸¸ç”¨äºåŒ»å­¦æˆåƒé—®é¢˜çš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å››ä¸ªä¸åŒçš„åŒ»å­¦æˆåƒæ•°æ®é›†ï¼Œæ¶‰åŠï¼ˆ1ï¼‰ç»“è‚ æ¯è‚‰ã€ï¼ˆ2ï¼‰ä¹³è…ºè‚¿ç˜¤ã€ï¼ˆ3ï¼‰é£Ÿé“ç‚ç—‡å’Œï¼ˆ4ï¼‰éª¨æŠ˜å’Œç¡¬ä»¶æ¤å…¥ç‰©çš„è¯†åˆ«ã€‚é€šè¿‡åœ¨ä¸Šè¿°æ•°æ®é›†ä¸Šè¿›è¡Œå¤§è§„æ¨¡å®éªŒï¼Œä½¿ç”¨å„ç§æœ‰ç›‘ç£å’Œè‡ªç›‘ç£çš„é¢„è®­ç»ƒViTsï¼Œæˆ‘ä»¬å‘ç°å°½ç®¡åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œæ³¨æ„åŠ›å›¾è¡¨ç°å‡ºæ½œåŠ›ï¼Œå¹¶ä¸”åœ¨è§£é‡Šæ€§æ–¹é¢é€šå¸¸è¶…è¿‡GradCAMï¼Œä½†å®ƒä»¬è¿˜æ˜¯è¢«é’ˆå¯¹å˜å‹å™¨çš„ç‰¹å®šè§£é‡Šæ–¹æ³•æ‰€è¶…è¶Šã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ³¨æ„åŠ›å›¾ä½œä¸ºè§£é‡Šæ–¹æ³•çš„æœ‰æ•ˆæ€§æ˜¯ä¾èµ–äºå…·ä½“æƒ…å¢ƒçš„ï¼Œå¹¶ä¸”å¯èƒ½ä¼šå—åˆ°é™åˆ¶ï¼Œå› ä¸ºå®ƒä»¬å¹¶ä¸èƒ½å§‹ç»ˆæä¾›ç”¨äºç¨³å¥åŒ»å­¦å†³ç­–æ‰€éœ€çš„å…¨é¢æ´å¯Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09535v1">PDF</a> Accepted for publication in MICCAI 2024 Workshop on Interpretability   of Machine Intelligence in Medical Image Computing (iMIMIC)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†ä½¿ç”¨æ³¨æ„åŠ›æ˜ å°„æŠ€æœ¯å¢å¼ºåŒ»ç–—å›¾åƒåˆ†æä¸­çš„å¯è§£é‡Šæ€§é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨æŸäº›æ¡ä»¶ä¸‹ï¼Œæ³¨æ„åŠ›æ˜ å°„æŠ€æœ¯èƒ½å¤Ÿæé«˜è§£é‡Šèƒ½åŠ›å¹¶è¶…è¶ŠGradCAMï¼Œä½†åœ¨ä¸å…¶ä»–ç‰¹å®šäºTransformerçš„è§£é‡Šæ–¹æ³•æ¯”è¾ƒæ—¶è¡¨ç°è¾ƒå·®ã€‚ç ”ç©¶è¡¨æ˜æ³¨æ„åŠ›æ˜ å°„çš„æœ‰æ•ˆæ€§ä¾èµ–äºä¸Šä¸‹æ–‡ç¯å¢ƒï¼Œä¸”å…¶åœ¨æä¾›å…¨é¢çš„å†³ç­–æ”¯æŒæ–¹é¢å¯èƒ½å—åˆ°é™åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) åœ¨åŒ»ç–—æˆåƒé—®é¢˜ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†é¢ä¸´å¯è§£é‡Šæ€§é—®é¢˜ã€‚</li>
<li>æ³¨æ„åŠ›æ˜ å°„æŠ€æœ¯å¯è¯†åˆ«å½±å“é¢„æµ‹çš„åŒºåŸŸï¼Œä»è€Œè§£å†³ViTsçš„å¯è§£é‡Šæ€§é—®é¢˜ã€‚</li>
<li>å¯¹æ¯”äº†æ³¨æ„åŠ›æ˜ å°„ä¸å…¶ä»–å¸¸ç”¨äºåŒ»ç–—æˆåƒé—®é¢˜çš„è§£é‡Šæ–¹æ³•ã€‚</li>
<li>åœ¨å››ä¸ªä¸åŒçš„åŒ»ç–—æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œå¤§è§„æ¨¡å®éªŒã€‚</li>
<li>æ³¨æ„åŠ›æ˜ å°„æŠ€æœ¯åœ¨æŸäº›æ¡ä»¶ä¸‹å¯æé«˜è§£é‡Šèƒ½åŠ›ï¼Œä½†ä¸å…¶ä»–Transformerç‰¹å®šçš„è§£é‡Šæ–¹æ³•ç›¸æ¯”è¡¨ç°è¾ƒå·®ã€‚</li>
<li>æ³¨æ„åŠ›æ˜ å°„æŠ€æœ¯çš„æœ‰æ•ˆæ€§ä¾èµ–äºä¸Šä¸‹æ–‡ç¯å¢ƒï¼Œå¯èƒ½æ— æ³•æä¾›å…¨é¢çš„å†³ç­–æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09535">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-175035fbd831834046c37eab15133751.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b57fc0a81ca93720ff1db3a2e06760c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1541c158a0579febbe18704599927e6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-505908cc84dd4111c5b39d1bc8855686.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09ec19c8400c8e91f81056a8933f9ebf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b6246da52cecb16ccef50e84cbc16e9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SurgicalVLM-Agent-Towards-an-Interactive-AI-Co-Pilot-for-Pituitary-Surgery"><a href="#SurgicalVLM-Agent-Towards-an-Interactive-AI-Co-Pilot-for-Pituitary-Surgery" class="headerlink" title="SurgicalVLM-Agent: Towards an Interactive AI Co-Pilot for Pituitary   Surgery"></a>SurgicalVLM-Agent: Towards an Interactive AI Co-Pilot for Pituitary   Surgery</h2><p><strong>Authors:Jiayuan Huang, Runlong He, Danyal Z. Khan, Evangelos Mazomenos, Danail Stoyanov, Hani J. Marcus, Matthew J. Clarkson, Mobarakol Islam</strong></p>
<p>Image-guided surgery demands adaptive, real-time decision support, yet static AI models struggle with structured task planning and providing interactive guidance. Large vision-language models (VLMs) offer a promising solution by enabling dynamic task planning and predictive decision support. We introduce SurgicalVLM-Agent, an AI co-pilot for image-guided pituitary surgery, capable of conversation, planning, and task execution. The agent dynamically processes surgeon queries and plans the tasks such as MRI tumor segmentation, endoscope anatomy segmentation, overlaying preoperative imaging with intraoperative views, instrument tracking, and surgical visual question answering (VQA). To enable structured task planning, we develop the PitAgent dataset, a surgical context-aware dataset covering segmentation, overlaying, instrument localization, tool tracking, tool-tissue interactions, phase identification, and surgical activity recognition. Additionally, we propose FFT-GaLore, a fast Fourier transform (FFT)-based gradient projection technique for efficient low-rank adaptation, optimizing fine-tuning for LLaMA 3.2 in surgical environments. We validate SurgicalVLM-Agent by assessing task planning and prompt generation on our PitAgent dataset and evaluating zero-shot VQA using a public pituitary dataset. Results demonstrate state-of-the-art performance in task planning and query interpretation, with highly semantically meaningful VQA responses, advancing AI-driven surgical assistance. </p>
<blockquote>
<p>å›¾åƒå¼•å¯¼æ‰‹æœ¯éœ€è¦è‡ªé€‚åº”ã€å®æ—¶å†³ç­–æ”¯æŒï¼Œä½†é™æ€çš„AIæ¨¡å‹åœ¨ç»“æ„åŒ–ä»»åŠ¡è§„åˆ’å’Œæä¾›äº¤äº’å¼æŒ‡å¯¼æ–¹é¢å­˜åœ¨å›°éš¾ã€‚å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå®ç°åŠ¨æ€ä»»åŠ¡è§„åˆ’å’Œé¢„æµ‹å†³ç­–æ”¯æŒã€‚æˆ‘ä»¬å¼•å…¥äº†SurgicalVLM-Agentï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå›¾åƒå¼•å¯¼å‚ä½“æ‰‹æœ¯çš„AIå‰¯é©¾é©¶ï¼Œèƒ½å¤Ÿè¿›è¡Œå¯¹è¯ã€è§„åˆ’å’Œä»»åŠ¡æ‰§è¡Œã€‚è¯¥ä»£ç†èƒ½å¤ŸåŠ¨æ€å¤„ç†å¤–ç§‘åŒ»ç”ŸæŸ¥è¯¢ï¼Œå¹¶è®¡åˆ’å¦‚MRIè‚¿ç˜¤åˆ†å‰²ã€å†…çª¥é•œè§£å‰–åˆ†å‰²ã€å°†æœ¯å‰å½±åƒä¸æœ¯ä¸­è§†å›¾å åŠ ã€ä»ªå™¨è·Ÿè¸ªå’Œæ‰‹æœ¯è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ç­‰ä»»åŠ¡ã€‚ä¸ºäº†æ”¯æŒç»“æ„åŒ–ä»»åŠ¡è§„åˆ’ï¼Œæˆ‘ä»¬å¼€å‘äº†PitAgentæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‰‹æœ¯ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ•°æ®é›†ï¼Œæ¶µç›–åˆ†å‰²ã€å åŠ ã€ä»ªå™¨å®šä½ã€å·¥å…·è·Ÿè¸ªã€å·¥å…·-ç»„ç»‡äº¤äº’ã€é˜¶æ®µè¯†åˆ«ä»¥åŠæ‰‹æœ¯æ´»åŠ¨è¯†åˆ«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†FFT-GaLoreï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆFFTï¼‰çš„æ¢¯åº¦æŠ•å½±æŠ€æœ¯ï¼Œç”¨äºæœ‰æ•ˆçš„ä½ç§©é€‚åº”ï¼Œä¼˜åŒ–åœ¨æ‰‹æœ¯ç¯å¢ƒä¸­å¯¹LLaMA 3.2çš„å¾®è°ƒã€‚æˆ‘ä»¬é€šè¿‡è¯„ä¼°æˆ‘ä»¬çš„PitAgentæ•°æ®é›†ä¸Šçš„ä»»åŠ¡è§„åˆ’å’Œæç¤ºç”Ÿæˆï¼Œä»¥åŠä½¿ç”¨å…¬å…±å‚ä½“æ•°æ®é›†çš„é›¶æ ·æœ¬VQAæ¥éªŒè¯SurgicalVLM-Agentã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨ä»»åŠ¡è§„åˆ’å’ŒæŸ¥è¯¢è§£é‡Šæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œå…·æœ‰é«˜åº¦è¯­ä¹‰æ„ä¹‰çš„VQAå“åº”ï¼Œæ¨åŠ¨äº†AIé©±åŠ¨çš„æ‰‹æœ¯è¾…åŠ©çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09474v1">PDF</a> 11 pages</p>
<p><strong>Summary</strong><br>     å›¾åƒå¼•å¯¼æ‰‹æœ¯éœ€è¦è‡ªé€‚åº”ã€å®æ—¶å†³ç­–æ”¯æŒï¼Œè€Œé™æ€AIæ¨¡å‹åœ¨ç»“æ„åŒ–ä»»åŠ¡è§„åˆ’å’Œæä¾›äº¤äº’å¼æŒ‡å¯¼æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥SurgicalVLM-Agentï¼Œä¸€ä¸ªç”¨äºå›¾åƒå¼•å¯¼å‚ä½“æ‰‹æœ¯çš„AIå‰¯é©¾é©¶ï¼Œå…·å¤‡å¯¹è¯ã€è§„åˆ’å’Œä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚é€šè¿‡åŠ¨æ€å¤„ç†å¤–ç§‘åŒ»ç”ŸæŸ¥è¯¢ï¼Œå®ç°MRIè‚¿ç˜¤åˆ†å‰²ã€å†…çª¥é•œè§£å‰–åˆ†å‰²ã€æœ¯å‰å½±åƒä¸æœ¯ä¸­è§†å›¾çš„å åŠ ã€ä»ªå™¨è¿½è¸ªå’Œæ‰‹æœ¯è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ç­‰ä»»åŠ¡è§„åˆ’ã€‚ä¸ºæ”¯æŒç»“æ„åŒ–ä»»åŠ¡è§„åˆ’ï¼Œæˆ‘ä»¬å¼€å‘äº†PitAgentæ•°æ®é›†ï¼Œæ¶µç›–åˆ†å‰²ã€å åŠ ã€ä»ªå™¨å®šä½ã€å·¥å…·è¿½è¸ªã€å·¥å…·-ç»„ç»‡äº¤äº’ã€é˜¶æ®µè¯†åˆ«å’Œæ´»åŠ¨è¯†åˆ«ç­‰æ‰‹æœ¯ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºFFT-GaLoreï¼Œä¸€ç§åŸºäºå¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆFFTï¼‰çš„æ¢¯åº¦æŠ•å½±æŠ€æœ¯ï¼Œç”¨äºå®ç°é«˜æ•ˆä½ç§©é€‚åº”ï¼Œä¼˜åŒ–åœ¨æ‰‹æœ¯ç¯å¢ƒä¸­çš„LLaMA 3.2å¾®è°ƒã€‚é€šè¿‡åœ¨æˆ‘ä»¬çš„PitAgentæ•°æ®é›†ä¸Šè¯„ä¼°ä»»åŠ¡è§„åˆ’å’Œæç¤ºç”Ÿæˆï¼Œä»¥åŠåœ¨å…¬å…±å‚ä½“æ•°æ®é›†ä¸Šè¯„ä¼°é›¶æ ·æœ¬VQAï¼ŒéªŒè¯äº†SurgicalVLM-Agentçš„å…ˆè¿›æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒå¼•å¯¼æ‰‹æœ¯éœ€è¦è‡ªé€‚åº”ã€å®æ—¶çš„å†³ç­–æ”¯æŒã€‚</li>
<li>é™æ€AIæ¨¡å‹åœ¨ç»“æ„åŒ–ä»»åŠ¡è§„åˆ’å’Œäº¤äº’å¼æŒ‡å¯¼æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>SurgicalVLM-Agentæ˜¯ä¸€ä¸ªç”¨äºå›¾åƒå¼•å¯¼å‚ä½“æ‰‹æœ¯çš„AIç³»ç»Ÿï¼Œå…·å¤‡å¯¹è¯ã€è§„åˆ’å’Œä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚</li>
<li>SurgicalVLM-Agentå¯ä»¥å®ç°å¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬MRIè‚¿ç˜¤åˆ†å‰²ã€å†…çª¥é•œè§£å‰–åˆ†å‰²ã€æœ¯å‰å½±åƒä¸æœ¯ä¸­è§†å›¾çš„å åŠ ç­‰ã€‚</li>
<li>ä¸ºæ”¯æŒç»“æ„åŒ–ä»»åŠ¡è§„åˆ’ï¼Œå¼€å‘äº†PitAgentæ•°æ®é›†ã€‚</li>
<li>FFT-GaLoreæŠ€æœ¯ç”¨äºæé«˜SurgicalVLM-Agentåœ¨æ‰‹æœ¯ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09474">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e1c6eb7ef8a5c9fc6ad90a0dde8e25f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07061ac4f1e30eb765d5281a58907b49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07e541b1bfafd500aedd9cfb9d19cfaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e9e5a843116a5b1aa95f3703fc33283.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Diff-CL-A-Novel-Cross-Pseudo-Supervision-Method-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Diff-CL-A-Novel-Cross-Pseudo-Supervision-Method-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Diff-CL: A Novel Cross Pseudo-Supervision Method for Semi-supervised   Medical Image Segmentation"></a>Diff-CL: A Novel Cross Pseudo-Supervision Method for Semi-supervised   Medical Image Segmentation</h2><p><strong>Authors:Xiuzhen Guo, Lianyuan Yu, Ji Shi, Na Lei, Hongxiao Wang</strong></p>
<p>Semi-supervised learning utilizes insights from unlabeled data to improve model generalization, thereby reducing reliance on large labeled datasets. Most existing studies focus on limited samples and fail to capture the overall data distribution. We contend that combining distributional information with detailed information is crucial for achieving more robust and accurate segmentation results. On the one hand, with its robust generative capabilities, diffusion models (DM) learn data distribution effectively. However, it struggles with fine detail capture, leading to generated images with misleading details. Combining DM with convolutional neural networks (CNNs) enables the former to learn data distribution while the latter corrects fine details. While capturing complete high-frequency details by CNNs requires substantial computational resources and is susceptible to local noise. On the other hand, given that both labeled and unlabeled data come from the same distribution, we believe that regions in unlabeled data similar to overall class semantics to labeled data are likely to belong to the same class, while regions with minimal similarity are less likely to. This work introduces a semi-supervised medical image segmentation framework from the distribution perspective (Diff-CL). Firstly, we propose a cross-pseudo-supervision learning mechanism between diffusion and convolution segmentation networks. Secondly, we design a high-frequency mamba module to capture boundary and detail information globally. Finally, we apply contrastive learning for label propagation from labeled to unlabeled data. Our method achieves state-of-the-art (SOTA) performance across three datasets, including left atrium, brain tumor, and NIH pancreas datasets. </p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®çš„è§è§£æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œå‡å°‘äº†å¯¹å¤§é‡æœ‰æ ‡ç­¾æ•°æ®é›†çš„ä¾èµ–ã€‚ç°æœ‰çš„å¤§å¤šæ•°ç ”ç©¶éƒ½é›†ä¸­åœ¨æœ‰é™æ ·æœ¬ä¸Šï¼Œè€Œæ— æ³•æ•æ‰æ•´ä½“æ•°æ®åˆ†å¸ƒã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå°†åˆ†å¸ƒä¿¡æ¯ä¸è¯¦ç»†ä¿¡æ¯ç›¸ç»“åˆå¯¹äºå®ç°æ›´ç¨³å¥å’Œå‡†ç¡®çš„åˆ†å‰²ç»“æœè‡³å…³é‡è¦ã€‚ä¸€æ–¹é¢ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ æ•°æ®åˆ†å¸ƒã€‚ç„¶è€Œï¼Œå®ƒåœ¨æ•æ‰ç»†èŠ‚æ–¹é¢è¡¨ç°æŒ£æ‰ï¼Œå¯¼è‡´ç”Ÿæˆçš„å›¾åƒå…·æœ‰è¯¯å¯¼æ€§çš„ç»†èŠ‚ã€‚å°†DMä¸å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ç›¸ç»“åˆï¼Œä½¿å‰è€…èƒ½å¤Ÿå­¦ä¹ æ•°æ®åˆ†å¸ƒï¼Œè€Œåè€…åˆ™æ ¡æ­£ç»†èŠ‚ã€‚è™½ç„¶CNNsæ•æ‰å®Œæ•´çš„é«˜é¢‘ç»†èŠ‚éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œå¹¶å®¹æ˜“å—åˆ°å±€éƒ¨å™ªå£°çš„å½±å“ã€‚å¦ä¸€æ–¹é¢ï¼Œé‰´äºæœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾æ•°æ®æ¥è‡ªåŒä¸€åˆ†å¸ƒï¼Œæˆ‘ä»¬è®¤ä¸ºæ— æ ‡ç­¾æ•°æ®ä¸­ä¸æœ‰æ ‡ç­¾æ•°æ®çš„æ•´ä½“ç±»è¯­ä¹‰ç›¸ä¼¼çš„åŒºåŸŸå¾ˆå¯èƒ½å±äºåŒä¸€ç±»ï¼Œè€Œç›¸ä¼¼æ€§æå°çš„åŒºåŸŸåˆ™ä¸å¤ªå¯èƒ½å±äºã€‚è¿™é¡¹å·¥ä½œä»åˆ†å¸ƒè§’åº¦å¼•å…¥äº†ä¸€ä¸ªåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼ˆDiff-CLï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†æ‰©æ•£å’Œå·ç§¯åˆ†å‰²ç½‘ç»œä¹‹é—´çš„è·¨ä¼ªç›‘ç£å­¦ä¹ æœºåˆ¶ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé«˜é¢‘mambaæ¨¡å—ï¼Œä»¥å…¨å±€æ•æ‰è¾¹ç•Œå’Œè¯¦ç»†ä¿¡æ¯ã€‚æœ€åï¼Œæˆ‘ä»¬åº”ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œå®ç°ä»æœ‰æ ‡ç­¾æ•°æ®åˆ°æ— æ ‡ç­¾æ•°æ®çš„æ ‡ç­¾ä¼ æ’­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å·¦å¿ƒæˆ¿ã€è„‘è‚¿ç˜¤å’ŒNIHèƒ°è…ºæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ˆSOTAï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09408v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŠç›‘ç£å­¦ä¹ åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®çš„è§è§£æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œå‡å°‘ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®é›†ã€‚æœ¬ç ”ç©¶ä»åˆ†å¸ƒè§’åº¦å‡ºå‘ï¼Œæå‡ºä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹å’Œå·ç§¯ç¥ç»ç½‘ç»œè¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†å‰²çš„åŠç›‘ç£æ¡†æ¶ï¼Œå®ç°å…ˆè¿›æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£å­¦ä¹ åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¤šå…³æ³¨æœ‰é™æ ·æœ¬ï¼Œéš¾ä»¥æ•æ‰æ•´ä½“æ•°æ®åˆ†å¸ƒã€‚</li>
<li>ç»“åˆæ•°æ®åˆ†å¸ƒä¸è¯¦ç»†ä¿¡æ¯å¯¹å®ç°ç¨³å¥å‡†ç¡®çš„åˆ†å‰²ç»“æœè‡³å…³é‡è¦ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹èƒ½æœ‰æ•ˆå­¦ä¹ æ•°æ®åˆ†å¸ƒï¼Œä½†éš¾ä»¥æ•æ‰ç²¾ç»†ç»†èŠ‚ã€‚</li>
<li>å·ç§¯ç¥ç»ç½‘ç»œå¯çº æ­£æ‰©æ•£æ¨¡å‹çš„ç»†èŠ‚ç¼ºé™·ï¼Œä½†éœ€å¤§é‡è®¡ç®—èµ„æºä¸”æ˜“å—å±€éƒ¨å™ªå£°å½±å“ã€‚</li>
<li>æå‡ºä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹å’Œå·ç§¯ç¥ç»ç½‘ç»œçš„åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼ˆDiff-CLï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09408">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed42306ff3be5eb7a6fa8448a1561fb7.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-14\./crop_åŒ»å­¦å›¾åƒ/2503.09408v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Exploring-coronal-abundances-of-M-dwarfs-at-moderate-activity-levels"><a href="#Exploring-coronal-abundances-of-M-dwarfs-at-moderate-activity-levels" class="headerlink" title="Exploring coronal abundances of M dwarfs at moderate activity levels"></a>Exploring coronal abundances of M dwarfs at moderate activity levels</h2><p><strong>Authors:J. J. Chebly, K. PoppenhÃ¤ger, J. D. Alvarado-GÃ³mez, B. E. Wood</strong></p>
<p>Main sequence stars of spectral types F, G, and K with low to moderate activity levels exhibit a recognizable pattern known as the first ionization potential effect (FIP effect), where elements with lower first ionization potentials are more abundant in the stellar corona than in the photosphere. In contrast, high activity main sequence stars such as AB Dor (K0), active binaries, and M dwarfs exhibit an inverse pattern known as iFIP. We aim to determine whether or not the iFIP pattern persists in moderate-activity M dwarfs. We used XMM-Newton to observe the moderately active M dwarf HD 223889 that has an X-ray surface flux of log FX,surf &#x3D; 5.26, the lowest for an M dwarf studied so far for coronal abundance patterns. We used low-resolution CCD spectra of the star to calculate the strength of the FIP effect quantified by the FIP bias (Fbias) to assess the persistence of the iFIP effect in M dwarfs. Our findings reveal an iFIP effect similar to that of another moderately active binary star, GJ 338 AB, with a comparable error margin. The results hint at a possible plateau in the Teff-Fbias diagram for moderately active M dwarfs. Targeting stars with low coronal activity that have a coronal temperature between 2 MK and 4 MK is essential for refining our understanding of (i)FIP patterns and their causes. </p>
<blockquote>
<p>å…·æœ‰Fã€Gå’ŒKå…‰è°±å‹çš„ä½æ´»è·ƒåº¦è‡³ä¸­ç­‰æ´»è·ƒåº¦çš„ä¸»åºæ˜Ÿè¡¨ç°å‡ºä¸€ç§ç§°ä¸ºç¬¬ä¸€ç”µç¦»ç”µä½æ•ˆåº”ï¼ˆFIPæ•ˆåº”ï¼‰çš„æ¨¡å¼ï¼Œå…¶ä¸­ç¬¬ä¸€ç”µç¦»ç”µä½è¾ƒä½çš„å…ƒç´ åœ¨æ’æ˜Ÿå†•å±‚ä¸­çš„å«é‡é«˜äºå…‰çƒå±‚ã€‚ç„¶è€Œï¼ŒåƒAB Dorï¼ˆK0ï¼‰ã€æ´»åŠ¨åŒæ˜Ÿå’ŒMçŸ®æ˜Ÿè¿™æ ·çš„é«˜æ´»è·ƒåº¦ä¸»åºæ˜Ÿåˆ™å±•ç°å‡ºä¸€ç§åå‘æ¨¡å¼ï¼Œç§°ä¸ºåFIPæ•ˆåº”ï¼ˆiFIPï¼‰ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç¡®å®šä¸­ç­‰æ´»è·ƒåº¦çš„MçŸ®æ˜Ÿæ˜¯å¦ä¹Ÿå­˜åœ¨iFIPæ¨¡å¼ã€‚æˆ‘ä»¬ä½¿ç”¨XMM-ç‰›é¡¿æœ›è¿œé•œè§‚æµ‹äº†ä¸­ç­‰æ´»è·ƒåº¦çš„MçŸ®æ˜ŸHD 223889ï¼Œå…¶Xå°„çº¿è¡¨é¢é€šé‡ä¸ºlog FXï¼Œsurf &#x3D; 5.26ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢ç ”ç©¶è¿‡çš„å…·æœ‰å† çŠ¶æ¨¡å¼ä¸°åº¦çš„MçŸ®æ˜Ÿä¸­æœ€ä½çš„ã€‚æˆ‘ä»¬ä½¿ç”¨è¯¥æ˜Ÿçš„ä½åˆ†è¾¨ç‡CCDå…‰è°±è®¡ç®—äº†ç”±FIPåå·®ï¼ˆFbiasï¼‰é‡åŒ–çš„FIPæ•ˆåº”çš„å¼ºå¼±ï¼Œä»¥è¯„ä¼°MçŸ®æ˜Ÿä¸­iFIPæ•ˆåº”çš„æŒä¹…æ€§ã€‚æˆ‘ä»¬çš„å‘ç°æ˜¾ç¤ºï¼Œä¸å¦ä¸€ä¸ªä¸­ç­‰æ´»è·ƒçš„åŒæ˜ŸGJ 338 ABç›¸æ¯”ï¼Œå­˜åœ¨ç±»ä¼¼çš„iFIPæ•ˆåº”ï¼Œè¯¯å·®èŒƒå›´ä¹Ÿç›¸ä¼¼ã€‚ç»“æœæç¤ºåœ¨æœ‰æ•ˆæ¸©åº¦ï¼ˆTeffï¼‰- Fbiaså›¾ä¸­ï¼Œå¯¹äºä¸­ç­‰æ´»è·ƒåº¦çš„MçŸ®æ˜Ÿå¯èƒ½å­˜åœ¨ä¸€ä¸ªå¹³å°æœŸã€‚é’ˆå¯¹å…·æœ‰ä»‹äº2 MKå’Œ4 MKä¹‹é—´å† çŠ¶æ¸©åº¦ä¸”ä½å† çŠ¶æ´»æ€§çš„æ’æ˜Ÿè¿›è¡Œç ”ç©¶ï¼Œå¯¹äºå®Œå–„æˆ‘ä»¬å¯¹ï¼ˆiï¼‰FIPæ¨¡å¼åŠå…¶åŸå› çš„ç†è§£è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09316v1">PDF</a> 7 pages, 5 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä¸»åºåˆ—æ˜Ÿä¸­çš„Fã€Gã€Kå‹ä½è‡³ä¸­ç­‰æ´»è·ƒåº¦æ’æ˜Ÿè¡¨ç°å‡ºçš„ç¬¬ä¸€ç”µç¦»ç”µä½æ•ˆåº”ï¼ˆFIPæ•ˆåº”ï¼‰ï¼Œå³ä½ç¬¬ä¸€ç”µç¦»ç”µä½å…ƒç´ åœ¨æ’æ˜Ÿå†•ä¸­æ¯”å…‰çƒä¸­æ›´ä¸°å¯Œã€‚ç›¸åï¼Œé«˜æ´»è·ƒåº¦ä¸»åºåˆ—æ˜Ÿå¦‚AB Dorï¼ˆK0ï¼‰ã€æ´»è·ƒåŒæ˜Ÿå’ŒMçŸ®æ˜Ÿåˆ™è¡¨ç°å‡ºé€†å‘çš„iFIPæ¨¡å¼ã€‚ç ”ç©¶æ—¨åœ¨ç¡®å®šiFIPæ¨¡å¼åœ¨ä¸­ç­‰æ´»è·ƒåº¦MçŸ®æ˜Ÿä¸­æ˜¯å¦æŒç»­å­˜åœ¨ã€‚ä½¿ç”¨XMM-Newtonè§‚æµ‹ä¸­ç­‰æ´»è·ƒåº¦MçŸ®æ˜ŸHD 223889ï¼Œå‘ç°å…¶iFIPæ•ˆåº”ä¸å¦ä¸€ä¸ªä¸­ç­‰æ´»è·ƒåº¦åŒæ˜ŸGJ 338 ABç›¸ä¼¼ã€‚ç»“æœæš—ç¤ºåœ¨æœ‰æ•ˆæ¸©åº¦å’ŒFbiaså›¾ä¸­ï¼Œä¸­ç­‰æ´»è·ƒåº¦MçŸ®æ˜Ÿå¯èƒ½å­˜åœ¨ä¸€ä¸ªå¹³å°æœŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸»åºåˆ—Fã€Gã€Kå‹ä½è‡³ä¸­ç­‰æ´»è·ƒåº¦æ’æ˜Ÿè¡¨ç°å‡ºFIPæ•ˆåº”ï¼Œå³ä½ç¬¬ä¸€ç”µç¦»ç”µä½å…ƒç´ åœ¨æ’æ˜Ÿå†•ä¸­æ›´ä¸°å¯Œã€‚</li>
<li>é«˜æ´»è·ƒåº¦ä¸»åºåˆ—æ˜Ÿå¦‚AB Dorã€æ´»è·ƒåŒæ˜Ÿå’ŒMçŸ®æ˜Ÿè¡¨ç°å‡ºiFIPæ¨¡å¼ã€‚</li>
<li>ä½¿ç”¨XMM-Newtonå¯¹ä¸­ç­‰æ´»è·ƒåº¦MçŸ®æ˜ŸHD 223889è¿›è¡Œè§‚æµ‹ï¼Œå‘ç°å…¶iFIPæ•ˆåº”ä¸å¦ä¸€åŒæ˜Ÿç›¸ä¼¼ã€‚</li>
<li>ç»“æœæ˜¾ç¤ºï¼Œåœ¨æœ‰æ•ˆæ¸©åº¦å’ŒFbiaså›¾ä¸­ï¼Œä¸­ç­‰æ´»è·ƒåº¦MçŸ®æ˜Ÿå¯èƒ½å­˜åœ¨ä¸€ä¸ªå¹³å°æœŸã€‚</li>
<li>ç ”ç©¶é‡ç‚¹åœ¨äºè§‚å¯Ÿå…·æœ‰ä½å†•æ´»åŠ¨æ€§å’Œç‰¹å®šæ¸©åº¦èŒƒå›´çš„æ’æ˜Ÿï¼Œä»¥è¿›ä¸€æ­¥äº†è§£iFIPæ¨¡å¼åŠå…¶æˆå› ã€‚</li>
<li>FIPæ•ˆåº”å’ŒiFIPæ¨¡å¼çš„è¯†åˆ«å¯¹äºç†è§£æ’æ˜Ÿå¤§æ°”ç»“æ„å’Œèƒ½é‡å¹³è¡¡è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09316">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9b78b0c2926817fb0f675109046f5f9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48f958ba88cfb2512eea09f4c2863775.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94018f87e18a2245470159afb9948e93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-875a2503127848b6f33ce9179e80e0a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a9830457d8d05d61f3154fa5e3ca52d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0944625efbe260cebe063f7f6b34edce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfabec44840ad37f5b32a974b899d5bb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Mono2D-A-Trainable-Monogenic-Layer-for-Robust-Knee-Cartilage-Segmentation-on-Out-of-Distribution-2D-Ultrasound-Data"><a href="#Mono2D-A-Trainable-Monogenic-Layer-for-Robust-Knee-Cartilage-Segmentation-on-Out-of-Distribution-2D-Ultrasound-Data" class="headerlink" title="Mono2D: A Trainable Monogenic Layer for Robust Knee Cartilage   Segmentation on Out-of-Distribution 2D Ultrasound Data"></a>Mono2D: A Trainable Monogenic Layer for Robust Knee Cartilage   Segmentation on Out-of-Distribution 2D Ultrasound Data</h2><p><strong>Authors:Alvin Kimbowa, Arjun Parmar, Maziar Badii, David Liu, Matthew Harkey, Ilker Hacihaliloglu</strong></p>
<p>Automated knee cartilage segmentation using point-of-care ultrasound devices and deep-learning networks has the potential to enhance the management of knee osteoarthritis. However, segmentation algorithms often struggle with domain shifts caused by variations in ultrasound devices and acquisition parameters, limiting their generalizability. In this paper, we propose Mono2D, a monogenic layer that extracts multi-scale, contrast- and intensity-invariant local phase features using trainable bandpass quadrature filters. This layer mitigates domain shifts, improving generalization to out-of-distribution domains. Mono2D is integrated before the first layer of a segmentation network, and its parameters jointly trained alongside the networkâ€™s parameters. We evaluated Mono2D on a multi-domain 2D ultrasound knee cartilage dataset for single-source domain generalization (SSDG). Our results demonstrate that Mono2D outperforms other SSDG methods in terms of Dice score and mean average surface distance. To further assess its generalizability, we evaluate Mono2D on a multi-site prostate MRI dataset, where it continues to outperform other SSDG methods, highlighting its potential to improve domain generalization in medical imaging. Nevertheless, further evaluation on diverse datasets is still necessary to assess its clinical utility. </p>
<blockquote>
<p>ä½¿ç”¨å³æ—¶åŒ»ç–—æŠ¤ç†è¶…å£°æ³¢è®¾å¤‡å’Œæ·±åº¦å­¦ä¹ ç½‘ç»œå¯¹è†å…³èŠ‚è½¯éª¨è¿›è¡Œè‡ªåŠ¨åˆ†å‰²ï¼Œåœ¨æ”¹å–„è†å…³èŠ‚éª¨å…³èŠ‚ç‚ç®¡ç†æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚ç„¶è€Œï¼Œåˆ†å‰²ç®—æ³•ç»å¸¸å› è¶…å£°æ³¢è®¾å¤‡å’Œé‡‡é›†å‚æ•°çš„å·®å¼‚å¯¼è‡´çš„é¢†åŸŸå˜åŒ–è€Œé¢ä¸´æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Mono2Dï¼Œä¸€ç§å•åŸºå› å±‚ï¼Œé€šè¿‡ä½¿ç”¨å¯è®­ç»ƒçš„å¸¦é€šæ­£äº¤æ»¤æ³¢å™¨æå–å¤šå°ºåº¦ã€å¯¹æ¯”åº¦å’Œå¼ºåº¦ä¸å˜å±€éƒ¨ç›¸ä½ç‰¹å¾ã€‚è¿™ä¸€å±‚å‡è½»äº†é¢†åŸŸå˜åŒ–ï¼Œæé«˜äº†å¯¹åˆ†å¸ƒå¤–é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚Mono2Dè¢«é›†æˆåœ¨åˆ†å‰²ç½‘ç»œçš„ç¬¬ä¸€å±‚ä¹‹å‰ï¼Œå¹¶ä¸å…¶å‚æ•°è”åˆè®­ç»ƒã€‚æˆ‘ä»¬åœ¨å¤šé¢†åŸŸäºŒç»´è¶…å£°è†å…³èŠ‚è½¯éª¨æ•°æ®é›†ä¸Šå¯¹Mono2Dè¿›è¡Œäº†å•æºåŸŸæ³›åŒ–ï¼ˆSSDGï¼‰çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨è¿ªå…‹åˆ†æ•°å’Œå¹³å‡è¡¨é¢è·ç¦»æ–¹é¢ï¼ŒMono2Dä¼˜äºå…¶ä»–SSDGæ–¹æ³•ã€‚ä¸ºäº†è¿›ä¸€æ­¥è¯„ä¼°å…¶æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨å¤šç«™ç‚¹å‰åˆ—è…ºMRIæ•°æ®é›†ä¸Šè¯„ä¼°äº†Mono2Dï¼Œå®ƒåœ¨è¯¥æ•°æ®é›†ä¸Šä»ç„¶ä¼˜äºå…¶ä»–SSDGæ–¹æ³•ï¼Œå‡¸æ˜¾å…¶åœ¨åŒ»å­¦å½±åƒä¸­æé«˜é¢†åŸŸæ³›åŒ–çš„æ½œåŠ›ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä»ç„¶éœ€è¦åœ¨å„ç§æ•°æ®é›†ä¸Šè¿›è¡Œè¿›ä¸€æ­¥çš„è¯„ä¼°ä»¥è¯„ä¼°å…¶ä¸´åºŠå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09050v1">PDF</a> </p>
<p><strong>Summary</strong><br>    ä½¿ç”¨å•åŸºå› å±‚Mono2Dæå–å¤šå°ºåº¦ã€å¯¹æ¯”åº¦å’Œå¼ºåº¦ä¸å˜å±€éƒ¨ç›¸ä½ç‰¹å¾ï¼Œé€šè¿‡è®­ç»ƒå¸¦é€šæ­£äº¤æ»¤æ³¢å™¨ï¼Œæé«˜æ·±åº¦å­¦ä¹ ç½‘ç»œå¯¹è†å…³èŠ‚è½¯éª¨çš„è‡ªåŠ¨åŒ–åˆ†å‰²æ€§èƒ½ï¼Œå¹¶å¢å¼ºå¯¹è†å…³èŠ‚éª¨å…³èŠ‚ç‚çš„ç®¡ç†ã€‚Mono2Dæœ‰åŠ©äºç¼“è§£å› è¶…å£°è®¾å¤‡å’Œé‡‡é›†å‚æ•°å˜åŒ–å¼•èµ·çš„é¢†åŸŸåç§»é—®é¢˜ï¼Œæå‡æ¨¡å‹åœ¨è·¨åˆ†å¸ƒé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒMono2Dåœ¨å•æºåŸŸæ³›åŒ–ï¼ˆSSDGï¼‰çš„å¤šåŸŸäºŒç»´è¶…å£°è†å…³èŠ‚è½¯éª¨æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ä¸”åœ¨å¤šç«™ç‚¹å‰åˆ—è…ºMRIæ•°æ®é›†ä¸Šä¹Ÿæœ‰è‰¯å¥½è¡¨ç°ã€‚ç„¶è€Œï¼Œä»éœ€è¦åœ¨æ›´å¤šæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ä»¥è¯„ä¼°å…¶ä¸´åºŠå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mono2Dæ˜¯ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒå¤„ç†çš„å•åŸºå› å±‚æŠ€æœ¯ï¼Œèƒ½å¤Ÿä»åŸå§‹è¶…å£°å›¾åƒä¸­æå–å¤šå°ºåº¦ã€å¯¹æ¯”åº¦å’Œå¼ºåº¦ä¸å˜çš„å±€éƒ¨ç›¸ä½ç‰¹å¾ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨è®­ç»ƒå¸¦é€šæ­£äº¤æ»¤æ³¢å™¨ï¼ŒMono2Då¯ä»¥æœ‰æ•ˆå‡è½»ä¸åŒè¶…å£°è®¾å¤‡å’Œé‡‡é›†å‚æ•°é€ æˆçš„é¢†åŸŸåç§»é—®é¢˜ã€‚</li>
<li>Mono2DæŠ€æœ¯åœ¨è†å…³èŠ‚è½¯éª¨è‡ªåŠ¨åˆ†å‰²æ–¹é¢å±•ç°å‡ºä¼˜åŠ¿ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒMono2Dåœ¨å•æºåŸŸæ³›åŒ–ï¼ˆSSDGï¼‰çš„å¤šåŸŸäºŒç»´è¶…å£°è†å…³èŠ‚è½¯éª¨æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>Mono2Dåœ¨å¤šç«™ç‚¹å‰åˆ—è…ºMRIæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¹Ÿè¯æ˜äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å°½ç®¡Mono2Då·²ç»å–å¾—äº†ä¸€å®šçš„æˆæœï¼Œä½†ä»éœ€è¿›ä¸€æ­¥åœ¨æ›´å¤šæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ä»¥ç¡®è®¤å…¶ä¸´åºŠå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b05e5b9523ae31f95c063683c287adb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a11da372bc99e22e8c0a1057e50b2a1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-624977f50b98c4ae3ec3e525ad085c9d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Frequency-resolved-time-lags-due-to-X-ray-disk-reprocessing-in-AGN"><a href="#Frequency-resolved-time-lags-due-to-X-ray-disk-reprocessing-in-AGN" class="headerlink" title="Frequency-resolved time lags due to X-ray disk reprocessing in AGN"></a>Frequency-resolved time lags due to X-ray disk reprocessing in AGN</h2><p><strong>Authors:Christos Panagiotou, Iossif Papadakis, Erin Kara, Marios Papoutsis, Edward M. Cackett, Michal DovÄiak, Javier A. GarcÃ­a, Elias Kammoun, Collin Lewin</strong></p>
<p>Over the last years, a number of broadband reverberation mapping campaigns have been conducted to explore the short-term UV and optical variability of nearby AGN. Despite the extensive data collected, the origin of the observed variability is still debated in the literature. Frequency-resolved time lags offer a promising approach to distinguish between different scenarios, as they probe variability on different time scales. In this study, we present the expected frequency-resolved lags resulting from X-ray reprocessing in the accretion disk. The predicted lags are found to feature a general shape that resembles that of observational measurements, while exhibiting strong dependence on various physical parameters. Additionally, we compare our model predictions to observational data for the case of NGC 5548, concluding that the X-ray illumination of the disk can effectively account for the observed frequency-resolved lags and power spectra in a self-consistent way. To date, X-ray disk reprocessing is the only physical model that has successfully reproduced the observed multi-wavelength variability, in both amplitude and time delays, across a range of temporal frequencies. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå·²ç»è¿›è¡Œäº†å¤šæ¬¡å®½å¸¦æ··å“æ˜ å°„æ´»åŠ¨ï¼Œä»¥æ¢ç´¢é™„è¿‘æ´»åŠ¨æ˜Ÿç³»æ ¸çš„ç´«å¤–å’Œå…‰å­¦çŸ­æœŸå˜åŒ–ã€‚å°½ç®¡æ”¶é›†äº†å¤§é‡æ•°æ®ï¼Œä½†è§‚å¯Ÿåˆ°çš„å˜åŒ–çš„åŸå› åœ¨æ–‡çŒ®ä¸­ä»æœ‰äº‰è®®ã€‚é¢‘ç‡è§£æçš„æ—¶é—´å»¶è¿Ÿæä¾›äº†ä¸€ç§å¾ˆæœ‰å‰æ™¯çš„æ–¹æ³•æ¥åŒºåˆ†ä¸åŒçš„åœºæ™¯ï¼Œå› ä¸ºå®ƒä»¬æ¢æµ‹çš„æ˜¯ä¸åŒæ—¶é—´å°ºåº¦çš„å˜åŒ–ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç”±å¸ç§¯ç›˜ä¸­çš„Xå°„çº¿å†å¤„ç†å¼•èµ·çš„é¢„æœŸé¢‘ç‡è§£æå»¶è¿Ÿã€‚é¢„æµ‹çš„å»¶è¿Ÿå‘ˆç°äº†ä¸€ç§ç±»ä¼¼äºè§‚æµ‹æµ‹é‡çš„æ€»ä½“å½¢çŠ¶ï¼Œè¡¨ç°å‡ºå¯¹å„ç§ç‰©ç†å‚æ•°çš„å¼ºçƒˆä¾èµ–æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ¨¡å‹é¢„æµ‹ä¸NGC 5548çš„è§‚æµ‹æ•°æ®è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶å¾—å‡ºç»“è®ºè®¤ä¸ºï¼ŒXå°„çº¿å¯¹ç£ç›˜çš„ç…§æ˜å¯ä»¥æœ‰æ•ˆåœ°è§£é‡Šè§‚å¯Ÿåˆ°çš„é¢‘ç‡è§£æå»¶è¿Ÿå’ŒåŠŸç‡è°±ï¼Œå¹¶ä¸”å¯ä»¥åœ¨è‡ªæ´½çš„æ–¹å¼ä¸‹è¿›è¡Œã€‚è¿„ä»Šä¸ºæ­¢ï¼ŒXå°„çº¿ç£ç›˜å†å¤„ç†æ˜¯å”¯ä¸€æˆåŠŸå†ç°äº†è§‚å¯Ÿåˆ°çš„å¤šæ³¢é•¿å˜åŒ–çš„ç‰©ç†æ¨¡å‹ï¼Œåœ¨æŒ¯å¹…å’Œæ—¶é—´å»¶è¿Ÿæ–¹é¢ï¼Œå…¶åœ¨å„ç§æ—¶é—´é¢‘ç‡èŒƒå›´å†…å‡è¡¨ç°è‰¯å¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09036v1">PDF</a> 10 (+2 in Appendix) pages, 5 figures, accepted for publication by ApJ</p>
<p><strong>Summary</strong><br>     è¿‘å¹´å¯¹é™„è¿‘æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰çš„ç´«å¤–çº¿åŠå…‰å­¦å¯å˜æ€§çš„çŸ­æœŸå®½å¸¦å›å£°æ˜ å°„æ´»åŠ¨ä¸°å¯Œï¼Œä½†å¯¹è§‚æµ‹åˆ°çš„å¯å˜æ€§çš„èµ·æºä»å­˜åœ¨äº‰è®®ã€‚é¢‘ç‡è§£ææ—¶é—´å»¶è¿Ÿä¸ºåŒºåˆ†ä¸åŒåœºæ™¯æä¾›äº†æœ‰å‰é€”çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒä»¬èƒ½æ¢æµ‹ä¸åŒæ—¶é—´å°ºåº¦ä¸Šçš„å˜åŒ–ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†Xå°„çº¿åœ¨å¸ç§¯ç›˜å†åŠ å·¥è¿‡ç¨‹ä¸­é¢„æœŸäº§ç”Ÿçš„é¢‘ç‡è§£æå»¶è¿Ÿã€‚é¢„æµ‹çš„æ—¶é—´å»¶è¿Ÿè¡¨ç°å‡ºä¸è§‚æµ‹æµ‹é‡ç›¸ä¼¼çš„æ€»ä½“å½¢æ€ï¼Œå¹¶å¼ºçƒˆä¾èµ–äºå„ç§ç‰©ç†å‚æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ¨¡å‹é¢„æµ‹ä¸NGC 5548çš„è§‚æµ‹æ•°æ®è¿›è¡Œäº†æ¯”è¾ƒï¼Œè®¤ä¸ºXå°„çº¿çš„ç›˜å†ç…§æ˜å¯ä»¥æœ‰æ•ˆåœ°è§£é‡Šè§‚å¯Ÿåˆ°çš„é¢‘ç‡è§£æå»¶è¿Ÿå’ŒåŠŸç‡è°±ï¼Œå…·æœ‰è‡ªæ´½æ€§ã€‚è¿„ä»Šä¸ºæ­¢ï¼ŒXå°„çº¿ç›˜å†åŠ å·¥æ˜¯å”¯ä¸€æˆåŠŸé‡ç°å¤šæ³¢é•¿è§‚æµ‹å¯å˜æ€§çš„ç‰©ç†æ¨¡å‹ï¼Œæ— è®ºåœ¨æŒ¯å¹…è¿˜æ˜¯æ—¶é—´å»¶è¿Ÿæ–¹é¢ï¼Œéƒ½èƒ½è·¨è¶Šä¸€ç³»åˆ—çš„ä¸´æ—¶é¢‘ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘å¹´å¯¹é™„è¿‘æ´»åŠ¨æ˜Ÿç³»æ ¸çš„çŸ­æœŸUVå’Œå…‰å­¦å¯å˜æ€§çš„ç ”ç©¶é€šè¿‡å®½å¸¦å›å£°æ˜ å°„æ´»åŠ¨ä¸°å¯Œã€‚</li>
<li>é¢‘ç‡è§£ææ—¶é—´å»¶è¿Ÿæœ‰åŠ©äºåŒºåˆ†ä¸åŒçš„å¯å˜æ€§èµ·æºåœºæ™¯ã€‚</li>
<li>Xå°„çº¿åœ¨å¸ç§¯ç›˜å†åŠ å·¥è¿‡ç¨‹ä¸­çš„é¢„æœŸé¢‘ç‡è§£æå»¶è¿Ÿè¡¨ç°å‡ºä¸è§‚æµ‹ç›¸ä¼¼çš„æ€»ä½“å½¢æ€ã€‚</li>
<li>é¢„æµ‹çš„æ—¶é—´å»¶è¿Ÿå¼ºçƒˆä¾èµ–äºå¤šç§ç‰©ç†å‚æ•°ã€‚</li>
<li>Xå°„çº¿ç›˜å†ç…§æ˜èƒ½è§£é‡Šè§‚å¯Ÿåˆ°çš„é¢‘ç‡è§£æå»¶è¿Ÿå’ŒåŠŸç‡è°±ï¼Œå…·æœ‰è‡ªæ´½æ€§ã€‚</li>
<li>æ¯”è¾ƒæ¨¡å‹é¢„æµ‹ä¸NGC 5548çš„è§‚æµ‹æ•°æ®ï¼Œå‘ç°Xå°„çº¿æ¨¡å‹æˆåŠŸé‡ç°äº†å¤šæ³¢é•¿è§‚æµ‹çš„å¯å˜æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09036">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d83e2b4536190e8ac55eac6a77b81a5f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c58cd900dfcca9b2bb5ed1d56d477fb8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40a3265d7f003202f00e0d4890d7f969.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5294436c80ff8a0eeaf8e10205fa3773.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Flares-in-the-Changing-Look-AGN-Mrk-590-II-Deep-X-ray-observations-reveal-a-Comptonizing-inner-accretion-flow"><a href="#Flares-in-the-Changing-Look-AGN-Mrk-590-II-Deep-X-ray-observations-reveal-a-Comptonizing-inner-accretion-flow" class="headerlink" title="Flares in the Changing Look AGN Mrk 590. II: Deep X-ray observations   reveal a Comptonizing inner accretion flow"></a>Flares in the Changing Look AGN Mrk 590. II: Deep X-ray observations   reveal a Comptonizing inner accretion flow</h2><p><strong>Authors:Daniel Lawther, Marianne Vestergaard, Sandra Raimundo, Xiaohui Fan, Jun Yi Koay</strong></p>
<p>Mrk 590 is a Changing Look AGN currently in an unusual repeat X-ray and UV flaring state. Here, we report on deep X-ray observations with XMM-Newton, NuSTAR, and NICER, obtained at a range of X-ray flux levels. We detect a prominent soft excess below 2 keV; its flux is tightly correlated with that of both the X-ray and UV continuum, and it persists at the lowest flux levels captured. Our Bayesian model comparison strongly favors inverse Comptonization as the origin of this soft excess, instead of blurred reflection. We find only weak reflection features, with R~0.4 assuming Compton-thick reflection. Most of this reprocessing occurs at least $\sim$800 gravitational radii (roughly three light-days) from the continuum source. Relativistically broadened emission is weak or absent, suggesting the lack of a standard <code>thin disk&#39; at small radii. We confirm that the predicted broad-band emission due to Comptonization is roughly consistent with the observed UV--optical photometry. This implies an optically thick, warm ($kT_e\sim0.3$ keV) scattering region that extends to at least $\sim10^3$ gravitational radii, reprocessing any UV thermal emission. The lack of a standard </code>thin diskâ€™ may also explain the puzzling $\sim3$-day X-ray to UV delay previously measured for Mrk 590. Overall, we find that the X-ray spectral changes in Mrk 590 are minimal, despite substantial luminosity changes. Other well-studied changing look AGN display more dramatic spectral evolution, e.g., disappearing continuum or soft excess. This suggests that a diversity of physical mechanisms in the inner accretion flow may produce a UVâ€“optical changing-look event. </p>
<blockquote>
<p>Mrk 590æ˜¯ä¸€é¢—å¤–è§‚ä¸æ–­å˜åŒ–çš„æ´»è·ƒæ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰ï¼Œç›®å‰å¤„äºä¸å¯»å¸¸çš„é‡å¤Xå°„çº¿å’Œç´«å¤–è€€å‘çŠ¶æ€ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†ä½¿ç”¨XMM-Newtonã€NuSTARå’ŒNICERè¿›è¡Œçš„æ·±åº¦Xå°„çº¿è§‚æµ‹ç»“æœï¼Œè¿™äº›è§‚æµ‹æ¶µç›–äº†å¤šç§Xå°„çº¿æµé‡æ°´å¹³ã€‚æˆ‘ä»¬æ£€æµ‹åˆ°äº†ä¸€ä¸ªæ˜¾è‘—çš„ä½äº2keVçš„è½¯è¿‡å‰©ç°è±¡ï¼›å…¶æµé‡ä¸Xå°„çº¿å’Œç´«å¤–è¿ç»­ä½“çš„æµé‡ç´§å¯†ç›¸å…³ï¼Œå¹¶ä¸”åœ¨æ•è·çš„æœ€ä½æµé‡æ°´å¹³ä¸‹ä»ç„¶å­˜åœ¨ã€‚æˆ‘ä»¬çš„è´å¶æ–¯æ¨¡å‹æ¯”è¾ƒå¼ºçƒˆæ”¯æŒé€†åº·æ™®é¡¿åŒ–æ˜¯è½¯è¿‡å‰©çš„èµ·æºï¼Œè€Œä¸æ˜¯æ¨¡ç³Šçš„åå°„ã€‚æˆ‘ä»¬å‘ç°åªæœ‰å¾®å¼±çš„åå°„ç‰¹å¾ï¼Œå‡è®¾åº·æ™®é¡¿åšåå°„æ—¶Rçº¦ä¸º0.4ã€‚å¤§éƒ¨åˆ†å†å¤„ç†è¿‡ç¨‹å‘ç”Ÿåœ¨è¿ç»­å…‰æºè‡³å°‘<del>800ä¸ªå¼•åŠ›åŠå¾„ï¼ˆå¤§çº¦ä¸‰å¤©ï¼‰å¤„ã€‚ç›¸å¯¹è®ºæ€§å±•å®½å‘å°„å¾ˆå¼±æˆ–ä¸å­˜åœ¨ï¼Œè¡¨æ˜å°åŠå¾„å¤„æ²¡æœ‰æ ‡å‡†çš„â€œè–„ç›˜â€ã€‚æˆ‘ä»¬ç¡®è®¤ï¼Œç”±äºåº·æ™®é¡¿åŒ–è€Œé¢„æµ‹çš„å®½é¢‘å¸¦å‘å°„å¤§è‡´ä¸è§‚å¯Ÿåˆ°çš„ç´«å¤–å…‰å­¦å…‰åº¦æ³•ä¸€è‡´ã€‚è¿™æš—ç¤ºå­˜åœ¨ä¸€ä¸ªå»¶ä¼¸åˆ°è‡³å°‘</del>10^3ä¸ªå¼•åŠ›åŠå¾„çš„å…‰å­¦åšã€æ¸©æš–çš„ï¼ˆkT_e~0.3keVï¼‰æ•£å°„åŒºåŸŸï¼Œè¯¥åŒºåŸŸå¤„ç†ä»»ä½•ç´«å¤–çƒ­å‘å°„ã€‚æ²¡æœ‰æ ‡å‡†çš„â€œè–„ç›˜â€ä¹Ÿå¯èƒ½è§£é‡Šäº†ä¹‹å‰ä¸ºMrk 590æµ‹é‡çš„çº¦3å¤©çš„Xå°„çº¿åˆ°ç´«å¤–å»¶è¿Ÿã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°å°½ç®¡äº®åº¦æœ‰é‡å¤§å˜åŒ–ï¼Œä½†Mrk 590çš„Xå°„çº¿å…‰è°±å˜åŒ–å¾ˆå°ã€‚å…¶ä»–ç»è¿‡è‰¯å¥½ç ”ç©¶çš„å¤–è§‚ä¸æ–­å˜åŒ–çš„æ´»è·ƒæ˜Ÿç³»æ ¸æ˜¾ç¤ºå‡ºæ›´å‰§çƒˆçš„è°±æ¼”åŒ–ï¼Œä¾‹å¦‚è¿ç»­å…‰è°±æˆ–è½¯è¿‡å‰©æ¶ˆå¤±ã€‚è¿™è¡¨æ˜å†…æµç§¯ä¸­çš„å¤šç§ç‰©ç†æœºåˆ¶å¯èƒ½äº§ç”Ÿç´«å¤–å…‰å­¦å˜åŒ–äº‹ä»¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08959v1">PDF</a> Accepted for publication in MNRAS. 47 pages, 41 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŠ¥å‘Šäº†å¯¹Mrk 590çš„æ·±å…¥Xå°„çº¿è§‚æµ‹ç»“æœã€‚å‘ç°å…¶è½¯è¿‡å‰©ç°è±¡ä¸Xå°„çº¿å’Œç´«å¤–è¿ç»­å…‰è°±ç´§å¯†ç›¸å…³ï¼Œå¹¶åœ¨æœ€ä½é€šé‡æ°´å¹³æ—¶ä»ç„¶å­˜åœ¨ã€‚å€¾å‘äºé€šè¿‡é€†åº·æ™®é¡¿åŒ–æ¥è§£é‡Šè½¯è¿‡å‰©çš„èµ·æºï¼Œè€Œéæ¨¡ç³Šåå°„ã€‚å‘ç°å¾®å¼±çš„åå°„ç‰¹å¾ï¼Œå¤§éƒ¨åˆ†å†å¤„ç†å‘ç”Ÿåœ¨è¿ç»­å…‰æºçš„å¼•åŠ›åŠå¾„è‡³å°‘çº¦800å¤„ã€‚ç›¸å¯¹è®ºæ€§å±•å®½å‘å°„è¾ƒå¼±æˆ–ç¼ºå¤±ï¼Œæš—ç¤ºå°åŠå¾„å¤„ç¼ºä¹æ ‡å‡†çš„â€œè–„ç›˜â€ã€‚æ¨æµ‹å­˜åœ¨ä¸€ä¸ªå»¶ä¼¸åˆ°è‡³å°‘æ•°åƒå¼•åŠ›åŠå¾„çš„å…‰åšã€æ¸©æš–çš„ï¼ˆkT_e~0.3 keVï¼‰æ•£å°„åŒºåŸŸï¼Œå¤„ç†ä»»ä½•ç´«å¤–çƒ­å‘å°„ã€‚Mrk 590çš„Xå°„çº¿å…‰è°±å˜åŒ–æœ€å°ï¼Œå°½ç®¡å…‰åº¦å˜åŒ–æ˜¾è‘—ã€‚ä¸å…¶ä»–ç ”ç©¶è¿‡çš„å¤–è§‚æ”¹å˜çš„æ´»è·ƒæ˜Ÿç³»æ ¸ç›¸æ¯”ï¼Œå…¶å…‰è°±æ¼”åŒ–è¾ƒå°ï¼Œè¿™è¡¨æ˜å†…æµç§¯ç‰©çš„ç‰©ç†æœºåˆ¶å¤šæ ·æ€§å¯èƒ½ä¼šäº§ç”Ÿç´«å¤–å…‰åˆ°å…‰å­¦å˜åŒ–çš„å¤–è§‚äº‹ä»¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mrk 590å¤„äºç‰¹æ®Šçš„é‡å¤Xå°„çº¿å’Œç´«å¤–å…‰è€€æ€ï¼Œæ˜¾ç¤ºå‡ºXå°„çº¿å’Œç´«å¤–å…‰çš„æŒç»­å˜åŒ–ã€‚</li>
<li>æ·±Xå°„çº¿è§‚æµ‹æ˜¾ç¤ºå­˜åœ¨ä¸€ä¸ªç´§å¯†ç›¸å…³çš„è½¯è¿‡å‰©ç°è±¡ä¸Xå°„çº¿å’Œç´«å¤–è¿ç»­å…‰è°±ã€‚</li>
<li>è½¯è¿‡å‰©ç°è±¡æ›´å€¾å‘äºé€šè¿‡é€†åº·æ™®é¡¿åŒ–è§£é‡Šè€Œéæ¨¡ç³Šåå°„ã€‚</li>
<li>å­˜åœ¨å¾®å¼±çš„åå°„ç‰¹å¾ï¼Œå¤§éƒ¨åˆ†åå°„å‘ç”Ÿåœ¨è·ç¦»è¿ç»­å…‰æºè¾ƒè¿œå¤„ã€‚</li>
<li>ç›¸å¯¹è®ºæ€§å±•å®½å‘å°„å¼±æˆ–ç¼ºå¤±ï¼Œæš—ç¤ºå†…éƒ¨å¯èƒ½ç¼ºä¹å…¸å‹çš„è–„ç›˜ç»“æ„ã€‚</li>
<li>æœ‰ä¸€ä¸ªå¤§èŒƒå›´çš„å…‰åšæ¸©æš–æ•£å°„åŒºåŸŸå¤„ç†ç´«å¤–çƒ­å‘å°„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f56edec3fcc7774521a179f9453478e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61614749997dbeb17edb77036601706a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a8be523258c3650c202283781199af1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1b379893e52911733c8786bc7ea6a58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3b82cc55f4eeb0e39e7b91cfc24d3bf.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Deep-Bayesian-Nonparametric-Framework-for-Robust-Mutual-Information-Estimation"><a href="#A-Deep-Bayesian-Nonparametric-Framework-for-Robust-Mutual-Information-Estimation" class="headerlink" title="A Deep Bayesian Nonparametric Framework for Robust Mutual Information   Estimation"></a>A Deep Bayesian Nonparametric Framework for Robust Mutual Information   Estimation</h2><p><strong>Authors:Forough Fazeliasl, Michael Minyi Zhang, Bei Jiang, Linglong Kong</strong></p>
<p>Mutual Information (MI) is a crucial measure for capturing dependencies between variables, but exact computation is challenging in high dimensions with intractable likelihoods, impacting accuracy and robustness. One idea is to use an auxiliary neural network to train an MI estimator; however, methods based on the empirical distribution function (EDF) can introduce sharp fluctuations in the MI loss due to poor out-of-sample performance, destabilizing convergence. We present a Bayesian nonparametric (BNP) solution for training an MI estimator by constructing the MI loss with a finite representation of the Dirichlet process posterior to incorporate regularization in the training process. With this regularization, the MI loss integrates both prior knowledge and empirical data to reduce the loss sensitivity to fluctuations and outliers in the sample data, especially in small sample settings like mini-batches. This approach addresses the challenge of balancing accuracy and low variance by effectively reducing variance, leading to stabilized and robust MI loss gradients during training and enhancing the convergence of the MI approximation while offering stronger theoretical guarantees for convergence. We explore the application of our estimator in maximizing MI between the data space and the latent space of a variational autoencoder. Experimental results demonstrate significant improvements in convergence over EDF-based methods, with applications across synthetic and real datasets, notably in 3D CT image generation, yielding enhanced structure discovery and reduced overfitting in data synthesis. While this paper focuses on generative models in application, the proposed estimator is not restricted to this setting and can be applied more broadly in various BNP learning procedures. </p>
<blockquote>
<p>äº’ä¿¡æ¯ï¼ˆMIï¼‰æ˜¯è¡¡é‡å˜é‡é—´ä¾èµ–æ€§çš„é‡è¦æŒ‡æ ‡ï¼Œä½†åœ¨é«˜ç»´ç©ºé—´ä¸­å¯¹å…¶è¿›è¡Œç²¾ç¡®è®¡ç®—å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºéš¾ä»¥å¤„ç†çš„å¯èƒ½æ€§ä¼šå½±å“å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚ä¸€ç§æƒ³æ³•æ˜¯ä½¿ç”¨è¾…åŠ©ç¥ç»ç½‘ç»œæ¥è®­ç»ƒMIä¼°è®¡å™¨ï¼›ç„¶è€Œï¼ŒåŸºäºç»éªŒåˆ†å¸ƒå‡½æ•°ï¼ˆEDFï¼‰çš„æ–¹æ³•å¯èƒ½ä¼šåœ¨MIæŸå¤±ä¸­å¼•å…¥æ€¥å‰§æ³¢åŠ¨ï¼Œè¿™æ˜¯ç”±äºæ ·æœ¬å¤–çš„æ€§èƒ½ä¸ä½³å¯¼è‡´æ”¶æ•›ä¸ç¨³å®šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è´å¶æ–¯éå‚æ•°ï¼ˆBNPï¼‰è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ„å»ºMIæŸå¤±å¹¶ä½¿ç”¨ç‹„åˆ©å…‹é›·è¿‡ç¨‹åéªŒçš„æœ‰é™è¡¨ç¤ºæ¥è®­ç»ƒMIä¼°è®¡å™¨ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥æ­£åˆ™åŒ–ã€‚é€šè¿‡æ­¤æ­£åˆ™åŒ–ï¼ŒMIæŸå¤±ç»“åˆäº†å…ˆéªŒçŸ¥è¯†å’Œç»éªŒæ•°æ®ï¼Œå‡å°‘äº†å…¶å¯¹æ ·æœ¬æ•°æ®ä¸­çš„æ³¢åŠ¨å’Œå¼‚å¸¸å€¼çš„æ•æ„Ÿæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬ç¯å¢ƒå¦‚å°æ‰¹é‡æ•°æ®ä¸­ã€‚è¯¥æ–¹æ³•é€šè¿‡æœ‰æ•ˆå‡å°‘æ–¹å·®æ¥è§£å†³å‡†ç¡®æ€§å’Œä½æ–¹å·®çš„å¹³è¡¡æŒ‘æˆ˜ï¼Œä»è€Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°äº†ç¨³å®šä¸”ç¨³å¥çš„MIæŸå¤±æ¢¯åº¦ï¼Œå¢å¼ºäº†MIé€¼è¿‘çš„æ”¶æ•›æ€§ï¼Œå¹¶ä¸ºæ”¶æ•›æä¾›äº†æ›´å¼ºçš„ç†è®ºä¿è¯ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¼°è®¡é‡åœ¨æœ€å¤§åŒ–å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨æ•°æ®ç©ºé—´å’Œæ½œåœ¨ç©ºé—´ä¹‹é—´çš„äº’ä¿¡æ¯ä¸­çš„åº”ç”¨ã€‚å®éªŒç»“æœè¯æ˜ï¼Œä¸åŸºäºEDFçš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ”¶æ•›æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹å–„ï¼Œåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ï¼ˆå°¤å…¶åœ¨3D CTå›¾åƒç”Ÿæˆä¸­ï¼‰çš„åº”ç”¨ä¸­ï¼Œå®ç°äº†æ›´å¥½çš„ç»“æ„å‘ç°å’Œæ•°æ®åˆæˆä¸­çš„è¿‡æ‹Ÿåˆå‡å°‘ã€‚è™½ç„¶æœ¬æ–‡çš„é‡ç‚¹æ˜¯åœ¨åº”ç”¨ä¸­çš„ç”Ÿæˆæ¨¡å‹ï¼Œä½†æ‰€æå‡ºçš„ä¼°è®¡å™¨å¹¶ä¸é™äºè¿™ä¸€ç¯å¢ƒï¼Œå¯ä»¥æ›´å¹¿æ³›åœ°åº”ç”¨äºå„ç§BNPå­¦ä¹ ç¨‹åºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08902v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºè´å¶æ–¯éå‚æ•°ï¼ˆBNPï¼‰çš„äº’ä¿¡æ¯ï¼ˆMIï¼‰ä¼°è®¡å™¨è®­ç»ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆDirichletè¿‡ç¨‹åéªŒçš„æœ‰é™è¡¨ç¤ºæ¥æ„å»ºMIæŸå¤±ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥æ­£åˆ™åŒ–ï¼Œä»¥è§£å†³é«˜ç»´ç©ºé—´ä¸­ç²¾ç¡®è®¡ç®—MIçš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚è¯¥æ–¹æ³•é™ä½äº†MIæŸå¤±å¯¹æ ·æœ¬æ•°æ®æ³¢åŠ¨å’Œå¼‚å¸¸å€¼çš„æ•æ„Ÿæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬ç¯å¢ƒä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ä¼°è®¡å™¨åœ¨ä¼˜åŒ–å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨çš„æ•°æ®ç©ºé—´å’Œæ½œåœ¨ç©ºé—´ä¹‹é—´çš„MIæ—¶ï¼Œç›¸è¾ƒäºåŸºäºç»éªŒåˆ†å¸ƒå‡½æ•°ï¼ˆEDFï¼‰çš„æ–¹æ³•ï¼Œå…·æœ‰æ›´å¥½çš„æ”¶æ•›æ€§ï¼Œå¹¶åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ï¼ˆå¦‚3D CTå›¾åƒç”Ÿæˆï¼‰ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº’ä¿¡æ¯ï¼ˆMIï¼‰æ˜¯æ•æ‰å˜é‡ä¹‹é—´ä¾èµ–æ€§çš„é‡è¦åº¦é‡ï¼Œä½†åœ¨é«˜ç»´ç©ºé—´å’Œå…·æœ‰ä¸å¯é¢„æµ‹æ¦‚ç‡æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå…¶ç²¾ç¡®è®¡ç®—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ä½¿ç”¨è¾…åŠ©ç¥ç»ç½‘ç»œè®­ç»ƒMIä¼°è®¡å™¨æ˜¯ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œä½†åŸºäºç»éªŒåˆ†å¸ƒå‡½æ•°ï¼ˆEDFï¼‰çš„æ–¹æ³•å¯èƒ½å¯¼è‡´MIæŸå¤±å‡ºç°å‰§çƒˆæ³¢åŠ¨ï¼Œä»è€Œå½±å“è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚</li>
<li>æå‡ºäº†åŸºäºè´å¶æ–¯éå‚æ•°ï¼ˆBNPï¼‰çš„MIä¼°è®¡å™¨è®­ç»ƒæ–¹æ¡ˆï¼Œé€šè¿‡ç»“åˆDirichletè¿‡ç¨‹åéªŒè¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»¥æé«˜MIæŸå¤±å¯¹æ ·æœ¬æ•°æ®æ³¢åŠ¨å’Œå¼‚å¸¸å€¼çš„ç¨³å¥æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆé™ä½MIæŸå¤±çš„æ•æ„Ÿæ€§ï¼Œåœ¨å°å‹æ ·æœ¬æ‰¹æ¬¡ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œæœ‰åŠ©äºå¹³è¡¡å‡†ç¡®æ€§å’Œä½æ–¹å·®ã€‚</li>
<li>åœ¨å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨çš„æ•°æ®ç©ºé—´å’Œæ½œåœ¨ç©ºé—´ä¹‹é—´æœ€å¤§åŒ–MIçš„å®éªŒä¸­ï¼Œè¯¥ä¼°è®¡å™¨ç›¸è¾ƒäºEDFæ–¹æ³•å…·æœ‰æ˜¾è‘—æ”¹å–„çš„æ”¶æ•›æ€§ã€‚</li>
<li>è¯¥ä¼°è®¡å™¨åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ï¼ˆå¦‚3D CTå›¾åƒç”Ÿæˆï¼‰ä¸­æœ‰å¹¿æ³›åº”ç”¨ï¼Œèƒ½å¢å¼ºç»“æ„å‘ç°å¹¶å‡å°‘æ•°æ®åˆæˆä¸­çš„è¿‡æ‹Ÿåˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e6f261c7280aaa35daf1c5940b874af1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Deformable-Registration-Framework-for-Augmented-Reality-based-Surgical-Guidance-in-Head-and-Neck-Tumor-Resection"><a href="#Deformable-Registration-Framework-for-Augmented-Reality-based-Surgical-Guidance-in-Head-and-Neck-Tumor-Resection" class="headerlink" title="Deformable Registration Framework for Augmented Reality-based Surgical   Guidance in Head and Neck Tumor Resection"></a>Deformable Registration Framework for Augmented Reality-based Surgical   Guidance in Head and Neck Tumor Resection</h2><p><strong>Authors:Qingyun Yang, Fangjie Li, Jiayi Xu, Zixuan Liu, Sindhura Sridhar, Whitney Jin, Jennifer Du, Jon Heiselman, Michael Miga, Michael Topf, Jie Ying Wu</strong></p>
<p>Head and neck squamous cell carcinoma (HNSCC) has one of the highest rates of recurrence cases among solid malignancies. Recurrence rates can be reduced by improving positive margins localization. Frozen section analysis (FSA) of resected specimens is the gold standard for intraoperative margin assessment. However, because of the complex 3D anatomy and the significant shrinkage of resected specimens, accurate margin relocation from specimen back onto the resection site based on FSA results remains challenging. We propose a novel deformable registration framework that uses both the pre-resection upper surface and the post-resection site of the specimen to incorporate thickness information into the registration process. The proposed method significantly improves target registration error (TRE), demonstrating enhanced adaptability to thicker specimens. In tongue specimens, the proposed framework improved TRE by up to 33% as compared to prior deformable registration. Notably, tongue specimens exhibit complex 3D anatomies and hold the highest clinical significance compared to other head and neck specimens from the buccal and skin. We analyzed distinct deformation behaviors in different specimens, highlighting the need for tailored deformation strategies. To further aid intraoperative visualization, we also integrated this framework with an augmented reality-based auto-alignment system. The combined system can accurately and automatically overlay the deformed 3D specimen mesh with positive margin annotation onto the resection site. With a pilot study of the AR guided framework involving two surgeons, the integrated system improved the surgeonsâ€™ average target relocation error from 9.8 cm to 4.8 cm. </p>
<blockquote>
<p>å¤´é¢ˆé³çŠ¶ç»†èƒç™Œï¼ˆHNSCCï¼‰åœ¨å®ä½“æ¶æ€§è‚¿ç˜¤ä¸­å…·æœ‰æœ€é«˜çš„å¤å‘ç‡ä¹‹ä¸€ã€‚é€šè¿‡æ”¹å–„é˜³æ€§è¾¹ç¼˜å®šä½å¯ä»¥é™ä½å¤å‘ç‡ã€‚åˆ‡é™¤æ ‡æœ¬çš„å†·å†»åˆ‡ç‰‡åˆ†æï¼ˆFSAï¼‰æ˜¯æœ¯ä¸­è¾¹ç¼˜è¯„ä¼°çš„é‡‘æ ‡å‡†ã€‚ç„¶è€Œï¼Œç”±äºå¤æ‚çš„3Dè§£å‰–ç»“æ„å’Œåˆ‡é™¤æ ‡æœ¬çš„æ˜¾è‘—æ”¶ç¼©ï¼Œæ ¹æ®FSAç»“æœä»æ ‡æœ¬é‡æ–°å®šä½åˆ°åˆ‡é™¤éƒ¨ä½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯å˜å½¢æ³¨å†Œæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æœ¯å‰åˆ‡é™¤çš„ä¸Šæ–¹è¡¨é¢å’Œæœ¯åæ ‡æœ¬çš„åˆ‡é™¤éƒ¨ä½ï¼Œå°†åšåº¦ä¿¡æ¯çº³å…¥æ³¨å†Œè¿‡ç¨‹ä¸­ã€‚æ‰€æå‡ºçš„æ–¹æ³•æ˜¾è‘—æé«˜äº†ç›®æ ‡æ³¨å†Œè¯¯å·®ï¼ˆTREï¼‰ï¼Œå¹¶è¡¨ç°å‡ºå¯¹è¾ƒåšæ ‡æœ¬çš„é€‚åº”æ€§å¢å¼ºã€‚ä¸å…ˆå‰çš„å¯å˜å½¢æ³¨å†Œç›¸æ¯”ï¼Œåœ¨èˆŒæ ‡æœ¬ä¸­ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å°†TREæé«˜äº†é«˜è¾¾33%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒèˆŒæ ‡æœ¬å…·æœ‰å¤æ‚çš„3Dè§£å‰–ç»“æ„ï¼Œç›¸è¾ƒäºæ¥è‡ªé¢Šéƒ¨å’Œçš®è‚¤çš„å…¶ä»–å¤´é¢ˆæ ‡æœ¬ï¼Œå…¶ä¸´åºŠæ„ä¹‰æœ€é«˜ã€‚æˆ‘ä»¬åˆ†æäº†ä¸åŒæ ‡æœ¬ä¸­ä¸åŒçš„å˜å½¢è¡Œä¸ºï¼Œå¼ºè°ƒäº†éœ€è¦æœ‰é’ˆå¯¹æ€§çš„å˜å½¢ç­–ç•¥ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¸®åŠ©æœ¯ä¸­å¯è§†åŒ–ï¼Œæˆ‘ä»¬è¿˜å°†è¯¥æ¡†æ¶ä¸åŸºäºå¢å¼ºç°å®çš„è‡ªåŠ¨å¯¹é½ç³»ç»Ÿç›¸ç»“åˆã€‚è¯¥ç»¼åˆç³»ç»Ÿèƒ½å¤Ÿå‡†ç¡®ã€è‡ªåŠ¨åœ°å°†å¸¦æœ‰é˜³æ€§è¾¹ç¼˜æ³¨é‡Šçš„å˜å½¢3Dæ ‡æœ¬ç½‘æ ¼è¦†ç›–åœ¨åˆ‡é™¤éƒ¨ä½ä¸Šã€‚åœ¨æ¶‰åŠä¸¤åå¤–ç§‘åŒ»ç”Ÿçš„ARå¼•å¯¼æ¡†æ¶è¯•ç‚¹ç ”ç©¶ä¸­ï¼Œé›†æˆç³»ç»Ÿé™ä½äº†åŒ»ç”Ÿçš„ç›®æ ‡å¹³å‡é‡æ–°å®šä½è¯¯å·®ï¼Œä»9.8å˜ç±³é™è‡³4.8å˜ç±³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08802v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„å˜å½¢æ³¨å†Œæ¡†æ¶ï¼Œåˆ©ç”¨æœ¯å‰å’Œæœ¯åçš„è¡¨é¢ä¿¡æ¯ï¼Œç»“åˆåšåº¦ä¿¡æ¯ï¼Œå¯¹æ‰‹æœ¯åˆ‡é™¤æ ‡æœ¬è¿›è¡Œå‡†ç¡®çš„è¾¹ç¼˜å®šä½ã€‚è¯¥æ¡†æ¶å¯¹å¤æ‚ä¸‰ç»´è§£å‰–ç»“æ„çš„èˆŒå¤´æ ‡æœ¬æ˜¾ç¤ºå‡ºæ›´é«˜çš„é€‚åº”æ€§ï¼Œæ˜¾è‘—æé«˜äº†ç›®æ ‡æ³¨å†Œè¯¯å·®ï¼ˆTREï¼‰ã€‚åŒæ—¶ï¼Œä¸å¢å¼ºç°å®è‡ªåŠ¨å¯¹é½ç³»ç»Ÿç»“åˆï¼Œå¯è‡ªåŠ¨å°†å˜å½¢åçš„ä¸‰ç»´æ ‡æœ¬ç½‘æ ¼ä¸é˜³æ€§è¾¹ç¼˜æ³¨é‡Šå åŠ åˆ°åˆ‡é™¤éƒ¨ä½ï¼Œæé«˜äº†åŒ»ç”Ÿçš„å®šä½ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤´é¢ˆéƒ¨é³çŠ¶ç»†èƒç™Œï¼ˆHNSCCï¼‰å¤å‘ç‡é«˜ï¼Œæ”¹å–„é˜³æ€§è¾¹ç¼˜å®šä½å¯é™ä½å¤å‘ç‡ã€‚</li>
<li>å†°å†»åˆ‡ç‰‡åˆ†æï¼ˆFSAï¼‰æ˜¯æœ¯ä¸­è¾¹ç¼˜è¯„ä¼°çš„é‡‘æ ‡å‡†ï¼Œä½†æ ‡æœ¬çš„å¤æ‚ä¸‰ç»´è§£å‰–ç»“æ„å’Œæ”¶ç¼©æ€§ä½¿å¾—å‡†ç¡®è¾¹ç¼˜å®šä½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æ–°å‹å˜å½¢æ³¨å†Œæ¡†æ¶ç»“åˆæœ¯å‰å’Œæœ¯åçš„è¡¨é¢ä¿¡æ¯ï¼Œæé«˜äº†ç›®æ ‡æ³¨å†Œè¯¯å·®ï¼ˆTREï¼‰ã€‚</li>
<li>è¯¥æ¡†æ¶å¯¹èˆŒå¤´æ ‡æœ¬çš„å¤æ‚ä¸‰ç»´ç»“æ„é€‚åº”æ€§æ›´å¼ºï¼Œå¯æ˜¾è‘—æé«˜è¾¹ç¼˜å®šä½å‡†ç¡®æ€§ã€‚</li>
<li>æ¡†æ¶ä¸å¢å¼ºç°å®è‡ªåŠ¨å¯¹é½ç³»ç»Ÿç»“åˆï¼Œå¯è¿›ä¸€æ­¥æé«˜åŒ»ç”Ÿå¯¹åˆ‡é™¤éƒ¨ä½çš„å®šä½ç²¾åº¦ã€‚</li>
<li>è¯•ç‚¹ç ”ç©¶è¡¨æ˜ï¼Œè¯¥é›†æˆç³»ç»Ÿå¯å°†åŒ»ç”Ÿçš„å¹³å‡ç›®æ ‡è½¬ç§»è¯¯å·®ä»9.8å˜ç±³å‡å°‘åˆ°4.8å˜ç±³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7fdc575f15fa60287051f6755687122d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a04f213d223c927dc052873df3be866e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-469ba1f5dd0b1ed2e9690310a40716a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a419bf450f65c1a2715ed88419c83068.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="IA-generativa-aplicada-a-la-deteccion-del-cancer-a-traves-de-Resonancia-Magnetica"><a href="#IA-generativa-aplicada-a-la-deteccion-del-cancer-a-traves-de-Resonancia-Magnetica" class="headerlink" title="IA generativa aplicada a la detecciÃ³n del cÃ¡ncer a travÃ©s de   Resonancia MagnÃ©tica"></a>IA generativa aplicada a la detecciÃ³n del cÃ¡ncer a travÃ©s de   Resonancia MagnÃ©tica</h2><p><strong>Authors:Virginia del Campo, Iker Malaina</strong></p>
<p>Cognitive delegation to artificial intelligence (AI) systems is transforming scientific research by enabling the automation of analytical processes and the discovery of new patterns in large datasets. This study examines the ability of AI to complement and expand knowledge in the analysis of breast cancer using dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Building on a previous study, we assess the extent to which AI can generate novel approaches and successfully solve them. For this purpose, AI models, specifically ChatGPT-4o, were used for data preprocessing, hypothesis generation, and the application of clustering techniques, predictive modeling, and correlation network analysis. The results obtained were compared with manually computed outcomes, revealing limitations in process transparency and the accuracy of certain calculations. However, as AI reduces errors and improves reasoning capabilities, an important question arises regarding the future of scientific research: could automation replace the human role in science? This study seeks to open the debate on the methodological and ethical implications of a science dominated by artificial intelligence. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç³»ç»Ÿçš„è®¤çŸ¥ä»£ç†æ­£åœ¨é€šè¿‡å®ç°åˆ†æè¿‡ç¨‹çš„è‡ªåŠ¨åŒ–å’Œåœ¨å¤§æ•°æ®é›†ä¸­å‘ç°æ–°æ¨¡å¼æ¥å˜é©ç§‘å­¦ç ”ç©¶ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨äººå·¥æ™ºèƒ½åœ¨åˆ©ç”¨åŠ¨æ€å¯¹æ¯”å¢å¼ºç£å…±æŒ¯æˆåƒï¼ˆDCE-MRIï¼‰åˆ†æä¹³è…ºç™Œæ—¶è¡¥å……å’Œæ‰©å±•çŸ¥è¯†çš„èƒ½åŠ›ã€‚åŸºäºä¹‹å‰çš„ç ”ç©¶ï¼Œæˆ‘ä»¬è¯„ä¼°äººå·¥æ™ºèƒ½èƒ½å¤Ÿäº§ç”Ÿæ–°æ–¹æ³•å¹¶æˆåŠŸè§£å†³è¿™äº›é—®é¢˜çš„ç¨‹åº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯ChatGPT-4oï¼Œç”¨äºæ•°æ®é¢„å¤„ç†ã€å‡è®¾ç”Ÿæˆä»¥åŠèšç±»æŠ€æœ¯çš„åº”ç”¨ã€é¢„æµ‹å»ºæ¨¡å’Œå…³è”ç½‘ç»œåˆ†æã€‚å°†æ‰€å¾—ç»“æœä¸æ‰‹åŠ¨è®¡ç®—çš„ç»“æœè¿›è¡Œæ¯”è¾ƒï¼Œå‘ç°å…¶åœ¨è¿‡ç¨‹é€æ˜åº¦å’ŒæŸäº›è®¡ç®—çš„å‡†ç¡®æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ç„¶è€Œï¼Œéšç€äººå·¥æ™ºèƒ½å‡å°‘é”™è¯¯å¹¶æ”¹å–„æ¨ç†èƒ½åŠ›ï¼Œä¸€ä¸ªé‡è¦çš„é—®é¢˜å‡ºç°äº†ï¼šåœ¨æœªæ¥ç§‘å­¦ç ”ç©¶ä¸­ï¼Œè‡ªåŠ¨åŒ–èƒ½å¦å–ä»£äººç±»çš„ä½œç”¨ï¼Ÿæœ¬ç ”ç©¶æ—¨åœ¨å°±äººå·¥æ™ºèƒ½ä¸»å¯¼ç§‘å­¦çš„æ–¹æ³•å’Œä¼¦ç†å½±å“å±•å¼€è¾©è®ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08747v1">PDF</a> in Spanish language</p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨åŠ¨æ€å¢å¼ºç£å…±æŒ¯æˆåƒï¼ˆDCE-MRIï¼‰åˆ†æä¹³è…ºç™Œæ–¹é¢çš„èƒ½åŠ›ç ”ç©¶ã€‚ç ”ç©¶å‘ç°AIèƒ½å¤Ÿè¾…åŠ©å¹¶æ‰©å±•çŸ¥è¯†ï¼Œç”Ÿæˆæ–°æ–¹æ³•å¹¶æˆåŠŸè§£å†³æ–°é—®é¢˜ã€‚ä½¿ç”¨AIæ¨¡å‹è¿›è¡Œæ•°æ®å¤„ç†ã€å‡è®¾ç”Ÿæˆå’Œèšç±»æŠ€æœ¯ï¼Œä½†å­˜åœ¨ä¸€å®šè¿‡ç¨‹é€æ˜åº¦å’Œè®¡ç®—å‡†ç¡®æ€§çš„å±€é™ã€‚è¯¥ç ”ç©¶å¼•å‘å…³äºè‡ªåŠ¨åŒ–æ˜¯å¦å¯èƒ½å–ä»£ç§‘å­¦å®¶è§’è‰²çš„è®¨è®ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨DCE-MRIåˆ†æä¹³è…ºç™Œæ–¹é¢çš„èƒ½åŠ›ç ”ç©¶å…·æœ‰è¿›å±•ï¼Œèƒ½è¾…åŠ©å’Œæ‰©å±•çŸ¥è¯†ã€‚</li>
<li>AIé€šè¿‡æ•°æ®å¤„ç†ã€å‡è®¾ç”Ÿæˆå’Œèšç±»æŠ€æœ¯ç­‰æ–¹æ³•ç”¨äºä¹³è…ºç™Œç ”ç©¶ã€‚</li>
<li>AIçš„åº”ç”¨èƒ½å¤Ÿæé«˜è¿‡ç¨‹è‡ªåŠ¨åŒ–ç¨‹åº¦å¹¶å‡å°‘é”™è¯¯ã€‚</li>
<li>ä¸æ‰‹åŠ¨è®¡ç®—ç»“æœç›¸æ¯”ï¼ŒAIå­˜åœ¨è¿‡ç¨‹é€æ˜åº¦å’Œè®¡ç®—å‡†ç¡®æ€§æ–¹é¢çš„å±€é™ã€‚</li>
<li>ç ”ç©¶å¼•å‡ºå…³äºè‡ªåŠ¨åŒ–å¯¹ç§‘å­¦æœªæ¥å½±å“çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯æ–¹æ³•è®ºå’Œä¼¦ç†é—®é¢˜ã€‚</li>
<li>AIåœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„åº”ç”¨å¼•å‘å…³äºäººå·¥æ™ºèƒ½ä¸»å¯¼ç§‘å­¦çš„è®¨è®ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08747">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-54a050c1e97e593b45bf6ae1c1d73e46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2fcb84b7f53530580b2489dae8f5b85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a7b7d9e6e493c7114c31e7ea58cf88f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Posterior-Mean-Denoising-Diffusion-Model-for-Realistic-PET-Image-Reconstruction"><a href="#Posterior-Mean-Denoising-Diffusion-Model-for-Realistic-PET-Image-Reconstruction" class="headerlink" title="Posterior-Mean Denoising Diffusion Model for Realistic PET Image   Reconstruction"></a>Posterior-Mean Denoising Diffusion Model for Realistic PET Image   Reconstruction</h2><p><strong>Authors:Yiran Sun, Osama Mawlawi</strong></p>
<p>Positron Emission Tomography (PET) is a functional imaging modality that enables the visualization of biochemical and physiological processes across various tissues. Recently, deep learning (DL)-based methods have demonstrated significant progress in directly mapping sinograms to PET images. However, regression-based DL models often yield overly smoothed reconstructions lacking of details (i.e., low distortion, low perceptual quality), whereas GAN-based and likelihood-based posterior sampling models tend to introduce undesirable artifacts in predictions (i.e., high distortion, high perceptual quality), limiting their clinical applicability. To achieve a robust perception-distortion tradeoff, we propose Posterior-Mean Denoising Diffusion Model (PMDM-PET), a novel approach that builds upon a recently established mathematical theory to explore the closed-form expression of perception-distortion function in diffusion model space for PET image reconstruction from sinograms. Specifically, PMDM-PET first obtained posterior-mean PET predictions under minimum mean square error (MSE), then optimally transports the distribution of them to the ground-truth PET images distribution. Experimental results demonstrate that PMDM-PET not only generates realistic PET images with possible minimum distortion and optimal perceptual quality but also outperforms five recent state-of-the-art (SOTA) DL baselines in both qualitative visual inspection and quantitative pixel-wise metrics PSNR (dB)&#x2F;SSIM&#x2F;NRMSE. </p>
<blockquote>
<p>æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰æ˜¯ä¸€ç§åŠŸèƒ½æˆåƒæŠ€æœ¯ï¼Œèƒ½å¤Ÿå¯è§†åŒ–å„ç§ç»„ç»‡ä¸­çš„ç”Ÿç‰©åŒ–å­¦å’Œç”Ÿç†è¿‡ç¨‹ã€‚æœ€è¿‘ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰çš„æ–¹æ³•åœ¨ç›´æ¥å°†è¾›æ›²çº¿å›¾æ˜ å°„åˆ°PETå›¾åƒä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼ŒåŸºäºå›å½’çš„DLæ¨¡å‹å¾€å¾€ä¼šäº§ç”Ÿè¿‡äºå¹³æ»‘çš„é‡å»ºç»“æœï¼Œç¼ºä¹ç»†èŠ‚ï¼ˆå³ä½å¤±çœŸã€ä½æ„ŸçŸ¥è´¨é‡ï¼‰ï¼Œè€ŒåŸºäºGANå’ŒåŸºäºä¼¼ç„¶çš„åéªŒé‡‡æ ·æ¨¡å‹åˆ™å¾€å¾€åœ¨é¢„æµ‹ä¸­å¼•å…¥ä¸å¿…è¦çš„ä¼ªå½±ï¼ˆå³é«˜å¤±çœŸã€é«˜æ„ŸçŸ¥è´¨é‡ï¼‰ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠä¸Šçš„é€‚ç”¨æ€§ã€‚ä¸ºäº†å®ç°ç¨³å¥çš„æ„ŸçŸ¥-å¤±çœŸæƒè¡¡ï¼Œæˆ‘ä»¬æå‡ºäº†åéªŒå‡å€¼å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆPMDM-PETï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå®ƒå»ºç«‹åœ¨æœ€è¿‘å»ºç«‹çš„æ•°å­¦ç†è®ºä¸Šï¼Œæ—¨åœ¨æ¢ç´¢æ‰©æ•£æ¨¡å‹ç©ºé—´ä¸­æ„ŸçŸ¥-å¤±çœŸå‡½æ•°çš„å°é—­å½¢å¼è¡¨è¾¾å¼ï¼Œç”¨äºä»è¾›æ›²çº¿å›¾é‡å»ºPETå›¾åƒã€‚å…·ä½“æ¥è¯´ï¼ŒPMDM-PETé¦–å…ˆä»¥æœ€å°å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰è·å¾—åéªŒå‡å€¼PETé¢„æµ‹ï¼Œç„¶åå°†å…¶åˆ†å¸ƒæœ€ä¼˜åœ°ä¼ è¾“åˆ°çœŸå®PETå›¾åƒåˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPMDM-PETä¸ä»…ç”Ÿæˆäº†å…·æœ‰æœ€å°å¯èƒ½å¤±çœŸå’Œæœ€ä½³æ„ŸçŸ¥è´¨é‡çš„ç°å®PETå›¾åƒï¼Œè€Œä¸”åœ¨å®šæ€§å’Œå®šé‡åƒç´ çº§çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ï¼ˆdBï¼‰&#x2F;ç»“æ„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆSSIMï¼‰&#x2F;å½’ä¸€åŒ–å‡æ–¹æ ¹è¯¯å·®ï¼ˆNRMSEï¼‰æ–¹é¢éƒ½ä¼˜äºäº”ç§æœ€æ–°çš„å…ˆè¿›DLåŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08546v1">PDF</a> 12 pages, 2 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„åŒ»å­¦å›¾åƒé‡å»ºæ–¹æ³•â€”â€”åå‡å€¼å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆPMDM-PETï¼‰ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æ‰©æ•£æ¨¡å‹ç©ºé—´å’Œæ„ŸçŸ¥å¤±çœŸå‡½æ•°çš„é—­å¼è¡¨è¾¾å¼ï¼Œä»æ­£å¼¦å›¾é‡å»ºPETå›¾åƒã€‚è¯¥æ–¹æ³•åœ¨æœ€å°å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä¸‹è·å¾—åéªŒå‡å€¼PETé¢„æµ‹ï¼Œç„¶åå°†å…¶æœ€ä¼˜ä¼ è¾“åˆ°çœŸå®PETå›¾åƒåˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPMDM-PETä¸ä»…ç”Ÿæˆäº†å…·æœ‰æœ€å°å¤±çœŸå’Œæœ€ä½³æ„ŸçŸ¥è´¨é‡çš„ç°å®PETå›¾åƒï¼Œè€Œä¸”åœ¨è§†è§‰æ£€æŸ¥å’Œåƒç´ çº§æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºäº”ç§æœ€æ–°çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PETæ˜¯ä¸€ç§åŠŸèƒ½æˆåƒæŠ€æœ¯ï¼Œå¯ä»¥å¯è§†åŒ–å„ç§ç»„ç»‡çš„ç”Ÿç‰©åŒ–å­¦å’Œç”Ÿç†è¿‡ç¨‹ã€‚</li>
<li>åŸºäºæ·±åº¦å­¦ä¹ çš„ç›´æ¥ä»æ­£å¼¦å›¾æ˜ å°„åˆ°PETå›¾åƒçš„æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>å½“å‰æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨PETå›¾åƒé‡å»ºä¸­é¢ä¸´æ„ŸçŸ¥å¤±çœŸå¹³è¡¡é—®é¢˜ã€‚</li>
<li>PMDM-PETæ–¹æ³•ç»“åˆäº†æ‰©æ•£æ¨¡å‹ç©ºé—´å’Œæ„ŸçŸ¥å¤±çœŸå‡½æ•°çš„é—­å¼è¡¨è¾¾å¼æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>PMDM-PETåœ¨æœ€å°å‡æ–¹è¯¯å·®ä¸‹è·å¾—åéªŒå‡å€¼PETé¢„æµ‹ã€‚</li>
<li>PMDM-PETé€šè¿‡æœ€ä¼˜ä¼ è¾“å°†é¢„æµ‹åˆ†å¸ƒæ¥è¿‘çœŸå®PETå›¾åƒåˆ†å¸ƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPMDM-PETåœ¨è§†è§‰æ£€æŸ¥å’Œåƒç´ çº§æŒ‡æ ‡ä¸Šå‡ä¼˜äºå…¶ä»–äº”ç§æœ€æ–°æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6975baec2c1938aebdf2f90c3a7a66c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dc2e9f11edafb6e7d77773500cc4b25.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Layton-Latent-Consistency-Tokenizer-for-1024-pixel-Image-Reconstruction-and-Generation-by-256-Tokens"><a href="#Layton-Latent-Consistency-Tokenizer-for-1024-pixel-Image-Reconstruction-and-Generation-by-256-Tokens" class="headerlink" title="Layton: Latent Consistency Tokenizer for 1024-pixel Image Reconstruction   and Generation by 256 Tokens"></a>Layton: Latent Consistency Tokenizer for 1024-pixel Image Reconstruction   and Generation by 256 Tokens</h2><p><strong>Authors:Qingsong Xie, Zhao Zhang, Zhe Huang, Yanhao Zhang, Haonan Lu, Zhenyu Yang</strong></p>
<p>Image tokenization has significantly advanced visual generation and multimodal modeling, particularly when paired with autoregressive models. However, current methods face challenges in balancing efficiency and fidelity: high-resolution image reconstruction either requires an excessive number of tokens or compromises critical details through token reduction. To resolve this, we propose Latent Consistency Tokenizer (Layton) that bridges discrete visual tokens with the compact latent space of pre-trained Latent Diffusion Models (LDMs), enabling efficient representation of 1024x1024 images using only 256 tokens-a 16 times compression over VQGAN. Layton integrates a transformer encoder, a quantized codebook, and a latent consistency decoder. Direct application of LDM as the decoder results in color and brightness discrepancies. Thus, we convert it to latent consistency decoder, reducing multi-step sampling to 1-2 steps for direct pixel-level supervision. Experiments demonstrate Laytonâ€™s superiority in high-fidelity reconstruction, with 10.8 reconstruction Frechet Inception Distance on MSCOCO-2017 5K benchmark for 1024x1024 image reconstruction. We also extend Layton to a text-to-image generation model, LaytonGen, working in autoregression. It achieves 0.73 score on GenEval benchmark, surpassing current state-of-the-art methods. Project homepage: <a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/Layton">https://github.com/OPPO-Mente-Lab/Layton</a> </p>
<blockquote>
<p>å›¾åƒåˆ†è¯æŠ€æœ¯å·²åœ¨è§†è§‰ç”Ÿæˆå’Œå¤šæ¨¡æ€å»ºæ¨¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯ä¸è‡ªå›å½’æ¨¡å‹ç»“åˆä½¿ç”¨æ—¶ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•åœ¨å¹³è¡¡æ•ˆç‡å’Œä¿çœŸåº¦æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼šé«˜åˆ†è¾¨ç‡å›¾åƒé‡å»ºéœ€è¦è¿‡å¤šçš„ä»¤ç‰Œï¼Œæˆ–è€…åœ¨ä»¤ç‰Œå‡å°‘æ—¶æŸå¤±é‡è¦ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨ä¸€è‡´æ€§åˆ†è¯å™¨ï¼ˆLaytonï¼‰ï¼Œå®ƒæ¶èµ·äº†ç¦»æ•£è§†è§‰ä»¤ç‰Œå’Œé¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ç´§å‡‘æ½œåœ¨ç©ºé—´ä¹‹é—´çš„æ¡¥æ¢ï¼Œèƒ½å¤Ÿä»…ä½¿ç”¨256ä¸ªä»¤ç‰Œæœ‰æ•ˆåœ°è¡¨ç¤º1024x1024çš„å›¾åƒï¼Œè¿™æ˜¯VQGANçš„16å€å‹ç¼©ã€‚Laytoné›†æˆäº†å˜å‹å™¨ç¼–ç å™¨ã€é‡åŒ–ä»£ç æœ¬å’Œæ½œåœ¨ä¸€è‡´æ€§è§£ç å™¨ã€‚ç›´æ¥ä½¿ç”¨LDMä½œä¸ºè§£ç å™¨ä¼šå¯¼è‡´é¢œè‰²å’Œäº®åº¦å·®å¼‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å…¶è½¬æ¢ä¸ºæ½œåœ¨ä¸€è‡´æ€§è§£ç å™¨ï¼Œå°†å¤šæ­¥é‡‡æ ·å‡å°‘åˆ°1-2æ­¥ï¼Œå®ç°ç›´æ¥åƒç´ çº§ç›‘ç£ã€‚å®éªŒè¡¨æ˜ï¼ŒLaytonåœ¨é«˜ä¿çœŸé‡å»ºæ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œåœ¨MSCOCO-2017 5KåŸºå‡†æµ‹è¯•ä¸Šï¼Œ1024x1024å›¾åƒé‡å»ºçš„Frechet Inception Distanceä¸º10.8ã€‚æˆ‘ä»¬è¿˜æŠŠLaytonæ‰©å±•åˆ°è‡ªå›å½’çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹LaytonGenã€‚å®ƒåœ¨GenEvalåŸºå‡†æµ‹è¯•ä¸Šè·å¾—äº†0.73çš„åˆ†æ•°ï¼Œè¶…è¶Šäº†å½“å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚é¡¹ç›®ä¸»é¡µï¼š<a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/Layton">https://github.com/OPPO-Mente-Lab/Layton</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08377v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å›¾åƒä»¤ç‰ŒåŒ–æŠ€æœ¯åœ¨è§†è§‰ç”Ÿæˆå’Œå¤šæ¨¡å¼å»ºæ¨¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯ä¸è‡ªå›å½’æ¨¡å‹ç»“åˆæ—¶ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•åœ¨æ•ˆç‡å’Œä¿çœŸåº¦ä¹‹é—´é¢ä¸´å¹³è¡¡æŒ‘æˆ˜ï¼šé«˜åˆ†è¾¨ç‡å›¾åƒé‡å»ºéœ€è¦å¤§é‡ä»¤ç‰Œæˆ–é€šè¿‡åœ¨ä»¤ç‰Œå‡å°‘ä¸­ç‰ºç‰²å…³é”®ç»†èŠ‚ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨ä¸€è‡´æ€§ä»¤ç‰ŒåŒ–å™¨ï¼ˆLaytonï¼‰ï¼Œå®ƒæ¡¥æ¥äº†ç¦»æ•£è§†è§‰ä»¤ç‰Œå’Œé¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„ç´§å‡‘æ½œåœ¨ç©ºé—´ï¼Œä»…ä½¿ç”¨256ä¸ªä»¤ç‰Œå°±èƒ½æœ‰æ•ˆåœ°è¡¨ç¤º1024x1024çš„å›¾åƒï¼Œè¿™æ˜¯VQGANçš„16å€å‹ç¼©ã€‚Laytoné›†æˆäº†å˜å‹å™¨ç¼–ç å™¨ã€é‡åŒ–ä»£ç æœ¬å’Œæ½œåœ¨ä¸€è‡´æ€§è§£ç å™¨ã€‚ç›´æ¥ä½¿ç”¨LDMä½œä¸ºè§£ç å™¨ä¼šå¯¼è‡´é¢œè‰²å’Œäº®åº¦å·®å¼‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å…¶è½¬æ¢ä¸ºæ½œåœ¨ä¸€è‡´æ€§è§£ç å™¨ï¼Œå°†å¤šæ­¥é‡‡æ ·å‡å°‘åˆ°1-2æ­¥ï¼Œä»¥å®ç°ç›´æ¥åƒç´ çº§ç›‘ç£ã€‚å®éªŒè¯æ˜ï¼ŒLaytonåœ¨é«˜ä¿çœŸé‡å»ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåœ¨MSCOCO-2017 5KåŸºå‡†æµ‹è¯•ä¸Šï¼Œ1024x1024å›¾åƒé‡å»ºçš„Frechet Inception Distanceä¸º10.8ã€‚æˆ‘ä»¬è¿˜æ‰©å±•äº†Laytonåˆ°è‡ªå›å½’çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹LaytonGenï¼Œå…¶åœ¨GenEvalåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†0.73çš„åˆ†æ•°ï¼Œè¶…è¶Šäº†å½“å‰å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒä»¤ç‰ŒåŒ–æŠ€æœ¯åœ¨è§†è§‰ç”Ÿæˆå’Œå¤šæ¨¡å¼å»ºæ¨¡æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†æ•ˆç‡å’Œä¿çœŸåº¦ä¹‹é—´ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>Laytoné€šè¿‡ç»“åˆç¦»æ•£è§†è§‰ä»¤ç‰Œå’Œé¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ç´§å‡‘æ½œåœ¨ç©ºé—´ï¼Œå®ç°äº†é«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡å›¾åƒè¡¨ç¤ºã€‚</li>
<li>Laytonä½¿ç”¨å˜å‹å™¨ç¼–ç å™¨ã€é‡åŒ–ä»£ç æœ¬å’Œæ½œåœ¨ä¸€è‡´æ€§è§£ç å™¨ï¼Œæé«˜äº†å›¾åƒé‡å»ºçš„ä¿çœŸåº¦ã€‚</li>
<li>ç›´æ¥åº”ç”¨LDMä½œä¸ºè§£ç å™¨ä¼šå¯¼è‡´é¢œè‰²å’Œäº®åº¦å·®å¼‚ï¼Œå› æ­¤è¿›è¡Œäº†ç›¸åº”çš„ä¼˜åŒ–ã€‚</li>
<li>Laytonåœ¨MSCOCO-2017 5KåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„é«˜åˆ†è¾¨ç‡å›¾åƒé‡å»ºæ€§èƒ½ã€‚</li>
<li>LaytonGenä½œä¸ºè‡ªå›å½’çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œåœ¨GenEvalåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†é¢†å…ˆçš„åˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69b7e34fc6d968c74d5178a841ea11da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5983764aa67dbfff512cd7f3a550fd00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9f668a19a7e7392f2ebc263eec6016d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d029e28a78b68b9363f4cbd06ae70e4.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="3D-Medical-Imaging-Segmentation-on-Non-Contrast-CT"><a href="#3D-Medical-Imaging-Segmentation-on-Non-Contrast-CT" class="headerlink" title="3D Medical Imaging Segmentation on Non-Contrast CT"></a>3D Medical Imaging Segmentation on Non-Contrast CT</h2><p><strong>Authors:Canxuan Gang, Yuhan Peng</strong></p>
<p>This technical report analyzes non-contrast CT image segmentation in computer vision. It revisits a proposed method, examines the background of non-contrast CT imaging, and highlights the significance of segmentation. The study reviews representative methods, including convolutional-based and CNN-Transformer hybrid approaches, discussing their contributions, advantages, and limitations. The nnUNet stands out as the state-of-the-art method across various segmentation tasks. The report explores the relationship between the proposed method and existing approaches, emphasizing the role of global context modeling in semantic labeling and mask generation. Future directions include addressing the long-tail problem, utilizing pre-trained models for medical imaging, and exploring self-supervised or contrastive pre-training techniques. This report offers insights into non-contrast CT image segmentation and potential advancements in the field. </p>
<blockquote>
<p>æœ¬æŠ€æœ¯æŠ¥å‘Šå¯¹è®¡ç®—æœºè§†è§‰ä¸­çš„éå¯¹æ¯”CTå›¾åƒåˆ†å‰²è¿›è¡Œäº†åˆ†æã€‚å®ƒå›é¡¾äº†ä¸€ç§æå‡ºçš„æ–¹æ³•ï¼Œæ¢è®¨äº†éå¯¹æ¯”CTæˆåƒçš„èƒŒæ™¯ï¼Œå¹¶å¼ºè°ƒäº†åˆ†å‰²çš„é‡è¦æ€§ã€‚è¯¥ç ”ç©¶è¯„è¿°äº†å…·æœ‰ä»£è¡¨æ€§çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºå·ç§¯å’ŒCNN-Transformeræ··åˆæ–¹æ³•ï¼Œè®¨è®ºäº†å…¶è´¡çŒ®ã€ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚åœ¨å„ç§åˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒnnUNetè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æŠ¥å‘Šæ¢è®¨äº†æ‰€æå‡ºæ–¹æ³•ä¸ç°æœ‰æ–¹æ³•ä¹‹é—´çš„å…³ç³»ï¼Œé‡ç‚¹å¼ºè°ƒäº†å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡åœ¨è¯­ä¹‰æ ‡æ³¨å’Œæ©è†œç”Ÿæˆä¸­çš„ä½œç”¨ã€‚æœªæ¥çš„ç ”ç©¶æ–¹å‘åŒ…æ‹¬è§£å†³é•¿å°¾é—®é¢˜ã€åˆ©ç”¨åŒ»å­¦æˆåƒçš„é¢„è®­ç»ƒæ¨¡å‹å’Œæ¢ç´¢è‡ªç›‘ç£æˆ–å¯¹æ¯”é¢„è®­ç»ƒæŠ€æœ¯ã€‚æœ¬æŠ¥å‘Šä¸ºæ·±å…¥äº†è§£éå¯¹æ¯”CTå›¾åƒåˆ†å‰²ä»¥åŠè¯¥é¢†åŸŸçš„æ½œåœ¨è¿›å±•æä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08361v1">PDF</a> tech report</p>
<p><strong>Summary</strong><br>     æœ¬æŠ¥å‘Šåˆ†æäº†è®¡ç®—æœºè§†è§‰ä¸­éå¯¹æ¯”CTå›¾åƒåˆ†å‰²æŠ€æœ¯ï¼Œå›é¡¾äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œä»‹ç»äº†éå¯¹æ¯”CTæˆåƒçš„èƒŒæ™¯ï¼Œå¹¶å¼ºè°ƒäº†åˆ†å‰²çš„é‡è¦æ€§ã€‚æŠ¥å‘Šè¯„è¿°äº†ä»£è¡¨æ€§æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºå·ç§¯å’ŒCNN-Transformeræ··åˆæ–¹æ³•ï¼Œè®¨è®ºäº†å…¶è´¡çŒ®ã€ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚nnUNetåœ¨ä¸åŒåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æŠ¥å‘Šæ¢è®¨äº†æ–°æ–¹æ³•ä¸ç°æœ‰æ–¹æ³•ä¹‹é—´çš„å…³ç³»ï¼Œå¼ºè°ƒäº†å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡åœ¨è¯­ä¹‰æ ‡ç­¾å’Œæ©è†œç”Ÿæˆä¸­çš„ä½œç”¨ã€‚æœªæ¥æ–¹å‘åŒ…æ‹¬è§£å†³é•¿å°¾é—®é¢˜ã€åˆ©ç”¨åŒ»å­¦å›¾åƒé¢„è®­ç»ƒæ¨¡å‹å’Œæ¢ç´¢è‡ªç›‘ç£æˆ–å¯¹æ¯”é¢„è®­ç»ƒæŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŠ¥å‘Šä»‹ç»äº†éå¯¹æ¯”CTå›¾åƒåˆ†å‰²æŠ€æœ¯çš„èƒŒæ™¯å’Œé‡è¦æ€§ã€‚</li>
<li>åˆ†æäº†å·ç§¯åŸºå’ŒCNN-Transformeræ··åˆæ–¹æ³•ä¸ºä»£è¡¨çš„åˆ†å‰²æŠ€æœ¯åŠå…¶è´¡çŒ®ã€ä¼˜åŠ¿å’Œå±€é™ã€‚</li>
<li>nnUNetåœ¨å„ç§åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>æŠ¥å‘Šå¼ºè°ƒäº†å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡åœ¨è¯­ä¹‰æ ‡ç­¾å’Œæ©è†œç”Ÿæˆä¸­çš„ä½œç”¨ã€‚</li>
<li>æåˆ°äº†ç°æœ‰ç ”ç©¶çš„ä¸è¶³å’Œæœªæ¥ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬è§£å†³é•¿å°¾é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒåŒ»å­¦æˆåƒå’Œè‡ªç›‘ç£æˆ–å¯¹æ¯”é¢„è®­ç»ƒæŠ€æœ¯æ˜¯æœªæ¥çš„ç ”ç©¶è¶‹åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08361">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c8c339d9496569be030dc65331a3db6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68b7cf1b5231ff534378cd434c4810e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-584c5d8d58123ad243a473d1751b1daf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87f4aabcc84df688423216c137bf41f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8921296df911b7fd947e30334e6d8ffb.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Design-and-Implementation-of-FourCropNet-A-CNN-Based-System-for-Efficient-Multi-Crop-Disease-Detection-and-Management"><a href="#Design-and-Implementation-of-FourCropNet-A-CNN-Based-System-for-Efficient-Multi-Crop-Disease-Detection-and-Management" class="headerlink" title="Design and Implementation of FourCropNet: A CNN-Based System for   Efficient Multi-Crop Disease Detection and Management"></a>Design and Implementation of FourCropNet: A CNN-Based System for   Efficient Multi-Crop Disease Detection and Management</h2><p><strong>Authors:H. P. Khandagale, Sangram Patil, V. S. Gavali, S. V. Chavan, P. P. Halkarnikar, Prateek A. Meshram</strong></p>
<p>Plant disease detection is a critical task in agriculture, directly impacting crop yield, food security, and sustainable farming practices. This study proposes FourCropNet, a novel deep learning model designed to detect diseases in multiple crops, including CottonLeaf, Grape, Soybean, and Corn. The model leverages an advanced architecture comprising residual blocks for efficient feature extraction, attention mechanisms to enhance focus on disease-relevant regions, and lightweight layers for computational efficiency. These components collectively enable FourCropNet to achieve superior performance across varying datasets and class complexities, from single-crop datasets to combined datasets with 15 classes. The proposed model was evaluated on diverse datasets, demonstrating high accuracy, specificity, sensitivity, and F1 scores. Notably, FourCropNet achieved the highest accuracy of 99.7% for Grape, 99.5% for Corn, and 95.3% for the combined dataset. Its scalability and ability to generalize across datasets underscore its robustness. Comparative analysis shows that FourCropNet consistently outperforms state-of-the-art models such as MobileNet, VGG16, and EfficientNet across various metrics. FourCropNetâ€™s innovative design and consistent performance make it a reliable solution for real-time disease detection in agriculture. This model has the potential to assist farmers in timely disease diagnosis, reducing economic losses and promoting sustainable agricultural practices. </p>
<blockquote>
<p>æ¤ç‰©ç—…å®³æ£€æµ‹æ˜¯å†œä¸šä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œç›´æ¥å½±å“ä½œç‰©äº§é‡ã€ç²®é£Ÿå®‰å…¨å’Œå¯æŒç»­å†œä¸šå®è·µã€‚æœ¬ç ”ç©¶æå‡ºäº†FourCropNetï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨æ£€æµ‹å¤šç§ä½œç‰©ï¼ˆåŒ…æ‹¬æ£‰èŠ±å¶ã€è‘¡è„ã€å¤§è±†å’Œç‰ç±³ï¼‰çš„ç—…å®³ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å…ˆè¿›çš„æ¶æ„ï¼ŒåŒ…æ‹¬æ®‹å·®å—è¿›è¡Œé«˜æ•ˆç‰¹å¾æå–ã€æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºå¯¹ç—…å®³ç›¸å…³åŒºåŸŸçš„å…³æ³¨ï¼Œä»¥åŠè½»è´¨å±‚æé«˜è®¡ç®—æ•ˆç‡ã€‚è¿™äº›ç»„ä»¶å…±åŒä½¿FourCropNetèƒ½å¤Ÿåœ¨ä¸åŒçš„æ•°æ®é›†å’Œç±»åˆ«å¤æ‚åº¦ä¸Šå®ç°å“è¶Šæ€§èƒ½ï¼Œä»å•ä¸€ä½œç‰©æ•°æ®é›†åˆ°åŒ…å«15ä¸ªç±»åˆ«çš„ç»„åˆæ•°æ®é›†ã€‚æ‰€æå‡ºæ¨¡å‹åœ¨å¤šç§æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¡¨ç°å‡ºé«˜å‡†ç¡®æ€§ã€ç‰¹å¼‚æ€§ã€æ•æ„Ÿæ€§å’ŒF1åˆ†æ•°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒFourCropNetåœ¨è‘¡è„ä¸Šè¾¾åˆ°äº†æœ€é«˜çš„99.7%å‡†ç¡®ç‡ï¼Œåœ¨ç‰ç±³ä¸Šè¾¾åˆ°äº†99.5%çš„å‡†ç¡®ç‡ï¼Œä»¥åŠåœ¨ç»„åˆæ•°æ®é›†ä¸Šè¾¾åˆ°äº†95.3%çš„å‡†ç¡®ç‡ã€‚å…¶å¯æ‰©å±•æ€§å’Œè·¨æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›è¯æ˜äº†å…¶ç¨³å¥æ€§ã€‚æ¯”è¾ƒåˆ†æè¡¨æ˜ï¼Œåœ¨å„ç§æŒ‡æ ‡ä¸Šï¼ŒFourCropNetå§‹ç»ˆä¼˜äºæœ€æ–°æ¨¡å‹ï¼ˆå¦‚MobileNetã€VGG16å’ŒEfficientNetï¼‰ã€‚FourCropNetçš„åˆ›æ–°è®¾è®¡å’ŒæŒç»­æ€§èƒ½ä½¿å…¶æˆä¸ºå†œä¸šå®æ—¶ç—…å®³æ£€æµ‹çš„å¯é è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¨¡å‹æœ‰å¯èƒ½å¸®åŠ©å†œæ°‘åŠæ—¶è¯Šæ–­ç—…å®³ï¼Œå‡å°‘ç»æµæŸå¤±ï¼Œä¿ƒè¿›å¯æŒç»­å†œä¸šå®è·µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08348v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹æ·±åº¦å­¦ä¹ æ¨¡å‹FourCropNetï¼Œå¯ç”¨äºæ£€æµ‹å¤šç§ä½œç‰©ç–¾ç—…ï¼ŒåŒ…æ‹¬æ£‰èŠ±å¶ç—…ã€è‘¡è„ç—…ã€å¤§è±†ç—…å’Œç‰ç±³ç—…ç­‰ã€‚è¯¥æ¨¡å‹ä½¿ç”¨å…ˆè¿›çš„æ¶æ„ï¼ŒåŒ…æ‹¬æ®‹å·®å—è¿›è¡Œé«˜æ•ˆç‰¹å¾æå–ã€æ³¨æ„åŠ›æœºåˆ¶ä»¥å¢å¼ºå¯¹ç–¾ç—…ç›¸å…³åŒºåŸŸçš„å…³æ³¨ï¼Œä»¥åŠè½»é‡çº§å±‚ä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚FourCropNetåœ¨å¤šä¸ªæ•°æ®é›†å’Œç±»åˆ«å¤æ‚åº¦ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å•ä¸€ä½œç‰©æ•°æ®é›†å’ŒåŒ…å«15ä¸ªç±»åˆ«çš„ç»„åˆæ•°æ®é›†ã€‚è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è‘¡è„ã€ç‰ç±³å’Œç»„åˆæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†99.7%ã€99.5%å’Œ95.3%ã€‚ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ¨¡å‹ï¼ŒFourCropNetåœ¨å„é¡¹æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¼˜ç§€ï¼Œæ˜¯å†œä¸šå®æ—¶ç–¾ç—…æ£€æµ‹çš„å¯é è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FourCropNetæ˜¯ä¸€ç§ç”¨äºæ£€æµ‹å¤šç§ä½œç‰©ç–¾ç—…çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹ä½¿ç”¨æ®‹å·®å—è¿›è¡Œç‰¹å¾æå–ï¼Œæ³¨æ„åŠ›æœºåˆ¶å¢å¼ºå¯¹ç–¾ç—…ç›¸å…³åŒºåŸŸçš„å…³æ³¨ï¼Œè½»é‡çº§å±‚æé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li>FourCropNetåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†é«˜å‡†ç¡®ç‡ã€ç‰¹å¼‚æ€§ã€æ•æ„Ÿæ€§å’ŒF1åˆ†æ•°ã€‚</li>
<li>æ¨¡å‹åœ¨è‘¡è„ã€ç‰ç±³å’Œç»„åˆæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†99.7%ã€99.5%å’Œ95.3%ã€‚</li>
<li>FourCropNetç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ¨¡å‹è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>FourCropNetå…·æœ‰å¯æ‰©å±•æ€§å’Œè·¨æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08348">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-76aaa1ab5209b63100a7e9fb22f7a61d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5e1620972681a98a189c8922ec07854.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e7a78b1ffafba108a27e37a6c835569.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-777e36e93c27a6bfea6bcd0ea35480a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7eb4460523394afcd24a19f3f4b59569.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fab60e34e21e47e6e4657586a0bfb152.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Pathology-Aware-Adaptive-Watermarking-for-Text-Driven-Medical-Image-Synthesis"><a href="#Pathology-Aware-Adaptive-Watermarking-for-Text-Driven-Medical-Image-Synthesis" class="headerlink" title="Pathology-Aware Adaptive Watermarking for Text-Driven Medical Image   Synthesis"></a>Pathology-Aware Adaptive Watermarking for Text-Driven Medical Image   Synthesis</h2><p><strong>Authors:Chanyoung Kim, Dayun Ju, Jinyeong Kim, Woojung Han, Roberto Alcover-Couso, Seong Jae Hwang</strong></p>
<p>As recent text-conditioned diffusion models have enabled the generation of high-quality images, concerns over their potential misuse have also grown. This issue is critical in the medical domain, where text-conditioned generated medical images could enable insurance fraud or falsified records, highlighting the urgent need for reliable safeguards against unethical use. While watermarking techniques have emerged as a promising solution in general image domains, their direct application to medical imaging presents significant challenges. A key challenge is preserving fine-grained disease manifestations, as even minor distortions from a watermark may lead to clinical misinterpretation, which compromises diagnostic integrity. To overcome this gap, we present MedSign, a deep learning-based watermarking framework specifically designed for text-to-medical image synthesis, which preserves pathologically significant regions by adaptively adjusting watermark strength. Specifically, we generate a pathology localization map using cross-attention between medical text tokens and the diffusion denoising network, aggregating token-wise attention across layers, heads, and time steps. Leveraging this map, we optimize the LDM decoder to incorporate watermarking during image synthesis, ensuring cohesive integration while minimizing interference in diagnostically critical regions. Experimental results show that our MedSign preserves diagnostic integrity while ensuring watermark robustness, achieving state-of-the-art performance in image quality and detection accuracy on MIMIC-CXR and OIA-ODIR datasets. </p>
<blockquote>
<p>éšç€è¿‘æœŸæ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œå¯¹å…¶æ½œåœ¨è¯¯ç”¨çš„æ‹…å¿§ä¹Ÿåœ¨å¢é•¿ã€‚åœ¨åŒ»å­¦é¢†åŸŸï¼Œè¿™ä¸ªé—®é¢˜å°¤ä¸ºå…³é”®ï¼Œå› ä¸ºæ–‡æœ¬æ¡ä»¶ç”Ÿæˆçš„åŒ»å­¦å›¾åƒå¯èƒ½å¯¼è‡´ä¿é™©æ¬ºè¯ˆæˆ–è™šå‡è®°å½•ï¼Œè¿™å‡¸æ˜¾äº†å¯¹é˜²æ­¢ä¸é“å¾·ä½¿ç”¨çš„å¯é ä¿éšœæªæ–½çš„è¿«åˆ‡éœ€æ±‚ã€‚è™½ç„¶æ°´å°æŠ€æœ¯å·²ä½œä¸ºé€šç”¨å›¾åƒé¢†åŸŸçš„ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆå‡ºç°ï¼Œä½†å®ƒä»¬ç›´æ¥åº”ç”¨äºåŒ»å­¦æˆåƒå´é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯ä¿æŒç»†å¾®çš„ç–¾ç—…è¡¨ç°ï¼Œå› ä¸ºæ°´å°çš„å¾®å°å¤±çœŸä¹Ÿå¯èƒ½å¯¼è‡´ä¸´åºŠè¯¯è§£ï¼Œä»è€ŒæŸå®³è¯Šæ–­çš„å®Œæ•´æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MedSignï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„æ°´å°æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºæ–‡æœ¬åˆ°åŒ»å­¦å›¾åƒåˆæˆã€‚é€šè¿‡è‡ªé€‚åº”è°ƒæ•´æ°´å°å¼ºåº¦æ¥ä¿æŒç—…ç†æ„ä¹‰åŒºåŸŸã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨åŒ»å­¦æ–‡æœ¬æ ‡è®°å’Œæ‰©æ•£å»å™ªç½‘ç»œä¹‹é—´çš„äº¤å‰æ³¨æ„åŠ›ç”Ÿæˆç—…ç†å®šä½å›¾ï¼Œè·¨å±‚ã€å¤´å’Œæ—¶é—´æ­¥é•¿èšåˆæ ‡è®°å¼æ³¨æ„åŠ›ã€‚åˆ©ç”¨æ­¤å›¾ï¼Œæˆ‘ä»¬ä¼˜åŒ–LDMè§£ç å™¨ä»¥åœ¨å›¾åƒåˆæˆè¿‡ç¨‹ä¸­èå…¥æ°´å°ï¼Œç¡®ä¿æ— ç¼é›†æˆï¼ŒåŒæ—¶æœ€å°åŒ–å¯¹è¯Šæ–­å…³é”®åŒºåŸŸçš„å¹²æ‰°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MedSignèƒ½å¤Ÿä¿æŒè¯Šæ–­çš„å®Œæ•´æ€§ï¼ŒåŒæ—¶ç¡®ä¿æ°´å°çš„ç¨³å¥æ€§ï¼Œåœ¨MIMIC-CXRå’ŒOIA-ODIRæ•°æ®é›†ä¸Šå®ç°äº†å›¾åƒè´¨é‡å’Œæ£€æµ‹ç²¾åº¦çš„æœ€ä½³æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08346v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬é©±åŠ¨çš„æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†åœ¨åŒ»ç–—é¢†åŸŸï¼Œè¿™ç§æŠ€æœ¯å¯èƒ½è¢«ç”¨äºä¿é™©æ¬ºè¯ˆæˆ–ä¼ªé€ è®°å½•ç­‰ä¸é“å¾·è¡Œä¸ºï¼Œå› æ­¤è¿«åˆ‡éœ€è¦å¯é çš„ä¿éšœæªæ–½ã€‚é’ˆå¯¹æ°´å°æŠ€æœ¯åœ¨åŒ»å­¦å½±åƒé¢†åŸŸçš„åº”ç”¨å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œå¦‚ç–¾ç—…ç»†å¾®è¡¨ç°çš„å¤±çœŸå¯èƒ½å¯¼è‡´ä¸´åºŠè¯¯åˆ¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ°´å°æ¡†æ¶MedSignã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªé€‚åº”è°ƒæ•´æ°´å°å¼ºåº¦ï¼Œä¿æŠ¤ç—…ç†é‡è¦åŒºåŸŸï¼Œå¹¶åˆ©ç”¨åŒ»å­¦æ–‡æœ¬æ ‡è®°å’Œæ‰©æ•£å»å™ªç½‘ç»œçš„äº¤å‰æ³¨æ„åŠ›ç”Ÿæˆç—…ç†å®šä½å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMedSignåœ¨ä¿æŒè¯Šæ–­å®Œæ•´æ€§çš„åŒæ—¶ï¼Œç¡®ä¿äº†æ°´å°çš„ç¨³å¥æ€§ï¼Œåœ¨MIMIC-CXRå’ŒOIA-ODIRæ•°æ®é›†ä¸Šå®ç°äº†å›¾åƒè´¨é‡å’Œæ£€æµ‹ç²¾åº¦çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬é©±åŠ¨çš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„åŒ»ç–—å›¾åƒï¼Œä½†å­˜åœ¨æ½œåœ¨è¯¯ç”¨é£é™©ï¼Œéœ€é‡‡å–æªæ–½é˜²æ­¢ä¸é“å¾·ä½¿ç”¨ã€‚</li>
<li>æ°´å°æŠ€æœ¯åœ¨åŒ»å­¦å½±åƒé¢†åŸŸçš„åº”ç”¨é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ä¿æŠ¤ç—…ç†ç»†å¾®è¡¨ç°ã€é¿å…ä¸´åºŠè¯¯åˆ¤ã€‚</li>
<li>MedSignæ¡†æ¶æ˜¯ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ°´å°è§£å†³æ–¹æ¡ˆï¼Œä¸“ä¸ºæ–‡æœ¬åˆ°åŒ»ç–—å›¾åƒçš„åˆæˆè®¾è®¡ã€‚</li>
<li>MedSigné€šè¿‡è‡ªé€‚åº”è°ƒæ•´æ°´å°å¼ºåº¦ä¿æŠ¤ç—…ç†é‡è¦åŒºåŸŸã€‚</li>
<li>MedSignåˆ©ç”¨åŒ»å­¦æ–‡æœ¬æ ‡è®°å’Œæ‰©æ•£å»å™ªç½‘ç»œçš„äº¤å‰æ³¨æ„åŠ›ç”Ÿæˆç—…ç†å®šä½å›¾ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒMedSignåœ¨ä¿æŒè¯Šæ–­å®Œæ•´æ€§çš„åŒæ—¶ï¼Œç¡®ä¿äº†æ°´å°çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08346">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7c978352e6e72289a51836eca17bccd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b851854325e8a6a0ce2c0f75b29eaae5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd1c6401a643f5d944fc1883bddb2867.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66d087b6392d628296f4412c940b329f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SegDesicNet-Lightweight-Semantic-Segmentation-in-Remote-Sensing-with-Geo-Coordinate-Embeddings-for-Domain-Adaptation"><a href="#SegDesicNet-Lightweight-Semantic-Segmentation-in-Remote-Sensing-with-Geo-Coordinate-Embeddings-for-Domain-Adaptation" class="headerlink" title="SegDesicNet: Lightweight Semantic Segmentation in Remote Sensing with   Geo-Coordinate Embeddings for Domain Adaptation"></a>SegDesicNet: Lightweight Semantic Segmentation in Remote Sensing with   Geo-Coordinate Embeddings for Domain Adaptation</h2><p><strong>Authors:Sachin Verma, Frank Lindseth, Gabriel Kiss</strong></p>
<p>Semantic segmentation is essential for analyzing highdefinition remote sensing images (HRSIs) because it allows the precise classification of objects and regions at the pixel level. However, remote sensing data present challenges owing to geographical location, weather, and environmental variations, making it difficult for semantic segmentation models to generalize across diverse scenarios. Existing methods are often limited to specific data domains and require expert annotators and specialized equipment for semantic labeling. In this study, we propose a novel unsupervised domain adaptation technique for remote sensing semantic segmentation by utilizing geographical coordinates that are readily accessible in remote sensing setups as metadata in a dataset. To bridge the domain gap, we propose a novel approach that considers the combination of an image&#39;s location encoding trait and the spherical nature of Earth&#39;s surface. Our proposed SegDesicNet module regresses the GRID positional encoding of the geo coordinates projected over the unit sphere to obtain the domain loss. Our experimental results demonstrate that the proposed SegDesicNet outperforms state of the art domain adaptation methods in remote sensing image segmentation, achieving an improvement of approximately ~6% in the mean intersection over union (MIoU) with a ~ 27% drop in parameter count on benchmarked subsets of the publicly available FLAIR #1 dataset. We also benchmarked our method performance on the custom split of the ISPRS Potsdam dataset. Our algorithm seeks to reduce the modeling disparity between artificial neural networks and human comprehension of the physical world, making the technology more human centric and scalable. </p>
<blockquote>
<p>è¯­ä¹‰åˆ†å‰²å¯¹äºåˆ†æé«˜åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒï¼ˆHRSIsï¼‰è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿåœ¨åƒç´ çº§åˆ«å¯¹å¯¹è±¡å’ŒåŒºåŸŸè¿›è¡Œç²¾ç¡®åˆ†ç±»ã€‚ç„¶è€Œï¼Œç”±äºåœ°ç†ä½ç½®ã€å¤©æ°”å’Œç¯å¢ƒå˜åŒ–çš„å½±å“ï¼Œé¥æ„Ÿæ•°æ®å¸¦æ¥äº†è¯¸å¤šæŒ‘æˆ˜ï¼Œä½¿å¾—è¯­ä¹‰åˆ†å‰²æ¨¡å‹éš¾ä»¥åœ¨å¤šç§åœºæ™¯ä¸­è¿›è¡Œæ³›åŒ–ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å±€é™äºç‰¹å®šæ•°æ®åŸŸï¼Œå¹¶éœ€è¦ä¸“å®¶æ³¨é‡Šå™¨å’Œä¸“ç”¨è®¾å¤‡è¿›è¡Œè¯­ä¹‰æ ‡æ³¨ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„é¥æ„Ÿè¯­ä¹‰åˆ†å‰²æ— ç›‘ç£åŸŸé€‚åº”æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯åˆ©ç”¨é¥æ„Ÿè®¾ç½®ä¸­å¯è½»æ¾è·å–çš„åœ°ç†åæ ‡ä½œä¸ºæ•°æ®é›†ä¸­çš„å…ƒæ•°æ®ã€‚ä¸ºäº†ç¼©å°åŸŸé—´å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å›¾åƒçš„ä½ç½®ç¼–ç ç‰¹æ€§å’Œåœ°çƒè¡¨é¢çš„çƒå½¢ç‰¹æ€§ã€‚æˆ‘ä»¬æå‡ºçš„SegDesicNetæ¨¡å—å¯¹å•ä½çƒä½“ä¸ŠæŠ•å½±çš„åœ°ç†åæ ‡è¿›è¡ŒGRIDä½ç½®ç¼–ç ï¼Œä»¥è·å¾—åŸŸæŸå¤±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSegDesicNetåœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­çš„æ€§èƒ½ä¼˜äºæœ€æ–°çš„åŸŸé€‚åº”æ–¹æ³•ï¼Œåœ¨å…¬å¼€å¯ç”¨çš„FLAIR #1æ•°æ®é›†çš„æ ‡å‡†å­é›†ä¸Šï¼Œå¹³å‡äº¤å¹¶æ¯”ï¼ˆMIoUï¼‰æé«˜äº†çº¦6%ï¼Œå‚æ•°æ•°é‡å‡å°‘äº†çº¦27%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹æˆ‘ä»¬çš„æ–¹æ³•åœ¨ISPRS Potsdamæ•°æ®é›†çš„è‡ªå®šåˆ†å‰²ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç®—æ³•æ—¨åœ¨å‡å°‘äººå·¥ç¥ç»ç½‘ç»œä¸äººç±»å¯¹ç‰©ç†ä¸–ç•Œçš„ç†è§£ä¹‹é—´çš„å»ºæ¨¡å·®å¼‚ï¼Œä½¿æŠ€æœ¯æ›´åŠ ä»¥äººä¸ºä¸­å¿ƒå¹¶å¯æ‰©å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08290v1">PDF</a> <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/WACV2025/papers/Verma_SegDesicNet_Lightweight_Semantic_Segmentation_in_Remote_Sensing_with_Geo-Coordinate_Embeddings_WACV_2025_paper.pdf">https://openaccess.thecvf.com/content/WACV2025/papers/Verma_SegDesicNet_Lightweight_Semantic_Segmentation_in_Remote_Sensing_with_Geo-Coordinate_Embeddings_WACV_2025_paper.pdf</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºåœ°ç†åæ ‡çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”é¥æ„Ÿè¯­ä¹‰åˆ†å‰²æŠ€æœ¯ã€‚é€šè¿‡åˆ©ç”¨é¥æ„Ÿè®¾ç½®ä¸­çš„å…ƒæ•°æ®â€”â€”åœ°ç†åæ ‡ï¼Œç¼©å°äº†é¢†åŸŸå·®è·ã€‚æå‡ºçš„SegDesicNetæ¨¡å—é€šè¿‡å›å½’åœ°çƒè¡¨é¢å•ä½çƒä½“ä¸Šçš„åœ°ç†åæ ‡æŠ•å½±çš„GRIDä½ç½®ç¼–ç ç‰¹å¾ï¼Œè·å¾—é¢†åŸŸæŸå¤±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSegDesicNetåœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²é¢†åŸŸçš„åŸŸè‡ªé€‚åº”æ–¹æ³•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåœ¨å…¬å¼€å¯ç”¨çš„FLAIR #1æ•°æ®é›†çš„æ ‡å‡†å­é›†ä¸Šï¼Œå¹³å‡äº¤å¹¶æ¯”ï¼ˆMIoUï¼‰æé«˜äº†çº¦6%ï¼Œå‚æ•°è®¡æ•°å‡å°‘äº†çº¦27%ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•åœ¨ISPRS Potsdamæ•°æ®é›†çš„è‡ªå®šåˆ†å‰²ä¸Šä¹Ÿè¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œæ—¨åœ¨å‡å°‘äººå·¥ç¥ç»ç½‘ç»œä¸äººç±»å¯¹ç°å®ä¸–ç•Œç†è§£ä¹‹é—´çš„å»ºæ¨¡å·®å¼‚ï¼Œä½¿æŠ€æœ¯æ›´åŠ ä»¥äººç±»ä¸ºä¸­å¿ƒå’Œå¯è§„æ¨¡åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰åˆ†å‰²åœ¨é«˜åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒåˆ†æä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œèƒ½å¤Ÿå®ç°åƒç´ çº§å¯¹è±¡å’ŒåŒºåŸŸç²¾ç¡®åˆ†ç±»ã€‚</li>
<li>é¥æ„Ÿæ•°æ®å› åœ°ç†ä½ç½®ã€å¤©æ°”å’Œç¯å¢ƒå˜åŒ–è€Œå¸¦æ¥æŒ‘æˆ˜ï¼Œä½¿å¾—è¯­ä¹‰åˆ†å‰²æ¨¡å‹åœ¨è·¨åœºæ™¯æ¨å¹¿æ—¶é¢ä¸´å›°éš¾ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å±€é™äºç‰¹å®šæ•°æ®åŸŸï¼Œéœ€è¦ä¸“å®¶æ ‡æ³¨å™¨å’Œä¸“é—¨è®¾å¤‡è¿›è¡Œè¯­ä¹‰æ ‡æ³¨ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºåœ°ç†åæ ‡çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”æŠ€æœ¯ï¼Œåˆ©ç”¨é¥æ„Ÿè®¾ç½®ä¸­çš„å…ƒæ•°æ®ï¼ˆåœ°ç†åæ ‡ï¼‰æ¥ç¼©å°é¢†åŸŸå·®è·ã€‚</li>
<li>SegDesicNetæ¨¡å—é€šè¿‡å›å½’åœ°ç†åæ ‡çš„ä½ç½®ç¼–ç ç‰¹å¾ï¼Œè·å¾—é¢†åŸŸæŸå¤±ï¼Œå®ç°é¥æ„Ÿå›¾åƒåˆ†å‰²çš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSegDesicNetåœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²çš„åŸŸè‡ªé€‚åº”æ–¹æ³•ä¸Šè¾ƒç°æœ‰æŠ€æœ¯æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5569660a87a46e080703c1f8d3e31578.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d16d33194aec0c883572a405df984d1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cca623faced14bef73fe8c7f3956fdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79993605caaac9fae3ebeb7e7e965246.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-981a2d517b3fa03e2075462996681c0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96d1505f9a91831c2cb502f69967bb55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6089824ad79aeedf6a4b9311eecffd0e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Towards-All-in-One-Medical-Image-Re-Identification"><a href="#Towards-All-in-One-Medical-Image-Re-Identification" class="headerlink" title="Towards All-in-One Medical Image Re-Identification"></a>Towards All-in-One Medical Image Re-Identification</h2><p><strong>Authors:Yuan Tian, Kaiyuan Ji, Rongzhao Zhang, Yankai Jiang, Chunyi Li, Xiaosong Wang, Guangtao Zhai</strong></p>
<p>Medical image re-identification (MedReID) is under-explored so far, despite its critical applications in personalized healthcare and privacy protection. In this paper, we introduce a thorough benchmark and a unified model for this problem. First, to handle various medical modalities, we propose a novel Continuous Modality-based Parameter Adapter (ComPA). ComPA condenses medical content into a continuous modality representation and dynamically adjusts the modality-agnostic model with modality-specific parameters at runtime. This allows a single model to adaptively learn and process diverse modality data. Furthermore, we integrate medical priors into our model by aligning it with a bag of pre-trained medical foundation models, in terms of the differential features. Compared to single-image feature, modeling the inter-image difference better fits the re-identification problem, which involves discriminating multiple images. We evaluate the proposed model against 25 foundation models and 8 large multi-modal language models across 11 image datasets, demonstrating consistently superior performance. Additionally, we deploy the proposed MedReID technique to two real-world applications, i.e., history-augmented personalized diagnosis and medical privacy protection. Codes and model is available at \href{<a target="_blank" rel="noopener" href="https://github.com/tianyuan168326/All-in-One-MedReID-Pytorch%7D%7Bhttps://github.com/tianyuan168326/All-in-One-MedReID-Pytorch%7D">https://github.com/tianyuan168326/All-in-One-MedReID-Pytorch}{https://github.com/tianyuan168326/All-in-One-MedReID-Pytorch}</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒé‡æ–°è¯†åˆ«ï¼ˆMedReIDï¼‰å°½ç®¡åœ¨ä¸ªæ€§åŒ–åŒ»ç–—å’Œéšç§ä¿æŠ¤æ–¹é¢æœ‰ç€è‡³å…³é‡è¦çš„åº”ç”¨ï¼Œä½†ç›®å‰å°šæœªå¾—åˆ°æ·±å…¥ç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸ºæ­¤é—®é¢˜å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å’Œç»Ÿä¸€æ¨¡å‹ã€‚é¦–å…ˆï¼Œä¸ºäº†å¤„ç†å„ç§åŒ»å­¦æ¨¡æ€ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°é¢–çš„è¿ç»­æ¨¡æ€å‚æ•°é€‚é…å™¨ï¼ˆComPAï¼‰ã€‚ComPAå°†åŒ»å­¦å†…å®¹æµ“ç¼©ä¸ºè¿ç»­æ¨¡æ€è¡¨ç¤ºï¼Œå¹¶åœ¨è¿è¡Œæ—¶åŠ¨æ€è°ƒæ•´ä¸ç‰¹å®šæ¨¡æ€æ— å…³çš„æ¨¡å‹çš„æ¨¡æ€ç‰¹å®šå‚æ•°ã€‚è¿™å…è®¸å•ä¸ªæ¨¡å‹è‡ªé€‚åº”å­¦ä¹ å’Œå¤„ç†å¤šç§æ¨¡æ€æ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å°†å…¶ä¸é¢„è®­ç»ƒçš„åŒ»å­¦åŸºç¡€æ¨¡å‹è¢‹è¿›è¡Œå¯¹é½ï¼Œå°†åŒ»å­¦å…ˆéªŒçŸ¥è¯†é›†æˆåˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­ï¼Œä»¥å·®å¼‚ç‰¹å¾ä¸ºåŸºå‡†ã€‚ä¸å•å›¾åƒç‰¹å¾ç›¸æ¯”ï¼Œæ¨¡æ‹Ÿå›¾åƒä¹‹é—´çš„å·®å¼‚æ›´é€‚åˆé‡æ–°è¯†åˆ«é—®é¢˜ï¼Œè¯¥é—®é¢˜æ¶‰åŠåŒºåˆ†å¤šä¸ªå›¾åƒã€‚æˆ‘ä»¬åœ¨11ä¸ªå›¾åƒæ•°æ®é›†ä¸Šå¯¹æå‡ºçš„æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œä¸25ä¸ªåŸºç¡€æ¨¡å‹å’Œ8ä¸ªå¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¡¨ç°å‡ºäº†å“è¶Šä¸”ç¨³å®šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ‰€æMedReIDæŠ€æœ¯éƒ¨ç½²åˆ°ä¸¤ä¸ªå®é™…åº”ç”¨ç¨‹åºä¸­ï¼Œå³å†å²å¢å¼ºä¸ªæ€§åŒ–è¯Šæ–­å’ŒåŒ»ç–—éšç§ä¿æŠ¤ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯é€šè¿‡è®¿é—®é“¾æ¥<a target="_blank" rel="noopener" href="https://github.com/tianyuan168326/All-in-One-MedReID-Pytorch%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/tianyuan168326/All-in-One-MedReID-Pytorchè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08173v1">PDF</a> Accepted to CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒå†è¯†åˆ«ï¼ˆMedReIDï¼‰é—®é¢˜çš„å…¨é¢åŸºå‡†æ¨¡å‹å’Œç»Ÿä¸€æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡è¿ç»­æ¨¡æ€å‚æ•°é€‚é…å™¨ï¼ˆComPAï¼‰å¤„ç†å¤šç§åŒ»å­¦æ¨¡æ€æ•°æ®ï¼Œå¹¶å°†å…¶ä¸é¢„è®­ç»ƒçš„åŒ»å­¦åŸºç¡€æ¨¡å‹å¯¹é½ï¼Œä»¥èå…¥åŒ»å­¦å…ˆéªŒçŸ¥è¯†ã€‚å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºå•å›¾åƒç‰¹å¾æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å†è¯†åˆ«é—®é¢˜ä¸Šè¡¨ç°æ›´ä¼˜å¼‚ï¼Œå¯åº”ç”¨äºä¸ªæ€§åŒ–è¯Šæ–­å’ŒåŒ»å­¦éšç§ä¿æŠ¤ç­‰å®é™…åº”ç”¨åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒå†è¯†åˆ«ï¼ˆMedReIDï¼‰æ˜¯ä¸€ä¸ªå°šæœªè¢«å……åˆ†ç ”ç©¶çš„é—®é¢˜ï¼Œå¯¹äºä¸ªæ€§åŒ–åŒ»ç–—å’Œéšç§ä¿æŠ¤å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§å…¨é¢åŸºå‡†æ¨¡å‹å’Œç»Ÿä¸€æ¨¡å‹æ¥è§£å†³MedReIDé—®é¢˜ã€‚</li>
<li>æå‡ºäº†è¿ç»­æ¨¡æ€å‚æ•°é€‚é…å™¨ï¼ˆComPAï¼‰ï¼Œå¯å¤„ç†å¤šç§åŒ»å­¦æ¨¡æ€æ•°æ®å¹¶å®ç°è‡ªé€‚åº”å­¦ä¹ ã€‚</li>
<li>å°†åŒ»å­¦å…ˆéªŒçŸ¥è¯†èå…¥æ¨¡å‹ï¼Œé€šè¿‡ä¸é¢„è®­ç»ƒçš„åŒ»å­¦åŸºç¡€æ¨¡å‹å¯¹é½æ¥å®ç°ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ç›¸è¾ƒäºå•å›¾åƒç‰¹å¾æ¨¡å‹åœ¨å†è¯†åˆ«é—®é¢˜ä¸Šè¡¨ç°æ›´ä¼˜å¼‚ã€‚</li>
<li>æ¨¡å‹å¯åº”ç”¨äºä¸ªæ€§åŒ–è¯Šæ–­å’ŒåŒ»å­¦éšç§ä¿æŠ¤ç­‰å®é™…åº”ç”¨åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08173">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6c78ec8960c880811768bd5eea179e31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78515affe83b6297b568729b8e90d433.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f51feefb6562bf9317b329c6684bb4a0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Deep-Perceptual-Enhancement-for-Medical-Image-Analysis"><a href="#Deep-Perceptual-Enhancement-for-Medical-Image-Analysis" class="headerlink" title="Deep Perceptual Enhancement for Medical Image Analysis"></a>Deep Perceptual Enhancement for Medical Image Analysis</h2><p><strong>Authors:S M A Sharif, Rizwan Ali Naqvi, Mithun Biswas, Woong-Kee Loh</strong></p>
<p>Due to numerous hardware shortcomings, medical image acquisition devices are susceptible to producing low-quality (i.e., low contrast, inappropriate brightness, noisy, etc.) images. Regrettably, perceptually degraded images directly impact the diagnosis process and make the decision-making manoeuvre of medical practitioners notably complicated. This study proposes to enhance such low-quality images by incorporating end-to-end learning strategies for accelerating medical image analysis tasks. To the best concern, this is the first work in medical imaging which comprehensively tackles perceptual enhancement, including contrast correction, luminance correction, denoising, etc., with a fully convolutional deep network. The proposed network leverages residual blocks and a residual gating mechanism for diminishing visual artefacts and is guided by a multi-term objective function to perceive the perceptually plausible enhanced images. The practicability of the deep medical image enhancement method has been extensively investigated with sophisticated experiments. The experimental outcomes illustrate that the proposed method could outperform the existing enhancement methods for different medical image modalities by 5.00 to 7.00 dB in peak signal-to-noise ratio (PSNR) metrics and 4.00 to 6.00 in DeltaE metrics. Additionally, the proposed method can drastically improve the medical image analysis tasksâ€™ performance and reveal the potentiality of such an enhancement method in real-world applications. Code Available: <a target="_blank" rel="noopener" href="https://github.com/sharif-apu/DPE_JBHI">https://github.com/sharif-apu/DPE_JBHI</a> </p>
<blockquote>
<p>ç”±äºç¡¬ä»¶çš„è¯¸å¤šç¼ºé™·ï¼ŒåŒ»å­¦å½±åƒé‡‡é›†è®¾å¤‡å®¹æ˜“äº§ç”Ÿä½è´¨é‡ï¼ˆå¦‚å¯¹æ¯”åº¦ä½ã€äº®åº¦ä¸å½“ã€å™ªå£°ç­‰ï¼‰çš„å›¾åƒã€‚é—æ†¾çš„æ˜¯ï¼Œæ„ŸçŸ¥é€€åŒ–å›¾åƒç›´æ¥å½±å“è¯Šæ–­è¿‡ç¨‹ï¼Œä½¿åŒ»åŠ¡äººå‘˜çš„å†³ç­–è¿‡ç¨‹å˜å¾—å¤æ‚ã€‚æœ¬ç ”ç©¶æå‡ºé€šè¿‡èå…¥ç«¯åˆ°ç«¯å­¦ä¹ ç­–ç•¥æ¥æå‡è¿™ç§ä½è´¨é‡çš„å›¾åƒï¼Œä»¥åŠ é€ŸåŒ»å­¦å½±åƒåˆ†æä»»åŠ¡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯åŒ»å­¦å½±åƒé¢†åŸŸä¸­é¦–æ¬¡å…¨é¢è§£å†³æ„ŸçŸ¥å¢å¼ºé—®é¢˜çš„å·¥ä½œï¼ŒåŒ…æ‹¬å¯¹æ¯”åº¦æ ¡æ­£ã€äº®åº¦æ ¡æ­£ã€å»å™ªç­‰ï¼Œé‡‡ç”¨å…¨å·ç§¯æ·±åº¦ç½‘ç»œã€‚æ‰€æå‡ºçš„ç½‘ç»œåˆ©ç”¨æ®‹å·®å—å’Œæ®‹å·®é—¨æ§æœºåˆ¶æ¥å‡å°‘è§†è§‰ä¼ªå½±ï¼Œå¹¶ç”±å¤šç›®æ ‡å‡½æ•°å¼•å¯¼ä»¥æ„ŸçŸ¥åˆç†çš„å¢å¼ºå›¾åƒã€‚æ·±åº¦åŒ»å­¦å½±åƒå¢å¼ºæ–¹æ³•çš„å®ç”¨æ€§å·²ç»é€šè¿‡å¤æ‚çš„å®éªŒè¿›è¡Œäº†å¹¿æ³›ç ”ç©¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æŒ‡æ ‡ä¸Šè¾ƒå…¶ä»–ç°æœ‰çš„å¢å¼ºæ–¹æ³•å¯æé«˜5.00è‡³7.00åˆ†è´ï¼Œåœ¨DeltaEæŒ‡æ ‡ä¸Šå¯æé«˜4.00è‡³6.00ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜åŒ»å­¦å½±åƒåˆ†æä»»åŠ¡çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºè¿™ç§å¢å¼ºæ–¹æ³•åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/sharif-apu/DPE_JBHI">https://github.com/sharif-apu/DPE_JBHI</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08027v1">PDF</a> IEEE Journal of Biomedical and Health Informatics, 2022</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦é’ˆå¯¹åŒ»ç–—å›¾åƒè·å–è®¾å¤‡å› ç¡¬ä»¶ç¼ºé™·å¯¼è‡´å›¾åƒè´¨é‡ä½ä¸‹ï¼ˆå¦‚å¯¹æ¯”åº¦ä½ã€äº®åº¦ä¸å½“ã€å™ªå£°ç­‰ï¼‰çš„é—®é¢˜ã€‚ä¸ºæé«˜è¿™äº›å›¾åƒè´¨é‡ï¼Œæœ¬ç ”ç©¶ç»“åˆç«¯åˆ°ç«¯å­¦ä¹ ç­–ç•¥ï¼ŒåŠ é€ŸåŒ»ç–—å›¾åƒåˆ†æä»»åŠ¡ã€‚è¯¥ç ”ç©¶é¦–æ¬¡å…¨é¢è§£å†³æ„ŸçŸ¥å¢å¼ºé—®é¢˜ï¼ŒåŒ…æ‹¬å¯¹æ¯”åº¦æ ¡æ­£ã€äº®åº¦æ ¡æ­£ã€å»å™ªç­‰ï¼Œä½¿ç”¨å…¨å·ç§¯æ·±åº¦ç½‘ç»œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’ŒDeltaEæŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰å¢å¼ºæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å›¾åƒè·å–è®¾å¤‡å› ç¡¬ä»¶ç¼ºé™·å¸¸äº§ç”Ÿè´¨é‡ä¸ä½³çš„å›¾åƒï¼Œç›´æ¥å½±å“è¯Šæ–­è¿‡ç¨‹ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡æå‡ºä½¿ç”¨å…¨å·ç§¯æ·±åº¦ç½‘ç»œå…¨é¢è§£å†³åŒ»ç–—å›¾åƒçš„æ„ŸçŸ¥å¢å¼ºé—®é¢˜ã€‚</li>
<li>æ‰€æå‡ºçš„ç½‘ç»œåˆ©ç”¨æ®‹å·®å—å’Œæ®‹å·®é—¨æ§æœºåˆ¶æ¥å‡å°‘è§†è§‰ä¼ªå½±ã€‚</li>
<li>è¯¥æ–¹æ³•å—åˆ°å¤šç›®æ ‡å‡½æ•°çš„å¼•å¯¼ï¼Œä»¥æ„ŸçŸ¥åˆç†çš„å¢å¼ºå›¾åƒã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šç§åŒ»ç–—å›¾åƒæ¨¡æ€çš„å¢å¼ºæ•ˆæœä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå³°å€¼ä¿¡å™ªæ¯”æå‡5.00è‡³7.00 dBã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜åŒ»ç–—å›¾åƒåˆ†æä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08027">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-52421910ec64a78c9448553ef8946bb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f734cd5f421309a8d778e6aa279a6f7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e46f5dbd282c56ab974a11908dcd65f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-079555d97971f1fa083200c97bdd8e7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-010ec49367c9a9d5008f886ff13f4f08.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-bd3ade09a1ad7eeb7773864c1e648f0f.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Network Calculus-based Deadline-Adaptive Online Admission Control for ET   Traffic in TSN
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-518a217994b75df52e0f978f0486ea70.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  FCaS Fine-grained Cardiac Image Synthesis based on 3D Template   Conditional Diffusion Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">13632.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
