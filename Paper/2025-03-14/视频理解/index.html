<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="è§†é¢‘ç†è§£">
    <meta name="description" content="è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Memory-enhanced Retrieval Augmentation for Long Video Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>è§†é¢‘ç†è§£ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b657b10c8d2e5e2c6f07564b881637c9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">è§†é¢‘ç†è§£</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                è§†é¢‘ç†è§£
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    36 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-14-æ›´æ–°"><a href="#2025-03-14-æ›´æ–°" class="headerlink" title="2025-03-14 æ›´æ–°"></a>2025-03-14 æ›´æ–°</h1><h2 id="Memory-enhanced-Retrieval-Augmentation-for-Long-Video-Understanding"><a href="#Memory-enhanced-Retrieval-Augmentation-for-Long-Video-Understanding" class="headerlink" title="Memory-enhanced Retrieval Augmentation for Long Video Understanding"></a>Memory-enhanced Retrieval Augmentation for Long Video Understanding</h2><p><strong>Authors:Huaying Yuan, Zheng Liu, Minhao Qin, Hongjin Qian, Y Shu, Zhicheng Dou, Ji-Rong Wen</strong></p>
<p>Retrieval-augmented generation (RAG) shows strong potential in addressing long-video understanding (LVU) tasks. However, traditional RAG methods remain fundamentally limited due to their dependence on explicit search queries, which are unavailable in many situations. To overcome this challenge, we introduce a novel RAG-based LVU approach inspired by the cognitive memory of human beings, which is called MemVid. Our approach operates with four basics steps: memorizing holistic video information, reasoning about the taskâ€™s information needs based on the memory, retrieving critical moments based on the information needs, and focusing on the retrieved moments to produce the final answer. To enhance the systemâ€™s memory-grounded reasoning capabilities and achieve optimal end-to-end performance, we propose a curriculum learning strategy. This approach begins with supervised learning on well-annotated reasoning results, then progressively explores and reinforces more plausible reasoning outcomes through reinforcement learning. We perform extensive evaluations on popular LVU benchmarks, including MLVU, VideoMME and LVBench. In our experiment, MemVid significantly outperforms existing RAG-based methods and popular LVU models, which demonstrate the effectiveness of our approach. Our model and source code will be made publicly available upon acceptance. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨è§£å†³é•¿è§†é¢‘ç†è§£ï¼ˆLVUï¼‰ä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„RAGæ–¹æ³•ç”±äºå…¶ä¾èµ–äºæ˜ç¡®çš„æœç´¢æŸ¥è¯¢è€Œä»æ ¹æœ¬ä¸Šå—åˆ°é™åˆ¶ï¼Œè¿™åœ¨è®¸å¤šæƒ…å†µä¸‹æ˜¯ä¸å¯ç”¨çš„ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å—äººç±»è®¤çŸ¥è®°å¿†å¯å‘çš„æ–°å‹åŸºäºRAGçš„LVUæ–¹æ³•ï¼Œç§°ä¸ºMemVidã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰å››ä¸ªåŸºæœ¬æ­¥éª¤ï¼šè®°å¿†æ•´ä½“è§†é¢‘ä¿¡æ¯ï¼ŒåŸºäºè®°å¿†å¯¹ä»»åŠ¡ä¿¡æ¯éœ€æ±‚è¿›è¡Œæ¨ç†ï¼Œæ ¹æ®ä¿¡æ¯éœ€æ±‚æ£€ç´¢å…³é”®æ—¶åˆ»ï¼Œå¹¶ä¸“æ³¨äºæ£€ç´¢åˆ°çš„æ—¶åˆ»ä»¥äº§ç”Ÿæœ€ç»ˆç­”æ¡ˆã€‚ä¸ºäº†æé«˜ç³»ç»Ÿçš„åŸºäºè®°å¿†æ¨ç†çš„èƒ½åŠ›å¹¶å®ç°ç«¯åˆ°ç«¯çš„æœ€ä½³æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ã€‚è¯¥æ–¹æ³•é¦–å…ˆä»ç»è¿‡è‰¯å¥½æ³¨é‡Šçš„æ¨ç†ç»“æœå¼€å§‹è¿›è¡Œæœ‰ç›‘ç£å­¦ä¹ ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ é€æ­¥æ¢ç´¢å’Œå¼ºåŒ–æ›´åˆç†çš„æ¨ç†ç»“æœã€‚æˆ‘ä»¬åœ¨æµè¡Œçš„LVUåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼ŒåŒ…æ‹¬MLVUã€VideoMMEå’ŒLVBenchã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼ŒMemVidæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºRAGçš„æ–¹æ³•å’Œæµè¡Œçš„LVUæ¨¡å‹ï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹å’Œæºä»£ç å°†åœ¨æ¥å—åå…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09149v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè®°å¿†çš„è§†é¢‘ç†è§£æ–¹æ³•MemVidï¼Œå®ƒé€šè¿‡æ¨¡æ‹Ÿäººç±»çš„è®¤çŸ¥è®°å¿†è¿‡ç¨‹æ¥è§£å†³é•¿è§†é¢‘ç†è§£ä»»åŠ¡ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬å››ä¸ªæ­¥éª¤ï¼šè®°å¿†æ•´ä½“è§†é¢‘ä¿¡æ¯ã€åŸºäºä»»åŠ¡éœ€æ±‚è¿›è¡Œæ¨ç†ã€æ ¹æ®éœ€æ±‚æ£€ç´¢å…³é”®æ—¶åˆ»ï¼Œä»¥åŠä¸“æ³¨äºæ£€ç´¢åˆ°çš„æ—¶åˆ»ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚ä¸ºæé«˜ç³»ç»Ÿçš„è®°å¿†æ¨ç†èƒ½åŠ›å¹¶å®ç°ç«¯åˆ°ç«¯çš„ä¼˜åŒ–æ€§èƒ½ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä»ç›‘ç£å­¦ä¹ å¼€å§‹ï¼Œé€æ­¥æ¢ç´¢å¹¶å¼ºåŒ–æ›´åˆç†çš„æ¨ç†ç»“æœã€‚å®éªŒè¡¨æ˜ï¼ŒMemVidåœ¨æµè¡Œçš„é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ–¹æ³•å’Œæµè¡Œæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MemVidæ˜¯ä¸€ç§åŸºäºè®°å¿†çš„è§†é¢‘ç†è§£æ–¹æ³•ï¼Œé€‚ç”¨äºé•¿è§†é¢‘ç†è§£ä»»åŠ¡ã€‚</li>
<li>MemVidæ–¹æ³•åŒ…æ‹¬å››ä¸ªåŸºæœ¬æ­¥éª¤ï¼šè®°å¿†è§†é¢‘ä¿¡æ¯ã€æ¨ç†ä»»åŠ¡éœ€æ±‚ã€æ£€ç´¢å…³é”®æ—¶åˆ»å’Œç”Ÿæˆç­”æ¡ˆã€‚</li>
<li>ä¸ºæé«˜ç³»ç»Ÿæ€§èƒ½ï¼Œæå‡ºäº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œç»“åˆç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>MemVidåœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>MemVidæ˜¾è‘—æé«˜äº†åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹å’Œæºä»£ç åœ¨æ¥å—åå°†å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09149">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9a0ff267abfca3729ffbf9f81c3d245.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c8bdc041fc4fa19f242d873fd75bb82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd3b33737d08389bc981f9b064296c5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e47fab8e66a46b9e767d45e6280c49cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04a4fd82e7fe59241ef7f600abbdf2ac.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Generative-Frame-Sampler-for-Long-Video-Understanding"><a href="#Generative-Frame-Sampler-for-Long-Video-Understanding" class="headerlink" title="Generative Frame Sampler for Long Video Understanding"></a>Generative Frame Sampler for Long Video Understanding</h2><p><strong>Authors:Linli Yao, Haoning Wu, Kun Ouyang, Yuanxing Zhang, Caiming Xiong, Bei Chen, Xu Sun, Junnan Li</strong></p>
<p>Despite recent advances in Video Large Language Models (VideoLLMs), effectively understanding long-form videos remains a significant challenge. Perceiving lengthy videos containing thousands of frames poses substantial computational burden. To mitigate this issue, this paper introduces Generative Frame Sampler (GenS), a plug-and-play module integrated with VideoLLMs to facilitate efficient lengthy video perception. Built upon a lightweight VideoLLM, GenS leverages its inherent vision-language capabilities to identify question-relevant frames. To facilitate effective retrieval, we construct GenS-Video-150K, a large-scale video instruction dataset with dense frame relevance annotations. Extensive experiments demonstrate that GenS consistently boosts the performance of various VideoLLMs, including open-source models (Qwen2-VL-7B, Aria-25B, VILA-40B, LLaVA-Video-7B&#x2F;72B) and proprietary assistants (GPT-4o, Gemini). When equipped with GenS, open-source VideoLLMs achieve impressive state-of-the-art results on long-form video benchmarks: LLaVA-Video-72B reaches 66.8 (+4.3) on LongVideoBench and 77.0 (+2.7) on MLVU, while Aria obtains 39.2 on HourVideo surpassing the Gemini-1.5-pro by 1.9 points. We will release all datasets and models at <a target="_blank" rel="noopener" href="https://generative-sampler.github.io/">https://generative-sampler.github.io</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘åœ¨è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMsï¼‰æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†æœ‰æ•ˆåœ°ç†è§£é•¿è§†é¢‘ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚å¤„ç†åŒ…å«æ•°åƒå¸§çš„å†—é•¿è§†é¢‘å¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—è´Ÿæ‹…ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†ç”Ÿæˆå¸§é‡‡æ ·å™¨ï¼ˆGenSï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸VideoLLMsé›†æˆçš„å³æ’å³ç”¨æ¨¡å—ï¼Œå¯ä¿ƒè¿›é«˜æ•ˆçš„é•¿è§†é¢‘æ„ŸçŸ¥ã€‚åŸºäºè½»é‡çº§çš„VideoLLMï¼ŒGenSåˆ©ç”¨å…¶å›ºæœ‰çš„è§†è§‰è¯­è¨€åŠŸèƒ½æ¥è¯†åˆ«ä¸é—®é¢˜ç›¸å…³çš„å¸§ã€‚ä¸ºäº†ä¿ƒè¿›æœ‰æ•ˆæ£€ç´¢ï¼Œæˆ‘ä»¬æ„å»ºäº†GenS-Video-150Kï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„è§†é¢‘æŒ‡ä»¤æ•°æ®é›†ï¼Œå…·æœ‰å¯†é›†çš„å¸§ç›¸å…³æ€§æ³¨é‡Šã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGenSæŒç»­æå‡äº†å„ç§VideoLLMsçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¼€æºæ¨¡å‹ï¼ˆQwen2-VL-7Bã€Aria-25Bã€VILA-40Bã€LLaVA-Video-7B&#x2F;72Bï¼‰å’Œä¸“æœ‰åŠ©ç†ï¼ˆGPT-4oã€Geminiï¼‰ã€‚é…å¤‡GenSåï¼Œå¼€æºVideoLLMsåœ¨é•¿ç¯‡è§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æœ€å…ˆè¿›ç»“æœï¼šLLaVA-Video-72Båœ¨LongVideoBenchä¸Šè¾¾åˆ°66.8ï¼ˆ+4.3ï¼‰ï¼Œåœ¨MLVUä¸Šè¾¾åˆ°77.0ï¼ˆ+2.7ï¼‰ï¼Œè€ŒAriaåœ¨HourVideoä¸Šè·å¾—39.2ï¼Œè¶…è¶Šäº†Gemini-1.5-proçš„1.9åˆ†ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://generative-sampler.github.ioå‘å¸ƒæ‰€æœ‰æ•°æ®é›†å’Œæ¨¡å‹./">https://generative-sampler.github.ioå‘å¸ƒæ‰€æœ‰æ•°æ®é›†å’Œæ¨¡å‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09146v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹é•¿è§†é¢‘ç†è§£é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºGenerative Frame Samplerï¼ˆGenSï¼‰çš„æ’ä»¶æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä¸VideoLLMé›†æˆï¼Œä»¥å®ç°å¯¹é•¿è§†é¢‘çš„å¿«é€Ÿæ„ŸçŸ¥ã€‚GenSåŸºäºè½»é‡çº§VideoLLMæ„å»ºï¼Œåˆ©ç”¨å…¶å›ºæœ‰çš„è§†è§‰è¯­è¨€åŠŸèƒ½è¯†åˆ«ä¸é—®é¢˜ç›¸å…³çš„å¸§ã€‚åŒæ—¶ï¼Œä¸ºäº†ä¿ƒè¿›æœ‰æ•ˆæ£€ç´¢ï¼Œæ„å»ºäº†å¤§å‹è§†é¢‘æŒ‡ä»¤æ•°æ®é›†GenS-Video-150Kï¼Œå…¶ä¸­åŒ…å«å¯†é›†çš„å¸§ç›¸å…³æ€§æ³¨é‡Šã€‚å®éªŒè¡¨æ˜ï¼ŒGenSèƒ½æ˜¾è‘—æé«˜å„ç§VideoLLMçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¼€æºæ¨¡å‹å’Œä¸“æœ‰åŠ©æ‰‹ã€‚ç‰¹åˆ«æ˜¯LLaVA-Video-72Båœ¨é•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†66.8çš„ä¼˜å¼‚æˆç»©ï¼Œæé«˜äº†4.3ä¸ªç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åä¸ºGenerative Frame Samplerï¼ˆGenSï¼‰çš„æ¨¡å—ï¼Œç”¨äºè§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„è®¡ç®—è´Ÿæ‹…é—®é¢˜ã€‚</li>
<li>GenSæ¨¡å—å¯ä»¥é›†æˆåˆ°ç°æœ‰çš„VideoLLMä¸­ï¼Œä»¥æé«˜å¯¹é•¿è§†é¢‘çš„æ„ŸçŸ¥æ•ˆç‡ã€‚</li>
<li>GenSåˆ©ç”¨è½»é‡çº§VideoLLMçš„å›ºæœ‰è§†è§‰è¯­è¨€åŠŸèƒ½æ¥è¯†åˆ«ä¸é—®é¢˜ç›¸å…³çš„å¸§ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§å‹è§†é¢‘æŒ‡ä»¤æ•°æ®é›†GenS-Video-150Kï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°é•¿è§†é¢‘ç†è§£æ¨¡å‹ã€‚</li>
<li>GenSåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—æé«˜çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¼€æºæ¨¡å‹LLaVA-Video-72Bå’ŒAriaç­‰ã€‚</li>
<li>LLaVA-Video-72Båœ¨é•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œç›¸è¾ƒäºåŸºå‡†æµ‹è¯•æœ‰æ˜æ˜¾çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4c418c8bfe3f313968303e6d1e4672c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-309061176a9a15d52e334ae609543c73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3667fc09f2a0c792356db633d68c37d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bdea828af413b443386f2183cb03f2e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Exo2Ego-Exocentric-Knowledge-Guided-MLLM-for-Egocentric-Video-Understanding"><a href="#Exo2Ego-Exocentric-Knowledge-Guided-MLLM-for-Egocentric-Video-Understanding" class="headerlink" title="Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video   Understanding"></a>Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video   Understanding</h2><p><strong>Authors:Haoyu Zhang, Qiaohui Chu, Meng Liu, Yunxiao Wang, Bin Wen, Fan Yang, Tingting Gao, Di Zhang, Yaowei Wang, Liqiang Nie</strong></p>
<p>AI personal assistants, deployed through robots or wearables, require embodied understanding to collaborate effectively with humans. Current Multimodal Large Language Models (MLLMs) primarily focus on third-person (exocentric) vision, overlooking the unique aspects of first-person (egocentric) videos. Additionally, high acquisition costs limit data size, impairing MLLM performance. To address these challenges, we propose learning the mapping between exocentric and egocentric domains, leveraging the extensive exocentric knowledge within existing MLLMs to enhance egocentric video understanding. To this end, we introduce Ego-ExoClip, a pre-training dataset comprising 1.1M synchronized ego-exo clip-text pairs derived from Ego-Exo4D. Our approach features a progressive training pipeline with three stages: Teacher Self-Preparation, Teacher-Student Guidance, and Student Self-Practice. Additionally, we propose an instruction-tuning data EgoIT from multiple sources to strengthen the modelâ€™s instruction-following capabilities, along with the EgoBench benchmark comprising eight different tasks for thorough evaluation. Extensive experiments across diverse egocentric tasks reveal that existing MLLMs perform inadequately in egocentric video understanding, while our model significantly outperforms these leading models. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ä¸ªäººåŠ©ç†é€šè¿‡æœºå™¨äººæˆ–å¯ç©¿æˆ´è®¾å¤‡éƒ¨ç½²ï¼Œéœ€è¦å®ä½“ç†è§£æ‰èƒ½ä¸äººç±»æœ‰æ•ˆåä½œã€‚å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸»è¦å…³æ³¨ç¬¬ä¸‰äººç§°ï¼ˆç¦»å¿ƒï¼‰è§†è§‰ï¼Œå¿½è§†äº†ç¬¬ä¸€äººç§°ï¼ˆè‡ªæˆ‘ä¸­å¿ƒï¼‰è§†é¢‘çš„ç‹¬ç‰¹æ–¹é¢ã€‚æ­¤å¤–ï¼Œé«˜æ˜‚çš„é‡‡é›†æˆæœ¬é™åˆ¶äº†æ•°æ®é‡ï¼Œå½±å“äº†MLLMçš„æ€§èƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å­¦ä¹ ç¦»å¿ƒå’Œè‡ªæˆ‘ä¸­å¿ƒé¢†åŸŸä¹‹é—´çš„æ˜ å°„ï¼Œåˆ©ç”¨ç°æœ‰çš„MLLMsä¸­çš„ä¸°å¯Œç¦»å¿ƒçŸ¥è¯†æ¥å¢å¼ºè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†Ego-ExoClipé¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«ä»Ego-Exo4Dæ´¾ç”Ÿçš„110ä¸‡åŒæ­¥è‡ªæˆ‘ä¸­å¿ƒ-ç¦»å¿ƒå‰ªè¾‘æ–‡æœ¬å¯¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç®¡é“ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šæ•™å¸ˆè‡ªæˆ‘å‡†å¤‡ã€æ•™å¸ˆå­¦ç”ŸæŒ‡å¯¼å’Œå­¦ç”Ÿè‡ªæˆ‘å®è·µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»å¤šä¸ªæ¥æºæå‡ºäº†æŒ‡ä»¤è°ƒæ•´æ•°æ®EgoITï¼Œä»¥åŠ å¼ºæ¨¡å‹çš„æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ï¼Œä»¥åŠåŒ…å«å…«ä¸ªä¸åŒä»»åŠ¡çš„EgoBenchåŸºå‡†æµ‹è¯•é›†è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚åœ¨å¤šç§è‡ªæˆ‘ä¸­å¿ƒä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„MLLMåœ¨è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç†è§£æ–¹é¢è¡¨ç°ä¸è¶³ï¼Œè€Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è¿™äº›é¢†å…ˆæ¨¡å‹ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09143v1">PDF</a> Project: <a target="_blank" rel="noopener" href="https://egovisiongroup.github.io/Exo2Ego.github.io/">https://egovisiongroup.github.io/Exo2Ego.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>AIä¸ªäººåŠ©ç†åœ¨æœºå™¨äººæˆ–å¯ç©¿æˆ´è®¾å¤‡ä¸Šçš„å®ç°éœ€è¦èº«ä¸´å…¶å¢ƒçš„ç†è§£æ¥ä¸äººç±»æœ‰æ•ˆåä½œã€‚å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦å…³æ³¨ç¬¬ä¸‰äººç§°ï¼ˆå¤–éƒ¨è§†è§’ï¼‰çš„è§†è§‰ï¼Œå¿½ç•¥äº†ç¬¬ä¸€äººç§°ï¼ˆä¸ªäººè§†è§’ï¼‰è§†é¢‘çš„ç‹¬ç‰¹æ€§ã€‚ä¸ºäº†è§£å†³æ•°æ®è·å–æˆæœ¬é«˜ã€å½±å“MLLMæ€§èƒ½çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å­¦ä¹ å¤–éƒ¨è§†è§’å’Œå†…éƒ¨è§†è§’é¢†åŸŸä¹‹é—´çš„æ˜ å°„å…³ç³»çš„æ–¹æ³•ï¼Œåˆ©ç”¨ç°æœ‰çš„MLLMsä¸­çš„å¤–éƒ¨è§†è§’çŸ¥è¯†æ¥å¢å¼ºå†…éƒ¨è§†è§’çš„è§†é¢‘ç†è§£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†Ego-ExoClipé¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«ä»Ego-Exo4Dæ´¾ç”Ÿçš„110ä¸‡å¯¹åŒæ­¥çš„ä¸ªäººä¸å¤–éƒ¨å‰ªè¾‘æ–‡æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç®¡é“ï¼ŒåŒ…æ‹¬æ•™å¸ˆè‡ªæˆ‘å‡†å¤‡ã€æ•™å¸ˆ-å­¦ç”ŸæŒ‡å¯¼å’Œå­¦ç”Ÿçš„è‡ªæˆ‘å®è·µä¸‰ä¸ªé˜¶æ®µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†æ¥è‡ªå¤šä¸ªæ¥æºçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®EgoITï¼Œä»¥åŠ å¼ºæ¨¡å‹çš„æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ï¼Œä»¥åŠåŒ…å«å…«ä¸ªä¸åŒä»»åŠ¡çš„EgoBenchåŸºå‡†æµ‹è¯•é›†è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„MLLMåœ¨å†…éƒ¨è§†è§’è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œè€Œæˆ‘ä»¬çš„æ¨¡å‹åˆ™æ˜¾è‘—ä¼˜äºè¿™äº›é¢†å…ˆæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIä¸ªäººåŠ©ç†éœ€è¦èº«ä¸´å…¶å¢ƒçš„ç†è§£æ¥ä¸äººç±»åä½œã€‚</li>
<li>å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸»è¦å…³æ³¨ç¬¬ä¸‰äººç§°è§†è§‰ï¼Œå¿½è§†ç¬¬ä¸€äººç§°è§†é¢‘çš„ç‹¬ç‰¹æ€§ã€‚</li>
<li>æå‡ºå­¦ä¹ å¤–éƒ¨è§†è§’å’Œå†…éƒ¨è§†è§’é¢†åŸŸä¹‹é—´çš„æ˜ å°„å…³ç³»çš„æ–¹æ³•ã€‚</li>
<li>å¼•å…¥Ego-ExoClipé¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«åŒæ­¥çš„ä¸ªäººä¸å¤–éƒ¨å‰ªè¾‘æ–‡æœ¬ã€‚</li>
<li>é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç®¡é“ï¼ŒåŒ…æ‹¬æ•™å¸ˆè‡ªæˆ‘å‡†å¤‡ã€æ•™å¸ˆ-å­¦ç”ŸæŒ‡å¯¼å’Œå­¦ç”Ÿçš„è‡ªæˆ‘å®è·µé˜¶æ®µã€‚</li>
<li>æå‡ºæŒ‡ä»¤è°ƒæ•´æ•°æ®EgoITï¼ŒåŠ å¼ºæ¨¡å‹çš„æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e78efcd71dd3c20e4361915764922d23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b45e083b7d2925bed0ae315ac739666.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-258d6194034dd56c81793faef1b14ce3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-188e58234f29947ef48d8b4b7baa6afd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32f843066099c1f6d7714658dc8a1b73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c641c62c8bdb0d0ad13ebbfc960a574.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HierarQ-Task-Aware-Hierarchical-Q-Former-for-Enhanced-Video-Understanding"><a href="#HierarQ-Task-Aware-Hierarchical-Q-Former-for-Enhanced-Video-Understanding" class="headerlink" title="HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video   Understanding"></a>HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video   Understanding</h2><p><strong>Authors:Shehreen Azad, Vibhav Vineet, Yogesh Singh Rawat</strong></p>
<p>Despite advancements in multimodal large language models (MLLMs), current approaches struggle in medium-to-long video understanding due to frame and context length limitations. As a result, these models often depend on frame sampling, which risks missing key information over time and lacks task-specific relevance. To address these challenges, we introduce HierarQ, a task-aware hierarchical Q-Former based framework that sequentially processes frames to bypass the need for frame sampling, while avoiding LLMâ€™s context length limitations. We introduce a lightweight two-stream language-guided feature modulator to incorporate task awareness in video understanding, with the entity stream capturing frame-level object information within a short context and the scene stream identifying their broader interactions over longer period of time. Each stream is supported by dedicated memory banks which enables our proposed Hierachical Querying transformer (HierarQ) to effectively capture short and long-term context. Extensive evaluations on 10 video benchmarks across video understanding, question answering, and captioning tasks demonstrate HierarQâ€™s state-of-the-art performance across most datasets, proving its robustness and efficiency for comprehensive video analysis. </p>
<blockquote>
<p>å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ‰æ‰€è¿›å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•åœ¨ä¸­é•¿è§†é¢‘ç†è§£æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºå¸§å’Œä¸Šä¸‹æ–‡é•¿åº¦çš„é™åˆ¶æ‰€å¯¼è‡´çš„ã€‚å› æ­¤ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ä¾èµ–äºå¸§é‡‡æ ·ï¼Œè¿™æœ‰å¯èƒ½åœ¨æ—¶é—´ä¸Šé”™è¿‡å…³é”®ä¿¡æ¯ï¼Œå¹¶ä¸”ç¼ºä¹ç‰¹å®šä»»åŠ¡çš„å…³è”æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº† HierarQï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä»»åŠ¡æ„ŸçŸ¥çš„åˆ†å±‚ Q-Former æ¡†æ¶ï¼Œå®ƒæŒ‰é¡ºåºå¤„ç†å¸§ï¼Œä»è€Œç»•è¿‡å¯¹å¸§é‡‡æ ·çš„éœ€æ±‚ï¼ŒåŒæ—¶é¿å… LLM çš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„åŒæµè¯­è¨€å¼•å¯¼ç‰¹å¾è°ƒåˆ¶å™¨ï¼Œå°†ä»»åŠ¡æ„ŸçŸ¥èå…¥è§†é¢‘ç†è§£ä¸­ï¼Œå®ä½“æµåœ¨çŸ­è¯­å¢ƒä¸­æ•è·å¸§çº§å¯¹è±¡ä¿¡æ¯ï¼Œåœºæ™¯æµåˆ™è¯†åˆ«å®ƒä»¬åœ¨æ›´é•¿æ—¶é—´å†…çš„æ›´å¹¿æ³›çš„äº¤äº’ã€‚æ¯ä¸ªæµéƒ½ç”±ä¸“ç”¨å†…å­˜åº“æ”¯æŒï¼Œè¿™ä½¿å¾—æˆ‘ä»¬æå‡ºçš„åˆ†å±‚æŸ¥è¯¢å˜å‹å™¨ï¼ˆHierarQï¼‰å¯ä»¥æœ‰æ•ˆåœ°æ•è·çŸ­æœŸå’Œé•¿æœŸä¸Šä¸‹æ–‡ã€‚åœ¨10ä¸ªè§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼ŒåŒ…æ‹¬è§†é¢‘ç†è§£ã€é—®ç­”å’Œå­—å¹•ä»»åŠ¡ï¼Œè¯æ˜äº† HierarQ åœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šçš„æœ€æ–°æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨ç»¼åˆè§†é¢‘åˆ†æä¸­çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08585v1">PDF</a> Accepted in CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æŒ‡å‡ºï¼Œå°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨ä¸­ç­‰è‡³é•¿è§†é¢‘ç†è§£æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œå—é™äºå¸§å’Œä¸Šä¸‹æ–‡é•¿åº¦ã€‚å› æ­¤ï¼Œç°æœ‰æ¨¡å‹å¸¸ä¾èµ–å¸§é‡‡æ ·ï¼Œè¿™å¯èƒ½é—æ¼å…³é”®ä¿¡æ¯ä¸”ç¼ºä¹ä»»åŠ¡ç›¸å…³æ€§ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§ä»»åŠ¡æ„ŸçŸ¥çš„åˆ†å±‚Q-Formeræ¡†æ¶ï¼ˆHierarQï¼‰ï¼Œè¯¥æ¡†æ¶å¯é¡ºåºå¤„ç†å¸§ï¼Œæ— éœ€å¸§é‡‡æ ·ï¼ŒåŒæ—¶é¿å…LLMçš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ã€‚é€šè¿‡å¼•å…¥è½»é‡çº§åŒæµè¯­è¨€å¼•å¯¼ç‰¹å¾è°ƒåˆ¶å™¨å®ç°ä»»åŠ¡æ„ŸçŸ¥çš„è§†é¢‘ç†è§£ï¼Œå®ä½“æµæ•è·çŸ­ä¸Šä¸‹æ–‡å†…çš„å¸§çº§å¯¹è±¡ä¿¡æ¯ï¼Œåœºæ™¯æµè¯†åˆ«è¾ƒé•¿æ—¶é—´å†…çš„æ›´å¹¿æ³›äº¤äº’ã€‚æ¯æ¡æµéƒ½æœ‰ä¸“ç”¨å†…å­˜é“¶è¡Œï¼Œæ”¯æŒåˆ†å±‚æŸ¥è¯¢å˜å‹å™¨ï¼ˆHierarQï¼‰æœ‰æ•ˆæ•è·çŸ­æœŸå’Œé•¿æœŸä¸Šä¸‹æ–‡ã€‚åœ¨10ä¸ªè§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒHierarQåœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³ï¼Œè¯æ˜äº†å…¶åœ¨ç»¼åˆè§†é¢‘åˆ†æä¸­çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸­ç­‰è‡³é•¿è§†é¢‘ç†è§£ä¸Šå­˜æœ‰é™åˆ¶ï¼Œä¸»è¦å› ä¸ºå¸§å’Œä¸Šä¸‹æ–‡é•¿åº¦çš„åˆ¶çº¦ã€‚</li>
<li>ç°æœ‰æ¨¡å‹å¸¸ä¾èµ–å¸§é‡‡æ ·ï¼Œè¿™å¯èƒ½é€ æˆå…³é”®ä¿¡æ¯çš„é—æ¼ï¼Œå¹¶ä¸”ç¼ºä¹ä»»åŠ¡ç‰¹å¼‚æ€§ã€‚</li>
<li>æå‡ºä¸€ç§ä»»åŠ¡æ„ŸçŸ¥çš„åˆ†å±‚Q-Formeræ¡†æ¶ï¼ˆHierarQï¼‰ï¼Œèƒ½é¡ºåºå¤„ç†å¸§ï¼Œç»•è¿‡å¸§é‡‡æ ·çš„éœ€è¦ã€‚</li>
<li>HierarQæ¡†æ¶é¿å…äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ã€‚</li>
<li>é€šè¿‡å¼•å…¥è½»é‡çº§åŒæµè¯­è¨€å¼•å¯¼ç‰¹å¾è°ƒåˆ¶å™¨å®ç°ä»»åŠ¡æ„ŸçŸ¥çš„è§†é¢‘ç†è§£ï¼ŒåŒ…æ‹¬å®ä½“æµå’Œåœºæ™¯æµã€‚</li>
<li>å®ä½“æµæ•è·çŸ­ä¸Šä¸‹æ–‡å†…çš„å¸§çº§å¯¹è±¡ä¿¡æ¯ï¼Œåœºæ™¯æµè¯†åˆ«æ›´å¹¿æ³›çš„é•¿æœŸäº¤äº’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a9789d7137e0db0dbdc033e49696fda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b39db4d159f3ecd80ed2300cdf0799e1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-694c96fdb21142f0449281a55bba6a3a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-203f3d7ca2bc8cdae92ef62946b7a310.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RAG-Adapter-A-Plug-and-Play-RAG-enhanced-Framework-for-Long-Video-Understanding"><a href="#RAG-Adapter-A-Plug-and-Play-RAG-enhanced-Framework-for-Long-Video-Understanding" class="headerlink" title="RAG-Adapter: A Plug-and-Play RAG-enhanced Framework for Long Video   Understanding"></a>RAG-Adapter: A Plug-and-Play RAG-enhanced Framework for Long Video   Understanding</h2><p><strong>Authors:Xichen Tan, Yunfan Ye, Yuanjing Luo, Qian Wan, Fang Liu, Zhiping Cai</strong></p>
<p>Multi-modal Large Language Models (MLLMs) capable of video understanding are advancing rapidly. To effectively assess their video comprehension capabilities, long video understanding benchmarks, such as Video-MME and MLVU, are proposed. However, these benchmarks directly use uniform frame sampling for testing, which results in significant information loss and affects the accuracy of the evaluations in reflecting the true abilities of MLLMs. To address this, we propose RAG-Adapter, a plug-and-play framework that reduces information loss during testing by sampling frames most relevant to the given question. Additionally, we introduce a Grouped-supervised Contrastive Learning (GCL) method to further enhance sampling effectiveness of RAG-Adapter through fine-tuning on our constructed MMAT dataset. Finally, we test numerous baseline MLLMs on various video understanding benchmarks, finding that RAG-Adapter sampling consistently outperforms uniform sampling (e.g., Accuracy of GPT-4o increases by 9.3 percent on Video-MME), providing a more accurate testing method for long video benchmarks. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢æ­£åœ¨è¿…é€Ÿå‘å±•ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è¯„ä¼°å…¶è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œæå‡ºäº†é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ï¼Œå¦‚Video-MMEå’ŒMLVUç­‰ã€‚ç„¶è€Œï¼Œè¿™äº›åŸºå‡†æµ‹è¯•ç›´æ¥ä½¿ç”¨ç»Ÿä¸€çš„å¸§é‡‡æ ·è¿›è¡Œæµ‹è¯•ï¼Œå¯¼è‡´å¤§é‡ä¿¡æ¯ä¸¢å¤±ï¼Œå½±å“è¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œæ— æ³•çœŸæ­£åæ˜ MLLMsçš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RAG-Adapterï¼Œè¿™æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¡†æ¶ï¼Œé€šè¿‡é‡‡æ ·ä¸ç»™å®šé—®é¢˜æœ€ç›¸å…³çš„å¸§æ¥å‡å°‘æµ‹è¯•è¿‡ç¨‹ä¸­çš„ä¿¡æ¯ä¸¢å¤±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åˆ†ç»„ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼ˆGCLï¼‰æ–¹æ³•ï¼Œé€šè¿‡åœ¨æˆ‘ä»¬æ„å»ºçš„MMATæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¿›ä¸€æ­¥æé«˜RAG-Adapterçš„é‡‡æ ·æ•ˆæœã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å„ç§è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šå¯¹å¤šä¸ªåŸºå‡†MLLMsè¿›è¡Œäº†æµ‹è¯•ï¼Œå‘ç°RAG-Adapteré‡‡æ ·å§‹ç»ˆä¼˜äºå‡åŒ€é‡‡æ ·ï¼ˆä¾‹å¦‚ï¼ŒGPT-4oåœ¨Video-MMEä¸Šçš„å‡†ç¡®ç‡æé«˜9.3%ï¼‰ï¼Œä¸ºé•¿è§†é¢‘åŸºå‡†æµ‹è¯•æä¾›äº†æ›´å‡†ç¡®çš„æµ‹è¯•æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08576v1">PDF</a> 37 pages, 36 figures</p>
<p><strong>Summary</strong><br>     å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢è¿…é€Ÿè¿›æ­¥ï¼Œä½†ç°æœ‰è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ï¼ˆå¦‚Video-MMEå’ŒMLVUï¼‰é‡‡ç”¨å‡åŒ€å¸§é‡‡æ ·ï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±å¹¶å½±å“è¯„ä¼°å‡†ç¡®æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºRAG-Adapteræ¡†æ¶ï¼Œé€šè¿‡é‡‡æ ·ä¸é—®é¢˜æœ€ç›¸å…³çš„å¸§æ¥å‡å°‘æµ‹è¯•ä¸­çš„ä¿¡æ¯ä¸¢å¤±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥åˆ†ç»„ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼ˆGCLï¼‰æ¥æå‡RAG-Adapterçš„é‡‡æ ·æ•ˆæœã€‚åœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­æµ‹è¯•æ˜¾ç¤ºï¼ŒRAG-Adapteré‡‡æ ·æ–¹æ³•è¡¨ç°ä¼˜äºå‡åŒ€é‡‡æ ·ï¼Œå¦‚GPT-4oåœ¨Video-MMEä¸Šçš„å‡†ç¡®ç‡æé«˜9.3%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢è¿›å±•è¿…é€Ÿã€‚</li>
<li>ç›®å‰è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•å­˜åœ¨ä¿¡æ¯ä¸¢å¤±é—®é¢˜ã€‚</li>
<li>RAG-Adapteræ¡†æ¶é€šè¿‡é‡‡æ ·ä¸é—®é¢˜æœ€ç›¸å…³çš„å¸§æ¥å‡å°‘æµ‹è¯•ä¸­çš„ä¿¡æ¯ä¸¢å¤±ã€‚</li>
<li>å¼•å…¥åˆ†ç»„ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼ˆGCLï¼‰æå‡RAG-Adapterçš„é‡‡æ ·æ•ˆæœã€‚</li>
<li>RAG-Adapteré‡‡æ ·æ–¹æ³•è¡¨ç°ä¼˜äºå‡åŒ€é‡‡æ ·ã€‚</li>
<li>åœ¨ä¸åŒè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨RAG-Adapteræ¡†æ¶çš„MLLMsè¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b4fbf1ad6468dc0b28decad15fe35bdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de5cf11db3129d879078a2c3340fa732.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3daf69124abbaff3e4234988a2d8613e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c9202cdab1de2223f06dc4776b1cb05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c77ec33f1ee672236c8849c8e6e52ec.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DIV-FF-Dynamic-Image-Video-Feature-Fields-For-Environment-Understanding-in-Egocentric-Videos"><a href="#DIV-FF-Dynamic-Image-Video-Feature-Fields-For-Environment-Understanding-in-Egocentric-Videos" class="headerlink" title="DIV-FF: Dynamic Image-Video Feature Fields For Environment Understanding   in Egocentric Videos"></a>DIV-FF: Dynamic Image-Video Feature Fields For Environment Understanding   in Egocentric Videos</h2><p><strong>Authors:Lorenzo Mur-Labadia, Josechu Guerrero, Ruben Martinez-Cantin</strong></p>
<p>Environment understanding in egocentric videos is an important step for applications like robotics, augmented reality and assistive technologies. These videos are characterized by dynamic interactions and a strong dependence on the wearer engagement with the environment. Traditional approaches often focus on isolated clips or fail to integrate rich semantic and geometric information, limiting scene comprehension. We introduce Dynamic Image-Video Feature Fields (DIV FF), a framework that decomposes the egocentric scene into persistent, dynamic, and actor based components while integrating both image and video language features. Our model enables detailed segmentation, captures affordances, understands the surroundings and maintains consistent understanding over time. DIV-FF outperforms state-of-the-art methods, particularly in dynamically evolving scenarios, demonstrating its potential to advance long term, spatio temporal scene understanding. </p>
<blockquote>
<p>ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ä¸­çš„ç¯å¢ƒç†è§£å¯¹äºæœºå™¨äººæŠ€æœ¯ã€å¢å¼ºç°å®å’Œè¾…åŠ©æŠ€æœ¯ç­‰é¢†åŸŸçš„åº”ç”¨æ˜¯ä¸€ä¸ªé‡è¦æ­¥éª¤ã€‚è¿™äº›è§†é¢‘çš„ç‰¹ç‚¹æ˜¯åŠ¨æ€äº¤äº’æ€§å¼ºï¼Œå¼ºçƒˆä¾èµ–äºä½©æˆ´è€…ä¸ç¯å¢ƒçš„äº’åŠ¨ã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€ä¾§é‡äºå­¤ç«‹çš„ç‰‡æ®µæˆ–æ— æ³•æ•´åˆä¸°å¯Œçš„è¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯ï¼Œä»è€Œé™åˆ¶äº†åœºæ™¯çš„ç†è§£ã€‚æˆ‘ä»¬å¼•å…¥äº†åŠ¨æ€å›¾åƒè§†é¢‘ç‰¹å¾åœºï¼ˆDIV FFï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†è‡ªæˆ‘ä¸­å¿ƒåœºæ™¯åˆ†è§£ä¸ºæŒä¹…æ€§ã€åŠ¨æ€æ€§å’ŒåŸºäºæ¼”å‘˜çš„æˆåˆ†ï¼ŒåŒæ—¶æ•´åˆå›¾åƒå’Œè§†é¢‘è¯­è¨€ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿå®ç°è¯¦ç»†çš„åˆ†å‰²ï¼Œæ•æ‰è´Ÿæ‹…èƒ½åŠ›ï¼Œç†è§£å‘¨å›´ç¯å¢ƒï¼Œå¹¶éšæ—¶é—´ä¿æŒä¸€è‡´çš„ç†è§£ã€‚DIV-FFåœ¨åŠ¨æ€æ¼”åŒ–åœºæ™¯ä¸­è¡¨ç°å‡ºä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å®ƒåœ¨é•¿æœŸæ—¶ç©ºåœºæ™¯ç†è§£æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08344v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>åŠ¨æ€å›¾åƒè§†é¢‘ç‰¹å¾åœºï¼ˆDIV FFï¼‰æ¡†æ¶èƒ½å¤Ÿåˆ†è§£ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸ºæŒä¹…æ€§ã€åŠ¨æ€æ€§å’ŒåŸºäºæ¼”å‘˜çš„éƒ¨åˆ†ï¼ŒåŒæ—¶æ•´åˆå›¾åƒå’Œè§†é¢‘è¯­è¨€ç‰¹å¾ï¼Œå®ç°è¯¦ç»†åˆ†å‰²ã€æ•æ‰è´Ÿæ‹…ã€ç†è§£å‘¨å›´ç¯å¢ƒå’Œé•¿æœŸä¸€è‡´çš„ç†è§£ã€‚ä¸ä¼ ç»Ÿçš„ç¯å¢ƒç†è§£æ–¹æ³•ç›¸æ¯”ï¼ŒDIV FFåœ¨åŠ¨æ€æ¼”åŒ–åœºæ™¯ä¸­è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç¯å¢ƒç†è§£åœ¨æœºå™¨äººæŠ€æœ¯ã€å¢å¼ºç°å®å’Œè¾…åŠ©æŠ€æœ¯ç­‰é¢†åŸŸå…·æœ‰å…³é”®ä½œç”¨ã€‚</li>
<li>ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘å…·æœ‰åŠ¨æ€äº¤äº’æ€§å’Œå¯¹ä½©æˆ´è€…ä¸ç¯å¢ƒäº’åŠ¨çš„é«˜åº¦ä¾èµ–æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¸¸é›†ä¸­åœ¨å­¤ç«‹çš„ç‰‡æ®µä¸Šï¼Œæˆ–è€…æ— æ³•æ•´åˆä¸°å¯Œçš„è¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯ï¼Œé™åˆ¶äº†åœºæ™¯çš„ç†è§£ã€‚</li>
<li>DIV FFæ¡†æ¶å°†è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ç¯å¢ƒåˆ†è§£ä¸ºä¸åŒçš„ç»„ä»¶å¹¶æ•´åˆå›¾åƒå’Œè§†é¢‘è¯­è¨€ç‰¹å¾ã€‚</li>
<li>DIV FFæ¡†æ¶å¯å®ç°è¯¦ç»†åˆ†å‰²ã€æ•æ‰è´Ÿæ‹…å’Œç†è§£å‘¨å›´ç¯å¢ƒçš„èƒ½åŠ›ã€‚</li>
<li>DIV FFæ¡†æ¶åœ¨é•¿æœŸæ—¶ç©ºåœºæ™¯ç†è§£æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08344">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-280af576400f5822f909bac982764039.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21c1dfbdd4ad8caba90443d3f8d5bcb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a5ad3b25c6fe8f85b12f057533a41e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc9d77b6d48bd24828fc76ba554c6972.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Prompt2LVideos-Exploring-Prompts-for-Understanding-Long-Form-Multimodal-Videos"><a href="#Prompt2LVideos-Exploring-Prompts-for-Understanding-Long-Form-Multimodal-Videos" class="headerlink" title="Prompt2LVideos: Exploring Prompts for Understanding Long-Form Multimodal   Videos"></a>Prompt2LVideos: Exploring Prompts for Understanding Long-Form Multimodal   Videos</h2><p><strong>Authors:Soumya Shamarao Jahagirdar, Jayasree Saha, C V Jawahar</strong></p>
<p>Learning multimodal video understanding typically relies on datasets comprising video clips paired with manually annotated captions. However, this becomes even more challenging when dealing with long-form videos, lasting from minutes to hours, in educational and news domains due to the need for more annotators with subject expertise. Hence, there arises a need for automated solutions. Recent advancements in Large Language Models (LLMs) promise to capture concise and informative content that allows the comprehension of entire videos by leveraging Automatic Speech Recognition (ASR) and Optical Character Recognition (OCR) technologies. ASR provides textual content from audio, while OCR extracts textual content from specific frames. This paper introduces a dataset comprising long-form lectures and news videos. We present baseline approaches to understand their limitations on this dataset and advocate for exploring prompt engineering techniques to comprehend long-form multimodal video datasets comprehensively. </p>
<blockquote>
<p>å­¦ä¹ å¤šæ¨¡æ€è§†é¢‘ç†è§£é€šå¸¸ä¾èµ–äºç”±ä¸æ‰‹åŠ¨æ³¨é‡Šçš„æ ‡é¢˜é…å¯¹æ„æˆçš„è§†é¢‘å‰ªè¾‘æ•°æ®é›†ã€‚ç„¶è€Œï¼Œåœ¨å¤„ç†æ•™è‚²å’Œæ–°é¢†åŸŸçš„æŒç»­æ—¶é—´ä»å‡ åˆ†é’Ÿåˆ°å‡ å°æ—¶çš„é•¿è§†é¢‘æ—¶ï¼Œç”±äºéœ€è¦å¤§é‡æœ‰ä¸“ä¸šçŸ¥è¯†çš„ä¸»æ³¨é‡Šè€…ï¼Œè¿™ä¸€è¦æ±‚å˜å¾—æ›´å…·æŒ‘æˆ˜æ€§ã€‚å› æ­¤ï¼Œå¯¹è‡ªåŠ¨è§£å†³æ–¹æ¡ˆçš„éœ€æ±‚åº”è¿è€Œç”Ÿã€‚åŸºäºæœ€æ–°è¿›å±•çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰¿è¯ºæ•æ‰ç®€æ´è€Œå¯Œæœ‰ä¿¡æ¯çš„å†…å®¹ï¼Œåˆ©ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æŠ€æœ¯ï¼Œé€šè¿‡å¯¹æ•´ä¸ªè§†é¢‘è¿›è¡Œæ¨æ–­ä»¥å®ç°ç†è§£ã€‚ASRæä¾›äº†æ¥è‡ªéŸ³é¢‘çš„æ–‡æœ¬å†…å®¹ï¼Œè€ŒOCRä»ç‰¹å®šå¸§ä¸­æå–æ–‡æœ¬å†…å®¹ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåŒ…å«é•¿è®²åº§å’Œæ–°é—»è§†é¢‘çš„æ•°æ®é›†ã€‚æˆ‘ä»¬ä»‹ç»äº†åŸºçº¿æ–¹æ³•ï¼Œä»¥äº†è§£ä»–ä»¬åœ¨è¯¥æ•°æ®é›†ä¸Šçš„å±€é™æ€§ï¼Œå¹¶å€¡å¯¼æ¢ç´¢æç¤ºå·¥ç¨‹æŠ€æœ¯ä»¥å…¨é¢ç†è§£é•¿æ ¼å¼å¤šæ¨¡æ€è§†é¢‘æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08335v1">PDF</a> CVIP 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨æ•™è‚²å’Œæ–°é—»é¢†åŸŸä¸­å¯¹é•¿è§†é¢‘ï¼ˆä»å‡ åˆ†é’Ÿåˆ°å‡ å°æ—¶ä¸ç­‰ï¼‰è¿›è¡Œå¤šæ¨¡æ€ç†è§£æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œç°æœ‰çš„æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†æ— æ³•æ»¡è¶³é•¿è§†é¢‘çš„éœ€æ±‚ï¼Œéœ€è¦æ›´å¤šå…·æœ‰ä¸“ä¸šçŸ¥è¯†çš„æ ‡æ³¨è€…å‚ä¸ã€‚æ–‡ç« è¿˜ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•ï¼Œå®ƒå¯ä»¥åˆ©ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æŠ€æœ¯æ•æ‰è§†é¢‘ä¸­çš„å…³é”®ä¿¡æ¯ã€‚æœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ä¸ªåŒ…å«é•¿è®²åº§å’Œæ–°é—»è§†é¢‘çš„æ•°æ®é›†ï¼Œå¹¶æ¢è®¨äº†åŸºäºè¯¥æ•°æ®é›†çš„åŸºå‡†æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶æå€¡æ¢ç´¢æç¤ºå·¥ç¨‹æŠ€æœ¯æ¥å…¨é¢ç†è§£é•¿è§†é¢‘å¤šæ¨¡æ€æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦ä¹ å¤šæ¨¡æ€è§†é¢‘ç†è§£é€šå¸¸ä¾èµ–äºåŒ…å«è§†é¢‘å‰ªè¾‘å’Œæ‰‹åŠ¨æ³¨é‡Šå­—å¹•çš„æ•°æ®é›†ã€‚</li>
<li>å¤„ç†æ•™è‚²å’Œæ–°é—»é¢†åŸŸä¸­çš„é•¿è§†é¢‘é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ›´å¤šå…·æœ‰ä¸“ä¸šçŸ¥è¯†çš„æ ‡æ³¨è€…å‚ä¸ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥æ•æ‰è§†é¢‘ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œåˆ©ç”¨ASRå’ŒOCRæŠ€æœ¯ã€‚</li>
<li>æ–‡ç« å¼•å…¥äº†ä¸€ä¸ªåŒ…å«é•¿è®²åº§å’Œæ–°é—»è§†é¢‘çš„æ•°æ®é›†ã€‚</li>
<li>åŸºå‡†æ–¹æ³•åœ¨å¤„ç†æ­¤ç±»æ•°æ®é›†æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>éœ€è¦æ¢ç´¢æç¤ºå·¥ç¨‹æŠ€æœ¯æ¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿è§†é¢‘å¤šæ¨¡æ€ç†è§£æ–¹é¢çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fbe6c95c1b4dbdb0ab65d7d1344f1b36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b657b10c8d2e5e2c6f07564b881637c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5023425b67835ffb82a8e4a23f79007.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25ab7483b438218d293ad49505a415f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cd36fa92671f3f4be62c16689bde665.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GPT4Scene-Understand-3D-Scenes-from-Videos-with-Vision-Language-Models"><a href="#GPT4Scene-Understand-3D-Scenes-from-Videos-with-Vision-Language-Models" class="headerlink" title="GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models"></a>GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models</h2><p><strong>Authors:Zhangyang Qi, Zhixiong Zhang, Ye Fang, Jiaqi Wang, Hengshuang Zhao</strong></p>
<p>In recent years, 2D Vision-Language Models (VLMs) have made significant strides in image-text understanding tasks. However, their performance in 3D spatial comprehension, which is critical for embodied intelligence, remains limited. Recent advances have leveraged 3D point clouds and multi-view images as inputs, yielding promising results. However, we propose exploring a purely vision-based solution inspired by human perception, which merely relies on visual cues for 3D spatial understanding. This paper empirically investigates the limitations of VLMs in 3D spatial knowledge, revealing that their primary shortcoming lies in the lack of global-local correspondence between the scene and individual frames. To address this, we introduce GPT4Scene, a novel visual prompting paradigm in VLM training and inference that helps build the global-local relationship, significantly improving the 3D spatial understanding of indoor scenes. Specifically, GPT4Scene constructs a Birdâ€™s Eye View (BEV) image from the video and marks consistent object IDs across both frames and the BEV image. The model then inputs the concatenated BEV image and video frames with markers. In zero-shot evaluations, GPT4Scene improves performance over closed-source VLMs like GPT-4o. Additionally, we prepare a processed video dataset consisting of 165K text annotation to fine-tune open-source VLMs, achieving state-of-the-art performance on all 3D understanding tasks. Surprisingly, after training with the GPT4Scene paradigm, VLMs consistently improve during inference, even without object marker prompting and BEV image as explicit correspondence. It demonstrates that the proposed paradigm helps VLMs develop an intrinsic ability to understand 3D scenes, which paves the way for a seamless approach to extending pre-trained VLMs for 3D scene understanding. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒäºŒç»´è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å›¾æ–‡ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ä¸‰ç»´ç©ºé—´ç†è§£æ–¹é¢çš„è¡¨ç°ï¼Œå¯¹äºä½“ç°æ™ºèƒ½è‡³å…³é‡è¦ï¼Œä»ç„¶æœ‰é™ã€‚æœ€è¿‘çš„è¿›å±•åˆ©ç”¨ä¸‰ç»´ç‚¹äº‘å’Œå¤šè§†è§’å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æå‡ºæ¢ç´¢ä¸€ç§åŸºäºçº¯ç²¹è§†è§‰çš„è§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå—åˆ°äººç±»æ„ŸçŸ¥çš„å¯å‘ï¼Œä»…ä¾èµ–è§†è§‰çº¿ç´¢è¿›è¡Œä¸‰ç»´ç©ºé—´ç†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01428v4">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://gpt4scene.github.io/">https://gpt4scene.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†äºŒç»´è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ä¸‰ç»´ç©ºé—´ç†è§£ä¸Šçš„å±€é™æ€§ï¼Œå¹¶ä»‹ç»äº†ä¸€ç§åŸºäºè§†è§‰æç¤ºçš„æ–°æ–¹æ³•GPT4Sceneï¼Œç”¨äºæ”¹å–„VLMsåœ¨ä¸‰ç»´å®¤å†…åœºæ™¯ç†è§£æ–¹é¢çš„è¡¨ç°ã€‚GPT4Sceneé€šè¿‡å»ºç«‹é¸Ÿç°å›¾ï¼ˆBEVï¼‰å’Œå¯¹è±¡æ ‡è®°æŠ€æœ¯æ„å»ºå…¨å±€å±€éƒ¨å¯¹åº”å…³ç³»ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é›¶æ ·æœ¬è¯„ä¼°æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¤„ç†è¿‡çš„åŒ…å«æ–‡æœ¬æ³¨é‡Šçš„è§†é¢‘æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œä½¿å¾—æ¨¡å‹åœ¨ä¸‰ç»´ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šã€‚æœ€é‡è¦çš„æ˜¯ï¼ŒGPT4Sceneè®­ç»ƒåçš„æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†æ—¶å³ä½¿æ²¡æœ‰æ˜ç¡®çš„æç¤ºä¹Ÿèƒ½ç†è§£ä¸‰ç»´åœºæ™¯ï¼Œå±•ç°å‡ºå…¶å†…åœ¨çš„ä¸‰ç»´åœºæ™¯ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äºŒç»´è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ä¸‰ç»´ç©ºé—´ç†è§£æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>GPT4Sceneæ˜¯ä¸€ç§æ–°çš„è§†è§‰æç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„VLMsåœ¨ä¸‰ç»´å®¤å†…åœºæ™¯ç†è§£ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>GPT4Sceneé€šè¿‡å»ºç«‹é¸Ÿç°å›¾ï¼ˆBEVï¼‰å’Œå¯¹è±¡æ ‡è®°æŠ€æœ¯ï¼Œå»ºç«‹å…¨å±€å±€éƒ¨å¯¹åº”å…³ç³»ã€‚</li>
<li>GPT4Sceneèƒ½æé«˜æ¨¡å‹çš„é›¶æ ·æœ¬è¯„ä¼°æ€§èƒ½ã€‚</li>
<li>å¤„ç†è¿‡çš„è§†é¢‘æ•°æ®é›†ç”¨äºå¾®è°ƒæ¨¡å‹ï¼Œä½¿å…¶åœ¨ä¸‰ç»´ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šã€‚</li>
<li>GPT4Sceneè®­ç»ƒåçš„æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†æ—¶å³ä½¿æ²¡æœ‰æ˜ç¡®çš„æç¤ºä¹Ÿèƒ½ç†è§£ä¸‰ç»´åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01428">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c3e58e8cc1abf0f36ce698de357c3d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55af44b3c3db6b3270eaf9f7d834b327.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7bc1b554e0b17ff0257287621530619.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e7a21bbe8404077ab6dd48bc581886da.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ReTaKe-Reducing-Temporal-and-Knowledge-Redundancy-for-Long-Video-Understanding"><a href="#ReTaKe-Reducing-Temporal-and-Knowledge-Redundancy-for-Long-Video-Understanding" class="headerlink" title="ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video   Understanding"></a>ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video   Understanding</h2><p><strong>Authors:Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, Liqiang Nie</strong></p>
<p>Video Large Language Models (VideoLLMs) have achieved remarkable progress in video understanding. However, existing VideoLLMs often inherit the limitations of their backbone LLMs in handling long sequences, leading to challenges for long video understanding. Common solutions either simply uniformly sample videosâ€™ frames or compress visual tokens, which focus primarily on low-level temporal visual redundancy, overlooking high-level knowledge redundancy. This limits the achievable compression rate with minimal loss. To this end. we introduce a training-free method, $\textbf{ReTaKe}$, containing two novel modules DPSelect and PivotKV, to jointly model and reduce both temporal visual redundancy and knowledge redundancy for long video understanding. Specifically, DPSelect identifies keyframes with local maximum peak distance based on their visual features, which are closely aligned with human video perception. PivotKV employs the obtained keyframes as pivots and conducts KV-Cache compression for the non-pivot tokens with low attention scores, which are derived from the learned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and LVBench, show that ReTaKe can support 4x longer video sequences with minimal performance loss (&lt;1%) and outperform all similar-size VideoLLMs with 3%-5%, even surpassing or on par with much larger ones. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/SCZwangxiao/video-ReTaKe">https://github.com/SCZwangxiao/video-ReTaKe</a> </p>
<blockquote>
<p>è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VideoLLMsé€šå¸¸ç»§æ‰¿äº†å…¶ä¸»å¹²LLMsåœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„å±€é™æ€§ï¼Œè¿™ç»™é•¿è§†é¢‘ç†è§£å¸¦æ¥äº†æŒ‘æˆ˜ã€‚å¸¸è§çš„è§£å†³æ–¹æ¡ˆè¦ä¹ˆç®€å•åœ°ç»Ÿä¸€é‡‡æ ·è§†é¢‘å¸§ï¼Œè¦ä¹ˆå‹ç¼©è§†è§‰ä»¤ç‰Œï¼Œè¿™äº›è§£å†³æ–¹æ¡ˆä¸»è¦å…³æ³¨ä½çº§åˆ«çš„æ—¶åºè§†è§‰å†—ä½™ï¼Œè€Œå¿½ç•¥äº†é«˜çº§çŸ¥è¯†çš„å†—ä½™ã€‚è¿™é™åˆ¶äº†å¯å®ç°çš„å‹ç¼©ç‡ï¼Œå¹¶ä¼´éšæœ‰è¾ƒå¤šæŸå¤±ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ReTaKeï¼ŒåŒ…å«ä¸¤ä¸ªæ–°æ¨¡å—DPSelectå’ŒPivotKVï¼Œä»¥è”åˆå»ºæ¨¡å¹¶å‡å°‘æ—¶åºè§†è§‰å†—ä½™å’ŒçŸ¥è¯†å†—ä½™ï¼Œä»¥å®ç°é•¿è§†é¢‘ç†è§£ã€‚å…·ä½“æ¥è¯´ï¼ŒDPSelectåŸºäºè§†è§‰ç‰¹å¾è¯†åˆ«å…·æœ‰å±€éƒ¨æœ€å¤§å³°å€¼è·ç¦»çš„å…³é”®å¸§ï¼Œè¿™ä¸äººç±»è§†é¢‘æ„ŸçŸ¥ç´§å¯†ç›¸è¿ã€‚PivotKVå°†è·å¾—çš„å…³é”®å¸§ä½œä¸ºåŸºå‡†ç‚¹ï¼Œå¹¶ä½¿ç”¨KV-Cacheå‹ç¼©æ³¨æ„åŠ›å¾—åˆ†è¾ƒä½çš„éåŸºå‡†ä»¤ç‰Œï¼Œè¿™äº›ä»¤ç‰Œæ¥æºäºLLMså­¦åˆ°çš„å…ˆéªŒçŸ¥è¯†ã€‚åœ¨VideoMMEã€MLVUå’ŒLVBenchç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒReTaKeå¯ä»¥åœ¨æ€§èƒ½æŸå¤±æå°ï¼ˆ&lt;1%ï¼‰çš„æƒ…å†µä¸‹æ”¯æŒ4å€æ›´é•¿çš„è§†é¢‘åºåˆ—ï¼Œå¹¶ä¼˜äºæ‰€æœ‰ç±»ä¼¼è§„æ¨¡çš„VideoLLMsï¼ˆé«˜å‡º3%-5%ï¼‰ï¼Œç”šè‡³è¶…è¶Šæˆ–ç­‰åŒäºæ›´å¤§çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SCZwangxiao/video-ReTaKe%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SCZwangxiao/video-ReTaKeæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20504v3">PDF</a> Rewrite the methods section. Add more ablation studies and results in   LongVideoBench</p>
<p><strong>Summary</strong><br>     è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†é•¿åºåˆ—è§†é¢‘æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ä½çº§åˆ«çš„æ—¶é—´è§†è§‰å†—ä½™ï¼Œè€Œå¿½è§†é«˜çº§åˆ«çŸ¥è¯†å†—ä½™ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ReTaKeï¼ŒåŒ…å«DPSelectå’ŒPivotKVä¸¤ä¸ªæ–°æ¨¡å—ï¼Œè”åˆå»ºæ¨¡å¹¶å‡å°‘æ—¶é—´è§†è§‰å†—ä½™å’ŒçŸ¥è¯†å†—ä½™ï¼Œä»¥æé«˜é•¿è§†é¢‘ç†è§£çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VideoLLMsåœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†é•¿åºåˆ—è§†é¢‘æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ä½çº§åˆ«çš„æ—¶é—´è§†è§‰å†—ä½™ï¼Œå¿½è§†é«˜çº§åˆ«çŸ¥è¯†å†—ä½™ã€‚</li>
<li>ReTaKeæ–¹æ³•åŒ…å«DPSelectå’ŒPivotKVä¸¤ä¸ªæ–°æ¨¡å—ï¼Œæ—¨åœ¨è”åˆå»ºæ¨¡å¹¶å‡å°‘æ—¶é—´è§†è§‰å†—ä½™å’ŒçŸ¥è¯†å†—ä½™ã€‚</li>
<li>DPSelectåŸºäºè§†è§‰ç‰¹å¾è¯†åˆ«å…³é”®å¸§ï¼Œä¸äººçš„è§†é¢‘æ„ŸçŸ¥ç´§å¯†å¯¹é½ã€‚</li>
<li>PivotKVä½¿ç”¨è·å¾—çš„å…³é”®å¸§ä½œä¸ºåŸºå‡†ç‚¹ï¼Œå¯¹ä½å…³æ³¨åº¦éåŸºå‡†ç‚¹æ ‡è®°è¿›è¡ŒKV-Cacheå‹ç¼©ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒReTaKeæ–¹æ³•å¯ä»¥æ”¯æŒæ›´é•¿çš„è§†é¢‘åºåˆ—ï¼Œæ€§èƒ½æŸå¤±æå°ï¼Œä¼˜äºç±»ä¼¼å¤§å°çš„VideoLLMsï¼Œç”šè‡³è¶…è¿‡æˆ–ä¸å¤§æ¨¡å‹ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ff0243c6b878c1a3b7d28485ad81016.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6db153dfca6177d0c8180bb7801d989d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a00103e34a75a19679aa504eb809259.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9026225b9be41f510d7630003d48ea92.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60d990fba2e2cecf45f073fa1e6d99b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1cc8bd4295efd2a33e7d6b8d44fa980.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HumanVBench-Exploring-Human-Centric-Video-Understanding-Capabilities-of-MLLMs-with-Synthetic-Benchmark-Data"><a href="#HumanVBench-Exploring-Human-Centric-Video-Understanding-Capabilities-of-MLLMs-with-Synthetic-Benchmark-Data" class="headerlink" title="HumanVBench: Exploring Human-Centric Video Understanding Capabilities of   MLLMs with Synthetic Benchmark Data"></a>HumanVBench: Exploring Human-Centric Video Understanding Capabilities of   MLLMs with Synthetic Benchmark Data</h2><p><strong>Authors:Ting Zhou, Daoyuan Chen, Qirui Jiao, Bolin Ding, Yaliang Li, Ying Shen</strong></p>
<p>In the domain of Multimodal Large Language Models (MLLMs), achieving human-centric video understanding remains a formidable challenge. Existing benchmarks primarily emphasize object and action recognition, often neglecting the intricate nuances of human emotions, behaviors, and speech-visual alignment within video content. We present HumanVBench, an innovative benchmark meticulously crafted to bridge these gaps in the evaluation of video MLLMs. HumanVBench comprises 16 carefully designed tasks that explore two primary dimensions: inner emotion and outer manifestations, spanning static and dynamic, basic and complex, as well as single-modal and cross-modal aspects. With two advanced automated pipelines for video annotation and distractor-included QA generation, HumanVBench utilizes diverse state-of-the-art (SOTA) techniques to streamline benchmark data synthesis and quality assessment, minimizing human annotation dependency tailored to human-centric multimodal attributes. A comprehensive evaluation across 22 SOTA video MLLMs reveals notable limitations in current performance, especially in cross-modal and emotion perception, underscoring the necessity for further refinement toward achieving more human-like understanding. HumanVBench is open-sourced to facilitate future advancements and real-world applications in video MLLMs. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢†åŸŸï¼Œå®ç°ä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å¼ºè°ƒå¯¹è±¡å’ŒåŠ¨ä½œè¯†åˆ«ï¼Œå¾€å¾€å¿½è§†äº†è§†é¢‘å†…å®¹ä¸­äººç±»æƒ…ç»ªã€è¡Œä¸ºå’Œè¯­éŸ³è§†è§‰å¯¹é½çš„ç»†å¾®å·®åˆ«ã€‚æˆ‘ä»¬æ¨å‡ºäº†HumanVBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„åˆ›æ–°åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å¼¥è¡¥è§†é¢‘MLLMsè¯„ä¼°ä¸­çš„è¿™äº›å·®è·ã€‚HumanVBenchåŒ…å«16ä¸ªç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡ï¼Œæ¢ç´¢ä¸¤ä¸ªä¸»è¦ç»´åº¦ï¼šå†…åœ¨æƒ…ç»ªå’Œå¤–åœ¨è¡¨ç°ï¼Œæ¶µç›–é™æ€å’ŒåŠ¨æ€ã€åŸºæœ¬å’Œå¤æ‚ï¼Œä»¥åŠå•æ¨¡æ€å’Œè·¨æ¨¡æ€æ–¹é¢ã€‚HumanVBenchä½¿ç”¨ä¸¤ä¸ªå…ˆè¿›çš„è‡ªåŠ¨åŒ–ç®¡é“è¿›è¡Œè§†é¢‘æ³¨é‡Šå’ŒåŒ…å«å¹²æ‰°é¡¹çš„QAç”Ÿæˆï¼Œåˆ©ç”¨å¤šç§æœ€æ–°æŠ€æœ¯ä¼˜åŒ–åŸºå‡†æ•°æ®åˆæˆå’Œè´¨é‡è¯„ä¼°ï¼Œå‡å°‘å¯¹äººç±»æ³¨é‡Šçš„ä¾èµ–ï¼Œä¸“é—¨é’ˆå¯¹ä»¥äººç±»ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€å±æ€§ã€‚å¯¹22ä¸ªæœ€æ–°è§†é¢‘MLLMçš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æ€§èƒ½å­˜åœ¨æ˜¾è‘—å±€é™ï¼Œå°¤å…¶åœ¨è·¨æ¨¡æ€å’Œæƒ…æ„Ÿæ„ŸçŸ¥æ–¹é¢ï¼Œè¿™å¼ºè°ƒäº†è¿›ä¸€æ­¥æ”¹è¿›ä»¥å®ç°æ›´äººæ€§åŒ–çš„ç†è§£çš„å¿…è¦æ€§ã€‚HumanVBenchå¼€æºï¼Œä»¥ä¿ƒè¿›è§†é¢‘MLLMçš„æœªæ¥å‘å±•å’Œå®é™…åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17574v2">PDF</a> 22 pages, 23 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>HumanVBenchæ˜¯é’ˆå¯¹è§†é¢‘ç†è§£é¢†åŸŸçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æå‡ºçš„æ–°å‹åŸºå‡†æµ‹è¯•å¹³å°ã€‚è¯¥å¹³å°ç€é‡å…³æ³¨ç°æœ‰åŸºå‡†æµ‹è¯•æœªèƒ½è¦†ç›–çš„äººçš„æƒ…ç»ªã€è¡Œä¸ºå’Œè¯­éŸ³è§†è§‰å¯¹é½ç­‰ç»†èŠ‚æ–¹é¢ã€‚å®ƒåŒ…å«ç²¾å¿ƒè®¾è®¡çš„åå…­é¡¹ä»»åŠ¡ï¼Œæ¶‰åŠæƒ…æ„Ÿè®¤çŸ¥å’Œè¡Œä¸ºè¡¨ç°ç­‰æ–¹é¢ï¼Œæ—¨åœ¨å¼¥è¡¥ç°æœ‰è§†é¢‘ç†è§£æŠ€æœ¯çš„çŸ­æ¿ã€‚åŒæ—¶ï¼Œè¯¥å¹³å°è¿˜æä¾›äº†ä¸¤ä¸ªå…ˆè¿›çš„è‡ªåŠ¨åŒ–ç®¡é“ï¼Œç”¨äºè§†é¢‘æ ‡æ³¨å’ŒåŒ…å«å¹²æ‰°é¡¹çš„QAç”Ÿæˆï¼Œæ—¨åœ¨å‡å°‘äººå·¥æ ‡æ³¨ä¾èµ–ï¼Œå¹¶æ¨åŠ¨è§†é¢‘MLLMsçš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰æŠ€æœ¯çš„è¡¨ç°ä»æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨æ¨¡æ€å’Œæƒ…æ„Ÿæ„ŸçŸ¥æ–¹é¢æœ‰è¾ƒå¤§ä¸è¶³ã€‚è¯¥åŸºå‡†æµ‹è¯•å¹³å°é¢å‘å¼€æºç¤¾åŒºå¼€æ”¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HumanVBenché’ˆå¯¹è§†é¢‘ç†è§£çš„åŸºå‡†æµ‹è¯•ç¼ºä¹å…³æ³¨äººçš„æƒ…æ„Ÿå’Œè¡Œä¸ºç­‰ç»†èŠ‚é—®é¢˜ï¼Œæå‡ºäº†æ–°çš„è¯„ä»·æ ‡å‡†å’Œæ–¹æ³•ã€‚</li>
<li>å®ƒåŒ…æ‹¬åå…­é¡¹ä»»åŠ¡ï¼Œæ¶µç›–äº†æƒ…æ„Ÿå’Œè¡Œä¸ºçš„å†…åœ¨å’Œå¤–åœ¨è¡¨ç°ç­‰å¤šä¸ªæ–¹é¢ã€‚</li>
<li>HumanVBenchè®¾è®¡äº†ä¸¤ä¸ªè‡ªåŠ¨åŒ–ç®¡é“æ¥ä¼˜åŒ–è§†é¢‘æ ‡æ³¨å’ŒQAç”Ÿæˆè¿‡ç¨‹ï¼Œå‡å°‘äººå·¥å¹²é¢„çš„éœ€æ±‚ã€‚</li>
<li>å½“å‰è§†é¢‘MLLMsåœ¨è·¨æ¨¡æ€å’Œæƒ…æ„Ÿæ„ŸçŸ¥æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚</li>
<li>HumanVBenchå¼ºè°ƒæœªæ¥åœ¨è§†é¢‘MLLMsé¢†åŸŸçš„è¿›æ­¥å’Œå®é™…åº”ç”¨å‰æ™¯ã€‚</li>
<li>HumanVBenchçš„å¼€æºç‰¹æ€§å°†ä¿ƒè¿›ç¤¾åŒºå¯¹å…¶è´¡çŒ®å’Œå‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9aa31fd256e4e1214999c2c0675e76f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7dcbd9fac21bc08fff550b2f932ed8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c27fbacceb72e10420ea5305cd6a90f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-12251ebb19d0447c881400e74be59b48.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Evaluating Visual Explanations of Attention Maps for Transformer-based   Medical Imaging
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e9c42602c21b81f4a9b9c1e75bf0491a.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Electromyography-Informed Facial Expression Reconstruction for   Physiological-Based Synthesis and Analysis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17196.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
