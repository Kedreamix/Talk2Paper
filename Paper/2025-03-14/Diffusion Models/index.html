<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  FCaS Fine-grained Cardiac Image Synthesis based on 3D Template   Conditional Diffusion Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-518a217994b75df52e0f978f0486ea70.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-14-æ›´æ–°"><a href="#2025-03-14-æ›´æ–°" class="headerlink" title="2025-03-14 æ›´æ–°"></a>2025-03-14 æ›´æ–°</h1><h2 id="FCaS-Fine-grained-Cardiac-Image-Synthesis-based-on-3D-Template-Conditional-Diffusion-Model"><a href="#FCaS-Fine-grained-Cardiac-Image-Synthesis-based-on-3D-Template-Conditional-Diffusion-Model" class="headerlink" title="FCaS: Fine-grained Cardiac Image Synthesis based on 3D Template   Conditional Diffusion Model"></a>FCaS: Fine-grained Cardiac Image Synthesis based on 3D Template   Conditional Diffusion Model</h2><p><strong>Authors:Jiahao Xia, Yutao Hu, Yaolei Qi, Zhenliang Li, Wenqi Shao, Junjun He, Ying Fu, Longjiang Zhang, Guanyu Yang</strong></p>
<p>Solving medical imaging data scarcity through semantic image generation has attracted significant attention in recent years. However, existing methods primarily focus on generating whole-organ or large-tissue structures, showing limited effectiveness for organs with fine-grained structure. Due to stringent topological consistency, fragile coronary features, and complex 3D morphological heterogeneity in cardiac imaging, accurately reconstructing fine-grained anatomical details of the heart remains a great challenge. To address this problem, in this paper, we propose the Fine-grained Cardiac image Synthesis(FCaS) framework, established on 3D template conditional diffusion model. FCaS achieves precise cardiac structure generation using Template-guided Conditional Diffusion Model (TCDM) through bidirectional mechanisms, which provides the fine-grained topological structure information of target image through the guidance of template. Meanwhile, we design a deformable Mask Generation Module (MGM) to mitigate the scarcity of high-quality and diverse reference mask in the generation process. Furthermore, to alleviate the confusion caused by imprecise synthetic images, we propose a Confidence-aware Adaptive Learning (CAL) strategy to facilitate the pre-training of downstream segmentation tasks. Specifically, we introduce the Skip-Sampling Variance (SSV) estimation to obtain confidence maps, which are subsequently employed to rectify the pre-training on downstream tasks. Experimental results demonstrate that images generated from FCaS achieves state-of-the-art performance in topological consistency and visual quality, which significantly facilitates the downstream tasks as well. Code will be released in the future. </p>
<blockquote>
<p>é€šè¿‡è¯­ä¹‰å›¾åƒç”Ÿæˆè§£å†³åŒ»å­¦æˆåƒæ•°æ®ç¨€ç¼ºé—®é¢˜è¿‘å¹´æ¥å·²å¼•èµ·å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç”Ÿæˆæ•´ä¸ªå™¨å®˜æˆ–å¤§å‹ç»„ç»‡ç»“æ„ï¼Œå¯¹äºå…·æœ‰ç²¾ç»†ç»“æ„ï¼ˆå¦‚å¿ƒè„ï¼‰çš„å™¨å®˜æ˜¾ç¤ºæœ‰é™çš„æœ‰æ•ˆæ€§ã€‚ç”±äºä¸¥æ ¼çš„åœ°å½¢ä¸€è‡´æ€§ã€è„†å¼±çš„å† çŠ¶åŠ¨è„‰ç‰¹å¾å’Œå¿ƒè„æˆåƒä¸­å¤æ‚çš„3Då½¢æ€å¼‚è´¨æ€§ï¼Œç²¾ç¡®é‡å»ºå¿ƒè„ç­‰ç²¾ç»†è§£å‰–ç»“æ„ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºä¸‰ç»´æ¨¡æ¿æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„ç²¾ç»†å¿ƒè„å›¾åƒåˆæˆï¼ˆFCaSï¼‰æ¡†æ¶ã€‚FCaSåˆ©ç”¨æ¨¡æ¿å¼•å¯¼æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆTCDMï¼‰é€šè¿‡åŒå‘æœºåˆ¶å®ç°ç²¾ç¡®çš„å¿ƒè„ç»“æ„ç”Ÿæˆã€‚è¯¥æ¨¡å‹é€šè¿‡æ¨¡æ¿æŒ‡å¯¼æä¾›ç›®æ ‡å›¾åƒçš„ç²¾ç»†æ‹“æ‰‘ç»“æ„ä¿¡æ¯ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯å˜å½¢æ©è†œç”Ÿæˆæ¨¡å—ï¼ˆMGMï¼‰ï¼Œä»¥ç¼“è§£ç”Ÿæˆè¿‡ç¨‹ä¸­é«˜è´¨é‡å’Œå¤šæ ·åŒ–å‚è€ƒæ©è†œçš„ç¨€ç¼ºæ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡è½»ç”±ä¸ç²¾ç¡®åˆæˆå›¾åƒå¼•èµ·çš„æ··æ·†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªä¿¡åº¦è‡ªé€‚åº”å­¦ä¹ ï¼ˆCALï¼‰ç­–ç•¥ï¼Œä»¥ä¿ƒè¿›ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡çš„é¢„è®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥è·³è¿‡é‡‡æ ·æ–¹å·®ï¼ˆSSVï¼‰ä¼°è®¡æ¥è·å¾—ç½®ä¿¡å›¾ï¼Œç„¶åç”¨äºæ ¡æ­£ä¸‹æ¸¸ä»»åŠ¡çš„é¢„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»FCaSç”Ÿæˆçš„å›¾åƒåœ¨æ‹“æ‰‘ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œè¿™æå¤§åœ°ä¿ƒè¿›äº†ä¸‹æ¸¸ä»»åŠ¡ã€‚æœªæ¥æˆ‘ä»¬å°†å‘å¸ƒç›¸å…³ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09560v1">PDF</a> 16 pages, 9 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºä¸‰ç»´æ¨¡æ¿æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„ç²¾ç»†å¿ƒè„å›¾åƒåˆæˆï¼ˆFCaSï¼‰æ¡†æ¶ï¼Œä»¥è§£å†³åŒ»å­¦æˆåƒä¸­ç²¾ç»†å¿ƒè„ç»“æ„é‡å»ºçš„æŒ‘æˆ˜ã€‚é€šè¿‡æ¨¡æ¿å¼•å¯¼çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆTCDMï¼‰å®ç°å¿ƒè„ç»“æ„çš„ç²¾ç¡®ç”Ÿæˆï¼Œé€šè¿‡åŒå‘æœºåˆ¶åˆ©ç”¨æ¨¡æ¿æä¾›çš„ç›®æ ‡å›¾åƒçš„ç²¾ç»†æ‹“æ‰‘ç»“æ„ä¿¡æ¯ã€‚åŒæ—¶è®¾è®¡äº†ä¸€ä¸ªå¯å˜å½¢æ©è†œç”Ÿæˆæ¨¡å—ï¼ˆMGMï¼‰ï¼Œä»¥ç¼“è§£ç”Ÿæˆè¿‡ç¨‹ä¸­é«˜è´¨é‡å’Œå¤šæ ·åŒ–å‚è€ƒæ©è†œçš„ç¨€ç¼ºé—®é¢˜ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡è½»ç”±åˆæˆå›¾åƒä¸ç²¾ç¡®å¼•èµ·çš„æ··æ·†ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¿¡å¿ƒçš„è‡ªé€‚åº”å­¦ä¹ ï¼ˆCALï¼‰ç­–ç•¥ï¼Œä¿ƒè¿›ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡çš„é¢„è®­ç»ƒã€‚é€šè¿‡å¼•å…¥è·³è¿‡é‡‡æ ·æ–¹å·®ï¼ˆSSVï¼‰ä¼°è®¡å¾—åˆ°ç½®ä¿¡å›¾ï¼Œç”¨äºä¿®æ­£ä¸‹æ¸¸ä»»åŠ¡çš„é¢„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFCaSç”Ÿæˆçš„å›¾åƒåœ¨æ‹“æ‰‘ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œæå¤§åœ°ä¿ƒè¿›äº†ä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>FCaSæ¡†æ¶æ˜¯åŸºäºä¸‰ç»´æ¨¡æ¿æ¡ä»¶æ‰©æ•£æ¨¡å‹æå‡ºçš„ï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦æˆåƒä¸­ç²¾ç»†å¿ƒè„ç»“æ„é‡å»ºçš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡æ¨¡æ¿å¼•å¯¼çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆTCDMï¼‰å’ŒåŒå‘æœºåˆ¶å®ç°å¿ƒè„ç»“æ„çš„ç²¾ç¡®ç”Ÿæˆã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªå¯å˜å½¢æ©è†œç”Ÿæˆæ¨¡å—ï¼ˆMGMï¼‰ï¼Œä»¥åº”å¯¹ç”Ÿæˆè¿‡ç¨‹ä¸­é«˜è´¨é‡å’Œå¤šæ ·åŒ–å‚è€ƒæ©è†œçš„ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>å¼•å…¥åŸºäºä¿¡å¿ƒçš„è‡ªé€‚åº”å­¦ä¹ ï¼ˆCALï¼‰ç­–ç•¥ï¼Œé€šè¿‡è·³è¿‡é‡‡æ ·æ–¹å·®ï¼ˆSSVï¼‰ä¼°è®¡å¾—åˆ°ç½®ä¿¡å›¾ï¼Œä¿®æ­£ä¸‹æ¸¸ä»»åŠ¡çš„é¢„è®­ç»ƒã€‚</li>
<li>å®éªŒç»“æœå±•ç¤ºäº†FCaSåœ¨æ‹“æ‰‘ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>FCaSæ˜¾è‘—ä¿ƒè¿›äº†ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚åŒ»å­¦å›¾åƒåˆ†å‰²ç­‰ã€‚</li>
<li>æœªæ¥å°†å‘å¸ƒç›¸å…³ä»£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b83c36b78b1e23c9b825f6ba780e6126.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93c4fd92ccd0a4a53e3e91579a16b389.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dff7d91fe0845c0166aedcbd9484b6a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb194e7e8f3a8d669108cf7810111c86.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CM-Diff-A-Single-Generative-Network-for-Bidirectional-Cross-Modality-Translation-Diffusion-Model-Between-Infrared-and-Visible-Images"><a href="#CM-Diff-A-Single-Generative-Network-for-Bidirectional-Cross-Modality-Translation-Diffusion-Model-Between-Infrared-and-Visible-Images" class="headerlink" title="CM-Diff: A Single Generative Network for Bidirectional Cross-Modality   Translation Diffusion Model Between Infrared and Visible Images"></a>CM-Diff: A Single Generative Network for Bidirectional Cross-Modality   Translation Diffusion Model Between Infrared and Visible Images</h2><p><strong>Authors:Bin Hu, Chenqiang Gao, Shurui Liu, Junjie Guo, Fang Chen, Fangcen Liu</strong></p>
<p>The image translation method represents a crucial approach for mitigating information deficiencies in the infrared and visible modalities, while also facilitating the enhancement of modality-specific datasets. However, existing methods for infrared and visible image translation either achieve unidirectional modality translation or rely on cycle consistency for bidirectional modality translation, which may result in suboptimal performance. In this work, we present the cross-modality translation diffusion model (CM-Diff) for simultaneously modeling data distributions in both the infrared and visible modalities. We address this challenge by combining translation direction labels for guidance during training with cross-modality feature control. Specifically, we view the establishment of the mapping relationship between the two modalities as the process of learning data distributions and understanding modality differences, achieved through a novel Bidirectional Diffusion Training (BDT) strategy. Additionally, we propose a Statistical Constraint Inference (SCI) strategy to ensure the generated image closely adheres to the data distribution of the target modality. Experimental results demonstrate the superiority of our CM-Diff over state-of-the-art methods, highlighting its potential for generating dual-modality datasets. </p>
<blockquote>
<p>å›¾åƒè½¬æ¢æ–¹æ³•å¯¹äºå‡è½»çº¢å¤–å’Œå¯è§æ¨¡æ€ä¸­çš„ä¿¡æ¯ç¼ºå¤±è‡³å…³é‡è¦ï¼ŒåŒæ—¶æœ‰åŠ©äºå¢å¼ºç‰¹å®šæ¨¡æ€çš„æ•°æ®é›†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„çº¢å¤–å’Œå¯è§å›¾åƒè½¬æ¢æ–¹æ³•è¦ä¹ˆå®ç°å•å‘æ¨¡æ€è½¬æ¢ï¼Œè¦ä¹ˆä¾èµ–äºå¾ªç¯ä¸€è‡´æ€§è¿›è¡ŒåŒå‘æ¨¡æ€è½¬æ¢ï¼Œè¿™å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨æ¨¡æ€è½¬æ¢æ‰©æ•£æ¨¡å‹ï¼ˆCM-Diffï¼‰ï¼Œç”¨äºåŒæ—¶å»ºæ¨¡çº¢å¤–å’Œå¯è§æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒã€‚æˆ‘ä»¬é€šè¿‡ç»“åˆè®­ç»ƒè¿‡ç¨‹ä¸­çš„è½¬æ¢æ–¹å‘æ ‡ç­¾è¿›è¡Œå¼•å¯¼ä»¥åŠæ§åˆ¶è·¨æ¨¡æ€ç‰¹å¾æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å»ºç«‹ä¸¤ä¸ªæ¨¡æ€ä¹‹é—´çš„æ˜ å°„å…³ç³»è§†ä¸ºå­¦ä¹ è¿‡ç¨‹æ•°æ®åˆ†å¸ƒå’Œç†è§£æ¨¡æ€å·®å¼‚çš„è¿‡ç¨‹ï¼Œé€šè¿‡ä¸€ç§æ–°çš„åŒå‘æ‰©æ•£è®­ç»ƒï¼ˆBDTï¼‰ç­–ç•¥æ¥å®ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿè®¡çº¦æŸæ¨æ–­ï¼ˆSCIï¼‰ç­–ç•¥ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„å›¾åƒç´§å¯†ç¬¦åˆç›®æ ‡æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒã€‚å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„CM-Diffç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ï¼Œå‡¸æ˜¾äº†å…¶ç”ŸæˆåŒæ¨¡æ€æ•°æ®é›†çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09514v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€ç¿»è¯‘æ‰©æ•£æ¨¡å‹ï¼ˆCM-Diffï¼‰ï¼Œè¯¥æ¨¡å‹å¯ä»¥åŒæ—¶å»ºæ¨¡çº¢å¤–å’Œå¯è§æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒã€‚é€šè¿‡ç»“åˆç¿»è¯‘æ–¹å‘æ ‡ç­¾è¿›è¡Œè®­ç»ƒæŒ‡å¯¼ä»¥åŠè·¨æ¨¡æ€ç‰¹å¾æ§åˆ¶æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚é€šè¿‡æ–°çš„åŒå‘æ‰©æ•£è®­ç»ƒï¼ˆBDTï¼‰ç­–ç•¥æ¥å»ºç«‹ä¸¤ç§æ¨¡æ€ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œå¹¶æå‡ºç»Ÿè®¡çº¦æŸæ¨æ–­ï¼ˆSCIï¼‰ç­–ç•¥ä»¥ç¡®ä¿ç”Ÿæˆçš„å›¾åƒç´§å¯†ç¬¦åˆç›®æ ‡æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCM-Diffç›¸è¾ƒäºç°æœ‰æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ï¼Œå…·æœ‰ç”ŸæˆåŒæ¨¡æ€æ•°æ®é›†çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨æ¨¡æ€ç¿»è¯‘æ‰©æ•£æ¨¡å‹ï¼ˆCM-Diffï¼‰å¯ä»¥åŒæ—¶å»ºæ¨¡çº¢å¤–å’Œå¯è§æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡ç»“åˆç¿»è¯‘æ–¹å‘æ ‡ç­¾è¿›è¡Œè®­ç»ƒæŒ‡å¯¼ï¼Œä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>åŒå‘æ‰©æ•£è®­ç»ƒï¼ˆBDTï¼‰ç­–ç•¥ç”¨äºå»ºç«‹ä¸¤ç§æ¨¡æ€ä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚</li>
<li>ç»Ÿè®¡çº¦æŸæ¨æ–­ï¼ˆSCIï¼‰ç­–ç•¥ç¡®ä¿ç”Ÿæˆçš„å›¾åƒç¬¦åˆç›®æ ‡æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒã€‚</li>
<li>CM-Diffæ¨¡å‹åœ¨å®éªŒä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
<li>CM-Diffæ¨¡å‹æœ‰åŠ©äºå¢å¼ºæ¨¡æ€ç‰¹å®šæ•°æ®é›†ï¼Œå¹¶å‡è½»ä¿¡æ¯ç¼ºé™·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-99d504687f34b48899dcc9ca73832461.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21315c90871e6474222bf15aab7b3802.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Sparse-Autoencoder-as-a-Zero-Shot-Classifier-for-Concept-Erasing-in-Text-to-Image-Diffusion-Models"><a href="#Sparse-Autoencoder-as-a-Zero-Shot-Classifier-for-Concept-Erasing-in-Text-to-Image-Diffusion-Models" class="headerlink" title="Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in   Text-to-Image Diffusion Models"></a>Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in   Text-to-Image Diffusion Models</h2><p><strong>Authors:Zhihua Tian, Sirun Nan, Ming Xu, Shengfang Zhai, Wenjie Qu, Jian Liu, Kui Ren, Ruoxi Jia, Jiaheng Zhang</strong></p>
<p>Text-to-image (T2I) diffusion models have achieved remarkable progress in generating high-quality images but also raise peopleâ€™s concerns about generating harmful or misleading content. While extensive approaches have been proposed to erase unwanted concepts without requiring retraining from scratch, they inadvertently degrade performance on normal generation tasks. In this work, we propose Interpret then Deactivate (ItD), a novel framework to enable precise concept removal in T2I diffusion models while preserving overall performance. ItD first employs a sparse autoencoder (SAE) to interpret each concept as a combination of multiple features. By permanently deactivating the specific features associated with target concepts, we repurpose SAE as a zero-shot classifier that identifies whether the input prompt includes target concepts, allowing selective concept erasure in diffusion models. Moreover, we demonstrate that ItD can be easily extended to erase multiple concepts without requiring further training. Comprehensive experiments across celebrity identities, artistic styles, and explicit content demonstrate ItDâ€™s effectiveness in eliminating targeted concepts without interfering with normal concept generation. Additionally, ItD is also robust against adversarial prompts designed to circumvent content filters. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/NANSirun/Interpret-then-deactivate">https://github.com/NANSirun/Interpret-then-deactivate</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ä¹Ÿå¼•å‘äº†äººä»¬å¯¹ç”Ÿæˆæœ‰å®³æˆ–è¯¯å¯¼æ€§å†…å®¹çš„æ‹…å¿§ã€‚è™½ç„¶å·²æå‡ºå¹¿æ³›çš„æ–¹æ³•åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æ¶ˆé™¤ä¸éœ€è¦çš„æ¦‚å¿µï¼Œä½†å®ƒä»¬ä¼šæ— æ„ä¸­é™ä½æ­£å¸¸ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè§£é‡Šç„¶ååœç”¨â€ï¼ˆItDï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨T2Iæ‰©æ•£æ¨¡å‹ä¸­å®ç°ç²¾ç¡®çš„æ¦‚å¿µç§»é™¤ï¼ŒåŒæ—¶ä¿ç•™æ•´ä½“æ€§èƒ½ã€‚ItDé¦–å…ˆé‡‡ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰æ¥è§£é‡Šæ¯ä¸ªæ¦‚å¿µæ˜¯å¤šä¸ªç‰¹å¾çš„ç»„åˆã€‚é€šè¿‡æ°¸ä¹…åœç”¨ä¸ç›®æ ‡æ¦‚å¿µç›¸å…³çš„ç‰¹å®šç‰¹å¾ï¼Œæˆ‘ä»¬å°†SAEé‡æ–°å®šä½ä¸ºä¸€ç§é›¶æ ·æœ¬åˆ†ç±»å™¨ï¼Œç”¨äºç¡®å®šè¾“å…¥æç¤ºæ˜¯å¦åŒ…å«ç›®æ ‡æ¦‚å¿µï¼Œä»è€Œåœ¨æ‰©æ•£æ¨¡å‹ä¸­å®ç°é€‰æ‹©æ€§æ¦‚å¿µåˆ é™¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ItDå¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•åˆ°åˆ é™¤å¤šä¸ªæ¦‚å¿µï¼Œè€Œæ— éœ€è¿›ä¸€æ­¥è®­ç»ƒã€‚åœ¨åäººèº«ä»½ã€è‰ºæœ¯é£æ ¼å’Œæ˜ç¡®å†…å®¹æ–¹é¢çš„ç»¼åˆå®éªŒè¯æ˜äº†ItDåœ¨æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè€Œä¸ä¼šå¹²æ‰°æ­£å¸¸çš„æ¦‚å¿µç”Ÿæˆã€‚æ­¤å¤–ï¼ŒItDè¿˜èƒ½æœ‰æ•ˆå¯¹æŠ—æ—¨åœ¨ç»•è¿‡å†…å®¹è¿‡æ»¤å™¨çš„å¯¹æŠ—æ€§æç¤ºã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/NANSirun/Interpret-then-deactivate%E8%8E%B7%E5.html">https://github.com/NANSirun/Interpret-then-deactivateè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09446v1">PDF</a> 25 pages</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ç”Ÿæˆå›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä¹Ÿå¼•å‘äº†äººä»¬å¯¹ç”Ÿæˆæœ‰å®³æˆ–è¯¯å¯¼æ€§å†…å®¹çš„æ‹…å¿§ã€‚å°½ç®¡å·²ç»æå‡ºäº†è®¸å¤šæ–¹æ³•æ¥æ¶ˆé™¤ä¸éœ€è¦çš„æ¦‚å¿µï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒï¼Œä½†å®ƒä»¬ä¼šæ— æ„ä¸­é™ä½æ­£å¸¸ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œè§£é‡Šååœç”¨â€ï¼ˆItDï¼‰çš„æ–°æ¡†æ¶ï¼Œå¯ä»¥åœ¨T2Iæ‰©æ•£æ¨¡å‹ä¸­å®ç°ç²¾ç¡®çš„æ¦‚å¿µç§»é™¤ï¼ŒåŒæ—¶ä¿ç•™æ•´ä½“æ€§èƒ½ã€‚ItDé¦–å…ˆä½¿ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰æ¥è§£é‡Šæ¯ä¸ªæ¦‚å¿µæ˜¯å¤šä¸ªç‰¹å¾çš„ç»„åˆã€‚é€šè¿‡æ°¸ä¹…åœç”¨ä¸ç›®æ ‡æ¦‚å¿µç›¸å…³çš„ç‰¹å®šç‰¹å¾ï¼Œæˆ‘ä»¬é‡æ–°ä½¿ç”¨SAEä½œä¸ºé›¶å°„å‡»åˆ†ç±»å™¨ï¼Œç¡®å®šè¾“å…¥æç¤ºæ˜¯å¦åŒ…å«ç›®æ ‡æ¦‚å¿µï¼Œä»è€Œåœ¨æ‰©æ•£æ¨¡å‹ä¸­é€‰æ‹©æ€§åœ°æ¶ˆé™¤æ¦‚å¿µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†ItDå¯ä»¥è½»æ¾åœ°æ‰©å±•åˆ°æ¶ˆé™¤å¤šä¸ªæ¦‚å¿µï¼Œæ— éœ€è¿›ä¸€æ­¥è®­ç»ƒã€‚åœ¨åäººèº«ä»½ã€è‰ºæœ¯é£æ ¼å’Œæ˜ç¡®å†…å®¹æ–¹é¢çš„ç»¼åˆå®éªŒè¯æ˜äº†ItDåœ¨æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè€Œä¸ä¼šå¹²æ‰°æ­£å¸¸çš„æ¦‚å¿µç”Ÿæˆã€‚æ­¤å¤–ï¼ŒItDå¯¹è®¾è®¡ç”¨äºç»•è¿‡å†…å®¹è¿‡æ»¤å™¨çš„å¯¹æŠ—æ€§æç¤ºä¹Ÿå…·æœ‰é²æ£’æ€§ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/NANSirun/Interpret-then-deactivate">https://github.com/NANSirun/Interpret-then-deactivate</a>ã€‚â€‹â€‹â€‹â€‹<br>â€‹â€‹ <strong>Key Takeaways</strong></p>
<ol>
<li>T2Iæ‰©æ•£æ¨¡å‹è™½ç„¶èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†å­˜åœ¨ç”Ÿæˆæœ‰å®³æˆ–è¯¯å¯¼æ€§å†…å®¹çš„é£é™©ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å»é™¤éå¿…è¦æ¦‚å¿µæ—¶å¯èƒ½ä¼šå½±å“æ¨¡å‹çš„æ­£å¸¸ç”Ÿæˆä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>æå‡ºçš„ItDæ¡†æ¶åˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰è§£é‡Šæ¦‚å¿µå¹¶åœç”¨ç‰¹å®šç‰¹å¾ï¼Œå®ç°ç²¾ç¡®çš„æ¦‚å¿µç§»é™¤åŒæ—¶ä¿ç•™æ•´ä½“æ€§èƒ½ã€‚</li>
<li>ItDèƒ½å¤Ÿé€‰æ‹©æ€§åœ°æ¶ˆé™¤å•ä¸ªæˆ–å¤šä¸ªæ¦‚å¿µï¼Œæ— éœ€è¿›ä¸€æ­¥è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>å®éªŒè¯æ˜ItDåœ¨æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µçš„åŒæ—¶ä¸å½±å“æ­£å¸¸æ¦‚å¿µç”Ÿæˆçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ItDå¯¹å¯¹æŠ—æ€§æç¤ºå…·æœ‰é²æ£’æ€§ï¼Œèƒ½æœ‰æ•ˆé˜²æ­¢ç»•è¿‡å†…å®¹è¿‡æ»¤å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b0a082fafa63aa25f9ae8498b602059.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b078675ce50e533623857092fa441d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f10a3fd75563cb57d4626e668164615.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2dd786f3a40be6b3e0cc15428d46751f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3281c0fbb3653ba7686ca80bb34ba1dd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SuperCarver-Texture-Consistent-3D-Geometry-Super-Resolution-for-High-Fidelity-Surface-Detail-Generation"><a href="#SuperCarver-Texture-Consistent-3D-Geometry-Super-Resolution-for-High-Fidelity-Surface-Detail-Generation" class="headerlink" title="SuperCarver: Texture-Consistent 3D Geometry Super-Resolution for   High-Fidelity Surface Detail Generation"></a>SuperCarver: Texture-Consistent 3D Geometry Super-Resolution for   High-Fidelity Surface Detail Generation</h2><p><strong>Authors:Qijian Zhang, Xiaozheng Jian, Xuan Zhang, Wenping Wang, Junhui Hou</strong></p>
<p>Traditional production workflow of high-precision 3D mesh assets necessitates a cumbersome and laborious process of manual sculpting by specialized modelers. The recent years have witnessed remarkable advances in AI-empowered 3D content creation. However, although the latest state-of-the-arts are already capable of generating plausible structures and intricate appearances from images or text prompts, the actual mesh surfaces are typically over-smoothing and lack geometric details. This paper introduces SuperCarver, a 3D geometry super-resolution framework particularly tailored for adding texture-consistent surface details to given coarse meshes. Technically, we start by rendering the original textured mesh into the image domain from multiple viewpoints. To achieve geometric detail generation, we develop a deterministic prior-guided normal diffusion model fine-tuned on a carefully curated dataset of paired low-poly and high-poly normal renderings. To optimize mesh structures from potentially imperfect normal map predictions, we design a simple yet effective noise-resistant inverse rendering scheme based on distance field deformation. Extensive experiments show that SuperCarver generates realistic and expressive surface details as depicted by specific texture appearances, making it a powerful tool for automatically upgrading massive outdated low-quality assets and shortening the iteration cycle of high-quality mesh production in practical applications. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„é«˜ç²¾åº¦3Dç½‘æ ¼èµ„äº§ç”Ÿäº§æµç¨‹éœ€è¦ä¸“ä¸šå»ºæ¨¡å¸ˆè¿›è¡Œç¹çè€Œè€—æ—¶çš„æ‰‹åŠ¨é›•å¡‘ã€‚è¿‘å¹´æ¥ï¼ŒAIèµ‹èƒ½çš„3Då†…å®¹åˆ›ä½œé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå°½ç®¡æœ€æ–°çš„å…ˆè¿›æŠ€æœ¯å·²ç»èƒ½å¤Ÿä»å›¾åƒæˆ–æ–‡æœ¬æç¤ºä¸­ç”Ÿæˆåˆç†çš„ç»“æ„å’Œå¤æ‚çš„å¤–è²Œï¼Œä½†å®é™…çš„ç½‘æ ¼è¡¨é¢é€šå¸¸è¿‡äºå¹³æ»‘ï¼Œç¼ºä¹å‡ ä½•ç»†èŠ‚ã€‚æœ¬æ–‡ä»‹ç»äº†SuperCarverï¼Œè¿™æ˜¯ä¸€ä¸ª3Då‡ ä½•è¶…åˆ†è¾¨ç‡æ¡†æ¶ï¼Œç‰¹åˆ«ç”¨äºä¸ºç»™å®šçš„ç²—ç³™ç½‘æ ¼æ·»åŠ çº¹ç†ä¸€è‡´çš„è¡¨é¢ç»†èŠ‚ã€‚</p>
</blockquote>
<p>æŠ€æœ¯ä¸Šï¼Œæˆ‘ä»¬é¦–å…ˆä»å¤šä¸ªè§†ç‚¹å°†åŸå§‹çº¹ç†ç½‘æ ¼æ¸²æŸ“åˆ°å›¾åƒåŸŸã€‚ä¸ºäº†å®ç°å‡ ä½•ç»†èŠ‚ç”Ÿæˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç¡®å®šæ€§å…ˆéªŒå¼•å¯¼çš„æ­£å¸¸æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ç²¾å¿ƒæŒ‘é€‰çš„ä½å¤šè¾¹å½¢å’Œé«˜å¤šè¾¹å½¢æ­£å¸¸æ¸²æŸ“é…å¯¹æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†ä»å¯èƒ½ä¸å®Œç¾çš„æ³•çº¿æ˜ å°„é¢„æµ‹ç»“æœä¼˜åŒ–ç½‘æ ¼ç»“æ„ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æŠ—å™ªå£°é€†å‘æ¸²æŸ“æ–¹æ¡ˆï¼ŒåŸºäºè·ç¦»åœºå˜å½¢ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSuperCarverèƒ½å¤Ÿç”Ÿæˆä¸ç‰¹å®šçº¹ç†å¤–è§‚ç›¸ç¬¦çš„ç°å®å’Œè¡¨è¾¾æ€§è¡¨é¢ç»†èŠ‚ï¼Œä½¿å…¶æˆä¸ºè‡ªåŠ¨å‡çº§å¤§é‡è¿‡æ—¶ä½è´¨é‡èµ„äº§ã€ç¼©çŸ­é«˜è´¨é‡ç½‘æ ¼åˆ¶ä½œè¿­ä»£å‘¨æœŸçš„å¼ºå¤§å·¥å…·ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09439v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºSuperCarverçš„3Då‡ ä½•è¶…åˆ†è¾¨ç‡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç‰¹åˆ«è®¾è®¡ç”¨äºä¸ºç»™å®šçš„ç²—ç³™ç½‘æ ¼æ·»åŠ çº¹ç†ä¸€è‡´çš„è¡¨é¢ç»†èŠ‚ã€‚é€šè¿‡ä»å¤šä¸ªè§†ç‚¹å°†åŸå§‹çº¹ç†ç½‘æ ¼æ¸²æŸ“åˆ°å›¾åƒåŸŸï¼Œå¹¶ç»“åˆç¡®å®šæ€§å…ˆéªŒå¼•å¯¼çš„æ­£å¸¸æ‰©æ•£æ¨¡å‹ï¼Œåœ¨ç²¾å¿ƒæŒ‘é€‰çš„ä½å¤šè¾¹å½¢å’Œé«˜å¤šè¾¹å½¢æ­£å¸¸æ¸²æŸ“æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥ç”Ÿæˆå‡ ä½•ç»†èŠ‚ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æŠ—å™ªå£°é€†å‘æ¸²æŸ“æ–¹æ¡ˆï¼ŒåŸºäºè·ç¦»åœºå˜å½¢ä¼˜åŒ–ç½‘æ ¼ç»“æ„ã€‚SuperCarverèƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„è¡¨é¢ç»†èŠ‚ï¼Œç¼©çŸ­é«˜è´¨é‡ç½‘æ ¼ç”Ÿäº§çš„è¿­ä»£å‘¨æœŸï¼Œæ˜¯å‡çº§å¤§é‡æ—§çš„ä½è´¨é‡èµ„äº§çš„æœ‰åŠ›å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SuperCarveræ¡†æ¶ç”¨äºä¸ºç»™å®šçš„ç²—ç³™ç½‘æ ¼æ·»åŠ çº¹ç†ä¸€è‡´çš„è¡¨é¢ç»†èŠ‚ã€‚</li>
<li>é€šè¿‡ä»å¤šä¸ªè§†ç‚¹æ¸²æŸ“åŸå§‹çº¹ç†ç½‘æ ¼åˆ°å›¾åƒåŸŸæ¥å®ç°ç»†èŠ‚ç”Ÿæˆã€‚</li>
<li>ç»“åˆç¡®å®šæ€§å…ˆéªŒå¼•å¯¼çš„æ­£å¸¸æ‰©æ•£æ¨¡å‹ï¼Œåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒä»¥ç”Ÿæˆå‡ ä½•ç»†èŠ‚ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§åŸºäºè·ç¦»åœºå˜å½¢çš„æŠ—å™ªå£°é€†å‘æ¸²æŸ“æ–¹æ¡ˆæ¥ä¼˜åŒ–ç½‘æ ¼ç»“æ„ã€‚</li>
<li>SuperCarverèƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„è¡¨é¢ç»†èŠ‚ï¼Œç‰¹åˆ«é€‚åˆå‡çº§æ—§çš„ä½è´¨é‡èµ„äº§ã€‚</li>
<li>è¯¥æ¡†æ¶ç¼©çŸ­äº†é«˜è´¨é‡ç½‘æ ¼ç”Ÿäº§çš„è¿­ä»£å‘¨æœŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c80c88de51dd02eb60309db43fc9ebe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0cfef260e539755edaa9fe60e475830.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7b2fdd85bf18117d0600051730d6a88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d84c162a9cd32a9087485679946dec26.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Alias-Free-Latent-Diffusion-Models-Improving-Fractional-Shift-Equivariance-of-Diffusion-Latent-Space"><a href="#Alias-Free-Latent-Diffusion-Models-Improving-Fractional-Shift-Equivariance-of-Diffusion-Latent-Space" class="headerlink" title="Alias-Free Latent Diffusion Models:Improving Fractional Shift   Equivariance of Diffusion Latent Space"></a>Alias-Free Latent Diffusion Models:Improving Fractional Shift   Equivariance of Diffusion Latent Space</h2><p><strong>Authors:Yifan Zhou, Zeqi Xiao, Shuai Yang, Xingang Pan</strong></p>
<p>Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) self-attention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/SingleZombie/AFLDM">https://github.com/SingleZombie/AFLDM</a> </p>
<blockquote>
<p>æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰çš„ç”Ÿæˆè¿‡ç¨‹ä¸ç¨³å®šï¼Œè¾“å…¥å™ªå£°çš„å¾®å°æ‰°åŠ¨æˆ–å˜åŒ–éƒ½å¯èƒ½å¯¼è‡´æ˜¾è‘—ä¸åŒçš„è¾“å‡ºã€‚è¿™é˜»ç¢äº†å®ƒä»¬åœ¨éœ€è¦ä¸€è‡´ç»“æœçš„åº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°è®¾è®¡äº†LDMsï¼Œé€šè¿‡ä½¿å…¶å…·æœ‰å¹³ç§»ç­‰å˜æ€§æ¥å¢å¼ºä¸€è‡´æ€§ã€‚è™½ç„¶å¼•å…¥æŠ—æ··å æ“ä½œå¯ä»¥éƒ¨åˆ†æ”¹å–„å¹³ç§»ç­‰å˜æ€§ï¼Œä½†ç”±äºLDMä¸­çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œä»ç„¶å­˜åœ¨ä¸¥é‡çš„æ··å å’Œä¸ä¸€è‡´æ€§ï¼ŒåŒ…æ‹¬1ï¼‰åœ¨VAEè®­ç»ƒå’Œå¤šU-Netæ¨ç†è¿‡ç¨‹ä¸­çš„æ··å æ”¾å¤§ï¼Œä»¥åŠ2ï¼‰æœ¬è´¨ä¸Šç¼ºä¹å¹³ç§»ç­‰å˜æ€§çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é‡æ–°è®¾è®¡äº†æ³¨æ„åŠ›æ¨¡å—ä»¥å®ç°å¹³ç§»ç­‰å˜æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç­‰æ•ˆæ€§æŸå¤±ï¼Œè¯¥æŸå¤±æœ‰æ•ˆåœ°æŠ‘åˆ¶äº†è¿ç»­åŸŸä¸­ç‰¹å¾çš„é¢‘ç‡å¸¦å®½ã€‚ç”±æ­¤äº§ç”Ÿçš„æ— æ··å LDMï¼ˆAF-LDMï¼‰å®ç°äº†å¼ºå¤§çš„å¹³ç§»ç­‰å˜æ€§ï¼Œå¹¶ä¸”å¯¹ä¸è§„åˆ™å˜å½¢å…·æœ‰é²æ£’æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAF-LDMåœ¨å„ç§åº”ç”¨ä¸­äº§ç”Ÿçš„ç»“æœæ¯”åŸå§‹LDMæ›´åŠ ä¸€è‡´ï¼ŒåŒ…æ‹¬è§†é¢‘ç¼–è¾‘å’Œå›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/AFLDM%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/SingleZombie/AFLDMè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09419v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦å…³æ³¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼Œç®€ç§°LDMsï¼‰çš„ä¸€è‡´æ€§é—®é¢˜ã€‚ä¸ºäº†æé«˜æ¨¡å‹åœ¨ä¸åŒåº”ç”¨ä¸­çš„ç¨³å®šæ€§å’Œä¸€è‡´æ€§ï¼Œç ”ç©¶äººå‘˜å¯¹å…¶è¿›è¡Œé‡æ–°è®¾è®¡ä»¥æé«˜å…¶ä½ç§»ç­‰ä»·æ€§ã€‚ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä¼˜åŒ–åæ··å æ“ä½œåŠé‡æ„æ³¨æ„æ¨¡å—ä»¥å¢åŠ æ¨¡å‹çš„ä¸€è‡´æ€§å¹¶å…‹æœç‰¹å®šçš„æ¨¡å‹æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶æˆæœä»¥ä¸€ä¸ªæ”¹è¿›ç‰ˆå‘½åä¸ºåˆ«åæ— LDMï¼ˆAF-LDMï¼‰ï¼Œåœ¨è§†é¢‘ç¼–è¾‘å’Œå›¾åƒåˆ°å›¾åƒç¿»è¯‘ç­‰åº”ç”¨ä¸­è¡¨ç°æ˜¾è‘—ä¼˜äºå¸¸è§„LDMã€‚ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDMå…·æœ‰ä¸ç¨³å®šçš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå¯¹è¾“å…¥å™ªå£°çš„å¾®å°å˜åŒ–ååº”æ•æ„Ÿï¼Œå¯èƒ½å¯¼è‡´è¾“å‡ºç»“æœçš„å·¨å¤§å·®å¼‚ã€‚è¿™é™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚</li>
<li>ä¸ºäº†æé«˜LDMçš„ä¸€è‡´æ€§ï¼Œç ”ç©¶äººå‘˜é€šè¿‡è®¾è®¡ä½¿å…¶å…·æœ‰ä½ç§»ç­‰ä»·æ€§æ¥æ”¹è¿›æ¨¡å‹ã€‚è¿™æœ‰åŠ©äºæ¨¡å‹åœ¨ä¸åŒæƒ…å¢ƒä¸‹ç”Ÿæˆæ›´ç¨³å®šçš„ç»“æœã€‚</li>
<li>åæ··å æ“ä½œçš„éƒ¨åˆ†å¼•å…¥æœ‰åŠ©äºå¢å¼ºæ¨¡å‹çš„ä½ç§»ç­‰ä»·æ€§ï¼Œä½†LDMçš„å›ºæœ‰æŒ‘æˆ˜ä»å¯èƒ½å¯¼è‡´æ··å å’Œä¸ä¸€è‡´æ€§ã€‚è¿™äº›æŒ‘æˆ˜åŒ…æ‹¬åœ¨VAEè®­ç»ƒæœŸé—´çš„æ··å æ”¾å¤§å’Œå¤šä¸ªU-Netæ¨æ–­ç­‰ã€‚</li>
<li>LDMä¸­çš„è‡ªæ³¨æ„åŠ›æ¨¡å—æœ¬è´¨ä¸Šç¼ºä¹ä½ç§»ç­‰ä»·æ€§ï¼Œå› æ­¤ç ”ç©¶äººå‘˜é‡æ–°è®¾è®¡äº†æ³¨æ„åŠ›æ¨¡å—ä»¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ä¸ºäº†æ›´æœ‰æ•ˆåœ°æé«˜ä¸€è‡´æ€§ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§ç­‰ä»·æŸå¤±å‡½æ•°ï¼Œç”¨äºæŠ‘åˆ¶è¿ç»­åŸŸä¸­çš„ç‰¹å¾é¢‘ç‡å¸¦å®½ã€‚è¿™ä¸€æ”¹è¿›æœ‰åŠ©äºAF-LDMå®ç°æ›´ç¨³å®šçš„æ€§èƒ½ã€‚</li>
<li>AF-LDMåœ¨è§†é¢‘ç¼–è¾‘å’Œå›¾åƒåˆ°å›¾åƒç¿»è¯‘ç­‰åº”ç”¨ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œç›¸è¾ƒäºå¸¸è§„LDMèƒ½ç”Ÿæˆæ›´ä¸€è‡´çš„ç»“æœã€‚è¿™è¡¨æ˜AF-LDMåœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰æ›´é«˜çš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2b1012620b0eba499948e53db16ffe2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a671201e0c757dbebc4713a57d2bcda.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-898d72dd75d4a3c968cf26cef050c908.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4598a30be8cdac79f2877ee3991ae5f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6997a1ba7f8d12f60da090caead5a444.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Diff-CL-A-Novel-Cross-Pseudo-Supervision-Method-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Diff-CL-A-Novel-Cross-Pseudo-Supervision-Method-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Diff-CL: A Novel Cross Pseudo-Supervision Method for Semi-supervised   Medical Image Segmentation"></a>Diff-CL: A Novel Cross Pseudo-Supervision Method for Semi-supervised   Medical Image Segmentation</h2><p><strong>Authors:Xiuzhen Guo, Lianyuan Yu, Ji Shi, Na Lei, Hongxiao Wang</strong></p>
<p>Semi-supervised learning utilizes insights from unlabeled data to improve model generalization, thereby reducing reliance on large labeled datasets. Most existing studies focus on limited samples and fail to capture the overall data distribution. We contend that combining distributional information with detailed information is crucial for achieving more robust and accurate segmentation results. On the one hand, with its robust generative capabilities, diffusion models (DM) learn data distribution effectively. However, it struggles with fine detail capture, leading to generated images with misleading details. Combining DM with convolutional neural networks (CNNs) enables the former to learn data distribution while the latter corrects fine details. While capturing complete high-frequency details by CNNs requires substantial computational resources and is susceptible to local noise. On the other hand, given that both labeled and unlabeled data come from the same distribution, we believe that regions in unlabeled data similar to overall class semantics to labeled data are likely to belong to the same class, while regions with minimal similarity are less likely to. This work introduces a semi-supervised medical image segmentation framework from the distribution perspective (Diff-CL). Firstly, we propose a cross-pseudo-supervision learning mechanism between diffusion and convolution segmentation networks. Secondly, we design a high-frequency mamba module to capture boundary and detail information globally. Finally, we apply contrastive learning for label propagation from labeled to unlabeled data. Our method achieves state-of-the-art (SOTA) performance across three datasets, including left atrium, brain tumor, and NIH pancreas datasets. </p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®çš„è§è§£æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œå‡å°‘äº†å¯¹å¤§é‡æœ‰æ ‡ç­¾æ•°æ®é›†çš„ä¾èµ–ã€‚ç°æœ‰çš„å¤§å¤šæ•°ç ”ç©¶é›†ä¸­åœ¨æœ‰é™æ ·æœ¬ä¸Šï¼Œæ— æ³•æ•æ‰æ•´ä½“æ•°æ®åˆ†å¸ƒã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå°†åˆ†å¸ƒä¿¡æ¯ä¸è¯¦ç»†ä¿¡æ¯ç›¸ç»“åˆå¯¹äºå®ç°æ›´ç¨³å¥å’Œå‡†ç¡®çš„åˆ†å‰²ç»“æœè‡³å…³é‡è¦ã€‚ä¸€æ–¹é¢ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ æ•°æ®åˆ†å¸ƒã€‚ç„¶è€Œï¼Œå®ƒåœ¨æ•æ‰ç»†èŠ‚æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´ç”Ÿæˆçš„å›¾åƒç»†èŠ‚è¯¯å¯¼ã€‚å°†DMä¸å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ç›¸ç»“åˆï¼Œä½¿DMèƒ½å¤Ÿå­¦ä¹ æ•°æ®åˆ†å¸ƒï¼Œè€Œåè€…åˆ™æ ¡æ­£ç»†èŠ‚ã€‚è™½ç„¶CNNæ•æ‰å®Œæ•´çš„é«˜é¢‘ç»†èŠ‚éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œå¹¶å®¹æ˜“å—åˆ°å±€éƒ¨å™ªå£°çš„å½±å“ã€‚å¦ä¸€æ–¹é¢ï¼Œé‰´äºæœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾æ•°æ®æ¥è‡ªåŒä¸€åˆ†å¸ƒï¼Œæˆ‘ä»¬è®¤ä¸ºæ— æ ‡ç­¾æ•°æ®ä¸­ä¸æœ‰æ ‡ç­¾æ•°æ®çš„æ•´ä½“ç±»åˆ«è¯­ä¹‰ç›¸ä¼¼çš„åŒºåŸŸå¾ˆå¯èƒ½å±äºåŒä¸€ç±»åˆ«ï¼Œè€Œç›¸ä¼¼æ€§æå°çš„åŒºåŸŸåˆ™ä¸å¤ªå¯èƒ½å±äºåŒä¸€ç±»åˆ«ã€‚è¿™é¡¹å·¥ä½œä»åˆ†å¸ƒè§’åº¦å¼•å…¥äº†ä¸€ä¸ªåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼ˆDiff-CLï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ‰©æ•£å’Œå·ç§¯åˆ†å‰²ç½‘ç»œä¹‹é—´çš„è·¨ä¼ªç›‘ç£å­¦ä¹ æœºåˆ¶ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé«˜é¢‘mambaæ¨¡å—ï¼Œä»¥å…¨å±€æ•è·è¾¹ç•Œå’Œè¯¦ç»†ä¿¡æ¯ã€‚æœ€åï¼Œæˆ‘ä»¬åº”ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œå®ç°ä»æœ‰æ ‡ç­¾æ•°æ®åˆ°æ— æ ‡ç­¾æ•°æ®çš„æ ‡ç­¾ä¼ æ’­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å·¦å¿ƒæˆ¿ã€è„‘è‚¿ç˜¤å’ŒNIHèƒ°è…ºæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09408v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŠç›‘ç£å­¦ä¹ é€šè¿‡åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®çš„è§è§£æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œå‡å°‘ä¾èµ–å¤§é‡æœ‰æ ‡ç­¾æ•°æ®é›†ã€‚æœ¬ç ”ç©¶ç»“åˆåˆ†å¸ƒä¿¡æ¯å’Œç»†èŠ‚ä¿¡æ¯å¯¹äºå®ç°æ›´ç¨³å¥å’Œå‡†ç¡®çš„åˆ†å‰²ç»“æœè‡³å…³é‡è¦ã€‚æ‰©æ•£æ¨¡å‹æœ‰æ•ˆå­¦ä¹ æ•°æ®åˆ†å¸ƒï¼Œä½†éš¾ä»¥æ•æ‰ç²¾ç»†ç»†èŠ‚ã€‚å°†å…¶ä¸å·ç§¯ç¥ç»ç½‘ç»œç»“åˆï¼Œå‰è€…å­¦ä¹ æ•°æ®åˆ†å¸ƒè€Œåè€…æ ¡æ­£ç»†èŠ‚ã€‚æœ¬ç ”ç©¶ä»åˆ†å¸ƒè§’åº¦å¼•å…¥åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼ˆDiff-CLï¼‰ï¼Œæå‡ºäº¤å‰ä¼ªç›‘ç£å­¦ä¹ æœºåˆ¶ã€è®¾è®¡é«˜é¢‘mambaæ¨¡å—ä»¥å…¨å±€æ•è·è¾¹ç•Œå’Œç»†èŠ‚ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ è¿›è¡Œæ ‡ç­¾ä»æœ‰æ ‡ç­¾æ•°æ®å‘æ— æ ‡ç­¾æ•°æ®çš„ä¼ æ’­ã€‚è¯¥æ–¹æ³•åœ¨å·¦å¿ƒæˆ¿ã€è„‘è‚¿ç˜¤å’ŒNIHèƒ°è…ºæ•°æ®é›†ä¸Šå‡è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£å­¦ä¹ åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ä¸å·ç§¯ç¥ç»ç½‘ç»œçš„ç»“åˆèƒ½äº’è¡¥å„è‡ªçš„ä¼˜åŠ¿ï¼Œæé«˜å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æ“…é•¿å­¦ä¹ æ•°æ®åˆ†å¸ƒï¼Œè€Œå·ç§¯ç¥ç»ç½‘ç»œæ“…é•¿æ•æ‰ç²¾ç»†ç»†èŠ‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼ˆDiff-CLï¼‰ï¼ŒåŒ…å«äº¤å‰ä¼ªç›‘ç£å­¦ä¹ æœºåˆ¶å’Œé«˜é¢‘mambaæ¨¡å—ã€‚</li>
<li>é«˜é¢‘mambaæ¨¡å—èƒ½å¤Ÿå…¨å±€æ•è·è¾¹ç•Œå’Œç»†èŠ‚ä¿¡æ¯ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ ç”¨äºæ ‡ç­¾ä»æœ‰æ ‡ç­¾æ•°æ®å‘æ— æ ‡ç­¾æ•°æ®çš„ä¼ æ’­ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09408">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed42306ff3be5eb7a6fa8448a1561fb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b510491eca16c247dcc159a0f7810630.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="UniCombine-Unified-Multi-Conditional-Combination-with-Diffusion-Transformer"><a href="#UniCombine-Unified-Multi-Conditional-Combination-with-Diffusion-Transformer" class="headerlink" title="UniCombine: Unified Multi-Conditional Combination with Diffusion   Transformer"></a>UniCombine: Unified Multi-Conditional Combination with Diffusion   Transformer</h2><p><strong>Authors:Haoxuan Wang, Jinlong Peng, Qingdong He, Hao Yang, Ying Jin, Jiafu Wu, Xiaobin Hu, Yanjie Pan, Zhenye Gan, Mingmin Chi, Bo Peng, Yabiao Wang</strong></p>
<p>With the rapid development of diffusion models in image generation, the demand for more powerful and flexible controllable frameworks is increasing. Although existing methods can guide generation beyond text prompts, the challenge of effectively combining multiple conditional inputs while maintaining consistency with all of them remains unsolved. To address this, we introduce UniCombine, a DiT-based multi-conditional controllable generative framework capable of handling any combination of conditions, including but not limited to text prompts, spatial maps, and subject images. Specifically, we introduce a novel Conditional MMDiT Attention mechanism and incorporate a trainable LoRA module to build both the training-free and training-based versions. Additionally, we propose a new pipeline to construct SubjectSpatial200K, the first dataset designed for multi-conditional generative tasks covering both the subject-driven and spatially-aligned conditions. Extensive experimental results on multi-conditional generation demonstrate the outstanding universality and powerful capability of our approach with state-of-the-art performance. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå¯¹æ›´å¼ºå¤§ã€æ›´çµæ´»çš„å¯æ§æ¡†æ¶çš„éœ€æ±‚ä¹Ÿåœ¨å¢åŠ ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•å¯ä»¥åœ¨æ–‡æœ¬æç¤ºä¹‹å¤–å¼•å¯¼ç”Ÿæˆï¼Œä½†å¦‚ä½•åœ¨ä¿æŒä¸æ‰€æœ‰æ¡ä»¶ä¸€è‡´çš„åŒæ—¶æœ‰æ•ˆåœ°ç»“åˆå¤šä¸ªæ¡ä»¶è¾“å…¥ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniCombineï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºDiTçš„å¤šæ¡ä»¶å¯æ§ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå¤„ç†ä»»ä½•æ¡ä»¶ç»„åˆï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ–‡æœ¬æç¤ºã€ç©ºé—´åœ°å›¾å’Œä¸»é¢˜å›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„Conditional MMDiT Attentionæœºåˆ¶ï¼Œå¹¶èå…¥äº†ä¸€ä¸ªå¯è®­ç»ƒçš„LoRAæ¨¡å—ï¼Œæ¥æ„å»ºå…è®­ç»ƒçš„å’ŒåŸºäºè®­ç»ƒçš„ä¸¤ä¸ªç‰ˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æµç¨‹æ¥æ„å»ºSubjectSpatial200Kæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸“ä¸ºå¤šæ¡ä»¶ç”Ÿæˆä»»åŠ¡è®¾è®¡çš„ç¬¬ä¸€ä¸ªæ•°æ®é›†ï¼Œæ¶µç›–ä¸»é¢˜é©±åŠ¨å’Œç©ºé—´å¯¹é½çš„æ¡ä»¶ã€‚åœ¨å¤šç§æ¡ä»¶ä¸‹çš„ç”Ÿæˆå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰å‡ºè‰²çš„é€šç”¨æ€§å’Œå¼ºå¤§çš„èƒ½åŠ›ï¼Œæ€§èƒ½å¤„äºä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09277v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå¯¹æ›´å¼ºå¤§ã€æ›´çµæ´»çš„å¯æ§æ¡†æ¶çš„éœ€æ±‚ä¸æ–­å¢åŠ ã€‚ç°æœ‰æ–¹æ³•è™½èƒ½æŒ‡å¯¼è¶…è¶Šæ–‡æœ¬æç¤ºçš„ç”Ÿæˆï¼Œä½†å¦‚ä½•åœ¨ä¿æŒæ‰€æœ‰æ¡ä»¶ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆç»“åˆå¤šä¸ªæ¡ä»¶è¾“å…¥ä»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†UniCombineï¼Œä¸€ä¸ªåŸºäºDiTçš„å¤šæ¡ä»¶å¯æ§ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå¤„ç†å„ç§æ¡ä»¶ç»„åˆï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ–‡æœ¬æç¤ºã€ç©ºé—´åœ°å›¾å’Œä¸»é¢˜å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„åº”ç”¨å¿«é€Ÿå‘å±•ï¼Œå¯¹æ›´å…ˆè¿›ã€å¤šåŠŸèƒ½çš„å¯æ§æ¡†æ¶éœ€æ±‚å¢åŠ ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨ç»“åˆå¤šä¸ªæ¡ä»¶è¾“å…¥æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥ä¿æŒæ‰€æœ‰æ¡ä»¶çš„ä¸€è‡´æ€§ã€‚</li>
<li>UniCombineæ˜¯ä¸€ä¸ªåŸºäºDiTçš„å¤šæ¡ä»¶å¯æ§ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå¤„ç†å„ç§æ¡ä»¶ç»„åˆã€‚</li>
<li>å¼•å…¥äº†æ–°å‹çš„Conditional MMDiT Attentionæœºåˆ¶ï¼Œå¹¶ç»“åˆå¯è®­ç»ƒçš„LoRAæ¨¡å—ï¼Œæ„å»ºæ— éœ€è®­ç»ƒæˆ–åŸºäºè®­ç»ƒçš„ä¸¤ä¸ªç‰ˆæœ¬ã€‚</li>
<li>æå‡ºäº†æ„å»ºSubjectSpatial200Kæ•°æ®é›†çš„æ–°æµç¨‹ï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºå¤šæ¡ä»¶ç”Ÿæˆä»»åŠ¡è®¾è®¡çš„ç¬¬ä¸€ä¸ªæ•°æ®é›†ï¼Œæ¶µç›–ä¸»é¢˜é©±åŠ¨å’Œç©ºé—´å¯¹é½çš„æ¡ä»¶ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒUniCombineæ–¹æ³•åœ¨å¤šç§æ¡ä»¶ä¸‹çš„ç”Ÿæˆèƒ½åŠ›å…·æœ‰å‡ºè‰²çš„é€šç”¨æ€§å’Œå¼ºå¤§æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d2c7fa5da37ee6817d6f071cf84fc6f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd0055c24e424f2d16fc16aedce0bffa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c847aa88a7a0b80830017929e6f57718.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4950abf13e55b8cf46b275226e31996d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Reangle-A-Video-4D-Video-Generation-as-Video-to-Video-Translation"><a href="#Reangle-A-Video-4D-Video-Generation-as-Video-to-Video-Translation" class="headerlink" title="Reangle-A-Video: 4D Video Generation as Video-to-Video Translation"></a>Reangle-A-Video: 4D Video Generation as Video-to-Video Translation</h2><p><strong>Authors:Hyeonho Jeong, Suhyeon Lee, Jong Chul Ye</strong></p>
<p>We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: <a target="_blank" rel="noopener" href="https://hyeonho99.github.io/reangle-a-video/">https://hyeonho99.github.io/reangle-a-video/</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Reangle-A-Videoï¼Œè¿™æ˜¯ä¸€ä¸ªä»å•ä¸ªè¾“å…¥è§†é¢‘ç”ŸæˆåŒæ­¥å¤šè§†è§’è§†é¢‘çš„ç»Ÿä¸€æ¡†æ¶ã€‚ä¸åŒäºä¸»æµæ–¹æ³•åœ¨å¤§å‹4Dæ•°æ®é›†ä¸Šè®­ç»ƒå¤šè§†è§’è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†å¤šè§†è§’è§†é¢‘ç”Ÿæˆä»»åŠ¡é‡æ–°å®šä½ä¸ºè§†é¢‘åˆ°è§†é¢‘çš„ç¿»è¯‘ï¼Œå¹¶åˆ©ç”¨å¯å…¬å¼€è·å–çš„å›¾åƒå’Œè§†é¢‘æ‰©æ•£å…ˆéªŒã€‚æœ¬è´¨ä¸Šï¼ŒReangle-A-Videoåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯å¤šè§†è§’è¿åŠ¨å­¦ä¹ ï¼šä»¥è‡ªç›‘ç£çš„æ–¹å¼åŒæ­¥å¾®è°ƒå›¾åƒåˆ°è§†é¢‘æ‰©æ•£è½¬æ¢å™¨ï¼Œä»ä¸€ç»„å˜å½¢è§†é¢‘ä¸­æå–è§†è§’ä¸å˜çš„è¿åŠ¨ã€‚ç¬¬äºŒé˜¶æ®µæ˜¯å¤šè§†è§’ä¸€è‡´å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ï¼šè¾“å…¥è§†é¢‘çš„ç¬¬ä¸€å¸§åœ¨æ¨ç†æ—¶é—´è·¨è§†è§’ä¸€è‡´æ€§æŒ‡å¯¼ä¸‹è¢«å˜å½¢å’Œå¡«å……åˆ°å„ç§ç›¸æœºè§†è§’ï¼Œä½¿ç”¨DUSt3Rç”Ÿæˆå¤šè§†è§’ä¸€è‡´çš„é¦–å¸§å›¾åƒã€‚åœ¨é™æ€è§†è§’ä¼ è¾“å’ŒåŠ¨æ€ç›¸æœºæ§åˆ¶æ–¹é¢çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒReangle-A-Videoè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºå¤šè§†è§’è§†é¢‘ç”Ÿæˆå»ºç«‹äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://hyeonho99.github.io/reangle-a-video/%E3%80%82">https://hyeonho99.github.io/reangle-a-video/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09151v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://hyeonho99.github.io/reangle-a-video/">https://hyeonho99.github.io/reangle-a-video/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Reangle-A-Videoæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»å•ä¸€è¾“å…¥è§†é¢‘ç”ŸæˆåŒæ­¥å¤šè§†è§’è§†é¢‘ã€‚ä¸åŒäºä¸»æµæ–¹æ³•åœ¨å¤§å‹4Dæ•°æ®é›†ä¸Šè®­ç»ƒå¤šè§†è§’è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ŒReangle-A-Videoå°†å¤šè§†è§’è§†é¢‘ç”Ÿæˆä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºè§†é¢‘åˆ°è§†é¢‘çš„ç¿»è¯‘ï¼Œå¹¶åˆ©ç”¨å…¬å¼€å¯ç”¨çš„å›¾åƒå’Œè§†é¢‘æ‰©æ•£å…ˆéªŒã€‚å…¶æ ¸å¿ƒæ“ä½œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆæ˜¯å¤šè§†è§’è¿åŠ¨å­¦ä¹ ï¼Œé€šè¿‡è‡ªç›‘ç£æ–¹å¼åŒæ­¥å¾®è°ƒå›¾åƒåˆ°è§†é¢‘æ‰©æ•£è½¬æ¢å™¨ï¼Œä»ä¸€ç»„å˜å½¢è§†é¢‘ä¸­æå–è§†è§’ä¸å˜çš„è¿åŠ¨ï¼›å…¶æ¬¡æ˜¯å¤šè§†è§’ä¸€è‡´å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ï¼Œå°†è¾“å…¥è§†é¢‘çš„ç¬¬ä¸€å¸§åœ¨æ¨ç†æ—¶é—´ä¸‹è¿›è¡Œè·¨è§†è§’ä¸€è‡´æ€§æŒ‡å¯¼ï¼Œå˜å½¢å¹¶å¡«å……æˆå„ç§ç›¸æœºè§†è§’ï¼Œç”Ÿæˆå¤šè§†è§’ä¸€è‡´çš„å¼€å§‹å›¾åƒã€‚å®éªŒè¡¨æ˜ï¼ŒReangle-A-Videoåœ¨é™æ€è§†è§’ä¼ è¾“å’ŒåŠ¨æ€ç›¸æœºæ§åˆ¶æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºå¤šè§†è§’è§†é¢‘ç”Ÿæˆæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Reangle-A-Videoæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå¯ä»¥ä»å•ä¸€è¾“å…¥è§†é¢‘ç”ŸæˆåŒæ­¥å¤šè§†è§’è§†é¢‘ã€‚</li>
<li>ä¸ä¸»æµæ–¹æ³•ä¸åŒï¼ŒReangle-A-Videoå°†å¤šè§†è§’è§†é¢‘ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºè§†é¢‘åˆ°è§†é¢‘çš„ç¿»è¯‘ä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å›¾åƒå’Œè§†é¢‘æ‰©æ•£å…ˆéªŒã€‚</li>
<li>Reangle-A-Videoæ“ä½œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå¤šè§†è§’è¿åŠ¨å­¦ä¹ å’Œå¤šè§†è§’ä¸€è‡´å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ã€‚</li>
<li>é€šè¿‡è‡ªç›‘ç£æ–¹å¼åŒæ­¥å¾®è°ƒå›¾åƒåˆ°è§†é¢‘æ‰©æ•£è½¬æ¢å™¨ï¼Œæå–è§†è§’ä¸å˜çš„è¿åŠ¨ã€‚</li>
<li>åœ¨æ¨ç†æ—¶é—´ä¸‹ï¼Œåˆ©ç”¨è·¨è§†è§’ä¸€è‡´æ€§æŒ‡å¯¼ç”Ÿæˆå¤šè§†è§’ä¸€è‡´çš„å¼€å§‹å›¾åƒã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒReangle-A-Videoåœ¨é™æ€è§†è§’ä¼ è¾“å’ŒåŠ¨æ€ç›¸æœºæ§åˆ¶æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºå¤šè§†è§’è§†é¢‘ç”Ÿæˆæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09151">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-170cd7be7f5ed78ae237c5de7aecd105.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f78a1bb6134454b0aa5d26d13966d989.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9c42602c21b81f4a9b9c1e75bf0491a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6b7950510b9a10e64706e857d8e34f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4cc5663c575eb9a60228af9158e38ef4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AdvAD-Exploring-Non-Parametric-Diffusion-for-Imperceptible-Adversarial-Attacks"><a href="#AdvAD-Exploring-Non-Parametric-Diffusion-for-Imperceptible-Adversarial-Attacks" class="headerlink" title="AdvAD: Exploring Non-Parametric Diffusion for Imperceptible Adversarial   Attacks"></a>AdvAD: Exploring Non-Parametric Diffusion for Imperceptible Adversarial   Attacks</h2><p><strong>Authors:Jin Li, Ziqiang He, Anwei Luo, Jian-Fang Hu, Z. Jane Wang, Xiangui Kang</strong></p>
<p>Imperceptible adversarial attacks aim to fool DNNs by adding imperceptible perturbation to the input data. Previous methods typically improve the imperceptibility of attacks by integrating common attack paradigms with specifically designed perception-based losses or the capabilities of generative models. In this paper, we propose Adversarial Attacks in Diffusion (AdvAD), a novel modeling framework distinct from existing attack paradigms. AdvAD innovatively conceptualizes attacking as a non-parametric diffusion process by theoretically exploring basic modeling approach rather than using the denoising or generation abilities of regular diffusion models requiring neural networks. At each step, much subtler yet effective adversarial guidance is crafted using only the attacked model without any additional network, which gradually leads the end of diffusion process from the original image to a desired imperceptible adversarial example. Grounded in a solid theoretical foundation of the proposed non-parametric diffusion process, AdvAD achieves high attack efficacy and imperceptibility with intrinsically lower overall perturbation strength. Additionally, an enhanced version AdvAD-X is proposed to evaluate the extreme of our novel framework under an ideal scenario. Extensive experiments demonstrate the effectiveness of the proposed AdvAD and AdvAD-X. Compared with state-of-the-art imperceptible attacks, AdvAD achieves an average of 99.9$%$ (+17.3$%$) ASR with 1.34 (-0.97) $l_2$ distance, 49.74 (+4.76) PSNR and 0.9971 (+0.0043) SSIM against four prevalent DNNs with three different architectures on the ImageNet-compatible dataset. Code is available at <a target="_blank" rel="noopener" href="https://github.com/XianguiKang/AdvAD">https://github.com/XianguiKang/AdvAD</a>. </p>
<blockquote>
<p>éš¾ä»¥å¯Ÿè§‰çš„å¯¹æŠ—æ€§æ”»å‡»æ—¨åœ¨é€šè¿‡å‘è¾“å…¥æ•°æ®æ·»åŠ éš¾ä»¥å¯Ÿè§‰çš„æ‰°åŠ¨æ¥æ¬ºéª—æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰ã€‚ä¹‹å‰çš„æ–¹æ³•é€šå¸¸é€šè¿‡æ•´åˆå¸¸è§çš„æ”»å‡»æ¨¡å¼ä¸ä¸“é—¨è®¾è®¡çš„åŸºäºæ„ŸçŸ¥çš„æŸå¤±æˆ–ç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›æ¥æ”¹å–„æ”»å‡»çš„éš¾ä»¥å¯Ÿè§‰æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ‰©æ•£æ¨¡å‹ä¸­çš„å¯¹æŠ—æ€§æ”»å‡»ï¼ˆAdvADï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸åŒäºç°æœ‰æ”»å‡»æ¨¡å¼çš„æ–°å‹å»ºæ¨¡æ¡†æ¶ã€‚AdvADåˆ›æ–°åœ°å°†æ”»å‡»æ¦‚å¿µåŒ–ä¸ºéå‚æ•°æ‰©æ•£è¿‡ç¨‹ï¼Œé€šè¿‡ç†è®ºæ¢ç´¢åŸºæœ¬çš„å»ºæ¨¡æ–¹æ³•ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å¸¸è§„æ‰©æ•£æ¨¡å‹çš„é™å™ªæˆ–ç”Ÿæˆèƒ½åŠ›ï¼Œè¿™éœ€è¦ç¥ç»ç½‘ç»œã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»…å—æ”»å‡»æ¨¡å‹æ„å»ºçš„æ›´ä¸ºå¾®å¦™ä½†æœ‰æ•ˆçš„å¯¹æŠ—æ€§æŒ‡å¯¼ï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„ç½‘ç»œï¼Œè¿™é€æ¸å¼•å¯¼æ‰©æ•£è¿‡ç¨‹çš„æœ€ç»ˆçŠ¶æ€ä»åŸå§‹å›¾åƒåˆ°æœŸæœ›çš„éš¾ä»¥å¯Ÿè§‰çš„å¯¹æŠ—æ€§ç¤ºä¾‹ã€‚åŸºäºæ‰€æå‡ºçš„éå‚æ•°æ‰©æ•£è¿‡ç¨‹çš„åšå®ç†è®ºåŸºç¡€ï¼ŒAdvADå®ç°äº†é«˜æ”»å‡»æ•ˆæœå’Œéš¾ä»¥å¯Ÿè§‰æ€§ï¼Œå¹¶ä¸”å…·æœ‰æœ¬è´¨ä¸Šæ›´ä½çš„æ€»ä½“æ‰°åŠ¨å¼ºåº¦ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†å¢å¼ºç‰ˆAdvAD-Xï¼Œä»¥åœ¨ç†æƒ³åœºæ™¯ä¸‹è¯„ä¼°æˆ‘ä»¬æ–°æ¡†æ¶çš„æé™ã€‚å¤§é‡å®éªŒè¯æ˜äº†æ‰€æå‡ºçš„AdvADå’ŒAdvAD-Xçš„æœ‰æ•ˆæ€§ã€‚ä¸æœ€æ–°çš„éš¾ä»¥å¯Ÿè§‰çš„æ”»å‡»ç›¸æ¯”ï¼ŒAdvADåœ¨é’ˆå¯¹å…·æœ‰ä¸‰ç§ä¸åŒæ¶æ„çš„å››ç§æ·±åº¦ç¥ç»ç½‘ç»œçš„ImageNetå…¼å®¹æ•°æ®é›†ä¸Šï¼Œå®ç°äº†å¹³å‡99.9ï¼…ï¼ˆ+ 17.3ï¼…ï¼‰çš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ï¼ŒåŒæ—¶å…·æœ‰1.34ï¼ˆ- 0.97ï¼‰çš„L2è·ç¦»ï¼Œ49.74ï¼ˆ+ 4.76ï¼‰çš„å³°å€¼ä¿¡å·å™ªå£°æ¯”ï¼ˆPSNRï¼‰å’Œ0.9971ï¼ˆ+ 0.0043ï¼‰çš„ç»“æ„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆSSIMï¼‰ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/XianguiKang/AdvAD">https://github.com/XianguiKang/AdvAD</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09124v1">PDF</a> Accept by NeurIPS 2024. Please cite this paper using the following   format: J. Li, Z. He, A. Luo, J. Hu, Z. Wang, X. Kang*, â€œAdvAD: Exploring   Non-Parametric Diffusion for Imperceptible Adversarial Attacksâ€, the 38th   Annual Conference on Neural Information Processing Systems (NeurIPS),   Vancouver, Canada, Dec 9-15, 2024. Code: <a target="_blank" rel="noopener" href="https://github.com/XianguiKang/AdvAD">https://github.com/XianguiKang/AdvAD</a></p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºAdvADçš„æ–°å‹æ”»å‡»æ¡†æ¶ï¼Œå°†æ”»å‡»æ¦‚å¿µåŒ–ä¸ºéå‚æ•°æ‰©æ•£è¿‡ç¨‹ï¼Œä¸åŒäºç°æœ‰çš„æ”»å‡»èŒƒå¼ã€‚è¯¥æ–¹æ³•åœ¨ç†è®ºä¸Šæ¢ç´¢åŸºæœ¬çš„å»ºæ¨¡æ–¹æ³•ï¼Œä¸ä½¿ç”¨å¸¸è§„æ‰©æ•£æ¨¡å‹çš„å»å™ªæˆ–ç”Ÿæˆèƒ½åŠ›ï¼Œè€Œæ˜¯é€šè¿‡å¾®å¦™çš„å¯¹æŠ—æ€§æŒ‡å¯¼é€æ­¥å¼•å¯¼æ‰©æ•£è¿‡ç¨‹çš„ç»“æŸï¼Œä»åŸå§‹å›¾åƒåˆ°æœŸæœ›çš„å‡ ä¹æ— æ³•å¯Ÿè§‰çš„å¯¹æŠ—æ€§ç¤ºä¾‹ã€‚AdvADåœ¨åšå®çš„éå‚æ•°æ‰©æ•£è¿‡ç¨‹ç†è®ºåŸºç¡€ä¸Šï¼Œå®ç°äº†é«˜æ”»å‡»æ•ˆæœå’Œéš¾ä»¥å¯Ÿè§‰çš„ç‰¹æ€§ï¼Œå…·æœ‰å†…åœ¨æ›´ä½çš„æ€»ä½“æ‰°åŠ¨å¼ºåº¦ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†å¢å¼ºç‰ˆAdvAD-Xæ¥è¯„ä¼°ç†æƒ³åœºæ™¯ä¸‹çš„æ–°å‹æ¡†æ¶æé™ã€‚å®éªŒè¯æ˜ï¼Œä¸æœ€å…ˆè¿›çš„éš¾ä»¥å¯Ÿè§‰çš„æ”»å‡»ç›¸æ¯”ï¼ŒAdvADåœ¨é’ˆå¯¹å››ä¸ªæµè¡ŒDNNçš„ImageNetå…¼å®¹æ•°æ®é›†ä¸Šï¼Œå¹³å‡æ”»å‡»æˆåŠŸç‡æé«˜17.3%ï¼ŒL2è·ç¦»é™ä½0.97ï¼Œå³°å€¼ä¿¡å™ªæ¯”æé«˜4.76ï¼Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°æé«˜0.0043ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>AdvADæ˜¯ä¸€ç§æ–°å‹çš„æ”»å‡»æ¡†æ¶ï¼Œå°†æ”»å‡»æ¦‚å¿µåŒ–ä¸ºéå‚æ•°æ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä¾èµ–äºå¸¸è§„æ‰©æ•£æ¨¡å‹çš„å»å™ªæˆ–ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>AdvADé€šè¿‡å¾®å¦™çš„å¯¹æŠ—æ€§æŒ‡å¯¼é€æ­¥å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ç†è®ºä¸Šæ¢ç´¢åŸºæœ¬çš„å»ºæ¨¡æ–¹æ³•ï¼Œå®ç°é«˜æ”»å‡»æ•ˆæœå’Œéš¾ä»¥å¯Ÿè§‰çš„ç‰¹æ€§ã€‚</li>
<li>AdvADå…·æœ‰å†…åœ¨æ›´ä½çš„æ€»ä½“æ‰°åŠ¨å¼ºåº¦ã€‚</li>
<li>æå‡ºäº†å¢å¼ºç‰ˆAdvAD-Xæ¥è¯„ä¼°æ¡†æ¶çš„æé™ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒAdvADåœ¨æ”»å‡»æˆåŠŸç‡ã€L2è·ç¦»ã€å³°å€¼ä¿¡å™ªæ¯”å’Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ç­‰æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09124">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4401f099cffcd9583c54e9e622921a07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f02fd6ea2580072887d91c7077994f8a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GarmentCrafter-Progressive-Novel-View-Synthesis-for-Single-View-3D-Garment-Reconstruction-and-Editing"><a href="#GarmentCrafter-Progressive-Novel-View-Synthesis-for-Single-View-3D-Garment-Reconstruction-and-Editing" class="headerlink" title="GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D   Garment Reconstruction and Editing"></a>GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D   Garment Reconstruction and Editing</h2><p><strong>Authors:Yuanhao Wang, Cheng Zhang, GonÃ§alo FrazÃ£o, Jinlong Yang, Alexandru-Eugen Ichim, Thabo Beeler, Fernando De la Torre</strong></p>
<p>We introduce GarmentCrafter, a new approach that enables non-professional users to create and modify 3D garments from a single-view image. While recent advances in image generation have facilitated 2D garment design, creating and editing 3D garments remains challenging for non-professional users. Existing methods for single-view 3D reconstruction often rely on pre-trained generative models to synthesize novel views conditioning on the reference image and camera pose, yet they lack cross-view consistency, failing to capture the internal relationships across different views. In this paper, we tackle this challenge through progressive depth prediction and image warping to approximate novel views. Subsequently, we train a multi-view diffusion model to complete occluded and unknown clothing regions, informed by the evolving camera pose. By jointly inferring RGB and depth, GarmentCrafter enforces inter-view coherence and reconstructs precise geometries and fine details. Extensive experiments demonstrate that our method achieves superior visual fidelity and inter-view coherence compared to state-of-the-art single-view 3D garment reconstruction methods. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†GarmentCrafterï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œä½¿éä¸“ä¸šç”¨æˆ·èƒ½å¤Ÿä»å•è§†å›¾å›¾åƒåˆ›å»ºå’Œä¿®æ”¹3Dæœè£…ã€‚è™½ç„¶æœ€è¿‘å›¾åƒç”Ÿæˆçš„è¿›æ­¥ä¿ƒè¿›äº†2Dæœè£…è®¾è®¡ï¼Œä½†å¯¹äºéä¸“ä¸šç”¨æˆ·æ¥è¯´ï¼Œåˆ›å»ºå’Œç¼–è¾‘3Dæœè£…ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„å•è§†å›¾3Dé‡å»ºæ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹ï¼Œæ ¹æ®å‚è€ƒå›¾åƒå’Œç›¸æœºå§¿æ€åˆæˆæ–°è§†å›¾ï¼Œä½†å®ƒä»¬ç¼ºä¹è·¨è§†å›¾çš„ä¸€è‡´æ€§ï¼Œæ— æ³•æ•æ‰ä¸åŒè§†å›¾ä¹‹é—´çš„å†…éƒ¨å…³ç³»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ¸è¿›çš„æ·±åº¦é¢„æµ‹å’Œå›¾åƒæ‰­æ›²æ¥è¿‘ä¼¼æ–°è§†å›¾ï¼Œä»¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚éšåï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼Œä»¥æ ¹æ®ä¸æ–­å˜åŒ–çš„ç›¸æœºå§¿æ€ï¼Œå®Œæˆè¢«é®æŒ¡å’ŒæœªçŸ¥çš„æœè£…åŒºåŸŸã€‚é€šè¿‡è”åˆæ¨æ–­RGBå’Œæ·±åº¦ï¼ŒGarmentCrafterå¼ºåˆ¶å®æ–½è·¨è§†å›¾ä¸€è‡´æ€§ï¼Œå¹¶é‡å»ºç²¾ç¡®å‡ ä½•å’Œç²¾ç»†ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„å•è§†å›¾3Dæœè£…é‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æ›´é«˜çš„è§†è§‰ä¿çœŸåº¦å’Œè·¨è§†å›¾ä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08678v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://humansensinglab.github.io/garment-crafter/">https://humansensinglab.github.io/garment-crafter/</a></p>
<p><strong>Summary</strong></p>
<p>GarmentCrafteræ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿè®©éä¸“ä¸šç”¨æˆ·ä»å•è§†è§’å›¾åƒåˆ›å»ºå’Œä¿®æ”¹3Dæœè£…ã€‚å°½ç®¡æœ€è¿‘å›¾åƒç”ŸæˆæŠ€æœ¯çš„è¿›æ­¥ä¿ƒè¿›äº†2Dæœè£…è®¾è®¡ï¼Œä½†ä¸ºéè¥åˆ©ç”¨æˆ·åˆ›å»ºå’Œç¼–è¾‘3Dæœè£…ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰å•è§†å›¾3Dé‡å»ºæ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹æ¥æ ¹æ®å‚è€ƒå›¾åƒå’Œç›¸æœºå§¿æ€åˆæˆæ–°è§†å›¾ï¼Œä½†å®ƒä»¬ç¼ºä¹è·¨è§†å›¾çš„ä¸€è‡´æ€§ï¼Œæ— æ³•æ•æ‰ä¸åŒè§†å›¾ä¹‹é—´çš„å†…éƒ¨å…³ç³»ã€‚æœ¬ç ”ç©¶é€šè¿‡æ¸è¿›çš„æ·±åº¦é¢„æµ‹å’Œå›¾åƒæ‰­æ›²æ¥è¿‘ä¼¼æ–°è§†å›¾ï¼Œç„¶åè®­ç»ƒä¸€ä¸ªå¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼Œä»¥æ ¹æ®ä¸æ–­å˜åŒ–çš„ç›¸æœºå§¿æ€å®Œæˆè¢«é®æŒ¡å’ŒæœªçŸ¥çš„æœè£…åŒºåŸŸã€‚é€šè¿‡è”åˆæ¨æ–­RGBå’Œæ·±åº¦ï¼ŒGarmentCrafterå¼ºåˆ¶è·¨è§†å›¾ä¸€è‡´æ€§å¹¶é‡å»ºç²¾ç¡®çš„å‡ ä½•å½¢çŠ¶å’Œç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„å•è§†å›¾3Dæœè£…é‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æ›´é«˜çš„è§†è§‰ä¿çœŸåº¦å’Œè·¨è§†å›¾ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GarmentCrafteræ˜¯ä¸€ç§é¢å‘éä¸“ä¸šç”¨æˆ·çš„3Dæœè£…è®¾è®¡æ–°æ–¹æ³•ã€‚</li>
<li>ç°æœ‰å•è§†å›¾3Dé‡å»ºæ–¹æ³•å­˜åœ¨è·¨è§†å›¾ä¸€è‡´æ€§ç¼ºå¤±çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ¸è¿›çš„æ·±åº¦é¢„æµ‹å’Œå›¾åƒæ‰­æ›²æ¥è¿‘ä¼¼æ–°è§†å›¾æ˜¯GarmentCrafterè§£å†³æŒ‘æˆ˜çš„å…³é”®æ­¥éª¤ã€‚</li>
<li>å¤šè§†å›¾æ‰©æ•£æ¨¡å‹æ ¹æ®ç›¸æœºå§¿æ€å®Œæˆè¢«é®æŒ¡å’ŒæœªçŸ¥çš„æœè£…åŒºåŸŸã€‚</li>
<li>GarmentCrafteré€šè¿‡è”åˆæ¨æ–­RGBå’Œæ·±åº¦ä¿¡æ¯æ¥å¼ºåˆ¶è·¨è§†å›¾ä¸€è‡´æ€§ã€‚</li>
<li>GarmentCrafteråœ¨ç²¾ç¡®é‡å»ºå‡ ä½•å½¢çŠ¶å’Œç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9d8aced2c548a6bc757b31f057e21497.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa72d5139a4c1f589b809b50ac063497.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-677e1bd9ef923a13165cc30893951d8c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MEAT-Multiview-Diffusion-Model-for-Human-Generation-on-Megapixels-with-Mesh-Attention"><a href="#MEAT-Multiview-Diffusion-Model-for-Human-Generation-on-Megapixels-with-Mesh-Attention" class="headerlink" title="MEAT: Multiview Diffusion Model for Human Generation on Megapixels with   Mesh Attention"></a>MEAT: Multiview Diffusion Model for Human Generation on Megapixels with   Mesh Attention</h2><p><strong>Authors:Yuhan Wang, Fangzhou Hong, Shuai Yang, Liming Jiang, Wayne Wu, Chen Change Loy</strong></p>
<p>Multiview diffusion models have shown considerable success in image-to-3D generation for general objects. However, when applied to human data, existing methods have yet to deliver promising results, largely due to the challenges of scaling multiview attention to higher resolutions. In this paper, we explore human multiview diffusion models at the megapixel level and introduce a solution called mesh attention to enable training at 1024x1024 resolution. Using a clothed human mesh as a central coarse geometric representation, the proposed mesh attention leverages rasterization and projection to establish direct cross-view coordinate correspondences. This approach significantly reduces the complexity of multiview attention while maintaining cross-view consistency. Building on this foundation, we devise a mesh attention block and combine it with keypoint conditioning to create our human-specific multiview diffusion model, MEAT. In addition, we present valuable insights into applying multiview human motion videos for diffusion training, addressing the longstanding issue of data scarcity. Extensive experiments show that MEAT effectively generates dense, consistent multiview human images at the megapixel level, outperforming existing multiview diffusion methods. </p>
<blockquote>
<p>å¤šè§†è§’æ‰©æ•£æ¨¡å‹åœ¨ä¸€èˆ¬ç‰©ä½“çš„å›¾åƒåˆ°3Dç”Ÿæˆä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå½“åº”ç”¨äºäººç±»æ•°æ®æ—¶ï¼Œç°æœ‰æ–¹æ³•å°šæœªå–å¾—ä»¤äººæ»¡æ„çš„ç»“æœï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå°†å¤šè§†è§’æ³¨æ„åŠ›æ‰©å±•åˆ°é«˜åˆ†è¾¨ç‡é¢ä¸´çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨å…†åƒç´ çº§åˆ«æ¢ç´¢äº†äººç±»å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åä¸ºç½‘æ ¼æ³¨æ„åŠ›çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥å®ç°åœ¨1024x1024åˆ†è¾¨ç‡ä¸‹çš„è®­ç»ƒã€‚ä»¥ç©¿è¡£äººä½“ç½‘æ ¼ä½œä¸ºä¸­å¿ƒç²—ç³™å‡ ä½•è¡¨ç¤ºï¼Œæ‰€æå‡ºçš„ç½‘æ ¼æ³¨æ„åŠ›åˆ©ç”¨å…‰çº¿è¿½è¸ªå’ŒæŠ•å½±å»ºç«‹ç›´æ¥è·¨è§†å›¾åæ ‡å¯¹åº”å…³ç³»ã€‚è¿™ç§æ–¹æ³•åœ¨ä¿æŒè·¨è§†å›¾ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œå¤§å¤§é™ä½äº†å¤šè§†è§’æ³¨æ„åŠ›çš„å¤æ‚æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç½‘æ ¼æ³¨æ„åŠ›å—ï¼Œå¹¶ä¸å…³é”®ç‚¹æ¡ä»¶ç›¸ç»“åˆï¼Œåˆ›å»ºäº†ä¸“é—¨é’ˆå¯¹äººç±»çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹MEATã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å°†å¤šè§†è§’äººç±»è¿åŠ¨è§†é¢‘åº”ç”¨äºæ‰©æ•£è®­ç»ƒçš„å®è´µè§è§£ï¼Œè§£å†³äº†é•¿æœŸå­˜åœ¨çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMEATèƒ½å¤Ÿåœ¨å…†åƒç´ çº§åˆ«æœ‰æ•ˆåœ°ç”Ÿæˆå¯†é›†ã€ä¸€è‡´çš„å¤šè§†è§’äººç±»å›¾åƒï¼Œä¼˜äºç°æœ‰çš„å¤šè§†è§’æ‰©æ•£æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08664v1">PDF</a> CVPR 2025. Code <a target="_blank" rel="noopener" href="https://github.com/johannwyh/MEAT">https://github.com/johannwyh/MEAT</a> Project Page   <a target="_blank" rel="noopener" href="https://johann.wang/MEAT/">https://johann.wang/MEAT/</a></p>
<p><strong>Summary</strong><br>å¤šè§†è§’æ‰©æ•£æ¨¡å‹åœ¨é€šç”¨å¯¹è±¡çš„ä¸‰ç»´ç”Ÿæˆå›¾åƒä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨å¤„ç†äººç±»æ•°æ®æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æ¢ç´¢äº†åœ¨ç™¾ä¸‡åƒç´ çº§åˆ«ä¸Šçš„äººç±»å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºç½‘æ ¼æ³¨æ„åŠ›çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥å®ç°1024x1024åˆ†è¾¨ç‡çš„è®­ç»ƒã€‚è¯¥è§£å†³æ–¹æ¡ˆåˆ©ç”¨ç€è£…äººä½“ç½‘æ ¼ä½œä¸ºåŸºæœ¬çš„ç²—ç³™å‡ ä½•è¡¨ç¤ºï¼Œé€šè¿‡å…‰çº¿è¿½è¸ªå’ŒæŠ•å½±å»ºç«‹è·¨è§†å›¾åæ ‡çš„å¯¹åº”å…³ç³»ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼€å‘äº†ç½‘æ ¼æ³¨æ„åŠ›æ¨¡å—ï¼Œå¹¶ä¸å…³é”®ç‚¹æ¡ä»¶ç›¸ç»“åˆï¼Œåˆ›å»ºäº†ç‰¹å®šäºäººç±»çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹MEATã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†åˆ©ç”¨å¤šè§†è§’äººç±»è¿åŠ¨è§†é¢‘è¿›è¡Œæ‰©æ•£è®­ç»ƒçš„è§è§£ï¼Œè§£å†³äº†é•¿æœŸå­˜åœ¨çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒMEATèƒ½å¤Ÿåœ¨ç™¾ä¸‡åƒç´ çº§åˆ«ç”Ÿæˆå¯†é›†ã€ä¸€è‡´çš„å¤šè§†è§’äººç±»å›¾åƒï¼Œä¼˜äºç°æœ‰çš„å¤šè§†è§’æ‰©æ•£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè§†è§’æ‰©æ•£æ¨¡å‹åœ¨é€šç”¨å¯¹è±¡çš„ä¸‰ç»´ç”Ÿæˆå›¾åƒä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†äººç±»æ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç½‘æ ¼æ³¨æ„åŠ›è§£å†³æ–¹æ¡ˆè¢«æå‡ºæ¥è§£å†³åœ¨æ›´é«˜åˆ†è¾¨ç‡ï¼ˆå¦‚1024x1024ï¼‰ä¸‹çš„äººç±»å¤šè§†è§’æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒé—®é¢˜ã€‚</li>
<li>ç½‘æ ¼æ³¨æ„åŠ›åˆ©ç”¨äººä½“ç½‘æ ¼ä½œä¸ºåŸºæœ¬å‡ ä½•è¡¨ç¤ºï¼Œé€šè¿‡å…‰çº¿è¿½è¸ªå’ŒæŠ•å½±å®ç°è·¨è§†å›¾åæ ‡çš„å¯¹åº”å…³ç³»ã€‚</li>
<li>ç½‘æ ¼æ³¨æ„åŠ›æ¨¡å—ä¸å…³é”®ç‚¹æ¡ä»¶ç›¸ç»“åˆï¼Œåˆ›å»ºäº†ç‰¹å®šäºäººç±»çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹MEATã€‚</li>
<li>MEATèƒ½å¤Ÿåœ¨ç™¾ä¸‡åƒç´ çº§åˆ«ç”Ÿæˆå¯†é›†ã€ä¸€è‡´çš„å¤šè§†è§’äººç±»å›¾åƒï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡è¿˜æä¾›äº†åˆ©ç”¨å¤šè§†è§’äººç±»è¿åŠ¨è§†é¢‘è¿›è¡Œæ‰©æ•£è®­ç»ƒçš„è§è§£ï¼Œæœ‰åŠ©äºè§£å†³æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b453cf1e82ec0644fee568845aa2101f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df6c9dde793e591b412bdc5a1f6edc8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-518a217994b75df52e0f978f0486ea70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-224f727158fc8870e7ea0662af985330.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-987e8e5d22f78ac9caf7400ac5629b1c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MF-VITON-High-Fidelity-Mask-Free-Virtual-Try-On-with-Minimal-Input"><a href="#MF-VITON-High-Fidelity-Mask-Free-Virtual-Try-On-with-Minimal-Input" class="headerlink" title="MF-VITON: High-Fidelity Mask-Free Virtual Try-On with Minimal Input"></a>MF-VITON: High-Fidelity Mask-Free Virtual Try-On with Minimal Input</h2><p><strong>Authors:Zhenchen Wan, Yanwu xu, Dongting Hu, Weilun Cheng, Tianxi Chen, Zhaoqing Wang, Feng Liu, Tongliang Liu, Mingming Gong</strong></p>
<p>Recent advancements in Virtual Try-On (VITON) have significantly improved image realism and garment detail preservation, driven by powerful text-to-image (T2I) diffusion models. However, existing methods often rely on user-provided masks, introducing complexity and performance degradation due to imperfect inputs, as shown in Fig.1(a). To address this, we propose a Mask-Free VITON (MF-VITON) framework that achieves realistic VITON using only a single person image and a target garment, eliminating the requirement for auxiliary masks. Our approach introduces a novel two-stage pipeline: (1) We leverage existing Mask-based VITON models to synthesize a high-quality dataset. This dataset contains diverse, realistic pairs of person images and corresponding garments, augmented with varied backgrounds to mimic real-world scenarios. (2) The pre-trained Mask-based model is fine-tuned on the generated dataset, enabling garment transfer without mask dependencies. This stage simplifies the input requirements while preserving garment texture and shape fidelity. Our framework achieves state-of-the-art (SOTA) performance regarding garment transfer accuracy and visual realism. Notably, the proposed Mask-Free model significantly outperforms existing Mask-based approaches, setting a new benchmark and demonstrating a substantial lead over previous approaches. For more details, visit our project page: <a target="_blank" rel="noopener" href="https://zhenchenwan.github.io/MF-VITON/">https://zhenchenwan.github.io/MF-VITON/</a>. </p>
<blockquote>
<p>æœ€æ–°çš„è™šæ‹Ÿè¯•ç©¿ï¼ˆVITONï¼‰æŠ€æœ¯è¿›å±•ï¼Œåœ¨å¼ºå¤§çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æ¨åŠ¨ä¸‹ï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒçš„çœŸå®æ„Ÿå’Œæœè£…ç»†èŠ‚ä¿ç•™ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºç”¨æˆ·æä¾›çš„è’™ç‰ˆï¼Œç”±äºè¾“å…¥çš„ä¸å®Œç¾ï¼Œå¼•å…¥äº†å¤æ‚æ€§å’Œæ€§èƒ½ä¸‹é™ï¼Œå¦‚å›¾1ï¼ˆaï¼‰æ‰€ç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ— è’™ç‰ˆVITONï¼ˆMF-VITONï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»…ä½¿ç”¨å•äººå›¾åƒå’Œç›®æ ‡æœè£…å®ç°çœŸå®çš„è™šæ‹Ÿè¯•ç©¿ï¼Œæ— éœ€è¾…åŠ©è’™ç‰ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„ä¸¤é˜¶æ®µæµç¨‹ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬åˆ©ç”¨ç°æœ‰çš„åŸºäºè’™ç‰ˆçš„VITONæ¨¡å‹åˆæˆé«˜è´¨é‡æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«äººå‘˜å›¾åƒå’Œç›¸åº”æœè£…çš„å¤šæ ·åŒ–ã€çœŸå®çš„é…å¯¹ï¼Œå¹¶å¢åŠ äº†ä¸åŒçš„èƒŒæ™¯ä»¥æ¨¡æ‹ŸçœŸå®åœºæ™¯ã€‚ï¼ˆ2ï¼‰åœ¨ç”Ÿæˆçš„æ•°æ®é›†ä¸Šå¯¹é¢„è®­ç»ƒçš„åŸºäºè’™ç‰ˆæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå®ç°äº†æ— éœ€è’™ç‰ˆçš„æœè£…è½¬ç§»ã€‚è¿™ä¸€é˜¶æ®µç®€åŒ–äº†è¾“å…¥è¦æ±‚ï¼ŒåŒæ—¶ä¿æŒäº†æœè£…çº¹ç†å’Œå½¢çŠ¶çš„çœŸå®æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨æœè£…è½¬ç§»å‡†ç¡®æ€§å’Œè§†è§‰çœŸå®æ€§æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ‰€æå‡ºçš„æ— è’™ç‰ˆæ¨¡å‹æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºè’™ç‰ˆæ–¹æ³•ï¼Œè®¾å®šäº†æ–°çš„åŸºå‡†ï¼Œå¹¶åœ¨ä¹‹å‰çš„æ–¹æ³•ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„é¢†å…ˆä¼˜åŠ¿ã€‚æ›´å¤šç»†èŠ‚ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://zhenchenwan.github.io/MF-VITON/%E3%80%82">https://zhenchenwan.github.io/MF-VITON/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08650v1">PDF</a> The project page is available at:   <a target="_blank" rel="noopener" href="https://zhenchenwan.github.io/MF-VITON/">https://zhenchenwan.github.io/MF-VITON/</a></p>
<p><strong>Summary</strong><br>     æœ€æ–°è¿›å±•çš„è™šæ‹Ÿè¯•ç©¿ï¼ˆVITONï¼‰æŠ€æœ¯å·²æ˜¾è‘—æé«˜å›¾åƒçœŸå®æ„Ÿå’Œæœè£…ç»†èŠ‚ä¿ç•™åº¦ï¼Œå¾—ç›Šäºå¼ºå¤§çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ã€‚ä¸ºå…‹æœç°æœ‰æ–¹æ³•ä¾èµ–ç”¨æˆ·æä¾›çš„é®ç½©æ‰€å¸¦æ¥çš„å¤æ‚æ€§å’Œæ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ— é®ç½©VITONï¼ˆMF-VITONï¼‰æ¡†æ¶ï¼Œä»…ä½¿ç”¨å•äººå›¾åƒå’Œç›®æ ‡æœè£…å®ç°é€¼çœŸçš„è™šæ‹Ÿè¯•ç©¿ã€‚é€šè¿‡å¼•å…¥ä¸¤é˜¶æ®µç®¡é“ï¼Œåˆ©ç”¨åŸºäºé®ç½©çš„VITONæ¨¡å‹åˆæˆé«˜è´¨é‡æ•°æ®é›†å¹¶è¿›è¡Œå¾®è°ƒï¼Œæ¶ˆé™¤å¯¹è¾…åŠ©é®ç½©çš„éœ€æ±‚ï¼Œç®€åŒ–è¾“å…¥è¦æ±‚çš„åŒæ—¶ä¿æŒæœè£…çº¹ç†å’Œå½¢çŠ¶ä¿çœŸåº¦ã€‚MF-VITONæ¡†æ¶åœ¨æœè£…è½¬ç§»å‡†ç¡®æ€§å’Œè§†è§‰é€¼çœŸåº¦æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºäºé®ç½©çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€æ–°VITONæŠ€æœ¯æé«˜äº†å›¾åƒçœŸå®æ„Ÿå’Œæœè£…ç»†èŠ‚ä¿ç•™åº¦ï¼Œå€ŸåŠ©T2Iæ‰©æ•£æ¨¡å‹æ¨åŠ¨å‘å±•ã€‚</li>
<li>ç°æœ‰VITONæ–¹æ³•ä¾èµ–ç”¨æˆ·æä¾›çš„é®ç½©ï¼Œå¯¼è‡´å¤æ‚æ€§å’Œæ€§èƒ½ä¸‹é™ã€‚</li>
<li>MF-VITONæ¡†æ¶å®ç°æ— é®ç½©çš„è™šæ‹Ÿè¯•ç©¿ï¼Œä»…ä½¿ç”¨å•äººå›¾åƒå’Œç›®æ ‡æœè£…ã€‚</li>
<li>MF-VITONé€šè¿‡ä¸¤é˜¶æ®µç®¡é“å¼•å…¥é«˜è´¨é‡æ•°æ®é›†åˆæˆå’Œæ¨¡å‹å¾®è°ƒã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨åŸºäºé®ç½©çš„VITONæ¨¡å‹åˆæˆå¤šæ ·åŒ–ã€é€¼çœŸçš„ä¸ªäººå›¾åƒå’Œå¯¹åº”æœè£…å›¾åƒã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µå¯¹é¢„è®­ç»ƒçš„åŸºäºé®ç½©çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå®ç°æ— é®ç½©çš„æœè£…è½¬ç§»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a38e09e471ffde212b994a01c1727c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a98da891f9fc770a1e70759abcd6a2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e85b71f32dab67e120939d53343f6c8.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Layton-Latent-Consistency-Tokenizer-for-1024-pixel-Image-Reconstruction-and-Generation-by-256-Tokens"><a href="#Layton-Latent-Consistency-Tokenizer-for-1024-pixel-Image-Reconstruction-and-Generation-by-256-Tokens" class="headerlink" title="Layton: Latent Consistency Tokenizer for 1024-pixel Image Reconstruction   and Generation by 256 Tokens"></a>Layton: Latent Consistency Tokenizer for 1024-pixel Image Reconstruction   and Generation by 256 Tokens</h2><p><strong>Authors:Qingsong Xie, Zhao Zhang, Zhe Huang, Yanhao Zhang, Haonan Lu, Zhenyu Yang</strong></p>
<p>Image tokenization has significantly advanced visual generation and multimodal modeling, particularly when paired with autoregressive models. However, current methods face challenges in balancing efficiency and fidelity: high-resolution image reconstruction either requires an excessive number of tokens or compromises critical details through token reduction. To resolve this, we propose Latent Consistency Tokenizer (Layton) that bridges discrete visual tokens with the compact latent space of pre-trained Latent Diffusion Models (LDMs), enabling efficient representation of 1024x1024 images using only 256 tokens-a 16 times compression over VQGAN. Layton integrates a transformer encoder, a quantized codebook, and a latent consistency decoder. Direct application of LDM as the decoder results in color and brightness discrepancies. Thus, we convert it to latent consistency decoder, reducing multi-step sampling to 1-2 steps for direct pixel-level supervision. Experiments demonstrate Laytonâ€™s superiority in high-fidelity reconstruction, with 10.8 reconstruction Frechet Inception Distance on MSCOCO-2017 5K benchmark for 1024x1024 image reconstruction. We also extend Layton to a text-to-image generation model, LaytonGen, working in autoregression. It achieves 0.73 score on GenEval benchmark, surpassing current state-of-the-art methods. Project homepage: <a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/Layton">https://github.com/OPPO-Mente-Lab/Layton</a> </p>
<blockquote>
<p>å›¾åƒç¬¦å·åŒ–ï¼ˆImage tokenizationï¼‰åœ¨è§†è§‰ç”Ÿæˆå’Œå¤šæ¨¡æ€å»ºæ¨¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨ä¸è‡ªå›å½’æ¨¡å‹ç»“åˆä½¿ç”¨æ—¶ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•åœ¨å¹³è¡¡æ•ˆç‡å’Œä¿çœŸåº¦æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼šé«˜åˆ†è¾¨ç‡å›¾åƒé‡å»ºéœ€è¦å¤§é‡ç¬¦å·æˆ–éœ€è¦é€šè¿‡å‡å°‘ç¬¦å·æ¥å¦¥åå…³é”®ç»†èŠ‚ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Latent Consistency Tokenizerï¼ˆLaytonï¼‰ï¼Œå®ƒå°†ç¦»æ•£è§†è§‰ç¬¦å·ä¸é¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„ç´§å‡‘æ½œåœ¨ç©ºé—´ç›¸ç»“åˆï¼Œä»…ä½¿ç”¨256ä¸ªç¬¦å·å°±èƒ½æœ‰æ•ˆåœ°è¡¨ç¤º1024x1024çš„å›¾åƒï¼Œè¿™æ˜¯VQGANçš„16å€å‹ç¼©ã€‚Laytoné›†æˆäº†å˜å‹å™¨ç¼–ç å™¨ã€é‡åŒ–ä»£ç æœ¬å’Œæ½œåœ¨ä¸€è‡´æ€§è§£ç å™¨ã€‚ç›´æ¥ä½¿ç”¨LDMä½œä¸ºè§£ç å™¨ä¼šå¯¼è‡´é¢œè‰²å’Œäº®åº¦å·®å¼‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å…¶è½¬æ¢ä¸ºæ½œåœ¨ä¸€è‡´æ€§è§£ç å™¨ï¼Œå°†å¤šæ­¥é‡‡æ ·å‡å°‘åˆ°1-2æ­¥ï¼Œå®ç°ç›´æ¥åƒç´ çº§çš„ç›‘ç£ã€‚å®éªŒè¯æ˜Laytonåœ¨é«˜ä¿çœŸé‡å»ºæ–¹é¢çš„ä¼˜åŠ¿ï¼Œåœ¨MSCOCO-2017 5KåŸºå‡†çš„1024x1024å›¾åƒé‡å»ºä¸­ï¼Œé‡å»ºFrechet Inception Distanceè¾¾åˆ°10.8ã€‚æˆ‘ä»¬è¿˜æŠŠLaytonæ‰©å±•åˆ°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹LaytonGenï¼Œé‡‡ç”¨è‡ªå›å½’æ–¹å¼å·¥ä½œã€‚å®ƒåœ¨GenEvalåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°0.73åˆ†ï¼Œè¶…è¿‡äº†å½“å‰å…ˆè¿›çš„æ–¹æ³•ã€‚é¡¹ç›®ä¸»é¡µï¼š<a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/Layton">https://github.com/OPPO-Mente-Lab/Layton</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08377v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Latent Consistency Tokenizerï¼ˆLaytonï¼‰çš„ç ”ç©¶ï¼Œè¯¥æŠ€æœ¯åœ¨å›¾åƒæ ‡è®°åŒ–æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå®ç°äº†é«˜æ•ˆä¸”é«˜ä¿çœŸåº¦çš„å›¾åƒé‡å»ºã€‚é€šè¿‡å°†ç¦»æ•£è§†è§‰æ ‡è®°ä¸é¢„è®­ç»ƒçš„Latent Diffusion Modelsï¼ˆLDMï¼‰çš„æ½œåœ¨ç©ºé—´ç›¸ç»“åˆï¼ŒLaytonèƒ½å¤Ÿä½¿ç”¨è¾ƒå°‘çš„æ ‡è®°è¡¨ç¤ºé«˜åˆ†è¾¨ç‡å›¾åƒã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ä¸ªæ½œåœ¨ä¸€è‡´æ€§è§£ç å™¨ï¼Œä»¥æ”¹è¿›é¢œè‰²ä¸äº®åº¦çš„å·®å¼‚é—®é¢˜ã€‚å®éªŒç»“æœè¯æ˜äº†Laytonåœ¨é«˜ä¿çœŸé‡å»ºä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ï¼Œå¹¶ä¸”æ‰©å±•åˆ°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹LaytonGenï¼Œå®ç°äº†è‡ªåŠ¨å›å½’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Laytoné€šè¿‡ç»“åˆç¦»æ•£è§†è§‰æ ‡è®°å’Œé¢„è®­ç»ƒçš„Latent Diffusion Modelsï¼ˆLDMï¼‰çš„æ½œåœ¨ç©ºé—´ï¼Œå®ç°äº†å›¾åƒæ ‡è®°åŒ–çš„æ˜¾è‘—è¿›å±•ã€‚</li>
<li>Laytonèƒ½å¤Ÿä½¿ç”¨è¾ƒå°‘çš„æ ‡è®°ï¼ˆä»…256ä¸ªæ ‡è®°ï¼‰è¡¨ç¤º1024x1024çš„é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå®ç°äº†é«˜æ•ˆçš„å›¾åƒè¡¨ç¤ºã€‚</li>
<li>æå‡ºçš„æ½œåœ¨ä¸€è‡´æ€§è§£ç å™¨æ”¹å–„äº†é¢œè‰²ä¸äº®åº¦çš„å·®å¼‚é—®é¢˜ã€‚</li>
<li>Laytonåœ¨é«˜ä¿çœŸé‡å»ºä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œå…¶Frechet Inception Distanceåœ¨MSCOCO-2017 5KåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†ä¼˜ç§€çš„è¡¨ç°ã€‚</li>
<li>Laytonæ‰©å±•åˆ°äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹LaytonGenï¼Œå®ç°äº†è‡ªåŠ¨å›å½’åŠŸèƒ½ã€‚</li>
<li>LaytonGenåœ¨GenEvalåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†è¾ƒé«˜çš„å¾—åˆ†ï¼Œè¶…è¿‡äº†å½“å‰çš„ä¸»æµæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-69b7e34fc6d968c74d5178a841ea11da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5983764aa67dbfff512cd7f3a550fd00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9f668a19a7e7392f2ebc263eec6016d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d029e28a78b68b9363f4cbd06ae70e4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CDI3D-Cross-guided-Dense-view-Interpolation-for-3D-Reconstruction"><a href="#CDI3D-Cross-guided-Dense-view-Interpolation-for-3D-Reconstruction" class="headerlink" title="CDI3D: Cross-guided Dense-view Interpolation for 3D Reconstruction"></a>CDI3D: Cross-guided Dense-view Interpolation for 3D Reconstruction</h2><p><strong>Authors:Zhiyuan Wu, Xibin Song, Senbo Wang, Weizhe Liu, Jiayu Yang, Ziang Cheng, Shenzhou Chen, Taizhang Shang, Weixuan Sun, Shan Luo, Pan Ji</strong></p>
<p>3D object reconstruction from single-view image is a fundamental task in computer vision with wide-ranging applications. Recent advancements in Large Reconstruction Models (LRMs) have shown great promise in leveraging multi-view images generated by 2D diffusion models to extract 3D content. However, challenges remain as 2D diffusion models often struggle to produce dense images with strong multi-view consistency, and LRMs tend to amplify these inconsistencies during the 3D reconstruction process. Addressing these issues is critical for achieving high-quality and efficient 3D reconstruction. In this paper, we present CDI3D, a feed-forward framework designed for efficient, high-quality image-to-3D generation with view interpolation. To tackle the aforementioned challenges, we propose to integrate 2D diffusion-based view interpolation into the LRM pipeline to enhance the quality and consistency of the generated mesh. Specifically, our approach introduces a Dense View Interpolation (DVI) module, which synthesizes interpolated images between main views generated by the 2D diffusion model, effectively densifying the input views with better multi-view consistency. We also design a tilt camera pose trajectory to capture views with different elevations and perspectives. Subsequently, we employ a tri-plane-based mesh reconstruction strategy to extract robust tokens from these interpolated and original views, enabling the generation of high-quality 3D meshes with superior texture and geometry. Extensive experiments demonstrate that our method significantly outperforms previous state-of-the-art approaches across various benchmarks, producing 3D content with enhanced texture fidelity and geometric accuracy. </p>
<blockquote>
<p>ä»å•è§†å›¾å›¾åƒè¿›è¡Œ3Då¯¹è±¡é‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚æœ€è¿‘çš„å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆLRMsï¼‰çš„è¿›å±•è¡¨æ˜ï¼Œåˆ©ç”¨2Dæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å¤šè§†å›¾å›¾åƒæå–3Då†…å®¹å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸º2Dæ‰©æ•£æ¨¡å‹å¾€å¾€éš¾ä»¥ç”Ÿæˆå…·æœ‰å¼ºçƒˆå¤šè§†å›¾ä¸€è‡´æ€§çš„å¯†é›†å›¾åƒï¼Œè€ŒLRMsåœ¨3Dé‡å»ºè¿‡ç¨‹ä¸­å¾€å¾€ä¼šæ”¾å¤§è¿™äº›ä¸ä¸€è‡´æ€§ã€‚è§£å†³è¿™äº›é—®é¢˜å¯¹äºå®ç°é«˜è´¨é‡å’Œé«˜æ•ˆçš„3Dé‡å»ºè‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CDI3Dï¼Œè¿™æ˜¯ä¸€ä¸ªå‰é¦ˆæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆã€é«˜è´¨é‡çš„å›¾ç‰‡åˆ°3Dç”Ÿæˆä»¥åŠè§†è§’æ’å€¼ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æè®®å°†åŸºäº2Dæ‰©æ•£çš„è§†å›¾æ’å€¼é›†æˆåˆ°LRMç®¡é“ä¸­ï¼Œä»¥æé«˜ç”Ÿæˆç½‘æ ¼çš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªå¯†é›†è§†å›¾æ’å€¼ï¼ˆDVIï¼‰æ¨¡å—ï¼Œå®ƒåˆæˆç”±2Dæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„ä¸»è§†å›¾ä¹‹é—´çš„æ’å€¼å›¾åƒï¼Œæœ‰æ•ˆåœ°ä½¿è¾“å…¥è§†å›¾å¯†é›†åŒ–ï¼Œæé«˜äº†å¤šè§†å›¾çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªå€¾æ–œçš„ç›¸æœºå§¿æ€è½¨è¿¹æ¥æ•æ‰ä¸åŒæµ·æ‹”å’Œè§’åº¦çš„è§†å›¾ã€‚éšåï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºä¸‰é¢ç½‘çš„ç½‘æ ¼é‡å»ºç­–ç•¥ï¼Œä»è¿™äº›æ’å€¼å’ŒåŸå§‹è§†å›¾ä¸­æå–ç¨³å¥çš„ä»¤ç‰Œï¼Œä»è€Œç”Ÿæˆå…·æœ‰å‡ºè‰²çº¹ç†å’Œå‡ ä½•å½¢çŠ¶çš„é«˜è´¨é‡3Dç½‘æ ¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œç”Ÿæˆçš„3Då†…å®¹å…·æœ‰å¢å¼ºçš„çº¹ç†ä¿çœŸåº¦å’Œå‡ ä½•å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08005v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„ã€é«˜è´¨é‡çš„å›¾åƒåˆ°ä¸‰ç»´ç”Ÿæˆæ¡†æ¶CDI3Dï¼Œç”¨äºè§£å†³äºŒç»´æ‰©æ•£æ¨¡å‹äº§ç”Ÿçš„å¯†é›†å›¾åƒå¤šè§†è§’ä¸€è‡´æ€§å·®çš„é—®é¢˜ï¼ŒåŠå…¶åœ¨å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆLRMsï¼‰ä¸­æ”¾å¤§è¿™äº›é—®é¢˜çš„é—®é¢˜ã€‚æ–‡ç« å¼•å…¥äº†ä¸€ç§åä¸ºDense View Interpolationï¼ˆDVIï¼‰çš„æ¨¡å—ï¼Œç”¨äºåˆæˆç”±äºŒç»´æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„ä¸»è§†å›¾ä¹‹é—´çš„æ’å€¼å›¾åƒï¼Œæé«˜ç”Ÿæˆç½‘æ ¼çš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚åŒæ—¶é‡‡ç”¨å€¾æ–œç›¸æœºå§¿æ€è½¨è¿¹æ•è·ä¸åŒé«˜åº¦å’Œè§’åº¦çš„è§†å›¾ï¼Œå¹¶ä½¿ç”¨åŸºäºä¸‰è§’å¹³é¢çš„ç½‘æ ¼é‡å»ºç­–ç•¥ä»æ’å€¼å’ŒåŸå§‹è§†å›¾ä¸­æå–ç¨³å¥çš„æ ‡è®°ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´ç½‘æ ¼ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆçš„ä¸‰ç»´å†…å®¹å…·æœ‰å¢å¼ºçš„çº¹ç†ä¿çœŸåº¦å’Œå‡ ä½•å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä»‹ç»äº†äºŒç»´æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå¯†é›†å›¾åƒæ—¶é¢ä¸´çš„å¤šè§†è§’ä¸€è‡´æ€§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„ã€é«˜è´¨é‡çš„å›¾åƒåˆ°ä¸‰ç»´ç”Ÿæˆæ¡†æ¶CDI3Dï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜å¹¶å®ç°é«˜è´¨é‡çš„3Dé‡å»ºã€‚</li>
<li>å¼•å…¥äº†Dense View Interpolationï¼ˆDVIï¼‰æ¨¡å—æ¥åˆæˆæ’å€¼å›¾åƒï¼Œæé«˜ç”Ÿæˆç½‘æ ¼çš„ä¸€è‡´æ€§å’Œè´¨é‡ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨å€¾æ–œç›¸æœºå§¿æ€è½¨è¿¹å’ŒåŸºäºä¸‰è§’å¹³é¢çš„ç½‘æ ¼é‡å»ºç­–ç•¥ï¼Œå¢å¼ºäº†ç”Ÿæˆçš„3Då†…å®¹çš„çº¹ç†å’Œå‡ ä½•è´¨é‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-558ddb12c72192e8fdd276d0bdc43cd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e1ac9dc24ac10b3b53df9e4b3b4a77a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-596dd7363fb11e4e88d346622cff924c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2333115293710ef059639b9284600c9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-040d79b2f16adee75253d791addec347.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Learning-Few-Step-Diffusion-Models-by-Trajectory-Distribution-Matching"><a href="#Learning-Few-Step-Diffusion-Models-by-Trajectory-Distribution-Matching" class="headerlink" title="Learning Few-Step Diffusion Models by Trajectory Distribution Matching"></a>Learning Few-Step Diffusion Models by Trajectory Distribution Matching</h2><p><strong>Authors:Yihong Luo, Tianyang Hu, Jiacheng Sun, Yujun Cai, Jing Tang</strong></p>
<p>Accelerating diffusion model sampling is crucial for efficient AIGC deployment. While diffusion distillation methods â€“ based on distribution matching and trajectory matching â€“ reduce sampling to as few as one step, they fall short on complex tasks like text-to-image generation. Few-step generation offers a better balance between speed and quality, but existing approaches face a persistent trade-off: distribution matching lacks flexibility for multi-step sampling, while trajectory matching often yields suboptimal image quality. To bridge this gap, we propose learning few-step diffusion models by Trajectory Distribution Matching (TDM), a unified distillation paradigm that combines the strengths of distribution and trajectory matching. Our method introduces a data-free score distillation objective, aligning the studentâ€™s trajectory with the teacherâ€™s at the distribution level. Further, we develop a sampling-steps-aware objective that decouples learning targets across different steps, enabling more adjustable sampling. This approach supports both deterministic sampling for superior image quality and flexible multi-step adaptation, achieving state-of-the-art performance with remarkable efficiency. Our model, TDM, outperforms existing methods on various backbones, such as SDXL and PixArt-$\alpha$, delivering superior quality and significantly reduced training costs. In particular, our method distills PixArt-$\alpha$ into a 4-step generator that outperforms its teacher on real user preference at 1024 resolution. This is accomplished with 500 iterations and 2 A800 hours â€“ a mere 0.01% of the teacherâ€™s training cost. In addition, our proposed TDM can be extended to accelerate text-to-video diffusion. Notably, TDM can outperform its teacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total score from 80.91 to 81.65. Project page: <a target="_blank" rel="noopener" href="https://tdm-t2x.github.io/">https://tdm-t2x.github.io/</a> </p>
<blockquote>
<p>åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„é‡‡æ ·å¯¹äºé«˜æ•ˆçš„AIGCéƒ¨ç½²è‡³å…³é‡è¦ã€‚è™½ç„¶åŸºäºåˆ†å¸ƒåŒ¹é…å’Œè½¨è¿¹åŒ¹é…çš„æ‰©æ•£è’¸é¦æ–¹æ³•èƒ½å¤Ÿå°†é‡‡æ ·å‡å°‘åˆ°ä¸€æ­¥ï¼Œä½†åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç­‰å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³ã€‚å°‘æ­¥éª¤ç”Ÿæˆåœ¨é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´æä¾›äº†æ›´å¥½çš„å¹³è¡¡ï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´ç€æŒç»­çš„æƒè¡¡ï¼šåˆ†å¸ƒåŒ¹é…ç¼ºä¹å¤šæ­¥éª¤é‡‡æ ·çš„çµæ´»æ€§ï¼Œè€Œè½¨è¿¹åŒ¹é…å¾€å¾€äº§ç”Ÿæ¬¡ä¼˜çš„å›¾åƒè´¨é‡ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡è½¨è¿¹åˆ†å¸ƒåŒ¹é…ï¼ˆTDMï¼‰å­¦ä¹ å°‘æ­¥éª¤æ‰©æ•£æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆåˆ†å¸ƒåŒ¹é…å’Œè½¨è¿¹åŒ¹é…ä¼˜ç‚¹çš„ç»Ÿä¸€è’¸é¦èŒƒå¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªæ— æ•°æ®çš„åˆ†æ•°è’¸é¦ç›®æ ‡ï¼Œåœ¨åˆ†å¸ƒå±‚é¢ä½¿å­¦ç”Ÿçš„è½¨è¿¹ä¸æ•™å¸ˆå¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªé‡‡æ ·æ­¥éª¤æ„ŸçŸ¥çš„ç›®æ ‡ï¼Œè¯¥ç›®æ ‡èƒ½å¤Ÿåœ¨ä¸åŒçš„æ­¥éª¤ä¸­è§£è€¦å­¦ä¹ ç›®æ ‡ï¼Œä»è€Œå®ç°æ›´å¯è°ƒæ•´çš„é‡‡æ ·ã€‚è¯¥æ–¹æ³•æ—¢æ”¯æŒç¡®å®šæ€§çš„é‡‡æ ·ä»¥è·å¾—ä¼˜è´¨çš„å›¾åƒï¼Œåˆæ”¯æŒçµæ´»çš„å¤šæ­¥éª¤é€‚åº”ï¼Œä»¥å“è¶Šçš„æ•ˆç‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„TDMæ¨¡å‹åœ¨å„ç§backboneä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¦‚SDXLå’ŒPixArt-Î±ï¼Œåœ¨1024åˆ†è¾¨ç‡çš„çœŸå®ç”¨æˆ·åå¥½ä¸Šè¡¨ç°å‡ºè¶…è¶Šå…¶æ•™å¸ˆçš„è´¨é‡ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†PixArt-Î±è’¸é¦æˆä¸€ä¸ª4æ­¥ç”Ÿæˆå™¨ï¼Œåœ¨500æ¬¡è¿­ä»£å’Œ2ä¸ªA800å°æ—¶å†…å®Œæˆäº†è®­ç»ƒï¼Œä»…ä¸ºæ•™å¸ˆè®­ç»ƒæˆæœ¬çš„0.01%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„TDMå¯ä»¥æ‰©å±•åˆ°åŠ é€Ÿæ–‡æœ¬åˆ°è§†é¢‘çš„æ‰©æ•£ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒTDMèƒ½å¤Ÿåœ¨VBenchä¸Šä»…ä½¿ç”¨4ä¸ªNFEè¶…è¶Šå…¶æ•™å¸ˆæ¨¡å‹ï¼ˆCogVideoX-2Bï¼‰ï¼Œæ€»åˆ†æ•°ä»80.91æé«˜åˆ°81.65ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://tdm-t2x.github.io/">https://tdm-t2x.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06674v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://tdm-t2x.github.io/">https://tdm-t2x.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŠ é€Ÿæ‰©æ•£æ¨¡å‹é‡‡æ ·çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹è’¸é¦æ–¹æ³•â€”â€”è½¨è¿¹åˆ†å¸ƒåŒ¹é…ï¼ˆTDMï¼‰ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åˆ†å¸ƒåŒ¹é…å’Œè½¨è¿¹åŒ¹é…çš„ä¼˜åŠ¿ï¼Œé€šè¿‡å¼•å…¥æ— æ•°æ®è¯„åˆ†è’¸é¦ç›®æ ‡å’Œé‡‡æ ·æ­¥éª¤æ„ŸçŸ¥ç›®æ ‡ï¼Œå®ç°äº†åœ¨å¤æ‚ä»»åŠ¡å¦‚æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„é«˜æ•ˆæ€§èƒ½ã€‚TDMæ–¹æ³•æ”¯æŒç¡®å®šæ€§é‡‡æ ·ä»¥æä¾›é«˜è´¨é‡çš„å›¾åƒï¼Œå¹¶å¯å®ç°çµæ´»çš„å¤šæ­¥é€‚åº”ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒTDMè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼ŒTDMè¿˜å¯æ‰©å±•åº”ç”¨äºæ–‡æœ¬åˆ°è§†é¢‘çš„æ‰©æ•£ä»»åŠ¡ï¼Œå¹¶èƒ½åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šå…¶æ•™å¸ˆæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ é€Ÿæ‰©æ•£æ¨¡å‹é‡‡æ ·å¯¹é«˜æ•ˆçš„AIGCéƒ¨ç½²è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚åˆ†å¸ƒåŒ¹é…å’Œè½¨è¿¹åŒ¹é…åœ¨å¤æ‚ä»»åŠ¡ä¸Šå­˜åœ¨å±€é™ã€‚</li>
<li>TDMæ–¹æ³•ç»“åˆäº†åˆ†å¸ƒåŒ¹é…å’Œè½¨è¿¹åŒ¹é…çš„ä¼˜åŠ¿ï¼Œæé«˜äº†é‡‡æ ·é€Ÿåº¦å’Œå›¾åƒè´¨é‡ã€‚</li>
<li>TDMå¼•å…¥äº†æ— æ•°æ®è¯„åˆ†è’¸é¦ç›®æ ‡å’Œé‡‡æ ·æ­¥éª¤æ„ŸçŸ¥ç›®æ ‡ï¼Œå®ç°äº†çµæ´»çš„å¤šæ­¥é€‚åº”å’Œé«˜è´¨é‡å›¾åƒç”Ÿæˆã€‚</li>
<li>TDMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚</li>
<li>TDMå¯åº”ç”¨äºæ–‡æœ¬åˆ°è§†é¢‘çš„æ‰©æ•£ä»»åŠ¡ï¼Œå¹¶èƒ½åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šå…¶æ•™å¸ˆæ¨¡å‹ã€‚</li>
<li>TDMæ–¹æ³•åœ¨å¤šç§èƒŒæ™¯æ¨¡å‹ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚SDXLå’ŒPixArt-$\alpha$ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-69ae77723123f086ac30fcfa32caa5bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eed8fde73fd542a2e01580a5433f3068.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fe71b66745e752047faabb820035800.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3848c8ea56a6dc5aa92d1865de2192d1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Adding-Additional-Control-to-One-Step-Diffusion-with-Joint-Distribution-Matching"><a href="#Adding-Additional-Control-to-One-Step-Diffusion-with-Joint-Distribution-Matching" class="headerlink" title="Adding Additional Control to One-Step Diffusion with Joint Distribution   Matching"></a>Adding Additional Control to One-Step Diffusion with Joint Distribution   Matching</h2><p><strong>Authors:Yihong Luo, Tianyang Hu, Yifan Song, Jiacheng Sun, Zhenguo Li, Jing Tang</strong></p>
<p>While diffusion distillation has enabled one-step generation through methods like Variational Score Distillation, adapting distilled models to emerging new controls â€“ such as novel structural constraints or latest user preferences â€“ remains challenging. Conventional approaches typically requires modifying the base diffusion model and redistilling it â€“ a process that is both computationally intensive and time-consuming. To address these challenges, we introduce Joint Distribution Matching (JDM), a novel approach that minimizes the reverse KL divergence between image-condition joint distributions. By deriving a tractable upper bound, JDM decouples fidelity learning from condition learning. This asymmetric distillation scheme enables our one-step student to handle controls unknown to the teacher model and facilitates improved classifier-free guidance (CFG) usage and seamless integration of human feedback learning (HFL). Experimental results demonstrate that JDM surpasses baseline methods such as multi-step ControlNet by mere one-step in most cases, while achieving state-of-the-art performance in one-step text-to-image synthesis through improved usage of CFG or HFL integration. </p>
<blockquote>
<p>æ‰©æ•£è’¸é¦æŠ€æœ¯å·²ç»é€šè¿‡å˜åˆ†è¯„åˆ†è’¸é¦ç­‰æ–¹æ³•å®ç°äº†ä¸€æ­¥ç”Ÿæˆï¼Œä½†å°†è’¸é¦æ¨¡å‹é€‚åº”äºæ–°å…´çš„æ–°æ§åˆ¶å› ç´ ï¼Œä¾‹å¦‚æ–°å‹ç»“æ„çº¦æŸæˆ–æœ€æ–°ç”¨æˆ·åå¥½ï¼Œä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦ä¿®æ”¹åŸºç¡€æ‰©æ•£æ¨¡å‹å¹¶é‡æ–°è’¸é¦ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢è®¡ç®—å¯†é›†åˆè€—æ—¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è”åˆåˆ†å¸ƒåŒ¹é…ï¼ˆJDMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æœ€å°åŒ–å›¾åƒæ¡ä»¶è”åˆåˆ†å¸ƒä¹‹é—´åå‘KLæ•£åº¦çš„æ–°å‹æ–¹æ³•ã€‚é€šè¿‡æ¨å¯¼å¯è¡Œä¸Šç•Œï¼ŒJDMå°†ä¿çœŸåº¦å­¦ä¹ ä¸æ¡ä»¶å­¦ä¹ è§£è€¦ã€‚è¿™ç§ä¸å¯¹ç§°çš„è’¸é¦æ–¹æ¡ˆä½¿æˆ‘ä»¬çš„ä¸€æ­¥å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿå¤„ç†æ•™å¸ˆæ¨¡å‹æœªçŸ¥çš„æ§åˆ¶å› ç´ ï¼Œä¿ƒè¿›äº†æ”¹è¿›çš„æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ä½¿ç”¨å’Œæ— ç¼é›†æˆäººç±»åé¦ˆå­¦ä¹ ï¼ˆHFLï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒJDMä»…éœ€ä¸€æ­¥å³å¯è¶…è¶Šå¤šæ­¥ControlNetç­‰åŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶åœ¨åˆ©ç”¨CFGæˆ–HFLé›†æˆçš„ä¸€æ­¥æ–‡æœ¬åˆ°å›¾åƒåˆæˆä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06652v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£è’¸é¦è™½ç„¶é€šè¿‡è¯¸å¦‚å˜åˆ†è¯„åˆ†è’¸é¦ç­‰æ–¹æ³•å®ç°äº†ä¸€æ­¥ç”Ÿæˆï¼Œä½†åœ¨é€‚åº”æ–°å…´çš„æ–°æ§åˆ¶å› ç´ æ–¹é¢ï¼Œå¦‚æ–°çš„ç»“æ„çº¦æŸæˆ–æœ€æ–°çš„ç”¨æˆ·åå¥½ï¼Œä»å­˜åœ¨æŒ‘æˆ˜ã€‚å¸¸è§„æ–¹æ³•é€šå¸¸éœ€è¦ä¿®æ”¹åŸºç¡€æ‰©æ•£æ¨¡å‹å¹¶é‡æ–°è’¸é¦ï¼Œè¿™ä¸€è¿‡ç¨‹è®¡ç®—é‡å¤§ä¸”è€—æ—¶ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†è”åˆåˆ†å¸ƒåŒ¹é…ï¼ˆJDMï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡æœ€å°åŒ–å›¾åƒæ¡ä»¶è”åˆåˆ†å¸ƒä¹‹é—´çš„é€†å‘KLæ•£åº¦æ¥ä¼˜åŒ–ã€‚JDMé€šè¿‡æ¨å¯¼å¯è¡Œä¸Šç•Œï¼Œå°†ä¿çœŸåº¦å­¦ä¹ ä¸æ¡ä»¶å­¦ä¹ è§£è€¦ã€‚è¿™ç§å¯¹ç§°çš„è’¸é¦æ–¹æ¡ˆä½¿ä¸€æ­¥ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿå¤„ç†æ•™å¸ˆæ¨¡å‹æœªçŸ¥çš„æ§åˆ¶å› ç´ ï¼Œå¹¶ä¿ƒè¿›äº†æ— åˆ†ç±»å™¨å¼•å¯¼å’Œæ— ç¼é›†æˆäººç±»åé¦ˆå­¦ä¹ çš„æ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒJDMä»…éœ€ä¸€æ­¥å³å¯è¶…è¶Šå¤šæ­¥ControlNetç­‰åŸºçº¿æ–¹æ³•ï¼Œå¹¶é€šè¿‡æ”¹è¿›çš„æ— åˆ†ç±»å™¨å¼•å¯¼æˆ–äººç±»åé¦ˆå­¦ä¹ çš„é›†æˆï¼Œå®ç°äº†ä¸€æ­¥æ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£è’¸é¦å®ç°äº†ä¸€æ­¥ç”Ÿæˆï¼Œä½†é€‚åº”æ–°æ§åˆ¶å› ç´ ä»æœ‰æŒ‘æˆ˜ã€‚</li>
<li>å¸¸è§„æ–¹æ³•ä¿®æ”¹åŸºç¡€æ‰©æ•£æ¨¡å‹å¹¶é‡æ–°è’¸é¦ï¼Œè¿‡ç¨‹è®¡ç®—é‡å¤§ä¸”è€—æ—¶ã€‚</li>
<li>è”åˆåˆ†å¸ƒåŒ¹é…ï¼ˆJDMï¼‰æå‡ºæœ€å°åŒ–å›¾åƒæ¡ä»¶è”åˆåˆ†å¸ƒä¹‹é—´çš„é€†å‘KLæ•£åº¦æ¥ä¼˜åŒ–ã€‚</li>
<li>JDMé€šè¿‡å¯è¡Œä¸Šç•Œè§£è€¦ä¿çœŸåº¦å­¦ä¹ ä¸æ¡ä»¶å­¦ä¹ ã€‚</li>
<li>JDMä½¿ä¸€æ­¥ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿå¤„ç†æ•™å¸ˆæ¨¡å‹æœªçŸ¥çš„æ§åˆ¶å› ç´ ã€‚</li>
<li>JDMä¿ƒè¿›äº†æ— åˆ†ç±»å™¨å¼•å¯¼å’Œæ— ç¼é›†æˆäººç±»åé¦ˆå­¦ä¹ çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-490ff86cbdd1d44990ac009c93936092.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b29c3221f858ee44cb109552b4bfbf16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f45830777e311748a2a5c5d849c3a655.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-143f366d7b67dc513e484d7f70a4cd42.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Taming-Large-Multimodal-Agents-for-Ultra-low-Bitrate-Semantically-Disentangled-Image-Compression"><a href="#Taming-Large-Multimodal-Agents-for-Ultra-low-Bitrate-Semantically-Disentangled-Image-Compression" class="headerlink" title="Taming Large Multimodal Agents for Ultra-low Bitrate Semantically   Disentangled Image Compression"></a>Taming Large Multimodal Agents for Ultra-low Bitrate Semantically   Disentangled Image Compression</h2><p><strong>Authors:Juan Song, Lijie Yang, Mingtao Feng</strong></p>
<p>It remains a significant challenge to compress images at ultra-low bitrate while achieving both semantic consistency and high perceptual quality. We propose a novel image compression framework, Semantically Disentangled Image Compression (SEDIC) in this paper. Our proposed SEDIC leverages large multimodal models (LMMs) to disentangle the image into several essential semantic information, including an extremely compressed reference image, overall and object-level text descriptions, and the semantic masks. A multi-stage semantic decoder is designed to progressively restore the transmitted reference image object-by-object, ultimately producing high-quality and perceptually consistent reconstructions. In each decoding stage, a pre-trained controllable diffusion model is utilized to restore the object details on the reference image conditioned by the text descriptions and semantic masks. Experimental results demonstrate that SEDIC significantly outperforms state-of-the-art approaches, achieving superior perceptual quality and semantic consistency at ultra-low bitrates ($\le$ 0.05 bpp). </p>
<blockquote>
<p>åœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹å¯¹å›¾åƒè¿›è¡Œå‹ç¼©ï¼ŒåŒæ—¶å®ç°è¯­ä¹‰ä¸€è‡´æ€§å’Œé«˜æ„ŸçŸ¥è´¨é‡ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹å›¾åƒå‹ç¼©æ¡†æ¶â€”â€”è¯­ä¹‰åˆ†ç¦»å›¾åƒå‹ç¼©ï¼ˆSEDICï¼‰ã€‚æˆ‘ä»¬æå‡ºçš„SEDICåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å°†å›¾åƒåˆ†è§£ä¸ºå¤šç§åŸºæœ¬è¯­ä¹‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬æåº¦å‹ç¼©çš„å‚è€ƒå›¾åƒã€æ€»ä½“å’Œå¯¹è±¡çº§æ–‡æœ¬æè¿°ä»¥åŠè¯­ä¹‰æ©ç ã€‚è®¾è®¡äº†ä¸€ä¸ªå¤šé˜¶æ®µè¯­ä¹‰è§£ç å™¨ï¼Œä»¥é€æ­¥æ¢å¤ä¼ è¾“çš„å‚è€ƒå›¾åƒçš„å¯¹è±¡ï¼Œæœ€ç»ˆäº§ç”Ÿé«˜è´¨é‡å’Œæ„ŸçŸ¥ä¸€è‡´çš„é‡å»ºã€‚åœ¨æ¯ä¸ªè§£ç é˜¶æ®µï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å¯æ§æ‰©æ•£æ¨¡å‹æ ¹æ®æ–‡æœ¬æè¿°å’Œè¯­ä¹‰æ©è†œæ¢å¤å‚è€ƒå›¾åƒçš„å¯¹è±¡ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEDICåœ¨è¶…ä½æ¯”ç‰¹ç‡ï¼ˆâ‰¤0.05 bppï¼‰ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå®ç°äº†å‡ºè‰²çš„æ„ŸçŸ¥è´¨é‡å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00399v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å›¾åƒå‹ç¼©æ¡†æ¶â€”â€”è¯­ä¹‰åˆ†ç¦»å›¾åƒå‹ç¼©ï¼ˆSEDICï¼‰ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å°†å›¾åƒåˆ†è§£ä¸ºå¤šä¸ªåŸºæœ¬è¯­ä¹‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬é«˜åº¦å‹ç¼©çš„å‚è€ƒå›¾åƒã€æ•´ä½“å’Œå¯¹è±¡çº§åˆ«çš„æ–‡æœ¬æè¿°ä»¥åŠè¯­ä¹‰æ©ç ã€‚è®¾è®¡äº†ä¸€ä¸ªå¤šé˜¶æ®µè¯­ä¹‰è§£ç å™¨ï¼Œä»¥é€æ­¥æ¢å¤ä¼ è¾“çš„å‚è€ƒå›¾åƒçš„å¯¹è±¡ï¼Œæœ€ç»ˆäº§ç”Ÿé«˜è´¨é‡å’Œæ„ŸçŸ¥ä¸€è‡´çš„é‡å»ºå›¾åƒã€‚åœ¨æ¯ä¸ªè§£ç é˜¶æ®µï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å¯æ§æ‰©æ•£æ¨¡å‹æ ¹æ®æ–‡æœ¬æè¿°å’Œè¯­ä¹‰æ©è†œæ¢å¤å¯¹è±¡ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEDICåœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå®ç°äº†ä¼˜è¶Šçš„ä¸»è§‚æ„ŸçŸ¥è´¨é‡å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒå‹ç¼©æ¡†æ¶SEDICï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œå›¾åƒåˆ†è§£ã€‚</li>
<li>SEDICèƒ½å¤Ÿå°†å›¾åƒåˆ†è§£ä¸ºå¤šä¸ªåŸºæœ¬è¯­ä¹‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬å‚è€ƒå›¾åƒã€æ–‡æœ¬æè¿°å’Œè¯­ä¹‰æ©ç ã€‚</li>
<li>è®¾è®¡äº†å¤šé˜¶æ®µè¯­ä¹‰è§£ç å™¨ï¼Œå¯é€æ­¥æ¢å¤å¹¶é‡å»ºé«˜è´¨é‡å’Œæ„ŸçŸ¥ä¸€è‡´çš„å›¾åƒã€‚</li>
<li>åœ¨æ¯ä¸ªè§£ç é˜¶æ®µï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„å¯æ§æ‰©æ•£æ¨¡å‹æ ¹æ®æ–‡æœ¬å’Œè¯­ä¹‰ä¿¡æ¯æ¢å¤å¯¹è±¡ç»†èŠ‚ã€‚</li>
<li>SEDICåœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹å®ç°ä¼˜è¶Šæ€§èƒ½ï¼Œè¾¾åˆ°äº†â‰¤0.05 bppã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSEDICåœ¨è¯­ä¹‰ä¸€è‡´æ€§å’Œé«˜æ„ŸçŸ¥è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-586888a4e79e6046b13fe191f480b908.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4874b4258d2b0a6abf5a79cea1ebd2a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8bb3cf8d58d099985615499b97bd3a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d00cbc621a2ea996a8d18caf5385b42d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Autoregressive-Image-Generation-with-Vision-Full-view-Prompt"><a href="#Autoregressive-Image-Generation-with-Vision-Full-view-Prompt" class="headerlink" title="Autoregressive Image Generation with Vision Full-view Prompt"></a>Autoregressive Image Generation with Vision Full-view Prompt</h2><p><strong>Authors:Miaomiao Cai, Guanjie Wang, Wei Li, Zhijun Tu, Hanting Chen, Shaohui Lin, Jie Hu</strong></p>
<p>In autoregressive (AR) image generation, models based on the â€˜next-token predictionâ€™ paradigm of LLMs have shown comparable performance to diffusion models by reducing inductive biases. However, directly applying LLMs to complex image generation can struggle with reconstructing the imageâ€™s structure and details, impacting the generationâ€™s accuracy and stability. Additionally, the â€˜next-token predictionâ€™ paradigm in the AR model does not align with the contextual scanning and logical reasoning processes involved in human visual perception, limiting effective image generation. Prompt engineering, as a key technique for guiding LLMs, leverages specifically designed prompts to improve model performance on complex natural language processing (NLP) tasks, enhancing accuracy and stability of generation while maintaining contextual coherence and logical consistency, similar to human reasoning. Inspired by prompt engineering from the field of NLP, we propose Vision Full-view prompt (VF prompt) to enhance autoregressive image generation. Specifically, we design specialized image-related VF prompts for AR image generation to simulate the process of human image creation. This enhances contextual logic ability by allowing the model to first perceive overall distribution information before generating the image, and improve generation stability by increasing the inference steps. Compared to the AR method without VF prompts, our method shows outstanding performance and achieves an approximate improvement of 20%. </p>
<blockquote>
<p>åœ¨è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆä¸­ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„â€œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹â€èŒƒå¼çš„æ¨¡å‹é€šè¿‡å‡å°‘å½’çº³åè§è¡¨ç°å‡ºäº†ä¸æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç›´æ¥å°†å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨äºå¤æ‚å›¾åƒç”Ÿæˆä¼šé¢ä¸´é‡å»ºå›¾åƒç»“æ„å’Œç»†èŠ‚çš„æŒ‘æˆ˜ï¼Œä»è€Œå½±å“ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œè‡ªå›å½’æ¨¡å‹ä¸­çš„â€œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹â€èŒƒå¼å¹¶ä¸ç¬¦åˆäººç±»è§†è§‰æ„ŸçŸ¥æ‰€æ¶‰åŠçš„ç¯å¢ƒæ‰«æå’Œé€»è¾‘æ¨ç†è¿‡ç¨‹ï¼Œé™åˆ¶äº†æœ‰æ•ˆçš„å›¾åƒç”Ÿæˆã€‚ä½œä¸ºä¸€ç§å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹çš„å…³é”®æŠ€æœ¯ï¼Œæç¤ºå·¥ç¨‹åˆ©ç”¨ä¸“é—¨è®¾è®¡çš„æç¤ºæ¥æ”¹å–„æ¨¡å‹åœ¨å¤æ‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œæé«˜ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼ŒåŒæ—¶ä¿æŒä¸Šä¸‹æ–‡è¿è´¯å’Œé€»è¾‘ä¸€è‡´æ€§ï¼Œç±»ä¼¼äºäººç±»æ¨ç†ã€‚å—è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæç¤ºå·¥ç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºVision Full-view promptï¼ˆVFæç¤ºï¼‰æ¥å¢å¼ºè‡ªå›å½’å›¾åƒç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºARå›¾åƒç”Ÿæˆè®¾è®¡äº†ä¸“é—¨åŒ–çš„å›¾åƒç›¸å…³VFæç¤ºï¼Œä»¥æ¨¡æ‹Ÿäººç±»å›¾åƒåˆ›å»ºçš„è¿‡ç¨‹ã€‚è¿™é€šè¿‡å…è®¸æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒä¹‹å‰å…ˆæ„ŸçŸ¥æ•´ä½“åˆ†å¸ƒä¿¡æ¯ï¼Œå¢å¼ºäº†ä¸Šä¸‹æ–‡é€»è¾‘èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¢åŠ æ¨ç†æ­¥éª¤æé«˜äº†ç”Ÿæˆçš„ç¨³å®šæ€§ã€‚ä¸æ²¡æœ‰VFæç¤ºçš„ARæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶å®ç°äº†å¤§çº¦20%çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16965v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºVision Full-view promptï¼ˆVFæç¤ºï¼‰çš„æ–¹æ³•ï¼Œç”¨äºå¢å¼ºè‡ªå›å½’å›¾åƒç”Ÿæˆçš„æ•ˆæœã€‚é€šè¿‡è®¾è®¡ä¸“é—¨çš„å›¾åƒç›¸å…³VFæç¤ºï¼Œæ¨¡æ‹Ÿäººç±»å›¾åƒåˆ›å»ºè¿‡ç¨‹ï¼Œå¢å¼ºè‡ªå›å½’å›¾åƒç”Ÿæˆçš„ä¸Šä¸‹æ–‡é€»è¾‘èƒ½åŠ›ï¼Œå¹¶åœ¨å¢åŠ æ¨ç†æ­¥éª¤çš„æƒ…å†µä¸‹æé«˜ç”Ÿæˆç¨³å®šæ€§ï¼Œå®ç°äº†ç›¸è¾ƒäºæ²¡æœ‰VFæç¤ºçš„ARæ–¹æ³•çº¦20%çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆä¸­ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„â€œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹â€èŒƒå¼åœ¨å‡å°‘å½’çº³åè§æ–¹é¢è¡¨ç°å‡ºä¸æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>ç›´æ¥åº”ç”¨LLMsäºå¤æ‚å›¾åƒç”Ÿæˆé¢ä¸´é‡å»ºå›¾åƒç»“æ„å’Œç»†èŠ‚çš„æŒ‘æˆ˜ï¼Œå½±å“ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚</li>
<li>â€œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹â€èŒƒå¼ä¸ç¬¦åˆäººç±»è§†è§‰æ„ŸçŸ¥æ¶‰åŠçš„ä¸Šæ–‡æ‰«æå’Œé€»è¾‘æ¨ç†è¿‡ç¨‹ï¼Œé™åˆ¶äº†æœ‰æ•ˆçš„å›¾åƒç”Ÿæˆã€‚</li>
<li>æç¤ºå·¥ç¨‹æŠ€æœ¯é€šè¿‡ä¸“é—¨è®¾è®¡çš„æç¤ºæ¥æ”¹å–„LLMsåœ¨å¤æ‚è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œç»´æŒä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ï¼Œç±»ä¼¼äºäººç±»æ¨ç†ã€‚</li>
<li>å€Ÿé‰´è‡ªç„¶è¯­è¨€å¤„ç†çš„æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œæå‡ºVision Full-view promptï¼ˆVFæç¤ºï¼‰ä»¥å¢å¼ºè‡ªå›å½’å›¾åƒç”Ÿæˆã€‚</li>
<li>VFæç¤ºé€šè¿‡æ¨¡æ‹Ÿäººç±»å›¾åƒåˆ›å»ºè¿‡ç¨‹ï¼Œå…è®¸æ¨¡å‹å…ˆæ„ŸçŸ¥æ•´ä½“åˆ†å¸ƒä¿¡æ¯å†ç”Ÿæˆå›¾åƒï¼Œå¢å¼ºäº†ä¸Šä¸‹æ–‡é€»è¾‘èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5b252fa429ab4ec13a2157e10d6a13de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63555639ec6eb587d611a611259601ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b725730f2f122c374b695d070f9afc30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-643d2f6847a7fe69826056f6ed053657.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5dbcd4e79bd4bca49e9b290179c544c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5437b87bcab3fde6e5ece04db7fb2282.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Light-A-Video-Training-free-Video-Relighting-via-Progressive-Light-Fusion"><a href="#Light-A-Video-Training-free-Video-Relighting-via-Progressive-Light-Fusion" class="headerlink" title="Light-A-Video: Training-free Video Relighting via Progressive Light   Fusion"></a>Light-A-Video: Training-free Video Relighting via Progressive Light   Fusion</h2><p><strong>Authors:Yujie Zhou, Jiazi Bu, Pengyang Ling, Pan Zhang, Tong Wu, Qidong Huang, Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Anyi Rao, Jiaqi Wang, Li Niu</strong></p>
<p>Recent advancements in image relighting models, driven by large-scale datasets and pre-trained diffusion models, have enabled the imposition of consistent lighting. However, video relighting still lags, primarily due to the excessive training costs and the scarcity of diverse, high-quality video relighting datasets. A simple application of image relighting models on a frame-by-frame basis leads to several issues: lighting source inconsistency and relighted appearance inconsistency, resulting in flickers in the generated videos. In this work, we propose Light-A-Video, a training-free approach to achieve temporally smooth video relighting. Adapted from image relighting models, Light-A-Video introduces two key techniques to enhance lighting consistency. First, we design a Consistent Light Attention (CLA) module, which enhances cross-frame interactions within the self-attention layers of the image relight model to stabilize the generation of the background lighting source. Second, leveraging the physical principle of light transport independence, we apply linear blending between the source videoâ€™s appearance and the relighted appearance, using a Progressive Light Fusion (PLF) strategy to ensure smooth temporal transitions in illumination. Experiments show that Light-A-Video improves the temporal consistency of relighted video while maintaining the relighted image quality, ensuring coherent lighting transitions across frames. Project page: <a target="_blank" rel="noopener" href="https://bujiazi.github.io/light-a-video.github.io/">https://bujiazi.github.io/light-a-video.github.io/</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œç”±äºå¤§è§„æ¨¡æ•°æ®é›†å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ¨åŠ¨ï¼Œå›¾åƒè¡¥å…‰æ¨¡å‹çš„å‘å±•å·²ç»å®ç°äº†è¿è´¯çš„ç…§æ˜ã€‚ç„¶è€Œï¼Œè§†é¢‘è¡¥å…‰ä»ç„¶æ»åï¼Œä¸»è¦æ˜¯ç”±äºè®­ç»ƒæˆæœ¬è¿‡é«˜å’Œå¤šæ ·ã€é«˜è´¨é‡è§†é¢‘è¡¥å…‰æ•°æ®é›†çš„ç¨€ç¼ºã€‚ç®€å•åœ°å°†å›¾åƒè¡¥å…‰æ¨¡å‹é€å¸§åº”ç”¨ä¼šå¯¼è‡´å‡ ä¸ªé—®é¢˜ï¼šå…‰æºä¸ä¸€è‡´å’Œè¡¥å…‰å¤–è§‚ä¸ä¸€è‡´ï¼Œå¯¼è‡´ç”Ÿæˆçš„è§†é¢‘å‡ºç°é—ªçƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Light-A-Videoï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„ã€å®ç°æ—¶é—´å¹³æ»‘è§†é¢‘è¡¥å…‰çš„æ–¹æ³•ã€‚Light-A-Videoä»å›¾åƒè¡¥å…‰æ¨¡å‹ä¸­æ±²å–çµæ„Ÿï¼Œå¼•å…¥äº†ä¸¤ç§å…³é”®æŠ€æœ¯æ¥æé«˜ç…§æ˜çš„ä¸€è‡´æ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸€è‡´çš„å…‰æ³¨æ„åŠ›ï¼ˆCLAï¼‰æ¨¡å—ï¼Œå®ƒå¢å¼ºäº†å›¾åƒè¡¥å…‰æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›å±‚ä¸­çš„è·¨å¸§äº¤äº’ï¼Œä»¥ç¨³å®šèƒŒæ™¯å…‰æºçš„ç”Ÿæˆã€‚å…¶æ¬¡ï¼Œåˆ©ç”¨å…‰çº¿ä¼ è¾“ç‹¬ç«‹çš„ç‰©ç†åŸç†ï¼Œæˆ‘ä»¬é‡‡ç”¨æ¸è¿›å¼å…‰èåˆï¼ˆPLFï¼‰ç­–ç•¥ï¼Œå¯¹æºè§†é¢‘çš„å¤–è§‚å’Œè¡¥å…‰çš„å¤–è§‚è¿›è¡Œçº¿æ€§æ··åˆï¼Œä»¥ç¡®ä¿ç…§æ˜çš„æ—¶é—´è¿‡æ¸¡å¹³æ»‘ã€‚å®éªŒè¡¨æ˜ï¼ŒLight-A-Videoæé«˜äº†è¡¥å…‰è§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è¡¥å…‰å›¾åƒçš„è´¨é‡ï¼Œç¡®ä¿äº†è·¨å¸§çš„ç…§æ˜è¿‡æ¸¡è¿è´¯ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://bujiazi.github.io/light-a-video.github.io/%E3%80%82">https://bujiazi.github.io/light-a-video.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08590v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://bujiazi.github.io/light-a-video.github.io/">https://bujiazi.github.io/light-a-video.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå›¾åƒé‡ç…§æ˜æ¨¡å‹çš„è§†é¢‘é‡ç…§æ˜æŠ€æœ¯çš„æ–°è¿›å±•ã€‚ç”±äºå¤§è§„æ¨¡æ•°æ®é›†å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ¨åŠ¨ï¼Œå›¾åƒé‡ç…§æ˜æŠ€æœ¯å·²ç»èƒ½å¤Ÿå®ç°ä¸€è‡´æ€§çš„ç…§æ˜ã€‚ç„¶è€Œï¼Œè§†é¢‘é‡ç…§æ˜ä»å­˜åœ¨è®­ç»ƒæˆæœ¬è¿‡é«˜ã€é«˜è´¨é‡è§†é¢‘é‡ç…§æ˜æ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ã€‚ç®€å•åœ°å°†å›¾åƒé‡ç…§æ˜æ¨¡å‹é€å¸§åº”ç”¨äºè§†é¢‘ä¼šå¯¼è‡´å…‰æºä¸ä¸€è‡´å’Œé‡ç…§æ˜å¤–è§‚ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œç”Ÿæˆçš„è§†é¢‘ä¼šå‡ºç°é—ªçƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLight-A-Videoçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— éœ€è®­ç»ƒå³å¯å®ç°æ—¶é—´å¹³æ»‘çš„è§†é¢‘é‡ç…§æ˜ã€‚å®ƒå¼•å…¥äº†ä¸¤ç§å…³é”®æŠ€æœ¯æ¥æé«˜ç…§æ˜ä¸€è‡´æ€§ï¼šä¸€æ˜¯è®¾è®¡äº†ä¸€è‡´çš„ç¯å…‰æ³¨æ„åŠ›æ¨¡å—ï¼Œå¢å¼ºå›¾åƒé‡ç…§æ˜æ¨¡å‹ä¸­è‡ªæ³¨æ„åŠ›å±‚çš„è·¨å¸§äº¤äº’ï¼Œç¨³å®šèƒŒæ™¯å…‰æºçš„ç”Ÿæˆï¼›äºŒæ˜¯åˆ©ç”¨å…‰çº¿ä¼ è¾“çš„ç‹¬ç«‹ç‰©ç†åŸç†ï¼Œé‡‡ç”¨æ¸è¿›å…‰èåˆç­–ç•¥ï¼Œå¯¹æºè§†é¢‘çš„å¤–è§‚å’Œé‡ç…§æ˜å¤–è§‚è¿›è¡Œçº¿æ€§æ··åˆï¼Œç¡®ä¿ç…§æ˜çš„æ—¶é—´è¿‡æ¸¡å¹³æ»‘ã€‚å®éªŒè¡¨æ˜ï¼ŒLight-A-Videoåœ¨ä¿æŒé‡ç…§æ˜å›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œæé«˜äº†é‡ç…§æ˜è§†é¢‘çš„æ—¶ç©ºä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸå›¾åƒé‡ç…§æ˜æ¨¡å‹çš„è¿›å±•ä¸ºè§†é¢‘é‡ç…§æ˜æä¾›äº†åŸºç¡€ï¼Œä½†éœ€è¦è§£å†³è®­ç»ƒæˆæœ¬å’Œæ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ã€‚</li>
<li>ç®€å•åº”ç”¨å›¾åƒé‡ç…§æ˜æ¨¡å‹äºè§†é¢‘ä¼šå¯¼è‡´å…‰æºå’Œå¤–è§‚ä¸ä¸€è‡´ï¼Œé€ æˆè§†é¢‘é—ªçƒã€‚</li>
<li>Light-A-Videoæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„è§†é¢‘é‡ç…§æ˜æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ä¸€è‡´çš„ç¯å…‰æ³¨æ„åŠ›æ¨¡å—å’Œæ¸è¿›å…‰èåˆç­–ç•¥æ¥æé«˜ç…§æ˜ä¸€è‡´æ€§ã€‚</li>
<li>ä¸€è‡´çš„ç¯å…‰æ³¨æ„åŠ›æ¨¡å—å¢å¼ºäº†è·¨å¸§äº¤äº’ï¼Œç¨³å®šèƒŒæ™¯å…‰æºç”Ÿæˆã€‚</li>
<li>æ¸è¿›å…‰èåˆç­–ç•¥ç¡®ä¿ç…§æ˜çš„æ—¶é—´è¿‡æ¸¡å¹³æ»‘ã€‚</li>
<li>Light-A-Videoåœ¨ä¿æŒé‡ç…§æ˜å›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œæé«˜äº†é‡ç…§æ˜è§†é¢‘çš„æ—¶ç©ºä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08590">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e274db7d5b7cf1228431bc2eb2094dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2825002ce3b8121c78c639d6a25ad5c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-add21383699704023e5b0c812039167a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-318a48696a658edbda1cf38fafa10160.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="GraPE-A-Generate-Plan-Edit-Framework-for-Compositional-T2I-Synthesis"><a href="#GraPE-A-Generate-Plan-Edit-Framework-for-Compositional-T2I-Synthesis" class="headerlink" title="GraPE: A Generate-Plan-Edit Framework for Compositional T2I Synthesis"></a>GraPE: A Generate-Plan-Edit Framework for Compositional T2I Synthesis</h2><p><strong>Authors:Ashish Goswami, Satyam Kumar Modi, Santhosh Rishi Deshineni, Harman Singh, Prathosh A. P, Parag Singla</strong></p>
<p>Text-to-image (T2I) generation has seen significant progress with diffusion models, enabling generation of photo-realistic images from text prompts. Despite this progress, existing methods still face challenges in following complex text prompts, especially those requiring compositional and multi-step reasoning. Given such complex instructions, SOTA models often make mistakes in faithfully modeling object attributes, and relationships among them. In this work, we present an alternate paradigm for T2I synthesis, decomposing the task of complex multi-step generation into three steps, (a) Generate: we first generate an image using existing diffusion models (b) Plan: we make use of Multi-Modal LLMs (MLLMs) to identify the mistakes in the generated image expressed in terms of individual objects and their properties, and produce a sequence of corrective steps required in the form of an edit-plan. (c) Edit: we make use of an existing text-guided image editing models to sequentially execute our edit-plan over the generated image to get the desired image which is faithful to the original instruction. Our approach derives its strength from the fact that it is modular in nature, is training free, and can be applied over any combination of image generation and editing models. As an added contribution, we also develop a model capable of compositional editing, which further helps improve the overall accuracy of our proposed approach. Our method flexibly trades inference time compute with performance on compositional text prompts. We perform extensive experimental evaluation across 3 benchmarks and 10 T2I models including DALLE-3 and the latest â€“ SD-3.5-Large. Our approach not only improves the performance of the SOTA models, by upto 3 points, it also reduces the performance gap between weaker and stronger models. $\href{<a target="_blank" rel="noopener" href="https://dair-iitd.github.io/GraPE/%7D%7Bhttps://dair-iitd.github.io/GraPE/%7D$">https://dair-iitd.github.io/GraPE/}{https://dair-iitd.github.io/GraPE/}$</a> </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆåœ¨æ‰©æ•£æ¨¡å‹çš„æ¨åŠ¨ä¸‹å–å¾—äº†é‡å¤§è¿›å±•ï¼Œèƒ½å¤Ÿå®ç°ä»æ–‡æœ¬æç¤ºç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚å°½ç®¡å¦‚æ­¤ï¼Œç°æœ‰æ–¹æ³•ä»ç„¶é¢ä¸´éµå¾ªå¤æ‚æ–‡æœ¬æç¤ºçš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯é‚£äº›éœ€è¦ç»„åˆå’Œå¤šæ­¥æ¨ç†çš„æç¤ºã€‚é¢å¯¹è¿™æ ·çš„å¤æ‚æŒ‡ä»¤ï¼Œå½“å‰é¡¶å°–æ¨¡å‹åœ¨å¿ å®å»ºæ¨¡ç‰©ä½“å±æ€§ä»¥åŠå®ƒä»¬ä¹‹é—´çš„å…³ç³»æ—¶ç»å¸¸å‡ºé”™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ›¿ä»£çš„T2IåˆæˆèŒƒå¼ï¼Œå°†å¤æ‚çš„å¤šæ­¥ç”Ÿæˆä»»åŠ¡åˆ†è§£ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼šï¼ˆaï¼‰ç”Ÿæˆï¼šæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸€ä¸ªå›¾åƒï¼›ï¼ˆbï¼‰è§„åˆ’ï¼šæˆ‘ä»¬åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯†åˆ«ç”Ÿæˆå›¾åƒä¸­å•ä¸ªå¯¹è±¡åŠå…¶å±æ€§çš„é”™è¯¯ï¼Œå¹¶äº§ç”Ÿä¸€ç³»åˆ—ä»¥ç¼–è¾‘è®¡åˆ’å½¢å¼å­˜åœ¨çš„æ ¡æ­£æ­¥éª¤ã€‚ï¼ˆcï¼‰ç¼–è¾‘ï¼šæˆ‘ä»¬åˆ©ç”¨ç°æœ‰çš„æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘æ¨¡å‹ï¼ŒæŒ‰åºåœ¨ç”Ÿæˆçš„å›¾åƒä¸Šæ‰§è¡Œæˆ‘ä»¬çš„ç¼–è¾‘è®¡åˆ’ï¼Œä»¥è·å¾—å¿ å®äºåŸå§‹æŒ‡ä»¤çš„ç†æƒ³å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¹‹æ‰€ä»¥å¼ºå¤§ï¼Œæ˜¯å› ä¸ºå®ƒå…·æœ‰æ¨¡å—åŒ–æ€§è´¨ã€æ— éœ€è®­ç»ƒï¼Œå¹¶ä¸”å¯ä»¥åº”ç”¨äºä»»ä½•å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ¨¡å‹çš„ç»„åˆã€‚ä½œä¸ºé¢å¤–çš„è´¡çŒ®ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªèƒ½å¤Ÿè¿›è¡Œç»„åˆç¼–è¾‘çš„æ¨¡å‹ï¼Œè¿™æœ‰åŠ©äºè¿›ä¸€æ­¥æé«˜æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æ•´ä½“å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•çµæ´»åœ°å¹³è¡¡äº†ç»„åˆæ–‡æœ¬æç¤ºçš„æ¨ç†æ—¶é—´ä¸æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•å’Œåä¸ªT2Iæ¨¡å‹ï¼ˆåŒ…æ‹¬DALLE-3å’Œæœ€æ–°çš„SD-3.5-Largeï¼‰ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒè¯„ä¼°ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æé«˜äº†SOTAæ¨¡å‹é«˜è¾¾3ä¸ªç‚¹çš„æ€§èƒ½ï¼Œè¿˜ç¼©å°äº†å¼ºå¼±æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚è¯¦æƒ…è¯·è®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://dair-iitd.github.io/GraPE/]">https://dair-iitd.github.io/GraPE/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06089v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆé¢†åŸŸå€ŸåŠ©æ‰©æ•£æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤ŸåŸºäºæ–‡æœ¬æç¤ºç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ–‡æœ¬æç¤ºæ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠç»„åˆå’Œå¤šæ­¥æ¨ç†çš„æƒ…å†µä¸‹ã€‚é’ˆå¯¹å¤æ‚æŒ‡ä»¤ï¼Œç°æœ‰é¡¶å°–æ¨¡å‹åœ¨å¿ å®å»ºæ¨¡ç‰©ä½“å±æ€§å’Œå®ƒä»¬ä¹‹é—´çš„å…³ç³»æ—¶ç»å¸¸å‡ºé”™ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„T2IåˆæˆèŒƒå¼ï¼Œå°†å¤æ‚çš„å¤šæ­¥ç”Ÿæˆä»»åŠ¡åˆ†è§£ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼šç”Ÿæˆã€è®¡åˆ’å’Œç¼–è¾‘ã€‚é¦–å…ˆï¼Œåˆ©ç”¨ç°æœ‰æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒï¼›å…¶æ¬¡ï¼Œè¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¯†åˆ«å›¾åƒä¸­å•ä¸ªå¯¹è±¡åŠå…¶å±æ€§çš„é”™è¯¯ï¼Œå¹¶ç”Ÿæˆä¿®æ­£æ­¥éª¤åºåˆ—ï¼›æœ€åï¼Œå€ŸåŠ©æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ¨¡å‹ï¼ŒæŒ‰ä¿®æ­£æ­¥éª¤å¯¹ç”Ÿæˆå›¾åƒè¿›è¡Œç¼–è¾‘ï¼Œå¾—åˆ°å¿ å®äºåŸå§‹æŒ‡ä»¤çš„å›¾åƒã€‚æœ¬ç ”ç©¶çš„ä¼˜åŠ¿åœ¨äºå…¶æ¨¡å—åŒ–ç‰¹æ€§ã€æ— éœ€è®­ç»ƒï¼Œä¸”å¯åº”ç”¨äºä»»ä½•å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ¨¡å‹çš„ç»„åˆã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ä¸ªèƒ½å¤Ÿè¿›è¡Œç»„åˆç¼–è¾‘çš„æ¨¡å‹ï¼Œè¿›ä¸€æ­¥æé«˜æ•´ä½“å‡†ç¡®æ€§ï¼Œå¹¶çµæ´»åœ°åœ¨æ¨ç†æ—¶é—´å’Œæ€§èƒ½ä¹‹é—´æƒè¡¡ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•å’Œåä¸ªT2Iæ¨¡å‹ï¼ˆåŒ…æ‹¬DALLE-3å’Œæœ€æ–°çš„SD-3.5-Largeï¼‰ä¸Šçš„å¹¿æ³›å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…æé«˜äº†ç°æœ‰é¡¶å°–æ¨¡å‹æ€§èƒ½ï¼Œå‡å°‘äº†é«˜è¾¾3ä¸ªç‚¹ï¼Œè¿˜ç¼©å°äº†å¼ºå¼±æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆé¢†åŸŸè™½ç„¶å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†å¤æ‚æ–‡æœ¬æç¤ºæ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰é¡¶å°–æ¨¡å‹åœ¨å¿ å®å»ºæ¨¡ç‰©ä½“å±æ€§å’Œå®ƒä»¬ä¹‹é—´çš„å…³ç³»æ—¶ç»å¸¸å‡ºé”™ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„T2IåˆæˆèŒƒå¼ï¼ŒåŒ…æ‹¬ç”Ÿæˆã€è®¡åˆ’å’Œç¼–è¾‘ä¸‰ä¸ªæ­¥éª¤ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¯†åˆ«å›¾åƒä¸­çš„é”™è¯¯ï¼Œå¹¶ç”Ÿæˆä¿®æ­£æ­¥éª¤åºåˆ—ã€‚</li>
<li>å€ŸåŠ©æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ¨¡å‹æŒ‰ä¿®æ­£æ­¥éª¤è¿›è¡Œç¼–è¾‘ï¼Œå¾—åˆ°å¿ å®äºåŸå§‹æŒ‡ä»¤çš„å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰æ¨¡å—åŒ–ç‰¹æ€§ã€æ— éœ€è®­ç»ƒï¼Œä¸”å¯çµæ´»åº”ç”¨äºä¸åŒçš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ¨¡å‹ç»„åˆã€‚</li>
<li>ç ”ç©¶è¿˜å¼€å‘äº†èƒ½å¤Ÿè¿›è¡Œç»„åˆç¼–è¾‘çš„æ¨¡å‹ï¼Œæé«˜äº†æ•´ä½“å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡çµæ´»è°ƒæ•´æ¨ç†æ—¶é—´æ¥å®ç°æ€§èƒ½ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06089">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5568f59e3344194e24ec9243893970e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cc827e741254223ff3adf9513ead5e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74eb3a419875edf5c733f51ddb5bab66.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-365125c6aa93e432e06308a06b0693f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78a43d2da1227d7f3846792e65fcce4a.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6089824ad79aeedf6a4b9311eecffd0e.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Fair Federated Medical Image Classification Against Quality Shift via   Inter-Client Progressive State Matching
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ad456a537422b2a82cd6af9b1e5c7a92.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Uni-Gaussians Unifying Camera and Lidar Simulation with Gaussians for   Dynamic Driving Scenarios
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">15332k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
