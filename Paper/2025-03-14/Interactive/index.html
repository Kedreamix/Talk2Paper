<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Interactive">
    <meta name="description" content="Interactive 方向最新论文已更新，请持续关注 Update in 2025-03-14  Interpretable and Robust Dialogue State Tracking via Natural Language   Summarization with LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Interactive | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-fe6e87275f26f3c4fd621db6788b6e6d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Interactive</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Interactive/">
                                <span class="chip bg-color">Interactive</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                Interactive
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-14-更新"><a href="#2025-03-14-更新" class="headerlink" title="2025-03-14 更新"></a>2025-03-14 更新</h1><h2 id="Interpretable-and-Robust-Dialogue-State-Tracking-via-Natural-Language-Summarization-with-LLMs"><a href="#Interpretable-and-Robust-Dialogue-State-Tracking-via-Natural-Language-Summarization-with-LLMs" class="headerlink" title="Interpretable and Robust Dialogue State Tracking via Natural Language   Summarization with LLMs"></a>Interpretable and Robust Dialogue State Tracking via Natural Language   Summarization with LLMs</h2><p><strong>Authors:Rafael Carranza, Mateo Alejandro Rojas</strong></p>
<p>This paper introduces a novel approach to Dialogue State Tracking (DST) that leverages Large Language Models (LLMs) to generate natural language descriptions of dialogue states, moving beyond traditional slot-value representations. Conventional DST methods struggle with open-domain dialogues and noisy inputs. Motivated by the generative capabilities of LLMs, our Natural Language DST (NL-DST) framework trains an LLM to directly synthesize human-readable state descriptions. We demonstrate through extensive experiments on MultiWOZ 2.1 and Taskmaster-1 datasets that NL-DST significantly outperforms rule-based and discriminative BERT-based DST baselines, as well as generative slot-filling GPT-2 DST models, in both Joint Goal Accuracy and Slot Accuracy. Ablation studies and human evaluations further validate the effectiveness of natural language state generation, highlighting its robustness to noise and enhanced interpretability. Our findings suggest that NL-DST offers a more flexible, accurate, and human-understandable approach to dialogue state tracking, paving the way for more robust and adaptable task-oriented dialogue systems. </p>
<blockquote>
<p>本文介绍了一种利用大型语言模型（LLM）生成对话状态自然语言描述的新型对话状态跟踪（DST）方法，该方法超越了传统的插槽值表示。传统的DST方法难以处理开放域对话和嘈杂的输入。受LLM生成能力的启发，我们的自然语言DST（NL-DST）框架训练LLM直接合成可读的状态描述。我们在MultiWOZ 2.1和Taskmaster-1数据集上进行了大量实验，证明了NL-DST在联合目标准确性和插槽准确性方面都显著优于基于规则的、基于判别BERT的DST基线以及基于生成插槽填充GPT-2的DST模型。消融研究（Ablation studies）和人类评估进一步验证了自然语言状态生成的有效性，突出了其对噪声的鲁棒性和增强的可解释性。我们的研究结果表明，NL-DST提供了一种更灵活、准确、易于人类理解的对对话状态进行跟踪的方法，为更稳健、适应性更强的任务导向型对话系统铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08857v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>新一代对话状态追踪技术NL-DST通过运用大型语言模型生成对话状态的自然语言描述，改变了传统槽值表示方式。其在MultiWOZ 2.1和Taskmaster-1数据集上的实验结果显示，NL-DST在联合目标准确率和槽准确率上显著优于基于规则和判别式BERT的DST基线模型以及基于GPT-2的生成式槽填充DST模型。其有效性和优越性通过消融研究和人类评估得到进一步验证。NL-DST为任务导向型对话系统提供了更灵活、准确和人性化的对话状态追踪方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NL-DST技术运用大型语言模型（LLMs）生成对话状态的自然语言描述，摒弃了传统的槽值表示方式。</li>
<li>NL-DST在MultiWOZ 2.1和Taskmaster-1数据集上的实验表现优于其他方法。</li>
<li>NL-DST在联合目标准确率和槽准确率上取得了显著的提升。</li>
<li>消融研究和人类评估验证了NL-DST的有效性和优越性。</li>
<li>NL-DST技术提高了任务导向型对话系统的灵活性和准确性。</li>
<li>NL-DST能够增强对话系统的适应性和鲁棒性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08857">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ec432d6f0e4f875a2eae008fb4a4b347.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d342af86b39ad7682e4baf08cdd8b374.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfd032116a610ecd34e900ddccc9a0f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89e2d768de30ef530d2218b3e94eb0f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8162cacca9c61d1913796f0ff1a74047.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-725c145684380170546cce660ac936d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a2bb18aadf90c647f12fba92fb30748.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Contrastive-Speaker-Aware-Learning-for-Multi-party-Dialogue-Generation-with-LLMs"><a href="#Contrastive-Speaker-Aware-Learning-for-Multi-party-Dialogue-Generation-with-LLMs" class="headerlink" title="Contrastive Speaker-Aware Learning for Multi-party Dialogue Generation   with LLMs"></a>Contrastive Speaker-Aware Learning for Multi-party Dialogue Generation   with LLMs</h2><p><strong>Authors:Tianyu Sun, Kun Qian, Wenhong Wang</strong></p>
<p>Multi-party dialogue generation presents significant challenges due to the complex interplay of multiple speakers and interwoven conversational threads. Traditional approaches often fall short in capturing these complexities, particularly when relying on manually annotated dialogue relations. This paper introduces Speaker-Attentive LLM (SA-LLM), a novel generative model that leverages pre-trained Large Language Models (LLMs) and a speaker-aware contrastive learning strategy to address these challenges. SA-LLM incorporates a speaker-attributed input encoding and a contrastive learning objective to implicitly learn contextual coherence and speaker roles without explicit relation annotations. Extensive experiments on the Ubuntu IRC and Movie Dialogues datasets demonstrate that SA-LLM significantly outperforms state-of-the-art baselines in automatic and human evaluations, achieving superior performance in fluency, coherence, informativeness, and response diversity. Ablation studies and detailed error analyses further validate the effectiveness of the proposed speaker-attentive training approach, highlighting its robustness across different speaker roles and context lengths. The results underscore the potential of SA-LLM as a powerful and annotation-free solution for high-quality multi-party dialogue generation. </p>
<blockquote>
<p>多方对话生成存在显著挑战，主要由于多个发言者之间的复杂交互以及相互交织的对话线程。传统方法往往难以捕捉这些复杂性，特别是当依赖手动注释的对话关系时。本文介绍了Speaker-Attentive LLM（SA-LLM），这是一种新型生成模型，它利用预训练的大型语言模型（LLM）和发言者感知对比学习策略来解决这些挑战。SA-LLM结合了发言者属性输入编码和对比学习目标，可以隐式地学习上下文连贯性和发言者角色，而无需明确的关联注释。在Ubuntu IRC和电影对话数据集上的大量实验表明，SA-LLM在自动和人类评估中均显著优于最新基线，在流畅性、连贯性、信息丰富度和响应多样性方面表现出卓越性能。消融研究和详细的误差分析进一步验证了所提出的基于发言者注意力的训练方法的有效性，突出了其在不同发言者角色和上下文长度方面的稳健性。结果强调了SA-LLM作为无需注释的高质量多方对话生成解决方案的巨大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08842v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一个名为Speaker-Attentive LLM（SA-LLM）的新型生成模型，该模型利用预训练的大型语言模型（LLM）和说话者感知对比学习策略，解决了多方对话生成中的复杂挑战。SA-LLM通过说话者属性输入编码和对比学习目标，可以隐式地学习上下文连贯性和说话者角色，无需明确的关系注释。在Ubuntu IRC和电影对话数据集上的大量实验表明，SA-LLM在自动和人类评估中均显著优于最新基线，在流畅性、连贯性、信息性和响应多样性方面表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多方对话生成面临复杂挑战，因为涉及多个发言者的复杂交互和交织的会话线程。</li>
<li>传统方法常常难以捕捉这些复杂性，特别是在依赖手动注释的对话关系时。</li>
<li>SA-LLM是一个新型生成模型，利用预训练的大型语言模型和说话者感知对比学习策略来解决这些挑战。</li>
<li>SA-LLM通过说话者属性输入编码和对比学习目标，可以隐式地学习上下文连贯性和说话者角色。</li>
<li>在Ubuntu IRC和电影对话数据集上的实验表明，SA-LLM在流畅性、连贯性、信息性和响应多样性方面优于其他模型。</li>
<li>消融研究和详细的错误分析进一步验证了所提出的说话者注意训练方法的有效性，突显其在不同说话者角色和上下文长度上的稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08842">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-76a7f3157e0e59784726d0d8cb62ee94.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ESPnet-SDS-Unified-Toolkit-and-Demo-for-Spoken-Dialogue-Systems"><a href="#ESPnet-SDS-Unified-Toolkit-and-Demo-for-Spoken-Dialogue-Systems" class="headerlink" title="ESPnet-SDS: Unified Toolkit and Demo for Spoken Dialogue Systems"></a>ESPnet-SDS: Unified Toolkit and Demo for Spoken Dialogue Systems</h2><p><strong>Authors:Siddhant Arora, Yifan Peng, Jiatong Shi, Jinchuan Tian, William Chen, Shikhar Bharadwaj, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Shuichiro Shimizu, Vaibhav Srivastav, Shinji Watanabe</strong></p>
<p>Advancements in audio foundation models (FMs) have fueled interest in end-to-end (E2E) spoken dialogue systems, but different web interfaces for each system makes it challenging to compare and contrast them effectively. Motivated by this, we introduce an open-source, user-friendly toolkit designed to build unified web interfaces for various cascaded and E2E spoken dialogue systems. Our demo further provides users with the option to get on-the-fly automated evaluation metrics such as (1) latency, (2) ability to understand user input, (3) coherence, diversity, and relevance of system response, and (4) intelligibility and audio quality of system output. Using the evaluation metrics, we compare various cascaded and E2E spoken dialogue systems with a human-human conversation dataset as a proxy. Our analysis demonstrates that the toolkit allows researchers to effortlessly compare and contrast different technologies, providing valuable insights such as current E2E systems having poorer audio quality and less diverse responses. An example demo produced using our toolkit is publicly available here: <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Siddhant/Voice_Assistant_Demo">https://huggingface.co/spaces/Siddhant/Voice_Assistant_Demo</a>. </p>
<blockquote>
<p>音频基础模型（FMs）的进展激发了人们对端到端（E2E）口语对话系统的兴趣，但每个系统不同的网络界面使得有效地比较和对比它们具有挑战性。基于此，我们引入了一个开源、用户友好的工具包，旨在为各种级联和端到端的口语对话系统构建统一的网络界面。我们的演示还为用户提供即时自动评估指标选项，例如（1）延迟时间、（2）理解用户输入的能力、（3）系统响应的一致性、多样性和相关性以及（4）系统输出的清晰度和音频质量。使用评估指标，我们将各种级联和端到端的口语对话系统与人类对话数据集进行比对分析。分析表明，该工具包允许研究人员轻松比较不同的技术，提供有价值的见解，例如当前的端到端系统在音频质量方面较差且响应不够多样化。使用我们的工具包制作的演示实例可在以下网址找到：<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Siddhant/Voice_Assistant_Demo">https://huggingface.co/spaces/Siddhant/Voice_Assistant_Demo</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08533v1">PDF</a> Accepted at NAACL 2025 Demo Track</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个开源的、用户友好的工具包，该工具包旨在为各种不同的级联和端到端对话系统构建统一的网络界面。用户可以通过该工具包提供的在线即时评估指标，对不同系统的性能进行比较和对比。分析表明，该工具包能够帮助研究人员轻松比较不同的技术，并提供有价值的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了开源、用户友好的工具包，为多种级联和端到端对话系统提供统一的网络界面。</li>
<li>工具包提供即时评估指标，包括延迟、理解用户输入的能力、系统响应的连贯性、多样性和相关性以及系统输出音频的质量和清晰度。</li>
<li>通过与人类-人类对话数据集的比较，分析了不同级联和端到端对话系统的性能。</li>
<li>目前的端到端系统存在音频质量较差和响应不够多样化的问题。</li>
<li>工具包使得比较和对比不同技术变得轻松。</li>
<li>提供了使用此工具包的示例演示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08533">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d35c752774b2accd537889e2e4e26d05.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76f93e1312ba7db872b0a424eb37753d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1be63e562a057bdcbffbd8665e06aa91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85503aaf96bee95eec6a360a8751574a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5d9c955af00195e7e716b7f974797ae.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HERO-Human-Reaction-Generation-from-Videos"><a href="#HERO-Human-Reaction-Generation-from-Videos" class="headerlink" title="HERO: Human Reaction Generation from Videos"></a>HERO: Human Reaction Generation from Videos</h2><p><strong>Authors:Chengjun Yu, Wei Zhai, Yuhang Yang, Yang Cao, Zheng-Jun Zha</strong></p>
<p>Human reaction generation represents a significant research domain for interactive AI, as humans constantly interact with their surroundings. Previous works focus mainly on synthesizing the reactive motion given a human motion sequence. This paradigm limits interaction categories to human-human interactions and ignores emotions that may influence reaction generation. In this work, we propose to generate 3D human reactions from RGB videos, which involves a wider range of interaction categories and naturally provides information about expressions that may reflect the subject’s emotions. To cope with this task, we present HERO, a simple yet powerful framework for Human rEaction geneRation from videOs. HERO considers both global and frame-level local representations of the video to extract the interaction intention, and then uses the extracted interaction intention to guide the synthesis of the reaction. Besides, local visual representations are continuously injected into the model to maximize the exploitation of the dynamic properties inherent in videos. Furthermore, the ViMo dataset containing paired Video-Motion data is collected to support the task. In addition to human-human interactions, these video-motion pairs also cover animal-human interactions and scene-human interactions. Extensive experiments demonstrate the superiority of our methodology. The code and dataset will be publicly available at <a target="_blank" rel="noopener" href="https://jackyu6.github.io/HERO">https://jackyu6.github.io/HERO</a>. </p>
<blockquote>
<p>人类反应生成对于交互式AI来说是一个重要的研究领域，因为人类不断地与周围环境进行交互。以前的工作主要集中在给定人类运动序列后合成反应运动。这种范式将交互类别限制在人与人之间的互动上，并忽略了可能影响反应生成的情绪。在这项工作中，我们提出从RGB视频中生成3D人类反应，这涉及更广泛的交互类别，并自然地提供有关可能反映主题情绪的表达的信息。为了应对这项任务，我们推出了HERO，一个简单而强大的从视频中生成人类反应的框架。HERO考虑了视频的全局和帧级局部表示来提取交互意图，然后使用提取的交互意图来指导反应的合成。此外，将局部视觉表示持续注入模型，以最大限度地利用视频的内在动态属性。此外，还收集了包含配对视频-运动数据的ViMo数据集以支持这项任务。除了人与人之间的互动外，这些视频-运动对还包括动物-人与场景-人之间的交互。大量实验证明了我们方法的优越性。代码和数据集将在<a target="_blank" rel="noopener" href="https://jackyu6.github.io/HERO%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://jackyu6.github.io/HERO公开可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08270v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于RGB视频的三维人类反应生成方法，旨在从更广泛的交互类别中生成反应，并自然地获取表达情感的信息。为此，研究团队设计了一个名为HERO的简洁而强大的框架，用于从视频中提取全局和帧级局部表示来提取交互意图，并据此合成反应。同时，连续注入局部视觉表示以充分利用视频中的动态属性。此外，该研究还收集了支持任务的ViMo数据集，包含视频动作配对数据，涵盖人与人、动物与人和场景与人的交互。实验证明该方法具有优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人类反应生成是交互式AI的重要研究领域，涉及更广泛的交互类别和情绪影响。</li>
<li>现有研究主要关注基于人类运动序列的反应动作合成，忽略了情感因素。</li>
<li>本文提出一种基于RGB视频的三维人类反应生成方法，利用HERO框架从视频中提取交互意图并生成反应。</li>
<li>HERO框架结合了全局和帧级局部表示来提高交互意图的提取准确性。</li>
<li>局部视觉表示的持续注入有助于充分利用视频中的动态属性。</li>
<li>收集了包含视频动作配对数据的ViMo数据集，支持多种交互类别的反应生成任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08270">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ce21e913ee40b9561333d703af1ee36d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cda399d934b6c76bf972bf1d1bbc069d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0448dd44c19b9a0a27ff1348645656e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-319bc121833d90e9805690a368168588.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Dialogue-Injection-Attack-Jailbreaking-LLMs-through-Context-Manipulation"><a href="#Dialogue-Injection-Attack-Jailbreaking-LLMs-through-Context-Manipulation" class="headerlink" title="Dialogue Injection Attack: Jailbreaking LLMs through Context   Manipulation"></a>Dialogue Injection Attack: Jailbreaking LLMs through Context   Manipulation</h2><p><strong>Authors:Wenlong Meng, Fan Zhang, Wendao Yao, Zhenyuan Guo, Yuwei Li, Chengkun Wei, Wenzhi Chen</strong></p>
<p>Large language models (LLMs) have demonstrated significant utility in a wide range of applications; however, their deployment is plagued by security vulnerabilities, notably jailbreak attacks. These attacks manipulate LLMs to generate harmful or unethical content by crafting adversarial prompts. While much of the current research on jailbreak attacks has focused on single-turn interactions, it has largely overlooked the impact of historical dialogues on model behavior. In this paper, we introduce a novel jailbreak paradigm, Dialogue Injection Attack (DIA), which leverages the dialogue history to enhance the success rates of such attacks. DIA operates in a black-box setting, requiring only access to the chat API or knowledge of the LLM’s chat template. We propose two methods for constructing adversarial historical dialogues: one adapts gray-box prefilling attacks, and the other exploits deferred responses. Our experiments show that DIA achieves state-of-the-art attack success rates on recent LLMs, including Llama-3.1 and GPT-4o. Additionally, we demonstrate that DIA can bypass 5 different defense mechanisms, highlighting its robustness and effectiveness. </p>
<blockquote>
<p>大型语言模型（LLMs）在广泛的应用领域已经展现出显著的实用性；然而，它们的部署却受到安全漏洞的困扰，特别是越狱攻击。这些攻击通过制造对抗性提示来操纵LLMs生成有害或不道德的内容。虽然目前关于越狱攻击的研究大多集中在单轮交互上，但它对模型行为的历史对话的影响却被大大忽视了。在本文中，我们介绍了一种新型的越狱范式——对话注入攻击（DIA），它利用对话历史来提高此类攻击的成功率。DIA在黑箱环境中运行，只需要访问聊天API或了解LLM的聊天模板。我们提出了两种构建对抗性历史对话的方法：一种是对灰盒预填充攻击进行适应，另一种是利用延迟响应。我们的实验表明，DIA在最新的LLMs上达到了最先进的攻击成功率，包括Llama-3.1和GPT-4o。此外，我们还证明了DIA可以绕过五种不同的防御机制，这突出了其稳健性和有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08195v1">PDF</a> 17 pages, 10 figures</p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLMs）在众多应用中展现出显著效用，但其部署面临安全漏洞问题，尤以监狱突破攻击最为突出。这些攻击通过制作对抗性提示来操纵LLMs生成有害或不道德的内容。当前关于监狱突破攻击的研究主要关注单轮交互，却忽视了历史对话对模型行为的影响。本文引入了一种新的监狱突破范式——对话注入攻击（DIA），它利用对话历史来提高此类攻击的成功率。DIA在黑色盒子环境中运行，只需访问聊天API或了解LLM的聊天模板。我们提出了两种构建对抗性历史对话的方法：一种采用灰色预填充攻击，另一种则利用延迟响应。实验表明，DIA在最近的LLMs上达到了最先进的攻击成功率，包括Llama-3.1和GPT-4o。此外，我们还证明了DIA能够绕过5种不同的防御机制，凸显了其稳健性和有效性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>对话注入攻击（DIA）是一种新型监狱突破攻击范式，利用历史对话信息提高攻击成功率。</li>
<li>DIA在黑色盒子环境中运行，仅需要访问聊天API或了解LLM的聊天模板。</li>
<li>提出了两种构建对抗性历史对话的方法：灰色预填充攻击和延迟响应方法。</li>
<li>DIA在多种大型语言模型上实现了高攻击成功率，包括Llama-3.1和GPT-4o。</li>
<li>DIA能够绕过现有防御机制，显示出其强大的稳健性和有效性。</li>
<li>当前对LLM安全性的研究主要关注单轮交互，而本文强调了历史对话对模型行为的影响，为未来的研究提供了新的视角。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08195">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f90549aacbd8366f85f5ef68013b5a2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf809e9a010910cf2c27895351b5d72c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76b71c693a098ddc1ca446a6d1fca646.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9327da1ab30e8cb4197507bfcba023fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45fac756edc881b7b680a853a2d3a397.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Towards-Large-scale-Chemical-Reaction-Image-Parsing-via-a-Multimodal-Large-Language-Model"><a href="#Towards-Large-scale-Chemical-Reaction-Image-Parsing-via-a-Multimodal-Large-Language-Model" class="headerlink" title="Towards Large-scale Chemical Reaction Image Parsing via a Multimodal   Large Language Model"></a>Towards Large-scale Chemical Reaction Image Parsing via a Multimodal   Large Language Model</h2><p><strong>Authors:Yufan Chen, Ching Ting Leung, Jianwei Sun, Yong Huang, Linyan Li, Hao Chen, Hanyu Gao</strong></p>
<p>Artificial intelligence (AI) has demonstrated significant promise in advancing organic chemistry research; however, its effectiveness depends on the availability of high-quality chemical reaction data. Currently, most published chemical reactions are not available in machine-readable form, limiting the broader application of AI in this field. The extraction of published chemical reactions into structured databases still relies heavily on manual curation, and robust automatic parsing of chemical reaction images into machine-readable data remains a significant challenge. To address this, we introduce the Reaction Image Multimodal large language model (RxnIM), the first multimodal large language model specifically designed to parse chemical reaction images into machine-readable reaction data. RxnIM not only extracts key chemical components from reaction images but also interprets the textual content that describes reaction conditions. Together with specially designed large-scale dataset generation method to support model training, our approach achieves excellent performance, with an average F1 score of 88% on various benchmarks, surpassing literature methods by 5%. This represents a crucial step toward the automatic construction of large databases of machine-readable reaction data parsed from images in the chemistry literature, providing essential data resources for AI research in chemistry. The source code, model checkpoints, and datasets developed in this work are released under permissive licenses. An instance of the RxnIM web application can be accessed at <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/CYF200127/RxnIM">https://huggingface.co/spaces/CYF200127/RxnIM</a>. </p>
<blockquote>
<p>人工智能在推动有机化学研究方面显示出巨大潜力，但其有效性取决于高质量化学反应数据的可用性。目前，大多数已发表的化学反应并非以机器可读的形式存在，这限制了人工智能在这一领域的更广泛应用。从文献中提取化学反应并将其输入结构化数据库仍严重依赖于手动处理，而自动解析化学反应图像以产生机器可读数据仍是一个重大挑战。为解决这一问题，我们引入了反应图像多模态大型语言模型（RxnIM），这是专门设计用于解析化学反应图像以产生机器可读反应数据的第一款多模态大型语言模型。RxnIM不仅从反应图像中提取关键化学成分，还解释描述反应条件的文本内容。结合专门设计的用于支持模型训练的大规模数据集生成方法，我们的方法在各种基准测试上表现出优异的性能，平均F1分数达到88%，比文献方法高出5%。这是朝着自动构建从化学文献图像中解析出的机器可读反应数据大型数据库的重要一步，为化学人工智能研究提供了必要的数据资源。本工作中开发的源代码、模型检查点和数据集均在许可下发布。RxnIM网页应用的一个实例可访问<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/CYF200127/RxnIM%E3%80%82">https://huggingface.co/spaces/CYF200127/RxnIM。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08156v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>人工智能在推动有机化学反应研究方面展现出巨大潜力，但其有效性取决于高质量化学反应数据的可用性。目前，大多数已发布的化学反应并非以机器可读的形式存在，限制了人工智能在该领域的广泛应用。针对这一问题，我们推出了反应图像多模态大型语言模型（RxnIM），该模型能够解析化学反应图像并生成机器可读的反应数据。该模型不仅能从反应图像中提取关键化学成分，还能解读描述反应条件的文本内容。配合专门设计的大型数据集生成方法支持模型训练，该方法在多个基准测试上取得了平均F1分数为88%的优异性能，较文献方法高出5%。这是朝着自动构建大型机器可读反应数据库迈出的重要一步，为化学领域的人工智能研究提供了宝贵的数据资源。该工作的源代码、模型检查点和数据集均在许可下发布。RxnIM web应用程序的实例可访问<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/CYF200127/RxnIM%E3%80%82">https://huggingface.co/spaces/CYF200127/RxnIM。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人工智能在有机化学反应研究中的应用受限于高质量化学反应数据的缺乏。</li>
<li>目前化学反应数据主要从图像中提取，这一过程主要依赖手动整理，自动解析存在挑战。</li>
<li>RxnIM模型能够解析化学反应图像并生成机器可读的反应数据，包括关键化学成分和反应条件。</li>
<li>RxnIM模型在多个基准测试上表现出优异的性能，平均F1分数为88%。</li>
<li>该方法较文献方法有所提升，提高了5%的性能。</li>
<li>该工作朝着自动构建大型机器可读反应数据库迈出了重要的一步。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08156">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-86d9d8c678a03a73c4a14dd195d4b154.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3773c0086faf452711757baa64a23701.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2cfa69fbce013bdfb2049b0c7961ade5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-903c477a2e4389b8dfcebc0d3c295709.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9702bfd41413e3128a7c811634b2ec0e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Transformer-Model-for-Predicting-Chemical-Reaction-Products-from-Generic-Templates"><a href="#A-Transformer-Model-for-Predicting-Chemical-Reaction-Products-from-Generic-Templates" class="headerlink" title="A Transformer Model for Predicting Chemical Reaction Products from   Generic Templates"></a>A Transformer Model for Predicting Chemical Reaction Products from   Generic Templates</h2><p><strong>Authors:Derin Ozer, Sylvain Lamprier, Thomas Cauchy, Nicolas Gutowski, Benoit Da Mota</strong></p>
<p>The accurate prediction of chemical reaction outcomes is a major challenge in computational chemistry. Current models rely heavily on either highly specific reaction templates or template-free methods, both of which present limitations. To address these limitations, this work proposes the Broad Reaction Set (BRS), a dataset featuring 20 generic reaction templates that allow for the efficient exploration of the chemical space. Additionally, ProPreT5 is introduced, a T5 model tailored to chemistry that achieves a balance between rigid templates and template-free methods. ProPreT5 demonstrates its capability to generate accurate, valid, and realistic reaction products, making it a promising solution that goes beyond the current state-of-the-art on the complex reaction product prediction task. </p>
<blockquote>
<p>化学反应结果的准确预测是计算化学领域的一个重大挑战。当前模型严重依赖于高度特定的反应模板或无模板方法，这两者都存在一定的局限性。为了解决这些局限性，本研究提出了Broad Reaction Set（BRS），这是一组包含20个通用反应模板的数据集，能够高效地探索化学空间。此外，还介绍了ProPreT5，这是一个针对化学领域的T5模型，在刚性模板和无模板方法之间取得了平衡。ProPreT5展示出了生成准确、有效且逼真的反应产物的能力，成为了一项有前途的解决方案，在复杂的反应产物预测任务上超越了当前最先进的水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05810v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了计算化学中预测化学反应结果的主要挑战。为解决当前模型存在的局限性，提出了一种名为Broad Reaction Set（BRS）的通用反应模板数据集，并引入了专为化学定制的T5模型——ProPreT5。该模型能够在不使用刚性模板的情况下，实现高效探索化学空间，生成准确、有效且真实的反应产物，为复杂反应产物预测任务提供了超越当前最新技术的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>计算化学面临准确预测化学反应结果的主要挑战。</li>
<li>当前模型依赖于特定反应模板或无模板方法，但都存在局限性。</li>
<li>为解决这些挑战，提出了Broad Reaction Set（BRS），包含20个通用反应模板，可高效探索化学空间。</li>
<li>引入了ProPreT5模型，它是专为化学定制的T5模型，实现了在刚性模板和无模板方法之间的平衡。</li>
<li>ProPreT5模型能够生成准确、有效且真实的反应产物。</li>
<li>ProPreT5模型在复杂反应产物预测任务上表现优异，超越了当前最新技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05810">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b63820b6956d9a8a6f0bdc1b1571c950.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-14\./crop_Interactive/2503.05810v2/page_2_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02a93e5237972c82a483e852ac3a59a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8937d9deeb1fb51189797e6cff733ae.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CS-Dialogue-A-104-Hour-Dataset-of-Spontaneous-Mandarin-English-Code-Switching-Dialogues-for-Speech-Recognition"><a href="#CS-Dialogue-A-104-Hour-Dataset-of-Spontaneous-Mandarin-English-Code-Switching-Dialogues-for-Speech-Recognition" class="headerlink" title="CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English   Code-Switching Dialogues for Speech Recognition"></a>CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English   Code-Switching Dialogues for Speech Recognition</h2><p><strong>Authors:Jiaming Zhou, Yujie Guo, Shiwan Zhao, Haoqin Sun, Hui Wang, Jiabei He, Aobo Kong, Shiyao Wang, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin</strong></p>
<p>Code-switching (CS), the alternation between two or more languages within a single conversation, presents significant challenges for automatic speech recognition (ASR) systems. Existing Mandarin-English code-switching datasets often suffer from limitations in size, spontaneity, and the lack of full-length dialogue recordings with transcriptions, hindering the development of robust ASR models for real-world conversational scenarios. This paper introduces CS-Dialogue, a novel large-scale Mandarin-English code-switching speech dataset comprising 104 hours of spontaneous conversations from 200 speakers. Unlike previous datasets, CS-Dialogue provides full-length dialogue recordings with complete transcriptions, capturing naturalistic code-switching patterns in continuous speech. We describe the data collection and annotation processes, present detailed statistics of the dataset, and establish benchmark ASR performance using state-of-the-art models. Our experiments, using Transformer, Conformer, and Branchformer, demonstrate the challenges of code-switching ASR, and show that existing pre-trained models such as Whisper still have the space to improve. The CS-Dialogue dataset will be made freely available for all academic purposes. </p>
<blockquote>
<p>语言切换（CS）是指在单次对话中切换使用两种或多种语言，这给自动语音识别（ASR）系统带来了重大挑战。现有的普通话-英语切换数据集往往在规模、自然度以及缺乏带有转录的全长对话录音等方面存在局限，阻碍了为真实世界对话场景开发稳健的ASR模型。本文介绍了CS-Dialogue，这是一个新的大规模普通话-英语切换语音数据集，包含来自200名发言者的104小时自然对话。不同于以前的数据集，CS-Dialogue提供了带有完整转录的全长对话录音，捕捉连续语音中的自然语言切换模式。我们描述了数据收集和注释过程，给出了数据集的详细统计信息，并使用最新模型建立了基准ASR性能。我们的实验使用了Transformer、Conformer和Branchformer，展示了语言切换ASR的挑战性，并表明现有的预训练模型，如Whisper仍有改进空间。CS-Dialogue数据集将免费提供给所有学术用途。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18913v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该论文针对代码切换（CS）问题，即一种语言中在对话中切换到另一种语言的现象，在自动语音识别（ASR）系统中存在诸多挑战。现有汉语-英语代码切换数据集常常存在规模限制、缺乏自发性以及没有全程对话录音和转录等问题，阻碍了在真实世界对话场景中的稳健ASR模型的发展。本文介绍了CS-Dialogue，这是一个新的大型汉语-英语代码切换语音数据集，包含来自200名发言人的104小时自发对话。与以前的数据集不同，CS-Dialogue提供了全程对话录音和完整转录，捕捉连续语音中的自然代码切换模式。本文描述了数据收集和注释过程，提供了数据集的详细统计信息，并使用最新模型建立了基准ASR性能。我们的实验表明，使用Transformer、Conformer和Branchformer等现有预训练模型仍然存在改进空间。CS-Dialogue数据集将免费提供给所有学术用途。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>代码切换（CS）在自动语音识别（ASR）系统中存在挑战。</li>
<li>现有汉语-英语代码切换数据集存在规模、自发性及全程对话录音和转录的缺乏等问题。</li>
<li>CS-Dialogue是一个新的大型汉语-英语代码切换语音数据集，包含104小时的自发对话。</li>
<li>CS-Dialogue提供了全程对话录音和完整转录，捕捉连续语音中的自然代码切换模式。</li>
<li>数据集描述了数据收集和注释过程，并提供了详细的统计信息。</li>
<li>使用最新模型进行的实验表明，现有的预训练模型在代码切换ASR方面仍有改进空间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18913">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a5d41c0acd651035fb457987bf0cbd52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48a9ec32dcf88f8c93bd03b14049382c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f35b68213455057b5ddfdb72d5aef37.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-708d7cb6863592ae4ca5a03617a80d11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab3f45528557a7893dd8c474d60eaaef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8050fcc384ae9f363eee6e31f83bb6f8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/Interactive/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/Interactive/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Interactive/">
                                    <span class="chip bg-color">Interactive</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1cadea61e4907cd6a69432076455f3d4.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-03-14  Versatile Multimodal Controls for Whole-Body Talking Human Animation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-bd3ade09a1ad7eeb7773864c1e648f0f.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-03-14  Network Calculus-based Deadline-Adaptive Online Admission Control for ET   Traffic in TSN
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29739.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
