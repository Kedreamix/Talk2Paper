<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Electromyography-Informed Facial Expression Reconstruction for   Physiological-Based Synthesis and Analysis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e9c42602c21b81f4a9b9c1e75bf0491a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    44 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-14-æ›´æ–°"><a href="#2025-03-14-æ›´æ–°" class="headerlink" title="2025-03-14 æ›´æ–°"></a>2025-03-14 æ›´æ–°</h1><h2 id="Electromyography-Informed-Facial-Expression-Reconstruction-for-Physiological-Based-Synthesis-and-Analysis"><a href="#Electromyography-Informed-Facial-Expression-Reconstruction-for-Physiological-Based-Synthesis-and-Analysis" class="headerlink" title="Electromyography-Informed Facial Expression Reconstruction for   Physiological-Based Synthesis and Analysis"></a>Electromyography-Informed Facial Expression Reconstruction for   Physiological-Based Synthesis and Analysis</h2><p><strong>Authors:Tim BÃ¼chner, Christoph Anders, Orlando Guntinas-Lichius, Joachim Denzler</strong></p>
<p>The relationship between muscle activity and resulting facial expressions is crucial for various fields, including psychology, medicine, and entertainment. The synchronous recording of facial mimicry and muscular activity via surface electromyography (sEMG) provides a unique window into these complex dynamics. Unfortunately, existing methods for facial analysis cannot handle electrode occlusion, rendering them ineffective. Even with occlusion-free reference images of the same person, variations in expression intensity and execution are unmatchable. Our electromyography-informed facial expression reconstruction (EIFER) approach is a novel method to restore faces under sEMG occlusion faithfully in an adversarial manner. We decouple facial geometry and visual appearance (e.g., skin texture, lighting, electrodes) by combining a 3D Morphable Model (3DMM) with neural unpaired image-to-image translation via reference recordings. Then, EIFER learns a bidirectional mapping between 3DMM expression parameters and muscle activity, establishing correspondence between the two domains. We validate the effectiveness of our approach through experiments on a dataset of synchronized sEMG recordings and facial mimicry, demonstrating faithful geometry and appearance reconstruction. Further, we synthesize expressions based on muscle activity and how observed expressions can predict dynamic muscle activity. Consequently, EIFER introduces a new paradigm for facial electromyography, which could be extended to other forms of multi-modal face recordings. </p>
<blockquote>
<p>è‚Œè‚‰æ´»åŠ¨ä¸äº§ç”Ÿçš„é¢éƒ¨è¡¨æƒ…ä¹‹é—´çš„å…³ç³»å¯¹å¿ƒç†å­¦ã€åŒ»å­¦å’Œå¨±ä¹ç­‰å¤šä¸ªé¢†åŸŸéƒ½è‡³å…³é‡è¦ã€‚é€šè¿‡è¡¨é¢è‚Œç”µå›¾ï¼ˆsEMGï¼‰åŒæ­¥è®°å½•é¢éƒ¨è¡¨æƒ…å’Œè‚Œè‚‰æ´»åŠ¨ï¼Œä¸ºè¿™äº›å¤æ‚åŠ¨æ€æä¾›äº†ä¸€ä¸ªç‹¬ç‰¹çš„çª—å£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é¢éƒ¨åˆ†ææ–¹æ³•æ— æ³•å¤„ç†ç”µæé®æŒ¡é—®é¢˜ï¼Œå¯¼è‡´å®ƒä»¬æ— æ•ˆã€‚å³ä½¿ä½¿ç”¨åŒä¸€äººæ— é®æŒ¡çš„å‚è€ƒå›¾åƒï¼Œè¡¨æƒ…å¼ºåº¦å’Œæ‰§è¡Œçš„å·®å¼‚ä¹Ÿæ— æ³•åŒ¹é…ã€‚æˆ‘ä»¬çš„ç”µç”Ÿç†å­¦å¯å‘ä¸‹çš„é¢éƒ¨è¡¨æƒ…é‡å»ºï¼ˆEIFERï¼‰æ–¹æ³•æ˜¯ä¸€ç§é‡‡ç”¨å¯¹æŠ—æ–¹å¼åœ¨sEMGé®æŒ¡ä¸‹å¿ å®è¿˜åŸé¢éƒ¨çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡ç»“åˆä¸‰ç»´å¯å˜å½¢æ¨¡å‹ï¼ˆ3DMMï¼‰å’Œé€šè¿‡å‚è€ƒè®°å½•è¿›è¡Œç¥ç»æ— é…å¯¹å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ï¼Œæ¥è§£è€¦é¢éƒ¨å‡ ä½•å½¢çŠ¶å’Œè§†è§‰å¤–è§‚ï¼ˆå¦‚çš®è‚¤çº¹ç†ã€å…‰ç…§ã€ç”µæç­‰ï¼‰ã€‚ç„¶åï¼ŒEIFERå­¦ä¹ 3DMMè¡¨æƒ…å‚æ•°ä¸è‚Œè‚‰æ´»åŠ¨ä¹‹é—´çš„åŒå‘æ˜ å°„ï¼Œåœ¨ä¸¤ä¸ªé¢†åŸŸä¹‹é—´å»ºç«‹å¯¹åº”å…³ç³»ã€‚æˆ‘ä»¬é€šè¿‡åŒæ­¥sEMGè®°å½•å’Œé¢éƒ¨è¡¨æƒ…çš„æ•°æ®é›†è¿›è¡Œå®éªŒï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å¿ å®çš„å‡ ä½•å’Œå¤–è§‚é‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ ¹æ®è‚Œè‚‰æ´»åŠ¨åˆæˆè¡¨æƒ…ï¼Œå¹¶æ¢è®¨è§‚å¯Ÿåˆ°çš„è¡¨æƒ…å¦‚ä½•é¢„æµ‹åŠ¨æ€è‚Œè‚‰æ´»åŠ¨ã€‚å› æ­¤ï¼ŒEIFERä¸ºé¢éƒ¨è‚Œç”µå›¾å¼•å…¥äº†æ–°çš„èŒƒå¼ï¼Œå¯ä»¥æ‰©å±•åˆ°å…¶ä»–å½¢å¼çš„å¤šæ¨¡å¼é¢éƒ¨è®°å½•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09556v1">PDF</a> Accepted at CVPR 2025, 41 pages, 37 figures, 8 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‚Œè‚‰æ´»åŠ¨ä¸é¢éƒ¨è¡¨æƒ…ä¹‹é—´çš„å…³é”®å…³ç³»ï¼Œå¹¶æŒ‡å‡ºå…¶åœ¨å¿ƒç†å­¦ã€åŒ»å­¦å’Œå¨±ä¹ç­‰é¢†åŸŸçš„é‡è¦æ€§ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹çš„ç”µè‚Œå›¾å¼•å¯¼é¢éƒ¨è¡¨æƒ…é‡å»ºï¼ˆEIFERï¼‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨è¡¨é¢è‚Œç”µå›¾ï¼ˆsEMGï¼‰é®æŒ¡çš„æƒ…å†µä¸‹æ¢å¤é¢éƒ¨å›¾åƒã€‚é€šè¿‡ç»“åˆä¸‰ç»´å¯å˜å½¢æ¨¡å‹ï¼ˆ3DMMï¼‰å’Œç¥ç»éé…å¯¹å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè§£è€¦é¢éƒ¨å‡ ä½•å½¢çŠ¶å’Œè§†è§‰å¤–è§‚ï¼Œå¹¶é€šè¿‡åŒå‘æ˜ å°„å­¦ä¹ è‚Œè‚‰æ´»åŠ¨ä¸é¢éƒ¨è¡¨æƒ…å‚æ•°ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒæ­¥sEMGè®°å½•å’Œé¢éƒ¨è¡¨æƒ…æ¨¡ä»¿æ•°æ®é›†ä¸Šå…·æœ‰è‰¯å¥½çš„å‡ ä½•å½¢çŠ¶å’Œå¤–è§‚é‡å»ºæ•ˆæœã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½æ ¹æ®è‚Œè‚‰æ´»åŠ¨åˆæˆè¡¨æƒ…ï¼Œå¹¶æ ¹æ®è§‚å¯Ÿåˆ°çš„è¡¨æƒ…é¢„æµ‹åŠ¨æ€è‚Œè‚‰æ´»åŠ¨ã€‚å› æ­¤ï¼ŒEIFERä¸ºé¢éƒ¨ç”µè‚Œå›¾ç ”ç©¶å¸¦æ¥äº†æ–°çš„èŒƒå¼ï¼Œå¯æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€é¢éƒ¨è®°å½•å½¢å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚Œè‚‰æ´»åŠ¨ä¸é¢éƒ¨è¡¨æƒ…çš„å…³ç³»åœ¨å¿ƒç†å­¦ã€åŒ»å­¦å’Œå¨±ä¹ç­‰é¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ç°æœ‰é¢éƒ¨åˆ†ææ–¹æ³•æ— æ³•å¤„ç†ç”µæé®æŒ¡é—®é¢˜ï¼Œå¯¼è‡´åˆ†æå¤±æ•ˆã€‚</li>
<li>EIFERæ–¹æ³•ç»“åˆ3DMMå’Œç¥ç»éé…å¯¹å›¾åƒç¿»è¯‘æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨sEMGé®æŒ¡çš„æƒ…å†µä¸‹æ¢å¤é¢éƒ¨å›¾åƒã€‚</li>
<li>EIFERæ–¹æ³•é€šè¿‡åŒå‘æ˜ å°„å­¦ä¹ è‚Œè‚‰æ´»åŠ¨ä¸é¢éƒ¨è¡¨æƒ…å‚æ•°ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚</li>
<li>å®éªŒéªŒè¯äº†EIFERæ–¹æ³•åœ¨å‡ ä½•å½¢çŠ¶å’Œå¤–è§‚é‡å»ºä¸Šçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>EIFERèƒ½åˆæˆåŸºäºè‚Œè‚‰æ´»åŠ¨çš„è¡¨æƒ…ï¼Œå¹¶é¢„æµ‹åŠ¨æ€è‚Œè‚‰æ´»åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09556">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5009481d1340ef62d7d77e3f26f54380.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd2877b8348ca4e8c3ba1857a48617fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44683405bf33b4a626f2dc9c4ab4543c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcdf6c36d9844406edea987945752a6e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CM-Diff-A-Single-Generative-Network-for-Bidirectional-Cross-Modality-Translation-Diffusion-Model-Between-Infrared-and-Visible-Images"><a href="#CM-Diff-A-Single-Generative-Network-for-Bidirectional-Cross-Modality-Translation-Diffusion-Model-Between-Infrared-and-Visible-Images" class="headerlink" title="CM-Diff: A Single Generative Network for Bidirectional Cross-Modality   Translation Diffusion Model Between Infrared and Visible Images"></a>CM-Diff: A Single Generative Network for Bidirectional Cross-Modality   Translation Diffusion Model Between Infrared and Visible Images</h2><p><strong>Authors:Bin Hu, Chenqiang Gao, Shurui Liu, Junjie Guo, Fang Chen, Fangcen Liu</strong></p>
<p>The image translation method represents a crucial approach for mitigating information deficiencies in the infrared and visible modalities, while also facilitating the enhancement of modality-specific datasets. However, existing methods for infrared and visible image translation either achieve unidirectional modality translation or rely on cycle consistency for bidirectional modality translation, which may result in suboptimal performance. In this work, we present the cross-modality translation diffusion model (CM-Diff) for simultaneously modeling data distributions in both the infrared and visible modalities. We address this challenge by combining translation direction labels for guidance during training with cross-modality feature control. Specifically, we view the establishment of the mapping relationship between the two modalities as the process of learning data distributions and understanding modality differences, achieved through a novel Bidirectional Diffusion Training (BDT) strategy. Additionally, we propose a Statistical Constraint Inference (SCI) strategy to ensure the generated image closely adheres to the data distribution of the target modality. Experimental results demonstrate the superiority of our CM-Diff over state-of-the-art methods, highlighting its potential for generating dual-modality datasets. </p>
<blockquote>
<p>å›¾åƒè½¬æ¢æ–¹æ³•å¯¹äºå¼¥è¡¥çº¢å¤–å’Œå¯è§å…‰æ¨¡å¼ä¸­çš„ä¿¡æ¯ç¼ºå¤±è‡³å…³é‡è¦ï¼ŒåŒæ—¶ä¹Ÿæœ‰åŠ©äºå¢å¼ºç‰¹å®šæ¨¡å¼çš„æ•°æ®é›†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„çº¢å¤–å’Œå¯è§å›¾åƒè½¬æ¢æ–¹æ³•è¦ä¹ˆå®ç°å•å‘æ¨¡å¼è½¬æ¢ï¼Œè¦ä¹ˆä¾èµ–äºå¾ªç¯ä¸€è‡´æ€§è¿›è¡ŒåŒå‘æ¨¡å¼è½¬æ¢ï¼Œè¿™å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨æ¨¡æ€è½¬æ¢æ‰©æ•£æ¨¡å‹ï¼ˆCM-Diffï¼‰ï¼Œä»¥åŒæ—¶æ¨¡æ‹Ÿçº¢å¤–å’Œå¯è§å…‰æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒã€‚æˆ‘ä»¬é€šè¿‡ç»“åˆè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¿»è¯‘æ–¹å‘æ ‡ç­¾è¿›è¡Œå¼•å¯¼ï¼Œä»¥åŠè·¨æ¨¡æ€ç‰¹å¾æ§åˆ¶æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å»ºç«‹ä¸¤ç§æ¨¡å¼ä¹‹é—´çš„æ˜ å°„å…³ç³»è§†ä¸ºå­¦ä¹ è¿‡ç¨‹æ•°æ®åˆ†å¸ƒå’Œç†è§£æ¨¡å¼å·®å¼‚çš„è¿‡ç¨‹ï¼Œé€šè¿‡ä¸€ç§æ–°çš„åŒå‘æ‰©æ•£è®­ç»ƒï¼ˆBDTï¼‰ç­–ç•¥æ¥å®ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿè®¡çº¦æŸæ¨ç†ï¼ˆSCIï¼‰ç­–ç•¥ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„å›¾åƒç´§å¯†ç¬¦åˆç›®æ ‡æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒã€‚å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„CM-Diffç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ï¼Œå‡¸æ˜¾äº†å…¶ç”ŸæˆåŒæ¨¡æ€æ•°æ®é›†çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09514v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€ç¿»è¯‘æ‰©æ•£æ¨¡å‹ï¼ˆCM-Diffï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤ŸåŒæ—¶å¯¹çº¢å¤–å’Œå¯è§æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œè§£å†³äº†ç°æœ‰å›¾åƒç¿»è¯‘æ–¹æ³•åœ¨çº¢å¤–å’Œå¯è§å…‰å›¾åƒç¿»è¯‘æ–¹é¢å­˜åœ¨çš„å•å‘æ¨¡æ€ç¿»è¯‘æˆ–ä¾èµ–äºå¾ªç¯ä¸€è‡´æ€§è¿›è¡ŒåŒå‘æ¨¡æ€ç¿»è¯‘çš„é—®é¢˜ã€‚è¯¥ç ”ç©¶é€šè¿‡ç»“åˆç¿»è¯‘æ–¹å‘æ ‡ç­¾è¿›è¡Œè®­ç»ƒæŒ‡å¯¼ä»¥åŠè·¨æ¨¡æ€ç‰¹å¾æ§åˆ¶æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚é€šè¿‡é‡‡ç”¨åŒå‘æ‰©æ•£è®­ç»ƒï¼ˆBDTï¼‰ç­–ç•¥ï¼Œå»ºç«‹ä¸¤ç§æ¨¡æ€ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œç†è§£å’Œåˆ©ç”¨æ¨¡æ€å·®å¼‚ã€‚åŒæ—¶ï¼Œæå‡ºç»Ÿè®¡çº¦æŸæ¨ç†ï¼ˆSCIï¼‰ç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒç´§å¯†ç¬¦åˆç›®æ ‡æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCM-Diffç›¸è¾ƒäºç°æœ‰å…ˆè¿›æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ï¼Œå…·æœ‰ç”ŸæˆåŒæ¨¡æ€æ•°æ®é›†çš„æ½œåŠ›ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è·¨æ¨¡æ€ç¿»è¯‘æ‰©æ•£æ¨¡å‹ï¼ˆCM-Diffï¼‰è§£å†³äº†çº¢å¤–å’Œå¯è§æ¨¡æ€ä¿¡æ¯ç¼ºå¤±çš„é—®é¢˜ï¼ŒåŒæ—¶å¢å¼ºäº†æ¨¡æ€ç‰¹å®šæ•°æ®é›†ã€‚</li>
<li>ç°æœ‰å›¾åƒç¿»è¯‘æ–¹æ³•å­˜åœ¨å•å‘æˆ–ä¾èµ–äºå¾ªç¯ä¸€è‡´æ€§çš„åŒå‘æ¨¡æ€ç¿»è¯‘çš„é—®é¢˜ï¼Œè€ŒCM-Diffèƒ½åŒæ—¶è¿›è¡ŒåŒå‘ç¿»è¯‘ã€‚</li>
<li>CM-Diffé€šè¿‡ç»“åˆç¿»è¯‘æ–¹å‘æ ‡ç­¾è¿›è¡Œè®­ç»ƒæŒ‡å¯¼ä»¥åŠè·¨æ¨¡æ€ç‰¹å¾æ§åˆ¶æ¥è§£å†³æŒ‘æˆ˜ã€‚</li>
<li>é‡‡ç”¨åŒå‘æ‰©æ•£è®­ç»ƒï¼ˆBDTï¼‰ç­–ç•¥å»ºç«‹ä¸¤ç§æ¨¡æ€çš„æ˜ å°„å…³ç³»ï¼Œç†è§£å’Œåˆ©ç”¨æ¨¡æ€å·®å¼‚ã€‚</li>
<li>CM-Diffé‡‡ç”¨ç»Ÿè®¡çº¦æŸæ¨ç†ï¼ˆSCIï¼‰ç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒç¬¦åˆç›®æ ‡æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒã€‚</li>
<li>å®éªŒè¯æ˜CM-Diffåœ¨ç”ŸæˆåŒæ¨¡æ€æ•°æ®é›†æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡ç°æœ‰æ–¹æ³•ã€‚</li>
<li>CM-Diffçš„æ½œåœ¨åº”ç”¨åŒ…æ‹¬æ”¹å–„æ¨¡æ€è½¬æ¢çš„æ€§èƒ½ã€æ‰©å……æ•°æ®é›†ã€å¢å¼ºè·¨æ¨¡æ€é€šä¿¡ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-99d504687f34b48899dcc9ca73832461.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21315c90871e6474222bf15aab7b3802.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Alias-Free-Latent-Diffusion-Models-Improving-Fractional-Shift-Equivariance-of-Diffusion-Latent-Space"><a href="#Alias-Free-Latent-Diffusion-Models-Improving-Fractional-Shift-Equivariance-of-Diffusion-Latent-Space" class="headerlink" title="Alias-Free Latent Diffusion Models:Improving Fractional Shift   Equivariance of Diffusion Latent Space"></a>Alias-Free Latent Diffusion Models:Improving Fractional Shift   Equivariance of Diffusion Latent Space</h2><p><strong>Authors:Yifan Zhou, Zeqi Xiao, Shuai Yang, Xingang Pan</strong></p>
<p>Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) self-attention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/SingleZombie/AFLDM">https://github.com/SingleZombie/AFLDM</a> </p>
<blockquote>
<p>æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰çš„ç”Ÿæˆè¿‡ç¨‹ä¸ç¨³å®šï¼Œå³ä½¿è¾“å…¥å™ªå£°å‡ºç°å¾®å°çš„æ‰°åŠ¨æˆ–å˜åŒ–ï¼Œä¹Ÿå¯èƒ½å¯¼è‡´è¾“å‡ºç»“æœæ˜¾è‘—ä¸åŒã€‚è¿™é˜»ç¢äº†å®ƒä»¬åœ¨éœ€è¦ä¸€è‡´ç»“æœçš„åº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿æ¨¡å‹å…·æœ‰å¹³ç§»ç­‰å˜æ€§æ¥é‡æ–°è®¾è®¡LDMsï¼Œä»¥å¢å¼ºå…¶ä¸€è‡´æ€§ã€‚è™½ç„¶å¼•å…¥æŠ—æ··å æ“ä½œå¯ä»¥éƒ¨åˆ†æé«˜å¹³ç§»ç­‰å˜æ€§ï¼Œä½†ç”±äºLDMä¸­ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œä»ç„¶å­˜åœ¨ä¸¥é‡çš„æ··å å’Œä¸ä¸€è‡´æ€§ï¼ŒåŒ…æ‹¬1ï¼‰åœ¨VAEè®­ç»ƒæœŸé—´å’Œå¤šä¸ªU-Netæ¨æ–­æœŸé—´çš„æ··å æ”¾å¤§ï¼Œä»¥åŠ2ï¼‰æœ¬è´¨ä¸Šç¼ºä¹å¹³ç§»ç­‰å˜æ€§çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é‡æ–°è®¾è®¡äº†æ³¨æ„åŠ›æ¨¡å—ä»¥å®ç°å¹³ç§»ç­‰å˜æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç­‰å˜æ€§æŸå¤±ï¼Œæœ‰æ•ˆåœ°æŠ‘åˆ¶äº†è¿ç»­åŸŸä¸­ç‰¹å¾é¢‘ç‡å¸¦å®½ã€‚ç”±æ­¤äº§ç”Ÿçš„æ— æ··å LDMï¼ˆAF-LDMï¼‰å®ç°äº†å¼ºå¤§çš„å¹³ç§»ç­‰å˜æ€§ï¼Œå¹¶ä¸”å¯¹ä¸è§„åˆ™æ‰­æ›²ä¹Ÿå…·æœ‰é²æ£’æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAF-LDMåœ¨å„ç§åº”ç”¨ä¸­äº§ç”Ÿçš„ç»“æœæ¯”åŸå§‹LDMæ›´åŠ ä¸€è‡´ï¼ŒåŒ…æ‹¬è§†é¢‘ç¼–è¾‘å’Œå›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ã€‚ä»£ç å¯åœ¨æ­¤å¤„æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/AFLDM">https://github.com/SingleZombie/AFLDM</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09419v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šæ€§é—®é¢˜ï¼Œå¹¶é’ˆå¯¹è¯¥é—®é¢˜æå‡ºäº†ä¸€ç§æ”¹è¿›æ–¹æ³•ï¼Œå³é€šè¿‡è®¾è®¡åˆ«åå…è´¹æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆAF-LDMï¼‰æ¥å®ç°æ›´å¼ºçš„ç­‰å˜æ€§ã€‚ä¸ºäº†è§£å†³LDMé¢ä¸´çš„ç‰¹å®šæŒ‘æˆ˜ï¼Œå¦‚å˜åˆ†è‡ªç¼–ç å™¨è®­ç»ƒä¸­çš„åˆ«åæ”¾å¤§å’Œå¤šU-Netæ¨ç†é—®é¢˜ï¼Œä»¥åŠè‡ªæ³¨æ„åŠ›æ¨¡å—æœ¬èº«ç¼ºä¹ç­‰å˜æ€§ï¼Œè¯¥ç ”ç©¶é‡æ–°è®¾è®¡äº†æ³¨æ„åŠ›æ¨¡å—å¹¶æå‡ºäº†ç­‰å˜æ€§æŸå¤±æ¥æŠ‘åˆ¶è¿ç»­åŸŸä¸­çš„ç‰¹å¾é¢‘ç‡å¸¦å®½ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒAF-LDMåœ¨å„ç§åº”ç”¨ä¸­è¡¨ç°ä¼˜äºåŸå§‹LDMï¼Œå®ç°äº†æ›´ä¸€è‡´çš„ç”Ÿæˆç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDMså­˜åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸ç¨³å®šçš„é—®é¢˜ï¼Œå¯¼è‡´è¾“å…¥å™ªå£°çš„å°æ‰°åŠ¨æˆ–å˜åŒ–å¯èƒ½å¯¼è‡´æ˜¾è‘—ä¸åŒçš„è¾“å‡ºã€‚</li>
<li>ä¸ºæé«˜ä¸€è‡´æ€§ï¼Œç ”ç©¶è€…æå‡ºäº†AF-LDMæ¨¡å‹ä»¥å¢å¼ºå…¶ç­‰å˜æ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥æŠ—æ··å æ“ä½œï¼ŒAF-LDMèƒ½å¤Ÿéƒ¨åˆ†æ”¹å–„ç­‰å˜æ€§ã€‚ç„¶è€Œï¼Œç”±äºLDMé¢ä¸´çš„ç‰¹å®šæŒ‘æˆ˜ï¼Œä»å­˜åœ¨æ˜¾è‘—çš„æ··å å’Œä¸ä¸€è‡´æ€§ã€‚</li>
<li>LDMçš„æŒ‘æˆ˜åŒ…æ‹¬å˜åˆ†è‡ªç¼–ç å™¨è®­ç»ƒä¸­çš„åˆ«åæ”¾å¤§é—®é¢˜ä»¥åŠå¤šä¸ªU-Netæ¨ç†è¿‡ç¨‹ä¸­å‡ºç°çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œå…¶è‡ªæ³¨æ„åŠ›æ¨¡å—æœ¬è´¨ä¸Šç¼ºä¹ç­‰å˜æ€§ä¹Ÿæ˜¯é—®é¢˜æ‰€åœ¨ã€‚å¯¹æ­¤æå‡ºäº†ä¸¤ç§è§£å†³æ–¹æ¡ˆï¼šé‡æ–°è®¾è®¡æ³¨æ„åŠ›æ¨¡å—ä»¥å®ç°ç­‰å˜æ€§ï¼Œå¹¶å¼•å…¥ç­‰å˜æ€§æŸå¤±æ¥æŠ‘åˆ¶ç‰¹å¾é¢‘ç‡å¸¦å®½ã€‚è¿™äº›æªæ–½æ—¨åœ¨æ¶ˆé™¤åˆ«åå½±å“å¹¶æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b1012620b0eba499948e53db16ffe2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a671201e0c757dbebc4713a57d2bcda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-898d72dd75d4a3c968cf26cef050c908.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4598a30be8cdac79f2877ee3991ae5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6997a1ba7f8d12f60da090caead5a444.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Reangle-A-Video-4D-Video-Generation-as-Video-to-Video-Translation"><a href="#Reangle-A-Video-4D-Video-Generation-as-Video-to-Video-Translation" class="headerlink" title="Reangle-A-Video: 4D Video Generation as Video-to-Video Translation"></a>Reangle-A-Video: 4D Video Generation as Video-to-Video Translation</h2><p><strong>Authors:Hyeonho Jeong, Suhyeon Lee, Jong Chul Ye</strong></p>
<p>We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: <a target="_blank" rel="noopener" href="https://hyeonho99.github.io/reangle-a-video/">https://hyeonho99.github.io/reangle-a-video/</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»Reangle-A-Videoï¼Œè¿™æ˜¯ä¸€ä¸ªä»å•ä¸ªè¾“å…¥è§†é¢‘ç”ŸæˆåŒæ­¥å¤šè§†è§’è§†é¢‘çš„ç»Ÿä¸€æ¡†æ¶ã€‚ä¸åŒäºä¸»æµæ–¹æ³•åœ¨å¤§å‹4Dæ•°æ®é›†ä¸Šè®­ç»ƒå¤šè§†è§’è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†å¤šè§†è§’è§†é¢‘ç”Ÿæˆä»»åŠ¡é‡æ–°æ„å»ºä¸ºè§†é¢‘åˆ°è§†é¢‘çš„ç¿»è¯‘ï¼Œåˆ©ç”¨å…¬å¼€å¯ç”¨çš„å›¾åƒå’Œè§†é¢‘æ‰©æ•£å…ˆéªŒã€‚æœ¬è´¨ä¸Šï¼ŒReangle-A-Videoåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚ï¼ˆ1ï¼‰å¤šè§†è§’è¿åŠ¨å­¦ä¹ ï¼šä»¥è‡ªç›‘ç£çš„æ–¹å¼åŒæ­¥å¾®è°ƒå›¾åƒåˆ°è§†é¢‘æ‰©æ•£è½¬æ¢å™¨ï¼Œä»ä¸€ç»„æ‰­æ›²çš„è§†é¢‘ä¸­æç‚¼å‡ºè§†è§’ä¸å˜çš„è¿åŠ¨ã€‚ï¼ˆ2ï¼‰å¤šè§†è§’ä¸€è‡´å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ï¼šè¾“å…¥è§†é¢‘çš„ç¬¬ä¸€å¸§åœ¨æ¨ç†æ—¶é—´è·¨è§†è§’ä¸€è‡´æ€§æŒ‡å¯¼ä¸‹æ‰­æ›²å’Œå¡«å……ï¼Œç”Ÿæˆå¤šè§†è§’ä¸€è‡´çš„å¼€å§‹å›¾åƒã€‚å…³äºé™æ€è§†è§’ä¼ è¾“å’ŒåŠ¨æ€æ‘„åƒæœºæ§åˆ¶çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒReangle-A-Videoè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºå¤šè§†è§’è§†é¢‘ç”Ÿæˆå»ºç«‹äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://hyeonho99.github.io/reangle-a-video/">https://hyeonho99.github.io/reangle-a-video/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09151v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://hyeonho99.github.io/reangle-a-video/">https://hyeonho99.github.io/reangle-a-video/</a></p>
<p><strong>Summary</strong></p>
<p>Reangle-A-Videoæ¡†æ¶èƒ½å¤Ÿä»å•ä¸€è¾“å…¥è§†é¢‘ç”ŸæˆåŒæ­¥å¤šè§†è§’è§†é¢‘ã€‚å®ƒé‡‡ç”¨è§†é¢‘åˆ°è§†é¢‘çš„ç¿»è¯‘æ–¹å¼ï¼Œåˆ©ç”¨å…¬å¼€å¯ç”¨çš„å›¾åƒå’Œè§†é¢‘æ‰©æ•£å…ˆéªŒçŸ¥è¯†ï¼Œä¸åŒäºä¸»æµåœ¨å¤§å‹4Dæ•°æ®é›†ä¸Šè®­ç»ƒå¤šè§†è§’è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ã€‚Reangle-A-Videoåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå¤šè§†è§’è¿åŠ¨å­¦ä¹ å’Œå¤šè§†è§’ä¸€è‡´å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ã€‚é€šè¿‡åŒæ­¥è‡ªç›‘ç£æ–¹å¼å¾®è°ƒå›¾åƒåˆ°è§†é¢‘æ‰©æ•£è½¬æ¢å™¨ï¼Œä»ä¸€ç»„å˜å½¢è§†é¢‘ä¸­æå–è§†å›¾ä¸å˜çš„è¿åŠ¨ã€‚åœ¨è¾“å…¥è§†é¢‘çš„ç¬¬ä¸€å¸§ä¸­ï¼Œé‡‡ç”¨è·¨è§†è§’ä¸€è‡´æ€§æŒ‡å¯¼ç”Ÿæˆå¤šè§†è§’ä¸€è‡´èµ·å§‹å›¾åƒã€‚å®éªŒè¡¨æ˜ï¼ŒReangle-A-Videoåœ¨é™æ€è§†è§’è½¬æ¢å’ŒåŠ¨æ€æ‘„åƒæœºæ§åˆ¶ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºå¤šè§†è§’è§†é¢‘ç”Ÿæˆæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Reangle-A-Videoæ˜¯ä¸€ä¸ªä»å•ä¸€è¾“å…¥è§†é¢‘ç”ŸæˆåŒæ­¥å¤šè§†è§’è§†é¢‘çš„æ¡†æ¶ã€‚</li>
<li>å®ƒé‡‡ç”¨è§†é¢‘åˆ°è§†é¢‘çš„ç¿»è¯‘æ–¹å¼ï¼Œåˆ©ç”¨å›¾åƒå’Œè§†é¢‘æ‰©æ•£å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œï¼šå¤šè§†è§’è¿åŠ¨å­¦ä¹ å’Œå¤šè§†è§’ä¸€è‡´å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ã€‚</li>
<li>é€šè¿‡åŒæ­¥è‡ªç›‘ç£æ–¹å¼æå–è§†å›¾ä¸å˜çš„è¿åŠ¨ã€‚</li>
<li>ç”Ÿæˆå¤šè§†è§’ä¸€è‡´èµ·å§‹å›¾åƒæ—¶é‡‡ç”¨äº†è·¨è§†è§’ä¸€è‡´æ€§æŒ‡å¯¼ã€‚</li>
<li>Reangle-A-Videoåœ¨é™æ€å’ŒåŠ¨æ€è§†è§’è½¬æ¢ä¸Šè¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ–¹æ³•çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09151">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-170cd7be7f5ed78ae237c5de7aecd105.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f78a1bb6134454b0aa5d26d13966d989.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9c42602c21b81f4a9b9c1e75bf0491a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6b7950510b9a10e64706e857d8e34f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cc5663c575eb9a60228af9158e38ef4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Are-ECGs-enough-Deep-learning-classification-of-cardiac-anomalies-using-only-electrocardiograms"><a href="#Are-ECGs-enough-Deep-learning-classification-of-cardiac-anomalies-using-only-electrocardiograms" class="headerlink" title="Are ECGs enough? Deep learning classification of cardiac anomalies using   only electrocardiograms"></a>Are ECGs enough? Deep learning classification of cardiac anomalies using   only electrocardiograms</h2><p><strong>Authors:Joao D. S. Marques, Arlindo L. Oliveira</strong></p>
<p>Electrocardiography (ECG) is an essential tool for diagnosing multiple cardiac anomalies: it provides valuable clinical insights, while being affordable, fast and available in many settings. However, in the current literature, the role of ECG analysis is often unclear: many approaches either rely on additional imaging modalities, such as Computed Tomography Pulmonary Angiography (CTPA), which may not always be available, or do not effectively generalize across different classification problems. Furthermore, the availability of public ECG datasets is limited and, in practice, these datasets tend to be small, making it essential to optimize learning strategies. In this study, we investigate the performance of multiple neural network architectures in order to assess the impact of various approaches. Moreover, we check whether these practices enhance model generalization when transfer learning is used to translate information learned in larger ECG datasets, such as PTB-XL and CPSC18, to a smaller, more challenging dataset for pulmonary embolism (PE) detection. By leveraging transfer learning, we analyze the extent to which we can improve learning efficiency and predictive performance on limited data. Code available at <a target="_blank" rel="noopener" href="https://github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers">https://github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers</a> . </p>
<blockquote>
<p>å¿ƒç”µå›¾ï¼ˆECGï¼‰æ˜¯è¯Šæ–­å¤šç§å¿ƒè„å¼‚å¸¸çš„é‡è¦å·¥å…·ï¼šå®ƒæä¾›äº†å®è´µçš„ä¸´åºŠè§è§£ï¼ŒåŒæ—¶ç»æµå®æƒ ã€å¿«é€Ÿï¼Œå¹¶åœ¨è®¸å¤šç¯å¢ƒä¸­éƒ½å¯ä½¿ç”¨ã€‚ç„¶è€Œï¼Œåœ¨ç›®å‰çš„æ–‡çŒ®ä¸­ï¼Œå¿ƒç”µå›¾åˆ†æçš„ä½œç”¨å¾€å¾€ä¸æ˜ç¡®ï¼šè®¸å¤šæ–¹æ³•è¦ä¹ˆä¾èµ–äºå¯èƒ½å¹¶ä¸å¯ç”¨çš„å…¶ä»–æˆåƒæ¨¡å¼ï¼Œä¾‹å¦‚è®¡ç®—æœºæ–­å±‚æ‰«æè‚ºåŠ¨è„‰é€ å½±æœ¯ï¼ˆCTPAï¼‰ï¼Œè¦ä¹ˆåœ¨ä¸åŒçš„åˆ†ç±»é—®é¢˜ä¸­ä¸èƒ½æœ‰æ•ˆåœ°é€šç”¨ã€‚æ­¤å¤–ï¼Œå…¬å…±å¿ƒç”µå›¾æ•°æ®é›†çš„å¯è·å¾—æ€§æœ‰é™ï¼Œå®é™…ä¸Šè¿™äº›æ•°æ®é›†å¾€å¾€å¾ˆå°ï¼Œå› æ­¤å¿…é¡»ä¼˜åŒ–å­¦ä¹ ç­–ç•¥ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†å¤šç§ç¥ç»ç½‘ç»œæ¶æ„çš„æ€§èƒ½ï¼Œä»¥è¯„ä¼°å„ç§æ–¹æ³•çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ£€æŸ¥äº†å½“ä½¿ç”¨è¿ç§»å­¦ä¹ å°†åœ¨å¤§è§„æ¨¡å¿ƒç”µå›¾æ•°æ®é›†ä¸­å­¦ä¹ åˆ°çš„ä¿¡æ¯ï¼ˆå¦‚PTB-XLå’ŒCPSC18ï¼‰è¿ç§»åˆ°è¾ƒå°ä½†æ›´å…·æŒ‘æˆ˜æ€§çš„è‚ºæ “å¡ï¼ˆPEï¼‰æ£€æµ‹æ•°æ®é›†æ—¶ï¼Œè¿™äº›å®è·µæ˜¯å¦æé«˜äº†æ¨¡å‹çš„é€šç”¨æ€§ã€‚é€šè¿‡åˆ©ç”¨è¿ç§»å­¦ä¹ ï¼Œæˆ‘ä»¬åˆ†æäº†åœ¨æœ‰é™æ•°æ®ä¸Šï¼Œæˆ‘ä»¬èƒ½æé«˜å¤šå°‘å­¦ä¹ æ•ˆç‡å’Œé¢„æµ‹æ€§èƒ½ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers%EF%BC%88%E4%BB%A5%E4%BD%BF%E7%9C%8B%E5%8F%A4%EF%BC%89">https://github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiersã€‚ï¼ˆä»¥ä¾›å‚è€ƒï¼‰</a>ï¼ˆè¯·ä»¥å®é™…ç½‘ç«™å†…å®¹ä¸ºå‡†ï¼‰</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08960v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¿ƒç”µå›¾ï¼ˆECGï¼‰æ˜¯è¯Šæ–­å¤šç§å¿ƒè„å¼‚å¸¸çš„é‡è¦å·¥å…·ï¼Œå…·æœ‰ç»æµã€å¿«é€Ÿã€å¯åœ¨å¤šç§ç¯å¢ƒä¸­ä½¿ç”¨çš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œç°æœ‰æ–‡çŒ®ä¸­å…³äºå¿ƒç”µå›¾åˆ†æçš„è§’è‰²å¾€å¾€ä¸æ˜ç¡®ï¼Œè®¸å¤šæ–¹æ³•ä¾èµ–äºå¯èƒ½æ— æ³•å§‹ç»ˆè·å¾—çš„é™„åŠ æˆåƒæ¨¡å¼ï¼Œå¦‚è®¡ç®—æœºæ–­å±‚æ‰«æè‚ºåŠ¨è„‰é€ å½±æœ¯ï¼ˆCTPAï¼‰ï¼Œæˆ–è€…ä¸èƒ½æœ‰æ•ˆåœ°åœ¨ä¸åŒåˆ†ç±»é—®é¢˜ä¸­æ¨å¹¿ã€‚æ­¤å¤–ï¼Œå…¬å¼€çš„å¿ƒç”µå›¾æ•°æ®é›†æœ‰é™ï¼Œä¸”åœ¨å®è·µä¸­è¿™äº›æ•°æ®é›†å¾€å¾€å¾ˆå°ï¼Œå› æ­¤éœ€è¦ä¼˜åŒ–å­¦ä¹ ç­–ç•¥ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°å¤šç§ç¥ç»ç½‘ç»œæ¶æ„çš„æ€§èƒ½ï¼Œå¹¶æ£€æŸ¥è¿™äº›å®è·µåœ¨æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨è¿ç§»å­¦ä¹ å°†åœ¨å¤§è§„æ¨¡å¿ƒç”µå›¾æ•°æ®é›†ä¸­å­¦ä¹ çš„ä¿¡æ¯è½¬ç§»åˆ°è¾ƒå°çš„ã€æ›´å…·æŒ‘æˆ˜æ€§çš„è‚ºæ “å¡æ£€æµ‹æ•°æ®é›†æ—¶ã€‚é€šè¿‡åˆ©ç”¨è¿ç§»å­¦ä¹ ï¼Œæˆ‘ä»¬åˆ†æäº†åœ¨æœ‰é™æ•°æ®ä¸Šæé«˜å­¦ä¹ æ•ˆç‡å’Œé¢„æµ‹æ€§èƒ½çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¿ƒç”µå›¾æ˜¯è¯Šæ–­å¿ƒè„å¼‚å¸¸çš„é‡è¦å·¥å…·ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å’Œç»æµæ€§ä¼˜åŠ¿ã€‚</li>
<li>å½“å‰æ–‡çŒ®ä¸­å¿ƒç”µå›¾åˆ†æçš„è§’è‰²å­˜åœ¨ä¸æ˜ç¡®æ€§ï¼Œè®¸å¤šæ–¹æ³•ä¾èµ–äºå…¶ä»–æˆåƒæ¨¡å¼æˆ–éš¾ä»¥åœ¨ä¸åŒåˆ†ç±»é—®é¢˜ä¸­æ¨å¹¿ã€‚</li>
<li>å…¬å…±å¿ƒç”µå›¾æ•°æ®é›†æœ‰é™ä¸”å®é™…åº”ç”¨ä¸­å¾€å¾€è§„æ¨¡è¾ƒå°ï¼Œéœ€è¦ä¼˜åŒ–å­¦ä¹ ç­–ç•¥ã€‚</li>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†å¤šç§ç¥ç»ç½‘ç»œæ¶æ„çš„æ€§èƒ½ä»¥æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶é‡ç‚¹ä¹‹ä¸€æ˜¯ä½¿ç”¨è¿ç§»å­¦ä¹ åœ¨å¤§è§„æ¨¡å¿ƒç”µå›¾æ•°æ®é›†ä¸­å­¦ä¹ çš„ä¿¡æ¯è½¬ç§»åˆ°å°è§„æ¨¡æ•°æ®é›†ä¸Šçš„èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨è¿ç§»å­¦ä¹ å¯æ”¹å–„æœ‰é™æ•°æ®ä¸Šçš„å­¦ä¹ æ•ˆç‡å’Œé¢„æµ‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08960">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7533ed2e67bbfcebeeecbe6d61033031.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beae897000d09d52e37d256426cefe0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99994b4b274486ef7f2fe93fa38211ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b66a0d46ce613c790a74ade10b04846.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Versatile-Multimodal-Controls-for-Whole-Body-Talking-Human-Animation"><a href="#Versatile-Multimodal-Controls-for-Whole-Body-Talking-Human-Animation" class="headerlink" title="Versatile Multimodal Controls for Whole-Body Talking Human Animation"></a>Versatile Multimodal Controls for Whole-Body Talking Human Animation</h2><p><strong>Authors:Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Zixin Zhu, Minghui Yang, Ming Yang, Le Wang</strong></p>
<p>Human animation from a single reference image shall be flexible to synthesize whole-body motion for either a headshot or whole-body portrait, where the motions are readily controlled by audio signal and text prompts. This is hard for most existing methods as they only support producing pre-specified head or half-body motion aligned with audio inputs. In this paper, we propose a versatile human animation method, i.e., VersaAnimator, which generates whole-body talking human from arbitrary portrait images, not only driven by audio signal but also flexibly controlled by text prompts. Specifically, we design a text-controlled, audio-driven motion generator that produces whole-body motion representations in 3D synchronized with audio inputs while following textual motion descriptions. To promote natural smooth motion, we propose a code-pose translation module to link VAE codebooks with 2D DWposes extracted from template videos. Moreover, we introduce a multi-modal video diffusion that generates photorealistic human animation from a reference image according to both audio inputs and whole-body motion representations. Extensive experiments show that VersaAnimator outperforms existing methods in visual quality, identity preservation, and audio-lip synchronization. </p>
<blockquote>
<p>ä»å•ä¸€å‚è€ƒå›¾åƒè¿›è¡Œçš„äººè„¸åŠ¨ç”»åº”è¯¥èƒ½å¤Ÿçµæ´»åœ°åˆæˆå¤´éƒ¨ç‰¹å†™æˆ–å…¨èº«è‚–åƒçš„æ•´ä½“è¿åŠ¨ï¼Œè¿™äº›è¿åŠ¨å¯ä»¥ç”±éŸ³é¢‘ä¿¡å·å’Œæ–‡å­—æç¤ºè½»æ¾æ§åˆ¶ã€‚è¿™å¯¹å¤§å¤šæ•°ç°æœ‰æ–¹æ³•æ¥è¯´æ˜¯éå¸¸å›°éš¾çš„ï¼Œå› ä¸ºå®ƒä»¬ä»…æ”¯æŒç”Ÿæˆä¸éŸ³é¢‘è¾“å…¥å¯¹é½çš„é¢„è®¾å¤´éƒ¨æˆ–åŠèº«è¿åŠ¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„äººè„¸åŠ¨ç”»æ–¹æ³•ï¼Œå³VersaAnimatorï¼Œå®ƒå¯ä»¥ä»ä»»æ„çš„è‚–åƒå›¾åƒç”Ÿæˆå…¨èº«è¯´è¯çš„äººè„¸åŠ¨ç”»ï¼Œä¸ä»…ç”±éŸ³é¢‘ä¿¡å·é©±åŠ¨ï¼Œè¿˜å¯ä»¥çµæ´»åœ°ç”±æ–‡å­—æç¤ºæ§åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ–‡æœ¬æ§åˆ¶çš„ã€éŸ³é¢‘é©±åŠ¨çš„è¿åŠ¨ç”Ÿæˆå™¨ï¼Œè¯¥ç”Ÿæˆå™¨èƒ½å¤Ÿäº§ç”Ÿä¸éŸ³é¢‘è¾“å…¥åŒæ­¥çš„å…¨èº«è¿åŠ¨è¡¨ç¤ºï¼ˆåœ¨3Dä¸­ï¼‰ï¼ŒåŒæ—¶éµå¾ªæ–‡æœ¬è¿åŠ¨æè¿°ã€‚ä¸ºäº†ä¿ƒè¿›è‡ªç„¶æµç•…çš„è¿åŠ¨ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç¼–ç å§¿æ€è½¬æ¢æ¨¡å—ï¼Œç”¨äºå°†VAEä»£ç æœ¬ä¸ä»æ¨¡æ¿è§†é¢‘ä¸­æå–çš„2DDWå§¿æ€è”ç³»èµ·æ¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šæ¨¡æ€è§†é¢‘æ‰©æ•£æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯ä»¥æ ¹æ®å‚è€ƒå›¾åƒã€éŸ³é¢‘è¾“å…¥å’Œå…¨èº«è¿åŠ¨è¡¨ç¤ºç”Ÿæˆé€¼çœŸçš„äººè„¸åŠ¨ç”»ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVersaAnimatoråœ¨è§†è§‰è´¨é‡ã€èº«ä»½ä¿ç•™å’ŒéŸ³é¢‘ä¸å˜´å”‡åŒæ­¥æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08714v1">PDF</a> </p>
<p><strong>Summary</strong><br>å•å‚è€ƒå›¾åƒçš„äººåƒåŠ¨ç”»å¯ä»¥çµæ´»åˆæˆå…¨èº«åŠ¨ä½œï¼Œæ— è®ºæ˜¯å¤´åƒè¿˜æ˜¯å…¨èº«è‚–åƒï¼ŒåŠ¨ä½œéƒ½å¯ä»¥é€šè¿‡éŸ³é¢‘ä¿¡å·å’Œæ–‡å­—æç¤ºè½»æ¾æ§åˆ¶ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»…æ”¯æŒç”Ÿæˆä¸éŸ³é¢‘è¾“å…¥å¯¹é½çš„é¢„è®¾å¤´éƒ¨æˆ–åŠèº«åŠ¨ä½œï¼Œéš¾ä»¥å®ç°å…¨èº«åŠ¨ä½œçš„çµæ´»æ§åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„äººåƒåŠ¨ç”»æ–¹æ³•VersaAnimatorï¼Œå¯ä»¥ä»ä»»æ„è‚–åƒå›¾åƒç”Ÿæˆå…¨èº«åŠ¨ä½œï¼Œä¸ä»…ç”±éŸ³é¢‘ä¿¡å·é©±åŠ¨ï¼Œè¿˜èƒ½é€šè¿‡æ–‡å­—æç¤ºçµæ´»æ§åˆ¶ã€‚æ­¤æ–¹æ³•ç»“åˆäº†æ–‡æœ¬æ§åˆ¶ã€éŸ³é¢‘é©±åŠ¨çš„è¿åŠ¨ç”Ÿæˆå™¨ï¼Œäº§ç”Ÿä¸éŸ³é¢‘è¾“å…¥åŒæ­¥çš„å…¨èº«åŠ¨ä½œè¡¨ç¤ºï¼Œå¹¶é€šè¿‡æ¨¡æ¿è§†é¢‘æå–çš„DWposesä¸VAEä»£ç æœ¬çš„è½¬æ¢æ¨¡å—ï¼Œå®ç°è‡ªç„¶æµç•…çš„åŠ¨ä½œç”Ÿæˆã€‚åŒæ—¶å¼•å…¥äº†å¤šæ¨¡æ€è§†é¢‘æ‰©æ•£æŠ€æœ¯ï¼Œæ ¹æ®å‚è€ƒå›¾åƒã€éŸ³é¢‘è¾“å…¥å’Œå…¨èº«åŠ¨ä½œè¡¨ç¤ºç”Ÿæˆé€¼çœŸçš„åŠ¨ç”»ã€‚å®éªŒè¡¨æ˜ï¼ŒVersaAnimatoråœ¨è§†è§‰è´¨é‡ã€èº«ä»½ä¿ç•™å’ŒéŸ³é¢‘å”‡åŒæ­¥æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººåƒåŠ¨ç”»å¯ä»¥ä»å•å‚è€ƒå›¾åƒç”Ÿæˆå…¨èº«åŠ¨ä½œï¼ŒåŒ…æ‹¬å¤´éƒ¨å’Œå…¨èº«è‚–åƒã€‚</li>
<li>åŠ¨ä½œå¯ä»¥é€šè¿‡éŸ³é¢‘ä¿¡å·å’Œæ–‡å­—æç¤ºçµæ´»æ§åˆ¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä»…æ”¯æŒé¢„è®¾çš„å¤´éƒ¨æˆ–åŠèº«åŠ¨ä½œä¸éŸ³é¢‘è¾“å…¥å¯¹é½ï¼Œéš¾ä»¥å®ç°å…¨èº«åŠ¨ä½œçš„çµæ´»æ§åˆ¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€šç”¨çš„äººåƒåŠ¨ç”»æ–¹æ³•VersaAnimatorï¼Œèƒ½ç”Ÿæˆä¸éŸ³é¢‘è¾“å…¥åŒæ­¥çš„å…¨èº«åŠ¨ä½œè¡¨ç¤ºã€‚</li>
<li>é€šè¿‡æ¨¡æ¿è§†é¢‘æå–çš„DWposesä¸VAEä»£ç æœ¬çš„è½¬æ¢æ¨¡å—å®ç°è‡ªç„¶æµç•…çš„åŠ¨ä½œç”Ÿæˆã€‚</li>
<li>å¼•å…¥äº†å¤šæ¨¡æ€è§†é¢‘æ‰©æ•£æŠ€æœ¯ï¼Œæé«˜äº†åŠ¨ç”»çš„é€¼çœŸåº¦ã€‚</li>
<li>VersaAnimatoråœ¨è§†è§‰è´¨é‡ã€èº«ä»½ä¿ç•™å’ŒéŸ³é¢‘å”‡åŒæ­¥æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-544e44d51db45d3e5459d544cf44575e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cadea61e4907cd6a69432076455f3d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b111d2208a9e49f013d5550bffdaa200.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28fcbcf16b6a3381317a45ec3ee5082f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dfa8bd672ff361a77bf3c1459b8296e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Segmentation-Guided-CT-Synthesis-with-Pixel-Wise-Conformal-Uncertainty-Bounds"><a href="#Segmentation-Guided-CT-Synthesis-with-Pixel-Wise-Conformal-Uncertainty-Bounds" class="headerlink" title="Segmentation-Guided CT Synthesis with Pixel-Wise Conformal Uncertainty   Bounds"></a>Segmentation-Guided CT Synthesis with Pixel-Wise Conformal Uncertainty   Bounds</h2><p><strong>Authors:David Vallmanya Poch, Yorick Estievenart, Elnura Zhalieva, Sukanya Patra, Mohammad Yaqub, Souhaib Ben Taieb</strong></p>
<p>Accurate dose calculations in proton therapy rely on high-quality CT images. While planning CTs (pCTs) serve as a reference for dosimetric planning, Cone Beam CT (CBCT) is used throughout Adaptive Radiotherapy (ART) to generate sCTs for improved dose calculations. Despite its lower cost and reduced radiation exposure advantages, CBCT suffers from severe artefacts and poor image quality, making it unsuitable for precise dosimetry. Deep learning-based CBCT-to-CT translation has emerged as a promising approach. Still, existing methods often introduce anatomical inconsistencies and lack reliable uncertainty estimates, limiting their clinical adoption. To bridge this gap, we propose STF-RUE, a novel framework integrating two key components. First, STF, a segmentation-guided CBCT-to-CT translation method that enhances anatomical consistency by leveraging segmentation priors extracted from pCTs. Second, RUE, a conformal prediction method that augments predicted CTs with pixel-wise conformal prediction intervals, providing clinicians with robust reliability indicator. Comprehensive experiments using UNet++ and Fast-DDPM on two benchmark datasets demonstrate that STF-RUE significantly improves translation accuracy, as measured by a novel soft-tissue-focused metric designed for precise dose computation. Additionally, STF-RUE provides better-calibrated uncertainty sets for synthetic CT, reinforcing trust in synthetic CTs. By addressing both anatomical fidelity and uncertainty quantification, STF-RUE marks a crucial step toward safer and more effective adaptive proton therapy. Code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/cbct2ct_translation-B2D9/">https://anonymous.4open.science/r/cbct2ct_translation-B2D9/</a>. </p>
<blockquote>
<p>è´¨å­ç–—æ³•ä¸­çš„å‡†ç¡®å‰‚é‡è®¡ç®—ä¾èµ–äºé«˜è´¨é‡CTå›¾åƒã€‚è®¡åˆ’CTï¼ˆpCTï¼‰ä½œä¸ºå‰‚é‡è®¡åˆ’çš„å‚è€ƒï¼Œè€Œé”¥å½¢æŸCTï¼ˆCBCTï¼‰åœ¨è‡ªé€‚åº”æ”¾å°„æ²»ç–—ï¼ˆARTï¼‰ä¸­ç”¨äºç”Ÿæˆæ”¹è¿›å‰‚é‡è®¡ç®—çš„sCTã€‚å°½ç®¡CBCTå…·æœ‰æˆæœ¬ä½ã€è¾å°„æš´éœ²ä¼˜åŠ¿å°ç­‰ä¼˜ç‚¹ï¼Œä½†å®ƒå­˜åœ¨ä¸¥é‡çš„ä¼ªå½±å’Œå›¾åƒè´¨é‡å·®çš„é—®é¢˜ï¼Œå› æ­¤ä¸é€‚åˆè¿›è¡Œç²¾ç¡®çš„å‰‚é‡æµ‹å®šã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„CBCT-to-CTè½¬æ¢å·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å¼•å…¥è§£å‰–ç»“æ„ä¸ä¸€è‡´æ€§ï¼Œä¸”ç¼ºä¹å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠä¸Šçš„åº”ç”¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†STF-RUEï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†å…³é”®ç»„ä»¶çš„æ–°æ¡†æ¶ã€‚é¦–å…ˆï¼ŒSTFæ˜¯ä¸€ç§å—åˆ†å‰²æŒ‡å¯¼çš„CBCT-to-CTè½¬æ¢æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ä»pCTä¸­æå–çš„åˆ†å‰²å…ˆéªŒçŸ¥è¯†æ¥æé«˜è§£å‰–ç»“æ„çš„ä¸€è‡´æ€§ã€‚å…¶æ¬¡ï¼ŒRUEæ˜¯ä¸€ç§ç¬¦åˆé¢„æµ‹çš„æ–¹æ³•ï¼Œå®ƒä¸ºé¢„æµ‹çš„CTé…å¤‡åƒç´ çº§çš„é¢„æµ‹é—´éš”ï¼Œä¸ºä¸´åºŠåŒ»ç”Ÿæä¾›å¯é çš„å¯é æ€§æŒ‡æ ‡ã€‚åœ¨ä¸¤é¡¹åŸºå‡†æ•°æ®é›†ä¸Šä½¿ç”¨UNet++å’ŒFast-DDPMè¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒSTF-RUEåœ¨æ–°å‹è½¯ç»„ç»‡é’ˆå¯¹æ€§æŒ‡æ ‡è¡¡é‡ä¸‹æ˜¾è‘—æé«˜ç¿»è¯‘ç²¾åº¦ï¼Œè¯¥æŒ‡æ ‡ä¸“ä¸ºç²¾ç¡®å‰‚é‡è®¡ç®—è€Œè®¾è®¡ã€‚æ­¤å¤–ï¼ŒSTF-RUEä¸ºåˆæˆCTæä¾›æ›´å¥½çš„æ ¡å‡†ä¸ç¡®å®šæ€§é›†ï¼Œå¢å¼ºäº†å¯¹åˆæˆCTçš„ä¿¡ä»»ã€‚é€šè¿‡è§£å†³è§£å‰–ç»“æ„å¿ å®åº¦å’Œä¸ç¡®å®šæ€§é‡åŒ–é—®é¢˜ï¼ŒSTF-RUEæ ‡å¿—ç€è‡ªé€‚åº”è´¨å­ç–—æ³•æœç€æ›´å®‰å…¨ã€æ›´æœ‰æ•ˆçš„æ–¹å‘è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/cbct2ct_translation-B2D9/%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/cbct2ct_translation-B2D9/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08515v1">PDF</a> MICCAI 2025 Conference Submission. Follows the required LNCS format.   12 pages including references. Contains 4 figures and 1 table</p>
<p><strong>Summary</strong><br>     è´¨å­æ²»ç–—ä¸­çš„ç²¾ç¡®å‰‚é‡è®¡ç®—ä¾èµ–äºé«˜è´¨é‡çš„CTå›¾åƒã€‚åœ¨é€‚åº”æ€§æ”¾å°„æ²»ç–—ï¼ˆARTï¼‰ä¸­ï¼Œé”¥æŸCTï¼ˆCBCTï¼‰è¢«ç”¨äºç”ŸæˆsCTä»¥æ”¹è¿›å‰‚é‡è®¡ç®—ï¼Œå°½ç®¡å…¶æˆæœ¬è¾ƒä½ä¸”è¾å°„æš´éœ²å‡å°‘ï¼Œä½†å…¶å›¾åƒè´¨é‡å­˜åœ¨ä¸¥é‡ç¼ºé™·ï¼Œä¸é€‚å®œè¿›è¡Œç²¾ç¡®å‰‚é‡æµ‹å®šã€‚æ·±åº¦å­¦ä¹ åœ¨CBCTåˆ°CTçš„ç¿»è¯‘ä¸Šå±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å¸¸å¸¦æ¥è§£å‰–ç»“æ„ä¸ä¸€è‡´æ€§å’Œç¼ºä¹å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œé™åˆ¶äº†å…¶ä¸´åºŠåº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†STF-RUEæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ä¸¤é¡¹å…³é”®æŠ€æœ¯ï¼Œæå‡äº†è´¨å­æ²»ç–—çš„ç²¾å‡†åº¦å’Œå¯é æ€§ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<pre><code>* CBCTè™½æœ‰åŠ©äºå‰‚é‡è®¡ç®—ä½†å­˜åœ¨ä¸¥é‡å›¾åƒè´¨é‡ç¼ºé™·é—®é¢˜ï¼Œå¯¹ç²¾å‡†æ”¾å°„æ²»ç–—æ•ˆæœæœ‰å½±å“ã€‚ 
* å½“å‰ç”¨äºå¤„ç†CBCTåˆ°CTç¿»è¯‘çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯é¢ä¸´è§£å‰–ç»“æ„ä¸ä¸€è‡´æ€§å’Œä¸ç¡®å®šæ€§ä¼°è®¡çš„æŒ‘æˆ˜ã€‚ 
* STF-RUEæ¡†æ¶é€šè¿‡é›†æˆä¸¤é¡¹å…³é”®æŠ€æœ¯è§£å†³äº†ä¸Šè¿°é—®é¢˜ï¼ŒåŒ…æ‹¬åŸºäºåˆ†å‰²å¼•å¯¼çš„CBCTåˆ°CTç¿»è¯‘æ–¹æ³•å’ŒåŸºäºä¸€è‡´æ€§é¢„æµ‹çš„å¯é æ€§æŒ‡æ ‡ä¼°è®¡æ–¹æ³•ã€‚ 
* STF-RUEæ˜¾è‘—æé«˜äº†ç¿»è¯‘ç²¾åº¦å’Œå¯é æ€§æŒ‡æ ‡ï¼Œæ”¹å–„äº†åˆæˆCTçš„ä¸ç¡®å®šæ€§æ ¡å‡†ï¼Œå¯¹è´¨å­æ²»ç–—å®‰å…¨æœ‰æ•ˆæ€§å…·æœ‰å…³é”®æ€§å½±å“ã€‚è¿™ä¸€ç ”ç©¶æˆæœå·²ç»é€šè¿‡å®éªŒéªŒè¯ã€‚ â€‹â€‹
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08515">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-500efa926383894bb180f86afe434814.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bed1cbeb2a1dfc3c99e78c063d6a5212.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6879ddd99c5e2c361b5e97d3e35d12ef.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Data-Foundations-for-Large-Scale-Multimodal-Clinical-Foundation-Models"><a href="#Data-Foundations-for-Large-Scale-Multimodal-Clinical-Foundation-Models" class="headerlink" title="Data Foundations for Large Scale Multimodal Clinical Foundation Models"></a>Data Foundations for Large Scale Multimodal Clinical Foundation Models</h2><p><strong>Authors:Wei Dai, Peilin Chen, Malinda Lu, Daniel Li, Haowen Wei, Hejie Cui, Paul Pu Liang</strong></p>
<p>Recent advances in clinical AI have enabled remarkable progress across many clinical domains. However, existing benchmarks and models are primarily limited to a small set of modalities and tasks, which hinders the development of large-scale multimodal methods that can make holistic assessments of patient health and well-being. To bridge this gap, we introduce Clinical Large-Scale Integrative Multimodal Benchmark (CLIMB), a comprehensive clinical benchmark unifying diverse clinical data across imaging, language, temporal, and graph modalities. CLIMB comprises 4.51 million patient samples totaling 19.01 terabytes distributed across 2D imaging, 3D video, time series, graphs, and multimodal data. Through extensive empirical evaluation, we demonstrate that multitask pretraining significantly improves performance on understudied domains, achieving up to 29% improvement in ultrasound and 23% in ECG analysis over single-task learning. Pretraining on CLIMB also effectively improves modelsâ€™ generalization capability to new tasks, and strong unimodal encoder performance translates well to multimodal performance when paired with task-appropriate fusion strategies. Our findings provide a foundation for new architecture designs and pretraining strategies to advance clinical AI research. Code is released at <a target="_blank" rel="noopener" href="https://github.com/DDVD233/climb">https://github.com/DDVD233/climb</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œä¸´åºŠäººå·¥æ™ºèƒ½çš„è¿›æ­¥åœ¨è®¸å¤šä¸´åºŠé¢†åŸŸéƒ½å–å¾—äº†æ˜¾è‘—çš„æˆæ•ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹ä¸»è¦å±€é™äºå°‘æ•°å‡ ç§æ¨¡æ€å’Œä»»åŠ¡ï¼Œè¿™é˜»ç¢äº†èƒ½å¤Ÿå…¨é¢è¯„ä¼°æ‚£è€…å¥åº·å’Œç¦ç¥‰çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ–¹æ³•çš„å‘å±•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸´åºŠå¤§è§„æ¨¡ç»¼åˆå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼ˆCLIMBï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€äº†æˆåƒã€è¯­è¨€ã€æ—¶é—´å’Œå›¾å½¢ç­‰å¤šç§ä¸´åºŠæ•°æ®æ¨¡å¼çš„ç»¼åˆä¸´åºŠåŸºå‡†æµ‹è¯•ã€‚CLIMBåŒ…å«äº†è·¨è¶ŠäºŒç»´æˆåƒã€ä¸‰ç»´è§†é¢‘ã€æ—¶é—´åºåˆ—ã€å›¾å½¢å’Œå¤šæ¨¡æ€æ•°æ®çš„å…±è¾¾å››åƒäº”ç™¾ä¸€åä¸‡æ‚£è€…æ ·æœ¬æ•°æ®ï¼Œæ€»è®¡åä¹äº¿å­—èŠ‚æ•°æ®åˆ†å¸ƒã€‚é€šè¿‡å¹¿æ³›çš„å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬è¯æ˜äº†å¤šä»»åŠ¡é¢„è®­ç»ƒèƒ½å¤Ÿåœ¨ç¼ºä¹ç ”ç©¶çš„é¢†åŸŸé‡Œæ˜¾è‘—æé«˜æ€§èƒ½ï¼Œè¶…å£°æ³¢åˆ†æä¸­çš„æå‡ç‡é«˜è¾¾ç™¾åˆ†ä¹‹äºŒåä¹ï¼Œå¿ƒç”µå›¾åˆ†æä¸­è¾¾åˆ°ç™¾åˆ†ä¹‹äºŒåä¸‰äºå•ä»»åŠ¡å­¦ä¹ ä¹‹ä¸Šã€‚åœ¨CLIMBä¸Šè¿›è¡Œé¢„è®­ç»ƒè¿˜èƒ½æœ‰æ•ˆæé«˜æ¨¡å‹å¯¹æ–°ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¼ºå¤§çš„å•æ¨¡æ€ç¼–ç å™¨æ€§èƒ½åœ¨ä¸ä»»åŠ¡é€‚å½“çš„èåˆç­–ç•¥ç»“åˆæ—¶ï¼Œèƒ½å¾ˆå¥½åœ°è½¬åŒ–ä¸ºå¤šæ¨¡æ€æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ–°çš„æ¶æ„è®¾è®¡å’Œé¢„è®­ç»ƒç­–ç•¥æä¾›äº†åŸºç¡€ï¼Œä»¥æ¨åŠ¨ä¸´åºŠäººå·¥æ™ºèƒ½ç ”ç©¶çš„å‘å±•ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/DDVD233/climb%E4%B8%8A%E3%80%82">https://github.com/DDVD233/climbä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07667v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸´åºŠäººå·¥æ™ºèƒ½çš„æœ€æ–°è¿›å±•åŠå¤šæ¨¡æ€æ–¹æ³•çš„å±€é™æ€§ï¼Œä¸ºæ­¤æå‡ºäº†ä¸€ç§å¤§è§„æ¨¡çš„å¤šæ¨¡æ€ä¸´åºŠåŸºå‡†æµ‹è¯•ï¼ˆCLIMBï¼‰ï¼Œæ¶µç›–äº†å›¾åƒã€è¯­è¨€ã€æ—¶åºå’Œå›¾ç­‰å¤šç§ä¸´åºŠæ•°æ®æ¨¡æ€ã€‚é€šè¿‡å¤šä»»åŠ¡é¢„è®­ç»ƒï¼ŒCLIMBåœ¨è¶…å£°å’Œå¿ƒç”µå›¾åˆ†æç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æå‡æ•ˆæœã€‚æ­¤å¤–ï¼ŒCLIMBè¿˜ä¸ºä¸´åºŠäººå·¥æ™ºèƒ½ç ”ç©¶æä¾›äº†æ–°çš„æ¶æ„è®¾è®¡å’Œé¢„è®­ç»ƒç­–ç•¥çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸´åºŠäººå·¥æ™ºèƒ½åœ¨å¤šä¸ªé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å¤šæ¨¡æ€æ–¹æ³•çš„å¼€å‘å—é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ä¸´åºŠåŸºå‡†æµ‹è¯•CLIMBï¼Œæ¶µç›–å¤šç§ä¸´åºŠæ•°æ®æ¨¡æ€ã€‚</li>
<li>CLIMBåŒ…å«4.51ç™¾ä¸‡æ‚£è€…æ ·æœ¬ï¼Œæ€»è®¡19.01å¤ªå­—èŠ‚çš„æ•°æ®ã€‚</li>
<li>å¤šä»»åŠ¡é¢„è®­ç»ƒåœ¨CLIMBä¸Šèƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¶…å£°å’Œå¿ƒç”µå›¾åˆ†æé¢†åŸŸã€‚</li>
<li>é¢„è®­ç»ƒæœ‰åŠ©äºæ¨¡å‹å¯¹æ–°ä»»åŠ¡çš„é€‚åº”èƒ½åŠ›ã€‚</li>
<li>å¼ºå¤§çš„å•æ¨¡æ€ç¼–ç å™¨æ€§èƒ½ï¼Œåœ¨é…åˆé€‚å½“çš„èåˆç­–ç•¥æ—¶ï¼Œèƒ½å–å¾—è‰¯å¥½çš„å¤šæ¨¡æ€æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-719deba1687c0befc6f04e3b1af164af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a57dd9ae0a0380241033c2aa849e0488.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d84bf8e80c2d110cadd55dc18fd78e76.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-14\./crop_I2I Translation/2503.07667v1/page_3_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9816e50733bbd50466a98e12955c36b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-29ef902c04cb92589f4a1a7672a04812.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LLM-HDR-Bridging-LLM-based-Perception-and-Self-Supervision-for-Unpaired-LDR-to-HDR-Image-Reconstruction"><a href="#LLM-HDR-Bridging-LLM-based-Perception-and-Self-Supervision-for-Unpaired-LDR-to-HDR-Image-Reconstruction" class="headerlink" title="LLM-HDR: Bridging LLM-based Perception and Self-Supervision for Unpaired   LDR-to-HDR Image Reconstruction"></a>LLM-HDR: Bridging LLM-based Perception and Self-Supervision for Unpaired   LDR-to-HDR Image Reconstruction</h2><p><strong>Authors:Hrishav Bakul Barua, Kalin Stefanov, Lemuel Lai En Che, Abhinav Dhall, KokSheik Wong, Ganesh Krishnasamy</strong></p>
<p>The translation of Low Dynamic Range (LDR) to High Dynamic Range (HDR) images is an important computer vision task. There is a significant amount of research utilizing both conventional non-learning methods and modern data-driven approaches, focusing on using both single-exposed and multi-exposed LDR for HDR image reconstruction. However, most current state-of-the-art methods require high-quality paired {LDR,HDR} datasets for model training. In addition, there is limited literature on using unpaired datasets for this task, that is, the model learns a mapping between domains, i.e., {LDR,HDR}. This paper proposes LLM-HDR, a method that integrates the perception of Large Language Models (LLM) into a modified semantic- and cycle-consistent adversarial architecture that utilizes unpaired {LDR,HDR} datasets for training. The method introduces novel artifact- and exposure-aware generators to address visual artifact removal and an encoder and loss to address semantic consistency, another under-explored topic. LLM-HDR is the first to use an LLM for the {LDR,HDR} translation task in a self-supervised setup. The method achieves state-of-the-art performance across several benchmark datasets and reconstructs high-quality HDR images. The official website of this work is available at: <a target="_blank" rel="noopener" href="https://github.com/HrishavBakulBarua/LLM-HDR">https://github.com/HrishavBakulBarua/LLM-HDR</a> </p>
<blockquote>
<p>å°†ä½åŠ¨æ€èŒƒå›´ï¼ˆLDRï¼‰å›¾åƒç¿»è¯‘æˆé«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰å›¾åƒæ˜¯ä¸€é¡¹é‡è¦çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚è®¸å¤šç ”ç©¶éƒ½åˆ©ç”¨ä¼ ç»Ÿçš„éå­¦ä¹ æ–¹æ³•å’Œç°ä»£çš„æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œä¸“æ³¨äºä½¿ç”¨å•æ›å…‰å’Œå¤šæ›å…‰çš„LDRè¿›è¡ŒHDRå›¾åƒé‡å»ºã€‚ç„¶è€Œï¼Œç›®å‰å¤§å¤šæ•°æœ€å…ˆè¿›çš„æ–¹æ³•éƒ½éœ€è¦é«˜è´¨é‡æˆå¯¹çš„{LDRï¼ŒHDR}æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚æ­¤å¤–ï¼Œå…³äºä½¿ç”¨éé…å¯¹æ•°æ®é›†è¿›è¡Œæ­¤ä»»åŠ¡çš„æ–‡çŒ®å¾ˆå°‘ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ¨¡å‹åœ¨ä¸¤ä¸ªé¢†åŸŸä¹‹é—´è¿›è¡Œæ˜ å°„å­¦ä¹ ï¼Œå³{LDRï¼ŒHDR}ã€‚æœ¬æ–‡æå‡ºäº†LLM-HDRæ–¹æ³•ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ„ŸçŸ¥èƒ½åŠ›é›†æˆåˆ°ä¸€ä¸ªç»è¿‡ä¿®æ”¹çš„è¯­ä¹‰å’Œå¾ªç¯ä¸€è‡´çš„å¯¹æŠ—æ€§æ¶æ„ä¸­ï¼Œè¯¥æ¶æ„åˆ©ç”¨éé…å¯¹çš„{LDRï¼ŒHDR}æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ–°å‹çš„å»ä¼ªå½±å’Œæ›å…‰æ„ŸçŸ¥ç”Ÿæˆå™¨æ¥è§£å†³è§†è§‰ä¼ªå½±å»é™¤é—®é¢˜ï¼Œä»¥åŠä¸€ä¸ªè§£å†³è¯­ä¹‰ä¸€è‡´æ€§çš„ç¼–ç å™¨å’ŒæŸå¤±å‡½æ•°ï¼Œè¿™æ˜¯å¦ä¸€ä¸ªå°šæœªæ·±å…¥æ¢è®¨çš„ä¸»é¢˜ã€‚LLM-HDRæ˜¯ç¬¬ä¸€ä¸ªåœ¨è‡ªæˆ‘ç›‘ç£è®¾ç½®ä¸­ä½¿ç”¨LLMè¿›è¡Œ{LDRï¼ŒHDR}ç¿»è¯‘ä»»åŠ¡çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶é‡å»ºäº†é«˜è´¨é‡HDRå›¾åƒã€‚è¯¥å·¥ä½œçš„å®˜æ–¹ç½‘ç«™åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/HrishavBakulBarua/LLM-HDR">https://github.com/HrishavBakulBarua/LLM-HDR</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15068v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡æå‡ºä¸€ç§å°†ä½åŠ¨æ€èŒƒå›´ï¼ˆLDRï¼‰å›¾åƒç¿»è¯‘æˆé«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰å›¾åƒçš„æ–°æ–¹æ³•ï¼Œå³LLM-HDRã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œé‡‡ç”¨æ— é…å¯¹æ•°æ®é›†çš„è¯­ä¹‰ä¸€è‡´æ€§å¾ªç¯å¯¹æŠ—æ¶æ„è¿›è¡Œè®­ç»ƒã€‚å¼•å…¥æ–°é¢–çš„ä¼ªå½±æ„ŸçŸ¥å’Œæ›å…‰æ„ŸçŸ¥ç”Ÿæˆå™¨è§£å†³è§†è§‰ä¼ªå½±å»é™¤é—®é¢˜ï¼Œå¹¶é€šè¿‡ç¼–ç å™¨å’ŒæŸå¤±è§£å†³è¯­ä¹‰ä¸€è‡´æ€§é—®é¢˜ã€‚LLM-HDRé¦–æ¬¡åœ¨è‡ªç›‘ç£è®¾ç½®ä¸­åˆ©ç”¨LLMè¿›è¡ŒLDRåˆ°HDRçš„ç¿»è¯‘ä»»åŠ¡ï¼Œå®ç°äº†è·¨å¤šä¸ªåŸºå‡†æ•°æ®é›†çš„å“è¶Šæ€§èƒ½ï¼Œå¹¶èƒ½é‡å»ºé«˜è´¨é‡HDRå›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LLM-HDRæ˜¯ä¸€ç§æ–°çš„ä½åŠ¨æ€èŒƒå›´ï¼ˆLDRï¼‰åˆ°é«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰å›¾åƒç¿»è¯‘æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ„ŸçŸ¥èƒ½åŠ›è¿›è¡Œå›¾åƒç¿»è¯‘ã€‚</li>
<li>LLM-HDRé‡‡ç”¨æ— é…å¯¹æ•°æ®é›†çš„è¯­ä¹‰ä¸€è‡´æ€§å¾ªç¯å¯¹æŠ—æ¶æ„è¿›è¡Œè®­ç»ƒã€‚</li>
<li>è¯¥æ–¹æ³•å¼•å…¥äº†ä¼ªå½±æ„ŸçŸ¥å’Œæ›å…‰æ„ŸçŸ¥ç”Ÿæˆå™¨ä»¥è§£å†³è§†è§‰ä¼ªå½±å»é™¤é—®é¢˜ã€‚</li>
<li>LLM-HDRé¦–æ¬¡åœ¨è‡ªç›‘ç£è®¾ç½®ä¸­ä½¿ç”¨LLMè¿›è¡ŒHDRå›¾åƒé‡å»ºã€‚</li>
<li>LLM-HDRå®ç°äº†è·¨å¤šä¸ªåŸºå‡†æ•°æ®é›†çš„å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed0943f3f67423ae0c92ddaeb82166ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c796c8b662950542046b576ea2142a76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f7d1f0997d46770f2f236f98bb63377.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Towards-Zero-Shot-Multimodal-Machine-Translation"><a href="#Towards-Zero-Shot-Multimodal-Machine-Translation" class="headerlink" title="Towards Zero-Shot Multimodal Machine Translation"></a>Towards Zero-Shot Multimodal Machine Translation</h2><p><strong>Authors:Matthieu Futeral, Cordelia Schmid, BenoÃ®t Sagot, Rachel Bawden</strong></p>
<p>Current multimodal machine translation (MMT) systems rely on fully supervised data (i.e models are trained on sentences with their translations and accompanying images). However, this type of data is costly to collect, limiting the extension of MMT to other language pairs for which such data does not exist. In this work, we propose a method to bypass the need for fully supervised data to train MMT systems, using multimodal English data only. Our method, called ZeroMMT, consists in adapting a strong text-only machine translation (MT) model by training it on a mixture of two objectives: visually conditioned masked language modelling and the Kullback-Leibler divergence between the original and new MMT outputs. We evaluate on standard MMT benchmarks and the recently released CoMMuTE, a contrastive benchmark aiming to evaluate how well models use images to disambiguate English sentences. We obtain disambiguation performance close to state-of-the-art MMT models trained additionally on fully supervised examples. To prove that our method generalizes to languages with no fully supervised training data available, we extend the CoMMuTE evaluation dataset to three new languages: Arabic, Russian and Chinese. We further show that we can control the trade-off between disambiguation capabilities and translation fidelity at inference time using classifier-free guidance and without any additional data. Our code, data and trained models are publicly accessible. </p>
<blockquote>
<p>å½“å‰çš„å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ï¼ˆMMTï¼‰ç³»ç»Ÿä¾èµ–äºå®Œå…¨ç›‘ç£çš„æ•°æ®ï¼ˆå³æ¨¡å‹åœ¨å…·æœ‰ç¿»è¯‘å’Œç›¸åº”å›¾åƒçš„å¥å­ä¸Šè¿›è¡Œè®­ç»ƒï¼‰ã€‚ç„¶è€Œï¼Œè¿™ç§ç±»å‹çš„æ•°æ®æ”¶é›†æˆæœ¬å¾ˆé«˜ï¼Œé™åˆ¶äº†MMTåœ¨å…¶ä»–æ²¡æœ‰æ­¤ç±»æ•°æ®å­˜åœ¨çš„è¯­è¨€é…å¯¹ä¸­çš„æ‰©å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œç»•è¿‡å¯¹å®Œå…¨ç›‘ç£æ•°æ®è®­ç»ƒMMTç³»ç»Ÿçš„éœ€æ±‚ï¼Œä»…ä½¿ç”¨å¤šæ¨¡æ€è‹±è¯­æ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç§°ä¸ºZeroMMTï¼Œé€šè¿‡å¯¹å¼ºå¤§çš„çº¯æ–‡æœ¬æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰æ¨¡å‹è¿›è¡Œé€‚åº”ï¼Œä»¥ä¸¤ä¸ªç›®æ ‡è¿›è¡Œè®­ç»ƒï¼šè§†è§‰æ¡ä»¶ä¸‹çš„æ©ç è¯­è¨€å»ºæ¨¡å’ŒåŸå§‹ä¸æ–°çš„MMTè¾“å‡ºä¹‹é—´çš„Kullback-Leibleræ•£åº¦ã€‚æˆ‘ä»¬åœ¨æ ‡å‡†MMTåŸºå‡†æµ‹è¯•å’Œæœ€è¿‘å‘å¸ƒçš„CoMMuTEä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒCoMMuTEæ˜¯ä¸€ä¸ªå¯¹æ¯”åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹ä½¿ç”¨å›¾åƒå¯¹è‹±æ–‡å¥å­è¿›è¡Œæ¶ˆæ­§çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è·å¾—çš„æ¶ˆæ­§æ€§èƒ½æ¥è¿‘åœ¨å®Œå…¨ç›‘ç£ç¤ºä¾‹ä¸Šè®­ç»ƒçš„æœ€æ–°MMTæ¨¡å‹ã€‚ä¸ºäº†è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°æ²¡æœ‰å®Œå…¨ç›‘ç£è®­ç»ƒæ•°æ®å¯ç”¨çš„è¯­è¨€ï¼Œæˆ‘ä»¬å°†CoMMuTEè¯„ä¼°æ•°æ®é›†æ‰©å±•åˆ°ä¸‰ç§æ–°è¯­è¨€ï¼šé˜¿æ‹‰ä¼¯è¯­ã€ä¿„è¯­å’Œä¸­æ–‡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ¨ç†æ—¶é—´ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼å’Œæ— éœ€ä»»ä½•é¢å¤–æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ§åˆ¶æ¶ˆæ­§èƒ½åŠ›å’Œç¿»è¯‘å¿ å®åº¦ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œè®­ç»ƒæ¨¡å‹å¯å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.13579v2">PDF</a> NAACL 2025 (Findings)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸éœ€è¦å…¨ç›‘ç£æ•°æ®è®­ç»ƒå¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ç³»ç»Ÿçš„æ–¹æ³•ï¼Œåä¸ºZeroMMTã€‚è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒå¼ºå¤§çš„çº¯æ–‡æœ¬æœºå™¨ç¿»è¯‘æ¨¡å‹ï¼Œå®ç°ä¸¤ç§ç›®æ ‡æ··åˆï¼šè§†è§‰æ¡ä»¶ä¸‹çš„æ©ç è¯­è¨€å»ºæ¨¡å’ŒåŸå§‹ä¸æ–°å‹å¤šæ¨¡æ€ç¿»è¯‘è¾“å‡ºä¹‹é—´çš„Kullback-Leibleræ•£åº¦ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å‡†å¤šæ¨¡æ€ç¿»è¯‘åŸºå‡†æµ‹è¯•é›†å’Œæœ€æ–°å‘å¸ƒçš„æ—¨åœ¨è¯„ä¼°æ¨¡å‹å¦‚ä½•åˆ©ç”¨å›¾åƒæ¶ˆé™¤è‹±è¯­å¥å­æ­§ä¹‰çš„å¯¹æ¯”åŸºå‡†æµ‹è¯•é›†CoMMuTEä¸Šçš„è¡¨ç°æ¥è¿‘ä½¿ç”¨å…¨ç›‘ç£ç¤ºä¾‹è®­ç»ƒçš„å¤šæ¨¡æ€ç¿»è¯‘æ¨¡å‹çš„æœ€å…ˆè¿›æ°´å¹³ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯æ˜è¯¥æ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°æ²¡æœ‰å…¨ç›‘ç£è®­ç»ƒæ•°æ®å¯ç”¨çš„è¯­è¨€ï¼Œæˆ‘ä»¬å°†CoMMuTEè¯„ä¼°æ•°æ®é›†æ‰©å±•åˆ°äº†ä¸‰ç§æ–°è¯­è¨€ï¼šé˜¿æ‹‰ä¼¯è¯­ã€ä¿„è¯­å’Œä¸­æ–‡ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨æ¨ç†æ—¶é—´ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ— åˆ†ç±»å™¨å¼•å¯¼çš„æ–¹å¼ï¼Œæ— éœ€ä»»ä½•é¢å¤–æ•°æ®æ¥æ§åˆ¶æ¶ˆé™¤æ­§ä¹‰èƒ½åŠ›å’Œç¿»è¯‘å¿ å®åº¦ä¹‹é—´çš„æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰çš„å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ç³»ç»Ÿä¾èµ–äºå…¨ç›‘ç£æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ä¸”é™åˆ¶äº†å…¶åœ¨æ²¡æœ‰æ­¤ç±»æ•°æ®çš„è¯­è¨€å¯¹çš„æ‰©å±•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºZeroMMTçš„æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒå¼ºå¤§çš„çº¯æ–‡æœ¬æœºå™¨ç¿»è¯‘æ¨¡å‹æ¥é€‚åº”å¤šæ¨¡æ€ç¿»è¯‘ï¼Œæ— éœ€å…¨ç›‘ç£æ•°æ®ã€‚</li>
<li>ZeroMMTæ–¹æ³•ç»“åˆäº†ä¸¤ç§ç›®æ ‡ï¼šè§†è§‰æ¡ä»¶ä¸‹çš„æ©ç è¯­è¨€å»ºæ¨¡å’ŒåŸå§‹ä¸æ–°å‹å¤šæ¨¡æ€ç¿»è¯‘è¾“å‡ºä¹‹é—´çš„Kullback-Leibleræ•£åº¦ã€‚</li>
<li>åœ¨æ ‡å‡†å¤šæ¨¡æ€ç¿»è¯‘åŸºå‡†æµ‹è¯•é›†å’ŒCoMMuTEè¯„ä¼°æ•°æ®é›†ä¸Šï¼ŒZeroMMTçš„è¡¨ç°æ¥è¿‘æœ€å…ˆè¿›æ°´å¹³ã€‚</li>
<li>ZeroMMTæ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°æ²¡æœ‰å…¨ç›‘ç£è®­ç»ƒæ•°æ®çš„è¯­è¨€ï¼Œå¦‚é˜¿æ‹‰ä¼¯è¯­ã€ä¿„è¯­å’Œä¸­æ–‡ã€‚</li>
<li>é€šè¿‡æ— åˆ†ç±»å™¨å¼•å¯¼çš„æ–¹å¼ï¼Œå¯ä»¥åœ¨æ¨ç†æ—¶é—´æ§åˆ¶æ¶ˆé™¤æ­§ä¹‰èƒ½åŠ›å’Œç¿»è¯‘å¿ å®åº¦ä¹‹é—´çš„æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.13579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4029089afd410537fc1e7b6a76c39b7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d2987f3d40fc2b69d037cac28297811.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a114a08cd327cd33dbc03616a84b252.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29829cea5f32a96518940e4839d7709a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-639dc6f9f753fd6a66ba67eca0370368.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-074533f0c8d39cf0f33b3375a3f5be3d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Computing-high-dimensional-optimal-transport-by-flow-neural-networks"><a href="#Computing-high-dimensional-optimal-transport-by-flow-neural-networks" class="headerlink" title="Computing high-dimensional optimal transport by flow neural networks"></a>Computing high-dimensional optimal transport by flow neural networks</h2><p><strong>Authors:Chen Xu, Xiuyuan Cheng, Yao Xie</strong></p>
<p>Computing optimal transport (OT) for general high-dimensional data has been a long-standing challenge. Despite much progress, most of the efforts including neural network methods have been focused on the static formulation of the OT problem. The current work proposes to compute the dynamic OT between two arbitrary distributions $P$ and $Q$ by optimizing a flow model, where both distributions are only accessible via finite samples. Our method learns the dynamic OT by finding an invertible flow that minimizes the transport cost. The trained optimal transport flow subsequently allows for performing many downstream tasks, including infinitesimal density ratio estimation (DRE) and domain adaptation by interpolating distributions in the latent space. The effectiveness of the proposed model on high-dimensional data is demonstrated by strong empirical performance on OT baselines, image-to-image translation, and high-dimensional DRE. </p>
<blockquote>
<p>è®¡ç®—é€šç”¨é«˜ç»´æ•°æ®çš„æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰ä¸€ç›´æ˜¯ä¸€ä¸ªé•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ã€‚å°½ç®¡å–å¾—äº†è¯¸å¤šè¿›å±•ï¼Œä½†åŒ…æ‹¬ç¥ç»ç½‘ç»œæ–¹æ³•åœ¨å†…çš„å¤§å¤šæ•°åŠªåŠ›éƒ½é›†ä¸­åœ¨OTé—®é¢˜çš„é™æ€å…¬å¼ä¸Šã€‚å½“å‰çš„å·¥ä½œæå‡ºäº†ä¸€ç§é€šè¿‡ä¼˜åŒ–æµæ¨¡å‹æ¥è®¡ç®—ä¸¤ä¸ªä»»æ„åˆ†å¸ƒ$P$å’Œ$Q$ä¹‹é—´çš„åŠ¨æ€OTçš„æ–¹æ³•ï¼Œè¿™ä¸¤ä¸ªåˆ†å¸ƒåªèƒ½é€šè¿‡æœ‰é™æ ·æœ¬è®¿é—®ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å­¦ä¹ åŠ¨æ€OTï¼Œæ‰¾åˆ°ä¸€ä¸ªå¯é€†æµï¼Œä»¥æœ€å°åŒ–ä¼ è¾“æˆæœ¬ã€‚è®­ç»ƒåçš„æœ€ä¼˜ä¼ è¾“æµéšåå¯æ‰§è¡Œè®¸å¤šä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ— ç©·å°å¯†åº¦æ¯”ç‡ä¼°è®¡ï¼ˆDREï¼‰å’Œé€šè¿‡æ½œåœ¨ç©ºé—´ä¸­çš„åˆ†å¸ƒæ’å€¼è¿›è¡ŒåŸŸé€‚åº”ã€‚æ‰€æå‡ºæ¨¡å‹åœ¨é«˜ç»´æ•°æ®ä¸Šçš„æœ‰æ•ˆæ€§å·²é€šè¿‡OTåŸºå‡†æµ‹è¯•ã€å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘å’Œé«˜ç»´DREçš„å¼ºå¤§å®è¯æ€§èƒ½å¾—åˆ°è¯æ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.11857v5">PDF</a> Accepted by AISTATS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŠ¨æ€æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰çš„è®¡ç®—æ–¹æ³•ï¼Œç”¨äºè®¡ç®—ä¸¤ä¸ªä»»æ„åˆ†å¸ƒPå’ŒQä¹‹é—´çš„åŠ¨æ€OTã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–æµæ¨¡å‹æ¥å­¦ä¹ åŠ¨æ€OTï¼Œå…¶ä¸­ä¸¤ä¸ªåˆ†å¸ƒä»…é€šè¿‡æœ‰é™æ ·æœ¬è®¿é—®ã€‚è®­ç»ƒåçš„æœ€ä¼˜ä¼ è¾“æµå¯ä»¥è¿›è¡Œè®¸å¤šä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ— ç©·å¯†åº¦æ¯”ç‡ä¼°è®¡ï¼ˆDREï¼‰å’ŒåŸŸé€‚åº”ï¼Œé€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­å¯¹åˆ†å¸ƒè¿›è¡Œæ’å€¼ã€‚è¯¥æ¨¡å‹åœ¨é«˜ç»´æ•°æ®ä¸Šçš„æœ‰æ•ˆæ€§é€šè¿‡åœ¨å¯¹OTåŸºçº¿ã€å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘å’Œé«˜ç»´DREä¸Šçš„å¼ºå¤§ç»éªŒè¡¨ç°å¾—åˆ°è¯æ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†è®¡ç®—ä¸¤ä¸ªä»»æ„åˆ†å¸ƒé—´åŠ¨æ€æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–æµæ¨¡å‹æ¥å­¦ä¹ åŠ¨æ€OTã€‚</li>
<li>åˆ†å¸ƒä»…é€šè¿‡æœ‰é™æ ·æœ¬è¿›è¡Œè®¿é—®ã€‚</li>
<li>è®­ç»ƒåçš„æœ€ä¼˜ä¼ è¾“æµå¯ä»¥è¿›è¡Œå¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚æ— ç©·å¯†åº¦æ¯”ç‡ä¼°è®¡ï¼ˆDREï¼‰å’ŒåŸŸé€‚åº”ã€‚</li>
<li>æ’å€¼åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„åˆ†å¸ƒæœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨é«˜ç»´æ•°æ®ä¸Šçš„æœ‰æ•ˆæ€§å¾—åˆ°äº†å¼ºæœ‰åŠ›çš„å®è¯è¯æ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2305.11857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c60431ecbff06cbaed5fb309ab037c80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d68d4418286b70b4a62e3d0fcd7c3e04.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b657b10c8d2e5e2c6f07564b881637c9.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Memory-enhanced Retrieval Augmentation for Long Video Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b5713d02847055507997eb5be3d3f6db.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Membership Inference Attacks fueled by Few-Short Learning to detect   privacy leakage tackling data integrity
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23667.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
